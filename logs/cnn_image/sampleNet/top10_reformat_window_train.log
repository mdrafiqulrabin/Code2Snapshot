
DataLoader:

Started trainloader.
trainset = #10000
Finished trainloader.
Started valloader.
valset = #4847
Finished valloader.

Configuration:

DB_NAME = java-top10, IMG_TYPE = reformat_window, TRANS_SIZE = 1024
MAX_EPOCH = 100, BATCH_SIZE = 64

device = cuda:0



Started training epoch-1


Training epoch-1 batch-1
Running loss of epoch-1 batch-1 = 0.03608440235257149

Training epoch-1 batch-2
Running loss of epoch-1 batch-2 = 0.03605492040514946

Training epoch-1 batch-3
Running loss of epoch-1 batch-3 = 0.03606109321117401

Training epoch-1 batch-4
Running loss of epoch-1 batch-4 = 0.03563206270337105

Training epoch-1 batch-5
Running loss of epoch-1 batch-5 = 0.03643210977315903

Training epoch-1 batch-6
Running loss of epoch-1 batch-6 = 0.03600231185555458

Training epoch-1 batch-7
Running loss of epoch-1 batch-7 = 0.03651026263833046

Training epoch-1 batch-8
Running loss of epoch-1 batch-8 = 0.03623824939131737

Training epoch-1 batch-9
Running loss of epoch-1 batch-9 = 0.03626599162817001

Training epoch-1 batch-10
Running loss of epoch-1 batch-10 = 0.03668717294931412

Training epoch-1 batch-11
Running loss of epoch-1 batch-11 = 0.03598660230636597

Training epoch-1 batch-12
Running loss of epoch-1 batch-12 = 0.03593307361006737

Training epoch-1 batch-13
Running loss of epoch-1 batch-13 = 0.03685927763581276

Training epoch-1 batch-14
Running loss of epoch-1 batch-14 = 0.03640725463628769

Training epoch-1 batch-15
Running loss of epoch-1 batch-15 = 0.036159172654151917

Training epoch-1 batch-16
Running loss of epoch-1 batch-16 = 0.03639722242951393

Training epoch-1 batch-17
Running loss of epoch-1 batch-17 = 0.03616051748394966

Training epoch-1 batch-18
Running loss of epoch-1 batch-18 = 0.036086685955524445

Training epoch-1 batch-19
Running loss of epoch-1 batch-19 = 0.03597337007522583

Training epoch-1 batch-20
Running loss of epoch-1 batch-20 = 0.03614350035786629

Training epoch-1 batch-21
Running loss of epoch-1 batch-21 = 0.03603333234786987

Training epoch-1 batch-22
Running loss of epoch-1 batch-22 = 0.03595750778913498

Training epoch-1 batch-23
Running loss of epoch-1 batch-23 = 0.03615415096282959

Training epoch-1 batch-24
Running loss of epoch-1 batch-24 = 0.03589191287755966

Training epoch-1 batch-25
Running loss of epoch-1 batch-25 = 0.03603821247816086

Training epoch-1 batch-26
Running loss of epoch-1 batch-26 = 0.03601793199777603

Training epoch-1 batch-27
Running loss of epoch-1 batch-27 = 0.03574682027101517

Training epoch-1 batch-28
Running loss of epoch-1 batch-28 = 0.03602319583296776

Training epoch-1 batch-29
Running loss of epoch-1 batch-29 = 0.0359308160841465

Training epoch-1 batch-30
Running loss of epoch-1 batch-30 = 0.035853203386068344

Training epoch-1 batch-31
Running loss of epoch-1 batch-31 = 0.0362236350774765

Training epoch-1 batch-32
Running loss of epoch-1 batch-32 = 0.03575927019119263

Training epoch-1 batch-33
Running loss of epoch-1 batch-33 = 0.035889096558094025

Training epoch-1 batch-34
Running loss of epoch-1 batch-34 = 0.03577509894967079

Training epoch-1 batch-35
Running loss of epoch-1 batch-35 = 0.035941123962402344

Training epoch-1 batch-36
Running loss of epoch-1 batch-36 = 0.03609801083803177

Training epoch-1 batch-37
Running loss of epoch-1 batch-37 = 0.036324504762887955

Training epoch-1 batch-38
Running loss of epoch-1 batch-38 = 0.03605464845895767

Training epoch-1 batch-39
Running loss of epoch-1 batch-39 = 0.03587498143315315

Training epoch-1 batch-40
Running loss of epoch-1 batch-40 = 0.035739924758672714

Training epoch-1 batch-41
Running loss of epoch-1 batch-41 = 0.03640826418995857

Training epoch-1 batch-42
Running loss of epoch-1 batch-42 = 0.036307722330093384

Training epoch-1 batch-43
Running loss of epoch-1 batch-43 = 0.03604626655578613

Training epoch-1 batch-44
Running loss of epoch-1 batch-44 = 0.0363776832818985

Training epoch-1 batch-45
Running loss of epoch-1 batch-45 = 0.03582038730382919

Training epoch-1 batch-46
Running loss of epoch-1 batch-46 = 0.03613842651247978

Training epoch-1 batch-47
Running loss of epoch-1 batch-47 = 0.03599804267287254

Training epoch-1 batch-48
Running loss of epoch-1 batch-48 = 0.03603260964155197

Training epoch-1 batch-49
Running loss of epoch-1 batch-49 = 0.03601706027984619

Training epoch-1 batch-50
Running loss of epoch-1 batch-50 = 0.03579195588827133

Training epoch-1 batch-51
Running loss of epoch-1 batch-51 = 0.03594295680522919

Training epoch-1 batch-52
Running loss of epoch-1 batch-52 = 0.035701800137758255

Training epoch-1 batch-53
Running loss of epoch-1 batch-53 = 0.036045171320438385

Training epoch-1 batch-54
Running loss of epoch-1 batch-54 = 0.036031126976013184

Training epoch-1 batch-55
Running loss of epoch-1 batch-55 = 0.0360199436545372

Training epoch-1 batch-56
Running loss of epoch-1 batch-56 = 0.035849303007125854

Training epoch-1 batch-57
Running loss of epoch-1 batch-57 = 0.03610280156135559

Training epoch-1 batch-58
Running loss of epoch-1 batch-58 = 0.03565400093793869

Training epoch-1 batch-59
Running loss of epoch-1 batch-59 = 0.036055486649274826

Training epoch-1 batch-60
Running loss of epoch-1 batch-60 = 0.03593965619802475

Training epoch-1 batch-61
Running loss of epoch-1 batch-61 = 0.03586583212018013

Training epoch-1 batch-62
Running loss of epoch-1 batch-62 = 0.035765960812568665

Training epoch-1 batch-63
Running loss of epoch-1 batch-63 = 0.03622153028845787

Training epoch-1 batch-64
Running loss of epoch-1 batch-64 = 0.03602326288819313

Training epoch-1 batch-65
Running loss of epoch-1 batch-65 = 0.03564981743693352

Training epoch-1 batch-66
Running loss of epoch-1 batch-66 = 0.036103371530771255

Training epoch-1 batch-67
Running loss of epoch-1 batch-67 = 0.03594062104821205

Training epoch-1 batch-68
Running loss of epoch-1 batch-68 = 0.03584558889269829

Training epoch-1 batch-69
Running loss of epoch-1 batch-69 = 0.035894572734832764

Training epoch-1 batch-70
Running loss of epoch-1 batch-70 = 0.03578651696443558

Training epoch-1 batch-71
Running loss of epoch-1 batch-71 = 0.03601386770606041

Training epoch-1 batch-72
Running loss of epoch-1 batch-72 = 0.03571747615933418

Training epoch-1 batch-73
Running loss of epoch-1 batch-73 = 0.03634096682071686

Training epoch-1 batch-74
Running loss of epoch-1 batch-74 = 0.03613243252038956

Training epoch-1 batch-75
Running loss of epoch-1 batch-75 = 0.03594442456960678

Training epoch-1 batch-76
Running loss of epoch-1 batch-76 = 0.03614441305398941

Training epoch-1 batch-77
Running loss of epoch-1 batch-77 = 0.03586952015757561

Training epoch-1 batch-78
Running loss of epoch-1 batch-78 = 0.03565933555364609

Training epoch-1 batch-79
Running loss of epoch-1 batch-79 = 0.03589324653148651

Training epoch-1 batch-80
Running loss of epoch-1 batch-80 = 0.036052584648132324

Training epoch-1 batch-81
Running loss of epoch-1 batch-81 = 0.03610001504421234

Training epoch-1 batch-82
Running loss of epoch-1 batch-82 = 0.03601830080151558

Training epoch-1 batch-83
Running loss of epoch-1 batch-83 = 0.0363442488014698

Training epoch-1 batch-84
Running loss of epoch-1 batch-84 = 0.036188289523124695

Training epoch-1 batch-85
Running loss of epoch-1 batch-85 = 0.03596147149801254

Training epoch-1 batch-86
Running loss of epoch-1 batch-86 = 0.036331988871097565

Training epoch-1 batch-87
Running loss of epoch-1 batch-87 = 0.03607696667313576

Training epoch-1 batch-88
Running loss of epoch-1 batch-88 = 0.03615245968103409

Training epoch-1 batch-89
Running loss of epoch-1 batch-89 = 0.03633571416139603

Training epoch-1 batch-90
Running loss of epoch-1 batch-90 = 0.03598406910896301

Training epoch-1 batch-91
Running loss of epoch-1 batch-91 = 0.03625808283686638

Training epoch-1 batch-92
Running loss of epoch-1 batch-92 = 0.03620100021362305

Training epoch-1 batch-93
Running loss of epoch-1 batch-93 = 0.036447856575250626

Training epoch-1 batch-94
Running loss of epoch-1 batch-94 = 0.03607643023133278

Training epoch-1 batch-95
Running loss of epoch-1 batch-95 = 0.0359988808631897

Training epoch-1 batch-96
Running loss of epoch-1 batch-96 = 0.035724982619285583

Training epoch-1 batch-97
Running loss of epoch-1 batch-97 = 0.03640086576342583

Training epoch-1 batch-98
Running loss of epoch-1 batch-98 = 0.03613229840993881

Training epoch-1 batch-99
Running loss of epoch-1 batch-99 = 0.03591432049870491

Training epoch-1 batch-100
Running loss of epoch-1 batch-100 = 0.036027587950229645

Training epoch-1 batch-101
Running loss of epoch-1 batch-101 = 0.03591661900281906

Training epoch-1 batch-102
Running loss of epoch-1 batch-102 = 0.03602704778313637

Training epoch-1 batch-103
Running loss of epoch-1 batch-103 = 0.03574388474225998

Training epoch-1 batch-104
Running loss of epoch-1 batch-104 = 0.036070797592401505

Training epoch-1 batch-105
Running loss of epoch-1 batch-105 = 0.036028873175382614

Training epoch-1 batch-106
Running loss of epoch-1 batch-106 = 0.03578555956482887

Training epoch-1 batch-107
Running loss of epoch-1 batch-107 = 0.035972241312265396

Training epoch-1 batch-108
Running loss of epoch-1 batch-108 = 0.0361407995223999

Training epoch-1 batch-109
Running loss of epoch-1 batch-109 = 0.03603360056877136

Training epoch-1 batch-110
Running loss of epoch-1 batch-110 = 0.03627372533082962

Training epoch-1 batch-111
Running loss of epoch-1 batch-111 = 0.036174822598695755

Training epoch-1 batch-112
Running loss of epoch-1 batch-112 = 0.03600749000906944

Training epoch-1 batch-113
Running loss of epoch-1 batch-113 = 0.03621474653482437

Training epoch-1 batch-114
Running loss of epoch-1 batch-114 = 0.036180198192596436

Training epoch-1 batch-115
Running loss of epoch-1 batch-115 = 0.0360848568379879

Training epoch-1 batch-116
Running loss of epoch-1 batch-116 = 0.03616495430469513

Training epoch-1 batch-117
Running loss of epoch-1 batch-117 = 0.03581525757908821

Training epoch-1 batch-118
Running loss of epoch-1 batch-118 = 0.03615863248705864

Training epoch-1 batch-119
Running loss of epoch-1 batch-119 = 0.035746242851018906

Training epoch-1 batch-120
Running loss of epoch-1 batch-120 = 0.03609608858823776

Training epoch-1 batch-121
Running loss of epoch-1 batch-121 = 0.03594307601451874

Training epoch-1 batch-122
Running loss of epoch-1 batch-122 = 0.03594811260700226

Training epoch-1 batch-123
Running loss of epoch-1 batch-123 = 0.036014046519994736

Training epoch-1 batch-124
Running loss of epoch-1 batch-124 = 0.03605560585856438

Training epoch-1 batch-125
Running loss of epoch-1 batch-125 = 0.035924945026636124

Training epoch-1 batch-126
Running loss of epoch-1 batch-126 = 0.03607217222452164

Training epoch-1 batch-127
Running loss of epoch-1 batch-127 = 0.036062002182006836

Training epoch-1 batch-128
Running loss of epoch-1 batch-128 = 0.036126263439655304

Training epoch-1 batch-129
Running loss of epoch-1 batch-129 = 0.03598816320300102

Training epoch-1 batch-130
Running loss of epoch-1 batch-130 = 0.0357995368540287

Training epoch-1 batch-131
Running loss of epoch-1 batch-131 = 0.036277249455451965

Training epoch-1 batch-132
Running loss of epoch-1 batch-132 = 0.035974569618701935

Training epoch-1 batch-133
Running loss of epoch-1 batch-133 = 0.03600110858678818

Training epoch-1 batch-134
Running loss of epoch-1 batch-134 = 0.03619282320141792

Training epoch-1 batch-135
Running loss of epoch-1 batch-135 = 0.036008138209581375

Training epoch-1 batch-136
Running loss of epoch-1 batch-136 = 0.035779427736997604

Training epoch-1 batch-137
Running loss of epoch-1 batch-137 = 0.035922821611166

Training epoch-1 batch-138
Running loss of epoch-1 batch-138 = 0.03593282401561737

Training epoch-1 batch-139
Running loss of epoch-1 batch-139 = 0.03612438216805458

Training epoch-1 batch-140
Running loss of epoch-1 batch-140 = 0.035787634551525116

Training epoch-1 batch-141
Running loss of epoch-1 batch-141 = 0.036048587411642075

Training epoch-1 batch-142
Running loss of epoch-1 batch-142 = 0.035903338342905045

Training epoch-1 batch-143
Running loss of epoch-1 batch-143 = 0.03612938150763512

Training epoch-1 batch-144
Running loss of epoch-1 batch-144 = 0.035808369517326355

Training epoch-1 batch-145
Running loss of epoch-1 batch-145 = 0.03597636520862579

Training epoch-1 batch-146
Running loss of epoch-1 batch-146 = 0.03584147244691849

Training epoch-1 batch-147
Running loss of epoch-1 batch-147 = 0.03604651987552643

Training epoch-1 batch-148
Running loss of epoch-1 batch-148 = 0.03596411645412445

Training epoch-1 batch-149
Running loss of epoch-1 batch-149 = 0.03615172579884529

Training epoch-1 batch-150
Running loss of epoch-1 batch-150 = 0.03600885346531868

Training epoch-1 batch-151
Running loss of epoch-1 batch-151 = 0.03590742126107216

Training epoch-1 batch-152
Running loss of epoch-1 batch-152 = 0.03610313683748245

Training epoch-1 batch-153
Running loss of epoch-1 batch-153 = 0.03590806946158409

Training epoch-1 batch-154
Running loss of epoch-1 batch-154 = 0.035930559039115906

Training epoch-1 batch-155
Running loss of epoch-1 batch-155 = 0.036058370023965836

Training epoch-1 batch-156
Running loss of epoch-1 batch-156 = 0.03604916110634804

Training epoch-1 batch-157
Running loss of epoch-1 batch-157 = 0.14416159689426422

Finished training epoch-1.



Average train loss at epoch-1 = 0.03620909862518311

Started Evaluation

Average val loss at epoch-1 = 2.3204361733637358

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 0.00 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 0.00 %
Accuracy for class toString is: 0.00 %
Accuracy for class run is: 0.00 %
Accuracy for class hashCode is: 39.70 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 0.00 %
Accuracy for class get is: 100.00 %

Overall Accuracy = 10.23 %


Best Accuracy = 10.23 % at Epoch-1


Saving model after best epoch-1
Finished Evaluation



Started training epoch-2


Training epoch-2 batch-1
Running loss of epoch-2 batch-1 = 0.03597179427742958

Training epoch-2 batch-2
Running loss of epoch-2 batch-2 = 0.035975854843854904

Training epoch-2 batch-3
Running loss of epoch-2 batch-3 = 0.03625079244375229

Training epoch-2 batch-4
Running loss of epoch-2 batch-4 = 0.036131203174591064

Training epoch-2 batch-5
Running loss of epoch-2 batch-5 = 0.03603789210319519

Training epoch-2 batch-6
Running loss of epoch-2 batch-6 = 0.035912223160266876

Training epoch-2 batch-7
Running loss of epoch-2 batch-7 = 0.036070846021175385

Training epoch-2 batch-8
Running loss of epoch-2 batch-8 = 0.0357823483645916

Training epoch-2 batch-9
Running loss of epoch-2 batch-9 = 0.03602234274148941

Training epoch-2 batch-10
Running loss of epoch-2 batch-10 = 0.03587639331817627

Training epoch-2 batch-11
Running loss of epoch-2 batch-11 = 0.03601066768169403

Training epoch-2 batch-12
Running loss of epoch-2 batch-12 = 0.03613972291350365

Training epoch-2 batch-13
Running loss of epoch-2 batch-13 = 0.0359075590968132

Training epoch-2 batch-14
Running loss of epoch-2 batch-14 = 0.03605084493756294

Training epoch-2 batch-15
Running loss of epoch-2 batch-15 = 0.03601252660155296

Training epoch-2 batch-16
Running loss of epoch-2 batch-16 = 0.035861071199178696

Training epoch-2 batch-17
Running loss of epoch-2 batch-17 = 0.036133937537670135

Training epoch-2 batch-18
Running loss of epoch-2 batch-18 = 0.036185361444950104

Training epoch-2 batch-19
Running loss of epoch-2 batch-19 = 0.03591347113251686

Training epoch-2 batch-20
Running loss of epoch-2 batch-20 = 0.03606882691383362

Training epoch-2 batch-21
Running loss of epoch-2 batch-21 = 0.03610599413514137

Training epoch-2 batch-22
Running loss of epoch-2 batch-22 = 0.03598179668188095

Training epoch-2 batch-23
Running loss of epoch-2 batch-23 = 0.03625029698014259

Training epoch-2 batch-24
Running loss of epoch-2 batch-24 = 0.03617854788899422

Training epoch-2 batch-25
Running loss of epoch-2 batch-25 = 0.035974930971860886

Training epoch-2 batch-26
Running loss of epoch-2 batch-26 = 0.03595482558012009

Training epoch-2 batch-27
Running loss of epoch-2 batch-27 = 0.03590335696935654

Training epoch-2 batch-28
Running loss of epoch-2 batch-28 = 0.0361197330057621

Training epoch-2 batch-29
Running loss of epoch-2 batch-29 = 0.03604337200522423

Training epoch-2 batch-30
Running loss of epoch-2 batch-30 = 0.03593648597598076

Training epoch-2 batch-31
Running loss of epoch-2 batch-31 = 0.035944074392318726

Training epoch-2 batch-32
Running loss of epoch-2 batch-32 = 0.03608216717839241

Training epoch-2 batch-33
Running loss of epoch-2 batch-33 = 0.0359991155564785

Training epoch-2 batch-34
Running loss of epoch-2 batch-34 = 0.0361315980553627

Training epoch-2 batch-35
Running loss of epoch-2 batch-35 = 0.0359986387193203

Training epoch-2 batch-36
Running loss of epoch-2 batch-36 = 0.03598307445645332

Training epoch-2 batch-37
Running loss of epoch-2 batch-37 = 0.03585993871092796

Training epoch-2 batch-38
Running loss of epoch-2 batch-38 = 0.03613860905170441

Training epoch-2 batch-39
Running loss of epoch-2 batch-39 = 0.036065999418497086

Training epoch-2 batch-40
Running loss of epoch-2 batch-40 = 0.03580062463879585

Training epoch-2 batch-41
Running loss of epoch-2 batch-41 = 0.036006614565849304

Training epoch-2 batch-42
Running loss of epoch-2 batch-42 = 0.03604209050536156

Training epoch-2 batch-43
Running loss of epoch-2 batch-43 = 0.03605152294039726

Training epoch-2 batch-44
Running loss of epoch-2 batch-44 = 0.036010824143886566

Training epoch-2 batch-45
Running loss of epoch-2 batch-45 = 0.03595209866762161

Training epoch-2 batch-46
Running loss of epoch-2 batch-46 = 0.03602316230535507

Training epoch-2 batch-47
Running loss of epoch-2 batch-47 = 0.0360301248729229

Training epoch-2 batch-48
Running loss of epoch-2 batch-48 = 0.035860415548086166

Training epoch-2 batch-49
Running loss of epoch-2 batch-49 = 0.03604605421423912

Training epoch-2 batch-50
Running loss of epoch-2 batch-50 = 0.036151692271232605

Training epoch-2 batch-51
Running loss of epoch-2 batch-51 = 0.035811133682727814

Training epoch-2 batch-52
Running loss of epoch-2 batch-52 = 0.03586071729660034

Training epoch-2 batch-53
Running loss of epoch-2 batch-53 = 0.035988710820674896

Training epoch-2 batch-54
Running loss of epoch-2 batch-54 = 0.03589287027716637

Training epoch-2 batch-55
Running loss of epoch-2 batch-55 = 0.035872600972652435

Training epoch-2 batch-56
Running loss of epoch-2 batch-56 = 0.036101024597883224

Training epoch-2 batch-57
Running loss of epoch-2 batch-57 = 0.03609490394592285

Training epoch-2 batch-58
Running loss of epoch-2 batch-58 = 0.0358823761343956

Training epoch-2 batch-59
Running loss of epoch-2 batch-59 = 0.03597775101661682

Training epoch-2 batch-60
Running loss of epoch-2 batch-60 = 0.03601367026567459

Training epoch-2 batch-61
Running loss of epoch-2 batch-61 = 0.03595168516039848

Training epoch-2 batch-62
Running loss of epoch-2 batch-62 = 0.035931434482336044

Training epoch-2 batch-63
Running loss of epoch-2 batch-63 = 0.03584633022546768

Training epoch-2 batch-64
Running loss of epoch-2 batch-64 = 0.03600248321890831

Training epoch-2 batch-65
Running loss of epoch-2 batch-65 = 0.03596808388829231

Training epoch-2 batch-66
Running loss of epoch-2 batch-66 = 0.03597428277134895

Training epoch-2 batch-67
Running loss of epoch-2 batch-67 = 0.03602142259478569

Training epoch-2 batch-68
Running loss of epoch-2 batch-68 = 0.03595104068517685

Training epoch-2 batch-69
Running loss of epoch-2 batch-69 = 0.03586322069168091

Training epoch-2 batch-70
Running loss of epoch-2 batch-70 = 0.0360981784760952

Training epoch-2 batch-71
Running loss of epoch-2 batch-71 = 0.03605305030941963

Training epoch-2 batch-72
Running loss of epoch-2 batch-72 = 0.03614823520183563

Training epoch-2 batch-73
Running loss of epoch-2 batch-73 = 0.035950809717178345

Training epoch-2 batch-74
Running loss of epoch-2 batch-74 = 0.035899918526411057

Training epoch-2 batch-75
Running loss of epoch-2 batch-75 = 0.036030590534210205

Training epoch-2 batch-76
Running loss of epoch-2 batch-76 = 0.036003392189741135

Training epoch-2 batch-77
Running loss of epoch-2 batch-77 = 0.035728082060813904

Training epoch-2 batch-78
Running loss of epoch-2 batch-78 = 0.035891853272914886

Training epoch-2 batch-79
Running loss of epoch-2 batch-79 = 0.03601497411727905

Training epoch-2 batch-80
Running loss of epoch-2 batch-80 = 0.036006078124046326

Training epoch-2 batch-81
Running loss of epoch-2 batch-81 = 0.036110542714595795

Training epoch-2 batch-82
Running loss of epoch-2 batch-82 = 0.035848312079906464

Training epoch-2 batch-83
Running loss of epoch-2 batch-83 = 0.03590654581785202

Training epoch-2 batch-84
Running loss of epoch-2 batch-84 = 0.03617645055055618

Training epoch-2 batch-85
Running loss of epoch-2 batch-85 = 0.03605831786990166

Training epoch-2 batch-86
Running loss of epoch-2 batch-86 = 0.035879962146282196

Training epoch-2 batch-87
Running loss of epoch-2 batch-87 = 0.03581422567367554

Training epoch-2 batch-88
Running loss of epoch-2 batch-88 = 0.03603288531303406

Training epoch-2 batch-89
Running loss of epoch-2 batch-89 = 0.0360247939825058

Training epoch-2 batch-90
Running loss of epoch-2 batch-90 = 0.03604166582226753

Training epoch-2 batch-91
Running loss of epoch-2 batch-91 = 0.03587903454899788

Training epoch-2 batch-92
Running loss of epoch-2 batch-92 = 0.03606726974248886

Training epoch-2 batch-93
Running loss of epoch-2 batch-93 = 0.03601491451263428

Training epoch-2 batch-94
Running loss of epoch-2 batch-94 = 0.03590155765414238

Training epoch-2 batch-95
Running loss of epoch-2 batch-95 = 0.03589619696140289

Training epoch-2 batch-96
Running loss of epoch-2 batch-96 = 0.03587237372994423

Training epoch-2 batch-97
Running loss of epoch-2 batch-97 = 0.03604867309331894

Training epoch-2 batch-98
Running loss of epoch-2 batch-98 = 0.035880059003829956

Training epoch-2 batch-99
Running loss of epoch-2 batch-99 = 0.03596346452832222

Training epoch-2 batch-100
Running loss of epoch-2 batch-100 = 0.03602986037731171

Training epoch-2 batch-101
Running loss of epoch-2 batch-101 = 0.03596954047679901

Training epoch-2 batch-102
Running loss of epoch-2 batch-102 = 0.03590835630893707

Training epoch-2 batch-103
Running loss of epoch-2 batch-103 = 0.03596571832895279

Training epoch-2 batch-104
Running loss of epoch-2 batch-104 = 0.035817623138427734

Training epoch-2 batch-105
Running loss of epoch-2 batch-105 = 0.03589910641312599

Training epoch-2 batch-106
Running loss of epoch-2 batch-106 = 0.036010775715112686

Training epoch-2 batch-107
Running loss of epoch-2 batch-107 = 0.03600858524441719

Training epoch-2 batch-108
Running loss of epoch-2 batch-108 = 0.03579702228307724

Training epoch-2 batch-109
Running loss of epoch-2 batch-109 = 0.03596092015504837

Training epoch-2 batch-110
Running loss of epoch-2 batch-110 = 0.03605794534087181

Training epoch-2 batch-111
Running loss of epoch-2 batch-111 = 0.03600645810365677

Training epoch-2 batch-112
Running loss of epoch-2 batch-112 = 0.03598436340689659

Training epoch-2 batch-113
Running loss of epoch-2 batch-113 = 0.03621596097946167

Training epoch-2 batch-114
Running loss of epoch-2 batch-114 = 0.03590653836727142

Training epoch-2 batch-115
Running loss of epoch-2 batch-115 = 0.035856761038303375

Training epoch-2 batch-116
Running loss of epoch-2 batch-116 = 0.035933494567871094

Training epoch-2 batch-117
Running loss of epoch-2 batch-117 = 0.03579277545213699

Training epoch-2 batch-118
Running loss of epoch-2 batch-118 = 0.03594830632209778

Training epoch-2 batch-119
Running loss of epoch-2 batch-119 = 0.03602132201194763

Training epoch-2 batch-120
Running loss of epoch-2 batch-120 = 0.03607688844203949

Training epoch-2 batch-121
Running loss of epoch-2 batch-121 = 0.03606455400586128

Training epoch-2 batch-122
Running loss of epoch-2 batch-122 = 0.035945020616054535

Training epoch-2 batch-123
Running loss of epoch-2 batch-123 = 0.03597734123468399

Training epoch-2 batch-124
Running loss of epoch-2 batch-124 = 0.03583531454205513

Training epoch-2 batch-125
Running loss of epoch-2 batch-125 = 0.035998448729515076

Training epoch-2 batch-126
Running loss of epoch-2 batch-126 = 0.03610377758741379

Training epoch-2 batch-127
Running loss of epoch-2 batch-127 = 0.036020439118146896

Training epoch-2 batch-128
Running loss of epoch-2 batch-128 = 0.03599463775753975

Training epoch-2 batch-129
Running loss of epoch-2 batch-129 = 0.03598758950829506

Training epoch-2 batch-130
Running loss of epoch-2 batch-130 = 0.03593733534216881

Training epoch-2 batch-131
Running loss of epoch-2 batch-131 = 0.035877812653779984

Training epoch-2 batch-132
Running loss of epoch-2 batch-132 = 0.03596991300582886

Training epoch-2 batch-133
Running loss of epoch-2 batch-133 = 0.03606003150343895

Training epoch-2 batch-134
Running loss of epoch-2 batch-134 = 0.03606180474162102

Training epoch-2 batch-135
Running loss of epoch-2 batch-135 = 0.0359819196164608

Training epoch-2 batch-136
Running loss of epoch-2 batch-136 = 0.03598019853234291

Training epoch-2 batch-137
Running loss of epoch-2 batch-137 = 0.035904284566640854

Training epoch-2 batch-138
Running loss of epoch-2 batch-138 = 0.03598899021744728

Training epoch-2 batch-139
Running loss of epoch-2 batch-139 = 0.0359686054289341

Training epoch-2 batch-140
Running loss of epoch-2 batch-140 = 0.03598930314183235

Training epoch-2 batch-141
Running loss of epoch-2 batch-141 = 0.035946302115917206

Training epoch-2 batch-142
Running loss of epoch-2 batch-142 = 0.036052998155355453

Training epoch-2 batch-143
Running loss of epoch-2 batch-143 = 0.03595716133713722

Training epoch-2 batch-144
Running loss of epoch-2 batch-144 = 0.03589849919080734

Training epoch-2 batch-145
Running loss of epoch-2 batch-145 = 0.03599594160914421

Training epoch-2 batch-146
Running loss of epoch-2 batch-146 = 0.035945743322372437

Training epoch-2 batch-147
Running loss of epoch-2 batch-147 = 0.0359918549656868

Training epoch-2 batch-148
Running loss of epoch-2 batch-148 = 0.036104362457990646

Training epoch-2 batch-149
Running loss of epoch-2 batch-149 = 0.03586547449231148

Training epoch-2 batch-150
Running loss of epoch-2 batch-150 = 0.03585006669163704

Training epoch-2 batch-151
Running loss of epoch-2 batch-151 = 0.03600747510790825

Training epoch-2 batch-152
Running loss of epoch-2 batch-152 = 0.035846397280693054

Training epoch-2 batch-153
Running loss of epoch-2 batch-153 = 0.035903383046388626

Training epoch-2 batch-154
Running loss of epoch-2 batch-154 = 0.03589510917663574

Training epoch-2 batch-155
Running loss of epoch-2 batch-155 = 0.036063969135284424

Training epoch-2 batch-156
Running loss of epoch-2 batch-156 = 0.03597935289144516

Training epoch-2 batch-157
Running loss of epoch-2 batch-157 = 0.1442713886499405

Finished training epoch-2.



Average train loss at epoch-2 = 0.03615602684020996

Started Evaluation

Average val loss at epoch-2 = 2.3009736286966422

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 0.00 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 0.00 %
Accuracy for class toString is: 0.00 %
Accuracy for class run is: 0.46 %
Accuracy for class hashCode is: 0.00 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 98.80 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 5.12 %

Finished Evaluation



Started training epoch-3


Training epoch-3 batch-1
Running loss of epoch-3 batch-1 = 0.03594198077917099

Training epoch-3 batch-2
Running loss of epoch-3 batch-2 = 0.03593246638774872

Training epoch-3 batch-3
Running loss of epoch-3 batch-3 = 0.03591213375329971

Training epoch-3 batch-4
Running loss of epoch-3 batch-4 = 0.03604818135499954

Training epoch-3 batch-5
Running loss of epoch-3 batch-5 = 0.03579019755125046

Training epoch-3 batch-6
Running loss of epoch-3 batch-6 = 0.0361509695649147

Training epoch-3 batch-7
Running loss of epoch-3 batch-7 = 0.03595101833343506

Training epoch-3 batch-8
Running loss of epoch-3 batch-8 = 0.03591318055987358

Training epoch-3 batch-9
Running loss of epoch-3 batch-9 = 0.0359983816742897

Training epoch-3 batch-10
Running loss of epoch-3 batch-10 = 0.03599297255277634

Training epoch-3 batch-11
Running loss of epoch-3 batch-11 = 0.035982437431812286

Training epoch-3 batch-12
Running loss of epoch-3 batch-12 = 0.03591626510024071

Training epoch-3 batch-13
Running loss of epoch-3 batch-13 = 0.03587932139635086

Training epoch-3 batch-14
Running loss of epoch-3 batch-14 = 0.035911161452531815

Training epoch-3 batch-15
Running loss of epoch-3 batch-15 = 0.03601135313510895

Training epoch-3 batch-16
Running loss of epoch-3 batch-16 = 0.035839881747961044

Training epoch-3 batch-17
Running loss of epoch-3 batch-17 = 0.036012984812259674

Training epoch-3 batch-18
Running loss of epoch-3 batch-18 = 0.03595324978232384

Training epoch-3 batch-19
Running loss of epoch-3 batch-19 = 0.03588184714317322

Training epoch-3 batch-20
Running loss of epoch-3 batch-20 = 0.035971853882074356

Training epoch-3 batch-21
Running loss of epoch-3 batch-21 = 0.03588221222162247

Training epoch-3 batch-22
Running loss of epoch-3 batch-22 = 0.0360197052359581

Training epoch-3 batch-23
Running loss of epoch-3 batch-23 = 0.03603971004486084

Training epoch-3 batch-24
Running loss of epoch-3 batch-24 = 0.035969216376543045

Training epoch-3 batch-25
Running loss of epoch-3 batch-25 = 0.03590144217014313

Training epoch-3 batch-26
Running loss of epoch-3 batch-26 = 0.035950832068920135

Training epoch-3 batch-27
Running loss of epoch-3 batch-27 = 0.035954512655735016

Training epoch-3 batch-28
Running loss of epoch-3 batch-28 = 0.03593570366501808

Training epoch-3 batch-29
Running loss of epoch-3 batch-29 = 0.03592826426029205

Training epoch-3 batch-30
Running loss of epoch-3 batch-30 = 0.035984739661216736

Training epoch-3 batch-31
Running loss of epoch-3 batch-31 = 0.03592703118920326

Training epoch-3 batch-32
Running loss of epoch-3 batch-32 = 0.03596418723464012

Training epoch-3 batch-33
Running loss of epoch-3 batch-33 = 0.0359778068959713

Training epoch-3 batch-34
Running loss of epoch-3 batch-34 = 0.035854097455739975

Training epoch-3 batch-35
Running loss of epoch-3 batch-35 = 0.03593027591705322

Training epoch-3 batch-36
Running loss of epoch-3 batch-36 = 0.03605469688773155

Training epoch-3 batch-37
Running loss of epoch-3 batch-37 = 0.035955093801021576

Training epoch-3 batch-38
Running loss of epoch-3 batch-38 = 0.03598785772919655

Training epoch-3 batch-39
Running loss of epoch-3 batch-39 = 0.035912029445171356

Training epoch-3 batch-40
Running loss of epoch-3 batch-40 = 0.0359996035695076

Training epoch-3 batch-41
Running loss of epoch-3 batch-41 = 0.03587470203638077

Training epoch-3 batch-42
Running loss of epoch-3 batch-42 = 0.03593077138066292

Training epoch-3 batch-43
Running loss of epoch-3 batch-43 = 0.03586728498339653

Training epoch-3 batch-44
Running loss of epoch-3 batch-44 = 0.03579374775290489

Training epoch-3 batch-45
Running loss of epoch-3 batch-45 = 0.0359039306640625

Training epoch-3 batch-46
Running loss of epoch-3 batch-46 = 0.03597566857933998

Training epoch-3 batch-47
Running loss of epoch-3 batch-47 = 0.03594226762652397

Training epoch-3 batch-48
Running loss of epoch-3 batch-48 = 0.035903818905353546

Training epoch-3 batch-49
Running loss of epoch-3 batch-49 = 0.03590603917837143

Training epoch-3 batch-50
Running loss of epoch-3 batch-50 = 0.03599447384476662

Training epoch-3 batch-51
Running loss of epoch-3 batch-51 = 0.03584304824471474

Training epoch-3 batch-52
Running loss of epoch-3 batch-52 = 0.03600911423563957

Training epoch-3 batch-53
Running loss of epoch-3 batch-53 = 0.035857949405908585

Training epoch-3 batch-54
Running loss of epoch-3 batch-54 = 0.035959407687187195

Training epoch-3 batch-55
Running loss of epoch-3 batch-55 = 0.03595003858208656

Training epoch-3 batch-56
Running loss of epoch-3 batch-56 = 0.03584178537130356

Training epoch-3 batch-57
Running loss of epoch-3 batch-57 = 0.03596216440200806

Training epoch-3 batch-58
Running loss of epoch-3 batch-58 = 0.03604775294661522

Training epoch-3 batch-59
Running loss of epoch-3 batch-59 = 0.03590972349047661

Training epoch-3 batch-60
Running loss of epoch-3 batch-60 = 0.035927336663007736

Training epoch-3 batch-61
Running loss of epoch-3 batch-61 = 0.035970523953437805

Training epoch-3 batch-62
Running loss of epoch-3 batch-62 = 0.03588777780532837

Training epoch-3 batch-63
Running loss of epoch-3 batch-63 = 0.035915557295084

Training epoch-3 batch-64
Running loss of epoch-3 batch-64 = 0.036006130278110504

Training epoch-3 batch-65
Running loss of epoch-3 batch-65 = 0.03581977263092995

Training epoch-3 batch-66
Running loss of epoch-3 batch-66 = 0.03582693636417389

Training epoch-3 batch-67
Running loss of epoch-3 batch-67 = 0.0359051451086998

Training epoch-3 batch-68
Running loss of epoch-3 batch-68 = 0.03589053079485893

Training epoch-3 batch-69
Running loss of epoch-3 batch-69 = 0.03591135889291763

Training epoch-3 batch-70
Running loss of epoch-3 batch-70 = 0.035807054489851

Training epoch-3 batch-71
Running loss of epoch-3 batch-71 = 0.03583577647805214

Training epoch-3 batch-72
Running loss of epoch-3 batch-72 = 0.03600853309035301

Training epoch-3 batch-73
Running loss of epoch-3 batch-73 = 0.03578236699104309

Training epoch-3 batch-74
Running loss of epoch-3 batch-74 = 0.03595162555575371

Training epoch-3 batch-75
Running loss of epoch-3 batch-75 = 0.03587871417403221

Training epoch-3 batch-76
Running loss of epoch-3 batch-76 = 0.035827551037073135

Training epoch-3 batch-77
Running loss of epoch-3 batch-77 = 0.03613083437085152

Training epoch-3 batch-78
Running loss of epoch-3 batch-78 = 0.03595174476504326

Training epoch-3 batch-79
Running loss of epoch-3 batch-79 = 0.03609157353639603

Training epoch-3 batch-80
Running loss of epoch-3 batch-80 = 0.03594537079334259

Training epoch-3 batch-81
Running loss of epoch-3 batch-81 = 0.03599151223897934

Training epoch-3 batch-82
Running loss of epoch-3 batch-82 = 0.0359932966530323

Training epoch-3 batch-83
Running loss of epoch-3 batch-83 = 0.035969965159893036

Training epoch-3 batch-84
Running loss of epoch-3 batch-84 = 0.03588101640343666

Training epoch-3 batch-85
Running loss of epoch-3 batch-85 = 0.035947829484939575

Training epoch-3 batch-86
Running loss of epoch-3 batch-86 = 0.03596028313040733

Training epoch-3 batch-87
Running loss of epoch-3 batch-87 = 0.03591898828744888

Training epoch-3 batch-88
Running loss of epoch-3 batch-88 = 0.0359429195523262

Training epoch-3 batch-89
Running loss of epoch-3 batch-89 = 0.0357942171394825

Training epoch-3 batch-90
Running loss of epoch-3 batch-90 = 0.035831913352012634

Training epoch-3 batch-91
Running loss of epoch-3 batch-91 = 0.03598300367593765

Training epoch-3 batch-92
Running loss of epoch-3 batch-92 = 0.03598170354962349

Training epoch-3 batch-93
Running loss of epoch-3 batch-93 = 0.03606625646352768

Training epoch-3 batch-94
Running loss of epoch-3 batch-94 = 0.036122292280197144

Training epoch-3 batch-95
Running loss of epoch-3 batch-95 = 0.03584719076752663

Training epoch-3 batch-96
Running loss of epoch-3 batch-96 = 0.0358889177441597

Training epoch-3 batch-97
Running loss of epoch-3 batch-97 = 0.03581560030579567

Training epoch-3 batch-98
Running loss of epoch-3 batch-98 = 0.035860706120729446

Training epoch-3 batch-99
Running loss of epoch-3 batch-99 = 0.03583301231265068

Training epoch-3 batch-100
Running loss of epoch-3 batch-100 = 0.03583652526140213

Training epoch-3 batch-101
Running loss of epoch-3 batch-101 = 0.035902876406908035

Training epoch-3 batch-102
Running loss of epoch-3 batch-102 = 0.03586380183696747

Training epoch-3 batch-103
Running loss of epoch-3 batch-103 = 0.03601059690117836

Training epoch-3 batch-104
Running loss of epoch-3 batch-104 = 0.03576749563217163

Training epoch-3 batch-105
Running loss of epoch-3 batch-105 = 0.0360446497797966

Training epoch-3 batch-106
Running loss of epoch-3 batch-106 = 0.03613011911511421

Training epoch-3 batch-107
Running loss of epoch-3 batch-107 = 0.03591786324977875

Training epoch-3 batch-108
Running loss of epoch-3 batch-108 = 0.03580613061785698

Training epoch-3 batch-109
Running loss of epoch-3 batch-109 = 0.03583565726876259

Training epoch-3 batch-110
Running loss of epoch-3 batch-110 = 0.035873156040906906

Training epoch-3 batch-111
Running loss of epoch-3 batch-111 = 0.03580571711063385

Training epoch-3 batch-112
Running loss of epoch-3 batch-112 = 0.03586519509553909

Training epoch-3 batch-113
Running loss of epoch-3 batch-113 = 0.03587408363819122

Training epoch-3 batch-114
Running loss of epoch-3 batch-114 = 0.036055684089660645

Training epoch-3 batch-115
Running loss of epoch-3 batch-115 = 0.036004312336444855

Training epoch-3 batch-116
Running loss of epoch-3 batch-116 = 0.035885393619537354

Training epoch-3 batch-117
Running loss of epoch-3 batch-117 = 0.03592032566666603

Training epoch-3 batch-118
Running loss of epoch-3 batch-118 = 0.03584929183125496

Training epoch-3 batch-119
Running loss of epoch-3 batch-119 = 0.03596305474638939

Training epoch-3 batch-120
Running loss of epoch-3 batch-120 = 0.035921528935432434

Training epoch-3 batch-121
Running loss of epoch-3 batch-121 = 0.03587133064866066

Training epoch-3 batch-122
Running loss of epoch-3 batch-122 = 0.035876549780368805

Training epoch-3 batch-123
Running loss of epoch-3 batch-123 = 0.0358496718108654

Training epoch-3 batch-124
Running loss of epoch-3 batch-124 = 0.035946693271398544

Training epoch-3 batch-125
Running loss of epoch-3 batch-125 = 0.03581707179546356

Training epoch-3 batch-126
Running loss of epoch-3 batch-126 = 0.035863667726516724

Training epoch-3 batch-127
Running loss of epoch-3 batch-127 = 0.03586860001087189

Training epoch-3 batch-128
Running loss of epoch-3 batch-128 = 0.03592773526906967

Training epoch-3 batch-129
Running loss of epoch-3 batch-129 = 0.035924822092056274

Training epoch-3 batch-130
Running loss of epoch-3 batch-130 = 0.03575222194194794

Training epoch-3 batch-131
Running loss of epoch-3 batch-131 = 0.03582169488072395

Training epoch-3 batch-132
Running loss of epoch-3 batch-132 = 0.035749033093452454

Training epoch-3 batch-133
Running loss of epoch-3 batch-133 = 0.035844337195158005

Training epoch-3 batch-134
Running loss of epoch-3 batch-134 = 0.035899706184864044

Training epoch-3 batch-135
Running loss of epoch-3 batch-135 = 0.03598524630069733

Training epoch-3 batch-136
Running loss of epoch-3 batch-136 = 0.035704199224710464

Training epoch-3 batch-137
Running loss of epoch-3 batch-137 = 0.03610707446932793

Training epoch-3 batch-138
Running loss of epoch-3 batch-138 = 0.03604225441813469

Training epoch-3 batch-139
Running loss of epoch-3 batch-139 = 0.035957615822553635

Training epoch-3 batch-140
Running loss of epoch-3 batch-140 = 0.035990361124277115

Training epoch-3 batch-141
Running loss of epoch-3 batch-141 = 0.03586287796497345

Training epoch-3 batch-142
Running loss of epoch-3 batch-142 = 0.035792335867881775

Training epoch-3 batch-143
Running loss of epoch-3 batch-143 = 0.035870060324668884

Training epoch-3 batch-144
Running loss of epoch-3 batch-144 = 0.03600343316793442

Training epoch-3 batch-145
Running loss of epoch-3 batch-145 = 0.035967398434877396

Training epoch-3 batch-146
Running loss of epoch-3 batch-146 = 0.03581681475043297

Training epoch-3 batch-147
Running loss of epoch-3 batch-147 = 0.03597745671868324

Training epoch-3 batch-148
Running loss of epoch-3 batch-148 = 0.03586379066109657

Training epoch-3 batch-149
Running loss of epoch-3 batch-149 = 0.03593796491622925

Training epoch-3 batch-150
Running loss of epoch-3 batch-150 = 0.03565243259072304

Training epoch-3 batch-151
Running loss of epoch-3 batch-151 = 0.03577151522040367

Training epoch-3 batch-152
Running loss of epoch-3 batch-152 = 0.03595617413520813

Training epoch-3 batch-153
Running loss of epoch-3 batch-153 = 0.036022476851940155

Training epoch-3 batch-154
Running loss of epoch-3 batch-154 = 0.03595977649092674

Training epoch-3 batch-155
Running loss of epoch-3 batch-155 = 0.035934485495090485

Training epoch-3 batch-156
Running loss of epoch-3 batch-156 = 0.035762716084718704

Training epoch-3 batch-157
Running loss of epoch-3 batch-157 = 0.14358223974704742

Finished training epoch-3.



Average train loss at epoch-3 = 0.03609265921115875

Started Evaluation

Average val loss at epoch-3 = 2.2974285765698084

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 0.00 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 0.00 %
Accuracy for class toString is: 0.00 %
Accuracy for class run is: 1.37 %
Accuracy for class hashCode is: 0.00 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 97.99 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 5.16 %

Finished Evaluation



Started training epoch-4


Training epoch-4 batch-1
Running loss of epoch-4 batch-1 = 0.035875316709280014

Training epoch-4 batch-2
Running loss of epoch-4 batch-2 = 0.035893820226192474

Training epoch-4 batch-3
Running loss of epoch-4 batch-3 = 0.03584089130163193

Training epoch-4 batch-4
Running loss of epoch-4 batch-4 = 0.03585650771856308

Training epoch-4 batch-5
Running loss of epoch-4 batch-5 = 0.03593932464718819

Training epoch-4 batch-6
Running loss of epoch-4 batch-6 = 0.03581872209906578

Training epoch-4 batch-7
Running loss of epoch-4 batch-7 = 0.03584977611899376

Training epoch-4 batch-8
Running loss of epoch-4 batch-8 = 0.035936661064624786

Training epoch-4 batch-9
Running loss of epoch-4 batch-9 = 0.03585653752088547

Training epoch-4 batch-10
Running loss of epoch-4 batch-10 = 0.03580453619360924

Training epoch-4 batch-11
Running loss of epoch-4 batch-11 = 0.03587193787097931

Training epoch-4 batch-12
Running loss of epoch-4 batch-12 = 0.03584897890686989

Training epoch-4 batch-13
Running loss of epoch-4 batch-13 = 0.035835202783346176

Training epoch-4 batch-14
Running loss of epoch-4 batch-14 = 0.03581279143691063

Training epoch-4 batch-15
Running loss of epoch-4 batch-15 = 0.03575220704078674

Training epoch-4 batch-16
Running loss of epoch-4 batch-16 = 0.035846810787916183

Training epoch-4 batch-17
Running loss of epoch-4 batch-17 = 0.03583543375134468

Training epoch-4 batch-18
Running loss of epoch-4 batch-18 = 0.035944052040576935

Training epoch-4 batch-19
Running loss of epoch-4 batch-19 = 0.03575712442398071

Training epoch-4 batch-20
Running loss of epoch-4 batch-20 = 0.03585951775312424

Training epoch-4 batch-21
Running loss of epoch-4 batch-21 = 0.03585517033934593

Training epoch-4 batch-22
Running loss of epoch-4 batch-22 = 0.0357632078230381

Training epoch-4 batch-23
Running loss of epoch-4 batch-23 = 0.0357583649456501

Training epoch-4 batch-24
Running loss of epoch-4 batch-24 = 0.03584301844239235

Training epoch-4 batch-25
Running loss of epoch-4 batch-25 = 0.035703156143426895

Training epoch-4 batch-26
Running loss of epoch-4 batch-26 = 0.03591950610280037

Training epoch-4 batch-27
Running loss of epoch-4 batch-27 = 0.03573956713080406

Training epoch-4 batch-28
Running loss of epoch-4 batch-28 = 0.03572427108883858

Training epoch-4 batch-29
Running loss of epoch-4 batch-29 = 0.03580809384584427

Training epoch-4 batch-30
Running loss of epoch-4 batch-30 = 0.03578878566622734

Training epoch-4 batch-31
Running loss of epoch-4 batch-31 = 0.03570696339011192

Training epoch-4 batch-32
Running loss of epoch-4 batch-32 = 0.0359019972383976

Training epoch-4 batch-33
Running loss of epoch-4 batch-33 = 0.035783637315034866

Training epoch-4 batch-34
Running loss of epoch-4 batch-34 = 0.035758428275585175

Training epoch-4 batch-35
Running loss of epoch-4 batch-35 = 0.03585464879870415

Training epoch-4 batch-36
Running loss of epoch-4 batch-36 = 0.03579709306359291

Training epoch-4 batch-37
Running loss of epoch-4 batch-37 = 0.03569011390209198

Training epoch-4 batch-38
Running loss of epoch-4 batch-38 = 0.03579588979482651

Training epoch-4 batch-39
Running loss of epoch-4 batch-39 = 0.03605080768465996

Training epoch-4 batch-40
Running loss of epoch-4 batch-40 = 0.03579157218337059

Training epoch-4 batch-41
Running loss of epoch-4 batch-41 = 0.03572088107466698

Training epoch-4 batch-42
Running loss of epoch-4 batch-42 = 0.0358409583568573

Training epoch-4 batch-43
Running loss of epoch-4 batch-43 = 0.03584793210029602

Training epoch-4 batch-44
Running loss of epoch-4 batch-44 = 0.035771649330854416

Training epoch-4 batch-45
Running loss of epoch-4 batch-45 = 0.03588106855750084

Training epoch-4 batch-46
Running loss of epoch-4 batch-46 = 0.03584692254662514

Training epoch-4 batch-47
Running loss of epoch-4 batch-47 = 0.035872891545295715

Training epoch-4 batch-48
Running loss of epoch-4 batch-48 = 0.03579672425985336

Training epoch-4 batch-49
Running loss of epoch-4 batch-49 = 0.035635169595479965

Training epoch-4 batch-50
Running loss of epoch-4 batch-50 = 0.03574994578957558

Training epoch-4 batch-51
Running loss of epoch-4 batch-51 = 0.035846054553985596

Training epoch-4 batch-52
Running loss of epoch-4 batch-52 = 0.035691384226083755

Training epoch-4 batch-53
Running loss of epoch-4 batch-53 = 0.03591325134038925

Training epoch-4 batch-54
Running loss of epoch-4 batch-54 = 0.035965368151664734

Training epoch-4 batch-55
Running loss of epoch-4 batch-55 = 0.035805895924568176

Training epoch-4 batch-56
Running loss of epoch-4 batch-56 = 0.03560147434473038

Training epoch-4 batch-57
Running loss of epoch-4 batch-57 = 0.035876017063856125

Training epoch-4 batch-58
Running loss of epoch-4 batch-58 = 0.03598133474588394

Training epoch-4 batch-59
Running loss of epoch-4 batch-59 = 0.03574693202972412

Training epoch-4 batch-60
Running loss of epoch-4 batch-60 = 0.03572508692741394

Training epoch-4 batch-61
Running loss of epoch-4 batch-61 = 0.035744279623031616

Training epoch-4 batch-62
Running loss of epoch-4 batch-62 = 0.03575677424669266

Training epoch-4 batch-63
Running loss of epoch-4 batch-63 = 0.03568096458911896

Training epoch-4 batch-64
Running loss of epoch-4 batch-64 = 0.03573472425341606

Training epoch-4 batch-65
Running loss of epoch-4 batch-65 = 0.03584040328860283

Training epoch-4 batch-66
Running loss of epoch-4 batch-66 = 0.035657934844493866

Training epoch-4 batch-67
Running loss of epoch-4 batch-67 = 0.035745419561862946

Training epoch-4 batch-68
Running loss of epoch-4 batch-68 = 0.035635799169540405

Training epoch-4 batch-69
Running loss of epoch-4 batch-69 = 0.0359790101647377

Training epoch-4 batch-70
Running loss of epoch-4 batch-70 = 0.03569260612130165

Training epoch-4 batch-71
Running loss of epoch-4 batch-71 = 0.03582477942109108

Training epoch-4 batch-72
Running loss of epoch-4 batch-72 = 0.035606011748313904

Training epoch-4 batch-73
Running loss of epoch-4 batch-73 = 0.03576007857918739

Training epoch-4 batch-74
Running loss of epoch-4 batch-74 = 0.03585207462310791

Training epoch-4 batch-75
Running loss of epoch-4 batch-75 = 0.03578532114624977

Training epoch-4 batch-76
Running loss of epoch-4 batch-76 = 0.03574233129620552

Training epoch-4 batch-77
Running loss of epoch-4 batch-77 = 0.0357230119407177

Training epoch-4 batch-78
Running loss of epoch-4 batch-78 = 0.035829395055770874

Training epoch-4 batch-79
Running loss of epoch-4 batch-79 = 0.03583024442195892

Training epoch-4 batch-80
Running loss of epoch-4 batch-80 = 0.035591401159763336

Training epoch-4 batch-81
Running loss of epoch-4 batch-81 = 0.035615988075733185

Training epoch-4 batch-82
Running loss of epoch-4 batch-82 = 0.035773083567619324

Training epoch-4 batch-83
Running loss of epoch-4 batch-83 = 0.03567343205213547

Training epoch-4 batch-84
Running loss of epoch-4 batch-84 = 0.03564091771841049

Training epoch-4 batch-85
Running loss of epoch-4 batch-85 = 0.035789117217063904

Training epoch-4 batch-86
Running loss of epoch-4 batch-86 = 0.035839568823575974

Training epoch-4 batch-87
Running loss of epoch-4 batch-87 = 0.035659100860357285

Training epoch-4 batch-88
Running loss of epoch-4 batch-88 = 0.0355917327105999

Training epoch-4 batch-89
Running loss of epoch-4 batch-89 = 0.03573359176516533

Training epoch-4 batch-90
Running loss of epoch-4 batch-90 = 0.03581344336271286

Training epoch-4 batch-91
Running loss of epoch-4 batch-91 = 0.035719212144613266

Training epoch-4 batch-92
Running loss of epoch-4 batch-92 = 0.035498786717653275

Training epoch-4 batch-93
Running loss of epoch-4 batch-93 = 0.03555365651845932

Training epoch-4 batch-94
Running loss of epoch-4 batch-94 = 0.03561253473162651

Training epoch-4 batch-95
Running loss of epoch-4 batch-95 = 0.03568502143025398

Training epoch-4 batch-96
Running loss of epoch-4 batch-96 = 0.03603620082139969

Training epoch-4 batch-97
Running loss of epoch-4 batch-97 = 0.03618643060326576

Training epoch-4 batch-98
Running loss of epoch-4 batch-98 = 0.035712674260139465

Training epoch-4 batch-99
Running loss of epoch-4 batch-99 = 0.035575930029153824

Training epoch-4 batch-100
Running loss of epoch-4 batch-100 = 0.03572993725538254

Training epoch-4 batch-101
Running loss of epoch-4 batch-101 = 0.035780344158411026

Training epoch-4 batch-102
Running loss of epoch-4 batch-102 = 0.03572893515229225

Training epoch-4 batch-103
Running loss of epoch-4 batch-103 = 0.03571562096476555

Training epoch-4 batch-104
Running loss of epoch-4 batch-104 = 0.03562714159488678

Training epoch-4 batch-105
Running loss of epoch-4 batch-105 = 0.035723842680454254

Training epoch-4 batch-106
Running loss of epoch-4 batch-106 = 0.03587212413549423

Training epoch-4 batch-107
Running loss of epoch-4 batch-107 = 0.03580864891409874

Training epoch-4 batch-108
Running loss of epoch-4 batch-108 = 0.03581384941935539

Training epoch-4 batch-109
Running loss of epoch-4 batch-109 = 0.03567773848772049

Training epoch-4 batch-110
Running loss of epoch-4 batch-110 = 0.03563991189002991

Training epoch-4 batch-111
Running loss of epoch-4 batch-111 = 0.03560838848352432

Training epoch-4 batch-112
Running loss of epoch-4 batch-112 = 0.035586632788181305

Training epoch-4 batch-113
Running loss of epoch-4 batch-113 = 0.03556666150689125

Training epoch-4 batch-114
Running loss of epoch-4 batch-114 = 0.03566782921552658

Training epoch-4 batch-115
Running loss of epoch-4 batch-115 = 0.03569415211677551

Training epoch-4 batch-116
Running loss of epoch-4 batch-116 = 0.03561471402645111

Training epoch-4 batch-117
Running loss of epoch-4 batch-117 = 0.03567281737923622

Training epoch-4 batch-118
Running loss of epoch-4 batch-118 = 0.03552866727113724

Training epoch-4 batch-119
Running loss of epoch-4 batch-119 = 0.03572506830096245

Training epoch-4 batch-120
Running loss of epoch-4 batch-120 = 0.035992685705423355

Training epoch-4 batch-121
Running loss of epoch-4 batch-121 = 0.03557095676660538

Training epoch-4 batch-122
Running loss of epoch-4 batch-122 = 0.035492606461048126

Training epoch-4 batch-123
Running loss of epoch-4 batch-123 = 0.03570453077554703

Training epoch-4 batch-124
Running loss of epoch-4 batch-124 = 0.03576528653502464

Training epoch-4 batch-125
Running loss of epoch-4 batch-125 = 0.03541838750243187

Training epoch-4 batch-126
Running loss of epoch-4 batch-126 = 0.035606030374765396

Training epoch-4 batch-127
Running loss of epoch-4 batch-127 = 0.035478364676237106

Training epoch-4 batch-128
Running loss of epoch-4 batch-128 = 0.035582639276981354

Training epoch-4 batch-129
Running loss of epoch-4 batch-129 = 0.03550929203629494

Training epoch-4 batch-130
Running loss of epoch-4 batch-130 = 0.03560643643140793

Training epoch-4 batch-131
Running loss of epoch-4 batch-131 = 0.035422973334789276

Training epoch-4 batch-132
Running loss of epoch-4 batch-132 = 0.03549918159842491

Training epoch-4 batch-133
Running loss of epoch-4 batch-133 = 0.035721007734537125

Training epoch-4 batch-134
Running loss of epoch-4 batch-134 = 0.03574668616056442

Training epoch-4 batch-135
Running loss of epoch-4 batch-135 = 0.03559109568595886

Training epoch-4 batch-136
Running loss of epoch-4 batch-136 = 0.035608675330877304

Training epoch-4 batch-137
Running loss of epoch-4 batch-137 = 0.03550493344664574

Training epoch-4 batch-138
Running loss of epoch-4 batch-138 = 0.03565498813986778

Training epoch-4 batch-139
Running loss of epoch-4 batch-139 = 0.035531483590602875

Training epoch-4 batch-140
Running loss of epoch-4 batch-140 = 0.03565527871251106

Training epoch-4 batch-141
Running loss of epoch-4 batch-141 = 0.03546242415904999

Training epoch-4 batch-142
Running loss of epoch-4 batch-142 = 0.03582894802093506

Training epoch-4 batch-143
Running loss of epoch-4 batch-143 = 0.03533544763922691

Training epoch-4 batch-144
Running loss of epoch-4 batch-144 = 0.03548583760857582

Training epoch-4 batch-145
Running loss of epoch-4 batch-145 = 0.035531673580408096

Training epoch-4 batch-146
Running loss of epoch-4 batch-146 = 0.03551027178764343

Training epoch-4 batch-147
Running loss of epoch-4 batch-147 = 0.035652656108140945

Training epoch-4 batch-148
Running loss of epoch-4 batch-148 = 0.03554537147283554

Training epoch-4 batch-149
Running loss of epoch-4 batch-149 = 0.03550076112151146

Training epoch-4 batch-150
Running loss of epoch-4 batch-150 = 0.03537179157137871

Training epoch-4 batch-151
Running loss of epoch-4 batch-151 = 0.03555554151535034

Training epoch-4 batch-152
Running loss of epoch-4 batch-152 = 0.035228338092565536

Training epoch-4 batch-153
Running loss of epoch-4 batch-153 = 0.03551056236028671

Training epoch-4 batch-154
Running loss of epoch-4 batch-154 = 0.035370517522096634

Training epoch-4 batch-155
Running loss of epoch-4 batch-155 = 0.03552708774805069

Training epoch-4 batch-156
Running loss of epoch-4 batch-156 = 0.035466186702251434

Training epoch-4 batch-157
Running loss of epoch-4 batch-157 = 0.14162206649780273

Finished training epoch-4.



Average train loss at epoch-4 = 0.03588893346786499

Started Evaluation

Average val loss at epoch-4 = 2.2613072395324707

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 0.00 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 0.00 %
Accuracy for class toString is: 0.00 %
Accuracy for class run is: 53.42 %
Accuracy for class hashCode is: 44.57 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 55.42 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 10.13 %

Finished Evaluation



Started training epoch-5


Training epoch-5 batch-1
Running loss of epoch-5 batch-1 = 0.03552525117993355

Training epoch-5 batch-2
Running loss of epoch-5 batch-2 = 0.035385679453611374

Training epoch-5 batch-3
Running loss of epoch-5 batch-3 = 0.035336852073669434

Training epoch-5 batch-4
Running loss of epoch-5 batch-4 = 0.035578109323978424

Training epoch-5 batch-5
Running loss of epoch-5 batch-5 = 0.03561713546514511

Training epoch-5 batch-6
Running loss of epoch-5 batch-6 = 0.03562656790018082

Training epoch-5 batch-7
Running loss of epoch-5 batch-7 = 0.035547077655792236

Training epoch-5 batch-8
Running loss of epoch-5 batch-8 = 0.035611748695373535

Training epoch-5 batch-9
Running loss of epoch-5 batch-9 = 0.03559267520904541

Training epoch-5 batch-10
Running loss of epoch-5 batch-10 = 0.035227034240961075

Training epoch-5 batch-11
Running loss of epoch-5 batch-11 = 0.0353422649204731

Training epoch-5 batch-12
Running loss of epoch-5 batch-12 = 0.03583938628435135

Training epoch-5 batch-13
Running loss of epoch-5 batch-13 = 0.03534582629799843

Training epoch-5 batch-14
Running loss of epoch-5 batch-14 = 0.03541601076722145

Training epoch-5 batch-15
Running loss of epoch-5 batch-15 = 0.03549548611044884

Training epoch-5 batch-16
Running loss of epoch-5 batch-16 = 0.03531451150774956

Training epoch-5 batch-17
Running loss of epoch-5 batch-17 = 0.035452812910079956

Training epoch-5 batch-18
Running loss of epoch-5 batch-18 = 0.035468414425849915

Training epoch-5 batch-19
Running loss of epoch-5 batch-19 = 0.0356568843126297

Training epoch-5 batch-20
Running loss of epoch-5 batch-20 = 0.035300228744745255

Training epoch-5 batch-21
Running loss of epoch-5 batch-21 = 0.0355500765144825

Training epoch-5 batch-22
Running loss of epoch-5 batch-22 = 0.0354970321059227

Training epoch-5 batch-23
Running loss of epoch-5 batch-23 = 0.035664159804582596

Training epoch-5 batch-24
Running loss of epoch-5 batch-24 = 0.03548499196767807

Training epoch-5 batch-25
Running loss of epoch-5 batch-25 = 0.035720862448215485

Training epoch-5 batch-26
Running loss of epoch-5 batch-26 = 0.035536400973796844

Training epoch-5 batch-27
Running loss of epoch-5 batch-27 = 0.03545442596077919

Training epoch-5 batch-28
Running loss of epoch-5 batch-28 = 0.035295456647872925

Training epoch-5 batch-29
Running loss of epoch-5 batch-29 = 0.035434335470199585

Training epoch-5 batch-30
Running loss of epoch-5 batch-30 = 0.03548932448029518

Training epoch-5 batch-31
Running loss of epoch-5 batch-31 = 0.035555481910705566

Training epoch-5 batch-32
Running loss of epoch-5 batch-32 = 0.035549718886613846

Training epoch-5 batch-33
Running loss of epoch-5 batch-33 = 0.0352177657186985

Training epoch-5 batch-34
Running loss of epoch-5 batch-34 = 0.035575464367866516

Training epoch-5 batch-35
Running loss of epoch-5 batch-35 = 0.03527505695819855

Training epoch-5 batch-36
Running loss of epoch-5 batch-36 = 0.035198554396629333

Training epoch-5 batch-37
Running loss of epoch-5 batch-37 = 0.03528965264558792

Training epoch-5 batch-38
Running loss of epoch-5 batch-38 = 0.035132743418216705

Training epoch-5 batch-39
Running loss of epoch-5 batch-39 = 0.0353621281683445

Training epoch-5 batch-40
Running loss of epoch-5 batch-40 = 0.03542560711503029

Training epoch-5 batch-41
Running loss of epoch-5 batch-41 = 0.03546242415904999

Training epoch-5 batch-42
Running loss of epoch-5 batch-42 = 0.03495257720351219

Training epoch-5 batch-43
Running loss of epoch-5 batch-43 = 0.03531937673687935

Training epoch-5 batch-44
Running loss of epoch-5 batch-44 = 0.03534853830933571

Training epoch-5 batch-45
Running loss of epoch-5 batch-45 = 0.03526953607797623

Training epoch-5 batch-46
Running loss of epoch-5 batch-46 = 0.03563149645924568

Training epoch-5 batch-47
Running loss of epoch-5 batch-47 = 0.03543347492814064

Training epoch-5 batch-48
Running loss of epoch-5 batch-48 = 0.035283882170915604

Training epoch-5 batch-49
Running loss of epoch-5 batch-49 = 0.03522371128201485

Training epoch-5 batch-50
Running loss of epoch-5 batch-50 = 0.03530111163854599

Training epoch-5 batch-51
Running loss of epoch-5 batch-51 = 0.035354845225811005

Training epoch-5 batch-52
Running loss of epoch-5 batch-52 = 0.035198312252759933

Training epoch-5 batch-53
Running loss of epoch-5 batch-53 = 0.03542511165142059

Training epoch-5 batch-54
Running loss of epoch-5 batch-54 = 0.03526576980948448

Training epoch-5 batch-55
Running loss of epoch-5 batch-55 = 0.03477199748158455

Training epoch-5 batch-56
Running loss of epoch-5 batch-56 = 0.0360371433198452

Training epoch-5 batch-57
Running loss of epoch-5 batch-57 = 0.035208702087402344

Training epoch-5 batch-58
Running loss of epoch-5 batch-58 = 0.03533123433589935

Training epoch-5 batch-59
Running loss of epoch-5 batch-59 = 0.034961335361003876

Training epoch-5 batch-60
Running loss of epoch-5 batch-60 = 0.03528568521142006

Training epoch-5 batch-61
Running loss of epoch-5 batch-61 = 0.03530319780111313

Training epoch-5 batch-62
Running loss of epoch-5 batch-62 = 0.03571033105254173

Training epoch-5 batch-63
Running loss of epoch-5 batch-63 = 0.03535430133342743

Training epoch-5 batch-64
Running loss of epoch-5 batch-64 = 0.035405468195676804

Training epoch-5 batch-65
Running loss of epoch-5 batch-65 = 0.03534499183297157

Training epoch-5 batch-66
Running loss of epoch-5 batch-66 = 0.03487534075975418

Training epoch-5 batch-67
Running loss of epoch-5 batch-67 = 0.03560596704483032

Training epoch-5 batch-68
Running loss of epoch-5 batch-68 = 0.035229798406362534

Training epoch-5 batch-69
Running loss of epoch-5 batch-69 = 0.03518062084913254

Training epoch-5 batch-70
Running loss of epoch-5 batch-70 = 0.035164665430784225

Training epoch-5 batch-71
Running loss of epoch-5 batch-71 = 0.03529629856348038

Training epoch-5 batch-72
Running loss of epoch-5 batch-72 = 0.03506012260913849

Training epoch-5 batch-73
Running loss of epoch-5 batch-73 = 0.03532464802265167

Training epoch-5 batch-74
Running loss of epoch-5 batch-74 = 0.03478524088859558

Training epoch-5 batch-75
Running loss of epoch-5 batch-75 = 0.035520486533641815

Training epoch-5 batch-76
Running loss of epoch-5 batch-76 = 0.03549410402774811

Training epoch-5 batch-77
Running loss of epoch-5 batch-77 = 0.03535640984773636

Training epoch-5 batch-78
Running loss of epoch-5 batch-78 = 0.0355941578745842

Training epoch-5 batch-79
Running loss of epoch-5 batch-79 = 0.03547971323132515

Training epoch-5 batch-80
Running loss of epoch-5 batch-80 = 0.03500576317310333

Training epoch-5 batch-81
Running loss of epoch-5 batch-81 = 0.03502156585454941

Training epoch-5 batch-82
Running loss of epoch-5 batch-82 = 0.03492259979248047

Training epoch-5 batch-83
Running loss of epoch-5 batch-83 = 0.03524941951036453

Training epoch-5 batch-84
Running loss of epoch-5 batch-84 = 0.03530242666602135

Training epoch-5 batch-85
Running loss of epoch-5 batch-85 = 0.03572150692343712

Training epoch-5 batch-86
Running loss of epoch-5 batch-86 = 0.034939251840114594

Training epoch-5 batch-87
Running loss of epoch-5 batch-87 = 0.034601449966430664

Training epoch-5 batch-88
Running loss of epoch-5 batch-88 = 0.03555765002965927

Training epoch-5 batch-89
Running loss of epoch-5 batch-89 = 0.034973789006471634

Training epoch-5 batch-90
Running loss of epoch-5 batch-90 = 0.03500907123088837

Training epoch-5 batch-91
Running loss of epoch-5 batch-91 = 0.03491375967860222

Training epoch-5 batch-92
Running loss of epoch-5 batch-92 = 0.03534804284572601

Training epoch-5 batch-93
Running loss of epoch-5 batch-93 = 0.03551687300205231

Training epoch-5 batch-94
Running loss of epoch-5 batch-94 = 0.03531960770487785

Training epoch-5 batch-95
Running loss of epoch-5 batch-95 = 0.035251714289188385

Training epoch-5 batch-96
Running loss of epoch-5 batch-96 = 0.03483035787940025

Training epoch-5 batch-97
Running loss of epoch-5 batch-97 = 0.034830544143915176

Training epoch-5 batch-98
Running loss of epoch-5 batch-98 = 0.03537621349096298

Training epoch-5 batch-99
Running loss of epoch-5 batch-99 = 0.03541979193687439

Training epoch-5 batch-100
Running loss of epoch-5 batch-100 = 0.03489995747804642

Training epoch-5 batch-101
Running loss of epoch-5 batch-101 = 0.03489724546670914

Training epoch-5 batch-102
Running loss of epoch-5 batch-102 = 0.03485383465886116

Training epoch-5 batch-103
Running loss of epoch-5 batch-103 = 0.035792674869298935

Training epoch-5 batch-104
Running loss of epoch-5 batch-104 = 0.03471672907471657

Training epoch-5 batch-105
Running loss of epoch-5 batch-105 = 0.03504416346549988

Training epoch-5 batch-106
Running loss of epoch-5 batch-106 = 0.0347958579659462

Training epoch-5 batch-107
Running loss of epoch-5 batch-107 = 0.035377223044633865

Training epoch-5 batch-108
Running loss of epoch-5 batch-108 = 0.03482635319232941

Training epoch-5 batch-109
Running loss of epoch-5 batch-109 = 0.03509918972849846

Training epoch-5 batch-110
Running loss of epoch-5 batch-110 = 0.035314127802848816

Training epoch-5 batch-111
Running loss of epoch-5 batch-111 = 0.03475913405418396

Training epoch-5 batch-112
Running loss of epoch-5 batch-112 = 0.03464118391275406

Training epoch-5 batch-113
Running loss of epoch-5 batch-113 = 0.035105496644973755

Training epoch-5 batch-114
Running loss of epoch-5 batch-114 = 0.03498082235455513

Training epoch-5 batch-115
Running loss of epoch-5 batch-115 = 0.035001181066036224

Training epoch-5 batch-116
Running loss of epoch-5 batch-116 = 0.034834124147892

Training epoch-5 batch-117
Running loss of epoch-5 batch-117 = 0.03502088040113449

Training epoch-5 batch-118
Running loss of epoch-5 batch-118 = 0.03438795357942581

Training epoch-5 batch-119
Running loss of epoch-5 batch-119 = 0.03469160944223404

Training epoch-5 batch-120
Running loss of epoch-5 batch-120 = 0.034950487315654755

Training epoch-5 batch-121
Running loss of epoch-5 batch-121 = 0.03485528379678726

Training epoch-5 batch-122
Running loss of epoch-5 batch-122 = 0.03509707376360893

Training epoch-5 batch-123
Running loss of epoch-5 batch-123 = 0.034610114991664886

Training epoch-5 batch-124
Running loss of epoch-5 batch-124 = 0.03530299663543701

Training epoch-5 batch-125
Running loss of epoch-5 batch-125 = 0.035276543349027634

Training epoch-5 batch-126
Running loss of epoch-5 batch-126 = 0.034345224499702454

Training epoch-5 batch-127
Running loss of epoch-5 batch-127 = 0.035985879600048065

Training epoch-5 batch-128
Running loss of epoch-5 batch-128 = 0.034805990755558014

Training epoch-5 batch-129
Running loss of epoch-5 batch-129 = 0.035058580338954926

Training epoch-5 batch-130
Running loss of epoch-5 batch-130 = 0.035605356097221375

Training epoch-5 batch-131
Running loss of epoch-5 batch-131 = 0.03427128866314888

Training epoch-5 batch-132
Running loss of epoch-5 batch-132 = 0.03477760776877403

Training epoch-5 batch-133
Running loss of epoch-5 batch-133 = 0.034466736018657684

Training epoch-5 batch-134
Running loss of epoch-5 batch-134 = 0.03496461361646652

Training epoch-5 batch-135
Running loss of epoch-5 batch-135 = 0.03611423447728157

Training epoch-5 batch-136
Running loss of epoch-5 batch-136 = 0.03497573360800743

Training epoch-5 batch-137
Running loss of epoch-5 batch-137 = 0.034659892320632935

Training epoch-5 batch-138
Running loss of epoch-5 batch-138 = 0.03468485176563263

Training epoch-5 batch-139
Running loss of epoch-5 batch-139 = 0.034911248832941055

Training epoch-5 batch-140
Running loss of epoch-5 batch-140 = 0.03462006151676178

Training epoch-5 batch-141
Running loss of epoch-5 batch-141 = 0.03510797396302223

Training epoch-5 batch-142
Running loss of epoch-5 batch-142 = 0.03521161153912544

Training epoch-5 batch-143
Running loss of epoch-5 batch-143 = 0.03476480394601822

Training epoch-5 batch-144
Running loss of epoch-5 batch-144 = 0.03495948016643524

Training epoch-5 batch-145
Running loss of epoch-5 batch-145 = 0.03479716181755066

Training epoch-5 batch-146
Running loss of epoch-5 batch-146 = 0.035460248589515686

Training epoch-5 batch-147
Running loss of epoch-5 batch-147 = 0.034858107566833496

Training epoch-5 batch-148
Running loss of epoch-5 batch-148 = 0.03495157137513161

Training epoch-5 batch-149
Running loss of epoch-5 batch-149 = 0.03525910526514053

Training epoch-5 batch-150
Running loss of epoch-5 batch-150 = 0.03459187597036362

Training epoch-5 batch-151
Running loss of epoch-5 batch-151 = 0.034088134765625

Training epoch-5 batch-152
Running loss of epoch-5 batch-152 = 0.034607432782649994

Training epoch-5 batch-153
Running loss of epoch-5 batch-153 = 0.03528925031423569

Training epoch-5 batch-154
Running loss of epoch-5 batch-154 = 0.03391166031360626

Training epoch-5 batch-155
Running loss of epoch-5 batch-155 = 0.034726664423942566

Training epoch-5 batch-156
Running loss of epoch-5 batch-156 = 0.034281082451343536

Training epoch-5 batch-157
Running loss of epoch-5 batch-157 = 0.13835597038269043

Finished training epoch-5.



Average train loss at epoch-5 = 0.03535031952857971

Started Evaluation

Average val loss at epoch-5 = 2.1849553161545803

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 0.00 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 89.87 %
Accuracy for class toString is: 5.12 %
Accuracy for class run is: 0.00 %
Accuracy for class hashCode is: 0.00 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 38.96 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 19.70 %


Best Accuracy = 19.70 % at Epoch-5


Saving model after best epoch-5
Finished Evaluation



Started training epoch-6


Training epoch-6 batch-1
Running loss of epoch-6 batch-1 = 0.034419700503349304

Training epoch-6 batch-2
Running loss of epoch-6 batch-2 = 0.03450137376785278

Training epoch-6 batch-3
Running loss of epoch-6 batch-3 = 0.03511281684041023

Training epoch-6 batch-4
Running loss of epoch-6 batch-4 = 0.03481721878051758

Training epoch-6 batch-5
Running loss of epoch-6 batch-5 = 0.035408247262239456

Training epoch-6 batch-6
Running loss of epoch-6 batch-6 = 0.03468257933855057

Training epoch-6 batch-7
Running loss of epoch-6 batch-7 = 0.034567348659038544

Training epoch-6 batch-8
Running loss of epoch-6 batch-8 = 0.03407309949398041

Training epoch-6 batch-9
Running loss of epoch-6 batch-9 = 0.03490879759192467

Training epoch-6 batch-10
Running loss of epoch-6 batch-10 = 0.03471097722649574

Training epoch-6 batch-11
Running loss of epoch-6 batch-11 = 0.0347428098320961

Training epoch-6 batch-12
Running loss of epoch-6 batch-12 = 0.034915659576654434

Training epoch-6 batch-13
Running loss of epoch-6 batch-13 = 0.034475553780794144

Training epoch-6 batch-14
Running loss of epoch-6 batch-14 = 0.034309472888708115

Training epoch-6 batch-15
Running loss of epoch-6 batch-15 = 0.034473810344934464

Training epoch-6 batch-16
Running loss of epoch-6 batch-16 = 0.03451722115278244

Training epoch-6 batch-17
Running loss of epoch-6 batch-17 = 0.034607015550136566

Training epoch-6 batch-18
Running loss of epoch-6 batch-18 = 0.0350000336766243

Training epoch-6 batch-19
Running loss of epoch-6 batch-19 = 0.03411320969462395

Training epoch-6 batch-20
Running loss of epoch-6 batch-20 = 0.03487985208630562

Training epoch-6 batch-21
Running loss of epoch-6 batch-21 = 0.03408002853393555

Training epoch-6 batch-22
Running loss of epoch-6 batch-22 = 0.03532345965504646

Training epoch-6 batch-23
Running loss of epoch-6 batch-23 = 0.034697409719228745

Training epoch-6 batch-24
Running loss of epoch-6 batch-24 = 0.03452696278691292

Training epoch-6 batch-25
Running loss of epoch-6 batch-25 = 0.03472690284252167

Training epoch-6 batch-26
Running loss of epoch-6 batch-26 = 0.03475574776530266

Training epoch-6 batch-27
Running loss of epoch-6 batch-27 = 0.03435739129781723

Training epoch-6 batch-28
Running loss of epoch-6 batch-28 = 0.03406022861599922

Training epoch-6 batch-29
Running loss of epoch-6 batch-29 = 0.03490626439452171

Training epoch-6 batch-30
Running loss of epoch-6 batch-30 = 0.034403856843709946

Training epoch-6 batch-31
Running loss of epoch-6 batch-31 = 0.03436589241027832

Training epoch-6 batch-32
Running loss of epoch-6 batch-32 = 0.033835917711257935

Training epoch-6 batch-33
Running loss of epoch-6 batch-33 = 0.03421109914779663

Training epoch-6 batch-34
Running loss of epoch-6 batch-34 = 0.03363380581140518

Training epoch-6 batch-35
Running loss of epoch-6 batch-35 = 0.034527089446783066

Training epoch-6 batch-36
Running loss of epoch-6 batch-36 = 0.03419428691267967

Training epoch-6 batch-37
Running loss of epoch-6 batch-37 = 0.034072618931531906

Training epoch-6 batch-38
Running loss of epoch-6 batch-38 = 0.034042518585920334

Training epoch-6 batch-39
Running loss of epoch-6 batch-39 = 0.03410749137401581

Training epoch-6 batch-40
Running loss of epoch-6 batch-40 = 0.034294791519641876

Training epoch-6 batch-41
Running loss of epoch-6 batch-41 = 0.03390049189329147

Training epoch-6 batch-42
Running loss of epoch-6 batch-42 = 0.0342264361679554

Training epoch-6 batch-43
Running loss of epoch-6 batch-43 = 0.034344036132097244

Training epoch-6 batch-44
Running loss of epoch-6 batch-44 = 0.034451521933078766

Training epoch-6 batch-45
Running loss of epoch-6 batch-45 = 0.03396648168563843

Training epoch-6 batch-46
Running loss of epoch-6 batch-46 = 0.034240543842315674

Training epoch-6 batch-47
Running loss of epoch-6 batch-47 = 0.0339956171810627

Training epoch-6 batch-48
Running loss of epoch-6 batch-48 = 0.03368258848786354

Training epoch-6 batch-49
Running loss of epoch-6 batch-49 = 0.03364712372422218

Training epoch-6 batch-50
Running loss of epoch-6 batch-50 = 0.033554960042238235

Training epoch-6 batch-51
Running loss of epoch-6 batch-51 = 0.03395490720868111

Training epoch-6 batch-52
Running loss of epoch-6 batch-52 = 0.03414086624979973

Training epoch-6 batch-53
Running loss of epoch-6 batch-53 = 0.03319225087761879

Training epoch-6 batch-54
Running loss of epoch-6 batch-54 = 0.033500805497169495

Training epoch-6 batch-55
Running loss of epoch-6 batch-55 = 0.03370092809200287

Training epoch-6 batch-56
Running loss of epoch-6 batch-56 = 0.034174393862485886

Training epoch-6 batch-57
Running loss of epoch-6 batch-57 = 0.034368645399808884

Training epoch-6 batch-58
Running loss of epoch-6 batch-58 = 0.03389909863471985

Training epoch-6 batch-59
Running loss of epoch-6 batch-59 = 0.034018486738204956

Training epoch-6 batch-60
Running loss of epoch-6 batch-60 = 0.034842368215322495

Training epoch-6 batch-61
Running loss of epoch-6 batch-61 = 0.03339328244328499

Training epoch-6 batch-62
Running loss of epoch-6 batch-62 = 0.033389072865247726

Training epoch-6 batch-63
Running loss of epoch-6 batch-63 = 0.03331869840621948

Training epoch-6 batch-64
Running loss of epoch-6 batch-64 = 0.03361319378018379

Training epoch-6 batch-65
Running loss of epoch-6 batch-65 = 0.033416878432035446

Training epoch-6 batch-66
Running loss of epoch-6 batch-66 = 0.0337933711707592

Training epoch-6 batch-67
Running loss of epoch-6 batch-67 = 0.03338834270834923

Training epoch-6 batch-68
Running loss of epoch-6 batch-68 = 0.03300676494836807

Training epoch-6 batch-69
Running loss of epoch-6 batch-69 = 0.03248944133520126

Training epoch-6 batch-70
Running loss of epoch-6 batch-70 = 0.032749902456998825

Training epoch-6 batch-71
Running loss of epoch-6 batch-71 = 0.031816091388463974

Training epoch-6 batch-72
Running loss of epoch-6 batch-72 = 0.03383874520659447

Training epoch-6 batch-73
Running loss of epoch-6 batch-73 = 0.03345256671309471

Training epoch-6 batch-74
Running loss of epoch-6 batch-74 = 0.033383507281541824

Training epoch-6 batch-75
Running loss of epoch-6 batch-75 = 0.033134497702121735

Training epoch-6 batch-76
Running loss of epoch-6 batch-76 = 0.032037243247032166

Training epoch-6 batch-77
Running loss of epoch-6 batch-77 = 0.03378733620047569

Training epoch-6 batch-78
Running loss of epoch-6 batch-78 = 0.03322501480579376

Training epoch-6 batch-79
Running loss of epoch-6 batch-79 = 0.032515157014131546

Training epoch-6 batch-80
Running loss of epoch-6 batch-80 = 0.03402091562747955

Training epoch-6 batch-81
Running loss of epoch-6 batch-81 = 0.03306715190410614

Training epoch-6 batch-82
Running loss of epoch-6 batch-82 = 0.03234925866127014

Training epoch-6 batch-83
Running loss of epoch-6 batch-83 = 0.032450802624225616

Training epoch-6 batch-84
Running loss of epoch-6 batch-84 = 0.03237589821219444

Training epoch-6 batch-85
Running loss of epoch-6 batch-85 = 0.03253895789384842

Training epoch-6 batch-86
Running loss of epoch-6 batch-86 = 0.03179898485541344

Training epoch-6 batch-87
Running loss of epoch-6 batch-87 = 0.03222260624170303

Training epoch-6 batch-88
Running loss of epoch-6 batch-88 = 0.031087854877114296

Training epoch-6 batch-89
Running loss of epoch-6 batch-89 = 0.033687423914670944

Training epoch-6 batch-90
Running loss of epoch-6 batch-90 = 0.032464649528265

Training epoch-6 batch-91
Running loss of epoch-6 batch-91 = 0.03252476081252098

Training epoch-6 batch-92
Running loss of epoch-6 batch-92 = 0.03315315023064613

Training epoch-6 batch-93
Running loss of epoch-6 batch-93 = 0.03390028327703476

Training epoch-6 batch-94
Running loss of epoch-6 batch-94 = 0.03321831300854683

Training epoch-6 batch-95
Running loss of epoch-6 batch-95 = 0.03364317864179611

Training epoch-6 batch-96
Running loss of epoch-6 batch-96 = 0.0333951897919178

Training epoch-6 batch-97
Running loss of epoch-6 batch-97 = 0.03285383805632591

Training epoch-6 batch-98
Running loss of epoch-6 batch-98 = 0.031620949506759644

Training epoch-6 batch-99
Running loss of epoch-6 batch-99 = 0.033483173698186874

Training epoch-6 batch-100
Running loss of epoch-6 batch-100 = 0.03169843181967735

Training epoch-6 batch-101
Running loss of epoch-6 batch-101 = 0.03203820809721947

Training epoch-6 batch-102
Running loss of epoch-6 batch-102 = 0.033343374729156494

Training epoch-6 batch-103
Running loss of epoch-6 batch-103 = 0.032686203718185425

Training epoch-6 batch-104
Running loss of epoch-6 batch-104 = 0.03174592927098274

Training epoch-6 batch-105
Running loss of epoch-6 batch-105 = 0.0314083956182003

Training epoch-6 batch-106
Running loss of epoch-6 batch-106 = 0.031064584851264954

Training epoch-6 batch-107
Running loss of epoch-6 batch-107 = 0.031755585223436356

Training epoch-6 batch-108
Running loss of epoch-6 batch-108 = 0.03096599318087101

Training epoch-6 batch-109
Running loss of epoch-6 batch-109 = 0.03176959604024887

Training epoch-6 batch-110
Running loss of epoch-6 batch-110 = 0.031120922416448593

Training epoch-6 batch-111
Running loss of epoch-6 batch-111 = 0.0313541442155838

Training epoch-6 batch-112
Running loss of epoch-6 batch-112 = 0.029761888086795807

Training epoch-6 batch-113
Running loss of epoch-6 batch-113 = 0.02993643470108509

Training epoch-6 batch-114
Running loss of epoch-6 batch-114 = 0.03232137858867645

Training epoch-6 batch-115
Running loss of epoch-6 batch-115 = 0.03118918091058731

Training epoch-6 batch-116
Running loss of epoch-6 batch-116 = 0.03240850567817688

Training epoch-6 batch-117
Running loss of epoch-6 batch-117 = 0.03126559406518936

Training epoch-6 batch-118
Running loss of epoch-6 batch-118 = 0.031169939786195755

Training epoch-6 batch-119
Running loss of epoch-6 batch-119 = 0.031160807237029076

Training epoch-6 batch-120
Running loss of epoch-6 batch-120 = 0.03069141134619713

Training epoch-6 batch-121
Running loss of epoch-6 batch-121 = 0.03174919635057449

Training epoch-6 batch-122
Running loss of epoch-6 batch-122 = 0.030893612653017044

Training epoch-6 batch-123
Running loss of epoch-6 batch-123 = 0.03126691281795502

Training epoch-6 batch-124
Running loss of epoch-6 batch-124 = 0.03154284134507179

Training epoch-6 batch-125
Running loss of epoch-6 batch-125 = 0.03127991035580635

Training epoch-6 batch-126
Running loss of epoch-6 batch-126 = 0.030003614723682404

Training epoch-6 batch-127
Running loss of epoch-6 batch-127 = 0.03200497478246689

Training epoch-6 batch-128
Running loss of epoch-6 batch-128 = 0.02943628840148449

Training epoch-6 batch-129
Running loss of epoch-6 batch-129 = 0.029230384156107903

Training epoch-6 batch-130
Running loss of epoch-6 batch-130 = 0.03370584920048714

Training epoch-6 batch-131
Running loss of epoch-6 batch-131 = 0.03105968050658703

Training epoch-6 batch-132
Running loss of epoch-6 batch-132 = 0.02946687676012516

Training epoch-6 batch-133
Running loss of epoch-6 batch-133 = 0.03193524852395058

Training epoch-6 batch-134
Running loss of epoch-6 batch-134 = 0.03222024813294411

Training epoch-6 batch-135
Running loss of epoch-6 batch-135 = 0.030877456068992615

Training epoch-6 batch-136
Running loss of epoch-6 batch-136 = 0.03074883483350277

Training epoch-6 batch-137
Running loss of epoch-6 batch-137 = 0.030575096607208252

Training epoch-6 batch-138
Running loss of epoch-6 batch-138 = 0.03001529350876808

Training epoch-6 batch-139
Running loss of epoch-6 batch-139 = 0.029281368479132652

Training epoch-6 batch-140
Running loss of epoch-6 batch-140 = 0.030040202662348747

Training epoch-6 batch-141
Running loss of epoch-6 batch-141 = 0.029404427856206894

Training epoch-6 batch-142
Running loss of epoch-6 batch-142 = 0.030674679204821587

Training epoch-6 batch-143
Running loss of epoch-6 batch-143 = 0.029592733830213547

Training epoch-6 batch-144
Running loss of epoch-6 batch-144 = 0.032185427844524384

Training epoch-6 batch-145
Running loss of epoch-6 batch-145 = 0.030671874061226845

Training epoch-6 batch-146
Running loss of epoch-6 batch-146 = 0.030602414160966873

Training epoch-6 batch-147
Running loss of epoch-6 batch-147 = 0.03217945992946625

Training epoch-6 batch-148
Running loss of epoch-6 batch-148 = 0.030103307217359543

Training epoch-6 batch-149
Running loss of epoch-6 batch-149 = 0.02795158326625824

Training epoch-6 batch-150
Running loss of epoch-6 batch-150 = 0.02930116467177868

Training epoch-6 batch-151
Running loss of epoch-6 batch-151 = 0.03203865513205528

Training epoch-6 batch-152
Running loss of epoch-6 batch-152 = 0.028820283710956573

Training epoch-6 batch-153
Running loss of epoch-6 batch-153 = 0.028555724769830704

Training epoch-6 batch-154
Running loss of epoch-6 batch-154 = 0.030849233269691467

Training epoch-6 batch-155
Running loss of epoch-6 batch-155 = 0.033080894500017166

Training epoch-6 batch-156
Running loss of epoch-6 batch-156 = 0.028372611850500107

Training epoch-6 batch-157
Running loss of epoch-6 batch-157 = 0.13015422224998474

Finished training epoch-6.



Average train loss at epoch-6 = 0.03290165116786957

Started Evaluation

Average val loss at epoch-6 = 2.033623737724204

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 79.02 %
Accuracy for class setUp is: 43.44 %
Accuracy for class onCreate is: 2.45 %
Accuracy for class toString is: 19.80 %
Accuracy for class run is: 0.00 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 0.00 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 22.49 %


Best Accuracy = 22.49 % at Epoch-6


Saving model after best epoch-6
Finished Evaluation



Started training epoch-7


Training epoch-7 batch-1
Running loss of epoch-7 batch-1 = 0.029782399535179138

Training epoch-7 batch-2
Running loss of epoch-7 batch-2 = 0.03147341310977936

Training epoch-7 batch-3
Running loss of epoch-7 batch-3 = 0.029429089277982712

Training epoch-7 batch-4
Running loss of epoch-7 batch-4 = 0.02867342345416546

Training epoch-7 batch-5
Running loss of epoch-7 batch-5 = 0.026890506967902184

Training epoch-7 batch-6
Running loss of epoch-7 batch-6 = 0.03150581941008568

Training epoch-7 batch-7
Running loss of epoch-7 batch-7 = 0.027898071333765984

Training epoch-7 batch-8
Running loss of epoch-7 batch-8 = 0.02885308302938938

Training epoch-7 batch-9
Running loss of epoch-7 batch-9 = 0.028065800666809082

Training epoch-7 batch-10
Running loss of epoch-7 batch-10 = 0.03074892796576023

Training epoch-7 batch-11
Running loss of epoch-7 batch-11 = 0.027303971350193024

Training epoch-7 batch-12
Running loss of epoch-7 batch-12 = 0.02759133279323578

Training epoch-7 batch-13
Running loss of epoch-7 batch-13 = 0.030391164124011993

Training epoch-7 batch-14
Running loss of epoch-7 batch-14 = 0.02769436500966549

Training epoch-7 batch-15
Running loss of epoch-7 batch-15 = 0.026951240375638008

Training epoch-7 batch-16
Running loss of epoch-7 batch-16 = 0.026741422712802887

Training epoch-7 batch-17
Running loss of epoch-7 batch-17 = 0.029443128034472466

Training epoch-7 batch-18
Running loss of epoch-7 batch-18 = 0.02518007531762123

Training epoch-7 batch-19
Running loss of epoch-7 batch-19 = 0.02921559475362301

Training epoch-7 batch-20
Running loss of epoch-7 batch-20 = 0.02691694349050522

Training epoch-7 batch-21
Running loss of epoch-7 batch-21 = 0.02750520035624504

Training epoch-7 batch-22
Running loss of epoch-7 batch-22 = 0.029389845207333565

Training epoch-7 batch-23
Running loss of epoch-7 batch-23 = 0.029518866911530495

Training epoch-7 batch-24
Running loss of epoch-7 batch-24 = 0.027118466794490814

Training epoch-7 batch-25
Running loss of epoch-7 batch-25 = 0.028076229616999626

Training epoch-7 batch-26
Running loss of epoch-7 batch-26 = 0.028362473472952843

Training epoch-7 batch-27
Running loss of epoch-7 batch-27 = 0.027426308020949364

Training epoch-7 batch-28
Running loss of epoch-7 batch-28 = 0.028585925698280334

Training epoch-7 batch-29
Running loss of epoch-7 batch-29 = 0.026990078389644623

Training epoch-7 batch-30
Running loss of epoch-7 batch-30 = 0.026266418397426605

Training epoch-7 batch-31
Running loss of epoch-7 batch-31 = 0.02695634216070175

Training epoch-7 batch-32
Running loss of epoch-7 batch-32 = 0.026667671278119087

Training epoch-7 batch-33
Running loss of epoch-7 batch-33 = 0.029429573565721512

Training epoch-7 batch-34
Running loss of epoch-7 batch-34 = 0.027249960228800774

Training epoch-7 batch-35
Running loss of epoch-7 batch-35 = 0.02889976277947426

Training epoch-7 batch-36
Running loss of epoch-7 batch-36 = 0.027429204434156418

Training epoch-7 batch-37
Running loss of epoch-7 batch-37 = 0.02498031035065651

Training epoch-7 batch-38
Running loss of epoch-7 batch-38 = 0.027220331132411957

Training epoch-7 batch-39
Running loss of epoch-7 batch-39 = 0.028529463335871696

Training epoch-7 batch-40
Running loss of epoch-7 batch-40 = 0.02771422080695629

Training epoch-7 batch-41
Running loss of epoch-7 batch-41 = 0.02677394263446331

Training epoch-7 batch-42
Running loss of epoch-7 batch-42 = 0.026958080008625984

Training epoch-7 batch-43
Running loss of epoch-7 batch-43 = 0.02731362171471119

Training epoch-7 batch-44
Running loss of epoch-7 batch-44 = 0.026997368782758713

Training epoch-7 batch-45
Running loss of epoch-7 batch-45 = 0.025925111025571823

Training epoch-7 batch-46
Running loss of epoch-7 batch-46 = 0.023514235392212868

Training epoch-7 batch-47
Running loss of epoch-7 batch-47 = 0.02681366354227066

Training epoch-7 batch-48
Running loss of epoch-7 batch-48 = 0.026570409536361694

Training epoch-7 batch-49
Running loss of epoch-7 batch-49 = 0.02527189627289772

Training epoch-7 batch-50
Running loss of epoch-7 batch-50 = 0.023789003491401672

Training epoch-7 batch-51
Running loss of epoch-7 batch-51 = 0.02630366012454033

Training epoch-7 batch-52
Running loss of epoch-7 batch-52 = 0.025169547647237778

Training epoch-7 batch-53
Running loss of epoch-7 batch-53 = 0.024504609405994415

Training epoch-7 batch-54
Running loss of epoch-7 batch-54 = 0.02668686769902706

Training epoch-7 batch-55
Running loss of epoch-7 batch-55 = 0.023548271507024765

Training epoch-7 batch-56
Running loss of epoch-7 batch-56 = 0.02707911841571331

Training epoch-7 batch-57
Running loss of epoch-7 batch-57 = 0.02405504696071148

Training epoch-7 batch-58
Running loss of epoch-7 batch-58 = 0.02553587220609188

Training epoch-7 batch-59
Running loss of epoch-7 batch-59 = 0.022468911483883858

Training epoch-7 batch-60
Running loss of epoch-7 batch-60 = 0.025467831641435623

Training epoch-7 batch-61
Running loss of epoch-7 batch-61 = 0.02664278633892536

Training epoch-7 batch-62
Running loss of epoch-7 batch-62 = 0.0243189986795187

Training epoch-7 batch-63
Running loss of epoch-7 batch-63 = 0.02611646056175232

Training epoch-7 batch-64
Running loss of epoch-7 batch-64 = 0.027294859290122986

Training epoch-7 batch-65
Running loss of epoch-7 batch-65 = 0.02557659149169922

Training epoch-7 batch-66
Running loss of epoch-7 batch-66 = 0.024039290845394135

Training epoch-7 batch-67
Running loss of epoch-7 batch-67 = 0.026254262775182724

Training epoch-7 batch-68
Running loss of epoch-7 batch-68 = 0.023330049589276314

Training epoch-7 batch-69
Running loss of epoch-7 batch-69 = 0.02253718301653862

Training epoch-7 batch-70
Running loss of epoch-7 batch-70 = 0.024348322302103043

Training epoch-7 batch-71
Running loss of epoch-7 batch-71 = 0.023013107478618622

Training epoch-7 batch-72
Running loss of epoch-7 batch-72 = 0.027184542268514633

Training epoch-7 batch-73
Running loss of epoch-7 batch-73 = 0.023976536467671394

Training epoch-7 batch-74
Running loss of epoch-7 batch-74 = 0.021741950884461403

Training epoch-7 batch-75
Running loss of epoch-7 batch-75 = 0.02269020304083824

Training epoch-7 batch-76
Running loss of epoch-7 batch-76 = 0.02328488789498806

Training epoch-7 batch-77
Running loss of epoch-7 batch-77 = 0.02434278465807438

Training epoch-7 batch-78
Running loss of epoch-7 batch-78 = 0.025090046226978302

Training epoch-7 batch-79
Running loss of epoch-7 batch-79 = 0.02245977148413658

Training epoch-7 batch-80
Running loss of epoch-7 batch-80 = 0.024855388328433037

Training epoch-7 batch-81
Running loss of epoch-7 batch-81 = 0.022934895008802414

Training epoch-7 batch-82
Running loss of epoch-7 batch-82 = 0.023330695927143097

Training epoch-7 batch-83
Running loss of epoch-7 batch-83 = 0.02767825685441494

Training epoch-7 batch-84
Running loss of epoch-7 batch-84 = 0.02510678581893444

Training epoch-7 batch-85
Running loss of epoch-7 batch-85 = 0.021853379905223846

Training epoch-7 batch-86
Running loss of epoch-7 batch-86 = 0.02084045112133026

Training epoch-7 batch-87
Running loss of epoch-7 batch-87 = 0.021372631192207336

Training epoch-7 batch-88
Running loss of epoch-7 batch-88 = 0.019588682800531387

Training epoch-7 batch-89
Running loss of epoch-7 batch-89 = 0.024354886263608932

Training epoch-7 batch-90
Running loss of epoch-7 batch-90 = 0.024220295250415802

Training epoch-7 batch-91
Running loss of epoch-7 batch-91 = 0.021406663581728935

Training epoch-7 batch-92
Running loss of epoch-7 batch-92 = 0.020595969632267952

Training epoch-7 batch-93
Running loss of epoch-7 batch-93 = 0.022844888269901276

Training epoch-7 batch-94
Running loss of epoch-7 batch-94 = 0.021803267300128937

Training epoch-7 batch-95
Running loss of epoch-7 batch-95 = 0.02227136678993702

Training epoch-7 batch-96
Running loss of epoch-7 batch-96 = 0.017218701541423798

Training epoch-7 batch-97
Running loss of epoch-7 batch-97 = 0.0220174640417099

Training epoch-7 batch-98
Running loss of epoch-7 batch-98 = 0.02216484025120735

Training epoch-7 batch-99
Running loss of epoch-7 batch-99 = 0.019893700256943703

Training epoch-7 batch-100
Running loss of epoch-7 batch-100 = 0.020725207403302193

Training epoch-7 batch-101
Running loss of epoch-7 batch-101 = 0.020082853734493256

Training epoch-7 batch-102
Running loss of epoch-7 batch-102 = 0.02120617777109146

Training epoch-7 batch-103
Running loss of epoch-7 batch-103 = 0.020343773066997528

Training epoch-7 batch-104
Running loss of epoch-7 batch-104 = 0.01867368444800377

Training epoch-7 batch-105
Running loss of epoch-7 batch-105 = 0.020316870883107185

Training epoch-7 batch-106
Running loss of epoch-7 batch-106 = 0.025505520403385162

Training epoch-7 batch-107
Running loss of epoch-7 batch-107 = 0.01905023120343685

Training epoch-7 batch-108
Running loss of epoch-7 batch-108 = 0.0196829866617918

Training epoch-7 batch-109
Running loss of epoch-7 batch-109 = 0.024503951892256737

Training epoch-7 batch-110
Running loss of epoch-7 batch-110 = 0.019285349175333977

Training epoch-7 batch-111
Running loss of epoch-7 batch-111 = 0.023734111338853836

Training epoch-7 batch-112
Running loss of epoch-7 batch-112 = 0.020644165575504303

Training epoch-7 batch-113
Running loss of epoch-7 batch-113 = 0.019443947821855545

Training epoch-7 batch-114
Running loss of epoch-7 batch-114 = 0.021453017368912697

Training epoch-7 batch-115
Running loss of epoch-7 batch-115 = 0.023278018459677696

Training epoch-7 batch-116
Running loss of epoch-7 batch-116 = 0.018237873911857605

Training epoch-7 batch-117
Running loss of epoch-7 batch-117 = 0.01977372169494629

Training epoch-7 batch-118
Running loss of epoch-7 batch-118 = 0.021099699661135674

Training epoch-7 batch-119
Running loss of epoch-7 batch-119 = 0.018376324325799942

Training epoch-7 batch-120
Running loss of epoch-7 batch-120 = 0.01721627451479435

Training epoch-7 batch-121
Running loss of epoch-7 batch-121 = 0.01993008330464363

Training epoch-7 batch-122
Running loss of epoch-7 batch-122 = 0.025498908013105392

Training epoch-7 batch-123
Running loss of epoch-7 batch-123 = 0.016496533527970314

Training epoch-7 batch-124
Running loss of epoch-7 batch-124 = 0.021301258355379105

Training epoch-7 batch-125
Running loss of epoch-7 batch-125 = 0.017121773213148117

Training epoch-7 batch-126
Running loss of epoch-7 batch-126 = 0.019414091482758522

Training epoch-7 batch-127
Running loss of epoch-7 batch-127 = 0.023640010505914688

Training epoch-7 batch-128
Running loss of epoch-7 batch-128 = 0.021305767819285393

Training epoch-7 batch-129
Running loss of epoch-7 batch-129 = 0.019124183803796768

Training epoch-7 batch-130
Running loss of epoch-7 batch-130 = 0.017238298431038857

Training epoch-7 batch-131
Running loss of epoch-7 batch-131 = 0.01874055154621601

Training epoch-7 batch-132
Running loss of epoch-7 batch-132 = 0.018183596432209015

Training epoch-7 batch-133
Running loss of epoch-7 batch-133 = 0.019578538835048676

Training epoch-7 batch-134
Running loss of epoch-7 batch-134 = 0.01935618929564953

Training epoch-7 batch-135
Running loss of epoch-7 batch-135 = 0.018399007618427277

Training epoch-7 batch-136
Running loss of epoch-7 batch-136 = 0.017765970900654793

Training epoch-7 batch-137
Running loss of epoch-7 batch-137 = 0.022858209908008575

Training epoch-7 batch-138
Running loss of epoch-7 batch-138 = 0.019267773255705833

Training epoch-7 batch-139
Running loss of epoch-7 batch-139 = 0.021120425313711166

Training epoch-7 batch-140
Running loss of epoch-7 batch-140 = 0.016037074849009514

Training epoch-7 batch-141
Running loss of epoch-7 batch-141 = 0.018733711913228035

Training epoch-7 batch-142
Running loss of epoch-7 batch-142 = 0.019227972254157066

Training epoch-7 batch-143
Running loss of epoch-7 batch-143 = 0.017411261796951294

Training epoch-7 batch-144
Running loss of epoch-7 batch-144 = 0.015223797410726547

Training epoch-7 batch-145
Running loss of epoch-7 batch-145 = 0.016356227919459343

Training epoch-7 batch-146
Running loss of epoch-7 batch-146 = 0.01751074567437172

Training epoch-7 batch-147
Running loss of epoch-7 batch-147 = 0.02366553619503975

Training epoch-7 batch-148
Running loss of epoch-7 batch-148 = 0.018009493127465248

Training epoch-7 batch-149
Running loss of epoch-7 batch-149 = 0.016456477344036102

Training epoch-7 batch-150
Running loss of epoch-7 batch-150 = 0.018021894618868828

Training epoch-7 batch-151
Running loss of epoch-7 batch-151 = 0.014949795790016651

Training epoch-7 batch-152
Running loss of epoch-7 batch-152 = 0.018067892640829086

Training epoch-7 batch-153
Running loss of epoch-7 batch-153 = 0.017418479546904564

Training epoch-7 batch-154
Running loss of epoch-7 batch-154 = 0.01946169324219227

Training epoch-7 batch-155
Running loss of epoch-7 batch-155 = 0.020423732697963715

Training epoch-7 batch-156
Running loss of epoch-7 batch-156 = 0.0177496038377285

Training epoch-7 batch-157
Running loss of epoch-7 batch-157 = 0.06761457026004791

Finished training epoch-7.



Average train loss at epoch-7 = 0.023532885164022445

Started Evaluation

Average val loss at epoch-7 = 1.0226546245578088

Accuracy for classes:
Accuracy for class equals is: 80.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 70.82 %
Accuracy for class onCreate is: 82.09 %
Accuracy for class toString is: 80.89 %
Accuracy for class run is: 45.89 %
Accuracy for class hashCode is: 93.26 %
Accuracy for class init is: 23.99 %
Accuracy for class execute is: 23.69 %
Accuracy for class get is: 48.46 %

Overall Accuracy = 68.54 %


Best Accuracy = 68.54 % at Epoch-7


Saving model after best epoch-7
Finished Evaluation



Started training epoch-8


Training epoch-8 batch-1
Running loss of epoch-8 batch-1 = 0.01801125332713127

Training epoch-8 batch-2
Running loss of epoch-8 batch-2 = 0.01668798364698887

Training epoch-8 batch-3
Running loss of epoch-8 batch-3 = 0.016709037125110626

Training epoch-8 batch-4
Running loss of epoch-8 batch-4 = 0.021097682416439056

Training epoch-8 batch-5
Running loss of epoch-8 batch-5 = 0.01779577136039734

Training epoch-8 batch-6
Running loss of epoch-8 batch-6 = 0.01864568330347538

Training epoch-8 batch-7
Running loss of epoch-8 batch-7 = 0.019357332959771156

Training epoch-8 batch-8
Running loss of epoch-8 batch-8 = 0.015866434201598167

Training epoch-8 batch-9
Running loss of epoch-8 batch-9 = 0.01579156145453453

Training epoch-8 batch-10
Running loss of epoch-8 batch-10 = 0.01527951005846262

Training epoch-8 batch-11
Running loss of epoch-8 batch-11 = 0.017604142427444458

Training epoch-8 batch-12
Running loss of epoch-8 batch-12 = 0.012076478451490402

Training epoch-8 batch-13
Running loss of epoch-8 batch-13 = 0.01765204779803753

Training epoch-8 batch-14
Running loss of epoch-8 batch-14 = 0.014507799409329891

Training epoch-8 batch-15
Running loss of epoch-8 batch-15 = 0.013766391202807426

Training epoch-8 batch-16
Running loss of epoch-8 batch-16 = 0.016295185312628746

Training epoch-8 batch-17
Running loss of epoch-8 batch-17 = 0.015955058857798576

Training epoch-8 batch-18
Running loss of epoch-8 batch-18 = 0.016732363030314445

Training epoch-8 batch-19
Running loss of epoch-8 batch-19 = 0.0159640833735466

Training epoch-8 batch-20
Running loss of epoch-8 batch-20 = 0.017287757247686386

Training epoch-8 batch-21
Running loss of epoch-8 batch-21 = 0.015484794043004513

Training epoch-8 batch-22
Running loss of epoch-8 batch-22 = 0.014313112944364548

Training epoch-8 batch-23
Running loss of epoch-8 batch-23 = 0.016140166670084

Training epoch-8 batch-24
Running loss of epoch-8 batch-24 = 0.01677376590669155

Training epoch-8 batch-25
Running loss of epoch-8 batch-25 = 0.014759616926312447

Training epoch-8 batch-26
Running loss of epoch-8 batch-26 = 0.016517851501703262

Training epoch-8 batch-27
Running loss of epoch-8 batch-27 = 0.014320516027510166

Training epoch-8 batch-28
Running loss of epoch-8 batch-28 = 0.018820391967892647

Training epoch-8 batch-29
Running loss of epoch-8 batch-29 = 0.016483992338180542

Training epoch-8 batch-30
Running loss of epoch-8 batch-30 = 0.018745634704828262

Training epoch-8 batch-31
Running loss of epoch-8 batch-31 = 0.018671467900276184

Training epoch-8 batch-32
Running loss of epoch-8 batch-32 = 0.012837352231144905

Training epoch-8 batch-33
Running loss of epoch-8 batch-33 = 0.014543292112648487

Training epoch-8 batch-34
Running loss of epoch-8 batch-34 = 0.0142825897783041

Training epoch-8 batch-35
Running loss of epoch-8 batch-35 = 0.018380921334028244

Training epoch-8 batch-36
Running loss of epoch-8 batch-36 = 0.017246270552277565

Training epoch-8 batch-37
Running loss of epoch-8 batch-37 = 0.016929587349295616

Training epoch-8 batch-38
Running loss of epoch-8 batch-38 = 0.016384780406951904

Training epoch-8 batch-39
Running loss of epoch-8 batch-39 = 0.015469862148165703

Training epoch-8 batch-40
Running loss of epoch-8 batch-40 = 0.013478608801960945

Training epoch-8 batch-41
Running loss of epoch-8 batch-41 = 0.015739763155579567

Training epoch-8 batch-42
Running loss of epoch-8 batch-42 = 0.019890815019607544

Training epoch-8 batch-43
Running loss of epoch-8 batch-43 = 0.013702045194804668

Training epoch-8 batch-44
Running loss of epoch-8 batch-44 = 0.015637995675206184

Training epoch-8 batch-45
Running loss of epoch-8 batch-45 = 0.019620293751358986

Training epoch-8 batch-46
Running loss of epoch-8 batch-46 = 0.01661895401775837

Training epoch-8 batch-47
Running loss of epoch-8 batch-47 = 0.01897093467414379

Training epoch-8 batch-48
Running loss of epoch-8 batch-48 = 0.01553717814385891

Training epoch-8 batch-49
Running loss of epoch-8 batch-49 = 0.01815384440124035

Training epoch-8 batch-50
Running loss of epoch-8 batch-50 = 0.01880299672484398

Training epoch-8 batch-51
Running loss of epoch-8 batch-51 = 0.014292014762759209

Training epoch-8 batch-52
Running loss of epoch-8 batch-52 = 0.016347574070096016

Training epoch-8 batch-53
Running loss of epoch-8 batch-53 = 0.015458240173757076

Training epoch-8 batch-54
Running loss of epoch-8 batch-54 = 0.015918860211968422

Training epoch-8 batch-55
Running loss of epoch-8 batch-55 = 0.015182935632765293

Training epoch-8 batch-56
Running loss of epoch-8 batch-56 = 0.01209370419383049

Training epoch-8 batch-57
Running loss of epoch-8 batch-57 = 0.01371555496007204

Training epoch-8 batch-58
Running loss of epoch-8 batch-58 = 0.011010629124939442

Training epoch-8 batch-59
Running loss of epoch-8 batch-59 = 0.01649409905076027

Training epoch-8 batch-60
Running loss of epoch-8 batch-60 = 0.015099598094820976

Training epoch-8 batch-61
Running loss of epoch-8 batch-61 = 0.014987275935709476

Training epoch-8 batch-62
Running loss of epoch-8 batch-62 = 0.01567140407860279

Training epoch-8 batch-63
Running loss of epoch-8 batch-63 = 0.015725985169410706

Training epoch-8 batch-64
Running loss of epoch-8 batch-64 = 0.014874572865664959

Training epoch-8 batch-65
Running loss of epoch-8 batch-65 = 0.016927266493439674

Training epoch-8 batch-66
Running loss of epoch-8 batch-66 = 0.011127258650958538

Training epoch-8 batch-67
Running loss of epoch-8 batch-67 = 0.015880269929766655

Training epoch-8 batch-68
Running loss of epoch-8 batch-68 = 0.01360052078962326

Training epoch-8 batch-69
Running loss of epoch-8 batch-69 = 0.01568693108856678

Training epoch-8 batch-70
Running loss of epoch-8 batch-70 = 0.012819989584386349

Training epoch-8 batch-71
Running loss of epoch-8 batch-71 = 0.01558045856654644

Training epoch-8 batch-72
Running loss of epoch-8 batch-72 = 0.01660333201289177

Training epoch-8 batch-73
Running loss of epoch-8 batch-73 = 0.012259768322110176

Training epoch-8 batch-74
Running loss of epoch-8 batch-74 = 0.012873401865363121

Training epoch-8 batch-75
Running loss of epoch-8 batch-75 = 0.015614272095263004

Training epoch-8 batch-76
Running loss of epoch-8 batch-76 = 0.01710604690015316

Training epoch-8 batch-77
Running loss of epoch-8 batch-77 = 0.012401058338582516

Training epoch-8 batch-78
Running loss of epoch-8 batch-78 = 0.013008068315684795

Training epoch-8 batch-79
Running loss of epoch-8 batch-79 = 0.01751895062625408

Training epoch-8 batch-80
Running loss of epoch-8 batch-80 = 0.014480537734925747

Training epoch-8 batch-81
Running loss of epoch-8 batch-81 = 0.0157117061316967

Training epoch-8 batch-82
Running loss of epoch-8 batch-82 = 0.018273884430527687

Training epoch-8 batch-83
Running loss of epoch-8 batch-83 = 0.016595657914876938

Training epoch-8 batch-84
Running loss of epoch-8 batch-84 = 0.01237388327717781

Training epoch-8 batch-85
Running loss of epoch-8 batch-85 = 0.013476123102009296

Training epoch-8 batch-86
Running loss of epoch-8 batch-86 = 0.01369372010231018

Training epoch-8 batch-87
Running loss of epoch-8 batch-87 = 0.01219063438475132

Training epoch-8 batch-88
Running loss of epoch-8 batch-88 = 0.013504821807146072

Training epoch-8 batch-89
Running loss of epoch-8 batch-89 = 0.015439397655427456

Training epoch-8 batch-90
Running loss of epoch-8 batch-90 = 0.014270870946347713

Training epoch-8 batch-91
Running loss of epoch-8 batch-91 = 0.01727760024368763

Training epoch-8 batch-92
Running loss of epoch-8 batch-92 = 0.015644412487745285

Training epoch-8 batch-93
Running loss of epoch-8 batch-93 = 0.017230257391929626

Training epoch-8 batch-94
Running loss of epoch-8 batch-94 = 0.012973028235137463

Training epoch-8 batch-95
Running loss of epoch-8 batch-95 = 0.011641266755759716

Training epoch-8 batch-96
Running loss of epoch-8 batch-96 = 0.014390839263796806

Training epoch-8 batch-97
Running loss of epoch-8 batch-97 = 0.012608282268047333

Training epoch-8 batch-98
Running loss of epoch-8 batch-98 = 0.01072151679545641

Training epoch-8 batch-99
Running loss of epoch-8 batch-99 = 0.012547173537313938

Training epoch-8 batch-100
Running loss of epoch-8 batch-100 = 0.014253171160817146

Training epoch-8 batch-101
Running loss of epoch-8 batch-101 = 0.017738470807671547

Training epoch-8 batch-102
Running loss of epoch-8 batch-102 = 0.015859274193644524

Training epoch-8 batch-103
Running loss of epoch-8 batch-103 = 0.014715664088726044

Training epoch-8 batch-104
Running loss of epoch-8 batch-104 = 0.018053431063890457

Training epoch-8 batch-105
Running loss of epoch-8 batch-105 = 0.013226729817688465

Training epoch-8 batch-106
Running loss of epoch-8 batch-106 = 0.012664349749684334

Training epoch-8 batch-107
Running loss of epoch-8 batch-107 = 0.012950300239026546

Training epoch-8 batch-108
Running loss of epoch-8 batch-108 = 0.01304748933762312

Training epoch-8 batch-109
Running loss of epoch-8 batch-109 = 0.012971222400665283

Training epoch-8 batch-110
Running loss of epoch-8 batch-110 = 0.008872805163264275

Training epoch-8 batch-111
Running loss of epoch-8 batch-111 = 0.013818835839629173

Training epoch-8 batch-112
Running loss of epoch-8 batch-112 = 0.013074420392513275

Training epoch-8 batch-113
Running loss of epoch-8 batch-113 = 0.013004641979932785

Training epoch-8 batch-114
Running loss of epoch-8 batch-114 = 0.011788039468228817

Training epoch-8 batch-115
Running loss of epoch-8 batch-115 = 0.014886351302266121

Training epoch-8 batch-116
Running loss of epoch-8 batch-116 = 0.017777686938643456

Training epoch-8 batch-117
Running loss of epoch-8 batch-117 = 0.01386653445661068

Training epoch-8 batch-118
Running loss of epoch-8 batch-118 = 0.014276089146733284

Training epoch-8 batch-119
Running loss of epoch-8 batch-119 = 0.013198661617934704

Training epoch-8 batch-120
Running loss of epoch-8 batch-120 = 0.012133251875638962

Training epoch-8 batch-121
Running loss of epoch-8 batch-121 = 0.010182823985815048

Training epoch-8 batch-122
Running loss of epoch-8 batch-122 = 0.013330325484275818

Training epoch-8 batch-123
Running loss of epoch-8 batch-123 = 0.01160252932459116

Training epoch-8 batch-124
Running loss of epoch-8 batch-124 = 0.014927258715033531

Training epoch-8 batch-125
Running loss of epoch-8 batch-125 = 0.012257901020348072

Training epoch-8 batch-126
Running loss of epoch-8 batch-126 = 0.01135971024632454

Training epoch-8 batch-127
Running loss of epoch-8 batch-127 = 0.014877616427838802

Training epoch-8 batch-128
Running loss of epoch-8 batch-128 = 0.013325018808245659

Training epoch-8 batch-129
Running loss of epoch-8 batch-129 = 0.011734897270798683

Training epoch-8 batch-130
Running loss of epoch-8 batch-130 = 0.01594555750489235

Training epoch-8 batch-131
Running loss of epoch-8 batch-131 = 0.013213242404162884

Training epoch-8 batch-132
Running loss of epoch-8 batch-132 = 0.014178925193846226

Training epoch-8 batch-133
Running loss of epoch-8 batch-133 = 0.014516658149659634

Training epoch-8 batch-134
Running loss of epoch-8 batch-134 = 0.012265913188457489

Training epoch-8 batch-135
Running loss of epoch-8 batch-135 = 0.008803940378129482

Training epoch-8 batch-136
Running loss of epoch-8 batch-136 = 0.007630219683051109

Training epoch-8 batch-137
Running loss of epoch-8 batch-137 = 0.012076801620423794

Training epoch-8 batch-138
Running loss of epoch-8 batch-138 = 0.011712933890521526

Training epoch-8 batch-139
Running loss of epoch-8 batch-139 = 0.011923233047127724

Training epoch-8 batch-140
Running loss of epoch-8 batch-140 = 0.015638940036296844

Training epoch-8 batch-141
Running loss of epoch-8 batch-141 = 0.015918156132102013

Training epoch-8 batch-142
Running loss of epoch-8 batch-142 = 0.011289446614682674

Training epoch-8 batch-143
Running loss of epoch-8 batch-143 = 0.01086369063705206

Training epoch-8 batch-144
Running loss of epoch-8 batch-144 = 0.013261377811431885

Training epoch-8 batch-145
Running loss of epoch-8 batch-145 = 0.011189593002200127

Training epoch-8 batch-146
Running loss of epoch-8 batch-146 = 0.015623554587364197

Training epoch-8 batch-147
Running loss of epoch-8 batch-147 = 0.012419001199305058

Training epoch-8 batch-148
Running loss of epoch-8 batch-148 = 0.012247840873897076

Training epoch-8 batch-149
Running loss of epoch-8 batch-149 = 0.014678213745355606

Training epoch-8 batch-150
Running loss of epoch-8 batch-150 = 0.012216472998261452

Training epoch-8 batch-151
Running loss of epoch-8 batch-151 = 0.0094774030148983

Training epoch-8 batch-152
Running loss of epoch-8 batch-152 = 0.009561365470290184

Training epoch-8 batch-153
Running loss of epoch-8 batch-153 = 0.010561351664364338

Training epoch-8 batch-154
Running loss of epoch-8 batch-154 = 0.012394437566399574

Training epoch-8 batch-155
Running loss of epoch-8 batch-155 = 0.012114204466342926

Training epoch-8 batch-156
Running loss of epoch-8 batch-156 = 0.009343032725155354

Training epoch-8 batch-157
Running loss of epoch-8 batch-157 = 0.04920858144760132

Finished training epoch-8.



Average train loss at epoch-8 = 0.014633035296201706

Started Evaluation

Average val loss at epoch-8 = 0.7457913532608041

Accuracy for classes:
Accuracy for class equals is: 91.09 %
Accuracy for class main is: 97.21 %
Accuracy for class setUp is: 82.13 %
Accuracy for class onCreate is: 80.60 %
Accuracy for class toString is: 84.64 %
Accuracy for class run is: 70.09 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 3.21 %
Accuracy for class get is: 56.41 %

Overall Accuracy = 76.05 %


Best Accuracy = 76.05 % at Epoch-8


Saving model after best epoch-8
Finished Evaluation



Started training epoch-9


Training epoch-9 batch-1
Running loss of epoch-9 batch-1 = 0.01230545248836279

Training epoch-9 batch-2
Running loss of epoch-9 batch-2 = 0.01271736714988947

Training epoch-9 batch-3
Running loss of epoch-9 batch-3 = 0.013833983801305294

Training epoch-9 batch-4
Running loss of epoch-9 batch-4 = 0.008575358428061008

Training epoch-9 batch-5
Running loss of epoch-9 batch-5 = 0.01413755863904953

Training epoch-9 batch-6
Running loss of epoch-9 batch-6 = 0.012574157677590847

Training epoch-9 batch-7
Running loss of epoch-9 batch-7 = 0.016511917114257812

Training epoch-9 batch-8
Running loss of epoch-9 batch-8 = 0.013262289576232433

Training epoch-9 batch-9
Running loss of epoch-9 batch-9 = 0.009967663325369358

Training epoch-9 batch-10
Running loss of epoch-9 batch-10 = 0.00988089945167303

Training epoch-9 batch-11
Running loss of epoch-9 batch-11 = 0.013853471726179123

Training epoch-9 batch-12
Running loss of epoch-9 batch-12 = 0.014340770430862904

Training epoch-9 batch-13
Running loss of epoch-9 batch-13 = 0.011285320855677128

Training epoch-9 batch-14
Running loss of epoch-9 batch-14 = 0.01222789753228426

Training epoch-9 batch-15
Running loss of epoch-9 batch-15 = 0.008475152775645256

Training epoch-9 batch-16
Running loss of epoch-9 batch-16 = 0.011343519203364849

Training epoch-9 batch-17
Running loss of epoch-9 batch-17 = 0.007877085357904434

Training epoch-9 batch-18
Running loss of epoch-9 batch-18 = 0.016227317973971367

Training epoch-9 batch-19
Running loss of epoch-9 batch-19 = 0.009054725989699364

Training epoch-9 batch-20
Running loss of epoch-9 batch-20 = 0.012672871351242065

Training epoch-9 batch-21
Running loss of epoch-9 batch-21 = 0.00883188471198082

Training epoch-9 batch-22
Running loss of epoch-9 batch-22 = 0.010101393796503544

Training epoch-9 batch-23
Running loss of epoch-9 batch-23 = 0.012304111383855343

Training epoch-9 batch-24
Running loss of epoch-9 batch-24 = 0.012995530851185322

Training epoch-9 batch-25
Running loss of epoch-9 batch-25 = 0.012467533349990845

Training epoch-9 batch-26
Running loss of epoch-9 batch-26 = 0.010566587559878826

Training epoch-9 batch-27
Running loss of epoch-9 batch-27 = 0.010021541267633438

Training epoch-9 batch-28
Running loss of epoch-9 batch-28 = 0.01081441156566143

Training epoch-9 batch-29
Running loss of epoch-9 batch-29 = 0.009551562368869781

Training epoch-9 batch-30
Running loss of epoch-9 batch-30 = 0.012465273961424828

Training epoch-9 batch-31
Running loss of epoch-9 batch-31 = 0.011911517940461636

Training epoch-9 batch-32
Running loss of epoch-9 batch-32 = 0.01075146347284317

Training epoch-9 batch-33
Running loss of epoch-9 batch-33 = 0.008570745587348938

Training epoch-9 batch-34
Running loss of epoch-9 batch-34 = 0.011647428385913372

Training epoch-9 batch-35
Running loss of epoch-9 batch-35 = 0.009062489494681358

Training epoch-9 batch-36
Running loss of epoch-9 batch-36 = 0.01093249674886465

Training epoch-9 batch-37
Running loss of epoch-9 batch-37 = 0.012001627124845982

Training epoch-9 batch-38
Running loss of epoch-9 batch-38 = 0.01055891066789627

Training epoch-9 batch-39
Running loss of epoch-9 batch-39 = 0.010634895414113998

Training epoch-9 batch-40
Running loss of epoch-9 batch-40 = 0.011418179608881474

Training epoch-9 batch-41
Running loss of epoch-9 batch-41 = 0.011383406817913055

Training epoch-9 batch-42
Running loss of epoch-9 batch-42 = 0.010490170679986477

Training epoch-9 batch-43
Running loss of epoch-9 batch-43 = 0.011108217760920525

Training epoch-9 batch-44
Running loss of epoch-9 batch-44 = 0.009639345109462738

Training epoch-9 batch-45
Running loss of epoch-9 batch-45 = 0.010239193215966225

Training epoch-9 batch-46
Running loss of epoch-9 batch-46 = 0.011316902935504913

Training epoch-9 batch-47
Running loss of epoch-9 batch-47 = 0.00928745698183775

Training epoch-9 batch-48
Running loss of epoch-9 batch-48 = 0.010959262028336525

Training epoch-9 batch-49
Running loss of epoch-9 batch-49 = 0.010026144795119762

Training epoch-9 batch-50
Running loss of epoch-9 batch-50 = 0.009122896939516068

Training epoch-9 batch-51
Running loss of epoch-9 batch-51 = 0.010666397400200367

Training epoch-9 batch-52
Running loss of epoch-9 batch-52 = 0.008625947870314121

Training epoch-9 batch-53
Running loss of epoch-9 batch-53 = 0.010271172970533371

Training epoch-9 batch-54
Running loss of epoch-9 batch-54 = 0.009874861687421799

Training epoch-9 batch-55
Running loss of epoch-9 batch-55 = 0.008119508624076843

Training epoch-9 batch-56
Running loss of epoch-9 batch-56 = 0.015005357563495636

Training epoch-9 batch-57
Running loss of epoch-9 batch-57 = 0.009618930518627167

Training epoch-9 batch-58
Running loss of epoch-9 batch-58 = 0.009680763818323612

Training epoch-9 batch-59
Running loss of epoch-9 batch-59 = 0.009274796582758427

Training epoch-9 batch-60
Running loss of epoch-9 batch-60 = 0.011410502716898918

Training epoch-9 batch-61
Running loss of epoch-9 batch-61 = 0.00794204231351614

Training epoch-9 batch-62
Running loss of epoch-9 batch-62 = 0.012201854959130287

Training epoch-9 batch-63
Running loss of epoch-9 batch-63 = 0.010169558227062225

Training epoch-9 batch-64
Running loss of epoch-9 batch-64 = 0.00850673858076334

Training epoch-9 batch-65
Running loss of epoch-9 batch-65 = 0.01399816945195198

Training epoch-9 batch-66
Running loss of epoch-9 batch-66 = 0.007023897022008896

Training epoch-9 batch-67
Running loss of epoch-9 batch-67 = 0.010000639595091343

Training epoch-9 batch-68
Running loss of epoch-9 batch-68 = 0.010395713150501251

Training epoch-9 batch-69
Running loss of epoch-9 batch-69 = 0.010639788582921028

Training epoch-9 batch-70
Running loss of epoch-9 batch-70 = 0.00873821321874857

Training epoch-9 batch-71
Running loss of epoch-9 batch-71 = 0.0119393952190876

Training epoch-9 batch-72
Running loss of epoch-9 batch-72 = 0.011382334865629673

Training epoch-9 batch-73
Running loss of epoch-9 batch-73 = 0.008917565457522869

Training epoch-9 batch-74
Running loss of epoch-9 batch-74 = 0.009718292392790318

Training epoch-9 batch-75
Running loss of epoch-9 batch-75 = 0.014184685423970222

Training epoch-9 batch-76
Running loss of epoch-9 batch-76 = 0.010274822823703289

Training epoch-9 batch-77
Running loss of epoch-9 batch-77 = 0.012938574887812138

Training epoch-9 batch-78
Running loss of epoch-9 batch-78 = 0.010690150782465935

Training epoch-9 batch-79
Running loss of epoch-9 batch-79 = 0.008243922144174576

Training epoch-9 batch-80
Running loss of epoch-9 batch-80 = 0.009183865040540695

Training epoch-9 batch-81
Running loss of epoch-9 batch-81 = 0.014563742093741894

Training epoch-9 batch-82
Running loss of epoch-9 batch-82 = 0.008690227754414082

Training epoch-9 batch-83
Running loss of epoch-9 batch-83 = 0.010737832635641098

Training epoch-9 batch-84
Running loss of epoch-9 batch-84 = 0.010175343602895737

Training epoch-9 batch-85
Running loss of epoch-9 batch-85 = 0.012892229482531548

Training epoch-9 batch-86
Running loss of epoch-9 batch-86 = 0.012427737936377525

Training epoch-9 batch-87
Running loss of epoch-9 batch-87 = 0.012886203825473785

Training epoch-9 batch-88
Running loss of epoch-9 batch-88 = 0.010525180026888847

Training epoch-9 batch-89
Running loss of epoch-9 batch-89 = 0.008917314931750298

Training epoch-9 batch-90
Running loss of epoch-9 batch-90 = 0.010726592503488064

Training epoch-9 batch-91
Running loss of epoch-9 batch-91 = 0.010357826016843319

Training epoch-9 batch-92
Running loss of epoch-9 batch-92 = 0.011634663678705692

Training epoch-9 batch-93
Running loss of epoch-9 batch-93 = 0.009672792628407478

Training epoch-9 batch-94
Running loss of epoch-9 batch-94 = 0.011473886668682098

Training epoch-9 batch-95
Running loss of epoch-9 batch-95 = 0.008399279788136482

Training epoch-9 batch-96
Running loss of epoch-9 batch-96 = 0.014092527329921722

Training epoch-9 batch-97
Running loss of epoch-9 batch-97 = 0.008265637792646885

Training epoch-9 batch-98
Running loss of epoch-9 batch-98 = 0.01003903616219759

Training epoch-9 batch-99
Running loss of epoch-9 batch-99 = 0.00889382604509592

Training epoch-9 batch-100
Running loss of epoch-9 batch-100 = 0.01282387226819992

Training epoch-9 batch-101
Running loss of epoch-9 batch-101 = 0.00904895644634962

Training epoch-9 batch-102
Running loss of epoch-9 batch-102 = 0.009781572967767715

Training epoch-9 batch-103
Running loss of epoch-9 batch-103 = 0.009477294981479645

Training epoch-9 batch-104
Running loss of epoch-9 batch-104 = 0.008718057535588741

Training epoch-9 batch-105
Running loss of epoch-9 batch-105 = 0.00785021297633648

Training epoch-9 batch-106
Running loss of epoch-9 batch-106 = 0.008709502406418324

Training epoch-9 batch-107
Running loss of epoch-9 batch-107 = 0.012352071702480316

Training epoch-9 batch-108
Running loss of epoch-9 batch-108 = 0.011320607736706734

Training epoch-9 batch-109
Running loss of epoch-9 batch-109 = 0.00987753365188837

Training epoch-9 batch-110
Running loss of epoch-9 batch-110 = 0.009438426233828068

Training epoch-9 batch-111
Running loss of epoch-9 batch-111 = 0.008827459067106247

Training epoch-9 batch-112
Running loss of epoch-9 batch-112 = 0.009081817232072353

Training epoch-9 batch-113
Running loss of epoch-9 batch-113 = 0.0073416451923549175

Training epoch-9 batch-114
Running loss of epoch-9 batch-114 = 0.010426743887364864

Training epoch-9 batch-115
Running loss of epoch-9 batch-115 = 0.010894205421209335

Training epoch-9 batch-116
Running loss of epoch-9 batch-116 = 0.010223657824099064

Training epoch-9 batch-117
Running loss of epoch-9 batch-117 = 0.013481092639267445

Training epoch-9 batch-118
Running loss of epoch-9 batch-118 = 0.012414410710334778

Training epoch-9 batch-119
Running loss of epoch-9 batch-119 = 0.00951125007122755

Training epoch-9 batch-120
Running loss of epoch-9 batch-120 = 0.009776899591088295

Training epoch-9 batch-121
Running loss of epoch-9 batch-121 = 0.010189913213253021

Training epoch-9 batch-122
Running loss of epoch-9 batch-122 = 0.010552005842328072

Training epoch-9 batch-123
Running loss of epoch-9 batch-123 = 0.009521830826997757

Training epoch-9 batch-124
Running loss of epoch-9 batch-124 = 0.009797724895179272

Training epoch-9 batch-125
Running loss of epoch-9 batch-125 = 0.015567686408758163

Training epoch-9 batch-126
Running loss of epoch-9 batch-126 = 0.011046912521123886

Training epoch-9 batch-127
Running loss of epoch-9 batch-127 = 0.010179117321968079

Training epoch-9 batch-128
Running loss of epoch-9 batch-128 = 0.012074694968760014

Training epoch-9 batch-129
Running loss of epoch-9 batch-129 = 0.00946894008666277

Training epoch-9 batch-130
Running loss of epoch-9 batch-130 = 0.011494724079966545

Training epoch-9 batch-131
Running loss of epoch-9 batch-131 = 0.008901288732886314

Training epoch-9 batch-132
Running loss of epoch-9 batch-132 = 0.0088075315579772

Training epoch-9 batch-133
Running loss of epoch-9 batch-133 = 0.009547860361635685

Training epoch-9 batch-134
Running loss of epoch-9 batch-134 = 0.005677743814885616

Training epoch-9 batch-135
Running loss of epoch-9 batch-135 = 0.00832087267190218

Training epoch-9 batch-136
Running loss of epoch-9 batch-136 = 0.012363349087536335

Training epoch-9 batch-137
Running loss of epoch-9 batch-137 = 0.008161289617419243

Training epoch-9 batch-138
Running loss of epoch-9 batch-138 = 0.01329091191291809

Training epoch-9 batch-139
Running loss of epoch-9 batch-139 = 0.013012693263590336

Training epoch-9 batch-140
Running loss of epoch-9 batch-140 = 0.010448549874126911

Training epoch-9 batch-141
Running loss of epoch-9 batch-141 = 0.008138975128531456

Training epoch-9 batch-142
Running loss of epoch-9 batch-142 = 0.01084950752556324

Training epoch-9 batch-143
Running loss of epoch-9 batch-143 = 0.008146367967128754

Training epoch-9 batch-144
Running loss of epoch-9 batch-144 = 0.01197060290724039

Training epoch-9 batch-145
Running loss of epoch-9 batch-145 = 0.008230047300457954

Training epoch-9 batch-146
Running loss of epoch-9 batch-146 = 0.010549869388341904

Training epoch-9 batch-147
Running loss of epoch-9 batch-147 = 0.01116703450679779

Training epoch-9 batch-148
Running loss of epoch-9 batch-148 = 0.009981543757021427

Training epoch-9 batch-149
Running loss of epoch-9 batch-149 = 0.010750796645879745

Training epoch-9 batch-150
Running loss of epoch-9 batch-150 = 0.013057975098490715

Training epoch-9 batch-151
Running loss of epoch-9 batch-151 = 0.009994348511099815

Training epoch-9 batch-152
Running loss of epoch-9 batch-152 = 0.012106427922844887

Training epoch-9 batch-153
Running loss of epoch-9 batch-153 = 0.006462857127189636

Training epoch-9 batch-154
Running loss of epoch-9 batch-154 = 0.00947555247694254

Training epoch-9 batch-155
Running loss of epoch-9 batch-155 = 0.010150633752346039

Training epoch-9 batch-156
Running loss of epoch-9 batch-156 = 0.00940350815653801

Training epoch-9 batch-157
Running loss of epoch-9 batch-157 = 0.028425998985767365

Finished training epoch-9.



Average train loss at epoch-9 = 0.010653844401240349

Started Evaluation

Average val loss at epoch-9 = 0.6449051863562903

Accuracy for classes:
Accuracy for class equals is: 92.57 %
Accuracy for class main is: 94.92 %
Accuracy for class setUp is: 91.97 %
Accuracy for class onCreate is: 85.71 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 74.43 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 38.34 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 78.77 %


Best Accuracy = 78.77 % at Epoch-9


Saving model after best epoch-9
Finished Evaluation



Started training epoch-10


Training epoch-10 batch-1
Running loss of epoch-10 batch-1 = 0.009247162379324436

Training epoch-10 batch-2
Running loss of epoch-10 batch-2 = 0.008629781194031239

Training epoch-10 batch-3
Running loss of epoch-10 batch-3 = 0.007901465520262718

Training epoch-10 batch-4
Running loss of epoch-10 batch-4 = 0.010845030657947063

Training epoch-10 batch-5
Running loss of epoch-10 batch-5 = 0.009793746285140514

Training epoch-10 batch-6
Running loss of epoch-10 batch-6 = 0.010050815530121326

Training epoch-10 batch-7
Running loss of epoch-10 batch-7 = 0.011809570714831352

Training epoch-10 batch-8
Running loss of epoch-10 batch-8 = 0.01020639855414629

Training epoch-10 batch-9
Running loss of epoch-10 batch-9 = 0.007047194521874189

Training epoch-10 batch-10
Running loss of epoch-10 batch-10 = 0.007591253146529198

Training epoch-10 batch-11
Running loss of epoch-10 batch-11 = 0.005007656291127205

Training epoch-10 batch-12
Running loss of epoch-10 batch-12 = 0.00960858166217804

Training epoch-10 batch-13
Running loss of epoch-10 batch-13 = 0.008987443521618843

Training epoch-10 batch-14
Running loss of epoch-10 batch-14 = 0.010768232867121696

Training epoch-10 batch-15
Running loss of epoch-10 batch-15 = 0.010564692318439484

Training epoch-10 batch-16
Running loss of epoch-10 batch-16 = 0.005601212847977877

Training epoch-10 batch-17
Running loss of epoch-10 batch-17 = 0.008738457225263119

Training epoch-10 batch-18
Running loss of epoch-10 batch-18 = 0.011333752423524857

Training epoch-10 batch-19
Running loss of epoch-10 batch-19 = 0.010394498705863953

Training epoch-10 batch-20
Running loss of epoch-10 batch-20 = 0.00937038753181696

Training epoch-10 batch-21
Running loss of epoch-10 batch-21 = 0.006826429627835751

Training epoch-10 batch-22
Running loss of epoch-10 batch-22 = 0.010796025395393372

Training epoch-10 batch-23
Running loss of epoch-10 batch-23 = 0.00877087377011776

Training epoch-10 batch-24
Running loss of epoch-10 batch-24 = 0.0070760780945420265

Training epoch-10 batch-25
Running loss of epoch-10 batch-25 = 0.008578134700655937

Training epoch-10 batch-26
Running loss of epoch-10 batch-26 = 0.006350015755742788

Training epoch-10 batch-27
Running loss of epoch-10 batch-27 = 0.008169051259756088

Training epoch-10 batch-28
Running loss of epoch-10 batch-28 = 0.010604256764054298

Training epoch-10 batch-29
Running loss of epoch-10 batch-29 = 0.008883991278707981

Training epoch-10 batch-30
Running loss of epoch-10 batch-30 = 0.008891436271369457

Training epoch-10 batch-31
Running loss of epoch-10 batch-31 = 0.009161796420812607

Training epoch-10 batch-32
Running loss of epoch-10 batch-32 = 0.010531355626881123

Training epoch-10 batch-33
Running loss of epoch-10 batch-33 = 0.007516217418015003

Training epoch-10 batch-34
Running loss of epoch-10 batch-34 = 0.007508857175707817

Training epoch-10 batch-35
Running loss of epoch-10 batch-35 = 0.010804257355630398

Training epoch-10 batch-36
Running loss of epoch-10 batch-36 = 0.008394167758524418

Training epoch-10 batch-37
Running loss of epoch-10 batch-37 = 0.011868768371641636

Training epoch-10 batch-38
Running loss of epoch-10 batch-38 = 0.012303562834858894

Training epoch-10 batch-39
Running loss of epoch-10 batch-39 = 0.00946455541998148

Training epoch-10 batch-40
Running loss of epoch-10 batch-40 = 0.010479053482413292

Training epoch-10 batch-41
Running loss of epoch-10 batch-41 = 0.011409156024456024

Training epoch-10 batch-42
Running loss of epoch-10 batch-42 = 0.01308770477771759

Training epoch-10 batch-43
Running loss of epoch-10 batch-43 = 0.009945094585418701

Training epoch-10 batch-44
Running loss of epoch-10 batch-44 = 0.012279416434466839

Training epoch-10 batch-45
Running loss of epoch-10 batch-45 = 0.00824015773832798

Training epoch-10 batch-46
Running loss of epoch-10 batch-46 = 0.009309656918048859

Training epoch-10 batch-47
Running loss of epoch-10 batch-47 = 0.008169612847268581

Training epoch-10 batch-48
Running loss of epoch-10 batch-48 = 0.009856423363089561

Training epoch-10 batch-49
Running loss of epoch-10 batch-49 = 0.006411971524357796

Training epoch-10 batch-50
Running loss of epoch-10 batch-50 = 0.00795048289000988

Training epoch-10 batch-51
Running loss of epoch-10 batch-51 = 0.009765880182385445

Training epoch-10 batch-52
Running loss of epoch-10 batch-52 = 0.011286660097539425

Training epoch-10 batch-53
Running loss of epoch-10 batch-53 = 0.0064564975909888744

Training epoch-10 batch-54
Running loss of epoch-10 batch-54 = 0.00970473699271679

Training epoch-10 batch-55
Running loss of epoch-10 batch-55 = 0.00870021153241396

Training epoch-10 batch-56
Running loss of epoch-10 batch-56 = 0.011075523681938648

Training epoch-10 batch-57
Running loss of epoch-10 batch-57 = 0.01229572482407093

Training epoch-10 batch-58
Running loss of epoch-10 batch-58 = 0.010322576388716698

Training epoch-10 batch-59
Running loss of epoch-10 batch-59 = 0.010722608305513859

Training epoch-10 batch-60
Running loss of epoch-10 batch-60 = 0.009907937608659267

Training epoch-10 batch-61
Running loss of epoch-10 batch-61 = 0.007399565540254116

Training epoch-10 batch-62
Running loss of epoch-10 batch-62 = 0.011240369640290737

Training epoch-10 batch-63
Running loss of epoch-10 batch-63 = 0.013862881809473038

Training epoch-10 batch-64
Running loss of epoch-10 batch-64 = 0.007165011018514633

Training epoch-10 batch-65
Running loss of epoch-10 batch-65 = 0.012905270792543888

Training epoch-10 batch-66
Running loss of epoch-10 batch-66 = 0.00778721971437335

Training epoch-10 batch-67
Running loss of epoch-10 batch-67 = 0.010030611418187618

Training epoch-10 batch-68
Running loss of epoch-10 batch-68 = 0.00783184077590704

Training epoch-10 batch-69
Running loss of epoch-10 batch-69 = 0.01205095648765564

Training epoch-10 batch-70
Running loss of epoch-10 batch-70 = 0.010835747234523296

Training epoch-10 batch-71
Running loss of epoch-10 batch-71 = 0.006348775699734688

Training epoch-10 batch-72
Running loss of epoch-10 batch-72 = 0.008612955920398235

Training epoch-10 batch-73
Running loss of epoch-10 batch-73 = 0.006000842899084091

Training epoch-10 batch-74
Running loss of epoch-10 batch-74 = 0.007803125306963921

Training epoch-10 batch-75
Running loss of epoch-10 batch-75 = 0.008762741461396217

Training epoch-10 batch-76
Running loss of epoch-10 batch-76 = 0.004674813710153103

Training epoch-10 batch-77
Running loss of epoch-10 batch-77 = 0.008237569592893124

Training epoch-10 batch-78
Running loss of epoch-10 batch-78 = 0.008119156584143639

Training epoch-10 batch-79
Running loss of epoch-10 batch-79 = 0.008580704219639301

Training epoch-10 batch-80
Running loss of epoch-10 batch-80 = 0.008420368656516075

Training epoch-10 batch-81
Running loss of epoch-10 batch-81 = 0.01191403716802597

Training epoch-10 batch-82
Running loss of epoch-10 batch-82 = 0.006019279360771179

Training epoch-10 batch-83
Running loss of epoch-10 batch-83 = 0.010715587064623833

Training epoch-10 batch-84
Running loss of epoch-10 batch-84 = 0.00760180689394474

Training epoch-10 batch-85
Running loss of epoch-10 batch-85 = 0.00680084154009819

Training epoch-10 batch-86
Running loss of epoch-10 batch-86 = 0.00575336255133152

Training epoch-10 batch-87
Running loss of epoch-10 batch-87 = 0.006727697793394327

Training epoch-10 batch-88
Running loss of epoch-10 batch-88 = 0.005817058961838484

Training epoch-10 batch-89
Running loss of epoch-10 batch-89 = 0.00904638972133398

Training epoch-10 batch-90
Running loss of epoch-10 batch-90 = 0.008554250001907349

Training epoch-10 batch-91
Running loss of epoch-10 batch-91 = 0.010300014168024063

Training epoch-10 batch-92
Running loss of epoch-10 batch-92 = 0.008920853026211262

Training epoch-10 batch-93
Running loss of epoch-10 batch-93 = 0.010283980518579483

Training epoch-10 batch-94
Running loss of epoch-10 batch-94 = 0.006654904689639807

Training epoch-10 batch-95
Running loss of epoch-10 batch-95 = 0.006382999010384083

Training epoch-10 batch-96
Running loss of epoch-10 batch-96 = 0.00673556188121438

Training epoch-10 batch-97
Running loss of epoch-10 batch-97 = 0.00758300069719553

Training epoch-10 batch-98
Running loss of epoch-10 batch-98 = 0.0064679100178182125

Training epoch-10 batch-99
Running loss of epoch-10 batch-99 = 0.007860539481043816

Training epoch-10 batch-100
Running loss of epoch-10 batch-100 = 0.00856462586671114

Training epoch-10 batch-101
Running loss of epoch-10 batch-101 = 0.008671718649566174

Training epoch-10 batch-102
Running loss of epoch-10 batch-102 = 0.009392259642481804

Training epoch-10 batch-103
Running loss of epoch-10 batch-103 = 0.006263538729399443

Training epoch-10 batch-104
Running loss of epoch-10 batch-104 = 0.008402659557759762

Training epoch-10 batch-105
Running loss of epoch-10 batch-105 = 0.00594930537045002

Training epoch-10 batch-106
Running loss of epoch-10 batch-106 = 0.007010339759290218

Training epoch-10 batch-107
Running loss of epoch-10 batch-107 = 0.007600503973662853

Training epoch-10 batch-108
Running loss of epoch-10 batch-108 = 0.008901574648916721

Training epoch-10 batch-109
Running loss of epoch-10 batch-109 = 0.010323278605937958

Training epoch-10 batch-110
Running loss of epoch-10 batch-110 = 0.006842856761068106

Training epoch-10 batch-111
Running loss of epoch-10 batch-111 = 0.007224694825708866

Training epoch-10 batch-112
Running loss of epoch-10 batch-112 = 0.0079074427485466

Training epoch-10 batch-113
Running loss of epoch-10 batch-113 = 0.004900435917079449

Training epoch-10 batch-114
Running loss of epoch-10 batch-114 = 0.009417552500963211

Training epoch-10 batch-115
Running loss of epoch-10 batch-115 = 0.007096286863088608

Training epoch-10 batch-116
Running loss of epoch-10 batch-116 = 0.008545802906155586

Training epoch-10 batch-117
Running loss of epoch-10 batch-117 = 0.008975128643214703

Training epoch-10 batch-118
Running loss of epoch-10 batch-118 = 0.006380746141076088

Training epoch-10 batch-119
Running loss of epoch-10 batch-119 = 0.008488059043884277

Training epoch-10 batch-120
Running loss of epoch-10 batch-120 = 0.006797474343329668

Training epoch-10 batch-121
Running loss of epoch-10 batch-121 = 0.01055474579334259

Training epoch-10 batch-122
Running loss of epoch-10 batch-122 = 0.009901175275444984

Training epoch-10 batch-123
Running loss of epoch-10 batch-123 = 0.007430105470120907

Training epoch-10 batch-124
Running loss of epoch-10 batch-124 = 0.0059019289910793304

Training epoch-10 batch-125
Running loss of epoch-10 batch-125 = 0.006623837631195784

Training epoch-10 batch-126
Running loss of epoch-10 batch-126 = 0.011593391187489033

Training epoch-10 batch-127
Running loss of epoch-10 batch-127 = 0.009043488651514053

Training epoch-10 batch-128
Running loss of epoch-10 batch-128 = 0.007149507757276297

Training epoch-10 batch-129
Running loss of epoch-10 batch-129 = 0.0095862140879035

Training epoch-10 batch-130
Running loss of epoch-10 batch-130 = 0.0066931648179888725

Training epoch-10 batch-131
Running loss of epoch-10 batch-131 = 0.0086791617795825

Training epoch-10 batch-132
Running loss of epoch-10 batch-132 = 0.00722049456089735

Training epoch-10 batch-133
Running loss of epoch-10 batch-133 = 0.007561902981251478

Training epoch-10 batch-134
Running loss of epoch-10 batch-134 = 0.009263644926249981

Training epoch-10 batch-135
Running loss of epoch-10 batch-135 = 0.006836613640189171

Training epoch-10 batch-136
Running loss of epoch-10 batch-136 = 0.00550867710262537

Training epoch-10 batch-137
Running loss of epoch-10 batch-137 = 0.008723773062229156

Training epoch-10 batch-138
Running loss of epoch-10 batch-138 = 0.008710553869605064

Training epoch-10 batch-139
Running loss of epoch-10 batch-139 = 0.004844815004616976

Training epoch-10 batch-140
Running loss of epoch-10 batch-140 = 0.009470977820456028

Training epoch-10 batch-141
Running loss of epoch-10 batch-141 = 0.007396392989903688

Training epoch-10 batch-142
Running loss of epoch-10 batch-142 = 0.006057153455913067

Training epoch-10 batch-143
Running loss of epoch-10 batch-143 = 0.009648646228015423

Training epoch-10 batch-144
Running loss of epoch-10 batch-144 = 0.009627193212509155

Training epoch-10 batch-145
Running loss of epoch-10 batch-145 = 0.008325738832354546

Training epoch-10 batch-146
Running loss of epoch-10 batch-146 = 0.009092508815228939

Training epoch-10 batch-147
Running loss of epoch-10 batch-147 = 0.008262141607701778

Training epoch-10 batch-148
Running loss of epoch-10 batch-148 = 0.007278164383023977

Training epoch-10 batch-149
Running loss of epoch-10 batch-149 = 0.007330076768994331

Training epoch-10 batch-150
Running loss of epoch-10 batch-150 = 0.005570715758949518

Training epoch-10 batch-151
Running loss of epoch-10 batch-151 = 0.009814407676458359

Training epoch-10 batch-152
Running loss of epoch-10 batch-152 = 0.007902577519416809

Training epoch-10 batch-153
Running loss of epoch-10 batch-153 = 0.009864820167422295

Training epoch-10 batch-154
Running loss of epoch-10 batch-154 = 0.007606929168105125

Training epoch-10 batch-155
Running loss of epoch-10 batch-155 = 0.00713624432682991

Training epoch-10 batch-156
Running loss of epoch-10 batch-156 = 0.007502716034650803

Training epoch-10 batch-157
Running loss of epoch-10 batch-157 = 0.036647167056798935

Finished training epoch-10.



Average train loss at epoch-10 = 0.008657392221689224

Started Evaluation

Average val loss at epoch-10 = 0.5764638836201477

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 95.57 %
Accuracy for class setUp is: 93.93 %
Accuracy for class onCreate is: 91.15 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 56.39 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 75.34 %
Accuracy for class execute is: 35.34 %
Accuracy for class get is: 47.18 %

Overall Accuracy = 81.53 %


Best Accuracy = 81.53 % at Epoch-10


Saving model after best epoch-10
Finished Evaluation



Started training epoch-11


Training epoch-11 batch-1
Running loss of epoch-11 batch-1 = 0.006294610910117626

Training epoch-11 batch-2
Running loss of epoch-11 batch-2 = 0.0068773976527154446

Training epoch-11 batch-3
Running loss of epoch-11 batch-3 = 0.009522593580186367

Training epoch-11 batch-4
Running loss of epoch-11 batch-4 = 0.008612655103206635

Training epoch-11 batch-5
Running loss of epoch-11 batch-5 = 0.008908447809517384

Training epoch-11 batch-6
Running loss of epoch-11 batch-6 = 0.008126061409711838

Training epoch-11 batch-7
Running loss of epoch-11 batch-7 = 0.008358201943337917

Training epoch-11 batch-8
Running loss of epoch-11 batch-8 = 0.007445341441780329

Training epoch-11 batch-9
Running loss of epoch-11 batch-9 = 0.006627622991800308

Training epoch-11 batch-10
Running loss of epoch-11 batch-10 = 0.007125940173864365

Training epoch-11 batch-11
Running loss of epoch-11 batch-11 = 0.006361408159136772

Training epoch-11 batch-12
Running loss of epoch-11 batch-12 = 0.006577013060450554

Training epoch-11 batch-13
Running loss of epoch-11 batch-13 = 0.008480235002934933

Training epoch-11 batch-14
Running loss of epoch-11 batch-14 = 0.0049543255008757114

Training epoch-11 batch-15
Running loss of epoch-11 batch-15 = 0.007908676750957966

Training epoch-11 batch-16
Running loss of epoch-11 batch-16 = 0.007377710193395615

Training epoch-11 batch-17
Running loss of epoch-11 batch-17 = 0.010899141430854797

Training epoch-11 batch-18
Running loss of epoch-11 batch-18 = 0.006008970085531473

Training epoch-11 batch-19
Running loss of epoch-11 batch-19 = 0.007547553163021803

Training epoch-11 batch-20
Running loss of epoch-11 batch-20 = 0.006433064118027687

Training epoch-11 batch-21
Running loss of epoch-11 batch-21 = 0.009834134951233864

Training epoch-11 batch-22
Running loss of epoch-11 batch-22 = 0.005989967379719019

Training epoch-11 batch-23
Running loss of epoch-11 batch-23 = 0.006450201850384474

Training epoch-11 batch-24
Running loss of epoch-11 batch-24 = 0.010538194328546524

Training epoch-11 batch-25
Running loss of epoch-11 batch-25 = 0.007733199745416641

Training epoch-11 batch-26
Running loss of epoch-11 batch-26 = 0.007915306836366653

Training epoch-11 batch-27
Running loss of epoch-11 batch-27 = 0.006419606041163206

Training epoch-11 batch-28
Running loss of epoch-11 batch-28 = 0.007942874915897846

Training epoch-11 batch-29
Running loss of epoch-11 batch-29 = 0.005648879334330559

Training epoch-11 batch-30
Running loss of epoch-11 batch-30 = 0.005699400324374437

Training epoch-11 batch-31
Running loss of epoch-11 batch-31 = 0.006671739276498556

Training epoch-11 batch-32
Running loss of epoch-11 batch-32 = 0.010241626761853695

Training epoch-11 batch-33
Running loss of epoch-11 batch-33 = 0.009120039641857147

Training epoch-11 batch-34
Running loss of epoch-11 batch-34 = 0.0074944146908819675

Training epoch-11 batch-35
Running loss of epoch-11 batch-35 = 0.005706475581973791

Training epoch-11 batch-36
Running loss of epoch-11 batch-36 = 0.00855568703263998

Training epoch-11 batch-37
Running loss of epoch-11 batch-37 = 0.0077311797067523

Training epoch-11 batch-38
Running loss of epoch-11 batch-38 = 0.006960150320082903

Training epoch-11 batch-39
Running loss of epoch-11 batch-39 = 0.006654402241110802

Training epoch-11 batch-40
Running loss of epoch-11 batch-40 = 0.0072251660749316216

Training epoch-11 batch-41
Running loss of epoch-11 batch-41 = 0.010385259985923767

Training epoch-11 batch-42
Running loss of epoch-11 batch-42 = 0.006295536644756794

Training epoch-11 batch-43
Running loss of epoch-11 batch-43 = 0.008276330307126045

Training epoch-11 batch-44
Running loss of epoch-11 batch-44 = 0.00813366286456585

Training epoch-11 batch-45
Running loss of epoch-11 batch-45 = 0.007594143971800804

Training epoch-11 batch-46
Running loss of epoch-11 batch-46 = 0.007244969252496958

Training epoch-11 batch-47
Running loss of epoch-11 batch-47 = 0.006901861168444157

Training epoch-11 batch-48
Running loss of epoch-11 batch-48 = 0.00760470237582922

Training epoch-11 batch-49
Running loss of epoch-11 batch-49 = 0.008834978565573692

Training epoch-11 batch-50
Running loss of epoch-11 batch-50 = 0.006113071460276842

Training epoch-11 batch-51
Running loss of epoch-11 batch-51 = 0.009165124036371708

Training epoch-11 batch-52
Running loss of epoch-11 batch-52 = 0.009275738149881363

Training epoch-11 batch-53
Running loss of epoch-11 batch-53 = 0.004754336085170507

Training epoch-11 batch-54
Running loss of epoch-11 batch-54 = 0.0073427301831543446

Training epoch-11 batch-55
Running loss of epoch-11 batch-55 = 0.010497499257326126

Training epoch-11 batch-56
Running loss of epoch-11 batch-56 = 0.006273526698350906

Training epoch-11 batch-57
Running loss of epoch-11 batch-57 = 0.006340549793094397

Training epoch-11 batch-58
Running loss of epoch-11 batch-58 = 0.007510661613196135

Training epoch-11 batch-59
Running loss of epoch-11 batch-59 = 0.004932792857289314

Training epoch-11 batch-60
Running loss of epoch-11 batch-60 = 0.004487123340368271

Training epoch-11 batch-61
Running loss of epoch-11 batch-61 = 0.004806093405932188

Training epoch-11 batch-62
Running loss of epoch-11 batch-62 = 0.00869261845946312

Training epoch-11 batch-63
Running loss of epoch-11 batch-63 = 0.007242433726787567

Training epoch-11 batch-64
Running loss of epoch-11 batch-64 = 0.008408676832914352

Training epoch-11 batch-65
Running loss of epoch-11 batch-65 = 0.005824331194162369

Training epoch-11 batch-66
Running loss of epoch-11 batch-66 = 0.010419221594929695

Training epoch-11 batch-67
Running loss of epoch-11 batch-67 = 0.006117386743426323

Training epoch-11 batch-68
Running loss of epoch-11 batch-68 = 0.007532600313425064

Training epoch-11 batch-69
Running loss of epoch-11 batch-69 = 0.006529755890369415

Training epoch-11 batch-70
Running loss of epoch-11 batch-70 = 0.0070722405798733234

Training epoch-11 batch-71
Running loss of epoch-11 batch-71 = 0.008432873524725437

Training epoch-11 batch-72
Running loss of epoch-11 batch-72 = 0.007583406753838062

Training epoch-11 batch-73
Running loss of epoch-11 batch-73 = 0.0058000339195132256

Training epoch-11 batch-74
Running loss of epoch-11 batch-74 = 0.005872885696589947

Training epoch-11 batch-75
Running loss of epoch-11 batch-75 = 0.00699791731312871

Training epoch-11 batch-76
Running loss of epoch-11 batch-76 = 0.006614570505917072

Training epoch-11 batch-77
Running loss of epoch-11 batch-77 = 0.007382573559880257

Training epoch-11 batch-78
Running loss of epoch-11 batch-78 = 0.004146225284785032

Training epoch-11 batch-79
Running loss of epoch-11 batch-79 = 0.00490172952413559

Training epoch-11 batch-80
Running loss of epoch-11 batch-80 = 0.005856775213032961

Training epoch-11 batch-81
Running loss of epoch-11 batch-81 = 0.01023902278393507

Training epoch-11 batch-82
Running loss of epoch-11 batch-82 = 0.00775560550391674

Training epoch-11 batch-83
Running loss of epoch-11 batch-83 = 0.009316661395132542

Training epoch-11 batch-84
Running loss of epoch-11 batch-84 = 0.006232455838471651

Training epoch-11 batch-85
Running loss of epoch-11 batch-85 = 0.00897711981087923

Training epoch-11 batch-86
Running loss of epoch-11 batch-86 = 0.006032101809978485

Training epoch-11 batch-87
Running loss of epoch-11 batch-87 = 0.006643453612923622

Training epoch-11 batch-88
Running loss of epoch-11 batch-88 = 0.007883654907345772

Training epoch-11 batch-89
Running loss of epoch-11 batch-89 = 0.007441230583935976

Training epoch-11 batch-90
Running loss of epoch-11 batch-90 = 0.005034156143665314

Training epoch-11 batch-91
Running loss of epoch-11 batch-91 = 0.009588927030563354

Training epoch-11 batch-92
Running loss of epoch-11 batch-92 = 0.007904470898211002

Training epoch-11 batch-93
Running loss of epoch-11 batch-93 = 0.008461630903184414

Training epoch-11 batch-94
Running loss of epoch-11 batch-94 = 0.006963615771383047

Training epoch-11 batch-95
Running loss of epoch-11 batch-95 = 0.005638996604830027

Training epoch-11 batch-96
Running loss of epoch-11 batch-96 = 0.008699902333319187

Training epoch-11 batch-97
Running loss of epoch-11 batch-97 = 0.004856549669057131

Training epoch-11 batch-98
Running loss of epoch-11 batch-98 = 0.005229136906564236

Training epoch-11 batch-99
Running loss of epoch-11 batch-99 = 0.008789047598838806

Training epoch-11 batch-100
Running loss of epoch-11 batch-100 = 0.009036382660269737

Training epoch-11 batch-101
Running loss of epoch-11 batch-101 = 0.004142220132052898

Training epoch-11 batch-102
Running loss of epoch-11 batch-102 = 0.0070286765694618225

Training epoch-11 batch-103
Running loss of epoch-11 batch-103 = 0.009606302715837955

Training epoch-11 batch-104
Running loss of epoch-11 batch-104 = 0.0073099760338664055

Training epoch-11 batch-105
Running loss of epoch-11 batch-105 = 0.005228341091424227

Training epoch-11 batch-106
Running loss of epoch-11 batch-106 = 0.0066800182685256

Training epoch-11 batch-107
Running loss of epoch-11 batch-107 = 0.006923616398125887

Training epoch-11 batch-108
Running loss of epoch-11 batch-108 = 0.004659753292798996

Training epoch-11 batch-109
Running loss of epoch-11 batch-109 = 0.005465026944875717

Training epoch-11 batch-110
Running loss of epoch-11 batch-110 = 0.006828907877206802

Training epoch-11 batch-111
Running loss of epoch-11 batch-111 = 0.006359427236020565

Training epoch-11 batch-112
Running loss of epoch-11 batch-112 = 0.004735142923891544

Training epoch-11 batch-113
Running loss of epoch-11 batch-113 = 0.0061144414357841015

Training epoch-11 batch-114
Running loss of epoch-11 batch-114 = 0.0078701451420784

Training epoch-11 batch-115
Running loss of epoch-11 batch-115 = 0.006733639631420374

Training epoch-11 batch-116
Running loss of epoch-11 batch-116 = 0.00850887130945921

Training epoch-11 batch-117
Running loss of epoch-11 batch-117 = 0.008903056383132935

Training epoch-11 batch-118
Running loss of epoch-11 batch-118 = 0.0072228205390274525

Training epoch-11 batch-119
Running loss of epoch-11 batch-119 = 0.00760686956346035

Training epoch-11 batch-120
Running loss of epoch-11 batch-120 = 0.0058204252272844315

Training epoch-11 batch-121
Running loss of epoch-11 batch-121 = 0.0032003927044570446

Training epoch-11 batch-122
Running loss of epoch-11 batch-122 = 0.005144908558577299

Training epoch-11 batch-123
Running loss of epoch-11 batch-123 = 0.007834414020180702

Training epoch-11 batch-124
Running loss of epoch-11 batch-124 = 0.007146949879825115

Training epoch-11 batch-125
Running loss of epoch-11 batch-125 = 0.005494779907166958

Training epoch-11 batch-126
Running loss of epoch-11 batch-126 = 0.008250835351645947

Training epoch-11 batch-127
Running loss of epoch-11 batch-127 = 0.009845614433288574

Training epoch-11 batch-128
Running loss of epoch-11 batch-128 = 0.009452280588448048

Training epoch-11 batch-129
Running loss of epoch-11 batch-129 = 0.00640803063288331

Training epoch-11 batch-130
Running loss of epoch-11 batch-130 = 0.008341308683156967

Training epoch-11 batch-131
Running loss of epoch-11 batch-131 = 0.00677621690556407

Training epoch-11 batch-132
Running loss of epoch-11 batch-132 = 0.006503412500023842

Training epoch-11 batch-133
Running loss of epoch-11 batch-133 = 0.004128457047045231

Training epoch-11 batch-134
Running loss of epoch-11 batch-134 = 0.007726642768830061

Training epoch-11 batch-135
Running loss of epoch-11 batch-135 = 0.006258023902773857

Training epoch-11 batch-136
Running loss of epoch-11 batch-136 = 0.004753123037517071

Training epoch-11 batch-137
Running loss of epoch-11 batch-137 = 0.006305451970547438

Training epoch-11 batch-138
Running loss of epoch-11 batch-138 = 0.004285998176783323

Training epoch-11 batch-139
Running loss of epoch-11 batch-139 = 0.008441489189863205

Training epoch-11 batch-140
Running loss of epoch-11 batch-140 = 0.004125005099922419

Training epoch-11 batch-141
Running loss of epoch-11 batch-141 = 0.006598056759685278

Training epoch-11 batch-142
Running loss of epoch-11 batch-142 = 0.009187980554997921

Training epoch-11 batch-143
Running loss of epoch-11 batch-143 = 0.00639868900179863

Training epoch-11 batch-144
Running loss of epoch-11 batch-144 = 0.004475558176636696

Training epoch-11 batch-145
Running loss of epoch-11 batch-145 = 0.006317685823887587

Training epoch-11 batch-146
Running loss of epoch-11 batch-146 = 0.005488939117640257

Training epoch-11 batch-147
Running loss of epoch-11 batch-147 = 0.006582106463611126

Training epoch-11 batch-148
Running loss of epoch-11 batch-148 = 0.0059882779605686665

Training epoch-11 batch-149
Running loss of epoch-11 batch-149 = 0.005260057281702757

Training epoch-11 batch-150
Running loss of epoch-11 batch-150 = 0.00607792753726244

Training epoch-11 batch-151
Running loss of epoch-11 batch-151 = 0.006986803375184536

Training epoch-11 batch-152
Running loss of epoch-11 batch-152 = 0.008402219973504543

Training epoch-11 batch-153
Running loss of epoch-11 batch-153 = 0.00812599342316389

Training epoch-11 batch-154
Running loss of epoch-11 batch-154 = 0.0036330881994217634

Training epoch-11 batch-155
Running loss of epoch-11 batch-155 = 0.008181759156286716

Training epoch-11 batch-156
Running loss of epoch-11 batch-156 = 0.0066545093432068825

Training epoch-11 batch-157
Running loss of epoch-11 batch-157 = 0.021196965128183365

Finished training epoch-11.



Average train loss at epoch-11 = 0.007102288617193699

Started Evaluation

Average val loss at epoch-11 = 0.6236916979174375

Accuracy for classes:
Accuracy for class equals is: 93.23 %
Accuracy for class main is: 94.75 %
Accuracy for class setUp is: 96.72 %
Accuracy for class onCreate is: 86.99 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 52.28 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 58.52 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 74.62 %

Overall Accuracy = 80.87 %

Finished Evaluation



Started training epoch-12


Training epoch-12 batch-1
Running loss of epoch-12 batch-1 = 0.007899506948888302

Training epoch-12 batch-2
Running loss of epoch-12 batch-2 = 0.006224398501217365

Training epoch-12 batch-3
Running loss of epoch-12 batch-3 = 0.00490402989089489

Training epoch-12 batch-4
Running loss of epoch-12 batch-4 = 0.008264976553618908

Training epoch-12 batch-5
Running loss of epoch-12 batch-5 = 0.008135234005749226

Training epoch-12 batch-6
Running loss of epoch-12 batch-6 = 0.007208531256765127

Training epoch-12 batch-7
Running loss of epoch-12 batch-7 = 0.004622888285666704

Training epoch-12 batch-8
Running loss of epoch-12 batch-8 = 0.004543081857264042

Training epoch-12 batch-9
Running loss of epoch-12 batch-9 = 0.007735707797110081

Training epoch-12 batch-10
Running loss of epoch-12 batch-10 = 0.006288561504334211

Training epoch-12 batch-11
Running loss of epoch-12 batch-11 = 0.007411221507936716

Training epoch-12 batch-12
Running loss of epoch-12 batch-12 = 0.005365137010812759

Training epoch-12 batch-13
Running loss of epoch-12 batch-13 = 0.009577051736414433

Training epoch-12 batch-14
Running loss of epoch-12 batch-14 = 0.00808275118470192

Training epoch-12 batch-15
Running loss of epoch-12 batch-15 = 0.005136473104357719

Training epoch-12 batch-16
Running loss of epoch-12 batch-16 = 0.007668797392398119

Training epoch-12 batch-17
Running loss of epoch-12 batch-17 = 0.00572481844574213

Training epoch-12 batch-18
Running loss of epoch-12 batch-18 = 0.0058436281979084015

Training epoch-12 batch-19
Running loss of epoch-12 batch-19 = 0.006360526662319899

Training epoch-12 batch-20
Running loss of epoch-12 batch-20 = 0.006163635291159153

Training epoch-12 batch-21
Running loss of epoch-12 batch-21 = 0.004771575331687927

Training epoch-12 batch-22
Running loss of epoch-12 batch-22 = 0.005076390225440264

Training epoch-12 batch-23
Running loss of epoch-12 batch-23 = 0.008026206865906715

Training epoch-12 batch-24
Running loss of epoch-12 batch-24 = 0.007566672749817371

Training epoch-12 batch-25
Running loss of epoch-12 batch-25 = 0.006547444965690374

Training epoch-12 batch-26
Running loss of epoch-12 batch-26 = 0.005890494678169489

Training epoch-12 batch-27
Running loss of epoch-12 batch-27 = 0.005844355560839176

Training epoch-12 batch-28
Running loss of epoch-12 batch-28 = 0.005508212372660637

Training epoch-12 batch-29
Running loss of epoch-12 batch-29 = 0.005752068478614092

Training epoch-12 batch-30
Running loss of epoch-12 batch-30 = 0.0066267563961446285

Training epoch-12 batch-31
Running loss of epoch-12 batch-31 = 0.0036795262712985277

Training epoch-12 batch-32
Running loss of epoch-12 batch-32 = 0.004170353524386883

Training epoch-12 batch-33
Running loss of epoch-12 batch-33 = 0.004808259196579456

Training epoch-12 batch-34
Running loss of epoch-12 batch-34 = 0.0041796923615038395

Training epoch-12 batch-35
Running loss of epoch-12 batch-35 = 0.006452456116676331

Training epoch-12 batch-36
Running loss of epoch-12 batch-36 = 0.0038184525910764933

Training epoch-12 batch-37
Running loss of epoch-12 batch-37 = 0.006508906837552786

Training epoch-12 batch-38
Running loss of epoch-12 batch-38 = 0.009726280346512794

Training epoch-12 batch-39
Running loss of epoch-12 batch-39 = 0.005542785860598087

Training epoch-12 batch-40
Running loss of epoch-12 batch-40 = 0.005598478019237518

Training epoch-12 batch-41
Running loss of epoch-12 batch-41 = 0.006544332019984722

Training epoch-12 batch-42
Running loss of epoch-12 batch-42 = 0.005129014607518911

Training epoch-12 batch-43
Running loss of epoch-12 batch-43 = 0.005000011529773474

Training epoch-12 batch-44
Running loss of epoch-12 batch-44 = 0.003642429830506444

Training epoch-12 batch-45
Running loss of epoch-12 batch-45 = 0.00509330490604043

Training epoch-12 batch-46
Running loss of epoch-12 batch-46 = 0.006352961994707584

Training epoch-12 batch-47
Running loss of epoch-12 batch-47 = 0.003793478012084961

Training epoch-12 batch-48
Running loss of epoch-12 batch-48 = 0.0038945337291806936

Training epoch-12 batch-49
Running loss of epoch-12 batch-49 = 0.0049607702530920506

Training epoch-12 batch-50
Running loss of epoch-12 batch-50 = 0.008586754091084003

Training epoch-12 batch-51
Running loss of epoch-12 batch-51 = 0.00605671014636755

Training epoch-12 batch-52
Running loss of epoch-12 batch-52 = 0.006483616307377815

Training epoch-12 batch-53
Running loss of epoch-12 batch-53 = 0.00742363603785634

Training epoch-12 batch-54
Running loss of epoch-12 batch-54 = 0.006360785569995642

Training epoch-12 batch-55
Running loss of epoch-12 batch-55 = 0.006433903239667416

Training epoch-12 batch-56
Running loss of epoch-12 batch-56 = 0.0023987230379134417

Training epoch-12 batch-57
Running loss of epoch-12 batch-57 = 0.005504710599780083

Training epoch-12 batch-58
Running loss of epoch-12 batch-58 = 0.0082158287987113

Training epoch-12 batch-59
Running loss of epoch-12 batch-59 = 0.010147050954401493

Training epoch-12 batch-60
Running loss of epoch-12 batch-60 = 0.005424837581813335

Training epoch-12 batch-61
Running loss of epoch-12 batch-61 = 0.003910668659955263

Training epoch-12 batch-62
Running loss of epoch-12 batch-62 = 0.0050725615583360195

Training epoch-12 batch-63
Running loss of epoch-12 batch-63 = 0.004342824220657349

Training epoch-12 batch-64
Running loss of epoch-12 batch-64 = 0.004384935833513737

Training epoch-12 batch-65
Running loss of epoch-12 batch-65 = 0.006460709031671286

Training epoch-12 batch-66
Running loss of epoch-12 batch-66 = 0.006141894031316042

Training epoch-12 batch-67
Running loss of epoch-12 batch-67 = 0.003978134132921696

Training epoch-12 batch-68
Running loss of epoch-12 batch-68 = 0.003598492592573166

Training epoch-12 batch-69
Running loss of epoch-12 batch-69 = 0.004184550140053034

Training epoch-12 batch-70
Running loss of epoch-12 batch-70 = 0.0063210297375917435

Training epoch-12 batch-71
Running loss of epoch-12 batch-71 = 0.004573328420519829

Training epoch-12 batch-72
Running loss of epoch-12 batch-72 = 0.010041859932243824

Training epoch-12 batch-73
Running loss of epoch-12 batch-73 = 0.006764402147382498

Training epoch-12 batch-74
Running loss of epoch-12 batch-74 = 0.0034314615186303854

Training epoch-12 batch-75
Running loss of epoch-12 batch-75 = 0.008915490470826626

Training epoch-12 batch-76
Running loss of epoch-12 batch-76 = 0.004834415856748819

Training epoch-12 batch-77
Running loss of epoch-12 batch-77 = 0.007313976529985666

Training epoch-12 batch-78
Running loss of epoch-12 batch-78 = 0.006342525593936443

Training epoch-12 batch-79
Running loss of epoch-12 batch-79 = 0.0042821974493563175

Training epoch-12 batch-80
Running loss of epoch-12 batch-80 = 0.0020192391239106655

Training epoch-12 batch-81
Running loss of epoch-12 batch-81 = 0.0069015068002045155

Training epoch-12 batch-82
Running loss of epoch-12 batch-82 = 0.005687308497726917

Training epoch-12 batch-83
Running loss of epoch-12 batch-83 = 0.007754824589937925

Training epoch-12 batch-84
Running loss of epoch-12 batch-84 = 0.005077945068478584

Training epoch-12 batch-85
Running loss of epoch-12 batch-85 = 0.007556919939815998

Training epoch-12 batch-86
Running loss of epoch-12 batch-86 = 0.004231367260217667

Training epoch-12 batch-87
Running loss of epoch-12 batch-87 = 0.003952731378376484

Training epoch-12 batch-88
Running loss of epoch-12 batch-88 = 0.0074663483537733555

Training epoch-12 batch-89
Running loss of epoch-12 batch-89 = 0.005283717066049576

Training epoch-12 batch-90
Running loss of epoch-12 batch-90 = 0.0062969159334897995

Training epoch-12 batch-91
Running loss of epoch-12 batch-91 = 0.006315893493592739

Training epoch-12 batch-92
Running loss of epoch-12 batch-92 = 0.004740475676953793

Training epoch-12 batch-93
Running loss of epoch-12 batch-93 = 0.006367253139615059

Training epoch-12 batch-94
Running loss of epoch-12 batch-94 = 0.005744426045566797

Training epoch-12 batch-95
Running loss of epoch-12 batch-95 = 0.004338260740041733

Training epoch-12 batch-96
Running loss of epoch-12 batch-96 = 0.003931126557290554

Training epoch-12 batch-97
Running loss of epoch-12 batch-97 = 0.005605731625109911

Training epoch-12 batch-98
Running loss of epoch-12 batch-98 = 0.003772050142288208

Training epoch-12 batch-99
Running loss of epoch-12 batch-99 = 0.004506762605160475

Training epoch-12 batch-100
Running loss of epoch-12 batch-100 = 0.006200745236128569

Training epoch-12 batch-101
Running loss of epoch-12 batch-101 = 0.0063455114141106606

Training epoch-12 batch-102
Running loss of epoch-12 batch-102 = 0.005277554038912058

Training epoch-12 batch-103
Running loss of epoch-12 batch-103 = 0.004130658693611622

Training epoch-12 batch-104
Running loss of epoch-12 batch-104 = 0.003930788021534681

Training epoch-12 batch-105
Running loss of epoch-12 batch-105 = 0.007060586009174585

Training epoch-12 batch-106
Running loss of epoch-12 batch-106 = 0.007632621098309755

Training epoch-12 batch-107
Running loss of epoch-12 batch-107 = 0.006390444468706846

Training epoch-12 batch-108
Running loss of epoch-12 batch-108 = 0.0031350571662187576

Training epoch-12 batch-109
Running loss of epoch-12 batch-109 = 0.005083148367702961

Training epoch-12 batch-110
Running loss of epoch-12 batch-110 = 0.0069771138951182365

Training epoch-12 batch-111
Running loss of epoch-12 batch-111 = 0.004702202510088682

Training epoch-12 batch-112
Running loss of epoch-12 batch-112 = 0.003183352993801236

Training epoch-12 batch-113
Running loss of epoch-12 batch-113 = 0.005692478269338608

Training epoch-12 batch-114
Running loss of epoch-12 batch-114 = 0.004868983756750822

Training epoch-12 batch-115
Running loss of epoch-12 batch-115 = 0.005895552225410938

Training epoch-12 batch-116
Running loss of epoch-12 batch-116 = 0.0034694205969572067

Training epoch-12 batch-117
Running loss of epoch-12 batch-117 = 0.004143313504755497

Training epoch-12 batch-118
Running loss of epoch-12 batch-118 = 0.008170717395842075

Training epoch-12 batch-119
Running loss of epoch-12 batch-119 = 0.007303561549633741

Training epoch-12 batch-120
Running loss of epoch-12 batch-120 = 0.004518009256571531

Training epoch-12 batch-121
Running loss of epoch-12 batch-121 = 0.004959911108016968

Training epoch-12 batch-122
Running loss of epoch-12 batch-122 = 0.008465569466352463

Training epoch-12 batch-123
Running loss of epoch-12 batch-123 = 0.008057854138314724

Training epoch-12 batch-124
Running loss of epoch-12 batch-124 = 0.005545846186578274

Training epoch-12 batch-125
Running loss of epoch-12 batch-125 = 0.004861687310039997

Training epoch-12 batch-126
Running loss of epoch-12 batch-126 = 0.00578852416947484

Training epoch-12 batch-127
Running loss of epoch-12 batch-127 = 0.007882209494709969

Training epoch-12 batch-128
Running loss of epoch-12 batch-128 = 0.0036724358797073364

Training epoch-12 batch-129
Running loss of epoch-12 batch-129 = 0.004862015135586262

Training epoch-12 batch-130
Running loss of epoch-12 batch-130 = 0.005518737714737654

Training epoch-12 batch-131
Running loss of epoch-12 batch-131 = 0.004781709983944893

Training epoch-12 batch-132
Running loss of epoch-12 batch-132 = 0.006075476296246052

Training epoch-12 batch-133
Running loss of epoch-12 batch-133 = 0.004474370274692774

Training epoch-12 batch-134
Running loss of epoch-12 batch-134 = 0.005008026957511902

Training epoch-12 batch-135
Running loss of epoch-12 batch-135 = 0.005177428014576435

Training epoch-12 batch-136
Running loss of epoch-12 batch-136 = 0.005859981290996075

Training epoch-12 batch-137
Running loss of epoch-12 batch-137 = 0.006128556095063686

Training epoch-12 batch-138
Running loss of epoch-12 batch-138 = 0.0053827958181500435

Training epoch-12 batch-139
Running loss of epoch-12 batch-139 = 0.006250360514968634

Training epoch-12 batch-140
Running loss of epoch-12 batch-140 = 0.0068477364256978035

Training epoch-12 batch-141
Running loss of epoch-12 batch-141 = 0.005107092205435038

Training epoch-12 batch-142
Running loss of epoch-12 batch-142 = 0.0051809935830533504

Training epoch-12 batch-143
Running loss of epoch-12 batch-143 = 0.004832737613469362

Training epoch-12 batch-144
Running loss of epoch-12 batch-144 = 0.005337424576282501

Training epoch-12 batch-145
Running loss of epoch-12 batch-145 = 0.00538934301584959

Training epoch-12 batch-146
Running loss of epoch-12 batch-146 = 0.005657370667904615

Training epoch-12 batch-147
Running loss of epoch-12 batch-147 = 0.003330207895487547

Training epoch-12 batch-148
Running loss of epoch-12 batch-148 = 0.005540169775485992

Training epoch-12 batch-149
Running loss of epoch-12 batch-149 = 0.0050015877932310104

Training epoch-12 batch-150
Running loss of epoch-12 batch-150 = 0.005776839330792427

Training epoch-12 batch-151
Running loss of epoch-12 batch-151 = 0.0046509490348398685

Training epoch-12 batch-152
Running loss of epoch-12 batch-152 = 0.011790577322244644

Training epoch-12 batch-153
Running loss of epoch-12 batch-153 = 0.005398545414209366

Training epoch-12 batch-154
Running loss of epoch-12 batch-154 = 0.006269598379731178

Training epoch-12 batch-155
Running loss of epoch-12 batch-155 = 0.0056024338118731976

Training epoch-12 batch-156
Running loss of epoch-12 batch-156 = 0.007633631583303213

Training epoch-12 batch-157
Running loss of epoch-12 batch-157 = 0.0041379425674676895

Finished training epoch-12.



Average train loss at epoch-12 = 0.005775340937077999

Started Evaluation

Average val loss at epoch-12 = 0.5758662731141636

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 93.93 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 89.55 %
Accuracy for class toString is: 86.01 %
Accuracy for class run is: 66.21 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 59.64 %
Accuracy for class execute is: 63.05 %
Accuracy for class get is: 54.62 %

Overall Accuracy = 82.15 %


Best Accuracy = 82.15 % at Epoch-12


Saving model after best epoch-12
Finished Evaluation



Started training epoch-13


Training epoch-13 batch-1
Running loss of epoch-13 batch-1 = 0.00389546575024724

Training epoch-13 batch-2
Running loss of epoch-13 batch-2 = 0.0031279386021196842

Training epoch-13 batch-3
Running loss of epoch-13 batch-3 = 0.003950242884457111

Training epoch-13 batch-4
Running loss of epoch-13 batch-4 = 0.005384436808526516

Training epoch-13 batch-5
Running loss of epoch-13 batch-5 = 0.003741692751646042

Training epoch-13 batch-6
Running loss of epoch-13 batch-6 = 0.003574163420125842

Training epoch-13 batch-7
Running loss of epoch-13 batch-7 = 0.004617927595973015

Training epoch-13 batch-8
Running loss of epoch-13 batch-8 = 0.006452952977269888

Training epoch-13 batch-9
Running loss of epoch-13 batch-9 = 0.004058644641190767

Training epoch-13 batch-10
Running loss of epoch-13 batch-10 = 0.00329453032463789

Training epoch-13 batch-11
Running loss of epoch-13 batch-11 = 0.004172837361693382

Training epoch-13 batch-12
Running loss of epoch-13 batch-12 = 0.0073468382470309734

Training epoch-13 batch-13
Running loss of epoch-13 batch-13 = 0.005999111570417881

Training epoch-13 batch-14
Running loss of epoch-13 batch-14 = 0.004918303340673447

Training epoch-13 batch-15
Running loss of epoch-13 batch-15 = 0.0055962419137358665

Training epoch-13 batch-16
Running loss of epoch-13 batch-16 = 0.005766388028860092

Training epoch-13 batch-17
Running loss of epoch-13 batch-17 = 0.0046365223824977875

Training epoch-13 batch-18
Running loss of epoch-13 batch-18 = 0.006367006339132786

Training epoch-13 batch-19
Running loss of epoch-13 batch-19 = 0.006317196413874626

Training epoch-13 batch-20
Running loss of epoch-13 batch-20 = 0.006822939030826092

Training epoch-13 batch-21
Running loss of epoch-13 batch-21 = 0.0032468580175191164

Training epoch-13 batch-22
Running loss of epoch-13 batch-22 = 0.0025147178675979376

Training epoch-13 batch-23
Running loss of epoch-13 batch-23 = 0.003404080867767334

Training epoch-13 batch-24
Running loss of epoch-13 batch-24 = 0.004709597211331129

Training epoch-13 batch-25
Running loss of epoch-13 batch-25 = 0.004493657499551773

Training epoch-13 batch-26
Running loss of epoch-13 batch-26 = 0.0050316364504396915

Training epoch-13 batch-27
Running loss of epoch-13 batch-27 = 0.0034619607031345367

Training epoch-13 batch-28
Running loss of epoch-13 batch-28 = 0.004466789308935404

Training epoch-13 batch-29
Running loss of epoch-13 batch-29 = 0.004109671339392662

Training epoch-13 batch-30
Running loss of epoch-13 batch-30 = 0.004154129885137081

Training epoch-13 batch-31
Running loss of epoch-13 batch-31 = 0.005505175329744816

Training epoch-13 batch-32
Running loss of epoch-13 batch-32 = 0.005170120857656002

Training epoch-13 batch-33
Running loss of epoch-13 batch-33 = 0.0058461627922952175

Training epoch-13 batch-34
Running loss of epoch-13 batch-34 = 0.005154050886631012

Training epoch-13 batch-35
Running loss of epoch-13 batch-35 = 0.002839260268956423

Training epoch-13 batch-36
Running loss of epoch-13 batch-36 = 0.005428398959338665

Training epoch-13 batch-37
Running loss of epoch-13 batch-37 = 0.0030346072744578123

Training epoch-13 batch-38
Running loss of epoch-13 batch-38 = 0.006277836859226227

Training epoch-13 batch-39
Running loss of epoch-13 batch-39 = 0.00353141943924129

Training epoch-13 batch-40
Running loss of epoch-13 batch-40 = 0.0045875562354922295

Training epoch-13 batch-41
Running loss of epoch-13 batch-41 = 0.004700046498328447

Training epoch-13 batch-42
Running loss of epoch-13 batch-42 = 0.0032620381098240614

Training epoch-13 batch-43
Running loss of epoch-13 batch-43 = 0.003889402374625206

Training epoch-13 batch-44
Running loss of epoch-13 batch-44 = 0.0025485651567578316

Training epoch-13 batch-45
Running loss of epoch-13 batch-45 = 0.0038679426070302725

Training epoch-13 batch-46
Running loss of epoch-13 batch-46 = 0.0034794555976986885

Training epoch-13 batch-47
Running loss of epoch-13 batch-47 = 0.007340971380472183

Training epoch-13 batch-48
Running loss of epoch-13 batch-48 = 0.006468222942203283

Training epoch-13 batch-49
Running loss of epoch-13 batch-49 = 0.004721814766526222

Training epoch-13 batch-50
Running loss of epoch-13 batch-50 = 0.003502387087792158

Training epoch-13 batch-51
Running loss of epoch-13 batch-51 = 0.004270263947546482

Training epoch-13 batch-52
Running loss of epoch-13 batch-52 = 0.005126338452100754

Training epoch-13 batch-53
Running loss of epoch-13 batch-53 = 0.0035273521207273006

Training epoch-13 batch-54
Running loss of epoch-13 batch-54 = 0.005386960227042437

Training epoch-13 batch-55
Running loss of epoch-13 batch-55 = 0.004048326052725315

Training epoch-13 batch-56
Running loss of epoch-13 batch-56 = 0.004015373066067696

Training epoch-13 batch-57
Running loss of epoch-13 batch-57 = 0.007221758831292391

Training epoch-13 batch-58
Running loss of epoch-13 batch-58 = 0.0034173119347542524

Training epoch-13 batch-59
Running loss of epoch-13 batch-59 = 0.0042702918872237206

Training epoch-13 batch-60
Running loss of epoch-13 batch-60 = 0.006820984184741974

Training epoch-13 batch-61
Running loss of epoch-13 batch-61 = 0.00565345399081707

Training epoch-13 batch-62
Running loss of epoch-13 batch-62 = 0.003975222818553448

Training epoch-13 batch-63
Running loss of epoch-13 batch-63 = 0.007548924535512924

Training epoch-13 batch-64
Running loss of epoch-13 batch-64 = 0.00518739502876997

Training epoch-13 batch-65
Running loss of epoch-13 batch-65 = 0.006946055684238672

Training epoch-13 batch-66
Running loss of epoch-13 batch-66 = 0.0056337276473641396

Training epoch-13 batch-67
Running loss of epoch-13 batch-67 = 0.0048662275075912476

Training epoch-13 batch-68
Running loss of epoch-13 batch-68 = 0.0023369069676846266

Training epoch-13 batch-69
Running loss of epoch-13 batch-69 = 0.005762032233178616

Training epoch-13 batch-70
Running loss of epoch-13 batch-70 = 0.0052048275247216225

Training epoch-13 batch-71
Running loss of epoch-13 batch-71 = 0.005656477063894272

Training epoch-13 batch-72
Running loss of epoch-13 batch-72 = 0.009396515786647797

Training epoch-13 batch-73
Running loss of epoch-13 batch-73 = 0.0037945350632071495

Training epoch-13 batch-74
Running loss of epoch-13 batch-74 = 0.004904001019895077

Training epoch-13 batch-75
Running loss of epoch-13 batch-75 = 0.005735145881772041

Training epoch-13 batch-76
Running loss of epoch-13 batch-76 = 0.005647582001984119

Training epoch-13 batch-77
Running loss of epoch-13 batch-77 = 0.007721440866589546

Training epoch-13 batch-78
Running loss of epoch-13 batch-78 = 0.003468022681772709

Training epoch-13 batch-79
Running loss of epoch-13 batch-79 = 0.0045860144309699535

Training epoch-13 batch-80
Running loss of epoch-13 batch-80 = 0.003598187118768692

Training epoch-13 batch-81
Running loss of epoch-13 batch-81 = 0.004766071680933237

Training epoch-13 batch-82
Running loss of epoch-13 batch-82 = 0.0018722579116001725

Training epoch-13 batch-83
Running loss of epoch-13 batch-83 = 0.0021302218083292246

Training epoch-13 batch-84
Running loss of epoch-13 batch-84 = 0.00517208082601428

Training epoch-13 batch-85
Running loss of epoch-13 batch-85 = 0.005457970779389143

Training epoch-13 batch-86
Running loss of epoch-13 batch-86 = 0.0049637057818472385

Training epoch-13 batch-87
Running loss of epoch-13 batch-87 = 0.0038463419768959284

Training epoch-13 batch-88
Running loss of epoch-13 batch-88 = 0.005693398416042328

Training epoch-13 batch-89
Running loss of epoch-13 batch-89 = 0.004684844985604286

Training epoch-13 batch-90
Running loss of epoch-13 batch-90 = 0.004807591903954744

Training epoch-13 batch-91
Running loss of epoch-13 batch-91 = 0.0058357855305075645

Training epoch-13 batch-92
Running loss of epoch-13 batch-92 = 0.0054169269278645515

Training epoch-13 batch-93
Running loss of epoch-13 batch-93 = 0.0046059805899858475

Training epoch-13 batch-94
Running loss of epoch-13 batch-94 = 0.0041893115267157555

Training epoch-13 batch-95
Running loss of epoch-13 batch-95 = 0.006682408507913351

Training epoch-13 batch-96
Running loss of epoch-13 batch-96 = 0.004399174824357033

Training epoch-13 batch-97
Running loss of epoch-13 batch-97 = 0.0023594193626195192

Training epoch-13 batch-98
Running loss of epoch-13 batch-98 = 0.00384081294760108

Training epoch-13 batch-99
Running loss of epoch-13 batch-99 = 0.003098358865827322

Training epoch-13 batch-100
Running loss of epoch-13 batch-100 = 0.0033988305367529392

Training epoch-13 batch-101
Running loss of epoch-13 batch-101 = 0.002542275469750166

Training epoch-13 batch-102
Running loss of epoch-13 batch-102 = 0.0029672537930309772

Training epoch-13 batch-103
Running loss of epoch-13 batch-103 = 0.0056024277582764626

Training epoch-13 batch-104
Running loss of epoch-13 batch-104 = 0.00408965116366744

Training epoch-13 batch-105
Running loss of epoch-13 batch-105 = 0.003594186156988144

Training epoch-13 batch-106
Running loss of epoch-13 batch-106 = 0.005712530575692654

Training epoch-13 batch-107
Running loss of epoch-13 batch-107 = 0.001940442598424852

Training epoch-13 batch-108
Running loss of epoch-13 batch-108 = 0.0064228614792227745

Training epoch-13 batch-109
Running loss of epoch-13 batch-109 = 0.004486078396439552

Training epoch-13 batch-110
Running loss of epoch-13 batch-110 = 0.003996020182967186

Training epoch-13 batch-111
Running loss of epoch-13 batch-111 = 0.0038933930918574333

Training epoch-13 batch-112
Running loss of epoch-13 batch-112 = 0.004427468404173851

Training epoch-13 batch-113
Running loss of epoch-13 batch-113 = 0.002861917717382312

Training epoch-13 batch-114
Running loss of epoch-13 batch-114 = 0.003798747668042779

Training epoch-13 batch-115
Running loss of epoch-13 batch-115 = 0.004058585036545992

Training epoch-13 batch-116
Running loss of epoch-13 batch-116 = 0.004098362289369106

Training epoch-13 batch-117
Running loss of epoch-13 batch-117 = 0.0028461881447583437

Training epoch-13 batch-118
Running loss of epoch-13 batch-118 = 0.003989213611930609

Training epoch-13 batch-119
Running loss of epoch-13 batch-119 = 0.003325589932501316

Training epoch-13 batch-120
Running loss of epoch-13 batch-120 = 0.004492542706429958

Training epoch-13 batch-121
Running loss of epoch-13 batch-121 = 0.003564317710697651

Training epoch-13 batch-122
Running loss of epoch-13 batch-122 = 0.0046668886207044125

Training epoch-13 batch-123
Running loss of epoch-13 batch-123 = 0.00386608112603426

Training epoch-13 batch-124
Running loss of epoch-13 batch-124 = 0.003747155424207449

Training epoch-13 batch-125
Running loss of epoch-13 batch-125 = 0.00593920424580574

Training epoch-13 batch-126
Running loss of epoch-13 batch-126 = 0.008996456861495972

Training epoch-13 batch-127
Running loss of epoch-13 batch-127 = 0.007234504446387291

Training epoch-13 batch-128
Running loss of epoch-13 batch-128 = 0.004614860285073519

Training epoch-13 batch-129
Running loss of epoch-13 batch-129 = 0.0032240438740700483

Training epoch-13 batch-130
Running loss of epoch-13 batch-130 = 0.0028690393082797527

Training epoch-13 batch-131
Running loss of epoch-13 batch-131 = 0.0045644380152225494

Training epoch-13 batch-132
Running loss of epoch-13 batch-132 = 0.003082782030105591

Training epoch-13 batch-133
Running loss of epoch-13 batch-133 = 0.003611495718359947

Training epoch-13 batch-134
Running loss of epoch-13 batch-134 = 0.0056783040054142475

Training epoch-13 batch-135
Running loss of epoch-13 batch-135 = 0.0027369840536266565

Training epoch-13 batch-136
Running loss of epoch-13 batch-136 = 0.004903088323771954

Training epoch-13 batch-137
Running loss of epoch-13 batch-137 = 0.007259040139615536

Training epoch-13 batch-138
Running loss of epoch-13 batch-138 = 0.0061516184359788895

Training epoch-13 batch-139
Running loss of epoch-13 batch-139 = 0.005758640822023153

Training epoch-13 batch-140
Running loss of epoch-13 batch-140 = 0.006378915160894394

Training epoch-13 batch-141
Running loss of epoch-13 batch-141 = 0.003393741324543953

Training epoch-13 batch-142
Running loss of epoch-13 batch-142 = 0.004887437913566828

Training epoch-13 batch-143
Running loss of epoch-13 batch-143 = 0.004332350566983223

Training epoch-13 batch-144
Running loss of epoch-13 batch-144 = 0.004053186159580946

Training epoch-13 batch-145
Running loss of epoch-13 batch-145 = 0.002585263689979911

Training epoch-13 batch-146
Running loss of epoch-13 batch-146 = 0.004914421588182449

Training epoch-13 batch-147
Running loss of epoch-13 batch-147 = 0.0032473562750965357

Training epoch-13 batch-148
Running loss of epoch-13 batch-148 = 0.002875463105738163

Training epoch-13 batch-149
Running loss of epoch-13 batch-149 = 0.0036487234756350517

Training epoch-13 batch-150
Running loss of epoch-13 batch-150 = 0.006125434767454863

Training epoch-13 batch-151
Running loss of epoch-13 batch-151 = 0.004846575669944286

Training epoch-13 batch-152
Running loss of epoch-13 batch-152 = 0.004777008201926947

Training epoch-13 batch-153
Running loss of epoch-13 batch-153 = 0.00629786029458046

Training epoch-13 batch-154
Running loss of epoch-13 batch-154 = 0.0031903195194900036

Training epoch-13 batch-155
Running loss of epoch-13 batch-155 = 0.003638428868725896

Training epoch-13 batch-156
Running loss of epoch-13 batch-156 = 0.007426978554576635

Training epoch-13 batch-157
Running loss of epoch-13 batch-157 = 0.019936297088861465

Finished training epoch-13.



Average train loss at epoch-13 = 0.004652526631951332

Started Evaluation

Average val loss at epoch-13 = 0.5688700700227759

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.38 %
Accuracy for class setUp is: 94.59 %
Accuracy for class onCreate is: 90.41 %
Accuracy for class toString is: 91.13 %
Accuracy for class run is: 75.11 %
Accuracy for class hashCode is: 99.25 %
Accuracy for class init is: 46.41 %
Accuracy for class execute is: 30.52 %
Accuracy for class get is: 67.95 %

Overall Accuracy = 82.73 %


Best Accuracy = 82.73 % at Epoch-13


Saving model after best epoch-13
Finished Evaluation



Started training epoch-14


Training epoch-14 batch-1
Running loss of epoch-14 batch-1 = 0.0045109461061656475

Training epoch-14 batch-2
Running loss of epoch-14 batch-2 = 0.0036793462932109833

Training epoch-14 batch-3
Running loss of epoch-14 batch-3 = 0.0035879304632544518

Training epoch-14 batch-4
Running loss of epoch-14 batch-4 = 0.002636466408148408

Training epoch-14 batch-5
Running loss of epoch-14 batch-5 = 0.003974377177655697

Training epoch-14 batch-6
Running loss of epoch-14 batch-6 = 0.0021665161475539207

Training epoch-14 batch-7
Running loss of epoch-14 batch-7 = 0.0037361467257142067

Training epoch-14 batch-8
Running loss of epoch-14 batch-8 = 0.0022265720181167126

Training epoch-14 batch-9
Running loss of epoch-14 batch-9 = 0.0024031694047152996

Training epoch-14 batch-10
Running loss of epoch-14 batch-10 = 0.004054762423038483

Training epoch-14 batch-11
Running loss of epoch-14 batch-11 = 0.0019329879432916641

Training epoch-14 batch-12
Running loss of epoch-14 batch-12 = 0.002996897790580988

Training epoch-14 batch-13
Running loss of epoch-14 batch-13 = 0.0038361279293894768

Training epoch-14 batch-14
Running loss of epoch-14 batch-14 = 0.0035673161037266254

Training epoch-14 batch-15
Running loss of epoch-14 batch-15 = 0.003102019429206848

Training epoch-14 batch-16
Running loss of epoch-14 batch-16 = 0.002180279465392232

Training epoch-14 batch-17
Running loss of epoch-14 batch-17 = 0.004323090426623821

Training epoch-14 batch-18
Running loss of epoch-14 batch-18 = 0.0036922236904501915

Training epoch-14 batch-19
Running loss of epoch-14 batch-19 = 0.0039012073539197445

Training epoch-14 batch-20
Running loss of epoch-14 batch-20 = 0.0034143265802413225

Training epoch-14 batch-21
Running loss of epoch-14 batch-21 = 0.003691845340654254

Training epoch-14 batch-22
Running loss of epoch-14 batch-22 = 0.002708572195842862

Training epoch-14 batch-23
Running loss of epoch-14 batch-23 = 0.0053885518573224545

Training epoch-14 batch-24
Running loss of epoch-14 batch-24 = 0.004487685393542051

Training epoch-14 batch-25
Running loss of epoch-14 batch-25 = 0.0038957628421485424

Training epoch-14 batch-26
Running loss of epoch-14 batch-26 = 0.004577466752380133

Training epoch-14 batch-27
Running loss of epoch-14 batch-27 = 0.003025717567652464

Training epoch-14 batch-28
Running loss of epoch-14 batch-28 = 0.0053586577996611595

Training epoch-14 batch-29
Running loss of epoch-14 batch-29 = 0.002367099281400442

Training epoch-14 batch-30
Running loss of epoch-14 batch-30 = 0.003499117912724614

Training epoch-14 batch-31
Running loss of epoch-14 batch-31 = 0.004175629001110792

Training epoch-14 batch-32
Running loss of epoch-14 batch-32 = 0.0037161665968596935

Training epoch-14 batch-33
Running loss of epoch-14 batch-33 = 0.0036869447212666273

Training epoch-14 batch-34
Running loss of epoch-14 batch-34 = 0.004692526068538427

Training epoch-14 batch-35
Running loss of epoch-14 batch-35 = 0.0030704380478709936

Training epoch-14 batch-36
Running loss of epoch-14 batch-36 = 0.0023499035742133856

Training epoch-14 batch-37
Running loss of epoch-14 batch-37 = 0.001843613339588046

Training epoch-14 batch-38
Running loss of epoch-14 batch-38 = 0.004456250928342342

Training epoch-14 batch-39
Running loss of epoch-14 batch-39 = 0.006379270926117897

Training epoch-14 batch-40
Running loss of epoch-14 batch-40 = 0.004443780053406954

Training epoch-14 batch-41
Running loss of epoch-14 batch-41 = 0.0018614660948514938

Training epoch-14 batch-42
Running loss of epoch-14 batch-42 = 0.003148260060697794

Training epoch-14 batch-43
Running loss of epoch-14 batch-43 = 0.003744144458323717

Training epoch-14 batch-44
Running loss of epoch-14 batch-44 = 0.0013589538866654038

Training epoch-14 batch-45
Running loss of epoch-14 batch-45 = 0.0020898825023323298

Training epoch-14 batch-46
Running loss of epoch-14 batch-46 = 0.0029314428102225065

Training epoch-14 batch-47
Running loss of epoch-14 batch-47 = 0.00464856019243598

Training epoch-14 batch-48
Running loss of epoch-14 batch-48 = 0.0019981665536761284

Training epoch-14 batch-49
Running loss of epoch-14 batch-49 = 0.004748983308672905

Training epoch-14 batch-50
Running loss of epoch-14 batch-50 = 0.0030367919243872166

Training epoch-14 batch-51
Running loss of epoch-14 batch-51 = 0.003771827556192875

Training epoch-14 batch-52
Running loss of epoch-14 batch-52 = 0.0042535425163805485

Training epoch-14 batch-53
Running loss of epoch-14 batch-53 = 0.0019106542458757758

Training epoch-14 batch-54
Running loss of epoch-14 batch-54 = 0.002328227972611785

Training epoch-14 batch-55
Running loss of epoch-14 batch-55 = 0.002323208376765251

Training epoch-14 batch-56
Running loss of epoch-14 batch-56 = 0.0056731668300926685

Training epoch-14 batch-57
Running loss of epoch-14 batch-57 = 0.004455929156392813

Training epoch-14 batch-58
Running loss of epoch-14 batch-58 = 0.001986945513635874

Training epoch-14 batch-59
Running loss of epoch-14 batch-59 = 0.002818026579916477

Training epoch-14 batch-60
Running loss of epoch-14 batch-60 = 0.004222123883664608

Training epoch-14 batch-61
Running loss of epoch-14 batch-61 = 0.0025227838195860386

Training epoch-14 batch-62
Running loss of epoch-14 batch-62 = 0.003856478026136756

Training epoch-14 batch-63
Running loss of epoch-14 batch-63 = 0.003986431285738945

Training epoch-14 batch-64
Running loss of epoch-14 batch-64 = 0.004889939446002245

Training epoch-14 batch-65
Running loss of epoch-14 batch-65 = 0.0032657349947839975

Training epoch-14 batch-66
Running loss of epoch-14 batch-66 = 0.003742830827832222

Training epoch-14 batch-67
Running loss of epoch-14 batch-67 = 0.0027644792571663857

Training epoch-14 batch-68
Running loss of epoch-14 batch-68 = 0.004493992310017347

Training epoch-14 batch-69
Running loss of epoch-14 batch-69 = 0.002701959339901805

Training epoch-14 batch-70
Running loss of epoch-14 batch-70 = 0.0029227216728031635

Training epoch-14 batch-71
Running loss of epoch-14 batch-71 = 0.004004722461104393

Training epoch-14 batch-72
Running loss of epoch-14 batch-72 = 0.003934960812330246

Training epoch-14 batch-73
Running loss of epoch-14 batch-73 = 0.004773424006998539

Training epoch-14 batch-74
Running loss of epoch-14 batch-74 = 0.0031259250827133656

Training epoch-14 batch-75
Running loss of epoch-14 batch-75 = 0.004125974141061306

Training epoch-14 batch-76
Running loss of epoch-14 batch-76 = 0.0029948088340461254

Training epoch-14 batch-77
Running loss of epoch-14 batch-77 = 0.0033317035995423794

Training epoch-14 batch-78
Running loss of epoch-14 batch-78 = 0.001864127116277814

Training epoch-14 batch-79
Running loss of epoch-14 batch-79 = 0.004174503497779369

Training epoch-14 batch-80
Running loss of epoch-14 batch-80 = 0.0038032059092074633

Training epoch-14 batch-81
Running loss of epoch-14 batch-81 = 0.0023662708699703217

Training epoch-14 batch-82
Running loss of epoch-14 batch-82 = 0.0025216962676495314

Training epoch-14 batch-83
Running loss of epoch-14 batch-83 = 0.004052942618727684

Training epoch-14 batch-84
Running loss of epoch-14 batch-84 = 0.003201054409146309

Training epoch-14 batch-85
Running loss of epoch-14 batch-85 = 0.002273297868669033

Training epoch-14 batch-86
Running loss of epoch-14 batch-86 = 0.002124803140759468

Training epoch-14 batch-87
Running loss of epoch-14 batch-87 = 0.0037392268422991037

Training epoch-14 batch-88
Running loss of epoch-14 batch-88 = 0.0017783230869099498

Training epoch-14 batch-89
Running loss of epoch-14 batch-89 = 0.0023581015411764383

Training epoch-14 batch-90
Running loss of epoch-14 batch-90 = 0.002231481485068798

Training epoch-14 batch-91
Running loss of epoch-14 batch-91 = 0.004431420937180519

Training epoch-14 batch-92
Running loss of epoch-14 batch-92 = 0.008113752119243145

Training epoch-14 batch-93
Running loss of epoch-14 batch-93 = 0.0023516067303717136

Training epoch-14 batch-94
Running loss of epoch-14 batch-94 = 0.003317247610539198

Training epoch-14 batch-95
Running loss of epoch-14 batch-95 = 0.0043884264305233955

Training epoch-14 batch-96
Running loss of epoch-14 batch-96 = 0.0043843998573720455

Training epoch-14 batch-97
Running loss of epoch-14 batch-97 = 0.0019413401605561376

Training epoch-14 batch-98
Running loss of epoch-14 batch-98 = 0.0014579714043065906

Training epoch-14 batch-99
Running loss of epoch-14 batch-99 = 0.003087619785219431

Training epoch-14 batch-100
Running loss of epoch-14 batch-100 = 0.005062432494014502

Training epoch-14 batch-101
Running loss of epoch-14 batch-101 = 0.0022846136707812548

Training epoch-14 batch-102
Running loss of epoch-14 batch-102 = 0.0030830134637653828

Training epoch-14 batch-103
Running loss of epoch-14 batch-103 = 0.0035653694067150354

Training epoch-14 batch-104
Running loss of epoch-14 batch-104 = 0.002525382675230503

Training epoch-14 batch-105
Running loss of epoch-14 batch-105 = 0.002620148938149214

Training epoch-14 batch-106
Running loss of epoch-14 batch-106 = 0.003270268440246582

Training epoch-14 batch-107
Running loss of epoch-14 batch-107 = 0.0025789854116737843

Training epoch-14 batch-108
Running loss of epoch-14 batch-108 = 0.002684477949514985

Training epoch-14 batch-109
Running loss of epoch-14 batch-109 = 0.004769633524119854

Training epoch-14 batch-110
Running loss of epoch-14 batch-110 = 0.0025566802360117435

Training epoch-14 batch-111
Running loss of epoch-14 batch-111 = 0.004118343815207481

Training epoch-14 batch-112
Running loss of epoch-14 batch-112 = 0.003161536529660225

Training epoch-14 batch-113
Running loss of epoch-14 batch-113 = 0.005454008001834154

Training epoch-14 batch-114
Running loss of epoch-14 batch-114 = 0.002444313606247306

Training epoch-14 batch-115
Running loss of epoch-14 batch-115 = 0.0015203990042209625

Training epoch-14 batch-116
Running loss of epoch-14 batch-116 = 0.0034548589028418064

Training epoch-14 batch-117
Running loss of epoch-14 batch-117 = 0.0031289434991776943

Training epoch-14 batch-118
Running loss of epoch-14 batch-118 = 0.003328738734126091

Training epoch-14 batch-119
Running loss of epoch-14 batch-119 = 0.0032891402952373028

Training epoch-14 batch-120
Running loss of epoch-14 batch-120 = 0.0027788716834038496

Training epoch-14 batch-121
Running loss of epoch-14 batch-121 = 0.00247783400118351

Training epoch-14 batch-122
Running loss of epoch-14 batch-122 = 0.003868397092446685

Training epoch-14 batch-123
Running loss of epoch-14 batch-123 = 0.00311463326215744

Training epoch-14 batch-124
Running loss of epoch-14 batch-124 = 0.003919688053429127

Training epoch-14 batch-125
Running loss of epoch-14 batch-125 = 0.004871071316301823

Training epoch-14 batch-126
Running loss of epoch-14 batch-126 = 0.0025500846095383167

Training epoch-14 batch-127
Running loss of epoch-14 batch-127 = 0.001453022239729762

Training epoch-14 batch-128
Running loss of epoch-14 batch-128 = 0.0034910407848656178

Training epoch-14 batch-129
Running loss of epoch-14 batch-129 = 0.004116415977478027

Training epoch-14 batch-130
Running loss of epoch-14 batch-130 = 0.00512322410941124

Training epoch-14 batch-131
Running loss of epoch-14 batch-131 = 0.004763838369399309

Training epoch-14 batch-132
Running loss of epoch-14 batch-132 = 0.002965045627206564

Training epoch-14 batch-133
Running loss of epoch-14 batch-133 = 0.003273388836532831

Training epoch-14 batch-134
Running loss of epoch-14 batch-134 = 0.005587717983871698

Training epoch-14 batch-135
Running loss of epoch-14 batch-135 = 0.00565706379711628

Training epoch-14 batch-136
Running loss of epoch-14 batch-136 = 0.0028267826419323683

Training epoch-14 batch-137
Running loss of epoch-14 batch-137 = 0.004162966273725033

Training epoch-14 batch-138
Running loss of epoch-14 batch-138 = 0.003896894631907344

Training epoch-14 batch-139
Running loss of epoch-14 batch-139 = 0.003441176377236843

Training epoch-14 batch-140
Running loss of epoch-14 batch-140 = 0.0037907776422798634

Training epoch-14 batch-141
Running loss of epoch-14 batch-141 = 0.003153327852487564

Training epoch-14 batch-142
Running loss of epoch-14 batch-142 = 0.0050352392718195915

Training epoch-14 batch-143
Running loss of epoch-14 batch-143 = 0.004267461597919464

Training epoch-14 batch-144
Running loss of epoch-14 batch-144 = 0.002683852566406131

Training epoch-14 batch-145
Running loss of epoch-14 batch-145 = 0.0026439442299306393

Training epoch-14 batch-146
Running loss of epoch-14 batch-146 = 0.002895047888159752

Training epoch-14 batch-147
Running loss of epoch-14 batch-147 = 0.006830704398453236

Training epoch-14 batch-148
Running loss of epoch-14 batch-148 = 0.002843329682946205

Training epoch-14 batch-149
Running loss of epoch-14 batch-149 = 0.002481071511283517

Training epoch-14 batch-150
Running loss of epoch-14 batch-150 = 0.0032861880026757717

Training epoch-14 batch-151
Running loss of epoch-14 batch-151 = 0.007169805932790041

Training epoch-14 batch-152
Running loss of epoch-14 batch-152 = 0.0035289661027491093

Training epoch-14 batch-153
Running loss of epoch-14 batch-153 = 0.004329276736825705

Training epoch-14 batch-154
Running loss of epoch-14 batch-154 = 0.003134452737867832

Training epoch-14 batch-155
Running loss of epoch-14 batch-155 = 0.005167677532881498

Training epoch-14 batch-156
Running loss of epoch-14 batch-156 = 0.0058557335287332535

Training epoch-14 batch-157
Running loss of epoch-14 batch-157 = 0.03288011625409126

Finished training epoch-14.



Average train loss at epoch-14 = 0.0035520450346171856

Started Evaluation

Average val loss at epoch-14 = 0.5944473984311213

Accuracy for classes:
Accuracy for class equals is: 96.86 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.82 %
Accuracy for class onCreate is: 94.14 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 76.26 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 39.69 %
Accuracy for class execute is: 45.78 %
Accuracy for class get is: 68.72 %

Overall Accuracy = 83.16 %


Best Accuracy = 83.16 % at Epoch-14


Saving model after best epoch-14
Finished Evaluation



Started training epoch-15


Training epoch-15 batch-1
Running loss of epoch-15 batch-1 = 0.0029744571074843407

Training epoch-15 batch-2
Running loss of epoch-15 batch-2 = 0.0033981679007411003

Training epoch-15 batch-3
Running loss of epoch-15 batch-3 = 0.0026893671602010727

Training epoch-15 batch-4
Running loss of epoch-15 batch-4 = 0.004063598811626434

Training epoch-15 batch-5
Running loss of epoch-15 batch-5 = 0.0025820170994848013

Training epoch-15 batch-6
Running loss of epoch-15 batch-6 = 0.002674682065844536

Training epoch-15 batch-7
Running loss of epoch-15 batch-7 = 0.003308321349322796

Training epoch-15 batch-8
Running loss of epoch-15 batch-8 = 0.0013773281825706363

Training epoch-15 batch-9
Running loss of epoch-15 batch-9 = 0.003336517605930567

Training epoch-15 batch-10
Running loss of epoch-15 batch-10 = 0.0031699216924607754

Training epoch-15 batch-11
Running loss of epoch-15 batch-11 = 0.0027646049857139587

Training epoch-15 batch-12
Running loss of epoch-15 batch-12 = 0.0049787601456046104

Training epoch-15 batch-13
Running loss of epoch-15 batch-13 = 0.004045411013066769

Training epoch-15 batch-14
Running loss of epoch-15 batch-14 = 0.002326288726180792

Training epoch-15 batch-15
Running loss of epoch-15 batch-15 = 0.0027232039719820023

Training epoch-15 batch-16
Running loss of epoch-15 batch-16 = 0.002874421887099743

Training epoch-15 batch-17
Running loss of epoch-15 batch-17 = 0.004405073821544647

Training epoch-15 batch-18
Running loss of epoch-15 batch-18 = 0.004955114796757698

Training epoch-15 batch-19
Running loss of epoch-15 batch-19 = 0.002927381545305252

Training epoch-15 batch-20
Running loss of epoch-15 batch-20 = 0.00267725414596498

Training epoch-15 batch-21
Running loss of epoch-15 batch-21 = 0.002991028595715761

Training epoch-15 batch-22
Running loss of epoch-15 batch-22 = 0.0029422922525554895

Training epoch-15 batch-23
Running loss of epoch-15 batch-23 = 0.0026283985935151577

Training epoch-15 batch-24
Running loss of epoch-15 batch-24 = 0.002249086042866111

Training epoch-15 batch-25
Running loss of epoch-15 batch-25 = 0.0037988866679370403

Training epoch-15 batch-26
Running loss of epoch-15 batch-26 = 0.002877481048926711

Training epoch-15 batch-27
Running loss of epoch-15 batch-27 = 0.0017278926679864526

Training epoch-15 batch-28
Running loss of epoch-15 batch-28 = 0.0038740835152566433

Training epoch-15 batch-29
Running loss of epoch-15 batch-29 = 0.003098571440204978

Training epoch-15 batch-30
Running loss of epoch-15 batch-30 = 0.003701594192534685

Training epoch-15 batch-31
Running loss of epoch-15 batch-31 = 0.003963314928114414

Training epoch-15 batch-32
Running loss of epoch-15 batch-32 = 0.003080698661506176

Training epoch-15 batch-33
Running loss of epoch-15 batch-33 = 0.0025200922973454

Training epoch-15 batch-34
Running loss of epoch-15 batch-34 = 0.00223141280002892

Training epoch-15 batch-35
Running loss of epoch-15 batch-35 = 0.0022161374799907207

Training epoch-15 batch-36
Running loss of epoch-15 batch-36 = 0.004322968889027834

Training epoch-15 batch-37
Running loss of epoch-15 batch-37 = 0.0031402050517499447

Training epoch-15 batch-38
Running loss of epoch-15 batch-38 = 0.002121945843100548

Training epoch-15 batch-39
Running loss of epoch-15 batch-39 = 0.002717879367992282

Training epoch-15 batch-40
Running loss of epoch-15 batch-40 = 0.003102223388850689

Training epoch-15 batch-41
Running loss of epoch-15 batch-41 = 0.0020688436925411224

Training epoch-15 batch-42
Running loss of epoch-15 batch-42 = 0.003305109916254878

Training epoch-15 batch-43
Running loss of epoch-15 batch-43 = 0.0026812050491571426

Training epoch-15 batch-44
Running loss of epoch-15 batch-44 = 0.0024132421240210533

Training epoch-15 batch-45
Running loss of epoch-15 batch-45 = 0.0027675211895257235

Training epoch-15 batch-46
Running loss of epoch-15 batch-46 = 0.003173796460032463

Training epoch-15 batch-47
Running loss of epoch-15 batch-47 = 0.0013780128210783005

Training epoch-15 batch-48
Running loss of epoch-15 batch-48 = 0.0038285343907773495

Training epoch-15 batch-49
Running loss of epoch-15 batch-49 = 0.0015233216108754277

Training epoch-15 batch-50
Running loss of epoch-15 batch-50 = 0.0023850356228649616

Training epoch-15 batch-51
Running loss of epoch-15 batch-51 = 0.003106263466179371

Training epoch-15 batch-52
Running loss of epoch-15 batch-52 = 0.0017814332386478782

Training epoch-15 batch-53
Running loss of epoch-15 batch-53 = 0.0028076269663870335

Training epoch-15 batch-54
Running loss of epoch-15 batch-54 = 0.0016431357944384217

Training epoch-15 batch-55
Running loss of epoch-15 batch-55 = 0.002172622364014387

Training epoch-15 batch-56
Running loss of epoch-15 batch-56 = 0.001307510887272656

Training epoch-15 batch-57
Running loss of epoch-15 batch-57 = 0.00167562544811517

Training epoch-15 batch-58
Running loss of epoch-15 batch-58 = 0.0037798285484313965

Training epoch-15 batch-59
Running loss of epoch-15 batch-59 = 0.0025793411768972874

Training epoch-15 batch-60
Running loss of epoch-15 batch-60 = 0.0026971488259732723

Training epoch-15 batch-61
Running loss of epoch-15 batch-61 = 0.0018061299342662096

Training epoch-15 batch-62
Running loss of epoch-15 batch-62 = 0.002558289561420679

Training epoch-15 batch-63
Running loss of epoch-15 batch-63 = 0.002321880776435137

Training epoch-15 batch-64
Running loss of epoch-15 batch-64 = 0.0013200996909290552

Training epoch-15 batch-65
Running loss of epoch-15 batch-65 = 0.002478730631992221

Training epoch-15 batch-66
Running loss of epoch-15 batch-66 = 0.001965483883395791

Training epoch-15 batch-67
Running loss of epoch-15 batch-67 = 0.004728821571916342

Training epoch-15 batch-68
Running loss of epoch-15 batch-68 = 0.001777642173692584

Training epoch-15 batch-69
Running loss of epoch-15 batch-69 = 0.0027814237400889397

Training epoch-15 batch-70
Running loss of epoch-15 batch-70 = 0.0015531647950410843

Training epoch-15 batch-71
Running loss of epoch-15 batch-71 = 0.0009421640424989164

Training epoch-15 batch-72
Running loss of epoch-15 batch-72 = 0.002058727666735649

Training epoch-15 batch-73
Running loss of epoch-15 batch-73 = 0.004390024114400148

Training epoch-15 batch-74
Running loss of epoch-15 batch-74 = 0.0025094286538660526

Training epoch-15 batch-75
Running loss of epoch-15 batch-75 = 0.002096919808536768

Training epoch-15 batch-76
Running loss of epoch-15 batch-76 = 0.002860570326447487

Training epoch-15 batch-77
Running loss of epoch-15 batch-77 = 0.0024649270344525576

Training epoch-15 batch-78
Running loss of epoch-15 batch-78 = 0.0013689817860722542

Training epoch-15 batch-79
Running loss of epoch-15 batch-79 = 0.001977779669687152

Training epoch-15 batch-80
Running loss of epoch-15 batch-80 = 0.00195555598475039

Training epoch-15 batch-81
Running loss of epoch-15 batch-81 = 0.0019871904514729977

Training epoch-15 batch-82
Running loss of epoch-15 batch-82 = 0.002731375163421035

Training epoch-15 batch-83
Running loss of epoch-15 batch-83 = 0.002211099024862051

Training epoch-15 batch-84
Running loss of epoch-15 batch-84 = 0.0030856637749820948

Training epoch-15 batch-85
Running loss of epoch-15 batch-85 = 0.0017649431247264147

Training epoch-15 batch-86
Running loss of epoch-15 batch-86 = 0.00205026101320982

Training epoch-15 batch-87
Running loss of epoch-15 batch-87 = 0.0029045112896710634

Training epoch-15 batch-88
Running loss of epoch-15 batch-88 = 0.001218099961988628

Training epoch-15 batch-89
Running loss of epoch-15 batch-89 = 0.0030601152684539557

Training epoch-15 batch-90
Running loss of epoch-15 batch-90 = 0.001081238966435194

Training epoch-15 batch-91
Running loss of epoch-15 batch-91 = 0.0025784478057175875

Training epoch-15 batch-92
Running loss of epoch-15 batch-92 = 0.004165302962064743

Training epoch-15 batch-93
Running loss of epoch-15 batch-93 = 0.0020576317328959703

Training epoch-15 batch-94
Running loss of epoch-15 batch-94 = 0.0018792561022564769

Training epoch-15 batch-95
Running loss of epoch-15 batch-95 = 0.003286313731223345

Training epoch-15 batch-96
Running loss of epoch-15 batch-96 = 0.0023133934009820223

Training epoch-15 batch-97
Running loss of epoch-15 batch-97 = 0.0028399338480085135

Training epoch-15 batch-98
Running loss of epoch-15 batch-98 = 0.0012259582290425897

Training epoch-15 batch-99
Running loss of epoch-15 batch-99 = 0.0022807640489190817

Training epoch-15 batch-100
Running loss of epoch-15 batch-100 = 0.0031634399201720953

Training epoch-15 batch-101
Running loss of epoch-15 batch-101 = 0.004358311183750629

Training epoch-15 batch-102
Running loss of epoch-15 batch-102 = 0.0021774983033537865

Training epoch-15 batch-103
Running loss of epoch-15 batch-103 = 0.0016109507996588945

Training epoch-15 batch-104
Running loss of epoch-15 batch-104 = 0.00238444353453815

Training epoch-15 batch-105
Running loss of epoch-15 batch-105 = 0.0019562430679798126

Training epoch-15 batch-106
Running loss of epoch-15 batch-106 = 0.003049717750400305

Training epoch-15 batch-107
Running loss of epoch-15 batch-107 = 0.0037949958350509405

Training epoch-15 batch-108
Running loss of epoch-15 batch-108 = 0.003818885423243046

Training epoch-15 batch-109
Running loss of epoch-15 batch-109 = 0.0021048972848802805

Training epoch-15 batch-110
Running loss of epoch-15 batch-110 = 0.002623492619022727

Training epoch-15 batch-111
Running loss of epoch-15 batch-111 = 0.0036395834758877754

Training epoch-15 batch-112
Running loss of epoch-15 batch-112 = 0.004277192056179047

Training epoch-15 batch-113
Running loss of epoch-15 batch-113 = 0.0028508342802524567

Training epoch-15 batch-114
Running loss of epoch-15 batch-114 = 0.002297227969393134

Training epoch-15 batch-115
Running loss of epoch-15 batch-115 = 0.0044188117608428

Training epoch-15 batch-116
Running loss of epoch-15 batch-116 = 0.003709706012159586

Training epoch-15 batch-117
Running loss of epoch-15 batch-117 = 0.0028223711997270584

Training epoch-15 batch-118
Running loss of epoch-15 batch-118 = 0.00207009119912982

Training epoch-15 batch-119
Running loss of epoch-15 batch-119 = 0.001912675448693335

Training epoch-15 batch-120
Running loss of epoch-15 batch-120 = 0.0026628910563886166

Training epoch-15 batch-121
Running loss of epoch-15 batch-121 = 0.002212880179286003

Training epoch-15 batch-122
Running loss of epoch-15 batch-122 = 0.0012660280335694551

Training epoch-15 batch-123
Running loss of epoch-15 batch-123 = 0.002712213434278965

Training epoch-15 batch-124
Running loss of epoch-15 batch-124 = 0.003186628455296159

Training epoch-15 batch-125
Running loss of epoch-15 batch-125 = 0.0025848408695310354

Training epoch-15 batch-126
Running loss of epoch-15 batch-126 = 0.003267361782491207

Training epoch-15 batch-127
Running loss of epoch-15 batch-127 = 0.0015819588443264365

Training epoch-15 batch-128
Running loss of epoch-15 batch-128 = 0.0016581281088292599

Training epoch-15 batch-129
Running loss of epoch-15 batch-129 = 0.0028087764512747526

Training epoch-15 batch-130
Running loss of epoch-15 batch-130 = 0.003788416041061282

Training epoch-15 batch-131
Running loss of epoch-15 batch-131 = 0.0030043618753552437

Training epoch-15 batch-132
Running loss of epoch-15 batch-132 = 0.003086658427491784

Training epoch-15 batch-133
Running loss of epoch-15 batch-133 = 0.0066325366497039795

Training epoch-15 batch-134
Running loss of epoch-15 batch-134 = 0.0013835594290867448

Training epoch-15 batch-135
Running loss of epoch-15 batch-135 = 0.003705053823068738

Training epoch-15 batch-136
Running loss of epoch-15 batch-136 = 0.003715276252478361

Training epoch-15 batch-137
Running loss of epoch-15 batch-137 = 0.0022200378589332104

Training epoch-15 batch-138
Running loss of epoch-15 batch-138 = 0.004236617591232061

Training epoch-15 batch-139
Running loss of epoch-15 batch-139 = 0.001535941381007433

Training epoch-15 batch-140
Running loss of epoch-15 batch-140 = 0.003024986945092678

Training epoch-15 batch-141
Running loss of epoch-15 batch-141 = 0.003556303447112441

Training epoch-15 batch-142
Running loss of epoch-15 batch-142 = 0.0014708557864651084

Training epoch-15 batch-143
Running loss of epoch-15 batch-143 = 0.0035481576342135668

Training epoch-15 batch-144
Running loss of epoch-15 batch-144 = 0.002251158468425274

Training epoch-15 batch-145
Running loss of epoch-15 batch-145 = 0.0024277393240481615

Training epoch-15 batch-146
Running loss of epoch-15 batch-146 = 0.0033872679341584444

Training epoch-15 batch-147
Running loss of epoch-15 batch-147 = 0.003794766031205654

Training epoch-15 batch-148
Running loss of epoch-15 batch-148 = 0.0024092956446111202

Training epoch-15 batch-149
Running loss of epoch-15 batch-149 = 0.00452016107738018

Training epoch-15 batch-150
Running loss of epoch-15 batch-150 = 0.002717310795560479

Training epoch-15 batch-151
Running loss of epoch-15 batch-151 = 0.003191506490111351

Training epoch-15 batch-152
Running loss of epoch-15 batch-152 = 0.0038550635799765587

Training epoch-15 batch-153
Running loss of epoch-15 batch-153 = 0.0022797745186835527

Training epoch-15 batch-154
Running loss of epoch-15 batch-154 = 0.0012757176300510764

Training epoch-15 batch-155
Running loss of epoch-15 batch-155 = 0.003357925685122609

Training epoch-15 batch-156
Running loss of epoch-15 batch-156 = 0.0009751771576702595

Training epoch-15 batch-157
Running loss of epoch-15 batch-157 = 0.012314094230532646

Finished training epoch-15.



Average train loss at epoch-15 = 0.0027563822109252212

Started Evaluation

Average val loss at epoch-15 = 0.5463002721203098

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 98.03 %
Accuracy for class setUp is: 90.66 %
Accuracy for class onCreate is: 92.54 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 67.81 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 57.85 %
Accuracy for class execute is: 40.56 %
Accuracy for class get is: 75.90 %

Overall Accuracy = 83.99 %


Best Accuracy = 83.99 % at Epoch-15


Saving model after best epoch-15
Finished Evaluation



Started training epoch-16


Training epoch-16 batch-1
Running loss of epoch-16 batch-1 = 0.0012556567089632154

Training epoch-16 batch-2
Running loss of epoch-16 batch-2 = 0.0026912156026810408

Training epoch-16 batch-3
Running loss of epoch-16 batch-3 = 0.0016537749907001853

Training epoch-16 batch-4
Running loss of epoch-16 batch-4 = 0.00035715987905859947

Training epoch-16 batch-5
Running loss of epoch-16 batch-5 = 0.0016637255903333426

Training epoch-16 batch-6
Running loss of epoch-16 batch-6 = 0.0033589834347367287

Training epoch-16 batch-7
Running loss of epoch-16 batch-7 = 0.0014652858953922987

Training epoch-16 batch-8
Running loss of epoch-16 batch-8 = 0.0017852929886430502

Training epoch-16 batch-9
Running loss of epoch-16 batch-9 = 0.0006696785567328334

Training epoch-16 batch-10
Running loss of epoch-16 batch-10 = 0.0027927032206207514

Training epoch-16 batch-11
Running loss of epoch-16 batch-11 = 0.001366639044135809

Training epoch-16 batch-12
Running loss of epoch-16 batch-12 = 0.002517526503652334

Training epoch-16 batch-13
Running loss of epoch-16 batch-13 = 0.0018780685495585203

Training epoch-16 batch-14
Running loss of epoch-16 batch-14 = 0.0018205411033704877

Training epoch-16 batch-15
Running loss of epoch-16 batch-15 = 0.0031058909371495247

Training epoch-16 batch-16
Running loss of epoch-16 batch-16 = 0.002673929091542959

Training epoch-16 batch-17
Running loss of epoch-16 batch-17 = 0.003162030130624771

Training epoch-16 batch-18
Running loss of epoch-16 batch-18 = 0.0013189752353355289

Training epoch-16 batch-19
Running loss of epoch-16 batch-19 = 0.0009710175218060613

Training epoch-16 batch-20
Running loss of epoch-16 batch-20 = 0.0028063736390322447

Training epoch-16 batch-21
Running loss of epoch-16 batch-21 = 0.001903081894852221

Training epoch-16 batch-22
Running loss of epoch-16 batch-22 = 0.0021238834597170353

Training epoch-16 batch-23
Running loss of epoch-16 batch-23 = 0.001083852956071496

Training epoch-16 batch-24
Running loss of epoch-16 batch-24 = 0.0032129541505128145

Training epoch-16 batch-25
Running loss of epoch-16 batch-25 = 0.002452933695167303

Training epoch-16 batch-26
Running loss of epoch-16 batch-26 = 0.002118028001859784

Training epoch-16 batch-27
Running loss of epoch-16 batch-27 = 0.0016721717547625303

Training epoch-16 batch-28
Running loss of epoch-16 batch-28 = 0.0017227802891284227

Training epoch-16 batch-29
Running loss of epoch-16 batch-29 = 0.0034771286882460117

Training epoch-16 batch-30
Running loss of epoch-16 batch-30 = 0.001785483444109559

Training epoch-16 batch-31
Running loss of epoch-16 batch-31 = 0.0015553811099380255

Training epoch-16 batch-32
Running loss of epoch-16 batch-32 = 0.0014422686072066426

Training epoch-16 batch-33
Running loss of epoch-16 batch-33 = 0.001733423676341772

Training epoch-16 batch-34
Running loss of epoch-16 batch-34 = 0.0032303070183843374

Training epoch-16 batch-35
Running loss of epoch-16 batch-35 = 0.0010912618599832058

Training epoch-16 batch-36
Running loss of epoch-16 batch-36 = 0.001440951251424849

Training epoch-16 batch-37
Running loss of epoch-16 batch-37 = 0.0008242457406595349

Training epoch-16 batch-38
Running loss of epoch-16 batch-38 = 0.0014122341526672244

Training epoch-16 batch-39
Running loss of epoch-16 batch-39 = 0.0019550658762454987

Training epoch-16 batch-40
Running loss of epoch-16 batch-40 = 0.001096219290047884

Training epoch-16 batch-41
Running loss of epoch-16 batch-41 = 0.0015864474698901176

Training epoch-16 batch-42
Running loss of epoch-16 batch-42 = 0.0029732007533311844

Training epoch-16 batch-43
Running loss of epoch-16 batch-43 = 0.0011405518744140863

Training epoch-16 batch-44
Running loss of epoch-16 batch-44 = 0.001795335439965129

Training epoch-16 batch-45
Running loss of epoch-16 batch-45 = 0.0036161530297249556

Training epoch-16 batch-46
Running loss of epoch-16 batch-46 = 0.002635757904499769

Training epoch-16 batch-47
Running loss of epoch-16 batch-47 = 0.0018040189752355218

Training epoch-16 batch-48
Running loss of epoch-16 batch-48 = 0.0007767998613417149

Training epoch-16 batch-49
Running loss of epoch-16 batch-49 = 0.0010757872369140387

Training epoch-16 batch-50
Running loss of epoch-16 batch-50 = 0.0012207195395603776

Training epoch-16 batch-51
Running loss of epoch-16 batch-51 = 0.0019560351502150297

Training epoch-16 batch-52
Running loss of epoch-16 batch-52 = 0.003200167790055275

Training epoch-16 batch-53
Running loss of epoch-16 batch-53 = 0.001278953393921256

Training epoch-16 batch-54
Running loss of epoch-16 batch-54 = 0.0014818417839705944

Training epoch-16 batch-55
Running loss of epoch-16 batch-55 = 0.0026679281145334244

Training epoch-16 batch-56
Running loss of epoch-16 batch-56 = 0.001716310391202569

Training epoch-16 batch-57
Running loss of epoch-16 batch-57 = 0.0011990279890596867

Training epoch-16 batch-58
Running loss of epoch-16 batch-58 = 0.0010757693089544773

Training epoch-16 batch-59
Running loss of epoch-16 batch-59 = 0.0004364405758678913

Training epoch-16 batch-60
Running loss of epoch-16 batch-60 = 0.0013073364971205592

Training epoch-16 batch-61
Running loss of epoch-16 batch-61 = 0.0021472026128321886

Training epoch-16 batch-62
Running loss of epoch-16 batch-62 = 0.0024517099373042583

Training epoch-16 batch-63
Running loss of epoch-16 batch-63 = 0.0015835448866710067

Training epoch-16 batch-64
Running loss of epoch-16 batch-64 = 0.0011749006807804108

Training epoch-16 batch-65
Running loss of epoch-16 batch-65 = 0.001446455717086792

Training epoch-16 batch-66
Running loss of epoch-16 batch-66 = 0.0024827311281114817

Training epoch-16 batch-67
Running loss of epoch-16 batch-67 = 0.0020509217865765095

Training epoch-16 batch-68
Running loss of epoch-16 batch-68 = 0.0022099539637565613

Training epoch-16 batch-69
Running loss of epoch-16 batch-69 = 0.002748309401795268

Training epoch-16 batch-70
Running loss of epoch-16 batch-70 = 0.0009482905152253807

Training epoch-16 batch-71
Running loss of epoch-16 batch-71 = 0.0020227667409926653

Training epoch-16 batch-72
Running loss of epoch-16 batch-72 = 0.0008982230792753398

Training epoch-16 batch-73
Running loss of epoch-16 batch-73 = 0.0009608186082914472

Training epoch-16 batch-74
Running loss of epoch-16 batch-74 = 0.0010996715864166617

Training epoch-16 batch-75
Running loss of epoch-16 batch-75 = 0.0018358458764851093

Training epoch-16 batch-76
Running loss of epoch-16 batch-76 = 0.0018528428627178073

Training epoch-16 batch-77
Running loss of epoch-16 batch-77 = 0.002430623397231102

Training epoch-16 batch-78
Running loss of epoch-16 batch-78 = 0.002326395595446229

Training epoch-16 batch-79
Running loss of epoch-16 batch-79 = 0.001742829568684101

Training epoch-16 batch-80
Running loss of epoch-16 batch-80 = 0.0012128057423979044

Training epoch-16 batch-81
Running loss of epoch-16 batch-81 = 0.002777497284114361

Training epoch-16 batch-82
Running loss of epoch-16 batch-82 = 0.001921340124681592

Training epoch-16 batch-83
Running loss of epoch-16 batch-83 = 0.0018502984894439578

Training epoch-16 batch-84
Running loss of epoch-16 batch-84 = 0.0019196526845917106

Training epoch-16 batch-85
Running loss of epoch-16 batch-85 = 0.0016923773800954223

Training epoch-16 batch-86
Running loss of epoch-16 batch-86 = 0.0010659635299816728

Training epoch-16 batch-87
Running loss of epoch-16 batch-87 = 0.002334676682949066

Training epoch-16 batch-88
Running loss of epoch-16 batch-88 = 0.0009444104507565498

Training epoch-16 batch-89
Running loss of epoch-16 batch-89 = 0.0019408330554142594

Training epoch-16 batch-90
Running loss of epoch-16 batch-90 = 0.0014592299703508615

Training epoch-16 batch-91
Running loss of epoch-16 batch-91 = 0.002032939810305834

Training epoch-16 batch-92
Running loss of epoch-16 batch-92 = 0.0008204999612644315

Training epoch-16 batch-93
Running loss of epoch-16 batch-93 = 0.001290752086788416

Training epoch-16 batch-94
Running loss of epoch-16 batch-94 = 0.0008386416593566537

Training epoch-16 batch-95
Running loss of epoch-16 batch-95 = 0.005796811543405056

Training epoch-16 batch-96
Running loss of epoch-16 batch-96 = 0.0015206102980300784

Training epoch-16 batch-97
Running loss of epoch-16 batch-97 = 0.0009820746490731835

Training epoch-16 batch-98
Running loss of epoch-16 batch-98 = 0.0027596482541412115

Training epoch-16 batch-99
Running loss of epoch-16 batch-99 = 0.0011546264868229628

Training epoch-16 batch-100
Running loss of epoch-16 batch-100 = 0.0010880900081247091

Training epoch-16 batch-101
Running loss of epoch-16 batch-101 = 0.0016445321962237358

Training epoch-16 batch-102
Running loss of epoch-16 batch-102 = 0.0008153949165716767

Training epoch-16 batch-103
Running loss of epoch-16 batch-103 = 0.0018205710221081972

Training epoch-16 batch-104
Running loss of epoch-16 batch-104 = 0.0017548173200339079

Training epoch-16 batch-105
Running loss of epoch-16 batch-105 = 0.002058917423710227

Training epoch-16 batch-106
Running loss of epoch-16 batch-106 = 0.0026683330070227385

Training epoch-16 batch-107
Running loss of epoch-16 batch-107 = 0.0016401436878368258

Training epoch-16 batch-108
Running loss of epoch-16 batch-108 = 0.003076195949688554

Training epoch-16 batch-109
Running loss of epoch-16 batch-109 = 0.0013541457010433078

Training epoch-16 batch-110
Running loss of epoch-16 batch-110 = 0.0011579000856727362

Training epoch-16 batch-111
Running loss of epoch-16 batch-111 = 0.0015716641210019588

Training epoch-16 batch-112
Running loss of epoch-16 batch-112 = 0.0013258974067866802

Training epoch-16 batch-113
Running loss of epoch-16 batch-113 = 0.0013018251629546285

Training epoch-16 batch-114
Running loss of epoch-16 batch-114 = 0.0019284276058897376

Training epoch-16 batch-115
Running loss of epoch-16 batch-115 = 0.002441530814394355

Training epoch-16 batch-116
Running loss of epoch-16 batch-116 = 0.0020452633034437895

Training epoch-16 batch-117
Running loss of epoch-16 batch-117 = 0.0016882531344890594

Training epoch-16 batch-118
Running loss of epoch-16 batch-118 = 0.0012684769462794065

Training epoch-16 batch-119
Running loss of epoch-16 batch-119 = 0.0011406580451875925

Training epoch-16 batch-120
Running loss of epoch-16 batch-120 = 0.0031111501157283783

Training epoch-16 batch-121
Running loss of epoch-16 batch-121 = 0.0012862844159826636

Training epoch-16 batch-122
Running loss of epoch-16 batch-122 = 0.0011252041440457106

Training epoch-16 batch-123
Running loss of epoch-16 batch-123 = 0.0021742251701653004

Training epoch-16 batch-124
Running loss of epoch-16 batch-124 = 0.002572083380073309

Training epoch-16 batch-125
Running loss of epoch-16 batch-125 = 0.0013993785250931978

Training epoch-16 batch-126
Running loss of epoch-16 batch-126 = 0.0017758049070835114

Training epoch-16 batch-127
Running loss of epoch-16 batch-127 = 0.0010842292103916407

Training epoch-16 batch-128
Running loss of epoch-16 batch-128 = 0.0008477931842207909

Training epoch-16 batch-129
Running loss of epoch-16 batch-129 = 0.002164251869544387

Training epoch-16 batch-130
Running loss of epoch-16 batch-130 = 0.002396067837253213

Training epoch-16 batch-131
Running loss of epoch-16 batch-131 = 0.001440906198695302

Training epoch-16 batch-132
Running loss of epoch-16 batch-132 = 0.003342393785715103

Training epoch-16 batch-133
Running loss of epoch-16 batch-133 = 0.0014354714658111334

Training epoch-16 batch-134
Running loss of epoch-16 batch-134 = 0.0018163800705224276

Training epoch-16 batch-135
Running loss of epoch-16 batch-135 = 0.0018758656224235892

Training epoch-16 batch-136
Running loss of epoch-16 batch-136 = 0.002671755850315094

Training epoch-16 batch-137
Running loss of epoch-16 batch-137 = 0.0032715669367462397

Training epoch-16 batch-138
Running loss of epoch-16 batch-138 = 0.0011867036810144782

Training epoch-16 batch-139
Running loss of epoch-16 batch-139 = 0.002417254028841853

Training epoch-16 batch-140
Running loss of epoch-16 batch-140 = 0.0011556862154975533

Training epoch-16 batch-141
Running loss of epoch-16 batch-141 = 0.0024297721683979034

Training epoch-16 batch-142
Running loss of epoch-16 batch-142 = 0.0016245709266513586

Training epoch-16 batch-143
Running loss of epoch-16 batch-143 = 0.0023527604062110186

Training epoch-16 batch-144
Running loss of epoch-16 batch-144 = 0.0017010149313136935

Training epoch-16 batch-145
Running loss of epoch-16 batch-145 = 0.0016490834532305598

Training epoch-16 batch-146
Running loss of epoch-16 batch-146 = 0.004233286250382662

Training epoch-16 batch-147
Running loss of epoch-16 batch-147 = 0.0038518363144248724

Training epoch-16 batch-148
Running loss of epoch-16 batch-148 = 0.002240118570625782

Training epoch-16 batch-149
Running loss of epoch-16 batch-149 = 0.0022897222079336643

Training epoch-16 batch-150
Running loss of epoch-16 batch-150 = 0.0023261215537786484

Training epoch-16 batch-151
Running loss of epoch-16 batch-151 = 0.003970308229327202

Training epoch-16 batch-152
Running loss of epoch-16 batch-152 = 0.003115197876468301

Training epoch-16 batch-153
Running loss of epoch-16 batch-153 = 0.002235041931271553

Training epoch-16 batch-154
Running loss of epoch-16 batch-154 = 0.0015621978091076016

Training epoch-16 batch-155
Running loss of epoch-16 batch-155 = 0.0017028363654389977

Training epoch-16 batch-156
Running loss of epoch-16 batch-156 = 0.0015715101035311818

Training epoch-16 batch-157
Running loss of epoch-16 batch-157 = 0.0004442185163497925

Finished training epoch-16.



Average train loss at epoch-16 = 0.0018920343354344369

Started Evaluation

Average val loss at epoch-16 = 0.6895235269211729

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 93.44 %
Accuracy for class onCreate is: 90.19 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 40.87 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 63.68 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 70.77 %

Overall Accuracy = 82.09 %

Finished Evaluation



Started training epoch-17


Training epoch-17 batch-1
Running loss of epoch-17 batch-1 = 0.0010452755959704518

Training epoch-17 batch-2
Running loss of epoch-17 batch-2 = 0.0007764765759930015

Training epoch-17 batch-3
Running loss of epoch-17 batch-3 = 0.005563435610383749

Training epoch-17 batch-4
Running loss of epoch-17 batch-4 = 0.0034124674275517464

Training epoch-17 batch-5
Running loss of epoch-17 batch-5 = 0.0013104693498462439

Training epoch-17 batch-6
Running loss of epoch-17 batch-6 = 0.0013097986811771989

Training epoch-17 batch-7
Running loss of epoch-17 batch-7 = 0.0018530659144744277

Training epoch-17 batch-8
Running loss of epoch-17 batch-8 = 0.001980952685698867

Training epoch-17 batch-9
Running loss of epoch-17 batch-9 = 0.003094747429713607

Training epoch-17 batch-10
Running loss of epoch-17 batch-10 = 0.001304046018049121

Training epoch-17 batch-11
Running loss of epoch-17 batch-11 = 0.0018446100875735283

Training epoch-17 batch-12
Running loss of epoch-17 batch-12 = 0.0013481667265295982

Training epoch-17 batch-13
Running loss of epoch-17 batch-13 = 0.0014579128473997116

Training epoch-17 batch-14
Running loss of epoch-17 batch-14 = 0.0035826019011437893

Training epoch-17 batch-15
Running loss of epoch-17 batch-15 = 0.0011295430595055223

Training epoch-17 batch-16
Running loss of epoch-17 batch-16 = 0.002313121221959591

Training epoch-17 batch-17
Running loss of epoch-17 batch-17 = 0.0016824291087687016

Training epoch-17 batch-18
Running loss of epoch-17 batch-18 = 0.0004462650394998491

Training epoch-17 batch-19
Running loss of epoch-17 batch-19 = 0.0026405092794448137

Training epoch-17 batch-20
Running loss of epoch-17 batch-20 = 0.004912338685244322

Training epoch-17 batch-21
Running loss of epoch-17 batch-21 = 0.0022613315377384424

Training epoch-17 batch-22
Running loss of epoch-17 batch-22 = 0.0007475097663700581

Training epoch-17 batch-23
Running loss of epoch-17 batch-23 = 0.001034135464578867

Training epoch-17 batch-24
Running loss of epoch-17 batch-24 = 0.002196860034018755

Training epoch-17 batch-25
Running loss of epoch-17 batch-25 = 0.002480509225279093

Training epoch-17 batch-26
Running loss of epoch-17 batch-26 = 0.0020313605200499296

Training epoch-17 batch-27
Running loss of epoch-17 batch-27 = 0.001488545211032033

Training epoch-17 batch-28
Running loss of epoch-17 batch-28 = 0.001421114313416183

Training epoch-17 batch-29
Running loss of epoch-17 batch-29 = 0.0007879090262576938

Training epoch-17 batch-30
Running loss of epoch-17 batch-30 = 0.0017726868391036987

Training epoch-17 batch-31
Running loss of epoch-17 batch-31 = 0.0018470551585778594

Training epoch-17 batch-32
Running loss of epoch-17 batch-32 = 0.0009502737084403634

Training epoch-17 batch-33
Running loss of epoch-17 batch-33 = 0.0009758670348674059

Training epoch-17 batch-34
Running loss of epoch-17 batch-34 = 0.0016953705344349146

Training epoch-17 batch-35
Running loss of epoch-17 batch-35 = 0.0018168895039707422

Training epoch-17 batch-36
Running loss of epoch-17 batch-36 = 0.0010072062723338604

Training epoch-17 batch-37
Running loss of epoch-17 batch-37 = 0.00038844288792461157

Training epoch-17 batch-38
Running loss of epoch-17 batch-38 = 0.0009973397245630622

Training epoch-17 batch-39
Running loss of epoch-17 batch-39 = 0.0017639194848015904

Training epoch-17 batch-40
Running loss of epoch-17 batch-40 = 0.002580782864242792

Training epoch-17 batch-41
Running loss of epoch-17 batch-41 = 0.0006575940642505884

Training epoch-17 batch-42
Running loss of epoch-17 batch-42 = 0.001880191732198

Training epoch-17 batch-43
Running loss of epoch-17 batch-43 = 0.00182163727004081

Training epoch-17 batch-44
Running loss of epoch-17 batch-44 = 0.0009443106828257442

Training epoch-17 batch-45
Running loss of epoch-17 batch-45 = 0.001246394356712699

Training epoch-17 batch-46
Running loss of epoch-17 batch-46 = 0.001230792491696775

Training epoch-17 batch-47
Running loss of epoch-17 batch-47 = 0.0014844260876998305

Training epoch-17 batch-48
Running loss of epoch-17 batch-48 = 0.0016307929763570428

Training epoch-17 batch-49
Running loss of epoch-17 batch-49 = 0.0009856254328042269

Training epoch-17 batch-50
Running loss of epoch-17 batch-50 = 0.0014484045095741749

Training epoch-17 batch-51
Running loss of epoch-17 batch-51 = 0.0023567057214677334

Training epoch-17 batch-52
Running loss of epoch-17 batch-52 = 0.002737471368163824

Training epoch-17 batch-53
Running loss of epoch-17 batch-53 = 0.0024448009207844734

Training epoch-17 batch-54
Running loss of epoch-17 batch-54 = 0.002714433940127492

Training epoch-17 batch-55
Running loss of epoch-17 batch-55 = 0.0020977435633540154

Training epoch-17 batch-56
Running loss of epoch-17 batch-56 = 0.0021895538084208965

Training epoch-17 batch-57
Running loss of epoch-17 batch-57 = 0.0006023735040798783

Training epoch-17 batch-58
Running loss of epoch-17 batch-58 = 0.001191745395772159

Training epoch-17 batch-59
Running loss of epoch-17 batch-59 = 0.0029225049074739218

Training epoch-17 batch-60
Running loss of epoch-17 batch-60 = 0.0031324131414294243

Training epoch-17 batch-61
Running loss of epoch-17 batch-61 = 0.001463673310354352

Training epoch-17 batch-62
Running loss of epoch-17 batch-62 = 0.0019205675926059484

Training epoch-17 batch-63
Running loss of epoch-17 batch-63 = 0.001394903170876205

Training epoch-17 batch-64
Running loss of epoch-17 batch-64 = 0.0013097366318106651

Training epoch-17 batch-65
Running loss of epoch-17 batch-65 = 0.0009467111085541546

Training epoch-17 batch-66
Running loss of epoch-17 batch-66 = 0.0018035260727629066

Training epoch-17 batch-67
Running loss of epoch-17 batch-67 = 0.0007338441209867597

Training epoch-17 batch-68
Running loss of epoch-17 batch-68 = 0.0013176588108763099

Training epoch-17 batch-69
Running loss of epoch-17 batch-69 = 0.0008648574585095048

Training epoch-17 batch-70
Running loss of epoch-17 batch-70 = 0.0007126785349100828

Training epoch-17 batch-71
Running loss of epoch-17 batch-71 = 0.0029729329980909824

Training epoch-17 batch-72
Running loss of epoch-17 batch-72 = 0.0008738065371289849

Training epoch-17 batch-73
Running loss of epoch-17 batch-73 = 0.0011630939552560449

Training epoch-17 batch-74
Running loss of epoch-17 batch-74 = 0.0018416863167658448

Training epoch-17 batch-75
Running loss of epoch-17 batch-75 = 0.001009704894386232

Training epoch-17 batch-76
Running loss of epoch-17 batch-76 = 0.0010890711564570665

Training epoch-17 batch-77
Running loss of epoch-17 batch-77 = 0.0008292244747281075

Training epoch-17 batch-78
Running loss of epoch-17 batch-78 = 0.0011230155359953642

Training epoch-17 batch-79
Running loss of epoch-17 batch-79 = 0.0022129963617771864

Training epoch-17 batch-80
Running loss of epoch-17 batch-80 = 0.0014588028425350785

Training epoch-17 batch-81
Running loss of epoch-17 batch-81 = 0.0009871533839032054

Training epoch-17 batch-82
Running loss of epoch-17 batch-82 = 0.0012377887032926083

Training epoch-17 batch-83
Running loss of epoch-17 batch-83 = 0.0025397655554115772

Training epoch-17 batch-84
Running loss of epoch-17 batch-84 = 0.0014890165766701102

Training epoch-17 batch-85
Running loss of epoch-17 batch-85 = 0.001607483602128923

Training epoch-17 batch-86
Running loss of epoch-17 batch-86 = 0.0014018584042787552

Training epoch-17 batch-87
Running loss of epoch-17 batch-87 = 0.0010456235613673925

Training epoch-17 batch-88
Running loss of epoch-17 batch-88 = 0.0012068728683516383

Training epoch-17 batch-89
Running loss of epoch-17 batch-89 = 0.001073953928425908

Training epoch-17 batch-90
Running loss of epoch-17 batch-90 = 0.0011867535067722201

Training epoch-17 batch-91
Running loss of epoch-17 batch-91 = 0.0014105646405369043

Training epoch-17 batch-92
Running loss of epoch-17 batch-92 = 0.0019579287618398666

Training epoch-17 batch-93
Running loss of epoch-17 batch-93 = 0.0013288080226629972

Training epoch-17 batch-94
Running loss of epoch-17 batch-94 = 0.001646764692850411

Training epoch-17 batch-95
Running loss of epoch-17 batch-95 = 0.0014515817165374756

Training epoch-17 batch-96
Running loss of epoch-17 batch-96 = 0.0026359285693615675

Training epoch-17 batch-97
Running loss of epoch-17 batch-97 = 0.0006253456231206656

Training epoch-17 batch-98
Running loss of epoch-17 batch-98 = 0.002169471001252532

Training epoch-17 batch-99
Running loss of epoch-17 batch-99 = 0.0020493008196353912

Training epoch-17 batch-100
Running loss of epoch-17 batch-100 = 0.0010204873979091644

Training epoch-17 batch-101
Running loss of epoch-17 batch-101 = 0.0012648426927626133

Training epoch-17 batch-102
Running loss of epoch-17 batch-102 = 0.0013886485248804092

Training epoch-17 batch-103
Running loss of epoch-17 batch-103 = 0.00274684838950634

Training epoch-17 batch-104
Running loss of epoch-17 batch-104 = 0.0012090973323211074

Training epoch-17 batch-105
Running loss of epoch-17 batch-105 = 0.0017133868532255292

Training epoch-17 batch-106
Running loss of epoch-17 batch-106 = 0.0029931869357824326

Training epoch-17 batch-107
Running loss of epoch-17 batch-107 = 0.0015154069988057017

Training epoch-17 batch-108
Running loss of epoch-17 batch-108 = 0.0012754444032907486

Training epoch-17 batch-109
Running loss of epoch-17 batch-109 = 0.001609917264431715

Training epoch-17 batch-110
Running loss of epoch-17 batch-110 = 0.0017161479918286204

Training epoch-17 batch-111
Running loss of epoch-17 batch-111 = 0.0009186145616695285

Training epoch-17 batch-112
Running loss of epoch-17 batch-112 = 0.0014407159760594368

Training epoch-17 batch-113
Running loss of epoch-17 batch-113 = 0.0012600390473380685

Training epoch-17 batch-114
Running loss of epoch-17 batch-114 = 0.00207415665499866

Training epoch-17 batch-115
Running loss of epoch-17 batch-115 = 0.0010319259017705917

Training epoch-17 batch-116
Running loss of epoch-17 batch-116 = 0.0014822807861492038

Training epoch-17 batch-117
Running loss of epoch-17 batch-117 = 0.0008203615434467793

Training epoch-17 batch-118
Running loss of epoch-17 batch-118 = 0.0017602620646357536

Training epoch-17 batch-119
Running loss of epoch-17 batch-119 = 0.0010853094281628728

Training epoch-17 batch-120
Running loss of epoch-17 batch-120 = 0.00198703003115952

Training epoch-17 batch-121
Running loss of epoch-17 batch-121 = 0.0009319663513451815

Training epoch-17 batch-122
Running loss of epoch-17 batch-122 = 0.0015033447416499257

Training epoch-17 batch-123
Running loss of epoch-17 batch-123 = 0.0011973477667197585

Training epoch-17 batch-124
Running loss of epoch-17 batch-124 = 0.000678784679621458

Training epoch-17 batch-125
Running loss of epoch-17 batch-125 = 0.0010288066696375608

Training epoch-17 batch-126
Running loss of epoch-17 batch-126 = 0.0007761743618175387

Training epoch-17 batch-127
Running loss of epoch-17 batch-127 = 0.000884384848177433

Training epoch-17 batch-128
Running loss of epoch-17 batch-128 = 0.0007424583891406655

Training epoch-17 batch-129
Running loss of epoch-17 batch-129 = 0.0018601184710860252

Training epoch-17 batch-130
Running loss of epoch-17 batch-130 = 0.0019510602578520775

Training epoch-17 batch-131
Running loss of epoch-17 batch-131 = 0.0017022831598296762

Training epoch-17 batch-132
Running loss of epoch-17 batch-132 = 0.0008209284860640764

Training epoch-17 batch-133
Running loss of epoch-17 batch-133 = 0.0006783588323742151

Training epoch-17 batch-134
Running loss of epoch-17 batch-134 = 0.0014719751197844744

Training epoch-17 batch-135
Running loss of epoch-17 batch-135 = 0.0011945340083912015

Training epoch-17 batch-136
Running loss of epoch-17 batch-136 = 0.0014055852079764009

Training epoch-17 batch-137
Running loss of epoch-17 batch-137 = 0.0013351856032386422

Training epoch-17 batch-138
Running loss of epoch-17 batch-138 = 0.0013686558231711388

Training epoch-17 batch-139
Running loss of epoch-17 batch-139 = 0.00030508870258927345

Training epoch-17 batch-140
Running loss of epoch-17 batch-140 = 0.0007853318238630891

Training epoch-17 batch-141
Running loss of epoch-17 batch-141 = 0.0015647925902158022

Training epoch-17 batch-142
Running loss of epoch-17 batch-142 = 0.0021270427387207747

Training epoch-17 batch-143
Running loss of epoch-17 batch-143 = 0.0015246650436893106

Training epoch-17 batch-144
Running loss of epoch-17 batch-144 = 0.0022074400912970304

Training epoch-17 batch-145
Running loss of epoch-17 batch-145 = 0.0008480221731588244

Training epoch-17 batch-146
Running loss of epoch-17 batch-146 = 0.0016378998989239335

Training epoch-17 batch-147
Running loss of epoch-17 batch-147 = 0.0010374982375651598

Training epoch-17 batch-148
Running loss of epoch-17 batch-148 = 0.0018509412184357643

Training epoch-17 batch-149
Running loss of epoch-17 batch-149 = 0.0013176561333239079

Training epoch-17 batch-150
Running loss of epoch-17 batch-150 = 0.0012661742512136698

Training epoch-17 batch-151
Running loss of epoch-17 batch-151 = 0.0014845081605017185

Training epoch-17 batch-152
Running loss of epoch-17 batch-152 = 0.0010919577907770872

Training epoch-17 batch-153
Running loss of epoch-17 batch-153 = 0.0011764748487621546

Training epoch-17 batch-154
Running loss of epoch-17 batch-154 = 0.0025285575538873672

Training epoch-17 batch-155
Running loss of epoch-17 batch-155 = 0.0005079320399090648

Training epoch-17 batch-156
Running loss of epoch-17 batch-156 = 0.0023628922645002604

Training epoch-17 batch-157
Running loss of epoch-17 batch-157 = 0.003825247287750244

Finished training epoch-17.



Average train loss at epoch-17 = 0.0015711677759885788

Started Evaluation

Average val loss at epoch-17 = 0.6305895330741279

Accuracy for classes:
Accuracy for class equals is: 97.03 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 93.28 %
Accuracy for class onCreate is: 91.68 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 66.67 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.05 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 70.00 %

Overall Accuracy = 83.93 %

Finished Evaluation



Started training epoch-18


Training epoch-18 batch-1
Running loss of epoch-18 batch-1 = 0.001492517301812768

Training epoch-18 batch-2
Running loss of epoch-18 batch-2 = 0.0018882809672504663

Training epoch-18 batch-3
Running loss of epoch-18 batch-3 = 0.0009064505575224757

Training epoch-18 batch-4
Running loss of epoch-18 batch-4 = 0.0008390931179746985

Training epoch-18 batch-5
Running loss of epoch-18 batch-5 = 0.0005231245886534452

Training epoch-18 batch-6
Running loss of epoch-18 batch-6 = 0.0011631360976025462

Training epoch-18 batch-7
Running loss of epoch-18 batch-7 = 0.0004351863171905279

Training epoch-18 batch-8
Running loss of epoch-18 batch-8 = 0.000731552136130631

Training epoch-18 batch-9
Running loss of epoch-18 batch-9 = 0.0013128422433510423

Training epoch-18 batch-10
Running loss of epoch-18 batch-10 = 0.0017415664624422789

Training epoch-18 batch-11
Running loss of epoch-18 batch-11 = 0.0008767936378717422

Training epoch-18 batch-12
Running loss of epoch-18 batch-12 = 0.0016273914370685816

Training epoch-18 batch-13
Running loss of epoch-18 batch-13 = 0.001071339938789606

Training epoch-18 batch-14
Running loss of epoch-18 batch-14 = 0.0006609698757529259

Training epoch-18 batch-15
Running loss of epoch-18 batch-15 = 0.0009252488380298018

Training epoch-18 batch-16
Running loss of epoch-18 batch-16 = 0.00031763268634676933

Training epoch-18 batch-17
Running loss of epoch-18 batch-17 = 0.00041734613478183746

Training epoch-18 batch-18
Running loss of epoch-18 batch-18 = 0.001157349324785173

Training epoch-18 batch-19
Running loss of epoch-18 batch-19 = 0.0013537739869207144

Training epoch-18 batch-20
Running loss of epoch-18 batch-20 = 0.0012170264963060617

Training epoch-18 batch-21
Running loss of epoch-18 batch-21 = 0.0010913467267528176

Training epoch-18 batch-22
Running loss of epoch-18 batch-22 = 0.0006367778405547142

Training epoch-18 batch-23
Running loss of epoch-18 batch-23 = 0.0017664417391642928

Training epoch-18 batch-24
Running loss of epoch-18 batch-24 = 0.0012245508842170238

Training epoch-18 batch-25
Running loss of epoch-18 batch-25 = 0.001230136607773602

Training epoch-18 batch-26
Running loss of epoch-18 batch-26 = 0.0018240315839648247

Training epoch-18 batch-27
Running loss of epoch-18 batch-27 = 0.0003549759276211262

Training epoch-18 batch-28
Running loss of epoch-18 batch-28 = 0.0011012626346200705

Training epoch-18 batch-29
Running loss of epoch-18 batch-29 = 0.0006151427514851093

Training epoch-18 batch-30
Running loss of epoch-18 batch-30 = 0.0018568538362160325

Training epoch-18 batch-31
Running loss of epoch-18 batch-31 = 0.0006471250671893358

Training epoch-18 batch-32
Running loss of epoch-18 batch-32 = 0.0019069135887548327

Training epoch-18 batch-33
Running loss of epoch-18 batch-33 = 0.001161999418400228

Training epoch-18 batch-34
Running loss of epoch-18 batch-34 = 0.0005753797013312578

Training epoch-18 batch-35
Running loss of epoch-18 batch-35 = 0.0006062701577320695

Training epoch-18 batch-36
Running loss of epoch-18 batch-36 = 0.001565411570481956

Training epoch-18 batch-37
Running loss of epoch-18 batch-37 = 0.001309419283643365

Training epoch-18 batch-38
Running loss of epoch-18 batch-38 = 0.000678627984598279

Training epoch-18 batch-39
Running loss of epoch-18 batch-39 = 0.0014785915846005082

Training epoch-18 batch-40
Running loss of epoch-18 batch-40 = 0.0007822269108146429

Training epoch-18 batch-41
Running loss of epoch-18 batch-41 = 0.0005835153860971332

Training epoch-18 batch-42
Running loss of epoch-18 batch-42 = 0.0003901515156030655

Training epoch-18 batch-43
Running loss of epoch-18 batch-43 = 0.0013057786272838712

Training epoch-18 batch-44
Running loss of epoch-18 batch-44 = 0.0012068705400452018

Training epoch-18 batch-45
Running loss of epoch-18 batch-45 = 0.0009772585472092032

Training epoch-18 batch-46
Running loss of epoch-18 batch-46 = 0.0011023327242583036

Training epoch-18 batch-47
Running loss of epoch-18 batch-47 = 0.0009318752563558519

Training epoch-18 batch-48
Running loss of epoch-18 batch-48 = 0.000813503167591989

Training epoch-18 batch-49
Running loss of epoch-18 batch-49 = 0.0007701772265136242

Training epoch-18 batch-50
Running loss of epoch-18 batch-50 = 0.0010339526925235987

Training epoch-18 batch-51
Running loss of epoch-18 batch-51 = 0.001520913210697472

Training epoch-18 batch-52
Running loss of epoch-18 batch-52 = 0.0008882526308298111

Training epoch-18 batch-53
Running loss of epoch-18 batch-53 = 0.0006829932099208236

Training epoch-18 batch-54
Running loss of epoch-18 batch-54 = 0.000399561133235693

Training epoch-18 batch-55
Running loss of epoch-18 batch-55 = 0.0006253805477172136

Training epoch-18 batch-56
Running loss of epoch-18 batch-56 = 0.0004376394208520651

Training epoch-18 batch-57
Running loss of epoch-18 batch-57 = 0.0011785280657932162

Training epoch-18 batch-58
Running loss of epoch-18 batch-58 = 0.0019461195915937424

Training epoch-18 batch-59
Running loss of epoch-18 batch-59 = 0.000642517814412713

Training epoch-18 batch-60
Running loss of epoch-18 batch-60 = 0.0006962058832868934

Training epoch-18 batch-61
Running loss of epoch-18 batch-61 = 0.002026566304266453

Training epoch-18 batch-62
Running loss of epoch-18 batch-62 = 0.0005797891644760966

Training epoch-18 batch-63
Running loss of epoch-18 batch-63 = 0.0010744811734184623

Training epoch-18 batch-64
Running loss of epoch-18 batch-64 = 0.0008206908823922276

Training epoch-18 batch-65
Running loss of epoch-18 batch-65 = 0.0013312658993527293

Training epoch-18 batch-66
Running loss of epoch-18 batch-66 = 0.0002568034688010812

Training epoch-18 batch-67
Running loss of epoch-18 batch-67 = 0.0009785614674910903

Training epoch-18 batch-68
Running loss of epoch-18 batch-68 = 0.0004913696320727468

Training epoch-18 batch-69
Running loss of epoch-18 batch-69 = 0.000963680911809206

Training epoch-18 batch-70
Running loss of epoch-18 batch-70 = 0.0007271562935784459

Training epoch-18 batch-71
Running loss of epoch-18 batch-71 = 0.0004282820736989379

Training epoch-18 batch-72
Running loss of epoch-18 batch-72 = 0.00043011957313865423

Training epoch-18 batch-73
Running loss of epoch-18 batch-73 = 0.0011091041378676891

Training epoch-18 batch-74
Running loss of epoch-18 batch-74 = 0.0008178387070074677

Training epoch-18 batch-75
Running loss of epoch-18 batch-75 = 0.0008046270813792944

Training epoch-18 batch-76
Running loss of epoch-18 batch-76 = 0.0013944681268185377

Training epoch-18 batch-77
Running loss of epoch-18 batch-77 = 0.0005364578682929277

Training epoch-18 batch-78
Running loss of epoch-18 batch-78 = 0.0005735873710364103

Training epoch-18 batch-79
Running loss of epoch-18 batch-79 = 0.0004319369327276945

Training epoch-18 batch-80
Running loss of epoch-18 batch-80 = 0.0003618313930928707

Training epoch-18 batch-81
Running loss of epoch-18 batch-81 = 0.0002901097759604454

Training epoch-18 batch-82
Running loss of epoch-18 batch-82 = 0.002036136342212558

Training epoch-18 batch-83
Running loss of epoch-18 batch-83 = 0.0010477787582203746

Training epoch-18 batch-84
Running loss of epoch-18 batch-84 = 0.0013843479100614786

Training epoch-18 batch-85
Running loss of epoch-18 batch-85 = 0.0011304051149636507

Training epoch-18 batch-86
Running loss of epoch-18 batch-86 = 0.0006075772689655423

Training epoch-18 batch-87
Running loss of epoch-18 batch-87 = 0.0005287665408104658

Training epoch-18 batch-88
Running loss of epoch-18 batch-88 = 0.0006563020870089531

Training epoch-18 batch-89
Running loss of epoch-18 batch-89 = 0.0006695361225865781

Training epoch-18 batch-90
Running loss of epoch-18 batch-90 = 0.0015205065719783306

Training epoch-18 batch-91
Running loss of epoch-18 batch-91 = 0.0005362443625926971

Training epoch-18 batch-92
Running loss of epoch-18 batch-92 = 0.0018219195771962404

Training epoch-18 batch-93
Running loss of epoch-18 batch-93 = 0.0013491434510797262

Training epoch-18 batch-94
Running loss of epoch-18 batch-94 = 0.0013454827712848783

Training epoch-18 batch-95
Running loss of epoch-18 batch-95 = 0.000400810269638896

Training epoch-18 batch-96
Running loss of epoch-18 batch-96 = 0.0006782837444916368

Training epoch-18 batch-97
Running loss of epoch-18 batch-97 = 0.0015899207210168242

Training epoch-18 batch-98
Running loss of epoch-18 batch-98 = 0.0010082798544317484

Training epoch-18 batch-99
Running loss of epoch-18 batch-99 = 0.0006990457186475396

Training epoch-18 batch-100
Running loss of epoch-18 batch-100 = 0.000573247903957963

Training epoch-18 batch-101
Running loss of epoch-18 batch-101 = 0.0026551210321485996

Training epoch-18 batch-102
Running loss of epoch-18 batch-102 = 0.0013520708307623863

Training epoch-18 batch-103
Running loss of epoch-18 batch-103 = 0.0002526694443076849

Training epoch-18 batch-104
Running loss of epoch-18 batch-104 = 0.0005253517301753163

Training epoch-18 batch-105
Running loss of epoch-18 batch-105 = 0.0018414112273603678

Training epoch-18 batch-106
Running loss of epoch-18 batch-106 = 0.0004014067817479372

Training epoch-18 batch-107
Running loss of epoch-18 batch-107 = 0.0012708310969173908

Training epoch-18 batch-108
Running loss of epoch-18 batch-108 = 0.00043295626528561115

Training epoch-18 batch-109
Running loss of epoch-18 batch-109 = 0.0004962219391018152

Training epoch-18 batch-110
Running loss of epoch-18 batch-110 = 0.001240488258190453

Training epoch-18 batch-111
Running loss of epoch-18 batch-111 = 0.0007153669139370322

Training epoch-18 batch-112
Running loss of epoch-18 batch-112 = 0.0008770427666604519

Training epoch-18 batch-113
Running loss of epoch-18 batch-113 = 0.0013038866454735398

Training epoch-18 batch-114
Running loss of epoch-18 batch-114 = 0.0005863284459337592

Training epoch-18 batch-115
Running loss of epoch-18 batch-115 = 0.0008400962688028812

Training epoch-18 batch-116
Running loss of epoch-18 batch-116 = 0.0008471664041280746

Training epoch-18 batch-117
Running loss of epoch-18 batch-117 = 0.00035165436565876007

Training epoch-18 batch-118
Running loss of epoch-18 batch-118 = 0.0006292659090831876

Training epoch-18 batch-119
Running loss of epoch-18 batch-119 = 0.0006829720223322511

Training epoch-18 batch-120
Running loss of epoch-18 batch-120 = 0.0010664251167327166

Training epoch-18 batch-121
Running loss of epoch-18 batch-121 = 0.0008641800377517939

Training epoch-18 batch-122
Running loss of epoch-18 batch-122 = 0.0004158499650657177

Training epoch-18 batch-123
Running loss of epoch-18 batch-123 = 0.001538977725431323

Training epoch-18 batch-124
Running loss of epoch-18 batch-124 = 0.0012679083738476038

Training epoch-18 batch-125
Running loss of epoch-18 batch-125 = 0.0008670167298987508

Training epoch-18 batch-126
Running loss of epoch-18 batch-126 = 0.0011224610498175025

Training epoch-18 batch-127
Running loss of epoch-18 batch-127 = 0.00043495302088558674

Training epoch-18 batch-128
Running loss of epoch-18 batch-128 = 0.0007173458579927683

Training epoch-18 batch-129
Running loss of epoch-18 batch-129 = 0.0014464634004980326

Training epoch-18 batch-130
Running loss of epoch-18 batch-130 = 0.001043992699123919

Training epoch-18 batch-131
Running loss of epoch-18 batch-131 = 0.0010845920769497752

Training epoch-18 batch-132
Running loss of epoch-18 batch-132 = 0.0009032798698171973

Training epoch-18 batch-133
Running loss of epoch-18 batch-133 = 0.0007542097009718418

Training epoch-18 batch-134
Running loss of epoch-18 batch-134 = 0.000741582945920527

Training epoch-18 batch-135
Running loss of epoch-18 batch-135 = 0.0005865449784323573

Training epoch-18 batch-136
Running loss of epoch-18 batch-136 = 0.0018796026706695557

Training epoch-18 batch-137
Running loss of epoch-18 batch-137 = 0.0004130951128900051

Training epoch-18 batch-138
Running loss of epoch-18 batch-138 = 0.0014266445068642497

Training epoch-18 batch-139
Running loss of epoch-18 batch-139 = 0.0005380627699196339

Training epoch-18 batch-140
Running loss of epoch-18 batch-140 = 0.0008609251817688346

Training epoch-18 batch-141
Running loss of epoch-18 batch-141 = 0.0011569953057914972

Training epoch-18 batch-142
Running loss of epoch-18 batch-142 = 0.0011308097746223211

Training epoch-18 batch-143
Running loss of epoch-18 batch-143 = 0.0011950429761782289

Training epoch-18 batch-144
Running loss of epoch-18 batch-144 = 0.0008290096302516758

Training epoch-18 batch-145
Running loss of epoch-18 batch-145 = 0.0009327174630016088

Training epoch-18 batch-146
Running loss of epoch-18 batch-146 = 0.0014470175374299288

Training epoch-18 batch-147
Running loss of epoch-18 batch-147 = 0.0008032341138459742

Training epoch-18 batch-148
Running loss of epoch-18 batch-148 = 0.0003965673968195915

Training epoch-18 batch-149
Running loss of epoch-18 batch-149 = 0.0010793291730806231

Training epoch-18 batch-150
Running loss of epoch-18 batch-150 = 0.0008048121817409992

Training epoch-18 batch-151
Running loss of epoch-18 batch-151 = 0.000779166875872761

Training epoch-18 batch-152
Running loss of epoch-18 batch-152 = 0.0010258802212774754

Training epoch-18 batch-153
Running loss of epoch-18 batch-153 = 0.0005449516465887427

Training epoch-18 batch-154
Running loss of epoch-18 batch-154 = 0.0021180494222790003

Training epoch-18 batch-155
Running loss of epoch-18 batch-155 = 0.00039012019988149405

Training epoch-18 batch-156
Running loss of epoch-18 batch-156 = 0.00035247672349214554

Training epoch-18 batch-157
Running loss of epoch-18 batch-157 = 0.002441827207803726

Finished training epoch-18.



Average train loss at epoch-18 = 0.0009605834115296603

Started Evaluation

Average val loss at epoch-18 = 0.7324271240814644

Accuracy for classes:
Accuracy for class equals is: 97.52 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 78.85 %
Accuracy for class onCreate is: 94.35 %
Accuracy for class toString is: 79.86 %
Accuracy for class run is: 61.42 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 56.50 %
Accuracy for class execute is: 64.26 %
Accuracy for class get is: 65.38 %

Overall Accuracy = 82.05 %

Finished Evaluation



Started training epoch-19


Training epoch-19 batch-1
Running loss of epoch-19 batch-1 = 0.0006784226279705763

Training epoch-19 batch-2
Running loss of epoch-19 batch-2 = 0.000871397671289742

Training epoch-19 batch-3
Running loss of epoch-19 batch-3 = 0.0004813368432223797

Training epoch-19 batch-4
Running loss of epoch-19 batch-4 = 0.0002620285376906395

Training epoch-19 batch-5
Running loss of epoch-19 batch-5 = 0.00018100603483617306

Training epoch-19 batch-6
Running loss of epoch-19 batch-6 = 0.0006506104255095124

Training epoch-19 batch-7
Running loss of epoch-19 batch-7 = 0.0015082216123118997

Training epoch-19 batch-8
Running loss of epoch-19 batch-8 = 0.00049442274030298

Training epoch-19 batch-9
Running loss of epoch-19 batch-9 = 0.0006250764126889408

Training epoch-19 batch-10
Running loss of epoch-19 batch-10 = 0.0003482323372736573

Training epoch-19 batch-11
Running loss of epoch-19 batch-11 = 0.0014707687078043818

Training epoch-19 batch-12
Running loss of epoch-19 batch-12 = 0.0002864155685529113

Training epoch-19 batch-13
Running loss of epoch-19 batch-13 = 0.0008682030020281672

Training epoch-19 batch-14
Running loss of epoch-19 batch-14 = 0.0016805166378617287

Training epoch-19 batch-15
Running loss of epoch-19 batch-15 = 0.00012830307241529226

Training epoch-19 batch-16
Running loss of epoch-19 batch-16 = 0.001523424405604601

Training epoch-19 batch-17
Running loss of epoch-19 batch-17 = 0.00045005278661847115

Training epoch-19 batch-18
Running loss of epoch-19 batch-18 = 0.0005046190926805139

Training epoch-19 batch-19
Running loss of epoch-19 batch-19 = 0.00021337461657822132

Training epoch-19 batch-20
Running loss of epoch-19 batch-20 = 0.0006503636250272393

Training epoch-19 batch-21
Running loss of epoch-19 batch-21 = 0.0002255607396364212

Training epoch-19 batch-22
Running loss of epoch-19 batch-22 = 0.0007775495760142803

Training epoch-19 batch-23
Running loss of epoch-19 batch-23 = 0.0016416908474639058

Training epoch-19 batch-24
Running loss of epoch-19 batch-24 = 0.0006981766782701015

Training epoch-19 batch-25
Running loss of epoch-19 batch-25 = 0.0007803159533068538

Training epoch-19 batch-26
Running loss of epoch-19 batch-26 = 0.0002959233243018389

Training epoch-19 batch-27
Running loss of epoch-19 batch-27 = 0.0002245510695502162

Training epoch-19 batch-28
Running loss of epoch-19 batch-28 = 0.0008671452524140477

Training epoch-19 batch-29
Running loss of epoch-19 batch-29 = 0.0004782231990247965

Training epoch-19 batch-30
Running loss of epoch-19 batch-30 = 0.0006754982750862837

Training epoch-19 batch-31
Running loss of epoch-19 batch-31 = 0.0013626058353111148

Training epoch-19 batch-32
Running loss of epoch-19 batch-32 = 0.00038304185727611184

Training epoch-19 batch-33
Running loss of epoch-19 batch-33 = 0.0005287075182422996

Training epoch-19 batch-34
Running loss of epoch-19 batch-34 = 0.0008005690760910511

Training epoch-19 batch-35
Running loss of epoch-19 batch-35 = 0.0006063049659132957

Training epoch-19 batch-36
Running loss of epoch-19 batch-36 = 0.0011081978445872664

Training epoch-19 batch-37
Running loss of epoch-19 batch-37 = 0.0011814908357337117

Training epoch-19 batch-38
Running loss of epoch-19 batch-38 = 0.0004289192147552967

Training epoch-19 batch-39
Running loss of epoch-19 batch-39 = 0.00015870179049670696

Training epoch-19 batch-40
Running loss of epoch-19 batch-40 = 0.0006278941291384399

Training epoch-19 batch-41
Running loss of epoch-19 batch-41 = 0.0008491607150062919

Training epoch-19 batch-42
Running loss of epoch-19 batch-42 = 0.0006518769077956676

Training epoch-19 batch-43
Running loss of epoch-19 batch-43 = 0.0007947389967739582

Training epoch-19 batch-44
Running loss of epoch-19 batch-44 = 0.0023056117352098227

Training epoch-19 batch-45
Running loss of epoch-19 batch-45 = 0.0004772072425112128

Training epoch-19 batch-46
Running loss of epoch-19 batch-46 = 0.0004972027381882071

Training epoch-19 batch-47
Running loss of epoch-19 batch-47 = 0.0001671205973252654

Training epoch-19 batch-48
Running loss of epoch-19 batch-48 = 0.0005975059466436505

Training epoch-19 batch-49
Running loss of epoch-19 batch-49 = 0.0005192671669647098

Training epoch-19 batch-50
Running loss of epoch-19 batch-50 = 0.0004898299230262637

Training epoch-19 batch-51
Running loss of epoch-19 batch-51 = 0.0007184334681369364

Training epoch-19 batch-52
Running loss of epoch-19 batch-52 = 0.0006669138092547655

Training epoch-19 batch-53
Running loss of epoch-19 batch-53 = 0.0006988975219428539

Training epoch-19 batch-54
Running loss of epoch-19 batch-54 = 0.0001949480501934886

Training epoch-19 batch-55
Running loss of epoch-19 batch-55 = 0.0006703119724988937

Training epoch-19 batch-56
Running loss of epoch-19 batch-56 = 0.00013815308921039104

Training epoch-19 batch-57
Running loss of epoch-19 batch-57 = 0.001730718999169767

Training epoch-19 batch-58
Running loss of epoch-19 batch-58 = 0.0006369814509525895

Training epoch-19 batch-59
Running loss of epoch-19 batch-59 = 0.0002960064448416233

Training epoch-19 batch-60
Running loss of epoch-19 batch-60 = 0.0004863973008468747

Training epoch-19 batch-61
Running loss of epoch-19 batch-61 = 0.0006237345514819026

Training epoch-19 batch-62
Running loss of epoch-19 batch-62 = 0.00053202616982162

Training epoch-19 batch-63
Running loss of epoch-19 batch-63 = 0.001145690679550171

Training epoch-19 batch-64
Running loss of epoch-19 batch-64 = 0.0007281115977093577

Training epoch-19 batch-65
Running loss of epoch-19 batch-65 = 0.00043588096741586924

Training epoch-19 batch-66
Running loss of epoch-19 batch-66 = 0.0006393244257196784

Training epoch-19 batch-67
Running loss of epoch-19 batch-67 = 0.0003242281964048743

Training epoch-19 batch-68
Running loss of epoch-19 batch-68 = 0.0002560550346970558

Training epoch-19 batch-69
Running loss of epoch-19 batch-69 = 0.00042278633918613195

Training epoch-19 batch-70
Running loss of epoch-19 batch-70 = 0.0005797388730570674

Training epoch-19 batch-71
Running loss of epoch-19 batch-71 = 0.0010047300020232797

Training epoch-19 batch-72
Running loss of epoch-19 batch-72 = 0.0004508581478148699

Training epoch-19 batch-73
Running loss of epoch-19 batch-73 = 0.000741128227673471

Training epoch-19 batch-74
Running loss of epoch-19 batch-74 = 0.00045698205940425396

Training epoch-19 batch-75
Running loss of epoch-19 batch-75 = 0.00042300636414438486

Training epoch-19 batch-76
Running loss of epoch-19 batch-76 = 0.0014494301285594702

Training epoch-19 batch-77
Running loss of epoch-19 batch-77 = 0.0007502500666305423

Training epoch-19 batch-78
Running loss of epoch-19 batch-78 = 0.00046188896521925926

Training epoch-19 batch-79
Running loss of epoch-19 batch-79 = 0.0003948895027860999

Training epoch-19 batch-80
Running loss of epoch-19 batch-80 = 0.00038492970634251833

Training epoch-19 batch-81
Running loss of epoch-19 batch-81 = 0.0006418481934815645

Training epoch-19 batch-82
Running loss of epoch-19 batch-82 = 0.0003011138178408146

Training epoch-19 batch-83
Running loss of epoch-19 batch-83 = 0.00020167569164186716

Training epoch-19 batch-84
Running loss of epoch-19 batch-84 = 0.000734301982447505

Training epoch-19 batch-85
Running loss of epoch-19 batch-85 = 0.0005643793847411871

Training epoch-19 batch-86
Running loss of epoch-19 batch-86 = 0.0011841630330309272

Training epoch-19 batch-87
Running loss of epoch-19 batch-87 = 0.0006146867526695132

Training epoch-19 batch-88
Running loss of epoch-19 batch-88 = 0.0006417820695787668

Training epoch-19 batch-89
Running loss of epoch-19 batch-89 = 0.0016111412551254034

Training epoch-19 batch-90
Running loss of epoch-19 batch-90 = 0.0009568779496476054

Training epoch-19 batch-91
Running loss of epoch-19 batch-91 = 0.0004929136484861374

Training epoch-19 batch-92
Running loss of epoch-19 batch-92 = 0.0003186736721545458

Training epoch-19 batch-93
Running loss of epoch-19 batch-93 = 0.00040006719063967466

Training epoch-19 batch-94
Running loss of epoch-19 batch-94 = 0.0013518976047635078

Training epoch-19 batch-95
Running loss of epoch-19 batch-95 = 0.0005815569311380386

Training epoch-19 batch-96
Running loss of epoch-19 batch-96 = 0.0002556716790422797

Training epoch-19 batch-97
Running loss of epoch-19 batch-97 = 0.0006058939034119248

Training epoch-19 batch-98
Running loss of epoch-19 batch-98 = 0.0007077595219016075

Training epoch-19 batch-99
Running loss of epoch-19 batch-99 = 0.0005537030519917607

Training epoch-19 batch-100
Running loss of epoch-19 batch-100 = 0.0007201220141723752

Training epoch-19 batch-101
Running loss of epoch-19 batch-101 = 0.00015495193656533957

Training epoch-19 batch-102
Running loss of epoch-19 batch-102 = 0.001439701416529715

Training epoch-19 batch-103
Running loss of epoch-19 batch-103 = 0.00037770182825624943

Training epoch-19 batch-104
Running loss of epoch-19 batch-104 = 0.001013189903460443

Training epoch-19 batch-105
Running loss of epoch-19 batch-105 = 0.00044642516877502203

Training epoch-19 batch-106
Running loss of epoch-19 batch-106 = 0.0004718117415904999

Training epoch-19 batch-107
Running loss of epoch-19 batch-107 = 0.0006446631159633398

Training epoch-19 batch-108
Running loss of epoch-19 batch-108 = 0.0008050680626183748

Training epoch-19 batch-109
Running loss of epoch-19 batch-109 = 0.0005651487736031413

Training epoch-19 batch-110
Running loss of epoch-19 batch-110 = 0.0006040093721821904

Training epoch-19 batch-111
Running loss of epoch-19 batch-111 = 0.000728983897715807

Training epoch-19 batch-112
Running loss of epoch-19 batch-112 = 0.0011667414801195264

Training epoch-19 batch-113
Running loss of epoch-19 batch-113 = 0.0016486673848703504

Training epoch-19 batch-114
Running loss of epoch-19 batch-114 = 0.0008653020486235619

Training epoch-19 batch-115
Running loss of epoch-19 batch-115 = 7.397588342428207e-05

Training epoch-19 batch-116
Running loss of epoch-19 batch-116 = 0.00022497493773698807

Training epoch-19 batch-117
Running loss of epoch-19 batch-117 = 0.0009570852853357792

Training epoch-19 batch-118
Running loss of epoch-19 batch-118 = 0.001936506014317274

Training epoch-19 batch-119
Running loss of epoch-19 batch-119 = 0.0003210422582924366

Training epoch-19 batch-120
Running loss of epoch-19 batch-120 = 0.0003814938245341182

Training epoch-19 batch-121
Running loss of epoch-19 batch-121 = 0.0006575691513717175

Training epoch-19 batch-122
Running loss of epoch-19 batch-122 = 0.0007479202467948198

Training epoch-19 batch-123
Running loss of epoch-19 batch-123 = 0.0006684520049020648

Training epoch-19 batch-124
Running loss of epoch-19 batch-124 = 0.0009667832055129111

Training epoch-19 batch-125
Running loss of epoch-19 batch-125 = 0.0008285765652544796

Training epoch-19 batch-126
Running loss of epoch-19 batch-126 = 0.00030349427834153175

Training epoch-19 batch-127
Running loss of epoch-19 batch-127 = 0.0004557637730613351

Training epoch-19 batch-128
Running loss of epoch-19 batch-128 = 0.0008548101177439094

Training epoch-19 batch-129
Running loss of epoch-19 batch-129 = 0.000769810052588582

Training epoch-19 batch-130
Running loss of epoch-19 batch-130 = 0.0007721288129687309

Training epoch-19 batch-131
Running loss of epoch-19 batch-131 = 0.00033975555561482906

Training epoch-19 batch-132
Running loss of epoch-19 batch-132 = 0.001502207014709711

Training epoch-19 batch-133
Running loss of epoch-19 batch-133 = 0.00040540145710110664

Training epoch-19 batch-134
Running loss of epoch-19 batch-134 = 0.0005647706566378474

Training epoch-19 batch-135
Running loss of epoch-19 batch-135 = 0.0004914359888061881

Training epoch-19 batch-136
Running loss of epoch-19 batch-136 = 0.00036889524199068546

Training epoch-19 batch-137
Running loss of epoch-19 batch-137 = 0.0007415158906951547

Training epoch-19 batch-138
Running loss of epoch-19 batch-138 = 0.00027051125653088093

Training epoch-19 batch-139
Running loss of epoch-19 batch-139 = 0.0005044619319960475

Training epoch-19 batch-140
Running loss of epoch-19 batch-140 = 0.0008300013141706586

Training epoch-19 batch-141
Running loss of epoch-19 batch-141 = 0.0007640174590051174

Training epoch-19 batch-142
Running loss of epoch-19 batch-142 = 0.00039472233038395643

Training epoch-19 batch-143
Running loss of epoch-19 batch-143 = 0.0009161904454231262

Training epoch-19 batch-144
Running loss of epoch-19 batch-144 = 0.0006376003148034215

Training epoch-19 batch-145
Running loss of epoch-19 batch-145 = 0.000490225967951119

Training epoch-19 batch-146
Running loss of epoch-19 batch-146 = 0.0002907748566940427

Training epoch-19 batch-147
Running loss of epoch-19 batch-147 = 0.000978544820100069

Training epoch-19 batch-148
Running loss of epoch-19 batch-148 = 0.00033833994530141354

Training epoch-19 batch-149
Running loss of epoch-19 batch-149 = 0.000835558632388711

Training epoch-19 batch-150
Running loss of epoch-19 batch-150 = 0.0007644196739420295

Training epoch-19 batch-151
Running loss of epoch-19 batch-151 = 0.001565316691994667

Training epoch-19 batch-152
Running loss of epoch-19 batch-152 = 0.0012258193455636501

Training epoch-19 batch-153
Running loss of epoch-19 batch-153 = 0.000812637503258884

Training epoch-19 batch-154
Running loss of epoch-19 batch-154 = 0.0003057286376133561

Training epoch-19 batch-155
Running loss of epoch-19 batch-155 = 0.0004421272315084934

Training epoch-19 batch-156
Running loss of epoch-19 batch-156 = 0.001547984778881073

Training epoch-19 batch-157
Running loss of epoch-19 batch-157 = 0.002030603587627411

Finished training epoch-19.



Average train loss at epoch-19 = 0.0006889626659452915

Started Evaluation

Average val loss at epoch-19 = 0.7464328345798238

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 92.79 %
Accuracy for class onCreate is: 90.09 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 70.78 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 49.78 %
Accuracy for class execute is: 49.80 %
Accuracy for class get is: 76.15 %

Overall Accuracy = 83.83 %

Finished Evaluation



Started training epoch-20


Training epoch-20 batch-1
Running loss of epoch-20 batch-1 = 0.0007335988339036703

Training epoch-20 batch-2
Running loss of epoch-20 batch-2 = 0.0003735849168151617

Training epoch-20 batch-3
Running loss of epoch-20 batch-3 = 0.00030915916431695223

Training epoch-20 batch-4
Running loss of epoch-20 batch-4 = 0.000580625724978745

Training epoch-20 batch-5
Running loss of epoch-20 batch-5 = 0.00024045794270932674

Training epoch-20 batch-6
Running loss of epoch-20 batch-6 = 0.0003621279029175639

Training epoch-20 batch-7
Running loss of epoch-20 batch-7 = 0.0012039896100759506

Training epoch-20 batch-8
Running loss of epoch-20 batch-8 = 0.00016864226199686527

Training epoch-20 batch-9
Running loss of epoch-20 batch-9 = 0.0007116962224245071

Training epoch-20 batch-10
Running loss of epoch-20 batch-10 = 0.0002172860549762845

Training epoch-20 batch-11
Running loss of epoch-20 batch-11 = 0.00033209414687007666

Training epoch-20 batch-12
Running loss of epoch-20 batch-12 = 0.0018863797886297107

Training epoch-20 batch-13
Running loss of epoch-20 batch-13 = 0.00031380949076265097

Training epoch-20 batch-14
Running loss of epoch-20 batch-14 = 0.001063209492713213

Training epoch-20 batch-15
Running loss of epoch-20 batch-15 = 0.0005303273210301995

Training epoch-20 batch-16
Running loss of epoch-20 batch-16 = 0.0010783582692965865

Training epoch-20 batch-17
Running loss of epoch-20 batch-17 = 0.0009192980360239744

Training epoch-20 batch-18
Running loss of epoch-20 batch-18 = 0.0007245619781315327

Training epoch-20 batch-19
Running loss of epoch-20 batch-19 = 0.00156407430768013

Training epoch-20 batch-20
Running loss of epoch-20 batch-20 = 0.00015636812895536423

Training epoch-20 batch-21
Running loss of epoch-20 batch-21 = 0.00022375653497874737

Training epoch-20 batch-22
Running loss of epoch-20 batch-22 = 0.0006710846209898591

Training epoch-20 batch-23
Running loss of epoch-20 batch-23 = 0.0003950985847041011

Training epoch-20 batch-24
Running loss of epoch-20 batch-24 = 0.0005817104829475284

Training epoch-20 batch-25
Running loss of epoch-20 batch-25 = 0.00011498993262648582

Training epoch-20 batch-26
Running loss of epoch-20 batch-26 = 0.00045201019383966923

Training epoch-20 batch-27
Running loss of epoch-20 batch-27 = 0.00027496658731251955

Training epoch-20 batch-28
Running loss of epoch-20 batch-28 = 0.0002863231347873807

Training epoch-20 batch-29
Running loss of epoch-20 batch-29 = 0.0004481399664655328

Training epoch-20 batch-30
Running loss of epoch-20 batch-30 = 0.0002021309919655323

Training epoch-20 batch-31
Running loss of epoch-20 batch-31 = 0.0008173388196155429

Training epoch-20 batch-32
Running loss of epoch-20 batch-32 = 0.0003695135237649083

Training epoch-20 batch-33
Running loss of epoch-20 batch-33 = 0.00038250337820500135

Training epoch-20 batch-34
Running loss of epoch-20 batch-34 = 0.000346764805726707

Training epoch-20 batch-35
Running loss of epoch-20 batch-35 = 0.0013240668922662735

Training epoch-20 batch-36
Running loss of epoch-20 batch-36 = 0.00026320386677980423

Training epoch-20 batch-37
Running loss of epoch-20 batch-37 = 0.0005757001927122474

Training epoch-20 batch-38
Running loss of epoch-20 batch-38 = 0.0004912028089165688

Training epoch-20 batch-39
Running loss of epoch-20 batch-39 = 0.00046297290828078985

Training epoch-20 batch-40
Running loss of epoch-20 batch-40 = 0.0001809094101190567

Training epoch-20 batch-41
Running loss of epoch-20 batch-41 = 0.0003310033353045583

Training epoch-20 batch-42
Running loss of epoch-20 batch-42 = 0.0004930973518639803

Training epoch-20 batch-43
Running loss of epoch-20 batch-43 = 0.00035699247382581234

Training epoch-20 batch-44
Running loss of epoch-20 batch-44 = 0.0008452120237052441

Training epoch-20 batch-45
Running loss of epoch-20 batch-45 = 0.00040845724288374186

Training epoch-20 batch-46
Running loss of epoch-20 batch-46 = 0.0016285096062347293

Training epoch-20 batch-47
Running loss of epoch-20 batch-47 = 0.0005596218397840858

Training epoch-20 batch-48
Running loss of epoch-20 batch-48 = 0.0006414479576051235

Training epoch-20 batch-49
Running loss of epoch-20 batch-49 = 0.0005298135802149773

Training epoch-20 batch-50
Running loss of epoch-20 batch-50 = 0.0005651944084092975

Training epoch-20 batch-51
Running loss of epoch-20 batch-51 = 0.0003746144939213991

Training epoch-20 batch-52
Running loss of epoch-20 batch-52 = 0.00035322143230587244

Training epoch-20 batch-53
Running loss of epoch-20 batch-53 = 0.0005246405489742756

Training epoch-20 batch-54
Running loss of epoch-20 batch-54 = 0.00041725055780261755

Training epoch-20 batch-55
Running loss of epoch-20 batch-55 = 0.0013700142735615373

Training epoch-20 batch-56
Running loss of epoch-20 batch-56 = 0.0003665839321911335

Training epoch-20 batch-57
Running loss of epoch-20 batch-57 = 0.0003803897416219115

Training epoch-20 batch-58
Running loss of epoch-20 batch-58 = 0.0005883362609893084

Training epoch-20 batch-59
Running loss of epoch-20 batch-59 = 0.0004056228790432215

Training epoch-20 batch-60
Running loss of epoch-20 batch-60 = 0.0006151201669126749

Training epoch-20 batch-61
Running loss of epoch-20 batch-61 = 0.00047720526345074177

Training epoch-20 batch-62
Running loss of epoch-20 batch-62 = 0.0007598144002258778

Training epoch-20 batch-63
Running loss of epoch-20 batch-63 = 0.00035165436565876007

Training epoch-20 batch-64
Running loss of epoch-20 batch-64 = 0.00030480080749839544

Training epoch-20 batch-65
Running loss of epoch-20 batch-65 = 0.0006817138637416065

Training epoch-20 batch-66
Running loss of epoch-20 batch-66 = 0.00028984434902668

Training epoch-20 batch-67
Running loss of epoch-20 batch-67 = 0.00021982239559292793

Training epoch-20 batch-68
Running loss of epoch-20 batch-68 = 0.0002477742964401841

Training epoch-20 batch-69
Running loss of epoch-20 batch-69 = 0.0006330015603452921

Training epoch-20 batch-70
Running loss of epoch-20 batch-70 = 0.0006839284906163812

Training epoch-20 batch-71
Running loss of epoch-20 batch-71 = 0.00013278843834996223

Training epoch-20 batch-72
Running loss of epoch-20 batch-72 = 0.00018990249373018742

Training epoch-20 batch-73
Running loss of epoch-20 batch-73 = 0.0010479551274329424

Training epoch-20 batch-74
Running loss of epoch-20 batch-74 = 0.0007396673317998648

Training epoch-20 batch-75
Running loss of epoch-20 batch-75 = 0.00022479810286313295

Training epoch-20 batch-76
Running loss of epoch-20 batch-76 = 0.0010658262763172388

Training epoch-20 batch-77
Running loss of epoch-20 batch-77 = 0.0008369704009965062

Training epoch-20 batch-78
Running loss of epoch-20 batch-78 = 0.00045752106234431267

Training epoch-20 batch-79
Running loss of epoch-20 batch-79 = 0.0002797722117975354

Training epoch-20 batch-80
Running loss of epoch-20 batch-80 = 0.0004823198541998863

Training epoch-20 batch-81
Running loss of epoch-20 batch-81 = 0.00027452653739601374

Training epoch-20 batch-82
Running loss of epoch-20 batch-82 = 0.0008738067699596286

Training epoch-20 batch-83
Running loss of epoch-20 batch-83 = 0.00018899922724813223

Training epoch-20 batch-84
Running loss of epoch-20 batch-84 = 0.00023947330191731453

Training epoch-20 batch-85
Running loss of epoch-20 batch-85 = 0.000555256032384932

Training epoch-20 batch-86
Running loss of epoch-20 batch-86 = 0.00017454673070460558

Training epoch-20 batch-87
Running loss of epoch-20 batch-87 = 0.00014699401799589396

Training epoch-20 batch-88
Running loss of epoch-20 batch-88 = 0.0015524898190051317

Training epoch-20 batch-89
Running loss of epoch-20 batch-89 = 0.00013379589654505253

Training epoch-20 batch-90
Running loss of epoch-20 batch-90 = 0.00024788256268948317

Training epoch-20 batch-91
Running loss of epoch-20 batch-91 = 0.00047586834989488125

Training epoch-20 batch-92
Running loss of epoch-20 batch-92 = 0.0003244502004235983

Training epoch-20 batch-93
Running loss of epoch-20 batch-93 = 0.00020622368901968002

Training epoch-20 batch-94
Running loss of epoch-20 batch-94 = 0.0007928527193143964

Training epoch-20 batch-95
Running loss of epoch-20 batch-95 = 0.0002891211770474911

Training epoch-20 batch-96
Running loss of epoch-20 batch-96 = 0.0003583460347726941

Training epoch-20 batch-97
Running loss of epoch-20 batch-97 = 0.00015696510672569275

Training epoch-20 batch-98
Running loss of epoch-20 batch-98 = 0.0003317986847832799

Training epoch-20 batch-99
Running loss of epoch-20 batch-99 = 0.0001015916932374239

Training epoch-20 batch-100
Running loss of epoch-20 batch-100 = 0.00027938641142100096

Training epoch-20 batch-101
Running loss of epoch-20 batch-101 = 0.0011021095560863614

Training epoch-20 batch-102
Running loss of epoch-20 batch-102 = 0.001090716803446412

Training epoch-20 batch-103
Running loss of epoch-20 batch-103 = 0.00023123598657548428

Training epoch-20 batch-104
Running loss of epoch-20 batch-104 = 0.00040729157626628876

Training epoch-20 batch-105
Running loss of epoch-20 batch-105 = 0.000246060430072248

Training epoch-20 batch-106
Running loss of epoch-20 batch-106 = 0.0003732815384864807

Training epoch-20 batch-107
Running loss of epoch-20 batch-107 = 0.00020565290469676256

Training epoch-20 batch-108
Running loss of epoch-20 batch-108 = 0.0007858655881136656

Training epoch-20 batch-109
Running loss of epoch-20 batch-109 = 0.0008741369238123298

Training epoch-20 batch-110
Running loss of epoch-20 batch-110 = 0.0001879427582025528

Training epoch-20 batch-111
Running loss of epoch-20 batch-111 = 0.0001225401647388935

Training epoch-20 batch-112
Running loss of epoch-20 batch-112 = 0.000461099436506629

Training epoch-20 batch-113
Running loss of epoch-20 batch-113 = 0.000254726386629045

Training epoch-20 batch-114
Running loss of epoch-20 batch-114 = 0.000632790382951498

Training epoch-20 batch-115
Running loss of epoch-20 batch-115 = 0.0009430025238543749

Training epoch-20 batch-116
Running loss of epoch-20 batch-116 = 0.0012645793613046408

Training epoch-20 batch-117
Running loss of epoch-20 batch-117 = 0.0002182347234338522

Training epoch-20 batch-118
Running loss of epoch-20 batch-118 = 0.00015662424266338348

Training epoch-20 batch-119
Running loss of epoch-20 batch-119 = 0.0004846415831707418

Training epoch-20 batch-120
Running loss of epoch-20 batch-120 = 0.00014970079064369202

Training epoch-20 batch-121
Running loss of epoch-20 batch-121 = 0.0003618987975642085

Training epoch-20 batch-122
Running loss of epoch-20 batch-122 = 0.00015992240514606237

Training epoch-20 batch-123
Running loss of epoch-20 batch-123 = 0.00028751499485224485

Training epoch-20 batch-124
Running loss of epoch-20 batch-124 = 0.0006174687296152115

Training epoch-20 batch-125
Running loss of epoch-20 batch-125 = 0.0009111870313063264

Training epoch-20 batch-126
Running loss of epoch-20 batch-126 = 0.000689089298248291

Training epoch-20 batch-127
Running loss of epoch-20 batch-127 = 0.00017372542060911655

Training epoch-20 batch-128
Running loss of epoch-20 batch-128 = 0.0004251886857673526

Training epoch-20 batch-129
Running loss of epoch-20 batch-129 = 0.00040021492168307304

Training epoch-20 batch-130
Running loss of epoch-20 batch-130 = 0.00021261826623231173

Training epoch-20 batch-131
Running loss of epoch-20 batch-131 = 0.00026967411395162344

Training epoch-20 batch-132
Running loss of epoch-20 batch-132 = 0.0003583197249099612

Training epoch-20 batch-133
Running loss of epoch-20 batch-133 = 0.0007964541437104344

Training epoch-20 batch-134
Running loss of epoch-20 batch-134 = 0.0006259543588384986

Training epoch-20 batch-135
Running loss of epoch-20 batch-135 = 0.0002410451415926218

Training epoch-20 batch-136
Running loss of epoch-20 batch-136 = 0.00040637352503836155

Training epoch-20 batch-137
Running loss of epoch-20 batch-137 = 0.0005444040289148688

Training epoch-20 batch-138
Running loss of epoch-20 batch-138 = 7.076305337250233e-05

Training epoch-20 batch-139
Running loss of epoch-20 batch-139 = 0.000317553523927927

Training epoch-20 batch-140
Running loss of epoch-20 batch-140 = 0.0006325223948806524

Training epoch-20 batch-141
Running loss of epoch-20 batch-141 = 0.00037522148340940475

Training epoch-20 batch-142
Running loss of epoch-20 batch-142 = 0.0004384425701573491

Training epoch-20 batch-143
Running loss of epoch-20 batch-143 = 0.00044137693475931883

Training epoch-20 batch-144
Running loss of epoch-20 batch-144 = 0.0003269539447501302

Training epoch-20 batch-145
Running loss of epoch-20 batch-145 = 0.0005075223743915558

Training epoch-20 batch-146
Running loss of epoch-20 batch-146 = 0.0010496120667085052

Training epoch-20 batch-147
Running loss of epoch-20 batch-147 = 0.0005588290514424443

Training epoch-20 batch-148
Running loss of epoch-20 batch-148 = 0.00016294862143695354

Training epoch-20 batch-149
Running loss of epoch-20 batch-149 = 0.0001939440844580531

Training epoch-20 batch-150
Running loss of epoch-20 batch-150 = 0.00016831012908369303

Training epoch-20 batch-151
Running loss of epoch-20 batch-151 = 0.00031226861756294966

Training epoch-20 batch-152
Running loss of epoch-20 batch-152 = 0.0002628256333991885

Training epoch-20 batch-153
Running loss of epoch-20 batch-153 = 0.0010452389251440763

Training epoch-20 batch-154
Running loss of epoch-20 batch-154 = 0.00042302487418055534

Training epoch-20 batch-155
Running loss of epoch-20 batch-155 = 0.0003925689961761236

Training epoch-20 batch-156
Running loss of epoch-20 batch-156 = 0.0003595181042328477

Training epoch-20 batch-157
Running loss of epoch-20 batch-157 = 0.0017052888870239258

Finished training epoch-20.



Average train loss at epoch-20 = 0.0004994540303945541

Started Evaluation

Average val loss at epoch-20 = 0.7351018109482922

Accuracy for classes:
Accuracy for class equals is: 97.19 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.97 %
Accuracy for class onCreate is: 92.32 %
Accuracy for class toString is: 90.10 %
Accuracy for class run is: 70.09 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 49.55 %
Accuracy for class execute is: 56.22 %
Accuracy for class get is: 61.79 %

Overall Accuracy = 83.37 %

Finished Evaluation



Started training epoch-21


Training epoch-21 batch-1
Running loss of epoch-21 batch-1 = 0.0005519409896805882

Training epoch-21 batch-2
Running loss of epoch-21 batch-2 = 0.00031473860144615173

Training epoch-21 batch-3
Running loss of epoch-21 batch-3 = 0.000418968265876174

Training epoch-21 batch-4
Running loss of epoch-21 batch-4 = 0.0003880083095282316

Training epoch-21 batch-5
Running loss of epoch-21 batch-5 = 0.0002957789693027735

Training epoch-21 batch-6
Running loss of epoch-21 batch-6 = 0.00010269891936331987

Training epoch-21 batch-7
Running loss of epoch-21 batch-7 = 0.00011251983232796192

Training epoch-21 batch-8
Running loss of epoch-21 batch-8 = 0.00026591343339532614

Training epoch-21 batch-9
Running loss of epoch-21 batch-9 = 0.00015118822921067476

Training epoch-21 batch-10
Running loss of epoch-21 batch-10 = 0.00023029465228319168

Training epoch-21 batch-11
Running loss of epoch-21 batch-11 = 0.0003378443652763963

Training epoch-21 batch-12
Running loss of epoch-21 batch-12 = 0.0003124503418803215

Training epoch-21 batch-13
Running loss of epoch-21 batch-13 = 0.00047852983698248863

Training epoch-21 batch-14
Running loss of epoch-21 batch-14 = 0.0001970080193132162

Training epoch-21 batch-15
Running loss of epoch-21 batch-15 = 0.00035177229437977076

Training epoch-21 batch-16
Running loss of epoch-21 batch-16 = 0.00010277435649186373

Training epoch-21 batch-17
Running loss of epoch-21 batch-17 = 0.00019317073747515678

Training epoch-21 batch-18
Running loss of epoch-21 batch-18 = 0.0004047637339681387

Training epoch-21 batch-19
Running loss of epoch-21 batch-19 = 0.0001898379996418953

Training epoch-21 batch-20
Running loss of epoch-21 batch-20 = 0.0005964182782918215

Training epoch-21 batch-21
Running loss of epoch-21 batch-21 = 0.00012645567767322063

Training epoch-21 batch-22
Running loss of epoch-21 batch-22 = 0.00025634991470724344

Training epoch-21 batch-23
Running loss of epoch-21 batch-23 = 0.00032369617838412523

Training epoch-21 batch-24
Running loss of epoch-21 batch-24 = 0.000388016109354794

Training epoch-21 batch-25
Running loss of epoch-21 batch-25 = 0.000677499920129776

Training epoch-21 batch-26
Running loss of epoch-21 batch-26 = 0.00043363776057958603

Training epoch-21 batch-27
Running loss of epoch-21 batch-27 = 0.0004518491914495826

Training epoch-21 batch-28
Running loss of epoch-21 batch-28 = 0.00039185897912830114

Training epoch-21 batch-29
Running loss of epoch-21 batch-29 = 0.0005172084784135222

Training epoch-21 batch-30
Running loss of epoch-21 batch-30 = 0.0003429051721468568

Training epoch-21 batch-31
Running loss of epoch-21 batch-31 = 0.00046363298315554857

Training epoch-21 batch-32
Running loss of epoch-21 batch-32 = 0.0003534007119014859

Training epoch-21 batch-33
Running loss of epoch-21 batch-33 = 0.00022851605899631977

Training epoch-21 batch-34
Running loss of epoch-21 batch-34 = 0.0001399102620780468

Training epoch-21 batch-35
Running loss of epoch-21 batch-35 = 0.00046541797928512096

Training epoch-21 batch-36
Running loss of epoch-21 batch-36 = 0.00017360574565827847

Training epoch-21 batch-37
Running loss of epoch-21 batch-37 = 0.00028559460770338774

Training epoch-21 batch-38
Running loss of epoch-21 batch-38 = 0.00024224608205258846

Training epoch-21 batch-39
Running loss of epoch-21 batch-39 = 0.00016175676137208939

Training epoch-21 batch-40
Running loss of epoch-21 batch-40 = 0.00022021401673555374

Training epoch-21 batch-41
Running loss of epoch-21 batch-41 = 0.0003020073054358363

Training epoch-21 batch-42
Running loss of epoch-21 batch-42 = 0.00020897132344543934

Training epoch-21 batch-43
Running loss of epoch-21 batch-43 = 0.00020980415865778923

Training epoch-21 batch-44
Running loss of epoch-21 batch-44 = 0.00018733949400484562

Training epoch-21 batch-45
Running loss of epoch-21 batch-45 = 0.0002500808332115412

Training epoch-21 batch-46
Running loss of epoch-21 batch-46 = 0.0001142874825745821

Training epoch-21 batch-47
Running loss of epoch-21 batch-47 = 0.0002366490662097931

Training epoch-21 batch-48
Running loss of epoch-21 batch-48 = 0.00027110602241009474

Training epoch-21 batch-49
Running loss of epoch-21 batch-49 = 0.00014048395678400993

Training epoch-21 batch-50
Running loss of epoch-21 batch-50 = 0.0005662745097652078

Training epoch-21 batch-51
Running loss of epoch-21 batch-51 = 0.00022234662901610136

Training epoch-21 batch-52
Running loss of epoch-21 batch-52 = 0.00017985026352107525

Training epoch-21 batch-53
Running loss of epoch-21 batch-53 = 7.760070730000734e-05

Training epoch-21 batch-54
Running loss of epoch-21 batch-54 = 4.673155490309e-05

Training epoch-21 batch-55
Running loss of epoch-21 batch-55 = 0.0011001896345987916

Training epoch-21 batch-56
Running loss of epoch-21 batch-56 = 0.00042123766615986824

Training epoch-21 batch-57
Running loss of epoch-21 batch-57 = 0.00030229834374040365

Training epoch-21 batch-58
Running loss of epoch-21 batch-58 = 0.00023099128156900406

Training epoch-21 batch-59
Running loss of epoch-21 batch-59 = 0.0002761369105428457

Training epoch-21 batch-60
Running loss of epoch-21 batch-60 = 0.0003258251817896962

Training epoch-21 batch-61
Running loss of epoch-21 batch-61 = 0.00011403625831007957

Training epoch-21 batch-62
Running loss of epoch-21 batch-62 = 0.0006201290525496006

Training epoch-21 batch-63
Running loss of epoch-21 batch-63 = 0.00031312473583966494

Training epoch-21 batch-64
Running loss of epoch-21 batch-64 = 0.00042106176260858774

Training epoch-21 batch-65
Running loss of epoch-21 batch-65 = 8.001027163118124e-05

Training epoch-21 batch-66
Running loss of epoch-21 batch-66 = 0.0009672687156125903

Training epoch-21 batch-67
Running loss of epoch-21 batch-67 = 0.000290192780084908

Training epoch-21 batch-68
Running loss of epoch-21 batch-68 = 0.00010963622480630875

Training epoch-21 batch-69
Running loss of epoch-21 batch-69 = 0.0004985963460057974

Training epoch-21 batch-70
Running loss of epoch-21 batch-70 = 0.0001283884048461914

Training epoch-21 batch-71
Running loss of epoch-21 batch-71 = 0.00022027920931577682

Training epoch-21 batch-72
Running loss of epoch-21 batch-72 = 0.00023265881463885307

Training epoch-21 batch-73
Running loss of epoch-21 batch-73 = 0.0014285356737673283

Training epoch-21 batch-74
Running loss of epoch-21 batch-74 = 0.00011064356658607721

Training epoch-21 batch-75
Running loss of epoch-21 batch-75 = 0.000327424262650311

Training epoch-21 batch-76
Running loss of epoch-21 batch-76 = 0.00022004160564392805

Training epoch-21 batch-77
Running loss of epoch-21 batch-77 = 8.954224176704884e-05

Training epoch-21 batch-78
Running loss of epoch-21 batch-78 = 0.0006338789826259017

Training epoch-21 batch-79
Running loss of epoch-21 batch-79 = 0.0005245740758255124

Training epoch-21 batch-80
Running loss of epoch-21 batch-80 = 0.00012112909462302923

Training epoch-21 batch-81
Running loss of epoch-21 batch-81 = 0.0003878565039485693

Training epoch-21 batch-82
Running loss of epoch-21 batch-82 = 0.00010641582775861025

Training epoch-21 batch-83
Running loss of epoch-21 batch-83 = 0.00033360207453370094

Training epoch-21 batch-84
Running loss of epoch-21 batch-84 = 0.0002925045555457473

Training epoch-21 batch-85
Running loss of epoch-21 batch-85 = 0.0010440468322485685

Training epoch-21 batch-86
Running loss of epoch-21 batch-86 = 0.0006173084257170558

Training epoch-21 batch-87
Running loss of epoch-21 batch-87 = 0.00014068069867789745

Training epoch-21 batch-88
Running loss of epoch-21 batch-88 = 0.00023942324332892895

Training epoch-21 batch-89
Running loss of epoch-21 batch-89 = 0.0001912560546770692

Training epoch-21 batch-90
Running loss of epoch-21 batch-90 = 0.00029885442927479744

Training epoch-21 batch-91
Running loss of epoch-21 batch-91 = 0.0002768910489976406

Training epoch-21 batch-92
Running loss of epoch-21 batch-92 = 0.0005607953062281013

Training epoch-21 batch-93
Running loss of epoch-21 batch-93 = 0.0002549942582845688

Training epoch-21 batch-94
Running loss of epoch-21 batch-94 = 0.00025890266988426447

Training epoch-21 batch-95
Running loss of epoch-21 batch-95 = 0.0003351594787091017

Training epoch-21 batch-96
Running loss of epoch-21 batch-96 = 0.0005512183997780085

Training epoch-21 batch-97
Running loss of epoch-21 batch-97 = 0.0005209828959777951

Training epoch-21 batch-98
Running loss of epoch-21 batch-98 = 0.00030743679963052273

Training epoch-21 batch-99
Running loss of epoch-21 batch-99 = 7.074617315083742e-05

Training epoch-21 batch-100
Running loss of epoch-21 batch-100 = 0.0006109699606895447

Training epoch-21 batch-101
Running loss of epoch-21 batch-101 = 0.00011140899732708931

Training epoch-21 batch-102
Running loss of epoch-21 batch-102 = 0.00010620453394949436

Training epoch-21 batch-103
Running loss of epoch-21 batch-103 = 0.0008705497020855546

Training epoch-21 batch-104
Running loss of epoch-21 batch-104 = 0.0014468403533101082

Training epoch-21 batch-105
Running loss of epoch-21 batch-105 = 0.0001794182462617755

Training epoch-21 batch-106
Running loss of epoch-21 batch-106 = 0.0014235620619729161

Training epoch-21 batch-107
Running loss of epoch-21 batch-107 = 0.0014735994627699256

Training epoch-21 batch-108
Running loss of epoch-21 batch-108 = 0.0003080646274611354

Training epoch-21 batch-109
Running loss of epoch-21 batch-109 = 0.0003467976348474622

Training epoch-21 batch-110
Running loss of epoch-21 batch-110 = 0.000204471405595541

Training epoch-21 batch-111
Running loss of epoch-21 batch-111 = 0.0003148723626509309

Training epoch-21 batch-112
Running loss of epoch-21 batch-112 = 0.00016313244123011827

Training epoch-21 batch-113
Running loss of epoch-21 batch-113 = 0.00018069660291075706

Training epoch-21 batch-114
Running loss of epoch-21 batch-114 = 0.00014499039389193058

Training epoch-21 batch-115
Running loss of epoch-21 batch-115 = 0.0006662288215011358

Training epoch-21 batch-116
Running loss of epoch-21 batch-116 = 0.0001807637745514512

Training epoch-21 batch-117
Running loss of epoch-21 batch-117 = 0.0003420277498662472

Training epoch-21 batch-118
Running loss of epoch-21 batch-118 = 0.0003406702307984233

Training epoch-21 batch-119
Running loss of epoch-21 batch-119 = 0.0001553129404783249

Training epoch-21 batch-120
Running loss of epoch-21 batch-120 = 0.00036057073157280684

Training epoch-21 batch-121
Running loss of epoch-21 batch-121 = 0.00019104406237602234

Training epoch-21 batch-122
Running loss of epoch-21 batch-122 = 0.0007047116523608565

Training epoch-21 batch-123
Running loss of epoch-21 batch-123 = 0.0003105662763118744

Training epoch-21 batch-124
Running loss of epoch-21 batch-124 = 0.0005610415246337652

Training epoch-21 batch-125
Running loss of epoch-21 batch-125 = 0.0004884528461843729

Training epoch-21 batch-126
Running loss of epoch-21 batch-126 = 0.00029544346034526825

Training epoch-21 batch-127
Running loss of epoch-21 batch-127 = 0.0001269780332222581

Training epoch-21 batch-128
Running loss of epoch-21 batch-128 = 0.00032105413265526295

Training epoch-21 batch-129
Running loss of epoch-21 batch-129 = 0.00024325947742909193

Training epoch-21 batch-130
Running loss of epoch-21 batch-130 = 0.00041501515079289675

Training epoch-21 batch-131
Running loss of epoch-21 batch-131 = 0.00024194899015128613

Training epoch-21 batch-132
Running loss of epoch-21 batch-132 = 0.0003859002608805895

Training epoch-21 batch-133
Running loss of epoch-21 batch-133 = 0.0001004277728497982

Training epoch-21 batch-134
Running loss of epoch-21 batch-134 = 0.0008482502307742834

Training epoch-21 batch-135
Running loss of epoch-21 batch-135 = 0.00041944661643356085

Training epoch-21 batch-136
Running loss of epoch-21 batch-136 = 0.00018184632062911987

Training epoch-21 batch-137
Running loss of epoch-21 batch-137 = 0.00024573446717113256

Training epoch-21 batch-138
Running loss of epoch-21 batch-138 = 0.0002730271080508828

Training epoch-21 batch-139
Running loss of epoch-21 batch-139 = 0.00019276619423180819

Training epoch-21 batch-140
Running loss of epoch-21 batch-140 = 0.0001756143756210804

Training epoch-21 batch-141
Running loss of epoch-21 batch-141 = 0.0005520264385268092

Training epoch-21 batch-142
Running loss of epoch-21 batch-142 = 0.00014434882905334234

Training epoch-21 batch-143
Running loss of epoch-21 batch-143 = 0.00013296783436089754

Training epoch-21 batch-144
Running loss of epoch-21 batch-144 = 0.0003495876444503665

Training epoch-21 batch-145
Running loss of epoch-21 batch-145 = 0.000365577288903296

Training epoch-21 batch-146
Running loss of epoch-21 batch-146 = 0.0006896416889503598

Training epoch-21 batch-147
Running loss of epoch-21 batch-147 = 0.00017496803775429726

Training epoch-21 batch-148
Running loss of epoch-21 batch-148 = 0.00039895798545330763

Training epoch-21 batch-149
Running loss of epoch-21 batch-149 = 0.00017597491387277842

Training epoch-21 batch-150
Running loss of epoch-21 batch-150 = 0.0002245818031951785

Training epoch-21 batch-151
Running loss of epoch-21 batch-151 = 0.0013165305135771632

Training epoch-21 batch-152
Running loss of epoch-21 batch-152 = 0.00023479817900806665

Training epoch-21 batch-153
Running loss of epoch-21 batch-153 = 0.00040101539343595505

Training epoch-21 batch-154
Running loss of epoch-21 batch-154 = 0.0007321786833927035

Training epoch-21 batch-155
Running loss of epoch-21 batch-155 = 0.0002974716480821371

Training epoch-21 batch-156
Running loss of epoch-21 batch-156 = 0.00014217058196663857

Training epoch-21 batch-157
Running loss of epoch-21 batch-157 = 0.0001923777163028717

Finished training epoch-21.



Average train loss at epoch-21 = 0.000355746603012085

Started Evaluation

Average val loss at epoch-21 = 0.7903105984159514

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 93.28 %
Accuracy for class onCreate is: 89.98 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 71.92 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.69 %
Accuracy for class execute is: 40.96 %
Accuracy for class get is: 76.67 %

Overall Accuracy = 83.74 %

Finished Evaluation



Started training epoch-22


Training epoch-22 batch-1
Running loss of epoch-22 batch-1 = 0.0005036127986386418

Training epoch-22 batch-2
Running loss of epoch-22 batch-2 = 0.0009298085933551192

Training epoch-22 batch-3
Running loss of epoch-22 batch-3 = 0.0003253202885389328

Training epoch-22 batch-4
Running loss of epoch-22 batch-4 = 0.00019153975881636143

Training epoch-22 batch-5
Running loss of epoch-22 batch-5 = 0.00023469817824661732

Training epoch-22 batch-6
Running loss of epoch-22 batch-6 = 8.558598347008228e-05

Training epoch-22 batch-7
Running loss of epoch-22 batch-7 = 0.00023669109214097261

Training epoch-22 batch-8
Running loss of epoch-22 batch-8 = 0.00018163758795708418

Training epoch-22 batch-9
Running loss of epoch-22 batch-9 = 0.00018954533152282238

Training epoch-22 batch-10
Running loss of epoch-22 batch-10 = 0.0003814286319538951

Training epoch-22 batch-11
Running loss of epoch-22 batch-11 = 0.00031755270902067423

Training epoch-22 batch-12
Running loss of epoch-22 batch-12 = 0.00033976277336478233

Training epoch-22 batch-13
Running loss of epoch-22 batch-13 = 5.999975837767124e-05

Training epoch-22 batch-14
Running loss of epoch-22 batch-14 = 7.86780146881938e-05

Training epoch-22 batch-15
Running loss of epoch-22 batch-15 = 0.0002655761782079935

Training epoch-22 batch-16
Running loss of epoch-22 batch-16 = 0.001005128724500537

Training epoch-22 batch-17
Running loss of epoch-22 batch-17 = 0.00026394904125481844

Training epoch-22 batch-18
Running loss of epoch-22 batch-18 = 0.00012805999722331762

Training epoch-22 batch-19
Running loss of epoch-22 batch-19 = 0.0003706782590597868

Training epoch-22 batch-20
Running loss of epoch-22 batch-20 = 0.00011321669444441795

Training epoch-22 batch-21
Running loss of epoch-22 batch-21 = 0.00021559791639447212

Training epoch-22 batch-22
Running loss of epoch-22 batch-22 = 0.0004557979991659522

Training epoch-22 batch-23
Running loss of epoch-22 batch-23 = 7.142266258597374e-05

Training epoch-22 batch-24
Running loss of epoch-22 batch-24 = 0.00033904239535331726

Training epoch-22 batch-25
Running loss of epoch-22 batch-25 = 0.0001582438126206398

Training epoch-22 batch-26
Running loss of epoch-22 batch-26 = 0.00027712725568562746

Training epoch-22 batch-27
Running loss of epoch-22 batch-27 = 0.00032111629843711853

Training epoch-22 batch-28
Running loss of epoch-22 batch-28 = 0.00013526238035410643

Training epoch-22 batch-29
Running loss of epoch-22 batch-29 = 0.00044045376125723124

Training epoch-22 batch-30
Running loss of epoch-22 batch-30 = 0.00019287585746496916

Training epoch-22 batch-31
Running loss of epoch-22 batch-31 = 0.00011053064372390509

Training epoch-22 batch-32
Running loss of epoch-22 batch-32 = 0.0005497807869687676

Training epoch-22 batch-33
Running loss of epoch-22 batch-33 = 0.0001641690032556653

Training epoch-22 batch-34
Running loss of epoch-22 batch-34 = 9.771494660526514e-05

Training epoch-22 batch-35
Running loss of epoch-22 batch-35 = 0.00016525015234947205

Training epoch-22 batch-36
Running loss of epoch-22 batch-36 = 0.00012782681733369827

Training epoch-22 batch-37
Running loss of epoch-22 batch-37 = 9.983847849071026e-05

Training epoch-22 batch-38
Running loss of epoch-22 batch-38 = 0.0003445282345637679

Training epoch-22 batch-39
Running loss of epoch-22 batch-39 = 0.00038640527054667473

Training epoch-22 batch-40
Running loss of epoch-22 batch-40 = 0.00011486920993775129

Training epoch-22 batch-41
Running loss of epoch-22 batch-41 = 8.087605237960815e-05

Training epoch-22 batch-42
Running loss of epoch-22 batch-42 = 5.663256160914898e-05

Training epoch-22 batch-43
Running loss of epoch-22 batch-43 = 0.0001727946801111102

Training epoch-22 batch-44
Running loss of epoch-22 batch-44 = 0.00013420742470771074

Training epoch-22 batch-45
Running loss of epoch-22 batch-45 = 0.00025765516329556704

Training epoch-22 batch-46
Running loss of epoch-22 batch-46 = 0.00021965359337627888

Training epoch-22 batch-47
Running loss of epoch-22 batch-47 = 0.00028935191221535206

Training epoch-22 batch-48
Running loss of epoch-22 batch-48 = 0.0001708773197606206

Training epoch-22 batch-49
Running loss of epoch-22 batch-49 = 0.00013741804286837578

Training epoch-22 batch-50
Running loss of epoch-22 batch-50 = 0.0005356814945116639

Training epoch-22 batch-51
Running loss of epoch-22 batch-51 = 0.0002590912627056241

Training epoch-22 batch-52
Running loss of epoch-22 batch-52 = 4.314095713198185e-05

Training epoch-22 batch-53
Running loss of epoch-22 batch-53 = 0.0008381318766623735

Training epoch-22 batch-54
Running loss of epoch-22 batch-54 = 8.970452472567558e-05

Training epoch-22 batch-55
Running loss of epoch-22 batch-55 = 0.00018377159722149372

Training epoch-22 batch-56
Running loss of epoch-22 batch-56 = 0.0001900610513985157

Training epoch-22 batch-57
Running loss of epoch-22 batch-57 = 0.0001477799378335476

Training epoch-22 batch-58
Running loss of epoch-22 batch-58 = 6.425450555980206e-05

Training epoch-22 batch-59
Running loss of epoch-22 batch-59 = 0.00013737683184444904

Training epoch-22 batch-60
Running loss of epoch-22 batch-60 = 0.00041818595491349697

Training epoch-22 batch-61
Running loss of epoch-22 batch-61 = 0.0002784993266686797

Training epoch-22 batch-62
Running loss of epoch-22 batch-62 = 8.620694279670715e-05

Training epoch-22 batch-63
Running loss of epoch-22 batch-63 = 0.00011284730862826109

Training epoch-22 batch-64
Running loss of epoch-22 batch-64 = 8.935481309890747e-05

Training epoch-22 batch-65
Running loss of epoch-22 batch-65 = 0.00014857924543321133

Training epoch-22 batch-66
Running loss of epoch-22 batch-66 = 0.00045961199793964624

Training epoch-22 batch-67
Running loss of epoch-22 batch-67 = 0.0003685600822791457

Training epoch-22 batch-68
Running loss of epoch-22 batch-68 = 0.00048480613622814417

Training epoch-22 batch-69
Running loss of epoch-22 batch-69 = 0.00032059825025498867

Training epoch-22 batch-70
Running loss of epoch-22 batch-70 = 0.00018090358935296535

Training epoch-22 batch-71
Running loss of epoch-22 batch-71 = 0.0005825322587043047

Training epoch-22 batch-72
Running loss of epoch-22 batch-72 = 0.00039101357106119394

Training epoch-22 batch-73
Running loss of epoch-22 batch-73 = 0.0002933358773589134

Training epoch-22 batch-74
Running loss of epoch-22 batch-74 = 0.0003003462916240096

Training epoch-22 batch-75
Running loss of epoch-22 batch-75 = 0.0002928974572569132

Training epoch-22 batch-76
Running loss of epoch-22 batch-76 = 0.0001863038633018732

Training epoch-22 batch-77
Running loss of epoch-22 batch-77 = 0.00010772154200822115

Training epoch-22 batch-78
Running loss of epoch-22 batch-78 = 0.00035852380096912384

Training epoch-22 batch-79
Running loss of epoch-22 batch-79 = 0.00017509458120912313

Training epoch-22 batch-80
Running loss of epoch-22 batch-80 = 0.00035133142955601215

Training epoch-22 batch-81
Running loss of epoch-22 batch-81 = 0.00043398397974669933

Training epoch-22 batch-82
Running loss of epoch-22 batch-82 = 6.399548146873713e-05

Training epoch-22 batch-83
Running loss of epoch-22 batch-83 = 0.00024272268638014793

Training epoch-22 batch-84
Running loss of epoch-22 batch-84 = 0.00015708012506365776

Training epoch-22 batch-85
Running loss of epoch-22 batch-85 = 0.00020834675524383783

Training epoch-22 batch-86
Running loss of epoch-22 batch-86 = 0.0001467966940253973

Training epoch-22 batch-87
Running loss of epoch-22 batch-87 = 0.00033680815249681473

Training epoch-22 batch-88
Running loss of epoch-22 batch-88 = 0.00016685482114553452

Training epoch-22 batch-89
Running loss of epoch-22 batch-89 = 0.0002387984422966838

Training epoch-22 batch-90
Running loss of epoch-22 batch-90 = 0.00033406796865165234

Training epoch-22 batch-91
Running loss of epoch-22 batch-91 = 0.00029522250406444073

Training epoch-22 batch-92
Running loss of epoch-22 batch-92 = 0.00021469267085194588

Training epoch-22 batch-93
Running loss of epoch-22 batch-93 = 0.0005997174885123968

Training epoch-22 batch-94
Running loss of epoch-22 batch-94 = 0.0006143525242805481

Training epoch-22 batch-95
Running loss of epoch-22 batch-95 = 9.62552148848772e-05

Training epoch-22 batch-96
Running loss of epoch-22 batch-96 = 0.00010465411469340324

Training epoch-22 batch-97
Running loss of epoch-22 batch-97 = 0.0005770714487880468

Training epoch-22 batch-98
Running loss of epoch-22 batch-98 = 0.0014713112032040954

Training epoch-22 batch-99
Running loss of epoch-22 batch-99 = 0.00023803720250725746

Training epoch-22 batch-100
Running loss of epoch-22 batch-100 = 0.0003409956116229296

Training epoch-22 batch-101
Running loss of epoch-22 batch-101 = 0.0002322332002222538

Training epoch-22 batch-102
Running loss of epoch-22 batch-102 = 0.00043217791244387627

Training epoch-22 batch-103
Running loss of epoch-22 batch-103 = 0.000252655241638422

Training epoch-22 batch-104
Running loss of epoch-22 batch-104 = 0.00020003155805170536

Training epoch-22 batch-105
Running loss of epoch-22 batch-105 = 0.00027409137692302465

Training epoch-22 batch-106
Running loss of epoch-22 batch-106 = 0.00021248171105980873

Training epoch-22 batch-107
Running loss of epoch-22 batch-107 = 8.774059824645519e-05

Training epoch-22 batch-108
Running loss of epoch-22 batch-108 = 0.00027356925420463085

Training epoch-22 batch-109
Running loss of epoch-22 batch-109 = 0.00020149874035269022

Training epoch-22 batch-110
Running loss of epoch-22 batch-110 = 0.0018815548392012715

Training epoch-22 batch-111
Running loss of epoch-22 batch-111 = 0.0004627502057701349

Training epoch-22 batch-112
Running loss of epoch-22 batch-112 = 0.00028990593273192644

Training epoch-22 batch-113
Running loss of epoch-22 batch-113 = 9.101140312850475e-05

Training epoch-22 batch-114
Running loss of epoch-22 batch-114 = 0.000262859626673162

Training epoch-22 batch-115
Running loss of epoch-22 batch-115 = 0.00016550743021070957

Training epoch-22 batch-116
Running loss of epoch-22 batch-116 = 0.0001994958147406578

Training epoch-22 batch-117
Running loss of epoch-22 batch-117 = 0.00045951560605317354

Training epoch-22 batch-118
Running loss of epoch-22 batch-118 = 0.0006025928305462003

Training epoch-22 batch-119
Running loss of epoch-22 batch-119 = 0.00016411510296165943

Training epoch-22 batch-120
Running loss of epoch-22 batch-120 = 0.000256099971011281

Training epoch-22 batch-121
Running loss of epoch-22 batch-121 = 0.00027216854505240917

Training epoch-22 batch-122
Running loss of epoch-22 batch-122 = 0.00038380047772079706

Training epoch-22 batch-123
Running loss of epoch-22 batch-123 = 0.0006948502268642187

Training epoch-22 batch-124
Running loss of epoch-22 batch-124 = 0.00048337667249143124

Training epoch-22 batch-125
Running loss of epoch-22 batch-125 = 0.00028513860888779163

Training epoch-22 batch-126
Running loss of epoch-22 batch-126 = 0.00028941361233592033

Training epoch-22 batch-127
Running loss of epoch-22 batch-127 = 0.00013592129107564688

Training epoch-22 batch-128
Running loss of epoch-22 batch-128 = 0.0001879432238638401

Training epoch-22 batch-129
Running loss of epoch-22 batch-129 = 9.932066313922405e-05

Training epoch-22 batch-130
Running loss of epoch-22 batch-130 = 0.00014414172619581223

Training epoch-22 batch-131
Running loss of epoch-22 batch-131 = 0.000323442742228508

Training epoch-22 batch-132
Running loss of epoch-22 batch-132 = 0.0005373649764806032

Training epoch-22 batch-133
Running loss of epoch-22 batch-133 = 0.0005557344993576407

Training epoch-22 batch-134
Running loss of epoch-22 batch-134 = 0.00018380023539066315

Training epoch-22 batch-135
Running loss of epoch-22 batch-135 = 0.00013964716345071793

Training epoch-22 batch-136
Running loss of epoch-22 batch-136 = 0.00017227418720722198

Training epoch-22 batch-137
Running loss of epoch-22 batch-137 = 0.0002813349710777402

Training epoch-22 batch-138
Running loss of epoch-22 batch-138 = 0.0008147993357852101

Training epoch-22 batch-139
Running loss of epoch-22 batch-139 = 0.0001045160461217165

Training epoch-22 batch-140
Running loss of epoch-22 batch-140 = 0.00011646677739918232

Training epoch-22 batch-141
Running loss of epoch-22 batch-141 = 0.00012594496365636587

Training epoch-22 batch-142
Running loss of epoch-22 batch-142 = 0.00045842956751585007

Training epoch-22 batch-143
Running loss of epoch-22 batch-143 = 0.00023759936448186636

Training epoch-22 batch-144
Running loss of epoch-22 batch-144 = 0.00013027223758399487

Training epoch-22 batch-145
Running loss of epoch-22 batch-145 = 0.00022018118761479855

Training epoch-22 batch-146
Running loss of epoch-22 batch-146 = 0.00022941839415580034

Training epoch-22 batch-147
Running loss of epoch-22 batch-147 = 8.231634274125099e-05

Training epoch-22 batch-148
Running loss of epoch-22 batch-148 = 0.0004322178428992629

Training epoch-22 batch-149
Running loss of epoch-22 batch-149 = 7.716927211731672e-05

Training epoch-22 batch-150
Running loss of epoch-22 batch-150 = 0.0007738998392596841

Training epoch-22 batch-151
Running loss of epoch-22 batch-151 = 0.00012300803791731596

Training epoch-22 batch-152
Running loss of epoch-22 batch-152 = 0.00027620478067547083

Training epoch-22 batch-153
Running loss of epoch-22 batch-153 = 0.00039536599069833755

Training epoch-22 batch-154
Running loss of epoch-22 batch-154 = 0.00029826711397618055

Training epoch-22 batch-155
Running loss of epoch-22 batch-155 = 0.000151375075802207

Training epoch-22 batch-156
Running loss of epoch-22 batch-156 = 7.849314715713263e-05

Training epoch-22 batch-157
Running loss of epoch-22 batch-157 = 0.0005046315491199493

Finished training epoch-22.



Average train loss at epoch-22 = 0.00028973740711808207

Started Evaluation

Average val loss at epoch-22 = 0.8477301575528958

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 92.95 %
Accuracy for class onCreate is: 92.64 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 60.73 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 50.22 %
Accuracy for class execute is: 63.45 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 83.06 %

Finished Evaluation



Started training epoch-23


Training epoch-23 batch-1
Running loss of epoch-23 batch-1 = 9.511003736406565e-05

Training epoch-23 batch-2
Running loss of epoch-23 batch-2 = 0.00020999845582991838

Training epoch-23 batch-3
Running loss of epoch-23 batch-3 = 0.0001967304851859808

Training epoch-23 batch-4
Running loss of epoch-23 batch-4 = 0.00010073278099298477

Training epoch-23 batch-5
Running loss of epoch-23 batch-5 = 7.068552076816559e-05

Training epoch-23 batch-6
Running loss of epoch-23 batch-6 = 0.0001700376160442829

Training epoch-23 batch-7
Running loss of epoch-23 batch-7 = 0.00021132477559149265

Training epoch-23 batch-8
Running loss of epoch-23 batch-8 = 0.0001459283521398902

Training epoch-23 batch-9
Running loss of epoch-23 batch-9 = 0.00028001307509839535

Training epoch-23 batch-10
Running loss of epoch-23 batch-10 = 0.00014612311497330666

Training epoch-23 batch-11
Running loss of epoch-23 batch-11 = 0.00016825355123728514

Training epoch-23 batch-12
Running loss of epoch-23 batch-12 = 0.00010140438098460436

Training epoch-23 batch-13
Running loss of epoch-23 batch-13 = 8.462590631097555e-05

Training epoch-23 batch-14
Running loss of epoch-23 batch-14 = 0.0001741829328238964

Training epoch-23 batch-15
Running loss of epoch-23 batch-15 = 0.00019732723012566566

Training epoch-23 batch-16
Running loss of epoch-23 batch-16 = 0.00012212339788675308

Training epoch-23 batch-17
Running loss of epoch-23 batch-17 = 0.0004983633989468217

Training epoch-23 batch-18
Running loss of epoch-23 batch-18 = 6.551248952746391e-05

Training epoch-23 batch-19
Running loss of epoch-23 batch-19 = 8.454069029539824e-05

Training epoch-23 batch-20
Running loss of epoch-23 batch-20 = 4.934542812407017e-05

Training epoch-23 batch-21
Running loss of epoch-23 batch-21 = 0.0007676569512113929

Training epoch-23 batch-22
Running loss of epoch-23 batch-22 = 0.0001488151028752327

Training epoch-23 batch-23
Running loss of epoch-23 batch-23 = 0.00024334294721484184

Training epoch-23 batch-24
Running loss of epoch-23 batch-24 = 0.00031974934972822666

Training epoch-23 batch-25
Running loss of epoch-23 batch-25 = 0.00012919167056679726

Training epoch-23 batch-26
Running loss of epoch-23 batch-26 = 0.00010864529758691788

Training epoch-23 batch-27
Running loss of epoch-23 batch-27 = 0.00012375717051327229

Training epoch-23 batch-28
Running loss of epoch-23 batch-28 = 9.435671381652355e-05

Training epoch-23 batch-29
Running loss of epoch-23 batch-29 = 7.930770516395569e-05

Training epoch-23 batch-30
Running loss of epoch-23 batch-30 = 0.00011986191384494305

Training epoch-23 batch-31
Running loss of epoch-23 batch-31 = 5.557446274906397e-05

Training epoch-23 batch-32
Running loss of epoch-23 batch-32 = 0.00015713588800281286

Training epoch-23 batch-33
Running loss of epoch-23 batch-33 = 0.00016106327529996634

Training epoch-23 batch-34
Running loss of epoch-23 batch-34 = 0.0001646522432565689

Training epoch-23 batch-35
Running loss of epoch-23 batch-35 = 8.678028825670481e-05

Training epoch-23 batch-36
Running loss of epoch-23 batch-36 = 0.00022708135657012463

Training epoch-23 batch-37
Running loss of epoch-23 batch-37 = 0.00011674338020384312

Training epoch-23 batch-38
Running loss of epoch-23 batch-38 = 0.00010906450916081667

Training epoch-23 batch-39
Running loss of epoch-23 batch-39 = 4.3664127588272095e-05

Training epoch-23 batch-40
Running loss of epoch-23 batch-40 = 0.0002527001779526472

Training epoch-23 batch-41
Running loss of epoch-23 batch-41 = 8.00692941993475e-05

Training epoch-23 batch-42
Running loss of epoch-23 batch-42 = 0.00014082062989473343

Training epoch-23 batch-43
Running loss of epoch-23 batch-43 = 0.00023597001563757658

Training epoch-23 batch-44
Running loss of epoch-23 batch-44 = 0.00012901879381388426

Training epoch-23 batch-45
Running loss of epoch-23 batch-45 = 0.00010304572060704231

Training epoch-23 batch-46
Running loss of epoch-23 batch-46 = 0.00014268152881413698

Training epoch-23 batch-47
Running loss of epoch-23 batch-47 = 0.00010816717986017466

Training epoch-23 batch-48
Running loss of epoch-23 batch-48 = 0.00010551232844591141

Training epoch-23 batch-49
Running loss of epoch-23 batch-49 = 0.00015783461276441813

Training epoch-23 batch-50
Running loss of epoch-23 batch-50 = 8.137128315865993e-05

Training epoch-23 batch-51
Running loss of epoch-23 batch-51 = 0.0001167074078693986

Training epoch-23 batch-52
Running loss of epoch-23 batch-52 = 9.24102496355772e-05

Training epoch-23 batch-53
Running loss of epoch-23 batch-53 = 7.983564864844084e-05

Training epoch-23 batch-54
Running loss of epoch-23 batch-54 = 0.0001271297223865986

Training epoch-23 batch-55
Running loss of epoch-23 batch-55 = 7.404910866171122e-05

Training epoch-23 batch-56
Running loss of epoch-23 batch-56 = 6.332423072308302e-05

Training epoch-23 batch-57
Running loss of epoch-23 batch-57 = 0.00020216242410242558

Training epoch-23 batch-58
Running loss of epoch-23 batch-58 = 5.205872002989054e-05

Training epoch-23 batch-59
Running loss of epoch-23 batch-59 = 7.536658085882664e-05

Training epoch-23 batch-60
Running loss of epoch-23 batch-60 = 5.838519427925348e-05

Training epoch-23 batch-61
Running loss of epoch-23 batch-61 = 5.75813464820385e-05

Training epoch-23 batch-62
Running loss of epoch-23 batch-62 = 0.00030842050909996033

Training epoch-23 batch-63
Running loss of epoch-23 batch-63 = 7.800944149494171e-05

Training epoch-23 batch-64
Running loss of epoch-23 batch-64 = 0.0002393672475591302

Training epoch-23 batch-65
Running loss of epoch-23 batch-65 = 7.37806549295783e-05

Training epoch-23 batch-66
Running loss of epoch-23 batch-66 = 0.00021098495926707983

Training epoch-23 batch-67
Running loss of epoch-23 batch-67 = 6.215262692421675e-05

Training epoch-23 batch-68
Running loss of epoch-23 batch-68 = 7.097073830664158e-05

Training epoch-23 batch-69
Running loss of epoch-23 batch-69 = 0.0001603979617357254

Training epoch-23 batch-70
Running loss of epoch-23 batch-70 = 7.997150532901287e-05

Training epoch-23 batch-71
Running loss of epoch-23 batch-71 = 0.00019959942437708378

Training epoch-23 batch-72
Running loss of epoch-23 batch-72 = 7.889349944889545e-05

Training epoch-23 batch-73
Running loss of epoch-23 batch-73 = 0.00017454521730542183

Training epoch-23 batch-74
Running loss of epoch-23 batch-74 = 7.194152567535639e-05

Training epoch-23 batch-75
Running loss of epoch-23 batch-75 = 0.0001240939600393176

Training epoch-23 batch-76
Running loss of epoch-23 batch-76 = 0.00017841369844973087

Training epoch-23 batch-77
Running loss of epoch-23 batch-77 = 7.145153358578682e-05

Training epoch-23 batch-78
Running loss of epoch-23 batch-78 = 0.0001224726438522339

Training epoch-23 batch-79
Running loss of epoch-23 batch-79 = 9.815674275159836e-05

Training epoch-23 batch-80
Running loss of epoch-23 batch-80 = 4.825100768357515e-05

Training epoch-23 batch-81
Running loss of epoch-23 batch-81 = 0.00011467467993497849

Training epoch-23 batch-82
Running loss of epoch-23 batch-82 = 0.00011142564471811056

Training epoch-23 batch-83
Running loss of epoch-23 batch-83 = 8.246651850640774e-05

Training epoch-23 batch-84
Running loss of epoch-23 batch-84 = 0.0009977711597457528

Training epoch-23 batch-85
Running loss of epoch-23 batch-85 = 0.0006763159763067961

Training epoch-23 batch-86
Running loss of epoch-23 batch-86 = 5.784863606095314e-05

Training epoch-23 batch-87
Running loss of epoch-23 batch-87 = 0.0002847685245797038

Training epoch-23 batch-88
Running loss of epoch-23 batch-88 = 0.0009703210089355707

Training epoch-23 batch-89
Running loss of epoch-23 batch-89 = 8.347129914909601e-05

Training epoch-23 batch-90
Running loss of epoch-23 batch-90 = 0.00016329449135810137

Training epoch-23 batch-91
Running loss of epoch-23 batch-91 = 0.00023820321075618267

Training epoch-23 batch-92
Running loss of epoch-23 batch-92 = 0.00010224699508398771

Training epoch-23 batch-93
Running loss of epoch-23 batch-93 = 7.951934821903706e-05

Training epoch-23 batch-94
Running loss of epoch-23 batch-94 = 6.332830525934696e-05

Training epoch-23 batch-95
Running loss of epoch-23 batch-95 = 0.0007207762682810426

Training epoch-23 batch-96
Running loss of epoch-23 batch-96 = 0.00014617294073104858

Training epoch-23 batch-97
Running loss of epoch-23 batch-97 = 0.0002533631632104516

Training epoch-23 batch-98
Running loss of epoch-23 batch-98 = 0.00024503725580871105

Training epoch-23 batch-99
Running loss of epoch-23 batch-99 = 9.993149433284998e-05

Training epoch-23 batch-100
Running loss of epoch-23 batch-100 = 0.0001438827021047473

Training epoch-23 batch-101
Running loss of epoch-23 batch-101 = 0.00019801477901637554

Training epoch-23 batch-102
Running loss of epoch-23 batch-102 = 0.0003680749796330929

Training epoch-23 batch-103
Running loss of epoch-23 batch-103 = 0.00012517021968960762

Training epoch-23 batch-104
Running loss of epoch-23 batch-104 = 0.0002958975965157151

Training epoch-23 batch-105
Running loss of epoch-23 batch-105 = 9.781727567315102e-05

Training epoch-23 batch-106
Running loss of epoch-23 batch-106 = 0.00019646354485303164

Training epoch-23 batch-107
Running loss of epoch-23 batch-107 = 0.00034892186522483826

Training epoch-23 batch-108
Running loss of epoch-23 batch-108 = 0.0007601251127198339

Training epoch-23 batch-109
Running loss of epoch-23 batch-109 = 5.523907020688057e-05

Training epoch-23 batch-110
Running loss of epoch-23 batch-110 = 0.000109461834654212

Training epoch-23 batch-111
Running loss of epoch-23 batch-111 = 0.00013974355533719063

Training epoch-23 batch-112
Running loss of epoch-23 batch-112 = 0.0002854971680790186

Training epoch-23 batch-113
Running loss of epoch-23 batch-113 = 7.54091888666153e-05

Training epoch-23 batch-114
Running loss of epoch-23 batch-114 = 0.00010155781637877226

Training epoch-23 batch-115
Running loss of epoch-23 batch-115 = 0.0005747387185692787

Training epoch-23 batch-116
Running loss of epoch-23 batch-116 = 0.00020502833649516106

Training epoch-23 batch-117
Running loss of epoch-23 batch-117 = 0.00015398417599499226

Training epoch-23 batch-118
Running loss of epoch-23 batch-118 = 0.0003525677602738142

Training epoch-23 batch-119
Running loss of epoch-23 batch-119 = 0.0007432636339217424

Training epoch-23 batch-120
Running loss of epoch-23 batch-120 = 0.00013824005145579576

Training epoch-23 batch-121
Running loss of epoch-23 batch-121 = 0.00011614547111093998

Training epoch-23 batch-122
Running loss of epoch-23 batch-122 = 0.00014210038352757692

Training epoch-23 batch-123
Running loss of epoch-23 batch-123 = 0.00023327593225985765

Training epoch-23 batch-124
Running loss of epoch-23 batch-124 = 0.00017232145182788372

Training epoch-23 batch-125
Running loss of epoch-23 batch-125 = 0.00028039992321282625

Training epoch-23 batch-126
Running loss of epoch-23 batch-126 = 9.955454152077436e-05

Training epoch-23 batch-127
Running loss of epoch-23 batch-127 = 0.00015342968981713057

Training epoch-23 batch-128
Running loss of epoch-23 batch-128 = 8.404406253248453e-05

Training epoch-23 batch-129
Running loss of epoch-23 batch-129 = 0.00013701757416129112

Training epoch-23 batch-130
Running loss of epoch-23 batch-130 = 0.0002072955248877406

Training epoch-23 batch-131
Running loss of epoch-23 batch-131 = 0.00010552036110311747

Training epoch-23 batch-132
Running loss of epoch-23 batch-132 = 9.312271140515804e-05

Training epoch-23 batch-133
Running loss of epoch-23 batch-133 = 0.00048248283565044403

Training epoch-23 batch-134
Running loss of epoch-23 batch-134 = 0.00018268777057528496

Training epoch-23 batch-135
Running loss of epoch-23 batch-135 = 0.0003045744961127639

Training epoch-23 batch-136
Running loss of epoch-23 batch-136 = 2.7731526643037796e-05

Training epoch-23 batch-137
Running loss of epoch-23 batch-137 = 0.00029150827322155237

Training epoch-23 batch-138
Running loss of epoch-23 batch-138 = 0.00014833046589046717

Training epoch-23 batch-139
Running loss of epoch-23 batch-139 = 0.000156820984557271

Training epoch-23 batch-140
Running loss of epoch-23 batch-140 = 0.0003407433396205306

Training epoch-23 batch-141
Running loss of epoch-23 batch-141 = 9.62837366387248e-05

Training epoch-23 batch-142
Running loss of epoch-23 batch-142 = 8.975365199148655e-05

Training epoch-23 batch-143
Running loss of epoch-23 batch-143 = 7.972505409270525e-05

Training epoch-23 batch-144
Running loss of epoch-23 batch-144 = 0.0002390323206782341

Training epoch-23 batch-145
Running loss of epoch-23 batch-145 = 8.874689228832722e-05

Training epoch-23 batch-146
Running loss of epoch-23 batch-146 = 8.43023881316185e-05

Training epoch-23 batch-147
Running loss of epoch-23 batch-147 = 0.00017181993462145329

Training epoch-23 batch-148
Running loss of epoch-23 batch-148 = 0.00018221233040094376

Training epoch-23 batch-149
Running loss of epoch-23 batch-149 = 0.00016443058848381042

Training epoch-23 batch-150
Running loss of epoch-23 batch-150 = 0.0001518151257187128

Training epoch-23 batch-151
Running loss of epoch-23 batch-151 = 5.175592377781868e-05

Training epoch-23 batch-152
Running loss of epoch-23 batch-152 = 6.993860006332397e-05

Training epoch-23 batch-153
Running loss of epoch-23 batch-153 = 0.0001229546032845974

Training epoch-23 batch-154
Running loss of epoch-23 batch-154 = 0.00024029973428696394

Training epoch-23 batch-155
Running loss of epoch-23 batch-155 = 9.643530938774347e-05

Training epoch-23 batch-156
Running loss of epoch-23 batch-156 = 0.0003556014271453023

Training epoch-23 batch-157
Running loss of epoch-23 batch-157 = 0.001420268788933754

Finished training epoch-23.



Average train loss at epoch-23 = 0.00018416770249605178

Started Evaluation

Average val loss at epoch-23 = 0.8057485498725567

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 97.38 %
Accuracy for class setUp is: 92.13 %
Accuracy for class onCreate is: 93.18 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 67.35 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.89 %

Finished Evaluation



Started training epoch-24


Training epoch-24 batch-1
Running loss of epoch-24 batch-1 = 0.00012627372052520514

Training epoch-24 batch-2
Running loss of epoch-24 batch-2 = 0.00010482280049473047

Training epoch-24 batch-3
Running loss of epoch-24 batch-3 = 0.0001234032679349184

Training epoch-24 batch-4
Running loss of epoch-24 batch-4 = 8.166604675352573e-05

Training epoch-24 batch-5
Running loss of epoch-24 batch-5 = 0.00011514360085129738

Training epoch-24 batch-6
Running loss of epoch-24 batch-6 = 0.0001332010142505169

Training epoch-24 batch-7
Running loss of epoch-24 batch-7 = 8.095265366137028e-05

Training epoch-24 batch-8
Running loss of epoch-24 batch-8 = 8.562032599002123e-05

Training epoch-24 batch-9
Running loss of epoch-24 batch-9 = 3.3501069992780685e-05

Training epoch-24 batch-10
Running loss of epoch-24 batch-10 = 0.00014141190331429243

Training epoch-24 batch-11
Running loss of epoch-24 batch-11 = 2.043158747255802e-05

Training epoch-24 batch-12
Running loss of epoch-24 batch-12 = 0.00017907319124788046

Training epoch-24 batch-13
Running loss of epoch-24 batch-13 = 3.4308061003685e-05

Training epoch-24 batch-14
Running loss of epoch-24 batch-14 = 6.963545456528664e-05

Training epoch-24 batch-15
Running loss of epoch-24 batch-15 = 7.014931179583073e-05

Training epoch-24 batch-16
Running loss of epoch-24 batch-16 = 4.5703956857323647e-05

Training epoch-24 batch-17
Running loss of epoch-24 batch-17 = 0.00010931328870356083

Training epoch-24 batch-18
Running loss of epoch-24 batch-18 = 0.00017592241056263447

Training epoch-24 batch-19
Running loss of epoch-24 batch-19 = 0.00010012928396463394

Training epoch-24 batch-20
Running loss of epoch-24 batch-20 = 8.658028673380613e-05

Training epoch-24 batch-21
Running loss of epoch-24 batch-21 = 7.558707147836685e-05

Training epoch-24 batch-22
Running loss of epoch-24 batch-22 = 5.2759889513254166e-05

Training epoch-24 batch-23
Running loss of epoch-24 batch-23 = 0.0001380500616505742

Training epoch-24 batch-24
Running loss of epoch-24 batch-24 = 5.7239318266510963e-05

Training epoch-24 batch-25
Running loss of epoch-24 batch-25 = 0.00010787544306367636

Training epoch-24 batch-26
Running loss of epoch-24 batch-26 = 0.00011231075040996075

Training epoch-24 batch-27
Running loss of epoch-24 batch-27 = 0.0001272023655474186

Training epoch-24 batch-28
Running loss of epoch-24 batch-28 = 9.822146967053413e-05

Training epoch-24 batch-29
Running loss of epoch-24 batch-29 = 3.473786637187004e-05

Training epoch-24 batch-30
Running loss of epoch-24 batch-30 = 0.00012672506272792816

Training epoch-24 batch-31
Running loss of epoch-24 batch-31 = 3.7693651393055916e-05

Training epoch-24 batch-32
Running loss of epoch-24 batch-32 = 6.913417018949986e-05

Training epoch-24 batch-33
Running loss of epoch-24 batch-33 = 6.153690628707409e-05

Training epoch-24 batch-34
Running loss of epoch-24 batch-34 = 7.79035035520792e-05

Training epoch-24 batch-35
Running loss of epoch-24 batch-35 = 8.456187788397074e-05

Training epoch-24 batch-36
Running loss of epoch-24 batch-36 = 6.479304283857346e-05

Training epoch-24 batch-37
Running loss of epoch-24 batch-37 = 0.00010526296682655811

Training epoch-24 batch-38
Running loss of epoch-24 batch-38 = 9.833660442382097e-05

Training epoch-24 batch-39
Running loss of epoch-24 batch-39 = 0.00021876790560781956

Training epoch-24 batch-40
Running loss of epoch-24 batch-40 = 9.30295791476965e-05

Training epoch-24 batch-41
Running loss of epoch-24 batch-41 = 6.087950896471739e-05

Training epoch-24 batch-42
Running loss of epoch-24 batch-42 = 6.355741061270237e-05

Training epoch-24 batch-43
Running loss of epoch-24 batch-43 = 0.00046681088861078024

Training epoch-24 batch-44
Running loss of epoch-24 batch-44 = 0.00010908604599535465

Training epoch-24 batch-45
Running loss of epoch-24 batch-45 = 7.248681504279375e-05

Training epoch-24 batch-46
Running loss of epoch-24 batch-46 = 0.00015119544696062803

Training epoch-24 batch-47
Running loss of epoch-24 batch-47 = 0.00017974956426769495

Training epoch-24 batch-48
Running loss of epoch-24 batch-48 = 0.0001565595157444477

Training epoch-24 batch-49
Running loss of epoch-24 batch-49 = 7.689697667956352e-05

Training epoch-24 batch-50
Running loss of epoch-24 batch-50 = 4.942272789776325e-05

Training epoch-24 batch-51
Running loss of epoch-24 batch-51 = 0.000154580338858068

Training epoch-24 batch-52
Running loss of epoch-24 batch-52 = 0.00010368484072387218

Training epoch-24 batch-53
Running loss of epoch-24 batch-53 = 0.00010763679165393114

Training epoch-24 batch-54
Running loss of epoch-24 batch-54 = 8.055637590587139e-05

Training epoch-24 batch-55
Running loss of epoch-24 batch-55 = 0.00014531216584146023

Training epoch-24 batch-56
Running loss of epoch-24 batch-56 = 0.00035849655978381634

Training epoch-24 batch-57
Running loss of epoch-24 batch-57 = 0.00013032322749495506

Training epoch-24 batch-58
Running loss of epoch-24 batch-58 = 9.611505083739758e-05

Training epoch-24 batch-59
Running loss of epoch-24 batch-59 = 0.00047772808466106653

Training epoch-24 batch-60
Running loss of epoch-24 batch-60 = 0.00010714784730225801

Training epoch-24 batch-61
Running loss of epoch-24 batch-61 = 0.00022546469699591398

Training epoch-24 batch-62
Running loss of epoch-24 batch-62 = 3.848015330731869e-05

Training epoch-24 batch-63
Running loss of epoch-24 batch-63 = 0.00014478352386504412

Training epoch-24 batch-64
Running loss of epoch-24 batch-64 = 8.185859769582748e-05

Training epoch-24 batch-65
Running loss of epoch-24 batch-65 = 0.00021208054386079311

Training epoch-24 batch-66
Running loss of epoch-24 batch-66 = 0.00027771980967372656

Training epoch-24 batch-67
Running loss of epoch-24 batch-67 = 0.00025258061941713095

Training epoch-24 batch-68
Running loss of epoch-24 batch-68 = 9.548582602292299e-05

Training epoch-24 batch-69
Running loss of epoch-24 batch-69 = 4.349183291196823e-05

Training epoch-24 batch-70
Running loss of epoch-24 batch-70 = 6.136298179626465e-05

Training epoch-24 batch-71
Running loss of epoch-24 batch-71 = 0.00015397346578538418

Training epoch-24 batch-72
Running loss of epoch-24 batch-72 = 4.1201477870345116e-05

Training epoch-24 batch-73
Running loss of epoch-24 batch-73 = 8.295767474919558e-05

Training epoch-24 batch-74
Running loss of epoch-24 batch-74 = 5.0926581025123596e-05

Training epoch-24 batch-75
Running loss of epoch-24 batch-75 = 0.00010029901750385761

Training epoch-24 batch-76
Running loss of epoch-24 batch-76 = 0.00012296659406274557

Training epoch-24 batch-77
Running loss of epoch-24 batch-77 = 9.074248373508453e-05

Training epoch-24 batch-78
Running loss of epoch-24 batch-78 = 6.020907312631607e-05

Training epoch-24 batch-79
Running loss of epoch-24 batch-79 = 0.00021574937272816896

Training epoch-24 batch-80
Running loss of epoch-24 batch-80 = 0.00014431634917855263

Training epoch-24 batch-81
Running loss of epoch-24 batch-81 = 0.00013640744145959616

Training epoch-24 batch-82
Running loss of epoch-24 batch-82 = 0.00013197027146816254

Training epoch-24 batch-83
Running loss of epoch-24 batch-83 = 0.00018793938215821981

Training epoch-24 batch-84
Running loss of epoch-24 batch-84 = 1.671374775469303e-05

Training epoch-24 batch-85
Running loss of epoch-24 batch-85 = 0.00012266437988728285

Training epoch-24 batch-86
Running loss of epoch-24 batch-86 = 5.344650708138943e-05

Training epoch-24 batch-87
Running loss of epoch-24 batch-87 = 6.290769670158625e-05

Training epoch-24 batch-88
Running loss of epoch-24 batch-88 = 4.248623736202717e-05

Training epoch-24 batch-89
Running loss of epoch-24 batch-89 = 7.245945744216442e-05

Training epoch-24 batch-90
Running loss of epoch-24 batch-90 = 6.9298199377954e-05

Training epoch-24 batch-91
Running loss of epoch-24 batch-91 = 7.152324542403221e-05

Training epoch-24 batch-92
Running loss of epoch-24 batch-92 = 0.0009522426407784224

Training epoch-24 batch-93
Running loss of epoch-24 batch-93 = 6.1189872212708e-05

Training epoch-24 batch-94
Running loss of epoch-24 batch-94 = 7.35048670321703e-05

Training epoch-24 batch-95
Running loss of epoch-24 batch-95 = 0.00014658458530902863

Training epoch-24 batch-96
Running loss of epoch-24 batch-96 = 0.00016389228403568268

Training epoch-24 batch-97
Running loss of epoch-24 batch-97 = 0.00012474763207137585

Training epoch-24 batch-98
Running loss of epoch-24 batch-98 = 0.00014313182327896357

Training epoch-24 batch-99
Running loss of epoch-24 batch-99 = 0.00015024153981357813

Training epoch-24 batch-100
Running loss of epoch-24 batch-100 = 0.00012392818462103605

Training epoch-24 batch-101
Running loss of epoch-24 batch-101 = 9.432877413928509e-05

Training epoch-24 batch-102
Running loss of epoch-24 batch-102 = 9.899446740746498e-05

Training epoch-24 batch-103
Running loss of epoch-24 batch-103 = 0.00010135001502931118

Training epoch-24 batch-104
Running loss of epoch-24 batch-104 = 0.00010861596092581749

Training epoch-24 batch-105
Running loss of epoch-24 batch-105 = 6.523437332361937e-05

Training epoch-24 batch-106
Running loss of epoch-24 batch-106 = 0.000414306647144258

Training epoch-24 batch-107
Running loss of epoch-24 batch-107 = 0.0007237226236611605

Training epoch-24 batch-108
Running loss of epoch-24 batch-108 = 0.00012424716260284185

Training epoch-24 batch-109
Running loss of epoch-24 batch-109 = 0.0001332234824076295

Training epoch-24 batch-110
Running loss of epoch-24 batch-110 = 5.718506872653961e-05

Training epoch-24 batch-111
Running loss of epoch-24 batch-111 = 3.14770732074976e-05

Training epoch-24 batch-112
Running loss of epoch-24 batch-112 = 7.262127473950386e-05

Training epoch-24 batch-113
Running loss of epoch-24 batch-113 = 8.75258119776845e-05

Training epoch-24 batch-114
Running loss of epoch-24 batch-114 = 8.993037045001984e-05

Training epoch-24 batch-115
Running loss of epoch-24 batch-115 = 9.327114094048738e-05

Training epoch-24 batch-116
Running loss of epoch-24 batch-116 = 0.0001554496120661497

Training epoch-24 batch-117
Running loss of epoch-24 batch-117 = 5.682255141437054e-05

Training epoch-24 batch-118
Running loss of epoch-24 batch-118 = 8.608249481767416e-05

Training epoch-24 batch-119
Running loss of epoch-24 batch-119 = 0.00010487576946616173

Training epoch-24 batch-120
Running loss of epoch-24 batch-120 = 0.00037884688936173916

Training epoch-24 batch-121
Running loss of epoch-24 batch-121 = 0.00012209592387080193

Training epoch-24 batch-122
Running loss of epoch-24 batch-122 = 6.27618283033371e-05

Training epoch-24 batch-123
Running loss of epoch-24 batch-123 = 9.851902723312378e-05

Training epoch-24 batch-124
Running loss of epoch-24 batch-124 = 7.637497037649155e-05

Training epoch-24 batch-125
Running loss of epoch-24 batch-125 = 0.0004413902061060071

Training epoch-24 batch-126
Running loss of epoch-24 batch-126 = 6.147928070276976e-05

Training epoch-24 batch-127
Running loss of epoch-24 batch-127 = 5.652196705341339e-05

Training epoch-24 batch-128
Running loss of epoch-24 batch-128 = 0.00015389418695122004

Training epoch-24 batch-129
Running loss of epoch-24 batch-129 = 0.00012871576473116875

Training epoch-24 batch-130
Running loss of epoch-24 batch-130 = 7.028295658528805e-05

Training epoch-24 batch-131
Running loss of epoch-24 batch-131 = 0.0002480759285390377

Training epoch-24 batch-132
Running loss of epoch-24 batch-132 = 9.000557474792004e-05

Training epoch-24 batch-133
Running loss of epoch-24 batch-133 = 3.788783214986324e-05

Training epoch-24 batch-134
Running loss of epoch-24 batch-134 = 9.523634798824787e-05

Training epoch-24 batch-135
Running loss of epoch-24 batch-135 = 0.0001249432098120451

Training epoch-24 batch-136
Running loss of epoch-24 batch-136 = 0.00017809320706874132

Training epoch-24 batch-137
Running loss of epoch-24 batch-137 = 5.8812787756323814e-05

Training epoch-24 batch-138
Running loss of epoch-24 batch-138 = 9.913742542266846e-05

Training epoch-24 batch-139
Running loss of epoch-24 batch-139 = 0.0001404059585183859

Training epoch-24 batch-140
Running loss of epoch-24 batch-140 = 4.175538197159767e-05

Training epoch-24 batch-141
Running loss of epoch-24 batch-141 = 8.240924216806889e-05

Training epoch-24 batch-142
Running loss of epoch-24 batch-142 = 5.34130958840251e-05

Training epoch-24 batch-143
Running loss of epoch-24 batch-143 = 0.0004958264762535691

Training epoch-24 batch-144
Running loss of epoch-24 batch-144 = 0.00030423467978835106

Training epoch-24 batch-145
Running loss of epoch-24 batch-145 = 6.534624844789505e-05

Training epoch-24 batch-146
Running loss of epoch-24 batch-146 = 9.489897638559341e-05

Training epoch-24 batch-147
Running loss of epoch-24 batch-147 = 6.992591079324484e-05

Training epoch-24 batch-148
Running loss of epoch-24 batch-148 = 6.452598609030247e-05

Training epoch-24 batch-149
Running loss of epoch-24 batch-149 = 0.00041152769699692726

Training epoch-24 batch-150
Running loss of epoch-24 batch-150 = 0.0001343744806945324

Training epoch-24 batch-151
Running loss of epoch-24 batch-151 = 0.0002474964130669832

Training epoch-24 batch-152
Running loss of epoch-24 batch-152 = 0.0001778934383764863

Training epoch-24 batch-153
Running loss of epoch-24 batch-153 = 0.0009643711382523179

Training epoch-24 batch-154
Running loss of epoch-24 batch-154 = 6.828410550951958e-05

Training epoch-24 batch-155
Running loss of epoch-24 batch-155 = 0.00010056956671178341

Training epoch-24 batch-156
Running loss of epoch-24 batch-156 = 7.871573325246572e-05

Training epoch-24 batch-157
Running loss of epoch-24 batch-157 = 0.00040071457624435425

Finished training epoch-24.



Average train loss at epoch-24 = 0.00013625358566641808

Started Evaluation

Average val loss at epoch-24 = 0.8758186185488835

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 91.97 %
Accuracy for class onCreate is: 91.90 %
Accuracy for class toString is: 86.35 %
Accuracy for class run is: 60.96 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.14 %
Accuracy for class execute is: 58.63 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.49 %

Finished Evaluation



Started training epoch-25


Training epoch-25 batch-1
Running loss of epoch-25 batch-1 = 0.0003582341596484184

Training epoch-25 batch-2
Running loss of epoch-25 batch-2 = 7.711723446846008e-05

Training epoch-25 batch-3
Running loss of epoch-25 batch-3 = 0.0002986376639455557

Training epoch-25 batch-4
Running loss of epoch-25 batch-4 = 0.00013366318307816982

Training epoch-25 batch-5
Running loss of epoch-25 batch-5 = 7.07810977473855e-05

Training epoch-25 batch-6
Running loss of epoch-25 batch-6 = 6.554578430950642e-05

Training epoch-25 batch-7
Running loss of epoch-25 batch-7 = 3.0159950256347656e-05

Training epoch-25 batch-8
Running loss of epoch-25 batch-8 = 6.641715299338102e-05

Training epoch-25 batch-9
Running loss of epoch-25 batch-9 = 0.00041267648339271545

Training epoch-25 batch-10
Running loss of epoch-25 batch-10 = 2.6365509256720543e-05

Training epoch-25 batch-11
Running loss of epoch-25 batch-11 = 0.00011929555330425501

Training epoch-25 batch-12
Running loss of epoch-25 batch-12 = 6.792740896344185e-05

Training epoch-25 batch-13
Running loss of epoch-25 batch-13 = 8.965481538325548e-05

Training epoch-25 batch-14
Running loss of epoch-25 batch-14 = 2.8961338102817535e-05

Training epoch-25 batch-15
Running loss of epoch-25 batch-15 = 0.0002242540940642357

Training epoch-25 batch-16
Running loss of epoch-25 batch-16 = 2.427259460091591e-05

Training epoch-25 batch-17
Running loss of epoch-25 batch-17 = 9.118067100644112e-05

Training epoch-25 batch-18
Running loss of epoch-25 batch-18 = 0.0005101162241771817

Training epoch-25 batch-19
Running loss of epoch-25 batch-19 = 0.00010769406799226999

Training epoch-25 batch-20
Running loss of epoch-25 batch-20 = 8.958193939179182e-05

Training epoch-25 batch-21
Running loss of epoch-25 batch-21 = 2.7613947167992592e-05

Training epoch-25 batch-22
Running loss of epoch-25 batch-22 = 8.255802094936371e-05

Training epoch-25 batch-23
Running loss of epoch-25 batch-23 = 3.387127071619034e-05

Training epoch-25 batch-24
Running loss of epoch-25 batch-24 = 9.153410792350769e-05

Training epoch-25 batch-25
Running loss of epoch-25 batch-25 = 0.00017463346011936665

Training epoch-25 batch-26
Running loss of epoch-25 batch-26 = 6.173946894705296e-05

Training epoch-25 batch-27
Running loss of epoch-25 batch-27 = 6.804417353123426e-05

Training epoch-25 batch-28
Running loss of epoch-25 batch-28 = 0.00013304268941283226

Training epoch-25 batch-29
Running loss of epoch-25 batch-29 = 0.00011763465590775013

Training epoch-25 batch-30
Running loss of epoch-25 batch-30 = 0.0001215183874592185

Training epoch-25 batch-31
Running loss of epoch-25 batch-31 = 8.384115062654018e-05

Training epoch-25 batch-32
Running loss of epoch-25 batch-32 = 0.00047334248665720224

Training epoch-25 batch-33
Running loss of epoch-25 batch-33 = 4.352745600044727e-05

Training epoch-25 batch-34
Running loss of epoch-25 batch-34 = 0.00015123689081519842

Training epoch-25 batch-35
Running loss of epoch-25 batch-35 = 5.7960394769907e-05

Training epoch-25 batch-36
Running loss of epoch-25 batch-36 = 6.04784581810236e-05

Training epoch-25 batch-37
Running loss of epoch-25 batch-37 = 0.00010066304821521044

Training epoch-25 batch-38
Running loss of epoch-25 batch-38 = 4.9693044275045395e-05

Training epoch-25 batch-39
Running loss of epoch-25 batch-39 = 0.00014839426148682833

Training epoch-25 batch-40
Running loss of epoch-25 batch-40 = 0.0002903647255152464

Training epoch-25 batch-41
Running loss of epoch-25 batch-41 = 0.00015279592480510473

Training epoch-25 batch-42
Running loss of epoch-25 batch-42 = 6.353156641125679e-05

Training epoch-25 batch-43
Running loss of epoch-25 batch-43 = 2.4815788492560387e-05

Training epoch-25 batch-44
Running loss of epoch-25 batch-44 = 0.00010561151430010796

Training epoch-25 batch-45
Running loss of epoch-25 batch-45 = 8.263532072305679e-05

Training epoch-25 batch-46
Running loss of epoch-25 batch-46 = 6.908481009304523e-05

Training epoch-25 batch-47
Running loss of epoch-25 batch-47 = 0.00012043025344610214

Training epoch-25 batch-48
Running loss of epoch-25 batch-48 = 6.000441499054432e-05

Training epoch-25 batch-49
Running loss of epoch-25 batch-49 = 4.258612170815468e-05

Training epoch-25 batch-50
Running loss of epoch-25 batch-50 = 0.00011947914026677608

Training epoch-25 batch-51
Running loss of epoch-25 batch-51 = 7.119518704712391e-05

Training epoch-25 batch-52
Running loss of epoch-25 batch-52 = 3.5680364817380905e-05

Training epoch-25 batch-53
Running loss of epoch-25 batch-53 = 4.8854853957891464e-05

Training epoch-25 batch-54
Running loss of epoch-25 batch-54 = 9.661389049142599e-05

Training epoch-25 batch-55
Running loss of epoch-25 batch-55 = 8.302507922053337e-05

Training epoch-25 batch-56
Running loss of epoch-25 batch-56 = 4.649418406188488e-05

Training epoch-25 batch-57
Running loss of epoch-25 batch-57 = 0.00023250607773661613

Training epoch-25 batch-58
Running loss of epoch-25 batch-58 = 0.00010610884055495262

Training epoch-25 batch-59
Running loss of epoch-25 batch-59 = 9.679829236119986e-05

Training epoch-25 batch-60
Running loss of epoch-25 batch-60 = 6.5237982198596e-05

Training epoch-25 batch-61
Running loss of epoch-25 batch-61 = 6.239628419280052e-05

Training epoch-25 batch-62
Running loss of epoch-25 batch-62 = 3.455323167145252e-05

Training epoch-25 batch-63
Running loss of epoch-25 batch-63 = 5.2330782637000084e-05

Training epoch-25 batch-64
Running loss of epoch-25 batch-64 = 7.82795250415802e-05

Training epoch-25 batch-65
Running loss of epoch-25 batch-65 = 9.51690599322319e-05

Training epoch-25 batch-66
Running loss of epoch-25 batch-66 = 7.823738269507885e-05

Training epoch-25 batch-67
Running loss of epoch-25 batch-67 = 7.419218309223652e-05

Training epoch-25 batch-68
Running loss of epoch-25 batch-68 = 0.00020336394663900137

Training epoch-25 batch-69
Running loss of epoch-25 batch-69 = 8.457223884761333e-05

Training epoch-25 batch-70
Running loss of epoch-25 batch-70 = 5.0416914746165276e-05

Training epoch-25 batch-71
Running loss of epoch-25 batch-71 = 5.339994095265865e-05

Training epoch-25 batch-72
Running loss of epoch-25 batch-72 = 0.0001295639667659998

Training epoch-25 batch-73
Running loss of epoch-25 batch-73 = 8.719251491129398e-05

Training epoch-25 batch-74
Running loss of epoch-25 batch-74 = 7.852097041904926e-05

Training epoch-25 batch-75
Running loss of epoch-25 batch-75 = 2.887798473238945e-05

Training epoch-25 batch-76
Running loss of epoch-25 batch-76 = 0.00010112649761140347

Training epoch-25 batch-77
Running loss of epoch-25 batch-77 = 7.38631933927536e-05

Training epoch-25 batch-78
Running loss of epoch-25 batch-78 = 9.6686533652246e-05

Training epoch-25 batch-79
Running loss of epoch-25 batch-79 = 5.7628960348665714e-05

Training epoch-25 batch-80
Running loss of epoch-25 batch-80 = 6.875908002257347e-05

Training epoch-25 batch-81
Running loss of epoch-25 batch-81 = 2.966984175145626e-05

Training epoch-25 batch-82
Running loss of epoch-25 batch-82 = 0.0005885132122784853

Training epoch-25 batch-83
Running loss of epoch-25 batch-83 = 5.881441757082939e-05

Training epoch-25 batch-84
Running loss of epoch-25 batch-84 = 7.382221519947052e-05

Training epoch-25 batch-85
Running loss of epoch-25 batch-85 = 0.0001010700361803174

Training epoch-25 batch-86
Running loss of epoch-25 batch-86 = 0.00018095155246555805

Training epoch-25 batch-87
Running loss of epoch-25 batch-87 = 0.00012193736620247364

Training epoch-25 batch-88
Running loss of epoch-25 batch-88 = 4.0684593841433525e-05

Training epoch-25 batch-89
Running loss of epoch-25 batch-89 = 0.00029520411044359207

Training epoch-25 batch-90
Running loss of epoch-25 batch-90 = 9.522144682705402e-05

Training epoch-25 batch-91
Running loss of epoch-25 batch-91 = 0.00023272493854165077

Training epoch-25 batch-92
Running loss of epoch-25 batch-92 = 8.331576827913523e-05

Training epoch-25 batch-93
Running loss of epoch-25 batch-93 = 2.7883215807378292e-05

Training epoch-25 batch-94
Running loss of epoch-25 batch-94 = 7.389020174741745e-05

Training epoch-25 batch-95
Running loss of epoch-25 batch-95 = 0.00015420326963067055

Training epoch-25 batch-96
Running loss of epoch-25 batch-96 = 7.383804768323898e-05

Training epoch-25 batch-97
Running loss of epoch-25 batch-97 = 0.00017911603208631277

Training epoch-25 batch-98
Running loss of epoch-25 batch-98 = 0.00017733569256961346

Training epoch-25 batch-99
Running loss of epoch-25 batch-99 = 0.00036114035174250603

Training epoch-25 batch-100
Running loss of epoch-25 batch-100 = 4.5815249904990196e-05

Training epoch-25 batch-101
Running loss of epoch-25 batch-101 = 2.5926623493433e-05

Training epoch-25 batch-102
Running loss of epoch-25 batch-102 = 2.549402415752411e-05

Training epoch-25 batch-103
Running loss of epoch-25 batch-103 = 3.199954517185688e-05

Training epoch-25 batch-104
Running loss of epoch-25 batch-104 = 5.20758330821991e-05

Training epoch-25 batch-105
Running loss of epoch-25 batch-105 = 7.50967301428318e-05

Training epoch-25 batch-106
Running loss of epoch-25 batch-106 = 9.776209481060505e-05

Training epoch-25 batch-107
Running loss of epoch-25 batch-107 = 0.0001146125141531229

Training epoch-25 batch-108
Running loss of epoch-25 batch-108 = 3.085983917117119e-05

Training epoch-25 batch-109
Running loss of epoch-25 batch-109 = 0.00012965244241058826

Training epoch-25 batch-110
Running loss of epoch-25 batch-110 = 0.00013963272795081139

Training epoch-25 batch-111
Running loss of epoch-25 batch-111 = 9.277812205255032e-05

Training epoch-25 batch-112
Running loss of epoch-25 batch-112 = 0.0001312353415414691

Training epoch-25 batch-113
Running loss of epoch-25 batch-113 = 9.055901318788528e-05

Training epoch-25 batch-114
Running loss of epoch-25 batch-114 = 0.00013791630044579506

Training epoch-25 batch-115
Running loss of epoch-25 batch-115 = 8.825655095279217e-05

Training epoch-25 batch-116
Running loss of epoch-25 batch-116 = 4.636950325220823e-05

Training epoch-25 batch-117
Running loss of epoch-25 batch-117 = 0.00013774156104773283

Training epoch-25 batch-118
Running loss of epoch-25 batch-118 = 8.78508435562253e-05

Training epoch-25 batch-119
Running loss of epoch-25 batch-119 = 5.9952493757009506e-05

Training epoch-25 batch-120
Running loss of epoch-25 batch-120 = 0.0001031520077958703

Training epoch-25 batch-121
Running loss of epoch-25 batch-121 = 0.00023632729426026344

Training epoch-25 batch-122
Running loss of epoch-25 batch-122 = 6.87082065269351e-05

Training epoch-25 batch-123
Running loss of epoch-25 batch-123 = 6.330362521111965e-05

Training epoch-25 batch-124
Running loss of epoch-25 batch-124 = 9.42449551075697e-05

Training epoch-25 batch-125
Running loss of epoch-25 batch-125 = 6.874825339764357e-05

Training epoch-25 batch-126
Running loss of epoch-25 batch-126 = 0.0002599484287202358

Training epoch-25 batch-127
Running loss of epoch-25 batch-127 = 0.0001096783671528101

Training epoch-25 batch-128
Running loss of epoch-25 batch-128 = 0.00010921689681708813

Training epoch-25 batch-129
Running loss of epoch-25 batch-129 = 0.0005790675058960915

Training epoch-25 batch-130
Running loss of epoch-25 batch-130 = 9.38244629651308e-05

Training epoch-25 batch-131
Running loss of epoch-25 batch-131 = 7.947534322738647e-05

Training epoch-25 batch-132
Running loss of epoch-25 batch-132 = 0.0002569236094132066

Training epoch-25 batch-133
Running loss of epoch-25 batch-133 = 0.00014473928604274988

Training epoch-25 batch-134
Running loss of epoch-25 batch-134 = 9.096739813685417e-05

Training epoch-25 batch-135
Running loss of epoch-25 batch-135 = 0.0001635565422475338

Training epoch-25 batch-136
Running loss of epoch-25 batch-136 = 0.00020394590683281422

Training epoch-25 batch-137
Running loss of epoch-25 batch-137 = 0.00011218525469303131

Training epoch-25 batch-138
Running loss of epoch-25 batch-138 = 5.883525591343641e-05

Training epoch-25 batch-139
Running loss of epoch-25 batch-139 = 5.473126657307148e-05

Training epoch-25 batch-140
Running loss of epoch-25 batch-140 = 7.708673365414143e-05

Training epoch-25 batch-141
Running loss of epoch-25 batch-141 = 7.384712807834148e-05

Training epoch-25 batch-142
Running loss of epoch-25 batch-142 = 4.4521642848849297e-05

Training epoch-25 batch-143
Running loss of epoch-25 batch-143 = 3.924313932657242e-05

Training epoch-25 batch-144
Running loss of epoch-25 batch-144 = 0.00017621507868170738

Training epoch-25 batch-145
Running loss of epoch-25 batch-145 = 6.381259299814701e-05

Training epoch-25 batch-146
Running loss of epoch-25 batch-146 = 0.00010334351100027561

Training epoch-25 batch-147
Running loss of epoch-25 batch-147 = 6.460072472691536e-05

Training epoch-25 batch-148
Running loss of epoch-25 batch-148 = 3.6695972084999084e-05

Training epoch-25 batch-149
Running loss of epoch-25 batch-149 = 3.867899067699909e-05

Training epoch-25 batch-150
Running loss of epoch-25 batch-150 = 0.00020825467072427273

Training epoch-25 batch-151
Running loss of epoch-25 batch-151 = 0.00011892861220985651

Training epoch-25 batch-152
Running loss of epoch-25 batch-152 = 9.184482041746378e-05

Training epoch-25 batch-153
Running loss of epoch-25 batch-153 = 6.315810605883598e-05

Training epoch-25 batch-154
Running loss of epoch-25 batch-154 = 7.887044921517372e-05

Training epoch-25 batch-155
Running loss of epoch-25 batch-155 = 0.00017353147268295288

Training epoch-25 batch-156
Running loss of epoch-25 batch-156 = 8.709682151675224e-05

Training epoch-25 batch-157
Running loss of epoch-25 batch-157 = 0.00010352954268455505

Finished training epoch-25.



Average train loss at epoch-25 = 0.00011318378373980522

Started Evaluation

Average val loss at epoch-25 = 0.8999691926313206

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 90.16 %
Accuracy for class onCreate is: 90.51 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 57.17 %
Accuracy for class execute is: 55.42 %
Accuracy for class get is: 73.08 %

Overall Accuracy = 83.39 %

Finished Evaluation



Started training epoch-26


Training epoch-26 batch-1
Running loss of epoch-26 batch-1 = 9.986408986151218e-05

Training epoch-26 batch-2
Running loss of epoch-26 batch-2 = 7.15733040124178e-05

Training epoch-26 batch-3
Running loss of epoch-26 batch-3 = 6.911077070981264e-05

Training epoch-26 batch-4
Running loss of epoch-26 batch-4 = 8.434359915554523e-05

Training epoch-26 batch-5
Running loss of epoch-26 batch-5 = 0.00011017383076250553

Training epoch-26 batch-6
Running loss of epoch-26 batch-6 = 6.77292700856924e-05

Training epoch-26 batch-7
Running loss of epoch-26 batch-7 = 7.534236647188663e-05

Training epoch-26 batch-8
Running loss of epoch-26 batch-8 = 6.1823520809412e-05

Training epoch-26 batch-9
Running loss of epoch-26 batch-9 = 9.778537787497044e-05

Training epoch-26 batch-10
Running loss of epoch-26 batch-10 = 1.9317958503961563e-05

Training epoch-26 batch-11
Running loss of epoch-26 batch-11 = 3.180559724569321e-05

Training epoch-26 batch-12
Running loss of epoch-26 batch-12 = 4.5012217015028e-05

Training epoch-26 batch-13
Running loss of epoch-26 batch-13 = 6.430805660784245e-05

Training epoch-26 batch-14
Running loss of epoch-26 batch-14 = 6.196950562298298e-05

Training epoch-26 batch-15
Running loss of epoch-26 batch-15 = 6.632425356656313e-05

Training epoch-26 batch-16
Running loss of epoch-26 batch-16 = 5.8430247008800507e-05

Training epoch-26 batch-17
Running loss of epoch-26 batch-17 = 2.7520698495209217e-05

Training epoch-26 batch-18
Running loss of epoch-26 batch-18 = 8.129538036882877e-05

Training epoch-26 batch-19
Running loss of epoch-26 batch-19 = 4.666880704462528e-05

Training epoch-26 batch-20
Running loss of epoch-26 batch-20 = 8.472171612083912e-05

Training epoch-26 batch-21
Running loss of epoch-26 batch-21 = 4.6487199142575264e-05

Training epoch-26 batch-22
Running loss of epoch-26 batch-22 = 6.95177586749196e-05

Training epoch-26 batch-23
Running loss of epoch-26 batch-23 = 4.0360610000789165e-05

Training epoch-26 batch-24
Running loss of epoch-26 batch-24 = 0.00010882178321480751

Training epoch-26 batch-25
Running loss of epoch-26 batch-25 = 9.19088488444686e-05

Training epoch-26 batch-26
Running loss of epoch-26 batch-26 = 4.4825486838817596e-05

Training epoch-26 batch-27
Running loss of epoch-26 batch-27 = 9.044818580150604e-05

Training epoch-26 batch-28
Running loss of epoch-26 batch-28 = 5.863094702363014e-05

Training epoch-26 batch-29
Running loss of epoch-26 batch-29 = 7.350975647568703e-05

Training epoch-26 batch-30
Running loss of epoch-26 batch-30 = 7.942342199385166e-05

Training epoch-26 batch-31
Running loss of epoch-26 batch-31 = 5.096173845231533e-05

Training epoch-26 batch-32
Running loss of epoch-26 batch-32 = 0.00013912166468799114

Training epoch-26 batch-33
Running loss of epoch-26 batch-33 = 4.752050153911114e-05

Training epoch-26 batch-34
Running loss of epoch-26 batch-34 = 4.058796912431717e-05

Training epoch-26 batch-35
Running loss of epoch-26 batch-35 = 0.00010217982344329357

Training epoch-26 batch-36
Running loss of epoch-26 batch-36 = 5.6217191740870476e-05

Training epoch-26 batch-37
Running loss of epoch-26 batch-37 = 5.8735255151987076e-05

Training epoch-26 batch-38
Running loss of epoch-26 batch-38 = 5.700974725186825e-05

Training epoch-26 batch-39
Running loss of epoch-26 batch-39 = 4.991923924535513e-05

Training epoch-26 batch-40
Running loss of epoch-26 batch-40 = 0.00011181097943335772

Training epoch-26 batch-41
Running loss of epoch-26 batch-41 = 0.00012634822633117437

Training epoch-26 batch-42
Running loss of epoch-26 batch-42 = 6.684614345431328e-05

Training epoch-26 batch-43
Running loss of epoch-26 batch-43 = 3.820122219622135e-05

Training epoch-26 batch-44
Running loss of epoch-26 batch-44 = 8.079037070274353e-05

Training epoch-26 batch-45
Running loss of epoch-26 batch-45 = 5.4729171097278595e-05

Training epoch-26 batch-46
Running loss of epoch-26 batch-46 = 5.5369921028614044e-05

Training epoch-26 batch-47
Running loss of epoch-26 batch-47 = 4.60140872746706e-05

Training epoch-26 batch-48
Running loss of epoch-26 batch-48 = 4.7000590711832047e-05

Training epoch-26 batch-49
Running loss of epoch-26 batch-49 = 5.7495664805173874e-05

Training epoch-26 batch-50
Running loss of epoch-26 batch-50 = 7.819919846951962e-05

Training epoch-26 batch-51
Running loss of epoch-26 batch-51 = 3.80899291485548e-05

Training epoch-26 batch-52
Running loss of epoch-26 batch-52 = 8.058291859924793e-05

Training epoch-26 batch-53
Running loss of epoch-26 batch-53 = 8.876202628016472e-05

Training epoch-26 batch-54
Running loss of epoch-26 batch-54 = 3.93774826079607e-05

Training epoch-26 batch-55
Running loss of epoch-26 batch-55 = 5.2641844376921654e-05

Training epoch-26 batch-56
Running loss of epoch-26 batch-56 = 0.00016753620002418756

Training epoch-26 batch-57
Running loss of epoch-26 batch-57 = 8.01656860858202e-05

Training epoch-26 batch-58
Running loss of epoch-26 batch-58 = 8.763361256569624e-05

Training epoch-26 batch-59
Running loss of epoch-26 batch-59 = 0.0001137403305619955

Training epoch-26 batch-60
Running loss of epoch-26 batch-60 = 5.482579581439495e-05

Training epoch-26 batch-61
Running loss of epoch-26 batch-61 = 3.7256861105561256e-05

Training epoch-26 batch-62
Running loss of epoch-26 batch-62 = 0.00014274113345891237

Training epoch-26 batch-63
Running loss of epoch-26 batch-63 = 7.404200732707977e-05

Training epoch-26 batch-64
Running loss of epoch-26 batch-64 = 9.593507274985313e-05

Training epoch-26 batch-65
Running loss of epoch-26 batch-65 = 4.325178451836109e-05

Training epoch-26 batch-66
Running loss of epoch-26 batch-66 = 4.21751756221056e-05

Training epoch-26 batch-67
Running loss of epoch-26 batch-67 = 6.178510375320911e-05

Training epoch-26 batch-68
Running loss of epoch-26 batch-68 = 3.449129872024059e-05

Training epoch-26 batch-69
Running loss of epoch-26 batch-69 = 0.00010722130537033081

Training epoch-26 batch-70
Running loss of epoch-26 batch-70 = 9.371095802634954e-05

Training epoch-26 batch-71
Running loss of epoch-26 batch-71 = 5.758553743362427e-05

Training epoch-26 batch-72
Running loss of epoch-26 batch-72 = 6.722891703248024e-05

Training epoch-26 batch-73
Running loss of epoch-26 batch-73 = 2.3916363716125488e-05

Training epoch-26 batch-74
Running loss of epoch-26 batch-74 = 8.484488353133202e-05

Training epoch-26 batch-75
Running loss of epoch-26 batch-75 = 0.0001318188151344657

Training epoch-26 batch-76
Running loss of epoch-26 batch-76 = 4.7297216951847076e-05

Training epoch-26 batch-77
Running loss of epoch-26 batch-77 = 3.747781738638878e-05

Training epoch-26 batch-78
Running loss of epoch-26 batch-78 = 0.0004660466220229864

Training epoch-26 batch-79
Running loss of epoch-26 batch-79 = 9.205262176692486e-05

Training epoch-26 batch-80
Running loss of epoch-26 batch-80 = 5.727703683078289e-05

Training epoch-26 batch-81
Running loss of epoch-26 batch-81 = 2.5644199922680855e-05

Training epoch-26 batch-82
Running loss of epoch-26 batch-82 = 6.620213389396667e-05

Training epoch-26 batch-83
Running loss of epoch-26 batch-83 = 0.00011108093895018101

Training epoch-26 batch-84
Running loss of epoch-26 batch-84 = 8.666724897921085e-05

Training epoch-26 batch-85
Running loss of epoch-26 batch-85 = 4.0407292544841766e-05

Training epoch-26 batch-86
Running loss of epoch-26 batch-86 = 5.040084943175316e-05

Training epoch-26 batch-87
Running loss of epoch-26 batch-87 = 0.00010063080117106438

Training epoch-26 batch-88
Running loss of epoch-26 batch-88 = 7.389974780380726e-05

Training epoch-26 batch-89
Running loss of epoch-26 batch-89 = 0.00011100864503532648

Training epoch-26 batch-90
Running loss of epoch-26 batch-90 = 6.114691495895386e-05

Training epoch-26 batch-91
Running loss of epoch-26 batch-91 = 6.272201426327229e-05

Training epoch-26 batch-92
Running loss of epoch-26 batch-92 = 5.055195651948452e-05

Training epoch-26 batch-93
Running loss of epoch-26 batch-93 = 0.00011180131696164608

Training epoch-26 batch-94
Running loss of epoch-26 batch-94 = 0.00015769782476127148

Training epoch-26 batch-95
Running loss of epoch-26 batch-95 = 3.479444421827793e-05

Training epoch-26 batch-96
Running loss of epoch-26 batch-96 = 4.59328293800354e-05

Training epoch-26 batch-97
Running loss of epoch-26 batch-97 = 4.833843559026718e-05

Training epoch-26 batch-98
Running loss of epoch-26 batch-98 = 3.397488035261631e-05

Training epoch-26 batch-99
Running loss of epoch-26 batch-99 = 7.404014468193054e-05

Training epoch-26 batch-100
Running loss of epoch-26 batch-100 = 9.742379188537598e-05

Training epoch-26 batch-101
Running loss of epoch-26 batch-101 = 4.113721661269665e-05

Training epoch-26 batch-102
Running loss of epoch-26 batch-102 = 7.504178211092949e-05

Training epoch-26 batch-103
Running loss of epoch-26 batch-103 = 4.345877096056938e-05

Training epoch-26 batch-104
Running loss of epoch-26 batch-104 = 0.0005389023572206497

Training epoch-26 batch-105
Running loss of epoch-26 batch-105 = 4.842318594455719e-05

Training epoch-26 batch-106
Running loss of epoch-26 batch-106 = 2.5456072762608528e-05

Training epoch-26 batch-107
Running loss of epoch-26 batch-107 = 4.996219649910927e-05

Training epoch-26 batch-108
Running loss of epoch-26 batch-108 = 7.702852599322796e-05

Training epoch-26 batch-109
Running loss of epoch-26 batch-109 = 5.7755038142204285e-05

Training epoch-26 batch-110
Running loss of epoch-26 batch-110 = 0.00014372763689607382

Training epoch-26 batch-111
Running loss of epoch-26 batch-111 = 0.00013365712948143482

Training epoch-26 batch-112
Running loss of epoch-26 batch-112 = 0.0003229421563446522

Training epoch-26 batch-113
Running loss of epoch-26 batch-113 = 0.00010024616494774818

Training epoch-26 batch-114
Running loss of epoch-26 batch-114 = 5.131191574037075e-05

Training epoch-26 batch-115
Running loss of epoch-26 batch-115 = 0.00012313539627939463

Training epoch-26 batch-116
Running loss of epoch-26 batch-116 = 6.779935210943222e-05

Training epoch-26 batch-117
Running loss of epoch-26 batch-117 = 4.6579865738749504e-05

Training epoch-26 batch-118
Running loss of epoch-26 batch-118 = 2.932013012468815e-05

Training epoch-26 batch-119
Running loss of epoch-26 batch-119 = 7.035746239125729e-05

Training epoch-26 batch-120
Running loss of epoch-26 batch-120 = 5.109421908855438e-05

Training epoch-26 batch-121
Running loss of epoch-26 batch-121 = 7.940479554235935e-05

Training epoch-26 batch-122
Running loss of epoch-26 batch-122 = 2.785632386803627e-05

Training epoch-26 batch-123
Running loss of epoch-26 batch-123 = 0.0001271929359063506

Training epoch-26 batch-124
Running loss of epoch-26 batch-124 = 4.3097417801618576e-05

Training epoch-26 batch-125
Running loss of epoch-26 batch-125 = 5.465024150907993e-05

Training epoch-26 batch-126
Running loss of epoch-26 batch-126 = 6.319256499409676e-05

Training epoch-26 batch-127
Running loss of epoch-26 batch-127 = 8.521962445229292e-05

Training epoch-26 batch-128
Running loss of epoch-26 batch-128 = 9.122851770371199e-05

Training epoch-26 batch-129
Running loss of epoch-26 batch-129 = 6.912671960890293e-05

Training epoch-26 batch-130
Running loss of epoch-26 batch-130 = 0.0001229464542120695

Training epoch-26 batch-131
Running loss of epoch-26 batch-131 = 8.79303552210331e-05

Training epoch-26 batch-132
Running loss of epoch-26 batch-132 = 5.0338683649897575e-05

Training epoch-26 batch-133
Running loss of epoch-26 batch-133 = 8.038664236664772e-05

Training epoch-26 batch-134
Running loss of epoch-26 batch-134 = 4.551210440695286e-05

Training epoch-26 batch-135
Running loss of epoch-26 batch-135 = 4.040147177875042e-05

Training epoch-26 batch-136
Running loss of epoch-26 batch-136 = 9.225890971720219e-05

Training epoch-26 batch-137
Running loss of epoch-26 batch-137 = 0.0003548109671100974

Training epoch-26 batch-138
Running loss of epoch-26 batch-138 = 6.226450204849243e-05

Training epoch-26 batch-139
Running loss of epoch-26 batch-139 = 5.30777033418417e-05

Training epoch-26 batch-140
Running loss of epoch-26 batch-140 = 5.42511697858572e-05

Training epoch-26 batch-141
Running loss of epoch-26 batch-141 = 8.977623656392097e-05

Training epoch-26 batch-142
Running loss of epoch-26 batch-142 = 0.0003075178246945143

Training epoch-26 batch-143
Running loss of epoch-26 batch-143 = 0.000607461086474359

Training epoch-26 batch-144
Running loss of epoch-26 batch-144 = 8.765154052525759e-05

Training epoch-26 batch-145
Running loss of epoch-26 batch-145 = 0.0001547337742522359

Training epoch-26 batch-146
Running loss of epoch-26 batch-146 = 0.000114374328404665

Training epoch-26 batch-147
Running loss of epoch-26 batch-147 = 3.3797696232795715e-05

Training epoch-26 batch-148
Running loss of epoch-26 batch-148 = 0.0002684441860765219

Training epoch-26 batch-149
Running loss of epoch-26 batch-149 = 7.748696953058243e-05

Training epoch-26 batch-150
Running loss of epoch-26 batch-150 = 5.94977755099535e-05

Training epoch-26 batch-151
Running loss of epoch-26 batch-151 = 0.00013883074279874563

Training epoch-26 batch-152
Running loss of epoch-26 batch-152 = 5.811022128909826e-05

Training epoch-26 batch-153
Running loss of epoch-26 batch-153 = 0.00012707780115306377

Training epoch-26 batch-154
Running loss of epoch-26 batch-154 = 5.305861122906208e-05

Training epoch-26 batch-155
Running loss of epoch-26 batch-155 = 3.362633287906647e-05

Training epoch-26 batch-156
Running loss of epoch-26 batch-156 = 4.915287718176842e-05

Training epoch-26 batch-157
Running loss of epoch-26 batch-157 = 0.0002441816031932831

Finished training epoch-26.



Average train loss at epoch-26 = 8.668236881494523e-05

Started Evaluation

Average val loss at epoch-26 = 0.8936613742208991

Accuracy for classes:
Accuracy for class equals is: 96.86 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.64 %
Accuracy for class onCreate is: 91.47 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 70.09 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 40.56 %
Accuracy for class get is: 73.08 %

Overall Accuracy = 83.39 %

Finished Evaluation



Started training epoch-27


Training epoch-27 batch-1
Running loss of epoch-27 batch-1 = 0.0003431758377701044

Training epoch-27 batch-2
Running loss of epoch-27 batch-2 = 0.00014509866014122963

Training epoch-27 batch-3
Running loss of epoch-27 batch-3 = 6.09660055488348e-05

Training epoch-27 batch-4
Running loss of epoch-27 batch-4 = 7.377495057880878e-05

Training epoch-27 batch-5
Running loss of epoch-27 batch-5 = 0.00016683759167790413

Training epoch-27 batch-6
Running loss of epoch-27 batch-6 = 0.0001645311713218689

Training epoch-27 batch-7
Running loss of epoch-27 batch-7 = 4.509277641773224e-05

Training epoch-27 batch-8
Running loss of epoch-27 batch-8 = 5.8810925111174583e-05

Training epoch-27 batch-9
Running loss of epoch-27 batch-9 = 8.68913484737277e-05

Training epoch-27 batch-10
Running loss of epoch-27 batch-10 = 0.00017386639956384897

Training epoch-27 batch-11
Running loss of epoch-27 batch-11 = 0.0001592088956385851

Training epoch-27 batch-12
Running loss of epoch-27 batch-12 = 9.794184006750584e-05

Training epoch-27 batch-13
Running loss of epoch-27 batch-13 = 4.405132494866848e-05

Training epoch-27 batch-14
Running loss of epoch-27 batch-14 = 0.00010018516331911087

Training epoch-27 batch-15
Running loss of epoch-27 batch-15 = 0.00010448612738400698

Training epoch-27 batch-16
Running loss of epoch-27 batch-16 = 8.00394918769598e-05

Training epoch-27 batch-17
Running loss of epoch-27 batch-17 = 0.00014131166972219944

Training epoch-27 batch-18
Running loss of epoch-27 batch-18 = 5.1541952416300774e-05

Training epoch-27 batch-19
Running loss of epoch-27 batch-19 = 5.6907301768660545e-05

Training epoch-27 batch-20
Running loss of epoch-27 batch-20 = 2.812896855175495e-05

Training epoch-27 batch-21
Running loss of epoch-27 batch-21 = 1.8199672922492027e-05

Training epoch-27 batch-22
Running loss of epoch-27 batch-22 = 5.436502397060394e-05

Training epoch-27 batch-23
Running loss of epoch-27 batch-23 = 9.941519238054752e-05

Training epoch-27 batch-24
Running loss of epoch-27 batch-24 = 5.433103069663048e-05

Training epoch-27 batch-25
Running loss of epoch-27 batch-25 = 0.00030785438138991594

Training epoch-27 batch-26
Running loss of epoch-27 batch-26 = 7.694028317928314e-05

Training epoch-27 batch-27
Running loss of epoch-27 batch-27 = 3.748910967260599e-05

Training epoch-27 batch-28
Running loss of epoch-27 batch-28 = 9.499187581241131e-05

Training epoch-27 batch-29
Running loss of epoch-27 batch-29 = 5.6472024880349636e-05

Training epoch-27 batch-30
Running loss of epoch-27 batch-30 = 0.00026617187540978193

Training epoch-27 batch-31
Running loss of epoch-27 batch-31 = 6.062467582523823e-05

Training epoch-27 batch-32
Running loss of epoch-27 batch-32 = 9.319756645709276e-05

Training epoch-27 batch-33
Running loss of epoch-27 batch-33 = 4.38180286437273e-05

Training epoch-27 batch-34
Running loss of epoch-27 batch-34 = 5.360972136259079e-05

Training epoch-27 batch-35
Running loss of epoch-27 batch-35 = 8.194334805011749e-05

Training epoch-27 batch-36
Running loss of epoch-27 batch-36 = 8.621113374829292e-05

Training epoch-27 batch-37
Running loss of epoch-27 batch-37 = 3.2689888030290604e-05

Training epoch-27 batch-38
Running loss of epoch-27 batch-38 = 0.00011743581853806973

Training epoch-27 batch-39
Running loss of epoch-27 batch-39 = 7.414305582642555e-05

Training epoch-27 batch-40
Running loss of epoch-27 batch-40 = 7.470487616956234e-05

Training epoch-27 batch-41
Running loss of epoch-27 batch-41 = 0.00010048144031316042

Training epoch-27 batch-42
Running loss of epoch-27 batch-42 = 0.0001367183867841959

Training epoch-27 batch-43
Running loss of epoch-27 batch-43 = 3.460654988884926e-05

Training epoch-27 batch-44
Running loss of epoch-27 batch-44 = 1.5313271433115005e-05

Training epoch-27 batch-45
Running loss of epoch-27 batch-45 = 7.349858060479164e-05

Training epoch-27 batch-46
Running loss of epoch-27 batch-46 = 3.6803772673010826e-05

Training epoch-27 batch-47
Running loss of epoch-27 batch-47 = 3.790273331105709e-05

Training epoch-27 batch-48
Running loss of epoch-27 batch-48 = 2.8040260076522827e-05

Training epoch-27 batch-49
Running loss of epoch-27 batch-49 = 4.259776324033737e-05

Training epoch-27 batch-50
Running loss of epoch-27 batch-50 = 8.117477409541607e-05

Training epoch-27 batch-51
Running loss of epoch-27 batch-51 = 5.4465606808662415e-05

Training epoch-27 batch-52
Running loss of epoch-27 batch-52 = 7.40464311093092e-05

Training epoch-27 batch-53
Running loss of epoch-27 batch-53 = 9.312748443335295e-05

Training epoch-27 batch-54
Running loss of epoch-27 batch-54 = 5.6160613894462585e-05

Training epoch-27 batch-55
Running loss of epoch-27 batch-55 = 0.000181607436388731

Training epoch-27 batch-56
Running loss of epoch-27 batch-56 = 3.23181739076972e-05

Training epoch-27 batch-57
Running loss of epoch-27 batch-57 = 5.5461772717535496e-05

Training epoch-27 batch-58
Running loss of epoch-27 batch-58 = 4.364410415291786e-05

Training epoch-27 batch-59
Running loss of epoch-27 batch-59 = 4.9776630476117134e-05

Training epoch-27 batch-60
Running loss of epoch-27 batch-60 = 3.110198304057121e-05

Training epoch-27 batch-61
Running loss of epoch-27 batch-61 = 1.886347308754921e-05

Training epoch-27 batch-62
Running loss of epoch-27 batch-62 = 7.602432742714882e-05

Training epoch-27 batch-63
Running loss of epoch-27 batch-63 = 0.00012519536539912224

Training epoch-27 batch-64
Running loss of epoch-27 batch-64 = 7.136259227991104e-05

Training epoch-27 batch-65
Running loss of epoch-27 batch-65 = 3.626965917646885e-05

Training epoch-27 batch-66
Running loss of epoch-27 batch-66 = 4.7443085350096226e-05

Training epoch-27 batch-67
Running loss of epoch-27 batch-67 = 0.00010568439029157162

Training epoch-27 batch-68
Running loss of epoch-27 batch-68 = 0.00010574574116617441

Training epoch-27 batch-69
Running loss of epoch-27 batch-69 = 5.3683994337916374e-05

Training epoch-27 batch-70
Running loss of epoch-27 batch-70 = 7.826625369489193e-05

Training epoch-27 batch-71
Running loss of epoch-27 batch-71 = 9.039416909217834e-05

Training epoch-27 batch-72
Running loss of epoch-27 batch-72 = 3.490922972559929e-05

Training epoch-27 batch-73
Running loss of epoch-27 batch-73 = 3.6887358874082565e-05

Training epoch-27 batch-74
Running loss of epoch-27 batch-74 = 1.179426908493042e-05

Training epoch-27 batch-75
Running loss of epoch-27 batch-75 = 6.64892140775919e-05

Training epoch-27 batch-76
Running loss of epoch-27 batch-76 = 6.394810043275356e-05

Training epoch-27 batch-77
Running loss of epoch-27 batch-77 = 5.895888898521662e-05

Training epoch-27 batch-78
Running loss of epoch-27 batch-78 = 3.817398101091385e-05

Training epoch-27 batch-79
Running loss of epoch-27 batch-79 = 3.951927646994591e-05

Training epoch-27 batch-80
Running loss of epoch-27 batch-80 = 4.190509207546711e-05

Training epoch-27 batch-81
Running loss of epoch-27 batch-81 = 4.364899359643459e-05

Training epoch-27 batch-82
Running loss of epoch-27 batch-82 = 8.594966493546963e-05

Training epoch-27 batch-83
Running loss of epoch-27 batch-83 = 0.00011827005073428154

Training epoch-27 batch-84
Running loss of epoch-27 batch-84 = 0.00011409888975322247

Training epoch-27 batch-85
Running loss of epoch-27 batch-85 = 2.3484928533434868e-05

Training epoch-27 batch-86
Running loss of epoch-27 batch-86 = 0.00010509230196475983

Training epoch-27 batch-87
Running loss of epoch-27 batch-87 = 2.3434171453118324e-05

Training epoch-27 batch-88
Running loss of epoch-27 batch-88 = 4.58727590739727e-05

Training epoch-27 batch-89
Running loss of epoch-27 batch-89 = 2.9549002647399902e-05

Training epoch-27 batch-90
Running loss of epoch-27 batch-90 = 3.651599399745464e-05

Training epoch-27 batch-91
Running loss of epoch-27 batch-91 = 8.704583160579205e-05

Training epoch-27 batch-92
Running loss of epoch-27 batch-92 = 6.367452442646027e-05

Training epoch-27 batch-93
Running loss of epoch-27 batch-93 = 4.0929531678557396e-05

Training epoch-27 batch-94
Running loss of epoch-27 batch-94 = 4.2580533772706985e-05

Training epoch-27 batch-95
Running loss of epoch-27 batch-95 = 6.39383215457201e-05

Training epoch-27 batch-96
Running loss of epoch-27 batch-96 = 3.25995497405529e-05

Training epoch-27 batch-97
Running loss of epoch-27 batch-97 = 6.001465953886509e-05

Training epoch-27 batch-98
Running loss of epoch-27 batch-98 = 3.3180927857756615e-05

Training epoch-27 batch-99
Running loss of epoch-27 batch-99 = 9.07557550817728e-05

Training epoch-27 batch-100
Running loss of epoch-27 batch-100 = 3.2553449273109436e-05

Training epoch-27 batch-101
Running loss of epoch-27 batch-101 = 4.804902710020542e-05

Training epoch-27 batch-102
Running loss of epoch-27 batch-102 = 0.00015373830683529377

Training epoch-27 batch-103
Running loss of epoch-27 batch-103 = 6.0595106333494186e-05

Training epoch-27 batch-104
Running loss of epoch-27 batch-104 = 4.7258101403713226e-05

Training epoch-27 batch-105
Running loss of epoch-27 batch-105 = 9.866093751043081e-05

Training epoch-27 batch-106
Running loss of epoch-27 batch-106 = 4.9095251597464085e-05

Training epoch-27 batch-107
Running loss of epoch-27 batch-107 = 5.207071080803871e-05

Training epoch-27 batch-108
Running loss of epoch-27 batch-108 = 6.838934496045113e-05

Training epoch-27 batch-109
Running loss of epoch-27 batch-109 = 5.194498226046562e-05

Training epoch-27 batch-110
Running loss of epoch-27 batch-110 = 8.692755363881588e-05

Training epoch-27 batch-111
Running loss of epoch-27 batch-111 = 4.597799852490425e-05

Training epoch-27 batch-112
Running loss of epoch-27 batch-112 = 1.976010389626026e-05

Training epoch-27 batch-113
Running loss of epoch-27 batch-113 = 8.541811257600784e-05

Training epoch-27 batch-114
Running loss of epoch-27 batch-114 = 0.00013439171016216278

Training epoch-27 batch-115
Running loss of epoch-27 batch-115 = 0.00016381079331040382

Training epoch-27 batch-116
Running loss of epoch-27 batch-116 = 5.674944259226322e-05

Training epoch-27 batch-117
Running loss of epoch-27 batch-117 = 1.6730977222323418e-05

Training epoch-27 batch-118
Running loss of epoch-27 batch-118 = 9.20700840651989e-05

Training epoch-27 batch-119
Running loss of epoch-27 batch-119 = 4.8961490392684937e-05

Training epoch-27 batch-120
Running loss of epoch-27 batch-120 = 6.603659130632877e-05

Training epoch-27 batch-121
Running loss of epoch-27 batch-121 = 3.2869866117835045e-05

Training epoch-27 batch-122
Running loss of epoch-27 batch-122 = 2.5362474843859673e-05

Training epoch-27 batch-123
Running loss of epoch-27 batch-123 = 6.261293310672045e-05

Training epoch-27 batch-124
Running loss of epoch-27 batch-124 = 2.2651860490441322e-05

Training epoch-27 batch-125
Running loss of epoch-27 batch-125 = 6.454368121922016e-05

Training epoch-27 batch-126
Running loss of epoch-27 batch-126 = 1.5943078324198723e-05

Training epoch-27 batch-127
Running loss of epoch-27 batch-127 = 5.634874105453491e-05

Training epoch-27 batch-128
Running loss of epoch-27 batch-128 = 5.02739567309618e-05

Training epoch-27 batch-129
Running loss of epoch-27 batch-129 = 3.135320730507374e-05

Training epoch-27 batch-130
Running loss of epoch-27 batch-130 = 4.511431325227022e-05

Training epoch-27 batch-131
Running loss of epoch-27 batch-131 = 5.511799827218056e-05

Training epoch-27 batch-132
Running loss of epoch-27 batch-132 = 2.1859770640730858e-05

Training epoch-27 batch-133
Running loss of epoch-27 batch-133 = 7.337681017816067e-05

Training epoch-27 batch-134
Running loss of epoch-27 batch-134 = 4.627835005521774e-05

Training epoch-27 batch-135
Running loss of epoch-27 batch-135 = 5.17482403665781e-05

Training epoch-27 batch-136
Running loss of epoch-27 batch-136 = 4.7498615458607674e-05

Training epoch-27 batch-137
Running loss of epoch-27 batch-137 = 3.984093200415373e-05

Training epoch-27 batch-138
Running loss of epoch-27 batch-138 = 5.292752757668495e-05

Training epoch-27 batch-139
Running loss of epoch-27 batch-139 = 7.002917118370533e-05

Training epoch-27 batch-140
Running loss of epoch-27 batch-140 = 0.00025213975459337234

Training epoch-27 batch-141
Running loss of epoch-27 batch-141 = 0.0001459348713979125

Training epoch-27 batch-142
Running loss of epoch-27 batch-142 = 6.23171217739582e-05

Training epoch-27 batch-143
Running loss of epoch-27 batch-143 = 7.025944069027901e-05

Training epoch-27 batch-144
Running loss of epoch-27 batch-144 = 6.906257476657629e-05

Training epoch-27 batch-145
Running loss of epoch-27 batch-145 = 9.527301881462336e-05

Training epoch-27 batch-146
Running loss of epoch-27 batch-146 = 9.266450069844723e-05

Training epoch-27 batch-147
Running loss of epoch-27 batch-147 = 3.423960879445076e-05

Training epoch-27 batch-148
Running loss of epoch-27 batch-148 = 1.7798738554120064e-05

Training epoch-27 batch-149
Running loss of epoch-27 batch-149 = 6.363191641867161e-05

Training epoch-27 batch-150
Running loss of epoch-27 batch-150 = 2.0657433196902275e-05

Training epoch-27 batch-151
Running loss of epoch-27 batch-151 = 2.1411804482340813e-05

Training epoch-27 batch-152
Running loss of epoch-27 batch-152 = 2.9978342354297638e-05

Training epoch-27 batch-153
Running loss of epoch-27 batch-153 = 1.7777783796191216e-05

Training epoch-27 batch-154
Running loss of epoch-27 batch-154 = 3.432086668908596e-05

Training epoch-27 batch-155
Running loss of epoch-27 batch-155 = 4.634808283299208e-05

Training epoch-27 batch-156
Running loss of epoch-27 batch-156 = 0.00016430136747658253

Training epoch-27 batch-157
Running loss of epoch-27 batch-157 = 0.0009466037154197693

Finished training epoch-27.



Average train loss at epoch-27 = 7.288428395986557e-05

Started Evaluation

Average val loss at epoch-27 = 0.8792267451217652

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 93.11 %
Accuracy for class onCreate is: 92.00 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 65.53 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 69.74 %

Overall Accuracy = 83.72 %

Finished Evaluation



Started training epoch-28


Training epoch-28 batch-1
Running loss of epoch-28 batch-1 = 0.00011204322800040245

Training epoch-28 batch-2
Running loss of epoch-28 batch-2 = 2.8320588171482086e-05

Training epoch-28 batch-3
Running loss of epoch-28 batch-3 = 9.613868314772844e-05

Training epoch-28 batch-4
Running loss of epoch-28 batch-4 = 3.368849866092205e-05

Training epoch-28 batch-5
Running loss of epoch-28 batch-5 = 5.9880781918764114e-05

Training epoch-28 batch-6
Running loss of epoch-28 batch-6 = 3.223540261387825e-05

Training epoch-28 batch-7
Running loss of epoch-28 batch-7 = 0.00011207396164536476

Training epoch-28 batch-8
Running loss of epoch-28 batch-8 = 2.5216955691576004e-05

Training epoch-28 batch-9
Running loss of epoch-28 batch-9 = 2.9615359380841255e-05

Training epoch-28 batch-10
Running loss of epoch-28 batch-10 = 7.271626964211464e-05

Training epoch-28 batch-11
Running loss of epoch-28 batch-11 = 5.845376290380955e-05

Training epoch-28 batch-12
Running loss of epoch-28 batch-12 = 5.5598560720682144e-05

Training epoch-28 batch-13
Running loss of epoch-28 batch-13 = 3.22805717587471e-05

Training epoch-28 batch-14
Running loss of epoch-28 batch-14 = 4.742364399135113e-05

Training epoch-28 batch-15
Running loss of epoch-28 batch-15 = 7.482478395104408e-05

Training epoch-28 batch-16
Running loss of epoch-28 batch-16 = 0.00010762922465801239

Training epoch-28 batch-17
Running loss of epoch-28 batch-17 = 3.6586541682481766e-05

Training epoch-28 batch-18
Running loss of epoch-28 batch-18 = 6.294739432632923e-05

Training epoch-28 batch-19
Running loss of epoch-28 batch-19 = 6.23140949755907e-05

Training epoch-28 batch-20
Running loss of epoch-28 batch-20 = 7.721851579844952e-05

Training epoch-28 batch-21
Running loss of epoch-28 batch-21 = 4.55295667052269e-05

Training epoch-28 batch-22
Running loss of epoch-28 batch-22 = 3.312225453555584e-05

Training epoch-28 batch-23
Running loss of epoch-28 batch-23 = 2.0337989553809166e-05

Training epoch-28 batch-24
Running loss of epoch-28 batch-24 = 3.6410754546523094e-05

Training epoch-28 batch-25
Running loss of epoch-28 batch-25 = 4.10198699682951e-05

Training epoch-28 batch-26
Running loss of epoch-28 batch-26 = 0.00016652746126055717

Training epoch-28 batch-27
Running loss of epoch-28 batch-27 = 4.0724873542785645e-05

Training epoch-28 batch-28
Running loss of epoch-28 batch-28 = 5.85110392421484e-05

Training epoch-28 batch-29
Running loss of epoch-28 batch-29 = 3.639352507889271e-05

Training epoch-28 batch-30
Running loss of epoch-28 batch-30 = 4.369951784610748e-05

Training epoch-28 batch-31
Running loss of epoch-28 batch-31 = 3.125215880572796e-05

Training epoch-28 batch-32
Running loss of epoch-28 batch-32 = 2.0855339244008064e-05

Training epoch-28 batch-33
Running loss of epoch-28 batch-33 = 3.793742507696152e-05

Training epoch-28 batch-34
Running loss of epoch-28 batch-34 = 0.00016089074779301882

Training epoch-28 batch-35
Running loss of epoch-28 batch-35 = 1.972564496099949e-05

Training epoch-28 batch-36
Running loss of epoch-28 batch-36 = 7.847580127418041e-05

Training epoch-28 batch-37
Running loss of epoch-28 batch-37 = 5.19738532602787e-05

Training epoch-28 batch-38
Running loss of epoch-28 batch-38 = 6.211572326719761e-05

Training epoch-28 batch-39
Running loss of epoch-28 batch-39 = 5.842885002493858e-05

Training epoch-28 batch-40
Running loss of epoch-28 batch-40 = 4.189624451100826e-05

Training epoch-28 batch-41
Running loss of epoch-28 batch-41 = 4.817033186554909e-05

Training epoch-28 batch-42
Running loss of epoch-28 batch-42 = 4.920479841530323e-05

Training epoch-28 batch-43
Running loss of epoch-28 batch-43 = 4.473491571843624e-05

Training epoch-28 batch-44
Running loss of epoch-28 batch-44 = 7.004407234489918e-05

Training epoch-28 batch-45
Running loss of epoch-28 batch-45 = 2.6478199288249016e-05

Training epoch-28 batch-46
Running loss of epoch-28 batch-46 = 6.853463128209114e-05

Training epoch-28 batch-47
Running loss of epoch-28 batch-47 = 2.2629741579294205e-05

Training epoch-28 batch-48
Running loss of epoch-28 batch-48 = 3.2138777896761894e-05

Training epoch-28 batch-49
Running loss of epoch-28 batch-49 = 9.088870137929916e-05

Training epoch-28 batch-50
Running loss of epoch-28 batch-50 = 6.0003018006682396e-05

Training epoch-28 batch-51
Running loss of epoch-28 batch-51 = 4.558940418064594e-05

Training epoch-28 batch-52
Running loss of epoch-28 batch-52 = 5.284813232719898e-05

Training epoch-28 batch-53
Running loss of epoch-28 batch-53 = 4.332978278398514e-05

Training epoch-28 batch-54
Running loss of epoch-28 batch-54 = 0.00022712978534400463

Training epoch-28 batch-55
Running loss of epoch-28 batch-55 = 3.813835792243481e-05

Training epoch-28 batch-56
Running loss of epoch-28 batch-56 = 2.3520318791270256e-05

Training epoch-28 batch-57
Running loss of epoch-28 batch-57 = 0.0002934313379228115

Training epoch-28 batch-58
Running loss of epoch-28 batch-58 = 4.17900737375021e-05

Training epoch-28 batch-59
Running loss of epoch-28 batch-59 = 4.090380389243364e-05

Training epoch-28 batch-60
Running loss of epoch-28 batch-60 = 4.548579454421997e-05

Training epoch-28 batch-61
Running loss of epoch-28 batch-61 = 7.52441119402647e-05

Training epoch-28 batch-62
Running loss of epoch-28 batch-62 = 0.00010942132212221622

Training epoch-28 batch-63
Running loss of epoch-28 batch-63 = 9.860051795840263e-05

Training epoch-28 batch-64
Running loss of epoch-28 batch-64 = 0.00013701780699193478

Training epoch-28 batch-65
Running loss of epoch-28 batch-65 = 4.4172629714012146e-05

Training epoch-28 batch-66
Running loss of epoch-28 batch-66 = 2.79964879155159e-05

Training epoch-28 batch-67
Running loss of epoch-28 batch-67 = 3.1886156648397446e-05

Training epoch-28 batch-68
Running loss of epoch-28 batch-68 = 3.166962414979935e-05

Training epoch-28 batch-69
Running loss of epoch-28 batch-69 = 8.551729843020439e-05

Training epoch-28 batch-70
Running loss of epoch-28 batch-70 = 1.3192649930715561e-05

Training epoch-28 batch-71
Running loss of epoch-28 batch-71 = 0.00013962597586214542

Training epoch-28 batch-72
Running loss of epoch-28 batch-72 = 4.7023408114910126e-05

Training epoch-28 batch-73
Running loss of epoch-28 batch-73 = 5.31364930793643e-05

Training epoch-28 batch-74
Running loss of epoch-28 batch-74 = 4.545669071376324e-05

Training epoch-28 batch-75
Running loss of epoch-28 batch-75 = 5.932082422077656e-05

Training epoch-28 batch-76
Running loss of epoch-28 batch-76 = 6.117881275713444e-05

Training epoch-28 batch-77
Running loss of epoch-28 batch-77 = 3.941054455935955e-05

Training epoch-28 batch-78
Running loss of epoch-28 batch-78 = 0.00016205781139433384

Training epoch-28 batch-79
Running loss of epoch-28 batch-79 = 3.788643516600132e-05

Training epoch-28 batch-80
Running loss of epoch-28 batch-80 = 0.00010700745042413473

Training epoch-28 batch-81
Running loss of epoch-28 batch-81 = 3.213807940483093e-05

Training epoch-28 batch-82
Running loss of epoch-28 batch-82 = 6.466847844421864e-05

Training epoch-28 batch-83
Running loss of epoch-28 batch-83 = 3.949669189751148e-05

Training epoch-28 batch-84
Running loss of epoch-28 batch-84 = 5.634967237710953e-05

Training epoch-28 batch-85
Running loss of epoch-28 batch-85 = 3.0088471248745918e-05

Training epoch-28 batch-86
Running loss of epoch-28 batch-86 = 0.00012684776447713375

Training epoch-28 batch-87
Running loss of epoch-28 batch-87 = 4.938547499477863e-05

Training epoch-28 batch-88
Running loss of epoch-28 batch-88 = 5.107768811285496e-05

Training epoch-28 batch-89
Running loss of epoch-28 batch-89 = 0.0001585969002917409

Training epoch-28 batch-90
Running loss of epoch-28 batch-90 = 7.543480023741722e-05

Training epoch-28 batch-91
Running loss of epoch-28 batch-91 = 4.009413532912731e-05

Training epoch-28 batch-92
Running loss of epoch-28 batch-92 = 2.934853546321392e-05

Training epoch-28 batch-93
Running loss of epoch-28 batch-93 = 0.0001052196603268385

Training epoch-28 batch-94
Running loss of epoch-28 batch-94 = 0.000276926439255476

Training epoch-28 batch-95
Running loss of epoch-28 batch-95 = 4.1875988245010376e-05

Training epoch-28 batch-96
Running loss of epoch-28 batch-96 = 5.299854092299938e-05

Training epoch-28 batch-97
Running loss of epoch-28 batch-97 = 5.4345233365893364e-05

Training epoch-28 batch-98
Running loss of epoch-28 batch-98 = 9.44978091865778e-05

Training epoch-28 batch-99
Running loss of epoch-28 batch-99 = 6.51779118925333e-05

Training epoch-28 batch-100
Running loss of epoch-28 batch-100 = 4.121451638638973e-05

Training epoch-28 batch-101
Running loss of epoch-28 batch-101 = 2.4461885914206505e-05

Training epoch-28 batch-102
Running loss of epoch-28 batch-102 = 6.394903175532818e-05

Training epoch-28 batch-103
Running loss of epoch-28 batch-103 = 1.937616616487503e-05

Training epoch-28 batch-104
Running loss of epoch-28 batch-104 = 4.5640161260962486e-05

Training epoch-28 batch-105
Running loss of epoch-28 batch-105 = 1.4429213479161263e-05

Training epoch-28 batch-106
Running loss of epoch-28 batch-106 = 2.86793801933527e-05

Training epoch-28 batch-107
Running loss of epoch-28 batch-107 = 5.283532664179802e-05

Training epoch-28 batch-108
Running loss of epoch-28 batch-108 = 3.0845869332551956e-05

Training epoch-28 batch-109
Running loss of epoch-28 batch-109 = 0.0001239595003426075

Training epoch-28 batch-110
Running loss of epoch-28 batch-110 = 5.862268153578043e-05

Training epoch-28 batch-111
Running loss of epoch-28 batch-111 = 5.048047751188278e-05

Training epoch-28 batch-112
Running loss of epoch-28 batch-112 = 7.815193384885788e-05

Training epoch-28 batch-113
Running loss of epoch-28 batch-113 = 7.868744432926178e-05

Training epoch-28 batch-114
Running loss of epoch-28 batch-114 = 6.375613156706095e-05

Training epoch-28 batch-115
Running loss of epoch-28 batch-115 = 6.753858178853989e-05

Training epoch-28 batch-116
Running loss of epoch-28 batch-116 = 5.3181895054876804e-05

Training epoch-28 batch-117
Running loss of epoch-28 batch-117 = 3.7037767469882965e-05

Training epoch-28 batch-118
Running loss of epoch-28 batch-118 = 2.9993243515491486e-05

Training epoch-28 batch-119
Running loss of epoch-28 batch-119 = 7.172441110014915e-05

Training epoch-28 batch-120
Running loss of epoch-28 batch-120 = 0.00029125576838850975

Training epoch-28 batch-121
Running loss of epoch-28 batch-121 = 2.866308204829693e-05

Training epoch-28 batch-122
Running loss of epoch-28 batch-122 = 7.531186565756798e-05

Training epoch-28 batch-123
Running loss of epoch-28 batch-123 = 0.00011495989747345448

Training epoch-28 batch-124
Running loss of epoch-28 batch-124 = 0.00010990770533680916

Training epoch-28 batch-125
Running loss of epoch-28 batch-125 = 5.94104640185833e-05

Training epoch-28 batch-126
Running loss of epoch-28 batch-126 = 5.0588278099894524e-05

Training epoch-28 batch-127
Running loss of epoch-28 batch-127 = 6.195250898599625e-05

Training epoch-28 batch-128
Running loss of epoch-28 batch-128 = 6.044958718121052e-05

Training epoch-28 batch-129
Running loss of epoch-28 batch-129 = 4.391581751406193e-05

Training epoch-28 batch-130
Running loss of epoch-28 batch-130 = 8.490518666803837e-05

Training epoch-28 batch-131
Running loss of epoch-28 batch-131 = 0.00010700942948460579

Training epoch-28 batch-132
Running loss of epoch-28 batch-132 = 6.869190838187933e-05

Training epoch-28 batch-133
Running loss of epoch-28 batch-133 = 2.5704968720674515e-05

Training epoch-28 batch-134
Running loss of epoch-28 batch-134 = 0.00015634309966117144

Training epoch-28 batch-135
Running loss of epoch-28 batch-135 = 2.7353642508387566e-05

Training epoch-28 batch-136
Running loss of epoch-28 batch-136 = 4.077516496181488e-05

Training epoch-28 batch-137
Running loss of epoch-28 batch-137 = 4.136841744184494e-05

Training epoch-28 batch-138
Running loss of epoch-28 batch-138 = 2.5374582037329674e-05

Training epoch-28 batch-139
Running loss of epoch-28 batch-139 = 5.9761456213891506e-05

Training epoch-28 batch-140
Running loss of epoch-28 batch-140 = 4.7600362449884415e-05

Training epoch-28 batch-141
Running loss of epoch-28 batch-141 = 5.181063897907734e-05

Training epoch-28 batch-142
Running loss of epoch-28 batch-142 = 2.5700312107801437e-05

Training epoch-28 batch-143
Running loss of epoch-28 batch-143 = 7.267901673913002e-05

Training epoch-28 batch-144
Running loss of epoch-28 batch-144 = 0.0001613008789718151

Training epoch-28 batch-145
Running loss of epoch-28 batch-145 = 2.5289366021752357e-05

Training epoch-28 batch-146
Running loss of epoch-28 batch-146 = 5.426513962447643e-05

Training epoch-28 batch-147
Running loss of epoch-28 batch-147 = 4.059774801135063e-05

Training epoch-28 batch-148
Running loss of epoch-28 batch-148 = 7.18521187081933e-05

Training epoch-28 batch-149
Running loss of epoch-28 batch-149 = 9.238091297447681e-05

Training epoch-28 batch-150
Running loss of epoch-28 batch-150 = 2.6664230972528458e-05

Training epoch-28 batch-151
Running loss of epoch-28 batch-151 = 4.3658772483468056e-05

Training epoch-28 batch-152
Running loss of epoch-28 batch-152 = 4.015350714325905e-05

Training epoch-28 batch-153
Running loss of epoch-28 batch-153 = 1.6215024515986443e-05

Training epoch-28 batch-154
Running loss of epoch-28 batch-154 = 7.222406566143036e-05

Training epoch-28 batch-155
Running loss of epoch-28 batch-155 = 3.980635665357113e-05

Training epoch-28 batch-156
Running loss of epoch-28 batch-156 = 3.7100398913025856e-05

Training epoch-28 batch-157
Running loss of epoch-28 batch-157 = 0.0001392364501953125

Finished training epoch-28.



Average train loss at epoch-28 = 6.42675019800663e-05

Started Evaluation

Average val loss at epoch-28 = 0.8972008403066675

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 92.13 %
Accuracy for class onCreate is: 91.79 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 66.44 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.50 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.68 %

Finished Evaluation



Started training epoch-29


Training epoch-29 batch-1
Running loss of epoch-29 batch-1 = 2.8532231226563454e-05

Training epoch-29 batch-2
Running loss of epoch-29 batch-2 = 4.88748773932457e-05

Training epoch-29 batch-3
Running loss of epoch-29 batch-3 = 6.794324144721031e-05

Training epoch-29 batch-4
Running loss of epoch-29 batch-4 = 5.646911449730396e-05

Training epoch-29 batch-5
Running loss of epoch-29 batch-5 = 3.9638252928853035e-05

Training epoch-29 batch-6
Running loss of epoch-29 batch-6 = 3.889948129653931e-05

Training epoch-29 batch-7
Running loss of epoch-29 batch-7 = 0.0001066222321242094

Training epoch-29 batch-8
Running loss of epoch-29 batch-8 = 4.912004806101322e-05

Training epoch-29 batch-9
Running loss of epoch-29 batch-9 = 3.714137710630894e-05

Training epoch-29 batch-10
Running loss of epoch-29 batch-10 = 3.786617890000343e-05

Training epoch-29 batch-11
Running loss of epoch-29 batch-11 = 5.3691910579800606e-05

Training epoch-29 batch-12
Running loss of epoch-29 batch-12 = 5.251704715192318e-05

Training epoch-29 batch-13
Running loss of epoch-29 batch-13 = 5.901162512600422e-05

Training epoch-29 batch-14
Running loss of epoch-29 batch-14 = 5.623255856335163e-05

Training epoch-29 batch-15
Running loss of epoch-29 batch-15 = 3.0441908165812492e-05

Training epoch-29 batch-16
Running loss of epoch-29 batch-16 = 2.01414804905653e-05

Training epoch-29 batch-17
Running loss of epoch-29 batch-17 = 3.761064726859331e-05

Training epoch-29 batch-18
Running loss of epoch-29 batch-18 = 2.298690378665924e-05

Training epoch-29 batch-19
Running loss of epoch-29 batch-19 = 2.8613372705876827e-05

Training epoch-29 batch-20
Running loss of epoch-29 batch-20 = 2.3137778043746948e-05

Training epoch-29 batch-21
Running loss of epoch-29 batch-21 = 8.364301174879074e-05

Training epoch-29 batch-22
Running loss of epoch-29 batch-22 = 4.118261858820915e-05

Training epoch-29 batch-23
Running loss of epoch-29 batch-23 = 2.0626001060009003e-05

Training epoch-29 batch-24
Running loss of epoch-29 batch-24 = 7.38657545298338e-05

Training epoch-29 batch-25
Running loss of epoch-29 batch-25 = 0.00015242933295667171

Training epoch-29 batch-26
Running loss of epoch-29 batch-26 = 2.9376475140452385e-05

Training epoch-29 batch-27
Running loss of epoch-29 batch-27 = 2.665841020643711e-05

Training epoch-29 batch-28
Running loss of epoch-29 batch-28 = 2.984190359711647e-05

Training epoch-29 batch-29
Running loss of epoch-29 batch-29 = 1.8309336155653e-05

Training epoch-29 batch-30
Running loss of epoch-29 batch-30 = 2.039177343249321e-05

Training epoch-29 batch-31
Running loss of epoch-29 batch-31 = 3.5779085010290146e-05

Training epoch-29 batch-32
Running loss of epoch-29 batch-32 = 0.00021181150805205107

Training epoch-29 batch-33
Running loss of epoch-29 batch-33 = 2.423301339149475e-05

Training epoch-29 batch-34
Running loss of epoch-29 batch-34 = 3.209593705832958e-05

Training epoch-29 batch-35
Running loss of epoch-29 batch-35 = 3.551505506038666e-05

Training epoch-29 batch-36
Running loss of epoch-29 batch-36 = 0.00011599110439419746

Training epoch-29 batch-37
Running loss of epoch-29 batch-37 = 4.230625927448273e-05

Training epoch-29 batch-38
Running loss of epoch-29 batch-38 = 7.47202429920435e-05

Training epoch-29 batch-39
Running loss of epoch-29 batch-39 = 6.860983557999134e-05

Training epoch-29 batch-40
Running loss of epoch-29 batch-40 = 3.1660543754696846e-05

Training epoch-29 batch-41
Running loss of epoch-29 batch-41 = 5.412660539150238e-05

Training epoch-29 batch-42
Running loss of epoch-29 batch-42 = 6.0540856793522835e-05

Training epoch-29 batch-43
Running loss of epoch-29 batch-43 = 5.657668225467205e-05

Training epoch-29 batch-44
Running loss of epoch-29 batch-44 = 3.476860001683235e-05

Training epoch-29 batch-45
Running loss of epoch-29 batch-45 = 4.372117109596729e-05

Training epoch-29 batch-46
Running loss of epoch-29 batch-46 = 8.921371772885323e-05

Training epoch-29 batch-47
Running loss of epoch-29 batch-47 = 9.736884385347366e-05

Training epoch-29 batch-48
Running loss of epoch-29 batch-48 = 3.594229929149151e-05

Training epoch-29 batch-49
Running loss of epoch-29 batch-49 = 6.845314055681229e-05

Training epoch-29 batch-50
Running loss of epoch-29 batch-50 = 6.181490607559681e-05

Training epoch-29 batch-51
Running loss of epoch-29 batch-51 = 1.3018492609262466e-05

Training epoch-29 batch-52
Running loss of epoch-29 batch-52 = 0.0001208367757499218

Training epoch-29 batch-53
Running loss of epoch-29 batch-53 = 2.1625077351927757e-05

Training epoch-29 batch-54
Running loss of epoch-29 batch-54 = 0.00017076940275728703

Training epoch-29 batch-55
Running loss of epoch-29 batch-55 = 3.1419796869158745e-05

Training epoch-29 batch-56
Running loss of epoch-29 batch-56 = 2.4894485250115395e-05

Training epoch-29 batch-57
Running loss of epoch-29 batch-57 = 3.567594103515148e-05

Training epoch-29 batch-58
Running loss of epoch-29 batch-58 = 2.6207882910966873e-05

Training epoch-29 batch-59
Running loss of epoch-29 batch-59 = 2.3602740839123726e-05

Training epoch-29 batch-60
Running loss of epoch-29 batch-60 = 7.400894537568092e-05

Training epoch-29 batch-61
Running loss of epoch-29 batch-61 = 8.403474930673838e-05

Training epoch-29 batch-62
Running loss of epoch-29 batch-62 = 0.00012668943963944912

Training epoch-29 batch-63
Running loss of epoch-29 batch-63 = 4.4680899009108543e-05

Training epoch-29 batch-64
Running loss of epoch-29 batch-64 = 0.00011052028276026249

Training epoch-29 batch-65
Running loss of epoch-29 batch-65 = 3.532972186803818e-05

Training epoch-29 batch-66
Running loss of epoch-29 batch-66 = 3.065611235797405e-05

Training epoch-29 batch-67
Running loss of epoch-29 batch-67 = 3.08305025100708e-05

Training epoch-29 batch-68
Running loss of epoch-29 batch-68 = 5.481252446770668e-05

Training epoch-29 batch-69
Running loss of epoch-29 batch-69 = 3.098277375102043e-05

Training epoch-29 batch-70
Running loss of epoch-29 batch-70 = 2.659042365849018e-05

Training epoch-29 batch-71
Running loss of epoch-29 batch-71 = 3.685825504362583e-05

Training epoch-29 batch-72
Running loss of epoch-29 batch-72 = 2.210773527622223e-05

Training epoch-29 batch-73
Running loss of epoch-29 batch-73 = 6.166822277009487e-05

Training epoch-29 batch-74
Running loss of epoch-29 batch-74 = 1.8090475350618362e-05

Training epoch-29 batch-75
Running loss of epoch-29 batch-75 = 4.958605859428644e-05

Training epoch-29 batch-76
Running loss of epoch-29 batch-76 = 2.7082860469818115e-05

Training epoch-29 batch-77
Running loss of epoch-29 batch-77 = 1.4542369171977043e-05

Training epoch-29 batch-78
Running loss of epoch-29 batch-78 = 4.909269046038389e-05

Training epoch-29 batch-79
Running loss of epoch-29 batch-79 = 3.245146945118904e-05

Training epoch-29 batch-80
Running loss of epoch-29 batch-80 = 5.427515134215355e-05

Training epoch-29 batch-81
Running loss of epoch-29 batch-81 = 2.9951799660921097e-05

Training epoch-29 batch-82
Running loss of epoch-29 batch-82 = 9.826209861785173e-05

Training epoch-29 batch-83
Running loss of epoch-29 batch-83 = 2.8379610739648342e-05

Training epoch-29 batch-84
Running loss of epoch-29 batch-84 = 0.00014425709377974272

Training epoch-29 batch-85
Running loss of epoch-29 batch-85 = 3.406591713428497e-05

Training epoch-29 batch-86
Running loss of epoch-29 batch-86 = 4.1978200897574425e-05

Training epoch-29 batch-87
Running loss of epoch-29 batch-87 = 2.709974069148302e-05

Training epoch-29 batch-88
Running loss of epoch-29 batch-88 = 3.3065443858504295e-05

Training epoch-29 batch-89
Running loss of epoch-29 batch-89 = 6.803299766033888e-05

Training epoch-29 batch-90
Running loss of epoch-29 batch-90 = 3.900239244103432e-05

Training epoch-29 batch-91
Running loss of epoch-29 batch-91 = 8.37638508528471e-05

Training epoch-29 batch-92
Running loss of epoch-29 batch-92 = 2.255779691040516e-05

Training epoch-29 batch-93
Running loss of epoch-29 batch-93 = 6.181164644658566e-05

Training epoch-29 batch-94
Running loss of epoch-29 batch-94 = 3.386428579688072e-05

Training epoch-29 batch-95
Running loss of epoch-29 batch-95 = 2.3945001885294914e-05

Training epoch-29 batch-96
Running loss of epoch-29 batch-96 = 2.3109838366508484e-05

Training epoch-29 batch-97
Running loss of epoch-29 batch-97 = 3.9443839341402054e-05

Training epoch-29 batch-98
Running loss of epoch-29 batch-98 = 2.6609981432557106e-05

Training epoch-29 batch-99
Running loss of epoch-29 batch-99 = 3.4492695704102516e-05

Training epoch-29 batch-100
Running loss of epoch-29 batch-100 = 4.852958954870701e-05

Training epoch-29 batch-101
Running loss of epoch-29 batch-101 = 1.974077895283699e-05

Training epoch-29 batch-102
Running loss of epoch-29 batch-102 = 5.2231596782803535e-05

Training epoch-29 batch-103
Running loss of epoch-29 batch-103 = 5.5711716413497925e-05

Training epoch-29 batch-104
Running loss of epoch-29 batch-104 = 4.535028710961342e-05

Training epoch-29 batch-105
Running loss of epoch-29 batch-105 = 0.0001264538150280714

Training epoch-29 batch-106
Running loss of epoch-29 batch-106 = 6.686535198241472e-05

Training epoch-29 batch-107
Running loss of epoch-29 batch-107 = 0.00010790280066430569

Training epoch-29 batch-108
Running loss of epoch-29 batch-108 = 1.885136589407921e-05

Training epoch-29 batch-109
Running loss of epoch-29 batch-109 = 5.526863969862461e-05

Training epoch-29 batch-110
Running loss of epoch-29 batch-110 = 4.0431274101138115e-05

Training epoch-29 batch-111
Running loss of epoch-29 batch-111 = 4.1351187974214554e-05

Training epoch-29 batch-112
Running loss of epoch-29 batch-112 = 4.7890469431877136e-05

Training epoch-29 batch-113
Running loss of epoch-29 batch-113 = 4.3794745579361916e-05

Training epoch-29 batch-114
Running loss of epoch-29 batch-114 = 7.698882836848497e-05

Training epoch-29 batch-115
Running loss of epoch-29 batch-115 = 3.0526891350746155e-05

Training epoch-29 batch-116
Running loss of epoch-29 batch-116 = 3.8797035813331604e-05

Training epoch-29 batch-117
Running loss of epoch-29 batch-117 = 1.8082791939377785e-05

Training epoch-29 batch-118
Running loss of epoch-29 batch-118 = 6.354786455631256e-05

Training epoch-29 batch-119
Running loss of epoch-29 batch-119 = 3.0455412343144417e-05

Training epoch-29 batch-120
Running loss of epoch-29 batch-120 = 2.291216515004635e-05

Training epoch-29 batch-121
Running loss of epoch-29 batch-121 = 5.583278834819794e-05

Training epoch-29 batch-122
Running loss of epoch-29 batch-122 = 3.4899916499853134e-05

Training epoch-29 batch-123
Running loss of epoch-29 batch-123 = 3.63239087164402e-05

Training epoch-29 batch-124
Running loss of epoch-29 batch-124 = 4.235352389514446e-05

Training epoch-29 batch-125
Running loss of epoch-29 batch-125 = 6.030371878296137e-05

Training epoch-29 batch-126
Running loss of epoch-29 batch-126 = 3.935955464839935e-05

Training epoch-29 batch-127
Running loss of epoch-29 batch-127 = 4.982692189514637e-05

Training epoch-29 batch-128
Running loss of epoch-29 batch-128 = 2.679065801203251e-05

Training epoch-29 batch-129
Running loss of epoch-29 batch-129 = 2.4869106709957123e-05

Training epoch-29 batch-130
Running loss of epoch-29 batch-130 = 2.845143899321556e-05

Training epoch-29 batch-131
Running loss of epoch-29 batch-131 = 3.318255767226219e-05

Training epoch-29 batch-132
Running loss of epoch-29 batch-132 = 5.021924152970314e-05

Training epoch-29 batch-133
Running loss of epoch-29 batch-133 = 3.24789434671402e-05

Training epoch-29 batch-134
Running loss of epoch-29 batch-134 = 2.727564424276352e-05

Training epoch-29 batch-135
Running loss of epoch-29 batch-135 = 3.097672015428543e-05

Training epoch-29 batch-136
Running loss of epoch-29 batch-136 = 5.0070229917764664e-05

Training epoch-29 batch-137
Running loss of epoch-29 batch-137 = 1.522270031273365e-05

Training epoch-29 batch-138
Running loss of epoch-29 batch-138 = 1.1409865692257881e-05

Training epoch-29 batch-139
Running loss of epoch-29 batch-139 = 4.5578693971037865e-05

Training epoch-29 batch-140
Running loss of epoch-29 batch-140 = 2.462044358253479e-05

Training epoch-29 batch-141
Running loss of epoch-29 batch-141 = 2.0371749997138977e-05

Training epoch-29 batch-142
Running loss of epoch-29 batch-142 = 0.00014581135474145412

Training epoch-29 batch-143
Running loss of epoch-29 batch-143 = 0.00010190252214670181

Training epoch-29 batch-144
Running loss of epoch-29 batch-144 = 2.9855873435735703e-05

Training epoch-29 batch-145
Running loss of epoch-29 batch-145 = 7.246504537761211e-05

Training epoch-29 batch-146
Running loss of epoch-29 batch-146 = 2.7667498216032982e-05

Training epoch-29 batch-147
Running loss of epoch-29 batch-147 = 6.905756890773773e-05

Training epoch-29 batch-148
Running loss of epoch-29 batch-148 = 2.7788570150732994e-05

Training epoch-29 batch-149
Running loss of epoch-29 batch-149 = 6.388034671545029e-05

Training epoch-29 batch-150
Running loss of epoch-29 batch-150 = 4.486215766519308e-05

Training epoch-29 batch-151
Running loss of epoch-29 batch-151 = 3.284751437604427e-05

Training epoch-29 batch-152
Running loss of epoch-29 batch-152 = 3.521796315908432e-05

Training epoch-29 batch-153
Running loss of epoch-29 batch-153 = 6.837397813796997e-05

Training epoch-29 batch-154
Running loss of epoch-29 batch-154 = 3.5072211176157e-05

Training epoch-29 batch-155
Running loss of epoch-29 batch-155 = 4.79905866086483e-05

Training epoch-29 batch-156
Running loss of epoch-29 batch-156 = 9.963661432266235e-05

Training epoch-29 batch-157
Running loss of epoch-29 batch-157 = 7.674098014831543e-06

Finished training epoch-29.



Average train loss at epoch-29 = 4.9536985903978345e-05

Started Evaluation

Average val loss at epoch-29 = 0.9211592946201348

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 91.47 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 58.52 %
Accuracy for class execute is: 46.99 %
Accuracy for class get is: 76.41 %

Overall Accuracy = 83.95 %

Finished Evaluation



Started training epoch-30


Training epoch-30 batch-1
Running loss of epoch-30 batch-1 = 5.063577555119991e-05

Training epoch-30 batch-2
Running loss of epoch-30 batch-2 = 3.272620961070061e-05

Training epoch-30 batch-3
Running loss of epoch-30 batch-3 = 5.86614478379488e-05

Training epoch-30 batch-4
Running loss of epoch-30 batch-4 = 3.366568125784397e-05

Training epoch-30 batch-5
Running loss of epoch-30 batch-5 = 4.695984534919262e-05

Training epoch-30 batch-6
Running loss of epoch-30 batch-6 = 2.5052344426512718e-05

Training epoch-30 batch-7
Running loss of epoch-30 batch-7 = 3.303936682641506e-05

Training epoch-30 batch-8
Running loss of epoch-30 batch-8 = 3.0027702450752258e-05

Training epoch-30 batch-9
Running loss of epoch-30 batch-9 = 2.545444294810295e-05

Training epoch-30 batch-10
Running loss of epoch-30 batch-10 = 4.131533205509186e-05

Training epoch-30 batch-11
Running loss of epoch-30 batch-11 = 4.53294487670064e-05

Training epoch-30 batch-12
Running loss of epoch-30 batch-12 = 2.7727801352739334e-05

Training epoch-30 batch-13
Running loss of epoch-30 batch-13 = 1.8406659364700317e-05

Training epoch-30 batch-14
Running loss of epoch-30 batch-14 = 2.966867759823799e-05

Training epoch-30 batch-15
Running loss of epoch-30 batch-15 = 3.329385071992874e-05

Training epoch-30 batch-16
Running loss of epoch-30 batch-16 = 2.619274891912937e-05

Training epoch-30 batch-17
Running loss of epoch-30 batch-17 = 4.543829709291458e-05

Training epoch-30 batch-18
Running loss of epoch-30 batch-18 = 3.1656352803111076e-05

Training epoch-30 batch-19
Running loss of epoch-30 batch-19 = 3.2033771276474e-05

Training epoch-30 batch-20
Running loss of epoch-30 batch-20 = 4.8053450882434845e-05

Training epoch-30 batch-21
Running loss of epoch-30 batch-21 = 1.829187385737896e-05

Training epoch-30 batch-22
Running loss of epoch-30 batch-22 = 5.196966230869293e-05

Training epoch-30 batch-23
Running loss of epoch-30 batch-23 = 3.645545803010464e-05

Training epoch-30 batch-24
Running loss of epoch-30 batch-24 = 3.9471546187996864e-05

Training epoch-30 batch-25
Running loss of epoch-30 batch-25 = 4.488846752792597e-05

Training epoch-30 batch-26
Running loss of epoch-30 batch-26 = 6.012246012687683e-05

Training epoch-30 batch-27
Running loss of epoch-30 batch-27 = 4.250975325703621e-05

Training epoch-30 batch-28
Running loss of epoch-30 batch-28 = 4.867278039455414e-05

Training epoch-30 batch-29
Running loss of epoch-30 batch-29 = 3.459665458649397e-05

Training epoch-30 batch-30
Running loss of epoch-30 batch-30 = 3.133667632937431e-05

Training epoch-30 batch-31
Running loss of epoch-30 batch-31 = 3.137718886137009e-05

Training epoch-30 batch-32
Running loss of epoch-30 batch-32 = 2.1387822926044464e-05

Training epoch-30 batch-33
Running loss of epoch-30 batch-33 = 4.1557708755135536e-05

Training epoch-30 batch-34
Running loss of epoch-30 batch-34 = 2.2115884348750114e-05

Training epoch-30 batch-35
Running loss of epoch-30 batch-35 = 3.963173367083073e-05

Training epoch-30 batch-36
Running loss of epoch-30 batch-36 = 3.824871964752674e-05

Training epoch-30 batch-37
Running loss of epoch-30 batch-37 = 3.7404708564281464e-05

Training epoch-30 batch-38
Running loss of epoch-30 batch-38 = 2.2817635908722878e-05

Training epoch-30 batch-39
Running loss of epoch-30 batch-39 = 4.368438385426998e-05

Training epoch-30 batch-40
Running loss of epoch-30 batch-40 = 1.7135636880993843e-05

Training epoch-30 batch-41
Running loss of epoch-30 batch-41 = 2.9080314561724663e-05

Training epoch-30 batch-42
Running loss of epoch-30 batch-42 = 2.7914298698306084e-05

Training epoch-30 batch-43
Running loss of epoch-30 batch-43 = 1.7949147149920464e-05

Training epoch-30 batch-44
Running loss of epoch-30 batch-44 = 4.5708613470196724e-05

Training epoch-30 batch-45
Running loss of epoch-30 batch-45 = 0.00011478317901492119

Training epoch-30 batch-46
Running loss of epoch-30 batch-46 = 1.3835495337843895e-05

Training epoch-30 batch-47
Running loss of epoch-30 batch-47 = 3.1522708013653755e-05

Training epoch-30 batch-48
Running loss of epoch-30 batch-48 = 3.351038321852684e-05

Training epoch-30 batch-49
Running loss of epoch-30 batch-49 = 3.0502676963806152e-05

Training epoch-30 batch-50
Running loss of epoch-30 batch-50 = 4.452303983271122e-05

Training epoch-30 batch-51
Running loss of epoch-30 batch-51 = 1.997128129005432e-05

Training epoch-30 batch-52
Running loss of epoch-30 batch-52 = 1.6901642084121704e-05

Training epoch-30 batch-53
Running loss of epoch-30 batch-53 = 3.178603947162628e-05

Training epoch-30 batch-54
Running loss of epoch-30 batch-54 = 2.6140594854950905e-05

Training epoch-30 batch-55
Running loss of epoch-30 batch-55 = 4.269578494131565e-05

Training epoch-30 batch-56
Running loss of epoch-30 batch-56 = 2.517376560717821e-05

Training epoch-30 batch-57
Running loss of epoch-30 batch-57 = 2.698577009141445e-05

Training epoch-30 batch-58
Running loss of epoch-30 batch-58 = 2.7212081477046013e-05

Training epoch-30 batch-59
Running loss of epoch-30 batch-59 = 1.013604924082756e-05

Training epoch-30 batch-60
Running loss of epoch-30 batch-60 = 0.00013544689863920212

Training epoch-30 batch-61
Running loss of epoch-30 batch-61 = 4.2974832467734814e-05

Training epoch-30 batch-62
Running loss of epoch-30 batch-62 = 2.2099586203694344e-05

Training epoch-30 batch-63
Running loss of epoch-30 batch-63 = 4.623504355549812e-05

Training epoch-30 batch-64
Running loss of epoch-30 batch-64 = 4.7939131036400795e-05

Training epoch-30 batch-65
Running loss of epoch-30 batch-65 = 4.49460931122303e-05

Training epoch-30 batch-66
Running loss of epoch-30 batch-66 = 2.1030427888035774e-05

Training epoch-30 batch-67
Running loss of epoch-30 batch-67 = 4.8722606152296066e-05

Training epoch-30 batch-68
Running loss of epoch-30 batch-68 = 5.553918890655041e-05

Training epoch-30 batch-69
Running loss of epoch-30 batch-69 = 8.98563303053379e-06

Training epoch-30 batch-70
Running loss of epoch-30 batch-70 = 1.896871253848076e-05

Training epoch-30 batch-71
Running loss of epoch-30 batch-71 = 0.00029250304214656353

Training epoch-30 batch-72
Running loss of epoch-30 batch-72 = 1.629907637834549e-05

Training epoch-30 batch-73
Running loss of epoch-30 batch-73 = 4.262709990143776e-05

Training epoch-30 batch-74
Running loss of epoch-30 batch-74 = 5.216104909777641e-05

Training epoch-30 batch-75
Running loss of epoch-30 batch-75 = 6.042933091521263e-05

Training epoch-30 batch-76
Running loss of epoch-30 batch-76 = 3.412726800888777e-05

Training epoch-30 batch-77
Running loss of epoch-30 batch-77 = 3.3331336453557014e-05

Training epoch-30 batch-78
Running loss of epoch-30 batch-78 = 5.128071643412113e-05

Training epoch-30 batch-79
Running loss of epoch-30 batch-79 = 8.61769076436758e-05

Training epoch-30 batch-80
Running loss of epoch-30 batch-80 = 0.00016677542589604855

Training epoch-30 batch-81
Running loss of epoch-30 batch-81 = 7.470790296792984e-05

Training epoch-30 batch-82
Running loss of epoch-30 batch-82 = 2.213660627603531e-05

Training epoch-30 batch-83
Running loss of epoch-30 batch-83 = 5.763024091720581e-05

Training epoch-30 batch-84
Running loss of epoch-30 batch-84 = 6.176135502755642e-05

Training epoch-30 batch-85
Running loss of epoch-30 batch-85 = 0.00020946725271642208

Training epoch-30 batch-86
Running loss of epoch-30 batch-86 = 2.7463538572192192e-05

Training epoch-30 batch-87
Running loss of epoch-30 batch-87 = 3.057043068110943e-05

Training epoch-30 batch-88
Running loss of epoch-30 batch-88 = 4.100636579096317e-05

Training epoch-30 batch-89
Running loss of epoch-30 batch-89 = 3.6093639209866524e-05

Training epoch-30 batch-90
Running loss of epoch-30 batch-90 = 5.704606883227825e-05

Training epoch-30 batch-91
Running loss of epoch-30 batch-91 = 4.1617779061198235e-05

Training epoch-30 batch-92
Running loss of epoch-30 batch-92 = 7.071136496961117e-05

Training epoch-30 batch-93
Running loss of epoch-30 batch-93 = 0.00010912865400314331

Training epoch-30 batch-94
Running loss of epoch-30 batch-94 = 2.9564369469881058e-05

Training epoch-30 batch-95
Running loss of epoch-30 batch-95 = 7.164024282246828e-05

Training epoch-30 batch-96
Running loss of epoch-30 batch-96 = 4.491768777370453e-05

Training epoch-30 batch-97
Running loss of epoch-30 batch-97 = 6.8097491748631e-05

Training epoch-30 batch-98
Running loss of epoch-30 batch-98 = 7.133523467928171e-05

Training epoch-30 batch-99
Running loss of epoch-30 batch-99 = 6.23499508947134e-05

Training epoch-30 batch-100
Running loss of epoch-30 batch-100 = 1.5848316252231598e-05

Training epoch-30 batch-101
Running loss of epoch-30 batch-101 = 7.114908657968044e-05

Training epoch-30 batch-102
Running loss of epoch-30 batch-102 = 2.3578759282827377e-05

Training epoch-30 batch-103
Running loss of epoch-30 batch-103 = 2.395804040133953e-05

Training epoch-30 batch-104
Running loss of epoch-30 batch-104 = 5.2516115829348564e-05

Training epoch-30 batch-105
Running loss of epoch-30 batch-105 = 3.982649650424719e-05

Training epoch-30 batch-106
Running loss of epoch-30 batch-106 = 6.832997314631939e-05

Training epoch-30 batch-107
Running loss of epoch-30 batch-107 = 5.003041587769985e-05

Training epoch-30 batch-108
Running loss of epoch-30 batch-108 = 3.696279600262642e-05

Training epoch-30 batch-109
Running loss of epoch-30 batch-109 = 4.431186243891716e-05

Training epoch-30 batch-110
Running loss of epoch-30 batch-110 = 0.00014320574700832367

Training epoch-30 batch-111
Running loss of epoch-30 batch-111 = 3.922870382666588e-05

Training epoch-30 batch-112
Running loss of epoch-30 batch-112 = 1.988094300031662e-05

Training epoch-30 batch-113
Running loss of epoch-30 batch-113 = 1.910468563437462e-05

Training epoch-30 batch-114
Running loss of epoch-30 batch-114 = 3.158790059387684e-05

Training epoch-30 batch-115
Running loss of epoch-30 batch-115 = 3.4332508221268654e-05

Training epoch-30 batch-116
Running loss of epoch-30 batch-116 = 1.5253666788339615e-05

Training epoch-30 batch-117
Running loss of epoch-30 batch-117 = 1.8759164959192276e-05

Training epoch-30 batch-118
Running loss of epoch-30 batch-118 = 2.2478168830275536e-05

Training epoch-30 batch-119
Running loss of epoch-30 batch-119 = 2.4896347895264626e-05

Training epoch-30 batch-120
Running loss of epoch-30 batch-120 = 3.56622040271759e-05

Training epoch-30 batch-121
Running loss of epoch-30 batch-121 = 1.0013813152909279e-05

Training epoch-30 batch-122
Running loss of epoch-30 batch-122 = 1.3703946024179459e-05

Training epoch-30 batch-123
Running loss of epoch-30 batch-123 = 4.4415006414055824e-05

Training epoch-30 batch-124
Running loss of epoch-30 batch-124 = 4.060589708387852e-05

Training epoch-30 batch-125
Running loss of epoch-30 batch-125 = 6.902986206114292e-05

Training epoch-30 batch-126
Running loss of epoch-30 batch-126 = 4.842993803322315e-05

Training epoch-30 batch-127
Running loss of epoch-30 batch-127 = 2.7907430194318295e-05

Training epoch-30 batch-128
Running loss of epoch-30 batch-128 = 1.2808479368686676e-05

Training epoch-30 batch-129
Running loss of epoch-30 batch-129 = 4.8383837565779686e-05

Training epoch-30 batch-130
Running loss of epoch-30 batch-130 = 4.354235716164112e-05

Training epoch-30 batch-131
Running loss of epoch-30 batch-131 = 5.633290857076645e-05

Training epoch-30 batch-132
Running loss of epoch-30 batch-132 = 8.069025352597237e-05

Training epoch-30 batch-133
Running loss of epoch-30 batch-133 = 2.8942828066647053e-05

Training epoch-30 batch-134
Running loss of epoch-30 batch-134 = 3.9017293602228165e-05

Training epoch-30 batch-135
Running loss of epoch-30 batch-135 = 4.306109622120857e-05

Training epoch-30 batch-136
Running loss of epoch-30 batch-136 = 2.3609725758433342e-05

Training epoch-30 batch-137
Running loss of epoch-30 batch-137 = 5.902606062591076e-05

Training epoch-30 batch-138
Running loss of epoch-30 batch-138 = 1.665297895669937e-05

Training epoch-30 batch-139
Running loss of epoch-30 batch-139 = 2.515292726457119e-05

Training epoch-30 batch-140
Running loss of epoch-30 batch-140 = 3.697327338159084e-05

Training epoch-30 batch-141
Running loss of epoch-30 batch-141 = 2.398923970758915e-05

Training epoch-30 batch-142
Running loss of epoch-30 batch-142 = 2.5847693905234337e-05

Training epoch-30 batch-143
Running loss of epoch-30 batch-143 = 2.7745962142944336e-05

Training epoch-30 batch-144
Running loss of epoch-30 batch-144 = 6.90738670527935e-05

Training epoch-30 batch-145
Running loss of epoch-30 batch-145 = 1.1606840416789055e-05

Training epoch-30 batch-146
Running loss of epoch-30 batch-146 = 1.9074417650699615e-05

Training epoch-30 batch-147
Running loss of epoch-30 batch-147 = 2.3513566702604294e-05

Training epoch-30 batch-148
Running loss of epoch-30 batch-148 = 4.91156242787838e-05

Training epoch-30 batch-149
Running loss of epoch-30 batch-149 = 0.00010603643022477627

Training epoch-30 batch-150
Running loss of epoch-30 batch-150 = 8.301367051899433e-05

Training epoch-30 batch-151
Running loss of epoch-30 batch-151 = 2.300553023815155e-05

Training epoch-30 batch-152
Running loss of epoch-30 batch-152 = 3.713928163051605e-05

Training epoch-30 batch-153
Running loss of epoch-30 batch-153 = 1.1315103620290756e-05

Training epoch-30 batch-154
Running loss of epoch-30 batch-154 = 2.0873267203569412e-05

Training epoch-30 batch-155
Running loss of epoch-30 batch-155 = 4.919455386698246e-05

Training epoch-30 batch-156
Running loss of epoch-30 batch-156 = 3.3410731703042984e-05

Training epoch-30 batch-157
Running loss of epoch-30 batch-157 = 1.9591301679611206e-05

Finished training epoch-30.



Average train loss at epoch-30 = 4.3451198935508726e-05

Started Evaluation

Average val loss at epoch-30 = 0.9404759971278892

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 92.62 %
Accuracy for class onCreate is: 92.75 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 66.67 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 50.60 %
Accuracy for class get is: 69.74 %

Overall Accuracy = 83.66 %

Finished Evaluation



Started training epoch-31


Training epoch-31 batch-1
Running loss of epoch-31 batch-1 = 3.44554428011179e-05

Training epoch-31 batch-2
Running loss of epoch-31 batch-2 = 2.9038172215223312e-05

Training epoch-31 batch-3
Running loss of epoch-31 batch-3 = 1.962226815521717e-05

Training epoch-31 batch-4
Running loss of epoch-31 batch-4 = 4.2903004214167595e-05

Training epoch-31 batch-5
Running loss of epoch-31 batch-5 = 2.901582047343254e-05

Training epoch-31 batch-6
Running loss of epoch-31 batch-6 = 2.2507039830088615e-05

Training epoch-31 batch-7
Running loss of epoch-31 batch-7 = 2.0697712898254395e-05

Training epoch-31 batch-8
Running loss of epoch-31 batch-8 = 1.3731652870774269e-05

Training epoch-31 batch-9
Running loss of epoch-31 batch-9 = 3.0095456168055534e-05

Training epoch-31 batch-10
Running loss of epoch-31 batch-10 = 1.5686731785535812e-05

Training epoch-31 batch-11
Running loss of epoch-31 batch-11 = 2.6710564270615578e-05

Training epoch-31 batch-12
Running loss of epoch-31 batch-12 = 2.4980399757623672e-05

Training epoch-31 batch-13
Running loss of epoch-31 batch-13 = 5.5467127822339535e-05

Training epoch-31 batch-14
Running loss of epoch-31 batch-14 = 1.0139774531126022e-05

Training epoch-31 batch-15
Running loss of epoch-31 batch-15 = 2.243323251605034e-05

Training epoch-31 batch-16
Running loss of epoch-31 batch-16 = 3.396160900592804e-05

Training epoch-31 batch-17
Running loss of epoch-31 batch-17 = 2.2301916033029556e-05

Training epoch-31 batch-18
Running loss of epoch-31 batch-18 = 1.5110941603779793e-05

Training epoch-31 batch-19
Running loss of epoch-31 batch-19 = 7.207063026726246e-05

Training epoch-31 batch-20
Running loss of epoch-31 batch-20 = 4.227273166179657e-05

Training epoch-31 batch-21
Running loss of epoch-31 batch-21 = 1.689232885837555e-05

Training epoch-31 batch-22
Running loss of epoch-31 batch-22 = 3.5569071769714355e-05

Training epoch-31 batch-23
Running loss of epoch-31 batch-23 = 6.458768621087074e-05

Training epoch-31 batch-24
Running loss of epoch-31 batch-24 = 2.3142434656620026e-05

Training epoch-31 batch-25
Running loss of epoch-31 batch-25 = 2.050376497209072e-05

Training epoch-31 batch-26
Running loss of epoch-31 batch-26 = 3.8907863199710846e-05

Training epoch-31 batch-27
Running loss of epoch-31 batch-27 = 1.980387605726719e-05

Training epoch-31 batch-28
Running loss of epoch-31 batch-28 = 3.942265175282955e-05

Training epoch-31 batch-29
Running loss of epoch-31 batch-29 = 3.6872923374176025e-05

Training epoch-31 batch-30
Running loss of epoch-31 batch-30 = 2.451147884130478e-05

Training epoch-31 batch-31
Running loss of epoch-31 batch-31 = 4.001811612397432e-05

Training epoch-31 batch-32
Running loss of epoch-31 batch-32 = 2.8671231120824814e-05

Training epoch-31 batch-33
Running loss of epoch-31 batch-33 = 1.9895145669579506e-05

Training epoch-31 batch-34
Running loss of epoch-31 batch-34 = 3.9762118831276894e-05

Training epoch-31 batch-35
Running loss of epoch-31 batch-35 = 2.615712583065033e-05

Training epoch-31 batch-36
Running loss of epoch-31 batch-36 = 2.1636951714754105e-05

Training epoch-31 batch-37
Running loss of epoch-31 batch-37 = 2.7771107852458954e-05

Training epoch-31 batch-38
Running loss of epoch-31 batch-38 = 4.1683437302708626e-05

Training epoch-31 batch-39
Running loss of epoch-31 batch-39 = 4.66157216578722e-05

Training epoch-31 batch-40
Running loss of epoch-31 batch-40 = 3.235740587115288e-05

Training epoch-31 batch-41
Running loss of epoch-31 batch-41 = 1.8178950995206833e-05

Training epoch-31 batch-42
Running loss of epoch-31 batch-42 = 2.6958296075463295e-05

Training epoch-31 batch-43
Running loss of epoch-31 batch-43 = 3.823288716375828e-05

Training epoch-31 batch-44
Running loss of epoch-31 batch-44 = 4.7678593546152115e-05

Training epoch-31 batch-45
Running loss of epoch-31 batch-45 = 3.553135320544243e-05

Training epoch-31 batch-46
Running loss of epoch-31 batch-46 = 4.9961963668465614e-05

Training epoch-31 batch-47
Running loss of epoch-31 batch-47 = 1.16035807877779e-05

Training epoch-31 batch-48
Running loss of epoch-31 batch-48 = 4.897569306194782e-05

Training epoch-31 batch-49
Running loss of epoch-31 batch-49 = 1.2940960004925728e-05

Training epoch-31 batch-50
Running loss of epoch-31 batch-50 = 1.9473489373922348e-05

Training epoch-31 batch-51
Running loss of epoch-31 batch-51 = 1.9687926396727562e-05

Training epoch-31 batch-52
Running loss of epoch-31 batch-52 = 2.2656400687992573e-05

Training epoch-31 batch-53
Running loss of epoch-31 batch-53 = 3.192434087395668e-05

Training epoch-31 batch-54
Running loss of epoch-31 batch-54 = 6.676116026937962e-05

Training epoch-31 batch-55
Running loss of epoch-31 batch-55 = 3.12337651848793e-05

Training epoch-31 batch-56
Running loss of epoch-31 batch-56 = 1.9862549379467964e-05

Training epoch-31 batch-57
Running loss of epoch-31 batch-57 = 1.4930963516235352e-05

Training epoch-31 batch-58
Running loss of epoch-31 batch-58 = 3.2886629924178123e-05

Training epoch-31 batch-59
Running loss of epoch-31 batch-59 = 6.389128975570202e-05

Training epoch-31 batch-60
Running loss of epoch-31 batch-60 = 2.921675331890583e-05

Training epoch-31 batch-61
Running loss of epoch-31 batch-61 = 2.4218345060944557e-05

Training epoch-31 batch-62
Running loss of epoch-31 batch-62 = 2.24909745156765e-05

Training epoch-31 batch-63
Running loss of epoch-31 batch-63 = 9.96328890323639e-06

Training epoch-31 batch-64
Running loss of epoch-31 batch-64 = 3.138801548629999e-05

Training epoch-31 batch-65
Running loss of epoch-31 batch-65 = 2.6593683287501335e-05

Training epoch-31 batch-66
Running loss of epoch-31 batch-66 = 8.57468694448471e-06

Training epoch-31 batch-67
Running loss of epoch-31 batch-67 = 3.2359734177589417e-05

Training epoch-31 batch-68
Running loss of epoch-31 batch-68 = 4.256784450262785e-05

Training epoch-31 batch-69
Running loss of epoch-31 batch-69 = 2.3008324205875397e-05

Training epoch-31 batch-70
Running loss of epoch-31 batch-70 = 3.201537765562534e-05

Training epoch-31 batch-71
Running loss of epoch-31 batch-71 = 6.472179666161537e-05

Training epoch-31 batch-72
Running loss of epoch-31 batch-72 = 4.449603147804737e-05

Training epoch-31 batch-73
Running loss of epoch-31 batch-73 = 2.096523530781269e-05

Training epoch-31 batch-74
Running loss of epoch-31 batch-74 = 3.2069627195596695e-05

Training epoch-31 batch-75
Running loss of epoch-31 batch-75 = 4.2978907003998756e-05

Training epoch-31 batch-76
Running loss of epoch-31 batch-76 = 2.8410926461219788e-05

Training epoch-31 batch-77
Running loss of epoch-31 batch-77 = 2.845027483999729e-05

Training epoch-31 batch-78
Running loss of epoch-31 batch-78 = 3.3103395253419876e-05

Training epoch-31 batch-79
Running loss of epoch-31 batch-79 = 1.6354722902178764e-05

Training epoch-31 batch-80
Running loss of epoch-31 batch-80 = 4.201708361506462e-05

Training epoch-31 batch-81
Running loss of epoch-31 batch-81 = 3.3212825655937195e-05

Training epoch-31 batch-82
Running loss of epoch-31 batch-82 = 2.13750172406435e-05

Training epoch-31 batch-83
Running loss of epoch-31 batch-83 = 3.5847071558237076e-05

Training epoch-31 batch-84
Running loss of epoch-31 batch-84 = 2.128584310412407e-05

Training epoch-31 batch-85
Running loss of epoch-31 batch-85 = 1.2378208339214325e-05

Training epoch-31 batch-86
Running loss of epoch-31 batch-86 = 0.00012137903831899166

Training epoch-31 batch-87
Running loss of epoch-31 batch-87 = 6.542890332639217e-05

Training epoch-31 batch-88
Running loss of epoch-31 batch-88 = 3.333762288093567e-05

Training epoch-31 batch-89
Running loss of epoch-31 batch-89 = 3.891647793352604e-05

Training epoch-31 batch-90
Running loss of epoch-31 batch-90 = 0.00016637658700346947

Training epoch-31 batch-91
Running loss of epoch-31 batch-91 = 4.7399778850376606e-05

Training epoch-31 batch-92
Running loss of epoch-31 batch-92 = 2.6645371690392494e-05

Training epoch-31 batch-93
Running loss of epoch-31 batch-93 = 2.6624184101819992e-05

Training epoch-31 batch-94
Running loss of epoch-31 batch-94 = 5.6928955018520355e-05

Training epoch-31 batch-95
Running loss of epoch-31 batch-95 = 3.076321445405483e-05

Training epoch-31 batch-96
Running loss of epoch-31 batch-96 = 0.00017270271200686693

Training epoch-31 batch-97
Running loss of epoch-31 batch-97 = 5.7950615882873535e-05

Training epoch-31 batch-98
Running loss of epoch-31 batch-98 = 1.4266232028603554e-05

Training epoch-31 batch-99
Running loss of epoch-31 batch-99 = 5.002180114388466e-05

Training epoch-31 batch-100
Running loss of epoch-31 batch-100 = 3.475463017821312e-05

Training epoch-31 batch-101
Running loss of epoch-31 batch-101 = 0.0001158365048468113

Training epoch-31 batch-102
Running loss of epoch-31 batch-102 = 4.564225673675537e-05

Training epoch-31 batch-103
Running loss of epoch-31 batch-103 = 2.904771827161312e-05

Training epoch-31 batch-104
Running loss of epoch-31 batch-104 = 1.8653925508260727e-05

Training epoch-31 batch-105
Running loss of epoch-31 batch-105 = 4.983250983059406e-05

Training epoch-31 batch-106
Running loss of epoch-31 batch-106 = 3.877433482557535e-05

Training epoch-31 batch-107
Running loss of epoch-31 batch-107 = 4.195910878479481e-05

Training epoch-31 batch-108
Running loss of epoch-31 batch-108 = 5.7743629440665245e-05

Training epoch-31 batch-109
Running loss of epoch-31 batch-109 = 2.5538727641105652e-05

Training epoch-31 batch-110
Running loss of epoch-31 batch-110 = 3.415648825466633e-05

Training epoch-31 batch-111
Running loss of epoch-31 batch-111 = 4.909164272248745e-05

Training epoch-31 batch-112
Running loss of epoch-31 batch-112 = 1.8574297428131104e-05

Training epoch-31 batch-113
Running loss of epoch-31 batch-113 = 5.2711693570017815e-05

Training epoch-31 batch-114
Running loss of epoch-31 batch-114 = 6.31858129054308e-05

Training epoch-31 batch-115
Running loss of epoch-31 batch-115 = 3.4271273761987686e-05

Training epoch-31 batch-116
Running loss of epoch-31 batch-116 = 2.4817651137709618e-05

Training epoch-31 batch-117
Running loss of epoch-31 batch-117 = 3.480399027466774e-05

Training epoch-31 batch-118
Running loss of epoch-31 batch-118 = 3.860797733068466e-05

Training epoch-31 batch-119
Running loss of epoch-31 batch-119 = 5.749892443418503e-05

Training epoch-31 batch-120
Running loss of epoch-31 batch-120 = 6.198277696967125e-05

Training epoch-31 batch-121
Running loss of epoch-31 batch-121 = 2.5803223252296448e-05

Training epoch-31 batch-122
Running loss of epoch-31 batch-122 = 4.71633393317461e-05

Training epoch-31 batch-123
Running loss of epoch-31 batch-123 = 2.9742252081632614e-05

Training epoch-31 batch-124
Running loss of epoch-31 batch-124 = 5.477340891957283e-05

Training epoch-31 batch-125
Running loss of epoch-31 batch-125 = 3.8249650970101357e-05

Training epoch-31 batch-126
Running loss of epoch-31 batch-126 = 3.406452015042305e-05

Training epoch-31 batch-127
Running loss of epoch-31 batch-127 = 1.618475653231144e-05

Training epoch-31 batch-128
Running loss of epoch-31 batch-128 = 2.963608130812645e-05

Training epoch-31 batch-129
Running loss of epoch-31 batch-129 = 3.336183726787567e-05

Training epoch-31 batch-130
Running loss of epoch-31 batch-130 = 3.613857552409172e-05

Training epoch-31 batch-131
Running loss of epoch-31 batch-131 = 0.00012146309018135071

Training epoch-31 batch-132
Running loss of epoch-31 batch-132 = 3.553042188286781e-05

Training epoch-31 batch-133
Running loss of epoch-31 batch-133 = 1.8437625840306282e-05

Training epoch-31 batch-134
Running loss of epoch-31 batch-134 = 3.158603794872761e-05

Training epoch-31 batch-135
Running loss of epoch-31 batch-135 = 3.8916245102882385e-05

Training epoch-31 batch-136
Running loss of epoch-31 batch-136 = 2.958253026008606e-05

Training epoch-31 batch-137
Running loss of epoch-31 batch-137 = 4.109647125005722e-05

Training epoch-31 batch-138
Running loss of epoch-31 batch-138 = 1.6933539882302284e-05

Training epoch-31 batch-139
Running loss of epoch-31 batch-139 = 3.417162224650383e-05

Training epoch-31 batch-140
Running loss of epoch-31 batch-140 = 5.370471626520157e-05

Training epoch-31 batch-141
Running loss of epoch-31 batch-141 = 2.5910791009664536e-05

Training epoch-31 batch-142
Running loss of epoch-31 batch-142 = 3.647548146545887e-05

Training epoch-31 batch-143
Running loss of epoch-31 batch-143 = 8.140713907778263e-05

Training epoch-31 batch-144
Running loss of epoch-31 batch-144 = 3.0262162908911705e-05

Training epoch-31 batch-145
Running loss of epoch-31 batch-145 = 4.252151120454073e-05

Training epoch-31 batch-146
Running loss of epoch-31 batch-146 = 4.838849417865276e-05

Training epoch-31 batch-147
Running loss of epoch-31 batch-147 = 3.800913691520691e-05

Training epoch-31 batch-148
Running loss of epoch-31 batch-148 = 3.0315713956952095e-05

Training epoch-31 batch-149
Running loss of epoch-31 batch-149 = 3.355229273438454e-05

Training epoch-31 batch-150
Running loss of epoch-31 batch-150 = 3.1771138310432434e-05

Training epoch-31 batch-151
Running loss of epoch-31 batch-151 = 4.5168446376919746e-05

Training epoch-31 batch-152
Running loss of epoch-31 batch-152 = 4.9901194870471954e-05

Training epoch-31 batch-153
Running loss of epoch-31 batch-153 = 1.7561251297593117e-05

Training epoch-31 batch-154
Running loss of epoch-31 batch-154 = 5.4024625569581985e-05

Training epoch-31 batch-155
Running loss of epoch-31 batch-155 = 1.792795956134796e-05

Training epoch-31 batch-156
Running loss of epoch-31 batch-156 = 5.318236071616411e-05

Training epoch-31 batch-157
Running loss of epoch-31 batch-157 = 7.434561848640442e-05

Finished training epoch-31.



Average train loss at epoch-31 = 3.749857842922211e-05

Started Evaluation

Average val loss at epoch-31 = 0.9507275294481827

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.64 %
Accuracy for class onCreate is: 92.75 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 67.58 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 48.19 %
Accuracy for class get is: 72.82 %

Overall Accuracy = 83.83 %

Finished Evaluation



Started training epoch-32


Training epoch-32 batch-1
Running loss of epoch-32 batch-1 = 1.5100231394171715e-05

Training epoch-32 batch-2
Running loss of epoch-32 batch-2 = 4.329485818743706e-05

Training epoch-32 batch-3
Running loss of epoch-32 batch-3 = 3.716675564646721e-05

Training epoch-32 batch-4
Running loss of epoch-32 batch-4 = 4.95037529617548e-05

Training epoch-32 batch-5
Running loss of epoch-32 batch-5 = 1.2939563021063805e-05

Training epoch-32 batch-6
Running loss of epoch-32 batch-6 = 4.436774179339409e-05

Training epoch-32 batch-7
Running loss of epoch-32 batch-7 = 3.973767161369324e-05

Training epoch-32 batch-8
Running loss of epoch-32 batch-8 = 3.13569325953722e-05

Training epoch-32 batch-9
Running loss of epoch-32 batch-9 = 3.297952935099602e-05

Training epoch-32 batch-10
Running loss of epoch-32 batch-10 = 1.4727702364325523e-05

Training epoch-32 batch-11
Running loss of epoch-32 batch-11 = 4.7137727960944176e-05

Training epoch-32 batch-12
Running loss of epoch-32 batch-12 = 2.0208070054650307e-05

Training epoch-32 batch-13
Running loss of epoch-32 batch-13 = 2.0153122022747993e-05

Training epoch-32 batch-14
Running loss of epoch-32 batch-14 = 3.5996316000819206e-05

Training epoch-32 batch-15
Running loss of epoch-32 batch-15 = 4.93896659463644e-05

Training epoch-32 batch-16
Running loss of epoch-32 batch-16 = 2.330145798623562e-05

Training epoch-32 batch-17
Running loss of epoch-32 batch-17 = 2.047722227871418e-05

Training epoch-32 batch-18
Running loss of epoch-32 batch-18 = 1.9237399101257324e-05

Training epoch-32 batch-19
Running loss of epoch-32 batch-19 = 3.8400059565901756e-05

Training epoch-32 batch-20
Running loss of epoch-32 batch-20 = 2.4845125153660774e-05

Training epoch-32 batch-21
Running loss of epoch-32 batch-21 = 1.2610340490937233e-05

Training epoch-32 batch-22
Running loss of epoch-32 batch-22 = 1.589232124388218e-05

Training epoch-32 batch-23
Running loss of epoch-32 batch-23 = 6.472691893577576e-06

Training epoch-32 batch-24
Running loss of epoch-32 batch-24 = 4.469091072678566e-05

Training epoch-32 batch-25
Running loss of epoch-32 batch-25 = 3.0209310352802277e-05

Training epoch-32 batch-26
Running loss of epoch-32 batch-26 = 3.719329833984375e-05

Training epoch-32 batch-27
Running loss of epoch-32 batch-27 = 1.8225517123937607e-05

Training epoch-32 batch-28
Running loss of epoch-32 batch-28 = 2.1857209503650665e-05

Training epoch-32 batch-29
Running loss of epoch-32 batch-29 = 2.1360348910093307e-05

Training epoch-32 batch-30
Running loss of epoch-32 batch-30 = 4.62816096842289e-05

Training epoch-32 batch-31
Running loss of epoch-32 batch-31 = 2.363184466958046e-05

Training epoch-32 batch-32
Running loss of epoch-32 batch-32 = 4.789489321410656e-05

Training epoch-32 batch-33
Running loss of epoch-32 batch-33 = 7.737614214420319e-05

Training epoch-32 batch-34
Running loss of epoch-32 batch-34 = 2.1594460122287273e-05

Training epoch-32 batch-35
Running loss of epoch-32 batch-35 = 5.589751526713371e-05

Training epoch-32 batch-36
Running loss of epoch-32 batch-36 = 1.4581484720110893e-05

Training epoch-32 batch-37
Running loss of epoch-32 batch-37 = 5.0499336794018745e-05

Training epoch-32 batch-38
Running loss of epoch-32 batch-38 = 3.068358637392521e-05

Training epoch-32 batch-39
Running loss of epoch-32 batch-39 = 2.0642997696995735e-05

Training epoch-32 batch-40
Running loss of epoch-32 batch-40 = 4.27768100053072e-05

Training epoch-32 batch-41
Running loss of epoch-32 batch-41 = 5.34537248313427e-05

Training epoch-32 batch-42
Running loss of epoch-32 batch-42 = 3.6393292248249054e-05

Training epoch-32 batch-43
Running loss of epoch-32 batch-43 = 3.0503375455737114e-05

Training epoch-32 batch-44
Running loss of epoch-32 batch-44 = 2.361251972615719e-05

Training epoch-32 batch-45
Running loss of epoch-32 batch-45 = 3.019697032868862e-05

Training epoch-32 batch-46
Running loss of epoch-32 batch-46 = 3.7479912862181664e-05

Training epoch-32 batch-47
Running loss of epoch-32 batch-47 = 3.5710399970412254e-05

Training epoch-32 batch-48
Running loss of epoch-32 batch-48 = 1.623295247554779e-05

Training epoch-32 batch-49
Running loss of epoch-32 batch-49 = 3.0050519853830338e-05

Training epoch-32 batch-50
Running loss of epoch-32 batch-50 = 2.1065818145871162e-05

Training epoch-32 batch-51
Running loss of epoch-32 batch-51 = 2.600252628326416e-05

Training epoch-32 batch-52
Running loss of epoch-32 batch-52 = 3.358209505677223e-05

Training epoch-32 batch-53
Running loss of epoch-32 batch-53 = 4.90616075694561e-05

Training epoch-32 batch-54
Running loss of epoch-32 batch-54 = 5.075428634881973e-05

Training epoch-32 batch-55
Running loss of epoch-32 batch-55 = 3.2429350540041924e-05

Training epoch-32 batch-56
Running loss of epoch-32 batch-56 = 0.00010168121661990881

Training epoch-32 batch-57
Running loss of epoch-32 batch-57 = 2.864515408873558e-05

Training epoch-32 batch-58
Running loss of epoch-32 batch-58 = 2.555013634264469e-05

Training epoch-32 batch-59
Running loss of epoch-32 batch-59 = 4.3896259739995e-05

Training epoch-32 batch-60
Running loss of epoch-32 batch-60 = 1.5864381566643715e-05

Training epoch-32 batch-61
Running loss of epoch-32 batch-61 = 4.470977000892162e-05

Training epoch-32 batch-62
Running loss of epoch-32 batch-62 = 3.53821087628603e-05

Training epoch-32 batch-63
Running loss of epoch-32 batch-63 = 1.946999691426754e-05

Training epoch-32 batch-64
Running loss of epoch-32 batch-64 = 7.853377610445023e-05

Training epoch-32 batch-65
Running loss of epoch-32 batch-65 = 7.067108526825905e-06

Training epoch-32 batch-66
Running loss of epoch-32 batch-66 = 1.1068535968661308e-05

Training epoch-32 batch-67
Running loss of epoch-32 batch-67 = 2.121366560459137e-05

Training epoch-32 batch-68
Running loss of epoch-32 batch-68 = 4.0421728044748306e-05

Training epoch-32 batch-69
Running loss of epoch-32 batch-69 = 1.8818536773324013e-05

Training epoch-32 batch-70
Running loss of epoch-32 batch-70 = 1.731421798467636e-05

Training epoch-32 batch-71
Running loss of epoch-32 batch-71 = 3.6886194720864296e-05

Training epoch-32 batch-72
Running loss of epoch-32 batch-72 = 2.099340781569481e-05

Training epoch-32 batch-73
Running loss of epoch-32 batch-73 = 2.0486535504460335e-05

Training epoch-32 batch-74
Running loss of epoch-32 batch-74 = 2.2250693291425705e-05

Training epoch-32 batch-75
Running loss of epoch-32 batch-75 = 1.7972197383642197e-05

Training epoch-32 batch-76
Running loss of epoch-32 batch-76 = 3.307545557618141e-05

Training epoch-32 batch-77
Running loss of epoch-32 batch-77 = 3.0196039006114006e-05

Training epoch-32 batch-78
Running loss of epoch-32 batch-78 = 3.3611664548516273e-05

Training epoch-32 batch-79
Running loss of epoch-32 batch-79 = 7.9302117228508e-06

Training epoch-32 batch-80
Running loss of epoch-32 batch-80 = 3.821682184934616e-05

Training epoch-32 batch-81
Running loss of epoch-32 batch-81 = 4.178972449153662e-05

Training epoch-32 batch-82
Running loss of epoch-32 batch-82 = 3.7370482459664345e-05

Training epoch-32 batch-83
Running loss of epoch-32 batch-83 = 3.765826113522053e-05

Training epoch-32 batch-84
Running loss of epoch-32 batch-84 = 2.394593320786953e-05

Training epoch-32 batch-85
Running loss of epoch-32 batch-85 = 1.2680655345320702e-05

Training epoch-32 batch-86
Running loss of epoch-32 batch-86 = 1.2827105820178986e-05

Training epoch-32 batch-87
Running loss of epoch-32 batch-87 = 1.550675369799137e-05

Training epoch-32 batch-88
Running loss of epoch-32 batch-88 = 3.523798659443855e-05

Training epoch-32 batch-89
Running loss of epoch-32 batch-89 = 4.954426549375057e-05

Training epoch-32 batch-90
Running loss of epoch-32 batch-90 = 3.6206794902682304e-05

Training epoch-32 batch-91
Running loss of epoch-32 batch-91 = 1.3006152585148811e-05

Training epoch-32 batch-92
Running loss of epoch-32 batch-92 = 2.6543159037828445e-05

Training epoch-32 batch-93
Running loss of epoch-32 batch-93 = 0.00013792782556265593

Training epoch-32 batch-94
Running loss of epoch-32 batch-94 = 8.628936484456062e-06

Training epoch-32 batch-95
Running loss of epoch-32 batch-95 = 3.064493648707867e-05

Training epoch-32 batch-96
Running loss of epoch-32 batch-96 = 2.9025599360466003e-05

Training epoch-32 batch-97
Running loss of epoch-32 batch-97 = 3.5988050512969494e-05

Training epoch-32 batch-98
Running loss of epoch-32 batch-98 = 4.722224548459053e-05

Training epoch-32 batch-99
Running loss of epoch-32 batch-99 = 2.1505402401089668e-05

Training epoch-32 batch-100
Running loss of epoch-32 batch-100 = 1.2257369235157967e-05

Training epoch-32 batch-101
Running loss of epoch-32 batch-101 = 1.3017794117331505e-05

Training epoch-32 batch-102
Running loss of epoch-32 batch-102 = 3.646570257842541e-05

Training epoch-32 batch-103
Running loss of epoch-32 batch-103 = 6.865244358778e-05

Training epoch-32 batch-104
Running loss of epoch-32 batch-104 = 2.775038592517376e-05

Training epoch-32 batch-105
Running loss of epoch-32 batch-105 = 4.960247315466404e-05

Training epoch-32 batch-106
Running loss of epoch-32 batch-106 = 7.388647645711899e-05

Training epoch-32 batch-107
Running loss of epoch-32 batch-107 = 7.315212860703468e-05

Training epoch-32 batch-108
Running loss of epoch-32 batch-108 = 8.143461309373379e-05

Training epoch-32 batch-109
Running loss of epoch-32 batch-109 = 4.099938087165356e-05

Training epoch-32 batch-110
Running loss of epoch-32 batch-110 = 4.151742905378342e-05

Training epoch-32 batch-111
Running loss of epoch-32 batch-111 = 4.2309053242206573e-05

Training epoch-32 batch-112
Running loss of epoch-32 batch-112 = 2.5957589969038963e-05

Training epoch-32 batch-113
Running loss of epoch-32 batch-113 = 2.2836728021502495e-05

Training epoch-32 batch-114
Running loss of epoch-32 batch-114 = 4.419987089931965e-05

Training epoch-32 batch-115
Running loss of epoch-32 batch-115 = 7.168296724557877e-05

Training epoch-32 batch-116
Running loss of epoch-32 batch-116 = 3.0623399652540684e-05

Training epoch-32 batch-117
Running loss of epoch-32 batch-117 = 1.5798723325133324e-05

Training epoch-32 batch-118
Running loss of epoch-32 batch-118 = 5.314592272043228e-05

Training epoch-32 batch-119
Running loss of epoch-32 batch-119 = 2.7031870558857918e-05

Training epoch-32 batch-120
Running loss of epoch-32 batch-120 = 3.7026358768343925e-05

Training epoch-32 batch-121
Running loss of epoch-32 batch-121 = 3.758864477276802e-05

Training epoch-32 batch-122
Running loss of epoch-32 batch-122 = 1.2092990800738335e-05

Training epoch-32 batch-123
Running loss of epoch-32 batch-123 = 5.519087426364422e-05

Training epoch-32 batch-124
Running loss of epoch-32 batch-124 = 4.2167725041508675e-05

Training epoch-32 batch-125
Running loss of epoch-32 batch-125 = 3.431295044720173e-05

Training epoch-32 batch-126
Running loss of epoch-32 batch-126 = 3.409944474697113e-05

Training epoch-32 batch-127
Running loss of epoch-32 batch-127 = 5.17421867698431e-05

Training epoch-32 batch-128
Running loss of epoch-32 batch-128 = 4.048692062497139e-05

Training epoch-32 batch-129
Running loss of epoch-32 batch-129 = 9.430572390556335e-06

Training epoch-32 batch-130
Running loss of epoch-32 batch-130 = 5.394476465880871e-05

Training epoch-32 batch-131
Running loss of epoch-32 batch-131 = 4.205130971968174e-05

Training epoch-32 batch-132
Running loss of epoch-32 batch-132 = 9.321141988039017e-06

Training epoch-32 batch-133
Running loss of epoch-32 batch-133 = 2.1283747628331184e-05

Training epoch-32 batch-134
Running loss of epoch-32 batch-134 = 1.429906114935875e-05

Training epoch-32 batch-135
Running loss of epoch-32 batch-135 = 4.472280852496624e-05

Training epoch-32 batch-136
Running loss of epoch-32 batch-136 = 1.4999881386756897e-05

Training epoch-32 batch-137
Running loss of epoch-32 batch-137 = 2.7925241738557816e-05

Training epoch-32 batch-138
Running loss of epoch-32 batch-138 = 0.0001745580229908228

Training epoch-32 batch-139
Running loss of epoch-32 batch-139 = 3.7922291085124016e-05

Training epoch-32 batch-140
Running loss of epoch-32 batch-140 = 3.304635174572468e-05

Training epoch-32 batch-141
Running loss of epoch-32 batch-141 = 3.0024908483028412e-05

Training epoch-32 batch-142
Running loss of epoch-32 batch-142 = 2.995284739881754e-05

Training epoch-32 batch-143
Running loss of epoch-32 batch-143 = 2.754153683781624e-05

Training epoch-32 batch-144
Running loss of epoch-32 batch-144 = 3.59478872269392e-05

Training epoch-32 batch-145
Running loss of epoch-32 batch-145 = 1.8672319129109383e-05

Training epoch-32 batch-146
Running loss of epoch-32 batch-146 = 2.2063730284571648e-05

Training epoch-32 batch-147
Running loss of epoch-32 batch-147 = 2.4504028260707855e-05

Training epoch-32 batch-148
Running loss of epoch-32 batch-148 = 7.264222949743271e-05

Training epoch-32 batch-149
Running loss of epoch-32 batch-149 = 2.8116395696997643e-05

Training epoch-32 batch-150
Running loss of epoch-32 batch-150 = 2.0248815417289734e-05

Training epoch-32 batch-151
Running loss of epoch-32 batch-151 = 2.1068844944238663e-05

Training epoch-32 batch-152
Running loss of epoch-32 batch-152 = 4.810898099094629e-05

Training epoch-32 batch-153
Running loss of epoch-32 batch-153 = 3.017531707882881e-05

Training epoch-32 batch-154
Running loss of epoch-32 batch-154 = 1.4756107702851295e-05

Training epoch-32 batch-155
Running loss of epoch-32 batch-155 = 3.8343481719493866e-05

Training epoch-32 batch-156
Running loss of epoch-32 batch-156 = 4.0211714804172516e-05

Training epoch-32 batch-157
Running loss of epoch-32 batch-157 = 3.3777207136154175e-05

Finished training epoch-32.



Average train loss at epoch-32 = 3.445182740688324e-05

Started Evaluation

Average val loss at epoch-32 = 0.9480010594459504

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 91.97 %
Accuracy for class onCreate is: 91.36 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 66.67 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 57.40 %
Accuracy for class execute is: 46.18 %
Accuracy for class get is: 73.59 %

Overall Accuracy = 83.76 %

Finished Evaluation



Started training epoch-33


Training epoch-33 batch-1
Running loss of epoch-33 batch-1 = 1.8843216821551323e-05

Training epoch-33 batch-2
Running loss of epoch-33 batch-2 = 7.87968747317791e-06

Training epoch-33 batch-3
Running loss of epoch-33 batch-3 = 4.2926985770463943e-05

Training epoch-33 batch-4
Running loss of epoch-33 batch-4 = 8.609145879745483e-06

Training epoch-33 batch-5
Running loss of epoch-33 batch-5 = 1.9999220967292786e-05

Training epoch-33 batch-6
Running loss of epoch-33 batch-6 = 4.209298640489578e-05

Training epoch-33 batch-7
Running loss of epoch-33 batch-7 = 5.349353887140751e-05

Training epoch-33 batch-8
Running loss of epoch-33 batch-8 = 1.8593156710267067e-05

Training epoch-33 batch-9
Running loss of epoch-33 batch-9 = 1.835380680859089e-05

Training epoch-33 batch-10
Running loss of epoch-33 batch-10 = 2.2582709789276123e-05

Training epoch-33 batch-11
Running loss of epoch-33 batch-11 = 2.047303132712841e-05

Training epoch-33 batch-12
Running loss of epoch-33 batch-12 = 1.973891630768776e-05

Training epoch-33 batch-13
Running loss of epoch-33 batch-13 = 2.5199493393301964e-05

Training epoch-33 batch-14
Running loss of epoch-33 batch-14 = 3.603461664170027e-05

Training epoch-33 batch-15
Running loss of epoch-33 batch-15 = 1.0663177818059921e-05

Training epoch-33 batch-16
Running loss of epoch-33 batch-16 = 1.566787250339985e-05

Training epoch-33 batch-17
Running loss of epoch-33 batch-17 = 2.45517585426569e-05

Training epoch-33 batch-18
Running loss of epoch-33 batch-18 = 2.1812738850712776e-05

Training epoch-33 batch-19
Running loss of epoch-33 batch-19 = 2.9395101591944695e-05

Training epoch-33 batch-20
Running loss of epoch-33 batch-20 = 1.586764119565487e-05

Training epoch-33 batch-21
Running loss of epoch-33 batch-21 = 2.4176668375730515e-05

Training epoch-33 batch-22
Running loss of epoch-33 batch-22 = 1.5483703464269638e-05

Training epoch-33 batch-23
Running loss of epoch-33 batch-23 = 7.48706515878439e-05

Training epoch-33 batch-24
Running loss of epoch-33 batch-24 = 2.4147098883986473e-05

Training epoch-33 batch-25
Running loss of epoch-33 batch-25 = 2.2810185328125954e-05

Training epoch-33 batch-26
Running loss of epoch-33 batch-26 = 2.961675636470318e-05

Training epoch-33 batch-27
Running loss of epoch-33 batch-27 = 3.6451732739806175e-05

Training epoch-33 batch-28
Running loss of epoch-33 batch-28 = 2.166023477911949e-05

Training epoch-33 batch-29
Running loss of epoch-33 batch-29 = 1.4550983905792236e-05

Training epoch-33 batch-30
Running loss of epoch-33 batch-30 = 3.170105628669262e-05

Training epoch-33 batch-31
Running loss of epoch-33 batch-31 = 1.757405698299408e-05

Training epoch-33 batch-32
Running loss of epoch-33 batch-32 = 1.2273900210857391e-05

Training epoch-33 batch-33
Running loss of epoch-33 batch-33 = 3.5020289942622185e-05

Training epoch-33 batch-34
Running loss of epoch-33 batch-34 = 4.406203515827656e-05

Training epoch-33 batch-35
Running loss of epoch-33 batch-35 = 1.7342856153845787e-05

Training epoch-33 batch-36
Running loss of epoch-33 batch-36 = 9.989365935325623e-06

Training epoch-33 batch-37
Running loss of epoch-33 batch-37 = 5.4798321798443794e-05

Training epoch-33 batch-38
Running loss of epoch-33 batch-38 = 2.9205111786723137e-05

Training epoch-33 batch-39
Running loss of epoch-33 batch-39 = 1.6183359548449516e-05

Training epoch-33 batch-40
Running loss of epoch-33 batch-40 = 5.9497542679309845e-05

Training epoch-33 batch-41
Running loss of epoch-33 batch-41 = 3.604055382311344e-05

Training epoch-33 batch-42
Running loss of epoch-33 batch-42 = 2.5472836568951607e-05

Training epoch-33 batch-43
Running loss of epoch-33 batch-43 = 1.1805444955825806e-05

Training epoch-33 batch-44
Running loss of epoch-33 batch-44 = 8.852453902363777e-06

Training epoch-33 batch-45
Running loss of epoch-33 batch-45 = 5.7506957091391087e-05

Training epoch-33 batch-46
Running loss of epoch-33 batch-46 = 3.0102208256721497e-05

Training epoch-33 batch-47
Running loss of epoch-33 batch-47 = 2.389354631304741e-05

Training epoch-33 batch-48
Running loss of epoch-33 batch-48 = 9.86107625067234e-06

Training epoch-33 batch-49
Running loss of epoch-33 batch-49 = 2.932013012468815e-05

Training epoch-33 batch-50
Running loss of epoch-33 batch-50 = 2.0820414647459984e-05

Training epoch-33 batch-51
Running loss of epoch-33 batch-51 = 1.205364242196083e-05

Training epoch-33 batch-52
Running loss of epoch-33 batch-52 = 1.9237399101257324e-05

Training epoch-33 batch-53
Running loss of epoch-33 batch-53 = 1.894054003059864e-05

Training epoch-33 batch-54
Running loss of epoch-33 batch-54 = 2.976972609758377e-05

Training epoch-33 batch-55
Running loss of epoch-33 batch-55 = 2.824561670422554e-05

Training epoch-33 batch-56
Running loss of epoch-33 batch-56 = 5.120714195072651e-05

Training epoch-33 batch-57
Running loss of epoch-33 batch-57 = 1.6106758266687393e-05

Training epoch-33 batch-58
Running loss of epoch-33 batch-58 = 1.8459279090166092e-05

Training epoch-33 batch-59
Running loss of epoch-33 batch-59 = 8.763279765844345e-06

Training epoch-33 batch-60
Running loss of epoch-33 batch-60 = 1.3302895240485668e-05

Training epoch-33 batch-61
Running loss of epoch-33 batch-61 = 2.4419743567705154e-05

Training epoch-33 batch-62
Running loss of epoch-33 batch-62 = 1.3392418622970581e-05

Training epoch-33 batch-63
Running loss of epoch-33 batch-63 = 1.3265060260891914e-05

Training epoch-33 batch-64
Running loss of epoch-33 batch-64 = 2.011796459555626e-05

Training epoch-33 batch-65
Running loss of epoch-33 batch-65 = 2.3341737687587738e-05

Training epoch-33 batch-66
Running loss of epoch-33 batch-66 = 2.5954563170671463e-05

Training epoch-33 batch-67
Running loss of epoch-33 batch-67 = 9.79541800916195e-06

Training epoch-33 batch-68
Running loss of epoch-33 batch-68 = 2.410542219877243e-05

Training epoch-33 batch-69
Running loss of epoch-33 batch-69 = 3.575906157493591e-05

Training epoch-33 batch-70
Running loss of epoch-33 batch-70 = 5.7096127420663834e-05

Training epoch-33 batch-71
Running loss of epoch-33 batch-71 = 4.5307911932468414e-05

Training epoch-33 batch-72
Running loss of epoch-33 batch-72 = 2.926238812506199e-05

Training epoch-33 batch-73
Running loss of epoch-33 batch-73 = 2.6127323508262634e-05

Training epoch-33 batch-74
Running loss of epoch-33 batch-74 = 1.345016062259674e-05

Training epoch-33 batch-75
Running loss of epoch-33 batch-75 = 3.458361607044935e-05

Training epoch-33 batch-76
Running loss of epoch-33 batch-76 = 2.034055069088936e-05

Training epoch-33 batch-77
Running loss of epoch-33 batch-77 = 4.387786611914635e-05

Training epoch-33 batch-78
Running loss of epoch-33 batch-78 = 1.589185558259487e-05

Training epoch-33 batch-79
Running loss of epoch-33 batch-79 = 7.852190174162388e-05

Training epoch-33 batch-80
Running loss of epoch-33 batch-80 = 4.222407005727291e-05

Training epoch-33 batch-81
Running loss of epoch-33 batch-81 = 2.0644860342144966e-05

Training epoch-33 batch-82
Running loss of epoch-33 batch-82 = 3.10682225972414e-05

Training epoch-33 batch-83
Running loss of epoch-33 batch-83 = 3.368256147950888e-05

Training epoch-33 batch-84
Running loss of epoch-33 batch-84 = 4.018470644950867e-05

Training epoch-33 batch-85
Running loss of epoch-33 batch-85 = 2.4806242436170578e-05

Training epoch-33 batch-86
Running loss of epoch-33 batch-86 = 3.189779818058014e-05

Training epoch-33 batch-87
Running loss of epoch-33 batch-87 = 2.368004061281681e-05

Training epoch-33 batch-88
Running loss of epoch-33 batch-88 = 6.489246152341366e-05

Training epoch-33 batch-89
Running loss of epoch-33 batch-89 = 3.13417986035347e-05

Training epoch-33 batch-90
Running loss of epoch-33 batch-90 = 3.7746736779809e-05

Training epoch-33 batch-91
Running loss of epoch-33 batch-91 = 5.769706331193447e-05

Training epoch-33 batch-92
Running loss of epoch-33 batch-92 = 2.9972288757562637e-05

Training epoch-33 batch-93
Running loss of epoch-33 batch-93 = 3.477348946034908e-05

Training epoch-33 batch-94
Running loss of epoch-33 batch-94 = 2.7540139853954315e-05

Training epoch-33 batch-95
Running loss of epoch-33 batch-95 = 2.117338590323925e-05

Training epoch-33 batch-96
Running loss of epoch-33 batch-96 = 2.169632352888584e-05

Training epoch-33 batch-97
Running loss of epoch-33 batch-97 = 4.1022198274731636e-05

Training epoch-33 batch-98
Running loss of epoch-33 batch-98 = 2.2377818822860718e-05

Training epoch-33 batch-99
Running loss of epoch-33 batch-99 = 1.3622688129544258e-05

Training epoch-33 batch-100
Running loss of epoch-33 batch-100 = 2.60921660810709e-05

Training epoch-33 batch-101
Running loss of epoch-33 batch-101 = 7.737893611192703e-06

Training epoch-33 batch-102
Running loss of epoch-33 batch-102 = 3.908062353730202e-05

Training epoch-33 batch-103
Running loss of epoch-33 batch-103 = 7.951690349727869e-05

Training epoch-33 batch-104
Running loss of epoch-33 batch-104 = 1.4886027202010155e-05

Training epoch-33 batch-105
Running loss of epoch-33 batch-105 = 2.37184576690197e-05

Training epoch-33 batch-106
Running loss of epoch-33 batch-106 = 9.377487003803253e-06

Training epoch-33 batch-107
Running loss of epoch-33 batch-107 = 2.6636291295289993e-05

Training epoch-33 batch-108
Running loss of epoch-33 batch-108 = 4.491698928177357e-05

Training epoch-33 batch-109
Running loss of epoch-33 batch-109 = 1.3006618246436119e-05

Training epoch-33 batch-110
Running loss of epoch-33 batch-110 = 2.4438602849841118e-05

Training epoch-33 batch-111
Running loss of epoch-33 batch-111 = 1.9205035641789436e-05

Training epoch-33 batch-112
Running loss of epoch-33 batch-112 = 7.623340934515e-05

Training epoch-33 batch-113
Running loss of epoch-33 batch-113 = 7.764529436826706e-05

Training epoch-33 batch-114
Running loss of epoch-33 batch-114 = 3.093271516263485e-05

Training epoch-33 batch-115
Running loss of epoch-33 batch-115 = 4.178890958428383e-05

Training epoch-33 batch-116
Running loss of epoch-33 batch-116 = 1.0173767805099487e-05

Training epoch-33 batch-117
Running loss of epoch-33 batch-117 = 1.8673250451683998e-05

Training epoch-33 batch-118
Running loss of epoch-33 batch-118 = 1.567695289850235e-05

Training epoch-33 batch-119
Running loss of epoch-33 batch-119 = 3.698933869600296e-05

Training epoch-33 batch-120
Running loss of epoch-33 batch-120 = 2.595921978354454e-05

Training epoch-33 batch-121
Running loss of epoch-33 batch-121 = 3.583391662687063e-05

Training epoch-33 batch-122
Running loss of epoch-33 batch-122 = 3.8179801777005196e-05

Training epoch-33 batch-123
Running loss of epoch-33 batch-123 = 4.7991983592510223e-05

Training epoch-33 batch-124
Running loss of epoch-33 batch-124 = 4.161917604506016e-05

Training epoch-33 batch-125
Running loss of epoch-33 batch-125 = 3.136531449854374e-05

Training epoch-33 batch-126
Running loss of epoch-33 batch-126 = 1.8796883523464203e-05

Training epoch-33 batch-127
Running loss of epoch-33 batch-127 = 3.6447541788220406e-05

Training epoch-33 batch-128
Running loss of epoch-33 batch-128 = 3.470876254141331e-05

Training epoch-33 batch-129
Running loss of epoch-33 batch-129 = 1.835566945374012e-05

Training epoch-33 batch-130
Running loss of epoch-33 batch-130 = 2.7341768145561218e-05

Training epoch-33 batch-131
Running loss of epoch-33 batch-131 = 1.778407022356987e-05

Training epoch-33 batch-132
Running loss of epoch-33 batch-132 = 3.7547899410128593e-05

Training epoch-33 batch-133
Running loss of epoch-33 batch-133 = 3.434484824538231e-05

Training epoch-33 batch-134
Running loss of epoch-33 batch-134 = 2.5220215320587158e-05

Training epoch-33 batch-135
Running loss of epoch-33 batch-135 = 6.958260200917721e-05

Training epoch-33 batch-136
Running loss of epoch-33 batch-136 = 4.016968887299299e-05

Training epoch-33 batch-137
Running loss of epoch-33 batch-137 = 1.9882339984178543e-05

Training epoch-33 batch-138
Running loss of epoch-33 batch-138 = 2.550496719777584e-05

Training epoch-33 batch-139
Running loss of epoch-33 batch-139 = 1.2319069355726242e-05

Training epoch-33 batch-140
Running loss of epoch-33 batch-140 = 6.64805993437767e-05

Training epoch-33 batch-141
Running loss of epoch-33 batch-141 = 1.0906485840678215e-05

Training epoch-33 batch-142
Running loss of epoch-33 batch-142 = 1.9459985196590424e-05

Training epoch-33 batch-143
Running loss of epoch-33 batch-143 = 2.83785630017519e-05

Training epoch-33 batch-144
Running loss of epoch-33 batch-144 = 3.071432001888752e-05

Training epoch-33 batch-145
Running loss of epoch-33 batch-145 = 3.118254244327545e-05

Training epoch-33 batch-146
Running loss of epoch-33 batch-146 = 0.00013241451233625412

Training epoch-33 batch-147
Running loss of epoch-33 batch-147 = 4.392978735268116e-05

Training epoch-33 batch-148
Running loss of epoch-33 batch-148 = 3.8752565160393715e-05

Training epoch-33 batch-149
Running loss of epoch-33 batch-149 = 3.237323835492134e-05

Training epoch-33 batch-150
Running loss of epoch-33 batch-150 = 3.1592557206749916e-05

Training epoch-33 batch-151
Running loss of epoch-33 batch-151 = 2.2485735826194286e-05

Training epoch-33 batch-152
Running loss of epoch-33 batch-152 = 4.951003938913345e-05

Training epoch-33 batch-153
Running loss of epoch-33 batch-153 = 3.510515671223402e-05

Training epoch-33 batch-154
Running loss of epoch-33 batch-154 = 4.839547909796238e-05

Training epoch-33 batch-155
Running loss of epoch-33 batch-155 = 3.91216017305851e-05

Training epoch-33 batch-156
Running loss of epoch-33 batch-156 = 6.291409954428673e-05

Training epoch-33 batch-157
Running loss of epoch-33 batch-157 = 3.6384910345077515e-05

Finished training epoch-33.



Average train loss at epoch-33 = 3.039429485797882e-05

Started Evaluation

Average val loss at epoch-33 = 0.9841467754710697

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 91.47 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.83 %
Accuracy for class execute is: 53.01 %
Accuracy for class get is: 69.74 %

Overall Accuracy = 83.41 %

Finished Evaluation



Started training epoch-34


Training epoch-34 batch-1
Running loss of epoch-34 batch-1 = 2.860231325030327e-05

Training epoch-34 batch-2
Running loss of epoch-34 batch-2 = 3.0339695513248444e-05

Training epoch-34 batch-3
Running loss of epoch-34 batch-3 = 2.9253307729959488e-05

Training epoch-34 batch-4
Running loss of epoch-34 batch-4 = 2.334360033273697e-05

Training epoch-34 batch-5
Running loss of epoch-34 batch-5 = 1.0205665603280067e-05

Training epoch-34 batch-6
Running loss of epoch-34 batch-6 = 2.4895183742046356e-05

Training epoch-34 batch-7
Running loss of epoch-34 batch-7 = 2.7252594009041786e-05

Training epoch-34 batch-8
Running loss of epoch-34 batch-8 = 1.053977757692337e-05

Training epoch-34 batch-9
Running loss of epoch-34 batch-9 = 1.390976831316948e-05

Training epoch-34 batch-10
Running loss of epoch-34 batch-10 = 2.385932020843029e-05

Training epoch-34 batch-11
Running loss of epoch-34 batch-11 = 1.659337431192398e-05

Training epoch-34 batch-12
Running loss of epoch-34 batch-12 = 3.373646177351475e-05

Training epoch-34 batch-13
Running loss of epoch-34 batch-13 = 9.697116911411285e-05

Training epoch-34 batch-14
Running loss of epoch-34 batch-14 = 2.3195752874016762e-05

Training epoch-34 batch-15
Running loss of epoch-34 batch-15 = 8.153729140758514e-06

Training epoch-34 batch-16
Running loss of epoch-34 batch-16 = 3.569014370441437e-05

Training epoch-34 batch-17
Running loss of epoch-34 batch-17 = 2.970942296087742e-05

Training epoch-34 batch-18
Running loss of epoch-34 batch-18 = 3.6515528336167336e-05

Training epoch-34 batch-19
Running loss of epoch-34 batch-19 = 4.486681427806616e-05

Training epoch-34 batch-20
Running loss of epoch-34 batch-20 = 4.9936817958950996e-05

Training epoch-34 batch-21
Running loss of epoch-34 batch-21 = 2.539972774684429e-05

Training epoch-34 batch-22
Running loss of epoch-34 batch-22 = 2.444535493850708e-05

Training epoch-34 batch-23
Running loss of epoch-34 batch-23 = 2.3325206711888313e-05

Training epoch-34 batch-24
Running loss of epoch-34 batch-24 = 3.5289907827973366e-05

Training epoch-34 batch-25
Running loss of epoch-34 batch-25 = 6.908434443175793e-05

Training epoch-34 batch-26
Running loss of epoch-34 batch-26 = 2.4352455511689186e-05

Training epoch-34 batch-27
Running loss of epoch-34 batch-27 = 1.1478085070848465e-05

Training epoch-34 batch-28
Running loss of epoch-34 batch-28 = 3.413099329918623e-05

Training epoch-34 batch-29
Running loss of epoch-34 batch-29 = 3.652786836028099e-05

Training epoch-34 batch-30
Running loss of epoch-34 batch-30 = 5.705002695322037e-05

Training epoch-34 batch-31
Running loss of epoch-34 batch-31 = 2.3388070985674858e-05

Training epoch-34 batch-32
Running loss of epoch-34 batch-32 = 1.6928650438785553e-05

Training epoch-34 batch-33
Running loss of epoch-34 batch-33 = 0.00010080949869006872

Training epoch-34 batch-34
Running loss of epoch-34 batch-34 = 1.3478565961122513e-05

Training epoch-34 batch-35
Running loss of epoch-34 batch-35 = 3.0834460631012917e-05

Training epoch-34 batch-36
Running loss of epoch-34 batch-36 = 2.1687010303139687e-05

Training epoch-34 batch-37
Running loss of epoch-34 batch-37 = 3.085634671151638e-05

Training epoch-34 batch-38
Running loss of epoch-34 batch-38 = 4.292058292776346e-05

Training epoch-34 batch-39
Running loss of epoch-34 batch-39 = 3.176555037498474e-05

Training epoch-34 batch-40
Running loss of epoch-34 batch-40 = 2.2970838472247124e-05

Training epoch-34 batch-41
Running loss of epoch-34 batch-41 = 4.15074173361063e-05

Training epoch-34 batch-42
Running loss of epoch-34 batch-42 = 3.1232135370373726e-05

Training epoch-34 batch-43
Running loss of epoch-34 batch-43 = 1.0466435924172401e-05

Training epoch-34 batch-44
Running loss of epoch-34 batch-44 = 2.5419751182198524e-05

Training epoch-34 batch-45
Running loss of epoch-34 batch-45 = 1.3049226254224777e-05

Training epoch-34 batch-46
Running loss of epoch-34 batch-46 = 7.177470251917839e-06

Training epoch-34 batch-47
Running loss of epoch-34 batch-47 = 2.4639302864670753e-05

Training epoch-34 batch-48
Running loss of epoch-34 batch-48 = 4.474911838769913e-05

Training epoch-34 batch-49
Running loss of epoch-34 batch-49 = 1.845136284828186e-05

Training epoch-34 batch-50
Running loss of epoch-34 batch-50 = 2.1405285224318504e-05

Training epoch-34 batch-51
Running loss of epoch-34 batch-51 = 6.09993003308773e-06

Training epoch-34 batch-52
Running loss of epoch-34 batch-52 = 2.4724053218960762e-05

Training epoch-34 batch-53
Running loss of epoch-34 batch-53 = 2.8679845854640007e-05

Training epoch-34 batch-54
Running loss of epoch-34 batch-54 = 1.9592000171542168e-05

Training epoch-34 batch-55
Running loss of epoch-34 batch-55 = 1.6887905076146126e-05

Training epoch-34 batch-56
Running loss of epoch-34 batch-56 = 2.3606233298778534e-05

Training epoch-34 batch-57
Running loss of epoch-34 batch-57 = 9.554624557495117e-05

Training epoch-34 batch-58
Running loss of epoch-34 batch-58 = 1.590559259057045e-05

Training epoch-34 batch-59
Running loss of epoch-34 batch-59 = 1.0645715519785881e-05

Training epoch-34 batch-60
Running loss of epoch-34 batch-60 = 3.7369318306446075e-05

Training epoch-34 batch-61
Running loss of epoch-34 batch-61 = 2.8126640245318413e-05

Training epoch-34 batch-62
Running loss of epoch-34 batch-62 = 1.677917316555977e-05

Training epoch-34 batch-63
Running loss of epoch-34 batch-63 = 1.7024576663970947e-05

Training epoch-34 batch-64
Running loss of epoch-34 batch-64 = 3.6242883652448654e-05

Training epoch-34 batch-65
Running loss of epoch-34 batch-65 = 2.225511707365513e-05

Training epoch-34 batch-66
Running loss of epoch-34 batch-66 = 5.839858204126358e-06

Training epoch-34 batch-67
Running loss of epoch-34 batch-67 = 3.8804253563284874e-05

Training epoch-34 batch-68
Running loss of epoch-34 batch-68 = 2.990150824189186e-05

Training epoch-34 batch-69
Running loss of epoch-34 batch-69 = 2.293451689183712e-05

Training epoch-34 batch-70
Running loss of epoch-34 batch-70 = 2.8739916160702705e-05

Training epoch-34 batch-71
Running loss of epoch-34 batch-71 = 2.6911497116088867e-05

Training epoch-34 batch-72
Running loss of epoch-34 batch-72 = 1.2576114386320114e-05

Training epoch-34 batch-73
Running loss of epoch-34 batch-73 = 1.8717022612690926e-05

Training epoch-34 batch-74
Running loss of epoch-34 batch-74 = 2.014869824051857e-05

Training epoch-34 batch-75
Running loss of epoch-34 batch-75 = 3.1390925869345665e-05

Training epoch-34 batch-76
Running loss of epoch-34 batch-76 = 5.646096542477608e-05

Training epoch-34 batch-77
Running loss of epoch-34 batch-77 = 2.3913104087114334e-05

Training epoch-34 batch-78
Running loss of epoch-34 batch-78 = 4.1794031858444214e-05

Training epoch-34 batch-79
Running loss of epoch-34 batch-79 = 2.514873631298542e-05

Training epoch-34 batch-80
Running loss of epoch-34 batch-80 = 1.6156351193785667e-05

Training epoch-34 batch-81
Running loss of epoch-34 batch-81 = 1.0124407708644867e-05

Training epoch-34 batch-82
Running loss of epoch-34 batch-82 = 9.068520739674568e-06

Training epoch-34 batch-83
Running loss of epoch-34 batch-83 = 1.3527227565646172e-05

Training epoch-34 batch-84
Running loss of epoch-34 batch-84 = 1.4985213056206703e-05

Training epoch-34 batch-85
Running loss of epoch-34 batch-85 = 2.8227921575307846e-05

Training epoch-34 batch-86
Running loss of epoch-34 batch-86 = 2.9357848688960075e-05

Training epoch-34 batch-87
Running loss of epoch-34 batch-87 = 1.8455320969223976e-05

Training epoch-34 batch-88
Running loss of epoch-34 batch-88 = 5.4246862418949604e-05

Training epoch-34 batch-89
Running loss of epoch-34 batch-89 = 3.1484756618738174e-05

Training epoch-34 batch-90
Running loss of epoch-34 batch-90 = 2.2719847038388252e-05

Training epoch-34 batch-91
Running loss of epoch-34 batch-91 = 1.191697083413601e-05

Training epoch-34 batch-92
Running loss of epoch-34 batch-92 = 8.20760615170002e-05

Training epoch-34 batch-93
Running loss of epoch-34 batch-93 = 3.105076029896736e-05

Training epoch-34 batch-94
Running loss of epoch-34 batch-94 = 2.017756924033165e-05

Training epoch-34 batch-95
Running loss of epoch-34 batch-95 = 1.2209173291921616e-05

Training epoch-34 batch-96
Running loss of epoch-34 batch-96 = 4.345853812992573e-05

Training epoch-34 batch-97
Running loss of epoch-34 batch-97 = 2.9654009267687798e-05

Training epoch-34 batch-98
Running loss of epoch-34 batch-98 = 4.309159703552723e-05

Training epoch-34 batch-99
Running loss of epoch-34 batch-99 = 2.409215085208416e-05

Training epoch-34 batch-100
Running loss of epoch-34 batch-100 = 1.0282383300364017e-05

Training epoch-34 batch-101
Running loss of epoch-34 batch-101 = 5.415547639131546e-05

Training epoch-34 batch-102
Running loss of epoch-34 batch-102 = 2.0027393475174904e-05

Training epoch-34 batch-103
Running loss of epoch-34 batch-103 = 3.52498609572649e-05

Training epoch-34 batch-104
Running loss of epoch-34 batch-104 = 2.898089587688446e-05

Training epoch-34 batch-105
Running loss of epoch-34 batch-105 = 1.6480451449751854e-05

Training epoch-34 batch-106
Running loss of epoch-34 batch-106 = 2.2028107196092606e-05

Training epoch-34 batch-107
Running loss of epoch-34 batch-107 = 1.476658508181572e-05

Training epoch-34 batch-108
Running loss of epoch-34 batch-108 = 2.5826506316661835e-05

Training epoch-34 batch-109
Running loss of epoch-34 batch-109 = 2.3183180019259453e-05

Training epoch-34 batch-110
Running loss of epoch-34 batch-110 = 2.4630920961499214e-05

Training epoch-34 batch-111
Running loss of epoch-34 batch-111 = 1.0889722034335136e-05

Training epoch-34 batch-112
Running loss of epoch-34 batch-112 = 3.2884301617741585e-05

Training epoch-34 batch-113
Running loss of epoch-34 batch-113 = 2.390192821621895e-05

Training epoch-34 batch-114
Running loss of epoch-34 batch-114 = 2.147653140127659e-05

Training epoch-34 batch-115
Running loss of epoch-34 batch-115 = 2.21962109208107e-05

Training epoch-34 batch-116
Running loss of epoch-34 batch-116 = 3.85441817343235e-05

Training epoch-34 batch-117
Running loss of epoch-34 batch-117 = 8.160481229424477e-06

Training epoch-34 batch-118
Running loss of epoch-34 batch-118 = 2.7452362701296806e-05

Training epoch-34 batch-119
Running loss of epoch-34 batch-119 = 2.098805271089077e-05

Training epoch-34 batch-120
Running loss of epoch-34 batch-120 = 4.8076966777443886e-05

Training epoch-34 batch-121
Running loss of epoch-34 batch-121 = 1.9540078938007355e-05

Training epoch-34 batch-122
Running loss of epoch-34 batch-122 = 1.614028587937355e-05

Training epoch-34 batch-123
Running loss of epoch-34 batch-123 = 1.9490020349621773e-05

Training epoch-34 batch-124
Running loss of epoch-34 batch-124 = 1.7500948160886765e-05

Training epoch-34 batch-125
Running loss of epoch-34 batch-125 = 1.5796860679984093e-05

Training epoch-34 batch-126
Running loss of epoch-34 batch-126 = 2.103822771459818e-05

Training epoch-34 batch-127
Running loss of epoch-34 batch-127 = 5.5458396673202515e-05

Training epoch-34 batch-128
Running loss of epoch-34 batch-128 = 2.530030906200409e-05

Training epoch-34 batch-129
Running loss of epoch-34 batch-129 = 2.366374246776104e-05

Training epoch-34 batch-130
Running loss of epoch-34 batch-130 = 2.1690968424081802e-05

Training epoch-34 batch-131
Running loss of epoch-34 batch-131 = 1.4632940292358398e-05

Training epoch-34 batch-132
Running loss of epoch-34 batch-132 = 2.06709373742342e-05

Training epoch-34 batch-133
Running loss of epoch-34 batch-133 = 1.206807792186737e-05

Training epoch-34 batch-134
Running loss of epoch-34 batch-134 = 2.031354233622551e-05

Training epoch-34 batch-135
Running loss of epoch-34 batch-135 = 1.7212936654686928e-05

Training epoch-34 batch-136
Running loss of epoch-34 batch-136 = 1.902645453810692e-05

Training epoch-34 batch-137
Running loss of epoch-34 batch-137 = 2.1597370505332947e-05

Training epoch-34 batch-138
Running loss of epoch-34 batch-138 = 2.9792077839374542e-05

Training epoch-34 batch-139
Running loss of epoch-34 batch-139 = 4.4372864067554474e-05

Training epoch-34 batch-140
Running loss of epoch-34 batch-140 = 1.2099510058760643e-05

Training epoch-34 batch-141
Running loss of epoch-34 batch-141 = 3.7607038393616676e-05

Training epoch-34 batch-142
Running loss of epoch-34 batch-142 = 8.469005115330219e-05

Training epoch-34 batch-143
Running loss of epoch-34 batch-143 = 9.769340977072716e-06

Training epoch-34 batch-144
Running loss of epoch-34 batch-144 = 8.067255839705467e-05

Training epoch-34 batch-145
Running loss of epoch-34 batch-145 = 9.595416486263275e-06

Training epoch-34 batch-146
Running loss of epoch-34 batch-146 = 1.1363299563527107e-05

Training epoch-34 batch-147
Running loss of epoch-34 batch-147 = 1.6167527064681053e-05

Training epoch-34 batch-148
Running loss of epoch-34 batch-148 = 3.3951131626963615e-05

Training epoch-34 batch-149
Running loss of epoch-34 batch-149 = 1.9891886040568352e-05

Training epoch-34 batch-150
Running loss of epoch-34 batch-150 = 3.044120967388153e-05

Training epoch-34 batch-151
Running loss of epoch-34 batch-151 = 9.081093594431877e-06

Training epoch-34 batch-152
Running loss of epoch-34 batch-152 = 4.137703217566013e-05

Training epoch-34 batch-153
Running loss of epoch-34 batch-153 = 1.6241101548075676e-05

Training epoch-34 batch-154
Running loss of epoch-34 batch-154 = 2.0830193534493446e-05

Training epoch-34 batch-155
Running loss of epoch-34 batch-155 = 9.036855772137642e-06

Training epoch-34 batch-156
Running loss of epoch-34 batch-156 = 1.3914890587329865e-05

Training epoch-34 batch-157
Running loss of epoch-34 batch-157 = 0.00026097148656845093

Finished training epoch-34.



Average train loss at epoch-34 = 2.770368978381157e-05

Started Evaluation

Average val loss at epoch-34 = 1.0090073008943818

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 92.95 %
Accuracy for class onCreate is: 92.00 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 53.01 %
Accuracy for class get is: 70.26 %

Overall Accuracy = 83.54 %

Finished Evaluation



Started training epoch-35


Training epoch-35 batch-1
Running loss of epoch-35 batch-1 = 1.556077040731907e-05

Training epoch-35 batch-2
Running loss of epoch-35 batch-2 = 3.832392394542694e-06

Training epoch-35 batch-3
Running loss of epoch-35 batch-3 = 2.032262273132801e-05

Training epoch-35 batch-4
Running loss of epoch-35 batch-4 = 2.246163785457611e-05

Training epoch-35 batch-5
Running loss of epoch-35 batch-5 = 1.2275762856006622e-05

Training epoch-35 batch-6
Running loss of epoch-35 batch-6 = 2.2526364773511887e-05

Training epoch-35 batch-7
Running loss of epoch-35 batch-7 = 1.2009171769022942e-05

Training epoch-35 batch-8
Running loss of epoch-35 batch-8 = 1.873471774160862e-05

Training epoch-35 batch-9
Running loss of epoch-35 batch-9 = 6.433040834963322e-05

Training epoch-35 batch-10
Running loss of epoch-35 batch-10 = 1.83724332600832e-05

Training epoch-35 batch-11
Running loss of epoch-35 batch-11 = 3.3993273973464966e-05

Training epoch-35 batch-12
Running loss of epoch-35 batch-12 = 2.016732469201088e-05

Training epoch-35 batch-13
Running loss of epoch-35 batch-13 = 3.399397246539593e-05

Training epoch-35 batch-14
Running loss of epoch-35 batch-14 = 2.669636160135269e-05

Training epoch-35 batch-15
Running loss of epoch-35 batch-15 = 2.1961983293294907e-05

Training epoch-35 batch-16
Running loss of epoch-35 batch-16 = 6.031710654497147e-06

Training epoch-35 batch-17
Running loss of epoch-35 batch-17 = 1.3393117114901543e-05

Training epoch-35 batch-18
Running loss of epoch-35 batch-18 = 1.0576564818620682e-05

Training epoch-35 batch-19
Running loss of epoch-35 batch-19 = 2.7253059670329094e-05

Training epoch-35 batch-20
Running loss of epoch-35 batch-20 = 6.142258644104004e-05

Training epoch-35 batch-21
Running loss of epoch-35 batch-21 = 1.7804792150855064e-05

Training epoch-35 batch-22
Running loss of epoch-35 batch-22 = 3.727385774254799e-06

Training epoch-35 batch-23
Running loss of epoch-35 batch-23 = 2.566794864833355e-05

Training epoch-35 batch-24
Running loss of epoch-35 batch-24 = 1.9993865862488747e-05

Training epoch-35 batch-25
Running loss of epoch-35 batch-25 = 1.1438736692070961e-05

Training epoch-35 batch-26
Running loss of epoch-35 batch-26 = 1.5033874660730362e-05

Training epoch-35 batch-27
Running loss of epoch-35 batch-27 = 2.006441354751587e-05

Training epoch-35 batch-28
Running loss of epoch-35 batch-28 = 1.8134014680981636e-05

Training epoch-35 batch-29
Running loss of epoch-35 batch-29 = 1.4387071132659912e-05

Training epoch-35 batch-30
Running loss of epoch-35 batch-30 = 2.537202090024948e-05

Training epoch-35 batch-31
Running loss of epoch-35 batch-31 = 1.548859290778637e-05

Training epoch-35 batch-32
Running loss of epoch-35 batch-32 = 1.1472264304757118e-05

Training epoch-35 batch-33
Running loss of epoch-35 batch-33 = 1.4926772564649582e-05

Training epoch-35 batch-34
Running loss of epoch-35 batch-34 = 7.734796963632107e-05

Training epoch-35 batch-35
Running loss of epoch-35 batch-35 = 9.553041309118271e-06

Training epoch-35 batch-36
Running loss of epoch-35 batch-36 = 3.799283877015114e-05

Training epoch-35 batch-37
Running loss of epoch-35 batch-37 = 1.864507794380188e-05

Training epoch-35 batch-38
Running loss of epoch-35 batch-38 = 2.7336878702044487e-05

Training epoch-35 batch-39
Running loss of epoch-35 batch-39 = 7.573794573545456e-05

Training epoch-35 batch-40
Running loss of epoch-35 batch-40 = 1.3009877875447273e-05

Training epoch-35 batch-41
Running loss of epoch-35 batch-41 = 9.537208825349808e-06

Training epoch-35 batch-42
Running loss of epoch-35 batch-42 = 1.9152183085680008e-05

Training epoch-35 batch-43
Running loss of epoch-35 batch-43 = 4.318729043006897e-05

Training epoch-35 batch-44
Running loss of epoch-35 batch-44 = 1.9332394003868103e-05

Training epoch-35 batch-45
Running loss of epoch-35 batch-45 = 1.1696480214595795e-05

Training epoch-35 batch-46
Running loss of epoch-35 batch-46 = 4.11253422498703e-05

Training epoch-35 batch-47
Running loss of epoch-35 batch-47 = 1.5184050425887108e-05

Training epoch-35 batch-48
Running loss of epoch-35 batch-48 = 4.190392792224884e-05

Training epoch-35 batch-49
Running loss of epoch-35 batch-49 = 1.28129031509161e-05

Training epoch-35 batch-50
Running loss of epoch-35 batch-50 = 3.3564865589141846e-05

Training epoch-35 batch-51
Running loss of epoch-35 batch-51 = 2.821977250277996e-05

Training epoch-35 batch-52
Running loss of epoch-35 batch-52 = 1.819990575313568e-05

Training epoch-35 batch-53
Running loss of epoch-35 batch-53 = 9.177951142191887e-06

Training epoch-35 batch-54
Running loss of epoch-35 batch-54 = 5.5412063375115395e-05

Training epoch-35 batch-55
Running loss of epoch-35 batch-55 = 4.615262150764465e-05

Training epoch-35 batch-56
Running loss of epoch-35 batch-56 = 1.4723977074027061e-05

Training epoch-35 batch-57
Running loss of epoch-35 batch-57 = 3.571971319615841e-05

Training epoch-35 batch-58
Running loss of epoch-35 batch-58 = 4.2397063225507736e-05

Training epoch-35 batch-59
Running loss of epoch-35 batch-59 = 3.202282823622227e-05

Training epoch-35 batch-60
Running loss of epoch-35 batch-60 = 5.752081051468849e-06

Training epoch-35 batch-61
Running loss of epoch-35 batch-61 = 2.3134751245379448e-05

Training epoch-35 batch-62
Running loss of epoch-35 batch-62 = 2.7899164706468582e-05

Training epoch-35 batch-63
Running loss of epoch-35 batch-63 = 1.3685319572687149e-05

Training epoch-35 batch-64
Running loss of epoch-35 batch-64 = 1.2659933418035507e-05

Training epoch-35 batch-65
Running loss of epoch-35 batch-65 = 1.8327496945858002e-05

Training epoch-35 batch-66
Running loss of epoch-35 batch-66 = 2.2717751562595367e-05

Training epoch-35 batch-67
Running loss of epoch-35 batch-67 = 5.468958988785744e-06

Training epoch-35 batch-68
Running loss of epoch-35 batch-68 = 3.4703873097896576e-05

Training epoch-35 batch-69
Running loss of epoch-35 batch-69 = 3.0035851523280144e-05

Training epoch-35 batch-70
Running loss of epoch-35 batch-70 = 1.4197779819369316e-05

Training epoch-35 batch-71
Running loss of epoch-35 batch-71 = 3.435742110013962e-05

Training epoch-35 batch-72
Running loss of epoch-35 batch-72 = 1.704576425254345e-05

Training epoch-35 batch-73
Running loss of epoch-35 batch-73 = 3.157160244882107e-05

Training epoch-35 batch-74
Running loss of epoch-35 batch-74 = 5.455734208226204e-05

Training epoch-35 batch-75
Running loss of epoch-35 batch-75 = 8.058780804276466e-05

Training epoch-35 batch-76
Running loss of epoch-35 batch-76 = 1.564272679388523e-05

Training epoch-35 batch-77
Running loss of epoch-35 batch-77 = 2.7596717700362206e-05

Training epoch-35 batch-78
Running loss of epoch-35 batch-78 = 2.126116305589676e-05

Training epoch-35 batch-79
Running loss of epoch-35 batch-79 = 3.444729372859001e-05

Training epoch-35 batch-80
Running loss of epoch-35 batch-80 = 2.458132803440094e-05

Training epoch-35 batch-81
Running loss of epoch-35 batch-81 = 1.1290423572063446e-05

Training epoch-35 batch-82
Running loss of epoch-35 batch-82 = 4.265294410288334e-05

Training epoch-35 batch-83
Running loss of epoch-35 batch-83 = 3.3188145607709885e-05

Training epoch-35 batch-84
Running loss of epoch-35 batch-84 = 2.142670564353466e-05

Training epoch-35 batch-85
Running loss of epoch-35 batch-85 = 2.012215554714203e-05

Training epoch-35 batch-86
Running loss of epoch-35 batch-86 = 1.8403399735689163e-05

Training epoch-35 batch-87
Running loss of epoch-35 batch-87 = 3.4809811040759087e-05

Training epoch-35 batch-88
Running loss of epoch-35 batch-88 = 5.7042576372623444e-05

Training epoch-35 batch-89
Running loss of epoch-35 batch-89 = 2.426491118967533e-05

Training epoch-35 batch-90
Running loss of epoch-35 batch-90 = 2.8659822419285774e-05

Training epoch-35 batch-91
Running loss of epoch-35 batch-91 = 3.4274766221642494e-05

Training epoch-35 batch-92
Running loss of epoch-35 batch-92 = 1.383037306368351e-05

Training epoch-35 batch-93
Running loss of epoch-35 batch-93 = 1.0409625247120857e-05

Training epoch-35 batch-94
Running loss of epoch-35 batch-94 = 4.855799488723278e-05

Training epoch-35 batch-95
Running loss of epoch-35 batch-95 = 8.039409294724464e-06

Training epoch-35 batch-96
Running loss of epoch-35 batch-96 = 4.183477722108364e-05

Training epoch-35 batch-97
Running loss of epoch-35 batch-97 = 2.466444857418537e-05

Training epoch-35 batch-98
Running loss of epoch-35 batch-98 = 2.548983320593834e-05

Training epoch-35 batch-99
Running loss of epoch-35 batch-99 = 1.0846182703971863e-05

Training epoch-35 batch-100
Running loss of epoch-35 batch-100 = 1.5864847227931023e-05

Training epoch-35 batch-101
Running loss of epoch-35 batch-101 = 5.026441067457199e-05

Training epoch-35 batch-102
Running loss of epoch-35 batch-102 = 2.7928268536925316e-05

Training epoch-35 batch-103
Running loss of epoch-35 batch-103 = 2.1248357370495796e-05

Training epoch-35 batch-104
Running loss of epoch-35 batch-104 = 1.6668811440467834e-05

Training epoch-35 batch-105
Running loss of epoch-35 batch-105 = 1.5316298231482506e-05

Training epoch-35 batch-106
Running loss of epoch-35 batch-106 = 2.764700911939144e-05

Training epoch-35 batch-107
Running loss of epoch-35 batch-107 = 4.075653851032257e-05

Training epoch-35 batch-108
Running loss of epoch-35 batch-108 = 2.186652272939682e-05

Training epoch-35 batch-109
Running loss of epoch-35 batch-109 = 1.6747508198022842e-05

Training epoch-35 batch-110
Running loss of epoch-35 batch-110 = 1.2837350368499756e-05

Training epoch-35 batch-111
Running loss of epoch-35 batch-111 = 7.899478077888489e-06

Training epoch-35 batch-112
Running loss of epoch-35 batch-112 = 1.6522128134965897e-05

Training epoch-35 batch-113
Running loss of epoch-35 batch-113 = 2.3094238713383675e-05

Training epoch-35 batch-114
Running loss of epoch-35 batch-114 = 1.821434125304222e-05

Training epoch-35 batch-115
Running loss of epoch-35 batch-115 = 1.1508585885167122e-05

Training epoch-35 batch-116
Running loss of epoch-35 batch-116 = 3.1002331525087357e-05

Training epoch-35 batch-117
Running loss of epoch-35 batch-117 = 1.7753103747963905e-05

Training epoch-35 batch-118
Running loss of epoch-35 batch-118 = 2.574350219219923e-05

Training epoch-35 batch-119
Running loss of epoch-35 batch-119 = 2.266978845000267e-05

Training epoch-35 batch-120
Running loss of epoch-35 batch-120 = 1.104455441236496e-05

Training epoch-35 batch-121
Running loss of epoch-35 batch-121 = 3.172270953655243e-05

Training epoch-35 batch-122
Running loss of epoch-35 batch-122 = 2.0198756828904152e-05

Training epoch-35 batch-123
Running loss of epoch-35 batch-123 = 2.457713708281517e-05

Training epoch-35 batch-124
Running loss of epoch-35 batch-124 = 4.8182206228375435e-05

Training epoch-35 batch-125
Running loss of epoch-35 batch-125 = 1.940387301146984e-05

Training epoch-35 batch-126
Running loss of epoch-35 batch-126 = 1.716380938887596e-05

Training epoch-35 batch-127
Running loss of epoch-35 batch-127 = 2.553430385887623e-05

Training epoch-35 batch-128
Running loss of epoch-35 batch-128 = 1.376541331410408e-05

Training epoch-35 batch-129
Running loss of epoch-35 batch-129 = 4.3104635551571846e-05

Training epoch-35 batch-130
Running loss of epoch-35 batch-130 = 2.8237001970410347e-05

Training epoch-35 batch-131
Running loss of epoch-35 batch-131 = 4.834961146116257e-05

Training epoch-35 batch-132
Running loss of epoch-35 batch-132 = 1.2427568435668945e-05

Training epoch-35 batch-133
Running loss of epoch-35 batch-133 = 1.08303502202034e-05

Training epoch-35 batch-134
Running loss of epoch-35 batch-134 = 4.150927998125553e-05

Training epoch-35 batch-135
Running loss of epoch-35 batch-135 = 2.5974586606025696e-05

Training epoch-35 batch-136
Running loss of epoch-35 batch-136 = 2.9267044737935066e-05

Training epoch-35 batch-137
Running loss of epoch-35 batch-137 = 1.314864493906498e-05

Training epoch-35 batch-138
Running loss of epoch-35 batch-138 = 2.1121930330991745e-05

Training epoch-35 batch-139
Running loss of epoch-35 batch-139 = 1.5558674931526184e-05

Training epoch-35 batch-140
Running loss of epoch-35 batch-140 = 2.2176653146743774e-05

Training epoch-35 batch-141
Running loss of epoch-35 batch-141 = 2.6821857318282127e-05

Training epoch-35 batch-142
Running loss of epoch-35 batch-142 = 3.953767009079456e-05

Training epoch-35 batch-143
Running loss of epoch-35 batch-143 = 9.97958704829216e-06

Training epoch-35 batch-144
Running loss of epoch-35 batch-144 = 1.7469050362706184e-05

Training epoch-35 batch-145
Running loss of epoch-35 batch-145 = 1.4424324035644531e-05

Training epoch-35 batch-146
Running loss of epoch-35 batch-146 = 2.196826972067356e-05

Training epoch-35 batch-147
Running loss of epoch-35 batch-147 = 1.6703270375728607e-05

Training epoch-35 batch-148
Running loss of epoch-35 batch-148 = 2.4937326088547707e-05

Training epoch-35 batch-149
Running loss of epoch-35 batch-149 = 1.2308591976761818e-05

Training epoch-35 batch-150
Running loss of epoch-35 batch-150 = 2.173054963350296e-05

Training epoch-35 batch-151
Running loss of epoch-35 batch-151 = 1.9254162907600403e-05

Training epoch-35 batch-152
Running loss of epoch-35 batch-152 = 1.4405930414795876e-05

Training epoch-35 batch-153
Running loss of epoch-35 batch-153 = 2.4002045392990112e-05

Training epoch-35 batch-154
Running loss of epoch-35 batch-154 = 2.1318206563591957e-05

Training epoch-35 batch-155
Running loss of epoch-35 batch-155 = 5.9861806221306324e-05

Training epoch-35 batch-156
Running loss of epoch-35 batch-156 = 2.4381326511502266e-05

Training epoch-35 batch-157
Running loss of epoch-35 batch-157 = 0.00041204318404197693

Finished training epoch-35.



Average train loss at epoch-35 = 2.5379721820354463e-05

Started Evaluation

Average val loss at epoch-35 = 0.9855295848964919

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 92.43 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.75 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.05 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 70.00 %

Overall Accuracy = 83.56 %

Finished Evaluation



Started training epoch-36


Training epoch-36 batch-1
Running loss of epoch-36 batch-1 = 1.4791497960686684e-05

Training epoch-36 batch-2
Running loss of epoch-36 batch-2 = 2.6055146008729935e-05

Training epoch-36 batch-3
Running loss of epoch-36 batch-3 = 8.499249815940857e-06

Training epoch-36 batch-4
Running loss of epoch-36 batch-4 = 2.706330269575119e-05

Training epoch-36 batch-5
Running loss of epoch-36 batch-5 = 1.895916648209095e-05

Training epoch-36 batch-6
Running loss of epoch-36 batch-6 = 2.2387830540537834e-05

Training epoch-36 batch-7
Running loss of epoch-36 batch-7 = 8.691102266311646e-06

Training epoch-36 batch-8
Running loss of epoch-36 batch-8 = 2.071796916425228e-05

Training epoch-36 batch-9
Running loss of epoch-36 batch-9 = 1.3493234291672707e-05

Training epoch-36 batch-10
Running loss of epoch-36 batch-10 = 6.743939593434334e-06

Training epoch-36 batch-11
Running loss of epoch-36 batch-11 = 6.98945950716734e-05

Training epoch-36 batch-12
Running loss of epoch-36 batch-12 = 8.134637027978897e-06

Training epoch-36 batch-13
Running loss of epoch-36 batch-13 = 5.7641416788101196e-05

Training epoch-36 batch-14
Running loss of epoch-36 batch-14 = 1.0909978300333023e-05

Training epoch-36 batch-15
Running loss of epoch-36 batch-15 = 1.6634585335850716e-05

Training epoch-36 batch-16
Running loss of epoch-36 batch-16 = 3.895186819136143e-05

Training epoch-36 batch-17
Running loss of epoch-36 batch-17 = 2.1957093849778175e-05

Training epoch-36 batch-18
Running loss of epoch-36 batch-18 = 1.1869939044117928e-05

Training epoch-36 batch-19
Running loss of epoch-36 batch-19 = 1.7591984942555428e-05

Training epoch-36 batch-20
Running loss of epoch-36 batch-20 = 1.249043270945549e-05

Training epoch-36 batch-21
Running loss of epoch-36 batch-21 = 2.5009503588080406e-05

Training epoch-36 batch-22
Running loss of epoch-36 batch-22 = 2.2761989384889603e-05

Training epoch-36 batch-23
Running loss of epoch-36 batch-23 = 1.4003366231918335e-05

Training epoch-36 batch-24
Running loss of epoch-36 batch-24 = 1.5332363545894623e-05

Training epoch-36 batch-25
Running loss of epoch-36 batch-25 = 2.378597855567932e-05

Training epoch-36 batch-26
Running loss of epoch-36 batch-26 = 2.1120533347129822e-05

Training epoch-36 batch-27
Running loss of epoch-36 batch-27 = 1.8780818209052086e-05

Training epoch-36 batch-28
Running loss of epoch-36 batch-28 = 2.227665390819311e-05

Training epoch-36 batch-29
Running loss of epoch-36 batch-29 = 3.063026815652847e-05

Training epoch-36 batch-30
Running loss of epoch-36 batch-30 = 1.5844590961933136e-05

Training epoch-36 batch-31
Running loss of epoch-36 batch-31 = 1.6023172065615654e-05

Training epoch-36 batch-32
Running loss of epoch-36 batch-32 = 4.793470725417137e-05

Training epoch-36 batch-33
Running loss of epoch-36 batch-33 = 1.0399380698800087e-05

Training epoch-36 batch-34
Running loss of epoch-36 batch-34 = 2.8194626793265343e-05

Training epoch-36 batch-35
Running loss of epoch-36 batch-35 = 1.5879282727837563e-05

Training epoch-36 batch-36
Running loss of epoch-36 batch-36 = 5.8573903515934944e-05

Training epoch-36 batch-37
Running loss of epoch-36 batch-37 = 2.7185771614313126e-05

Training epoch-36 batch-38
Running loss of epoch-36 batch-38 = 7.613562047481537e-06

Training epoch-36 batch-39
Running loss of epoch-36 batch-39 = 2.3079803213477135e-05

Training epoch-36 batch-40
Running loss of epoch-36 batch-40 = 5.740835331380367e-05

Training epoch-36 batch-41
Running loss of epoch-36 batch-41 = 3.6975834518671036e-05

Training epoch-36 batch-42
Running loss of epoch-36 batch-42 = 9.363517165184021e-06

Training epoch-36 batch-43
Running loss of epoch-36 batch-43 = 1.9342638552188873e-05

Training epoch-36 batch-44
Running loss of epoch-36 batch-44 = 2.0875129848718643e-05

Training epoch-36 batch-45
Running loss of epoch-36 batch-45 = 1.5410594642162323e-05

Training epoch-36 batch-46
Running loss of epoch-36 batch-46 = 2.263905480504036e-05

Training epoch-36 batch-47
Running loss of epoch-36 batch-47 = 1.3478100299835205e-05

Training epoch-36 batch-48
Running loss of epoch-36 batch-48 = 2.577388659119606e-05

Training epoch-36 batch-49
Running loss of epoch-36 batch-49 = 2.7051428332924843e-05

Training epoch-36 batch-50
Running loss of epoch-36 batch-50 = 4.611373879015446e-05

Training epoch-36 batch-51
Running loss of epoch-36 batch-51 = 2.3451633751392365e-05

Training epoch-36 batch-52
Running loss of epoch-36 batch-52 = 1.7426908016204834e-05

Training epoch-36 batch-53
Running loss of epoch-36 batch-53 = 3.970111720263958e-05

Training epoch-36 batch-54
Running loss of epoch-36 batch-54 = 1.057283952832222e-05

Training epoch-36 batch-55
Running loss of epoch-36 batch-55 = 7.158610969781876e-06

Training epoch-36 batch-56
Running loss of epoch-36 batch-56 = 2.6565277948975563e-05

Training epoch-36 batch-57
Running loss of epoch-36 batch-57 = 3.0655646696686745e-05

Training epoch-36 batch-58
Running loss of epoch-36 batch-58 = 1.5713274478912354e-05

Training epoch-36 batch-59
Running loss of epoch-36 batch-59 = 9.458744898438454e-06

Training epoch-36 batch-60
Running loss of epoch-36 batch-60 = 1.8823076970875263e-05

Training epoch-36 batch-61
Running loss of epoch-36 batch-61 = 1.4725839719176292e-05

Training epoch-36 batch-62
Running loss of epoch-36 batch-62 = 2.5461427867412567e-05

Training epoch-36 batch-63
Running loss of epoch-36 batch-63 = 1.4181481674313545e-05

Training epoch-36 batch-64
Running loss of epoch-36 batch-64 = 5.921628326177597e-05

Training epoch-36 batch-65
Running loss of epoch-36 batch-65 = 1.0916031897068024e-05

Training epoch-36 batch-66
Running loss of epoch-36 batch-66 = 2.07431148737669e-05

Training epoch-36 batch-67
Running loss of epoch-36 batch-67 = 2.0946725271642208e-05

Training epoch-36 batch-68
Running loss of epoch-36 batch-68 = 1.9989674910902977e-05

Training epoch-36 batch-69
Running loss of epoch-36 batch-69 = 3.201933577656746e-05

Training epoch-36 batch-70
Running loss of epoch-36 batch-70 = 1.2741656973958015e-05

Training epoch-36 batch-71
Running loss of epoch-36 batch-71 = 1.1344905942678452e-05

Training epoch-36 batch-72
Running loss of epoch-36 batch-72 = 8.97119753062725e-06

Training epoch-36 batch-73
Running loss of epoch-36 batch-73 = 5.100620910525322e-06

Training epoch-36 batch-74
Running loss of epoch-36 batch-74 = 1.1326279491186142e-05

Training epoch-36 batch-75
Running loss of epoch-36 batch-75 = 3.0410010367631912e-05

Training epoch-36 batch-76
Running loss of epoch-36 batch-76 = 2.8814654797315598e-05

Training epoch-36 batch-77
Running loss of epoch-36 batch-77 = 5.966285243630409e-06

Training epoch-36 batch-78
Running loss of epoch-36 batch-78 = 2.296105958521366e-05

Training epoch-36 batch-79
Running loss of epoch-36 batch-79 = 2.653617411851883e-05

Training epoch-36 batch-80
Running loss of epoch-36 batch-80 = 2.8390204533934593e-05

Training epoch-36 batch-81
Running loss of epoch-36 batch-81 = 2.9542716220021248e-05

Training epoch-36 batch-82
Running loss of epoch-36 batch-82 = 2.4496810510754585e-05

Training epoch-36 batch-83
Running loss of epoch-36 batch-83 = 5.59748150408268e-06

Training epoch-36 batch-84
Running loss of epoch-36 batch-84 = 2.637249417603016e-05

Training epoch-36 batch-85
Running loss of epoch-36 batch-85 = 1.1087395250797272e-05

Training epoch-36 batch-86
Running loss of epoch-36 batch-86 = 4.12485096603632e-05

Training epoch-36 batch-87
Running loss of epoch-36 batch-87 = 5.033588968217373e-05

Training epoch-36 batch-88
Running loss of epoch-36 batch-88 = 1.176539808511734e-05

Training epoch-36 batch-89
Running loss of epoch-36 batch-89 = 3.054062835872173e-05

Training epoch-36 batch-90
Running loss of epoch-36 batch-90 = 2.0832056179642677e-05

Training epoch-36 batch-91
Running loss of epoch-36 batch-91 = 1.3800803571939468e-05

Training epoch-36 batch-92
Running loss of epoch-36 batch-92 = 1.4751683920621872e-05

Training epoch-36 batch-93
Running loss of epoch-36 batch-93 = 1.3841316103935242e-05

Training epoch-36 batch-94
Running loss of epoch-36 batch-94 = 2.2493768483400345e-05

Training epoch-36 batch-95
Running loss of epoch-36 batch-95 = 2.3845583200454712e-05

Training epoch-36 batch-96
Running loss of epoch-36 batch-96 = 1.4930963516235352e-05

Training epoch-36 batch-97
Running loss of epoch-36 batch-97 = 2.1695159375667572e-05

Training epoch-36 batch-98
Running loss of epoch-36 batch-98 = 3.947317600250244e-05

Training epoch-36 batch-99
Running loss of epoch-36 batch-99 = 1.1056661605834961e-05

Training epoch-36 batch-100
Running loss of epoch-36 batch-100 = 8.99194274097681e-05

Training epoch-36 batch-101
Running loss of epoch-36 batch-101 = 2.505001612007618e-05

Training epoch-36 batch-102
Running loss of epoch-36 batch-102 = 2.108304761350155e-05

Training epoch-36 batch-103
Running loss of epoch-36 batch-103 = 1.0571908205747604e-05

Training epoch-36 batch-104
Running loss of epoch-36 batch-104 = 6.626523099839687e-05

Training epoch-36 batch-105
Running loss of epoch-36 batch-105 = 2.840394154191017e-05

Training epoch-36 batch-106
Running loss of epoch-36 batch-106 = 8.080084808170795e-05

Training epoch-36 batch-107
Running loss of epoch-36 batch-107 = 1.097843050956726e-05

Training epoch-36 batch-108
Running loss of epoch-36 batch-108 = 1.2294389307498932e-05

Training epoch-36 batch-109
Running loss of epoch-36 batch-109 = 1.6174279153347015e-05

Training epoch-36 batch-110
Running loss of epoch-36 batch-110 = 5.5879587307572365e-05

Training epoch-36 batch-111
Running loss of epoch-36 batch-111 = 2.85229180008173e-05

Training epoch-36 batch-112
Running loss of epoch-36 batch-112 = 2.05084215849638e-05

Training epoch-36 batch-113
Running loss of epoch-36 batch-113 = 2.5932444259524345e-05

Training epoch-36 batch-114
Running loss of epoch-36 batch-114 = 1.2628966942429543e-05

Training epoch-36 batch-115
Running loss of epoch-36 batch-115 = 1.5614088624715805e-05

Training epoch-36 batch-116
Running loss of epoch-36 batch-116 = 3.024912439286709e-05

Training epoch-36 batch-117
Running loss of epoch-36 batch-117 = 1.604878343641758e-05

Training epoch-36 batch-118
Running loss of epoch-36 batch-118 = 2.2448832169175148e-05

Training epoch-36 batch-119
Running loss of epoch-36 batch-119 = 2.8027454391121864e-05

Training epoch-36 batch-120
Running loss of epoch-36 batch-120 = 1.665297895669937e-05

Training epoch-36 batch-121
Running loss of epoch-36 batch-121 = 1.9688159227371216e-05

Training epoch-36 batch-122
Running loss of epoch-36 batch-122 = 3.158021718263626e-05

Training epoch-36 batch-123
Running loss of epoch-36 batch-123 = 1.8374528735876083e-05

Training epoch-36 batch-124
Running loss of epoch-36 batch-124 = 1.688930206000805e-05

Training epoch-36 batch-125
Running loss of epoch-36 batch-125 = 1.7262063920497894e-05

Training epoch-36 batch-126
Running loss of epoch-36 batch-126 = 2.3109838366508484e-05

Training epoch-36 batch-127
Running loss of epoch-36 batch-127 = 1.2451782822608948e-05

Training epoch-36 batch-128
Running loss of epoch-36 batch-128 = 1.553073525428772e-05

Training epoch-36 batch-129
Running loss of epoch-36 batch-129 = 1.8817605450749397e-05

Training epoch-36 batch-130
Running loss of epoch-36 batch-130 = 3.551412373781204e-05

Training epoch-36 batch-131
Running loss of epoch-36 batch-131 = 1.5701865777373314e-05

Training epoch-36 batch-132
Running loss of epoch-36 batch-132 = 9.275972843170166e-06

Training epoch-36 batch-133
Running loss of epoch-36 batch-133 = 1.2233620509505272e-05

Training epoch-36 batch-134
Running loss of epoch-36 batch-134 = 3.627571277320385e-05

Training epoch-36 batch-135
Running loss of epoch-36 batch-135 = 2.5755958631634712e-05

Training epoch-36 batch-136
Running loss of epoch-36 batch-136 = 5.657551810145378e-06

Training epoch-36 batch-137
Running loss of epoch-36 batch-137 = 8.685048669576645e-06

Training epoch-36 batch-138
Running loss of epoch-36 batch-138 = 1.758919097483158e-05

Training epoch-36 batch-139
Running loss of epoch-36 batch-139 = 2.216547727584839e-05

Training epoch-36 batch-140
Running loss of epoch-36 batch-140 = 4.909210838377476e-05

Training epoch-36 batch-141
Running loss of epoch-36 batch-141 = 2.1607615053653717e-05

Training epoch-36 batch-142
Running loss of epoch-36 batch-142 = 4.36943955719471e-05

Training epoch-36 batch-143
Running loss of epoch-36 batch-143 = 2.2135674953460693e-05

Training epoch-36 batch-144
Running loss of epoch-36 batch-144 = 1.4978228136897087e-05

Training epoch-36 batch-145
Running loss of epoch-36 batch-145 = 7.584458217024803e-06

Training epoch-36 batch-146
Running loss of epoch-36 batch-146 = 2.7931760996580124e-05

Training epoch-36 batch-147
Running loss of epoch-36 batch-147 = 4.3526291847229004e-05

Training epoch-36 batch-148
Running loss of epoch-36 batch-148 = 1.7556827515363693e-05

Training epoch-36 batch-149
Running loss of epoch-36 batch-149 = 1.554214395582676e-05

Training epoch-36 batch-150
Running loss of epoch-36 batch-150 = 3.6438461393117905e-05

Training epoch-36 batch-151
Running loss of epoch-36 batch-151 = 1.8342630937695503e-05

Training epoch-36 batch-152
Running loss of epoch-36 batch-152 = 2.7534784749150276e-05

Training epoch-36 batch-153
Running loss of epoch-36 batch-153 = 1.626298762857914e-05

Training epoch-36 batch-154
Running loss of epoch-36 batch-154 = 2.1845567971467972e-05

Training epoch-36 batch-155
Running loss of epoch-36 batch-155 = 9.623123332858086e-06

Training epoch-36 batch-156
Running loss of epoch-36 batch-156 = 3.09688039124012e-05

Training epoch-36 batch-157
Running loss of epoch-36 batch-157 = 0.00021953880786895752

Finished training epoch-36.



Average train loss at epoch-36 = 2.359362915158272e-05

Started Evaluation

Average val loss at epoch-36 = 1.0078893918229799

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 91.68 %
Accuracy for class toString is: 86.35 %
Accuracy for class run is: 65.75 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.83 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 72.82 %

Overall Accuracy = 83.58 %

Finished Evaluation



Started training epoch-37


Training epoch-37 batch-1
Running loss of epoch-37 batch-1 = 1.733819954097271e-05

Training epoch-37 batch-2
Running loss of epoch-37 batch-2 = 8.237781003117561e-06

Training epoch-37 batch-3
Running loss of epoch-37 batch-3 = 3.136228770017624e-05

Training epoch-37 batch-4
Running loss of epoch-37 batch-4 = 3.0210008844733238e-05

Training epoch-37 batch-5
Running loss of epoch-37 batch-5 = 8.113682270050049e-06

Training epoch-37 batch-6
Running loss of epoch-37 batch-6 = 1.0652001947164536e-05

Training epoch-37 batch-7
Running loss of epoch-37 batch-7 = 1.074373722076416e-05

Training epoch-37 batch-8
Running loss of epoch-37 batch-8 = 2.7826405130326748e-05

Training epoch-37 batch-9
Running loss of epoch-37 batch-9 = 1.2587988749146461e-05

Training epoch-37 batch-10
Running loss of epoch-37 batch-10 = 1.636892557144165e-05

Training epoch-37 batch-11
Running loss of epoch-37 batch-11 = 2.3777829483151436e-05

Training epoch-37 batch-12
Running loss of epoch-37 batch-12 = 1.064385287463665e-05

Training epoch-37 batch-13
Running loss of epoch-37 batch-13 = 2.392963506281376e-05

Training epoch-37 batch-14
Running loss of epoch-37 batch-14 = 2.271542325615883e-05

Training epoch-37 batch-15
Running loss of epoch-37 batch-15 = 2.563255839049816e-05

Training epoch-37 batch-16
Running loss of epoch-37 batch-16 = 1.4346325770020485e-05

Training epoch-37 batch-17
Running loss of epoch-37 batch-17 = 8.951174095273018e-06

Training epoch-37 batch-18
Running loss of epoch-37 batch-18 = 3.217277117073536e-05

Training epoch-37 batch-19
Running loss of epoch-37 batch-19 = 1.76925677806139e-05

Training epoch-37 batch-20
Running loss of epoch-37 batch-20 = 1.2408941984176636e-05

Training epoch-37 batch-21
Running loss of epoch-37 batch-21 = 3.5792356356978416e-05

Training epoch-37 batch-22
Running loss of epoch-37 batch-22 = 1.2040138244628906e-05

Training epoch-37 batch-23
Running loss of epoch-37 batch-23 = 1.4489749446511269e-05

Training epoch-37 batch-24
Running loss of epoch-37 batch-24 = 1.3151904568076134e-05

Training epoch-37 batch-25
Running loss of epoch-37 batch-25 = 1.559755764901638e-05

Training epoch-37 batch-26
Running loss of epoch-37 batch-26 = 1.295795664191246e-05

Training epoch-37 batch-27
Running loss of epoch-37 batch-27 = 2.4592038244009018e-05

Training epoch-37 batch-28
Running loss of epoch-37 batch-28 = 7.578171789646149e-06

Training epoch-37 batch-29
Running loss of epoch-37 batch-29 = 1.1266674846410751e-05

Training epoch-37 batch-30
Running loss of epoch-37 batch-30 = 2.53971666097641e-05

Training epoch-37 batch-31
Running loss of epoch-37 batch-31 = 1.294165849685669e-05

Training epoch-37 batch-32
Running loss of epoch-37 batch-32 = 9.186752140522003e-05

Training epoch-37 batch-33
Running loss of epoch-37 batch-33 = 2.537434920668602e-05

Training epoch-37 batch-34
Running loss of epoch-37 batch-34 = 1.3123033568263054e-05

Training epoch-37 batch-35
Running loss of epoch-37 batch-35 = 1.4560529962182045e-05

Training epoch-37 batch-36
Running loss of epoch-37 batch-36 = 5.493871867656708e-05

Training epoch-37 batch-37
Running loss of epoch-37 batch-37 = 1.7821788787841797e-05

Training epoch-37 batch-38
Running loss of epoch-37 batch-38 = 1.7464160919189453e-05

Training epoch-37 batch-39
Running loss of epoch-37 batch-39 = 1.2468546628952026e-05

Training epoch-37 batch-40
Running loss of epoch-37 batch-40 = 2.759648486971855e-05

Training epoch-37 batch-41
Running loss of epoch-37 batch-41 = 4.2889732867479324e-05

Training epoch-37 batch-42
Running loss of epoch-37 batch-42 = 1.871725544333458e-05

Training epoch-37 batch-43
Running loss of epoch-37 batch-43 = 2.7516856789588928e-05

Training epoch-37 batch-44
Running loss of epoch-37 batch-44 = 2.9724091291427612e-05

Training epoch-37 batch-45
Running loss of epoch-37 batch-45 = 4.871748387813568e-06

Training epoch-37 batch-46
Running loss of epoch-37 batch-46 = 1.8529826775193214e-05

Training epoch-37 batch-47
Running loss of epoch-37 batch-47 = 2.3212283849716187e-05

Training epoch-37 batch-48
Running loss of epoch-37 batch-48 = 1.9160564988851547e-05

Training epoch-37 batch-49
Running loss of epoch-37 batch-49 = 1.2230128049850464e-05

Training epoch-37 batch-50
Running loss of epoch-37 batch-50 = 1.0278308764100075e-05

Training epoch-37 batch-51
Running loss of epoch-37 batch-51 = 9.18773002922535e-06

Training epoch-37 batch-52
Running loss of epoch-37 batch-52 = 1.4241551980376244e-05

Training epoch-37 batch-53
Running loss of epoch-37 batch-53 = 3.882916644215584e-05

Training epoch-37 batch-54
Running loss of epoch-37 batch-54 = 1.7610611394047737e-05

Training epoch-37 batch-55
Running loss of epoch-37 batch-55 = 3.204541280865669e-05

Training epoch-37 batch-56
Running loss of epoch-37 batch-56 = 1.5403376892209053e-05

Training epoch-37 batch-57
Running loss of epoch-37 batch-57 = 2.228282392024994e-05

Training epoch-37 batch-58
Running loss of epoch-37 batch-58 = 1.3690674677491188e-05

Training epoch-37 batch-59
Running loss of epoch-37 batch-59 = 2.492882777005434e-05

Training epoch-37 batch-60
Running loss of epoch-37 batch-60 = 2.6004156097769737e-05

Training epoch-37 batch-61
Running loss of epoch-37 batch-61 = 1.597544178366661e-05

Training epoch-37 batch-62
Running loss of epoch-37 batch-62 = 3.3247750252485275e-05

Training epoch-37 batch-63
Running loss of epoch-37 batch-63 = 2.5054672732949257e-05

Training epoch-37 batch-64
Running loss of epoch-37 batch-64 = 1.839548349380493e-05

Training epoch-37 batch-65
Running loss of epoch-37 batch-65 = 1.3781245797872543e-05

Training epoch-37 batch-66
Running loss of epoch-37 batch-66 = 1.1538621038198471e-05

Training epoch-37 batch-67
Running loss of epoch-37 batch-67 = 1.1449912562966347e-05

Training epoch-37 batch-68
Running loss of epoch-37 batch-68 = 1.7219223082065582e-05

Training epoch-37 batch-69
Running loss of epoch-37 batch-69 = 1.4270655810832977e-05

Training epoch-37 batch-70
Running loss of epoch-37 batch-70 = 2.9325252398848534e-05

Training epoch-37 batch-71
Running loss of epoch-37 batch-71 = 1.4495337381958961e-05

Training epoch-37 batch-72
Running loss of epoch-37 batch-72 = 2.6204274035990238e-05

Training epoch-37 batch-73
Running loss of epoch-37 batch-73 = 2.0035775378346443e-05

Training epoch-37 batch-74
Running loss of epoch-37 batch-74 = 3.15012875944376e-05

Training epoch-37 batch-75
Running loss of epoch-37 batch-75 = 1.3692304491996765e-05

Training epoch-37 batch-76
Running loss of epoch-37 batch-76 = 2.1998537704348564e-05

Training epoch-37 batch-77
Running loss of epoch-37 batch-77 = 1.3437820598483086e-05

Training epoch-37 batch-78
Running loss of epoch-37 batch-78 = 2.603069879114628e-05

Training epoch-37 batch-79
Running loss of epoch-37 batch-79 = 1.4588935300707817e-05

Training epoch-37 batch-80
Running loss of epoch-37 batch-80 = 1.7232727259397507e-05

Training epoch-37 batch-81
Running loss of epoch-37 batch-81 = 2.6320572942495346e-05

Training epoch-37 batch-82
Running loss of epoch-37 batch-82 = 2.7099158614873886e-05

Training epoch-37 batch-83
Running loss of epoch-37 batch-83 = 1.9652070477604866e-05

Training epoch-37 batch-84
Running loss of epoch-37 batch-84 = 1.522805541753769e-05

Training epoch-37 batch-85
Running loss of epoch-37 batch-85 = 7.769744843244553e-05

Training epoch-37 batch-86
Running loss of epoch-37 batch-86 = 2.2626249119639397e-05

Training epoch-37 batch-87
Running loss of epoch-37 batch-87 = 8.834758773446083e-06

Training epoch-37 batch-88
Running loss of epoch-37 batch-88 = 2.340320497751236e-05

Training epoch-37 batch-89
Running loss of epoch-37 batch-89 = 1.4693010598421097e-05

Training epoch-37 batch-90
Running loss of epoch-37 batch-90 = 3.719143569469452e-05

Training epoch-37 batch-91
Running loss of epoch-37 batch-91 = 5.956273525953293e-06

Training epoch-37 batch-92
Running loss of epoch-37 batch-92 = 1.746043562889099e-05

Training epoch-37 batch-93
Running loss of epoch-37 batch-93 = 2.5106011889874935e-05

Training epoch-37 batch-94
Running loss of epoch-37 batch-94 = 1.653260551393032e-05

Training epoch-37 batch-95
Running loss of epoch-37 batch-95 = 1.6552163287997246e-05

Training epoch-37 batch-96
Running loss of epoch-37 batch-96 = 2.5202753022313118e-05

Training epoch-37 batch-97
Running loss of epoch-37 batch-97 = 1.5236437320709229e-05

Training epoch-37 batch-98
Running loss of epoch-37 batch-98 = 2.7416041120886803e-05

Training epoch-37 batch-99
Running loss of epoch-37 batch-99 = 2.868124283850193e-05

Training epoch-37 batch-100
Running loss of epoch-37 batch-100 = 1.1112075299024582e-05

Training epoch-37 batch-101
Running loss of epoch-37 batch-101 = 9.54512506723404e-06

Training epoch-37 batch-102
Running loss of epoch-37 batch-102 = 9.18540172278881e-06

Training epoch-37 batch-103
Running loss of epoch-37 batch-103 = 9.472714737057686e-06

Training epoch-37 batch-104
Running loss of epoch-37 batch-104 = 2.4368055164813995e-05

Training epoch-37 batch-105
Running loss of epoch-37 batch-105 = 1.8533552065491676e-05

Training epoch-37 batch-106
Running loss of epoch-37 batch-106 = 1.5396159142255783e-05

Training epoch-37 batch-107
Running loss of epoch-37 batch-107 = 1.0076677426695824e-05

Training epoch-37 batch-108
Running loss of epoch-37 batch-108 = 1.1989613994956017e-05

Training epoch-37 batch-109
Running loss of epoch-37 batch-109 = 2.7280999347567558e-05

Training epoch-37 batch-110
Running loss of epoch-37 batch-110 = 1.2979842722415924e-05

Training epoch-37 batch-111
Running loss of epoch-37 batch-111 = 4.3469248339533806e-05

Training epoch-37 batch-112
Running loss of epoch-37 batch-112 = 9.40915197134018e-06

Training epoch-37 batch-113
Running loss of epoch-37 batch-113 = 1.6553793102502823e-05

Training epoch-37 batch-114
Running loss of epoch-37 batch-114 = 1.376960426568985e-05

Training epoch-37 batch-115
Running loss of epoch-37 batch-115 = 7.643364369869232e-06

Training epoch-37 batch-116
Running loss of epoch-37 batch-116 = 1.4260644093155861e-05

Training epoch-37 batch-117
Running loss of epoch-37 batch-117 = 2.573244273662567e-05

Training epoch-37 batch-118
Running loss of epoch-37 batch-118 = 2.1121231839060783e-05

Training epoch-37 batch-119
Running loss of epoch-37 batch-119 = 2.0917272195219994e-05

Training epoch-37 batch-120
Running loss of epoch-37 batch-120 = 1.2750504538416862e-05

Training epoch-37 batch-121
Running loss of epoch-37 batch-121 = 2.644723281264305e-05

Training epoch-37 batch-122
Running loss of epoch-37 batch-122 = 1.8598046153783798e-05

Training epoch-37 batch-123
Running loss of epoch-37 batch-123 = 2.484791912138462e-05

Training epoch-37 batch-124
Running loss of epoch-37 batch-124 = 3.714696504175663e-05

Training epoch-37 batch-125
Running loss of epoch-37 batch-125 = 3.896537236869335e-05

Training epoch-37 batch-126
Running loss of epoch-37 batch-126 = 1.1810334399342537e-05

Training epoch-37 batch-127
Running loss of epoch-37 batch-127 = 2.7095084078609943e-05

Training epoch-37 batch-128
Running loss of epoch-37 batch-128 = 7.385620847344398e-05

Training epoch-37 batch-129
Running loss of epoch-37 batch-129 = 2.0035775378346443e-05

Training epoch-37 batch-130
Running loss of epoch-37 batch-130 = 3.029685467481613e-05

Training epoch-37 batch-131
Running loss of epoch-37 batch-131 = 8.03614966571331e-06

Training epoch-37 batch-132
Running loss of epoch-37 batch-132 = 1.5514902770519257e-05

Training epoch-37 batch-133
Running loss of epoch-37 batch-133 = 1.3796845450997353e-05

Training epoch-37 batch-134
Running loss of epoch-37 batch-134 = 2.4295644834637642e-05

Training epoch-37 batch-135
Running loss of epoch-37 batch-135 = 1.642969436943531e-05

Training epoch-37 batch-136
Running loss of epoch-37 batch-136 = 3.5011209547519684e-05

Training epoch-37 batch-137
Running loss of epoch-37 batch-137 = 1.630326732993126e-05

Training epoch-37 batch-138
Running loss of epoch-37 batch-138 = 1.934426836669445e-05

Training epoch-37 batch-139
Running loss of epoch-37 batch-139 = 9.774696081876755e-06

Training epoch-37 batch-140
Running loss of epoch-37 batch-140 = 3.38417012244463e-05

Training epoch-37 batch-141
Running loss of epoch-37 batch-141 = 2.378947101533413e-05

Training epoch-37 batch-142
Running loss of epoch-37 batch-142 = 4.318682476878166e-05

Training epoch-37 batch-143
Running loss of epoch-37 batch-143 = 1.4092307537794113e-05

Training epoch-37 batch-144
Running loss of epoch-37 batch-144 = 1.139054074883461e-05

Training epoch-37 batch-145
Running loss of epoch-37 batch-145 = 1.5092780813574791e-05

Training epoch-37 batch-146
Running loss of epoch-37 batch-146 = 2.220110036432743e-05

Training epoch-37 batch-147
Running loss of epoch-37 batch-147 = 1.7643440514802933e-05

Training epoch-37 batch-148
Running loss of epoch-37 batch-148 = 1.485086977481842e-05

Training epoch-37 batch-149
Running loss of epoch-37 batch-149 = 5.04031777381897e-05

Training epoch-37 batch-150
Running loss of epoch-37 batch-150 = 2.5530578568577766e-05

Training epoch-37 batch-151
Running loss of epoch-37 batch-151 = 9.597046300768852e-06

Training epoch-37 batch-152
Running loss of epoch-37 batch-152 = 2.5297049432992935e-05

Training epoch-37 batch-153
Running loss of epoch-37 batch-153 = 8.497852832078934e-06

Training epoch-37 batch-154
Running loss of epoch-37 batch-154 = 2.697634045034647e-05

Training epoch-37 batch-155
Running loss of epoch-37 batch-155 = 8.548842743039131e-06

Training epoch-37 batch-156
Running loss of epoch-37 batch-156 = 5.36586157977581e-05

Training epoch-37 batch-157
Running loss of epoch-37 batch-157 = 2.3327767848968506e-05

Finished training epoch-37.



Average train loss at epoch-37 = 2.11971715092659e-05

Started Evaluation

Average val loss at epoch-37 = 1.0168698833406922

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 92.13 %
Accuracy for class onCreate is: 91.79 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 65.98 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.93 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.58 %

Finished Evaluation



Started training epoch-38


Training epoch-38 batch-1
Running loss of epoch-38 batch-1 = 1.3477867469191551e-05

Training epoch-38 batch-2
Running loss of epoch-38 batch-2 = 4.431046545505524e-05

Training epoch-38 batch-3
Running loss of epoch-38 batch-3 = 2.5310320779681206e-05

Training epoch-38 batch-4
Running loss of epoch-38 batch-4 = 1.691468060016632e-05

Training epoch-38 batch-5
Running loss of epoch-38 batch-5 = 2.220342867076397e-05

Training epoch-38 batch-6
Running loss of epoch-38 batch-6 = 2.9398826882243156e-05

Training epoch-38 batch-7
Running loss of epoch-38 batch-7 = 5.2491435781121254e-05

Training epoch-38 batch-8
Running loss of epoch-38 batch-8 = 1.2059928849339485e-05

Training epoch-38 batch-9
Running loss of epoch-38 batch-9 = 3.8697849959135056e-05

Training epoch-38 batch-10
Running loss of epoch-38 batch-10 = 1.0829651728272438e-05

Training epoch-38 batch-11
Running loss of epoch-38 batch-11 = 9.3863345682621e-06

Training epoch-38 batch-12
Running loss of epoch-38 batch-12 = 2.508191391825676e-05

Training epoch-38 batch-13
Running loss of epoch-38 batch-13 = 7.283641025424004e-06

Training epoch-38 batch-14
Running loss of epoch-38 batch-14 = 3.088940866291523e-05

Training epoch-38 batch-15
Running loss of epoch-38 batch-15 = 1.7532380297780037e-05

Training epoch-38 batch-16
Running loss of epoch-38 batch-16 = 1.6307923942804337e-05

Training epoch-38 batch-17
Running loss of epoch-38 batch-17 = 2.2209947928786278e-05

Training epoch-38 batch-18
Running loss of epoch-38 batch-18 = 1.8171733245253563e-05

Training epoch-38 batch-19
Running loss of epoch-38 batch-19 = 1.590792089700699e-05

Training epoch-38 batch-20
Running loss of epoch-38 batch-20 = 7.2857365012168884e-06

Training epoch-38 batch-21
Running loss of epoch-38 batch-21 = 2.0095380023121834e-05

Training epoch-38 batch-22
Running loss of epoch-38 batch-22 = 5.0350790843367577e-05

Training epoch-38 batch-23
Running loss of epoch-38 batch-23 = 2.5289366021752357e-05

Training epoch-38 batch-24
Running loss of epoch-38 batch-24 = 1.1935364454984665e-05

Training epoch-38 batch-25
Running loss of epoch-38 batch-25 = 2.3692147806286812e-05

Training epoch-38 batch-26
Running loss of epoch-38 batch-26 = 1.7969170585274696e-05

Training epoch-38 batch-27
Running loss of epoch-38 batch-27 = 1.1340947821736336e-05

Training epoch-38 batch-28
Running loss of epoch-38 batch-28 = 1.3737473636865616e-05

Training epoch-38 batch-29
Running loss of epoch-38 batch-29 = 9.052455425262451e-06

Training epoch-38 batch-30
Running loss of epoch-38 batch-30 = 7.526949048042297e-06

Training epoch-38 batch-31
Running loss of epoch-38 batch-31 = 1.8007587641477585e-05

Training epoch-38 batch-32
Running loss of epoch-38 batch-32 = 1.2715114280581474e-05

Training epoch-38 batch-33
Running loss of epoch-38 batch-33 = 1.21991615742445e-05

Training epoch-38 batch-34
Running loss of epoch-38 batch-34 = 4.279753193259239e-05

Training epoch-38 batch-35
Running loss of epoch-38 batch-35 = 1.4652032405138016e-05

Training epoch-38 batch-36
Running loss of epoch-38 batch-36 = 7.267296314239502e-05

Training epoch-38 batch-37
Running loss of epoch-38 batch-37 = 1.1253869161009789e-05

Training epoch-38 batch-38
Running loss of epoch-38 batch-38 = 2.0985957235097885e-05

Training epoch-38 batch-39
Running loss of epoch-38 batch-39 = 2.032262273132801e-05

Training epoch-38 batch-40
Running loss of epoch-38 batch-40 = 1.0612886399030685e-05

Training epoch-38 batch-41
Running loss of epoch-38 batch-41 = 1.007108949124813e-05

Training epoch-38 batch-42
Running loss of epoch-38 batch-42 = 1.1789146810770035e-05

Training epoch-38 batch-43
Running loss of epoch-38 batch-43 = 9.606592357158661e-06

Training epoch-38 batch-44
Running loss of epoch-38 batch-44 = 1.5021534636616707e-05

Training epoch-38 batch-45
Running loss of epoch-38 batch-45 = 1.1914176866412163e-05

Training epoch-38 batch-46
Running loss of epoch-38 batch-46 = 1.31197739392519e-05

Training epoch-38 batch-47
Running loss of epoch-38 batch-47 = 1.900014467537403e-05

Training epoch-38 batch-48
Running loss of epoch-38 batch-48 = 3.837607800960541e-05

Training epoch-38 batch-49
Running loss of epoch-38 batch-49 = 1.4265067875385284e-05

Training epoch-38 batch-50
Running loss of epoch-38 batch-50 = 1.5212222933769226e-05

Training epoch-38 batch-51
Running loss of epoch-38 batch-51 = 2.780812792479992e-05

Training epoch-38 batch-52
Running loss of epoch-38 batch-52 = 3.125867806375027e-05

Training epoch-38 batch-53
Running loss of epoch-38 batch-53 = 2.0525185391306877e-05

Training epoch-38 batch-54
Running loss of epoch-38 batch-54 = 9.005190804600716e-06

Training epoch-38 batch-55
Running loss of epoch-38 batch-55 = 1.20548065751791e-05

Training epoch-38 batch-56
Running loss of epoch-38 batch-56 = 5.9925951063632965e-06

Training epoch-38 batch-57
Running loss of epoch-38 batch-57 = 1.2590084224939346e-05

Training epoch-38 batch-58
Running loss of epoch-38 batch-58 = 2.3150932975113392e-05

Training epoch-38 batch-59
Running loss of epoch-38 batch-59 = 1.9156374037265778e-05

Training epoch-38 batch-60
Running loss of epoch-38 batch-60 = 1.581478863954544e-05

Training epoch-38 batch-61
Running loss of epoch-38 batch-61 = 9.383540600538254e-06

Training epoch-38 batch-62
Running loss of epoch-38 batch-62 = 2.219690941274166e-05

Training epoch-38 batch-63
Running loss of epoch-38 batch-63 = 3.687338903546333e-05

Training epoch-38 batch-64
Running loss of epoch-38 batch-64 = 1.8055317923426628e-05

Training epoch-38 batch-65
Running loss of epoch-38 batch-65 = 1.5455996617674828e-05

Training epoch-38 batch-66
Running loss of epoch-38 batch-66 = 1.01535115391016e-05

Training epoch-38 batch-67
Running loss of epoch-38 batch-67 = 1.585274003446102e-05

Training epoch-38 batch-68
Running loss of epoch-38 batch-68 = 2.8624432161450386e-05

Training epoch-38 batch-69
Running loss of epoch-38 batch-69 = 1.9254162907600403e-05

Training epoch-38 batch-70
Running loss of epoch-38 batch-70 = 2.550194039940834e-05

Training epoch-38 batch-71
Running loss of epoch-38 batch-71 = 1.7380574718117714e-05

Training epoch-38 batch-72
Running loss of epoch-38 batch-72 = 1.8903985619544983e-05

Training epoch-38 batch-73
Running loss of epoch-38 batch-73 = 1.419917680323124e-05

Training epoch-38 batch-74
Running loss of epoch-38 batch-74 = 9.515322744846344e-06

Training epoch-38 batch-75
Running loss of epoch-38 batch-75 = 9.375624358654022e-06

Training epoch-38 batch-76
Running loss of epoch-38 batch-76 = 2.4404842406511307e-05

Training epoch-38 batch-77
Running loss of epoch-38 batch-77 = 1.0880175977945328e-05

Training epoch-38 batch-78
Running loss of epoch-38 batch-78 = 1.672143116593361e-05

Training epoch-38 batch-79
Running loss of epoch-38 batch-79 = 1.6267411410808563e-05

Training epoch-38 batch-80
Running loss of epoch-38 batch-80 = 2.1405168808996677e-05

Training epoch-38 batch-81
Running loss of epoch-38 batch-81 = 8.962815627455711e-06

Training epoch-38 batch-82
Running loss of epoch-38 batch-82 = 3.924500197172165e-05

Training epoch-38 batch-83
Running loss of epoch-38 batch-83 = 1.751701347529888e-05

Training epoch-38 batch-84
Running loss of epoch-38 batch-84 = 1.2704404070973396e-05

Training epoch-38 batch-85
Running loss of epoch-38 batch-85 = 7.015187293291092e-06

Training epoch-38 batch-86
Running loss of epoch-38 batch-86 = 2.4239765480160713e-05

Training epoch-38 batch-87
Running loss of epoch-38 batch-87 = 2.2531719878315926e-05

Training epoch-38 batch-88
Running loss of epoch-38 batch-88 = 1.961470115929842e-05

Training epoch-38 batch-89
Running loss of epoch-38 batch-89 = 2.206745557487011e-05

Training epoch-38 batch-90
Running loss of epoch-38 batch-90 = 1.4932127669453621e-05

Training epoch-38 batch-91
Running loss of epoch-38 batch-91 = 9.90135595202446e-06

Training epoch-38 batch-92
Running loss of epoch-38 batch-92 = 7.179193198680878e-05

Training epoch-38 batch-93
Running loss of epoch-38 batch-93 = 1.7758924514055252e-05

Training epoch-38 batch-94
Running loss of epoch-38 batch-94 = 4.40073199570179e-05

Training epoch-38 batch-95
Running loss of epoch-38 batch-95 = 2.5107525289058685e-05

Training epoch-38 batch-96
Running loss of epoch-38 batch-96 = 1.0584713891148567e-05

Training epoch-38 batch-97
Running loss of epoch-38 batch-97 = 2.128537744283676e-05

Training epoch-38 batch-98
Running loss of epoch-38 batch-98 = 1.015583984553814e-05

Training epoch-38 batch-99
Running loss of epoch-38 batch-99 = 1.2037809938192368e-05

Training epoch-38 batch-100
Running loss of epoch-38 batch-100 = 9.288312867283821e-06

Training epoch-38 batch-101
Running loss of epoch-38 batch-101 = 2.7806032449007034e-05

Training epoch-38 batch-102
Running loss of epoch-38 batch-102 = 1.841643825173378e-05

Training epoch-38 batch-103
Running loss of epoch-38 batch-103 = 1.852656714618206e-05

Training epoch-38 batch-104
Running loss of epoch-38 batch-104 = 1.0756310075521469e-05

Training epoch-38 batch-105
Running loss of epoch-38 batch-105 = 1.6392674297094345e-05

Training epoch-38 batch-106
Running loss of epoch-38 batch-106 = 2.583698369562626e-05

Training epoch-38 batch-107
Running loss of epoch-38 batch-107 = 3.215554170310497e-05

Training epoch-38 batch-108
Running loss of epoch-38 batch-108 = 1.8509570509195328e-05

Training epoch-38 batch-109
Running loss of epoch-38 batch-109 = 2.1305866539478302e-05

Training epoch-38 batch-110
Running loss of epoch-38 batch-110 = 2.480205148458481e-05

Training epoch-38 batch-111
Running loss of epoch-38 batch-111 = 1.7339130863547325e-05

Training epoch-38 batch-112
Running loss of epoch-38 batch-112 = 1.6550300642848015e-05

Training epoch-38 batch-113
Running loss of epoch-38 batch-113 = 1.0249204933643341e-05

Training epoch-38 batch-114
Running loss of epoch-38 batch-114 = 1.1804979294538498e-05

Training epoch-38 batch-115
Running loss of epoch-38 batch-115 = 3.484985791146755e-05

Training epoch-38 batch-116
Running loss of epoch-38 batch-116 = 4.063569940626621e-05

Training epoch-38 batch-117
Running loss of epoch-38 batch-117 = 1.4827121049165726e-05

Training epoch-38 batch-118
Running loss of epoch-38 batch-118 = 9.708106517791748e-06

Training epoch-38 batch-119
Running loss of epoch-38 batch-119 = 6.904592737555504e-06

Training epoch-38 batch-120
Running loss of epoch-38 batch-120 = 1.45037192851305e-05

Training epoch-38 batch-121
Running loss of epoch-38 batch-121 = 1.8626218661665916e-05

Training epoch-38 batch-122
Running loss of epoch-38 batch-122 = 1.8241116777062416e-05

Training epoch-38 batch-123
Running loss of epoch-38 batch-123 = 1.6418052837252617e-05

Training epoch-38 batch-124
Running loss of epoch-38 batch-124 = 1.0792165994644165e-05

Training epoch-38 batch-125
Running loss of epoch-38 batch-125 = 1.8114689737558365e-05

Training epoch-38 batch-126
Running loss of epoch-38 batch-126 = 3.885943442583084e-05

Training epoch-38 batch-127
Running loss of epoch-38 batch-127 = 2.6020687073469162e-05

Training epoch-38 batch-128
Running loss of epoch-38 batch-128 = 1.735263504087925e-05

Training epoch-38 batch-129
Running loss of epoch-38 batch-129 = 1.7735175788402557e-05

Training epoch-38 batch-130
Running loss of epoch-38 batch-130 = 2.5148503482341766e-05

Training epoch-38 batch-131
Running loss of epoch-38 batch-131 = 1.8239719793200493e-05

Training epoch-38 batch-132
Running loss of epoch-38 batch-132 = 1.6271835193037987e-05

Training epoch-38 batch-133
Running loss of epoch-38 batch-133 = 2.583768218755722e-05

Training epoch-38 batch-134
Running loss of epoch-38 batch-134 = 2.5405781343579292e-05

Training epoch-38 batch-135
Running loss of epoch-38 batch-135 = 2.3003900423645973e-05

Training epoch-38 batch-136
Running loss of epoch-38 batch-136 = 5.984911695122719e-06

Training epoch-38 batch-137
Running loss of epoch-38 batch-137 = 2.574012614786625e-05

Training epoch-38 batch-138
Running loss of epoch-38 batch-138 = 1.2394506484270096e-05

Training epoch-38 batch-139
Running loss of epoch-38 batch-139 = 1.3733282685279846e-05

Training epoch-38 batch-140
Running loss of epoch-38 batch-140 = 2.870825119316578e-05

Training epoch-38 batch-141
Running loss of epoch-38 batch-141 = 1.1580297723412514e-05

Training epoch-38 batch-142
Running loss of epoch-38 batch-142 = 3.202189691364765e-05

Training epoch-38 batch-143
Running loss of epoch-38 batch-143 = 3.2700831070542336e-05

Training epoch-38 batch-144
Running loss of epoch-38 batch-144 = 1.2343982234597206e-05

Training epoch-38 batch-145
Running loss of epoch-38 batch-145 = 1.0656891390681267e-05

Training epoch-38 batch-146
Running loss of epoch-38 batch-146 = 1.7425278201699257e-05

Training epoch-38 batch-147
Running loss of epoch-38 batch-147 = 1.61763746291399e-05

Training epoch-38 batch-148
Running loss of epoch-38 batch-148 = 8.821720257401466e-06

Training epoch-38 batch-149
Running loss of epoch-38 batch-149 = 1.2868084013462067e-05

Training epoch-38 batch-150
Running loss of epoch-38 batch-150 = 1.845136284828186e-05

Training epoch-38 batch-151
Running loss of epoch-38 batch-151 = 9.90019179880619e-06

Training epoch-38 batch-152
Running loss of epoch-38 batch-152 = 1.660711131989956e-05

Training epoch-38 batch-153
Running loss of epoch-38 batch-153 = 2.841954119503498e-05

Training epoch-38 batch-154
Running loss of epoch-38 batch-154 = 8.852919563651085e-06

Training epoch-38 batch-155
Running loss of epoch-38 batch-155 = 1.0893912985920906e-05

Training epoch-38 batch-156
Running loss of epoch-38 batch-156 = 2.379482612013817e-05

Training epoch-38 batch-157
Running loss of epoch-38 batch-157 = 3.2998621463775635e-05

Finished training epoch-38.



Average train loss at epoch-38 = 1.954638734459877e-05

Started Evaluation

Average val loss at epoch-38 = 1.0118067397567374

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 91.90 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 65.53 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.73 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.62 %

Finished Evaluation



Started training epoch-39


Training epoch-39 batch-1
Running loss of epoch-39 batch-1 = 1.8144259229302406e-05

Training epoch-39 batch-2
Running loss of epoch-39 batch-2 = 2.3857923224568367e-05

Training epoch-39 batch-3
Running loss of epoch-39 batch-3 = 1.3024779036641121e-05

Training epoch-39 batch-4
Running loss of epoch-39 batch-4 = 1.4028744772076607e-05

Training epoch-39 batch-5
Running loss of epoch-39 batch-5 = 3.666197881102562e-05

Training epoch-39 batch-6
Running loss of epoch-39 batch-6 = 1.9196653738617897e-05

Training epoch-39 batch-7
Running loss of epoch-39 batch-7 = 1.3856450095772743e-05

Training epoch-39 batch-8
Running loss of epoch-39 batch-8 = 1.1776108294725418e-05

Training epoch-39 batch-9
Running loss of epoch-39 batch-9 = 2.0510749891400337e-05

Training epoch-39 batch-10
Running loss of epoch-39 batch-10 = 1.014862209558487e-05

Training epoch-39 batch-11
Running loss of epoch-39 batch-11 = 2.1626707166433334e-05

Training epoch-39 batch-12
Running loss of epoch-39 batch-12 = 1.866556704044342e-05

Training epoch-39 batch-13
Running loss of epoch-39 batch-13 = 7.892260327935219e-06

Training epoch-39 batch-14
Running loss of epoch-39 batch-14 = 1.4080433174967766e-05

Training epoch-39 batch-15
Running loss of epoch-39 batch-15 = 1.7563113942742348e-05

Training epoch-39 batch-16
Running loss of epoch-39 batch-16 = 1.1457828804850578e-05

Training epoch-39 batch-17
Running loss of epoch-39 batch-17 = 1.892424188554287e-05

Training epoch-39 batch-18
Running loss of epoch-39 batch-18 = 9.951181709766388e-06

Training epoch-39 batch-19
Running loss of epoch-39 batch-19 = 1.071905717253685e-05

Training epoch-39 batch-20
Running loss of epoch-39 batch-20 = 1.858500763773918e-05

Training epoch-39 batch-21
Running loss of epoch-39 batch-21 = 1.3774493709206581e-05

Training epoch-39 batch-22
Running loss of epoch-39 batch-22 = 9.400071576237679e-06

Training epoch-39 batch-23
Running loss of epoch-39 batch-23 = 5.777226760983467e-06

Training epoch-39 batch-24
Running loss of epoch-39 batch-24 = 5.455687642097473e-06

Training epoch-39 batch-25
Running loss of epoch-39 batch-25 = 2.5301938876509666e-05

Training epoch-39 batch-26
Running loss of epoch-39 batch-26 = 1.2403354048728943e-05

Training epoch-39 batch-27
Running loss of epoch-39 batch-27 = 1.727626658976078e-05

Training epoch-39 batch-28
Running loss of epoch-39 batch-28 = 1.5780096873641014e-05

Training epoch-39 batch-29
Running loss of epoch-39 batch-29 = 1.2484146282076836e-05

Training epoch-39 batch-30
Running loss of epoch-39 batch-30 = 2.0876992493867874e-05

Training epoch-39 batch-31
Running loss of epoch-39 batch-31 = 4.795682616531849e-05

Training epoch-39 batch-32
Running loss of epoch-39 batch-32 = 1.4099758118391037e-05

Training epoch-39 batch-33
Running loss of epoch-39 batch-33 = 2.4706823751330376e-05

Training epoch-39 batch-34
Running loss of epoch-39 batch-34 = 2.324627712368965e-05

Training epoch-39 batch-35
Running loss of epoch-39 batch-35 = 1.8740305677056313e-05

Training epoch-39 batch-36
Running loss of epoch-39 batch-36 = 1.1203344911336899e-05

Training epoch-39 batch-37
Running loss of epoch-39 batch-37 = 2.5584828108549118e-05

Training epoch-39 batch-38
Running loss of epoch-39 batch-38 = 9.302515536546707e-06

Training epoch-39 batch-39
Running loss of epoch-39 batch-39 = 8.245464414358139e-06

Training epoch-39 batch-40
Running loss of epoch-39 batch-40 = 2.737506292760372e-05

Training epoch-39 batch-41
Running loss of epoch-39 batch-41 = 2.033635973930359e-05

Training epoch-39 batch-42
Running loss of epoch-39 batch-42 = 4.066154360771179e-06

Training epoch-39 batch-43
Running loss of epoch-39 batch-43 = 1.967395655810833e-05

Training epoch-39 batch-44
Running loss of epoch-39 batch-44 = 1.5940284356474876e-05

Training epoch-39 batch-45
Running loss of epoch-39 batch-45 = 1.7451820895075798e-05

Training epoch-39 batch-46
Running loss of epoch-39 batch-46 = 1.95216853171587e-05

Training epoch-39 batch-47
Running loss of epoch-39 batch-47 = 3.086915239691734e-05

Training epoch-39 batch-48
Running loss of epoch-39 batch-48 = 2.0265812054276466e-05

Training epoch-39 batch-49
Running loss of epoch-39 batch-49 = 1.2523960322141647e-05

Training epoch-39 batch-50
Running loss of epoch-39 batch-50 = 6.0107558965682983e-05

Training epoch-39 batch-51
Running loss of epoch-39 batch-51 = 1.0577961802482605e-05

Training epoch-39 batch-52
Running loss of epoch-39 batch-52 = 2.139247953891754e-05

Training epoch-39 batch-53
Running loss of epoch-39 batch-53 = 1.440953928977251e-05

Training epoch-39 batch-54
Running loss of epoch-39 batch-54 = 2.8964132070541382e-05

Training epoch-39 batch-55
Running loss of epoch-39 batch-55 = 1.6592442989349365e-05

Training epoch-39 batch-56
Running loss of epoch-39 batch-56 = 2.256454899907112e-05

Training epoch-39 batch-57
Running loss of epoch-39 batch-57 = 1.7380109056830406e-05

Training epoch-39 batch-58
Running loss of epoch-39 batch-58 = 9.082956239581108e-06

Training epoch-39 batch-59
Running loss of epoch-39 batch-59 = 7.3856208473443985e-06

Training epoch-39 batch-60
Running loss of epoch-39 batch-60 = 2.2101448848843575e-05

Training epoch-39 batch-61
Running loss of epoch-39 batch-61 = 1.094653271138668e-05

Training epoch-39 batch-62
Running loss of epoch-39 batch-62 = 3.2923417165875435e-05

Training epoch-39 batch-63
Running loss of epoch-39 batch-63 = 5.4394127801060677e-05

Training epoch-39 batch-64
Running loss of epoch-39 batch-64 = 3.112340345978737e-05

Training epoch-39 batch-65
Running loss of epoch-39 batch-65 = 9.253155440092087e-06

Training epoch-39 batch-66
Running loss of epoch-39 batch-66 = 2.0380131900310516e-05

Training epoch-39 batch-67
Running loss of epoch-39 batch-67 = 1.5405239537358284e-05

Training epoch-39 batch-68
Running loss of epoch-39 batch-68 = 1.8086517229676247e-05

Training epoch-39 batch-69
Running loss of epoch-39 batch-69 = 2.216012217104435e-05

Training epoch-39 batch-70
Running loss of epoch-39 batch-70 = 9.79006290435791e-06

Training epoch-39 batch-71
Running loss of epoch-39 batch-71 = 1.605716533958912e-05

Training epoch-39 batch-72
Running loss of epoch-39 batch-72 = 2.250494435429573e-05

Training epoch-39 batch-73
Running loss of epoch-39 batch-73 = 1.9720057025551796e-05

Training epoch-39 batch-74
Running loss of epoch-39 batch-74 = 2.8390903025865555e-05

Training epoch-39 batch-75
Running loss of epoch-39 batch-75 = 2.23375391215086e-05

Training epoch-39 batch-76
Running loss of epoch-39 batch-76 = 2.0343810319900513e-05

Training epoch-39 batch-77
Running loss of epoch-39 batch-77 = 2.3169443011283875e-05

Training epoch-39 batch-78
Running loss of epoch-39 batch-78 = 1.3908371329307556e-05

Training epoch-39 batch-79
Running loss of epoch-39 batch-79 = 1.4066696166992188e-05

Training epoch-39 batch-80
Running loss of epoch-39 batch-80 = 2.5698915123939514e-05

Training epoch-39 batch-81
Running loss of epoch-39 batch-81 = 9.811017662286758e-06

Training epoch-39 batch-82
Running loss of epoch-39 batch-82 = 2.0390376448631287e-05

Training epoch-39 batch-83
Running loss of epoch-39 batch-83 = 1.8883729353547096e-05

Training epoch-39 batch-84
Running loss of epoch-39 batch-84 = 2.619507722556591e-05

Training epoch-39 batch-85
Running loss of epoch-39 batch-85 = 1.6619684174656868e-05

Training epoch-39 batch-86
Running loss of epoch-39 batch-86 = 4.423921927809715e-05

Training epoch-39 batch-87
Running loss of epoch-39 batch-87 = 3.6822399124503136e-05

Training epoch-39 batch-88
Running loss of epoch-39 batch-88 = 4.015688318759203e-05

Training epoch-39 batch-89
Running loss of epoch-39 batch-89 = 2.6318244636058807e-05

Training epoch-39 batch-90
Running loss of epoch-39 batch-90 = 2.3575499653816223e-05

Training epoch-39 batch-91
Running loss of epoch-39 batch-91 = 9.054085239768028e-06

Training epoch-39 batch-92
Running loss of epoch-39 batch-92 = 2.018664963543415e-05

Training epoch-39 batch-93
Running loss of epoch-39 batch-93 = 1.9415048882365227e-05

Training epoch-39 batch-94
Running loss of epoch-39 batch-94 = 1.3978453353047371e-05

Training epoch-39 batch-95
Running loss of epoch-39 batch-95 = 1.8776627257466316e-05

Training epoch-39 batch-96
Running loss of epoch-39 batch-96 = 9.868992492556572e-06

Training epoch-39 batch-97
Running loss of epoch-39 batch-97 = 1.3048062101006508e-05

Training epoch-39 batch-98
Running loss of epoch-39 batch-98 = 1.9599683582782745e-05

Training epoch-39 batch-99
Running loss of epoch-39 batch-99 = 3.88056505471468e-05

Training epoch-39 batch-100
Running loss of epoch-39 batch-100 = 3.376300446689129e-05

Training epoch-39 batch-101
Running loss of epoch-39 batch-101 = 1.231231726706028e-05

Training epoch-39 batch-102
Running loss of epoch-39 batch-102 = 2.6008114218711853e-05

Training epoch-39 batch-103
Running loss of epoch-39 batch-103 = 2.0208070054650307e-05

Training epoch-39 batch-104
Running loss of epoch-39 batch-104 = 5.44434878975153e-05

Training epoch-39 batch-105
Running loss of epoch-39 batch-105 = 1.1011259630322456e-05

Training epoch-39 batch-106
Running loss of epoch-39 batch-106 = 7.5018033385276794e-06

Training epoch-39 batch-107
Running loss of epoch-39 batch-107 = 8.622650057077408e-06

Training epoch-39 batch-108
Running loss of epoch-39 batch-108 = 2.309097908437252e-05

Training epoch-39 batch-109
Running loss of epoch-39 batch-109 = 1.6464386135339737e-05

Training epoch-39 batch-110
Running loss of epoch-39 batch-110 = 1.580989919602871e-05

Training epoch-39 batch-111
Running loss of epoch-39 batch-111 = 1.1954689398407936e-05

Training epoch-39 batch-112
Running loss of epoch-39 batch-112 = 7.292255759239197e-06

Training epoch-39 batch-113
Running loss of epoch-39 batch-113 = 5.249399691820145e-06

Training epoch-39 batch-114
Running loss of epoch-39 batch-114 = 1.187971793115139e-05

Training epoch-39 batch-115
Running loss of epoch-39 batch-115 = 9.556999430060387e-06

Training epoch-39 batch-116
Running loss of epoch-39 batch-116 = 3.4162658266723156e-05

Training epoch-39 batch-117
Running loss of epoch-39 batch-117 = 4.1299499571323395e-06

Training epoch-39 batch-118
Running loss of epoch-39 batch-118 = 2.049235627055168e-05

Training epoch-39 batch-119
Running loss of epoch-39 batch-119 = 1.3727694749832153e-05

Training epoch-39 batch-120
Running loss of epoch-39 batch-120 = 3.3029355108737946e-06

Training epoch-39 batch-121
Running loss of epoch-39 batch-121 = 1.1286698281764984e-05

Training epoch-39 batch-122
Running loss of epoch-39 batch-122 = 1.3741198927164078e-05

Training epoch-39 batch-123
Running loss of epoch-39 batch-123 = 2.1397601813077927e-05

Training epoch-39 batch-124
Running loss of epoch-39 batch-124 = 1.727975904941559e-05

Training epoch-39 batch-125
Running loss of epoch-39 batch-125 = 2.1912390366196632e-05

Training epoch-39 batch-126
Running loss of epoch-39 batch-126 = 1.0316027328372002e-05

Training epoch-39 batch-127
Running loss of epoch-39 batch-127 = 1.0719755664467812e-05

Training epoch-39 batch-128
Running loss of epoch-39 batch-128 = 1.4316989108920097e-05

Training epoch-39 batch-129
Running loss of epoch-39 batch-129 = 2.6282155886292458e-05

Training epoch-39 batch-130
Running loss of epoch-39 batch-130 = 1.1409865692257881e-05

Training epoch-39 batch-131
Running loss of epoch-39 batch-131 = 2.64081172645092e-05

Training epoch-39 batch-132
Running loss of epoch-39 batch-132 = 2.167932689189911e-05

Training epoch-39 batch-133
Running loss of epoch-39 batch-133 = 2.9373914003372192e-05

Training epoch-39 batch-134
Running loss of epoch-39 batch-134 = 1.4620600268244743e-05

Training epoch-39 batch-135
Running loss of epoch-39 batch-135 = 7.679685950279236e-06

Training epoch-39 batch-136
Running loss of epoch-39 batch-136 = 3.8316939026117325e-06

Training epoch-39 batch-137
Running loss of epoch-39 batch-137 = 1.4487188309431076e-05

Training epoch-39 batch-138
Running loss of epoch-39 batch-138 = 8.953269571065903e-06

Training epoch-39 batch-139
Running loss of epoch-39 batch-139 = 3.231246955692768e-05

Training epoch-39 batch-140
Running loss of epoch-39 batch-140 = 2.2515421733260155e-05

Training epoch-39 batch-141
Running loss of epoch-39 batch-141 = 1.8571503460407257e-05

Training epoch-39 batch-142
Running loss of epoch-39 batch-142 = 1.6442034393548965e-05

Training epoch-39 batch-143
Running loss of epoch-39 batch-143 = 6.078742444515228e-06

Training epoch-39 batch-144
Running loss of epoch-39 batch-144 = 1.838034950196743e-05

Training epoch-39 batch-145
Running loss of epoch-39 batch-145 = 5.90737909078598e-06

Training epoch-39 batch-146
Running loss of epoch-39 batch-146 = 1.4872755855321884e-05

Training epoch-39 batch-147
Running loss of epoch-39 batch-147 = 1.8962426111102104e-05

Training epoch-39 batch-148
Running loss of epoch-39 batch-148 = 8.368398994207382e-06

Training epoch-39 batch-149
Running loss of epoch-39 batch-149 = 1.2002885341644287e-05

Training epoch-39 batch-150
Running loss of epoch-39 batch-150 = 4.122056998312473e-05

Training epoch-39 batch-151
Running loss of epoch-39 batch-151 = 6.720423698425293e-06

Training epoch-39 batch-152
Running loss of epoch-39 batch-152 = 8.198898285627365e-06

Training epoch-39 batch-153
Running loss of epoch-39 batch-153 = 7.854774594306946e-06

Training epoch-39 batch-154
Running loss of epoch-39 batch-154 = 6.100162863731384e-06

Training epoch-39 batch-155
Running loss of epoch-39 batch-155 = 1.569930464029312e-05

Training epoch-39 batch-156
Running loss of epoch-39 batch-156 = 2.3937318474054337e-05

Training epoch-39 batch-157
Running loss of epoch-39 batch-157 = 3.897026181221008e-05

Finished training epoch-39.



Average train loss at epoch-39 = 1.8290216475725172e-05

Started Evaluation

Average val loss at epoch-39 = 1.0222166245993034

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.80 %
Accuracy for class onCreate is: 92.22 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.75 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 49.40 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.66 %

Finished Evaluation



Started training epoch-40


Training epoch-40 batch-1
Running loss of epoch-40 batch-1 = 1.5461351722478867e-05

Training epoch-40 batch-2
Running loss of epoch-40 batch-2 = 1.3397308066487312e-05

Training epoch-40 batch-3
Running loss of epoch-40 batch-3 = 1.957174390554428e-05

Training epoch-40 batch-4
Running loss of epoch-40 batch-4 = 2.0541483536362648e-05

Training epoch-40 batch-5
Running loss of epoch-40 batch-5 = 2.2337771952152252e-05

Training epoch-40 batch-6
Running loss of epoch-40 batch-6 = 2.3596221581101418e-05

Training epoch-40 batch-7
Running loss of epoch-40 batch-7 = 7.383059710264206e-06

Training epoch-40 batch-8
Running loss of epoch-40 batch-8 = 1.0753748938441277e-05

Training epoch-40 batch-9
Running loss of epoch-40 batch-9 = 3.635103348642588e-05

Training epoch-40 batch-10
Running loss of epoch-40 batch-10 = 1.4930730685591698e-05

Training epoch-40 batch-11
Running loss of epoch-40 batch-11 = 2.19887588173151e-05

Training epoch-40 batch-12
Running loss of epoch-40 batch-12 = 1.3382639735937119e-05

Training epoch-40 batch-13
Running loss of epoch-40 batch-13 = 8.79308208823204e-06

Training epoch-40 batch-14
Running loss of epoch-40 batch-14 = 1.8796417862176895e-05

Training epoch-40 batch-15
Running loss of epoch-40 batch-15 = 3.9261067286133766e-05

Training epoch-40 batch-16
Running loss of epoch-40 batch-16 = 1.2262724339962006e-05

Training epoch-40 batch-17
Running loss of epoch-40 batch-17 = 1.975148916244507e-05

Training epoch-40 batch-18
Running loss of epoch-40 batch-18 = 1.2075994163751602e-05

Training epoch-40 batch-19
Running loss of epoch-40 batch-19 = 1.1592637747526169e-05

Training epoch-40 batch-20
Running loss of epoch-40 batch-20 = 3.178184852004051e-05

Training epoch-40 batch-21
Running loss of epoch-40 batch-21 = 1.8483027815818787e-05

Training epoch-40 batch-22
Running loss of epoch-40 batch-22 = 1.1846888810396194e-05

Training epoch-40 batch-23
Running loss of epoch-40 batch-23 = 1.5160534530878067e-05

Training epoch-40 batch-24
Running loss of epoch-40 batch-24 = 1.6727019101381302e-05

Training epoch-40 batch-25
Running loss of epoch-40 batch-25 = 1.858663745224476e-05

Training epoch-40 batch-26
Running loss of epoch-40 batch-26 = 5.673384293913841e-06

Training epoch-40 batch-27
Running loss of epoch-40 batch-27 = 9.109266102313995e-06

Training epoch-40 batch-28
Running loss of epoch-40 batch-28 = 7.234979420900345e-06

Training epoch-40 batch-29
Running loss of epoch-40 batch-29 = 1.9169645383954048e-05

Training epoch-40 batch-30
Running loss of epoch-40 batch-30 = 7.867114618420601e-06

Training epoch-40 batch-31
Running loss of epoch-40 batch-31 = 1.2486008927226067e-05

Training epoch-40 batch-32
Running loss of epoch-40 batch-32 = 1.3915589079260826e-05

Training epoch-40 batch-33
Running loss of epoch-40 batch-33 = 1.6264384612441063e-05

Training epoch-40 batch-34
Running loss of epoch-40 batch-34 = 1.526833511888981e-05

Training epoch-40 batch-35
Running loss of epoch-40 batch-35 = 3.336183726787567e-05

Training epoch-40 batch-36
Running loss of epoch-40 batch-36 = 1.0126270353794098e-05

Training epoch-40 batch-37
Running loss of epoch-40 batch-37 = 1.9960105419158936e-05

Training epoch-40 batch-38
Running loss of epoch-40 batch-38 = 2.3129628971219063e-05

Training epoch-40 batch-39
Running loss of epoch-40 batch-39 = 1.1676689609885216e-05

Training epoch-40 batch-40
Running loss of epoch-40 batch-40 = 7.635215297341347e-06

Training epoch-40 batch-41
Running loss of epoch-40 batch-41 = 1.5211990103125572e-05

Training epoch-40 batch-42
Running loss of epoch-40 batch-42 = 1.0350486263632774e-05

Training epoch-40 batch-43
Running loss of epoch-40 batch-43 = 1.2215692549943924e-05

Training epoch-40 batch-44
Running loss of epoch-40 batch-44 = 1.2598931789398193e-05

Training epoch-40 batch-45
Running loss of epoch-40 batch-45 = 1.0854331776499748e-05

Training epoch-40 batch-46
Running loss of epoch-40 batch-46 = 1.639546826481819e-05

Training epoch-40 batch-47
Running loss of epoch-40 batch-47 = 1.891213469207287e-05

Training epoch-40 batch-48
Running loss of epoch-40 batch-48 = 1.7267419025301933e-05

Training epoch-40 batch-49
Running loss of epoch-40 batch-49 = 2.9885675758123398e-05

Training epoch-40 batch-50
Running loss of epoch-40 batch-50 = 8.11973586678505e-06

Training epoch-40 batch-51
Running loss of epoch-40 batch-51 = 1.4902791008353233e-05

Training epoch-40 batch-52
Running loss of epoch-40 batch-52 = 1.7842045053839684e-05

Training epoch-40 batch-53
Running loss of epoch-40 batch-53 = 1.997058279812336e-05

Training epoch-40 batch-54
Running loss of epoch-40 batch-54 = 3.298325464129448e-05

Training epoch-40 batch-55
Running loss of epoch-40 batch-55 = 1.2029893696308136e-05

Training epoch-40 batch-56
Running loss of epoch-40 batch-56 = 9.341398254036903e-06

Training epoch-40 batch-57
Running loss of epoch-40 batch-57 = 2.3937784135341644e-05

Training epoch-40 batch-58
Running loss of epoch-40 batch-58 = 2.183695323765278e-05

Training epoch-40 batch-59
Running loss of epoch-40 batch-59 = 8.492963388562202e-06

Training epoch-40 batch-60
Running loss of epoch-40 batch-60 = 5.032471381127834e-05

Training epoch-40 batch-61
Running loss of epoch-40 batch-61 = 2.856156788766384e-05

Training epoch-40 batch-62
Running loss of epoch-40 batch-62 = 2.0738691091537476e-05

Training epoch-40 batch-63
Running loss of epoch-40 batch-63 = 1.0105548426508904e-05

Training epoch-40 batch-64
Running loss of epoch-40 batch-64 = 1.616077497601509e-05

Training epoch-40 batch-65
Running loss of epoch-40 batch-65 = 1.1644558981060982e-05

Training epoch-40 batch-66
Running loss of epoch-40 batch-66 = 1.638871617615223e-05

Training epoch-40 batch-67
Running loss of epoch-40 batch-67 = 6.460351869463921e-06

Training epoch-40 batch-68
Running loss of epoch-40 batch-68 = 5.118083208799362e-06

Training epoch-40 batch-69
Running loss of epoch-40 batch-69 = 6.09690323472023e-06

Training epoch-40 batch-70
Running loss of epoch-40 batch-70 = 6.463145837187767e-06

Training epoch-40 batch-71
Running loss of epoch-40 batch-71 = 5.413778126239777e-06

Training epoch-40 batch-72
Running loss of epoch-40 batch-72 = 1.6350531950592995e-05

Training epoch-40 batch-73
Running loss of epoch-40 batch-73 = 2.3988541215658188e-06

Training epoch-40 batch-74
Running loss of epoch-40 batch-74 = 1.92269217222929e-05

Training epoch-40 batch-75
Running loss of epoch-40 batch-75 = 1.048436388373375e-05

Training epoch-40 batch-76
Running loss of epoch-40 batch-76 = 1.1007534340023994e-05

Training epoch-40 batch-77
Running loss of epoch-40 batch-77 = 2.1981075406074524e-05

Training epoch-40 batch-78
Running loss of epoch-40 batch-78 = 5.198759026825428e-05

Training epoch-40 batch-79
Running loss of epoch-40 batch-79 = 1.8588034436106682e-05

Training epoch-40 batch-80
Running loss of epoch-40 batch-80 = 2.8263311833143234e-05

Training epoch-40 batch-81
Running loss of epoch-40 batch-81 = 2.8412090614438057e-05

Training epoch-40 batch-82
Running loss of epoch-40 batch-82 = 2.9022106900811195e-05

Training epoch-40 batch-83
Running loss of epoch-40 batch-83 = 2.3789703845977783e-05

Training epoch-40 batch-84
Running loss of epoch-40 batch-84 = 7.247552275657654e-06

Training epoch-40 batch-85
Running loss of epoch-40 batch-85 = 1.6117002815008163e-05

Training epoch-40 batch-86
Running loss of epoch-40 batch-86 = 1.8134480342268944e-05

Training epoch-40 batch-87
Running loss of epoch-40 batch-87 = 1.9473722204566002e-05

Training epoch-40 batch-88
Running loss of epoch-40 batch-88 = 1.080334186553955e-05

Training epoch-40 batch-89
Running loss of epoch-40 batch-89 = 3.921426832675934e-05

Training epoch-40 batch-90
Running loss of epoch-40 batch-90 = 1.290976069867611e-05

Training epoch-40 batch-91
Running loss of epoch-40 batch-91 = 1.7576152458786964e-05

Training epoch-40 batch-92
Running loss of epoch-40 batch-92 = 1.8847640603780746e-05

Training epoch-40 batch-93
Running loss of epoch-40 batch-93 = 1.3314653187990189e-05

Training epoch-40 batch-94
Running loss of epoch-40 batch-94 = 1.4482764527201653e-05

Training epoch-40 batch-95
Running loss of epoch-40 batch-95 = 9.634764865040779e-06

Training epoch-40 batch-96
Running loss of epoch-40 batch-96 = 1.1518364772200584e-05

Training epoch-40 batch-97
Running loss of epoch-40 batch-97 = 7.718801498413086e-06

Training epoch-40 batch-98
Running loss of epoch-40 batch-98 = 2.734665758907795e-05

Training epoch-40 batch-99
Running loss of epoch-40 batch-99 = 7.277121767401695e-06

Training epoch-40 batch-100
Running loss of epoch-40 batch-100 = 2.135150134563446e-05

Training epoch-40 batch-101
Running loss of epoch-40 batch-101 = 1.9877450540661812e-05

Training epoch-40 batch-102
Running loss of epoch-40 batch-102 = 1.3666227459907532e-05

Training epoch-40 batch-103
Running loss of epoch-40 batch-103 = 1.0818708688020706e-05

Training epoch-40 batch-104
Running loss of epoch-40 batch-104 = 1.6489531844854355e-05

Training epoch-40 batch-105
Running loss of epoch-40 batch-105 = 4.459638148546219e-05

Training epoch-40 batch-106
Running loss of epoch-40 batch-106 = 3.0784402042627335e-05

Training epoch-40 batch-107
Running loss of epoch-40 batch-107 = 1.1894386261701584e-05

Training epoch-40 batch-108
Running loss of epoch-40 batch-108 = 8.688075467944145e-06

Training epoch-40 batch-109
Running loss of epoch-40 batch-109 = 1.1035241186618805e-05

Training epoch-40 batch-110
Running loss of epoch-40 batch-110 = 1.3146549463272095e-05

Training epoch-40 batch-111
Running loss of epoch-40 batch-111 = 1.4427350834012032e-05

Training epoch-40 batch-112
Running loss of epoch-40 batch-112 = 5.9891026467084885e-06

Training epoch-40 batch-113
Running loss of epoch-40 batch-113 = 6.764661520719528e-06

Training epoch-40 batch-114
Running loss of epoch-40 batch-114 = 7.48317688703537e-06

Training epoch-40 batch-115
Running loss of epoch-40 batch-115 = 2.3055588826537132e-05

Training epoch-40 batch-116
Running loss of epoch-40 batch-116 = 1.1213822290301323e-05

Training epoch-40 batch-117
Running loss of epoch-40 batch-117 = 2.6646535843610764e-05

Training epoch-40 batch-118
Running loss of epoch-40 batch-118 = 1.2587057426571846e-05

Training epoch-40 batch-119
Running loss of epoch-40 batch-119 = 1.9037164747714996e-05

Training epoch-40 batch-120
Running loss of epoch-40 batch-120 = 5.963025614619255e-06

Training epoch-40 batch-121
Running loss of epoch-40 batch-121 = 4.336400888860226e-05

Training epoch-40 batch-122
Running loss of epoch-40 batch-122 = 3.056228160858154e-05

Training epoch-40 batch-123
Running loss of epoch-40 batch-123 = 1.5432480722665787e-05

Training epoch-40 batch-124
Running loss of epoch-40 batch-124 = 1.369439996778965e-05

Training epoch-40 batch-125
Running loss of epoch-40 batch-125 = 1.946208067238331e-05

Training epoch-40 batch-126
Running loss of epoch-40 batch-126 = 1.142546534538269e-05

Training epoch-40 batch-127
Running loss of epoch-40 batch-127 = 1.7553335055708885e-05

Training epoch-40 batch-128
Running loss of epoch-40 batch-128 = 1.523294486105442e-05

Training epoch-40 batch-129
Running loss of epoch-40 batch-129 = 1.615018118172884e-05

Training epoch-40 batch-130
Running loss of epoch-40 batch-130 = 1.6436679288744926e-05

Training epoch-40 batch-131
Running loss of epoch-40 batch-131 = 1.4755874872207642e-05

Training epoch-40 batch-132
Running loss of epoch-40 batch-132 = 8.797738701105118e-06

Training epoch-40 batch-133
Running loss of epoch-40 batch-133 = 1.1713709682226181e-05

Training epoch-40 batch-134
Running loss of epoch-40 batch-134 = 1.2089964002370834e-05

Training epoch-40 batch-135
Running loss of epoch-40 batch-135 = 1.8646474927663803e-05

Training epoch-40 batch-136
Running loss of epoch-40 batch-136 = 3.0298950150609016e-05

Training epoch-40 batch-137
Running loss of epoch-40 batch-137 = 2.8799287974834442e-05

Training epoch-40 batch-138
Running loss of epoch-40 batch-138 = 1.800619065761566e-05

Training epoch-40 batch-139
Running loss of epoch-40 batch-139 = 1.6348203644156456e-05

Training epoch-40 batch-140
Running loss of epoch-40 batch-140 = 1.6622943803668022e-05

Training epoch-40 batch-141
Running loss of epoch-40 batch-141 = 1.376238651573658e-05

Training epoch-40 batch-142
Running loss of epoch-40 batch-142 = 3.748340532183647e-06

Training epoch-40 batch-143
Running loss of epoch-40 batch-143 = 1.1329539120197296e-05

Training epoch-40 batch-144
Running loss of epoch-40 batch-144 = 8.103903383016586e-06

Training epoch-40 batch-145
Running loss of epoch-40 batch-145 = 9.94466245174408e-06

Training epoch-40 batch-146
Running loss of epoch-40 batch-146 = 1.6923528164625168e-05

Training epoch-40 batch-147
Running loss of epoch-40 batch-147 = 2.0298408344388008e-05

Training epoch-40 batch-148
Running loss of epoch-40 batch-148 = 1.7614802345633507e-05

Training epoch-40 batch-149
Running loss of epoch-40 batch-149 = 1.851748675107956e-05

Training epoch-40 batch-150
Running loss of epoch-40 batch-150 = 1.1508585885167122e-05

Training epoch-40 batch-151
Running loss of epoch-40 batch-151 = 7.72625207901001e-06

Training epoch-40 batch-152
Running loss of epoch-40 batch-152 = 2.6578782126307487e-05

Training epoch-40 batch-153
Running loss of epoch-40 batch-153 = 1.1177035048604012e-05

Training epoch-40 batch-154
Running loss of epoch-40 batch-154 = 2.1537067368626595e-05

Training epoch-40 batch-155
Running loss of epoch-40 batch-155 = 1.4392891898751259e-05

Training epoch-40 batch-156
Running loss of epoch-40 batch-156 = 2.169143408536911e-05

Training epoch-40 batch-157
Running loss of epoch-40 batch-157 = 3.512948751449585e-05

Finished training epoch-40.



Average train loss at epoch-40 = 1.6844773292541505e-05

Started Evaluation

Average val loss at epoch-40 = 1.0412190364101697

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 92.62 %
Accuracy for class onCreate is: 92.11 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 66.44 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 73.85 %

Overall Accuracy = 83.78 %

Finished Evaluation



Started training epoch-41


Training epoch-41 batch-1
Running loss of epoch-41 batch-1 = 7.719965651631355e-06

Training epoch-41 batch-2
Running loss of epoch-41 batch-2 = 1.1364929378032684e-05

Training epoch-41 batch-3
Running loss of epoch-41 batch-3 = 1.651863567531109e-05

Training epoch-41 batch-4
Running loss of epoch-41 batch-4 = 1.2212898582220078e-05

Training epoch-41 batch-5
Running loss of epoch-41 batch-5 = 3.5099219530820847e-05

Training epoch-41 batch-6
Running loss of epoch-41 batch-6 = 1.0797986760735512e-05

Training epoch-41 batch-7
Running loss of epoch-41 batch-7 = 1.1724652722477913e-05

Training epoch-41 batch-8
Running loss of epoch-41 batch-8 = 1.9456492736935616e-05

Training epoch-41 batch-9
Running loss of epoch-41 batch-9 = 2.4215085431933403e-05

Training epoch-41 batch-10
Running loss of epoch-41 batch-10 = 1.4113029465079308e-05

Training epoch-41 batch-11
Running loss of epoch-41 batch-11 = 1.4428514987230301e-05

Training epoch-41 batch-12
Running loss of epoch-41 batch-12 = 6.859190762042999e-06

Training epoch-41 batch-13
Running loss of epoch-41 batch-13 = 8.14441591501236e-06

Training epoch-41 batch-14
Running loss of epoch-41 batch-14 = 4.77558933198452e-06

Training epoch-41 batch-15
Running loss of epoch-41 batch-15 = 2.047303132712841e-05

Training epoch-41 batch-16
Running loss of epoch-41 batch-16 = 2.96151265501976e-05

Training epoch-41 batch-17
Running loss of epoch-41 batch-17 = 5.152076482772827e-06

Training epoch-41 batch-18
Running loss of epoch-41 batch-18 = 2.3497967049479485e-05

Training epoch-41 batch-19
Running loss of epoch-41 batch-19 = 3.77091346308589e-05

Training epoch-41 batch-20
Running loss of epoch-41 batch-20 = 2.594827674329281e-05

Training epoch-41 batch-21
Running loss of epoch-41 batch-21 = 9.420560672879219e-06

Training epoch-41 batch-22
Running loss of epoch-41 batch-22 = 1.3957265764474869e-05

Training epoch-41 batch-23
Running loss of epoch-41 batch-23 = 1.8624821677803993e-05

Training epoch-41 batch-24
Running loss of epoch-41 batch-24 = 2.275407314300537e-05

Training epoch-41 batch-25
Running loss of epoch-41 batch-25 = 8.986331522464752e-06

Training epoch-41 batch-26
Running loss of epoch-41 batch-26 = 1.2637348845601082e-05

Training epoch-41 batch-27
Running loss of epoch-41 batch-27 = 1.0612886399030685e-05

Training epoch-41 batch-28
Running loss of epoch-41 batch-28 = 1.4322809875011444e-05

Training epoch-41 batch-29
Running loss of epoch-41 batch-29 = 1.8466729670763016e-05

Training epoch-41 batch-30
Running loss of epoch-41 batch-30 = 2.3684697225689888e-05

Training epoch-41 batch-31
Running loss of epoch-41 batch-31 = 3.62214632332325e-06

Training epoch-41 batch-32
Running loss of epoch-41 batch-32 = 8.665723726153374e-06

Training epoch-41 batch-33
Running loss of epoch-41 batch-33 = 3.198930062353611e-05

Training epoch-41 batch-34
Running loss of epoch-41 batch-34 = 2.1191313862800598e-05

Training epoch-41 batch-35
Running loss of epoch-41 batch-35 = 1.6127945855259895e-05

Training epoch-41 batch-36
Running loss of epoch-41 batch-36 = 1.234840601682663e-05

Training epoch-41 batch-37
Running loss of epoch-41 batch-37 = 1.044292002916336e-05

Training epoch-41 batch-38
Running loss of epoch-41 batch-38 = 1.4588702470064163e-05

Training epoch-41 batch-39
Running loss of epoch-41 batch-39 = 1.501454971730709e-05

Training epoch-41 batch-40
Running loss of epoch-41 batch-40 = 9.576324373483658e-06

Training epoch-41 batch-41
Running loss of epoch-41 batch-41 = 1.0310439392924309e-05

Training epoch-41 batch-42
Running loss of epoch-41 batch-42 = 4.813540726900101e-06

Training epoch-41 batch-43
Running loss of epoch-41 batch-43 = 2.3716362193226814e-05

Training epoch-41 batch-44
Running loss of epoch-41 batch-44 = 1.3331649824976921e-05

Training epoch-41 batch-45
Running loss of epoch-41 batch-45 = 1.180148683488369e-05

Training epoch-41 batch-46
Running loss of epoch-41 batch-46 = 1.3832934200763702e-05

Training epoch-41 batch-47
Running loss of epoch-41 batch-47 = 3.46391461789608e-05

Training epoch-41 batch-48
Running loss of epoch-41 batch-48 = 1.6200123354792595e-05

Training epoch-41 batch-49
Running loss of epoch-41 batch-49 = 7.339753210544586e-06

Training epoch-41 batch-50
Running loss of epoch-41 batch-50 = 4.2061321437358856e-05

Training epoch-41 batch-51
Running loss of epoch-41 batch-51 = 2.7209054678678513e-05

Training epoch-41 batch-52
Running loss of epoch-41 batch-52 = 2.1630898118019104e-05

Training epoch-41 batch-53
Running loss of epoch-41 batch-53 = 1.706392504274845e-05

Training epoch-41 batch-54
Running loss of epoch-41 batch-54 = 1.2386124581098557e-05

Training epoch-41 batch-55
Running loss of epoch-41 batch-55 = 2.2690044716000557e-05

Training epoch-41 batch-56
Running loss of epoch-41 batch-56 = 1.431838609278202e-05

Training epoch-41 batch-57
Running loss of epoch-41 batch-57 = 7.080612704157829e-06

Training epoch-41 batch-58
Running loss of epoch-41 batch-58 = 3.5564880818128586e-05

Training epoch-41 batch-59
Running loss of epoch-41 batch-59 = 7.862458005547523e-06

Training epoch-41 batch-60
Running loss of epoch-41 batch-60 = 3.730528987944126e-05

Training epoch-41 batch-61
Running loss of epoch-41 batch-61 = 8.430331945419312e-06

Training epoch-41 batch-62
Running loss of epoch-41 batch-62 = 1.7638900317251682e-05

Training epoch-41 batch-63
Running loss of epoch-41 batch-63 = 1.6769394278526306e-05

Training epoch-41 batch-64
Running loss of epoch-41 batch-64 = 1.3208482414484024e-05

Training epoch-41 batch-65
Running loss of epoch-41 batch-65 = 1.9044382497668266e-05

Training epoch-41 batch-66
Running loss of epoch-41 batch-66 = 1.6203150153160095e-05

Training epoch-41 batch-67
Running loss of epoch-41 batch-67 = 7.756054401397705e-06

Training epoch-41 batch-68
Running loss of epoch-41 batch-68 = 1.0801712051033974e-05

Training epoch-41 batch-69
Running loss of epoch-41 batch-69 = 1.5653204172849655e-05

Training epoch-41 batch-70
Running loss of epoch-41 batch-70 = 2.0362669602036476e-05

Training epoch-41 batch-71
Running loss of epoch-41 batch-71 = 1.7054378986358643e-05

Training epoch-41 batch-72
Running loss of epoch-41 batch-72 = 1.675751991569996e-05

Training epoch-41 batch-73
Running loss of epoch-41 batch-73 = 1.7282087355852127e-05

Training epoch-41 batch-74
Running loss of epoch-41 batch-74 = 7.690861821174622e-06

Training epoch-41 batch-75
Running loss of epoch-41 batch-75 = 9.768642485141754e-06

Training epoch-41 batch-76
Running loss of epoch-41 batch-76 = 9.451759979128838e-06

Training epoch-41 batch-77
Running loss of epoch-41 batch-77 = 9.263865649700165e-06

Training epoch-41 batch-78
Running loss of epoch-41 batch-78 = 9.153038263320923e-06

Training epoch-41 batch-79
Running loss of epoch-41 batch-79 = 2.3413682356476784e-05

Training epoch-41 batch-80
Running loss of epoch-41 batch-80 = 1.0344432666897774e-05

Training epoch-41 batch-81
Running loss of epoch-41 batch-81 = 1.2604286894202232e-05

Training epoch-41 batch-82
Running loss of epoch-41 batch-82 = 1.2271339073777199e-05

Training epoch-41 batch-83
Running loss of epoch-41 batch-83 = 7.890397682785988e-06

Training epoch-41 batch-84
Running loss of epoch-41 batch-84 = 1.5291385352611542e-05

Training epoch-41 batch-85
Running loss of epoch-41 batch-85 = 1.1353986337780952e-05

Training epoch-41 batch-86
Running loss of epoch-41 batch-86 = 1.713423989713192e-05

Training epoch-41 batch-87
Running loss of epoch-41 batch-87 = 3.0707684345543385e-05

Training epoch-41 batch-88
Running loss of epoch-41 batch-88 = 1.5909085050225258e-05

Training epoch-41 batch-89
Running loss of epoch-41 batch-89 = 8.71601514518261e-06

Training epoch-41 batch-90
Running loss of epoch-41 batch-90 = 1.2527336366474628e-05

Training epoch-41 batch-91
Running loss of epoch-41 batch-91 = 1.5421072021126747e-05

Training epoch-41 batch-92
Running loss of epoch-41 batch-92 = 2.0981766283512115e-05

Training epoch-41 batch-93
Running loss of epoch-41 batch-93 = 1.3294164091348648e-05

Training epoch-41 batch-94
Running loss of epoch-41 batch-94 = 1.6086502000689507e-05

Training epoch-41 batch-95
Running loss of epoch-41 batch-95 = 1.1747237294912338e-05

Training epoch-41 batch-96
Running loss of epoch-41 batch-96 = 1.1762604117393494e-05

Training epoch-41 batch-97
Running loss of epoch-41 batch-97 = 1.2268777936697006e-05

Training epoch-41 batch-98
Running loss of epoch-41 batch-98 = 1.3803830370306969e-05

Training epoch-41 batch-99
Running loss of epoch-41 batch-99 = 1.4176825061440468e-05

Training epoch-41 batch-100
Running loss of epoch-41 batch-100 = 1.1854805052280426e-05

Training epoch-41 batch-101
Running loss of epoch-41 batch-101 = 1.0114628821611404e-05

Training epoch-41 batch-102
Running loss of epoch-41 batch-102 = 1.544598489999771e-05

Training epoch-41 batch-103
Running loss of epoch-41 batch-103 = 1.8159858882427216e-05

Training epoch-41 batch-104
Running loss of epoch-41 batch-104 = 1.0071555152535439e-05

Training epoch-41 batch-105
Running loss of epoch-41 batch-105 = 4.156399518251419e-05

Training epoch-41 batch-106
Running loss of epoch-41 batch-106 = 1.1740019544959068e-05

Training epoch-41 batch-107
Running loss of epoch-41 batch-107 = 4.1460152715444565e-05

Training epoch-41 batch-108
Running loss of epoch-41 batch-108 = 8.499016985297203e-06

Training epoch-41 batch-109
Running loss of epoch-41 batch-109 = 6.611458957195282e-06

Training epoch-41 batch-110
Running loss of epoch-41 batch-110 = 1.0654330253601074e-05

Training epoch-41 batch-111
Running loss of epoch-41 batch-111 = 1.2091128155589104e-05

Training epoch-41 batch-112
Running loss of epoch-41 batch-112 = 7.424503564834595e-06

Training epoch-41 batch-113
Running loss of epoch-41 batch-113 = 1.0720454156398773e-05

Training epoch-41 batch-114
Running loss of epoch-41 batch-114 = 1.398683525621891e-05

Training epoch-41 batch-115
Running loss of epoch-41 batch-115 = 8.87969508767128e-06

Training epoch-41 batch-116
Running loss of epoch-41 batch-116 = 4.337122663855553e-05

Training epoch-41 batch-117
Running loss of epoch-41 batch-117 = 1.4035729691386223e-05

Training epoch-41 batch-118
Running loss of epoch-41 batch-118 = 1.1999160051345825e-05

Training epoch-41 batch-119
Running loss of epoch-41 batch-119 = 3.55355441570282e-05

Training epoch-41 batch-120
Running loss of epoch-41 batch-120 = 1.5437137335538864e-05

Training epoch-41 batch-121
Running loss of epoch-41 batch-121 = 1.644040457904339e-05

Training epoch-41 batch-122
Running loss of epoch-41 batch-122 = 1.3797776773571968e-05

Training epoch-41 batch-123
Running loss of epoch-41 batch-123 = 1.828162930905819e-05

Training epoch-41 batch-124
Running loss of epoch-41 batch-124 = 1.3874145224690437e-05

Training epoch-41 batch-125
Running loss of epoch-41 batch-125 = 1.348857767879963e-05

Training epoch-41 batch-126
Running loss of epoch-41 batch-126 = 1.589907333254814e-05

Training epoch-41 batch-127
Running loss of epoch-41 batch-127 = 1.0939547792077065e-05

Training epoch-41 batch-128
Running loss of epoch-41 batch-128 = 7.638009265065193e-06

Training epoch-41 batch-129
Running loss of epoch-41 batch-129 = 6.19678758084774e-06

Training epoch-41 batch-130
Running loss of epoch-41 batch-130 = 1.1016614735126495e-05

Training epoch-41 batch-131
Running loss of epoch-41 batch-131 = 1.0920455679297447e-05

Training epoch-41 batch-132
Running loss of epoch-41 batch-132 = 9.11252573132515e-06

Training epoch-41 batch-133
Running loss of epoch-41 batch-133 = 1.028762198984623e-05

Training epoch-41 batch-134
Running loss of epoch-41 batch-134 = 2.7046771720051765e-05

Training epoch-41 batch-135
Running loss of epoch-41 batch-135 = 8.543254807591438e-06

Training epoch-41 batch-136
Running loss of epoch-41 batch-136 = 1.0193092748522758e-05

Training epoch-41 batch-137
Running loss of epoch-41 batch-137 = 6.400048732757568e-06

Training epoch-41 batch-138
Running loss of epoch-41 batch-138 = 1.3015465810894966e-05

Training epoch-41 batch-139
Running loss of epoch-41 batch-139 = 1.1344440281391144e-05

Training epoch-41 batch-140
Running loss of epoch-41 batch-140 = 3.7884339690208435e-05

Training epoch-41 batch-141
Running loss of epoch-41 batch-141 = 1.0870629921555519e-05

Training epoch-41 batch-142
Running loss of epoch-41 batch-142 = 1.2916279956698418e-05

Training epoch-41 batch-143
Running loss of epoch-41 batch-143 = 1.783529296517372e-05

Training epoch-41 batch-144
Running loss of epoch-41 batch-144 = 2.0222272723913193e-05

Training epoch-41 batch-145
Running loss of epoch-41 batch-145 = 2.7918722480535507e-05

Training epoch-41 batch-146
Running loss of epoch-41 batch-146 = 7.673166692256927e-06

Training epoch-41 batch-147
Running loss of epoch-41 batch-147 = 4.82914038002491e-06

Training epoch-41 batch-148
Running loss of epoch-41 batch-148 = 1.4889752492308617e-05

Training epoch-41 batch-149
Running loss of epoch-41 batch-149 = 1.1444790288805962e-05

Training epoch-41 batch-150
Running loss of epoch-41 batch-150 = 1.6768230125308037e-05

Training epoch-41 batch-151
Running loss of epoch-41 batch-151 = 1.1851079761981964e-05

Training epoch-41 batch-152
Running loss of epoch-41 batch-152 = 3.098789602518082e-05

Training epoch-41 batch-153
Running loss of epoch-41 batch-153 = 2.096523530781269e-05

Training epoch-41 batch-154
Running loss of epoch-41 batch-154 = 6.492482498288155e-06

Training epoch-41 batch-155
Running loss of epoch-41 batch-155 = 5.96093013882637e-06

Training epoch-41 batch-156
Running loss of epoch-41 batch-156 = 2.1311454474925995e-05

Training epoch-41 batch-157
Running loss of epoch-41 batch-157 = 8.748844265937805e-05

Finished training epoch-41.



Average train loss at epoch-41 = 1.5774744749069213e-05

Started Evaluation

Average val loss at epoch-41 = 1.0414762308197456

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 91.58 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.50 %
Accuracy for class execute is: 51.00 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 83.43 %

Finished Evaluation



Started training epoch-42


Training epoch-42 batch-1
Running loss of epoch-42 batch-1 = 1.4309538528323174e-05

Training epoch-42 batch-2
Running loss of epoch-42 batch-2 = 5.584908649325371e-06

Training epoch-42 batch-3
Running loss of epoch-42 batch-3 = 1.0068528354167938e-05

Training epoch-42 batch-4
Running loss of epoch-42 batch-4 = 1.8437393009662628e-05

Training epoch-42 batch-5
Running loss of epoch-42 batch-5 = 1.6243895515799522e-05

Training epoch-42 batch-6
Running loss of epoch-42 batch-6 = 2.4834182113409042e-05

Training epoch-42 batch-7
Running loss of epoch-42 batch-7 = 1.4746328815817833e-05

Training epoch-42 batch-8
Running loss of epoch-42 batch-8 = 6.89411535859108e-06

Training epoch-42 batch-9
Running loss of epoch-42 batch-9 = 2.1280255168676376e-05

Training epoch-42 batch-10
Running loss of epoch-42 batch-10 = 1.4125136658549309e-05

Training epoch-42 batch-11
Running loss of epoch-42 batch-11 = 1.9687926396727562e-05

Training epoch-42 batch-12
Running loss of epoch-42 batch-12 = 1.0586809366941452e-05

Training epoch-42 batch-13
Running loss of epoch-42 batch-13 = 8.677132427692413e-06

Training epoch-42 batch-14
Running loss of epoch-42 batch-14 = 5.527632310986519e-06

Training epoch-42 batch-15
Running loss of epoch-42 batch-15 = 1.430092379450798e-05

Training epoch-42 batch-16
Running loss of epoch-42 batch-16 = 1.3748183846473694e-05

Training epoch-42 batch-17
Running loss of epoch-42 batch-17 = 2.7708476409316063e-05

Training epoch-42 batch-18
Running loss of epoch-42 batch-18 = 2.332683652639389e-05

Training epoch-42 batch-19
Running loss of epoch-42 batch-19 = 1.241220161318779e-05

Training epoch-42 batch-20
Running loss of epoch-42 batch-20 = 1.3371231034398079e-05

Training epoch-42 batch-21
Running loss of epoch-42 batch-21 = 1.6704434528946877e-05

Training epoch-42 batch-22
Running loss of epoch-42 batch-22 = 1.3168202713131905e-05

Training epoch-42 batch-23
Running loss of epoch-42 batch-23 = 1.616543158888817e-05

Training epoch-42 batch-24
Running loss of epoch-42 batch-24 = 1.0724877938628197e-05

Training epoch-42 batch-25
Running loss of epoch-42 batch-25 = 6.828689947724342e-06

Training epoch-42 batch-26
Running loss of epoch-42 batch-26 = 4.796544089913368e-06

Training epoch-42 batch-27
Running loss of epoch-42 batch-27 = 7.789116352796555e-06

Training epoch-42 batch-28
Running loss of epoch-42 batch-28 = 1.7280224710702896e-05

Training epoch-42 batch-29
Running loss of epoch-42 batch-29 = 1.0954448953270912e-05

Training epoch-42 batch-30
Running loss of epoch-42 batch-30 = 3.6195851862430573e-06

Training epoch-42 batch-31
Running loss of epoch-42 batch-31 = 1.8089311197400093e-05

Training epoch-42 batch-32
Running loss of epoch-42 batch-32 = 1.9258586689829826e-05

Training epoch-42 batch-33
Running loss of epoch-42 batch-33 = 2.416130155324936e-05

Training epoch-42 batch-34
Running loss of epoch-42 batch-34 = 1.0855728760361671e-05

Training epoch-42 batch-35
Running loss of epoch-42 batch-35 = 2.1505402401089668e-05

Training epoch-42 batch-36
Running loss of epoch-42 batch-36 = 1.947837881743908e-05

Training epoch-42 batch-37
Running loss of epoch-42 batch-37 = 1.4135614037513733e-05

Training epoch-42 batch-38
Running loss of epoch-42 batch-38 = 2.2927997633814812e-05

Training epoch-42 batch-39
Running loss of epoch-42 batch-39 = 1.7043668776750565e-05

Training epoch-42 batch-40
Running loss of epoch-42 batch-40 = 1.6027595847845078e-05

Training epoch-42 batch-41
Running loss of epoch-42 batch-41 = 8.661765605211258e-06

Training epoch-42 batch-42
Running loss of epoch-42 batch-42 = 5.824491381645203e-06

Training epoch-42 batch-43
Running loss of epoch-42 batch-43 = 2.155546098947525e-05

Training epoch-42 batch-44
Running loss of epoch-42 batch-44 = 8.882954716682434e-06

Training epoch-42 batch-45
Running loss of epoch-42 batch-45 = 2.564932219684124e-05

Training epoch-42 batch-46
Running loss of epoch-42 batch-46 = 1.846696250140667e-05

Training epoch-42 batch-47
Running loss of epoch-42 batch-47 = 1.5952158719301224e-05

Training epoch-42 batch-48
Running loss of epoch-42 batch-48 = 5.73648139834404e-06

Training epoch-42 batch-49
Running loss of epoch-42 batch-49 = 2.09026038646698e-05

Training epoch-42 batch-50
Running loss of epoch-42 batch-50 = 1.005898229777813e-05

Training epoch-42 batch-51
Running loss of epoch-42 batch-51 = 3.568176180124283e-05

Training epoch-42 batch-52
Running loss of epoch-42 batch-52 = 1.6692792996764183e-05

Training epoch-42 batch-53
Running loss of epoch-42 batch-53 = 1.3569137081503868e-05

Training epoch-42 batch-54
Running loss of epoch-42 batch-54 = 4.0991464629769325e-05

Training epoch-42 batch-55
Running loss of epoch-42 batch-55 = 1.837813761085272e-05

Training epoch-42 batch-56
Running loss of epoch-42 batch-56 = 1.2016389518976212e-05

Training epoch-42 batch-57
Running loss of epoch-42 batch-57 = 1.4675315469503403e-05

Training epoch-42 batch-58
Running loss of epoch-42 batch-58 = 1.8315156921744347e-05

Training epoch-42 batch-59
Running loss of epoch-42 batch-59 = 4.0715327486395836e-05

Training epoch-42 batch-60
Running loss of epoch-42 batch-60 = 6.139511242508888e-06

Training epoch-42 batch-61
Running loss of epoch-42 batch-61 = 1.2049218639731407e-05

Training epoch-42 batch-62
Running loss of epoch-42 batch-62 = 9.852228686213493e-06

Training epoch-42 batch-63
Running loss of epoch-42 batch-63 = 1.61988427862525e-05

Training epoch-42 batch-64
Running loss of epoch-42 batch-64 = 1.5656929463148117e-05

Training epoch-42 batch-65
Running loss of epoch-42 batch-65 = 1.4351913705468178e-05

Training epoch-42 batch-66
Running loss of epoch-42 batch-66 = 8.816365152597427e-06

Training epoch-42 batch-67
Running loss of epoch-42 batch-67 = 1.862645149230957e-05

Training epoch-42 batch-68
Running loss of epoch-42 batch-68 = 2.0684907212853432e-05

Training epoch-42 batch-69
Running loss of epoch-42 batch-69 = 1.5649711713194847e-05

Training epoch-42 batch-70
Running loss of epoch-42 batch-70 = 7.360475137829781e-06

Training epoch-42 batch-71
Running loss of epoch-42 batch-71 = 6.717396900057793e-06

Training epoch-42 batch-72
Running loss of epoch-42 batch-72 = 2.5051645934581757e-05

Training epoch-42 batch-73
Running loss of epoch-42 batch-73 = 1.6890931874513626e-05

Training epoch-42 batch-74
Running loss of epoch-42 batch-74 = 9.744428098201752e-06

Training epoch-42 batch-75
Running loss of epoch-42 batch-75 = 1.256144605576992e-05

Training epoch-42 batch-76
Running loss of epoch-42 batch-76 = 1.1812662705779076e-05

Training epoch-42 batch-77
Running loss of epoch-42 batch-77 = 2.088211476802826e-05

Training epoch-42 batch-78
Running loss of epoch-42 batch-78 = 2.21331138163805e-05

Training epoch-42 batch-79
Running loss of epoch-42 batch-79 = 2.433639019727707e-05

Training epoch-42 batch-80
Running loss of epoch-42 batch-80 = 1.2653181329369545e-05

Training epoch-42 batch-81
Running loss of epoch-42 batch-81 = 9.104842320084572e-06

Training epoch-42 batch-82
Running loss of epoch-42 batch-82 = 1.816463191062212e-05

Training epoch-42 batch-83
Running loss of epoch-42 batch-83 = 1.2807780876755714e-05

Training epoch-42 batch-84
Running loss of epoch-42 batch-84 = 2.7278438210487366e-05

Training epoch-42 batch-85
Running loss of epoch-42 batch-85 = 5.8480072766542435e-06

Training epoch-42 batch-86
Running loss of epoch-42 batch-86 = 3.190990537405014e-05

Training epoch-42 batch-87
Running loss of epoch-42 batch-87 = 5.6747812777757645e-06

Training epoch-42 batch-88
Running loss of epoch-42 batch-88 = 1.5645520761609077e-05

Training epoch-42 batch-89
Running loss of epoch-42 batch-89 = 1.037190668284893e-05

Training epoch-42 batch-90
Running loss of epoch-42 batch-90 = 2.6926863938570023e-06

Training epoch-42 batch-91
Running loss of epoch-42 batch-91 = 3.257696516811848e-05

Training epoch-42 batch-92
Running loss of epoch-42 batch-92 = 1.1968892067670822e-05

Training epoch-42 batch-93
Running loss of epoch-42 batch-93 = 1.2532807886600494e-05

Training epoch-42 batch-94
Running loss of epoch-42 batch-94 = 6.198184564709663e-06

Training epoch-42 batch-95
Running loss of epoch-42 batch-95 = 1.531955786049366e-05

Training epoch-42 batch-96
Running loss of epoch-42 batch-96 = 1.4180084690451622e-05

Training epoch-42 batch-97
Running loss of epoch-42 batch-97 = 1.6978243365883827e-05

Training epoch-42 batch-98
Running loss of epoch-42 batch-98 = 1.2329546734690666e-05

Training epoch-42 batch-99
Running loss of epoch-42 batch-99 = 8.699717000126839e-06

Training epoch-42 batch-100
Running loss of epoch-42 batch-100 = 8.950242772698402e-06

Training epoch-42 batch-101
Running loss of epoch-42 batch-101 = 1.9877450540661812e-05

Training epoch-42 batch-102
Running loss of epoch-42 batch-102 = 9.807990863919258e-06

Training epoch-42 batch-103
Running loss of epoch-42 batch-103 = 1.1276453733444214e-05

Training epoch-42 batch-104
Running loss of epoch-42 batch-104 = 1.9645318388938904e-05

Training epoch-42 batch-105
Running loss of epoch-42 batch-105 = 1.50301493704319e-05

Training epoch-42 batch-106
Running loss of epoch-42 batch-106 = 1.9386177882552147e-05

Training epoch-42 batch-107
Running loss of epoch-42 batch-107 = 3.3817486837506294e-05

Training epoch-42 batch-108
Running loss of epoch-42 batch-108 = 7.609138265252113e-06

Training epoch-42 batch-109
Running loss of epoch-42 batch-109 = 1.2396369129419327e-05

Training epoch-42 batch-110
Running loss of epoch-42 batch-110 = 7.596099749207497e-06

Training epoch-42 batch-111
Running loss of epoch-42 batch-111 = 2.2209947928786278e-05

Training epoch-42 batch-112
Running loss of epoch-42 batch-112 = 1.1266907677054405e-05

Training epoch-42 batch-113
Running loss of epoch-42 batch-113 = 1.8391292542219162e-05

Training epoch-42 batch-114
Running loss of epoch-42 batch-114 = 7.90553167462349e-06

Training epoch-42 batch-115
Running loss of epoch-42 batch-115 = 1.7907004803419113e-05

Training epoch-42 batch-116
Running loss of epoch-42 batch-116 = 2.1391082555055618e-05

Training epoch-42 batch-117
Running loss of epoch-42 batch-117 = 8.482253178954124e-06

Training epoch-42 batch-118
Running loss of epoch-42 batch-118 = 1.2842472642660141e-05

Training epoch-42 batch-119
Running loss of epoch-42 batch-119 = 1.5023630112409592e-05

Training epoch-42 batch-120
Running loss of epoch-42 batch-120 = 8.941395208239555e-06

Training epoch-42 batch-121
Running loss of epoch-42 batch-121 = 8.130678907036781e-06

Training epoch-42 batch-122
Running loss of epoch-42 batch-122 = 1.6430160030722618e-05

Training epoch-42 batch-123
Running loss of epoch-42 batch-123 = 3.5690609365701675e-05

Training epoch-42 batch-124
Running loss of epoch-42 batch-124 = 5.055218935012817e-06

Training epoch-42 batch-125
Running loss of epoch-42 batch-125 = 9.32556577026844e-06

Training epoch-42 batch-126
Running loss of epoch-42 batch-126 = 1.2809643521904945e-05

Training epoch-42 batch-127
Running loss of epoch-42 batch-127 = 1.1249678209424019e-05

Training epoch-42 batch-128
Running loss of epoch-42 batch-128 = 1.3076001778244972e-05

Training epoch-42 batch-129
Running loss of epoch-42 batch-129 = 9.547220543026924e-06

Training epoch-42 batch-130
Running loss of epoch-42 batch-130 = 1.3962620869278908e-05

Training epoch-42 batch-131
Running loss of epoch-42 batch-131 = 1.3264594599604607e-05

Training epoch-42 batch-132
Running loss of epoch-42 batch-132 = 7.996335625648499e-06

Training epoch-42 batch-133
Running loss of epoch-42 batch-133 = 1.0421732440590858e-05

Training epoch-42 batch-134
Running loss of epoch-42 batch-134 = 1.6094418242573738e-05

Training epoch-42 batch-135
Running loss of epoch-42 batch-135 = 1.8443912267684937e-05

Training epoch-42 batch-136
Running loss of epoch-42 batch-136 = 2.6536639779806137e-05

Training epoch-42 batch-137
Running loss of epoch-42 batch-137 = 5.995621904730797e-06

Training epoch-42 batch-138
Running loss of epoch-42 batch-138 = 1.5242723748087883e-05

Training epoch-42 batch-139
Running loss of epoch-42 batch-139 = 1.760292798280716e-05

Training epoch-42 batch-140
Running loss of epoch-42 batch-140 = 1.841154880821705e-05

Training epoch-42 batch-141
Running loss of epoch-42 batch-141 = 1.4589400961995125e-05

Training epoch-42 batch-142
Running loss of epoch-42 batch-142 = 1.0783318430185318e-05

Training epoch-42 batch-143
Running loss of epoch-42 batch-143 = 1.795613206923008e-05

Training epoch-42 batch-144
Running loss of epoch-42 batch-144 = 1.7895828932523727e-05

Training epoch-42 batch-145
Running loss of epoch-42 batch-145 = 1.0651536285877228e-05

Training epoch-42 batch-146
Running loss of epoch-42 batch-146 = 2.452288754284382e-05

Training epoch-42 batch-147
Running loss of epoch-42 batch-147 = 1.0712305083870888e-05

Training epoch-42 batch-148
Running loss of epoch-42 batch-148 = 5.408423021435738e-06

Training epoch-42 batch-149
Running loss of epoch-42 batch-149 = 9.341398254036903e-06

Training epoch-42 batch-150
Running loss of epoch-42 batch-150 = 5.1085371524095535e-06

Training epoch-42 batch-151
Running loss of epoch-42 batch-151 = 8.919043466448784e-06

Training epoch-42 batch-152
Running loss of epoch-42 batch-152 = 6.84778206050396e-06

Training epoch-42 batch-153
Running loss of epoch-42 batch-153 = 1.2535369023680687e-05

Training epoch-42 batch-154
Running loss of epoch-42 batch-154 = 1.3177748769521713e-05

Training epoch-42 batch-155
Running loss of epoch-42 batch-155 = 9.577721357345581e-06

Training epoch-42 batch-156
Running loss of epoch-42 batch-156 = 6.123678758740425e-06

Training epoch-42 batch-157
Running loss of epoch-42 batch-157 = 2.8468668460845947e-05

Finished training epoch-42.



Average train loss at epoch-42 = 1.4751208573579789e-05

Started Evaluation

Average val loss at epoch-42 = 1.0404285510564932

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 92.13 %
Accuracy for class onCreate is: 92.96 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 50.60 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.85 %

Finished Evaluation



Started training epoch-43


Training epoch-43 batch-1
Running loss of epoch-43 batch-1 = 1.874077133834362e-05

Training epoch-43 batch-2
Running loss of epoch-43 batch-2 = 1.4149816706776619e-05

Training epoch-43 batch-3
Running loss of epoch-43 batch-3 = 3.4417957067489624e-05

Training epoch-43 batch-4
Running loss of epoch-43 batch-4 = 1.5247846022248268e-05

Training epoch-43 batch-5
Running loss of epoch-43 batch-5 = 7.219146937131882e-06

Training epoch-43 batch-6
Running loss of epoch-43 batch-6 = 8.700648322701454e-06

Training epoch-43 batch-7
Running loss of epoch-43 batch-7 = 1.3664830476045609e-05

Training epoch-43 batch-8
Running loss of epoch-43 batch-8 = 1.239660196006298e-05

Training epoch-43 batch-9
Running loss of epoch-43 batch-9 = 2.2245338186621666e-05

Training epoch-43 batch-10
Running loss of epoch-43 batch-10 = 9.626848623156548e-06

Training epoch-43 batch-11
Running loss of epoch-43 batch-11 = 7.4177514761686325e-06

Training epoch-43 batch-12
Running loss of epoch-43 batch-12 = 1.0088318958878517e-05

Training epoch-43 batch-13
Running loss of epoch-43 batch-13 = 9.596580639481544e-06

Training epoch-43 batch-14
Running loss of epoch-43 batch-14 = 6.472459062933922e-06

Training epoch-43 batch-15
Running loss of epoch-43 batch-15 = 1.110951416194439e-05

Training epoch-43 batch-16
Running loss of epoch-43 batch-16 = 1.0546064004302025e-05

Training epoch-43 batch-17
Running loss of epoch-43 batch-17 = 7.70576298236847e-06

Training epoch-43 batch-18
Running loss of epoch-43 batch-18 = 2.0492589101195335e-05

Training epoch-43 batch-19
Running loss of epoch-43 batch-19 = 3.555929288268089e-05

Training epoch-43 batch-20
Running loss of epoch-43 batch-20 = 1.619500108063221e-05

Training epoch-43 batch-21
Running loss of epoch-43 batch-21 = 1.0945368558168411e-05

Training epoch-43 batch-22
Running loss of epoch-43 batch-22 = 1.498381607234478e-05

Training epoch-43 batch-23
Running loss of epoch-43 batch-23 = 2.6456546038389206e-06

Training epoch-43 batch-24
Running loss of epoch-43 batch-24 = 1.4116056263446808e-05

Training epoch-43 batch-25
Running loss of epoch-43 batch-25 = 6.582587957382202e-06

Training epoch-43 batch-26
Running loss of epoch-43 batch-26 = 8.11251811683178e-06

Training epoch-43 batch-27
Running loss of epoch-43 batch-27 = 1.1499971151351929e-05

Training epoch-43 batch-28
Running loss of epoch-43 batch-28 = 1.2350501492619514e-05

Training epoch-43 batch-29
Running loss of epoch-43 batch-29 = 8.325092494487762e-06

Training epoch-43 batch-30
Running loss of epoch-43 batch-30 = 5.705980584025383e-06

Training epoch-43 batch-31
Running loss of epoch-43 batch-31 = 2.300320193171501e-05

Training epoch-43 batch-32
Running loss of epoch-43 batch-32 = 9.287381544709206e-06

Training epoch-43 batch-33
Running loss of epoch-43 batch-33 = 1.7876154743134975e-05

Training epoch-43 batch-34
Running loss of epoch-43 batch-34 = 1.4895107597112656e-05

Training epoch-43 batch-35
Running loss of epoch-43 batch-35 = 9.490875527262688e-06

Training epoch-43 batch-36
Running loss of epoch-43 batch-36 = 6.705522537231445e-06

Training epoch-43 batch-37
Running loss of epoch-43 batch-37 = 1.073651947081089e-05

Training epoch-43 batch-38
Running loss of epoch-43 batch-38 = 2.4135224521160126e-05

Training epoch-43 batch-39
Running loss of epoch-43 batch-39 = 5.835900083184242e-06

Training epoch-43 batch-40
Running loss of epoch-43 batch-40 = 2.226373180747032e-05

Training epoch-43 batch-41
Running loss of epoch-43 batch-41 = 8.967239409685135e-06

Training epoch-43 batch-42
Running loss of epoch-43 batch-42 = 7.700640708208084e-06

Training epoch-43 batch-43
Running loss of epoch-43 batch-43 = 1.0009855031967163e-05

Training epoch-43 batch-44
Running loss of epoch-43 batch-44 = 1.9300496205687523e-05

Training epoch-43 batch-45
Running loss of epoch-43 batch-45 = 8.566537871956825e-06

Training epoch-43 batch-46
Running loss of epoch-43 batch-46 = 5.644978955388069e-06

Training epoch-43 batch-47
Running loss of epoch-43 batch-47 = 1.2903707101941109e-05

Training epoch-43 batch-48
Running loss of epoch-43 batch-48 = 5.324836820363998e-06

Training epoch-43 batch-49
Running loss of epoch-43 batch-49 = 7.692258805036545e-06

Training epoch-43 batch-50
Running loss of epoch-43 batch-50 = 9.306240826845169e-06

Training epoch-43 batch-51
Running loss of epoch-43 batch-51 = 1.4186371117830276e-05

Training epoch-43 batch-52
Running loss of epoch-43 batch-52 = 8.850125595927238e-06

Training epoch-43 batch-53
Running loss of epoch-43 batch-53 = 1.8428778275847435e-05

Training epoch-43 batch-54
Running loss of epoch-43 batch-54 = 1.1785188689827919e-05

Training epoch-43 batch-55
Running loss of epoch-43 batch-55 = 1.1594733223319054e-05

Training epoch-43 batch-56
Running loss of epoch-43 batch-56 = 5.526235327124596e-06

Training epoch-43 batch-57
Running loss of epoch-43 batch-57 = 1.9389204680919647e-05

Training epoch-43 batch-58
Running loss of epoch-43 batch-58 = 1.993030309677124e-05

Training epoch-43 batch-59
Running loss of epoch-43 batch-59 = 1.5841331332921982e-05

Training epoch-43 batch-60
Running loss of epoch-43 batch-60 = 5.712732672691345e-06

Training epoch-43 batch-61
Running loss of epoch-43 batch-61 = 1.5294412150979042e-05

Training epoch-43 batch-62
Running loss of epoch-43 batch-62 = 7.014721632003784e-06

Training epoch-43 batch-63
Running loss of epoch-43 batch-63 = 1.4055054634809494e-05

Training epoch-43 batch-64
Running loss of epoch-43 batch-64 = 2.5002285838127136e-05

Training epoch-43 batch-65
Running loss of epoch-43 batch-65 = 1.5934696421027184e-05

Training epoch-43 batch-66
Running loss of epoch-43 batch-66 = 1.3661570847034454e-05

Training epoch-43 batch-67
Running loss of epoch-43 batch-67 = 1.772446557879448e-05

Training epoch-43 batch-68
Running loss of epoch-43 batch-68 = 2.8735725209116936e-05

Training epoch-43 batch-69
Running loss of epoch-43 batch-69 = 2.754875458776951e-05

Training epoch-43 batch-70
Running loss of epoch-43 batch-70 = 1.1105788871645927e-05

Training epoch-43 batch-71
Running loss of epoch-43 batch-71 = 4.977453500032425e-06

Training epoch-43 batch-72
Running loss of epoch-43 batch-72 = 1.193629577755928e-05

Training epoch-43 batch-73
Running loss of epoch-43 batch-73 = 8.36048275232315e-06

Training epoch-43 batch-74
Running loss of epoch-43 batch-74 = 1.044105738401413e-05

Training epoch-43 batch-75
Running loss of epoch-43 batch-75 = 9.3111302703619e-06

Training epoch-43 batch-76
Running loss of epoch-43 batch-76 = 3.355555236339569e-05

Training epoch-43 batch-77
Running loss of epoch-43 batch-77 = 9.344890713691711e-06

Training epoch-43 batch-78
Running loss of epoch-43 batch-78 = 1.3560988008975983e-05

Training epoch-43 batch-79
Running loss of epoch-43 batch-79 = 1.2579141184687614e-05

Training epoch-43 batch-80
Running loss of epoch-43 batch-80 = 1.4675315469503403e-05

Training epoch-43 batch-81
Running loss of epoch-43 batch-81 = 1.0168179869651794e-05

Training epoch-43 batch-82
Running loss of epoch-43 batch-82 = 8.079223334789276e-06

Training epoch-43 batch-83
Running loss of epoch-43 batch-83 = 8.18958505988121e-06

Training epoch-43 batch-84
Running loss of epoch-43 batch-84 = 1.0735588148236275e-05

Training epoch-43 batch-85
Running loss of epoch-43 batch-85 = 1.4513498172163963e-05

Training epoch-43 batch-86
Running loss of epoch-43 batch-86 = 9.177951142191887e-06

Training epoch-43 batch-87
Running loss of epoch-43 batch-87 = 4.397053271532059e-05

Training epoch-43 batch-88
Running loss of epoch-43 batch-88 = 9.356066584587097e-06

Training epoch-43 batch-89
Running loss of epoch-43 batch-89 = 8.204253390431404e-06

Training epoch-43 batch-90
Running loss of epoch-43 batch-90 = 1.308368518948555e-05

Training epoch-43 batch-91
Running loss of epoch-43 batch-91 = 1.2418488040566444e-05

Training epoch-43 batch-92
Running loss of epoch-43 batch-92 = 1.8563121557235718e-05

Training epoch-43 batch-93
Running loss of epoch-43 batch-93 = 9.130220860242844e-06

Training epoch-43 batch-94
Running loss of epoch-43 batch-94 = 1.212279312312603e-05

Training epoch-43 batch-95
Running loss of epoch-43 batch-95 = 1.39561016112566e-05

Training epoch-43 batch-96
Running loss of epoch-43 batch-96 = 1.534656621515751e-05

Training epoch-43 batch-97
Running loss of epoch-43 batch-97 = 1.518474891781807e-05

Training epoch-43 batch-98
Running loss of epoch-43 batch-98 = 1.55656598508358e-05

Training epoch-43 batch-99
Running loss of epoch-43 batch-99 = 1.9047409296035767e-05

Training epoch-43 batch-100
Running loss of epoch-43 batch-100 = 4.811212420463562e-06

Training epoch-43 batch-101
Running loss of epoch-43 batch-101 = 3.941357135772705e-05

Training epoch-43 batch-102
Running loss of epoch-43 batch-102 = 1.5660887584090233e-05

Training epoch-43 batch-103
Running loss of epoch-43 batch-103 = 4.334305413067341e-05

Training epoch-43 batch-104
Running loss of epoch-43 batch-104 = 9.23103652894497e-06

Training epoch-43 batch-105
Running loss of epoch-43 batch-105 = 1.59507617354393e-05

Training epoch-43 batch-106
Running loss of epoch-43 batch-106 = 1.6707926988601685e-05

Training epoch-43 batch-107
Running loss of epoch-43 batch-107 = 2.1372223272919655e-05

Training epoch-43 batch-108
Running loss of epoch-43 batch-108 = 1.1560739949345589e-05

Training epoch-43 batch-109
Running loss of epoch-43 batch-109 = 1.4682533219456673e-05

Training epoch-43 batch-110
Running loss of epoch-43 batch-110 = 7.30808824300766e-06

Training epoch-43 batch-111
Running loss of epoch-43 batch-111 = 8.948612958192825e-06

Training epoch-43 batch-112
Running loss of epoch-43 batch-112 = 1.3205921277403831e-05

Training epoch-43 batch-113
Running loss of epoch-43 batch-113 = 1.2588687241077423e-05

Training epoch-43 batch-114
Running loss of epoch-43 batch-114 = 3.839610144495964e-05

Training epoch-43 batch-115
Running loss of epoch-43 batch-115 = 1.5413155779242516e-05

Training epoch-43 batch-116
Running loss of epoch-43 batch-116 = 2.3378292098641396e-05

Training epoch-43 batch-117
Running loss of epoch-43 batch-117 = 2.0630890503525734e-05

Training epoch-43 batch-118
Running loss of epoch-43 batch-118 = 1.7618294805288315e-05

Training epoch-43 batch-119
Running loss of epoch-43 batch-119 = 1.949467696249485e-05

Training epoch-43 batch-120
Running loss of epoch-43 batch-120 = 1.1285068467259407e-05

Training epoch-43 batch-121
Running loss of epoch-43 batch-121 = 1.5533994883298874e-05

Training epoch-43 batch-122
Running loss of epoch-43 batch-122 = 6.817979738116264e-06

Training epoch-43 batch-123
Running loss of epoch-43 batch-123 = 7.009832188487053e-06

Training epoch-43 batch-124
Running loss of epoch-43 batch-124 = 1.0939547792077065e-05

Training epoch-43 batch-125
Running loss of epoch-43 batch-125 = 2.7578207664191723e-05

Training epoch-43 batch-126
Running loss of epoch-43 batch-126 = 2.207583747804165e-05

Training epoch-43 batch-127
Running loss of epoch-43 batch-127 = 1.0820571333169937e-05

Training epoch-43 batch-128
Running loss of epoch-43 batch-128 = 1.613120548427105e-05

Training epoch-43 batch-129
Running loss of epoch-43 batch-129 = 7.666647434234619e-06

Training epoch-43 batch-130
Running loss of epoch-43 batch-130 = 1.1429190635681152e-05

Training epoch-43 batch-131
Running loss of epoch-43 batch-131 = 7.992610335350037e-06

Training epoch-43 batch-132
Running loss of epoch-43 batch-132 = 1.624436117708683e-05

Training epoch-43 batch-133
Running loss of epoch-43 batch-133 = 1.050485298037529e-05

Training epoch-43 batch-134
Running loss of epoch-43 batch-134 = 7.964903488755226e-06

Training epoch-43 batch-135
Running loss of epoch-43 batch-135 = 1.575704663991928e-05

Training epoch-43 batch-136
Running loss of epoch-43 batch-136 = 1.6205478459596634e-05

Training epoch-43 batch-137
Running loss of epoch-43 batch-137 = 7.5569842010736465e-06

Training epoch-43 batch-138
Running loss of epoch-43 batch-138 = 1.2325122952461243e-05

Training epoch-43 batch-139
Running loss of epoch-43 batch-139 = 5.131121724843979e-06

Training epoch-43 batch-140
Running loss of epoch-43 batch-140 = 1.2419652193784714e-05

Training epoch-43 batch-141
Running loss of epoch-43 batch-141 = 4.40375879406929e-06

Training epoch-43 batch-142
Running loss of epoch-43 batch-142 = 1.1259689927101135e-05

Training epoch-43 batch-143
Running loss of epoch-43 batch-143 = 9.567011147737503e-06

Training epoch-43 batch-144
Running loss of epoch-43 batch-144 = 2.0281411707401276e-05

Training epoch-43 batch-145
Running loss of epoch-43 batch-145 = 2.25212424993515e-05

Training epoch-43 batch-146
Running loss of epoch-43 batch-146 = 7.31530599296093e-06

Training epoch-43 batch-147
Running loss of epoch-43 batch-147 = 1.3752840459346771e-05

Training epoch-43 batch-148
Running loss of epoch-43 batch-148 = 7.372815161943436e-06

Training epoch-43 batch-149
Running loss of epoch-43 batch-149 = 1.424551010131836e-05

Training epoch-43 batch-150
Running loss of epoch-43 batch-150 = 1.070578582584858e-05

Training epoch-43 batch-151
Running loss of epoch-43 batch-151 = 2.695387229323387e-05

Training epoch-43 batch-152
Running loss of epoch-43 batch-152 = 1.1967960745096207e-05

Training epoch-43 batch-153
Running loss of epoch-43 batch-153 = 2.6496825739741325e-05

Training epoch-43 batch-154
Running loss of epoch-43 batch-154 = 1.0670861229300499e-05

Training epoch-43 batch-155
Running loss of epoch-43 batch-155 = 1.7725862562656403e-05

Training epoch-43 batch-156
Running loss of epoch-43 batch-156 = 4.049157723784447e-06

Training epoch-43 batch-157
Running loss of epoch-43 batch-157 = 2.621859312057495e-05

Finished training epoch-43.



Average train loss at epoch-43 = 1.3979756832122803e-05

Started Evaluation

Average val loss at epoch-43 = 1.0523871032145937

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 91.48 %
Accuracy for class onCreate is: 92.00 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 66.89 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.16 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 73.08 %

Overall Accuracy = 83.62 %

Finished Evaluation



Started training epoch-44


Training epoch-44 batch-1
Running loss of epoch-44 batch-1 = 1.4030607417225838e-05

Training epoch-44 batch-2
Running loss of epoch-44 batch-2 = 8.475268259644508e-06

Training epoch-44 batch-3
Running loss of epoch-44 batch-3 = 5.844980478286743e-06

Training epoch-44 batch-4
Running loss of epoch-44 batch-4 = 1.4541670680046082e-05

Training epoch-44 batch-5
Running loss of epoch-44 batch-5 = 9.863171726465225e-06

Training epoch-44 batch-6
Running loss of epoch-44 batch-6 = 5.002832040190697e-06

Training epoch-44 batch-7
Running loss of epoch-44 batch-7 = 1.0592164471745491e-05

Training epoch-44 batch-8
Running loss of epoch-44 batch-8 = 2.2540567442774773e-05

Training epoch-44 batch-9
Running loss of epoch-44 batch-9 = 8.659902960062027e-06

Training epoch-44 batch-10
Running loss of epoch-44 batch-10 = 1.369020901620388e-05

Training epoch-44 batch-11
Running loss of epoch-44 batch-11 = 7.792375981807709e-06

Training epoch-44 batch-12
Running loss of epoch-44 batch-12 = 4.277098923921585e-06

Training epoch-44 batch-13
Running loss of epoch-44 batch-13 = 7.792608812451363e-06

Training epoch-44 batch-14
Running loss of epoch-44 batch-14 = 8.465955033898354e-06

Training epoch-44 batch-15
Running loss of epoch-44 batch-15 = 3.4191180020570755e-06

Training epoch-44 batch-16
Running loss of epoch-44 batch-16 = 1.2242933735251427e-05

Training epoch-44 batch-17
Running loss of epoch-44 batch-17 = 1.0380754247307777e-05

Training epoch-44 batch-18
Running loss of epoch-44 batch-18 = 1.0010786354541779e-05

Training epoch-44 batch-19
Running loss of epoch-44 batch-19 = 9.155133739113808e-06

Training epoch-44 batch-20
Running loss of epoch-44 batch-20 = 2.2997381165623665e-05

Training epoch-44 batch-21
Running loss of epoch-44 batch-21 = 9.322073310613632e-06

Training epoch-44 batch-22
Running loss of epoch-44 batch-22 = 6.584916263818741e-06

Training epoch-44 batch-23
Running loss of epoch-44 batch-23 = 1.1696713045239449e-05

Training epoch-44 batch-24
Running loss of epoch-44 batch-24 = 6.216811016201973e-06

Training epoch-44 batch-25
Running loss of epoch-44 batch-25 = 1.5913276001811028e-05

Training epoch-44 batch-26
Running loss of epoch-44 batch-26 = 1.119496300816536e-05

Training epoch-44 batch-27
Running loss of epoch-44 batch-27 = 1.3404292985796928e-05

Training epoch-44 batch-28
Running loss of epoch-44 batch-28 = 7.952330633997917e-06

Training epoch-44 batch-29
Running loss of epoch-44 batch-29 = 1.9884901121258736e-05

Training epoch-44 batch-30
Running loss of epoch-44 batch-30 = 1.2959586456418037e-05

Training epoch-44 batch-31
Running loss of epoch-44 batch-31 = 7.5909774750471115e-06

Training epoch-44 batch-32
Running loss of epoch-44 batch-32 = 1.8212245777249336e-05

Training epoch-44 batch-33
Running loss of epoch-44 batch-33 = 8.800998330116272e-06

Training epoch-44 batch-34
Running loss of epoch-44 batch-34 = 1.8450897186994553e-05

Training epoch-44 batch-35
Running loss of epoch-44 batch-35 = 1.1320342309772968e-05

Training epoch-44 batch-36
Running loss of epoch-44 batch-36 = 1.2251315638422966e-05

Training epoch-44 batch-37
Running loss of epoch-44 batch-37 = 2.253986895084381e-05

Training epoch-44 batch-38
Running loss of epoch-44 batch-38 = 1.2794975191354752e-05

Training epoch-44 batch-39
Running loss of epoch-44 batch-39 = 2.168305218219757e-05

Training epoch-44 batch-40
Running loss of epoch-44 batch-40 = 1.7355429008603096e-05

Training epoch-44 batch-41
Running loss of epoch-44 batch-41 = 8.740928024053574e-06

Training epoch-44 batch-42
Running loss of epoch-44 batch-42 = 1.1143740266561508e-05

Training epoch-44 batch-43
Running loss of epoch-44 batch-43 = 2.031540498137474e-05

Training epoch-44 batch-44
Running loss of epoch-44 batch-44 = 1.7805024981498718e-05

Training epoch-44 batch-45
Running loss of epoch-44 batch-45 = 1.390557736158371e-05

Training epoch-44 batch-46
Running loss of epoch-44 batch-46 = 1.0038726031780243e-05

Training epoch-44 batch-47
Running loss of epoch-44 batch-47 = 6.584450602531433e-06

Training epoch-44 batch-48
Running loss of epoch-44 batch-48 = 7.79423862695694e-06

Training epoch-44 batch-49
Running loss of epoch-44 batch-49 = 1.507950946688652e-05

Training epoch-44 batch-50
Running loss of epoch-44 batch-50 = 1.93142332136631e-05

Training epoch-44 batch-51
Running loss of epoch-44 batch-51 = 1.1724187061190605e-05

Training epoch-44 batch-52
Running loss of epoch-44 batch-52 = 1.1896947398781776e-05

Training epoch-44 batch-53
Running loss of epoch-44 batch-53 = 1.701340079307556e-05

Training epoch-44 batch-54
Running loss of epoch-44 batch-54 = 2.074078656733036e-05

Training epoch-44 batch-55
Running loss of epoch-44 batch-55 = 1.2099510058760643e-05

Training epoch-44 batch-56
Running loss of epoch-44 batch-56 = 1.489836722612381e-05

Training epoch-44 batch-57
Running loss of epoch-44 batch-57 = 1.9791536033153534e-05

Training epoch-44 batch-58
Running loss of epoch-44 batch-58 = 1.1856667697429657e-05

Training epoch-44 batch-59
Running loss of epoch-44 batch-59 = 1.0960735380649567e-05

Training epoch-44 batch-60
Running loss of epoch-44 batch-60 = 1.7558922991156578e-05

Training epoch-44 batch-61
Running loss of epoch-44 batch-61 = 2.0918436348438263e-05

Training epoch-44 batch-62
Running loss of epoch-44 batch-62 = 1.6535399481654167e-05

Training epoch-44 batch-63
Running loss of epoch-44 batch-63 = 5.905516445636749e-06

Training epoch-44 batch-64
Running loss of epoch-44 batch-64 = 1.3465061783790588e-05

Training epoch-44 batch-65
Running loss of epoch-44 batch-65 = 8.611241355538368e-06

Training epoch-44 batch-66
Running loss of epoch-44 batch-66 = 3.5983510315418243e-05

Training epoch-44 batch-67
Running loss of epoch-44 batch-67 = 1.2220814824104309e-05

Training epoch-44 batch-68
Running loss of epoch-44 batch-68 = 1.3996381312608719e-05

Training epoch-44 batch-69
Running loss of epoch-44 batch-69 = 1.5212688595056534e-05

Training epoch-44 batch-70
Running loss of epoch-44 batch-70 = 2.708076499402523e-05

Training epoch-44 batch-71
Running loss of epoch-44 batch-71 = 2.2252323105931282e-05

Training epoch-44 batch-72
Running loss of epoch-44 batch-72 = 1.6116071492433548e-05

Training epoch-44 batch-73
Running loss of epoch-44 batch-73 = 5.940673872828484e-06

Training epoch-44 batch-74
Running loss of epoch-44 batch-74 = 4.810048267245293e-06

Training epoch-44 batch-75
Running loss of epoch-44 batch-75 = 2.048444002866745e-06

Training epoch-44 batch-76
Running loss of epoch-44 batch-76 = 1.0824063792824745e-05

Training epoch-44 batch-77
Running loss of epoch-44 batch-77 = 4.759291186928749e-06

Training epoch-44 batch-78
Running loss of epoch-44 batch-78 = 1.977570354938507e-05

Training epoch-44 batch-79
Running loss of epoch-44 batch-79 = 6.938586011528969e-06

Training epoch-44 batch-80
Running loss of epoch-44 batch-80 = 1.4403136447072029e-05

Training epoch-44 batch-81
Running loss of epoch-44 batch-81 = 8.176080882549286e-06

Training epoch-44 batch-82
Running loss of epoch-44 batch-82 = 7.784692570567131e-06

Training epoch-44 batch-83
Running loss of epoch-44 batch-83 = 3.489665687084198e-05

Training epoch-44 batch-84
Running loss of epoch-44 batch-84 = 7.846159860491753e-06

Training epoch-44 batch-85
Running loss of epoch-44 batch-85 = 1.5618978068232536e-05

Training epoch-44 batch-86
Running loss of epoch-44 batch-86 = 1.5889527276158333e-05

Training epoch-44 batch-87
Running loss of epoch-44 batch-87 = 5.31109981238842e-06

Training epoch-44 batch-88
Running loss of epoch-44 batch-88 = 1.3220356777310371e-05

Training epoch-44 batch-89
Running loss of epoch-44 batch-89 = 1.0417308658361435e-05

Training epoch-44 batch-90
Running loss of epoch-44 batch-90 = 7.720664143562317e-06

Training epoch-44 batch-91
Running loss of epoch-44 batch-91 = 8.883420377969742e-06

Training epoch-44 batch-92
Running loss of epoch-44 batch-92 = 4.971399903297424e-06

Training epoch-44 batch-93
Running loss of epoch-44 batch-93 = 5.293870344758034e-06

Training epoch-44 batch-94
Running loss of epoch-44 batch-94 = 1.4137942343950272e-05

Training epoch-44 batch-95
Running loss of epoch-44 batch-95 = 2.6424648240208626e-05

Training epoch-44 batch-96
Running loss of epoch-44 batch-96 = 1.2920238077640533e-05

Training epoch-44 batch-97
Running loss of epoch-44 batch-97 = 8.739763870835304e-06

Training epoch-44 batch-98
Running loss of epoch-44 batch-98 = 2.1124258637428284e-05

Training epoch-44 batch-99
Running loss of epoch-44 batch-99 = 8.837785571813583e-06

Training epoch-44 batch-100
Running loss of epoch-44 batch-100 = 4.783738404512405e-06

Training epoch-44 batch-101
Running loss of epoch-44 batch-101 = 9.708572179079056e-06

Training epoch-44 batch-102
Running loss of epoch-44 batch-102 = 1.4592893421649933e-05

Training epoch-44 batch-103
Running loss of epoch-44 batch-103 = 1.246156170964241e-05

Training epoch-44 batch-104
Running loss of epoch-44 batch-104 = 6.018904969096184e-06

Training epoch-44 batch-105
Running loss of epoch-44 batch-105 = 7.405877113342285e-06

Training epoch-44 batch-106
Running loss of epoch-44 batch-106 = 1.209392212331295e-05

Training epoch-44 batch-107
Running loss of epoch-44 batch-107 = 2.2642547264695168e-05

Training epoch-44 batch-108
Running loss of epoch-44 batch-108 = 7.89109617471695e-06

Training epoch-44 batch-109
Running loss of epoch-44 batch-109 = 6.844056770205498e-06

Training epoch-44 batch-110
Running loss of epoch-44 batch-110 = 7.331371307373047e-06

Training epoch-44 batch-111
Running loss of epoch-44 batch-111 = 8.878763765096664e-06

Training epoch-44 batch-112
Running loss of epoch-44 batch-112 = 1.1533498764038086e-05

Training epoch-44 batch-113
Running loss of epoch-44 batch-113 = 1.7727725207805634e-05

Training epoch-44 batch-114
Running loss of epoch-44 batch-114 = 1.5418045222759247e-05

Training epoch-44 batch-115
Running loss of epoch-44 batch-115 = 1.7061829566955566e-05

Training epoch-44 batch-116
Running loss of epoch-44 batch-116 = 5.416804924607277e-06

Training epoch-44 batch-117
Running loss of epoch-44 batch-117 = 1.9694911316037178e-05

Training epoch-44 batch-118
Running loss of epoch-44 batch-118 = 1.2353062629699707e-05

Training epoch-44 batch-119
Running loss of epoch-44 batch-119 = 1.173466444015503e-05

Training epoch-44 batch-120
Running loss of epoch-44 batch-120 = 6.5374188125133514e-06

Training epoch-44 batch-121
Running loss of epoch-44 batch-121 = 1.227622851729393e-05

Training epoch-44 batch-122
Running loss of epoch-44 batch-122 = 1.0648975148797035e-05

Training epoch-44 batch-123
Running loss of epoch-44 batch-123 = 1.3488344848155975e-05

Training epoch-44 batch-124
Running loss of epoch-44 batch-124 = 2.0731240510940552e-05

Training epoch-44 batch-125
Running loss of epoch-44 batch-125 = 7.5874850153923035e-06

Training epoch-44 batch-126
Running loss of epoch-44 batch-126 = 1.4371704310178757e-05

Training epoch-44 batch-127
Running loss of epoch-44 batch-127 = 7.71903432905674e-06

Training epoch-44 batch-128
Running loss of epoch-44 batch-128 = 1.8742168322205544e-05

Training epoch-44 batch-129
Running loss of epoch-44 batch-129 = 1.1238502338528633e-05

Training epoch-44 batch-130
Running loss of epoch-44 batch-130 = 1.1663418263196945e-05

Training epoch-44 batch-131
Running loss of epoch-44 batch-131 = 7.294118404388428e-06

Training epoch-44 batch-132
Running loss of epoch-44 batch-132 = 1.9469764083623886e-05

Training epoch-44 batch-133
Running loss of epoch-44 batch-133 = 1.419801265001297e-05

Training epoch-44 batch-134
Running loss of epoch-44 batch-134 = 1.040380448102951e-05

Training epoch-44 batch-135
Running loss of epoch-44 batch-135 = 3.9351172745227814e-05

Training epoch-44 batch-136
Running loss of epoch-44 batch-136 = 1.0709278285503387e-05

Training epoch-44 batch-137
Running loss of epoch-44 batch-137 = 1.0239193215966225e-05

Training epoch-44 batch-138
Running loss of epoch-44 batch-138 = 1.4109071344137192e-05

Training epoch-44 batch-139
Running loss of epoch-44 batch-139 = 1.5396857634186745e-05

Training epoch-44 batch-140
Running loss of epoch-44 batch-140 = 1.8058577552437782e-05

Training epoch-44 batch-141
Running loss of epoch-44 batch-141 = 1.8925638869404793e-05

Training epoch-44 batch-142
Running loss of epoch-44 batch-142 = 1.3159820809960365e-05

Training epoch-44 batch-143
Running loss of epoch-44 batch-143 = 1.3616867363452911e-05

Training epoch-44 batch-144
Running loss of epoch-44 batch-144 = 1.347903162240982e-05

Training epoch-44 batch-145
Running loss of epoch-44 batch-145 = 1.9564060494303703e-05

Training epoch-44 batch-146
Running loss of epoch-44 batch-146 = 1.185503788292408e-05

Training epoch-44 batch-147
Running loss of epoch-44 batch-147 = 1.1316384188830853e-05

Training epoch-44 batch-148
Running loss of epoch-44 batch-148 = 1.4593824744224548e-05

Training epoch-44 batch-149
Running loss of epoch-44 batch-149 = 6.759772077202797e-06

Training epoch-44 batch-150
Running loss of epoch-44 batch-150 = 1.0266434401273727e-05

Training epoch-44 batch-151
Running loss of epoch-44 batch-151 = 1.5349825844168663e-05

Training epoch-44 batch-152
Running loss of epoch-44 batch-152 = 2.8226058930158615e-05

Training epoch-44 batch-153
Running loss of epoch-44 batch-153 = 3.905990161001682e-05

Training epoch-44 batch-154
Running loss of epoch-44 batch-154 = 1.6761943697929382e-05

Training epoch-44 batch-155
Running loss of epoch-44 batch-155 = 7.519032806158066e-06

Training epoch-44 batch-156
Running loss of epoch-44 batch-156 = 2.0770588889718056e-05

Training epoch-44 batch-157
Running loss of epoch-44 batch-157 = 4.377216100692749e-06

Finished training epoch-44.



Average train loss at epoch-44 = 1.3179700076580047e-05

Started Evaluation

Average val loss at epoch-44 = 1.0660630314286879

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 91.47 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.05 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.47 %

Finished Evaluation



Started training epoch-45


Training epoch-45 batch-1
Running loss of epoch-45 batch-1 = 1.4536548405885696e-05

Training epoch-45 batch-2
Running loss of epoch-45 batch-2 = 1.6557518392801285e-05

Training epoch-45 batch-3
Running loss of epoch-45 batch-3 = 1.0168412700295448e-05

Training epoch-45 batch-4
Running loss of epoch-45 batch-4 = 1.1078082025051117e-05

Training epoch-45 batch-5
Running loss of epoch-45 batch-5 = 9.103212505578995e-06

Training epoch-45 batch-6
Running loss of epoch-45 batch-6 = 1.483340747654438e-05

Training epoch-45 batch-7
Running loss of epoch-45 batch-7 = 9.085051715373993e-06

Training epoch-45 batch-8
Running loss of epoch-45 batch-8 = 3.525521606206894e-06

Training epoch-45 batch-9
Running loss of epoch-45 batch-9 = 7.246155291795731e-06

Training epoch-45 batch-10
Running loss of epoch-45 batch-10 = 4.819594323635101e-06

Training epoch-45 batch-11
Running loss of epoch-45 batch-11 = 1.3066921383142471e-05

Training epoch-45 batch-12
Running loss of epoch-45 batch-12 = 2.2758496925234795e-05

Training epoch-45 batch-13
Running loss of epoch-45 batch-13 = 1.749699003994465e-05

Training epoch-45 batch-14
Running loss of epoch-45 batch-14 = 2.3757806047797203e-05

Training epoch-45 batch-15
Running loss of epoch-45 batch-15 = 1.3124896213412285e-05

Training epoch-45 batch-16
Running loss of epoch-45 batch-16 = 8.379342034459114e-06

Training epoch-45 batch-17
Running loss of epoch-45 batch-17 = 9.232666343450546e-06

Training epoch-45 batch-18
Running loss of epoch-45 batch-18 = 9.190989658236504e-06

Training epoch-45 batch-19
Running loss of epoch-45 batch-19 = 1.8531689420342445e-05

Training epoch-45 batch-20
Running loss of epoch-45 batch-20 = 2.2413674741983414e-05

Training epoch-45 batch-21
Running loss of epoch-45 batch-21 = 1.2245262041687965e-05

Training epoch-45 batch-22
Running loss of epoch-45 batch-22 = 1.864507794380188e-05

Training epoch-45 batch-23
Running loss of epoch-45 batch-23 = 8.077127858996391e-06

Training epoch-45 batch-24
Running loss of epoch-45 batch-24 = 1.777149736881256e-05

Training epoch-45 batch-25
Running loss of epoch-45 batch-25 = 9.24011692404747e-06

Training epoch-45 batch-26
Running loss of epoch-45 batch-26 = 1.6284175217151642e-05

Training epoch-45 batch-27
Running loss of epoch-45 batch-27 = 9.340234100818634e-06

Training epoch-45 batch-28
Running loss of epoch-45 batch-28 = 1.2191245332360268e-05

Training epoch-45 batch-29
Running loss of epoch-45 batch-29 = 2.4004490114748478e-05

Training epoch-45 batch-30
Running loss of epoch-45 batch-30 = 8.9951790869236e-06

Training epoch-45 batch-31
Running loss of epoch-45 batch-31 = 5.6389253586530685e-06

Training epoch-45 batch-32
Running loss of epoch-45 batch-32 = 1.0261079296469688e-05

Training epoch-45 batch-33
Running loss of epoch-45 batch-33 = 8.656177669763565e-06

Training epoch-45 batch-34
Running loss of epoch-45 batch-34 = 4.368135705590248e-06

Training epoch-45 batch-35
Running loss of epoch-45 batch-35 = 8.704839274287224e-06

Training epoch-45 batch-36
Running loss of epoch-45 batch-36 = 1.5454599633812904e-05

Training epoch-45 batch-37
Running loss of epoch-45 batch-37 = 3.4151598811149597e-06

Training epoch-45 batch-38
Running loss of epoch-45 batch-38 = 1.4120712876319885e-05

Training epoch-45 batch-39
Running loss of epoch-45 batch-39 = 2.5365035980939865e-05

Training epoch-45 batch-40
Running loss of epoch-45 batch-40 = 8.942792192101479e-06

Training epoch-45 batch-41
Running loss of epoch-45 batch-41 = 4.076631739735603e-06

Training epoch-45 batch-42
Running loss of epoch-45 batch-42 = 1.1275988072156906e-05

Training epoch-45 batch-43
Running loss of epoch-45 batch-43 = 1.5878817066550255e-05

Training epoch-45 batch-44
Running loss of epoch-45 batch-44 = 7.85384327173233e-06

Training epoch-45 batch-45
Running loss of epoch-45 batch-45 = 7.956521585583687e-06

Training epoch-45 batch-46
Running loss of epoch-45 batch-46 = 2.2427644580602646e-05

Training epoch-45 batch-47
Running loss of epoch-45 batch-47 = 2.1041370928287506e-05

Training epoch-45 batch-48
Running loss of epoch-45 batch-48 = 1.2403586879372597e-05

Training epoch-45 batch-49
Running loss of epoch-45 batch-49 = 8.644303306937218e-06

Training epoch-45 batch-50
Running loss of epoch-45 batch-50 = 1.0274001397192478e-05

Training epoch-45 batch-51
Running loss of epoch-45 batch-51 = 4.741828888654709e-06

Training epoch-45 batch-52
Running loss of epoch-45 batch-52 = 1.0354677215218544e-05

Training epoch-45 batch-53
Running loss of epoch-45 batch-53 = 6.591202691197395e-06

Training epoch-45 batch-54
Running loss of epoch-45 batch-54 = 1.1457828804850578e-05

Training epoch-45 batch-55
Running loss of epoch-45 batch-55 = 1.2194272130727768e-05

Training epoch-45 batch-56
Running loss of epoch-45 batch-56 = 1.9402476027607918e-05

Training epoch-45 batch-57
Running loss of epoch-45 batch-57 = 8.729984983801842e-06

Training epoch-45 batch-58
Running loss of epoch-45 batch-58 = 4.693400114774704e-06

Training epoch-45 batch-59
Running loss of epoch-45 batch-59 = 8.625444024801254e-06

Training epoch-45 batch-60
Running loss of epoch-45 batch-60 = 2.105836756527424e-05

Training epoch-45 batch-61
Running loss of epoch-45 batch-61 = 9.205890819430351e-06

Training epoch-45 batch-62
Running loss of epoch-45 batch-62 = 6.164656952023506e-06

Training epoch-45 batch-63
Running loss of epoch-45 batch-63 = 1.3145385310053825e-05

Training epoch-45 batch-64
Running loss of epoch-45 batch-64 = 8.32253135740757e-06

Training epoch-45 batch-65
Running loss of epoch-45 batch-65 = 1.804996281862259e-05

Training epoch-45 batch-66
Running loss of epoch-45 batch-66 = 1.069856807589531e-05

Training epoch-45 batch-67
Running loss of epoch-45 batch-67 = 1.4591030776500702e-05

Training epoch-45 batch-68
Running loss of epoch-45 batch-68 = 9.383540600538254e-06

Training epoch-45 batch-69
Running loss of epoch-45 batch-69 = 7.2910916060209274e-06

Training epoch-45 batch-70
Running loss of epoch-45 batch-70 = 1.8882565200328827e-05

Training epoch-45 batch-71
Running loss of epoch-45 batch-71 = 8.227536454796791e-06

Training epoch-45 batch-72
Running loss of epoch-45 batch-72 = 1.1623837053775787e-05

Training epoch-45 batch-73
Running loss of epoch-45 batch-73 = 1.2832693755626678e-05

Training epoch-45 batch-74
Running loss of epoch-45 batch-74 = 2.7528498321771622e-05

Training epoch-45 batch-75
Running loss of epoch-45 batch-75 = 2.1748943254351616e-05

Training epoch-45 batch-76
Running loss of epoch-45 batch-76 = 6.870832294225693e-06

Training epoch-45 batch-77
Running loss of epoch-45 batch-77 = 1.030508428812027e-05

Training epoch-45 batch-78
Running loss of epoch-45 batch-78 = 9.925104677677155e-06

Training epoch-45 batch-79
Running loss of epoch-45 batch-79 = 1.0893447324633598e-05

Training epoch-45 batch-80
Running loss of epoch-45 batch-80 = 7.83754512667656e-06

Training epoch-45 batch-81
Running loss of epoch-45 batch-81 = 9.211711585521698e-06

Training epoch-45 batch-82
Running loss of epoch-45 batch-82 = 5.397479981184006e-06

Training epoch-45 batch-83
Running loss of epoch-45 batch-83 = 1.1266209185123444e-05

Training epoch-45 batch-84
Running loss of epoch-45 batch-84 = 1.810048706829548e-05

Training epoch-45 batch-85
Running loss of epoch-45 batch-85 = 2.5250716134905815e-05

Training epoch-45 batch-86
Running loss of epoch-45 batch-86 = 1.9205035641789436e-05

Training epoch-45 batch-87
Running loss of epoch-45 batch-87 = 1.0763527825474739e-05

Training epoch-45 batch-88
Running loss of epoch-45 batch-88 = 1.0045478120446205e-05

Training epoch-45 batch-89
Running loss of epoch-45 batch-89 = 1.4381017535924911e-05

Training epoch-45 batch-90
Running loss of epoch-45 batch-90 = 4.449393600225449e-06

Training epoch-45 batch-91
Running loss of epoch-45 batch-91 = 1.0751886293292046e-05

Training epoch-45 batch-92
Running loss of epoch-45 batch-92 = 4.1883205994963646e-05

Training epoch-45 batch-93
Running loss of epoch-45 batch-93 = 9.424285963177681e-06

Training epoch-45 batch-94
Running loss of epoch-45 batch-94 = 9.13604162633419e-06

Training epoch-45 batch-95
Running loss of epoch-45 batch-95 = 9.111128747463226e-06

Training epoch-45 batch-96
Running loss of epoch-45 batch-96 = 2.2430438548326492e-05

Training epoch-45 batch-97
Running loss of epoch-45 batch-97 = 1.4535384252667427e-05

Training epoch-45 batch-98
Running loss of epoch-45 batch-98 = 8.064089342951775e-06

Training epoch-45 batch-99
Running loss of epoch-45 batch-99 = 1.6535865142941475e-05

Training epoch-45 batch-100
Running loss of epoch-45 batch-100 = 1.2513948604464531e-05

Training epoch-45 batch-101
Running loss of epoch-45 batch-101 = 2.582324668765068e-05

Training epoch-45 batch-102
Running loss of epoch-45 batch-102 = 1.4828750863671303e-05

Training epoch-45 batch-103
Running loss of epoch-45 batch-103 = 1.8407590687274933e-05

Training epoch-45 batch-104
Running loss of epoch-45 batch-104 = 9.57166776061058e-06

Training epoch-45 batch-105
Running loss of epoch-45 batch-105 = 1.2312084436416626e-05

Training epoch-45 batch-106
Running loss of epoch-45 batch-106 = 1.0595656931400299e-05

Training epoch-45 batch-107
Running loss of epoch-45 batch-107 = 1.0595191270112991e-05

Training epoch-45 batch-108
Running loss of epoch-45 batch-108 = 1.2929784134030342e-05

Training epoch-45 batch-109
Running loss of epoch-45 batch-109 = 1.605600118637085e-05

Training epoch-45 batch-110
Running loss of epoch-45 batch-110 = 9.365146979689598e-06

Training epoch-45 batch-111
Running loss of epoch-45 batch-111 = 2.0195962861180305e-05

Training epoch-45 batch-112
Running loss of epoch-45 batch-112 = 1.2973789125680923e-05

Training epoch-45 batch-113
Running loss of epoch-45 batch-113 = 1.705787144601345e-05

Training epoch-45 batch-114
Running loss of epoch-45 batch-114 = 5.9138983488082886e-06

Training epoch-45 batch-115
Running loss of epoch-45 batch-115 = 5.68595714867115e-06

Training epoch-45 batch-116
Running loss of epoch-45 batch-116 = 2.0052073523402214e-05

Training epoch-45 batch-117
Running loss of epoch-45 batch-117 = 8.40681605041027e-06

Training epoch-45 batch-118
Running loss of epoch-45 batch-118 = 1.6912352293729782e-05

Training epoch-45 batch-119
Running loss of epoch-45 batch-119 = 8.916947990655899e-06

Training epoch-45 batch-120
Running loss of epoch-45 batch-120 = 7.268041372299194e-06

Training epoch-45 batch-121
Running loss of epoch-45 batch-121 = 4.475703462958336e-06

Training epoch-45 batch-122
Running loss of epoch-45 batch-122 = 1.085689291357994e-05

Training epoch-45 batch-123
Running loss of epoch-45 batch-123 = 1.3684853911399841e-05

Training epoch-45 batch-124
Running loss of epoch-45 batch-124 = 8.166534826159477e-06

Training epoch-45 batch-125
Running loss of epoch-45 batch-125 = 3.373343497514725e-05

Training epoch-45 batch-126
Running loss of epoch-45 batch-126 = 7.824739441275597e-06

Training epoch-45 batch-127
Running loss of epoch-45 batch-127 = 1.0378425940871239e-05

Training epoch-45 batch-128
Running loss of epoch-45 batch-128 = 9.32370312511921e-06

Training epoch-45 batch-129
Running loss of epoch-45 batch-129 = 1.3872748240828514e-05

Training epoch-45 batch-130
Running loss of epoch-45 batch-130 = 2.034055069088936e-05

Training epoch-45 batch-131
Running loss of epoch-45 batch-131 = 5.591195076704025e-06

Training epoch-45 batch-132
Running loss of epoch-45 batch-132 = 2.2831140086054802e-05

Training epoch-45 batch-133
Running loss of epoch-45 batch-133 = 1.3045035302639008e-05

Training epoch-45 batch-134
Running loss of epoch-45 batch-134 = 1.9024359062314034e-05

Training epoch-45 batch-135
Running loss of epoch-45 batch-135 = 8.510425686836243e-06

Training epoch-45 batch-136
Running loss of epoch-45 batch-136 = 6.23590312898159e-06

Training epoch-45 batch-137
Running loss of epoch-45 batch-137 = 1.411442644894123e-05

Training epoch-45 batch-138
Running loss of epoch-45 batch-138 = 8.564209565520287e-06

Training epoch-45 batch-139
Running loss of epoch-45 batch-139 = 9.56561416387558e-06

Training epoch-45 batch-140
Running loss of epoch-45 batch-140 = 1.1711614206433296e-05

Training epoch-45 batch-141
Running loss of epoch-45 batch-141 = 1.603667624294758e-05

Training epoch-45 batch-142
Running loss of epoch-45 batch-142 = 4.351837560534477e-06

Training epoch-45 batch-143
Running loss of epoch-45 batch-143 = 6.297370418906212e-06

Training epoch-45 batch-144
Running loss of epoch-45 batch-144 = 1.302408054471016e-05

Training epoch-45 batch-145
Running loss of epoch-45 batch-145 = 1.7838552594184875e-05

Training epoch-45 batch-146
Running loss of epoch-45 batch-146 = 1.261197030544281e-05

Training epoch-45 batch-147
Running loss of epoch-45 batch-147 = 8.924631401896477e-06

Training epoch-45 batch-148
Running loss of epoch-45 batch-148 = 7.762806490063667e-06

Training epoch-45 batch-149
Running loss of epoch-45 batch-149 = 8.042668923735619e-06

Training epoch-45 batch-150
Running loss of epoch-45 batch-150 = 5.508074536919594e-06

Training epoch-45 batch-151
Running loss of epoch-45 batch-151 = 1.2791715562343597e-05

Training epoch-45 batch-152
Running loss of epoch-45 batch-152 = 9.398674592375755e-06

Training epoch-45 batch-153
Running loss of epoch-45 batch-153 = 4.53670509159565e-06

Training epoch-45 batch-154
Running loss of epoch-45 batch-154 = 1.0760268196463585e-05

Training epoch-45 batch-155
Running loss of epoch-45 batch-155 = 8.256640285253525e-06

Training epoch-45 batch-156
Running loss of epoch-45 batch-156 = 2.8018606826663017e-05

Training epoch-45 batch-157
Running loss of epoch-45 batch-157 = 0.00023679062724113464

Finished training epoch-45.



Average train loss at epoch-45 = 1.283440887928009e-05

Started Evaluation

Average val loss at epoch-45 = 1.0688367590875332

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.80 %
Accuracy for class onCreate is: 91.47 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 65.98 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.38 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.56 %

Finished Evaluation



Started training epoch-46


Training epoch-46 batch-1
Running loss of epoch-46 batch-1 = 1.66487880051136e-05

Training epoch-46 batch-2
Running loss of epoch-46 batch-2 = 1.3001728802919388e-05

Training epoch-46 batch-3
Running loss of epoch-46 batch-3 = 1.4540040865540504e-05

Training epoch-46 batch-4
Running loss of epoch-46 batch-4 = 1.7685815691947937e-06

Training epoch-46 batch-5
Running loss of epoch-46 batch-5 = 1.6679405234754086e-05

Training epoch-46 batch-6
Running loss of epoch-46 batch-6 = 8.500879630446434e-06

Training epoch-46 batch-7
Running loss of epoch-46 batch-7 = 7.49039463698864e-06

Training epoch-46 batch-8
Running loss of epoch-46 batch-8 = 8.242903277277946e-06

Training epoch-46 batch-9
Running loss of epoch-46 batch-9 = 1.0768650099635124e-05

Training epoch-46 batch-10
Running loss of epoch-46 batch-10 = 1.3048294931650162e-05

Training epoch-46 batch-11
Running loss of epoch-46 batch-11 = 8.066883310675621e-06

Training epoch-46 batch-12
Running loss of epoch-46 batch-12 = 3.938563168048859e-06

Training epoch-46 batch-13
Running loss of epoch-46 batch-13 = 8.269911631941795e-06

Training epoch-46 batch-14
Running loss of epoch-46 batch-14 = 4.511792212724686e-06

Training epoch-46 batch-15
Running loss of epoch-46 batch-15 = 1.4755409210920334e-05

Training epoch-46 batch-16
Running loss of epoch-46 batch-16 = 1.0854331776499748e-05

Training epoch-46 batch-17
Running loss of epoch-46 batch-17 = 1.1141877621412277e-05

Training epoch-46 batch-18
Running loss of epoch-46 batch-18 = 7.425202056765556e-06

Training epoch-46 batch-19
Running loss of epoch-46 batch-19 = 7.712282240390778e-06

Training epoch-46 batch-20
Running loss of epoch-46 batch-20 = 8.766772225499153e-06

Training epoch-46 batch-21
Running loss of epoch-46 batch-21 = 1.0465271770954132e-05

Training epoch-46 batch-22
Running loss of epoch-46 batch-22 = 6.866874173283577e-06

Training epoch-46 batch-23
Running loss of epoch-46 batch-23 = 6.100162863731384e-06

Training epoch-46 batch-24
Running loss of epoch-46 batch-24 = 1.0539544746279716e-05

Training epoch-46 batch-25
Running loss of epoch-46 batch-25 = 9.177951142191887e-06

Training epoch-46 batch-26
Running loss of epoch-46 batch-26 = 1.2847362086176872e-05

Training epoch-46 batch-27
Running loss of epoch-46 batch-27 = 1.0505784302949905e-05

Training epoch-46 batch-28
Running loss of epoch-46 batch-28 = 7.772352546453476e-06

Training epoch-46 batch-29
Running loss of epoch-46 batch-29 = 9.442679584026337e-06

Training epoch-46 batch-30
Running loss of epoch-46 batch-30 = 9.077368304133415e-06

Training epoch-46 batch-31
Running loss of epoch-46 batch-31 = 7.702736184000969e-06

Training epoch-46 batch-32
Running loss of epoch-46 batch-32 = 1.6005709767341614e-05

Training epoch-46 batch-33
Running loss of epoch-46 batch-33 = 1.4133751392364502e-05

Training epoch-46 batch-34
Running loss of epoch-46 batch-34 = 2.202601172029972e-05

Training epoch-46 batch-35
Running loss of epoch-46 batch-35 = 1.0913703590631485e-05

Training epoch-46 batch-36
Running loss of epoch-46 batch-36 = 8.045462891459465e-06

Training epoch-46 batch-37
Running loss of epoch-46 batch-37 = 1.1101597920060158e-05

Training epoch-46 batch-38
Running loss of epoch-46 batch-38 = 1.4995690435171127e-05

Training epoch-46 batch-39
Running loss of epoch-46 batch-39 = 4.130881279706955e-06

Training epoch-46 batch-40
Running loss of epoch-46 batch-40 = 3.250874578952789e-05

Training epoch-46 batch-41
Running loss of epoch-46 batch-41 = 1.8869293853640556e-05

Training epoch-46 batch-42
Running loss of epoch-46 batch-42 = 1.2492993846535683e-05

Training epoch-46 batch-43
Running loss of epoch-46 batch-43 = 3.3427495509386063e-06

Training epoch-46 batch-44
Running loss of epoch-46 batch-44 = 4.973495379090309e-06

Training epoch-46 batch-45
Running loss of epoch-46 batch-45 = 5.258945748209953e-06

Training epoch-46 batch-46
Running loss of epoch-46 batch-46 = 2.1120533347129822e-05

Training epoch-46 batch-47
Running loss of epoch-46 batch-47 = 8.126255124807358e-06

Training epoch-46 batch-48
Running loss of epoch-46 batch-48 = 2.941838465631008e-05

Training epoch-46 batch-49
Running loss of epoch-46 batch-49 = 7.81053677201271e-06

Training epoch-46 batch-50
Running loss of epoch-46 batch-50 = 4.186294972896576e-06

Training epoch-46 batch-51
Running loss of epoch-46 batch-51 = 5.850102752447128e-06

Training epoch-46 batch-52
Running loss of epoch-46 batch-52 = 5.705514922738075e-06

Training epoch-46 batch-53
Running loss of epoch-46 batch-53 = 7.69016332924366e-06

Training epoch-46 batch-54
Running loss of epoch-46 batch-54 = 1.2274133041501045e-05

Training epoch-46 batch-55
Running loss of epoch-46 batch-55 = 9.472249075770378e-06

Training epoch-46 batch-56
Running loss of epoch-46 batch-56 = 5.911337211728096e-06

Training epoch-46 batch-57
Running loss of epoch-46 batch-57 = 1.0493211448192596e-05

Training epoch-46 batch-58
Running loss of epoch-46 batch-58 = 6.180722266435623e-06

Training epoch-46 batch-59
Running loss of epoch-46 batch-59 = 1.065409742295742e-05

Training epoch-46 batch-60
Running loss of epoch-46 batch-60 = 1.5182187780737877e-05

Training epoch-46 batch-61
Running loss of epoch-46 batch-61 = 1.4839693903923035e-05

Training epoch-46 batch-62
Running loss of epoch-46 batch-62 = 6.940215826034546e-06

Training epoch-46 batch-63
Running loss of epoch-46 batch-63 = 1.6286969184875488e-05

Training epoch-46 batch-64
Running loss of epoch-46 batch-64 = 1.9030412659049034e-05

Training epoch-46 batch-65
Running loss of epoch-46 batch-65 = 1.1170050129294395e-05

Training epoch-46 batch-66
Running loss of epoch-46 batch-66 = 1.3732584193348885e-05

Training epoch-46 batch-67
Running loss of epoch-46 batch-67 = 1.1778902262449265e-05

Training epoch-46 batch-68
Running loss of epoch-46 batch-68 = 1.112883910536766e-05

Training epoch-46 batch-69
Running loss of epoch-46 batch-69 = 6.406102329492569e-06

Training epoch-46 batch-70
Running loss of epoch-46 batch-70 = 8.526723831892014e-06

Training epoch-46 batch-71
Running loss of epoch-46 batch-71 = 3.911787644028664e-06

Training epoch-46 batch-72
Running loss of epoch-46 batch-72 = 8.556060492992401e-06

Training epoch-46 batch-73
Running loss of epoch-46 batch-73 = 2.35019251704216e-06

Training epoch-46 batch-74
Running loss of epoch-46 batch-74 = 1.5656230971217155e-05

Training epoch-46 batch-75
Running loss of epoch-46 batch-75 = 9.076204150915146e-06

Training epoch-46 batch-76
Running loss of epoch-46 batch-76 = 2.3523811250925064e-05

Training epoch-46 batch-77
Running loss of epoch-46 batch-77 = 7.980270311236382e-06

Training epoch-46 batch-78
Running loss of epoch-46 batch-78 = 2.262881025671959e-05

Training epoch-46 batch-79
Running loss of epoch-46 batch-79 = 1.4051329344511032e-05

Training epoch-46 batch-80
Running loss of epoch-46 batch-80 = 6.2498729676008224e-06

Training epoch-46 batch-81
Running loss of epoch-46 batch-81 = 1.5665777027606964e-05

Training epoch-46 batch-82
Running loss of epoch-46 batch-82 = 3.0978117138147354e-06

Training epoch-46 batch-83
Running loss of epoch-46 batch-83 = 8.404022082686424e-06

Training epoch-46 batch-84
Running loss of epoch-46 batch-84 = 8.74255783855915e-06

Training epoch-46 batch-85
Running loss of epoch-46 batch-85 = 5.364883691072464e-06

Training epoch-46 batch-86
Running loss of epoch-46 batch-86 = 1.1131633073091507e-05

Training epoch-46 batch-87
Running loss of epoch-46 batch-87 = 1.294771209359169e-05

Training epoch-46 batch-88
Running loss of epoch-46 batch-88 = 1.3207318261265755e-05

Training epoch-46 batch-89
Running loss of epoch-46 batch-89 = 2.250075340270996e-06

Training epoch-46 batch-90
Running loss of epoch-46 batch-90 = 4.541594535112381e-06

Training epoch-46 batch-91
Running loss of epoch-46 batch-91 = 4.694098606705666e-06

Training epoch-46 batch-92
Running loss of epoch-46 batch-92 = 2.386840060353279e-05

Training epoch-46 batch-93
Running loss of epoch-46 batch-93 = 1.0289251804351807e-05

Training epoch-46 batch-94
Running loss of epoch-46 batch-94 = 1.758662983775139e-05

Training epoch-46 batch-95
Running loss of epoch-46 batch-95 = 7.413327693939209e-06

Training epoch-46 batch-96
Running loss of epoch-46 batch-96 = 8.304137736558914e-06

Training epoch-46 batch-97
Running loss of epoch-46 batch-97 = 1.7549609765410423e-05

Training epoch-46 batch-98
Running loss of epoch-46 batch-98 = 7.033580914139748e-06

Training epoch-46 batch-99
Running loss of epoch-46 batch-99 = 1.125759445130825e-05

Training epoch-46 batch-100
Running loss of epoch-46 batch-100 = 7.539987564086914e-06

Training epoch-46 batch-101
Running loss of epoch-46 batch-101 = 1.819315366446972e-05

Training epoch-46 batch-102
Running loss of epoch-46 batch-102 = 1.042545773088932e-05

Training epoch-46 batch-103
Running loss of epoch-46 batch-103 = 1.0402407497167587e-05

Training epoch-46 batch-104
Running loss of epoch-46 batch-104 = 8.861534297466278e-06

Training epoch-46 batch-105
Running loss of epoch-46 batch-105 = 8.553266525268555e-06

Training epoch-46 batch-106
Running loss of epoch-46 batch-106 = 8.893199265003204e-06

Training epoch-46 batch-107
Running loss of epoch-46 batch-107 = 3.0427007004618645e-05

Training epoch-46 batch-108
Running loss of epoch-46 batch-108 = 7.539056241512299e-06

Training epoch-46 batch-109
Running loss of epoch-46 batch-109 = 2.1828804165124893e-05

Training epoch-46 batch-110
Running loss of epoch-46 batch-110 = 8.64686444401741e-06

Training epoch-46 batch-111
Running loss of epoch-46 batch-111 = 1.0649440810084343e-05

Training epoch-46 batch-112
Running loss of epoch-46 batch-112 = 4.285480827093124e-06

Training epoch-46 batch-113
Running loss of epoch-46 batch-113 = 1.8424121662974358e-05

Training epoch-46 batch-114
Running loss of epoch-46 batch-114 = 1.0290881618857384e-05

Training epoch-46 batch-115
Running loss of epoch-46 batch-115 = 9.172363206744194e-06

Training epoch-46 batch-116
Running loss of epoch-46 batch-116 = 2.9353424906730652e-05

Training epoch-46 batch-117
Running loss of epoch-46 batch-117 = 7.018446922302246e-06

Training epoch-46 batch-118
Running loss of epoch-46 batch-118 = 1.51144340634346e-05

Training epoch-46 batch-119
Running loss of epoch-46 batch-119 = 1.3877404853701591e-05

Training epoch-46 batch-120
Running loss of epoch-46 batch-120 = 1.4869729056954384e-05

Training epoch-46 batch-121
Running loss of epoch-46 batch-121 = 3.0209776014089584e-05

Training epoch-46 batch-122
Running loss of epoch-46 batch-122 = 1.6788020730018616e-05

Training epoch-46 batch-123
Running loss of epoch-46 batch-123 = 4.2790547013282776e-05

Training epoch-46 batch-124
Running loss of epoch-46 batch-124 = 2.011493779718876e-05

Training epoch-46 batch-125
Running loss of epoch-46 batch-125 = 1.2112781405448914e-05

Training epoch-46 batch-126
Running loss of epoch-46 batch-126 = 7.556518539786339e-06

Training epoch-46 batch-127
Running loss of epoch-46 batch-127 = 7.884576916694641e-06

Training epoch-46 batch-128
Running loss of epoch-46 batch-128 = 4.537869244813919e-06

Training epoch-46 batch-129
Running loss of epoch-46 batch-129 = 2.7112895622849464e-05

Training epoch-46 batch-130
Running loss of epoch-46 batch-130 = 1.1846423149108887e-05

Training epoch-46 batch-131
Running loss of epoch-46 batch-131 = 9.015435352921486e-06

Training epoch-46 batch-132
Running loss of epoch-46 batch-132 = 2.2503314539790154e-05

Training epoch-46 batch-133
Running loss of epoch-46 batch-133 = 8.751172572374344e-06

Training epoch-46 batch-134
Running loss of epoch-46 batch-134 = 2.19766516238451e-05

Training epoch-46 batch-135
Running loss of epoch-46 batch-135 = 7.635913789272308e-06

Training epoch-46 batch-136
Running loss of epoch-46 batch-136 = 7.994705811142921e-06

Training epoch-46 batch-137
Running loss of epoch-46 batch-137 = 9.332085028290749e-06

Training epoch-46 batch-138
Running loss of epoch-46 batch-138 = 2.1205749362707138e-05

Training epoch-46 batch-139
Running loss of epoch-46 batch-139 = 4.23286110162735e-06

Training epoch-46 batch-140
Running loss of epoch-46 batch-140 = 2.3473985493183136e-05

Training epoch-46 batch-141
Running loss of epoch-46 batch-141 = 1.2472039088606834e-05

Training epoch-46 batch-142
Running loss of epoch-46 batch-142 = 1.6874168068170547e-05

Training epoch-46 batch-143
Running loss of epoch-46 batch-143 = 9.343260899186134e-06

Training epoch-46 batch-144
Running loss of epoch-46 batch-144 = 1.827813684940338e-05

Training epoch-46 batch-145
Running loss of epoch-46 batch-145 = 1.0607065632939339e-05

Training epoch-46 batch-146
Running loss of epoch-46 batch-146 = 1.0704854503273964e-05

Training epoch-46 batch-147
Running loss of epoch-46 batch-147 = 1.8473481759428978e-05

Training epoch-46 batch-148
Running loss of epoch-46 batch-148 = 5.408888682723045e-06

Training epoch-46 batch-149
Running loss of epoch-46 batch-149 = 9.563984349370003e-06

Training epoch-46 batch-150
Running loss of epoch-46 batch-150 = 2.096151001751423e-05

Training epoch-46 batch-151
Running loss of epoch-46 batch-151 = 1.8490711227059364e-05

Training epoch-46 batch-152
Running loss of epoch-46 batch-152 = 1.674797385931015e-05

Training epoch-46 batch-153
Running loss of epoch-46 batch-153 = 6.118090823292732e-06

Training epoch-46 batch-154
Running loss of epoch-46 batch-154 = 6.091315299272537e-06

Training epoch-46 batch-155
Running loss of epoch-46 batch-155 = 1.2402655556797981e-05

Training epoch-46 batch-156
Running loss of epoch-46 batch-156 = 1.0544201359152794e-05

Training epoch-46 batch-157
Running loss of epoch-46 batch-157 = 7.586926221847534e-05

Finished training epoch-46.



Average train loss at epoch-46 = 1.200750693678856e-05

Started Evaluation

Average val loss at epoch-46 = 1.0698828392296011

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 92.30 %
Accuracy for class onCreate is: 92.00 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 65.53 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 72.82 %

Overall Accuracy = 83.74 %

Finished Evaluation



Started training epoch-47


Training epoch-47 batch-1
Running loss of epoch-47 batch-1 = 6.682705134153366e-06

Training epoch-47 batch-2
Running loss of epoch-47 batch-2 = 7.486436516046524e-06

Training epoch-47 batch-3
Running loss of epoch-47 batch-3 = 5.6228600442409515e-06

Training epoch-47 batch-4
Running loss of epoch-47 batch-4 = 6.600283086299896e-06

Training epoch-47 batch-5
Running loss of epoch-47 batch-5 = 7.185852155089378e-06

Training epoch-47 batch-6
Running loss of epoch-47 batch-6 = 6.70459121465683e-06

Training epoch-47 batch-7
Running loss of epoch-47 batch-7 = 1.1506257578730583e-05

Training epoch-47 batch-8
Running loss of epoch-47 batch-8 = 7.579335942864418e-06

Training epoch-47 batch-9
Running loss of epoch-47 batch-9 = 1.0522548109292984e-05

Training epoch-47 batch-10
Running loss of epoch-47 batch-10 = 7.448485121130943e-06

Training epoch-47 batch-11
Running loss of epoch-47 batch-11 = 3.983732312917709e-06

Training epoch-47 batch-12
Running loss of epoch-47 batch-12 = 1.2023840099573135e-05

Training epoch-47 batch-13
Running loss of epoch-47 batch-13 = 8.807051926851273e-06

Training epoch-47 batch-14
Running loss of epoch-47 batch-14 = 6.8410299718379974e-06

Training epoch-47 batch-15
Running loss of epoch-47 batch-15 = 1.3070879504084587e-05

Training epoch-47 batch-16
Running loss of epoch-47 batch-16 = 4.1048042476177216e-06

Training epoch-47 batch-17
Running loss of epoch-47 batch-17 = 7.824506610631943e-06

Training epoch-47 batch-18
Running loss of epoch-47 batch-18 = 1.135747879743576e-05

Training epoch-47 batch-19
Running loss of epoch-47 batch-19 = 1.109810546040535e-05

Training epoch-47 batch-20
Running loss of epoch-47 batch-20 = 1.5684636309742928e-05

Training epoch-47 batch-21
Running loss of epoch-47 batch-21 = 1.8354272469878197e-05

Training epoch-47 batch-22
Running loss of epoch-47 batch-22 = 1.899711787700653e-05

Training epoch-47 batch-23
Running loss of epoch-47 batch-23 = 6.0764141380786896e-06

Training epoch-47 batch-24
Running loss of epoch-47 batch-24 = 8.174451068043709e-06

Training epoch-47 batch-25
Running loss of epoch-47 batch-25 = 8.50064679980278e-06

Training epoch-47 batch-26
Running loss of epoch-47 batch-26 = 1.0050833225250244e-05

Training epoch-47 batch-27
Running loss of epoch-47 batch-27 = 7.843831554055214e-06

Training epoch-47 batch-28
Running loss of epoch-47 batch-28 = 8.970731869339943e-06

Training epoch-47 batch-29
Running loss of epoch-47 batch-29 = 7.092021405696869e-06

Training epoch-47 batch-30
Running loss of epoch-47 batch-30 = 7.876195013523102e-06

Training epoch-47 batch-31
Running loss of epoch-47 batch-31 = 9.848270565271378e-06

Training epoch-47 batch-32
Running loss of epoch-47 batch-32 = 1.872819848358631e-05

Training epoch-47 batch-33
Running loss of epoch-47 batch-33 = 2.291938289999962e-05

Training epoch-47 batch-34
Running loss of epoch-47 batch-34 = 1.76082830876112e-05

Training epoch-47 batch-35
Running loss of epoch-47 batch-35 = 1.1683441698551178e-05

Training epoch-47 batch-36
Running loss of epoch-47 batch-36 = 9.287381544709206e-06

Training epoch-47 batch-37
Running loss of epoch-47 batch-37 = 5.785608664155006e-06

Training epoch-47 batch-38
Running loss of epoch-47 batch-38 = 1.3334909453988075e-05

Training epoch-47 batch-39
Running loss of epoch-47 batch-39 = 8.14627856016159e-06

Training epoch-47 batch-40
Running loss of epoch-47 batch-40 = 4.600035026669502e-06

Training epoch-47 batch-41
Running loss of epoch-47 batch-41 = 5.258945748209953e-06

Training epoch-47 batch-42
Running loss of epoch-47 batch-42 = 8.4687490016222e-06

Training epoch-47 batch-43
Running loss of epoch-47 batch-43 = 8.29971395432949e-06

Training epoch-47 batch-44
Running loss of epoch-47 batch-44 = 8.913455531001091e-06

Training epoch-47 batch-45
Running loss of epoch-47 batch-45 = 1.139054074883461e-05

Training epoch-47 batch-46
Running loss of epoch-47 batch-46 = 5.588168278336525e-06

Training epoch-47 batch-47
Running loss of epoch-47 batch-47 = 9.018229320645332e-06

Training epoch-47 batch-48
Running loss of epoch-47 batch-48 = 7.682014256715775e-06

Training epoch-47 batch-49
Running loss of epoch-47 batch-49 = 2.7478672564029694e-06

Training epoch-47 batch-50
Running loss of epoch-47 batch-50 = 1.3317447155714035e-05

Training epoch-47 batch-51
Running loss of epoch-47 batch-51 = 2.191285602748394e-05

Training epoch-47 batch-52
Running loss of epoch-47 batch-52 = 5.697133019566536e-06

Training epoch-47 batch-53
Running loss of epoch-47 batch-53 = 9.357230737805367e-06

Training epoch-47 batch-54
Running loss of epoch-47 batch-54 = 1.538568176329136e-05

Training epoch-47 batch-55
Running loss of epoch-47 batch-55 = 1.0377494618296623e-05

Training epoch-47 batch-56
Running loss of epoch-47 batch-56 = 1.108366996049881e-05

Training epoch-47 batch-57
Running loss of epoch-47 batch-57 = 7.4747949838638306e-06

Training epoch-47 batch-58
Running loss of epoch-47 batch-58 = 1.3023149222135544e-05

Training epoch-47 batch-59
Running loss of epoch-47 batch-59 = 1.0958872735500336e-05

Training epoch-47 batch-60
Running loss of epoch-47 batch-60 = 1.1767027899622917e-05

Training epoch-47 batch-61
Running loss of epoch-47 batch-61 = 6.498303264379501e-06

Training epoch-47 batch-62
Running loss of epoch-47 batch-62 = 6.3327606767416e-06

Training epoch-47 batch-63
Running loss of epoch-47 batch-63 = 9.678071364760399e-06

Training epoch-47 batch-64
Running loss of epoch-47 batch-64 = 4.7923531383275986e-06

Training epoch-47 batch-65
Running loss of epoch-47 batch-65 = 1.5770085155963898e-05

Training epoch-47 batch-66
Running loss of epoch-47 batch-66 = 1.5299301594495773e-05

Training epoch-47 batch-67
Running loss of epoch-47 batch-67 = 1.107761636376381e-05

Training epoch-47 batch-68
Running loss of epoch-47 batch-68 = 1.2019649147987366e-05

Training epoch-47 batch-69
Running loss of epoch-47 batch-69 = 2.1835556253790855e-05

Training epoch-47 batch-70
Running loss of epoch-47 batch-70 = 6.817048415541649e-06

Training epoch-47 batch-71
Running loss of epoch-47 batch-71 = 1.0751653462648392e-05

Training epoch-47 batch-72
Running loss of epoch-47 batch-72 = 8.331378921866417e-06

Training epoch-47 batch-73
Running loss of epoch-47 batch-73 = 3.567896783351898e-06

Training epoch-47 batch-74
Running loss of epoch-47 batch-74 = 1.5335623174905777e-05

Training epoch-47 batch-75
Running loss of epoch-47 batch-75 = 1.3272743672132492e-05

Training epoch-47 batch-76
Running loss of epoch-47 batch-76 = 1.766253262758255e-05

Training epoch-47 batch-77
Running loss of epoch-47 batch-77 = 1.876743044704199e-05

Training epoch-47 batch-78
Running loss of epoch-47 batch-78 = 5.457084625959396e-06

Training epoch-47 batch-79
Running loss of epoch-47 batch-79 = 1.0839197784662247e-05

Training epoch-47 batch-80
Running loss of epoch-47 batch-80 = 1.2615695595741272e-05

Training epoch-47 batch-81
Running loss of epoch-47 batch-81 = 9.009148925542831e-06

Training epoch-47 batch-82
Running loss of epoch-47 batch-82 = 1.679244451224804e-05

Training epoch-47 batch-83
Running loss of epoch-47 batch-83 = 1.3944925740361214e-05

Training epoch-47 batch-84
Running loss of epoch-47 batch-84 = 2.14290339499712e-05

Training epoch-47 batch-85
Running loss of epoch-47 batch-85 = 4.875240847468376e-06

Training epoch-47 batch-86
Running loss of epoch-47 batch-86 = 9.613577276468277e-06

Training epoch-47 batch-87
Running loss of epoch-47 batch-87 = 7.769325748085976e-06

Training epoch-47 batch-88
Running loss of epoch-47 batch-88 = 1.255120150744915e-05

Training epoch-47 batch-89
Running loss of epoch-47 batch-89 = 1.1552125215530396e-05

Training epoch-47 batch-90
Running loss of epoch-47 batch-90 = 8.609378710389137e-06

Training epoch-47 batch-91
Running loss of epoch-47 batch-91 = 7.2999391704797745e-06

Training epoch-47 batch-92
Running loss of epoch-47 batch-92 = 1.9053928554058075e-05

Training epoch-47 batch-93
Running loss of epoch-47 batch-93 = 7.327180355787277e-06

Training epoch-47 batch-94
Running loss of epoch-47 batch-94 = 2.2673746570944786e-05

Training epoch-47 batch-95
Running loss of epoch-47 batch-95 = 2.0393868908286095e-05

Training epoch-47 batch-96
Running loss of epoch-47 batch-96 = 1.6249483451247215e-05

Training epoch-47 batch-97
Running loss of epoch-47 batch-97 = 6.8196095526218414e-06

Training epoch-47 batch-98
Running loss of epoch-47 batch-98 = 6.583519279956818e-06

Training epoch-47 batch-99
Running loss of epoch-47 batch-99 = 8.195405825972557e-06

Training epoch-47 batch-100
Running loss of epoch-47 batch-100 = 6.591202691197395e-06

Training epoch-47 batch-101
Running loss of epoch-47 batch-101 = 6.5818894654512405e-06

Training epoch-47 batch-102
Running loss of epoch-47 batch-102 = 1.2454111129045486e-05

Training epoch-47 batch-103
Running loss of epoch-47 batch-103 = 1.1381926015019417e-05

Training epoch-47 batch-104
Running loss of epoch-47 batch-104 = 1.1547701433300972e-05

Training epoch-47 batch-105
Running loss of epoch-47 batch-105 = 3.868713974952698e-06

Training epoch-47 batch-106
Running loss of epoch-47 batch-106 = 1.2646429240703583e-05

Training epoch-47 batch-107
Running loss of epoch-47 batch-107 = 1.0961899533867836e-05

Training epoch-47 batch-108
Running loss of epoch-47 batch-108 = 7.143709808588028e-06

Training epoch-47 batch-109
Running loss of epoch-47 batch-109 = 8.753035217523575e-06

Training epoch-47 batch-110
Running loss of epoch-47 batch-110 = 2.961023710668087e-05

Training epoch-47 batch-111
Running loss of epoch-47 batch-111 = 6.443588063120842e-06

Training epoch-47 batch-112
Running loss of epoch-47 batch-112 = 4.0015438571572304e-05

Training epoch-47 batch-113
Running loss of epoch-47 batch-113 = 7.86525197327137e-06

Training epoch-47 batch-114
Running loss of epoch-47 batch-114 = 8.519506081938744e-06

Training epoch-47 batch-115
Running loss of epoch-47 batch-115 = 1.2492062523961067e-05

Training epoch-47 batch-116
Running loss of epoch-47 batch-116 = 1.8880004063248634e-05

Training epoch-47 batch-117
Running loss of epoch-47 batch-117 = 1.41938216984272e-05

Training epoch-47 batch-118
Running loss of epoch-47 batch-118 = 2.156081609427929e-05

Training epoch-47 batch-119
Running loss of epoch-47 batch-119 = 1.5478814020752907e-05

Training epoch-47 batch-120
Running loss of epoch-47 batch-120 = 1.2152362614870071e-05

Training epoch-47 batch-121
Running loss of epoch-47 batch-121 = 1.8588732928037643e-05

Training epoch-47 batch-122
Running loss of epoch-47 batch-122 = 1.0749557986855507e-05

Training epoch-47 batch-123
Running loss of epoch-47 batch-123 = 7.627066224813461e-06

Training epoch-47 batch-124
Running loss of epoch-47 batch-124 = 9.609851986169815e-06

Training epoch-47 batch-125
Running loss of epoch-47 batch-125 = 9.560957551002502e-06

Training epoch-47 batch-126
Running loss of epoch-47 batch-126 = 1.5000579878687859e-05

Training epoch-47 batch-127
Running loss of epoch-47 batch-127 = 1.4406396076083183e-05

Training epoch-47 batch-128
Running loss of epoch-47 batch-128 = 1.4835735782980919e-05

Training epoch-47 batch-129
Running loss of epoch-47 batch-129 = 7.594237104058266e-06

Training epoch-47 batch-130
Running loss of epoch-47 batch-130 = 1.04543287307024e-05

Training epoch-47 batch-131
Running loss of epoch-47 batch-131 = 7.316702976822853e-06

Training epoch-47 batch-132
Running loss of epoch-47 batch-132 = 6.514601409435272e-06

Training epoch-47 batch-133
Running loss of epoch-47 batch-133 = 1.4463439583778381e-05

Training epoch-47 batch-134
Running loss of epoch-47 batch-134 = 2.1249987185001373e-05

Training epoch-47 batch-135
Running loss of epoch-47 batch-135 = 7.366528734564781e-06

Training epoch-47 batch-136
Running loss of epoch-47 batch-136 = 1.1200085282325745e-05

Training epoch-47 batch-137
Running loss of epoch-47 batch-137 = 8.0836471170187e-06

Training epoch-47 batch-138
Running loss of epoch-47 batch-138 = 8.374219760298729e-06

Training epoch-47 batch-139
Running loss of epoch-47 batch-139 = 3.185588866472244e-05

Training epoch-47 batch-140
Running loss of epoch-47 batch-140 = 2.6150140911340714e-05

Training epoch-47 batch-141
Running loss of epoch-47 batch-141 = 8.702278137207031e-06

Training epoch-47 batch-142
Running loss of epoch-47 batch-142 = 8.688773959875107e-06

Training epoch-47 batch-143
Running loss of epoch-47 batch-143 = 5.696667358279228e-06

Training epoch-47 batch-144
Running loss of epoch-47 batch-144 = 3.857770934700966e-06

Training epoch-47 batch-145
Running loss of epoch-47 batch-145 = 6.449408829212189e-06

Training epoch-47 batch-146
Running loss of epoch-47 batch-146 = 3.012223169207573e-05

Training epoch-47 batch-147
Running loss of epoch-47 batch-147 = 5.518551915884018e-06

Training epoch-47 batch-148
Running loss of epoch-47 batch-148 = 3.689667209982872e-06

Training epoch-47 batch-149
Running loss of epoch-47 batch-149 = 1.2082746252417564e-05

Training epoch-47 batch-150
Running loss of epoch-47 batch-150 = 7.299007847905159e-06

Training epoch-47 batch-151
Running loss of epoch-47 batch-151 = 2.2824620828032494e-05

Training epoch-47 batch-152
Running loss of epoch-47 batch-152 = 1.840014010667801e-05

Training epoch-47 batch-153
Running loss of epoch-47 batch-153 = 1.3521639630198479e-05

Training epoch-47 batch-154
Running loss of epoch-47 batch-154 = 1.311139203608036e-05

Training epoch-47 batch-155
Running loss of epoch-47 batch-155 = 8.506234735250473e-06

Training epoch-47 batch-156
Running loss of epoch-47 batch-156 = 1.1285766959190369e-05

Training epoch-47 batch-157
Running loss of epoch-47 batch-157 = 2.1576881408691406e-05

Finished training epoch-47.



Average train loss at epoch-47 = 1.136958822607994e-05

Started Evaluation

Average val loss at epoch-47 = 1.080584785333952

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 92.22 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 66.67 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.93 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 83.54 %

Finished Evaluation



Started training epoch-48


Training epoch-48 batch-1
Running loss of epoch-48 batch-1 = 8.096452802419662e-06

Training epoch-48 batch-2
Running loss of epoch-48 batch-2 = 1.4840392395853996e-05

Training epoch-48 batch-3
Running loss of epoch-48 batch-3 = 8.05896706879139e-06

Training epoch-48 batch-4
Running loss of epoch-48 batch-4 = 1.3699289411306381e-05

Training epoch-48 batch-5
Running loss of epoch-48 batch-5 = 1.0761432349681854e-05

Training epoch-48 batch-6
Running loss of epoch-48 batch-6 = 6.885034963488579e-06

Training epoch-48 batch-7
Running loss of epoch-48 batch-7 = 1.5260884538292885e-05

Training epoch-48 batch-8
Running loss of epoch-48 batch-8 = 1.0923482477664948e-05

Training epoch-48 batch-9
Running loss of epoch-48 batch-9 = 5.050329491496086e-06

Training epoch-48 batch-10
Running loss of epoch-48 batch-10 = 1.019984483718872e-05

Training epoch-48 batch-11
Running loss of epoch-48 batch-11 = 2.109748311340809e-05

Training epoch-48 batch-12
Running loss of epoch-48 batch-12 = 8.665258064866066e-06

Training epoch-48 batch-13
Running loss of epoch-48 batch-13 = 1.1851312592625618e-05

Training epoch-48 batch-14
Running loss of epoch-48 batch-14 = 9.283656254410744e-06

Training epoch-48 batch-15
Running loss of epoch-48 batch-15 = 7.73346982896328e-06

Training epoch-48 batch-16
Running loss of epoch-48 batch-16 = 1.6928883269429207e-05

Training epoch-48 batch-17
Running loss of epoch-48 batch-17 = 1.0499963536858559e-05

Training epoch-48 batch-18
Running loss of epoch-48 batch-18 = 7.551396265625954e-06

Training epoch-48 batch-19
Running loss of epoch-48 batch-19 = 5.491077899932861e-06

Training epoch-48 batch-20
Running loss of epoch-48 batch-20 = 1.5943776816129684e-05

Training epoch-48 batch-21
Running loss of epoch-48 batch-21 = 1.973542384803295e-05

Training epoch-48 batch-22
Running loss of epoch-48 batch-22 = 1.7955200746655464e-05

Training epoch-48 batch-23
Running loss of epoch-48 batch-23 = 1.2327218428254128e-05

Training epoch-48 batch-24
Running loss of epoch-48 batch-24 = 9.648269042372704e-06

Training epoch-48 batch-25
Running loss of epoch-48 batch-25 = 4.01446595788002e-06

Training epoch-48 batch-26
Running loss of epoch-48 batch-26 = 1.1631753295660019e-05

Training epoch-48 batch-27
Running loss of epoch-48 batch-27 = 2.1165702491998672e-05

Training epoch-48 batch-28
Running loss of epoch-48 batch-28 = 5.379319190979004e-06

Training epoch-48 batch-29
Running loss of epoch-48 batch-29 = 1.1817086488008499e-05

Training epoch-48 batch-30
Running loss of epoch-48 batch-30 = 1.2603355571627617e-05

Training epoch-48 batch-31
Running loss of epoch-48 batch-31 = 6.857793778181076e-06

Training epoch-48 batch-32
Running loss of epoch-48 batch-32 = 1.6728881746530533e-05

Training epoch-48 batch-33
Running loss of epoch-48 batch-33 = 1.3512326404452324e-05

Training epoch-48 batch-34
Running loss of epoch-48 batch-34 = 8.045230060815811e-06

Training epoch-48 batch-35
Running loss of epoch-48 batch-35 = 1.1343508958816528e-05

Training epoch-48 batch-36
Running loss of epoch-48 batch-36 = 4.442175850272179e-06

Training epoch-48 batch-37
Running loss of epoch-48 batch-37 = 1.0442454367876053e-05

Training epoch-48 batch-38
Running loss of epoch-48 batch-38 = 1.073232851922512e-05

Training epoch-48 batch-39
Running loss of epoch-48 batch-39 = 5.696667358279228e-06

Training epoch-48 batch-40
Running loss of epoch-48 batch-40 = 6.999587640166283e-06

Training epoch-48 batch-41
Running loss of epoch-48 batch-41 = 1.3805227354168892e-05

Training epoch-48 batch-42
Running loss of epoch-48 batch-42 = 1.2049917131662369e-05

Training epoch-48 batch-43
Running loss of epoch-48 batch-43 = 1.3532349839806557e-05

Training epoch-48 batch-44
Running loss of epoch-48 batch-44 = 3.95788811147213e-06

Training epoch-48 batch-45
Running loss of epoch-48 batch-45 = 1.896405592560768e-05

Training epoch-48 batch-46
Running loss of epoch-48 batch-46 = 8.781673386693e-06

Training epoch-48 batch-47
Running loss of epoch-48 batch-47 = 9.337905794382095e-06

Training epoch-48 batch-48
Running loss of epoch-48 batch-48 = 2.8347130864858627e-06

Training epoch-48 batch-49
Running loss of epoch-48 batch-49 = 1.5662051737308502e-05

Training epoch-48 batch-50
Running loss of epoch-48 batch-50 = 1.0313699021935463e-05

Training epoch-48 batch-51
Running loss of epoch-48 batch-51 = 5.096662789583206e-06

Training epoch-48 batch-52
Running loss of epoch-48 batch-52 = 9.291572496294975e-06

Training epoch-48 batch-53
Running loss of epoch-48 batch-53 = 1.891469582915306e-05

Training epoch-48 batch-54
Running loss of epoch-48 batch-54 = 1.4810822904109955e-05

Training epoch-48 batch-55
Running loss of epoch-48 batch-55 = 5.458248779177666e-06

Training epoch-48 batch-56
Running loss of epoch-48 batch-56 = 2.447841688990593e-05

Training epoch-48 batch-57
Running loss of epoch-48 batch-57 = 6.482237949967384e-06

Training epoch-48 batch-58
Running loss of epoch-48 batch-58 = 9.986339136958122e-06

Training epoch-48 batch-59
Running loss of epoch-48 batch-59 = 5.545793101191521e-06

Training epoch-48 batch-60
Running loss of epoch-48 batch-60 = 1.3350741937756538e-05

Training epoch-48 batch-61
Running loss of epoch-48 batch-61 = 1.3840384781360626e-05

Training epoch-48 batch-62
Running loss of epoch-48 batch-62 = 6.909715011715889e-06

Training epoch-48 batch-63
Running loss of epoch-48 batch-63 = 1.6231322661042213e-05

Training epoch-48 batch-64
Running loss of epoch-48 batch-64 = 1.0487623512744904e-05

Training epoch-48 batch-65
Running loss of epoch-48 batch-65 = 5.688285455107689e-06

Training epoch-48 batch-66
Running loss of epoch-48 batch-66 = 1.3071810826659203e-05

Training epoch-48 batch-67
Running loss of epoch-48 batch-67 = 1.0065734386444092e-05

Training epoch-48 batch-68
Running loss of epoch-48 batch-68 = 1.4297431334853172e-05

Training epoch-48 batch-69
Running loss of epoch-48 batch-69 = 7.213326171040535e-06

Training epoch-48 batch-70
Running loss of epoch-48 batch-70 = 7.54394568502903e-06

Training epoch-48 batch-71
Running loss of epoch-48 batch-71 = 1.9738683477044106e-05

Training epoch-48 batch-72
Running loss of epoch-48 batch-72 = 1.099251676350832e-05

Training epoch-48 batch-73
Running loss of epoch-48 batch-73 = 1.2408127076923847e-05

Training epoch-48 batch-74
Running loss of epoch-48 batch-74 = 1.906859688460827e-05

Training epoch-48 batch-75
Running loss of epoch-48 batch-75 = 1.808907836675644e-05

Training epoch-48 batch-76
Running loss of epoch-48 batch-76 = 8.465489372611046e-06

Training epoch-48 batch-77
Running loss of epoch-48 batch-77 = 6.709713488817215e-06

Training epoch-48 batch-78
Running loss of epoch-48 batch-78 = 1.3447832316160202e-05

Training epoch-48 batch-79
Running loss of epoch-48 batch-79 = 1.353491097688675e-05

Training epoch-48 batch-80
Running loss of epoch-48 batch-80 = 8.378876373171806e-06

Training epoch-48 batch-81
Running loss of epoch-48 batch-81 = 1.0217307135462761e-05

Training epoch-48 batch-82
Running loss of epoch-48 batch-82 = 1.9118189811706543e-05

Training epoch-48 batch-83
Running loss of epoch-48 batch-83 = 1.1867145076394081e-05

Training epoch-48 batch-84
Running loss of epoch-48 batch-84 = 7.631024345755577e-06

Training epoch-48 batch-85
Running loss of epoch-48 batch-85 = 9.323237463831902e-06

Training epoch-48 batch-86
Running loss of epoch-48 batch-86 = 1.7773360013961792e-05

Training epoch-48 batch-87
Running loss of epoch-48 batch-87 = 1.5293480828404427e-05

Training epoch-48 batch-88
Running loss of epoch-48 batch-88 = 1.67118851095438e-05

Training epoch-48 batch-89
Running loss of epoch-48 batch-89 = 8.078757673501968e-06

Training epoch-48 batch-90
Running loss of epoch-48 batch-90 = 1.4795688912272453e-05

Training epoch-48 batch-91
Running loss of epoch-48 batch-91 = 1.7450889572501183e-05

Training epoch-48 batch-92
Running loss of epoch-48 batch-92 = 5.859881639480591e-06

Training epoch-48 batch-93
Running loss of epoch-48 batch-93 = 9.54815186560154e-06

Training epoch-48 batch-94
Running loss of epoch-48 batch-94 = 9.193317964673042e-06

Training epoch-48 batch-95
Running loss of epoch-48 batch-95 = 3.934605047106743e-06

Training epoch-48 batch-96
Running loss of epoch-48 batch-96 = 6.827292963862419e-06

Training epoch-48 batch-97
Running loss of epoch-48 batch-97 = 7.326481863856316e-06

Training epoch-48 batch-98
Running loss of epoch-48 batch-98 = 1.5387777239084244e-05

Training epoch-48 batch-99
Running loss of epoch-48 batch-99 = 6.440095603466034e-06

Training epoch-48 batch-100
Running loss of epoch-48 batch-100 = 2.8756912797689438e-06

Training epoch-48 batch-101
Running loss of epoch-48 batch-101 = 9.998911991715431e-06

Training epoch-48 batch-102
Running loss of epoch-48 batch-102 = 1.0515563189983368e-05

Training epoch-48 batch-103
Running loss of epoch-48 batch-103 = 3.5848934203386307e-06

Training epoch-48 batch-104
Running loss of epoch-48 batch-104 = 1.0274117812514305e-05

Training epoch-48 batch-105
Running loss of epoch-48 batch-105 = 1.0234769433736801e-05

Training epoch-48 batch-106
Running loss of epoch-48 batch-106 = 6.005633622407913e-06

Training epoch-48 batch-107
Running loss of epoch-48 batch-107 = 3.4016557037830353e-06

Training epoch-48 batch-108
Running loss of epoch-48 batch-108 = 1.2473436072468758e-05

Training epoch-48 batch-109
Running loss of epoch-48 batch-109 = 1.5178462490439415e-05

Training epoch-48 batch-110
Running loss of epoch-48 batch-110 = 1.1792872101068497e-05

Training epoch-48 batch-111
Running loss of epoch-48 batch-111 = 7.607974112033844e-06

Training epoch-48 batch-112
Running loss of epoch-48 batch-112 = 4.5280903577804565e-06

Training epoch-48 batch-113
Running loss of epoch-48 batch-113 = 9.70880500972271e-06

Training epoch-48 batch-114
Running loss of epoch-48 batch-114 = 8.700648322701454e-06

Training epoch-48 batch-115
Running loss of epoch-48 batch-115 = 1.1041061952710152e-05

Training epoch-48 batch-116
Running loss of epoch-48 batch-116 = 5.175359547138214e-06

Training epoch-48 batch-117
Running loss of epoch-48 batch-117 = 9.152805432677269e-06

Training epoch-48 batch-118
Running loss of epoch-48 batch-118 = 8.789123967289925e-06

Training epoch-48 batch-119
Running loss of epoch-48 batch-119 = 5.105277523398399e-06

Training epoch-48 batch-120
Running loss of epoch-48 batch-120 = 9.159324690699577e-06

Training epoch-48 batch-121
Running loss of epoch-48 batch-121 = 1.637917011976242e-05

Training epoch-48 batch-122
Running loss of epoch-48 batch-122 = 1.6377540305256844e-05

Training epoch-48 batch-123
Running loss of epoch-48 batch-123 = 1.032976433634758e-05

Training epoch-48 batch-124
Running loss of epoch-48 batch-124 = 3.187241964042187e-05

Training epoch-48 batch-125
Running loss of epoch-48 batch-125 = 7.983529940247536e-06

Training epoch-48 batch-126
Running loss of epoch-48 batch-126 = 8.491799235343933e-06

Training epoch-48 batch-127
Running loss of epoch-48 batch-127 = 1.2594973668456078e-05

Training epoch-48 batch-128
Running loss of epoch-48 batch-128 = 9.255250915884972e-06

Training epoch-48 batch-129
Running loss of epoch-48 batch-129 = 1.4040619134902954e-05

Training epoch-48 batch-130
Running loss of epoch-48 batch-130 = 7.1052927523851395e-06

Training epoch-48 batch-131
Running loss of epoch-48 batch-131 = 1.2545380741357803e-05

Training epoch-48 batch-132
Running loss of epoch-48 batch-132 = 7.1569811552762985e-06

Training epoch-48 batch-133
Running loss of epoch-48 batch-133 = 1.3294746167957783e-05

Training epoch-48 batch-134
Running loss of epoch-48 batch-134 = 8.133938536047935e-06

Training epoch-48 batch-135
Running loss of epoch-48 batch-135 = 7.513677701354027e-06

Training epoch-48 batch-136
Running loss of epoch-48 batch-136 = 7.828231900930405e-06

Training epoch-48 batch-137
Running loss of epoch-48 batch-137 = 7.538124918937683e-06

Training epoch-48 batch-138
Running loss of epoch-48 batch-138 = 1.6952166333794594e-05

Training epoch-48 batch-139
Running loss of epoch-48 batch-139 = 7.347436621785164e-06

Training epoch-48 batch-140
Running loss of epoch-48 batch-140 = 1.5561236068606377e-05

Training epoch-48 batch-141
Running loss of epoch-48 batch-141 = 1.9113300368189812e-05

Training epoch-48 batch-142
Running loss of epoch-48 batch-142 = 6.922055035829544e-06

Training epoch-48 batch-143
Running loss of epoch-48 batch-143 = 7.547670975327492e-06

Training epoch-48 batch-144
Running loss of epoch-48 batch-144 = 1.0227784514427185e-05

Training epoch-48 batch-145
Running loss of epoch-48 batch-145 = 1.7012236639857292e-05

Training epoch-48 batch-146
Running loss of epoch-48 batch-146 = 4.457542672753334e-06

Training epoch-48 batch-147
Running loss of epoch-48 batch-147 = 1.0500196367502213e-05

Training epoch-48 batch-148
Running loss of epoch-48 batch-148 = 2.208258956670761e-05

Training epoch-48 batch-149
Running loss of epoch-48 batch-149 = 7.845927029848099e-06

Training epoch-48 batch-150
Running loss of epoch-48 batch-150 = 7.656868547201157e-06

Training epoch-48 batch-151
Running loss of epoch-48 batch-151 = 1.4933757483959198e-05

Training epoch-48 batch-152
Running loss of epoch-48 batch-152 = 8.26595351099968e-06

Training epoch-48 batch-153
Running loss of epoch-48 batch-153 = 1.0613817721605301e-05

Training epoch-48 batch-154
Running loss of epoch-48 batch-154 = 1.1484138667583466e-05

Training epoch-48 batch-155
Running loss of epoch-48 batch-155 = 5.909008905291557e-06

Training epoch-48 batch-156
Running loss of epoch-48 batch-156 = 7.31530599296093e-06

Training epoch-48 batch-157
Running loss of epoch-48 batch-157 = 3.965944051742554e-05

Finished training epoch-48.



Average train loss at epoch-48 = 1.0924338549375534e-05

Started Evaluation

Average val loss at epoch-48 = 1.0899404432169424

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 91.47 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 65.75 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 73.33 %

Overall Accuracy = 83.58 %

Finished Evaluation



Started training epoch-49


Training epoch-49 batch-1
Running loss of epoch-49 batch-1 = 2.7401605620980263e-05

Training epoch-49 batch-2
Running loss of epoch-49 batch-2 = 3.923662006855011e-06

Training epoch-49 batch-3
Running loss of epoch-49 batch-3 = 6.4747873693704605e-06

Training epoch-49 batch-4
Running loss of epoch-49 batch-4 = 9.671784937381744e-06

Training epoch-49 batch-5
Running loss of epoch-49 batch-5 = 1.3570766896009445e-05

Training epoch-49 batch-6
Running loss of epoch-49 batch-6 = 1.890026032924652e-05

Training epoch-49 batch-7
Running loss of epoch-49 batch-7 = 8.876202628016472e-06

Training epoch-49 batch-8
Running loss of epoch-49 batch-8 = 9.103678166866302e-06

Training epoch-49 batch-9
Running loss of epoch-49 batch-9 = 2.62304674834013e-05

Training epoch-49 batch-10
Running loss of epoch-49 batch-10 = 8.223112672567368e-06

Training epoch-49 batch-11
Running loss of epoch-49 batch-11 = 1.324433833360672e-05

Training epoch-49 batch-12
Running loss of epoch-49 batch-12 = 2.895016223192215e-06

Training epoch-49 batch-13
Running loss of epoch-49 batch-13 = 1.5046447515487671e-05

Training epoch-49 batch-14
Running loss of epoch-49 batch-14 = 9.061768651008606e-06

Training epoch-49 batch-15
Running loss of epoch-49 batch-15 = 1.3030599802732468e-05

Training epoch-49 batch-16
Running loss of epoch-49 batch-16 = 1.0329298675060272e-05

Training epoch-49 batch-17
Running loss of epoch-49 batch-17 = 1.1835945770144463e-05

Training epoch-49 batch-18
Running loss of epoch-49 batch-18 = 3.310851752758026e-06

Training epoch-49 batch-19
Running loss of epoch-49 batch-19 = 4.027504473924637e-06

Training epoch-49 batch-20
Running loss of epoch-49 batch-20 = 5.933456122875214e-06

Training epoch-49 batch-21
Running loss of epoch-49 batch-21 = 6.909947842359543e-06

Training epoch-49 batch-22
Running loss of epoch-49 batch-22 = 6.716931238770485e-06

Training epoch-49 batch-23
Running loss of epoch-49 batch-23 = 1.696520484983921e-05

Training epoch-49 batch-24
Running loss of epoch-49 batch-24 = 1.4261109754443169e-05

Training epoch-49 batch-25
Running loss of epoch-49 batch-25 = 5.491776391863823e-06

Training epoch-49 batch-26
Running loss of epoch-49 batch-26 = 8.259899914264679e-06

Training epoch-49 batch-27
Running loss of epoch-49 batch-27 = 2.071610651910305e-05

Training epoch-49 batch-28
Running loss of epoch-49 batch-28 = 6.398884579539299e-06

Training epoch-49 batch-29
Running loss of epoch-49 batch-29 = 1.1695083230733871e-05

Training epoch-49 batch-30
Running loss of epoch-49 batch-30 = 9.76957380771637e-06

Training epoch-49 batch-31
Running loss of epoch-49 batch-31 = 1.0715797543525696e-05

Training epoch-49 batch-32
Running loss of epoch-49 batch-32 = 8.560949936509132e-06

Training epoch-49 batch-33
Running loss of epoch-49 batch-33 = 6.366986781358719e-06

Training epoch-49 batch-34
Running loss of epoch-49 batch-34 = 1.1858530342578888e-05

Training epoch-49 batch-35
Running loss of epoch-49 batch-35 = 7.10715539753437e-06

Training epoch-49 batch-36
Running loss of epoch-49 batch-36 = 9.224982932209969e-06

Training epoch-49 batch-37
Running loss of epoch-49 batch-37 = 8.909497410058975e-06

Training epoch-49 batch-38
Running loss of epoch-49 batch-38 = 6.611458957195282e-06

Training epoch-49 batch-39
Running loss of epoch-49 batch-39 = 3.6857323721051216e-05

Training epoch-49 batch-40
Running loss of epoch-49 batch-40 = 2.0361971110105515e-05

Training epoch-49 batch-41
Running loss of epoch-49 batch-41 = 6.7534856498241425e-06

Training epoch-49 batch-42
Running loss of epoch-49 batch-42 = 8.652918040752411e-06

Training epoch-49 batch-43
Running loss of epoch-49 batch-43 = 1.29633117467165e-05

Training epoch-49 batch-44
Running loss of epoch-49 batch-44 = 7.307389751076698e-06

Training epoch-49 batch-45
Running loss of epoch-49 batch-45 = 7.460825145244598e-06

Training epoch-49 batch-46
Running loss of epoch-49 batch-46 = 8.068745955824852e-06

Training epoch-49 batch-47
Running loss of epoch-49 batch-47 = 9.422656148672104e-06

Training epoch-49 batch-48
Running loss of epoch-49 batch-48 = 8.239876478910446e-06

Training epoch-49 batch-49
Running loss of epoch-49 batch-49 = 1.7589889466762543e-05

Training epoch-49 batch-50
Running loss of epoch-49 batch-50 = 1.627020537853241e-05

Training epoch-49 batch-51
Running loss of epoch-49 batch-51 = 8.610077202320099e-06

Training epoch-49 batch-52
Running loss of epoch-49 batch-52 = 9.146519005298615e-06

Training epoch-49 batch-53
Running loss of epoch-49 batch-53 = 1.6993144527077675e-05

Training epoch-49 batch-54
Running loss of epoch-49 batch-54 = 5.433801561594009e-06

Training epoch-49 batch-55
Running loss of epoch-49 batch-55 = 1.4152145013213158e-05

Training epoch-49 batch-56
Running loss of epoch-49 batch-56 = 1.4746095985174179e-05

Training epoch-49 batch-57
Running loss of epoch-49 batch-57 = 1.0298099368810654e-05

Training epoch-49 batch-58
Running loss of epoch-49 batch-58 = 7.761875167489052e-06

Training epoch-49 batch-59
Running loss of epoch-49 batch-59 = 1.921132206916809e-05

Training epoch-49 batch-60
Running loss of epoch-49 batch-60 = 1.1044321581721306e-05

Training epoch-49 batch-61
Running loss of epoch-49 batch-61 = 1.0358053259551525e-05

Training epoch-49 batch-62
Running loss of epoch-49 batch-62 = 1.7928658053278923e-05

Training epoch-49 batch-63
Running loss of epoch-49 batch-63 = 8.241040632128716e-06

Training epoch-49 batch-64
Running loss of epoch-49 batch-64 = 5.4354313760995865e-06

Training epoch-49 batch-65
Running loss of epoch-49 batch-65 = 1.045968383550644e-05

Training epoch-49 batch-66
Running loss of epoch-49 batch-66 = 4.5585911720991135e-06

Training epoch-49 batch-67
Running loss of epoch-49 batch-67 = 1.0582152754068375e-05

Training epoch-49 batch-68
Running loss of epoch-49 batch-68 = 2.0694220438599586e-05

Training epoch-49 batch-69
Running loss of epoch-49 batch-69 = 9.190291166305542e-06

Training epoch-49 batch-70
Running loss of epoch-49 batch-70 = 1.2843403965234756e-05

Training epoch-49 batch-71
Running loss of epoch-49 batch-71 = 6.183050572872162e-06

Training epoch-49 batch-72
Running loss of epoch-49 batch-72 = 8.4687490016222e-06

Training epoch-49 batch-73
Running loss of epoch-49 batch-73 = 5.824258551001549e-06

Training epoch-49 batch-74
Running loss of epoch-49 batch-74 = 1.1288560926914215e-05

Training epoch-49 batch-75
Running loss of epoch-49 batch-75 = 1.2816861271858215e-05

Training epoch-49 batch-76
Running loss of epoch-49 batch-76 = 3.820285201072693e-06

Training epoch-49 batch-77
Running loss of epoch-49 batch-77 = 1.1591706424951553e-05

Training epoch-49 batch-78
Running loss of epoch-49 batch-78 = 1.0022195056080818e-05

Training epoch-49 batch-79
Running loss of epoch-49 batch-79 = 9.082723408937454e-06

Training epoch-49 batch-80
Running loss of epoch-49 batch-80 = 1.0994961485266685e-05

Training epoch-49 batch-81
Running loss of epoch-49 batch-81 = 6.638001650571823e-06

Training epoch-49 batch-82
Running loss of epoch-49 batch-82 = 9.552109986543655e-06

Training epoch-49 batch-83
Running loss of epoch-49 batch-83 = 5.7872384786605835e-06

Training epoch-49 batch-84
Running loss of epoch-49 batch-84 = 8.882489055395126e-06

Training epoch-49 batch-85
Running loss of epoch-49 batch-85 = 6.274320185184479e-06

Training epoch-49 batch-86
Running loss of epoch-49 batch-86 = 1.684972085058689e-05

Training epoch-49 batch-87
Running loss of epoch-49 batch-87 = 1.295073889195919e-05

Training epoch-49 batch-88
Running loss of epoch-49 batch-88 = 7.777241989970207e-06

Training epoch-49 batch-89
Running loss of epoch-49 batch-89 = 1.0888557881116867e-05

Training epoch-49 batch-90
Running loss of epoch-49 batch-90 = 9.316951036453247e-06

Training epoch-49 batch-91
Running loss of epoch-49 batch-91 = 4.417262971401215e-06

Training epoch-49 batch-92
Running loss of epoch-49 batch-92 = 1.8257880583405495e-05

Training epoch-49 batch-93
Running loss of epoch-49 batch-93 = 1.2526055797934532e-05

Training epoch-49 batch-94
Running loss of epoch-49 batch-94 = 1.90765131264925e-05

Training epoch-49 batch-95
Running loss of epoch-49 batch-95 = 4.755798727273941e-06

Training epoch-49 batch-96
Running loss of epoch-49 batch-96 = 7.991213351488113e-06

Training epoch-49 batch-97
Running loss of epoch-49 batch-97 = 2.0314939320087433e-05

Training epoch-49 batch-98
Running loss of epoch-49 batch-98 = 4.040775820612907e-06

Training epoch-49 batch-99
Running loss of epoch-49 batch-99 = 8.710892871022224e-06

Training epoch-49 batch-100
Running loss of epoch-49 batch-100 = 1.8667196854948997e-05

Training epoch-49 batch-101
Running loss of epoch-49 batch-101 = 9.560957551002502e-06

Training epoch-49 batch-102
Running loss of epoch-49 batch-102 = 4.7371722757816315e-06

Training epoch-49 batch-103
Running loss of epoch-49 batch-103 = 4.928559064865112e-06

Training epoch-49 batch-104
Running loss of epoch-49 batch-104 = 9.628012776374817e-06

Training epoch-49 batch-105
Running loss of epoch-49 batch-105 = 1.5120254829525948e-05

Training epoch-49 batch-106
Running loss of epoch-49 batch-106 = 9.728595614433289e-06

Training epoch-49 batch-107
Running loss of epoch-49 batch-107 = 9.921379387378693e-06

Training epoch-49 batch-108
Running loss of epoch-49 batch-108 = 8.93929973244667e-06

Training epoch-49 batch-109
Running loss of epoch-49 batch-109 = 1.3872981071472168e-05

Training epoch-49 batch-110
Running loss of epoch-49 batch-110 = 8.076196536421776e-06

Training epoch-49 batch-111
Running loss of epoch-49 batch-111 = 1.1638505384325981e-05

Training epoch-49 batch-112
Running loss of epoch-49 batch-112 = 8.713919669389725e-06

Training epoch-49 batch-113
Running loss of epoch-49 batch-113 = 5.289679393172264e-06

Training epoch-49 batch-114
Running loss of epoch-49 batch-114 = 4.1478779166936874e-06

Training epoch-49 batch-115
Running loss of epoch-49 batch-115 = 1.438031904399395e-05

Training epoch-49 batch-116
Running loss of epoch-49 batch-116 = 1.1249212548136711e-05

Training epoch-49 batch-117
Running loss of epoch-49 batch-117 = 9.55536961555481e-06

Training epoch-49 batch-118
Running loss of epoch-49 batch-118 = 5.432870239019394e-06

Training epoch-49 batch-119
Running loss of epoch-49 batch-119 = 5.397247150540352e-06

Training epoch-49 batch-120
Running loss of epoch-49 batch-120 = 4.809116944670677e-06

Training epoch-49 batch-121
Running loss of epoch-49 batch-121 = 6.031477823853493e-06

Training epoch-49 batch-122
Running loss of epoch-49 batch-122 = 1.2540258467197418e-05

Training epoch-49 batch-123
Running loss of epoch-49 batch-123 = 1.096096821129322e-05

Training epoch-49 batch-124
Running loss of epoch-49 batch-124 = 6.749527528882027e-06

Training epoch-49 batch-125
Running loss of epoch-49 batch-125 = 8.60472209751606e-06

Training epoch-49 batch-126
Running loss of epoch-49 batch-126 = 8.09389166533947e-06

Training epoch-49 batch-127
Running loss of epoch-49 batch-127 = 6.722286343574524e-06

Training epoch-49 batch-128
Running loss of epoch-49 batch-128 = 8.305534720420837e-06

Training epoch-49 batch-129
Running loss of epoch-49 batch-129 = 5.608890205621719e-06

Training epoch-49 batch-130
Running loss of epoch-49 batch-130 = 6.685731932520866e-06

Training epoch-49 batch-131
Running loss of epoch-49 batch-131 = 5.399109795689583e-06

Training epoch-49 batch-132
Running loss of epoch-49 batch-132 = 1.239054836332798e-05

Training epoch-49 batch-133
Running loss of epoch-49 batch-133 = 9.130686521530151e-06

Training epoch-49 batch-134
Running loss of epoch-49 batch-134 = 1.1405441910028458e-05

Training epoch-49 batch-135
Running loss of epoch-49 batch-135 = 9.434064850211143e-06

Training epoch-49 batch-136
Running loss of epoch-49 batch-136 = 1.320638693869114e-05

Training epoch-49 batch-137
Running loss of epoch-49 batch-137 = 7.047085091471672e-06

Training epoch-49 batch-138
Running loss of epoch-49 batch-138 = 1.3142591342329979e-05

Training epoch-49 batch-139
Running loss of epoch-49 batch-139 = 5.153939127922058e-06

Training epoch-49 batch-140
Running loss of epoch-49 batch-140 = 4.608882591128349e-06

Training epoch-49 batch-141
Running loss of epoch-49 batch-141 = 1.028645783662796e-05

Training epoch-49 batch-142
Running loss of epoch-49 batch-142 = 7.557449862360954e-06

Training epoch-49 batch-143
Running loss of epoch-49 batch-143 = 4.844507202506065e-06

Training epoch-49 batch-144
Running loss of epoch-49 batch-144 = 1.2904871255159378e-05

Training epoch-49 batch-145
Running loss of epoch-49 batch-145 = 9.57469455897808e-06

Training epoch-49 batch-146
Running loss of epoch-49 batch-146 = 3.087613731622696e-05

Training epoch-49 batch-147
Running loss of epoch-49 batch-147 = 1.66336540132761e-05

Training epoch-49 batch-148
Running loss of epoch-49 batch-148 = 2.7348287403583527e-05

Training epoch-49 batch-149
Running loss of epoch-49 batch-149 = 2.8065405786037445e-06

Training epoch-49 batch-150
Running loss of epoch-49 batch-150 = 8.015427738428116e-06

Training epoch-49 batch-151
Running loss of epoch-49 batch-151 = 1.883949153125286e-05

Training epoch-49 batch-152
Running loss of epoch-49 batch-152 = 3.634486347436905e-06

Training epoch-49 batch-153
Running loss of epoch-49 batch-153 = 3.3478718250989914e-06

Training epoch-49 batch-154
Running loss of epoch-49 batch-154 = 1.1577503755688667e-05

Training epoch-49 batch-155
Running loss of epoch-49 batch-155 = 9.261071681976318e-06

Training epoch-49 batch-156
Running loss of epoch-49 batch-156 = 1.0723015293478966e-05

Training epoch-49 batch-157
Running loss of epoch-49 batch-157 = 1.7490237951278687e-05

Finished training epoch-49.



Average train loss at epoch-49 = 1.0419147461652755e-05

Started Evaluation

Average val loss at epoch-49 = 1.0932816733638662

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 91.79 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.38 %
Accuracy for class execute is: 50.60 %
Accuracy for class get is: 72.56 %

Overall Accuracy = 83.52 %

Finished Evaluation



Started training epoch-50


Training epoch-50 batch-1
Running loss of epoch-50 batch-1 = 9.232200682163239e-06

Training epoch-50 batch-2
Running loss of epoch-50 batch-2 = 6.326241418719292e-06

Training epoch-50 batch-3
Running loss of epoch-50 batch-3 = 1.4575663954019547e-05

Training epoch-50 batch-4
Running loss of epoch-50 batch-4 = 6.58608041703701e-06

Training epoch-50 batch-5
Running loss of epoch-50 batch-5 = 1.3849232345819473e-05

Training epoch-50 batch-6
Running loss of epoch-50 batch-6 = 9.639188647270203e-06

Training epoch-50 batch-7
Running loss of epoch-50 batch-7 = 7.507391273975372e-06

Training epoch-50 batch-8
Running loss of epoch-50 batch-8 = 7.977709174156189e-06

Training epoch-50 batch-9
Running loss of epoch-50 batch-9 = 3.840075805783272e-06

Training epoch-50 batch-10
Running loss of epoch-50 batch-10 = 5.9462618082761765e-06

Training epoch-50 batch-11
Running loss of epoch-50 batch-11 = 5.533453077077866e-06

Training epoch-50 batch-12
Running loss of epoch-50 batch-12 = 5.744630470871925e-06

Training epoch-50 batch-13
Running loss of epoch-50 batch-13 = 1.2219883501529694e-05

Training epoch-50 batch-14
Running loss of epoch-50 batch-14 = 9.685289114713669e-06

Training epoch-50 batch-15
Running loss of epoch-50 batch-15 = 3.0386727303266525e-06

Training epoch-50 batch-16
Running loss of epoch-50 batch-16 = 1.827860251069069e-05

Training epoch-50 batch-17
Running loss of epoch-50 batch-17 = 5.529727786779404e-06

Training epoch-50 batch-18
Running loss of epoch-50 batch-18 = 4.719477146863937e-06

Training epoch-50 batch-19
Running loss of epoch-50 batch-19 = 9.638024494051933e-06

Training epoch-50 batch-20
Running loss of epoch-50 batch-20 = 1.0220799595117569e-05

Training epoch-50 batch-21
Running loss of epoch-50 batch-21 = 1.3403128832578659e-05

Training epoch-50 batch-22
Running loss of epoch-50 batch-22 = 4.1155144572257996e-06

Training epoch-50 batch-23
Running loss of epoch-50 batch-23 = 5.027279257774353e-06

Training epoch-50 batch-24
Running loss of epoch-50 batch-24 = 6.389804184436798e-06

Training epoch-50 batch-25
Running loss of epoch-50 batch-25 = 8.084578439593315e-06

Training epoch-50 batch-26
Running loss of epoch-50 batch-26 = 6.654765456914902e-06

Training epoch-50 batch-27
Running loss of epoch-50 batch-27 = 1.2600095942616463e-05

Training epoch-50 batch-28
Running loss of epoch-50 batch-28 = 1.550675369799137e-05

Training epoch-50 batch-29
Running loss of epoch-50 batch-29 = 7.173977792263031e-06

Training epoch-50 batch-30
Running loss of epoch-50 batch-30 = 1.565949060022831e-05

Training epoch-50 batch-31
Running loss of epoch-50 batch-31 = 9.38447192311287e-06

Training epoch-50 batch-32
Running loss of epoch-50 batch-32 = 9.294599294662476e-06

Training epoch-50 batch-33
Running loss of epoch-50 batch-33 = 7.974682375788689e-06

Training epoch-50 batch-34
Running loss of epoch-50 batch-34 = 1.0541640222072601e-05

Training epoch-50 batch-35
Running loss of epoch-50 batch-35 = 1.1211493983864784e-05

Training epoch-50 batch-36
Running loss of epoch-50 batch-36 = 1.1955155059695244e-05

Training epoch-50 batch-37
Running loss of epoch-50 batch-37 = 1.743575558066368e-05

Training epoch-50 batch-38
Running loss of epoch-50 batch-38 = 5.141366273164749e-06

Training epoch-50 batch-39
Running loss of epoch-50 batch-39 = 6.509711965918541e-06

Training epoch-50 batch-40
Running loss of epoch-50 batch-40 = 9.545823559165001e-06

Training epoch-50 batch-41
Running loss of epoch-50 batch-41 = 2.4726614356040955e-06

Training epoch-50 batch-42
Running loss of epoch-50 batch-42 = 1.4076009392738342e-05

Training epoch-50 batch-43
Running loss of epoch-50 batch-43 = 1.0338611900806427e-05

Training epoch-50 batch-44
Running loss of epoch-50 batch-44 = 5.902489647269249e-06

Training epoch-50 batch-45
Running loss of epoch-50 batch-45 = 6.6836364567279816e-06

Training epoch-50 batch-46
Running loss of epoch-50 batch-46 = 9.699724614620209e-06

Training epoch-50 batch-47
Running loss of epoch-50 batch-47 = 8.415430784225464e-06

Training epoch-50 batch-48
Running loss of epoch-50 batch-48 = 1.0137911885976791e-05

Training epoch-50 batch-49
Running loss of epoch-50 batch-49 = 3.3305492252111435e-05

Training epoch-50 batch-50
Running loss of epoch-50 batch-50 = 3.918539732694626e-06

Training epoch-50 batch-51
Running loss of epoch-50 batch-51 = 5.7711731642484665e-06

Training epoch-50 batch-52
Running loss of epoch-50 batch-52 = 6.159767508506775e-06

Training epoch-50 batch-53
Running loss of epoch-50 batch-53 = 6.074551492929459e-06

Training epoch-50 batch-54
Running loss of epoch-50 batch-54 = 9.501352906227112e-06

Training epoch-50 batch-55
Running loss of epoch-50 batch-55 = 5.9711746871471405e-06

Training epoch-50 batch-56
Running loss of epoch-50 batch-56 = 1.1231284588575363e-05

Training epoch-50 batch-57
Running loss of epoch-50 batch-57 = 3.811437636613846e-06

Training epoch-50 batch-58
Running loss of epoch-50 batch-58 = 1.1080410331487656e-05

Training epoch-50 batch-59
Running loss of epoch-50 batch-59 = 2.031051553785801e-05

Training epoch-50 batch-60
Running loss of epoch-50 batch-60 = 1.2662261724472046e-05

Training epoch-50 batch-61
Running loss of epoch-50 batch-61 = 1.2349104508757591e-05

Training epoch-50 batch-62
Running loss of epoch-50 batch-62 = 1.829303801059723e-05

Training epoch-50 batch-63
Running loss of epoch-50 batch-63 = 4.902016371488571e-06

Training epoch-50 batch-64
Running loss of epoch-50 batch-64 = 9.855721145868301e-06

Training epoch-50 batch-65
Running loss of epoch-50 batch-65 = 6.426125764846802e-06

Training epoch-50 batch-66
Running loss of epoch-50 batch-66 = 3.907131031155586e-06

Training epoch-50 batch-67
Running loss of epoch-50 batch-67 = 2.481334377080202e-05

Training epoch-50 batch-68
Running loss of epoch-50 batch-68 = 8.992617949843407e-06

Training epoch-50 batch-69
Running loss of epoch-50 batch-69 = 1.8311897292733192e-05

Training epoch-50 batch-70
Running loss of epoch-50 batch-70 = 8.711591362953186e-06

Training epoch-50 batch-71
Running loss of epoch-50 batch-71 = 2.1236948668956757e-05

Training epoch-50 batch-72
Running loss of epoch-50 batch-72 = 1.3066921383142471e-05

Training epoch-50 batch-73
Running loss of epoch-50 batch-73 = 7.41821713745594e-06

Training epoch-50 batch-74
Running loss of epoch-50 batch-74 = 4.920875653624535e-06

Training epoch-50 batch-75
Running loss of epoch-50 batch-75 = 1.1716503649950027e-05

Training epoch-50 batch-76
Running loss of epoch-50 batch-76 = 3.6803539842367172e-06

Training epoch-50 batch-77
Running loss of epoch-50 batch-77 = 1.594657078385353e-06

Training epoch-50 batch-78
Running loss of epoch-50 batch-78 = 6.87548890709877e-06

Training epoch-50 batch-79
Running loss of epoch-50 batch-79 = 9.307637810707092e-06

Training epoch-50 batch-80
Running loss of epoch-50 batch-80 = 9.312760084867477e-06

Training epoch-50 batch-81
Running loss of epoch-50 batch-81 = 1.5866709873080254e-05

Training epoch-50 batch-82
Running loss of epoch-50 batch-82 = 1.1262251064181328e-05

Training epoch-50 batch-83
Running loss of epoch-50 batch-83 = 1.2829666957259178e-05

Training epoch-50 batch-84
Running loss of epoch-50 batch-84 = 8.468050509691238e-06

Training epoch-50 batch-85
Running loss of epoch-50 batch-85 = 7.712515071034431e-06

Training epoch-50 batch-86
Running loss of epoch-50 batch-86 = 1.0254792869091034e-05

Training epoch-50 batch-87
Running loss of epoch-50 batch-87 = 1.3071112334728241e-05

Training epoch-50 batch-88
Running loss of epoch-50 batch-88 = 6.279442459344864e-06

Training epoch-50 batch-89
Running loss of epoch-50 batch-89 = 9.210547432303429e-06

Training epoch-50 batch-90
Running loss of epoch-50 batch-90 = 5.8175064623355865e-06

Training epoch-50 batch-91
Running loss of epoch-50 batch-91 = 3.4975819289684296e-06

Training epoch-50 batch-92
Running loss of epoch-50 batch-92 = 6.821472197771072e-06

Training epoch-50 batch-93
Running loss of epoch-50 batch-93 = 2.2453954443335533e-05

Training epoch-50 batch-94
Running loss of epoch-50 batch-94 = 4.375819116830826e-06

Training epoch-50 batch-95
Running loss of epoch-50 batch-95 = 5.795387551188469e-06

Training epoch-50 batch-96
Running loss of epoch-50 batch-96 = 6.374204531311989e-06

Training epoch-50 batch-97
Running loss of epoch-50 batch-97 = 1.6988487914204597e-05

Training epoch-50 batch-98
Running loss of epoch-50 batch-98 = 2.5538261979818344e-05

Training epoch-50 batch-99
Running loss of epoch-50 batch-99 = 4.939967766404152e-06

Training epoch-50 batch-100
Running loss of epoch-50 batch-100 = 1.1287396773695946e-05

Training epoch-50 batch-101
Running loss of epoch-50 batch-101 = 3.800727427005768e-06

Training epoch-50 batch-102
Running loss of epoch-50 batch-102 = 1.436704769730568e-05

Training epoch-50 batch-103
Running loss of epoch-50 batch-103 = 2.2390391677618027e-05

Training epoch-50 batch-104
Running loss of epoch-50 batch-104 = 8.36281105875969e-06

Training epoch-50 batch-105
Running loss of epoch-50 batch-105 = 4.681292921304703e-06

Training epoch-50 batch-106
Running loss of epoch-50 batch-106 = 6.208894774317741e-06

Training epoch-50 batch-107
Running loss of epoch-50 batch-107 = 1.3774260878562927e-05

Training epoch-50 batch-108
Running loss of epoch-50 batch-108 = 7.301336154341698e-06

Training epoch-50 batch-109
Running loss of epoch-50 batch-109 = 7.434980943799019e-06

Training epoch-50 batch-110
Running loss of epoch-50 batch-110 = 3.5581178963184357e-06

Training epoch-50 batch-111
Running loss of epoch-50 batch-111 = 5.498295649886131e-06

Training epoch-50 batch-112
Running loss of epoch-50 batch-112 = 1.0980060324072838e-05

Training epoch-50 batch-113
Running loss of epoch-50 batch-113 = 4.077330231666565e-06

Training epoch-50 batch-114
Running loss of epoch-50 batch-114 = 1.1321157217025757e-05

Training epoch-50 batch-115
Running loss of epoch-50 batch-115 = 9.796349331736565e-06

Training epoch-50 batch-116
Running loss of epoch-50 batch-116 = 1.5226658433675766e-05

Training epoch-50 batch-117
Running loss of epoch-50 batch-117 = 1.6304198652505875e-05

Training epoch-50 batch-118
Running loss of epoch-50 batch-118 = 8.804956451058388e-06

Training epoch-50 batch-119
Running loss of epoch-50 batch-119 = 9.649200364947319e-06

Training epoch-50 batch-120
Running loss of epoch-50 batch-120 = 1.2970762327313423e-05

Training epoch-50 batch-121
Running loss of epoch-50 batch-121 = 8.267350494861603e-06

Training epoch-50 batch-122
Running loss of epoch-50 batch-122 = 6.297137588262558e-06

Training epoch-50 batch-123
Running loss of epoch-50 batch-123 = 5.348818376660347e-06

Training epoch-50 batch-124
Running loss of epoch-50 batch-124 = 9.295297786593437e-06

Training epoch-50 batch-125
Running loss of epoch-50 batch-125 = 1.5904661267995834e-05

Training epoch-50 batch-126
Running loss of epoch-50 batch-126 = 7.1767717599868774e-06

Training epoch-50 batch-127
Running loss of epoch-50 batch-127 = 4.98979352414608e-06

Training epoch-50 batch-128
Running loss of epoch-50 batch-128 = 1.3369135558605194e-05

Training epoch-50 batch-129
Running loss of epoch-50 batch-129 = 9.021488949656487e-06

Training epoch-50 batch-130
Running loss of epoch-50 batch-130 = 8.576316758990288e-06

Training epoch-50 batch-131
Running loss of epoch-50 batch-131 = 1.2824777513742447e-05

Training epoch-50 batch-132
Running loss of epoch-50 batch-132 = 1.4926306903362274e-05

Training epoch-50 batch-133
Running loss of epoch-50 batch-133 = 5.316687747836113e-06

Training epoch-50 batch-134
Running loss of epoch-50 batch-134 = 1.665228046476841e-05

Training epoch-50 batch-135
Running loss of epoch-50 batch-135 = 1.133582554757595e-05

Training epoch-50 batch-136
Running loss of epoch-50 batch-136 = 1.1644326150417328e-05

Training epoch-50 batch-137
Running loss of epoch-50 batch-137 = 6.53252936899662e-06

Training epoch-50 batch-138
Running loss of epoch-50 batch-138 = 5.543697625398636e-06

Training epoch-50 batch-139
Running loss of epoch-50 batch-139 = 3.4011900424957275e-06

Training epoch-50 batch-140
Running loss of epoch-50 batch-140 = 1.0127667337656021e-05

Training epoch-50 batch-141
Running loss of epoch-50 batch-141 = 8.637551218271255e-06

Training epoch-50 batch-142
Running loss of epoch-50 batch-142 = 2.580764703452587e-05

Training epoch-50 batch-143
Running loss of epoch-50 batch-143 = 1.0746298357844353e-05

Training epoch-50 batch-144
Running loss of epoch-50 batch-144 = 7.601222023367882e-06

Training epoch-50 batch-145
Running loss of epoch-50 batch-145 = 5.888519808650017e-06

Training epoch-50 batch-146
Running loss of epoch-50 batch-146 = 6.39772042632103e-06

Training epoch-50 batch-147
Running loss of epoch-50 batch-147 = 4.8067886382341385e-06

Training epoch-50 batch-148
Running loss of epoch-50 batch-148 = 8.605187758803368e-06

Training epoch-50 batch-149
Running loss of epoch-50 batch-149 = 1.5938887372612953e-05

Training epoch-50 batch-150
Running loss of epoch-50 batch-150 = 1.2597069144248962e-05

Training epoch-50 batch-151
Running loss of epoch-50 batch-151 = 6.5176282078027725e-06

Training epoch-50 batch-152
Running loss of epoch-50 batch-152 = 1.8802937120199203e-05

Training epoch-50 batch-153
Running loss of epoch-50 batch-153 = 9.558862075209618e-06

Training epoch-50 batch-154
Running loss of epoch-50 batch-154 = 4.414469003677368e-06

Training epoch-50 batch-155
Running loss of epoch-50 batch-155 = 1.3063661754131317e-05

Training epoch-50 batch-156
Running loss of epoch-50 batch-156 = 7.99517147243023e-06

Training epoch-50 batch-157
Running loss of epoch-50 batch-157 = 6.165355443954468e-06

Finished training epoch-50.



Average train loss at epoch-50 = 9.777555614709855e-06

Started Evaluation

Average val loss at epoch-50 = 1.096596747977712

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 91.48 %
Accuracy for class onCreate is: 92.54 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 67.58 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.71 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 73.08 %

Overall Accuracy = 83.74 %

Finished Evaluation



Started training epoch-51


Training epoch-51 batch-1
Running loss of epoch-51 batch-1 = 5.7676807045936584e-06

Training epoch-51 batch-2
Running loss of epoch-51 batch-2 = 9.363284334540367e-06

Training epoch-51 batch-3
Running loss of epoch-51 batch-3 = 1.5976373106241226e-05

Training epoch-51 batch-4
Running loss of epoch-51 batch-4 = 5.93205913901329e-06

Training epoch-51 batch-5
Running loss of epoch-51 batch-5 = 1.0478775948286057e-05

Training epoch-51 batch-6
Running loss of epoch-51 batch-6 = 5.315057933330536e-06

Training epoch-51 batch-7
Running loss of epoch-51 batch-7 = 8.781673386693e-06

Training epoch-51 batch-8
Running loss of epoch-51 batch-8 = 3.755791112780571e-06

Training epoch-51 batch-9
Running loss of epoch-51 batch-9 = 9.349314495921135e-06

Training epoch-51 batch-10
Running loss of epoch-51 batch-10 = 7.983064278960228e-06

Training epoch-51 batch-11
Running loss of epoch-51 batch-11 = 9.424053132534027e-06

Training epoch-51 batch-12
Running loss of epoch-51 batch-12 = 8.843373507261276e-06

Training epoch-51 batch-13
Running loss of epoch-51 batch-13 = 2.7262140065431595e-06

Training epoch-51 batch-14
Running loss of epoch-51 batch-14 = 5.83217479288578e-06

Training epoch-51 batch-15
Running loss of epoch-51 batch-15 = 1.3454584404826164e-05

Training epoch-51 batch-16
Running loss of epoch-51 batch-16 = 5.791662260890007e-06

Training epoch-51 batch-17
Running loss of epoch-51 batch-17 = 2.7550384402275085e-05

Training epoch-51 batch-18
Running loss of epoch-51 batch-18 = 8.156290277838707e-06

Training epoch-51 batch-19
Running loss of epoch-51 batch-19 = 9.323004633188248e-06

Training epoch-51 batch-20
Running loss of epoch-51 batch-20 = 9.055249392986298e-06

Training epoch-51 batch-21
Running loss of epoch-51 batch-21 = 8.892267942428589e-06

Training epoch-51 batch-22
Running loss of epoch-51 batch-22 = 2.0633451640605927e-06

Training epoch-51 batch-23
Running loss of epoch-51 batch-23 = 2.0039500668644905e-05

Training epoch-51 batch-24
Running loss of epoch-51 batch-24 = 3.116088919341564e-05

Training epoch-51 batch-25
Running loss of epoch-51 batch-25 = 1.1337455362081528e-05

Training epoch-51 batch-26
Running loss of epoch-51 batch-26 = 2.5766436010599136e-05

Training epoch-51 batch-27
Running loss of epoch-51 batch-27 = 6.732996553182602e-06

Training epoch-51 batch-28
Running loss of epoch-51 batch-28 = 4.188157618045807e-06

Training epoch-51 batch-29
Running loss of epoch-51 batch-29 = 8.512753993272781e-06

Training epoch-51 batch-30
Running loss of epoch-51 batch-30 = 5.194684490561485e-06

Training epoch-51 batch-31
Running loss of epoch-51 batch-31 = 1.0151183232665062e-05

Training epoch-51 batch-32
Running loss of epoch-51 batch-32 = 1.111975871026516e-05

Training epoch-51 batch-33
Running loss of epoch-51 batch-33 = 4.316912963986397e-06

Training epoch-51 batch-34
Running loss of epoch-51 batch-34 = 1.0956311598420143e-05

Training epoch-51 batch-35
Running loss of epoch-51 batch-35 = 8.557690307497978e-06

Training epoch-51 batch-36
Running loss of epoch-51 batch-36 = 3.186054527759552e-05

Training epoch-51 batch-37
Running loss of epoch-51 batch-37 = 3.908993676304817e-06

Training epoch-51 batch-38
Running loss of epoch-51 batch-38 = 1.3449229300022125e-05

Training epoch-51 batch-39
Running loss of epoch-51 batch-39 = 1.4015007764101028e-05

Training epoch-51 batch-40
Running loss of epoch-51 batch-40 = 7.73952342569828e-06

Training epoch-51 batch-41
Running loss of epoch-51 batch-41 = 5.550682544708252e-06

Training epoch-51 batch-42
Running loss of epoch-51 batch-42 = 1.0525109246373177e-05

Training epoch-51 batch-43
Running loss of epoch-51 batch-43 = 1.0865740478038788e-05

Training epoch-51 batch-44
Running loss of epoch-51 batch-44 = 1.9224826246500015e-06

Training epoch-51 batch-45
Running loss of epoch-51 batch-45 = 7.120426744222641e-06

Training epoch-51 batch-46
Running loss of epoch-51 batch-46 = 4.22145240008831e-06

Training epoch-51 batch-47
Running loss of epoch-51 batch-47 = 6.832880899310112e-06

Training epoch-51 batch-48
Running loss of epoch-51 batch-48 = 8.316710591316223e-06

Training epoch-51 batch-49
Running loss of epoch-51 batch-49 = 5.333218723535538e-06

Training epoch-51 batch-50
Running loss of epoch-51 batch-50 = 1.2948643416166306e-05

Training epoch-51 batch-51
Running loss of epoch-51 batch-51 = 1.1638971045613289e-05

Training epoch-51 batch-52
Running loss of epoch-51 batch-52 = 1.4075078070163727e-05

Training epoch-51 batch-53
Running loss of epoch-51 batch-53 = 7.860129699110985e-06

Training epoch-51 batch-54
Running loss of epoch-51 batch-54 = 1.701503060758114e-05

Training epoch-51 batch-55
Running loss of epoch-51 batch-55 = 6.651272997260094e-06

Training epoch-51 batch-56
Running loss of epoch-51 batch-56 = 2.4442560970783234e-06

Training epoch-51 batch-57
Running loss of epoch-51 batch-57 = 5.105510354042053e-06

Training epoch-51 batch-58
Running loss of epoch-51 batch-58 = 4.0084123611450195e-06

Training epoch-51 batch-59
Running loss of epoch-51 batch-59 = 8.608214557170868e-06

Training epoch-51 batch-60
Running loss of epoch-51 batch-60 = 9.509967640042305e-06

Training epoch-51 batch-61
Running loss of epoch-51 batch-61 = 6.527407094836235e-06

Training epoch-51 batch-62
Running loss of epoch-51 batch-62 = 1.0514631867408752e-05

Training epoch-51 batch-63
Running loss of epoch-51 batch-63 = 7.401918992400169e-06

Training epoch-51 batch-64
Running loss of epoch-51 batch-64 = 8.549774065613747e-06

Training epoch-51 batch-65
Running loss of epoch-51 batch-65 = 9.15047712624073e-06

Training epoch-51 batch-66
Running loss of epoch-51 batch-66 = 1.6081612557172775e-05

Training epoch-51 batch-67
Running loss of epoch-51 batch-67 = 1.2594973668456078e-05

Training epoch-51 batch-68
Running loss of epoch-51 batch-68 = 7.393769919872284e-06

Training epoch-51 batch-69
Running loss of epoch-51 batch-69 = 8.873874321579933e-06

Training epoch-51 batch-70
Running loss of epoch-51 batch-70 = 9.077833965420723e-06

Training epoch-51 batch-71
Running loss of epoch-51 batch-71 = 5.087349563837051e-06

Training epoch-51 batch-72
Running loss of epoch-51 batch-72 = 3.380700945854187e-06

Training epoch-51 batch-73
Running loss of epoch-51 batch-73 = 8.133938536047935e-06

Training epoch-51 batch-74
Running loss of epoch-51 batch-74 = 1.0607531294226646e-05

Training epoch-51 batch-75
Running loss of epoch-51 batch-75 = 3.705499693751335e-06

Training epoch-51 batch-76
Running loss of epoch-51 batch-76 = 5.512731149792671e-06

Training epoch-51 batch-77
Running loss of epoch-51 batch-77 = 5.8712903410196304e-06

Training epoch-51 batch-78
Running loss of epoch-51 batch-78 = 1.9272789359092712e-05

Training epoch-51 batch-79
Running loss of epoch-51 batch-79 = 7.700175046920776e-06

Training epoch-51 batch-80
Running loss of epoch-51 batch-80 = 2.6866327971220016e-06

Training epoch-51 batch-81
Running loss of epoch-51 batch-81 = 1.4263438060879707e-05

Training epoch-51 batch-82
Running loss of epoch-51 batch-82 = 2.295244485139847e-06

Training epoch-51 batch-83
Running loss of epoch-51 batch-83 = 4.572328180074692e-06

Training epoch-51 batch-84
Running loss of epoch-51 batch-84 = 7.497146725654602e-06

Training epoch-51 batch-85
Running loss of epoch-51 batch-85 = 1.0537449270486832e-05

Training epoch-51 batch-86
Running loss of epoch-51 batch-86 = 1.2353993952274323e-05

Training epoch-51 batch-87
Running loss of epoch-51 batch-87 = 1.42629723995924e-05

Training epoch-51 batch-88
Running loss of epoch-51 batch-88 = 1.0631047189235687e-05

Training epoch-51 batch-89
Running loss of epoch-51 batch-89 = 1.0537216439843178e-05

Training epoch-51 batch-90
Running loss of epoch-51 batch-90 = 5.628447979688644e-06

Training epoch-51 batch-91
Running loss of epoch-51 batch-91 = 9.577488526701927e-06

Training epoch-51 batch-92
Running loss of epoch-51 batch-92 = 5.553010851144791e-06

Training epoch-51 batch-93
Running loss of epoch-51 batch-93 = 1.4565186575055122e-05

Training epoch-51 batch-94
Running loss of epoch-51 batch-94 = 9.930459782481194e-06

Training epoch-51 batch-95
Running loss of epoch-51 batch-95 = 3.847060725092888e-06

Training epoch-51 batch-96
Running loss of epoch-51 batch-96 = 9.592156857252121e-06

Training epoch-51 batch-97
Running loss of epoch-51 batch-97 = 3.757420927286148e-06

Training epoch-51 batch-98
Running loss of epoch-51 batch-98 = 9.21543687582016e-06

Training epoch-51 batch-99
Running loss of epoch-51 batch-99 = 3.2021664083004e-05

Training epoch-51 batch-100
Running loss of epoch-51 batch-100 = 8.067581802606583e-06

Training epoch-51 batch-101
Running loss of epoch-51 batch-101 = 1.2415694072842598e-05

Training epoch-51 batch-102
Running loss of epoch-51 batch-102 = 9.79006290435791e-06

Training epoch-51 batch-103
Running loss of epoch-51 batch-103 = 7.146038115024567e-06

Training epoch-51 batch-104
Running loss of epoch-51 batch-104 = 1.5203375369310379e-05

Training epoch-51 batch-105
Running loss of epoch-51 batch-105 = 1.6415026038885117e-05

Training epoch-51 batch-106
Running loss of epoch-51 batch-106 = 4.000263288617134e-06

Training epoch-51 batch-107
Running loss of epoch-51 batch-107 = 2.1142419427633286e-05

Training epoch-51 batch-108
Running loss of epoch-51 batch-108 = 9.492971003055573e-06

Training epoch-51 batch-109
Running loss of epoch-51 batch-109 = 6.398186087608337e-06

Training epoch-51 batch-110
Running loss of epoch-51 batch-110 = 9.92906279861927e-06

Training epoch-51 batch-111
Running loss of epoch-51 batch-111 = 9.39471647143364e-06

Training epoch-51 batch-112
Running loss of epoch-51 batch-112 = 7.142312824726105e-06

Training epoch-51 batch-113
Running loss of epoch-51 batch-113 = 3.0712690204381943e-06

Training epoch-51 batch-114
Running loss of epoch-51 batch-114 = 8.28225165605545e-06

Training epoch-51 batch-115
Running loss of epoch-51 batch-115 = 9.544426575303078e-06

Training epoch-51 batch-116
Running loss of epoch-51 batch-116 = 1.3483688235282898e-05

Training epoch-51 batch-117
Running loss of epoch-51 batch-117 = 6.41215592622757e-06

Training epoch-51 batch-118
Running loss of epoch-51 batch-118 = 4.396308213472366e-06

Training epoch-51 batch-119
Running loss of epoch-51 batch-119 = 8.100410923361778e-06

Training epoch-51 batch-120
Running loss of epoch-51 batch-120 = 1.806626096367836e-05

Training epoch-51 batch-121
Running loss of epoch-51 batch-121 = 5.5516138672828674e-06

Training epoch-51 batch-122
Running loss of epoch-51 batch-122 = 1.1949334293603897e-05

Training epoch-51 batch-123
Running loss of epoch-51 batch-123 = 1.1826399713754654e-05

Training epoch-51 batch-124
Running loss of epoch-51 batch-124 = 9.547686204314232e-06

Training epoch-51 batch-125
Running loss of epoch-51 batch-125 = 6.734160706400871e-06

Training epoch-51 batch-126
Running loss of epoch-51 batch-126 = 4.152767360210419e-06

Training epoch-51 batch-127
Running loss of epoch-51 batch-127 = 1.1884607374668121e-05

Training epoch-51 batch-128
Running loss of epoch-51 batch-128 = 7.257331162691116e-06

Training epoch-51 batch-129
Running loss of epoch-51 batch-129 = 1.0150251910090446e-05

Training epoch-51 batch-130
Running loss of epoch-51 batch-130 = 1.0487856343388557e-05

Training epoch-51 batch-131
Running loss of epoch-51 batch-131 = 7.305759936571121e-06

Training epoch-51 batch-132
Running loss of epoch-51 batch-132 = 1.8015969544649124e-05

Training epoch-51 batch-133
Running loss of epoch-51 batch-133 = 4.959758371114731e-06

Training epoch-51 batch-134
Running loss of epoch-51 batch-134 = 7.525552064180374e-06

Training epoch-51 batch-135
Running loss of epoch-51 batch-135 = 6.37909397482872e-06

Training epoch-51 batch-136
Running loss of epoch-51 batch-136 = 7.822178304195404e-06

Training epoch-51 batch-137
Running loss of epoch-51 batch-137 = 1.8837861716747284e-05

Training epoch-51 batch-138
Running loss of epoch-51 batch-138 = 9.889248758554459e-06

Training epoch-51 batch-139
Running loss of epoch-51 batch-139 = 1.0259449481964111e-05

Training epoch-51 batch-140
Running loss of epoch-51 batch-140 = 1.1877622455358505e-05

Training epoch-51 batch-141
Running loss of epoch-51 batch-141 = 1.0801944881677628e-05

Training epoch-51 batch-142
Running loss of epoch-51 batch-142 = 3.3318065106868744e-06

Training epoch-51 batch-143
Running loss of epoch-51 batch-143 = 5.483394488692284e-06

Training epoch-51 batch-144
Running loss of epoch-51 batch-144 = 1.4147255569696426e-05

Training epoch-51 batch-145
Running loss of epoch-51 batch-145 = 6.581190973520279e-06

Training epoch-51 batch-146
Running loss of epoch-51 batch-146 = 1.0969815775752068e-05

Training epoch-51 batch-147
Running loss of epoch-51 batch-147 = 5.8980658650398254e-06

Training epoch-51 batch-148
Running loss of epoch-51 batch-148 = 4.948582500219345e-06

Training epoch-51 batch-149
Running loss of epoch-51 batch-149 = 1.662410795688629e-05

Training epoch-51 batch-150
Running loss of epoch-51 batch-150 = 9.86526720225811e-06

Training epoch-51 batch-151
Running loss of epoch-51 batch-151 = 3.688270226120949e-06

Training epoch-51 batch-152
Running loss of epoch-51 batch-152 = 1.0495306923985481e-05

Training epoch-51 batch-153
Running loss of epoch-51 batch-153 = 5.899462848901749e-06

Training epoch-51 batch-154
Running loss of epoch-51 batch-154 = 6.662914529442787e-06

Training epoch-51 batch-155
Running loss of epoch-51 batch-155 = 5.4263509809970856e-06

Training epoch-51 batch-156
Running loss of epoch-51 batch-156 = 4.500849172472954e-06

Training epoch-51 batch-157
Running loss of epoch-51 batch-157 = 2.1666288375854492e-05

Finished training epoch-51.



Average train loss at epoch-51 = 9.42842960357666e-06

Started Evaluation

Average val loss at epoch-51 = 1.1044600335865198

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 92.46 %
Accuracy for class onCreate is: 92.00 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 65.75 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.16 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.64 %

Finished Evaluation



Started training epoch-52


Training epoch-52 batch-1
Running loss of epoch-52 batch-1 = 2.029072493314743e-05

Training epoch-52 batch-2
Running loss of epoch-52 batch-2 = 6.1478931456804276e-06

Training epoch-52 batch-3
Running loss of epoch-52 batch-3 = 1.3754935935139656e-05

Training epoch-52 batch-4
Running loss of epoch-52 batch-4 = 1.0401476174592972e-05

Training epoch-52 batch-5
Running loss of epoch-52 batch-5 = 7.907627150416374e-06

Training epoch-52 batch-6
Running loss of epoch-52 batch-6 = 1.2056669220328331e-05

Training epoch-52 batch-7
Running loss of epoch-52 batch-7 = 2.7136411517858505e-06

Training epoch-52 batch-8
Running loss of epoch-52 batch-8 = 9.541166946291924e-06

Training epoch-52 batch-9
Running loss of epoch-52 batch-9 = 3.5474076867103577e-06

Training epoch-52 batch-10
Running loss of epoch-52 batch-10 = 2.9979273676872253e-06

Training epoch-52 batch-11
Running loss of epoch-52 batch-11 = 1.0239658877253532e-05

Training epoch-52 batch-12
Running loss of epoch-52 batch-12 = 4.377448931336403e-06

Training epoch-52 batch-13
Running loss of epoch-52 batch-13 = 2.79303640127182e-06

Training epoch-52 batch-14
Running loss of epoch-52 batch-14 = 1.1641299352049828e-05

Training epoch-52 batch-15
Running loss of epoch-52 batch-15 = 1.469440758228302e-05

Training epoch-52 batch-16
Running loss of epoch-52 batch-16 = 2.9680319130420685e-05

Training epoch-52 batch-17
Running loss of epoch-52 batch-17 = 1.2285076081752777e-05

Training epoch-52 batch-18
Running loss of epoch-52 batch-18 = 4.469417035579681e-06

Training epoch-52 batch-19
Running loss of epoch-52 batch-19 = 1.2135831639170647e-05

Training epoch-52 batch-20
Running loss of epoch-52 batch-20 = 1.1294148862361908e-05

Training epoch-52 batch-21
Running loss of epoch-52 batch-21 = 1.8299906514585018e-05

Training epoch-52 batch-22
Running loss of epoch-52 batch-22 = 9.965384379029274e-06

Training epoch-52 batch-23
Running loss of epoch-52 batch-23 = 1.4383811503648758e-05

Training epoch-52 batch-24
Running loss of epoch-52 batch-24 = 1.1884141713380814e-05

Training epoch-52 batch-25
Running loss of epoch-52 batch-25 = 6.467336788773537e-06

Training epoch-52 batch-26
Running loss of epoch-52 batch-26 = 1.561129465699196e-05

Training epoch-52 batch-27
Running loss of epoch-52 batch-27 = 6.467103958129883e-06

Training epoch-52 batch-28
Running loss of epoch-52 batch-28 = 1.5277881175279617e-05

Training epoch-52 batch-29
Running loss of epoch-52 batch-29 = 9.287381544709206e-06

Training epoch-52 batch-30
Running loss of epoch-52 batch-30 = 4.8871152102947235e-06

Training epoch-52 batch-31
Running loss of epoch-52 batch-31 = 1.4437362551689148e-05

Training epoch-52 batch-32
Running loss of epoch-52 batch-32 = 9.371200576424599e-06

Training epoch-52 batch-33
Running loss of epoch-52 batch-33 = 5.544628947973251e-06

Training epoch-52 batch-34
Running loss of epoch-52 batch-34 = 3.2729003578424454e-06

Training epoch-52 batch-35
Running loss of epoch-52 batch-35 = 1.3368437066674232e-05

Training epoch-52 batch-36
Running loss of epoch-52 batch-36 = 2.719694748520851e-06

Training epoch-52 batch-37
Running loss of epoch-52 batch-37 = 8.479459211230278e-06

Training epoch-52 batch-38
Running loss of epoch-52 batch-38 = 1.790444366633892e-05

Training epoch-52 batch-39
Running loss of epoch-52 batch-39 = 8.092029020190239e-06

Training epoch-52 batch-40
Running loss of epoch-52 batch-40 = 1.0411953553557396e-05

Training epoch-52 batch-41
Running loss of epoch-52 batch-41 = 7.642898708581924e-06

Training epoch-52 batch-42
Running loss of epoch-52 batch-42 = 7.172580808401108e-06

Training epoch-52 batch-43
Running loss of epoch-52 batch-43 = 9.28947702050209e-06

Training epoch-52 batch-44
Running loss of epoch-52 batch-44 = 6.437534466385841e-06

Training epoch-52 batch-45
Running loss of epoch-52 batch-45 = 2.4533364921808243e-06

Training epoch-52 batch-46
Running loss of epoch-52 batch-46 = 5.191657692193985e-06

Training epoch-52 batch-47
Running loss of epoch-52 batch-47 = 3.8833823055028915e-06

Training epoch-52 batch-48
Running loss of epoch-52 batch-48 = 5.100155249238014e-06

Training epoch-52 batch-49
Running loss of epoch-52 batch-49 = 6.400630809366703e-06

Training epoch-52 batch-50
Running loss of epoch-52 batch-50 = 1.3461802154779434e-05

Training epoch-52 batch-51
Running loss of epoch-52 batch-51 = 9.108101949095726e-06

Training epoch-52 batch-52
Running loss of epoch-52 batch-52 = 8.275732398033142e-06

Training epoch-52 batch-53
Running loss of epoch-52 batch-53 = 4.952074959874153e-06

Training epoch-52 batch-54
Running loss of epoch-52 batch-54 = 1.5387078747153282e-05

Training epoch-52 batch-55
Running loss of epoch-52 batch-55 = 1.083756797015667e-05

Training epoch-52 batch-56
Running loss of epoch-52 batch-56 = 5.488051101565361e-06

Training epoch-52 batch-57
Running loss of epoch-52 batch-57 = 6.85499981045723e-06

Training epoch-52 batch-58
Running loss of epoch-52 batch-58 = 5.915295332670212e-06

Training epoch-52 batch-59
Running loss of epoch-52 batch-59 = 2.1759187802672386e-05

Training epoch-52 batch-60
Running loss of epoch-52 batch-60 = 1.5028752386569977e-05

Training epoch-52 batch-61
Running loss of epoch-52 batch-61 = 1.1103460565209389e-05

Training epoch-52 batch-62
Running loss of epoch-52 batch-62 = 4.227971658110619e-06

Training epoch-52 batch-63
Running loss of epoch-52 batch-63 = 4.9336813390254974e-06

Training epoch-52 batch-64
Running loss of epoch-52 batch-64 = 1.3180077075958252e-05

Training epoch-52 batch-65
Running loss of epoch-52 batch-65 = 2.2684456780552864e-05

Training epoch-52 batch-66
Running loss of epoch-52 batch-66 = 5.881069228053093e-06

Training epoch-52 batch-67
Running loss of epoch-52 batch-67 = 5.540437996387482e-06

Training epoch-52 batch-68
Running loss of epoch-52 batch-68 = 3.081047907471657e-06

Training epoch-52 batch-69
Running loss of epoch-52 batch-69 = 8.38213600218296e-06

Training epoch-52 batch-70
Running loss of epoch-52 batch-70 = 7.5944699347019196e-06

Training epoch-52 batch-71
Running loss of epoch-52 batch-71 = 8.539995178580284e-06

Training epoch-52 batch-72
Running loss of epoch-52 batch-72 = 7.295282557606697e-06

Training epoch-52 batch-73
Running loss of epoch-52 batch-73 = 7.641036063432693e-06

Training epoch-52 batch-74
Running loss of epoch-52 batch-74 = 6.026355549693108e-06

Training epoch-52 batch-75
Running loss of epoch-52 batch-75 = 2.0127976313233376e-05

Training epoch-52 batch-76
Running loss of epoch-52 batch-76 = 1.4065299183130264e-05

Training epoch-52 batch-77
Running loss of epoch-52 batch-77 = 1.1476222425699234e-05

Training epoch-52 batch-78
Running loss of epoch-52 batch-78 = 6.202375516295433e-06

Training epoch-52 batch-79
Running loss of epoch-52 batch-79 = 3.2933661714196205e-05

Training epoch-52 batch-80
Running loss of epoch-52 batch-80 = 7.494352757930756e-06

Training epoch-52 batch-81
Running loss of epoch-52 batch-81 = 6.506452336907387e-06

Training epoch-52 batch-82
Running loss of epoch-52 batch-82 = 4.370696842670441e-06

Training epoch-52 batch-83
Running loss of epoch-52 batch-83 = 6.812624633312225e-06

Training epoch-52 batch-84
Running loss of epoch-52 batch-84 = 1.239776611328125e-05

Training epoch-52 batch-85
Running loss of epoch-52 batch-85 = 6.016343832015991e-06

Training epoch-52 batch-86
Running loss of epoch-52 batch-86 = 1.1372147127985954e-05

Training epoch-52 batch-87
Running loss of epoch-52 batch-87 = 7.453840225934982e-06

Training epoch-52 batch-88
Running loss of epoch-52 batch-88 = 5.168374627828598e-06

Training epoch-52 batch-89
Running loss of epoch-52 batch-89 = 4.296889528632164e-06

Training epoch-52 batch-90
Running loss of epoch-52 batch-90 = 1.0672258213162422e-05

Training epoch-52 batch-91
Running loss of epoch-52 batch-91 = 6.722519174218178e-06

Training epoch-52 batch-92
Running loss of epoch-52 batch-92 = 1.003011129796505e-05

Training epoch-52 batch-93
Running loss of epoch-52 batch-93 = 1.3042706996202469e-05

Training epoch-52 batch-94
Running loss of epoch-52 batch-94 = 1.2590782716870308e-05

Training epoch-52 batch-95
Running loss of epoch-52 batch-95 = 7.842667400836945e-06

Training epoch-52 batch-96
Running loss of epoch-52 batch-96 = 1.7920508980751038e-05

Training epoch-52 batch-97
Running loss of epoch-52 batch-97 = 2.8945505619049072e-06

Training epoch-52 batch-98
Running loss of epoch-52 batch-98 = 3.983033820986748e-06

Training epoch-52 batch-99
Running loss of epoch-52 batch-99 = 6.387708708643913e-06

Training epoch-52 batch-100
Running loss of epoch-52 batch-100 = 8.523231372237206e-06

Training epoch-52 batch-101
Running loss of epoch-52 batch-101 = 1.2894859537482262e-05

Training epoch-52 batch-102
Running loss of epoch-52 batch-102 = 1.2757722288370132e-05

Training epoch-52 batch-103
Running loss of epoch-52 batch-103 = 1.6641337424516678e-05

Training epoch-52 batch-104
Running loss of epoch-52 batch-104 = 1.2555858120322227e-05

Training epoch-52 batch-105
Running loss of epoch-52 batch-105 = 1.2636417523026466e-05

Training epoch-52 batch-106
Running loss of epoch-52 batch-106 = 5.972571671009064e-06

Training epoch-52 batch-107
Running loss of epoch-52 batch-107 = 8.96211713552475e-06

Training epoch-52 batch-108
Running loss of epoch-52 batch-108 = 2.1235551685094833e-05

Training epoch-52 batch-109
Running loss of epoch-52 batch-109 = 5.025649443268776e-06

Training epoch-52 batch-110
Running loss of epoch-52 batch-110 = 7.3336996138095856e-06

Training epoch-52 batch-111
Running loss of epoch-52 batch-111 = 5.743233487010002e-06

Training epoch-52 batch-112
Running loss of epoch-52 batch-112 = 1.0358402505517006e-05

Training epoch-52 batch-113
Running loss of epoch-52 batch-113 = 4.991190508008003e-06

Training epoch-52 batch-114
Running loss of epoch-52 batch-114 = 5.995621904730797e-06

Training epoch-52 batch-115
Running loss of epoch-52 batch-115 = 1.081172376871109e-05

Training epoch-52 batch-116
Running loss of epoch-52 batch-116 = 1.2418953701853752e-05

Training epoch-52 batch-117
Running loss of epoch-52 batch-117 = 9.984476491808891e-06

Training epoch-52 batch-118
Running loss of epoch-52 batch-118 = 3.757653757929802e-06

Training epoch-52 batch-119
Running loss of epoch-52 batch-119 = 3.891531378030777e-06

Training epoch-52 batch-120
Running loss of epoch-52 batch-120 = 1.6146805137395859e-06

Training epoch-52 batch-121
Running loss of epoch-52 batch-121 = 6.844056770205498e-06

Training epoch-52 batch-122
Running loss of epoch-52 batch-122 = 4.405621439218521e-06

Training epoch-52 batch-123
Running loss of epoch-52 batch-123 = 6.881775334477425e-06

Training epoch-52 batch-124
Running loss of epoch-52 batch-124 = 3.100605681538582e-06

Training epoch-52 batch-125
Running loss of epoch-52 batch-125 = 2.441834658384323e-05

Training epoch-52 batch-126
Running loss of epoch-52 batch-126 = 1.0708346962928772e-05

Training epoch-52 batch-127
Running loss of epoch-52 batch-127 = 7.782597094774246e-06

Training epoch-52 batch-128
Running loss of epoch-52 batch-128 = 9.77981835603714e-06

Training epoch-52 batch-129
Running loss of epoch-52 batch-129 = 7.787486538290977e-06

Training epoch-52 batch-130
Running loss of epoch-52 batch-130 = 4.839152097702026e-06

Training epoch-52 batch-131
Running loss of epoch-52 batch-131 = 1.056981272995472e-05

Training epoch-52 batch-132
Running loss of epoch-52 batch-132 = 1.3320241123437881e-06

Training epoch-52 batch-133
Running loss of epoch-52 batch-133 = 5.654292181134224e-06

Training epoch-52 batch-134
Running loss of epoch-52 batch-134 = 1.0563991963863373e-05

Training epoch-52 batch-135
Running loss of epoch-52 batch-135 = 9.431736543774605e-06

Training epoch-52 batch-136
Running loss of epoch-52 batch-136 = 1.1938624083995819e-05

Training epoch-52 batch-137
Running loss of epoch-52 batch-137 = 4.574190825223923e-06

Training epoch-52 batch-138
Running loss of epoch-52 batch-138 = 1.3111624866724014e-05

Training epoch-52 batch-139
Running loss of epoch-52 batch-139 = 2.7792993932962418e-06

Training epoch-52 batch-140
Running loss of epoch-52 batch-140 = 6.064306944608688e-06

Training epoch-52 batch-141
Running loss of epoch-52 batch-141 = 8.101807907223701e-06

Training epoch-52 batch-142
Running loss of epoch-52 batch-142 = 4.66429628431797e-06

Training epoch-52 batch-143
Running loss of epoch-52 batch-143 = 6.928108632564545e-06

Training epoch-52 batch-144
Running loss of epoch-52 batch-144 = 1.3599172234535217e-05

Training epoch-52 batch-145
Running loss of epoch-52 batch-145 = 3.220280632376671e-06

Training epoch-52 batch-146
Running loss of epoch-52 batch-146 = 1.3310695067048073e-05

Training epoch-52 batch-147
Running loss of epoch-52 batch-147 = 4.407716915011406e-06

Training epoch-52 batch-148
Running loss of epoch-52 batch-148 = 7.456168532371521e-06

Training epoch-52 batch-149
Running loss of epoch-52 batch-149 = 4.044733941555023e-06

Training epoch-52 batch-150
Running loss of epoch-52 batch-150 = 5.251029506325722e-06

Training epoch-52 batch-151
Running loss of epoch-52 batch-151 = 1.661130227148533e-05

Training epoch-52 batch-152
Running loss of epoch-52 batch-152 = 4.64380718767643e-06

Training epoch-52 batch-153
Running loss of epoch-52 batch-153 = 9.443610906600952e-06

Training epoch-52 batch-154
Running loss of epoch-52 batch-154 = 7.662223652005196e-06

Training epoch-52 batch-155
Running loss of epoch-52 batch-155 = 1.3079261407256126e-05

Training epoch-52 batch-156
Running loss of epoch-52 batch-156 = 5.362555384635925e-06

Training epoch-52 batch-157
Running loss of epoch-52 batch-157 = 7.441267371177673e-05

Finished training epoch-52.



Average train loss at epoch-52 = 9.265288710594177e-06

Started Evaluation

Average val loss at epoch-52 = 1.1031271442022539

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 91.90 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 66.21 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 74.10 %

Overall Accuracy = 83.68 %

Finished Evaluation



Started training epoch-53


Training epoch-53 batch-1
Running loss of epoch-53 batch-1 = 5.281297490000725e-06

Training epoch-53 batch-2
Running loss of epoch-53 batch-2 = 1.3851793482899666e-05

Training epoch-53 batch-3
Running loss of epoch-53 batch-3 = 1.0205898433923721e-05

Training epoch-53 batch-4
Running loss of epoch-53 batch-4 = 9.312992915511131e-06

Training epoch-53 batch-5
Running loss of epoch-53 batch-5 = 1.9302591681480408e-05

Training epoch-53 batch-6
Running loss of epoch-53 batch-6 = 7.959315553307533e-06

Training epoch-53 batch-7
Running loss of epoch-53 batch-7 = 3.623543307185173e-06

Training epoch-53 batch-8
Running loss of epoch-53 batch-8 = 9.611714631319046e-06

Training epoch-53 batch-9
Running loss of epoch-53 batch-9 = 8.179806172847748e-06

Training epoch-53 batch-10
Running loss of epoch-53 batch-10 = 9.159790351986885e-06

Training epoch-53 batch-11
Running loss of epoch-53 batch-11 = 3.9422884583473206e-06

Training epoch-53 batch-12
Running loss of epoch-53 batch-12 = 3.093574196100235e-05

Training epoch-53 batch-13
Running loss of epoch-53 batch-13 = 5.9569720178842545e-06

Training epoch-53 batch-14
Running loss of epoch-53 batch-14 = 6.158137694001198e-06

Training epoch-53 batch-15
Running loss of epoch-53 batch-15 = 6.247544661164284e-06

Training epoch-53 batch-16
Running loss of epoch-53 batch-16 = 8.858973160386086e-06

Training epoch-53 batch-17
Running loss of epoch-53 batch-17 = 5.609123036265373e-06

Training epoch-53 batch-18
Running loss of epoch-53 batch-18 = 5.082227289676666e-06

Training epoch-53 batch-19
Running loss of epoch-53 batch-19 = 8.14325176179409e-06

Training epoch-53 batch-20
Running loss of epoch-53 batch-20 = 8.381903171539307e-06

Training epoch-53 batch-21
Running loss of epoch-53 batch-21 = 4.220288246870041e-06

Training epoch-53 batch-22
Running loss of epoch-53 batch-22 = 1.8686288967728615e-05

Training epoch-53 batch-23
Running loss of epoch-53 batch-23 = 5.718320608139038e-06

Training epoch-53 batch-24
Running loss of epoch-53 batch-24 = 8.831964805722237e-06

Training epoch-53 batch-25
Running loss of epoch-53 batch-25 = 8.487841114401817e-06

Training epoch-53 batch-26
Running loss of epoch-53 batch-26 = 8.360715582966805e-06

Training epoch-53 batch-27
Running loss of epoch-53 batch-27 = 1.2033386155962944e-05

Training epoch-53 batch-28
Running loss of epoch-53 batch-28 = 6.7802611738443375e-06

Training epoch-53 batch-29
Running loss of epoch-53 batch-29 = 9.007984772324562e-06

Training epoch-53 batch-30
Running loss of epoch-53 batch-30 = 6.811460480093956e-06

Training epoch-53 batch-31
Running loss of epoch-53 batch-31 = 7.724389433860779e-06

Training epoch-53 batch-32
Running loss of epoch-53 batch-32 = 1.0802876204252243e-05

Training epoch-53 batch-33
Running loss of epoch-53 batch-33 = 1.0551884770393372e-05

Training epoch-53 batch-34
Running loss of epoch-53 batch-34 = 7.5125135481357574e-06

Training epoch-53 batch-35
Running loss of epoch-53 batch-35 = 2.6695197448134422e-05

Training epoch-53 batch-36
Running loss of epoch-53 batch-36 = 3.280816599726677e-06

Training epoch-53 batch-37
Running loss of epoch-53 batch-37 = 1.1438271030783653e-05

Training epoch-53 batch-38
Running loss of epoch-53 batch-38 = 1.008063554763794e-05

Training epoch-53 batch-39
Running loss of epoch-53 batch-39 = 5.5069103837013245e-06

Training epoch-53 batch-40
Running loss of epoch-53 batch-40 = 7.579568773508072e-06

Training epoch-53 batch-41
Running loss of epoch-53 batch-41 = 2.239597961306572e-05

Training epoch-53 batch-42
Running loss of epoch-53 batch-42 = 1.9008293747901917e-05

Training epoch-53 batch-43
Running loss of epoch-53 batch-43 = 5.354871973395348e-06

Training epoch-53 batch-44
Running loss of epoch-53 batch-44 = 6.138579919934273e-06

Training epoch-53 batch-45
Running loss of epoch-53 batch-45 = 8.602393791079521e-06

Training epoch-53 batch-46
Running loss of epoch-53 batch-46 = 5.753012374043465e-06

Training epoch-53 batch-47
Running loss of epoch-53 batch-47 = 1.4263903722167015e-05

Training epoch-53 batch-48
Running loss of epoch-53 batch-48 = 8.388422429561615e-06

Training epoch-53 batch-49
Running loss of epoch-53 batch-49 = 5.937414243817329e-06

Training epoch-53 batch-50
Running loss of epoch-53 batch-50 = 9.609851986169815e-06

Training epoch-53 batch-51
Running loss of epoch-53 batch-51 = 4.481291398406029e-06

Training epoch-53 batch-52
Running loss of epoch-53 batch-52 = 1.2713484466075897e-05

Training epoch-53 batch-53
Running loss of epoch-53 batch-53 = 2.6435591280460358e-06

Training epoch-53 batch-54
Running loss of epoch-53 batch-54 = 5.462672561407089e-06

Training epoch-53 batch-55
Running loss of epoch-53 batch-55 = 8.05174931883812e-06

Training epoch-53 batch-56
Running loss of epoch-53 batch-56 = 6.495276466012001e-06

Training epoch-53 batch-57
Running loss of epoch-53 batch-57 = 1.391163095831871e-06

Training epoch-53 batch-58
Running loss of epoch-53 batch-58 = 1.2013828381896019e-05

Training epoch-53 batch-59
Running loss of epoch-53 batch-59 = 5.4889824241399765e-06

Training epoch-53 batch-60
Running loss of epoch-53 batch-60 = 5.137873813509941e-06

Training epoch-53 batch-61
Running loss of epoch-53 batch-61 = 9.052222594618797e-06

Training epoch-53 batch-62
Running loss of epoch-53 batch-62 = 5.05778007209301e-06

Training epoch-53 batch-63
Running loss of epoch-53 batch-63 = 6.501795724034309e-06

Training epoch-53 batch-64
Running loss of epoch-53 batch-64 = 1.4879275113344193e-05

Training epoch-53 batch-65
Running loss of epoch-53 batch-65 = 8.98679718375206e-06

Training epoch-53 batch-66
Running loss of epoch-53 batch-66 = 9.566312655806541e-06

Training epoch-53 batch-67
Running loss of epoch-53 batch-67 = 3.1641684472560883e-06

Training epoch-53 batch-68
Running loss of epoch-53 batch-68 = 7.780501618981361e-06

Training epoch-53 batch-69
Running loss of epoch-53 batch-69 = 8.528819307684898e-06

Training epoch-53 batch-70
Running loss of epoch-53 batch-70 = 7.770722731947899e-06

Training epoch-53 batch-71
Running loss of epoch-53 batch-71 = 1.0745134204626083e-05

Training epoch-53 batch-72
Running loss of epoch-53 batch-72 = 9.094132110476494e-06

Training epoch-53 batch-73
Running loss of epoch-53 batch-73 = 7.80448317527771e-06

Training epoch-53 batch-74
Running loss of epoch-53 batch-74 = 8.50670039653778e-06

Training epoch-53 batch-75
Running loss of epoch-53 batch-75 = 5.25452196598053e-06

Training epoch-53 batch-76
Running loss of epoch-53 batch-76 = 8.969102054834366e-06

Training epoch-53 batch-77
Running loss of epoch-53 batch-77 = 4.659639671444893e-06

Training epoch-53 batch-78
Running loss of epoch-53 batch-78 = 4.730187356472015e-06

Training epoch-53 batch-79
Running loss of epoch-53 batch-79 = 1.3940967619419098e-05

Training epoch-53 batch-80
Running loss of epoch-53 batch-80 = 4.882924258708954e-06

Training epoch-53 batch-81
Running loss of epoch-53 batch-81 = 1.0232441127300262e-05

Training epoch-53 batch-82
Running loss of epoch-53 batch-82 = 7.888302206993103e-06

Training epoch-53 batch-83
Running loss of epoch-53 batch-83 = 1.8259743228554726e-05

Training epoch-53 batch-84
Running loss of epoch-53 batch-84 = 6.01564534008503e-06

Training epoch-53 batch-85
Running loss of epoch-53 batch-85 = 1.1230353266000748e-05

Training epoch-53 batch-86
Running loss of epoch-53 batch-86 = 8.584465831518173e-06

Training epoch-53 batch-87
Running loss of epoch-53 batch-87 = 9.376788511872292e-06

Training epoch-53 batch-88
Running loss of epoch-53 batch-88 = 7.766298949718475e-06

Training epoch-53 batch-89
Running loss of epoch-53 batch-89 = 4.632631316781044e-06

Training epoch-53 batch-90
Running loss of epoch-53 batch-90 = 1.1759228073060513e-05

Training epoch-53 batch-91
Running loss of epoch-53 batch-91 = 1.1565862223505974e-05

Training epoch-53 batch-92
Running loss of epoch-53 batch-92 = 1.0519754141569138e-05

Training epoch-53 batch-93
Running loss of epoch-53 batch-93 = 9.549083188176155e-06

Training epoch-53 batch-94
Running loss of epoch-53 batch-94 = 5.664536729454994e-06

Training epoch-53 batch-95
Running loss of epoch-53 batch-95 = 1.791096292436123e-05

Training epoch-53 batch-96
Running loss of epoch-53 batch-96 = 8.184928447008133e-06

Training epoch-53 batch-97
Running loss of epoch-53 batch-97 = 1.753983087837696e-05

Training epoch-53 batch-98
Running loss of epoch-53 batch-98 = 6.15767203271389e-06

Training epoch-53 batch-99
Running loss of epoch-53 batch-99 = 1.3693934306502342e-05

Training epoch-53 batch-100
Running loss of epoch-53 batch-100 = 1.1615222319960594e-05

Training epoch-53 batch-101
Running loss of epoch-53 batch-101 = 3.4298282116651535e-06

Training epoch-53 batch-102
Running loss of epoch-53 batch-102 = 5.034962669014931e-06

Training epoch-53 batch-103
Running loss of epoch-53 batch-103 = 9.608687832951546e-06

Training epoch-53 batch-104
Running loss of epoch-53 batch-104 = 5.721813067793846e-06

Training epoch-53 batch-105
Running loss of epoch-53 batch-105 = 9.969109669327736e-06

Training epoch-53 batch-106
Running loss of epoch-53 batch-106 = 1.6529811546206474e-05

Training epoch-53 batch-107
Running loss of epoch-53 batch-107 = 8.406117558479309e-06

Training epoch-53 batch-108
Running loss of epoch-53 batch-108 = 8.7786465883255e-06

Training epoch-53 batch-109
Running loss of epoch-53 batch-109 = 5.920184776186943e-06

Training epoch-53 batch-110
Running loss of epoch-53 batch-110 = 6.3676852732896805e-06

Training epoch-53 batch-111
Running loss of epoch-53 batch-111 = 1.0297400876879692e-05

Training epoch-53 batch-112
Running loss of epoch-53 batch-112 = 1.0162359103560448e-05

Training epoch-53 batch-113
Running loss of epoch-53 batch-113 = 9.620795026421547e-06

Training epoch-53 batch-114
Running loss of epoch-53 batch-114 = 5.906214937567711e-06

Training epoch-53 batch-115
Running loss of epoch-53 batch-115 = 9.55001451075077e-06

Training epoch-53 batch-116
Running loss of epoch-53 batch-116 = 1.256866380572319e-05

Training epoch-53 batch-117
Running loss of epoch-53 batch-117 = 9.791925549507141e-06

Training epoch-53 batch-118
Running loss of epoch-53 batch-118 = 1.9366387277841568e-05

Training epoch-53 batch-119
Running loss of epoch-53 batch-119 = 5.488051101565361e-06

Training epoch-53 batch-120
Running loss of epoch-53 batch-120 = 1.2598000466823578e-05

Training epoch-53 batch-121
Running loss of epoch-53 batch-121 = 1.1130468919873238e-05

Training epoch-53 batch-122
Running loss of epoch-53 batch-122 = 5.359761416912079e-06

Training epoch-53 batch-123
Running loss of epoch-53 batch-123 = 1.5492551028728485e-05

Training epoch-53 batch-124
Running loss of epoch-53 batch-124 = 2.137618139386177e-06

Training epoch-53 batch-125
Running loss of epoch-53 batch-125 = 5.023786798119545e-06

Training epoch-53 batch-126
Running loss of epoch-53 batch-126 = 7.485970854759216e-06

Training epoch-53 batch-127
Running loss of epoch-53 batch-127 = 9.018927812576294e-06

Training epoch-53 batch-128
Running loss of epoch-53 batch-128 = 6.0389284044504166e-06

Training epoch-53 batch-129
Running loss of epoch-53 batch-129 = 2.061855047941208e-05

Training epoch-53 batch-130
Running loss of epoch-53 batch-130 = 6.573507562279701e-06

Training epoch-53 batch-131
Running loss of epoch-53 batch-131 = 4.489906132221222e-06

Training epoch-53 batch-132
Running loss of epoch-53 batch-132 = 7.123686373233795e-06

Training epoch-53 batch-133
Running loss of epoch-53 batch-133 = 6.395159289240837e-06

Training epoch-53 batch-134
Running loss of epoch-53 batch-134 = 6.305985152721405e-06

Training epoch-53 batch-135
Running loss of epoch-53 batch-135 = 7.754191756248474e-06

Training epoch-53 batch-136
Running loss of epoch-53 batch-136 = 7.087597623467445e-06

Training epoch-53 batch-137
Running loss of epoch-53 batch-137 = 7.69621692597866e-06

Training epoch-53 batch-138
Running loss of epoch-53 batch-138 = 5.4105184972286224e-06

Training epoch-53 batch-139
Running loss of epoch-53 batch-139 = 1.0441290214657784e-05

Training epoch-53 batch-140
Running loss of epoch-53 batch-140 = 9.803567081689835e-06

Training epoch-53 batch-141
Running loss of epoch-53 batch-141 = 5.686655640602112e-06

Training epoch-53 batch-142
Running loss of epoch-53 batch-142 = 3.70037741959095e-06

Training epoch-53 batch-143
Running loss of epoch-53 batch-143 = 1.4735385775566101e-05

Training epoch-53 batch-144
Running loss of epoch-53 batch-144 = 2.7585774660110474e-06

Training epoch-53 batch-145
Running loss of epoch-53 batch-145 = 8.979812264442444e-06

Training epoch-53 batch-146
Running loss of epoch-53 batch-146 = 3.5925768315792084e-06

Training epoch-53 batch-147
Running loss of epoch-53 batch-147 = 7.34720379114151e-06

Training epoch-53 batch-148
Running loss of epoch-53 batch-148 = 1.414143480360508e-05

Training epoch-53 batch-149
Running loss of epoch-53 batch-149 = 4.576053470373154e-06

Training epoch-53 batch-150
Running loss of epoch-53 batch-150 = 7.383525371551514e-06

Training epoch-53 batch-151
Running loss of epoch-53 batch-151 = 3.21841798722744e-06

Training epoch-53 batch-152
Running loss of epoch-53 batch-152 = 3.5392586141824722e-06

Training epoch-53 batch-153
Running loss of epoch-53 batch-153 = 4.555331543087959e-06

Training epoch-53 batch-154
Running loss of epoch-53 batch-154 = 8.012051694095135e-06

Training epoch-53 batch-155
Running loss of epoch-53 batch-155 = 4.302710294723511e-06

Training epoch-53 batch-156
Running loss of epoch-53 batch-156 = 9.112758561968803e-06

Training epoch-53 batch-157
Running loss of epoch-53 batch-157 = 6.471574306488037e-05

Finished training epoch-53.



Average train loss at epoch-53 = 8.842232823371887e-06

Started Evaluation

Average val loss at epoch-53 = 1.1162178396039113

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.64 %
Accuracy for class onCreate is: 91.90 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.16 %
Accuracy for class execute is: 49.40 %
Accuracy for class get is: 72.82 %

Overall Accuracy = 83.58 %

Finished Evaluation



Started training epoch-54


Training epoch-54 batch-1
Running loss of epoch-54 batch-1 = 6.3283368945121765e-06

Training epoch-54 batch-2
Running loss of epoch-54 batch-2 = 6.230315193533897e-06

Training epoch-54 batch-3
Running loss of epoch-54 batch-3 = 1.2108124792575836e-05

Training epoch-54 batch-4
Running loss of epoch-54 batch-4 = 5.471287295222282e-06

Training epoch-54 batch-5
Running loss of epoch-54 batch-5 = 2.327817492187023e-05

Training epoch-54 batch-6
Running loss of epoch-54 batch-6 = 8.65827314555645e-06

Training epoch-54 batch-7
Running loss of epoch-54 batch-7 = 6.8515073508024216e-06

Training epoch-54 batch-8
Running loss of epoch-54 batch-8 = 1.626601442694664e-05

Training epoch-54 batch-9
Running loss of epoch-54 batch-9 = 1.0783318430185318e-05

Training epoch-54 batch-10
Running loss of epoch-54 batch-10 = 8.318573236465454e-06

Training epoch-54 batch-11
Running loss of epoch-54 batch-11 = 2.0363135263323784e-05

Training epoch-54 batch-12
Running loss of epoch-54 batch-12 = 1.092487946152687e-05

Training epoch-54 batch-13
Running loss of epoch-54 batch-13 = 5.1353126764297485e-06

Training epoch-54 batch-14
Running loss of epoch-54 batch-14 = 1.3115117326378822e-05

Training epoch-54 batch-15
Running loss of epoch-54 batch-15 = 8.537666872143745e-06

Training epoch-54 batch-16
Running loss of epoch-54 batch-16 = 1.1758878827095032e-05

Training epoch-54 batch-17
Running loss of epoch-54 batch-17 = 7.74976797401905e-06

Training epoch-54 batch-18
Running loss of epoch-54 batch-18 = 7.544411346316338e-06

Training epoch-54 batch-19
Running loss of epoch-54 batch-19 = 1.1211493983864784e-05

Training epoch-54 batch-20
Running loss of epoch-54 batch-20 = 7.409835234284401e-06

Training epoch-54 batch-21
Running loss of epoch-54 batch-21 = 7.048714905977249e-06

Training epoch-54 batch-22
Running loss of epoch-54 batch-22 = 7.494352757930756e-06

Training epoch-54 batch-23
Running loss of epoch-54 batch-23 = 1.5564030036330223e-05

Training epoch-54 batch-24
Running loss of epoch-54 batch-24 = 1.3191485777497292e-05

Training epoch-54 batch-25
Running loss of epoch-54 batch-25 = 7.43684358894825e-06

Training epoch-54 batch-26
Running loss of epoch-54 batch-26 = 4.285946488380432e-06

Training epoch-54 batch-27
Running loss of epoch-54 batch-27 = 7.184222340583801e-06

Training epoch-54 batch-28
Running loss of epoch-54 batch-28 = 4.819827154278755e-06

Training epoch-54 batch-29
Running loss of epoch-54 batch-29 = 9.864801540970802e-06

Training epoch-54 batch-30
Running loss of epoch-54 batch-30 = 7.036840543150902e-06

Training epoch-54 batch-31
Running loss of epoch-54 batch-31 = 1.2807315215468407e-05

Training epoch-54 batch-32
Running loss of epoch-54 batch-32 = 2.564862370491028e-06

Training epoch-54 batch-33
Running loss of epoch-54 batch-33 = 1.0928371921181679e-05

Training epoch-54 batch-34
Running loss of epoch-54 batch-34 = 3.854511305689812e-06

Training epoch-54 batch-35
Running loss of epoch-54 batch-35 = 8.32253135740757e-06

Training epoch-54 batch-36
Running loss of epoch-54 batch-36 = 2.1135201677680016e-05

Training epoch-54 batch-37
Running loss of epoch-54 batch-37 = 2.017104998230934e-05

Training epoch-54 batch-38
Running loss of epoch-54 batch-38 = 1.1867843568325043e-05

Training epoch-54 batch-39
Running loss of epoch-54 batch-39 = 7.4212439358234406e-06

Training epoch-54 batch-40
Running loss of epoch-54 batch-40 = 5.945097655057907e-06

Training epoch-54 batch-41
Running loss of epoch-54 batch-41 = 8.534872904419899e-06

Training epoch-54 batch-42
Running loss of epoch-54 batch-42 = 5.783280357718468e-06

Training epoch-54 batch-43
Running loss of epoch-54 batch-43 = 7.960712537169456e-06

Training epoch-54 batch-44
Running loss of epoch-54 batch-44 = 1.0782619938254356e-05

Training epoch-54 batch-45
Running loss of epoch-54 batch-45 = 4.797009751200676e-06

Training epoch-54 batch-46
Running loss of epoch-54 batch-46 = 8.997973054647446e-06

Training epoch-54 batch-47
Running loss of epoch-54 batch-47 = 5.257781594991684e-06

Training epoch-54 batch-48
Running loss of epoch-54 batch-48 = 7.242197170853615e-06

Training epoch-54 batch-49
Running loss of epoch-54 batch-49 = 5.250563845038414e-06

Training epoch-54 batch-50
Running loss of epoch-54 batch-50 = 9.508104994893074e-06

Training epoch-54 batch-51
Running loss of epoch-54 batch-51 = 6.458023563027382e-06

Training epoch-54 batch-52
Running loss of epoch-54 batch-52 = 6.081303581595421e-06

Training epoch-54 batch-53
Running loss of epoch-54 batch-53 = 6.220769137144089e-06

Training epoch-54 batch-54
Running loss of epoch-54 batch-54 = 6.673857569694519e-06

Training epoch-54 batch-55
Running loss of epoch-54 batch-55 = 6.7730434238910675e-06

Training epoch-54 batch-56
Running loss of epoch-54 batch-56 = 4.897592589259148e-06

Training epoch-54 batch-57
Running loss of epoch-54 batch-57 = 1.803133636713028e-05

Training epoch-54 batch-58
Running loss of epoch-54 batch-58 = 5.574431270360947e-06

Training epoch-54 batch-59
Running loss of epoch-54 batch-59 = 8.64500179886818e-06

Training epoch-54 batch-60
Running loss of epoch-54 batch-60 = 7.132766768336296e-06

Training epoch-54 batch-61
Running loss of epoch-54 batch-61 = 4.964647814631462e-06

Training epoch-54 batch-62
Running loss of epoch-54 batch-62 = 7.001915946602821e-06

Training epoch-54 batch-63
Running loss of epoch-54 batch-63 = 4.907138645648956e-06

Training epoch-54 batch-64
Running loss of epoch-54 batch-64 = 5.01144677400589e-06

Training epoch-54 batch-65
Running loss of epoch-54 batch-65 = 8.112750947475433e-06

Training epoch-54 batch-66
Running loss of epoch-54 batch-66 = 1.2888340279459953e-05

Training epoch-54 batch-67
Running loss of epoch-54 batch-67 = 4.253815859556198e-06

Training epoch-54 batch-68
Running loss of epoch-54 batch-68 = 1.0950258001685143e-05

Training epoch-54 batch-69
Running loss of epoch-54 batch-69 = 1.0252231732010841e-05

Training epoch-54 batch-70
Running loss of epoch-54 batch-70 = 1.2849457561969757e-05

Training epoch-54 batch-71
Running loss of epoch-54 batch-71 = 6.482237949967384e-06

Training epoch-54 batch-72
Running loss of epoch-54 batch-72 = 4.252418875694275e-06

Training epoch-54 batch-73
Running loss of epoch-54 batch-73 = 1.3537239283323288e-05

Training epoch-54 batch-74
Running loss of epoch-54 batch-74 = 1.3302545994520187e-05

Training epoch-54 batch-75
Running loss of epoch-54 batch-75 = 1.9364291802048683e-05

Training epoch-54 batch-76
Running loss of epoch-54 batch-76 = 5.1113311201334e-06

Training epoch-54 batch-77
Running loss of epoch-54 batch-77 = 4.287809133529663e-06

Training epoch-54 batch-78
Running loss of epoch-54 batch-78 = 1.94706954061985e-05

Training epoch-54 batch-79
Running loss of epoch-54 batch-79 = 1.025758683681488e-05

Training epoch-54 batch-80
Running loss of epoch-54 batch-80 = 1.6061589121818542e-05

Training epoch-54 batch-81
Running loss of epoch-54 batch-81 = 5.644513294100761e-06

Training epoch-54 batch-82
Running loss of epoch-54 batch-82 = 7.718801498413086e-06

Training epoch-54 batch-83
Running loss of epoch-54 batch-83 = 6.4123887568712234e-06

Training epoch-54 batch-84
Running loss of epoch-54 batch-84 = 3.809574991464615e-06

Training epoch-54 batch-85
Running loss of epoch-54 batch-85 = 1.3832468539476395e-05

Training epoch-54 batch-86
Running loss of epoch-54 batch-86 = 1.4389166608452797e-05

Training epoch-54 batch-87
Running loss of epoch-54 batch-87 = 7.549067959189415e-06

Training epoch-54 batch-88
Running loss of epoch-54 batch-88 = 6.479676812887192e-06

Training epoch-54 batch-89
Running loss of epoch-54 batch-89 = 5.633803084492683e-06

Training epoch-54 batch-90
Running loss of epoch-54 batch-90 = 4.759756848216057e-06

Training epoch-54 batch-91
Running loss of epoch-54 batch-91 = 3.3527612686157227e-06

Training epoch-54 batch-92
Running loss of epoch-54 batch-92 = 9.896699339151382e-06

Training epoch-54 batch-93
Running loss of epoch-54 batch-93 = 1.0761898010969162e-05

Training epoch-54 batch-94
Running loss of epoch-54 batch-94 = 6.679678335785866e-06

Training epoch-54 batch-95
Running loss of epoch-54 batch-95 = 1.4910707250237465e-05

Training epoch-54 batch-96
Running loss of epoch-54 batch-96 = 6.806571036577225e-06

Training epoch-54 batch-97
Running loss of epoch-54 batch-97 = 7.881084457039833e-06

Training epoch-54 batch-98
Running loss of epoch-54 batch-98 = 4.477100446820259e-06

Training epoch-54 batch-99
Running loss of epoch-54 batch-99 = 2.482673153281212e-06

Training epoch-54 batch-100
Running loss of epoch-54 batch-100 = 7.058028131723404e-06

Training epoch-54 batch-101
Running loss of epoch-54 batch-101 = 1.0359100997447968e-05

Training epoch-54 batch-102
Running loss of epoch-54 batch-102 = 5.110166966915131e-06

Training epoch-54 batch-103
Running loss of epoch-54 batch-103 = 1.2808246538043022e-05

Training epoch-54 batch-104
Running loss of epoch-54 batch-104 = 2.8230715543031693e-06

Training epoch-54 batch-105
Running loss of epoch-54 batch-105 = 8.897623047232628e-06

Training epoch-54 batch-106
Running loss of epoch-54 batch-106 = 1.4552846550941467e-05

Training epoch-54 batch-107
Running loss of epoch-54 batch-107 = 9.031500667333603e-06

Training epoch-54 batch-108
Running loss of epoch-54 batch-108 = 6.7118089646101e-06

Training epoch-54 batch-109
Running loss of epoch-54 batch-109 = 5.964189767837524e-06

Training epoch-54 batch-110
Running loss of epoch-54 batch-110 = 3.7315767258405685e-06

Training epoch-54 batch-111
Running loss of epoch-54 batch-111 = 6.491551175713539e-06

Training epoch-54 batch-112
Running loss of epoch-54 batch-112 = 4.308298230171204e-06

Training epoch-54 batch-113
Running loss of epoch-54 batch-113 = 9.928131476044655e-06

Training epoch-54 batch-114
Running loss of epoch-54 batch-114 = 6.724381819367409e-06

Training epoch-54 batch-115
Running loss of epoch-54 batch-115 = 7.145572453737259e-06

Training epoch-54 batch-116
Running loss of epoch-54 batch-116 = 4.025408998131752e-06

Training epoch-54 batch-117
Running loss of epoch-54 batch-117 = 5.039619281888008e-06

Training epoch-54 batch-118
Running loss of epoch-54 batch-118 = 7.03614205121994e-06

Training epoch-54 batch-119
Running loss of epoch-54 batch-119 = 7.124384865164757e-06

Training epoch-54 batch-120
Running loss of epoch-54 batch-120 = 9.454553946852684e-06

Training epoch-54 batch-121
Running loss of epoch-54 batch-121 = 5.163485184311867e-06

Training epoch-54 batch-122
Running loss of epoch-54 batch-122 = 3.3155083656311035e-06

Training epoch-54 batch-123
Running loss of epoch-54 batch-123 = 4.9103982746601105e-06

Training epoch-54 batch-124
Running loss of epoch-54 batch-124 = 5.302252247929573e-06

Training epoch-54 batch-125
Running loss of epoch-54 batch-125 = 1.0219868272542953e-05

Training epoch-54 batch-126
Running loss of epoch-54 batch-126 = 2.653803676366806e-06

Training epoch-54 batch-127
Running loss of epoch-54 batch-127 = 7.924158126115799e-06

Training epoch-54 batch-128
Running loss of epoch-54 batch-128 = 5.920417606830597e-06

Training epoch-54 batch-129
Running loss of epoch-54 batch-129 = 4.259170964360237e-06

Training epoch-54 batch-130
Running loss of epoch-54 batch-130 = 9.131152182817459e-06

Training epoch-54 batch-131
Running loss of epoch-54 batch-131 = 6.263609975576401e-06

Training epoch-54 batch-132
Running loss of epoch-54 batch-132 = 6.100628525018692e-06

Training epoch-54 batch-133
Running loss of epoch-54 batch-133 = 5.9569720178842545e-06

Training epoch-54 batch-134
Running loss of epoch-54 batch-134 = 1.1832918971776962e-05

Training epoch-54 batch-135
Running loss of epoch-54 batch-135 = 4.5006163418293e-06

Training epoch-54 batch-136
Running loss of epoch-54 batch-136 = 4.383968189358711e-06

Training epoch-54 batch-137
Running loss of epoch-54 batch-137 = 6.5052881836891174e-06

Training epoch-54 batch-138
Running loss of epoch-54 batch-138 = 5.075940862298012e-06

Training epoch-54 batch-139
Running loss of epoch-54 batch-139 = 8.750008419156075e-06

Training epoch-54 batch-140
Running loss of epoch-54 batch-140 = 5.607493221759796e-06

Training epoch-54 batch-141
Running loss of epoch-54 batch-141 = 1.5875091776251793e-05

Training epoch-54 batch-142
Running loss of epoch-54 batch-142 = 1.1151889339089394e-05

Training epoch-54 batch-143
Running loss of epoch-54 batch-143 = 1.0185642167925835e-05

Training epoch-54 batch-144
Running loss of epoch-54 batch-144 = 7.588649168610573e-06

Training epoch-54 batch-145
Running loss of epoch-54 batch-145 = 2.4075619876384735e-05

Training epoch-54 batch-146
Running loss of epoch-54 batch-146 = 6.596790626645088e-06

Training epoch-54 batch-147
Running loss of epoch-54 batch-147 = 6.024027243256569e-06

Training epoch-54 batch-148
Running loss of epoch-54 batch-148 = 1.402897760272026e-05

Training epoch-54 batch-149
Running loss of epoch-54 batch-149 = 4.431465640664101e-06

Training epoch-54 batch-150
Running loss of epoch-54 batch-150 = 1.0200077667832375e-05

Training epoch-54 batch-151
Running loss of epoch-54 batch-151 = 3.2337848097085953e-06

Training epoch-54 batch-152
Running loss of epoch-54 batch-152 = 3.0777882784605026e-06

Training epoch-54 batch-153
Running loss of epoch-54 batch-153 = 7.168622687458992e-06

Training epoch-54 batch-154
Running loss of epoch-54 batch-154 = 4.895729944109917e-06

Training epoch-54 batch-155
Running loss of epoch-54 batch-155 = 1.4570076018571854e-05

Training epoch-54 batch-156
Running loss of epoch-54 batch-156 = 7.081078365445137e-06

Training epoch-54 batch-157
Running loss of epoch-54 batch-157 = 5.1856040954589844e-05

Finished training epoch-54.



Average train loss at epoch-54 = 8.484122157096862e-06

Started Evaluation

Average val loss at epoch-54 = 1.1105369426981102

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 90.98 %
Accuracy for class onCreate is: 91.68 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.98 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.28 %
Accuracy for class execute is: 49.40 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 83.47 %

Finished Evaluation



Started training epoch-55


Training epoch-55 batch-1
Running loss of epoch-55 batch-1 = 2.5727786123752594e-06

Training epoch-55 batch-2
Running loss of epoch-55 batch-2 = 7.73649662733078e-06

Training epoch-55 batch-3
Running loss of epoch-55 batch-3 = 4.903879016637802e-06

Training epoch-55 batch-4
Running loss of epoch-55 batch-4 = 8.480623364448547e-06

Training epoch-55 batch-5
Running loss of epoch-55 batch-5 = 1.0394956916570663e-05

Training epoch-55 batch-6
Running loss of epoch-55 batch-6 = 5.612382665276527e-06

Training epoch-55 batch-7
Running loss of epoch-55 batch-7 = 5.337642505764961e-06

Training epoch-55 batch-8
Running loss of epoch-55 batch-8 = 1.0901596397161484e-05

Training epoch-55 batch-9
Running loss of epoch-55 batch-9 = 1.2437812983989716e-05

Training epoch-55 batch-10
Running loss of epoch-55 batch-10 = 4.327856004238129e-06

Training epoch-55 batch-11
Running loss of epoch-55 batch-11 = 5.15463761985302e-06

Training epoch-55 batch-12
Running loss of epoch-55 batch-12 = 5.702720955014229e-06

Training epoch-55 batch-13
Running loss of epoch-55 batch-13 = 3.6370474845170975e-06

Training epoch-55 batch-14
Running loss of epoch-55 batch-14 = 1.300848089158535e-05

Training epoch-55 batch-15
Running loss of epoch-55 batch-15 = 6.0407910495996475e-06

Training epoch-55 batch-16
Running loss of epoch-55 batch-16 = 5.816342309117317e-06

Training epoch-55 batch-17
Running loss of epoch-55 batch-17 = 4.197470843791962e-06

Training epoch-55 batch-18
Running loss of epoch-55 batch-18 = 4.538102075457573e-06

Training epoch-55 batch-19
Running loss of epoch-55 batch-19 = 1.11432746052742e-05

Training epoch-55 batch-20
Running loss of epoch-55 batch-20 = 5.527399480342865e-06

Training epoch-55 batch-21
Running loss of epoch-55 batch-21 = 1.83845404535532e-05

Training epoch-55 batch-22
Running loss of epoch-55 batch-22 = 5.963258445262909e-06

Training epoch-55 batch-23
Running loss of epoch-55 batch-23 = 6.923684850335121e-06

Training epoch-55 batch-24
Running loss of epoch-55 batch-24 = 5.487818270921707e-06

Training epoch-55 batch-25
Running loss of epoch-55 batch-25 = 6.680609658360481e-06

Training epoch-55 batch-26
Running loss of epoch-55 batch-26 = 1.0405667126178741e-05

Training epoch-55 batch-27
Running loss of epoch-55 batch-27 = 4.556961357593536e-06

Training epoch-55 batch-28
Running loss of epoch-55 batch-28 = 8.787494152784348e-06

Training epoch-55 batch-29
Running loss of epoch-55 batch-29 = 7.58725218474865e-06

Training epoch-55 batch-30
Running loss of epoch-55 batch-30 = 2.862885594367981e-06

Training epoch-55 batch-31
Running loss of epoch-55 batch-31 = 3.405613824725151e-06

Training epoch-55 batch-32
Running loss of epoch-55 batch-32 = 1.4233402907848358e-05

Training epoch-55 batch-33
Running loss of epoch-55 batch-33 = 9.069684892892838e-06

Training epoch-55 batch-34
Running loss of epoch-55 batch-34 = 5.710171535611153e-06

Training epoch-55 batch-35
Running loss of epoch-55 batch-35 = 1.0833609849214554e-05

Training epoch-55 batch-36
Running loss of epoch-55 batch-36 = 5.752779543399811e-06

Training epoch-55 batch-37
Running loss of epoch-55 batch-37 = 1.4985213056206703e-05

Training epoch-55 batch-38
Running loss of epoch-55 batch-38 = 1.2913253158330917e-05

Training epoch-55 batch-39
Running loss of epoch-55 batch-39 = 1.196097582578659e-05

Training epoch-55 batch-40
Running loss of epoch-55 batch-40 = 1.3149343430995941e-05

Training epoch-55 batch-41
Running loss of epoch-55 batch-41 = 1.119193620979786e-05

Training epoch-55 batch-42
Running loss of epoch-55 batch-42 = 5.742302164435387e-06

Training epoch-55 batch-43
Running loss of epoch-55 batch-43 = 6.6785141825675964e-06

Training epoch-55 batch-44
Running loss of epoch-55 batch-44 = 1.2364238500595093e-05

Training epoch-55 batch-45
Running loss of epoch-55 batch-45 = 7.03311525285244e-06

Training epoch-55 batch-46
Running loss of epoch-55 batch-46 = 1.774379052221775e-05

Training epoch-55 batch-47
Running loss of epoch-55 batch-47 = 1.2033386155962944e-05

Training epoch-55 batch-48
Running loss of epoch-55 batch-48 = 8.042901754379272e-06

Training epoch-55 batch-49
Running loss of epoch-55 batch-49 = 3.875698894262314e-06

Training epoch-55 batch-50
Running loss of epoch-55 batch-50 = 4.1334424167871475e-06

Training epoch-55 batch-51
Running loss of epoch-55 batch-51 = 3.7909485399723053e-06

Training epoch-55 batch-52
Running loss of epoch-55 batch-52 = 6.408197805285454e-06

Training epoch-55 batch-53
Running loss of epoch-55 batch-53 = 1.0288087651133537e-05

Training epoch-55 batch-54
Running loss of epoch-55 batch-54 = 1.4981022104620934e-05

Training epoch-55 batch-55
Running loss of epoch-55 batch-55 = 2.2078631445765495e-05

Training epoch-55 batch-56
Running loss of epoch-55 batch-56 = 7.786089554429054e-06

Training epoch-55 batch-57
Running loss of epoch-55 batch-57 = 7.295515388250351e-06

Training epoch-55 batch-58
Running loss of epoch-55 batch-58 = 5.577690899372101e-06

Training epoch-55 batch-59
Running loss of epoch-55 batch-59 = 6.4568594098091125e-06

Training epoch-55 batch-60
Running loss of epoch-55 batch-60 = 5.2908435463905334e-06

Training epoch-55 batch-61
Running loss of epoch-55 batch-61 = 8.568866178393364e-06

Training epoch-55 batch-62
Running loss of epoch-55 batch-62 = 1.2836884707212448e-05

Training epoch-55 batch-63
Running loss of epoch-55 batch-63 = 6.842892616987228e-06

Training epoch-55 batch-64
Running loss of epoch-55 batch-64 = 4.487577825784683e-06

Training epoch-55 batch-65
Running loss of epoch-55 batch-65 = 8.896924555301666e-06

Training epoch-55 batch-66
Running loss of epoch-55 batch-66 = 1.1594733223319054e-05

Training epoch-55 batch-67
Running loss of epoch-55 batch-67 = 5.433103069663048e-06

Training epoch-55 batch-68
Running loss of epoch-55 batch-68 = 1.0480871424078941e-05

Training epoch-55 batch-69
Running loss of epoch-55 batch-69 = 9.26293432712555e-06

Training epoch-55 batch-70
Running loss of epoch-55 batch-70 = 2.957647666335106e-06

Training epoch-55 batch-71
Running loss of epoch-55 batch-71 = 7.0445239543914795e-06

Training epoch-55 batch-72
Running loss of epoch-55 batch-72 = 6.3676852732896805e-06

Training epoch-55 batch-73
Running loss of epoch-55 batch-73 = 9.375158697366714e-06

Training epoch-55 batch-74
Running loss of epoch-55 batch-74 = 1.705833710730076e-05

Training epoch-55 batch-75
Running loss of epoch-55 batch-75 = 4.83216717839241e-06

Training epoch-55 batch-76
Running loss of epoch-55 batch-76 = 1.314864493906498e-05

Training epoch-55 batch-77
Running loss of epoch-55 batch-77 = 8.985865861177444e-06

Training epoch-55 batch-78
Running loss of epoch-55 batch-78 = 6.841262802481651e-06

Training epoch-55 batch-79
Running loss of epoch-55 batch-79 = 4.1441526263952255e-06

Training epoch-55 batch-80
Running loss of epoch-55 batch-80 = 1.712469384074211e-05

Training epoch-55 batch-81
Running loss of epoch-55 batch-81 = 8.67457129061222e-06

Training epoch-55 batch-82
Running loss of epoch-55 batch-82 = 2.0578037947416306e-05

Training epoch-55 batch-83
Running loss of epoch-55 batch-83 = 7.741851732134819e-06

Training epoch-55 batch-84
Running loss of epoch-55 batch-84 = 5.762558430433273e-06

Training epoch-55 batch-85
Running loss of epoch-55 batch-85 = 6.329035386443138e-06

Training epoch-55 batch-86
Running loss of epoch-55 batch-86 = 3.7513673305511475e-06

Training epoch-55 batch-87
Running loss of epoch-55 batch-87 = 5.096662789583206e-06

Training epoch-55 batch-88
Running loss of epoch-55 batch-88 = 6.874324753880501e-06

Training epoch-55 batch-89
Running loss of epoch-55 batch-89 = 6.047077476978302e-06

Training epoch-55 batch-90
Running loss of epoch-55 batch-90 = 8.232193067669868e-06

Training epoch-55 batch-91
Running loss of epoch-55 batch-91 = 6.900401785969734e-06

Training epoch-55 batch-92
Running loss of epoch-55 batch-92 = 4.045199602842331e-06

Training epoch-55 batch-93
Running loss of epoch-55 batch-93 = 7.352791726589203e-06

Training epoch-55 batch-94
Running loss of epoch-55 batch-94 = 4.620291292667389e-06

Training epoch-55 batch-95
Running loss of epoch-55 batch-95 = 3.0195806175470352e-06

Training epoch-55 batch-96
Running loss of epoch-55 batch-96 = 6.46873377263546e-06

Training epoch-55 batch-97
Running loss of epoch-55 batch-97 = 4.511792212724686e-06

Training epoch-55 batch-98
Running loss of epoch-55 batch-98 = 9.814044460654259e-06

Training epoch-55 batch-99
Running loss of epoch-55 batch-99 = 2.8281938284635544e-06

Training epoch-55 batch-100
Running loss of epoch-55 batch-100 = 2.8030481189489365e-06

Training epoch-55 batch-101
Running loss of epoch-55 batch-101 = 4.824483767151833e-06

Training epoch-55 batch-102
Running loss of epoch-55 batch-102 = 5.994690582156181e-06

Training epoch-55 batch-103
Running loss of epoch-55 batch-103 = 5.959998816251755e-06

Training epoch-55 batch-104
Running loss of epoch-55 batch-104 = 1.398986205458641e-05

Training epoch-55 batch-105
Running loss of epoch-55 batch-105 = 7.524155080318451e-06

Training epoch-55 batch-106
Running loss of epoch-55 batch-106 = 1.0709511116147041e-05

Training epoch-55 batch-107
Running loss of epoch-55 batch-107 = 1.2047355994582176e-05

Training epoch-55 batch-108
Running loss of epoch-55 batch-108 = 7.1355607360601425e-06

Training epoch-55 batch-109
Running loss of epoch-55 batch-109 = 6.229151040315628e-06

Training epoch-55 batch-110
Running loss of epoch-55 batch-110 = 7.77142122387886e-06

Training epoch-55 batch-111
Running loss of epoch-55 batch-111 = 6.098533049225807e-06

Training epoch-55 batch-112
Running loss of epoch-55 batch-112 = 4.575122147798538e-06

Training epoch-55 batch-113
Running loss of epoch-55 batch-113 = 1.2288335710763931e-05

Training epoch-55 batch-114
Running loss of epoch-55 batch-114 = 4.532746970653534e-06

Training epoch-55 batch-115
Running loss of epoch-55 batch-115 = 9.736861102283001e-06

Training epoch-55 batch-116
Running loss of epoch-55 batch-116 = 6.633345037698746e-06

Training epoch-55 batch-117
Running loss of epoch-55 batch-117 = 7.382826879620552e-06

Training epoch-55 batch-118
Running loss of epoch-55 batch-118 = 4.31528314948082e-06

Training epoch-55 batch-119
Running loss of epoch-55 batch-119 = 1.2753764167428017e-05

Training epoch-55 batch-120
Running loss of epoch-55 batch-120 = 2.440321259200573e-05

Training epoch-55 batch-121
Running loss of epoch-55 batch-121 = 5.281064659357071e-06

Training epoch-55 batch-122
Running loss of epoch-55 batch-122 = 6.4605847001075745e-06

Training epoch-55 batch-123
Running loss of epoch-55 batch-123 = 8.473871275782585e-06

Training epoch-55 batch-124
Running loss of epoch-55 batch-124 = 4.83635812997818e-06

Training epoch-55 batch-125
Running loss of epoch-55 batch-125 = 2.2481661289930344e-05

Training epoch-55 batch-126
Running loss of epoch-55 batch-126 = 6.026355549693108e-06

Training epoch-55 batch-127
Running loss of epoch-55 batch-127 = 9.742798283696175e-06

Training epoch-55 batch-128
Running loss of epoch-55 batch-128 = 6.897840648889542e-06

Training epoch-55 batch-129
Running loss of epoch-55 batch-129 = 2.960674464702606e-06

Training epoch-55 batch-130
Running loss of epoch-55 batch-130 = 5.495501682162285e-06

Training epoch-55 batch-131
Running loss of epoch-55 batch-131 = 4.031462594866753e-06

Training epoch-55 batch-132
Running loss of epoch-55 batch-132 = 5.828682333230972e-06

Training epoch-55 batch-133
Running loss of epoch-55 batch-133 = 1.4628050848841667e-05

Training epoch-55 batch-134
Running loss of epoch-55 batch-134 = 5.050795152783394e-06

Training epoch-55 batch-135
Running loss of epoch-55 batch-135 = 3.1585805118083954e-06

Training epoch-55 batch-136
Running loss of epoch-55 batch-136 = 9.899493306875229e-06

Training epoch-55 batch-137
Running loss of epoch-55 batch-137 = 8.718809112906456e-06

Training epoch-55 batch-138
Running loss of epoch-55 batch-138 = 7.530907168984413e-06

Training epoch-55 batch-139
Running loss of epoch-55 batch-139 = 6.6335778683424e-06

Training epoch-55 batch-140
Running loss of epoch-55 batch-140 = 7.317168638110161e-06

Training epoch-55 batch-141
Running loss of epoch-55 batch-141 = 1.3115815818309784e-05

Training epoch-55 batch-142
Running loss of epoch-55 batch-142 = 8.547445759177208e-06

Training epoch-55 batch-143
Running loss of epoch-55 batch-143 = 1.059635542333126e-05

Training epoch-55 batch-144
Running loss of epoch-55 batch-144 = 9.25455242395401e-06

Training epoch-55 batch-145
Running loss of epoch-55 batch-145 = 1.587742008268833e-05

Training epoch-55 batch-146
Running loss of epoch-55 batch-146 = 4.341593012213707e-06

Training epoch-55 batch-147
Running loss of epoch-55 batch-147 = 1.0066665709018707e-05

Training epoch-55 batch-148
Running loss of epoch-55 batch-148 = 1.6005942597985268e-05

Training epoch-55 batch-149
Running loss of epoch-55 batch-149 = 7.465248927474022e-06

Training epoch-55 batch-150
Running loss of epoch-55 batch-150 = 5.0943344831466675e-06

Training epoch-55 batch-151
Running loss of epoch-55 batch-151 = 5.713198333978653e-06

Training epoch-55 batch-152
Running loss of epoch-55 batch-152 = 6.907153874635696e-06

Training epoch-55 batch-153
Running loss of epoch-55 batch-153 = 3.0812807381153107e-06

Training epoch-55 batch-154
Running loss of epoch-55 batch-154 = 1.0373303666710854e-05

Training epoch-55 batch-155
Running loss of epoch-55 batch-155 = 7.784925401210785e-06

Training epoch-55 batch-156
Running loss of epoch-55 batch-156 = 5.658483132719994e-06

Training epoch-55 batch-157
Running loss of epoch-55 batch-157 = 6.429851055145264e-06

Finished training epoch-55.



Average train loss at epoch-55 = 8.137651532888413e-06

Started Evaluation

Average val loss at epoch-55 = 1.1155322020075333

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.97 %
Accuracy for class onCreate is: 92.22 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.38 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.62 %

Finished Evaluation



Started training epoch-56


Training epoch-56 batch-1
Running loss of epoch-56 batch-1 = 1.0533258318901062e-05

Training epoch-56 batch-2
Running loss of epoch-56 batch-2 = 7.625436410307884e-06

Training epoch-56 batch-3
Running loss of epoch-56 batch-3 = 1.74851156771183e-05

Training epoch-56 batch-4
Running loss of epoch-56 batch-4 = 9.767012670636177e-06

Training epoch-56 batch-5
Running loss of epoch-56 batch-5 = 4.262896254658699e-06

Training epoch-56 batch-6
Running loss of epoch-56 batch-6 = 2.234126441180706e-05

Training epoch-56 batch-7
Running loss of epoch-56 batch-7 = 7.347436621785164e-06

Training epoch-56 batch-8
Running loss of epoch-56 batch-8 = 1.4846213161945343e-05

Training epoch-56 batch-9
Running loss of epoch-56 batch-9 = 3.966037184000015e-06

Training epoch-56 batch-10
Running loss of epoch-56 batch-10 = 3.6051496863365173e-06

Training epoch-56 batch-11
Running loss of epoch-56 batch-11 = 7.035210728645325e-06

Training epoch-56 batch-12
Running loss of epoch-56 batch-12 = 3.1383242458105087e-06

Training epoch-56 batch-13
Running loss of epoch-56 batch-13 = 3.498513251543045e-06

Training epoch-56 batch-14
Running loss of epoch-56 batch-14 = 5.904817953705788e-06

Training epoch-56 batch-15
Running loss of epoch-56 batch-15 = 8.522765710949898e-06

Training epoch-56 batch-16
Running loss of epoch-56 batch-16 = 6.309710443019867e-06

Training epoch-56 batch-17
Running loss of epoch-56 batch-17 = 5.905516445636749e-06

Training epoch-56 batch-18
Running loss of epoch-56 batch-18 = 1.5998492017388344e-05

Training epoch-56 batch-19
Running loss of epoch-56 batch-19 = 6.046844646334648e-06

Training epoch-56 batch-20
Running loss of epoch-56 batch-20 = 1.764995977282524e-05

Training epoch-56 batch-21
Running loss of epoch-56 batch-21 = 9.52952541410923e-06

Training epoch-56 batch-22
Running loss of epoch-56 batch-22 = 3.4517142921686172e-06

Training epoch-56 batch-23
Running loss of epoch-56 batch-23 = 6.621470674872398e-06

Training epoch-56 batch-24
Running loss of epoch-56 batch-24 = 1.8471619114279747e-05

Training epoch-56 batch-25
Running loss of epoch-56 batch-25 = 4.752771928906441e-06

Training epoch-56 batch-26
Running loss of epoch-56 batch-26 = 1.2907898053526878e-05

Training epoch-56 batch-27
Running loss of epoch-56 batch-27 = 2.598622813820839e-06

Training epoch-56 batch-28
Running loss of epoch-56 batch-28 = 5.153706297278404e-06

Training epoch-56 batch-29
Running loss of epoch-56 batch-29 = 1.0363291949033737e-05

Training epoch-56 batch-30
Running loss of epoch-56 batch-30 = 6.332295015454292e-06

Training epoch-56 batch-31
Running loss of epoch-56 batch-31 = 1.1576106771826744e-05

Training epoch-56 batch-32
Running loss of epoch-56 batch-32 = 1.2835022062063217e-05

Training epoch-56 batch-33
Running loss of epoch-56 batch-33 = 6.763497367501259e-06

Training epoch-56 batch-34
Running loss of epoch-56 batch-34 = 3.968598321080208e-06

Training epoch-56 batch-35
Running loss of epoch-56 batch-35 = 9.157927706837654e-06

Training epoch-56 batch-36
Running loss of epoch-56 batch-36 = 2.432381734251976e-06

Training epoch-56 batch-37
Running loss of epoch-56 batch-37 = 5.71063719689846e-06

Training epoch-56 batch-38
Running loss of epoch-56 batch-38 = 5.5693089962005615e-06

Training epoch-56 batch-39
Running loss of epoch-56 batch-39 = 4.417728632688522e-06

Training epoch-56 batch-40
Running loss of epoch-56 batch-40 = 4.87128272652626e-06

Training epoch-56 batch-41
Running loss of epoch-56 batch-41 = 3.195134922862053e-06

Training epoch-56 batch-42
Running loss of epoch-56 batch-42 = 7.190043106675148e-06

Training epoch-56 batch-43
Running loss of epoch-56 batch-43 = 7.525784894824028e-06

Training epoch-56 batch-44
Running loss of epoch-56 batch-44 = 4.929490387439728e-06

Training epoch-56 batch-45
Running loss of epoch-56 batch-45 = 3.10409814119339e-06

Training epoch-56 batch-46
Running loss of epoch-56 batch-46 = 1.0129064321517944e-05

Training epoch-56 batch-47
Running loss of epoch-56 batch-47 = 2.4759210646152496e-06

Training epoch-56 batch-48
Running loss of epoch-56 batch-48 = 4.004687070846558e-06

Training epoch-56 batch-49
Running loss of epoch-56 batch-49 = 1.9006896764039993e-05

Training epoch-56 batch-50
Running loss of epoch-56 batch-50 = 6.949296221137047e-06

Training epoch-56 batch-51
Running loss of epoch-56 batch-51 = 5.9409067034721375e-06

Training epoch-56 batch-52
Running loss of epoch-56 batch-52 = 5.915528163313866e-06

Training epoch-56 batch-53
Running loss of epoch-56 batch-53 = 4.100147634744644e-06

Training epoch-56 batch-54
Running loss of epoch-56 batch-54 = 1.608859747648239e-05

Training epoch-56 batch-55
Running loss of epoch-56 batch-55 = 8.593080565333366e-06

Training epoch-56 batch-56
Running loss of epoch-56 batch-56 = 8.536968380212784e-06

Training epoch-56 batch-57
Running loss of epoch-56 batch-57 = 8.078990504145622e-06

Training epoch-56 batch-58
Running loss of epoch-56 batch-58 = 5.9138983488082886e-06

Training epoch-56 batch-59
Running loss of epoch-56 batch-59 = 1.6361474990844727e-05

Training epoch-56 batch-60
Running loss of epoch-56 batch-60 = 6.360234692692757e-06

Training epoch-56 batch-61
Running loss of epoch-56 batch-61 = 3.5462435334920883e-06

Training epoch-56 batch-62
Running loss of epoch-56 batch-62 = 5.5085401982069016e-06

Training epoch-56 batch-63
Running loss of epoch-56 batch-63 = 4.084082320332527e-06

Training epoch-56 batch-64
Running loss of epoch-56 batch-64 = 8.286675438284874e-06

Training epoch-56 batch-65
Running loss of epoch-56 batch-65 = 7.806578651070595e-06

Training epoch-56 batch-66
Running loss of epoch-56 batch-66 = 6.5409112721681595e-06

Training epoch-56 batch-67
Running loss of epoch-56 batch-67 = 1.0977499186992645e-05

Training epoch-56 batch-68
Running loss of epoch-56 batch-68 = 5.197711288928986e-06

Training epoch-56 batch-69
Running loss of epoch-56 batch-69 = 9.049195796251297e-06

Training epoch-56 batch-70
Running loss of epoch-56 batch-70 = 3.471970558166504e-06

Training epoch-56 batch-71
Running loss of epoch-56 batch-71 = 5.928566679358482e-06

Training epoch-56 batch-72
Running loss of epoch-56 batch-72 = 1.300545409321785e-05

Training epoch-56 batch-73
Running loss of epoch-56 batch-73 = 9.938841685652733e-06

Training epoch-56 batch-74
Running loss of epoch-56 batch-74 = 1.0120449587702751e-05

Training epoch-56 batch-75
Running loss of epoch-56 batch-75 = 3.5157427191734314e-06

Training epoch-56 batch-76
Running loss of epoch-56 batch-76 = 7.99703411757946e-06

Training epoch-56 batch-77
Running loss of epoch-56 batch-77 = 6.347661837935448e-06

Training epoch-56 batch-78
Running loss of epoch-56 batch-78 = 7.296912372112274e-06

Training epoch-56 batch-79
Running loss of epoch-56 batch-79 = 1.2251082807779312e-05

Training epoch-56 batch-80
Running loss of epoch-56 batch-80 = 1.475936733186245e-05

Training epoch-56 batch-81
Running loss of epoch-56 batch-81 = 9.38749872148037e-06

Training epoch-56 batch-82
Running loss of epoch-56 batch-82 = 1.079123467206955e-05

Training epoch-56 batch-83
Running loss of epoch-56 batch-83 = 5.984678864479065e-06

Training epoch-56 batch-84
Running loss of epoch-56 batch-84 = 6.00842759013176e-06

Training epoch-56 batch-85
Running loss of epoch-56 batch-85 = 9.6119474619627e-06

Training epoch-56 batch-86
Running loss of epoch-56 batch-86 = 8.07526521384716e-06

Training epoch-56 batch-87
Running loss of epoch-56 batch-87 = 4.622619599103928e-06

Training epoch-56 batch-88
Running loss of epoch-56 batch-88 = 3.2265670597553253e-06

Training epoch-56 batch-89
Running loss of epoch-56 batch-89 = 9.605661034584045e-06

Training epoch-56 batch-90
Running loss of epoch-56 batch-90 = 9.757466614246368e-06

Training epoch-56 batch-91
Running loss of epoch-56 batch-91 = 4.823552444577217e-06

Training epoch-56 batch-92
Running loss of epoch-56 batch-92 = 8.69319774210453e-06

Training epoch-56 batch-93
Running loss of epoch-56 batch-93 = 7.932540029287338e-06

Training epoch-56 batch-94
Running loss of epoch-56 batch-94 = 1.0686693713068962e-05

Training epoch-56 batch-95
Running loss of epoch-56 batch-95 = 1.0925810784101486e-05

Training epoch-56 batch-96
Running loss of epoch-56 batch-96 = 1.035444438457489e-05

Training epoch-56 batch-97
Running loss of epoch-56 batch-97 = 9.652692824602127e-06

Training epoch-56 batch-98
Running loss of epoch-56 batch-98 = 1.7226673662662506e-05

Training epoch-56 batch-99
Running loss of epoch-56 batch-99 = 3.128545358777046e-06

Training epoch-56 batch-100
Running loss of epoch-56 batch-100 = 7.439637556672096e-06

Training epoch-56 batch-101
Running loss of epoch-56 batch-101 = 6.5302010625600815e-06

Training epoch-56 batch-102
Running loss of epoch-56 batch-102 = 9.841634891927242e-06

Training epoch-56 batch-103
Running loss of epoch-56 batch-103 = 6.5211206674575806e-06

Training epoch-56 batch-104
Running loss of epoch-56 batch-104 = 7.256865501403809e-06

Training epoch-56 batch-105
Running loss of epoch-56 batch-105 = 1.3623852282762527e-05

Training epoch-56 batch-106
Running loss of epoch-56 batch-106 = 8.075498044490814e-06

Training epoch-56 batch-107
Running loss of epoch-56 batch-107 = 7.198657840490341e-06

Training epoch-56 batch-108
Running loss of epoch-56 batch-108 = 4.403991624712944e-06

Training epoch-56 batch-109
Running loss of epoch-56 batch-109 = 1.140800304710865e-05

Training epoch-56 batch-110
Running loss of epoch-56 batch-110 = 7.38631933927536e-06

Training epoch-56 batch-111
Running loss of epoch-56 batch-111 = 4.657544195652008e-06

Training epoch-56 batch-112
Running loss of epoch-56 batch-112 = 7.932540029287338e-06

Training epoch-56 batch-113
Running loss of epoch-56 batch-113 = 4.119006916880608e-06

Training epoch-56 batch-114
Running loss of epoch-56 batch-114 = 5.936482921242714e-06

Training epoch-56 batch-115
Running loss of epoch-56 batch-115 = 1.0709278285503387e-05

Training epoch-56 batch-116
Running loss of epoch-56 batch-116 = 7.2873663157224655e-06

Training epoch-56 batch-117
Running loss of epoch-56 batch-117 = 6.511807441711426e-06

Training epoch-56 batch-118
Running loss of epoch-56 batch-118 = 5.130656063556671e-06

Training epoch-56 batch-119
Running loss of epoch-56 batch-119 = 4.963716492056847e-06

Training epoch-56 batch-120
Running loss of epoch-56 batch-120 = 8.541624993085861e-06

Training epoch-56 batch-121
Running loss of epoch-56 batch-121 = 7.923459634184837e-06

Training epoch-56 batch-122
Running loss of epoch-56 batch-122 = 9.924639016389847e-06

Training epoch-56 batch-123
Running loss of epoch-56 batch-123 = 5.490146577358246e-06

Training epoch-56 batch-124
Running loss of epoch-56 batch-124 = 9.527429938316345e-06

Training epoch-56 batch-125
Running loss of epoch-56 batch-125 = 8.708564564585686e-06

Training epoch-56 batch-126
Running loss of epoch-56 batch-126 = 7.702037692070007e-06

Training epoch-56 batch-127
Running loss of epoch-56 batch-127 = 7.966533303260803e-06

Training epoch-56 batch-128
Running loss of epoch-56 batch-128 = 9.021488949656487e-06

Training epoch-56 batch-129
Running loss of epoch-56 batch-129 = 4.881061613559723e-06

Training epoch-56 batch-130
Running loss of epoch-56 batch-130 = 1.6579637303948402e-05

Training epoch-56 batch-131
Running loss of epoch-56 batch-131 = 6.135087460279465e-06

Training epoch-56 batch-132
Running loss of epoch-56 batch-132 = 9.370967745780945e-06

Training epoch-56 batch-133
Running loss of epoch-56 batch-133 = 6.348360329866409e-06

Training epoch-56 batch-134
Running loss of epoch-56 batch-134 = 9.438954293727875e-06

Training epoch-56 batch-135
Running loss of epoch-56 batch-135 = 4.432862624526024e-06

Training epoch-56 batch-136
Running loss of epoch-56 batch-136 = 4.884321242570877e-06

Training epoch-56 batch-137
Running loss of epoch-56 batch-137 = 6.044749170541763e-06

Training epoch-56 batch-138
Running loss of epoch-56 batch-138 = 7.94464722275734e-06

Training epoch-56 batch-139
Running loss of epoch-56 batch-139 = 6.643589586019516e-06

Training epoch-56 batch-140
Running loss of epoch-56 batch-140 = 3.6174897104501724e-06

Training epoch-56 batch-141
Running loss of epoch-56 batch-141 = 2.991640940308571e-06

Training epoch-56 batch-142
Running loss of epoch-56 batch-142 = 3.829598426818848e-06

Training epoch-56 batch-143
Running loss of epoch-56 batch-143 = 6.470363587141037e-06

Training epoch-56 batch-144
Running loss of epoch-56 batch-144 = 4.174420610070229e-06

Training epoch-56 batch-145
Running loss of epoch-56 batch-145 = 3.873836249113083e-06

Training epoch-56 batch-146
Running loss of epoch-56 batch-146 = 4.6067871153354645e-06

Training epoch-56 batch-147
Running loss of epoch-56 batch-147 = 1.5264842659235e-05

Training epoch-56 batch-148
Running loss of epoch-56 batch-148 = 6.686197593808174e-06

Training epoch-56 batch-149
Running loss of epoch-56 batch-149 = 9.5427967607975e-06

Training epoch-56 batch-150
Running loss of epoch-56 batch-150 = 7.111579179763794e-06

Training epoch-56 batch-151
Running loss of epoch-56 batch-151 = 1.5412922948598862e-05

Training epoch-56 batch-152
Running loss of epoch-56 batch-152 = 9.822892025113106e-06

Training epoch-56 batch-153
Running loss of epoch-56 batch-153 = 6.4605847001075745e-06

Training epoch-56 batch-154
Running loss of epoch-56 batch-154 = 2.5853514671325684e-06

Training epoch-56 batch-155
Running loss of epoch-56 batch-155 = 4.677101969718933e-06

Training epoch-56 batch-156
Running loss of epoch-56 batch-156 = 8.408445864915848e-06

Training epoch-56 batch-157
Running loss of epoch-56 batch-157 = 4.275515675544739e-05

Finished training epoch-56.



Average train loss at epoch-56 = 7.875018566846848e-06

Started Evaluation

Average val loss at epoch-56 = 1.1185858057013225

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.80 %
Accuracy for class onCreate is: 92.43 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.38 %
Accuracy for class execute is: 49.40 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.64 %

Finished Evaluation



Started training epoch-57


Training epoch-57 batch-1
Running loss of epoch-57 batch-1 = 1.1358410120010376e-05

Training epoch-57 batch-2
Running loss of epoch-57 batch-2 = 3.2694078981876373e-06

Training epoch-57 batch-3
Running loss of epoch-57 batch-3 = 1.5431316569447517e-05

Training epoch-57 batch-4
Running loss of epoch-57 batch-4 = 7.610069587826729e-06

Training epoch-57 batch-5
Running loss of epoch-57 batch-5 = 5.895970389246941e-06

Training epoch-57 batch-6
Running loss of epoch-57 batch-6 = 3.865454345941544e-06

Training epoch-57 batch-7
Running loss of epoch-57 batch-7 = 6.148824468255043e-06

Training epoch-57 batch-8
Running loss of epoch-57 batch-8 = 2.8889626264572144e-06

Training epoch-57 batch-9
Running loss of epoch-57 batch-9 = 4.960689693689346e-06

Training epoch-57 batch-10
Running loss of epoch-57 batch-10 = 6.054295226931572e-06

Training epoch-57 batch-11
Running loss of epoch-57 batch-11 = 4.084315150976181e-06

Training epoch-57 batch-12
Running loss of epoch-57 batch-12 = 2.0586885511875153e-06

Training epoch-57 batch-13
Running loss of epoch-57 batch-13 = 1.2779142707586288e-05

Training epoch-57 batch-14
Running loss of epoch-57 batch-14 = 2.0416686311364174e-05

Training epoch-57 batch-15
Running loss of epoch-57 batch-15 = 5.39049506187439e-06

Training epoch-57 batch-16
Running loss of epoch-57 batch-16 = 7.850350812077522e-06

Training epoch-57 batch-17
Running loss of epoch-57 batch-17 = 6.980262696743011e-07

Training epoch-57 batch-18
Running loss of epoch-57 batch-18 = 8.350471034646034e-06

Training epoch-57 batch-19
Running loss of epoch-57 batch-19 = 2.9117800295352936e-06

Training epoch-57 batch-20
Running loss of epoch-57 batch-20 = 6.328802555799484e-06

Training epoch-57 batch-21
Running loss of epoch-57 batch-21 = 2.173474058508873e-06

Training epoch-57 batch-22
Running loss of epoch-57 batch-22 = 8.671777322888374e-06

Training epoch-57 batch-23
Running loss of epoch-57 batch-23 = 7.452908903360367e-06

Training epoch-57 batch-24
Running loss of epoch-57 batch-24 = 9.167240932583809e-06

Training epoch-57 batch-25
Running loss of epoch-57 batch-25 = 4.442175850272179e-06

Training epoch-57 batch-26
Running loss of epoch-57 batch-26 = 2.2600870579481125e-06

Training epoch-57 batch-27
Running loss of epoch-57 batch-27 = 5.014939233660698e-06

Training epoch-57 batch-28
Running loss of epoch-57 batch-28 = 8.209142833948135e-06

Training epoch-57 batch-29
Running loss of epoch-57 batch-29 = 3.0065421015024185e-06

Training epoch-57 batch-30
Running loss of epoch-57 batch-30 = 1.0709743946790695e-05

Training epoch-57 batch-31
Running loss of epoch-57 batch-31 = 1.5872064977884293e-05

Training epoch-57 batch-32
Running loss of epoch-57 batch-32 = 1.3258541002869606e-05

Training epoch-57 batch-33
Running loss of epoch-57 batch-33 = 5.756039172410965e-06

Training epoch-57 batch-34
Running loss of epoch-57 batch-34 = 9.37492586672306e-06

Training epoch-57 batch-35
Running loss of epoch-57 batch-35 = 3.8458965718746185e-06

Training epoch-57 batch-36
Running loss of epoch-57 batch-36 = 4.388159140944481e-06

Training epoch-57 batch-37
Running loss of epoch-57 batch-37 = 5.8407895267009735e-06

Training epoch-57 batch-38
Running loss of epoch-57 batch-38 = 8.300645276904106e-06

Training epoch-57 batch-39
Running loss of epoch-57 batch-39 = 6.143469363451004e-06

Training epoch-57 batch-40
Running loss of epoch-57 batch-40 = 7.685739547014236e-06

Training epoch-57 batch-41
Running loss of epoch-57 batch-41 = 8.877133950591087e-06

Training epoch-57 batch-42
Running loss of epoch-57 batch-42 = 1.2641306966543198e-05

Training epoch-57 batch-43
Running loss of epoch-57 batch-43 = 4.96511347591877e-06

Training epoch-57 batch-44
Running loss of epoch-57 batch-44 = 1.0187271982431412e-05

Training epoch-57 batch-45
Running loss of epoch-57 batch-45 = 8.269678801298141e-06

Training epoch-57 batch-46
Running loss of epoch-57 batch-46 = 6.977235898375511e-06

Training epoch-57 batch-47
Running loss of epoch-57 batch-47 = 9.011244401335716e-06

Training epoch-57 batch-48
Running loss of epoch-57 batch-48 = 1.339474692940712e-06

Training epoch-57 batch-49
Running loss of epoch-57 batch-49 = 5.3353141993284225e-06

Training epoch-57 batch-50
Running loss of epoch-57 batch-50 = 9.316718205809593e-06

Training epoch-57 batch-51
Running loss of epoch-57 batch-51 = 6.22146762907505e-06

Training epoch-57 batch-52
Running loss of epoch-57 batch-52 = 4.9578957259655e-06

Training epoch-57 batch-53
Running loss of epoch-57 batch-53 = 6.53672032058239e-06

Training epoch-57 batch-54
Running loss of epoch-57 batch-54 = 1.3119308277964592e-05

Training epoch-57 batch-55
Running loss of epoch-57 batch-55 = 8.133240044116974e-06

Training epoch-57 batch-56
Running loss of epoch-57 batch-56 = 3.307359293103218e-06

Training epoch-57 batch-57
Running loss of epoch-57 batch-57 = 7.685273885726929e-06

Training epoch-57 batch-58
Running loss of epoch-57 batch-58 = 6.0230959206819534e-06

Training epoch-57 batch-59
Running loss of epoch-57 batch-59 = 3.270106390118599e-06

Training epoch-57 batch-60
Running loss of epoch-57 batch-60 = 5.059642717242241e-06

Training epoch-57 batch-61
Running loss of epoch-57 batch-61 = 1.0592164471745491e-05

Training epoch-57 batch-62
Running loss of epoch-57 batch-62 = 1.378590241074562e-05

Training epoch-57 batch-63
Running loss of epoch-57 batch-63 = 4.532048478722572e-06

Training epoch-57 batch-64
Running loss of epoch-57 batch-64 = 7.254304364323616e-06

Training epoch-57 batch-65
Running loss of epoch-57 batch-65 = 2.5953631848096848e-06

Training epoch-57 batch-66
Running loss of epoch-57 batch-66 = 1.8269289284944534e-05

Training epoch-57 batch-67
Running loss of epoch-57 batch-67 = 8.138827979564667e-06

Training epoch-57 batch-68
Running loss of epoch-57 batch-68 = 9.223120287060738e-06

Training epoch-57 batch-69
Running loss of epoch-57 batch-69 = 6.5427739173173904e-06

Training epoch-57 batch-70
Running loss of epoch-57 batch-70 = 8.755596354603767e-06

Training epoch-57 batch-71
Running loss of epoch-57 batch-71 = 4.9460213631391525e-06

Training epoch-57 batch-72
Running loss of epoch-57 batch-72 = 2.3367349058389664e-05

Training epoch-57 batch-73
Running loss of epoch-57 batch-73 = 4.620291292667389e-06

Training epoch-57 batch-74
Running loss of epoch-57 batch-74 = 5.304813385009766e-06

Training epoch-57 batch-75
Running loss of epoch-57 batch-75 = 7.863156497478485e-06

Training epoch-57 batch-76
Running loss of epoch-57 batch-76 = 5.143927410244942e-06

Training epoch-57 batch-77
Running loss of epoch-57 batch-77 = 6.031710654497147e-06

Training epoch-57 batch-78
Running loss of epoch-57 batch-78 = 4.108762368559837e-06

Training epoch-57 batch-79
Running loss of epoch-57 batch-79 = 5.052424967288971e-06

Training epoch-57 batch-80
Running loss of epoch-57 batch-80 = 9.002862498164177e-06

Training epoch-57 batch-81
Running loss of epoch-57 batch-81 = 3.88571061193943e-06

Training epoch-57 batch-82
Running loss of epoch-57 batch-82 = 4.395376890897751e-06

Training epoch-57 batch-83
Running loss of epoch-57 batch-83 = 7.479684427380562e-06

Training epoch-57 batch-84
Running loss of epoch-57 batch-84 = 9.401701390743256e-06

Training epoch-57 batch-85
Running loss of epoch-57 batch-85 = 4.643574357032776e-06

Training epoch-57 batch-86
Running loss of epoch-57 batch-86 = 5.818670615553856e-06

Training epoch-57 batch-87
Running loss of epoch-57 batch-87 = 2.302229404449463e-06

Training epoch-57 batch-88
Running loss of epoch-57 batch-88 = 1.7415732145309448e-06

Training epoch-57 batch-89
Running loss of epoch-57 batch-89 = 7.813330739736557e-06

Training epoch-57 batch-90
Running loss of epoch-57 batch-90 = 8.124392479658127e-06

Training epoch-57 batch-91
Running loss of epoch-57 batch-91 = 1.0578194633126259e-05

Training epoch-57 batch-92
Running loss of epoch-57 batch-92 = 6.732530891895294e-06

Training epoch-57 batch-93
Running loss of epoch-57 batch-93 = 9.908340871334076e-06

Training epoch-57 batch-94
Running loss of epoch-57 batch-94 = 8.807750418782234e-06

Training epoch-57 batch-95
Running loss of epoch-57 batch-95 = 7.470371201634407e-06

Training epoch-57 batch-96
Running loss of epoch-57 batch-96 = 1.968350261449814e-06

Training epoch-57 batch-97
Running loss of epoch-57 batch-97 = 4.9674417823553085e-06

Training epoch-57 batch-98
Running loss of epoch-57 batch-98 = 1.0505784302949905e-05

Training epoch-57 batch-99
Running loss of epoch-57 batch-99 = 1.0087387636303902e-05

Training epoch-57 batch-100
Running loss of epoch-57 batch-100 = 1.1366326361894608e-05

Training epoch-57 batch-101
Running loss of epoch-57 batch-101 = 9.654788300395012e-06

Training epoch-57 batch-102
Running loss of epoch-57 batch-102 = 5.832873284816742e-06

Training epoch-57 batch-103
Running loss of epoch-57 batch-103 = 8.198898285627365e-06

Training epoch-57 batch-104
Running loss of epoch-57 batch-104 = 1.0788440704345703e-05

Training epoch-57 batch-105
Running loss of epoch-57 batch-105 = 3.421911969780922e-06

Training epoch-57 batch-106
Running loss of epoch-57 batch-106 = 1.204758882522583e-05

Training epoch-57 batch-107
Running loss of epoch-57 batch-107 = 2.847053110599518e-06

Training epoch-57 batch-108
Running loss of epoch-57 batch-108 = 1.606624573469162e-05

Training epoch-57 batch-109
Running loss of epoch-57 batch-109 = 9.809620678424835e-06

Training epoch-57 batch-110
Running loss of epoch-57 batch-110 = 7.891794666647911e-06

Training epoch-57 batch-111
Running loss of epoch-57 batch-111 = 8.1618782132864e-06

Training epoch-57 batch-112
Running loss of epoch-57 batch-112 = 4.304340109229088e-06

Training epoch-57 batch-113
Running loss of epoch-57 batch-113 = 4.07523475587368e-06

Training epoch-57 batch-114
Running loss of epoch-57 batch-114 = 8.642207831144333e-06

Training epoch-57 batch-115
Running loss of epoch-57 batch-115 = 7.4910931289196014e-06

Training epoch-57 batch-116
Running loss of epoch-57 batch-116 = 8.405419066548347e-06

Training epoch-57 batch-117
Running loss of epoch-57 batch-117 = 4.4673215597867966e-06

Training epoch-57 batch-118
Running loss of epoch-57 batch-118 = 1.0982854291796684e-05

Training epoch-57 batch-119
Running loss of epoch-57 batch-119 = 7.41099938750267e-06

Training epoch-57 batch-120
Running loss of epoch-57 batch-120 = 9.696697816252708e-06

Training epoch-57 batch-121
Running loss of epoch-57 batch-121 = 1.2707896530628204e-05

Training epoch-57 batch-122
Running loss of epoch-57 batch-122 = 6.326008588075638e-06

Training epoch-57 batch-123
Running loss of epoch-57 batch-123 = 6.982823833823204e-06

Training epoch-57 batch-124
Running loss of epoch-57 batch-124 = 7.892027497291565e-06

Training epoch-57 batch-125
Running loss of epoch-57 batch-125 = 5.587469786405563e-06

Training epoch-57 batch-126
Running loss of epoch-57 batch-126 = 3.1918752938508987e-06

Training epoch-57 batch-127
Running loss of epoch-57 batch-127 = 6.911111995577812e-06

Training epoch-57 batch-128
Running loss of epoch-57 batch-128 = 6.353715434670448e-06

Training epoch-57 batch-129
Running loss of epoch-57 batch-129 = 3.5141129046678543e-06

Training epoch-57 batch-130
Running loss of epoch-57 batch-130 = 4.148809239268303e-06

Training epoch-57 batch-131
Running loss of epoch-57 batch-131 = 4.264991730451584e-06

Training epoch-57 batch-132
Running loss of epoch-57 batch-132 = 1.853867433965206e-05

Training epoch-57 batch-133
Running loss of epoch-57 batch-133 = 7.591675966978073e-06

Training epoch-57 batch-134
Running loss of epoch-57 batch-134 = 6.260816007852554e-06

Training epoch-57 batch-135
Running loss of epoch-57 batch-135 = 3.834487870335579e-06

Training epoch-57 batch-136
Running loss of epoch-57 batch-136 = 7.83219002187252e-06

Training epoch-57 batch-137
Running loss of epoch-57 batch-137 = 4.509463906288147e-06

Training epoch-57 batch-138
Running loss of epoch-57 batch-138 = 1.3851094990968704e-05

Training epoch-57 batch-139
Running loss of epoch-57 batch-139 = 1.0217074304819107e-05

Training epoch-57 batch-140
Running loss of epoch-57 batch-140 = 5.081063136458397e-06

Training epoch-57 batch-141
Running loss of epoch-57 batch-141 = 5.531357601284981e-06

Training epoch-57 batch-142
Running loss of epoch-57 batch-142 = 8.736969903111458e-06

Training epoch-57 batch-143
Running loss of epoch-57 batch-143 = 1.1883676052093506e-05

Training epoch-57 batch-144
Running loss of epoch-57 batch-144 = 9.407289326190948e-06

Training epoch-57 batch-145
Running loss of epoch-57 batch-145 = 1.1218944564461708e-05

Training epoch-57 batch-146
Running loss of epoch-57 batch-146 = 7.0142559707164764e-06

Training epoch-57 batch-147
Running loss of epoch-57 batch-147 = 6.545102223753929e-06

Training epoch-57 batch-148
Running loss of epoch-57 batch-148 = 1.2981239706277847e-05

Training epoch-57 batch-149
Running loss of epoch-57 batch-149 = 8.627073839306831e-06

Training epoch-57 batch-150
Running loss of epoch-57 batch-150 = 1.9754748791456223e-05

Training epoch-57 batch-151
Running loss of epoch-57 batch-151 = 6.692251190543175e-06

Training epoch-57 batch-152
Running loss of epoch-57 batch-152 = 4.141591489315033e-06

Training epoch-57 batch-153
Running loss of epoch-57 batch-153 = 4.394445568323135e-06

Training epoch-57 batch-154
Running loss of epoch-57 batch-154 = 8.039874956011772e-06

Training epoch-57 batch-155
Running loss of epoch-57 batch-155 = 9.070383384823799e-06

Training epoch-57 batch-156
Running loss of epoch-57 batch-156 = 7.591675966978073e-06

Training epoch-57 batch-157
Running loss of epoch-57 batch-157 = 7.103011012077332e-05

Finished training epoch-57.



Average train loss at epoch-57 = 7.612630724906921e-06

Started Evaluation

Average val loss at epoch-57 = 1.1173882888320912

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 92.54 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.75 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.28 %
Accuracy for class execute is: 48.59 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.64 %

Finished Evaluation



Started training epoch-58


Training epoch-58 batch-1
Running loss of epoch-58 batch-1 = 5.685724318027496e-06

Training epoch-58 batch-2
Running loss of epoch-58 batch-2 = 4.982342943549156e-06

Training epoch-58 batch-3
Running loss of epoch-58 batch-3 = 6.860354915261269e-06

Training epoch-58 batch-4
Running loss of epoch-58 batch-4 = 1.4329329133033752e-05

Training epoch-58 batch-5
Running loss of epoch-58 batch-5 = 1.5115365386009216e-06

Training epoch-58 batch-6
Running loss of epoch-58 batch-6 = 8.326023817062378e-06

Training epoch-58 batch-7
Running loss of epoch-58 batch-7 = 7.811933755874634e-06

Training epoch-58 batch-8
Running loss of epoch-58 batch-8 = 7.247785106301308e-06

Training epoch-58 batch-9
Running loss of epoch-58 batch-9 = 1.0849209502339363e-05

Training epoch-58 batch-10
Running loss of epoch-58 batch-10 = 1.1224066838622093e-05

Training epoch-58 batch-11
Running loss of epoch-58 batch-11 = 4.526926204562187e-06

Training epoch-58 batch-12
Running loss of epoch-58 batch-12 = 7.36583024263382e-06

Training epoch-58 batch-13
Running loss of epoch-58 batch-13 = 2.1566171199083328e-05

Training epoch-58 batch-14
Running loss of epoch-58 batch-14 = 9.44104976952076e-06

Training epoch-58 batch-15
Running loss of epoch-58 batch-15 = 8.015194907784462e-06

Training epoch-58 batch-16
Running loss of epoch-58 batch-16 = 5.974434316158295e-06

Training epoch-58 batch-17
Running loss of epoch-58 batch-17 = 4.138564690947533e-06

Training epoch-58 batch-18
Running loss of epoch-58 batch-18 = 6.480608135461807e-06

Training epoch-58 batch-19
Running loss of epoch-58 batch-19 = 7.22263939678669e-06

Training epoch-58 batch-20
Running loss of epoch-58 batch-20 = 5.16069121658802e-06

Training epoch-58 batch-21
Running loss of epoch-58 batch-21 = 9.641982614994049e-06

Training epoch-58 batch-22
Running loss of epoch-58 batch-22 = 1.0264106094837189e-05

Training epoch-58 batch-23
Running loss of epoch-58 batch-23 = 4.421686753630638e-06

Training epoch-58 batch-24
Running loss of epoch-58 batch-24 = 8.034752681851387e-06

Training epoch-58 batch-25
Running loss of epoch-58 batch-25 = 1.1439668014645576e-05

Training epoch-58 batch-26
Running loss of epoch-58 batch-26 = 5.69014810025692e-06

Training epoch-58 batch-27
Running loss of epoch-58 batch-27 = 6.156275048851967e-06

Training epoch-58 batch-28
Running loss of epoch-58 batch-28 = 1.2494157999753952e-05

Training epoch-58 batch-29
Running loss of epoch-58 batch-29 = 7.394002750515938e-06

Training epoch-58 batch-30
Running loss of epoch-58 batch-30 = 7.833121344447136e-06

Training epoch-58 batch-31
Running loss of epoch-58 batch-31 = 9.010778740048409e-06

Training epoch-58 batch-32
Running loss of epoch-58 batch-32 = 5.759298801422119e-06

Training epoch-58 batch-33
Running loss of epoch-58 batch-33 = 5.04753552377224e-06

Training epoch-58 batch-34
Running loss of epoch-58 batch-34 = 1.968909054994583e-05

Training epoch-58 batch-35
Running loss of epoch-58 batch-35 = 7.104594260454178e-06

Training epoch-58 batch-36
Running loss of epoch-58 batch-36 = 5.1620882004499435e-06

Training epoch-58 batch-37
Running loss of epoch-58 batch-37 = 1.0972144082188606e-05

Training epoch-58 batch-38
Running loss of epoch-58 batch-38 = 5.698995664715767e-06

Training epoch-58 batch-39
Running loss of epoch-58 batch-39 = 1.0255957022309303e-05

Training epoch-58 batch-40
Running loss of epoch-58 batch-40 = 4.7658104449510574e-06

Training epoch-58 batch-41
Running loss of epoch-58 batch-41 = 8.054543286561966e-06

Training epoch-58 batch-42
Running loss of epoch-58 batch-42 = 7.174676284193993e-06

Training epoch-58 batch-43
Running loss of epoch-58 batch-43 = 1.1624768376350403e-05

Training epoch-58 batch-44
Running loss of epoch-58 batch-44 = 3.4030526876449585e-06

Training epoch-58 batch-45
Running loss of epoch-58 batch-45 = 8.070608600974083e-06

Training epoch-58 batch-46
Running loss of epoch-58 batch-46 = 1.7782440409064293e-05

Training epoch-58 batch-47
Running loss of epoch-58 batch-47 = 5.047302693128586e-06

Training epoch-58 batch-48
Running loss of epoch-58 batch-48 = 6.909249350428581e-06

Training epoch-58 batch-49
Running loss of epoch-58 batch-49 = 5.2647665143013e-06

Training epoch-58 batch-50
Running loss of epoch-58 batch-50 = 6.770947948098183e-06

Training epoch-58 batch-51
Running loss of epoch-58 batch-51 = 4.244502633810043e-06

Training epoch-58 batch-52
Running loss of epoch-58 batch-52 = 4.961621016263962e-06

Training epoch-58 batch-53
Running loss of epoch-58 batch-53 = 6.0908496379852295e-06

Training epoch-58 batch-54
Running loss of epoch-58 batch-54 = 1.0838266462087631e-05

Training epoch-58 batch-55
Running loss of epoch-58 batch-55 = 3.7481077015399933e-06

Training epoch-58 batch-56
Running loss of epoch-58 batch-56 = 4.217727109789848e-06

Training epoch-58 batch-57
Running loss of epoch-58 batch-57 = 7.709488272666931e-06

Training epoch-58 batch-58
Running loss of epoch-58 batch-58 = 7.751630619168282e-06

Training epoch-58 batch-59
Running loss of epoch-58 batch-59 = 3.4943222999572754e-06

Training epoch-58 batch-60
Running loss of epoch-58 batch-60 = 1.2236647307872772e-05

Training epoch-58 batch-61
Running loss of epoch-58 batch-61 = 3.584194928407669e-06

Training epoch-58 batch-62
Running loss of epoch-58 batch-62 = 5.52227720618248e-06

Training epoch-58 batch-63
Running loss of epoch-58 batch-63 = 5.980720743536949e-06

Training epoch-58 batch-64
Running loss of epoch-58 batch-64 = 1.1058058589696884e-05

Training epoch-58 batch-65
Running loss of epoch-58 batch-65 = 8.57585109770298e-06

Training epoch-58 batch-66
Running loss of epoch-58 batch-66 = 5.290377885103226e-06

Training epoch-58 batch-67
Running loss of epoch-58 batch-67 = 8.39773565530777e-06

Training epoch-58 batch-68
Running loss of epoch-58 batch-68 = 5.077570676803589e-06

Training epoch-58 batch-69
Running loss of epoch-58 batch-69 = 1.0064104571938515e-05

Training epoch-58 batch-70
Running loss of epoch-58 batch-70 = 1.050182618200779e-05

Training epoch-58 batch-71
Running loss of epoch-58 batch-71 = 1.0996358469128609e-05

Training epoch-58 batch-72
Running loss of epoch-58 batch-72 = 5.005626007914543e-06

Training epoch-58 batch-73
Running loss of epoch-58 batch-73 = 5.467794835567474e-06

Training epoch-58 batch-74
Running loss of epoch-58 batch-74 = 3.6561395972967148e-06

Training epoch-58 batch-75
Running loss of epoch-58 batch-75 = 1.3087643310427666e-05

Training epoch-58 batch-76
Running loss of epoch-58 batch-76 = 3.621447831392288e-06

Training epoch-58 batch-77
Running loss of epoch-58 batch-77 = 1.0935822501778603e-05

Training epoch-58 batch-78
Running loss of epoch-58 batch-78 = 8.829869329929352e-06

Training epoch-58 batch-79
Running loss of epoch-58 batch-79 = 3.5604462027549744e-06

Training epoch-58 batch-80
Running loss of epoch-58 batch-80 = 3.0354131013154984e-06

Training epoch-58 batch-81
Running loss of epoch-58 batch-81 = 1.1027324944734573e-05

Training epoch-58 batch-82
Running loss of epoch-58 batch-82 = 5.399342626333237e-06

Training epoch-58 batch-83
Running loss of epoch-58 batch-83 = 9.977258741855621e-06

Training epoch-58 batch-84
Running loss of epoch-58 batch-84 = 7.496681064367294e-06

Training epoch-58 batch-85
Running loss of epoch-58 batch-85 = 7.666414603590965e-06

Training epoch-58 batch-86
Running loss of epoch-58 batch-86 = 1.120823435485363e-05

Training epoch-58 batch-87
Running loss of epoch-58 batch-87 = 6.8729277700185776e-06

Training epoch-58 batch-88
Running loss of epoch-58 batch-88 = 3.3141113817691803e-06

Training epoch-58 batch-89
Running loss of epoch-58 batch-89 = 8.830567821860313e-06

Training epoch-58 batch-90
Running loss of epoch-58 batch-90 = 9.362353011965752e-06

Training epoch-58 batch-91
Running loss of epoch-58 batch-91 = 7.1390531957149506e-06

Training epoch-58 batch-92
Running loss of epoch-58 batch-92 = 1.6849953681230545e-06

Training epoch-58 batch-93
Running loss of epoch-58 batch-93 = 6.231712177395821e-06

Training epoch-58 batch-94
Running loss of epoch-58 batch-94 = 1.4811288565397263e-05

Training epoch-58 batch-95
Running loss of epoch-58 batch-95 = 9.035225957632065e-06

Training epoch-58 batch-96
Running loss of epoch-58 batch-96 = 5.7497527450323105e-06

Training epoch-58 batch-97
Running loss of epoch-58 batch-97 = 7.755355909466743e-06

Training epoch-58 batch-98
Running loss of epoch-58 batch-98 = 6.071990355849266e-06

Training epoch-58 batch-99
Running loss of epoch-58 batch-99 = 5.683861672878265e-06

Training epoch-58 batch-100
Running loss of epoch-58 batch-100 = 5.1103997975587845e-06

Training epoch-58 batch-101
Running loss of epoch-58 batch-101 = 2.9136426746845245e-06

Training epoch-58 batch-102
Running loss of epoch-58 batch-102 = 3.866851329803467e-06

Training epoch-58 batch-103
Running loss of epoch-58 batch-103 = 9.143026545643806e-06

Training epoch-58 batch-104
Running loss of epoch-58 batch-104 = 6.520887836813927e-06

Training epoch-58 batch-105
Running loss of epoch-58 batch-105 = 4.301546141505241e-06

Training epoch-58 batch-106
Running loss of epoch-58 batch-106 = 6.644055247306824e-06

Training epoch-58 batch-107
Running loss of epoch-58 batch-107 = 4.105735570192337e-06

Training epoch-58 batch-108
Running loss of epoch-58 batch-108 = 6.116926670074463e-06

Training epoch-58 batch-109
Running loss of epoch-58 batch-109 = 8.785165846347809e-06

Training epoch-58 batch-110
Running loss of epoch-58 batch-110 = 8.06385651230812e-06

Training epoch-58 batch-111
Running loss of epoch-58 batch-111 = 1.4615477994084358e-05

Training epoch-58 batch-112
Running loss of epoch-58 batch-112 = 6.425194442272186e-06

Training epoch-58 batch-113
Running loss of epoch-58 batch-113 = 7.763504981994629e-06

Training epoch-58 batch-114
Running loss of epoch-58 batch-114 = 1.4359597116708755e-05

Training epoch-58 batch-115
Running loss of epoch-58 batch-115 = 4.670117050409317e-06

Training epoch-58 batch-116
Running loss of epoch-58 batch-116 = 5.2924733608961105e-06

Training epoch-58 batch-117
Running loss of epoch-58 batch-117 = 2.434011548757553e-06

Training epoch-58 batch-118
Running loss of epoch-58 batch-118 = 5.662441253662109e-06

Training epoch-58 batch-119
Running loss of epoch-58 batch-119 = 5.200738087296486e-06

Training epoch-58 batch-120
Running loss of epoch-58 batch-120 = 4.489440470933914e-06

Training epoch-58 batch-121
Running loss of epoch-58 batch-121 = 3.364868462085724e-06

Training epoch-58 batch-122
Running loss of epoch-58 batch-122 = 9.003793820738792e-06

Training epoch-58 batch-123
Running loss of epoch-58 batch-123 = 1.0202638804912567e-05

Training epoch-58 batch-124
Running loss of epoch-58 batch-124 = 4.327390342950821e-06

Training epoch-58 batch-125
Running loss of epoch-58 batch-125 = 7.250579074025154e-06

Training epoch-58 batch-126
Running loss of epoch-58 batch-126 = 5.046837031841278e-06

Training epoch-58 batch-127
Running loss of epoch-58 batch-127 = 3.593042492866516e-06

Training epoch-58 batch-128
Running loss of epoch-58 batch-128 = 8.684815838932991e-06

Training epoch-58 batch-129
Running loss of epoch-58 batch-129 = 3.6300625652074814e-06

Training epoch-58 batch-130
Running loss of epoch-58 batch-130 = 1.9979197531938553e-06

Training epoch-58 batch-131
Running loss of epoch-58 batch-131 = 1.8351012840867043e-05

Training epoch-58 batch-132
Running loss of epoch-58 batch-132 = 3.826804459095001e-06

Training epoch-58 batch-133
Running loss of epoch-58 batch-133 = 5.1781535148620605e-06

Training epoch-58 batch-134
Running loss of epoch-58 batch-134 = 5.232170224189758e-06

Training epoch-58 batch-135
Running loss of epoch-58 batch-135 = 5.997717380523682e-06

Training epoch-58 batch-136
Running loss of epoch-58 batch-136 = 4.429370164871216e-06

Training epoch-58 batch-137
Running loss of epoch-58 batch-137 = 5.662906914949417e-06

Training epoch-58 batch-138
Running loss of epoch-58 batch-138 = 9.484821930527687e-06

Training epoch-58 batch-139
Running loss of epoch-58 batch-139 = 7.378170266747475e-06

Training epoch-58 batch-140
Running loss of epoch-58 batch-140 = 7.936032488942146e-06

Training epoch-58 batch-141
Running loss of epoch-58 batch-141 = 8.162111043930054e-06

Training epoch-58 batch-142
Running loss of epoch-58 batch-142 = 1.4620833098888397e-05

Training epoch-58 batch-143
Running loss of epoch-58 batch-143 = 4.717614501714706e-06

Training epoch-58 batch-144
Running loss of epoch-58 batch-144 = 2.8619542717933655e-06

Training epoch-58 batch-145
Running loss of epoch-58 batch-145 = 4.509696736931801e-06

Training epoch-58 batch-146
Running loss of epoch-58 batch-146 = 4.663597792387009e-06

Training epoch-58 batch-147
Running loss of epoch-58 batch-147 = 2.8565991669893265e-06

Training epoch-58 batch-148
Running loss of epoch-58 batch-148 = 3.066146746277809e-06

Training epoch-58 batch-149
Running loss of epoch-58 batch-149 = 3.052176907658577e-06

Training epoch-58 batch-150
Running loss of epoch-58 batch-150 = 2.2801104933023453e-06

Training epoch-58 batch-151
Running loss of epoch-58 batch-151 = 2.11130827665329e-06

Training epoch-58 batch-152
Running loss of epoch-58 batch-152 = 1.130928285419941e-05

Training epoch-58 batch-153
Running loss of epoch-58 batch-153 = 1.4521880075335503e-05

Training epoch-58 batch-154
Running loss of epoch-58 batch-154 = 4.664761945605278e-06

Training epoch-58 batch-155
Running loss of epoch-58 batch-155 = 5.5944547057151794e-06

Training epoch-58 batch-156
Running loss of epoch-58 batch-156 = 8.76234844326973e-06

Training epoch-58 batch-157
Running loss of epoch-58 batch-157 = 1.650303602218628e-05

Finished training epoch-58.



Average train loss at epoch-58 = 7.215774059295654e-06

Started Evaluation

Average val loss at epoch-58 = 1.1405044696776132

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.80 %
Accuracy for class onCreate is: 91.90 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.16 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.58 %

Finished Evaluation



Started training epoch-59


Training epoch-59 batch-1
Running loss of epoch-59 batch-1 = 6.885034963488579e-06

Training epoch-59 batch-2
Running loss of epoch-59 batch-2 = 9.098323062062263e-06

Training epoch-59 batch-3
Running loss of epoch-59 batch-3 = 1.0249204933643341e-05

Training epoch-59 batch-4
Running loss of epoch-59 batch-4 = 8.173054084181786e-06

Training epoch-59 batch-5
Running loss of epoch-59 batch-5 = 5.624489858746529e-06

Training epoch-59 batch-6
Running loss of epoch-59 batch-6 = 6.01867213845253e-06

Training epoch-59 batch-7
Running loss of epoch-59 batch-7 = 3.536231815814972e-06

Training epoch-59 batch-8
Running loss of epoch-59 batch-8 = 1.7072539776563644e-05

Training epoch-59 batch-9
Running loss of epoch-59 batch-9 = 6.195856258273125e-06

Training epoch-59 batch-10
Running loss of epoch-59 batch-10 = 7.82916322350502e-06

Training epoch-59 batch-11
Running loss of epoch-59 batch-11 = 2.061249688267708e-06

Training epoch-59 batch-12
Running loss of epoch-59 batch-12 = 4.706205800175667e-06

Training epoch-59 batch-13
Running loss of epoch-59 batch-13 = 4.607951268553734e-06

Training epoch-59 batch-14
Running loss of epoch-59 batch-14 = 2.6654452085494995e-06

Training epoch-59 batch-15
Running loss of epoch-59 batch-15 = 5.380483344197273e-06

Training epoch-59 batch-16
Running loss of epoch-59 batch-16 = 6.08246773481369e-06

Training epoch-59 batch-17
Running loss of epoch-59 batch-17 = 5.567213520407677e-06

Training epoch-59 batch-18
Running loss of epoch-59 batch-18 = 8.577480912208557e-06

Training epoch-59 batch-19
Running loss of epoch-59 batch-19 = 8.920906111598015e-06

Training epoch-59 batch-20
Running loss of epoch-59 batch-20 = 8.826376870274544e-06

Training epoch-59 batch-21
Running loss of epoch-59 batch-21 = 5.051027983427048e-06

Training epoch-59 batch-22
Running loss of epoch-59 batch-22 = 6.077112630009651e-06

Training epoch-59 batch-23
Running loss of epoch-59 batch-23 = 3.482680767774582e-06

Training epoch-59 batch-24
Running loss of epoch-59 batch-24 = 8.87666828930378e-06

Training epoch-59 batch-25
Running loss of epoch-59 batch-25 = 5.8354344218969345e-06

Training epoch-59 batch-26
Running loss of epoch-59 batch-26 = 1.344224438071251e-05

Training epoch-59 batch-27
Running loss of epoch-59 batch-27 = 2.1604355424642563e-06

Training epoch-59 batch-28
Running loss of epoch-59 batch-28 = 6.791902706027031e-06

Training epoch-59 batch-29
Running loss of epoch-59 batch-29 = 5.445908755064011e-06

Training epoch-59 batch-30
Running loss of epoch-59 batch-30 = 1.3417564332485199e-05

Training epoch-59 batch-31
Running loss of epoch-59 batch-31 = 3.444962203502655e-06

Training epoch-59 batch-32
Running loss of epoch-59 batch-32 = 5.714595317840576e-06

Training epoch-59 batch-33
Running loss of epoch-59 batch-33 = 2.4971086531877518e-06

Training epoch-59 batch-34
Running loss of epoch-59 batch-34 = 9.0042594820261e-06

Training epoch-59 batch-35
Running loss of epoch-59 batch-35 = 2.8370413929224014e-06

Training epoch-59 batch-36
Running loss of epoch-59 batch-36 = 1.4018965885043144e-05

Training epoch-59 batch-37
Running loss of epoch-59 batch-37 = 2.8850045055150986e-06

Training epoch-59 batch-38
Running loss of epoch-59 batch-38 = 6.101327016949654e-06

Training epoch-59 batch-39
Running loss of epoch-59 batch-39 = 4.234723746776581e-06

Training epoch-59 batch-40
Running loss of epoch-59 batch-40 = 1.0721851140260696e-05

Training epoch-59 batch-41
Running loss of epoch-59 batch-41 = 8.124858140945435e-06

Training epoch-59 batch-42
Running loss of epoch-59 batch-42 = 8.717644959688187e-06

Training epoch-59 batch-43
Running loss of epoch-59 batch-43 = 1.1374242603778839e-05

Training epoch-59 batch-44
Running loss of epoch-59 batch-44 = 1.394469290971756e-05

Training epoch-59 batch-45
Running loss of epoch-59 batch-45 = 4.541594535112381e-06

Training epoch-59 batch-46
Running loss of epoch-59 batch-46 = 9.76724550127983e-07

Training epoch-59 batch-47
Running loss of epoch-59 batch-47 = 5.085021257400513e-06

Training epoch-59 batch-48
Running loss of epoch-59 batch-48 = 9.431270882487297e-06

Training epoch-59 batch-49
Running loss of epoch-59 batch-49 = 8.86828638613224e-06

Training epoch-59 batch-50
Running loss of epoch-59 batch-50 = 1.45698431879282e-05

Training epoch-59 batch-51
Running loss of epoch-59 batch-51 = 7.781432941555977e-06

Training epoch-59 batch-52
Running loss of epoch-59 batch-52 = 6.414251402020454e-06

Training epoch-59 batch-53
Running loss of epoch-59 batch-53 = 6.534857675433159e-06

Training epoch-59 batch-54
Running loss of epoch-59 batch-54 = 5.492707714438438e-06

Training epoch-59 batch-55
Running loss of epoch-59 batch-55 = 4.643574357032776e-06

Training epoch-59 batch-56
Running loss of epoch-59 batch-56 = 1.2165633961558342e-05

Training epoch-59 batch-57
Running loss of epoch-59 batch-57 = 1.1696945875883102e-05

Training epoch-59 batch-58
Running loss of epoch-59 batch-58 = 5.188630893826485e-06

Training epoch-59 batch-59
Running loss of epoch-59 batch-59 = 1.039472408592701e-05

Training epoch-59 batch-60
Running loss of epoch-59 batch-60 = 6.183981895446777e-06

Training epoch-59 batch-61
Running loss of epoch-59 batch-61 = 2.0235544070601463e-05

Training epoch-59 batch-62
Running loss of epoch-59 batch-62 = 3.7797726690769196e-06

Training epoch-59 batch-63
Running loss of epoch-59 batch-63 = 7.262686267495155e-06

Training epoch-59 batch-64
Running loss of epoch-59 batch-64 = 4.0691811591386795e-06

Training epoch-59 batch-65
Running loss of epoch-59 batch-65 = 7.1604736149311066e-06

Training epoch-59 batch-66
Running loss of epoch-59 batch-66 = 4.0424056351184845e-06

Training epoch-59 batch-67
Running loss of epoch-59 batch-67 = 1.140916720032692e-05

Training epoch-59 batch-68
Running loss of epoch-59 batch-68 = 4.843343049287796e-06

Training epoch-59 batch-69
Running loss of epoch-59 batch-69 = 7.037771865725517e-06

Training epoch-59 batch-70
Running loss of epoch-59 batch-70 = 4.728790372610092e-06

Training epoch-59 batch-71
Running loss of epoch-59 batch-71 = 1.0614749044179916e-05

Training epoch-59 batch-72
Running loss of epoch-59 batch-72 = 3.139721229672432e-06

Training epoch-59 batch-73
Running loss of epoch-59 batch-73 = 2.0046019926667213e-05

Training epoch-59 batch-74
Running loss of epoch-59 batch-74 = 6.587011739611626e-06

Training epoch-59 batch-75
Running loss of epoch-59 batch-75 = 4.681525751948357e-06

Training epoch-59 batch-76
Running loss of epoch-59 batch-76 = 7.532769814133644e-06

Training epoch-59 batch-77
Running loss of epoch-59 batch-77 = 9.136507287621498e-06

Training epoch-59 batch-78
Running loss of epoch-59 batch-78 = 7.360475137829781e-06

Training epoch-59 batch-79
Running loss of epoch-59 batch-79 = 3.709690645337105e-06

Training epoch-59 batch-80
Running loss of epoch-59 batch-80 = 6.360234692692757e-06

Training epoch-59 batch-81
Running loss of epoch-59 batch-81 = 8.483417332172394e-06

Training epoch-59 batch-82
Running loss of epoch-59 batch-82 = 3.6191195249557495e-06

Training epoch-59 batch-83
Running loss of epoch-59 batch-83 = 3.824243322014809e-06

Training epoch-59 batch-84
Running loss of epoch-59 batch-84 = 1.2143049389123917e-05

Training epoch-59 batch-85
Running loss of epoch-59 batch-85 = 3.7963036447763443e-06

Training epoch-59 batch-86
Running loss of epoch-59 batch-86 = 1.2554693967103958e-05

Training epoch-59 batch-87
Running loss of epoch-59 batch-87 = 1.0195886716246605e-05

Training epoch-59 batch-88
Running loss of epoch-59 batch-88 = 6.587477400898933e-06

Training epoch-59 batch-89
Running loss of epoch-59 batch-89 = 6.720889359712601e-06

Training epoch-59 batch-90
Running loss of epoch-59 batch-90 = 6.626127287745476e-06

Training epoch-59 batch-91
Running loss of epoch-59 batch-91 = 5.153939127922058e-06

Training epoch-59 batch-92
Running loss of epoch-59 batch-92 = 2.341112121939659e-06

Training epoch-59 batch-93
Running loss of epoch-59 batch-93 = 8.45431350171566e-06

Training epoch-59 batch-94
Running loss of epoch-59 batch-94 = 6.657559424638748e-06

Training epoch-59 batch-95
Running loss of epoch-59 batch-95 = 4.939967766404152e-06

Training epoch-59 batch-96
Running loss of epoch-59 batch-96 = 1.8263235688209534e-06

Training epoch-59 batch-97
Running loss of epoch-59 batch-97 = 3.4857075661420822e-06

Training epoch-59 batch-98
Running loss of epoch-59 batch-98 = 7.72811472415924e-06

Training epoch-59 batch-99
Running loss of epoch-59 batch-99 = 4.967208951711655e-06

Training epoch-59 batch-100
Running loss of epoch-59 batch-100 = 1.0133720934391022e-05

Training epoch-59 batch-101
Running loss of epoch-59 batch-101 = 9.619398042559624e-06

Training epoch-59 batch-102
Running loss of epoch-59 batch-102 = 5.760928615927696e-06

Training epoch-59 batch-103
Running loss of epoch-59 batch-103 = 5.4622069001197815e-06

Training epoch-59 batch-104
Running loss of epoch-59 batch-104 = 5.265465006232262e-06

Training epoch-59 batch-105
Running loss of epoch-59 batch-105 = 5.521578714251518e-06

Training epoch-59 batch-106
Running loss of epoch-59 batch-106 = 1.0801944881677628e-05

Training epoch-59 batch-107
Running loss of epoch-59 batch-107 = 1.0415678843855858e-05

Training epoch-59 batch-108
Running loss of epoch-59 batch-108 = 1.1006137356162071e-05

Training epoch-59 batch-109
Running loss of epoch-59 batch-109 = 6.037531420588493e-06

Training epoch-59 batch-110
Running loss of epoch-59 batch-110 = 8.545815944671631e-06

Training epoch-59 batch-111
Running loss of epoch-59 batch-111 = 6.023328751325607e-06

Training epoch-59 batch-112
Running loss of epoch-59 batch-112 = 4.409812390804291e-06

Training epoch-59 batch-113
Running loss of epoch-59 batch-113 = 8.495757356286049e-06

Training epoch-59 batch-114
Running loss of epoch-59 batch-114 = 5.7604629546403885e-06

Training epoch-59 batch-115
Running loss of epoch-59 batch-115 = 5.12157566845417e-06

Training epoch-59 batch-116
Running loss of epoch-59 batch-116 = 5.226116627454758e-06

Training epoch-59 batch-117
Running loss of epoch-59 batch-117 = 1.0551884770393372e-05

Training epoch-59 batch-118
Running loss of epoch-59 batch-118 = 3.686407580971718e-06

Training epoch-59 batch-119
Running loss of epoch-59 batch-119 = 6.271759048104286e-06

Training epoch-59 batch-120
Running loss of epoch-59 batch-120 = 8.712057024240494e-06

Training epoch-59 batch-121
Running loss of epoch-59 batch-121 = 5.653593689203262e-06

Training epoch-59 batch-122
Running loss of epoch-59 batch-122 = 6.937189027667046e-06

Training epoch-59 batch-123
Running loss of epoch-59 batch-123 = 4.08967025578022e-06

Training epoch-59 batch-124
Running loss of epoch-59 batch-124 = 3.877794370055199e-06

Training epoch-59 batch-125
Running loss of epoch-59 batch-125 = 6.558606401085854e-06

Training epoch-59 batch-126
Running loss of epoch-59 batch-126 = 6.156740710139275e-06

Training epoch-59 batch-127
Running loss of epoch-59 batch-127 = 6.359303370118141e-06

Training epoch-59 batch-128
Running loss of epoch-59 batch-128 = 3.3730175346136093e-06

Training epoch-59 batch-129
Running loss of epoch-59 batch-129 = 1.0270392522215843e-05

Training epoch-59 batch-130
Running loss of epoch-59 batch-130 = 3.768596798181534e-06

Training epoch-59 batch-131
Running loss of epoch-59 batch-131 = 1.3327691704034805e-05

Training epoch-59 batch-132
Running loss of epoch-59 batch-132 = 3.309221938252449e-06

Training epoch-59 batch-133
Running loss of epoch-59 batch-133 = 2.3313332349061966e-06

Training epoch-59 batch-134
Running loss of epoch-59 batch-134 = 1.163245178759098e-05

Training epoch-59 batch-135
Running loss of epoch-59 batch-135 = 3.92179936170578e-06

Training epoch-59 batch-136
Running loss of epoch-59 batch-136 = 2.657761797308922e-06

Training epoch-59 batch-137
Running loss of epoch-59 batch-137 = 4.48920764029026e-06

Training epoch-59 batch-138
Running loss of epoch-59 batch-138 = 7.377937436103821e-06

Training epoch-59 batch-139
Running loss of epoch-59 batch-139 = 9.749317541718483e-06

Training epoch-59 batch-140
Running loss of epoch-59 batch-140 = 7.330905646085739e-06

Training epoch-59 batch-141
Running loss of epoch-59 batch-141 = 8.173752576112747e-06

Training epoch-59 batch-142
Running loss of epoch-59 batch-142 = 4.416098818182945e-06

Training epoch-59 batch-143
Running loss of epoch-59 batch-143 = 3.6535784602165222e-06

Training epoch-59 batch-144
Running loss of epoch-59 batch-144 = 5.85382804274559e-06

Training epoch-59 batch-145
Running loss of epoch-59 batch-145 = 6.383052095770836e-06

Training epoch-59 batch-146
Running loss of epoch-59 batch-146 = 9.29972156882286e-06

Training epoch-59 batch-147
Running loss of epoch-59 batch-147 = 4.653353244066238e-06

Training epoch-59 batch-148
Running loss of epoch-59 batch-148 = 7.013790309429169e-06

Training epoch-59 batch-149
Running loss of epoch-59 batch-149 = 1.4015240594744682e-05

Training epoch-59 batch-150
Running loss of epoch-59 batch-150 = 2.4973414838314056e-06

Training epoch-59 batch-151
Running loss of epoch-59 batch-151 = 1.0496936738491058e-05

Training epoch-59 batch-152
Running loss of epoch-59 batch-152 = 5.032401531934738e-06

Training epoch-59 batch-153
Running loss of epoch-59 batch-153 = 5.573267117142677e-06

Training epoch-59 batch-154
Running loss of epoch-59 batch-154 = 8.624978363513947e-06

Training epoch-59 batch-155
Running loss of epoch-59 batch-155 = 8.389586582779884e-06

Training epoch-59 batch-156
Running loss of epoch-59 batch-156 = 3.6195851862430573e-06

Training epoch-59 batch-157
Running loss of epoch-59 batch-157 = 3.353506326675415e-05

Finished training epoch-59.



Average train loss at epoch-59 = 7.028324902057647e-06

Started Evaluation

Average val loss at epoch-59 = 1.1358085510459965

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.48 %
Accuracy for class onCreate is: 91.68 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.83 %
Accuracy for class execute is: 48.19 %
Accuracy for class get is: 73.85 %

Overall Accuracy = 83.58 %

Finished Evaluation



Started training epoch-60


Training epoch-60 batch-1
Running loss of epoch-60 batch-1 = 3.511318936944008e-06

Training epoch-60 batch-2
Running loss of epoch-60 batch-2 = 1.743203029036522e-06

Training epoch-60 batch-3
Running loss of epoch-60 batch-3 = 8.119968697428703e-06

Training epoch-60 batch-4
Running loss of epoch-60 batch-4 = 1.8093269318342209e-06

Training epoch-60 batch-5
Running loss of epoch-60 batch-5 = 3.3192336559295654e-06

Training epoch-60 batch-6
Running loss of epoch-60 batch-6 = 1.0027317330241203e-05

Training epoch-60 batch-7
Running loss of epoch-60 batch-7 = 5.9157609939575195e-06

Training epoch-60 batch-8
Running loss of epoch-60 batch-8 = 4.7658104449510574e-06

Training epoch-60 batch-9
Running loss of epoch-60 batch-9 = 4.29665669798851e-06

Training epoch-60 batch-10
Running loss of epoch-60 batch-10 = 5.214707925915718e-06

Training epoch-60 batch-11
Running loss of epoch-60 batch-11 = 5.808193236589432e-06

Training epoch-60 batch-12
Running loss of epoch-60 batch-12 = 1.0016374289989471e-05

Training epoch-60 batch-13
Running loss of epoch-60 batch-13 = 6.41820952296257e-06

Training epoch-60 batch-14
Running loss of epoch-60 batch-14 = 8.553266525268555e-06

Training epoch-60 batch-15
Running loss of epoch-60 batch-15 = 3.177439793944359e-06

Training epoch-60 batch-16
Running loss of epoch-60 batch-16 = 2.0954757928848267e-06

Training epoch-60 batch-17
Running loss of epoch-60 batch-17 = 2.6745256036520004e-06

Training epoch-60 batch-18
Running loss of epoch-60 batch-18 = 9.875046089291573e-06

Training epoch-60 batch-19
Running loss of epoch-60 batch-19 = 5.440087988972664e-06

Training epoch-60 batch-20
Running loss of epoch-60 batch-20 = 5.417736247181892e-06

Training epoch-60 batch-21
Running loss of epoch-60 batch-21 = 4.336005076766014e-06

Training epoch-60 batch-22
Running loss of epoch-60 batch-22 = 3.2954849302768707e-06

Training epoch-60 batch-23
Running loss of epoch-60 batch-23 = 4.562782123684883e-06

Training epoch-60 batch-24
Running loss of epoch-60 batch-24 = 6.8051740527153015e-06

Training epoch-60 batch-25
Running loss of epoch-60 batch-25 = 6.01145438849926e-06

Training epoch-60 batch-26
Running loss of epoch-60 batch-26 = 8.576549589633942e-06

Training epoch-60 batch-27
Running loss of epoch-60 batch-27 = 6.091082468628883e-06

Training epoch-60 batch-28
Running loss of epoch-60 batch-28 = 8.841510862112045e-06

Training epoch-60 batch-29
Running loss of epoch-60 batch-29 = 1.0328367352485657e-05

Training epoch-60 batch-30
Running loss of epoch-60 batch-30 = 6.0319434851408005e-06

Training epoch-60 batch-31
Running loss of epoch-60 batch-31 = 6.967922672629356e-06

Training epoch-60 batch-32
Running loss of epoch-60 batch-32 = 1.5238765627145767e-06

Training epoch-60 batch-33
Running loss of epoch-60 batch-33 = 7.981667295098305e-06

Training epoch-60 batch-34
Running loss of epoch-60 batch-34 = 1.2895558029413223e-05

Training epoch-60 batch-35
Running loss of epoch-60 batch-35 = 3.1008385121822357e-06

Training epoch-60 batch-36
Running loss of epoch-60 batch-36 = 2.5329645723104477e-06

Training epoch-60 batch-37
Running loss of epoch-60 batch-37 = 4.678033292293549e-06

Training epoch-60 batch-38
Running loss of epoch-60 batch-38 = 5.044508725404739e-06

Training epoch-60 batch-39
Running loss of epoch-60 batch-39 = 7.3909759521484375e-06

Training epoch-60 batch-40
Running loss of epoch-60 batch-40 = 1.7657876014709473e-06

Training epoch-60 batch-41
Running loss of epoch-60 batch-41 = 5.8710575103759766e-06

Training epoch-60 batch-42
Running loss of epoch-60 batch-42 = 9.481096640229225e-06

Training epoch-60 batch-43
Running loss of epoch-60 batch-43 = 6.581656634807587e-06

Training epoch-60 batch-44
Running loss of epoch-60 batch-44 = 6.569316610693932e-06

Training epoch-60 batch-45
Running loss of epoch-60 batch-45 = 8.025672286748886e-06

Training epoch-60 batch-46
Running loss of epoch-60 batch-46 = 7.195398211479187e-06

Training epoch-60 batch-47
Running loss of epoch-60 batch-47 = 2.7746427804231644e-06

Training epoch-60 batch-48
Running loss of epoch-60 batch-48 = 6.282934918999672e-06

Training epoch-60 batch-49
Running loss of epoch-60 batch-49 = 6.173504516482353e-06

Training epoch-60 batch-50
Running loss of epoch-60 batch-50 = 4.71901148557663e-06

Training epoch-60 batch-51
Running loss of epoch-60 batch-51 = 2.3813918232917786e-06

Training epoch-60 batch-52
Running loss of epoch-60 batch-52 = 2.7467031031847e-06

Training epoch-60 batch-53
Running loss of epoch-60 batch-53 = 1.341640017926693e-05

Training epoch-60 batch-54
Running loss of epoch-60 batch-54 = 4.441011697053909e-06

Training epoch-60 batch-55
Running loss of epoch-60 batch-55 = 6.246846169233322e-06

Training epoch-60 batch-56
Running loss of epoch-60 batch-56 = 7.902970537543297e-06

Training epoch-60 batch-57
Running loss of epoch-60 batch-57 = 5.689216777682304e-06

Training epoch-60 batch-58
Running loss of epoch-60 batch-58 = 2.8640497475862503e-06

Training epoch-60 batch-59
Running loss of epoch-60 batch-59 = 6.855698302388191e-06

Training epoch-60 batch-60
Running loss of epoch-60 batch-60 = 8.969567716121674e-06

Training epoch-60 batch-61
Running loss of epoch-60 batch-61 = 5.501089617609978e-06

Training epoch-60 batch-62
Running loss of epoch-60 batch-62 = 8.726492524147034e-06

Training epoch-60 batch-63
Running loss of epoch-60 batch-63 = 6.19678758084774e-06

Training epoch-60 batch-64
Running loss of epoch-60 batch-64 = 2.6391353458166122e-06

Training epoch-60 batch-65
Running loss of epoch-60 batch-65 = 7.2962138801813126e-06

Training epoch-60 batch-66
Running loss of epoch-60 batch-66 = 4.952074959874153e-06

Training epoch-60 batch-67
Running loss of epoch-60 batch-67 = 1.0784249752759933e-05

Training epoch-60 batch-68
Running loss of epoch-60 batch-68 = 2.2687017917633057e-06

Training epoch-60 batch-69
Running loss of epoch-60 batch-69 = 1.850072294473648e-05

Training epoch-60 batch-70
Running loss of epoch-60 batch-70 = 5.453824996948242e-06

Training epoch-60 batch-71
Running loss of epoch-60 batch-71 = 1.0092509910464287e-05

Training epoch-60 batch-72
Running loss of epoch-60 batch-72 = 6.075948476791382e-06

Training epoch-60 batch-73
Running loss of epoch-60 batch-73 = 4.572328180074692e-06

Training epoch-60 batch-74
Running loss of epoch-60 batch-74 = 6.707152351737022e-06

Training epoch-60 batch-75
Running loss of epoch-60 batch-75 = 4.767905920743942e-06

Training epoch-60 batch-76
Running loss of epoch-60 batch-76 = 9.13906842470169e-06

Training epoch-60 batch-77
Running loss of epoch-60 batch-77 = 1.2170989066362381e-05

Training epoch-60 batch-78
Running loss of epoch-60 batch-78 = 5.736248567700386e-06

Training epoch-60 batch-79
Running loss of epoch-60 batch-79 = 2.020224928855896e-05

Training epoch-60 batch-80
Running loss of epoch-60 batch-80 = 6.28340058028698e-06

Training epoch-60 batch-81
Running loss of epoch-60 batch-81 = 5.323789082467556e-06

Training epoch-60 batch-82
Running loss of epoch-60 batch-82 = 5.5716373026371e-06

Training epoch-60 batch-83
Running loss of epoch-60 batch-83 = 1.1434545740485191e-05

Training epoch-60 batch-84
Running loss of epoch-60 batch-84 = 4.014233127236366e-06

Training epoch-60 batch-85
Running loss of epoch-60 batch-85 = 8.013332262635231e-06

Training epoch-60 batch-86
Running loss of epoch-60 batch-86 = 3.859633579850197e-06

Training epoch-60 batch-87
Running loss of epoch-60 batch-87 = 9.522773325443268e-06

Training epoch-60 batch-88
Running loss of epoch-60 batch-88 = 9.531155228614807e-06

Training epoch-60 batch-89
Running loss of epoch-60 batch-89 = 2.516200765967369e-06

Training epoch-60 batch-90
Running loss of epoch-60 batch-90 = 8.384231477975845e-06

Training epoch-60 batch-91
Running loss of epoch-60 batch-91 = 5.827052518725395e-06

Training epoch-60 batch-92
Running loss of epoch-60 batch-92 = 1.8492573872208595e-05

Training epoch-60 batch-93
Running loss of epoch-60 batch-93 = 5.918322131037712e-06

Training epoch-60 batch-94
Running loss of epoch-60 batch-94 = 1.0177260264754295e-05

Training epoch-60 batch-95
Running loss of epoch-60 batch-95 = 5.3229741752147675e-06

Training epoch-60 batch-96
Running loss of epoch-60 batch-96 = 1.2760516256093979e-05

Training epoch-60 batch-97
Running loss of epoch-60 batch-97 = 3.4517142921686172e-06

Training epoch-60 batch-98
Running loss of epoch-60 batch-98 = 2.6419293135404587e-06

Training epoch-60 batch-99
Running loss of epoch-60 batch-99 = 6.338115781545639e-06

Training epoch-60 batch-100
Running loss of epoch-60 batch-100 = 7.4375420808792114e-06

Training epoch-60 batch-101
Running loss of epoch-60 batch-101 = 6.013549864292145e-06

Training epoch-60 batch-102
Running loss of epoch-60 batch-102 = 8.908798918128014e-06

Training epoch-60 batch-103
Running loss of epoch-60 batch-103 = 5.615642294287682e-06

Training epoch-60 batch-104
Running loss of epoch-60 batch-104 = 7.297378033399582e-06

Training epoch-60 batch-105
Running loss of epoch-60 batch-105 = 3.6046840250492096e-06

Training epoch-60 batch-106
Running loss of epoch-60 batch-106 = 4.443805664777756e-06

Training epoch-60 batch-107
Running loss of epoch-60 batch-107 = 4.075001925230026e-06

Training epoch-60 batch-108
Running loss of epoch-60 batch-108 = 3.1671952456235886e-06

Training epoch-60 batch-109
Running loss of epoch-60 batch-109 = 4.288507625460625e-06

Training epoch-60 batch-110
Running loss of epoch-60 batch-110 = 7.417984306812286e-06

Training epoch-60 batch-111
Running loss of epoch-60 batch-111 = 7.682247087359428e-06

Training epoch-60 batch-112
Running loss of epoch-60 batch-112 = 7.929513230919838e-06

Training epoch-60 batch-113
Running loss of epoch-60 batch-113 = 8.631963282823563e-06

Training epoch-60 batch-114
Running loss of epoch-60 batch-114 = 2.7979258447885513e-06

Training epoch-60 batch-115
Running loss of epoch-60 batch-115 = 5.024950951337814e-06

Training epoch-60 batch-116
Running loss of epoch-60 batch-116 = 3.1474046409130096e-06

Training epoch-60 batch-117
Running loss of epoch-60 batch-117 = 1.7953570932149887e-06

Training epoch-60 batch-118
Running loss of epoch-60 batch-118 = 8.649658411741257e-06

Training epoch-60 batch-119
Running loss of epoch-60 batch-119 = 4.299217835068703e-06

Training epoch-60 batch-120
Running loss of epoch-60 batch-120 = 1.0083429515361786e-05

Training epoch-60 batch-121
Running loss of epoch-60 batch-121 = 1.889117993414402e-05

Training epoch-60 batch-122
Running loss of epoch-60 batch-122 = 5.514128133654594e-06

Training epoch-60 batch-123
Running loss of epoch-60 batch-123 = 5.170237272977829e-06

Training epoch-60 batch-124
Running loss of epoch-60 batch-124 = 3.6659184843301773e-06

Training epoch-60 batch-125
Running loss of epoch-60 batch-125 = 1.9121216610074043e-05

Training epoch-60 batch-126
Running loss of epoch-60 batch-126 = 7.359776645898819e-06

Training epoch-60 batch-127
Running loss of epoch-60 batch-127 = 4.4335611164569855e-06

Training epoch-60 batch-128
Running loss of epoch-60 batch-128 = 1.0358402505517006e-05

Training epoch-60 batch-129
Running loss of epoch-60 batch-129 = 4.246830940246582e-06

Training epoch-60 batch-130
Running loss of epoch-60 batch-130 = 5.767913535237312e-06

Training epoch-60 batch-131
Running loss of epoch-60 batch-131 = 8.18236730992794e-06

Training epoch-60 batch-132
Running loss of epoch-60 batch-132 = 7.299473509192467e-06

Training epoch-60 batch-133
Running loss of epoch-60 batch-133 = 1.0553514584898949e-05

Training epoch-60 batch-134
Running loss of epoch-60 batch-134 = 8.696923032402992e-06

Training epoch-60 batch-135
Running loss of epoch-60 batch-135 = 7.640570402145386e-06

Training epoch-60 batch-136
Running loss of epoch-60 batch-136 = 1.6840174794197083e-05

Training epoch-60 batch-137
Running loss of epoch-60 batch-137 = 8.882023394107819e-06

Training epoch-60 batch-138
Running loss of epoch-60 batch-138 = 1.030508428812027e-05

Training epoch-60 batch-139
Running loss of epoch-60 batch-139 = 4.557892680168152e-06

Training epoch-60 batch-140
Running loss of epoch-60 batch-140 = 4.5781489461660385e-06

Training epoch-60 batch-141
Running loss of epoch-60 batch-141 = 2.5497283786535263e-06

Training epoch-60 batch-142
Running loss of epoch-60 batch-142 = 9.91160050034523e-06

Training epoch-60 batch-143
Running loss of epoch-60 batch-143 = 7.552327588200569e-06

Training epoch-60 batch-144
Running loss of epoch-60 batch-144 = 7.263151928782463e-06

Training epoch-60 batch-145
Running loss of epoch-60 batch-145 = 1.8717488273978233e-05

Training epoch-60 batch-146
Running loss of epoch-60 batch-146 = 8.388655260205269e-06

Training epoch-60 batch-147
Running loss of epoch-60 batch-147 = 1.0828720405697823e-05

Training epoch-60 batch-148
Running loss of epoch-60 batch-148 = 3.4694094210863113e-06

Training epoch-60 batch-149
Running loss of epoch-60 batch-149 = 4.566274583339691e-06

Training epoch-60 batch-150
Running loss of epoch-60 batch-150 = 7.839873433113098e-06

Training epoch-60 batch-151
Running loss of epoch-60 batch-151 = 4.050321877002716e-06

Training epoch-60 batch-152
Running loss of epoch-60 batch-152 = 1.1475523933768272e-05

Training epoch-60 batch-153
Running loss of epoch-60 batch-153 = 5.582114681601524e-06

Training epoch-60 batch-154
Running loss of epoch-60 batch-154 = 7.923692464828491e-06

Training epoch-60 batch-155
Running loss of epoch-60 batch-155 = 8.297385647892952e-06

Training epoch-60 batch-156
Running loss of epoch-60 batch-156 = 3.634486347436905e-06

Training epoch-60 batch-157
Running loss of epoch-60 batch-157 = 2.6006251573562622e-05

Finished training epoch-60.



Average train loss at epoch-60 = 6.862392276525498e-06

Started Evaluation

Average val loss at epoch-60 = 1.1402893626415764

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 91.90 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 50.60 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.49 %

Finished Evaluation



Started training epoch-61


Training epoch-61 batch-1
Running loss of epoch-61 batch-1 = 3.849156200885773e-06

Training epoch-61 batch-2
Running loss of epoch-61 batch-2 = 5.061505362391472e-06

Training epoch-61 batch-3
Running loss of epoch-61 batch-3 = 2.568354830145836e-06

Training epoch-61 batch-4
Running loss of epoch-61 batch-4 = 9.133713319897652e-06

Training epoch-61 batch-5
Running loss of epoch-61 batch-5 = 7.547903805971146e-06

Training epoch-61 batch-6
Running loss of epoch-61 batch-6 = 4.305737093091011e-06

Training epoch-61 batch-7
Running loss of epoch-61 batch-7 = 6.912741810083389e-06

Training epoch-61 batch-8
Running loss of epoch-61 batch-8 = 5.344860255718231e-06

Training epoch-61 batch-9
Running loss of epoch-61 batch-9 = 1.1131400242447853e-05

Training epoch-61 batch-10
Running loss of epoch-61 batch-10 = 3.7481077015399933e-06

Training epoch-61 batch-11
Running loss of epoch-61 batch-11 = 5.604000762104988e-06

Training epoch-61 batch-12
Running loss of epoch-61 batch-12 = 5.934387445449829e-06

Training epoch-61 batch-13
Running loss of epoch-61 batch-13 = 5.679670721292496e-06

Training epoch-61 batch-14
Running loss of epoch-61 batch-14 = 5.5693089962005615e-06

Training epoch-61 batch-15
Running loss of epoch-61 batch-15 = 2.9103830456733704e-06

Training epoch-61 batch-16
Running loss of epoch-61 batch-16 = 6.4051710069179535e-06

Training epoch-61 batch-17
Running loss of epoch-61 batch-17 = 1.0546762496232986e-05

Training epoch-61 batch-18
Running loss of epoch-61 batch-18 = 4.439149051904678e-06

Training epoch-61 batch-19
Running loss of epoch-61 batch-19 = 6.0016755014657974e-06

Training epoch-61 batch-20
Running loss of epoch-61 batch-20 = 5.277106538414955e-06

Training epoch-61 batch-21
Running loss of epoch-61 batch-21 = 6.3141342252492905e-06

Training epoch-61 batch-22
Running loss of epoch-61 batch-22 = 3.209104761481285e-06

Training epoch-61 batch-23
Running loss of epoch-61 batch-23 = 4.237983375787735e-06

Training epoch-61 batch-24
Running loss of epoch-61 batch-24 = 3.0403025448322296e-06

Training epoch-61 batch-25
Running loss of epoch-61 batch-25 = 7.541617378592491e-06

Training epoch-61 batch-26
Running loss of epoch-61 batch-26 = 5.430541932582855e-06

Training epoch-61 batch-27
Running loss of epoch-61 batch-27 = 5.320413038134575e-06

Training epoch-61 batch-28
Running loss of epoch-61 batch-28 = 7.196096703410149e-06

Training epoch-61 batch-29
Running loss of epoch-61 batch-29 = 7.692258805036545e-06

Training epoch-61 batch-30
Running loss of epoch-61 batch-30 = 5.202367901802063e-06

Training epoch-61 batch-31
Running loss of epoch-61 batch-31 = 4.98979352414608e-06

Training epoch-61 batch-32
Running loss of epoch-61 batch-32 = 1.2879259884357452e-05

Training epoch-61 batch-33
Running loss of epoch-61 batch-33 = 6.532180123031139e-06

Training epoch-61 batch-34
Running loss of epoch-61 batch-34 = 5.173031240701675e-06

Training epoch-61 batch-35
Running loss of epoch-61 batch-35 = 1.0640360414981842e-05

Training epoch-61 batch-36
Running loss of epoch-61 batch-36 = 5.75813464820385e-06

Training epoch-61 batch-37
Running loss of epoch-61 batch-37 = 6.137881428003311e-06

Training epoch-61 batch-38
Running loss of epoch-61 batch-38 = 7.010763511061668e-06

Training epoch-61 batch-39
Running loss of epoch-61 batch-39 = 9.563285857439041e-06

Training epoch-61 batch-40
Running loss of epoch-61 batch-40 = 3.3157411962747574e-06

Training epoch-61 batch-41
Running loss of epoch-61 batch-41 = 7.031485438346863e-06

Training epoch-61 batch-42
Running loss of epoch-61 batch-42 = 1.1660158634185791e-05

Training epoch-61 batch-43
Running loss of epoch-61 batch-43 = 7.183291018009186e-06

Training epoch-61 batch-44
Running loss of epoch-61 batch-44 = 3.3853575587272644e-06

Training epoch-61 batch-45
Running loss of epoch-61 batch-45 = 3.898050636053085e-06

Training epoch-61 batch-46
Running loss of epoch-61 batch-46 = 6.270594894886017e-06

Training epoch-61 batch-47
Running loss of epoch-61 batch-47 = 9.780051186680794e-06

Training epoch-61 batch-48
Running loss of epoch-61 batch-48 = 3.545312210917473e-06

Training epoch-61 batch-49
Running loss of epoch-61 batch-49 = 4.967208951711655e-06

Training epoch-61 batch-50
Running loss of epoch-61 batch-50 = 4.270346835255623e-06

Training epoch-61 batch-51
Running loss of epoch-61 batch-51 = 3.7278514355421066e-06

Training epoch-61 batch-52
Running loss of epoch-61 batch-52 = 2.9639340937137604e-06

Training epoch-61 batch-53
Running loss of epoch-61 batch-53 = 4.857778549194336e-06

Training epoch-61 batch-54
Running loss of epoch-61 batch-54 = 7.534166797995567e-06

Training epoch-61 batch-55
Running loss of epoch-61 batch-55 = 3.1008385121822357e-06

Training epoch-61 batch-56
Running loss of epoch-61 batch-56 = 7.20098614692688e-06

Training epoch-61 batch-57
Running loss of epoch-61 batch-57 = 8.383067324757576e-06

Training epoch-61 batch-58
Running loss of epoch-61 batch-58 = 2.5962945073843002e-06

Training epoch-61 batch-59
Running loss of epoch-61 batch-59 = 3.994908183813095e-06

Training epoch-61 batch-60
Running loss of epoch-61 batch-60 = 7.712049409747124e-06

Training epoch-61 batch-61
Running loss of epoch-61 batch-61 = 3.0167866498231888e-06

Training epoch-61 batch-62
Running loss of epoch-61 batch-62 = 2.3937318474054337e-06

Training epoch-61 batch-63
Running loss of epoch-61 batch-63 = 8.726026862859726e-06

Training epoch-61 batch-64
Running loss of epoch-61 batch-64 = 5.797483026981354e-06

Training epoch-61 batch-65
Running loss of epoch-61 batch-65 = 4.187924787402153e-06

Training epoch-61 batch-66
Running loss of epoch-61 batch-66 = 5.3727999329566956e-06

Training epoch-61 batch-67
Running loss of epoch-61 batch-67 = 1.1758645996451378e-05

Training epoch-61 batch-68
Running loss of epoch-61 batch-68 = 4.398636519908905e-06

Training epoch-61 batch-69
Running loss of epoch-61 batch-69 = 4.187459126114845e-06

Training epoch-61 batch-70
Running loss of epoch-61 batch-70 = 5.542067810893059e-06

Training epoch-61 batch-71
Running loss of epoch-61 batch-71 = 6.990274414420128e-06

Training epoch-61 batch-72
Running loss of epoch-61 batch-72 = 5.6175049394369125e-06

Training epoch-61 batch-73
Running loss of epoch-61 batch-73 = 1.0003335773944855e-05

Training epoch-61 batch-74
Running loss of epoch-61 batch-74 = 4.863599315285683e-06

Training epoch-61 batch-75
Running loss of epoch-61 batch-75 = 9.537441655993462e-06

Training epoch-61 batch-76
Running loss of epoch-61 batch-76 = 1.0026618838310242e-05

Training epoch-61 batch-77
Running loss of epoch-61 batch-77 = 6.3891056925058365e-06

Training epoch-61 batch-78
Running loss of epoch-61 batch-78 = 6.386544555425644e-06

Training epoch-61 batch-79
Running loss of epoch-61 batch-79 = 5.6228600442409515e-06

Training epoch-61 batch-80
Running loss of epoch-61 batch-80 = 6.718793883919716e-06

Training epoch-61 batch-81
Running loss of epoch-61 batch-81 = 5.308305844664574e-06

Training epoch-61 batch-82
Running loss of epoch-61 batch-82 = 1.8324237316846848e-05

Training epoch-61 batch-83
Running loss of epoch-61 batch-83 = 6.832880899310112e-06

Training epoch-61 batch-84
Running loss of epoch-61 batch-84 = 3.895722329616547e-06

Training epoch-61 batch-85
Running loss of epoch-61 batch-85 = 1.0067364200949669e-05

Training epoch-61 batch-86
Running loss of epoch-61 batch-86 = 3.4961849451065063e-06

Training epoch-61 batch-87
Running loss of epoch-61 batch-87 = 1.55717134475708e-06

Training epoch-61 batch-88
Running loss of epoch-61 batch-88 = 2.6423949748277664e-06

Training epoch-61 batch-89
Running loss of epoch-61 batch-89 = 4.658708348870277e-06

Training epoch-61 batch-90
Running loss of epoch-61 batch-90 = 2.373591996729374e-05

Training epoch-61 batch-91
Running loss of epoch-61 batch-91 = 7.301103323698044e-06

Training epoch-61 batch-92
Running loss of epoch-61 batch-92 = 9.834300726652145e-06

Training epoch-61 batch-93
Running loss of epoch-61 batch-93 = 7.484108209609985e-06

Training epoch-61 batch-94
Running loss of epoch-61 batch-94 = 3.2063107937574387e-06

Training epoch-61 batch-95
Running loss of epoch-61 batch-95 = 5.783280357718468e-06

Training epoch-61 batch-96
Running loss of epoch-61 batch-96 = 1.1728843674063683e-05

Training epoch-61 batch-97
Running loss of epoch-61 batch-97 = 3.2938551157712936e-06

Training epoch-61 batch-98
Running loss of epoch-61 batch-98 = 2.2908207029104233e-06

Training epoch-61 batch-99
Running loss of epoch-61 batch-99 = 6.804941222071648e-06

Training epoch-61 batch-100
Running loss of epoch-61 batch-100 = 7.758848369121552e-06

Training epoch-61 batch-101
Running loss of epoch-61 batch-101 = 1.237308606505394e-05

Training epoch-61 batch-102
Running loss of epoch-61 batch-102 = 7.0338137447834015e-06

Training epoch-61 batch-103
Running loss of epoch-61 batch-103 = 6.869202479720116e-06

Training epoch-61 batch-104
Running loss of epoch-61 batch-104 = 5.4140109568834305e-06

Training epoch-61 batch-105
Running loss of epoch-61 batch-105 = 1.1132331565022469e-05

Training epoch-61 batch-106
Running loss of epoch-61 batch-106 = 5.136709660291672e-06

Training epoch-61 batch-107
Running loss of epoch-61 batch-107 = 6.848713383078575e-06

Training epoch-61 batch-108
Running loss of epoch-61 batch-108 = 5.802838131785393e-06

Training epoch-61 batch-109
Running loss of epoch-61 batch-109 = 6.348825991153717e-06

Training epoch-61 batch-110
Running loss of epoch-61 batch-110 = 5.489448085427284e-06

Training epoch-61 batch-111
Running loss of epoch-61 batch-111 = 6.7069195210933685e-06

Training epoch-61 batch-112
Running loss of epoch-61 batch-112 = 1.0587042197585106e-05

Training epoch-61 batch-113
Running loss of epoch-61 batch-113 = 1.3252953067421913e-05

Training epoch-61 batch-114
Running loss of epoch-61 batch-114 = 6.2801409512758255e-06

Training epoch-61 batch-115
Running loss of epoch-61 batch-115 = 5.062902346253395e-06

Training epoch-61 batch-116
Running loss of epoch-61 batch-116 = 4.850327968597412e-06

Training epoch-61 batch-117
Running loss of epoch-61 batch-117 = 1.0840361937880516e-05

Training epoch-61 batch-118
Running loss of epoch-61 batch-118 = 3.6749988794326782e-06

Training epoch-61 batch-119
Running loss of epoch-61 batch-119 = 3.8978178054094315e-06

Training epoch-61 batch-120
Running loss of epoch-61 batch-120 = 1.1745141819119453e-05

Training epoch-61 batch-121
Running loss of epoch-61 batch-121 = 1.307344064116478e-05

Training epoch-61 batch-122
Running loss of epoch-61 batch-122 = 7.594004273414612e-06

Training epoch-61 batch-123
Running loss of epoch-61 batch-123 = 3.3427495509386063e-06

Training epoch-61 batch-124
Running loss of epoch-61 batch-124 = 9.383074939250946e-06

Training epoch-61 batch-125
Running loss of epoch-61 batch-125 = 8.211471140384674e-06

Training epoch-61 batch-126
Running loss of epoch-61 batch-126 = 7.502036169171333e-06

Training epoch-61 batch-127
Running loss of epoch-61 batch-127 = 5.467329174280167e-06

Training epoch-61 batch-128
Running loss of epoch-61 batch-128 = 7.806345820426941e-06

Training epoch-61 batch-129
Running loss of epoch-61 batch-129 = 2.4044420570135117e-06

Training epoch-61 batch-130
Running loss of epoch-61 batch-130 = 7.947208359837532e-06

Training epoch-61 batch-131
Running loss of epoch-61 batch-131 = 8.076196536421776e-06

Training epoch-61 batch-132
Running loss of epoch-61 batch-132 = 9.370734915137291e-06

Training epoch-61 batch-133
Running loss of epoch-61 batch-133 = 2.784421667456627e-06

Training epoch-61 batch-134
Running loss of epoch-61 batch-134 = 9.3923881649971e-06

Training epoch-61 batch-135
Running loss of epoch-61 batch-135 = 8.965842425823212e-06

Training epoch-61 batch-136
Running loss of epoch-61 batch-136 = 7.917173206806183e-06

Training epoch-61 batch-137
Running loss of epoch-61 batch-137 = 6.1762984842062e-06

Training epoch-61 batch-138
Running loss of epoch-61 batch-138 = 1.1963536962866783e-05

Training epoch-61 batch-139
Running loss of epoch-61 batch-139 = 4.361150786280632e-06

Training epoch-61 batch-140
Running loss of epoch-61 batch-140 = 6.9623347371816635e-06

Training epoch-61 batch-141
Running loss of epoch-61 batch-141 = 5.035661160945892e-06

Training epoch-61 batch-142
Running loss of epoch-61 batch-142 = 4.503875970840454e-06

Training epoch-61 batch-143
Running loss of epoch-61 batch-143 = 4.482455551624298e-06

Training epoch-61 batch-144
Running loss of epoch-61 batch-144 = 2.7543865144252777e-06

Training epoch-61 batch-145
Running loss of epoch-61 batch-145 = 4.45428304374218e-06

Training epoch-61 batch-146
Running loss of epoch-61 batch-146 = 5.896436050534248e-06

Training epoch-61 batch-147
Running loss of epoch-61 batch-147 = 1.0519986972212791e-05

Training epoch-61 batch-148
Running loss of epoch-61 batch-148 = 4.521803930401802e-06

Training epoch-61 batch-149
Running loss of epoch-61 batch-149 = 4.791188985109329e-06

Training epoch-61 batch-150
Running loss of epoch-61 batch-150 = 6.247078999876976e-06

Training epoch-61 batch-151
Running loss of epoch-61 batch-151 = 6.388407200574875e-06

Training epoch-61 batch-152
Running loss of epoch-61 batch-152 = 3.1208619475364685e-06

Training epoch-61 batch-153
Running loss of epoch-61 batch-153 = 9.12160612642765e-06

Training epoch-61 batch-154
Running loss of epoch-61 batch-154 = 7.39656388759613e-06

Training epoch-61 batch-155
Running loss of epoch-61 batch-155 = 4.906440153717995e-06

Training epoch-61 batch-156
Running loss of epoch-61 batch-156 = 1.1913245543837547e-05

Training epoch-61 batch-157
Running loss of epoch-61 batch-157 = 3.6992132663726807e-06

Finished training epoch-61.



Average train loss at epoch-61 = 6.577868014574051e-06

Started Evaluation

Average val loss at epoch-61 = 1.1409113248557219

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.15 %
Accuracy for class onCreate is: 91.90 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.05 %
Accuracy for class execute is: 48.59 %
Accuracy for class get is: 74.10 %

Overall Accuracy = 83.68 %

Finished Evaluation



Started training epoch-62


Training epoch-62 batch-1
Running loss of epoch-62 batch-1 = 9.766081348061562e-06

Training epoch-62 batch-2
Running loss of epoch-62 batch-2 = 7.716938853263855e-06

Training epoch-62 batch-3
Running loss of epoch-62 batch-3 = 1.909211277961731e-06

Training epoch-62 batch-4
Running loss of epoch-62 batch-4 = 5.555339157581329e-06

Training epoch-62 batch-5
Running loss of epoch-62 batch-5 = 3.405613824725151e-06

Training epoch-62 batch-6
Running loss of epoch-62 batch-6 = 3.7816353142261505e-06

Training epoch-62 batch-7
Running loss of epoch-62 batch-7 = 7.1944668889045715e-06

Training epoch-62 batch-8
Running loss of epoch-62 batch-8 = 4.043104127049446e-06

Training epoch-62 batch-9
Running loss of epoch-62 batch-9 = 7.851514965295792e-06

Training epoch-62 batch-10
Running loss of epoch-62 batch-10 = 4.803529009222984e-06

Training epoch-62 batch-11
Running loss of epoch-62 batch-11 = 1.0210089385509491e-05

Training epoch-62 batch-12
Running loss of epoch-62 batch-12 = 7.249647751450539e-06

Training epoch-62 batch-13
Running loss of epoch-62 batch-13 = 1.0250136256217957e-05

Training epoch-62 batch-14
Running loss of epoch-62 batch-14 = 5.424488335847855e-06

Training epoch-62 batch-15
Running loss of epoch-62 batch-15 = 9.060138836503029e-06

Training epoch-62 batch-16
Running loss of epoch-62 batch-16 = 3.8929283618927e-06

Training epoch-62 batch-17
Running loss of epoch-62 batch-17 = 3.3779069781303406e-06

Training epoch-62 batch-18
Running loss of epoch-62 batch-18 = 9.66968946158886e-06

Training epoch-62 batch-19
Running loss of epoch-62 batch-19 = 3.933673724532127e-06

Training epoch-62 batch-20
Running loss of epoch-62 batch-20 = 4.516914486885071e-06

Training epoch-62 batch-21
Running loss of epoch-62 batch-21 = 4.875706508755684e-06

Training epoch-62 batch-22
Running loss of epoch-62 batch-22 = 7.860362529754639e-06

Training epoch-62 batch-23
Running loss of epoch-62 batch-23 = 5.655456334352493e-06

Training epoch-62 batch-24
Running loss of epoch-62 batch-24 = 8.957693353295326e-06

Training epoch-62 batch-25
Running loss of epoch-62 batch-25 = 8.538365364074707e-06

Training epoch-62 batch-26
Running loss of epoch-62 batch-26 = 3.7422869354486465e-06

Training epoch-62 batch-27
Running loss of epoch-62 batch-27 = 5.481066182255745e-06

Training epoch-62 batch-28
Running loss of epoch-62 batch-28 = 3.770226612687111e-06

Training epoch-62 batch-29
Running loss of epoch-62 batch-29 = 4.010507836937904e-06

Training epoch-62 batch-30
Running loss of epoch-62 batch-30 = 5.333218723535538e-06

Training epoch-62 batch-31
Running loss of epoch-62 batch-31 = 8.01566056907177e-06

Training epoch-62 batch-32
Running loss of epoch-62 batch-32 = 4.816334694623947e-06

Training epoch-62 batch-33
Running loss of epoch-62 batch-33 = 1.591816544532776e-05

Training epoch-62 batch-34
Running loss of epoch-62 batch-34 = 2.9795337468385696e-06

Training epoch-62 batch-35
Running loss of epoch-62 batch-35 = 3.3099204301834106e-06

Training epoch-62 batch-36
Running loss of epoch-62 batch-36 = 6.1800237745046616e-06

Training epoch-62 batch-37
Running loss of epoch-62 batch-37 = 4.488043487071991e-06

Training epoch-62 batch-38
Running loss of epoch-62 batch-38 = 9.902287274599075e-06

Training epoch-62 batch-39
Running loss of epoch-62 batch-39 = 5.559995770454407e-06

Training epoch-62 batch-40
Running loss of epoch-62 batch-40 = 3.682449460029602e-06

Training epoch-62 batch-41
Running loss of epoch-62 batch-41 = 7.553258910775185e-06

Training epoch-62 batch-42
Running loss of epoch-62 batch-42 = 6.680842489004135e-06

Training epoch-62 batch-43
Running loss of epoch-62 batch-43 = 1.153675839304924e-05

Training epoch-62 batch-44
Running loss of epoch-62 batch-44 = 6.91576860845089e-06

Training epoch-62 batch-45
Running loss of epoch-62 batch-45 = 3.863358870148659e-06

Training epoch-62 batch-46
Running loss of epoch-62 batch-46 = 1.5655532479286194e-06

Training epoch-62 batch-47
Running loss of epoch-62 batch-47 = 5.3532421588897705e-06

Training epoch-62 batch-48
Running loss of epoch-62 batch-48 = 5.2461400628089905e-06

Training epoch-62 batch-49
Running loss of epoch-62 batch-49 = 1.1413823813199997e-05

Training epoch-62 batch-50
Running loss of epoch-62 batch-50 = 5.94649463891983e-06

Training epoch-62 batch-51
Running loss of epoch-62 batch-51 = 1.3936543837189674e-05

Training epoch-62 batch-52
Running loss of epoch-62 batch-52 = 2.959510311484337e-06

Training epoch-62 batch-53
Running loss of epoch-62 batch-53 = 9.011942893266678e-06

Training epoch-62 batch-54
Running loss of epoch-62 batch-54 = 9.177252650260925e-06

Training epoch-62 batch-55
Running loss of epoch-62 batch-55 = 3.469642251729965e-06

Training epoch-62 batch-56
Running loss of epoch-62 batch-56 = 6.692949682474136e-06

Training epoch-62 batch-57
Running loss of epoch-62 batch-57 = 7.906230166554451e-06

Training epoch-62 batch-58
Running loss of epoch-62 batch-58 = 4.3502077460289e-06

Training epoch-62 batch-59
Running loss of epoch-62 batch-59 = 8.536968380212784e-06

Training epoch-62 batch-60
Running loss of epoch-62 batch-60 = 8.085975423455238e-06

Training epoch-62 batch-61
Running loss of epoch-62 batch-61 = 4.566274583339691e-06

Training epoch-62 batch-62
Running loss of epoch-62 batch-62 = 4.056142643094063e-06

Training epoch-62 batch-63
Running loss of epoch-62 batch-63 = 8.257804438471794e-06

Training epoch-62 batch-64
Running loss of epoch-62 batch-64 = 4.152301698923111e-06

Training epoch-62 batch-65
Running loss of epoch-62 batch-65 = 9.505543857812881e-06

Training epoch-62 batch-66
Running loss of epoch-62 batch-66 = 4.163011908531189e-06

Training epoch-62 batch-67
Running loss of epoch-62 batch-67 = 5.859648808836937e-06

Training epoch-62 batch-68
Running loss of epoch-62 batch-68 = 5.977693945169449e-06

Training epoch-62 batch-69
Running loss of epoch-62 batch-69 = 1.0162591934204102e-05

Training epoch-62 batch-70
Running loss of epoch-62 batch-70 = 8.160015568137169e-06

Training epoch-62 batch-71
Running loss of epoch-62 batch-71 = 4.184897989034653e-06

Training epoch-62 batch-72
Running loss of epoch-62 batch-72 = 5.305977538228035e-06

Training epoch-62 batch-73
Running loss of epoch-62 batch-73 = 5.779322236776352e-06

Training epoch-62 batch-74
Running loss of epoch-62 batch-74 = 5.548354238271713e-06

Training epoch-62 batch-75
Running loss of epoch-62 batch-75 = 6.348825991153717e-06

Training epoch-62 batch-76
Running loss of epoch-62 batch-76 = 3.846362233161926e-06

Training epoch-62 batch-77
Running loss of epoch-62 batch-77 = 9.607523679733276e-06

Training epoch-62 batch-78
Running loss of epoch-62 batch-78 = 7.088296115398407e-06

Training epoch-62 batch-79
Running loss of epoch-62 batch-79 = 6.427522748708725e-06

Training epoch-62 batch-80
Running loss of epoch-62 batch-80 = 3.950903192162514e-06

Training epoch-62 batch-81
Running loss of epoch-62 batch-81 = 9.487150236964226e-06

Training epoch-62 batch-82
Running loss of epoch-62 batch-82 = 1.011975109577179e-05

Training epoch-62 batch-83
Running loss of epoch-62 batch-83 = 4.449160769581795e-06

Training epoch-62 batch-84
Running loss of epoch-62 batch-84 = 1.0108808055520058e-05

Training epoch-62 batch-85
Running loss of epoch-62 batch-85 = 6.750458851456642e-06

Training epoch-62 batch-86
Running loss of epoch-62 batch-86 = 4.970934242010117e-06

Training epoch-62 batch-87
Running loss of epoch-62 batch-87 = 9.431038051843643e-06

Training epoch-62 batch-88
Running loss of epoch-62 batch-88 = 3.4011900424957275e-06

Training epoch-62 batch-89
Running loss of epoch-62 batch-89 = 5.632173269987106e-06

Training epoch-62 batch-90
Running loss of epoch-62 batch-90 = 5.882931873202324e-06

Training epoch-62 batch-91
Running loss of epoch-62 batch-91 = 7.859664037823677e-06

Training epoch-62 batch-92
Running loss of epoch-62 batch-92 = 3.180466592311859e-06

Training epoch-62 batch-93
Running loss of epoch-62 batch-93 = 2.1918676793575287e-06

Training epoch-62 batch-94
Running loss of epoch-62 batch-94 = 5.634734407067299e-06

Training epoch-62 batch-95
Running loss of epoch-62 batch-95 = 5.035195499658585e-06

Training epoch-62 batch-96
Running loss of epoch-62 batch-96 = 4.4978223741054535e-06

Training epoch-62 batch-97
Running loss of epoch-62 batch-97 = 4.339730367064476e-06

Training epoch-62 batch-98
Running loss of epoch-62 batch-98 = 4.203291609883308e-06

Training epoch-62 batch-99
Running loss of epoch-62 batch-99 = 6.682705134153366e-06

Training epoch-62 batch-100
Running loss of epoch-62 batch-100 = 4.420289769768715e-06

Training epoch-62 batch-101
Running loss of epoch-62 batch-101 = 5.237525328993797e-06

Training epoch-62 batch-102
Running loss of epoch-62 batch-102 = 1.849769614636898e-05

Training epoch-62 batch-103
Running loss of epoch-62 batch-103 = 9.74978320300579e-06

Training epoch-62 batch-104
Running loss of epoch-62 batch-104 = 5.7658180594444275e-06

Training epoch-62 batch-105
Running loss of epoch-62 batch-105 = 9.35862772166729e-06

Training epoch-62 batch-106
Running loss of epoch-62 batch-106 = 6.125308573246002e-06

Training epoch-62 batch-107
Running loss of epoch-62 batch-107 = 5.9409067034721375e-06

Training epoch-62 batch-108
Running loss of epoch-62 batch-108 = 3.0391383916139603e-06

Training epoch-62 batch-109
Running loss of epoch-62 batch-109 = 2.6479829102754593e-06

Training epoch-62 batch-110
Running loss of epoch-62 batch-110 = 7.941387593746185e-06

Training epoch-62 batch-111
Running loss of epoch-62 batch-111 = 7.731607183814049e-06

Training epoch-62 batch-112
Running loss of epoch-62 batch-112 = 1.0623829439282417e-05

Training epoch-62 batch-113
Running loss of epoch-62 batch-113 = 7.488997653126717e-06

Training epoch-62 batch-114
Running loss of epoch-62 batch-114 = 2.7616042643785477e-06

Training epoch-62 batch-115
Running loss of epoch-62 batch-115 = 9.192852303385735e-06

Training epoch-62 batch-116
Running loss of epoch-62 batch-116 = 1.9548460841178894e-06

Training epoch-62 batch-117
Running loss of epoch-62 batch-117 = 6.1122700572013855e-06

Training epoch-62 batch-118
Running loss of epoch-62 batch-118 = 5.668960511684418e-06

Training epoch-62 batch-119
Running loss of epoch-62 batch-119 = 7.826602086424828e-06

Training epoch-62 batch-120
Running loss of epoch-62 batch-120 = 6.461748853325844e-06

Training epoch-62 batch-121
Running loss of epoch-62 batch-121 = 5.5639538913965225e-06

Training epoch-62 batch-122
Running loss of epoch-62 batch-122 = 6.966525688767433e-06

Training epoch-62 batch-123
Running loss of epoch-62 batch-123 = 5.65708614885807e-06

Training epoch-62 batch-124
Running loss of epoch-62 batch-124 = 3.5830307751893997e-06

Training epoch-62 batch-125
Running loss of epoch-62 batch-125 = 2.0440202206373215e-05

Training epoch-62 batch-126
Running loss of epoch-62 batch-126 = 4.708068445324898e-06

Training epoch-62 batch-127
Running loss of epoch-62 batch-127 = 5.31109981238842e-06

Training epoch-62 batch-128
Running loss of epoch-62 batch-128 = 6.89411535859108e-06

Training epoch-62 batch-129
Running loss of epoch-62 batch-129 = 5.036592483520508e-06

Training epoch-62 batch-130
Running loss of epoch-62 batch-130 = 8.424976840615273e-06

Training epoch-62 batch-131
Running loss of epoch-62 batch-131 = 1.10710971057415e-06

Training epoch-62 batch-132
Running loss of epoch-62 batch-132 = 4.618661478161812e-06

Training epoch-62 batch-133
Running loss of epoch-62 batch-133 = 5.1585957407951355e-06

Training epoch-62 batch-134
Running loss of epoch-62 batch-134 = 8.4626954048872e-06

Training epoch-62 batch-135
Running loss of epoch-62 batch-135 = 7.543247193098068e-06

Training epoch-62 batch-136
Running loss of epoch-62 batch-136 = 7.753726094961166e-06

Training epoch-62 batch-137
Running loss of epoch-62 batch-137 = 6.751390174031258e-06

Training epoch-62 batch-138
Running loss of epoch-62 batch-138 = 6.602145731449127e-06

Training epoch-62 batch-139
Running loss of epoch-62 batch-139 = 1.5918631106615067e-06

Training epoch-62 batch-140
Running loss of epoch-62 batch-140 = 4.65824268758297e-06

Training epoch-62 batch-141
Running loss of epoch-62 batch-141 = 6.3998159021139145e-06

Training epoch-62 batch-142
Running loss of epoch-62 batch-142 = 1.3381941244006157e-05

Training epoch-62 batch-143
Running loss of epoch-62 batch-143 = 4.369998350739479e-06

Training epoch-62 batch-144
Running loss of epoch-62 batch-144 = 5.042180418968201e-06

Training epoch-62 batch-145
Running loss of epoch-62 batch-145 = 5.2675604820251465e-06

Training epoch-62 batch-146
Running loss of epoch-62 batch-146 = 5.100388079881668e-06

Training epoch-62 batch-147
Running loss of epoch-62 batch-147 = 1.3132113963365555e-05

Training epoch-62 batch-148
Running loss of epoch-62 batch-148 = 3.3352989703416824e-06

Training epoch-62 batch-149
Running loss of epoch-62 batch-149 = 9.17329452931881e-06

Training epoch-62 batch-150
Running loss of epoch-62 batch-150 = 3.954628482460976e-06

Training epoch-62 batch-151
Running loss of epoch-62 batch-151 = 9.239185601472855e-06

Training epoch-62 batch-152
Running loss of epoch-62 batch-152 = 4.752306267619133e-06

Training epoch-62 batch-153
Running loss of epoch-62 batch-153 = 7.611000910401344e-06

Training epoch-62 batch-154
Running loss of epoch-62 batch-154 = 7.97957181930542e-06

Training epoch-62 batch-155
Running loss of epoch-62 batch-155 = 2.562999725341797e-06

Training epoch-62 batch-156
Running loss of epoch-62 batch-156 = 6.58305361866951e-06

Training epoch-62 batch-157
Running loss of epoch-62 batch-157 = 1.329183578491211e-05

Finished training epoch-62.



Average train loss at epoch-62 = 6.440258026123047e-06

Started Evaluation

Average val loss at epoch-62 = 1.1482905526148222

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 91.48 %
Accuracy for class onCreate is: 91.68 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.83 %
Accuracy for class execute is: 49.40 %
Accuracy for class get is: 72.56 %

Overall Accuracy = 83.58 %

Finished Evaluation



Started training epoch-63


Training epoch-63 batch-1
Running loss of epoch-63 batch-1 = 3.7979334592819214e-06

Training epoch-63 batch-2
Running loss of epoch-63 batch-2 = 1.8193386495113373e-06

Training epoch-63 batch-3
Running loss of epoch-63 batch-3 = 6.229616701602936e-06

Training epoch-63 batch-4
Running loss of epoch-63 batch-4 = 6.5409112721681595e-06

Training epoch-63 batch-5
Running loss of epoch-63 batch-5 = 3.95369715988636e-06

Training epoch-63 batch-6
Running loss of epoch-63 batch-6 = 1.1855736374855042e-05

Training epoch-63 batch-7
Running loss of epoch-63 batch-7 = 4.684319719672203e-06

Training epoch-63 batch-8
Running loss of epoch-63 batch-8 = 2.5355257093906403e-06

Training epoch-63 batch-9
Running loss of epoch-63 batch-9 = 5.134381353855133e-06

Training epoch-63 batch-10
Running loss of epoch-63 batch-10 = 6.081070750951767e-06

Training epoch-63 batch-11
Running loss of epoch-63 batch-11 = 1.1247117072343826e-05

Training epoch-63 batch-12
Running loss of epoch-63 batch-12 = 3.4580007195472717e-06

Training epoch-63 batch-13
Running loss of epoch-63 batch-13 = 4.156259819865227e-06

Training epoch-63 batch-14
Running loss of epoch-63 batch-14 = 2.2705644369125366e-06

Training epoch-63 batch-15
Running loss of epoch-63 batch-15 = 2.7371570467948914e-06

Training epoch-63 batch-16
Running loss of epoch-63 batch-16 = 6.283866241574287e-06

Training epoch-63 batch-17
Running loss of epoch-63 batch-17 = 3.59327532351017e-06

Training epoch-63 batch-18
Running loss of epoch-63 batch-18 = 1.0367017239332199e-05

Training epoch-63 batch-19
Running loss of epoch-63 batch-19 = 7.3355622589588165e-06

Training epoch-63 batch-20
Running loss of epoch-63 batch-20 = 4.309928044676781e-06

Training epoch-63 batch-21
Running loss of epoch-63 batch-21 = 4.1530001908540726e-06

Training epoch-63 batch-22
Running loss of epoch-63 batch-22 = 7.173512130975723e-06

Training epoch-63 batch-23
Running loss of epoch-63 batch-23 = 4.283851012587547e-06

Training epoch-63 batch-24
Running loss of epoch-63 batch-24 = 9.879237040877342e-06

Training epoch-63 batch-25
Running loss of epoch-63 batch-25 = 4.4959597289562225e-06

Training epoch-63 batch-26
Running loss of epoch-63 batch-26 = 6.99702650308609e-06

Training epoch-63 batch-27
Running loss of epoch-63 batch-27 = 6.454996764659882e-06

Training epoch-63 batch-28
Running loss of epoch-63 batch-28 = 5.6782737374305725e-06

Training epoch-63 batch-29
Running loss of epoch-63 batch-29 = 4.230299964547157e-06

Training epoch-63 batch-30
Running loss of epoch-63 batch-30 = 8.167233318090439e-06

Training epoch-63 batch-31
Running loss of epoch-63 batch-31 = 4.158820956945419e-06

Training epoch-63 batch-32
Running loss of epoch-63 batch-32 = 1.2931879609823227e-05

Training epoch-63 batch-33
Running loss of epoch-63 batch-33 = 1.1762604117393494e-05

Training epoch-63 batch-34
Running loss of epoch-63 batch-34 = 4.165805876255035e-06

Training epoch-63 batch-35
Running loss of epoch-63 batch-35 = 4.149973392486572e-06

Training epoch-63 batch-36
Running loss of epoch-63 batch-36 = 2.0203879103064537e-05

Training epoch-63 batch-37
Running loss of epoch-63 batch-37 = 2.189306542277336e-06

Training epoch-63 batch-38
Running loss of epoch-63 batch-38 = 4.3995678424835205e-06

Training epoch-63 batch-39
Running loss of epoch-63 batch-39 = 6.71902671456337e-06

Training epoch-63 batch-40
Running loss of epoch-63 batch-40 = 8.059898391366005e-06

Training epoch-63 batch-41
Running loss of epoch-63 batch-41 = 4.818197339773178e-06

Training epoch-63 batch-42
Running loss of epoch-63 batch-42 = 8.056871592998505e-06

Training epoch-63 batch-43
Running loss of epoch-63 batch-43 = 9.529991075396538e-06

Training epoch-63 batch-44
Running loss of epoch-63 batch-44 = 3.052409738302231e-06

Training epoch-63 batch-45
Running loss of epoch-63 batch-45 = 4.688510671257973e-06

Training epoch-63 batch-46
Running loss of epoch-63 batch-46 = 3.5108532756567e-06

Training epoch-63 batch-47
Running loss of epoch-63 batch-47 = 2.410728484392166e-06

Training epoch-63 batch-48
Running loss of epoch-63 batch-48 = 5.98980113863945e-06

Training epoch-63 batch-49
Running loss of epoch-63 batch-49 = 1.5550758689641953e-05

Training epoch-63 batch-50
Running loss of epoch-63 batch-50 = 7.133465260267258e-06

Training epoch-63 batch-51
Running loss of epoch-63 batch-51 = 5.458248779177666e-06

Training epoch-63 batch-52
Running loss of epoch-63 batch-52 = 9.157694876194e-06

Training epoch-63 batch-53
Running loss of epoch-63 batch-53 = 1.1650146916508675e-05

Training epoch-63 batch-54
Running loss of epoch-63 batch-54 = 6.320886313915253e-06

Training epoch-63 batch-55
Running loss of epoch-63 batch-55 = 3.378838300704956e-06

Training epoch-63 batch-56
Running loss of epoch-63 batch-56 = 1.6828998923301697e-06

Training epoch-63 batch-57
Running loss of epoch-63 batch-57 = 4.392815753817558e-06

Training epoch-63 batch-58
Running loss of epoch-63 batch-58 = 9.718351066112518e-06

Training epoch-63 batch-59
Running loss of epoch-63 batch-59 = 4.141358658671379e-06

Training epoch-63 batch-60
Running loss of epoch-63 batch-60 = 1.6209203749895096e-05

Training epoch-63 batch-61
Running loss of epoch-63 batch-61 = 6.968388333916664e-06

Training epoch-63 batch-62
Running loss of epoch-63 batch-62 = 4.577916115522385e-06

Training epoch-63 batch-63
Running loss of epoch-63 batch-63 = 7.151858881115913e-06

Training epoch-63 batch-64
Running loss of epoch-63 batch-64 = 5.773268640041351e-06

Training epoch-63 batch-65
Running loss of epoch-63 batch-65 = 4.446832463145256e-06

Training epoch-63 batch-66
Running loss of epoch-63 batch-66 = 1.146760769188404e-05

Training epoch-63 batch-67
Running loss of epoch-63 batch-67 = 3.886176273226738e-06

Training epoch-63 batch-68
Running loss of epoch-63 batch-68 = 4.291068762540817e-06

Training epoch-63 batch-69
Running loss of epoch-63 batch-69 = 2.691522240638733e-06

Training epoch-63 batch-70
Running loss of epoch-63 batch-70 = 5.067326128482819e-06

Training epoch-63 batch-71
Running loss of epoch-63 batch-71 = 1.2313015758991241e-05

Training epoch-63 batch-72
Running loss of epoch-63 batch-72 = 8.893897756934166e-06

Training epoch-63 batch-73
Running loss of epoch-63 batch-73 = 1.1792872101068497e-05

Training epoch-63 batch-74
Running loss of epoch-63 batch-74 = 6.405636668205261e-06

Training epoch-63 batch-75
Running loss of epoch-63 batch-75 = 1.141335815191269e-05

Training epoch-63 batch-76
Running loss of epoch-63 batch-76 = 2.584420144557953e-06

Training epoch-63 batch-77
Running loss of epoch-63 batch-77 = 5.650334060192108e-06

Training epoch-63 batch-78
Running loss of epoch-63 batch-78 = 1.3582641258835793e-05

Training epoch-63 batch-79
Running loss of epoch-63 batch-79 = 4.439149051904678e-06

Training epoch-63 batch-80
Running loss of epoch-63 batch-80 = 6.182584911584854e-06

Training epoch-63 batch-81
Running loss of epoch-63 batch-81 = 4.159985110163689e-06

Training epoch-63 batch-82
Running loss of epoch-63 batch-82 = 5.954410880804062e-06

Training epoch-63 batch-83
Running loss of epoch-63 batch-83 = 1.5927478671073914e-05

Training epoch-63 batch-84
Running loss of epoch-63 batch-84 = 2.7765054255723953e-06

Training epoch-63 batch-85
Running loss of epoch-63 batch-85 = 2.548797056078911e-06

Training epoch-63 batch-86
Running loss of epoch-63 batch-86 = 4.962552338838577e-06

Training epoch-63 batch-87
Running loss of epoch-63 batch-87 = 3.874301910400391e-06

Training epoch-63 batch-88
Running loss of epoch-63 batch-88 = 5.149282515048981e-06

Training epoch-63 batch-89
Running loss of epoch-63 batch-89 = 1.0570744052529335e-05

Training epoch-63 batch-90
Running loss of epoch-63 batch-90 = 2.434011548757553e-06

Training epoch-63 batch-91
Running loss of epoch-63 batch-91 = 1.0147923603653908e-05

Training epoch-63 batch-92
Running loss of epoch-63 batch-92 = 9.092036634683609e-06

Training epoch-63 batch-93
Running loss of epoch-63 batch-93 = 9.049894288182259e-06

Training epoch-63 batch-94
Running loss of epoch-63 batch-94 = 2.739951014518738e-06

Training epoch-63 batch-95
Running loss of epoch-63 batch-95 = 6.760703399777412e-06

Training epoch-63 batch-96
Running loss of epoch-63 batch-96 = 7.956288754940033e-06

Training epoch-63 batch-97
Running loss of epoch-63 batch-97 = 5.292706191539764e-06

Training epoch-63 batch-98
Running loss of epoch-63 batch-98 = 2.7704518288373947e-06

Training epoch-63 batch-99
Running loss of epoch-63 batch-99 = 6.707152351737022e-06

Training epoch-63 batch-100
Running loss of epoch-63 batch-100 = 3.78279946744442e-06

Training epoch-63 batch-101
Running loss of epoch-63 batch-101 = 3.977911546826363e-06

Training epoch-63 batch-102
Running loss of epoch-63 batch-102 = 3.2822135835886e-06

Training epoch-63 batch-103
Running loss of epoch-63 batch-103 = 4.8193614929914474e-06

Training epoch-63 batch-104
Running loss of epoch-63 batch-104 = 5.170237272977829e-06

Training epoch-63 batch-105
Running loss of epoch-63 batch-105 = 7.061753422021866e-06

Training epoch-63 batch-106
Running loss of epoch-63 batch-106 = 5.808426067233086e-06

Training epoch-63 batch-107
Running loss of epoch-63 batch-107 = 5.583977326750755e-06

Training epoch-63 batch-108
Running loss of epoch-63 batch-108 = 1.3168435543775558e-05

Training epoch-63 batch-109
Running loss of epoch-63 batch-109 = 6.4836349338293076e-06

Training epoch-63 batch-110
Running loss of epoch-63 batch-110 = 8.847098797559738e-06

Training epoch-63 batch-111
Running loss of epoch-63 batch-111 = 6.384914740920067e-06

Training epoch-63 batch-112
Running loss of epoch-63 batch-112 = 2.49338336288929e-06

Training epoch-63 batch-113
Running loss of epoch-63 batch-113 = 1.0606599971652031e-05

Training epoch-63 batch-114
Running loss of epoch-63 batch-114 = 5.791662260890007e-06

Training epoch-63 batch-115
Running loss of epoch-63 batch-115 = 5.223788321018219e-06

Training epoch-63 batch-116
Running loss of epoch-63 batch-116 = 8.878996595740318e-06

Training epoch-63 batch-117
Running loss of epoch-63 batch-117 = 6.817048415541649e-06

Training epoch-63 batch-118
Running loss of epoch-63 batch-118 = 2.9960647225379944e-06

Training epoch-63 batch-119
Running loss of epoch-63 batch-119 = 1.128646545112133e-05

Training epoch-63 batch-120
Running loss of epoch-63 batch-120 = 5.038222298026085e-06

Training epoch-63 batch-121
Running loss of epoch-63 batch-121 = 4.887813702225685e-06

Training epoch-63 batch-122
Running loss of epoch-63 batch-122 = 5.6438148021698e-06

Training epoch-63 batch-123
Running loss of epoch-63 batch-123 = 4.3425243347883224e-06

Training epoch-63 batch-124
Running loss of epoch-63 batch-124 = 4.342291504144669e-06

Training epoch-63 batch-125
Running loss of epoch-63 batch-125 = 5.679670721292496e-06

Training epoch-63 batch-126
Running loss of epoch-63 batch-126 = 9.999144822359085e-06

Training epoch-63 batch-127
Running loss of epoch-63 batch-127 = 5.382345989346504e-06

Training epoch-63 batch-128
Running loss of epoch-63 batch-128 = 2.9206275939941406e-06

Training epoch-63 batch-129
Running loss of epoch-63 batch-129 = 1.3177748769521713e-05

Training epoch-63 batch-130
Running loss of epoch-63 batch-130 = 6.273854523897171e-06

Training epoch-63 batch-131
Running loss of epoch-63 batch-131 = 7.22263939678669e-06

Training epoch-63 batch-132
Running loss of epoch-63 batch-132 = 7.813330739736557e-06

Training epoch-63 batch-133
Running loss of epoch-63 batch-133 = 2.3706816136837006e-06

Training epoch-63 batch-134
Running loss of epoch-63 batch-134 = 4.9979425966739655e-06

Training epoch-63 batch-135
Running loss of epoch-63 batch-135 = 2.314569428563118e-06

Training epoch-63 batch-136
Running loss of epoch-63 batch-136 = 5.3104013204574585e-06

Training epoch-63 batch-137
Running loss of epoch-63 batch-137 = 6.438465788960457e-06

Training epoch-63 batch-138
Running loss of epoch-63 batch-138 = 5.059177055954933e-06

Training epoch-63 batch-139
Running loss of epoch-63 batch-139 = 2.485699951648712e-06

Training epoch-63 batch-140
Running loss of epoch-63 batch-140 = 4.3138861656188965e-06

Training epoch-63 batch-141
Running loss of epoch-63 batch-141 = 5.0156377255916595e-06

Training epoch-63 batch-142
Running loss of epoch-63 batch-142 = 9.217998012900352e-06

Training epoch-63 batch-143
Running loss of epoch-63 batch-143 = 5.2691902965307236e-06

Training epoch-63 batch-144
Running loss of epoch-63 batch-144 = 3.7597492337226868e-06

Training epoch-63 batch-145
Running loss of epoch-63 batch-145 = 4.11435030400753e-06

Training epoch-63 batch-146
Running loss of epoch-63 batch-146 = 4.582107067108154e-06

Training epoch-63 batch-147
Running loss of epoch-63 batch-147 = 6.443820893764496e-06

Training epoch-63 batch-148
Running loss of epoch-63 batch-148 = 4.3264590203762054e-06

Training epoch-63 batch-149
Running loss of epoch-63 batch-149 = 5.902256816625595e-06

Training epoch-63 batch-150
Running loss of epoch-63 batch-150 = 4.465458914637566e-06

Training epoch-63 batch-151
Running loss of epoch-63 batch-151 = 2.682209014892578e-06

Training epoch-63 batch-152
Running loss of epoch-63 batch-152 = 2.7471687644720078e-06

Training epoch-63 batch-153
Running loss of epoch-63 batch-153 = 5.373964086174965e-06

Training epoch-63 batch-154
Running loss of epoch-63 batch-154 = 4.373490810394287e-06

Training epoch-63 batch-155
Running loss of epoch-63 batch-155 = 5.772104486823082e-06

Training epoch-63 batch-156
Running loss of epoch-63 batch-156 = 5.607027560472488e-06

Training epoch-63 batch-157
Running loss of epoch-63 batch-157 = 7.829442620277405e-05

Finished training epoch-63.



Average train loss at epoch-63 = 6.321015954017639e-06

Started Evaluation
