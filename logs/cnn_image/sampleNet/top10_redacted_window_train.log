
DataLoader:

Started trainloader.
trainset = #10000
Finished trainloader.
Started valloader.
valset = #4847
Finished valloader.

Configuration:

DB_NAME = java-top10, IMG_TYPE = redacted_window, TRANS_SIZE = 1024
MAX_EPOCH = 100, BATCH_SIZE = 64

device = cuda:0



Started training epoch-1


Training epoch-1 batch-1
Running loss of epoch-1 batch-1 = 0.0360204353928566

Training epoch-1 batch-2
Running loss of epoch-1 batch-2 = 0.03608217462897301

Training epoch-1 batch-3
Running loss of epoch-1 batch-3 = 0.03608308732509613

Training epoch-1 batch-4
Running loss of epoch-1 batch-4 = 0.03612227737903595

Training epoch-1 batch-5
Running loss of epoch-1 batch-5 = 0.035895559936761856

Training epoch-1 batch-6
Running loss of epoch-1 batch-6 = 0.03587266802787781

Training epoch-1 batch-7
Running loss of epoch-1 batch-7 = 0.0360887311398983

Training epoch-1 batch-8
Running loss of epoch-1 batch-8 = 0.036294788122177124

Training epoch-1 batch-9
Running loss of epoch-1 batch-9 = 0.03600875288248062

Training epoch-1 batch-10
Running loss of epoch-1 batch-10 = 0.03671673312783241

Training epoch-1 batch-11
Running loss of epoch-1 batch-11 = 0.036333583295345306

Training epoch-1 batch-12
Running loss of epoch-1 batch-12 = 0.035753071308135986

Training epoch-1 batch-13
Running loss of epoch-1 batch-13 = 0.03589731082320213

Training epoch-1 batch-14
Running loss of epoch-1 batch-14 = 0.036034975200891495

Training epoch-1 batch-15
Running loss of epoch-1 batch-15 = 0.0359349362552166

Training epoch-1 batch-16
Running loss of epoch-1 batch-16 = 0.03599175065755844

Training epoch-1 batch-17
Running loss of epoch-1 batch-17 = 0.036130476742982864

Training epoch-1 batch-18
Running loss of epoch-1 batch-18 = 0.035959068685770035

Training epoch-1 batch-19
Running loss of epoch-1 batch-19 = 0.03597039729356766

Training epoch-1 batch-20
Running loss of epoch-1 batch-20 = 0.035719744861125946

Training epoch-1 batch-21
Running loss of epoch-1 batch-21 = 0.036056362092494965

Training epoch-1 batch-22
Running loss of epoch-1 batch-22 = 0.03602663427591324

Training epoch-1 batch-23
Running loss of epoch-1 batch-23 = 0.036081574857234955

Training epoch-1 batch-24
Running loss of epoch-1 batch-24 = 0.03603792190551758

Training epoch-1 batch-25
Running loss of epoch-1 batch-25 = 0.036231204867362976

Training epoch-1 batch-26
Running loss of epoch-1 batch-26 = 0.03617418184876442

Training epoch-1 batch-27
Running loss of epoch-1 batch-27 = 0.03567409887909889

Training epoch-1 batch-28
Running loss of epoch-1 batch-28 = 0.03588353469967842

Training epoch-1 batch-29
Running loss of epoch-1 batch-29 = 0.03572745993733406

Training epoch-1 batch-30
Running loss of epoch-1 batch-30 = 0.03594761714339256

Training epoch-1 batch-31
Running loss of epoch-1 batch-31 = 0.03571192920207977

Training epoch-1 batch-32
Running loss of epoch-1 batch-32 = 0.0356612391769886

Training epoch-1 batch-33
Running loss of epoch-1 batch-33 = 0.03616274520754814

Training epoch-1 batch-34
Running loss of epoch-1 batch-34 = 0.035848237574100494

Training epoch-1 batch-35
Running loss of epoch-1 batch-35 = 0.03575332462787628

Training epoch-1 batch-36
Running loss of epoch-1 batch-36 = 0.036086518317461014

Training epoch-1 batch-37
Running loss of epoch-1 batch-37 = 0.0355067178606987

Training epoch-1 batch-38
Running loss of epoch-1 batch-38 = 0.035700492560863495

Training epoch-1 batch-39
Running loss of epoch-1 batch-39 = 0.03547380864620209

Training epoch-1 batch-40
Running loss of epoch-1 batch-40 = 0.03596062585711479

Training epoch-1 batch-41
Running loss of epoch-1 batch-41 = 0.035577815026044846

Training epoch-1 batch-42
Running loss of epoch-1 batch-42 = 0.03558215871453285

Training epoch-1 batch-43
Running loss of epoch-1 batch-43 = 0.03648235276341438

Training epoch-1 batch-44
Running loss of epoch-1 batch-44 = 0.03567702695727348

Training epoch-1 batch-45
Running loss of epoch-1 batch-45 = 0.03558121249079704

Training epoch-1 batch-46
Running loss of epoch-1 batch-46 = 0.03579849749803543

Training epoch-1 batch-47
Running loss of epoch-1 batch-47 = 0.035292766988277435

Training epoch-1 batch-48
Running loss of epoch-1 batch-48 = 0.03587811812758446

Training epoch-1 batch-49
Running loss of epoch-1 batch-49 = 0.03539979085326195

Training epoch-1 batch-50
Running loss of epoch-1 batch-50 = 0.03565284609794617

Training epoch-1 batch-51
Running loss of epoch-1 batch-51 = 0.03563075512647629

Training epoch-1 batch-52
Running loss of epoch-1 batch-52 = 0.035298462957143784

Training epoch-1 batch-53
Running loss of epoch-1 batch-53 = 0.03558550029993057

Training epoch-1 batch-54
Running loss of epoch-1 batch-54 = 0.035702794790267944

Training epoch-1 batch-55
Running loss of epoch-1 batch-55 = 0.03558744490146637

Training epoch-1 batch-56
Running loss of epoch-1 batch-56 = 0.03547060862183571

Training epoch-1 batch-57
Running loss of epoch-1 batch-57 = 0.035552166402339935

Training epoch-1 batch-58
Running loss of epoch-1 batch-58 = 0.035537440329790115

Training epoch-1 batch-59
Running loss of epoch-1 batch-59 = 0.03548811376094818

Training epoch-1 batch-60
Running loss of epoch-1 batch-60 = 0.03541485220193863

Training epoch-1 batch-61
Running loss of epoch-1 batch-61 = 0.03535883128643036

Training epoch-1 batch-62
Running loss of epoch-1 batch-62 = 0.03536830097436905

Training epoch-1 batch-63
Running loss of epoch-1 batch-63 = 0.035297807306051254

Training epoch-1 batch-64
Running loss of epoch-1 batch-64 = 0.03551656752824783

Training epoch-1 batch-65
Running loss of epoch-1 batch-65 = 0.03544745594263077

Training epoch-1 batch-66
Running loss of epoch-1 batch-66 = 0.03531477600336075

Training epoch-1 batch-67
Running loss of epoch-1 batch-67 = 0.03553488105535507

Training epoch-1 batch-68
Running loss of epoch-1 batch-68 = 0.03509420156478882

Training epoch-1 batch-69
Running loss of epoch-1 batch-69 = 0.035106807947158813

Training epoch-1 batch-70
Running loss of epoch-1 batch-70 = 0.03505737707018852

Training epoch-1 batch-71
Running loss of epoch-1 batch-71 = 0.03504093363881111

Training epoch-1 batch-72
Running loss of epoch-1 batch-72 = 0.03518344834446907

Training epoch-1 batch-73
Running loss of epoch-1 batch-73 = 0.03504030033946037

Training epoch-1 batch-74
Running loss of epoch-1 batch-74 = 0.03539395332336426

Training epoch-1 batch-75
Running loss of epoch-1 batch-75 = 0.03561999276280403

Training epoch-1 batch-76
Running loss of epoch-1 batch-76 = 0.034646615386009216

Training epoch-1 batch-77
Running loss of epoch-1 batch-77 = 0.03493918105959892

Training epoch-1 batch-78
Running loss of epoch-1 batch-78 = 0.03530087694525719

Training epoch-1 batch-79
Running loss of epoch-1 batch-79 = 0.03531353548169136

Training epoch-1 batch-80
Running loss of epoch-1 batch-80 = 0.03528222441673279

Training epoch-1 batch-81
Running loss of epoch-1 batch-81 = 0.03521615266799927

Training epoch-1 batch-82
Running loss of epoch-1 batch-82 = 0.035404641181230545

Training epoch-1 batch-83
Running loss of epoch-1 batch-83 = 0.034611742943525314

Training epoch-1 batch-84
Running loss of epoch-1 batch-84 = 0.035207029432058334

Training epoch-1 batch-85
Running loss of epoch-1 batch-85 = 0.03526502475142479

Training epoch-1 batch-86
Running loss of epoch-1 batch-86 = 0.0351828895509243

Training epoch-1 batch-87
Running loss of epoch-1 batch-87 = 0.035140905529260635

Training epoch-1 batch-88
Running loss of epoch-1 batch-88 = 0.03498687595129013

Training epoch-1 batch-89
Running loss of epoch-1 batch-89 = 0.03483659029006958

Training epoch-1 batch-90
Running loss of epoch-1 batch-90 = 0.034962769597768784

Training epoch-1 batch-91
Running loss of epoch-1 batch-91 = 0.03497369587421417

Training epoch-1 batch-92
Running loss of epoch-1 batch-92 = 0.03442622348666191

Training epoch-1 batch-93
Running loss of epoch-1 batch-93 = 0.03549877926707268

Training epoch-1 batch-94
Running loss of epoch-1 batch-94 = 0.03459342569112778

Training epoch-1 batch-95
Running loss of epoch-1 batch-95 = 0.03513290360569954

Training epoch-1 batch-96
Running loss of epoch-1 batch-96 = 0.03471750393509865

Training epoch-1 batch-97
Running loss of epoch-1 batch-97 = 0.034411076456308365

Training epoch-1 batch-98
Running loss of epoch-1 batch-98 = 0.034687381237745285

Training epoch-1 batch-99
Running loss of epoch-1 batch-99 = 0.034268818795681

Training epoch-1 batch-100
Running loss of epoch-1 batch-100 = 0.03492346778512001

Training epoch-1 batch-101
Running loss of epoch-1 batch-101 = 0.03433215990662575

Training epoch-1 batch-102
Running loss of epoch-1 batch-102 = 0.03409656137228012

Training epoch-1 batch-103
Running loss of epoch-1 batch-103 = 0.03424400836229324

Training epoch-1 batch-104
Running loss of epoch-1 batch-104 = 0.03435537964105606

Training epoch-1 batch-105
Running loss of epoch-1 batch-105 = 0.035286981612443924

Training epoch-1 batch-106
Running loss of epoch-1 batch-106 = 0.0341869592666626

Training epoch-1 batch-107
Running loss of epoch-1 batch-107 = 0.034353435039520264

Training epoch-1 batch-108
Running loss of epoch-1 batch-108 = 0.03363416716456413

Training epoch-1 batch-109
Running loss of epoch-1 batch-109 = 0.03369405120611191

Training epoch-1 batch-110
Running loss of epoch-1 batch-110 = 0.034163184463977814

Training epoch-1 batch-111
Running loss of epoch-1 batch-111 = 0.03410240262746811

Training epoch-1 batch-112
Running loss of epoch-1 batch-112 = 0.03361034020781517

Training epoch-1 batch-113
Running loss of epoch-1 batch-113 = 0.03343231976032257

Training epoch-1 batch-114
Running loss of epoch-1 batch-114 = 0.03429946303367615

Training epoch-1 batch-115
Running loss of epoch-1 batch-115 = 0.033677782863378525

Training epoch-1 batch-116
Running loss of epoch-1 batch-116 = 0.033386461436748505

Training epoch-1 batch-117
Running loss of epoch-1 batch-117 = 0.03401791304349899

Training epoch-1 batch-118
Running loss of epoch-1 batch-118 = 0.034038983285427094

Training epoch-1 batch-119
Running loss of epoch-1 batch-119 = 0.033473558723926544

Training epoch-1 batch-120
Running loss of epoch-1 batch-120 = 0.032980579882860184

Training epoch-1 batch-121
Running loss of epoch-1 batch-121 = 0.03334870561957359

Training epoch-1 batch-122
Running loss of epoch-1 batch-122 = 0.033709898591041565

Training epoch-1 batch-123
Running loss of epoch-1 batch-123 = 0.03349590301513672

Training epoch-1 batch-124
Running loss of epoch-1 batch-124 = 0.032742515206336975

Training epoch-1 batch-125
Running loss of epoch-1 batch-125 = 0.03305717557668686

Training epoch-1 batch-126
Running loss of epoch-1 batch-126 = 0.032350681722164154

Training epoch-1 batch-127
Running loss of epoch-1 batch-127 = 0.03323044255375862

Training epoch-1 batch-128
Running loss of epoch-1 batch-128 = 0.032162267714738846

Training epoch-1 batch-129
Running loss of epoch-1 batch-129 = 0.03329571709036827

Training epoch-1 batch-130
Running loss of epoch-1 batch-130 = 0.03256257623434067

Training epoch-1 batch-131
Running loss of epoch-1 batch-131 = 0.033088475465774536

Training epoch-1 batch-132
Running loss of epoch-1 batch-132 = 0.0326911062002182

Training epoch-1 batch-133
Running loss of epoch-1 batch-133 = 0.033524904400110245

Training epoch-1 batch-134
Running loss of epoch-1 batch-134 = 0.03259769454598427

Training epoch-1 batch-135
Running loss of epoch-1 batch-135 = 0.03181479126214981

Training epoch-1 batch-136
Running loss of epoch-1 batch-136 = 0.03228065371513367

Training epoch-1 batch-137
Running loss of epoch-1 batch-137 = 0.032443415373563766

Training epoch-1 batch-138
Running loss of epoch-1 batch-138 = 0.032186705619096756

Training epoch-1 batch-139
Running loss of epoch-1 batch-139 = 0.03241560980677605

Training epoch-1 batch-140
Running loss of epoch-1 batch-140 = 0.031366702169179916

Training epoch-1 batch-141
Running loss of epoch-1 batch-141 = 0.03143610805273056

Training epoch-1 batch-142
Running loss of epoch-1 batch-142 = 0.03143668174743652

Training epoch-1 batch-143
Running loss of epoch-1 batch-143 = 0.032682836055755615

Training epoch-1 batch-144
Running loss of epoch-1 batch-144 = 0.030179567635059357

Training epoch-1 batch-145
Running loss of epoch-1 batch-145 = 0.03186705708503723

Training epoch-1 batch-146
Running loss of epoch-1 batch-146 = 0.031504061073064804

Training epoch-1 batch-147
Running loss of epoch-1 batch-147 = 0.03098795749247074

Training epoch-1 batch-148
Running loss of epoch-1 batch-148 = 0.03153863921761513

Training epoch-1 batch-149
Running loss of epoch-1 batch-149 = 0.030083414167165756

Training epoch-1 batch-150
Running loss of epoch-1 batch-150 = 0.032245147973299026

Training epoch-1 batch-151
Running loss of epoch-1 batch-151 = 0.03095763362944126

Training epoch-1 batch-152
Running loss of epoch-1 batch-152 = 0.029853221029043198

Training epoch-1 batch-153
Running loss of epoch-1 batch-153 = 0.031145889312028885

Training epoch-1 batch-154
Running loss of epoch-1 batch-154 = 0.031501755118370056

Training epoch-1 batch-155
Running loss of epoch-1 batch-155 = 0.031204666942358017

Training epoch-1 batch-156
Running loss of epoch-1 batch-156 = 0.03085312433540821

Training epoch-1 batch-157
Running loss of epoch-1 batch-157 = 0.12144535034894943

Finished training epoch-1.



Average train loss at epoch-1 = 0.03467318513393402

Started Evaluation

Average val loss at epoch-1 = 1.8699002312986475

Accuracy for classes:
Accuracy for class equals is: 80.20 %
Accuracy for class main is: 98.69 %
Accuracy for class setUp is: 22.62 %
Accuracy for class onCreate is: 85.93 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 7.53 %
Accuracy for class hashCode is: 88.01 %
Accuracy for class init is: 7.17 %
Accuracy for class execute is: 0.00 %
Accuracy for class get is: 16.92 %

Overall Accuracy = 52.09 %


Best Accuracy = 52.09 % at Epoch-1
Saving model after best epoch-1

Finished Evaluation



Started training epoch-2


Training epoch-2 batch-1
Running loss of epoch-2 batch-1 = 0.029908277094364166

Training epoch-2 batch-2
Running loss of epoch-2 batch-2 = 0.028695780783891678

Training epoch-2 batch-3
Running loss of epoch-2 batch-3 = 0.03046000935137272

Training epoch-2 batch-4
Running loss of epoch-2 batch-4 = 0.02846454828977585

Training epoch-2 batch-5
Running loss of epoch-2 batch-5 = 0.02909892052412033

Training epoch-2 batch-6
Running loss of epoch-2 batch-6 = 0.029436258599162102

Training epoch-2 batch-7
Running loss of epoch-2 batch-7 = 0.030399104580283165

Training epoch-2 batch-8
Running loss of epoch-2 batch-8 = 0.02828422375023365

Training epoch-2 batch-9
Running loss of epoch-2 batch-9 = 0.028255648910999298

Training epoch-2 batch-10
Running loss of epoch-2 batch-10 = 0.030794765800237656

Training epoch-2 batch-11
Running loss of epoch-2 batch-11 = 0.028254983946681023

Training epoch-2 batch-12
Running loss of epoch-2 batch-12 = 0.030652940273284912

Training epoch-2 batch-13
Running loss of epoch-2 batch-13 = 0.030525032430887222

Training epoch-2 batch-14
Running loss of epoch-2 batch-14 = 0.027975041419267654

Training epoch-2 batch-15
Running loss of epoch-2 batch-15 = 0.029605036601424217

Training epoch-2 batch-16
Running loss of epoch-2 batch-16 = 0.027127176523208618

Training epoch-2 batch-17
Running loss of epoch-2 batch-17 = 0.028177881613373756

Training epoch-2 batch-18
Running loss of epoch-2 batch-18 = 0.028764816001057625

Training epoch-2 batch-19
Running loss of epoch-2 batch-19 = 0.0309126153588295

Training epoch-2 batch-20
Running loss of epoch-2 batch-20 = 0.02747589722275734

Training epoch-2 batch-21
Running loss of epoch-2 batch-21 = 0.027502864599227905

Training epoch-2 batch-22
Running loss of epoch-2 batch-22 = 0.031591977924108505

Training epoch-2 batch-23
Running loss of epoch-2 batch-23 = 0.02758912369608879

Training epoch-2 batch-24
Running loss of epoch-2 batch-24 = 0.028057515621185303

Training epoch-2 batch-25
Running loss of epoch-2 batch-25 = 0.026997597888112068

Training epoch-2 batch-26
Running loss of epoch-2 batch-26 = 0.029020685702562332

Training epoch-2 batch-27
Running loss of epoch-2 batch-27 = 0.028070615604519844

Training epoch-2 batch-28
Running loss of epoch-2 batch-28 = 0.02623710036277771

Training epoch-2 batch-29
Running loss of epoch-2 batch-29 = 0.026644770056009293

Training epoch-2 batch-30
Running loss of epoch-2 batch-30 = 0.0275335144251585

Training epoch-2 batch-31
Running loss of epoch-2 batch-31 = 0.027845561504364014

Training epoch-2 batch-32
Running loss of epoch-2 batch-32 = 0.026855232194066048

Training epoch-2 batch-33
Running loss of epoch-2 batch-33 = 0.027638789266347885

Training epoch-2 batch-34
Running loss of epoch-2 batch-34 = 0.02787955105304718

Training epoch-2 batch-35
Running loss of epoch-2 batch-35 = 0.02682575210928917

Training epoch-2 batch-36
Running loss of epoch-2 batch-36 = 0.024918975308537483

Training epoch-2 batch-37
Running loss of epoch-2 batch-37 = 0.024982266128063202

Training epoch-2 batch-38
Running loss of epoch-2 batch-38 = 0.027692973613739014

Training epoch-2 batch-39
Running loss of epoch-2 batch-39 = 0.02838679403066635

Training epoch-2 batch-40
Running loss of epoch-2 batch-40 = 0.027895763516426086

Training epoch-2 batch-41
Running loss of epoch-2 batch-41 = 0.02497774362564087

Training epoch-2 batch-42
Running loss of epoch-2 batch-42 = 0.026467036455869675

Training epoch-2 batch-43
Running loss of epoch-2 batch-43 = 0.026303688064217567

Training epoch-2 batch-44
Running loss of epoch-2 batch-44 = 0.0264957994222641

Training epoch-2 batch-45
Running loss of epoch-2 batch-45 = 0.024892885237932205

Training epoch-2 batch-46
Running loss of epoch-2 batch-46 = 0.027486715465784073

Training epoch-2 batch-47
Running loss of epoch-2 batch-47 = 0.02285134792327881

Training epoch-2 batch-48
Running loss of epoch-2 batch-48 = 0.024256370961666107

Training epoch-2 batch-49
Running loss of epoch-2 batch-49 = 0.024698924273252487

Training epoch-2 batch-50
Running loss of epoch-2 batch-50 = 0.026222560554742813

Training epoch-2 batch-51
Running loss of epoch-2 batch-51 = 0.02538830041885376

Training epoch-2 batch-52
Running loss of epoch-2 batch-52 = 0.02429843135178089

Training epoch-2 batch-53
Running loss of epoch-2 batch-53 = 0.024143807590007782

Training epoch-2 batch-54
Running loss of epoch-2 batch-54 = 0.02394440956413746

Training epoch-2 batch-55
Running loss of epoch-2 batch-55 = 0.024872073903679848

Training epoch-2 batch-56
Running loss of epoch-2 batch-56 = 0.02429276704788208

Training epoch-2 batch-57
Running loss of epoch-2 batch-57 = 0.02266523614525795

Training epoch-2 batch-58
Running loss of epoch-2 batch-58 = 0.023315759375691414

Training epoch-2 batch-59
Running loss of epoch-2 batch-59 = 0.02285178378224373

Training epoch-2 batch-60
Running loss of epoch-2 batch-60 = 0.023366838693618774

Training epoch-2 batch-61
Running loss of epoch-2 batch-61 = 0.02207135409116745

Training epoch-2 batch-62
Running loss of epoch-2 batch-62 = 0.022740129381418228

Training epoch-2 batch-63
Running loss of epoch-2 batch-63 = 0.02659752406179905

Training epoch-2 batch-64
Running loss of epoch-2 batch-64 = 0.021780064329504967

Training epoch-2 batch-65
Running loss of epoch-2 batch-65 = 0.020788218826055527

Training epoch-2 batch-66
Running loss of epoch-2 batch-66 = 0.02296515554189682

Training epoch-2 batch-67
Running loss of epoch-2 batch-67 = 0.019978294149041176

Training epoch-2 batch-68
Running loss of epoch-2 batch-68 = 0.023392613977193832

Training epoch-2 batch-69
Running loss of epoch-2 batch-69 = 0.022913116961717606

Training epoch-2 batch-70
Running loss of epoch-2 batch-70 = 0.02205152064561844

Training epoch-2 batch-71
Running loss of epoch-2 batch-71 = 0.019102370366454124

Training epoch-2 batch-72
Running loss of epoch-2 batch-72 = 0.020820245146751404

Training epoch-2 batch-73
Running loss of epoch-2 batch-73 = 0.02094944939017296

Training epoch-2 batch-74
Running loss of epoch-2 batch-74 = 0.019377462565898895

Training epoch-2 batch-75
Running loss of epoch-2 batch-75 = 0.023825282230973244

Training epoch-2 batch-76
Running loss of epoch-2 batch-76 = 0.020052427425980568

Training epoch-2 batch-77
Running loss of epoch-2 batch-77 = 0.020404420793056488

Training epoch-2 batch-78
Running loss of epoch-2 batch-78 = 0.0223134383559227

Training epoch-2 batch-79
Running loss of epoch-2 batch-79 = 0.01677710749208927

Training epoch-2 batch-80
Running loss of epoch-2 batch-80 = 0.019329681992530823

Training epoch-2 batch-81
Running loss of epoch-2 batch-81 = 0.01796046271920204

Training epoch-2 batch-82
Running loss of epoch-2 batch-82 = 0.018804792314767838

Training epoch-2 batch-83
Running loss of epoch-2 batch-83 = 0.022977767512202263

Training epoch-2 batch-84
Running loss of epoch-2 batch-84 = 0.020525246858596802

Training epoch-2 batch-85
Running loss of epoch-2 batch-85 = 0.019371462985873222

Training epoch-2 batch-86
Running loss of epoch-2 batch-86 = 0.020979713648557663

Training epoch-2 batch-87
Running loss of epoch-2 batch-87 = 0.018318727612495422

Training epoch-2 batch-88
Running loss of epoch-2 batch-88 = 0.022251548245549202

Training epoch-2 batch-89
Running loss of epoch-2 batch-89 = 0.02035028673708439

Training epoch-2 batch-90
Running loss of epoch-2 batch-90 = 0.018087800592184067

Training epoch-2 batch-91
Running loss of epoch-2 batch-91 = 0.018410667777061462

Training epoch-2 batch-92
Running loss of epoch-2 batch-92 = 0.0186790581792593

Training epoch-2 batch-93
Running loss of epoch-2 batch-93 = 0.02107885293662548

Training epoch-2 batch-94
Running loss of epoch-2 batch-94 = 0.023153647780418396

Training epoch-2 batch-95
Running loss of epoch-2 batch-95 = 0.01548802386969328

Training epoch-2 batch-96
Running loss of epoch-2 batch-96 = 0.019467296078801155

Training epoch-2 batch-97
Running loss of epoch-2 batch-97 = 0.018905116245150566

Training epoch-2 batch-98
Running loss of epoch-2 batch-98 = 0.02165968529880047

Training epoch-2 batch-99
Running loss of epoch-2 batch-99 = 0.01923702470958233

Training epoch-2 batch-100
Running loss of epoch-2 batch-100 = 0.019093800336122513

Training epoch-2 batch-101
Running loss of epoch-2 batch-101 = 0.016997188329696655

Training epoch-2 batch-102
Running loss of epoch-2 batch-102 = 0.019783994182944298

Training epoch-2 batch-103
Running loss of epoch-2 batch-103 = 0.016073202714323997

Training epoch-2 batch-104
Running loss of epoch-2 batch-104 = 0.018185768276453018

Training epoch-2 batch-105
Running loss of epoch-2 batch-105 = 0.017716115340590477

Training epoch-2 batch-106
Running loss of epoch-2 batch-106 = 0.021074606105685234

Training epoch-2 batch-107
Running loss of epoch-2 batch-107 = 0.018521711230278015

Training epoch-2 batch-108
Running loss of epoch-2 batch-108 = 0.015395507216453552

Training epoch-2 batch-109
Running loss of epoch-2 batch-109 = 0.0150810731574893

Training epoch-2 batch-110
Running loss of epoch-2 batch-110 = 0.019144659861922264

Training epoch-2 batch-111
Running loss of epoch-2 batch-111 = 0.017011845484375954

Training epoch-2 batch-112
Running loss of epoch-2 batch-112 = 0.01913287863135338

Training epoch-2 batch-113
Running loss of epoch-2 batch-113 = 0.017491763457655907

Training epoch-2 batch-114
Running loss of epoch-2 batch-114 = 0.01884259656071663

Training epoch-2 batch-115
Running loss of epoch-2 batch-115 = 0.022282207384705544

Training epoch-2 batch-116
Running loss of epoch-2 batch-116 = 0.021935485303401947

Training epoch-2 batch-117
Running loss of epoch-2 batch-117 = 0.019149385392665863

Training epoch-2 batch-118
Running loss of epoch-2 batch-118 = 0.017564864829182625

Training epoch-2 batch-119
Running loss of epoch-2 batch-119 = 0.01811932399868965

Training epoch-2 batch-120
Running loss of epoch-2 batch-120 = 0.01662343554198742

Training epoch-2 batch-121
Running loss of epoch-2 batch-121 = 0.019731827080249786

Training epoch-2 batch-122
Running loss of epoch-2 batch-122 = 0.018271593376994133

Training epoch-2 batch-123
Running loss of epoch-2 batch-123 = 0.02065502665936947

Training epoch-2 batch-124
Running loss of epoch-2 batch-124 = 0.01828530803322792

Training epoch-2 batch-125
Running loss of epoch-2 batch-125 = 0.014201194047927856

Training epoch-2 batch-126
Running loss of epoch-2 batch-126 = 0.019331037998199463

Training epoch-2 batch-127
Running loss of epoch-2 batch-127 = 0.017663879320025444

Training epoch-2 batch-128
Running loss of epoch-2 batch-128 = 0.015989001840353012

Training epoch-2 batch-129
Running loss of epoch-2 batch-129 = 0.018770849332213402

Training epoch-2 batch-130
Running loss of epoch-2 batch-130 = 0.01429193839430809

Training epoch-2 batch-131
Running loss of epoch-2 batch-131 = 0.01708786003291607

Training epoch-2 batch-132
Running loss of epoch-2 batch-132 = 0.016034185886383057

Training epoch-2 batch-133
Running loss of epoch-2 batch-133 = 0.018547620624303818

Training epoch-2 batch-134
Running loss of epoch-2 batch-134 = 0.012481961399316788

Training epoch-2 batch-135
Running loss of epoch-2 batch-135 = 0.01800810918211937

Training epoch-2 batch-136
Running loss of epoch-2 batch-136 = 0.01579166203737259

Training epoch-2 batch-137
Running loss of epoch-2 batch-137 = 0.013349076732993126

Training epoch-2 batch-138
Running loss of epoch-2 batch-138 = 0.018845442682504654

Training epoch-2 batch-139
Running loss of epoch-2 batch-139 = 0.017993884161114693

Training epoch-2 batch-140
Running loss of epoch-2 batch-140 = 0.017961757257580757

Training epoch-2 batch-141
Running loss of epoch-2 batch-141 = 0.019969964399933815

Training epoch-2 batch-142
Running loss of epoch-2 batch-142 = 0.01599331945180893

Training epoch-2 batch-143
Running loss of epoch-2 batch-143 = 0.015152838081121445

Training epoch-2 batch-144
Running loss of epoch-2 batch-144 = 0.01702956110239029

Training epoch-2 batch-145
Running loss of epoch-2 batch-145 = 0.018565982580184937

Training epoch-2 batch-146
Running loss of epoch-2 batch-146 = 0.014893683604896069

Training epoch-2 batch-147
Running loss of epoch-2 batch-147 = 0.014274952001869678

Training epoch-2 batch-148
Running loss of epoch-2 batch-148 = 0.014574621804058552

Training epoch-2 batch-149
Running loss of epoch-2 batch-149 = 0.018350930884480476

Training epoch-2 batch-150
Running loss of epoch-2 batch-150 = 0.015853028744459152

Training epoch-2 batch-151
Running loss of epoch-2 batch-151 = 0.017469819635152817

Training epoch-2 batch-152
Running loss of epoch-2 batch-152 = 0.01484210416674614

Training epoch-2 batch-153
Running loss of epoch-2 batch-153 = 0.017552467063069344

Training epoch-2 batch-154
Running loss of epoch-2 batch-154 = 0.015982970595359802

Training epoch-2 batch-155
Running loss of epoch-2 batch-155 = 0.016415167599916458

Training epoch-2 batch-156
Running loss of epoch-2 batch-156 = 0.018069501966238022

Training epoch-2 batch-157
Running loss of epoch-2 batch-157 = 0.04591964930295944

Finished training epoch-2.



Average train loss at epoch-2 = 0.021991368824243546

Started Evaluation

Average val loss at epoch-2 = 0.9401747829427844

Accuracy for classes:
Accuracy for class equals is: 88.12 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 66.23 %
Accuracy for class onCreate is: 82.52 %
Accuracy for class toString is: 70.31 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 94.76 %
Accuracy for class init is: 1.35 %
Accuracy for class execute is: 40.16 %
Accuracy for class get is: 64.10 %

Overall Accuracy = 70.17 %


Best Accuracy = 70.17 % at Epoch-2
Saving model after best epoch-2

Finished Evaluation



Started training epoch-3


Training epoch-3 batch-1
Running loss of epoch-3 batch-1 = 0.015016055665910244

Training epoch-3 batch-2
Running loss of epoch-3 batch-2 = 0.014161337167024612

Training epoch-3 batch-3
Running loss of epoch-3 batch-3 = 0.01694904826581478

Training epoch-3 batch-4
Running loss of epoch-3 batch-4 = 0.01667449250817299

Training epoch-3 batch-5
Running loss of epoch-3 batch-5 = 0.012456650845706463

Training epoch-3 batch-6
Running loss of epoch-3 batch-6 = 0.016470663249492645

Training epoch-3 batch-7
Running loss of epoch-3 batch-7 = 0.015777941793203354

Training epoch-3 batch-8
Running loss of epoch-3 batch-8 = 0.016846895217895508

Training epoch-3 batch-9
Running loss of epoch-3 batch-9 = 0.013246670365333557

Training epoch-3 batch-10
Running loss of epoch-3 batch-10 = 0.015559932217001915

Training epoch-3 batch-11
Running loss of epoch-3 batch-11 = 0.0180561114102602

Training epoch-3 batch-12
Running loss of epoch-3 batch-12 = 0.01361480075865984

Training epoch-3 batch-13
Running loss of epoch-3 batch-13 = 0.016664471477270126

Training epoch-3 batch-14
Running loss of epoch-3 batch-14 = 0.016492709517478943

Training epoch-3 batch-15
Running loss of epoch-3 batch-15 = 0.014671913348138332

Training epoch-3 batch-16
Running loss of epoch-3 batch-16 = 0.01582673005759716

Training epoch-3 batch-17
Running loss of epoch-3 batch-17 = 0.013407343067228794

Training epoch-3 batch-18
Running loss of epoch-3 batch-18 = 0.016706127673387527

Training epoch-3 batch-19
Running loss of epoch-3 batch-19 = 0.014893111772835255

Training epoch-3 batch-20
Running loss of epoch-3 batch-20 = 0.016630638390779495

Training epoch-3 batch-21
Running loss of epoch-3 batch-21 = 0.012188171036541462

Training epoch-3 batch-22
Running loss of epoch-3 batch-22 = 0.017584865912795067

Training epoch-3 batch-23
Running loss of epoch-3 batch-23 = 0.012769045308232307

Training epoch-3 batch-24
Running loss of epoch-3 batch-24 = 0.01444187667220831

Training epoch-3 batch-25
Running loss of epoch-3 batch-25 = 0.014415731653571129

Training epoch-3 batch-26
Running loss of epoch-3 batch-26 = 0.012247073464095592

Training epoch-3 batch-27
Running loss of epoch-3 batch-27 = 0.014659814536571503

Training epoch-3 batch-28
Running loss of epoch-3 batch-28 = 0.013793078251183033

Training epoch-3 batch-29
Running loss of epoch-3 batch-29 = 0.012087952345609665

Training epoch-3 batch-30
Running loss of epoch-3 batch-30 = 0.016019122675061226

Training epoch-3 batch-31
Running loss of epoch-3 batch-31 = 0.015057992190122604

Training epoch-3 batch-32
Running loss of epoch-3 batch-32 = 0.012456812895834446

Training epoch-3 batch-33
Running loss of epoch-3 batch-33 = 0.011213839054107666

Training epoch-3 batch-34
Running loss of epoch-3 batch-34 = 0.013254644349217415

Training epoch-3 batch-35
Running loss of epoch-3 batch-35 = 0.009945737197995186

Training epoch-3 batch-36
Running loss of epoch-3 batch-36 = 0.010743722319602966

Training epoch-3 batch-37
Running loss of epoch-3 batch-37 = 0.00748885702341795

Training epoch-3 batch-38
Running loss of epoch-3 batch-38 = 0.013295973651111126

Training epoch-3 batch-39
Running loss of epoch-3 batch-39 = 0.015373986214399338

Training epoch-3 batch-40
Running loss of epoch-3 batch-40 = 0.012711264193058014

Training epoch-3 batch-41
Running loss of epoch-3 batch-41 = 0.012439601123332977

Training epoch-3 batch-42
Running loss of epoch-3 batch-42 = 0.013083502650260925

Training epoch-3 batch-43
Running loss of epoch-3 batch-43 = 0.013856533914804459

Training epoch-3 batch-44
Running loss of epoch-3 batch-44 = 0.014680374413728714

Training epoch-3 batch-45
Running loss of epoch-3 batch-45 = 0.013129065744578838

Training epoch-3 batch-46
Running loss of epoch-3 batch-46 = 0.014007688499987125

Training epoch-3 batch-47
Running loss of epoch-3 batch-47 = 0.013483998365700245

Training epoch-3 batch-48
Running loss of epoch-3 batch-48 = 0.010849244892597198

Training epoch-3 batch-49
Running loss of epoch-3 batch-49 = 0.015263589099049568

Training epoch-3 batch-50
Running loss of epoch-3 batch-50 = 0.013397598639130592

Training epoch-3 batch-51
Running loss of epoch-3 batch-51 = 0.013401169329881668

Training epoch-3 batch-52
Running loss of epoch-3 batch-52 = 0.01476164348423481

Training epoch-3 batch-53
Running loss of epoch-3 batch-53 = 0.014688489958643913

Training epoch-3 batch-54
Running loss of epoch-3 batch-54 = 0.015790782868862152

Training epoch-3 batch-55
Running loss of epoch-3 batch-55 = 0.015179811976850033

Training epoch-3 batch-56
Running loss of epoch-3 batch-56 = 0.01110069639980793

Training epoch-3 batch-57
Running loss of epoch-3 batch-57 = 0.013021972961723804

Training epoch-3 batch-58
Running loss of epoch-3 batch-58 = 0.01399949286133051

Training epoch-3 batch-59
Running loss of epoch-3 batch-59 = 0.01608862355351448

Training epoch-3 batch-60
Running loss of epoch-3 batch-60 = 0.012339569628238678

Training epoch-3 batch-61
Running loss of epoch-3 batch-61 = 0.013010836206376553

Training epoch-3 batch-62
Running loss of epoch-3 batch-62 = 0.016457464545965195

Training epoch-3 batch-63
Running loss of epoch-3 batch-63 = 0.012178522534668446

Training epoch-3 batch-64
Running loss of epoch-3 batch-64 = 0.012364267371594906

Training epoch-3 batch-65
Running loss of epoch-3 batch-65 = 0.009368585422635078

Training epoch-3 batch-66
Running loss of epoch-3 batch-66 = 0.015180233865976334

Training epoch-3 batch-67
Running loss of epoch-3 batch-67 = 0.01200185902416706

Training epoch-3 batch-68
Running loss of epoch-3 batch-68 = 0.01241639256477356

Training epoch-3 batch-69
Running loss of epoch-3 batch-69 = 0.011015531606972218

Training epoch-3 batch-70
Running loss of epoch-3 batch-70 = 0.01075028721243143

Training epoch-3 batch-71
Running loss of epoch-3 batch-71 = 0.017777053639292717

Training epoch-3 batch-72
Running loss of epoch-3 batch-72 = 0.009746881201863289

Training epoch-3 batch-73
Running loss of epoch-3 batch-73 = 0.013320639729499817

Training epoch-3 batch-74
Running loss of epoch-3 batch-74 = 0.011954452842473984

Training epoch-3 batch-75
Running loss of epoch-3 batch-75 = 0.01068593468517065

Training epoch-3 batch-76
Running loss of epoch-3 batch-76 = 0.01063840277493

Training epoch-3 batch-77
Running loss of epoch-3 batch-77 = 0.013766590505838394

Training epoch-3 batch-78
Running loss of epoch-3 batch-78 = 0.007097724825143814

Training epoch-3 batch-79
Running loss of epoch-3 batch-79 = 0.011269518174231052

Training epoch-3 batch-80
Running loss of epoch-3 batch-80 = 0.010184327140450478

Training epoch-3 batch-81
Running loss of epoch-3 batch-81 = 0.013860722072422504

Training epoch-3 batch-82
Running loss of epoch-3 batch-82 = 0.010652368888258934

Training epoch-3 batch-83
Running loss of epoch-3 batch-83 = 0.012294257059693336

Training epoch-3 batch-84
Running loss of epoch-3 batch-84 = 0.011809728108346462

Training epoch-3 batch-85
Running loss of epoch-3 batch-85 = 0.010809001512825489

Training epoch-3 batch-86
Running loss of epoch-3 batch-86 = 0.013102235272526741

Training epoch-3 batch-87
Running loss of epoch-3 batch-87 = 0.01087719015777111

Training epoch-3 batch-88
Running loss of epoch-3 batch-88 = 0.011923092417418957

Training epoch-3 batch-89
Running loss of epoch-3 batch-89 = 0.012196272611618042

Training epoch-3 batch-90
Running loss of epoch-3 batch-90 = 0.01426592655479908

Training epoch-3 batch-91
Running loss of epoch-3 batch-91 = 0.010175086557865143

Training epoch-3 batch-92
Running loss of epoch-3 batch-92 = 0.01047324389219284

Training epoch-3 batch-93
Running loss of epoch-3 batch-93 = 0.014480100013315678

Training epoch-3 batch-94
Running loss of epoch-3 batch-94 = 0.014389530755579472

Training epoch-3 batch-95
Running loss of epoch-3 batch-95 = 0.014626243151724339

Training epoch-3 batch-96
Running loss of epoch-3 batch-96 = 0.013824948109686375

Training epoch-3 batch-97
Running loss of epoch-3 batch-97 = 0.01350463554263115

Training epoch-3 batch-98
Running loss of epoch-3 batch-98 = 0.011866714805364609

Training epoch-3 batch-99
Running loss of epoch-3 batch-99 = 0.010730848647654057

Training epoch-3 batch-100
Running loss of epoch-3 batch-100 = 0.01552340853959322

Training epoch-3 batch-101
Running loss of epoch-3 batch-101 = 0.015800338238477707

Training epoch-3 batch-102
Running loss of epoch-3 batch-102 = 0.013692695647478104

Training epoch-3 batch-103
Running loss of epoch-3 batch-103 = 0.01161529403179884

Training epoch-3 batch-104
Running loss of epoch-3 batch-104 = 0.012327156029641628

Training epoch-3 batch-105
Running loss of epoch-3 batch-105 = 0.012790044769644737

Training epoch-3 batch-106
Running loss of epoch-3 batch-106 = 0.011845611035823822

Training epoch-3 batch-107
Running loss of epoch-3 batch-107 = 0.009601535275578499

Training epoch-3 batch-108
Running loss of epoch-3 batch-108 = 0.012695802375674248

Training epoch-3 batch-109
Running loss of epoch-3 batch-109 = 0.013556568883359432

Training epoch-3 batch-110
Running loss of epoch-3 batch-110 = 0.012665100395679474

Training epoch-3 batch-111
Running loss of epoch-3 batch-111 = 0.01329111959785223

Training epoch-3 batch-112
Running loss of epoch-3 batch-112 = 0.012273038737475872

Training epoch-3 batch-113
Running loss of epoch-3 batch-113 = 0.010189847089350224

Training epoch-3 batch-114
Running loss of epoch-3 batch-114 = 0.00900270976126194

Training epoch-3 batch-115
Running loss of epoch-3 batch-115 = 0.015521497465670109

Training epoch-3 batch-116
Running loss of epoch-3 batch-116 = 0.011175368912518024

Training epoch-3 batch-117
Running loss of epoch-3 batch-117 = 0.010465000756084919

Training epoch-3 batch-118
Running loss of epoch-3 batch-118 = 0.014043070375919342

Training epoch-3 batch-119
Running loss of epoch-3 batch-119 = 0.013731644488871098

Training epoch-3 batch-120
Running loss of epoch-3 batch-120 = 0.012157605960965157

Training epoch-3 batch-121
Running loss of epoch-3 batch-121 = 0.012692729942500591

Training epoch-3 batch-122
Running loss of epoch-3 batch-122 = 0.01039383839815855

Training epoch-3 batch-123
Running loss of epoch-3 batch-123 = 0.010512573644518852

Training epoch-3 batch-124
Running loss of epoch-3 batch-124 = 0.013681625947356224

Training epoch-3 batch-125
Running loss of epoch-3 batch-125 = 0.010648277588188648

Training epoch-3 batch-126
Running loss of epoch-3 batch-126 = 0.011784243397414684

Training epoch-3 batch-127
Running loss of epoch-3 batch-127 = 0.010748564265668392

Training epoch-3 batch-128
Running loss of epoch-3 batch-128 = 0.011250548996031284

Training epoch-3 batch-129
Running loss of epoch-3 batch-129 = 0.007873103953897953

Training epoch-3 batch-130
Running loss of epoch-3 batch-130 = 0.010894929990172386

Training epoch-3 batch-131
Running loss of epoch-3 batch-131 = 0.011378214694559574

Training epoch-3 batch-132
Running loss of epoch-3 batch-132 = 0.012586968950927258

Training epoch-3 batch-133
Running loss of epoch-3 batch-133 = 0.007675950415432453

Training epoch-3 batch-134
Running loss of epoch-3 batch-134 = 0.013848083093762398

Training epoch-3 batch-135
Running loss of epoch-3 batch-135 = 0.0092985350638628

Training epoch-3 batch-136
Running loss of epoch-3 batch-136 = 0.011780125088989735

Training epoch-3 batch-137
Running loss of epoch-3 batch-137 = 0.010651448741555214

Training epoch-3 batch-138
Running loss of epoch-3 batch-138 = 0.010762671008706093

Training epoch-3 batch-139
Running loss of epoch-3 batch-139 = 0.011521002277731895

Training epoch-3 batch-140
Running loss of epoch-3 batch-140 = 0.012328920885920525

Training epoch-3 batch-141
Running loss of epoch-3 batch-141 = 0.009719911962747574

Training epoch-3 batch-142
Running loss of epoch-3 batch-142 = 0.0110397944226861

Training epoch-3 batch-143
Running loss of epoch-3 batch-143 = 0.010469873435795307

Training epoch-3 batch-144
Running loss of epoch-3 batch-144 = 0.011283588595688343

Training epoch-3 batch-145
Running loss of epoch-3 batch-145 = 0.008552893996238708

Training epoch-3 batch-146
Running loss of epoch-3 batch-146 = 0.010049538686871529

Training epoch-3 batch-147
Running loss of epoch-3 batch-147 = 0.008183786645531654

Training epoch-3 batch-148
Running loss of epoch-3 batch-148 = 0.011782139539718628

Training epoch-3 batch-149
Running loss of epoch-3 batch-149 = 0.010809769853949547

Training epoch-3 batch-150
Running loss of epoch-3 batch-150 = 0.013222860172390938

Training epoch-3 batch-151
Running loss of epoch-3 batch-151 = 0.009393376298248768

Training epoch-3 batch-152
Running loss of epoch-3 batch-152 = 0.012605204246938229

Training epoch-3 batch-153
Running loss of epoch-3 batch-153 = 0.013244776986539364

Training epoch-3 batch-154
Running loss of epoch-3 batch-154 = 0.013209268450737

Training epoch-3 batch-155
Running loss of epoch-3 batch-155 = 0.013668333180248737

Training epoch-3 batch-156
Running loss of epoch-3 batch-156 = 0.014956838451325893

Training epoch-3 batch-157
Running loss of epoch-3 batch-157 = 0.05591336637735367

Finished training epoch-3.



Average train loss at epoch-3 = 0.012837961083650588

Started Evaluation

Average val loss at epoch-3 = 0.7217457427912833

Accuracy for classes:
Accuracy for class equals is: 94.72 %
Accuracy for class main is: 97.70 %
Accuracy for class setUp is: 82.30 %
Accuracy for class onCreate is: 82.94 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 56.16 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 45.07 %
Accuracy for class execute is: 55.82 %
Accuracy for class get is: 54.10 %

Overall Accuracy = 77.35 %


Best Accuracy = 77.35 % at Epoch-3
Saving model after best epoch-3

Finished Evaluation



Started training epoch-4


Training epoch-4 batch-1
Running loss of epoch-4 batch-1 = 0.011658895760774612

Training epoch-4 batch-2
Running loss of epoch-4 batch-2 = 0.012067564763128757

Training epoch-4 batch-3
Running loss of epoch-4 batch-3 = 0.009611434303224087

Training epoch-4 batch-4
Running loss of epoch-4 batch-4 = 0.009309270419180393

Training epoch-4 batch-5
Running loss of epoch-4 batch-5 = 0.009820138104259968

Training epoch-4 batch-6
Running loss of epoch-4 batch-6 = 0.01113819144666195

Training epoch-4 batch-7
Running loss of epoch-4 batch-7 = 0.007898632436990738

Training epoch-4 batch-8
Running loss of epoch-4 batch-8 = 0.008738141506910324

Training epoch-4 batch-9
Running loss of epoch-4 batch-9 = 0.010290486738085747

Training epoch-4 batch-10
Running loss of epoch-4 batch-10 = 0.01289338432252407

Training epoch-4 batch-11
Running loss of epoch-4 batch-11 = 0.009343918412923813

Training epoch-4 batch-12
Running loss of epoch-4 batch-12 = 0.010039713233709335

Training epoch-4 batch-13
Running loss of epoch-4 batch-13 = 0.011209436692297459

Training epoch-4 batch-14
Running loss of epoch-4 batch-14 = 0.010731257498264313

Training epoch-4 batch-15
Running loss of epoch-4 batch-15 = 0.009085679426789284

Training epoch-4 batch-16
Running loss of epoch-4 batch-16 = 0.010478129610419273

Training epoch-4 batch-17
Running loss of epoch-4 batch-17 = 0.0116982851177454

Training epoch-4 batch-18
Running loss of epoch-4 batch-18 = 0.007307981140911579

Training epoch-4 batch-19
Running loss of epoch-4 batch-19 = 0.009340913966298103

Training epoch-4 batch-20
Running loss of epoch-4 batch-20 = 0.0076275235041975975

Training epoch-4 batch-21
Running loss of epoch-4 batch-21 = 0.009365474805235863

Training epoch-4 batch-22
Running loss of epoch-4 batch-22 = 0.01039839442819357

Training epoch-4 batch-23
Running loss of epoch-4 batch-23 = 0.006912956014275551

Training epoch-4 batch-24
Running loss of epoch-4 batch-24 = 0.009855913929641247

Training epoch-4 batch-25
Running loss of epoch-4 batch-25 = 0.012757783755660057

Training epoch-4 batch-26
Running loss of epoch-4 batch-26 = 0.014726968482136726

Training epoch-4 batch-27
Running loss of epoch-4 batch-27 = 0.00995298009365797

Training epoch-4 batch-28
Running loss of epoch-4 batch-28 = 0.007522575091570616

Training epoch-4 batch-29
Running loss of epoch-4 batch-29 = 0.01183277741074562

Training epoch-4 batch-30
Running loss of epoch-4 batch-30 = 0.009627798572182655

Training epoch-4 batch-31
Running loss of epoch-4 batch-31 = 0.011542907916009426

Training epoch-4 batch-32
Running loss of epoch-4 batch-32 = 0.008749419823288918

Training epoch-4 batch-33
Running loss of epoch-4 batch-33 = 0.009304690174758434

Training epoch-4 batch-34
Running loss of epoch-4 batch-34 = 0.01361762173473835

Training epoch-4 batch-35
Running loss of epoch-4 batch-35 = 0.010475218296051025

Training epoch-4 batch-36
Running loss of epoch-4 batch-36 = 0.011652090586721897

Training epoch-4 batch-37
Running loss of epoch-4 batch-37 = 0.01047857291996479

Training epoch-4 batch-38
Running loss of epoch-4 batch-38 = 0.00924203172326088

Training epoch-4 batch-39
Running loss of epoch-4 batch-39 = 0.01133861392736435

Training epoch-4 batch-40
Running loss of epoch-4 batch-40 = 0.01203208789229393

Training epoch-4 batch-41
Running loss of epoch-4 batch-41 = 0.011095121502876282

Training epoch-4 batch-42
Running loss of epoch-4 batch-42 = 0.008916953578591347

Training epoch-4 batch-43
Running loss of epoch-4 batch-43 = 0.010293042287230492

Training epoch-4 batch-44
Running loss of epoch-4 batch-44 = 0.014050189405679703

Training epoch-4 batch-45
Running loss of epoch-4 batch-45 = 0.011008468456566334

Training epoch-4 batch-46
Running loss of epoch-4 batch-46 = 0.012522335164248943

Training epoch-4 batch-47
Running loss of epoch-4 batch-47 = 0.011810632422566414

Training epoch-4 batch-48
Running loss of epoch-4 batch-48 = 0.008867786265909672

Training epoch-4 batch-49
Running loss of epoch-4 batch-49 = 0.008960708975791931

Training epoch-4 batch-50
Running loss of epoch-4 batch-50 = 0.008928640745580196

Training epoch-4 batch-51
Running loss of epoch-4 batch-51 = 0.010841308161616325

Training epoch-4 batch-52
Running loss of epoch-4 batch-52 = 0.008293983526527882

Training epoch-4 batch-53
Running loss of epoch-4 batch-53 = 0.010419001802802086

Training epoch-4 batch-54
Running loss of epoch-4 batch-54 = 0.012951262295246124

Training epoch-4 batch-55
Running loss of epoch-4 batch-55 = 0.009215539321303368

Training epoch-4 batch-56
Running loss of epoch-4 batch-56 = 0.008261753246188164

Training epoch-4 batch-57
Running loss of epoch-4 batch-57 = 0.008726300671696663

Training epoch-4 batch-58
Running loss of epoch-4 batch-58 = 0.008548236452043056

Training epoch-4 batch-59
Running loss of epoch-4 batch-59 = 0.01151220127940178

Training epoch-4 batch-60
Running loss of epoch-4 batch-60 = 0.00935516320168972

Training epoch-4 batch-61
Running loss of epoch-4 batch-61 = 0.008167214691638947

Training epoch-4 batch-62
Running loss of epoch-4 batch-62 = 0.008266223594546318

Training epoch-4 batch-63
Running loss of epoch-4 batch-63 = 0.00918157771229744

Training epoch-4 batch-64
Running loss of epoch-4 batch-64 = 0.011610124260187149

Training epoch-4 batch-65
Running loss of epoch-4 batch-65 = 0.009297359734773636

Training epoch-4 batch-66
Running loss of epoch-4 batch-66 = 0.013279841281473637

Training epoch-4 batch-67
Running loss of epoch-4 batch-67 = 0.008747086860239506

Training epoch-4 batch-68
Running loss of epoch-4 batch-68 = 0.008930820040404797

Training epoch-4 batch-69
Running loss of epoch-4 batch-69 = 0.010604072362184525

Training epoch-4 batch-70
Running loss of epoch-4 batch-70 = 0.011598454788327217

Training epoch-4 batch-71
Running loss of epoch-4 batch-71 = 0.008398814126849174

Training epoch-4 batch-72
Running loss of epoch-4 batch-72 = 0.01216038316488266

Training epoch-4 batch-73
Running loss of epoch-4 batch-73 = 0.008692778646945953

Training epoch-4 batch-74
Running loss of epoch-4 batch-74 = 0.007138748653233051

Training epoch-4 batch-75
Running loss of epoch-4 batch-75 = 0.008857610635459423

Training epoch-4 batch-76
Running loss of epoch-4 batch-76 = 0.010064108297228813

Training epoch-4 batch-77
Running loss of epoch-4 batch-77 = 0.010793805122375488

Training epoch-4 batch-78
Running loss of epoch-4 batch-78 = 0.007323535159230232

Training epoch-4 batch-79
Running loss of epoch-4 batch-79 = 0.006538220681250095

Training epoch-4 batch-80
Running loss of epoch-4 batch-80 = 0.008154770359396935

Training epoch-4 batch-81
Running loss of epoch-4 batch-81 = 0.009316323325037956

Training epoch-4 batch-82
Running loss of epoch-4 batch-82 = 0.00944746658205986

Training epoch-4 batch-83
Running loss of epoch-4 batch-83 = 0.009581044316291809

Training epoch-4 batch-84
Running loss of epoch-4 batch-84 = 0.00774671183899045

Training epoch-4 batch-85
Running loss of epoch-4 batch-85 = 0.007310414221137762

Training epoch-4 batch-86
Running loss of epoch-4 batch-86 = 0.011607743799686432

Training epoch-4 batch-87
Running loss of epoch-4 batch-87 = 0.009910362772643566

Training epoch-4 batch-88
Running loss of epoch-4 batch-88 = 0.01180336531251669

Training epoch-4 batch-89
Running loss of epoch-4 batch-89 = 0.006362582091242075

Training epoch-4 batch-90
Running loss of epoch-4 batch-90 = 0.01123259961605072

Training epoch-4 batch-91
Running loss of epoch-4 batch-91 = 0.011296956799924374

Training epoch-4 batch-92
Running loss of epoch-4 batch-92 = 0.0074677313677966595

Training epoch-4 batch-93
Running loss of epoch-4 batch-93 = 0.011191364377737045

Training epoch-4 batch-94
Running loss of epoch-4 batch-94 = 0.012206240557134151

Training epoch-4 batch-95
Running loss of epoch-4 batch-95 = 0.008019386790692806

Training epoch-4 batch-96
Running loss of epoch-4 batch-96 = 0.005258789751678705

Training epoch-4 batch-97
Running loss of epoch-4 batch-97 = 0.009878389537334442

Training epoch-4 batch-98
Running loss of epoch-4 batch-98 = 0.011268170550465584

Training epoch-4 batch-99
Running loss of epoch-4 batch-99 = 0.011519299820065498

Training epoch-4 batch-100
Running loss of epoch-4 batch-100 = 0.011002562008798122

Training epoch-4 batch-101
Running loss of epoch-4 batch-101 = 0.013512752018868923

Training epoch-4 batch-102
Running loss of epoch-4 batch-102 = 0.012516746297478676

Training epoch-4 batch-103
Running loss of epoch-4 batch-103 = 0.00922200083732605

Training epoch-4 batch-104
Running loss of epoch-4 batch-104 = 0.01030544564127922

Training epoch-4 batch-105
Running loss of epoch-4 batch-105 = 0.00991961732506752

Training epoch-4 batch-106
Running loss of epoch-4 batch-106 = 0.008035961538553238

Training epoch-4 batch-107
Running loss of epoch-4 batch-107 = 0.011586646549403667

Training epoch-4 batch-108
Running loss of epoch-4 batch-108 = 0.008528746664524078

Training epoch-4 batch-109
Running loss of epoch-4 batch-109 = 0.010648219846189022

Training epoch-4 batch-110
Running loss of epoch-4 batch-110 = 0.010036005638539791

Training epoch-4 batch-111
Running loss of epoch-4 batch-111 = 0.01175812166184187

Training epoch-4 batch-112
Running loss of epoch-4 batch-112 = 0.009495886974036694

Training epoch-4 batch-113
Running loss of epoch-4 batch-113 = 0.010614569298923016

Training epoch-4 batch-114
Running loss of epoch-4 batch-114 = 0.010582231916487217

Training epoch-4 batch-115
Running loss of epoch-4 batch-115 = 0.015133765526115894

Training epoch-4 batch-116
Running loss of epoch-4 batch-116 = 0.011201715096831322

Training epoch-4 batch-117
Running loss of epoch-4 batch-117 = 0.010671127587556839

Training epoch-4 batch-118
Running loss of epoch-4 batch-118 = 0.012261545285582542

Training epoch-4 batch-119
Running loss of epoch-4 batch-119 = 0.007293294183909893

Training epoch-4 batch-120
Running loss of epoch-4 batch-120 = 0.009388609789311886

Training epoch-4 batch-121
Running loss of epoch-4 batch-121 = 0.0073752813041210175

Training epoch-4 batch-122
Running loss of epoch-4 batch-122 = 0.012689458206295967

Training epoch-4 batch-123
Running loss of epoch-4 batch-123 = 0.012330194003880024

Training epoch-4 batch-124
Running loss of epoch-4 batch-124 = 0.008101917803287506

Training epoch-4 batch-125
Running loss of epoch-4 batch-125 = 0.008817209862172604

Training epoch-4 batch-126
Running loss of epoch-4 batch-126 = 0.009863252751529217

Training epoch-4 batch-127
Running loss of epoch-4 batch-127 = 0.013384058140218258

Training epoch-4 batch-128
Running loss of epoch-4 batch-128 = 0.009380470030009747

Training epoch-4 batch-129
Running loss of epoch-4 batch-129 = 0.0070519051514565945

Training epoch-4 batch-130
Running loss of epoch-4 batch-130 = 0.008763596415519714

Training epoch-4 batch-131
Running loss of epoch-4 batch-131 = 0.009913398884236813

Training epoch-4 batch-132
Running loss of epoch-4 batch-132 = 0.013866240158677101

Training epoch-4 batch-133
Running loss of epoch-4 batch-133 = 0.010438838042318821

Training epoch-4 batch-134
Running loss of epoch-4 batch-134 = 0.011115580797195435

Training epoch-4 batch-135
Running loss of epoch-4 batch-135 = 0.010880127549171448

Training epoch-4 batch-136
Running loss of epoch-4 batch-136 = 0.012087691575288773

Training epoch-4 batch-137
Running loss of epoch-4 batch-137 = 0.011335235089063644

Training epoch-4 batch-138
Running loss of epoch-4 batch-138 = 0.012923158705234528

Training epoch-4 batch-139
Running loss of epoch-4 batch-139 = 0.011860727332532406

Training epoch-4 batch-140
Running loss of epoch-4 batch-140 = 0.0124735776335001

Training epoch-4 batch-141
Running loss of epoch-4 batch-141 = 0.011853573843836784

Training epoch-4 batch-142
Running loss of epoch-4 batch-142 = 0.007839826866984367

Training epoch-4 batch-143
Running loss of epoch-4 batch-143 = 0.012993169017136097

Training epoch-4 batch-144
Running loss of epoch-4 batch-144 = 0.006010851357132196

Training epoch-4 batch-145
Running loss of epoch-4 batch-145 = 0.011342517100274563

Training epoch-4 batch-146
Running loss of epoch-4 batch-146 = 0.0060819704085588455

Training epoch-4 batch-147
Running loss of epoch-4 batch-147 = 0.008327430114150047

Training epoch-4 batch-148
Running loss of epoch-4 batch-148 = 0.011160408146679401

Training epoch-4 batch-149
Running loss of epoch-4 batch-149 = 0.010896663181483746

Training epoch-4 batch-150
Running loss of epoch-4 batch-150 = 0.010361075401306152

Training epoch-4 batch-151
Running loss of epoch-4 batch-151 = 0.0066436477936804295

Training epoch-4 batch-152
Running loss of epoch-4 batch-152 = 0.009608923457562923

Training epoch-4 batch-153
Running loss of epoch-4 batch-153 = 0.0075791627168655396

Training epoch-4 batch-154
Running loss of epoch-4 batch-154 = 0.007748285308480263

Training epoch-4 batch-155
Running loss of epoch-4 batch-155 = 0.0060967011377215385

Training epoch-4 batch-156
Running loss of epoch-4 batch-156 = 0.011548776179552078

Training epoch-4 batch-157
Running loss of epoch-4 batch-157 = 0.03813685476779938

Finished training epoch-4.



Average train loss at epoch-4 = 0.010115069857239723

Started Evaluation

Average val loss at epoch-4 = 0.6381531479082218

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 94.43 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 83.69 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 61.87 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 69.51 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 75.90 %

Overall Accuracy = 81.04 %


Best Accuracy = 81.04 % at Epoch-4
Saving model after best epoch-4

Finished Evaluation



Started training epoch-5


Training epoch-5 batch-1
Running loss of epoch-5 batch-1 = 0.00822499766945839

Training epoch-5 batch-2
Running loss of epoch-5 batch-2 = 0.009102016687393188

Training epoch-5 batch-3
Running loss of epoch-5 batch-3 = 0.0059851184487342834

Training epoch-5 batch-4
Running loss of epoch-5 batch-4 = 0.008699110709130764

Training epoch-5 batch-5
Running loss of epoch-5 batch-5 = 0.007514709606766701

Training epoch-5 batch-6
Running loss of epoch-5 batch-6 = 0.008147934451699257

Training epoch-5 batch-7
Running loss of epoch-5 batch-7 = 0.005854357965290546

Training epoch-5 batch-8
Running loss of epoch-5 batch-8 = 0.01153408270329237

Training epoch-5 batch-9
Running loss of epoch-5 batch-9 = 0.009319043718278408

Training epoch-5 batch-10
Running loss of epoch-5 batch-10 = 0.009576681070029736

Training epoch-5 batch-11
Running loss of epoch-5 batch-11 = 0.012034574523568153

Training epoch-5 batch-12
Running loss of epoch-5 batch-12 = 0.00971334706991911

Training epoch-5 batch-13
Running loss of epoch-5 batch-13 = 0.010827498510479927

Training epoch-5 batch-14
Running loss of epoch-5 batch-14 = 0.007346838712692261

Training epoch-5 batch-15
Running loss of epoch-5 batch-15 = 0.008046175353229046

Training epoch-5 batch-16
Running loss of epoch-5 batch-16 = 0.008843164891004562

Training epoch-5 batch-17
Running loss of epoch-5 batch-17 = 0.009902941063046455

Training epoch-5 batch-18
Running loss of epoch-5 batch-18 = 0.010375235229730606

Training epoch-5 batch-19
Running loss of epoch-5 batch-19 = 0.013169530779123306

Training epoch-5 batch-20
Running loss of epoch-5 batch-20 = 0.008982835337519646

Training epoch-5 batch-21
Running loss of epoch-5 batch-21 = 0.009405127726495266

Training epoch-5 batch-22
Running loss of epoch-5 batch-22 = 0.006999771110713482

Training epoch-5 batch-23
Running loss of epoch-5 batch-23 = 0.0077000828459858894

Training epoch-5 batch-24
Running loss of epoch-5 batch-24 = 0.005286614876240492

Training epoch-5 batch-25
Running loss of epoch-5 batch-25 = 0.010616287589073181

Training epoch-5 batch-26
Running loss of epoch-5 batch-26 = 0.012026147916913033

Training epoch-5 batch-27
Running loss of epoch-5 batch-27 = 0.008236978203058243

Training epoch-5 batch-28
Running loss of epoch-5 batch-28 = 0.00716995308175683

Training epoch-5 batch-29
Running loss of epoch-5 batch-29 = 0.010247191414237022

Training epoch-5 batch-30
Running loss of epoch-5 batch-30 = 0.0063821845687925816

Training epoch-5 batch-31
Running loss of epoch-5 batch-31 = 0.009716393426060677

Training epoch-5 batch-32
Running loss of epoch-5 batch-32 = 0.006339072249829769

Training epoch-5 batch-33
Running loss of epoch-5 batch-33 = 0.007862456142902374

Training epoch-5 batch-34
Running loss of epoch-5 batch-34 = 0.007939779199659824

Training epoch-5 batch-35
Running loss of epoch-5 batch-35 = 0.009730379097163677

Training epoch-5 batch-36
Running loss of epoch-5 batch-36 = 0.010480361990630627

Training epoch-5 batch-37
Running loss of epoch-5 batch-37 = 0.011685801669955254

Training epoch-5 batch-38
Running loss of epoch-5 batch-38 = 0.010348174721002579

Training epoch-5 batch-39
Running loss of epoch-5 batch-39 = 0.007483441848307848

Training epoch-5 batch-40
Running loss of epoch-5 batch-40 = 0.004873351659625769

Training epoch-5 batch-41
Running loss of epoch-5 batch-41 = 0.006373461335897446

Training epoch-5 batch-42
Running loss of epoch-5 batch-42 = 0.006206049118191004

Training epoch-5 batch-43
Running loss of epoch-5 batch-43 = 0.011103104799985886

Training epoch-5 batch-44
Running loss of epoch-5 batch-44 = 0.008867575787007809

Training epoch-5 batch-45
Running loss of epoch-5 batch-45 = 0.008843604475259781

Training epoch-5 batch-46
Running loss of epoch-5 batch-46 = 0.0071538956835865974

Training epoch-5 batch-47
Running loss of epoch-5 batch-47 = 0.007531255483627319

Training epoch-5 batch-48
Running loss of epoch-5 batch-48 = 0.008961882442235947

Training epoch-5 batch-49
Running loss of epoch-5 batch-49 = 0.007696159183979034

Training epoch-5 batch-50
Running loss of epoch-5 batch-50 = 0.008647391572594643

Training epoch-5 batch-51
Running loss of epoch-5 batch-51 = 0.008239302784204483

Training epoch-5 batch-52
Running loss of epoch-5 batch-52 = 0.00833192653954029

Training epoch-5 batch-53
Running loss of epoch-5 batch-53 = 0.007158416323363781

Training epoch-5 batch-54
Running loss of epoch-5 batch-54 = 0.007662512827664614

Training epoch-5 batch-55
Running loss of epoch-5 batch-55 = 0.00848509930074215

Training epoch-5 batch-56
Running loss of epoch-5 batch-56 = 0.00851683970540762

Training epoch-5 batch-57
Running loss of epoch-5 batch-57 = 0.00979815423488617

Training epoch-5 batch-58
Running loss of epoch-5 batch-58 = 0.00727148586884141

Training epoch-5 batch-59
Running loss of epoch-5 batch-59 = 0.0063458592630922794

Training epoch-5 batch-60
Running loss of epoch-5 batch-60 = 0.013607853092253208

Training epoch-5 batch-61
Running loss of epoch-5 batch-61 = 0.010020590387284756

Training epoch-5 batch-62
Running loss of epoch-5 batch-62 = 0.005943391006439924

Training epoch-5 batch-63
Running loss of epoch-5 batch-63 = 0.010343169793486595

Training epoch-5 batch-64
Running loss of epoch-5 batch-64 = 0.007118280045688152

Training epoch-5 batch-65
Running loss of epoch-5 batch-65 = 0.009856813587248325

Training epoch-5 batch-66
Running loss of epoch-5 batch-66 = 0.011903543956577778

Training epoch-5 batch-67
Running loss of epoch-5 batch-67 = 0.007964414544403553

Training epoch-5 batch-68
Running loss of epoch-5 batch-68 = 0.010130074806511402

Training epoch-5 batch-69
Running loss of epoch-5 batch-69 = 0.007182367146015167

Training epoch-5 batch-70
Running loss of epoch-5 batch-70 = 0.007638521026819944

Training epoch-5 batch-71
Running loss of epoch-5 batch-71 = 0.010818417184054852

Training epoch-5 batch-72
Running loss of epoch-5 batch-72 = 0.011092901229858398

Training epoch-5 batch-73
Running loss of epoch-5 batch-73 = 0.007929321378469467

Training epoch-5 batch-74
Running loss of epoch-5 batch-74 = 0.008333709090948105

Training epoch-5 batch-75
Running loss of epoch-5 batch-75 = 0.01081487350165844

Training epoch-5 batch-76
Running loss of epoch-5 batch-76 = 0.007893221452832222

Training epoch-5 batch-77
Running loss of epoch-5 batch-77 = 0.012869061902165413

Training epoch-5 batch-78
Running loss of epoch-5 batch-78 = 0.007384589407593012

Training epoch-5 batch-79
Running loss of epoch-5 batch-79 = 0.008169582113623619

Training epoch-5 batch-80
Running loss of epoch-5 batch-80 = 0.006584016140550375

Training epoch-5 batch-81
Running loss of epoch-5 batch-81 = 0.006986659485846758

Training epoch-5 batch-82
Running loss of epoch-5 batch-82 = 0.007577716372907162

Training epoch-5 batch-83
Running loss of epoch-5 batch-83 = 0.011476651765406132

Training epoch-5 batch-84
Running loss of epoch-5 batch-84 = 0.006998051889240742

Training epoch-5 batch-85
Running loss of epoch-5 batch-85 = 0.006795598194003105

Training epoch-5 batch-86
Running loss of epoch-5 batch-86 = 0.011809347197413445

Training epoch-5 batch-87
Running loss of epoch-5 batch-87 = 0.006395068019628525

Training epoch-5 batch-88
Running loss of epoch-5 batch-88 = 0.007633752189576626

Training epoch-5 batch-89
Running loss of epoch-5 batch-89 = 0.010243979282677174

Training epoch-5 batch-90
Running loss of epoch-5 batch-90 = 0.010878035798668861

Training epoch-5 batch-91
Running loss of epoch-5 batch-91 = 0.007853694260120392

Training epoch-5 batch-92
Running loss of epoch-5 batch-92 = 0.007083530072122812

Training epoch-5 batch-93
Running loss of epoch-5 batch-93 = 0.008111177012324333

Training epoch-5 batch-94
Running loss of epoch-5 batch-94 = 0.0064281076192855835

Training epoch-5 batch-95
Running loss of epoch-5 batch-95 = 0.008345624431967735

Training epoch-5 batch-96
Running loss of epoch-5 batch-96 = 0.005957720801234245

Training epoch-5 batch-97
Running loss of epoch-5 batch-97 = 0.010419720783829689

Training epoch-5 batch-98
Running loss of epoch-5 batch-98 = 0.00700814975425601

Training epoch-5 batch-99
Running loss of epoch-5 batch-99 = 0.006264828611165285

Training epoch-5 batch-100
Running loss of epoch-5 batch-100 = 0.006980314385145903

Training epoch-5 batch-101
Running loss of epoch-5 batch-101 = 0.009312513284385204

Training epoch-5 batch-102
Running loss of epoch-5 batch-102 = 0.007270551286637783

Training epoch-5 batch-103
Running loss of epoch-5 batch-103 = 0.012750458903610706

Training epoch-5 batch-104
Running loss of epoch-5 batch-104 = 0.006130608730018139

Training epoch-5 batch-105
Running loss of epoch-5 batch-105 = 0.0107647106051445

Training epoch-5 batch-106
Running loss of epoch-5 batch-106 = 0.007301378063857555

Training epoch-5 batch-107
Running loss of epoch-5 batch-107 = 0.01406329870223999

Training epoch-5 batch-108
Running loss of epoch-5 batch-108 = 0.0072324532084167

Training epoch-5 batch-109
Running loss of epoch-5 batch-109 = 0.008472749032080173

Training epoch-5 batch-110
Running loss of epoch-5 batch-110 = 0.008203697390854359

Training epoch-5 batch-111
Running loss of epoch-5 batch-111 = 0.008897353895008564

Training epoch-5 batch-112
Running loss of epoch-5 batch-112 = 0.009863899089396

Training epoch-5 batch-113
Running loss of epoch-5 batch-113 = 0.007296592462807894

Training epoch-5 batch-114
Running loss of epoch-5 batch-114 = 0.004155716393142939

Training epoch-5 batch-115
Running loss of epoch-5 batch-115 = 0.005500081926584244

Training epoch-5 batch-116
Running loss of epoch-5 batch-116 = 0.007012324873358011

Training epoch-5 batch-117
Running loss of epoch-5 batch-117 = 0.010253342799842358

Training epoch-5 batch-118
Running loss of epoch-5 batch-118 = 0.008806186728179455

Training epoch-5 batch-119
Running loss of epoch-5 batch-119 = 0.00997764803469181

Training epoch-5 batch-120
Running loss of epoch-5 batch-120 = 0.007441017776727676

Training epoch-5 batch-121
Running loss of epoch-5 batch-121 = 0.009709784761071205

Training epoch-5 batch-122
Running loss of epoch-5 batch-122 = 0.011636179871857166

Training epoch-5 batch-123
Running loss of epoch-5 batch-123 = 0.005838111508637667

Training epoch-5 batch-124
Running loss of epoch-5 batch-124 = 0.012188885360956192

Training epoch-5 batch-125
Running loss of epoch-5 batch-125 = 0.007921171374619007

Training epoch-5 batch-126
Running loss of epoch-5 batch-126 = 0.006515109911561012

Training epoch-5 batch-127
Running loss of epoch-5 batch-127 = 0.009213605895638466

Training epoch-5 batch-128
Running loss of epoch-5 batch-128 = 0.009186375886201859

Training epoch-5 batch-129
Running loss of epoch-5 batch-129 = 0.010741017758846283

Training epoch-5 batch-130
Running loss of epoch-5 batch-130 = 0.008958149701356888

Training epoch-5 batch-131
Running loss of epoch-5 batch-131 = 0.011035455390810966

Training epoch-5 batch-132
Running loss of epoch-5 batch-132 = 0.007826602086424828

Training epoch-5 batch-133
Running loss of epoch-5 batch-133 = 0.007694690488278866

Training epoch-5 batch-134
Running loss of epoch-5 batch-134 = 0.008478178642690182

Training epoch-5 batch-135
Running loss of epoch-5 batch-135 = 0.007847292348742485

Training epoch-5 batch-136
Running loss of epoch-5 batch-136 = 0.0040217614732682705

Training epoch-5 batch-137
Running loss of epoch-5 batch-137 = 0.009769911877810955

Training epoch-5 batch-138
Running loss of epoch-5 batch-138 = 0.005516550038009882

Training epoch-5 batch-139
Running loss of epoch-5 batch-139 = 0.0115765780210495

Training epoch-5 batch-140
Running loss of epoch-5 batch-140 = 0.0077023254707455635

Training epoch-5 batch-141
Running loss of epoch-5 batch-141 = 0.0076078264974057674

Training epoch-5 batch-142
Running loss of epoch-5 batch-142 = 0.010122937150299549

Training epoch-5 batch-143
Running loss of epoch-5 batch-143 = 0.006698497571051121

Training epoch-5 batch-144
Running loss of epoch-5 batch-144 = 0.007759022992104292

Training epoch-5 batch-145
Running loss of epoch-5 batch-145 = 0.01180279441177845

Training epoch-5 batch-146
Running loss of epoch-5 batch-146 = 0.007416511420160532

Training epoch-5 batch-147
Running loss of epoch-5 batch-147 = 0.008262391202151775

Training epoch-5 batch-148
Running loss of epoch-5 batch-148 = 0.009958540089428425

Training epoch-5 batch-149
Running loss of epoch-5 batch-149 = 0.008463352918624878

Training epoch-5 batch-150
Running loss of epoch-5 batch-150 = 0.008538991212844849

Training epoch-5 batch-151
Running loss of epoch-5 batch-151 = 0.00886707752943039

Training epoch-5 batch-152
Running loss of epoch-5 batch-152 = 0.007520800456404686

Training epoch-5 batch-153
Running loss of epoch-5 batch-153 = 0.010371278040111065

Training epoch-5 batch-154
Running loss of epoch-5 batch-154 = 0.006375676486641169

Training epoch-5 batch-155
Running loss of epoch-5 batch-155 = 0.00898804608732462

Training epoch-5 batch-156
Running loss of epoch-5 batch-156 = 0.008400549180805683

Training epoch-5 batch-157
Running loss of epoch-5 batch-157 = 0.012016486376523972

Finished training epoch-5.



Average train loss at epoch-5 = 0.008614482554793359

Started Evaluation

Average val loss at epoch-5 = 0.5961894384511796

Accuracy for classes:
Accuracy for class equals is: 95.87 %
Accuracy for class main is: 98.36 %
Accuracy for class setUp is: 94.59 %
Accuracy for class onCreate is: 84.86 %
Accuracy for class toString is: 84.30 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 38.55 %
Accuracy for class get is: 73.33 %

Overall Accuracy = 81.64 %


Best Accuracy = 81.64 % at Epoch-5
Saving model after best epoch-5

Finished Evaluation



Started training epoch-6


Training epoch-6 batch-1
Running loss of epoch-6 batch-1 = 0.006055008620023727

Training epoch-6 batch-2
Running loss of epoch-6 batch-2 = 0.005128964781761169

Training epoch-6 batch-3
Running loss of epoch-6 batch-3 = 0.009318655356764793

Training epoch-6 batch-4
Running loss of epoch-6 batch-4 = 0.007920986972749233

Training epoch-6 batch-5
Running loss of epoch-6 batch-5 = 0.007754269056022167

Training epoch-6 batch-6
Running loss of epoch-6 batch-6 = 0.00915900245308876

Training epoch-6 batch-7
Running loss of epoch-6 batch-7 = 0.008039501495659351

Training epoch-6 batch-8
Running loss of epoch-6 batch-8 = 0.008596665225923061

Training epoch-6 batch-9
Running loss of epoch-6 batch-9 = 0.006440491881221533

Training epoch-6 batch-10
Running loss of epoch-6 batch-10 = 0.009046684019267559

Training epoch-6 batch-11
Running loss of epoch-6 batch-11 = 0.005419209599494934

Training epoch-6 batch-12
Running loss of epoch-6 batch-12 = 0.007117615081369877

Training epoch-6 batch-13
Running loss of epoch-6 batch-13 = 0.006125175394117832

Training epoch-6 batch-14
Running loss of epoch-6 batch-14 = 0.00944291427731514

Training epoch-6 batch-15
Running loss of epoch-6 batch-15 = 0.008007529191672802

Training epoch-6 batch-16
Running loss of epoch-6 batch-16 = 0.0074321553111076355

Training epoch-6 batch-17
Running loss of epoch-6 batch-17 = 0.006000994239002466

Training epoch-6 batch-18
Running loss of epoch-6 batch-18 = 0.00608832947909832

Training epoch-6 batch-19
Running loss of epoch-6 batch-19 = 0.008686673827469349

Training epoch-6 batch-20
Running loss of epoch-6 batch-20 = 0.004946683533489704

Training epoch-6 batch-21
Running loss of epoch-6 batch-21 = 0.00586322695016861

Training epoch-6 batch-22
Running loss of epoch-6 batch-22 = 0.006873037200421095

Training epoch-6 batch-23
Running loss of epoch-6 batch-23 = 0.006944009102880955

Training epoch-6 batch-24
Running loss of epoch-6 batch-24 = 0.007294905371963978

Training epoch-6 batch-25
Running loss of epoch-6 batch-25 = 0.004369700793176889

Training epoch-6 batch-26
Running loss of epoch-6 batch-26 = 0.008927692659199238

Training epoch-6 batch-27
Running loss of epoch-6 batch-27 = 0.007866487838327885

Training epoch-6 batch-28
Running loss of epoch-6 batch-28 = 0.006919028703123331

Training epoch-6 batch-29
Running loss of epoch-6 batch-29 = 0.007011433597654104

Training epoch-6 batch-30
Running loss of epoch-6 batch-30 = 0.007893812842667103

Training epoch-6 batch-31
Running loss of epoch-6 batch-31 = 0.007810827810317278

Training epoch-6 batch-32
Running loss of epoch-6 batch-32 = 0.004417847376316786

Training epoch-6 batch-33
Running loss of epoch-6 batch-33 = 0.007255761884152889

Training epoch-6 batch-34
Running loss of epoch-6 batch-34 = 0.006349500268697739

Training epoch-6 batch-35
Running loss of epoch-6 batch-35 = 0.0065154265612363815

Training epoch-6 batch-36
Running loss of epoch-6 batch-36 = 0.006569833494722843

Training epoch-6 batch-37
Running loss of epoch-6 batch-37 = 0.006243860814720392

Training epoch-6 batch-38
Running loss of epoch-6 batch-38 = 0.009074956178665161

Training epoch-6 batch-39
Running loss of epoch-6 batch-39 = 0.007670899853110313

Training epoch-6 batch-40
Running loss of epoch-6 batch-40 = 0.007579577621072531

Training epoch-6 batch-41
Running loss of epoch-6 batch-41 = 0.005673722829669714

Training epoch-6 batch-42
Running loss of epoch-6 batch-42 = 0.010677644982933998

Training epoch-6 batch-43
Running loss of epoch-6 batch-43 = 0.006486786529421806

Training epoch-6 batch-44
Running loss of epoch-6 batch-44 = 0.007236483972519636

Training epoch-6 batch-45
Running loss of epoch-6 batch-45 = 0.004611189942806959

Training epoch-6 batch-46
Running loss of epoch-6 batch-46 = 0.009750357829034328

Training epoch-6 batch-47
Running loss of epoch-6 batch-47 = 0.006688359193503857

Training epoch-6 batch-48
Running loss of epoch-6 batch-48 = 0.007269479800015688

Training epoch-6 batch-49
Running loss of epoch-6 batch-49 = 0.004613655619323254

Training epoch-6 batch-50
Running loss of epoch-6 batch-50 = 0.008033331483602524

Training epoch-6 batch-51
Running loss of epoch-6 batch-51 = 0.005587625317275524

Training epoch-6 batch-52
Running loss of epoch-6 batch-52 = 0.006876053754240274

Training epoch-6 batch-53
Running loss of epoch-6 batch-53 = 0.007958505302667618

Training epoch-6 batch-54
Running loss of epoch-6 batch-54 = 0.00820990838110447

Training epoch-6 batch-55
Running loss of epoch-6 batch-55 = 0.00892578810453415

Training epoch-6 batch-56
Running loss of epoch-6 batch-56 = 0.005172243341803551

Training epoch-6 batch-57
Running loss of epoch-6 batch-57 = 0.009966278448700905

Training epoch-6 batch-58
Running loss of epoch-6 batch-58 = 0.006389223039150238

Training epoch-6 batch-59
Running loss of epoch-6 batch-59 = 0.009936043992638588

Training epoch-6 batch-60
Running loss of epoch-6 batch-60 = 0.008865775540471077

Training epoch-6 batch-61
Running loss of epoch-6 batch-61 = 0.009385977871716022

Training epoch-6 batch-62
Running loss of epoch-6 batch-62 = 0.00937808956950903

Training epoch-6 batch-63
Running loss of epoch-6 batch-63 = 0.005572053138166666

Training epoch-6 batch-64
Running loss of epoch-6 batch-64 = 0.005918016191571951

Training epoch-6 batch-65
Running loss of epoch-6 batch-65 = 0.007233364507555962

Training epoch-6 batch-66
Running loss of epoch-6 batch-66 = 0.010625865310430527

Training epoch-6 batch-67
Running loss of epoch-6 batch-67 = 0.006786038167774677

Training epoch-6 batch-68
Running loss of epoch-6 batch-68 = 0.0067646377719938755

Training epoch-6 batch-69
Running loss of epoch-6 batch-69 = 0.008306868374347687

Training epoch-6 batch-70
Running loss of epoch-6 batch-70 = 0.005806205794215202

Training epoch-6 batch-71
Running loss of epoch-6 batch-71 = 0.008596409112215042

Training epoch-6 batch-72
Running loss of epoch-6 batch-72 = 0.005771183874458075

Training epoch-6 batch-73
Running loss of epoch-6 batch-73 = 0.007213052827864885

Training epoch-6 batch-74
Running loss of epoch-6 batch-74 = 0.007202042266726494

Training epoch-6 batch-75
Running loss of epoch-6 batch-75 = 0.005605161655694246

Training epoch-6 batch-76
Running loss of epoch-6 batch-76 = 0.010906866751611233

Training epoch-6 batch-77
Running loss of epoch-6 batch-77 = 0.004475160036236048

Training epoch-6 batch-78
Running loss of epoch-6 batch-78 = 0.011820518411695957

Training epoch-6 batch-79
Running loss of epoch-6 batch-79 = 0.005507836118340492

Training epoch-6 batch-80
Running loss of epoch-6 batch-80 = 0.005226260982453823

Training epoch-6 batch-81
Running loss of epoch-6 batch-81 = 0.006194329354912043

Training epoch-6 batch-82
Running loss of epoch-6 batch-82 = 0.010406398214399815

Training epoch-6 batch-83
Running loss of epoch-6 batch-83 = 0.009027242660522461

Training epoch-6 batch-84
Running loss of epoch-6 batch-84 = 0.008581578731536865

Training epoch-6 batch-85
Running loss of epoch-6 batch-85 = 0.006438593380153179

Training epoch-6 batch-86
Running loss of epoch-6 batch-86 = 0.008069540373980999

Training epoch-6 batch-87
Running loss of epoch-6 batch-87 = 0.008751138113439083

Training epoch-6 batch-88
Running loss of epoch-6 batch-88 = 0.008959854021668434

Training epoch-6 batch-89
Running loss of epoch-6 batch-89 = 0.007163673173636198

Training epoch-6 batch-90
Running loss of epoch-6 batch-90 = 0.0077252634800970554

Training epoch-6 batch-91
Running loss of epoch-6 batch-91 = 0.01103892084211111

Training epoch-6 batch-92
Running loss of epoch-6 batch-92 = 0.005403152666985989

Training epoch-6 batch-93
Running loss of epoch-6 batch-93 = 0.010175422765314579

Training epoch-6 batch-94
Running loss of epoch-6 batch-94 = 0.0072794267907738686

Training epoch-6 batch-95
Running loss of epoch-6 batch-95 = 0.00627285148948431

Training epoch-6 batch-96
Running loss of epoch-6 batch-96 = 0.008801800198853016

Training epoch-6 batch-97
Running loss of epoch-6 batch-97 = 0.007124342489987612

Training epoch-6 batch-98
Running loss of epoch-6 batch-98 = 0.0078963628038764

Training epoch-6 batch-99
Running loss of epoch-6 batch-99 = 0.009591009467840195

Training epoch-6 batch-100
Running loss of epoch-6 batch-100 = 0.006311399862170219

Training epoch-6 batch-101
Running loss of epoch-6 batch-101 = 0.006881507113575935

Training epoch-6 batch-102
Running loss of epoch-6 batch-102 = 0.007977686822414398

Training epoch-6 batch-103
Running loss of epoch-6 batch-103 = 0.012662786990404129

Training epoch-6 batch-104
Running loss of epoch-6 batch-104 = 0.006545155309140682

Training epoch-6 batch-105
Running loss of epoch-6 batch-105 = 0.0058271936140954494

Training epoch-6 batch-106
Running loss of epoch-6 batch-106 = 0.005151601042598486

Training epoch-6 batch-107
Running loss of epoch-6 batch-107 = 0.00997191108763218

Training epoch-6 batch-108
Running loss of epoch-6 batch-108 = 0.006847186014056206

Training epoch-6 batch-109
Running loss of epoch-6 batch-109 = 0.006404423154890537

Training epoch-6 batch-110
Running loss of epoch-6 batch-110 = 0.005336622707545757

Training epoch-6 batch-111
Running loss of epoch-6 batch-111 = 0.006918223574757576

Training epoch-6 batch-112
Running loss of epoch-6 batch-112 = 0.008427047170698643

Training epoch-6 batch-113
Running loss of epoch-6 batch-113 = 0.006503176875412464

Training epoch-6 batch-114
Running loss of epoch-6 batch-114 = 0.0050336746498942375

Training epoch-6 batch-115
Running loss of epoch-6 batch-115 = 0.008116105571389198

Training epoch-6 batch-116
Running loss of epoch-6 batch-116 = 0.00565141998231411

Training epoch-6 batch-117
Running loss of epoch-6 batch-117 = 0.00856101792305708

Training epoch-6 batch-118
Running loss of epoch-6 batch-118 = 0.0076286084949970245

Training epoch-6 batch-119
Running loss of epoch-6 batch-119 = 0.0030414960347115993

Training epoch-6 batch-120
Running loss of epoch-6 batch-120 = 0.0057203578762710094

Training epoch-6 batch-121
Running loss of epoch-6 batch-121 = 0.006647882051765919

Training epoch-6 batch-122
Running loss of epoch-6 batch-122 = 0.008149057626724243

Training epoch-6 batch-123
Running loss of epoch-6 batch-123 = 0.00854997057467699

Training epoch-6 batch-124
Running loss of epoch-6 batch-124 = 0.006236232351511717

Training epoch-6 batch-125
Running loss of epoch-6 batch-125 = 0.011358770541846752

Training epoch-6 batch-126
Running loss of epoch-6 batch-126 = 0.008874161168932915

Training epoch-6 batch-127
Running loss of epoch-6 batch-127 = 0.008424370549619198

Training epoch-6 batch-128
Running loss of epoch-6 batch-128 = 0.006347750313580036

Training epoch-6 batch-129
Running loss of epoch-6 batch-129 = 0.007183738052845001

Training epoch-6 batch-130
Running loss of epoch-6 batch-130 = 0.00548666063696146

Training epoch-6 batch-131
Running loss of epoch-6 batch-131 = 0.008309374563395977

Training epoch-6 batch-132
Running loss of epoch-6 batch-132 = 0.01236842293292284

Training epoch-6 batch-133
Running loss of epoch-6 batch-133 = 0.008005154319107533

Training epoch-6 batch-134
Running loss of epoch-6 batch-134 = 0.007688178215175867

Training epoch-6 batch-135
Running loss of epoch-6 batch-135 = 0.00834729615598917

Training epoch-6 batch-136
Running loss of epoch-6 batch-136 = 0.006557373329997063

Training epoch-6 batch-137
Running loss of epoch-6 batch-137 = 0.007260030601173639

Training epoch-6 batch-138
Running loss of epoch-6 batch-138 = 0.00436065997928381

Training epoch-6 batch-139
Running loss of epoch-6 batch-139 = 0.00635878462344408

Training epoch-6 batch-140
Running loss of epoch-6 batch-140 = 0.007839939557015896

Training epoch-6 batch-141
Running loss of epoch-6 batch-141 = 0.006390946917235851

Training epoch-6 batch-142
Running loss of epoch-6 batch-142 = 0.011925664730370045

Training epoch-6 batch-143
Running loss of epoch-6 batch-143 = 0.005197920836508274

Training epoch-6 batch-144
Running loss of epoch-6 batch-144 = 0.007250994443893433

Training epoch-6 batch-145
Running loss of epoch-6 batch-145 = 0.007511449512094259

Training epoch-6 batch-146
Running loss of epoch-6 batch-146 = 0.0051771062426269054

Training epoch-6 batch-147
Running loss of epoch-6 batch-147 = 0.008518366143107414

Training epoch-6 batch-148
Running loss of epoch-6 batch-148 = 0.0054337335750460625

Training epoch-6 batch-149
Running loss of epoch-6 batch-149 = 0.005728348158299923

Training epoch-6 batch-150
Running loss of epoch-6 batch-150 = 0.007905317470431328

Training epoch-6 batch-151
Running loss of epoch-6 batch-151 = 0.007691896986216307

Training epoch-6 batch-152
Running loss of epoch-6 batch-152 = 0.0073861293494701385

Training epoch-6 batch-153
Running loss of epoch-6 batch-153 = 0.004377550445497036

Training epoch-6 batch-154
Running loss of epoch-6 batch-154 = 0.006560053676366806

Training epoch-6 batch-155
Running loss of epoch-6 batch-155 = 0.005880087614059448

Training epoch-6 batch-156
Running loss of epoch-6 batch-156 = 0.008133335039019585

Training epoch-6 batch-157
Running loss of epoch-6 batch-157 = 0.019273104146122932

Finished training epoch-6.



Average train loss at epoch-6 = 0.007384767302870751

Started Evaluation

Average val loss at epoch-6 = 0.5622020592625057

Accuracy for classes:
Accuracy for class equals is: 95.71 %
Accuracy for class main is: 98.36 %
Accuracy for class setUp is: 90.98 %
Accuracy for class onCreate is: 88.70 %
Accuracy for class toString is: 80.55 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 48.88 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 77.44 %

Overall Accuracy = 82.07 %


Best Accuracy = 82.07 % at Epoch-6
Saving model after best epoch-6

Finished Evaluation



Started training epoch-7


Training epoch-7 batch-1
Running loss of epoch-7 batch-1 = 0.005503338761627674

Training epoch-7 batch-2
Running loss of epoch-7 batch-2 = 0.005494013894349337

Training epoch-7 batch-3
Running loss of epoch-7 batch-3 = 0.005339161958545446

Training epoch-7 batch-4
Running loss of epoch-7 batch-4 = 0.005299391224980354

Training epoch-7 batch-5
Running loss of epoch-7 batch-5 = 0.006810625549405813

Training epoch-7 batch-6
Running loss of epoch-7 batch-6 = 0.00792595837265253

Training epoch-7 batch-7
Running loss of epoch-7 batch-7 = 0.008133549243211746

Training epoch-7 batch-8
Running loss of epoch-7 batch-8 = 0.0054221912287175655

Training epoch-7 batch-9
Running loss of epoch-7 batch-9 = 0.005851292051374912

Training epoch-7 batch-10
Running loss of epoch-7 batch-10 = 0.006946330890059471

Training epoch-7 batch-11
Running loss of epoch-7 batch-11 = 0.007327216677367687

Training epoch-7 batch-12
Running loss of epoch-7 batch-12 = 0.007590681314468384

Training epoch-7 batch-13
Running loss of epoch-7 batch-13 = 0.00688882265239954

Training epoch-7 batch-14
Running loss of epoch-7 batch-14 = 0.005859868600964546

Training epoch-7 batch-15
Running loss of epoch-7 batch-15 = 0.008627297356724739

Training epoch-7 batch-16
Running loss of epoch-7 batch-16 = 0.006798209622502327

Training epoch-7 batch-17
Running loss of epoch-7 batch-17 = 0.004120260942727327

Training epoch-7 batch-18
Running loss of epoch-7 batch-18 = 0.00689006457105279

Training epoch-7 batch-19
Running loss of epoch-7 batch-19 = 0.00642431853339076

Training epoch-7 batch-20
Running loss of epoch-7 batch-20 = 0.005136237479746342

Training epoch-7 batch-21
Running loss of epoch-7 batch-21 = 0.008359916508197784

Training epoch-7 batch-22
Running loss of epoch-7 batch-22 = 0.0035023274831473827

Training epoch-7 batch-23
Running loss of epoch-7 batch-23 = 0.00611096853390336

Training epoch-7 batch-24
Running loss of epoch-7 batch-24 = 0.007334297057241201

Training epoch-7 batch-25
Running loss of epoch-7 batch-25 = 0.008478144183754921

Training epoch-7 batch-26
Running loss of epoch-7 batch-26 = 0.007200238294899464

Training epoch-7 batch-27
Running loss of epoch-7 batch-27 = 0.008671297691762447

Training epoch-7 batch-28
Running loss of epoch-7 batch-28 = 0.00648615462705493

Training epoch-7 batch-29
Running loss of epoch-7 batch-29 = 0.007164874114096165

Training epoch-7 batch-30
Running loss of epoch-7 batch-30 = 0.005787806585431099

Training epoch-7 batch-31
Running loss of epoch-7 batch-31 = 0.007397991605103016

Training epoch-7 batch-32
Running loss of epoch-7 batch-32 = 0.007475664839148521

Training epoch-7 batch-33
Running loss of epoch-7 batch-33 = 0.009684175252914429

Training epoch-7 batch-34
Running loss of epoch-7 batch-34 = 0.004184707999229431

Training epoch-7 batch-35
Running loss of epoch-7 batch-35 = 0.008236868306994438

Training epoch-7 batch-36
Running loss of epoch-7 batch-36 = 0.004281212575733662

Training epoch-7 batch-37
Running loss of epoch-7 batch-37 = 0.004058009944856167

Training epoch-7 batch-38
Running loss of epoch-7 batch-38 = 0.007031029090285301

Training epoch-7 batch-39
Running loss of epoch-7 batch-39 = 0.00713730463758111

Training epoch-7 batch-40
Running loss of epoch-7 batch-40 = 0.005259043537080288

Training epoch-7 batch-41
Running loss of epoch-7 batch-41 = 0.003806755878031254

Training epoch-7 batch-42
Running loss of epoch-7 batch-42 = 0.006403819657862186

Training epoch-7 batch-43
Running loss of epoch-7 batch-43 = 0.009838438592851162

Training epoch-7 batch-44
Running loss of epoch-7 batch-44 = 0.007949883118271828

Training epoch-7 batch-45
Running loss of epoch-7 batch-45 = 0.005172716919332743

Training epoch-7 batch-46
Running loss of epoch-7 batch-46 = 0.006112850736826658

Training epoch-7 batch-47
Running loss of epoch-7 batch-47 = 0.005861406680196524

Training epoch-7 batch-48
Running loss of epoch-7 batch-48 = 0.007109673228114843

Training epoch-7 batch-49
Running loss of epoch-7 batch-49 = 0.004583760164678097

Training epoch-7 batch-50
Running loss of epoch-7 batch-50 = 0.007021687924861908

Training epoch-7 batch-51
Running loss of epoch-7 batch-51 = 0.007460941560566425

Training epoch-7 batch-52
Running loss of epoch-7 batch-52 = 0.009308514185249805

Training epoch-7 batch-53
Running loss of epoch-7 batch-53 = 0.004884918220341206

Training epoch-7 batch-54
Running loss of epoch-7 batch-54 = 0.006559962406754494

Training epoch-7 batch-55
Running loss of epoch-7 batch-55 = 0.00442154798656702

Training epoch-7 batch-56
Running loss of epoch-7 batch-56 = 0.0035587346646934748

Training epoch-7 batch-57
Running loss of epoch-7 batch-57 = 0.007961871102452278

Training epoch-7 batch-58
Running loss of epoch-7 batch-58 = 0.007081042043864727

Training epoch-7 batch-59
Running loss of epoch-7 batch-59 = 0.008318704552948475

Training epoch-7 batch-60
Running loss of epoch-7 batch-60 = 0.005077525973320007

Training epoch-7 batch-61
Running loss of epoch-7 batch-61 = 0.006602393928915262

Training epoch-7 batch-62
Running loss of epoch-7 batch-62 = 0.006983668077737093

Training epoch-7 batch-63
Running loss of epoch-7 batch-63 = 0.003990952856838703

Training epoch-7 batch-64
Running loss of epoch-7 batch-64 = 0.004972847178578377

Training epoch-7 batch-65
Running loss of epoch-7 batch-65 = 0.007961448282003403

Training epoch-7 batch-66
Running loss of epoch-7 batch-66 = 0.007600503973662853

Training epoch-7 batch-67
Running loss of epoch-7 batch-67 = 0.0030190686229616404

Training epoch-7 batch-68
Running loss of epoch-7 batch-68 = 0.007607342675328255

Training epoch-7 batch-69
Running loss of epoch-7 batch-69 = 0.007304966915398836

Training epoch-7 batch-70
Running loss of epoch-7 batch-70 = 0.008141914382576942

Training epoch-7 batch-71
Running loss of epoch-7 batch-71 = 0.007768577896058559

Training epoch-7 batch-72
Running loss of epoch-7 batch-72 = 0.004629019647836685

Training epoch-7 batch-73
Running loss of epoch-7 batch-73 = 0.005019568372517824

Training epoch-7 batch-74
Running loss of epoch-7 batch-74 = 0.00990486890077591

Training epoch-7 batch-75
Running loss of epoch-7 batch-75 = 0.005861002951860428

Training epoch-7 batch-76
Running loss of epoch-7 batch-76 = 0.007694772910326719

Training epoch-7 batch-77
Running loss of epoch-7 batch-77 = 0.007135859690606594

Training epoch-7 batch-78
Running loss of epoch-7 batch-78 = 0.004996278788894415

Training epoch-7 batch-79
Running loss of epoch-7 batch-79 = 0.005635456182062626

Training epoch-7 batch-80
Running loss of epoch-7 batch-80 = 0.009754064492881298

Training epoch-7 batch-81
Running loss of epoch-7 batch-81 = 0.01155927311629057

Training epoch-7 batch-82
Running loss of epoch-7 batch-82 = 0.008875816129148006

Training epoch-7 batch-83
Running loss of epoch-7 batch-83 = 0.007580969482660294

Training epoch-7 batch-84
Running loss of epoch-7 batch-84 = 0.005973535589873791

Training epoch-7 batch-85
Running loss of epoch-7 batch-85 = 0.005917227361351252

Training epoch-7 batch-86
Running loss of epoch-7 batch-86 = 0.007712271064519882

Training epoch-7 batch-87
Running loss of epoch-7 batch-87 = 0.007136806845664978

Training epoch-7 batch-88
Running loss of epoch-7 batch-88 = 0.0039763678796589375

Training epoch-7 batch-89
Running loss of epoch-7 batch-89 = 0.007999666035175323

Training epoch-7 batch-90
Running loss of epoch-7 batch-90 = 0.004709932487457991

Training epoch-7 batch-91
Running loss of epoch-7 batch-91 = 0.005112059880048037

Training epoch-7 batch-92
Running loss of epoch-7 batch-92 = 0.0075883083045482635

Training epoch-7 batch-93
Running loss of epoch-7 batch-93 = 0.007572098635137081

Training epoch-7 batch-94
Running loss of epoch-7 batch-94 = 0.00550519023090601

Training epoch-7 batch-95
Running loss of epoch-7 batch-95 = 0.005789509043097496

Training epoch-7 batch-96
Running loss of epoch-7 batch-96 = 0.0051372479647397995

Training epoch-7 batch-97
Running loss of epoch-7 batch-97 = 0.004467824008315802

Training epoch-7 batch-98
Running loss of epoch-7 batch-98 = 0.003491021227091551

Training epoch-7 batch-99
Running loss of epoch-7 batch-99 = 0.0031481152400374413

Training epoch-7 batch-100
Running loss of epoch-7 batch-100 = 0.005312388297170401

Training epoch-7 batch-101
Running loss of epoch-7 batch-101 = 0.005566393490880728

Training epoch-7 batch-102
Running loss of epoch-7 batch-102 = 0.008873194456100464

Training epoch-7 batch-103
Running loss of epoch-7 batch-103 = 0.004810357466340065

Training epoch-7 batch-104
Running loss of epoch-7 batch-104 = 0.0035118120722472668

Training epoch-7 batch-105
Running loss of epoch-7 batch-105 = 0.0037755074445158243

Training epoch-7 batch-106
Running loss of epoch-7 batch-106 = 0.006316523998975754

Training epoch-7 batch-107
Running loss of epoch-7 batch-107 = 0.0052350787445902824

Training epoch-7 batch-108
Running loss of epoch-7 batch-108 = 0.0029517875518649817

Training epoch-7 batch-109
Running loss of epoch-7 batch-109 = 0.004547479562461376

Training epoch-7 batch-110
Running loss of epoch-7 batch-110 = 0.009171701967716217

Training epoch-7 batch-111
Running loss of epoch-7 batch-111 = 0.008162439800798893

Training epoch-7 batch-112
Running loss of epoch-7 batch-112 = 0.007022974081337452

Training epoch-7 batch-113
Running loss of epoch-7 batch-113 = 0.0062467181123793125

Training epoch-7 batch-114
Running loss of epoch-7 batch-114 = 0.003983763046562672

Training epoch-7 batch-115
Running loss of epoch-7 batch-115 = 0.005689214915037155

Training epoch-7 batch-116
Running loss of epoch-7 batch-116 = 0.004688544198870659

Training epoch-7 batch-117
Running loss of epoch-7 batch-117 = 0.005572089459747076

Training epoch-7 batch-118
Running loss of epoch-7 batch-118 = 0.008836719207465649

Training epoch-7 batch-119
Running loss of epoch-7 batch-119 = 0.004286270588636398

Training epoch-7 batch-120
Running loss of epoch-7 batch-120 = 0.006547730881720781

Training epoch-7 batch-121
Running loss of epoch-7 batch-121 = 0.0035292129032313824

Training epoch-7 batch-122
Running loss of epoch-7 batch-122 = 0.004657229874283075

Training epoch-7 batch-123
Running loss of epoch-7 batch-123 = 0.005690618883818388

Training epoch-7 batch-124
Running loss of epoch-7 batch-124 = 0.004487956874072552

Training epoch-7 batch-125
Running loss of epoch-7 batch-125 = 0.006212761625647545

Training epoch-7 batch-126
Running loss of epoch-7 batch-126 = 0.0064824107103049755

Training epoch-7 batch-127
Running loss of epoch-7 batch-127 = 0.007130216807126999

Training epoch-7 batch-128
Running loss of epoch-7 batch-128 = 0.004131693858653307

Training epoch-7 batch-129
Running loss of epoch-7 batch-129 = 0.00590174924582243

Training epoch-7 batch-130
Running loss of epoch-7 batch-130 = 0.008737253956496716

Training epoch-7 batch-131
Running loss of epoch-7 batch-131 = 0.006965979002416134

Training epoch-7 batch-132
Running loss of epoch-7 batch-132 = 0.007957478053867817

Training epoch-7 batch-133
Running loss of epoch-7 batch-133 = 0.003566041123121977

Training epoch-7 batch-134
Running loss of epoch-7 batch-134 = 0.006876803003251553

Training epoch-7 batch-135
Running loss of epoch-7 batch-135 = 0.0036329752765595913

Training epoch-7 batch-136
Running loss of epoch-7 batch-136 = 0.0050287856720387936

Training epoch-7 batch-137
Running loss of epoch-7 batch-137 = 0.007926815189421177

Training epoch-7 batch-138
Running loss of epoch-7 batch-138 = 0.006337717641144991

Training epoch-7 batch-139
Running loss of epoch-7 batch-139 = 0.006890483200550079

Training epoch-7 batch-140
Running loss of epoch-7 batch-140 = 0.0104819405823946

Training epoch-7 batch-141
Running loss of epoch-7 batch-141 = 0.006526172161102295

Training epoch-7 batch-142
Running loss of epoch-7 batch-142 = 0.004087943583726883

Training epoch-7 batch-143
Running loss of epoch-7 batch-143 = 0.004764355253428221

Training epoch-7 batch-144
Running loss of epoch-7 batch-144 = 0.00630551902577281

Training epoch-7 batch-145
Running loss of epoch-7 batch-145 = 0.006490851286798716

Training epoch-7 batch-146
Running loss of epoch-7 batch-146 = 0.00845489650964737

Training epoch-7 batch-147
Running loss of epoch-7 batch-147 = 0.007434315048158169

Training epoch-7 batch-148
Running loss of epoch-7 batch-148 = 0.005611858330667019

Training epoch-7 batch-149
Running loss of epoch-7 batch-149 = 0.0048551131039857864

Training epoch-7 batch-150
Running loss of epoch-7 batch-150 = 0.00645483797416091

Training epoch-7 batch-151
Running loss of epoch-7 batch-151 = 0.006771057844161987

Training epoch-7 batch-152
Running loss of epoch-7 batch-152 = 0.012424886226654053

Training epoch-7 batch-153
Running loss of epoch-7 batch-153 = 0.0040938095189630985

Training epoch-7 batch-154
Running loss of epoch-7 batch-154 = 0.004806259647011757

Training epoch-7 batch-155
Running loss of epoch-7 batch-155 = 0.005698481108993292

Training epoch-7 batch-156
Running loss of epoch-7 batch-156 = 0.005350195802748203

Training epoch-7 batch-157
Running loss of epoch-7 batch-157 = 0.005436427891254425

Finished training epoch-7.



Average train loss at epoch-7 = 0.0063218426167964935

Started Evaluation

Average val loss at epoch-7 = 0.690536449949784

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 97.87 %
Accuracy for class setUp is: 81.31 %
Accuracy for class onCreate is: 85.61 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 66.89 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 31.61 %
Accuracy for class execute is: 64.66 %
Accuracy for class get is: 57.18 %

Overall Accuracy = 78.69 %

Finished Evaluation



Started training epoch-8


Training epoch-8 batch-1
Running loss of epoch-8 batch-1 = 0.006792211439460516

Training epoch-8 batch-2
Running loss of epoch-8 batch-2 = 0.007478934247046709

Training epoch-8 batch-3
Running loss of epoch-8 batch-3 = 0.006236488930881023

Training epoch-8 batch-4
Running loss of epoch-8 batch-4 = 0.004270507954061031

Training epoch-8 batch-5
Running loss of epoch-8 batch-5 = 0.004582474939525127

Training epoch-8 batch-6
Running loss of epoch-8 batch-6 = 0.004977475386112928

Training epoch-8 batch-7
Running loss of epoch-8 batch-7 = 0.004820243455469608

Training epoch-8 batch-8
Running loss of epoch-8 batch-8 = 0.0051075308583676815

Training epoch-8 batch-9
Running loss of epoch-8 batch-9 = 0.005777810234576464

Training epoch-8 batch-10
Running loss of epoch-8 batch-10 = 0.0062058065086603165

Training epoch-8 batch-11
Running loss of epoch-8 batch-11 = 0.004093240946531296

Training epoch-8 batch-12
Running loss of epoch-8 batch-12 = 0.008048566989600658

Training epoch-8 batch-13
Running loss of epoch-8 batch-13 = 0.004395743366330862

Training epoch-8 batch-14
Running loss of epoch-8 batch-14 = 0.0032767027150839567

Training epoch-8 batch-15
Running loss of epoch-8 batch-15 = 0.0048988694325089455

Training epoch-8 batch-16
Running loss of epoch-8 batch-16 = 0.005936789326369762

Training epoch-8 batch-17
Running loss of epoch-8 batch-17 = 0.0053931293077766895

Training epoch-8 batch-18
Running loss of epoch-8 batch-18 = 0.005773033015429974

Training epoch-8 batch-19
Running loss of epoch-8 batch-19 = 0.005953763145953417

Training epoch-8 batch-20
Running loss of epoch-8 batch-20 = 0.00408718828111887

Training epoch-8 batch-21
Running loss of epoch-8 batch-21 = 0.004512190353125334

Training epoch-8 batch-22
Running loss of epoch-8 batch-22 = 0.0034130881540477276

Training epoch-8 batch-23
Running loss of epoch-8 batch-23 = 0.005610346328467131

Training epoch-8 batch-24
Running loss of epoch-8 batch-24 = 0.00548351276665926

Training epoch-8 batch-25
Running loss of epoch-8 batch-25 = 0.00405200757086277

Training epoch-8 batch-26
Running loss of epoch-8 batch-26 = 0.006033371202647686

Training epoch-8 batch-27
Running loss of epoch-8 batch-27 = 0.0051264711655676365

Training epoch-8 batch-28
Running loss of epoch-8 batch-28 = 0.004881004802882671

Training epoch-8 batch-29
Running loss of epoch-8 batch-29 = 0.0038789892569184303

Training epoch-8 batch-30
Running loss of epoch-8 batch-30 = 0.005064203403890133

Training epoch-8 batch-31
Running loss of epoch-8 batch-31 = 0.0035470861475914717

Training epoch-8 batch-32
Running loss of epoch-8 batch-32 = 0.004504557698965073

Training epoch-8 batch-33
Running loss of epoch-8 batch-33 = 0.004536929074674845

Training epoch-8 batch-34
Running loss of epoch-8 batch-34 = 0.007028584834188223

Training epoch-8 batch-35
Running loss of epoch-8 batch-35 = 0.006575344130396843

Training epoch-8 batch-36
Running loss of epoch-8 batch-36 = 0.0061918264254927635

Training epoch-8 batch-37
Running loss of epoch-8 batch-37 = 0.0060767182148993015

Training epoch-8 batch-38
Running loss of epoch-8 batch-38 = 0.006671643815934658

Training epoch-8 batch-39
Running loss of epoch-8 batch-39 = 0.003908327780663967

Training epoch-8 batch-40
Running loss of epoch-8 batch-40 = 0.008953852578997612

Training epoch-8 batch-41
Running loss of epoch-8 batch-41 = 0.004299017135053873

Training epoch-8 batch-42
Running loss of epoch-8 batch-42 = 0.003886127145960927

Training epoch-8 batch-43
Running loss of epoch-8 batch-43 = 0.008580257184803486

Training epoch-8 batch-44
Running loss of epoch-8 batch-44 = 0.0043161772191524506

Training epoch-8 batch-45
Running loss of epoch-8 batch-45 = 0.008428151719272137

Training epoch-8 batch-46
Running loss of epoch-8 batch-46 = 0.005173984449356794

Training epoch-8 batch-47
Running loss of epoch-8 batch-47 = 0.003356051165610552

Training epoch-8 batch-48
Running loss of epoch-8 batch-48 = 0.007498354651033878

Training epoch-8 batch-49
Running loss of epoch-8 batch-49 = 0.004019094631075859

Training epoch-8 batch-50
Running loss of epoch-8 batch-50 = 0.006260376423597336

Training epoch-8 batch-51
Running loss of epoch-8 batch-51 = 0.00684045534580946

Training epoch-8 batch-52
Running loss of epoch-8 batch-52 = 0.006746774539351463

Training epoch-8 batch-53
Running loss of epoch-8 batch-53 = 0.007444330025464296

Training epoch-8 batch-54
Running loss of epoch-8 batch-54 = 0.004709187429398298

Training epoch-8 batch-55
Running loss of epoch-8 batch-55 = 0.0035155732184648514

Training epoch-8 batch-56
Running loss of epoch-8 batch-56 = 0.005573599599301815

Training epoch-8 batch-57
Running loss of epoch-8 batch-57 = 0.004434854723513126

Training epoch-8 batch-58
Running loss of epoch-8 batch-58 = 0.004156348295509815

Training epoch-8 batch-59
Running loss of epoch-8 batch-59 = 0.00394346471875906

Training epoch-8 batch-60
Running loss of epoch-8 batch-60 = 0.004666740074753761

Training epoch-8 batch-61
Running loss of epoch-8 batch-61 = 0.005373368505388498

Training epoch-8 batch-62
Running loss of epoch-8 batch-62 = 0.005740630440413952

Training epoch-8 batch-63
Running loss of epoch-8 batch-63 = 0.006026820745319128

Training epoch-8 batch-64
Running loss of epoch-8 batch-64 = 0.006571307312697172

Training epoch-8 batch-65
Running loss of epoch-8 batch-65 = 0.005243575666099787

Training epoch-8 batch-66
Running loss of epoch-8 batch-66 = 0.011981521733105183

Training epoch-8 batch-67
Running loss of epoch-8 batch-67 = 0.006960823200643063

Training epoch-8 batch-68
Running loss of epoch-8 batch-68 = 0.007601615507155657

Training epoch-8 batch-69
Running loss of epoch-8 batch-69 = 0.002417556941509247

Training epoch-8 batch-70
Running loss of epoch-8 batch-70 = 0.0052462294697761536

Training epoch-8 batch-71
Running loss of epoch-8 batch-71 = 0.004591397941112518

Training epoch-8 batch-72
Running loss of epoch-8 batch-72 = 0.00359192187897861

Training epoch-8 batch-73
Running loss of epoch-8 batch-73 = 0.0036526471376419067

Training epoch-8 batch-74
Running loss of epoch-8 batch-74 = 0.00559115270152688

Training epoch-8 batch-75
Running loss of epoch-8 batch-75 = 0.005696756765246391

Training epoch-8 batch-76
Running loss of epoch-8 batch-76 = 0.004914260935038328

Training epoch-8 batch-77
Running loss of epoch-8 batch-77 = 0.005521479994058609

Training epoch-8 batch-78
Running loss of epoch-8 batch-78 = 0.005625774618238211

Training epoch-8 batch-79
Running loss of epoch-8 batch-79 = 0.0064201802015304565

Training epoch-8 batch-80
Running loss of epoch-8 batch-80 = 0.004776236601173878

Training epoch-8 batch-81
Running loss of epoch-8 batch-81 = 0.003563955193385482

Training epoch-8 batch-82
Running loss of epoch-8 batch-82 = 0.003751023206859827

Training epoch-8 batch-83
Running loss of epoch-8 batch-83 = 0.0067377109080553055

Training epoch-8 batch-84
Running loss of epoch-8 batch-84 = 0.004299783147871494

Training epoch-8 batch-85
Running loss of epoch-8 batch-85 = 0.006974108517169952

Training epoch-8 batch-86
Running loss of epoch-8 batch-86 = 0.003992969170212746

Training epoch-8 batch-87
Running loss of epoch-8 batch-87 = 0.0073674507439136505

Training epoch-8 batch-88
Running loss of epoch-8 batch-88 = 0.0048086559399962425

Training epoch-8 batch-89
Running loss of epoch-8 batch-89 = 0.004366165492683649

Training epoch-8 batch-90
Running loss of epoch-8 batch-90 = 0.008818168193101883

Training epoch-8 batch-91
Running loss of epoch-8 batch-91 = 0.006014398764818907

Training epoch-8 batch-92
Running loss of epoch-8 batch-92 = 0.008893721736967564

Training epoch-8 batch-93
Running loss of epoch-8 batch-93 = 0.00567430118098855

Training epoch-8 batch-94
Running loss of epoch-8 batch-94 = 0.004686715546995401

Training epoch-8 batch-95
Running loss of epoch-8 batch-95 = 0.004858843982219696

Training epoch-8 batch-96
Running loss of epoch-8 batch-96 = 0.004881299566477537

Training epoch-8 batch-97
Running loss of epoch-8 batch-97 = 0.005260980688035488

Training epoch-8 batch-98
Running loss of epoch-8 batch-98 = 0.0061388174071908

Training epoch-8 batch-99
Running loss of epoch-8 batch-99 = 0.002862348221242428

Training epoch-8 batch-100
Running loss of epoch-8 batch-100 = 0.003829967463389039

Training epoch-8 batch-101
Running loss of epoch-8 batch-101 = 0.005047154612839222

Training epoch-8 batch-102
Running loss of epoch-8 batch-102 = 0.006720145232975483

Training epoch-8 batch-103
Running loss of epoch-8 batch-103 = 0.005994373466819525

Training epoch-8 batch-104
Running loss of epoch-8 batch-104 = 0.003861108561977744

Training epoch-8 batch-105
Running loss of epoch-8 batch-105 = 0.0029994426295161247

Training epoch-8 batch-106
Running loss of epoch-8 batch-106 = 0.00498716626316309

Training epoch-8 batch-107
Running loss of epoch-8 batch-107 = 0.007013875059783459

Training epoch-8 batch-108
Running loss of epoch-8 batch-108 = 0.003973532933741808

Training epoch-8 batch-109
Running loss of epoch-8 batch-109 = 0.0057066334411501884

Training epoch-8 batch-110
Running loss of epoch-8 batch-110 = 0.0056107440032064915

Training epoch-8 batch-111
Running loss of epoch-8 batch-111 = 0.007806788664311171

Training epoch-8 batch-112
Running loss of epoch-8 batch-112 = 0.004065067041665316

Training epoch-8 batch-113
Running loss of epoch-8 batch-113 = 0.005488744005560875

Training epoch-8 batch-114
Running loss of epoch-8 batch-114 = 0.0047887638211250305

Training epoch-8 batch-115
Running loss of epoch-8 batch-115 = 0.006289398297667503

Training epoch-8 batch-116
Running loss of epoch-8 batch-116 = 0.005525066051632166

Training epoch-8 batch-117
Running loss of epoch-8 batch-117 = 0.005803554784506559

Training epoch-8 batch-118
Running loss of epoch-8 batch-118 = 0.004409223794937134

Training epoch-8 batch-119
Running loss of epoch-8 batch-119 = 0.004383528605103493

Training epoch-8 batch-120
Running loss of epoch-8 batch-120 = 0.0052314940840005875

Training epoch-8 batch-121
Running loss of epoch-8 batch-121 = 0.0035288967192173004

Training epoch-8 batch-122
Running loss of epoch-8 batch-122 = 0.008098965510725975

Training epoch-8 batch-123
Running loss of epoch-8 batch-123 = 0.0036314125172793865

Training epoch-8 batch-124
Running loss of epoch-8 batch-124 = 0.005056546535342932

Training epoch-8 batch-125
Running loss of epoch-8 batch-125 = 0.008112034760415554

Training epoch-8 batch-126
Running loss of epoch-8 batch-126 = 0.006510395091027021

Training epoch-8 batch-127
Running loss of epoch-8 batch-127 = 0.004778949543833733

Training epoch-8 batch-128
Running loss of epoch-8 batch-128 = 0.0033589419908821583

Training epoch-8 batch-129
Running loss of epoch-8 batch-129 = 0.003712679259479046

Training epoch-8 batch-130
Running loss of epoch-8 batch-130 = 0.008304040879011154

Training epoch-8 batch-131
Running loss of epoch-8 batch-131 = 0.004917611368000507

Training epoch-8 batch-132
Running loss of epoch-8 batch-132 = 0.004125298000872135

Training epoch-8 batch-133
Running loss of epoch-8 batch-133 = 0.004804271273314953

Training epoch-8 batch-134
Running loss of epoch-8 batch-134 = 0.005596134811639786

Training epoch-8 batch-135
Running loss of epoch-8 batch-135 = 0.008143393322825432

Training epoch-8 batch-136
Running loss of epoch-8 batch-136 = 0.004383866209536791

Training epoch-8 batch-137
Running loss of epoch-8 batch-137 = 0.004965410567820072

Training epoch-8 batch-138
Running loss of epoch-8 batch-138 = 0.007045820355415344

Training epoch-8 batch-139
Running loss of epoch-8 batch-139 = 0.009018496610224247

Training epoch-8 batch-140
Running loss of epoch-8 batch-140 = 0.004061463288962841

Training epoch-8 batch-141
Running loss of epoch-8 batch-141 = 0.006777075119316578

Training epoch-8 batch-142
Running loss of epoch-8 batch-142 = 0.0035744057968258858

Training epoch-8 batch-143
Running loss of epoch-8 batch-143 = 0.00335200154222548

Training epoch-8 batch-144
Running loss of epoch-8 batch-144 = 0.0043253907933831215

Training epoch-8 batch-145
Running loss of epoch-8 batch-145 = 0.007359550334513187

Training epoch-8 batch-146
Running loss of epoch-8 batch-146 = 0.006674601696431637

Training epoch-8 batch-147
Running loss of epoch-8 batch-147 = 0.004883816000074148

Training epoch-8 batch-148
Running loss of epoch-8 batch-148 = 0.006442871876060963

Training epoch-8 batch-149
Running loss of epoch-8 batch-149 = 0.007537387311458588

Training epoch-8 batch-150
Running loss of epoch-8 batch-150 = 0.008734025061130524

Training epoch-8 batch-151
Running loss of epoch-8 batch-151 = 0.004845386371016502

Training epoch-8 batch-152
Running loss of epoch-8 batch-152 = 0.006005880422890186

Training epoch-8 batch-153
Running loss of epoch-8 batch-153 = 0.007129015401005745

Training epoch-8 batch-154
Running loss of epoch-8 batch-154 = 0.006754164583981037

Training epoch-8 batch-155
Running loss of epoch-8 batch-155 = 0.004484818782657385

Training epoch-8 batch-156
Running loss of epoch-8 batch-156 = 0.008509274572134018

Training epoch-8 batch-157
Running loss of epoch-8 batch-157 = 0.022106163203716278

Finished training epoch-8.



Average train loss at epoch-8 = 0.0055302961260080335

Started Evaluation

Average val loss at epoch-8 = 0.6197627806901246

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 94.75 %
Accuracy for class setUp is: 94.75 %
Accuracy for class onCreate is: 85.71 %
Accuracy for class toString is: 79.18 %
Accuracy for class run is: 66.89 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 58.52 %
Accuracy for class execute is: 52.61 %
Accuracy for class get is: 69.49 %

Overall Accuracy = 82.22 %


Best Accuracy = 82.22 % at Epoch-8
Saving model after best epoch-8

Finished Evaluation



Started training epoch-9


Training epoch-9 batch-1
Running loss of epoch-9 batch-1 = 0.003879260504618287

Training epoch-9 batch-2
Running loss of epoch-9 batch-2 = 0.006374964024871588

Training epoch-9 batch-3
Running loss of epoch-9 batch-3 = 0.005948380101472139

Training epoch-9 batch-4
Running loss of epoch-9 batch-4 = 0.006430063396692276

Training epoch-9 batch-5
Running loss of epoch-9 batch-5 = 0.006499597802758217

Training epoch-9 batch-6
Running loss of epoch-9 batch-6 = 0.004439744167029858

Training epoch-9 batch-7
Running loss of epoch-9 batch-7 = 0.002895953133702278

Training epoch-9 batch-8
Running loss of epoch-9 batch-8 = 0.005266196560114622

Training epoch-9 batch-9
Running loss of epoch-9 batch-9 = 0.0042443727143108845

Training epoch-9 batch-10
Running loss of epoch-9 batch-10 = 0.008417392149567604

Training epoch-9 batch-11
Running loss of epoch-9 batch-11 = 0.0044567519798874855

Training epoch-9 batch-12
Running loss of epoch-9 batch-12 = 0.003485207911580801

Training epoch-9 batch-13
Running loss of epoch-9 batch-13 = 0.005961711052805185

Training epoch-9 batch-14
Running loss of epoch-9 batch-14 = 0.005331514868885279

Training epoch-9 batch-15
Running loss of epoch-9 batch-15 = 0.0029132179915905

Training epoch-9 batch-16
Running loss of epoch-9 batch-16 = 0.006231896113604307

Training epoch-9 batch-17
Running loss of epoch-9 batch-17 = 0.004433250054717064

Training epoch-9 batch-18
Running loss of epoch-9 batch-18 = 0.004802498500794172

Training epoch-9 batch-19
Running loss of epoch-9 batch-19 = 0.0029595457017421722

Training epoch-9 batch-20
Running loss of epoch-9 batch-20 = 0.0049257040955126286

Training epoch-9 batch-21
Running loss of epoch-9 batch-21 = 0.005074001848697662

Training epoch-9 batch-22
Running loss of epoch-9 batch-22 = 0.0051137288101017475

Training epoch-9 batch-23
Running loss of epoch-9 batch-23 = 0.003461476881057024

Training epoch-9 batch-24
Running loss of epoch-9 batch-24 = 0.003944258205592632

Training epoch-9 batch-25
Running loss of epoch-9 batch-25 = 0.00245329225435853

Training epoch-9 batch-26
Running loss of epoch-9 batch-26 = 0.0030635236762464046

Training epoch-9 batch-27
Running loss of epoch-9 batch-27 = 0.005215525161474943

Training epoch-9 batch-28
Running loss of epoch-9 batch-28 = 0.004346414469182491

Training epoch-9 batch-29
Running loss of epoch-9 batch-29 = 0.005692341364920139

Training epoch-9 batch-30
Running loss of epoch-9 batch-30 = 0.004758679307997227

Training epoch-9 batch-31
Running loss of epoch-9 batch-31 = 0.0069128042086958885

Training epoch-9 batch-32
Running loss of epoch-9 batch-32 = 0.003226707223802805

Training epoch-9 batch-33
Running loss of epoch-9 batch-33 = 0.0051710521802306175

Training epoch-9 batch-34
Running loss of epoch-9 batch-34 = 0.005665892269462347

Training epoch-9 batch-35
Running loss of epoch-9 batch-35 = 0.0032570757903158665

Training epoch-9 batch-36
Running loss of epoch-9 batch-36 = 0.005543437320739031

Training epoch-9 batch-37
Running loss of epoch-9 batch-37 = 0.003463317174464464

Training epoch-9 batch-38
Running loss of epoch-9 batch-38 = 0.004897172562777996

Training epoch-9 batch-39
Running loss of epoch-9 batch-39 = 0.0033559780567884445

Training epoch-9 batch-40
Running loss of epoch-9 batch-40 = 0.004783979617059231

Training epoch-9 batch-41
Running loss of epoch-9 batch-41 = 0.009639517404139042

Training epoch-9 batch-42
Running loss of epoch-9 batch-42 = 0.003746642963960767

Training epoch-9 batch-43
Running loss of epoch-9 batch-43 = 0.005263574421405792

Training epoch-9 batch-44
Running loss of epoch-9 batch-44 = 0.006344670429825783

Training epoch-9 batch-45
Running loss of epoch-9 batch-45 = 0.004026683047413826

Training epoch-9 batch-46
Running loss of epoch-9 batch-46 = 0.006302113179117441

Training epoch-9 batch-47
Running loss of epoch-9 batch-47 = 0.004547891207039356

Training epoch-9 batch-48
Running loss of epoch-9 batch-48 = 0.005262077786028385

Training epoch-9 batch-49
Running loss of epoch-9 batch-49 = 0.006616974249482155

Training epoch-9 batch-50
Running loss of epoch-9 batch-50 = 0.006233795080333948

Training epoch-9 batch-51
Running loss of epoch-9 batch-51 = 0.0030359525699168444

Training epoch-9 batch-52
Running loss of epoch-9 batch-52 = 0.003745822934433818

Training epoch-9 batch-53
Running loss of epoch-9 batch-53 = 0.003730816999450326

Training epoch-9 batch-54
Running loss of epoch-9 batch-54 = 0.0036797993816435337

Training epoch-9 batch-55
Running loss of epoch-9 batch-55 = 0.005541240330785513

Training epoch-9 batch-56
Running loss of epoch-9 batch-56 = 0.005047270096838474

Training epoch-9 batch-57
Running loss of epoch-9 batch-57 = 0.005835168994963169

Training epoch-9 batch-58
Running loss of epoch-9 batch-58 = 0.0043127406388521194

Training epoch-9 batch-59
Running loss of epoch-9 batch-59 = 0.006535758264362812

Training epoch-9 batch-60
Running loss of epoch-9 batch-60 = 0.004407634027302265

Training epoch-9 batch-61
Running loss of epoch-9 batch-61 = 0.004939055535942316

Training epoch-9 batch-62
Running loss of epoch-9 batch-62 = 0.0036557752173393965

Training epoch-9 batch-63
Running loss of epoch-9 batch-63 = 0.004771077074110508

Training epoch-9 batch-64
Running loss of epoch-9 batch-64 = 0.003040333278477192

Training epoch-9 batch-65
Running loss of epoch-9 batch-65 = 0.005084695760160685

Training epoch-9 batch-66
Running loss of epoch-9 batch-66 = 0.006414058618247509

Training epoch-9 batch-67
Running loss of epoch-9 batch-67 = 0.003985576797276735

Training epoch-9 batch-68
Running loss of epoch-9 batch-68 = 0.00445973128080368

Training epoch-9 batch-69
Running loss of epoch-9 batch-69 = 0.0020372229628264904

Training epoch-9 batch-70
Running loss of epoch-9 batch-70 = 0.0037307683378458023

Training epoch-9 batch-71
Running loss of epoch-9 batch-71 = 0.004571288358420134

Training epoch-9 batch-72
Running loss of epoch-9 batch-72 = 0.004232109058648348

Training epoch-9 batch-73
Running loss of epoch-9 batch-73 = 0.0035890964791178703

Training epoch-9 batch-74
Running loss of epoch-9 batch-74 = 0.003739569103345275

Training epoch-9 batch-75
Running loss of epoch-9 batch-75 = 0.004765817895531654

Training epoch-9 batch-76
Running loss of epoch-9 batch-76 = 0.0058591412380337715

Training epoch-9 batch-77
Running loss of epoch-9 batch-77 = 0.0050935884937644005

Training epoch-9 batch-78
Running loss of epoch-9 batch-78 = 0.0060788365080952644

Training epoch-9 batch-79
Running loss of epoch-9 batch-79 = 0.004310054704546928

Training epoch-9 batch-80
Running loss of epoch-9 batch-80 = 0.006010620389133692

Training epoch-9 batch-81
Running loss of epoch-9 batch-81 = 0.004708572756499052

Training epoch-9 batch-82
Running loss of epoch-9 batch-82 = 0.0030750399455428123

Training epoch-9 batch-83
Running loss of epoch-9 batch-83 = 0.005122061353176832

Training epoch-9 batch-84
Running loss of epoch-9 batch-84 = 0.00311863562092185

Training epoch-9 batch-85
Running loss of epoch-9 batch-85 = 0.005944219417870045

Training epoch-9 batch-86
Running loss of epoch-9 batch-86 = 0.004274517297744751

Training epoch-9 batch-87
Running loss of epoch-9 batch-87 = 0.002729219850152731

Training epoch-9 batch-88
Running loss of epoch-9 batch-88 = 0.005594443995505571

Training epoch-9 batch-89
Running loss of epoch-9 batch-89 = 0.007562320679426193

Training epoch-9 batch-90
Running loss of epoch-9 batch-90 = 0.001616812776774168

Training epoch-9 batch-91
Running loss of epoch-9 batch-91 = 0.002053451258689165

Training epoch-9 batch-92
Running loss of epoch-9 batch-92 = 0.004163742996752262

Training epoch-9 batch-93
Running loss of epoch-9 batch-93 = 0.004148472566157579

Training epoch-9 batch-94
Running loss of epoch-9 batch-94 = 0.00413599144667387

Training epoch-9 batch-95
Running loss of epoch-9 batch-95 = 0.005105999298393726

Training epoch-9 batch-96
Running loss of epoch-9 batch-96 = 0.0040892381221055984

Training epoch-9 batch-97
Running loss of epoch-9 batch-97 = 0.006605026312172413

Training epoch-9 batch-98
Running loss of epoch-9 batch-98 = 0.005555279087275267

Training epoch-9 batch-99
Running loss of epoch-9 batch-99 = 0.0034193918108940125

Training epoch-9 batch-100
Running loss of epoch-9 batch-100 = 0.002835960127413273

Training epoch-9 batch-101
Running loss of epoch-9 batch-101 = 0.005868822336196899

Training epoch-9 batch-102
Running loss of epoch-9 batch-102 = 0.003657472785562277

Training epoch-9 batch-103
Running loss of epoch-9 batch-103 = 0.005001401528716087

Training epoch-9 batch-104
Running loss of epoch-9 batch-104 = 0.004205315373837948

Training epoch-9 batch-105
Running loss of epoch-9 batch-105 = 0.0029828897677361965

Training epoch-9 batch-106
Running loss of epoch-9 batch-106 = 0.003743333276361227

Training epoch-9 batch-107
Running loss of epoch-9 batch-107 = 0.004861163906753063

Training epoch-9 batch-108
Running loss of epoch-9 batch-108 = 0.004087650217115879

Training epoch-9 batch-109
Running loss of epoch-9 batch-109 = 0.004216325469315052

Training epoch-9 batch-110
Running loss of epoch-9 batch-110 = 0.004066037945449352

Training epoch-9 batch-111
Running loss of epoch-9 batch-111 = 0.005284139886498451

Training epoch-9 batch-112
Running loss of epoch-9 batch-112 = 0.00269142328761518

Training epoch-9 batch-113
Running loss of epoch-9 batch-113 = 0.004752591252326965

Training epoch-9 batch-114
Running loss of epoch-9 batch-114 = 0.006882795132696629

Training epoch-9 batch-115
Running loss of epoch-9 batch-115 = 0.00565576134249568

Training epoch-9 batch-116
Running loss of epoch-9 batch-116 = 0.004272188991308212

Training epoch-9 batch-117
Running loss of epoch-9 batch-117 = 0.004860947374254465

Training epoch-9 batch-118
Running loss of epoch-9 batch-118 = 0.004951142705976963

Training epoch-9 batch-119
Running loss of epoch-9 batch-119 = 0.0038782053161412477

Training epoch-9 batch-120
Running loss of epoch-9 batch-120 = 0.004714072681963444

Training epoch-9 batch-121
Running loss of epoch-9 batch-121 = 0.0043428754433989525

Training epoch-9 batch-122
Running loss of epoch-9 batch-122 = 0.005399539601057768

Training epoch-9 batch-123
Running loss of epoch-9 batch-123 = 0.005023721139878035

Training epoch-9 batch-124
Running loss of epoch-9 batch-124 = 0.005741387140005827

Training epoch-9 batch-125
Running loss of epoch-9 batch-125 = 0.006707672495394945

Training epoch-9 batch-126
Running loss of epoch-9 batch-126 = 0.005921445786952972

Training epoch-9 batch-127
Running loss of epoch-9 batch-127 = 0.003999258391559124

Training epoch-9 batch-128
Running loss of epoch-9 batch-128 = 0.005430576391518116

Training epoch-9 batch-129
Running loss of epoch-9 batch-129 = 0.003277482930570841

Training epoch-9 batch-130
Running loss of epoch-9 batch-130 = 0.004767483565956354

Training epoch-9 batch-131
Running loss of epoch-9 batch-131 = 0.006662771105766296

Training epoch-9 batch-132
Running loss of epoch-9 batch-132 = 0.002738978248089552

Training epoch-9 batch-133
Running loss of epoch-9 batch-133 = 0.0035879635252058506

Training epoch-9 batch-134
Running loss of epoch-9 batch-134 = 0.005109902936965227

Training epoch-9 batch-135
Running loss of epoch-9 batch-135 = 0.0008534654043614864

Training epoch-9 batch-136
Running loss of epoch-9 batch-136 = 0.00318912323564291

Training epoch-9 batch-137
Running loss of epoch-9 batch-137 = 0.0027926196344196796

Training epoch-9 batch-138
Running loss of epoch-9 batch-138 = 0.005267779808491468

Training epoch-9 batch-139
Running loss of epoch-9 batch-139 = 0.005720051471143961

Training epoch-9 batch-140
Running loss of epoch-9 batch-140 = 0.004932454787194729

Training epoch-9 batch-141
Running loss of epoch-9 batch-141 = 0.005647071171551943

Training epoch-9 batch-142
Running loss of epoch-9 batch-142 = 0.003463554894551635

Training epoch-9 batch-143
Running loss of epoch-9 batch-143 = 0.005649871192872524

Training epoch-9 batch-144
Running loss of epoch-9 batch-144 = 0.00421040877699852

Training epoch-9 batch-145
Running loss of epoch-9 batch-145 = 0.00661082286387682

Training epoch-9 batch-146
Running loss of epoch-9 batch-146 = 0.005194119177758694

Training epoch-9 batch-147
Running loss of epoch-9 batch-147 = 0.007019977550953627

Training epoch-9 batch-148
Running loss of epoch-9 batch-148 = 0.004424632526934147

Training epoch-9 batch-149
Running loss of epoch-9 batch-149 = 0.003966596908867359

Training epoch-9 batch-150
Running loss of epoch-9 batch-150 = 0.004414290189743042

Training epoch-9 batch-151
Running loss of epoch-9 batch-151 = 0.0033486285246908665

Training epoch-9 batch-152
Running loss of epoch-9 batch-152 = 0.0047006262466311455

Training epoch-9 batch-153
Running loss of epoch-9 batch-153 = 0.007405879907310009

Training epoch-9 batch-154
Running loss of epoch-9 batch-154 = 0.0036029242910444736

Training epoch-9 batch-155
Running loss of epoch-9 batch-155 = 0.004662797786295414

Training epoch-9 batch-156
Running loss of epoch-9 batch-156 = 0.004701755940914154

Training epoch-9 batch-157
Running loss of epoch-9 batch-157 = 0.01025962084531784

Finished training epoch-9.



Average train loss at epoch-9 = 0.004691647589206696

Started Evaluation

Average val loss at epoch-9 = 0.6442283861902788

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 92.62 %
Accuracy for class setUp is: 91.97 %
Accuracy for class onCreate is: 84.54 %
Accuracy for class toString is: 89.42 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 71.52 %
Accuracy for class execute is: 36.95 %
Accuracy for class get is: 47.44 %

Overall Accuracy = 80.61 %

Finished Evaluation



Started training epoch-10


Training epoch-10 batch-1
Running loss of epoch-10 batch-1 = 0.00569708039984107

Training epoch-10 batch-2
Running loss of epoch-10 batch-2 = 0.004100004211068153

Training epoch-10 batch-3
Running loss of epoch-10 batch-3 = 0.0036695576272904873

Training epoch-10 batch-4
Running loss of epoch-10 batch-4 = 0.0036183481570333242

Training epoch-10 batch-5
Running loss of epoch-10 batch-5 = 0.0032381759956479073

Training epoch-10 batch-6
Running loss of epoch-10 batch-6 = 0.0021504682954400778

Training epoch-10 batch-7
Running loss of epoch-10 batch-7 = 0.0023116660304367542

Training epoch-10 batch-8
Running loss of epoch-10 batch-8 = 0.006050169467926025

Training epoch-10 batch-9
Running loss of epoch-10 batch-9 = 0.005991943646222353

Training epoch-10 batch-10
Running loss of epoch-10 batch-10 = 0.005237769801169634

Training epoch-10 batch-11
Running loss of epoch-10 batch-11 = 0.005601305980235338

Training epoch-10 batch-12
Running loss of epoch-10 batch-12 = 0.0020984169095754623

Training epoch-10 batch-13
Running loss of epoch-10 batch-13 = 0.00424598902463913

Training epoch-10 batch-14
Running loss of epoch-10 batch-14 = 0.0035917710047215223

Training epoch-10 batch-15
Running loss of epoch-10 batch-15 = 0.004917352460324764

Training epoch-10 batch-16
Running loss of epoch-10 batch-16 = 0.0030612428672611713

Training epoch-10 batch-17
Running loss of epoch-10 batch-17 = 0.0032555777579545975

Training epoch-10 batch-18
Running loss of epoch-10 batch-18 = 0.003712580306455493

Training epoch-10 batch-19
Running loss of epoch-10 batch-19 = 0.002298196544870734

Training epoch-10 batch-20
Running loss of epoch-10 batch-20 = 0.0033841263502836227

Training epoch-10 batch-21
Running loss of epoch-10 batch-21 = 0.004255353473126888

Training epoch-10 batch-22
Running loss of epoch-10 batch-22 = 0.004180490039288998

Training epoch-10 batch-23
Running loss of epoch-10 batch-23 = 0.005212113261222839

Training epoch-10 batch-24
Running loss of epoch-10 batch-24 = 0.0037586831022053957

Training epoch-10 batch-25
Running loss of epoch-10 batch-25 = 0.002461038762703538

Training epoch-10 batch-26
Running loss of epoch-10 batch-26 = 0.004266947507858276

Training epoch-10 batch-27
Running loss of epoch-10 batch-27 = 0.0017609123606234789

Training epoch-10 batch-28
Running loss of epoch-10 batch-28 = 0.004124762956053019

Training epoch-10 batch-29
Running loss of epoch-10 batch-29 = 0.009906662628054619

Training epoch-10 batch-30
Running loss of epoch-10 batch-30 = 0.0069638630375266075

Training epoch-10 batch-31
Running loss of epoch-10 batch-31 = 0.004335567820817232

Training epoch-10 batch-32
Running loss of epoch-10 batch-32 = 0.002650103997439146

Training epoch-10 batch-33
Running loss of epoch-10 batch-33 = 0.002429103245958686

Training epoch-10 batch-34
Running loss of epoch-10 batch-34 = 0.00508400984108448

Training epoch-10 batch-35
Running loss of epoch-10 batch-35 = 0.00507127121090889

Training epoch-10 batch-36
Running loss of epoch-10 batch-36 = 0.0031233387999236584

Training epoch-10 batch-37
Running loss of epoch-10 batch-37 = 0.00279279169626534

Training epoch-10 batch-38
Running loss of epoch-10 batch-38 = 0.005442374385893345

Training epoch-10 batch-39
Running loss of epoch-10 batch-39 = 0.002390037989243865

Training epoch-10 batch-40
Running loss of epoch-10 batch-40 = 0.003129254560917616

Training epoch-10 batch-41
Running loss of epoch-10 batch-41 = 0.008292362093925476

Training epoch-10 batch-42
Running loss of epoch-10 batch-42 = 0.0028019289020448923

Training epoch-10 batch-43
Running loss of epoch-10 batch-43 = 0.005088296718895435

Training epoch-10 batch-44
Running loss of epoch-10 batch-44 = 0.005168091971427202

Training epoch-10 batch-45
Running loss of epoch-10 batch-45 = 0.004796292632818222

Training epoch-10 batch-46
Running loss of epoch-10 batch-46 = 0.0033084743190556765

Training epoch-10 batch-47
Running loss of epoch-10 batch-47 = 0.00483280373737216

Training epoch-10 batch-48
Running loss of epoch-10 batch-48 = 0.002287754788994789

Training epoch-10 batch-49
Running loss of epoch-10 batch-49 = 0.0039423261769115925

Training epoch-10 batch-50
Running loss of epoch-10 batch-50 = 0.0041498104110360146

Training epoch-10 batch-51
Running loss of epoch-10 batch-51 = 0.004218028858304024

Training epoch-10 batch-52
Running loss of epoch-10 batch-52 = 0.004113663919270039

Training epoch-10 batch-53
Running loss of epoch-10 batch-53 = 0.002332395641133189

Training epoch-10 batch-54
Running loss of epoch-10 batch-54 = 0.003023361787199974

Training epoch-10 batch-55
Running loss of epoch-10 batch-55 = 0.00523374555632472

Training epoch-10 batch-56
Running loss of epoch-10 batch-56 = 0.0026508974842727184

Training epoch-10 batch-57
Running loss of epoch-10 batch-57 = 0.003480745479464531

Training epoch-10 batch-58
Running loss of epoch-10 batch-58 = 0.0021993848495185375

Training epoch-10 batch-59
Running loss of epoch-10 batch-59 = 0.005798671394586563

Training epoch-10 batch-60
Running loss of epoch-10 batch-60 = 0.002443661680445075

Training epoch-10 batch-61
Running loss of epoch-10 batch-61 = 0.0020626357290893793

Training epoch-10 batch-62
Running loss of epoch-10 batch-62 = 0.0038803773932158947

Training epoch-10 batch-63
Running loss of epoch-10 batch-63 = 0.001752163632772863

Training epoch-10 batch-64
Running loss of epoch-10 batch-64 = 0.0037528180982917547

Training epoch-10 batch-65
Running loss of epoch-10 batch-65 = 0.003522693645209074

Training epoch-10 batch-66
Running loss of epoch-10 batch-66 = 0.005310748238116503

Training epoch-10 batch-67
Running loss of epoch-10 batch-67 = 0.0041575124487280846

Training epoch-10 batch-68
Running loss of epoch-10 batch-68 = 0.004433740861713886

Training epoch-10 batch-69
Running loss of epoch-10 batch-69 = 0.0031025432981550694

Training epoch-10 batch-70
Running loss of epoch-10 batch-70 = 0.004641410429030657

Training epoch-10 batch-71
Running loss of epoch-10 batch-71 = 0.0034406078048050404

Training epoch-10 batch-72
Running loss of epoch-10 batch-72 = 0.004051116295158863

Training epoch-10 batch-73
Running loss of epoch-10 batch-73 = 0.00563061935827136

Training epoch-10 batch-74
Running loss of epoch-10 batch-74 = 0.0032437159679830074

Training epoch-10 batch-75
Running loss of epoch-10 batch-75 = 0.0036025510635226965

Training epoch-10 batch-76
Running loss of epoch-10 batch-76 = 0.005863708443939686

Training epoch-10 batch-77
Running loss of epoch-10 batch-77 = 0.00343304593116045

Training epoch-10 batch-78
Running loss of epoch-10 batch-78 = 0.0042146299965679646

Training epoch-10 batch-79
Running loss of epoch-10 batch-79 = 0.00443559605628252

Training epoch-10 batch-80
Running loss of epoch-10 batch-80 = 0.004166570026427507

Training epoch-10 batch-81
Running loss of epoch-10 batch-81 = 0.0056380475871264935

Training epoch-10 batch-82
Running loss of epoch-10 batch-82 = 0.003215611446648836

Training epoch-10 batch-83
Running loss of epoch-10 batch-83 = 0.004497093614190817

Training epoch-10 batch-84
Running loss of epoch-10 batch-84 = 0.0024205835070461035

Training epoch-10 batch-85
Running loss of epoch-10 batch-85 = 0.004517183639109135

Training epoch-10 batch-86
Running loss of epoch-10 batch-86 = 0.007486121263355017

Training epoch-10 batch-87
Running loss of epoch-10 batch-87 = 0.003681737696751952

Training epoch-10 batch-88
Running loss of epoch-10 batch-88 = 0.004917176440358162

Training epoch-10 batch-89
Running loss of epoch-10 batch-89 = 0.0029276059940457344

Training epoch-10 batch-90
Running loss of epoch-10 batch-90 = 0.0048657613806426525

Training epoch-10 batch-91
Running loss of epoch-10 batch-91 = 0.004399525001645088

Training epoch-10 batch-92
Running loss of epoch-10 batch-92 = 0.0073751527816057205

Training epoch-10 batch-93
Running loss of epoch-10 batch-93 = 0.003701993729919195

Training epoch-10 batch-94
Running loss of epoch-10 batch-94 = 0.002315003890544176

Training epoch-10 batch-95
Running loss of epoch-10 batch-95 = 0.005288276821374893

Training epoch-10 batch-96
Running loss of epoch-10 batch-96 = 0.0051091560162603855

Training epoch-10 batch-97
Running loss of epoch-10 batch-97 = 0.004279782064259052

Training epoch-10 batch-98
Running loss of epoch-10 batch-98 = 0.00377925718203187

Training epoch-10 batch-99
Running loss of epoch-10 batch-99 = 0.00587089080363512

Training epoch-10 batch-100
Running loss of epoch-10 batch-100 = 0.001955314539372921

Training epoch-10 batch-101
Running loss of epoch-10 batch-101 = 0.004687469452619553

Training epoch-10 batch-102
Running loss of epoch-10 batch-102 = 0.003161410801112652

Training epoch-10 batch-103
Running loss of epoch-10 batch-103 = 0.004394444637000561

Training epoch-10 batch-104
Running loss of epoch-10 batch-104 = 0.0022083381190896034

Training epoch-10 batch-105
Running loss of epoch-10 batch-105 = 0.004687904845923185

Training epoch-10 batch-106
Running loss of epoch-10 batch-106 = 0.003180278930813074

Training epoch-10 batch-107
Running loss of epoch-10 batch-107 = 0.004194881767034531

Training epoch-10 batch-108
Running loss of epoch-10 batch-108 = 0.004056531470268965

Training epoch-10 batch-109
Running loss of epoch-10 batch-109 = 0.005254688672721386

Training epoch-10 batch-110
Running loss of epoch-10 batch-110 = 0.0038031304720789194

Training epoch-10 batch-111
Running loss of epoch-10 batch-111 = 0.0052153076976537704

Training epoch-10 batch-112
Running loss of epoch-10 batch-112 = 0.006048077717423439

Training epoch-10 batch-113
Running loss of epoch-10 batch-113 = 0.003860509255900979

Training epoch-10 batch-114
Running loss of epoch-10 batch-114 = 0.007269634865224361

Training epoch-10 batch-115
Running loss of epoch-10 batch-115 = 0.003300722222775221

Training epoch-10 batch-116
Running loss of epoch-10 batch-116 = 0.0028122738003730774

Training epoch-10 batch-117
Running loss of epoch-10 batch-117 = 0.002508998615667224

Training epoch-10 batch-118
Running loss of epoch-10 batch-118 = 0.00498050544410944

Training epoch-10 batch-119
Running loss of epoch-10 batch-119 = 0.005726984702050686

Training epoch-10 batch-120
Running loss of epoch-10 batch-120 = 0.0049260579980909824

Training epoch-10 batch-121
Running loss of epoch-10 batch-121 = 0.003948186989873648

Training epoch-10 batch-122
Running loss of epoch-10 batch-122 = 0.002638663165271282

Training epoch-10 batch-123
Running loss of epoch-10 batch-123 = 0.00395205058157444

Training epoch-10 batch-124
Running loss of epoch-10 batch-124 = 0.004359775688499212

Training epoch-10 batch-125
Running loss of epoch-10 batch-125 = 0.004484281875193119

Training epoch-10 batch-126
Running loss of epoch-10 batch-126 = 0.0038716404233127832

Training epoch-10 batch-127
Running loss of epoch-10 batch-127 = 0.00453508784994483

Training epoch-10 batch-128
Running loss of epoch-10 batch-128 = 0.006169901229441166

Training epoch-10 batch-129
Running loss of epoch-10 batch-129 = 0.0024678613990545273

Training epoch-10 batch-130
Running loss of epoch-10 batch-130 = 0.004512992221862078

Training epoch-10 batch-131
Running loss of epoch-10 batch-131 = 0.0040273042395710945

Training epoch-10 batch-132
Running loss of epoch-10 batch-132 = 0.0032110726460814476

Training epoch-10 batch-133
Running loss of epoch-10 batch-133 = 0.0032318548765033484

Training epoch-10 batch-134
Running loss of epoch-10 batch-134 = 0.004697653464972973

Training epoch-10 batch-135
Running loss of epoch-10 batch-135 = 0.004328316077589989

Training epoch-10 batch-136
Running loss of epoch-10 batch-136 = 0.004689912777394056

Training epoch-10 batch-137
Running loss of epoch-10 batch-137 = 0.003568578278645873

Training epoch-10 batch-138
Running loss of epoch-10 batch-138 = 0.005734284408390522

Training epoch-10 batch-139
Running loss of epoch-10 batch-139 = 0.0034960568882524967

Training epoch-10 batch-140
Running loss of epoch-10 batch-140 = 0.004722245968878269

Training epoch-10 batch-141
Running loss of epoch-10 batch-141 = 0.0027973174583166838

Training epoch-10 batch-142
Running loss of epoch-10 batch-142 = 0.003556998912245035

Training epoch-10 batch-143
Running loss of epoch-10 batch-143 = 0.005034417845308781

Training epoch-10 batch-144
Running loss of epoch-10 batch-144 = 0.005204401910305023

Training epoch-10 batch-145
Running loss of epoch-10 batch-145 = 0.004528813064098358

Training epoch-10 batch-146
Running loss of epoch-10 batch-146 = 0.0013867933303117752

Training epoch-10 batch-147
Running loss of epoch-10 batch-147 = 0.004433621186763048

Training epoch-10 batch-148
Running loss of epoch-10 batch-148 = 0.004045119974762201

Training epoch-10 batch-149
Running loss of epoch-10 batch-149 = 0.0025775334797799587

Training epoch-10 batch-150
Running loss of epoch-10 batch-150 = 0.0024349880404770374

Training epoch-10 batch-151
Running loss of epoch-10 batch-151 = 0.004591976758092642

Training epoch-10 batch-152
Running loss of epoch-10 batch-152 = 0.004224071279168129

Training epoch-10 batch-153
Running loss of epoch-10 batch-153 = 0.006701040081679821

Training epoch-10 batch-154
Running loss of epoch-10 batch-154 = 0.00368869467638433

Training epoch-10 batch-155
Running loss of epoch-10 batch-155 = 0.002805850235745311

Training epoch-10 batch-156
Running loss of epoch-10 batch-156 = 0.0029032991733402014

Training epoch-10 batch-157
Running loss of epoch-10 batch-157 = 0.024794725701212883

Finished training epoch-10.



Average train loss at epoch-10 = 0.004126161237806082

Started Evaluation

Average val loss at epoch-10 = 0.5967705628396576

Accuracy for classes:
Accuracy for class equals is: 95.87 %
Accuracy for class main is: 98.03 %
Accuracy for class setUp is: 92.62 %
Accuracy for class onCreate is: 89.98 %
Accuracy for class toString is: 80.55 %
Accuracy for class run is: 63.47 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.12 %
Accuracy for class execute is: 56.22 %
Accuracy for class get is: 68.72 %

Overall Accuracy = 82.53 %


Best Accuracy = 82.53 % at Epoch-10
Saving model after best epoch-10

Finished Evaluation



Started training epoch-11


Training epoch-11 batch-1
Running loss of epoch-11 batch-1 = 0.002485946984961629

Training epoch-11 batch-2
Running loss of epoch-11 batch-2 = 0.0031469466630369425

Training epoch-11 batch-3
Running loss of epoch-11 batch-3 = 0.005462463945150375

Training epoch-11 batch-4
Running loss of epoch-11 batch-4 = 0.003841363126412034

Training epoch-11 batch-5
Running loss of epoch-11 batch-5 = 0.005057243630290031

Training epoch-11 batch-6
Running loss of epoch-11 batch-6 = 0.00269902590662241

Training epoch-11 batch-7
Running loss of epoch-11 batch-7 = 0.0025544017553329468

Training epoch-11 batch-8
Running loss of epoch-11 batch-8 = 0.002075833734124899

Training epoch-11 batch-9
Running loss of epoch-11 batch-9 = 0.003589505795389414

Training epoch-11 batch-10
Running loss of epoch-11 batch-10 = 0.002017504768446088

Training epoch-11 batch-11
Running loss of epoch-11 batch-11 = 0.004746230319142342

Training epoch-11 batch-12
Running loss of epoch-11 batch-12 = 0.0028410227969288826

Training epoch-11 batch-13
Running loss of epoch-11 batch-13 = 0.003997335210442543

Training epoch-11 batch-14
Running loss of epoch-11 batch-14 = 0.0029832865111529827

Training epoch-11 batch-15
Running loss of epoch-11 batch-15 = 0.0017089324537664652

Training epoch-11 batch-16
Running loss of epoch-11 batch-16 = 0.003008984960615635

Training epoch-11 batch-17
Running loss of epoch-11 batch-17 = 0.002438994124531746

Training epoch-11 batch-18
Running loss of epoch-11 batch-18 = 0.004355438984930515

Training epoch-11 batch-19
Running loss of epoch-11 batch-19 = 0.0014012993779033422

Training epoch-11 batch-20
Running loss of epoch-11 batch-20 = 0.0025836965069174767

Training epoch-11 batch-21
Running loss of epoch-11 batch-21 = 0.0015939396107569337

Training epoch-11 batch-22
Running loss of epoch-11 batch-22 = 0.0039597866125404835

Training epoch-11 batch-23
Running loss of epoch-11 batch-23 = 0.0044156224466860294

Training epoch-11 batch-24
Running loss of epoch-11 batch-24 = 0.0024866394232958555

Training epoch-11 batch-25
Running loss of epoch-11 batch-25 = 0.0024562603794038296

Training epoch-11 batch-26
Running loss of epoch-11 batch-26 = 0.0023755435831844807

Training epoch-11 batch-27
Running loss of epoch-11 batch-27 = 0.001799728604964912

Training epoch-11 batch-28
Running loss of epoch-11 batch-28 = 0.0021526122000068426

Training epoch-11 batch-29
Running loss of epoch-11 batch-29 = 0.0027694213204085827

Training epoch-11 batch-30
Running loss of epoch-11 batch-30 = 0.003518258221447468

Training epoch-11 batch-31
Running loss of epoch-11 batch-31 = 0.003073706291615963

Training epoch-11 batch-32
Running loss of epoch-11 batch-32 = 0.0027080317959189415

Training epoch-11 batch-33
Running loss of epoch-11 batch-33 = 0.002865888411179185

Training epoch-11 batch-34
Running loss of epoch-11 batch-34 = 0.0035089876037091017

Training epoch-11 batch-35
Running loss of epoch-11 batch-35 = 0.0036939282435923815

Training epoch-11 batch-36
Running loss of epoch-11 batch-36 = 0.0035390586126595736

Training epoch-11 batch-37
Running loss of epoch-11 batch-37 = 0.005713833495974541

Training epoch-11 batch-38
Running loss of epoch-11 batch-38 = 0.0021746400743722916

Training epoch-11 batch-39
Running loss of epoch-11 batch-39 = 0.004495706409215927

Training epoch-11 batch-40
Running loss of epoch-11 batch-40 = 0.0020619507413357496

Training epoch-11 batch-41
Running loss of epoch-11 batch-41 = 0.003646319033578038

Training epoch-11 batch-42
Running loss of epoch-11 batch-42 = 0.0057768914848566055

Training epoch-11 batch-43
Running loss of epoch-11 batch-43 = 0.004510978702455759

Training epoch-11 batch-44
Running loss of epoch-11 batch-44 = 0.00172943074721843

Training epoch-11 batch-45
Running loss of epoch-11 batch-45 = 0.0021933619864284992

Training epoch-11 batch-46
Running loss of epoch-11 batch-46 = 0.004105848725885153

Training epoch-11 batch-47
Running loss of epoch-11 batch-47 = 0.004755361936986446

Training epoch-11 batch-48
Running loss of epoch-11 batch-48 = 0.002493920736014843

Training epoch-11 batch-49
Running loss of epoch-11 batch-49 = 0.002728581428527832

Training epoch-11 batch-50
Running loss of epoch-11 batch-50 = 0.005128758028149605

Training epoch-11 batch-51
Running loss of epoch-11 batch-51 = 0.003986399155110121

Training epoch-11 batch-52
Running loss of epoch-11 batch-52 = 0.002139090094715357

Training epoch-11 batch-53
Running loss of epoch-11 batch-53 = 0.0033518304117023945

Training epoch-11 batch-54
Running loss of epoch-11 batch-54 = 0.0018383569549769163

Training epoch-11 batch-55
Running loss of epoch-11 batch-55 = 0.002974545815959573

Training epoch-11 batch-56
Running loss of epoch-11 batch-56 = 0.004629275295883417

Training epoch-11 batch-57
Running loss of epoch-11 batch-57 = 0.0031160847283899784

Training epoch-11 batch-58
Running loss of epoch-11 batch-58 = 0.0025797279085963964

Training epoch-11 batch-59
Running loss of epoch-11 batch-59 = 0.0030161398462951183

Training epoch-11 batch-60
Running loss of epoch-11 batch-60 = 0.0044707851484417915

Training epoch-11 batch-61
Running loss of epoch-11 batch-61 = 0.003070562146604061

Training epoch-11 batch-62
Running loss of epoch-11 batch-62 = 0.004379992373287678

Training epoch-11 batch-63
Running loss of epoch-11 batch-63 = 0.003001414006575942

Training epoch-11 batch-64
Running loss of epoch-11 batch-64 = 0.0036887158639729023

Training epoch-11 batch-65
Running loss of epoch-11 batch-65 = 0.004844063892960548

Training epoch-11 batch-66
Running loss of epoch-11 batch-66 = 0.0031142025254666805

Training epoch-11 batch-67
Running loss of epoch-11 batch-67 = 0.0015163482166826725

Training epoch-11 batch-68
Running loss of epoch-11 batch-68 = 0.0027924878522753716

Training epoch-11 batch-69
Running loss of epoch-11 batch-69 = 0.004150465130805969

Training epoch-11 batch-70
Running loss of epoch-11 batch-70 = 0.0023253140971064568

Training epoch-11 batch-71
Running loss of epoch-11 batch-71 = 0.0028188973665237427

Training epoch-11 batch-72
Running loss of epoch-11 batch-72 = 0.004085749853402376

Training epoch-11 batch-73
Running loss of epoch-11 batch-73 = 0.0030623136553913355

Training epoch-11 batch-74
Running loss of epoch-11 batch-74 = 0.0023034552577883005

Training epoch-11 batch-75
Running loss of epoch-11 batch-75 = 0.005603742320090532

Training epoch-11 batch-76
Running loss of epoch-11 batch-76 = 0.002821296686306596

Training epoch-11 batch-77
Running loss of epoch-11 batch-77 = 0.005313662812113762

Training epoch-11 batch-78
Running loss of epoch-11 batch-78 = 0.003346345154568553

Training epoch-11 batch-79
Running loss of epoch-11 batch-79 = 0.0032186054158955812

Training epoch-11 batch-80
Running loss of epoch-11 batch-80 = 0.00424432847648859

Training epoch-11 batch-81
Running loss of epoch-11 batch-81 = 0.003945206757634878

Training epoch-11 batch-82
Running loss of epoch-11 batch-82 = 0.0029395590536296368

Training epoch-11 batch-83
Running loss of epoch-11 batch-83 = 0.002316083060577512

Training epoch-11 batch-84
Running loss of epoch-11 batch-84 = 0.003955090884119272

Training epoch-11 batch-85
Running loss of epoch-11 batch-85 = 0.0034092706628143787

Training epoch-11 batch-86
Running loss of epoch-11 batch-86 = 0.003676463384181261

Training epoch-11 batch-87
Running loss of epoch-11 batch-87 = 0.0022022088523954153

Training epoch-11 batch-88
Running loss of epoch-11 batch-88 = 0.005439121276140213

Training epoch-11 batch-89
Running loss of epoch-11 batch-89 = 0.0018160324543714523

Training epoch-11 batch-90
Running loss of epoch-11 batch-90 = 0.003531325375661254

Training epoch-11 batch-91
Running loss of epoch-11 batch-91 = 0.0041826702654361725

Training epoch-11 batch-92
Running loss of epoch-11 batch-92 = 0.0020164111629128456

Training epoch-11 batch-93
Running loss of epoch-11 batch-93 = 0.002716466784477234

Training epoch-11 batch-94
Running loss of epoch-11 batch-94 = 0.002592936623841524

Training epoch-11 batch-95
Running loss of epoch-11 batch-95 = 0.005267186090350151

Training epoch-11 batch-96
Running loss of epoch-11 batch-96 = 0.0033189766108989716

Training epoch-11 batch-97
Running loss of epoch-11 batch-97 = 0.003226048080250621

Training epoch-11 batch-98
Running loss of epoch-11 batch-98 = 0.002610309049487114

Training epoch-11 batch-99
Running loss of epoch-11 batch-99 = 0.003660443238914013

Training epoch-11 batch-100
Running loss of epoch-11 batch-100 = 0.0019343208987265825

Training epoch-11 batch-101
Running loss of epoch-11 batch-101 = 0.004391441121697426

Training epoch-11 batch-102
Running loss of epoch-11 batch-102 = 0.005191063974052668

Training epoch-11 batch-103
Running loss of epoch-11 batch-103 = 0.0022189877927303314

Training epoch-11 batch-104
Running loss of epoch-11 batch-104 = 0.0027848179452121258

Training epoch-11 batch-105
Running loss of epoch-11 batch-105 = 0.0029993238858878613

Training epoch-11 batch-106
Running loss of epoch-11 batch-106 = 0.0021825223229825497

Training epoch-11 batch-107
Running loss of epoch-11 batch-107 = 0.0015970292733982205

Training epoch-11 batch-108
Running loss of epoch-11 batch-108 = 0.0031609581783413887

Training epoch-11 batch-109
Running loss of epoch-11 batch-109 = 0.0028024830389767885

Training epoch-11 batch-110
Running loss of epoch-11 batch-110 = 0.002536330372095108

Training epoch-11 batch-111
Running loss of epoch-11 batch-111 = 0.0038779310416430235

Training epoch-11 batch-112
Running loss of epoch-11 batch-112 = 0.004869254305958748

Training epoch-11 batch-113
Running loss of epoch-11 batch-113 = 0.0033259415067732334

Training epoch-11 batch-114
Running loss of epoch-11 batch-114 = 0.004251787904649973

Training epoch-11 batch-115
Running loss of epoch-11 batch-115 = 0.002598636085167527

Training epoch-11 batch-116
Running loss of epoch-11 batch-116 = 0.0023880875669419765

Training epoch-11 batch-117
Running loss of epoch-11 batch-117 = 0.003433206584304571

Training epoch-11 batch-118
Running loss of epoch-11 batch-118 = 0.0041307564824819565

Training epoch-11 batch-119
Running loss of epoch-11 batch-119 = 0.002493145875632763

Training epoch-11 batch-120
Running loss of epoch-11 batch-120 = 0.005277296528220177

Training epoch-11 batch-121
Running loss of epoch-11 batch-121 = 0.004674985539168119

Training epoch-11 batch-122
Running loss of epoch-11 batch-122 = 0.003367831464856863

Training epoch-11 batch-123
Running loss of epoch-11 batch-123 = 0.001146258320659399

Training epoch-11 batch-124
Running loss of epoch-11 batch-124 = 0.003967103082686663

Training epoch-11 batch-125
Running loss of epoch-11 batch-125 = 0.004147094674408436

Training epoch-11 batch-126
Running loss of epoch-11 batch-126 = 0.0028098004404455423

Training epoch-11 batch-127
Running loss of epoch-11 batch-127 = 0.004883521236479282

Training epoch-11 batch-128
Running loss of epoch-11 batch-128 = 0.005518416874110699

Training epoch-11 batch-129
Running loss of epoch-11 batch-129 = 0.004508370533585548

Training epoch-11 batch-130
Running loss of epoch-11 batch-130 = 0.004170384723693132

Training epoch-11 batch-131
Running loss of epoch-11 batch-131 = 0.0034857974387705326

Training epoch-11 batch-132
Running loss of epoch-11 batch-132 = 0.005001560319215059

Training epoch-11 batch-133
Running loss of epoch-11 batch-133 = 0.0036244697403162718

Training epoch-11 batch-134
Running loss of epoch-11 batch-134 = 0.005371161736547947

Training epoch-11 batch-135
Running loss of epoch-11 batch-135 = 0.0031259339302778244

Training epoch-11 batch-136
Running loss of epoch-11 batch-136 = 0.0024290059227496386

Training epoch-11 batch-137
Running loss of epoch-11 batch-137 = 0.003418576903641224

Training epoch-11 batch-138
Running loss of epoch-11 batch-138 = 0.004498009569942951

Training epoch-11 batch-139
Running loss of epoch-11 batch-139 = 0.005763974040746689

Training epoch-11 batch-140
Running loss of epoch-11 batch-140 = 0.004900485742837191

Training epoch-11 batch-141
Running loss of epoch-11 batch-141 = 0.004401621874421835

Training epoch-11 batch-142
Running loss of epoch-11 batch-142 = 0.0027265253011137247

Training epoch-11 batch-143
Running loss of epoch-11 batch-143 = 0.0035327456425875425

Training epoch-11 batch-144
Running loss of epoch-11 batch-144 = 0.002976811956614256

Training epoch-11 batch-145
Running loss of epoch-11 batch-145 = 0.002049356698989868

Training epoch-11 batch-146
Running loss of epoch-11 batch-146 = 0.0024806447327136993

Training epoch-11 batch-147
Running loss of epoch-11 batch-147 = 0.003066455014050007

Training epoch-11 batch-148
Running loss of epoch-11 batch-148 = 0.0030832639895379543

Training epoch-11 batch-149
Running loss of epoch-11 batch-149 = 0.0022158625070005655

Training epoch-11 batch-150
Running loss of epoch-11 batch-150 = 0.0025147274136543274

Training epoch-11 batch-151
Running loss of epoch-11 batch-151 = 0.004809319507330656

Training epoch-11 batch-152
Running loss of epoch-11 batch-152 = 0.004197263158857822

Training epoch-11 batch-153
Running loss of epoch-11 batch-153 = 0.0022342056035995483

Training epoch-11 batch-154
Running loss of epoch-11 batch-154 = 0.00442274147644639

Training epoch-11 batch-155
Running loss of epoch-11 batch-155 = 0.0030515294056385756

Training epoch-11 batch-156
Running loss of epoch-11 batch-156 = 0.004752464592456818

Training epoch-11 batch-157
Running loss of epoch-11 batch-157 = 0.006708710454404354

Finished training epoch-11.



Average train loss at epoch-11 = 0.00338883356153965

Started Evaluation

Average val loss at epoch-11 = 0.6539987174221192

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 94.92 %
Accuracy for class setUp is: 87.87 %
Accuracy for class onCreate is: 89.98 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 60.73 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 49.10 %
Accuracy for class execute is: 57.83 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 81.56 %

Finished Evaluation



Started training epoch-12


Training epoch-12 batch-1
Running loss of epoch-12 batch-1 = 0.002053516451269388

Training epoch-12 batch-2
Running loss of epoch-12 batch-2 = 0.0017336177406832576

Training epoch-12 batch-3
Running loss of epoch-12 batch-3 = 0.0026632717344909906

Training epoch-12 batch-4
Running loss of epoch-12 batch-4 = 0.0034471077378839254

Training epoch-12 batch-5
Running loss of epoch-12 batch-5 = 0.004142438992857933

Training epoch-12 batch-6
Running loss of epoch-12 batch-6 = 0.0017637093551456928

Training epoch-12 batch-7
Running loss of epoch-12 batch-7 = 0.004047253169119358

Training epoch-12 batch-8
Running loss of epoch-12 batch-8 = 0.0015232446603477001

Training epoch-12 batch-9
Running loss of epoch-12 batch-9 = 0.004748200066387653

Training epoch-12 batch-10
Running loss of epoch-12 batch-10 = 0.0035259565338492393

Training epoch-12 batch-11
Running loss of epoch-12 batch-11 = 0.001800520229153335

Training epoch-12 batch-12
Running loss of epoch-12 batch-12 = 0.0027173422276973724

Training epoch-12 batch-13
Running loss of epoch-12 batch-13 = 0.0020817576441913843

Training epoch-12 batch-14
Running loss of epoch-12 batch-14 = 0.002617602702230215

Training epoch-12 batch-15
Running loss of epoch-12 batch-15 = 0.0021540243178606033

Training epoch-12 batch-16
Running loss of epoch-12 batch-16 = 0.0011633099056780338

Training epoch-12 batch-17
Running loss of epoch-12 batch-17 = 0.0019047926180064678

Training epoch-12 batch-18
Running loss of epoch-12 batch-18 = 0.0031967535614967346

Training epoch-12 batch-19
Running loss of epoch-12 batch-19 = 0.0026170453056693077

Training epoch-12 batch-20
Running loss of epoch-12 batch-20 = 0.003403882496058941

Training epoch-12 batch-21
Running loss of epoch-12 batch-21 = 0.0012259874492883682

Training epoch-12 batch-22
Running loss of epoch-12 batch-22 = 0.0032327647786587477

Training epoch-12 batch-23
Running loss of epoch-12 batch-23 = 0.004544937051832676

Training epoch-12 batch-24
Running loss of epoch-12 batch-24 = 0.0024994402192533016

Training epoch-12 batch-25
Running loss of epoch-12 batch-25 = 0.0009477590210735798

Training epoch-12 batch-26
Running loss of epoch-12 batch-26 = 0.0014914534986019135

Training epoch-12 batch-27
Running loss of epoch-12 batch-27 = 0.0024593996349722147

Training epoch-12 batch-28
Running loss of epoch-12 batch-28 = 0.0030753256287425756

Training epoch-12 batch-29
Running loss of epoch-12 batch-29 = 0.004592505283653736

Training epoch-12 batch-30
Running loss of epoch-12 batch-30 = 0.002132563851773739

Training epoch-12 batch-31
Running loss of epoch-12 batch-31 = 0.002098972676321864

Training epoch-12 batch-32
Running loss of epoch-12 batch-32 = 0.0009795590303838253

Training epoch-12 batch-33
Running loss of epoch-12 batch-33 = 0.002598746679723263

Training epoch-12 batch-34
Running loss of epoch-12 batch-34 = 0.0013786688214167953

Training epoch-12 batch-35
Running loss of epoch-12 batch-35 = 0.002817682223394513

Training epoch-12 batch-36
Running loss of epoch-12 batch-36 = 0.0026180921122431755

Training epoch-12 batch-37
Running loss of epoch-12 batch-37 = 0.003972229547798634

Training epoch-12 batch-38
Running loss of epoch-12 batch-38 = 0.0026091968175023794

Training epoch-12 batch-39
Running loss of epoch-12 batch-39 = 0.004005257040262222

Training epoch-12 batch-40
Running loss of epoch-12 batch-40 = 0.0017297030426561832

Training epoch-12 batch-41
Running loss of epoch-12 batch-41 = 0.003994411788880825

Training epoch-12 batch-42
Running loss of epoch-12 batch-42 = 0.0027071742806583643

Training epoch-12 batch-43
Running loss of epoch-12 batch-43 = 0.003015347057953477

Training epoch-12 batch-44
Running loss of epoch-12 batch-44 = 0.0035913968458771706

Training epoch-12 batch-45
Running loss of epoch-12 batch-45 = 0.0007547260029241443

Training epoch-12 batch-46
Running loss of epoch-12 batch-46 = 0.0017008440336212516

Training epoch-12 batch-47
Running loss of epoch-12 batch-47 = 0.0037007685750722885

Training epoch-12 batch-48
Running loss of epoch-12 batch-48 = 0.0020898354705423117

Training epoch-12 batch-49
Running loss of epoch-12 batch-49 = 0.0013488903641700745

Training epoch-12 batch-50
Running loss of epoch-12 batch-50 = 0.004638712387531996

Training epoch-12 batch-51
Running loss of epoch-12 batch-51 = 0.003917428199201822

Training epoch-12 batch-52
Running loss of epoch-12 batch-52 = 0.0035330974496901035

Training epoch-12 batch-53
Running loss of epoch-12 batch-53 = 0.0036546902265399694

Training epoch-12 batch-54
Running loss of epoch-12 batch-54 = 0.0035471965093165636

Training epoch-12 batch-55
Running loss of epoch-12 batch-55 = 0.002345832996070385

Training epoch-12 batch-56
Running loss of epoch-12 batch-56 = 0.002813841449096799

Training epoch-12 batch-57
Running loss of epoch-12 batch-57 = 0.0031305947341024876

Training epoch-12 batch-58
Running loss of epoch-12 batch-58 = 0.002717116381973028

Training epoch-12 batch-59
Running loss of epoch-12 batch-59 = 0.007621598429977894

Training epoch-12 batch-60
Running loss of epoch-12 batch-60 = 0.0020342841744422913

Training epoch-12 batch-61
Running loss of epoch-12 batch-61 = 0.002328162081539631

Training epoch-12 batch-62
Running loss of epoch-12 batch-62 = 0.002318755956366658

Training epoch-12 batch-63
Running loss of epoch-12 batch-63 = 0.002514953725039959

Training epoch-12 batch-64
Running loss of epoch-12 batch-64 = 0.0031313386280089617

Training epoch-12 batch-65
Running loss of epoch-12 batch-65 = 0.002720077522099018

Training epoch-12 batch-66
Running loss of epoch-12 batch-66 = 0.003397714113816619

Training epoch-12 batch-67
Running loss of epoch-12 batch-67 = 0.0023220148868858814

Training epoch-12 batch-68
Running loss of epoch-12 batch-68 = 0.0016813622787594795

Training epoch-12 batch-69
Running loss of epoch-12 batch-69 = 0.003295390633866191

Training epoch-12 batch-70
Running loss of epoch-12 batch-70 = 0.0038443831726908684

Training epoch-12 batch-71
Running loss of epoch-12 batch-71 = 0.0016162277897819877

Training epoch-12 batch-72
Running loss of epoch-12 batch-72 = 0.002539107110351324

Training epoch-12 batch-73
Running loss of epoch-12 batch-73 = 0.0021818436216562986

Training epoch-12 batch-74
Running loss of epoch-12 batch-74 = 0.0033200650941580534

Training epoch-12 batch-75
Running loss of epoch-12 batch-75 = 0.00386525085195899

Training epoch-12 batch-76
Running loss of epoch-12 batch-76 = 0.0023501133546233177

Training epoch-12 batch-77
Running loss of epoch-12 batch-77 = 0.0015137994196265936

Training epoch-12 batch-78
Running loss of epoch-12 batch-78 = 0.0038671675138175488

Training epoch-12 batch-79
Running loss of epoch-12 batch-79 = 0.004940839949995279

Training epoch-12 batch-80
Running loss of epoch-12 batch-80 = 0.002922734012827277

Training epoch-12 batch-81
Running loss of epoch-12 batch-81 = 0.003226750995963812

Training epoch-12 batch-82
Running loss of epoch-12 batch-82 = 0.001365035423077643

Training epoch-12 batch-83
Running loss of epoch-12 batch-83 = 0.003382989903911948

Training epoch-12 batch-84
Running loss of epoch-12 batch-84 = 0.0031798877753317356

Training epoch-12 batch-85
Running loss of epoch-12 batch-85 = 0.003648399841040373

Training epoch-12 batch-86
Running loss of epoch-12 batch-86 = 0.0022347380872815847

Training epoch-12 batch-87
Running loss of epoch-12 batch-87 = 0.002950207330286503

Training epoch-12 batch-88
Running loss of epoch-12 batch-88 = 0.0034634913317859173

Training epoch-12 batch-89
Running loss of epoch-12 batch-89 = 0.002489246893674135

Training epoch-12 batch-90
Running loss of epoch-12 batch-90 = 0.0037605271209031343

Training epoch-12 batch-91
Running loss of epoch-12 batch-91 = 0.0022414412815123796

Training epoch-12 batch-92
Running loss of epoch-12 batch-92 = 0.0036202918272465467

Training epoch-12 batch-93
Running loss of epoch-12 batch-93 = 0.0017391475848853588

Training epoch-12 batch-94
Running loss of epoch-12 batch-94 = 0.001642670133151114

Training epoch-12 batch-95
Running loss of epoch-12 batch-95 = 0.0028292525094002485

Training epoch-12 batch-96
Running loss of epoch-12 batch-96 = 0.0027877986431121826

Training epoch-12 batch-97
Running loss of epoch-12 batch-97 = 0.0014906758442521095

Training epoch-12 batch-98
Running loss of epoch-12 batch-98 = 0.0027245087549090385

Training epoch-12 batch-99
Running loss of epoch-12 batch-99 = 0.0027553564868867397

Training epoch-12 batch-100
Running loss of epoch-12 batch-100 = 0.0025448985397815704

Training epoch-12 batch-101
Running loss of epoch-12 batch-101 = 0.0040962365455925465

Training epoch-12 batch-102
Running loss of epoch-12 batch-102 = 0.003416595980525017

Training epoch-12 batch-103
Running loss of epoch-12 batch-103 = 0.003554081078618765

Training epoch-12 batch-104
Running loss of epoch-12 batch-104 = 0.0021995785646140575

Training epoch-12 batch-105
Running loss of epoch-12 batch-105 = 0.0026241070590913296

Training epoch-12 batch-106
Running loss of epoch-12 batch-106 = 0.003558591939508915

Training epoch-12 batch-107
Running loss of epoch-12 batch-107 = 0.005818783771246672

Training epoch-12 batch-108
Running loss of epoch-12 batch-108 = 0.0026244851760566235

Training epoch-12 batch-109
Running loss of epoch-12 batch-109 = 0.002667977474629879

Training epoch-12 batch-110
Running loss of epoch-12 batch-110 = 0.0053881434723734856

Training epoch-12 batch-111
Running loss of epoch-12 batch-111 = 0.002728695748373866

Training epoch-12 batch-112
Running loss of epoch-12 batch-112 = 0.0022945499513298273

Training epoch-12 batch-113
Running loss of epoch-12 batch-113 = 0.003036621492356062

Training epoch-12 batch-114
Running loss of epoch-12 batch-114 = 0.002576718106865883

Training epoch-12 batch-115
Running loss of epoch-12 batch-115 = 0.002658190904185176

Training epoch-12 batch-116
Running loss of epoch-12 batch-116 = 0.0015927404165267944

Training epoch-12 batch-117
Running loss of epoch-12 batch-117 = 0.003299988340586424

Training epoch-12 batch-118
Running loss of epoch-12 batch-118 = 0.0012970498064532876

Training epoch-12 batch-119
Running loss of epoch-12 batch-119 = 0.0028848154470324516

Training epoch-12 batch-120
Running loss of epoch-12 batch-120 = 0.0024869164917618036

Training epoch-12 batch-121
Running loss of epoch-12 batch-121 = 0.0033965015318244696

Training epoch-12 batch-122
Running loss of epoch-12 batch-122 = 0.0024161674082279205

Training epoch-12 batch-123
Running loss of epoch-12 batch-123 = 0.0013980544172227383

Training epoch-12 batch-124
Running loss of epoch-12 batch-124 = 0.0013827075017616153

Training epoch-12 batch-125
Running loss of epoch-12 batch-125 = 0.0035241302102804184

Training epoch-12 batch-126
Running loss of epoch-12 batch-126 = 0.0028722903225570917

Training epoch-12 batch-127
Running loss of epoch-12 batch-127 = 0.0018203611252829432

Training epoch-12 batch-128
Running loss of epoch-12 batch-128 = 0.0009693324100226164

Training epoch-12 batch-129
Running loss of epoch-12 batch-129 = 0.0009431976359337568

Training epoch-12 batch-130
Running loss of epoch-12 batch-130 = 0.003119595581665635

Training epoch-12 batch-131
Running loss of epoch-12 batch-131 = 0.0013792539248242974

Training epoch-12 batch-132
Running loss of epoch-12 batch-132 = 0.002965061692520976

Training epoch-12 batch-133
Running loss of epoch-12 batch-133 = 0.001117365318350494

Training epoch-12 batch-134
Running loss of epoch-12 batch-134 = 0.003396606771275401

Training epoch-12 batch-135
Running loss of epoch-12 batch-135 = 0.00247588730417192

Training epoch-12 batch-136
Running loss of epoch-12 batch-136 = 0.002672612201422453

Training epoch-12 batch-137
Running loss of epoch-12 batch-137 = 0.004687086679041386

Training epoch-12 batch-138
Running loss of epoch-12 batch-138 = 0.0036822701804339886

Training epoch-12 batch-139
Running loss of epoch-12 batch-139 = 0.0023967144079506397

Training epoch-12 batch-140
Running loss of epoch-12 batch-140 = 0.0027860745321959257

Training epoch-12 batch-141
Running loss of epoch-12 batch-141 = 0.0007585613057017326

Training epoch-12 batch-142
Running loss of epoch-12 batch-142 = 0.0033210208639502525

Training epoch-12 batch-143
Running loss of epoch-12 batch-143 = 0.006565617397427559

Training epoch-12 batch-144
Running loss of epoch-12 batch-144 = 0.004312531091272831

Training epoch-12 batch-145
Running loss of epoch-12 batch-145 = 0.0015255307080224156

Training epoch-12 batch-146
Running loss of epoch-12 batch-146 = 0.0038088811561465263

Training epoch-12 batch-147
Running loss of epoch-12 batch-147 = 0.007091319188475609

Training epoch-12 batch-148
Running loss of epoch-12 batch-148 = 0.0032184268347918987

Training epoch-12 batch-149
Running loss of epoch-12 batch-149 = 0.004369966685771942

Training epoch-12 batch-150
Running loss of epoch-12 batch-150 = 0.0019441073527559638

Training epoch-12 batch-151
Running loss of epoch-12 batch-151 = 0.004686296917498112

Training epoch-12 batch-152
Running loss of epoch-12 batch-152 = 0.0024341195821762085

Training epoch-12 batch-153
Running loss of epoch-12 batch-153 = 0.003954005427658558

Training epoch-12 batch-154
Running loss of epoch-12 batch-154 = 0.004434062168002129

Training epoch-12 batch-155
Running loss of epoch-12 batch-155 = 0.0015523753827437758

Training epoch-12 batch-156
Running loss of epoch-12 batch-156 = 0.0014312595594674349

Training epoch-12 batch-157
Running loss of epoch-12 batch-157 = 0.00979342870414257

Finished training epoch-12.



Average train loss at epoch-12 = 0.0028543012797832487

Started Evaluation

Average val loss at epoch-12 = 0.6900419489921708

Accuracy for classes:
Accuracy for class equals is: 97.03 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.98 %
Accuracy for class toString is: 85.67 %
Accuracy for class run is: 58.22 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 60.64 %
Accuracy for class get is: 57.95 %

Overall Accuracy = 81.29 %

Finished Evaluation



Started training epoch-13


Training epoch-13 batch-1
Running loss of epoch-13 batch-1 = 0.0016265225131064653

Training epoch-13 batch-2
Running loss of epoch-13 batch-2 = 0.0029101786203682423

Training epoch-13 batch-3
Running loss of epoch-13 batch-3 = 0.0025036977604031563

Training epoch-13 batch-4
Running loss of epoch-13 batch-4 = 0.0020904093980789185

Training epoch-13 batch-5
Running loss of epoch-13 batch-5 = 0.0019130625296384096

Training epoch-13 batch-6
Running loss of epoch-13 batch-6 = 0.0020976299419999123

Training epoch-13 batch-7
Running loss of epoch-13 batch-7 = 0.0027557797729969025

Training epoch-13 batch-8
Running loss of epoch-13 batch-8 = 0.0008743032813072205

Training epoch-13 batch-9
Running loss of epoch-13 batch-9 = 0.0014336768072098494

Training epoch-13 batch-10
Running loss of epoch-13 batch-10 = 0.0015585243236273527

Training epoch-13 batch-11
Running loss of epoch-13 batch-11 = 0.001006561447866261

Training epoch-13 batch-12
Running loss of epoch-13 batch-12 = 0.0019415862625464797

Training epoch-13 batch-13
Running loss of epoch-13 batch-13 = 0.0030277157202363014

Training epoch-13 batch-14
Running loss of epoch-13 batch-14 = 0.0015530476812273264

Training epoch-13 batch-15
Running loss of epoch-13 batch-15 = 0.00288791349157691

Training epoch-13 batch-16
Running loss of epoch-13 batch-16 = 0.0019399157026782632

Training epoch-13 batch-17
Running loss of epoch-13 batch-17 = 0.001958726439625025

Training epoch-13 batch-18
Running loss of epoch-13 batch-18 = 0.002382341306656599

Training epoch-13 batch-19
Running loss of epoch-13 batch-19 = 0.0034701619297266006

Training epoch-13 batch-20
Running loss of epoch-13 batch-20 = 0.004333109594881535

Training epoch-13 batch-21
Running loss of epoch-13 batch-21 = 0.0037743374705314636

Training epoch-13 batch-22
Running loss of epoch-13 batch-22 = 0.0037934372667223215

Training epoch-13 batch-23
Running loss of epoch-13 batch-23 = 0.0008901979308575392

Training epoch-13 batch-24
Running loss of epoch-13 batch-24 = 0.0009984866483137012

Training epoch-13 batch-25
Running loss of epoch-13 batch-25 = 0.0033551002852618694

Training epoch-13 batch-26
Running loss of epoch-13 batch-26 = 0.002115144394338131

Training epoch-13 batch-27
Running loss of epoch-13 batch-27 = 0.0016523509984835982

Training epoch-13 batch-28
Running loss of epoch-13 batch-28 = 0.001673110295087099

Training epoch-13 batch-29
Running loss of epoch-13 batch-29 = 0.0020189648494124413

Training epoch-13 batch-30
Running loss of epoch-13 batch-30 = 0.0010514261666685343

Training epoch-13 batch-31
Running loss of epoch-13 batch-31 = 0.002107908483594656

Training epoch-13 batch-32
Running loss of epoch-13 batch-32 = 0.0008699698373675346

Training epoch-13 batch-33
Running loss of epoch-13 batch-33 = 0.002163425087928772

Training epoch-13 batch-34
Running loss of epoch-13 batch-34 = 0.0022485177032649517

Training epoch-13 batch-35
Running loss of epoch-13 batch-35 = 0.001960604917258024

Training epoch-13 batch-36
Running loss of epoch-13 batch-36 = 0.0020819753408432007

Training epoch-13 batch-37
Running loss of epoch-13 batch-37 = 0.0010586249409243464

Training epoch-13 batch-38
Running loss of epoch-13 batch-38 = 0.0031308152247220278

Training epoch-13 batch-39
Running loss of epoch-13 batch-39 = 0.0016912691062316298

Training epoch-13 batch-40
Running loss of epoch-13 batch-40 = 0.0012742672115564346

Training epoch-13 batch-41
Running loss of epoch-13 batch-41 = 0.0032980944961309433

Training epoch-13 batch-42
Running loss of epoch-13 batch-42 = 0.0035336650907993317

Training epoch-13 batch-43
Running loss of epoch-13 batch-43 = 0.002589212032034993

Training epoch-13 batch-44
Running loss of epoch-13 batch-44 = 0.0008455319330096245

Training epoch-13 batch-45
Running loss of epoch-13 batch-45 = 0.003233192954212427

Training epoch-13 batch-46
Running loss of epoch-13 batch-46 = 0.0016500408528372645

Training epoch-13 batch-47
Running loss of epoch-13 batch-47 = 0.0013629038585349917

Training epoch-13 batch-48
Running loss of epoch-13 batch-48 = 0.0035272864624857903

Training epoch-13 batch-49
Running loss of epoch-13 batch-49 = 0.002011408330872655

Training epoch-13 batch-50
Running loss of epoch-13 batch-50 = 0.0018459471175447106

Training epoch-13 batch-51
Running loss of epoch-13 batch-51 = 0.0026430769357830286

Training epoch-13 batch-52
Running loss of epoch-13 batch-52 = 0.0018525810446590185

Training epoch-13 batch-53
Running loss of epoch-13 batch-53 = 0.0020767466630786657

Training epoch-13 batch-54
Running loss of epoch-13 batch-54 = 0.0016709257615730166

Training epoch-13 batch-55
Running loss of epoch-13 batch-55 = 0.0021628690883517265

Training epoch-13 batch-56
Running loss of epoch-13 batch-56 = 0.00245292647741735

Training epoch-13 batch-57
Running loss of epoch-13 batch-57 = 0.0033471856731921434

Training epoch-13 batch-58
Running loss of epoch-13 batch-58 = 0.004353144206106663

Training epoch-13 batch-59
Running loss of epoch-13 batch-59 = 0.0038445729296654463

Training epoch-13 batch-60
Running loss of epoch-13 batch-60 = 0.0053603993728756905

Training epoch-13 batch-61
Running loss of epoch-13 batch-61 = 0.003012242028489709

Training epoch-13 batch-62
Running loss of epoch-13 batch-62 = 0.0026400722563266754

Training epoch-13 batch-63
Running loss of epoch-13 batch-63 = 0.003345505567267537

Training epoch-13 batch-64
Running loss of epoch-13 batch-64 = 0.002302496926859021

Training epoch-13 batch-65
Running loss of epoch-13 batch-65 = 0.0017248302465304732

Training epoch-13 batch-66
Running loss of epoch-13 batch-66 = 0.0021554380655288696

Training epoch-13 batch-67
Running loss of epoch-13 batch-67 = 0.002993079600855708

Training epoch-13 batch-68
Running loss of epoch-13 batch-68 = 0.002651098882779479

Training epoch-13 batch-69
Running loss of epoch-13 batch-69 = 0.0029716158751398325

Training epoch-13 batch-70
Running loss of epoch-13 batch-70 = 0.004898720420897007

Training epoch-13 batch-71
Running loss of epoch-13 batch-71 = 0.0027875343803316355

Training epoch-13 batch-72
Running loss of epoch-13 batch-72 = 0.0008800949435681105

Training epoch-13 batch-73
Running loss of epoch-13 batch-73 = 0.0014751646667718887

Training epoch-13 batch-74
Running loss of epoch-13 batch-74 = 0.001589977415278554

Training epoch-13 batch-75
Running loss of epoch-13 batch-75 = 0.0028529034461826086

Training epoch-13 batch-76
Running loss of epoch-13 batch-76 = 0.001259078737348318

Training epoch-13 batch-77
Running loss of epoch-13 batch-77 = 0.0019921655766665936

Training epoch-13 batch-78
Running loss of epoch-13 batch-78 = 0.0019888519309461117

Training epoch-13 batch-79
Running loss of epoch-13 batch-79 = 0.001987189520150423

Training epoch-13 batch-80
Running loss of epoch-13 batch-80 = 0.00046519096940755844

Training epoch-13 batch-81
Running loss of epoch-13 batch-81 = 0.0018462941516190767

Training epoch-13 batch-82
Running loss of epoch-13 batch-82 = 0.00290320860221982

Training epoch-13 batch-83
Running loss of epoch-13 batch-83 = 0.003110384801402688

Training epoch-13 batch-84
Running loss of epoch-13 batch-84 = 0.004037504084408283

Training epoch-13 batch-85
Running loss of epoch-13 batch-85 = 0.002345794579014182

Training epoch-13 batch-86
Running loss of epoch-13 batch-86 = 0.0016728086629882455

Training epoch-13 batch-87
Running loss of epoch-13 batch-87 = 0.002117210766300559

Training epoch-13 batch-88
Running loss of epoch-13 batch-88 = 0.005203906446695328

Training epoch-13 batch-89
Running loss of epoch-13 batch-89 = 0.0020148474723100662

Training epoch-13 batch-90
Running loss of epoch-13 batch-90 = 0.00252662249840796

Training epoch-13 batch-91
Running loss of epoch-13 batch-91 = 0.0023034452460706234

Training epoch-13 batch-92
Running loss of epoch-13 batch-92 = 0.002201872179284692

Training epoch-13 batch-93
Running loss of epoch-13 batch-93 = 0.0029865759424865246

Training epoch-13 batch-94
Running loss of epoch-13 batch-94 = 0.0009986022487282753

Training epoch-13 batch-95
Running loss of epoch-13 batch-95 = 0.0035107326693832874

Training epoch-13 batch-96
Running loss of epoch-13 batch-96 = 0.0014744594227522612

Training epoch-13 batch-97
Running loss of epoch-13 batch-97 = 0.0015420601703226566

Training epoch-13 batch-98
Running loss of epoch-13 batch-98 = 0.0012772234622389078

Training epoch-13 batch-99
Running loss of epoch-13 batch-99 = 0.001836880692280829

Training epoch-13 batch-100
Running loss of epoch-13 batch-100 = 0.004440344404429197

Training epoch-13 batch-101
Running loss of epoch-13 batch-101 = 0.001162962638773024

Training epoch-13 batch-102
Running loss of epoch-13 batch-102 = 0.003309061983600259

Training epoch-13 batch-103
Running loss of epoch-13 batch-103 = 0.0035016422625631094

Training epoch-13 batch-104
Running loss of epoch-13 batch-104 = 0.003092673607170582

Training epoch-13 batch-105
Running loss of epoch-13 batch-105 = 0.0024527981877326965

Training epoch-13 batch-106
Running loss of epoch-13 batch-106 = 0.002364393090829253

Training epoch-13 batch-107
Running loss of epoch-13 batch-107 = 0.0015628188848495483

Training epoch-13 batch-108
Running loss of epoch-13 batch-108 = 0.002258549677208066

Training epoch-13 batch-109
Running loss of epoch-13 batch-109 = 0.0031220316886901855

Training epoch-13 batch-110
Running loss of epoch-13 batch-110 = 0.003446970833465457

Training epoch-13 batch-111
Running loss of epoch-13 batch-111 = 0.0023410634603351355

Training epoch-13 batch-112
Running loss of epoch-13 batch-112 = 0.0026894500479102135

Training epoch-13 batch-113
Running loss of epoch-13 batch-113 = 0.002029719529673457

Training epoch-13 batch-114
Running loss of epoch-13 batch-114 = 0.0029851302970200777

Training epoch-13 batch-115
Running loss of epoch-13 batch-115 = 0.004010322503745556

Training epoch-13 batch-116
Running loss of epoch-13 batch-116 = 0.0010865735821425915

Training epoch-13 batch-117
Running loss of epoch-13 batch-117 = 0.0008441448444500566

Training epoch-13 batch-118
Running loss of epoch-13 batch-118 = 0.0023209773935377598

Training epoch-13 batch-119
Running loss of epoch-13 batch-119 = 0.001584160141646862

Training epoch-13 batch-120
Running loss of epoch-13 batch-120 = 0.0012574009597301483

Training epoch-13 batch-121
Running loss of epoch-13 batch-121 = 0.0023310405667871237

Training epoch-13 batch-122
Running loss of epoch-13 batch-122 = 0.004062722437083721

Training epoch-13 batch-123
Running loss of epoch-13 batch-123 = 0.0020209148060530424

Training epoch-13 batch-124
Running loss of epoch-13 batch-124 = 0.0009190919809043407

Training epoch-13 batch-125
Running loss of epoch-13 batch-125 = 0.0015627469401806593

Training epoch-13 batch-126
Running loss of epoch-13 batch-126 = 0.0032844673842191696

Training epoch-13 batch-127
Running loss of epoch-13 batch-127 = 0.001093076542019844

Training epoch-13 batch-128
Running loss of epoch-13 batch-128 = 0.0010229127947241068

Training epoch-13 batch-129
Running loss of epoch-13 batch-129 = 0.004180653020739555

Training epoch-13 batch-130
Running loss of epoch-13 batch-130 = 0.0053870901465415955

Training epoch-13 batch-131
Running loss of epoch-13 batch-131 = 0.0020554380025714636

Training epoch-13 batch-132
Running loss of epoch-13 batch-132 = 0.0023652578238397837

Training epoch-13 batch-133
Running loss of epoch-13 batch-133 = 0.0015238607302308083

Training epoch-13 batch-134
Running loss of epoch-13 batch-134 = 0.0023367293179035187

Training epoch-13 batch-135
Running loss of epoch-13 batch-135 = 0.0017160646384581923

Training epoch-13 batch-136
Running loss of epoch-13 batch-136 = 0.0025563733652234077

Training epoch-13 batch-137
Running loss of epoch-13 batch-137 = 0.0015715147601440549

Training epoch-13 batch-138
Running loss of epoch-13 batch-138 = 0.0018491153605282307

Training epoch-13 batch-139
Running loss of epoch-13 batch-139 = 0.0039180973544716835

Training epoch-13 batch-140
Running loss of epoch-13 batch-140 = 0.001715507241897285

Training epoch-13 batch-141
Running loss of epoch-13 batch-141 = 0.002101391786709428

Training epoch-13 batch-142
Running loss of epoch-13 batch-142 = 0.002347630448639393

Training epoch-13 batch-143
Running loss of epoch-13 batch-143 = 0.0019084917148575187

Training epoch-13 batch-144
Running loss of epoch-13 batch-144 = 0.002328770235180855

Training epoch-13 batch-145
Running loss of epoch-13 batch-145 = 0.002017186488956213

Training epoch-13 batch-146
Running loss of epoch-13 batch-146 = 0.0018827019957825541

Training epoch-13 batch-147
Running loss of epoch-13 batch-147 = 0.0014588449848815799

Training epoch-13 batch-148
Running loss of epoch-13 batch-148 = 0.002572364406660199

Training epoch-13 batch-149
Running loss of epoch-13 batch-149 = 0.0018361588008701801

Training epoch-13 batch-150
Running loss of epoch-13 batch-150 = 0.001274039619602263

Training epoch-13 batch-151
Running loss of epoch-13 batch-151 = 0.0011489479802548885

Training epoch-13 batch-152
Running loss of epoch-13 batch-152 = 0.004102020524442196

Training epoch-13 batch-153
Running loss of epoch-13 batch-153 = 0.0018397890962660313

Training epoch-13 batch-154
Running loss of epoch-13 batch-154 = 0.0023983167484402657

Training epoch-13 batch-155
Running loss of epoch-13 batch-155 = 0.002321490552276373

Training epoch-13 batch-156
Running loss of epoch-13 batch-156 = 0.0027328096330165863

Training epoch-13 batch-157
Running loss of epoch-13 batch-157 = 0.028322050347924232

Finished training epoch-13.



Average train loss at epoch-13 = 0.002370608181506395

Started Evaluation

Average val loss at epoch-13 = 0.7901302382731381

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 88.38 %
Accuracy for class toString is: 86.01 %
Accuracy for class run is: 66.67 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 42.38 %
Accuracy for class execute is: 61.85 %
Accuracy for class get is: 61.79 %

Overall Accuracy = 81.04 %

Finished Evaluation



Started training epoch-14


Training epoch-14 batch-1
Running loss of epoch-14 batch-1 = 0.0003804618027061224

Training epoch-14 batch-2
Running loss of epoch-14 batch-2 = 0.0012733298353850842

Training epoch-14 batch-3
Running loss of epoch-14 batch-3 = 0.0018260531360283494

Training epoch-14 batch-4
Running loss of epoch-14 batch-4 = 0.0012817810056731105

Training epoch-14 batch-5
Running loss of epoch-14 batch-5 = 0.002207027282565832

Training epoch-14 batch-6
Running loss of epoch-14 batch-6 = 0.0007810856914147735

Training epoch-14 batch-7
Running loss of epoch-14 batch-7 = 0.002153570530936122

Training epoch-14 batch-8
Running loss of epoch-14 batch-8 = 0.0014566209865733981

Training epoch-14 batch-9
Running loss of epoch-14 batch-9 = 0.0022219510283321142

Training epoch-14 batch-10
Running loss of epoch-14 batch-10 = 0.0020015505142509937

Training epoch-14 batch-11
Running loss of epoch-14 batch-11 = 0.002471961546689272

Training epoch-14 batch-12
Running loss of epoch-14 batch-12 = 0.0020557502284646034

Training epoch-14 batch-13
Running loss of epoch-14 batch-13 = 0.0013588068541139364

Training epoch-14 batch-14
Running loss of epoch-14 batch-14 = 0.001051216502673924

Training epoch-14 batch-15
Running loss of epoch-14 batch-15 = 0.0012920763110741973

Training epoch-14 batch-16
Running loss of epoch-14 batch-16 = 0.0017096840310841799

Training epoch-14 batch-17
Running loss of epoch-14 batch-17 = 0.0037000372540205717

Training epoch-14 batch-18
Running loss of epoch-14 batch-18 = 0.0014093979261815548

Training epoch-14 batch-19
Running loss of epoch-14 batch-19 = 0.002625854918733239

Training epoch-14 batch-20
Running loss of epoch-14 batch-20 = 0.002613113494589925

Training epoch-14 batch-21
Running loss of epoch-14 batch-21 = 0.001052290783263743

Training epoch-14 batch-22
Running loss of epoch-14 batch-22 = 0.002232367405667901

Training epoch-14 batch-23
Running loss of epoch-14 batch-23 = 0.0019921469502151012

Training epoch-14 batch-24
Running loss of epoch-14 batch-24 = 0.0018448135815560818

Training epoch-14 batch-25
Running loss of epoch-14 batch-25 = 0.0031971456483006477

Training epoch-14 batch-26
Running loss of epoch-14 batch-26 = 0.0011457272339612246

Training epoch-14 batch-27
Running loss of epoch-14 batch-27 = 0.0015653015580028296

Training epoch-14 batch-28
Running loss of epoch-14 batch-28 = 0.0014453381299972534

Training epoch-14 batch-29
Running loss of epoch-14 batch-29 = 0.0012840210692957044

Training epoch-14 batch-30
Running loss of epoch-14 batch-30 = 0.0020808002445846796

Training epoch-14 batch-31
Running loss of epoch-14 batch-31 = 0.0009369844337925315

Training epoch-14 batch-32
Running loss of epoch-14 batch-32 = 0.0016457763267681003

Training epoch-14 batch-33
Running loss of epoch-14 batch-33 = 0.0017637006239965558

Training epoch-14 batch-34
Running loss of epoch-14 batch-34 = 0.0019831352401524782

Training epoch-14 batch-35
Running loss of epoch-14 batch-35 = 0.0023978608660399914

Training epoch-14 batch-36
Running loss of epoch-14 batch-36 = 0.0014043031260371208

Training epoch-14 batch-37
Running loss of epoch-14 batch-37 = 0.0016478842590004206

Training epoch-14 batch-38
Running loss of epoch-14 batch-38 = 0.001472775125876069

Training epoch-14 batch-39
Running loss of epoch-14 batch-39 = 0.0021299761720001698

Training epoch-14 batch-40
Running loss of epoch-14 batch-40 = 0.001180732622742653

Training epoch-14 batch-41
Running loss of epoch-14 batch-41 = 0.0016986348200589418

Training epoch-14 batch-42
Running loss of epoch-14 batch-42 = 0.002620512852445245

Training epoch-14 batch-43
Running loss of epoch-14 batch-43 = 0.0024619055911898613

Training epoch-14 batch-44
Running loss of epoch-14 batch-44 = 0.001427402370609343

Training epoch-14 batch-45
Running loss of epoch-14 batch-45 = 0.003013896057382226

Training epoch-14 batch-46
Running loss of epoch-14 batch-46 = 0.0019049294060096145

Training epoch-14 batch-47
Running loss of epoch-14 batch-47 = 0.0035380697809159756

Training epoch-14 batch-48
Running loss of epoch-14 batch-48 = 0.0029039003420621157

Training epoch-14 batch-49
Running loss of epoch-14 batch-49 = 0.0018298359354957938

Training epoch-14 batch-50
Running loss of epoch-14 batch-50 = 0.0037036556750535965

Training epoch-14 batch-51
Running loss of epoch-14 batch-51 = 0.002361919265240431

Training epoch-14 batch-52
Running loss of epoch-14 batch-52 = 0.0010764317121356726

Training epoch-14 batch-53
Running loss of epoch-14 batch-53 = 0.000803418573923409

Training epoch-14 batch-54
Running loss of epoch-14 batch-54 = 0.0017148221377283335

Training epoch-14 batch-55
Running loss of epoch-14 batch-55 = 0.0007507903501391411

Training epoch-14 batch-56
Running loss of epoch-14 batch-56 = 0.002248861826956272

Training epoch-14 batch-57
Running loss of epoch-14 batch-57 = 0.0010637535015121102

Training epoch-14 batch-58
Running loss of epoch-14 batch-58 = 0.0012066104682162404

Training epoch-14 batch-59
Running loss of epoch-14 batch-59 = 0.0028078595642000437

Training epoch-14 batch-60
Running loss of epoch-14 batch-60 = 0.00042823166586458683

Training epoch-14 batch-61
Running loss of epoch-14 batch-61 = 0.0024106011260300875

Training epoch-14 batch-62
Running loss of epoch-14 batch-62 = 0.0016012415289878845

Training epoch-14 batch-63
Running loss of epoch-14 batch-63 = 0.0030227999668568373

Training epoch-14 batch-64
Running loss of epoch-14 batch-64 = 0.0018364159623160958

Training epoch-14 batch-65
Running loss of epoch-14 batch-65 = 0.0018698432249948382

Training epoch-14 batch-66
Running loss of epoch-14 batch-66 = 0.0023835613392293453

Training epoch-14 batch-67
Running loss of epoch-14 batch-67 = 0.0010956766782328486

Training epoch-14 batch-68
Running loss of epoch-14 batch-68 = 0.002417244017124176

Training epoch-14 batch-69
Running loss of epoch-14 batch-69 = 0.0025502811186015606

Training epoch-14 batch-70
Running loss of epoch-14 batch-70 = 0.0033896383829414845

Training epoch-14 batch-71
Running loss of epoch-14 batch-71 = 0.0017922988627105951

Training epoch-14 batch-72
Running loss of epoch-14 batch-72 = 0.0018936200067400932

Training epoch-14 batch-73
Running loss of epoch-14 batch-73 = 0.002329988172277808

Training epoch-14 batch-74
Running loss of epoch-14 batch-74 = 0.0020607321057468653

Training epoch-14 batch-75
Running loss of epoch-14 batch-75 = 0.001220356673002243

Training epoch-14 batch-76
Running loss of epoch-14 batch-76 = 0.0015110316453501582

Training epoch-14 batch-77
Running loss of epoch-14 batch-77 = 0.0028563300147652626

Training epoch-14 batch-78
Running loss of epoch-14 batch-78 = 0.002353406511247158

Training epoch-14 batch-79
Running loss of epoch-14 batch-79 = 0.00182799668982625

Training epoch-14 batch-80
Running loss of epoch-14 batch-80 = 0.0016794824041426182

Training epoch-14 batch-81
Running loss of epoch-14 batch-81 = 0.002635365817695856

Training epoch-14 batch-82
Running loss of epoch-14 batch-82 = 0.0017947761807590723

Training epoch-14 batch-83
Running loss of epoch-14 batch-83 = 0.0013089558342471719

Training epoch-14 batch-84
Running loss of epoch-14 batch-84 = 0.001014221808873117

Training epoch-14 batch-85
Running loss of epoch-14 batch-85 = 0.001278201467357576

Training epoch-14 batch-86
Running loss of epoch-14 batch-86 = 0.0019649770110845566

Training epoch-14 batch-87
Running loss of epoch-14 batch-87 = 0.0026473719626665115

Training epoch-14 batch-88
Running loss of epoch-14 batch-88 = 0.0019458814058452845

Training epoch-14 batch-89
Running loss of epoch-14 batch-89 = 0.0010473068105056882

Training epoch-14 batch-90
Running loss of epoch-14 batch-90 = 0.0024178195744752884

Training epoch-14 batch-91
Running loss of epoch-14 batch-91 = 0.00265390332788229

Training epoch-14 batch-92
Running loss of epoch-14 batch-92 = 0.001896705012768507

Training epoch-14 batch-93
Running loss of epoch-14 batch-93 = 0.0014147467445582151

Training epoch-14 batch-94
Running loss of epoch-14 batch-94 = 0.0016738756094127893

Training epoch-14 batch-95
Running loss of epoch-14 batch-95 = 0.002709838794544339

Training epoch-14 batch-96
Running loss of epoch-14 batch-96 = 0.0013196882791817188

Training epoch-14 batch-97
Running loss of epoch-14 batch-97 = 0.0021380435209721327

Training epoch-14 batch-98
Running loss of epoch-14 batch-98 = 0.0027578487060964108

Training epoch-14 batch-99
Running loss of epoch-14 batch-99 = 0.0013769883662462234

Training epoch-14 batch-100
Running loss of epoch-14 batch-100 = 0.0032694339752197266

Training epoch-14 batch-101
Running loss of epoch-14 batch-101 = 0.0024065645411610603

Training epoch-14 batch-102
Running loss of epoch-14 batch-102 = 0.0030440217815339565

Training epoch-14 batch-103
Running loss of epoch-14 batch-103 = 0.0014146690955385566

Training epoch-14 batch-104
Running loss of epoch-14 batch-104 = 0.004057864658534527

Training epoch-14 batch-105
Running loss of epoch-14 batch-105 = 0.0019326122710481286

Training epoch-14 batch-106
Running loss of epoch-14 batch-106 = 0.0025361431762576103

Training epoch-14 batch-107
Running loss of epoch-14 batch-107 = 0.0013490812852978706

Training epoch-14 batch-108
Running loss of epoch-14 batch-108 = 0.002618099795654416

Training epoch-14 batch-109
Running loss of epoch-14 batch-109 = 0.00229768268764019

Training epoch-14 batch-110
Running loss of epoch-14 batch-110 = 0.0029206848703324795

Training epoch-14 batch-111
Running loss of epoch-14 batch-111 = 0.001496286946348846

Training epoch-14 batch-112
Running loss of epoch-14 batch-112 = 0.003689191769808531

Training epoch-14 batch-113
Running loss of epoch-14 batch-113 = 0.002445672173053026

Training epoch-14 batch-114
Running loss of epoch-14 batch-114 = 0.004123803228139877

Training epoch-14 batch-115
Running loss of epoch-14 batch-115 = 0.00251921359449625

Training epoch-14 batch-116
Running loss of epoch-14 batch-116 = 0.0018515187548473477

Training epoch-14 batch-117
Running loss of epoch-14 batch-117 = 0.004046002868562937

Training epoch-14 batch-118
Running loss of epoch-14 batch-118 = 0.0009974581189453602

Training epoch-14 batch-119
Running loss of epoch-14 batch-119 = 0.0026083695702254772

Training epoch-14 batch-120
Running loss of epoch-14 batch-120 = 0.0031020627357065678

Training epoch-14 batch-121
Running loss of epoch-14 batch-121 = 0.0022221480030566454

Training epoch-14 batch-122
Running loss of epoch-14 batch-122 = 0.003627755446359515

Training epoch-14 batch-123
Running loss of epoch-14 batch-123 = 0.0011748062679544091

Training epoch-14 batch-124
Running loss of epoch-14 batch-124 = 0.002582839922979474

Training epoch-14 batch-125
Running loss of epoch-14 batch-125 = 0.0021411143243312836

Training epoch-14 batch-126
Running loss of epoch-14 batch-126 = 0.0019746238831430674

Training epoch-14 batch-127
Running loss of epoch-14 batch-127 = 0.0031000473536551

Training epoch-14 batch-128
Running loss of epoch-14 batch-128 = 0.001602549571543932

Training epoch-14 batch-129
Running loss of epoch-14 batch-129 = 0.0008666730718687177

Training epoch-14 batch-130
Running loss of epoch-14 batch-130 = 0.0025879822205752134

Training epoch-14 batch-131
Running loss of epoch-14 batch-131 = 0.00445969495922327

Training epoch-14 batch-132
Running loss of epoch-14 batch-132 = 0.004133853130042553

Training epoch-14 batch-133
Running loss of epoch-14 batch-133 = 0.0015004858141764998

Training epoch-14 batch-134
Running loss of epoch-14 batch-134 = 0.004036515951156616

Training epoch-14 batch-135
Running loss of epoch-14 batch-135 = 0.0028437613509595394

Training epoch-14 batch-136
Running loss of epoch-14 batch-136 = 0.003619733965024352

Training epoch-14 batch-137
Running loss of epoch-14 batch-137 = 0.003794729243963957

Training epoch-14 batch-138
Running loss of epoch-14 batch-138 = 0.0037801568396389484

Training epoch-14 batch-139
Running loss of epoch-14 batch-139 = 0.0018722759559750557

Training epoch-14 batch-140
Running loss of epoch-14 batch-140 = 0.004121528472751379

Training epoch-14 batch-141
Running loss of epoch-14 batch-141 = 0.003078354988247156

Training epoch-14 batch-142
Running loss of epoch-14 batch-142 = 0.0038471005391329527

Training epoch-14 batch-143
Running loss of epoch-14 batch-143 = 0.003663623007014394

Training epoch-14 batch-144
Running loss of epoch-14 batch-144 = 0.002440223703160882

Training epoch-14 batch-145
Running loss of epoch-14 batch-145 = 0.002261813497170806

Training epoch-14 batch-146
Running loss of epoch-14 batch-146 = 0.0015042179729789495

Training epoch-14 batch-147
Running loss of epoch-14 batch-147 = 0.004954412113875151

Training epoch-14 batch-148
Running loss of epoch-14 batch-148 = 0.004563663154840469

Training epoch-14 batch-149
Running loss of epoch-14 batch-149 = 0.003348670667037368

Training epoch-14 batch-150
Running loss of epoch-14 batch-150 = 0.0024179655592888594

Training epoch-14 batch-151
Running loss of epoch-14 batch-151 = 0.00280285463668406

Training epoch-14 batch-152
Running loss of epoch-14 batch-152 = 0.002896360121667385

Training epoch-14 batch-153
Running loss of epoch-14 batch-153 = 0.001984118949621916

Training epoch-14 batch-154
Running loss of epoch-14 batch-154 = 0.003585390280932188

Training epoch-14 batch-155
Running loss of epoch-14 batch-155 = 0.0037083302158862352

Training epoch-14 batch-156
Running loss of epoch-14 batch-156 = 0.002502567833289504

Training epoch-14 batch-157
Running loss of epoch-14 batch-157 = 0.009665057063102722

Finished training epoch-14.



Average train loss at epoch-14 = 0.002244381196796894

Started Evaluation

Average val loss at epoch-14 = 0.8476768007561699

Accuracy for classes:
Accuracy for class equals is: 97.03 %
Accuracy for class main is: 93.77 %
Accuracy for class setUp is: 89.18 %
Accuracy for class onCreate is: 92.11 %
Accuracy for class toString is: 80.89 %
Accuracy for class run is: 67.81 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 33.41 %
Accuracy for class execute is: 59.04 %
Accuracy for class get is: 60.51 %

Overall Accuracy = 80.38 %

Finished Evaluation



Started training epoch-15


Training epoch-15 batch-1
Running loss of epoch-15 batch-1 = 0.003023641649633646

Training epoch-15 batch-2
Running loss of epoch-15 batch-2 = 0.0022700398694723845

Training epoch-15 batch-3
Running loss of epoch-15 batch-3 = 0.003988686017692089

Training epoch-15 batch-4
Running loss of epoch-15 batch-4 = 0.002622673287987709

Training epoch-15 batch-5
Running loss of epoch-15 batch-5 = 0.0011214044643566012

Training epoch-15 batch-6
Running loss of epoch-15 batch-6 = 0.00271475687623024

Training epoch-15 batch-7
Running loss of epoch-15 batch-7 = 0.0032290189992636442

Training epoch-15 batch-8
Running loss of epoch-15 batch-8 = 0.0016942400252446532

Training epoch-15 batch-9
Running loss of epoch-15 batch-9 = 0.002962866798043251

Training epoch-15 batch-10
Running loss of epoch-15 batch-10 = 0.002517316723242402

Training epoch-15 batch-11
Running loss of epoch-15 batch-11 = 0.0012536912690848112

Training epoch-15 batch-12
Running loss of epoch-15 batch-12 = 0.0026100664399564266

Training epoch-15 batch-13
Running loss of epoch-15 batch-13 = 0.0015391289489343762

Training epoch-15 batch-14
Running loss of epoch-15 batch-14 = 0.0038507042918354273

Training epoch-15 batch-15
Running loss of epoch-15 batch-15 = 0.0018515164265409112

Training epoch-15 batch-16
Running loss of epoch-15 batch-16 = 0.002860219683498144

Training epoch-15 batch-17
Running loss of epoch-15 batch-17 = 0.0005127426702529192

Training epoch-15 batch-18
Running loss of epoch-15 batch-18 = 0.002237441251054406

Training epoch-15 batch-19
Running loss of epoch-15 batch-19 = 0.001714086625725031

Training epoch-15 batch-20
Running loss of epoch-15 batch-20 = 0.001924016047269106

Training epoch-15 batch-21
Running loss of epoch-15 batch-21 = 0.001052090898156166

Training epoch-15 batch-22
Running loss of epoch-15 batch-22 = 0.0014367757830768824

Training epoch-15 batch-23
Running loss of epoch-15 batch-23 = 0.001451381598599255

Training epoch-15 batch-24
Running loss of epoch-15 batch-24 = 0.0012016198597848415

Training epoch-15 batch-25
Running loss of epoch-15 batch-25 = 0.0009992776904255152

Training epoch-15 batch-26
Running loss of epoch-15 batch-26 = 0.0017488186713308096

Training epoch-15 batch-27
Running loss of epoch-15 batch-27 = 0.0021238576155155897

Training epoch-15 batch-28
Running loss of epoch-15 batch-28 = 0.0020001523662358522

Training epoch-15 batch-29
Running loss of epoch-15 batch-29 = 0.001763510867021978

Training epoch-15 batch-30
Running loss of epoch-15 batch-30 = 0.001284510362893343

Training epoch-15 batch-31
Running loss of epoch-15 batch-31 = 0.001895679859444499

Training epoch-15 batch-32
Running loss of epoch-15 batch-32 = 0.0034446276258677244

Training epoch-15 batch-33
Running loss of epoch-15 batch-33 = 0.0008810325525701046

Training epoch-15 batch-34
Running loss of epoch-15 batch-34 = 0.0014792380388826132

Training epoch-15 batch-35
Running loss of epoch-15 batch-35 = 0.000932772527448833

Training epoch-15 batch-36
Running loss of epoch-15 batch-36 = 0.00192766054533422

Training epoch-15 batch-37
Running loss of epoch-15 batch-37 = 0.003647182136774063

Training epoch-15 batch-38
Running loss of epoch-15 batch-38 = 0.0005913096247240901

Training epoch-15 batch-39
Running loss of epoch-15 batch-39 = 0.0019099380588158965

Training epoch-15 batch-40
Running loss of epoch-15 batch-40 = 0.001733157318085432

Training epoch-15 batch-41
Running loss of epoch-15 batch-41 = 0.0009975216817110777

Training epoch-15 batch-42
Running loss of epoch-15 batch-42 = 0.002413783222436905

Training epoch-15 batch-43
Running loss of epoch-15 batch-43 = 0.003048152197152376

Training epoch-15 batch-44
Running loss of epoch-15 batch-44 = 0.0016715861856937408

Training epoch-15 batch-45
Running loss of epoch-15 batch-45 = 0.0022414815612137318

Training epoch-15 batch-46
Running loss of epoch-15 batch-46 = 0.0012432570802047849

Training epoch-15 batch-47
Running loss of epoch-15 batch-47 = 0.0011015307391062379

Training epoch-15 batch-48
Running loss of epoch-15 batch-48 = 0.0012629085686057806

Training epoch-15 batch-49
Running loss of epoch-15 batch-49 = 0.001911637606099248

Training epoch-15 batch-50
Running loss of epoch-15 batch-50 = 0.0010327543132007122

Training epoch-15 batch-51
Running loss of epoch-15 batch-51 = 0.002401517005637288

Training epoch-15 batch-52
Running loss of epoch-15 batch-52 = 0.0021364493295550346

Training epoch-15 batch-53
Running loss of epoch-15 batch-53 = 0.0006011173827573657

Training epoch-15 batch-54
Running loss of epoch-15 batch-54 = 0.002888778690248728

Training epoch-15 batch-55
Running loss of epoch-15 batch-55 = 0.0021346770226955414

Training epoch-15 batch-56
Running loss of epoch-15 batch-56 = 0.0028368625789880753

Training epoch-15 batch-57
Running loss of epoch-15 batch-57 = 0.0015278976643458009

Training epoch-15 batch-58
Running loss of epoch-15 batch-58 = 0.0018428214825689793

Training epoch-15 batch-59
Running loss of epoch-15 batch-59 = 0.0010881501948460937

Training epoch-15 batch-60
Running loss of epoch-15 batch-60 = 0.0016700481064617634

Training epoch-15 batch-61
Running loss of epoch-15 batch-61 = 0.0032124342396855354

Training epoch-15 batch-62
Running loss of epoch-15 batch-62 = 0.0024429752957075834

Training epoch-15 batch-63
Running loss of epoch-15 batch-63 = 0.0006992215057834983

Training epoch-15 batch-64
Running loss of epoch-15 batch-64 = 0.0033538967836648226

Training epoch-15 batch-65
Running loss of epoch-15 batch-65 = 0.0013473856961354613

Training epoch-15 batch-66
Running loss of epoch-15 batch-66 = 0.001837729592807591

Training epoch-15 batch-67
Running loss of epoch-15 batch-67 = 0.0008283616043627262

Training epoch-15 batch-68
Running loss of epoch-15 batch-68 = 0.002598551567643881

Training epoch-15 batch-69
Running loss of epoch-15 batch-69 = 0.001913613872602582

Training epoch-15 batch-70
Running loss of epoch-15 batch-70 = 0.0022046843077987432

Training epoch-15 batch-71
Running loss of epoch-15 batch-71 = 0.0007633387576788664

Training epoch-15 batch-72
Running loss of epoch-15 batch-72 = 0.0014898293884471059

Training epoch-15 batch-73
Running loss of epoch-15 batch-73 = 0.001057394314557314

Training epoch-15 batch-74
Running loss of epoch-15 batch-74 = 0.001576820621266961

Training epoch-15 batch-75
Running loss of epoch-15 batch-75 = 0.0010667325695976615

Training epoch-15 batch-76
Running loss of epoch-15 batch-76 = 0.0012163313804194331

Training epoch-15 batch-77
Running loss of epoch-15 batch-77 = 0.001437470898963511

Training epoch-15 batch-78
Running loss of epoch-15 batch-78 = 0.00142784439958632

Training epoch-15 batch-79
Running loss of epoch-15 batch-79 = 0.001353458152152598

Training epoch-15 batch-80
Running loss of epoch-15 batch-80 = 0.0005979264387860894

Training epoch-15 batch-81
Running loss of epoch-15 batch-81 = 0.00219142553396523

Training epoch-15 batch-82
Running loss of epoch-15 batch-82 = 0.0012514141853898764

Training epoch-15 batch-83
Running loss of epoch-15 batch-83 = 0.001892408006824553

Training epoch-15 batch-84
Running loss of epoch-15 batch-84 = 0.0013345693005248904

Training epoch-15 batch-85
Running loss of epoch-15 batch-85 = 0.001306056510657072

Training epoch-15 batch-86
Running loss of epoch-15 batch-86 = 0.0020888368599116802

Training epoch-15 batch-87
Running loss of epoch-15 batch-87 = 0.0019741076976060867

Training epoch-15 batch-88
Running loss of epoch-15 batch-88 = 0.002828565426170826

Training epoch-15 batch-89
Running loss of epoch-15 batch-89 = 0.003187773283571005

Training epoch-15 batch-90
Running loss of epoch-15 batch-90 = 0.0003562414785847068

Training epoch-15 batch-91
Running loss of epoch-15 batch-91 = 0.0009529299568384886

Training epoch-15 batch-92
Running loss of epoch-15 batch-92 = 0.0025956295430660248

Training epoch-15 batch-93
Running loss of epoch-15 batch-93 = 0.0010100244544446468

Training epoch-15 batch-94
Running loss of epoch-15 batch-94 = 0.0008097263053059578

Training epoch-15 batch-95
Running loss of epoch-15 batch-95 = 0.0019866072107106447

Training epoch-15 batch-96
Running loss of epoch-15 batch-96 = 0.000959863536991179

Training epoch-15 batch-97
Running loss of epoch-15 batch-97 = 0.0014635541010648012

Training epoch-15 batch-98
Running loss of epoch-15 batch-98 = 0.0015764592681080103

Training epoch-15 batch-99
Running loss of epoch-15 batch-99 = 0.002230621175840497

Training epoch-15 batch-100
Running loss of epoch-15 batch-100 = 0.0009700682712718844

Training epoch-15 batch-101
Running loss of epoch-15 batch-101 = 0.0010520427022129297

Training epoch-15 batch-102
Running loss of epoch-15 batch-102 = 0.0006577236345037818

Training epoch-15 batch-103
Running loss of epoch-15 batch-103 = 0.0015000926796346903

Training epoch-15 batch-104
Running loss of epoch-15 batch-104 = 0.0028439885936677456

Training epoch-15 batch-105
Running loss of epoch-15 batch-105 = 0.002685264917090535

Training epoch-15 batch-106
Running loss of epoch-15 batch-106 = 0.0035195392556488514

Training epoch-15 batch-107
Running loss of epoch-15 batch-107 = 0.0014047424774616957

Training epoch-15 batch-108
Running loss of epoch-15 batch-108 = 0.0015853726072236896

Training epoch-15 batch-109
Running loss of epoch-15 batch-109 = 0.002130473032593727

Training epoch-15 batch-110
Running loss of epoch-15 batch-110 = 0.002601555548608303

Training epoch-15 batch-111
Running loss of epoch-15 batch-111 = 0.000856165075674653

Training epoch-15 batch-112
Running loss of epoch-15 batch-112 = 0.0007897596806287766

Training epoch-15 batch-113
Running loss of epoch-15 batch-113 = 0.001436883001588285

Training epoch-15 batch-114
Running loss of epoch-15 batch-114 = 0.002598118968307972

Training epoch-15 batch-115
Running loss of epoch-15 batch-115 = 0.003638410707935691

Training epoch-15 batch-116
Running loss of epoch-15 batch-116 = 0.0013636357616633177

Training epoch-15 batch-117
Running loss of epoch-15 batch-117 = 0.0020868319552391768

Training epoch-15 batch-118
Running loss of epoch-15 batch-118 = 0.0008087227470241487

Training epoch-15 batch-119
Running loss of epoch-15 batch-119 = 0.0020674392580986023

Training epoch-15 batch-120
Running loss of epoch-15 batch-120 = 0.0018685015384107828

Training epoch-15 batch-121
Running loss of epoch-15 batch-121 = 0.0025905512738972902

Training epoch-15 batch-122
Running loss of epoch-15 batch-122 = 0.0025708735920488834

Training epoch-15 batch-123
Running loss of epoch-15 batch-123 = 0.0030419842805713415

Training epoch-15 batch-124
Running loss of epoch-15 batch-124 = 0.0012079151347279549

Training epoch-15 batch-125
Running loss of epoch-15 batch-125 = 0.0018968891818076372

Training epoch-15 batch-126
Running loss of epoch-15 batch-126 = 0.0012560168979689479

Training epoch-15 batch-127
Running loss of epoch-15 batch-127 = 0.0028214158955961466

Training epoch-15 batch-128
Running loss of epoch-15 batch-128 = 0.0014556094538420439

Training epoch-15 batch-129
Running loss of epoch-15 batch-129 = 0.0018874196102842689

Training epoch-15 batch-130
Running loss of epoch-15 batch-130 = 0.0013068871339783072

Training epoch-15 batch-131
Running loss of epoch-15 batch-131 = 0.0023794742301106453

Training epoch-15 batch-132
Running loss of epoch-15 batch-132 = 0.003318876028060913

Training epoch-15 batch-133
Running loss of epoch-15 batch-133 = 0.0014018771471455693

Training epoch-15 batch-134
Running loss of epoch-15 batch-134 = 0.0012572372797876596

Training epoch-15 batch-135
Running loss of epoch-15 batch-135 = 0.001172584597952664

Training epoch-15 batch-136
Running loss of epoch-15 batch-136 = 0.002000219654291868

Training epoch-15 batch-137
Running loss of epoch-15 batch-137 = 0.001767885871231556

Training epoch-15 batch-138
Running loss of epoch-15 batch-138 = 0.002358803991228342

Training epoch-15 batch-139
Running loss of epoch-15 batch-139 = 0.0018873171648010612

Training epoch-15 batch-140
Running loss of epoch-15 batch-140 = 0.0022301271092146635

Training epoch-15 batch-141
Running loss of epoch-15 batch-141 = 0.001938076689839363

Training epoch-15 batch-142
Running loss of epoch-15 batch-142 = 0.003971738740801811

Training epoch-15 batch-143
Running loss of epoch-15 batch-143 = 0.004335792735219002

Training epoch-15 batch-144
Running loss of epoch-15 batch-144 = 0.0013597551733255386

Training epoch-15 batch-145
Running loss of epoch-15 batch-145 = 0.002094788709655404

Training epoch-15 batch-146
Running loss of epoch-15 batch-146 = 0.002878853352740407

Training epoch-15 batch-147
Running loss of epoch-15 batch-147 = 0.0020790165290236473

Training epoch-15 batch-148
Running loss of epoch-15 batch-148 = 0.001723718480207026

Training epoch-15 batch-149
Running loss of epoch-15 batch-149 = 0.0015459011774510145

Training epoch-15 batch-150
Running loss of epoch-15 batch-150 = 0.0008530794875696301

Training epoch-15 batch-151
Running loss of epoch-15 batch-151 = 0.0008795650210231543

Training epoch-15 batch-152
Running loss of epoch-15 batch-152 = 0.000747347017750144

Training epoch-15 batch-153
Running loss of epoch-15 batch-153 = 0.0014563672011718154

Training epoch-15 batch-154
Running loss of epoch-15 batch-154 = 0.001543864025734365

Training epoch-15 batch-155
Running loss of epoch-15 batch-155 = 0.0015391982160508633

Training epoch-15 batch-156
Running loss of epoch-15 batch-156 = 0.001511035137809813

Training epoch-15 batch-157
Running loss of epoch-15 batch-157 = 0.002623148262500763

Finished training epoch-15.



Average train loss at epoch-15 = 0.0018551538106054068

Started Evaluation

Average val loss at epoch-15 = 0.8245158966586276

Accuracy for classes:
Accuracy for class equals is: 94.72 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 93.93 %
Accuracy for class onCreate is: 88.27 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 61.87 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 50.90 %
Accuracy for class execute is: 45.38 %
Accuracy for class get is: 66.15 %

Overall Accuracy = 81.27 %

Finished Evaluation



Started training epoch-16


Training epoch-16 batch-1
Running loss of epoch-16 batch-1 = 0.0013244497822597623

Training epoch-16 batch-2
Running loss of epoch-16 batch-2 = 0.00046817632392048836

Training epoch-16 batch-3
Running loss of epoch-16 batch-3 = 0.0010631411569193006

Training epoch-16 batch-4
Running loss of epoch-16 batch-4 = 0.001755430013872683

Training epoch-16 batch-5
Running loss of epoch-16 batch-5 = 0.002381088910624385

Training epoch-16 batch-6
Running loss of epoch-16 batch-6 = 0.0012351245386525989

Training epoch-16 batch-7
Running loss of epoch-16 batch-7 = 0.0014646733179688454

Training epoch-16 batch-8
Running loss of epoch-16 batch-8 = 0.0015574137214571238

Training epoch-16 batch-9
Running loss of epoch-16 batch-9 = 0.0017251810058951378

Training epoch-16 batch-10
Running loss of epoch-16 batch-10 = 0.0012581441551446915

Training epoch-16 batch-11
Running loss of epoch-16 batch-11 = 0.0010623183334246278

Training epoch-16 batch-12
Running loss of epoch-16 batch-12 = 0.00046164740342646837

Training epoch-16 batch-13
Running loss of epoch-16 batch-13 = 0.0013371596578508615

Training epoch-16 batch-14
Running loss of epoch-16 batch-14 = 0.002684148261323571

Training epoch-16 batch-15
Running loss of epoch-16 batch-15 = 0.0004850855330005288

Training epoch-16 batch-16
Running loss of epoch-16 batch-16 = 0.0014676706632599235

Training epoch-16 batch-17
Running loss of epoch-16 batch-17 = 0.00242991023696959

Training epoch-16 batch-18
Running loss of epoch-16 batch-18 = 0.0010201743571087718

Training epoch-16 batch-19
Running loss of epoch-16 batch-19 = 0.002572613302618265

Training epoch-16 batch-20
Running loss of epoch-16 batch-20 = 0.0017861371161416173

Training epoch-16 batch-21
Running loss of epoch-16 batch-21 = 0.002431835513561964

Training epoch-16 batch-22
Running loss of epoch-16 batch-22 = 0.0013787468196824193

Training epoch-16 batch-23
Running loss of epoch-16 batch-23 = 0.0005860740784555674

Training epoch-16 batch-24
Running loss of epoch-16 batch-24 = 0.002123370533809066

Training epoch-16 batch-25
Running loss of epoch-16 batch-25 = 0.0012205963721498847

Training epoch-16 batch-26
Running loss of epoch-16 batch-26 = 0.0016129827126860619

Training epoch-16 batch-27
Running loss of epoch-16 batch-27 = 0.0011759694898501039

Training epoch-16 batch-28
Running loss of epoch-16 batch-28 = 0.0011637677671387792

Training epoch-16 batch-29
Running loss of epoch-16 batch-29 = 0.0018615358276292682

Training epoch-16 batch-30
Running loss of epoch-16 batch-30 = 0.0019973714370280504

Training epoch-16 batch-31
Running loss of epoch-16 batch-31 = 0.0015041574370115995

Training epoch-16 batch-32
Running loss of epoch-16 batch-32 = 0.0007938105845823884

Training epoch-16 batch-33
Running loss of epoch-16 batch-33 = 0.0013529284624382854

Training epoch-16 batch-34
Running loss of epoch-16 batch-34 = 0.002325687324628234

Training epoch-16 batch-35
Running loss of epoch-16 batch-35 = 0.001072671264410019

Training epoch-16 batch-36
Running loss of epoch-16 batch-36 = 0.0013372633839026093

Training epoch-16 batch-37
Running loss of epoch-16 batch-37 = 0.0021440102718770504

Training epoch-16 batch-38
Running loss of epoch-16 batch-38 = 0.0034341830760240555

Training epoch-16 batch-39
Running loss of epoch-16 batch-39 = 0.0006597012979909778

Training epoch-16 batch-40
Running loss of epoch-16 batch-40 = 0.0010197637602686882

Training epoch-16 batch-41
Running loss of epoch-16 batch-41 = 0.002237049164250493

Training epoch-16 batch-42
Running loss of epoch-16 batch-42 = 0.0011697016889229417

Training epoch-16 batch-43
Running loss of epoch-16 batch-43 = 0.0005529460031539202

Training epoch-16 batch-44
Running loss of epoch-16 batch-44 = 0.0017028787406161427

Training epoch-16 batch-45
Running loss of epoch-16 batch-45 = 0.0014423700049519539

Training epoch-16 batch-46
Running loss of epoch-16 batch-46 = 0.0012576007284224033

Training epoch-16 batch-47
Running loss of epoch-16 batch-47 = 0.0012135390425100923

Training epoch-16 batch-48
Running loss of epoch-16 batch-48 = 0.001394920633174479

Training epoch-16 batch-49
Running loss of epoch-16 batch-49 = 0.0004577802028506994

Training epoch-16 batch-50
Running loss of epoch-16 batch-50 = 0.001007028971798718

Training epoch-16 batch-51
Running loss of epoch-16 batch-51 = 0.0012147430097684264

Training epoch-16 batch-52
Running loss of epoch-16 batch-52 = 0.0019537522457540035

Training epoch-16 batch-53
Running loss of epoch-16 batch-53 = 0.0013343547470867634

Training epoch-16 batch-54
Running loss of epoch-16 batch-54 = 0.0019705407321453094

Training epoch-16 batch-55
Running loss of epoch-16 batch-55 = 0.0029454140458256006

Training epoch-16 batch-56
Running loss of epoch-16 batch-56 = 0.0011184851173311472

Training epoch-16 batch-57
Running loss of epoch-16 batch-57 = 0.0009738518856465816

Training epoch-16 batch-58
Running loss of epoch-16 batch-58 = 0.0010364410700276494

Training epoch-16 batch-59
Running loss of epoch-16 batch-59 = 0.000947928405366838

Training epoch-16 batch-60
Running loss of epoch-16 batch-60 = 0.0006135472794994712

Training epoch-16 batch-61
Running loss of epoch-16 batch-61 = 0.0011465781135484576

Training epoch-16 batch-62
Running loss of epoch-16 batch-62 = 0.001032811589539051

Training epoch-16 batch-63
Running loss of epoch-16 batch-63 = 0.0006756915245205164

Training epoch-16 batch-64
Running loss of epoch-16 batch-64 = 0.0013568159192800522

Training epoch-16 batch-65
Running loss of epoch-16 batch-65 = 0.0022227515000849962

Training epoch-16 batch-66
Running loss of epoch-16 batch-66 = 0.0004755309782922268

Training epoch-16 batch-67
Running loss of epoch-16 batch-67 = 0.0011441698297858238

Training epoch-16 batch-68
Running loss of epoch-16 batch-68 = 0.0004589069867506623

Training epoch-16 batch-69
Running loss of epoch-16 batch-69 = 0.0029519684612751007

Training epoch-16 batch-70
Running loss of epoch-16 batch-70 = 0.003184159519150853

Training epoch-16 batch-71
Running loss of epoch-16 batch-71 = 0.002378451870754361

Training epoch-16 batch-72
Running loss of epoch-16 batch-72 = 0.001990777440369129

Training epoch-16 batch-73
Running loss of epoch-16 batch-73 = 0.0015990601386874914

Training epoch-16 batch-74
Running loss of epoch-16 batch-74 = 0.000525309587828815

Training epoch-16 batch-75
Running loss of epoch-16 batch-75 = 0.0018565409118309617

Training epoch-16 batch-76
Running loss of epoch-16 batch-76 = 0.0031930021941661835

Training epoch-16 batch-77
Running loss of epoch-16 batch-77 = 0.0009173884755000472

Training epoch-16 batch-78
Running loss of epoch-16 batch-78 = 0.0011089962208643556

Training epoch-16 batch-79
Running loss of epoch-16 batch-79 = 0.0011875444324687123

Training epoch-16 batch-80
Running loss of epoch-16 batch-80 = 0.001706875511445105

Training epoch-16 batch-81
Running loss of epoch-16 batch-81 = 0.0011204980546608567

Training epoch-16 batch-82
Running loss of epoch-16 batch-82 = 0.0015774354105815291

Training epoch-16 batch-83
Running loss of epoch-16 batch-83 = 0.000896222423762083

Training epoch-16 batch-84
Running loss of epoch-16 batch-84 = 0.000976025010459125

Training epoch-16 batch-85
Running loss of epoch-16 batch-85 = 0.0017768489196896553

Training epoch-16 batch-86
Running loss of epoch-16 batch-86 = 0.000685486476868391

Training epoch-16 batch-87
Running loss of epoch-16 batch-87 = 0.001685543218627572

Training epoch-16 batch-88
Running loss of epoch-16 batch-88 = 0.001705881324596703

Training epoch-16 batch-89
Running loss of epoch-16 batch-89 = 0.0009102632757276297

Training epoch-16 batch-90
Running loss of epoch-16 batch-90 = 0.0007933569722808897

Training epoch-16 batch-91
Running loss of epoch-16 batch-91 = 0.0007220314582809806

Training epoch-16 batch-92
Running loss of epoch-16 batch-92 = 0.0012949313968420029

Training epoch-16 batch-93
Running loss of epoch-16 batch-93 = 0.001816291594877839

Training epoch-16 batch-94
Running loss of epoch-16 batch-94 = 0.002053016098216176

Training epoch-16 batch-95
Running loss of epoch-16 batch-95 = 0.001708701252937317

Training epoch-16 batch-96
Running loss of epoch-16 batch-96 = 0.0014907976146787405

Training epoch-16 batch-97
Running loss of epoch-16 batch-97 = 0.001226134249009192

Training epoch-16 batch-98
Running loss of epoch-16 batch-98 = 0.0005680760368704796

Training epoch-16 batch-99
Running loss of epoch-16 batch-99 = 0.002079956466332078

Training epoch-16 batch-100
Running loss of epoch-16 batch-100 = 0.0024240475613623857

Training epoch-16 batch-101
Running loss of epoch-16 batch-101 = 0.0012693319004029036

Training epoch-16 batch-102
Running loss of epoch-16 batch-102 = 0.0012096348218619823

Training epoch-16 batch-103
Running loss of epoch-16 batch-103 = 0.002548093441873789

Training epoch-16 batch-104
Running loss of epoch-16 batch-104 = 0.0012297395151108503

Training epoch-16 batch-105
Running loss of epoch-16 batch-105 = 0.0005991148063912988

Training epoch-16 batch-106
Running loss of epoch-16 batch-106 = 0.0014508834574371576

Training epoch-16 batch-107
Running loss of epoch-16 batch-107 = 0.0018754169577732682

Training epoch-16 batch-108
Running loss of epoch-16 batch-108 = 0.0028350469656288624

Training epoch-16 batch-109
Running loss of epoch-16 batch-109 = 0.002665541833266616

Training epoch-16 batch-110
Running loss of epoch-16 batch-110 = 0.0012870762730017304

Training epoch-16 batch-111
Running loss of epoch-16 batch-111 = 0.0002017761580646038

Training epoch-16 batch-112
Running loss of epoch-16 batch-112 = 0.001479871105402708

Training epoch-16 batch-113
Running loss of epoch-16 batch-113 = 0.0023790106642991304

Training epoch-16 batch-114
Running loss of epoch-16 batch-114 = 0.0016357667045667768

Training epoch-16 batch-115
Running loss of epoch-16 batch-115 = 0.0012549328384920955

Training epoch-16 batch-116
Running loss of epoch-16 batch-116 = 0.0015227212570607662

Training epoch-16 batch-117
Running loss of epoch-16 batch-117 = 0.0023283823393285275

Training epoch-16 batch-118
Running loss of epoch-16 batch-118 = 0.0010786703787744045

Training epoch-16 batch-119
Running loss of epoch-16 batch-119 = 0.001182992709800601

Training epoch-16 batch-120
Running loss of epoch-16 batch-120 = 0.0022257801610976458

Training epoch-16 batch-121
Running loss of epoch-16 batch-121 = 0.001054400927387178

Training epoch-16 batch-122
Running loss of epoch-16 batch-122 = 0.0007827392546460032

Training epoch-16 batch-123
Running loss of epoch-16 batch-123 = 0.0008343856316059828

Training epoch-16 batch-124
Running loss of epoch-16 batch-124 = 0.0012379217660054564

Training epoch-16 batch-125
Running loss of epoch-16 batch-125 = 0.00256513524800539

Training epoch-16 batch-126
Running loss of epoch-16 batch-126 = 0.0018248831620439887

Training epoch-16 batch-127
Running loss of epoch-16 batch-127 = 0.0013035556767135859

Training epoch-16 batch-128
Running loss of epoch-16 batch-128 = 0.001163159729912877

Training epoch-16 batch-129
Running loss of epoch-16 batch-129 = 0.002490852726623416

Training epoch-16 batch-130
Running loss of epoch-16 batch-130 = 0.0007469326956197619

Training epoch-16 batch-131
Running loss of epoch-16 batch-131 = 0.0006974806310608983

Training epoch-16 batch-132
Running loss of epoch-16 batch-132 = 0.0009890272049233317

Training epoch-16 batch-133
Running loss of epoch-16 batch-133 = 0.0009694840991869569

Training epoch-16 batch-134
Running loss of epoch-16 batch-134 = 0.0005863681435585022

Training epoch-16 batch-135
Running loss of epoch-16 batch-135 = 0.000528539065271616

Training epoch-16 batch-136
Running loss of epoch-16 batch-136 = 0.0011540595442056656

Training epoch-16 batch-137
Running loss of epoch-16 batch-137 = 0.002495696535333991

Training epoch-16 batch-138
Running loss of epoch-16 batch-138 = 0.002164982259273529

Training epoch-16 batch-139
Running loss of epoch-16 batch-139 = 0.0007023881189525127

Training epoch-16 batch-140
Running loss of epoch-16 batch-140 = 0.0012044836767017841

Training epoch-16 batch-141
Running loss of epoch-16 batch-141 = 0.0022640929091721773

Training epoch-16 batch-142
Running loss of epoch-16 batch-142 = 0.0020737824961543083

Training epoch-16 batch-143
Running loss of epoch-16 batch-143 = 0.002427654340863228

Training epoch-16 batch-144
Running loss of epoch-16 batch-144 = 0.0012060452718287706

Training epoch-16 batch-145
Running loss of epoch-16 batch-145 = 0.0004777668509632349

Training epoch-16 batch-146
Running loss of epoch-16 batch-146 = 0.0023493021726608276

Training epoch-16 batch-147
Running loss of epoch-16 batch-147 = 0.0017011617310345173

Training epoch-16 batch-148
Running loss of epoch-16 batch-148 = 0.0015798341482877731

Training epoch-16 batch-149
Running loss of epoch-16 batch-149 = 0.0008260421454906464

Training epoch-16 batch-150
Running loss of epoch-16 batch-150 = 0.00382768246345222

Training epoch-16 batch-151
Running loss of epoch-16 batch-151 = 0.001881684293039143

Training epoch-16 batch-152
Running loss of epoch-16 batch-152 = 0.0016475445590913296

Training epoch-16 batch-153
Running loss of epoch-16 batch-153 = 0.0006800814298912883

Training epoch-16 batch-154
Running loss of epoch-16 batch-154 = 0.0014969476033002138

Training epoch-16 batch-155
Running loss of epoch-16 batch-155 = 0.0011608595959842205

Training epoch-16 batch-156
Running loss of epoch-16 batch-156 = 0.0025257368106395006

Training epoch-16 batch-157
Running loss of epoch-16 batch-157 = 0.002180960029363632

Finished training epoch-16.



Average train loss at epoch-16 = 0.0014869349386543035

Started Evaluation

Average val loss at epoch-16 = 0.9528643834199964

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 86.23 %
Accuracy for class onCreate is: 86.35 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 70.32 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 41.26 %
Accuracy for class execute is: 45.38 %
Accuracy for class get is: 68.46 %

Overall Accuracy = 79.95 %

Finished Evaluation



Started training epoch-17


Training epoch-17 batch-1
Running loss of epoch-17 batch-1 = 0.001424425165168941

Training epoch-17 batch-2
Running loss of epoch-17 batch-2 = 0.0010477848118171096

Training epoch-17 batch-3
Running loss of epoch-17 batch-3 = 0.0006218849448487163

Training epoch-17 batch-4
Running loss of epoch-17 batch-4 = 0.000900506041944027

Training epoch-17 batch-5
Running loss of epoch-17 batch-5 = 0.0005004151025786996

Training epoch-17 batch-6
Running loss of epoch-17 batch-6 = 0.0011513207573443651

Training epoch-17 batch-7
Running loss of epoch-17 batch-7 = 0.001818378921598196

Training epoch-17 batch-8
Running loss of epoch-17 batch-8 = 0.0015938900178298354

Training epoch-17 batch-9
Running loss of epoch-17 batch-9 = 0.0007296557305380702

Training epoch-17 batch-10
Running loss of epoch-17 batch-10 = 0.0014502358390018344

Training epoch-17 batch-11
Running loss of epoch-17 batch-11 = 0.0010518181370571256

Training epoch-17 batch-12
Running loss of epoch-17 batch-12 = 0.0012423410080373287

Training epoch-17 batch-13
Running loss of epoch-17 batch-13 = 0.00031147338449954987

Training epoch-17 batch-14
Running loss of epoch-17 batch-14 = 0.0013341744197532535

Training epoch-17 batch-15
Running loss of epoch-17 batch-15 = 0.00200140243396163

Training epoch-17 batch-16
Running loss of epoch-17 batch-16 = 0.0011321972124278545

Training epoch-17 batch-17
Running loss of epoch-17 batch-17 = 0.001164437155239284

Training epoch-17 batch-18
Running loss of epoch-17 batch-18 = 0.0010578351793810725

Training epoch-17 batch-19
Running loss of epoch-17 batch-19 = 0.0008688129018992186

Training epoch-17 batch-20
Running loss of epoch-17 batch-20 = 0.0012845078017562628

Training epoch-17 batch-21
Running loss of epoch-17 batch-21 = 0.0006559885805472732

Training epoch-17 batch-22
Running loss of epoch-17 batch-22 = 0.0017921344842761755

Training epoch-17 batch-23
Running loss of epoch-17 batch-23 = 0.0013590705348178744

Training epoch-17 batch-24
Running loss of epoch-17 batch-24 = 0.001369051868095994

Training epoch-17 batch-25
Running loss of epoch-17 batch-25 = 0.0006783574353903532

Training epoch-17 batch-26
Running loss of epoch-17 batch-26 = 0.00044443050865083933

Training epoch-17 batch-27
Running loss of epoch-17 batch-27 = 0.0013049810659140348

Training epoch-17 batch-28
Running loss of epoch-17 batch-28 = 0.001460790866985917

Training epoch-17 batch-29
Running loss of epoch-17 batch-29 = 0.0010092855663970113

Training epoch-17 batch-30
Running loss of epoch-17 batch-30 = 0.001236847834661603

Training epoch-17 batch-31
Running loss of epoch-17 batch-31 = 0.0016930911224335432

Training epoch-17 batch-32
Running loss of epoch-17 batch-32 = 0.001852337154559791

Training epoch-17 batch-33
Running loss of epoch-17 batch-33 = 0.0012816960224881768

Training epoch-17 batch-34
Running loss of epoch-17 batch-34 = 0.001740694628097117

Training epoch-17 batch-35
Running loss of epoch-17 batch-35 = 0.0008027285803109407

Training epoch-17 batch-36
Running loss of epoch-17 batch-36 = 0.0004343705950304866

Training epoch-17 batch-37
Running loss of epoch-17 batch-37 = 0.0019084472442045808

Training epoch-17 batch-38
Running loss of epoch-17 batch-38 = 0.0009070673258975148

Training epoch-17 batch-39
Running loss of epoch-17 batch-39 = 0.0013767083873972297

Training epoch-17 batch-40
Running loss of epoch-17 batch-40 = 0.0017387360567227006

Training epoch-17 batch-41
Running loss of epoch-17 batch-41 = 0.0008051391923800111

Training epoch-17 batch-42
Running loss of epoch-17 batch-42 = 0.0006108623929321766

Training epoch-17 batch-43
Running loss of epoch-17 batch-43 = 0.001966514391824603

Training epoch-17 batch-44
Running loss of epoch-17 batch-44 = 0.0014834710163995624

Training epoch-17 batch-45
Running loss of epoch-17 batch-45 = 0.0004567420110106468

Training epoch-17 batch-46
Running loss of epoch-17 batch-46 = 0.001704042893834412

Training epoch-17 batch-47
Running loss of epoch-17 batch-47 = 0.002123326063156128

Training epoch-17 batch-48
Running loss of epoch-17 batch-48 = 0.0006558046443387866

Training epoch-17 batch-49
Running loss of epoch-17 batch-49 = 0.0009569500107318163

Training epoch-17 batch-50
Running loss of epoch-17 batch-50 = 0.0010556954657658935

Training epoch-17 batch-51
Running loss of epoch-17 batch-51 = 0.0006849878700450063

Training epoch-17 batch-52
Running loss of epoch-17 batch-52 = 0.0010709203779697418

Training epoch-17 batch-53
Running loss of epoch-17 batch-53 = 0.0009496423881500959

Training epoch-17 batch-54
Running loss of epoch-17 batch-54 = 0.0006464318139478564

Training epoch-17 batch-55
Running loss of epoch-17 batch-55 = 0.002406112616881728

Training epoch-17 batch-56
Running loss of epoch-17 batch-56 = 0.0015352198388427496

Training epoch-17 batch-57
Running loss of epoch-17 batch-57 = 0.0007784964982420206

Training epoch-17 batch-58
Running loss of epoch-17 batch-58 = 0.001407703384757042

Training epoch-17 batch-59
Running loss of epoch-17 batch-59 = 0.0011099119437858462

Training epoch-17 batch-60
Running loss of epoch-17 batch-60 = 0.0008544130250811577

Training epoch-17 batch-61
Running loss of epoch-17 batch-61 = 0.0008640228770673275

Training epoch-17 batch-62
Running loss of epoch-17 batch-62 = 0.002456428250297904

Training epoch-17 batch-63
Running loss of epoch-17 batch-63 = 0.0012619717745110393

Training epoch-17 batch-64
Running loss of epoch-17 batch-64 = 0.0009678887436166406

Training epoch-17 batch-65
Running loss of epoch-17 batch-65 = 0.0006101101171225309

Training epoch-17 batch-66
Running loss of epoch-17 batch-66 = 0.001723986817523837

Training epoch-17 batch-67
Running loss of epoch-17 batch-67 = 0.0009035302791744471

Training epoch-17 batch-68
Running loss of epoch-17 batch-68 = 0.00027873553335666656

Training epoch-17 batch-69
Running loss of epoch-17 batch-69 = 0.0007704360177740455

Training epoch-17 batch-70
Running loss of epoch-17 batch-70 = 0.0016499117482453585

Training epoch-17 batch-71
Running loss of epoch-17 batch-71 = 0.0006453873356804252

Training epoch-17 batch-72
Running loss of epoch-17 batch-72 = 0.0006317953811958432

Training epoch-17 batch-73
Running loss of epoch-17 batch-73 = 0.0008924667490646243

Training epoch-17 batch-74
Running loss of epoch-17 batch-74 = 0.0011510298354551196

Training epoch-17 batch-75
Running loss of epoch-17 batch-75 = 0.00038898992352187634

Training epoch-17 batch-76
Running loss of epoch-17 batch-76 = 0.0008488817838951945

Training epoch-17 batch-77
Running loss of epoch-17 batch-77 = 0.0014727681409567595

Training epoch-17 batch-78
Running loss of epoch-17 batch-78 = 0.0008965678280219436

Training epoch-17 batch-79
Running loss of epoch-17 batch-79 = 9.895057883113623e-05

Training epoch-17 batch-80
Running loss of epoch-17 batch-80 = 0.0007035224698483944

Training epoch-17 batch-81
Running loss of epoch-17 batch-81 = 0.002220678608864546

Training epoch-17 batch-82
Running loss of epoch-17 batch-82 = 0.0015987369697540998

Training epoch-17 batch-83
Running loss of epoch-17 batch-83 = 0.0011972454376518726

Training epoch-17 batch-84
Running loss of epoch-17 batch-84 = 0.0006643107626587152

Training epoch-17 batch-85
Running loss of epoch-17 batch-85 = 0.000489412690512836

Training epoch-17 batch-86
Running loss of epoch-17 batch-86 = 0.0009425277239643037

Training epoch-17 batch-87
Running loss of epoch-17 batch-87 = 0.0015374593203887343

Training epoch-17 batch-88
Running loss of epoch-17 batch-88 = 0.0008201290620490909

Training epoch-17 batch-89
Running loss of epoch-17 batch-89 = 0.0007773531833663583

Training epoch-17 batch-90
Running loss of epoch-17 batch-90 = 0.00083501311019063

Training epoch-17 batch-91
Running loss of epoch-17 batch-91 = 0.0009457905543968081

Training epoch-17 batch-92
Running loss of epoch-17 batch-92 = 0.002078406745567918

Training epoch-17 batch-93
Running loss of epoch-17 batch-93 = 0.00274639087729156

Training epoch-17 batch-94
Running loss of epoch-17 batch-94 = 0.001092168502509594

Training epoch-17 batch-95
Running loss of epoch-17 batch-95 = 0.0010338334832340479

Training epoch-17 batch-96
Running loss of epoch-17 batch-96 = 0.0012337521184235811

Training epoch-17 batch-97
Running loss of epoch-17 batch-97 = 0.001162846339866519

Training epoch-17 batch-98
Running loss of epoch-17 batch-98 = 0.001168081653304398

Training epoch-17 batch-99
Running loss of epoch-17 batch-99 = 0.0007266952889040112

Training epoch-17 batch-100
Running loss of epoch-17 batch-100 = 0.0017788787372410297

Training epoch-17 batch-101
Running loss of epoch-17 batch-101 = 0.0009470224613323808

Training epoch-17 batch-102
Running loss of epoch-17 batch-102 = 0.0015903969760984182

Training epoch-17 batch-103
Running loss of epoch-17 batch-103 = 0.0024854615330696106

Training epoch-17 batch-104
Running loss of epoch-17 batch-104 = 0.0011301398044452071

Training epoch-17 batch-105
Running loss of epoch-17 batch-105 = 0.001958863576874137

Training epoch-17 batch-106
Running loss of epoch-17 batch-106 = 0.0008125667227432132

Training epoch-17 batch-107
Running loss of epoch-17 batch-107 = 0.0011815122561529279

Training epoch-17 batch-108
Running loss of epoch-17 batch-108 = 0.0035540114622563124

Training epoch-17 batch-109
Running loss of epoch-17 batch-109 = 0.00204869476146996

Training epoch-17 batch-110
Running loss of epoch-17 batch-110 = 0.0014219953445717692

Training epoch-17 batch-111
Running loss of epoch-17 batch-111 = 0.002283556153997779

Training epoch-17 batch-112
Running loss of epoch-17 batch-112 = 0.0012868742924183607

Training epoch-17 batch-113
Running loss of epoch-17 batch-113 = 0.001920616370625794

Training epoch-17 batch-114
Running loss of epoch-17 batch-114 = 0.004148881416767836

Training epoch-17 batch-115
Running loss of epoch-17 batch-115 = 0.0014957594685256481

Training epoch-17 batch-116
Running loss of epoch-17 batch-116 = 0.0011101197451353073

Training epoch-17 batch-117
Running loss of epoch-17 batch-117 = 0.0027705642860382795

Training epoch-17 batch-118
Running loss of epoch-17 batch-118 = 0.0012112762778997421

Training epoch-17 batch-119
Running loss of epoch-17 batch-119 = 0.0014806221006438136

Training epoch-17 batch-120
Running loss of epoch-17 batch-120 = 0.001378472545184195

Training epoch-17 batch-121
Running loss of epoch-17 batch-121 = 0.0017637433484196663

Training epoch-17 batch-122
Running loss of epoch-17 batch-122 = 0.0004396033473312855

Training epoch-17 batch-123
Running loss of epoch-17 batch-123 = 0.0003081350587308407

Training epoch-17 batch-124
Running loss of epoch-17 batch-124 = 0.0020481348037719727

Training epoch-17 batch-125
Running loss of epoch-17 batch-125 = 0.003418958280235529

Training epoch-17 batch-126
Running loss of epoch-17 batch-126 = 0.0009035957045853138

Training epoch-17 batch-127
Running loss of epoch-17 batch-127 = 0.0006727564614266157

Training epoch-17 batch-128
Running loss of epoch-17 batch-128 = 0.0008708469104021788

Training epoch-17 batch-129
Running loss of epoch-17 batch-129 = 0.0016667121089994907

Training epoch-17 batch-130
Running loss of epoch-17 batch-130 = 0.0031838298309594393

Training epoch-17 batch-131
Running loss of epoch-17 batch-131 = 0.001870425185188651

Training epoch-17 batch-132
Running loss of epoch-17 batch-132 = 0.0006569233955815434

Training epoch-17 batch-133
Running loss of epoch-17 batch-133 = 0.0011553398799151182

Training epoch-17 batch-134
Running loss of epoch-17 batch-134 = 0.0031990003772079945

Training epoch-17 batch-135
Running loss of epoch-17 batch-135 = 0.0014704210916534066

Training epoch-17 batch-136
Running loss of epoch-17 batch-136 = 0.0007846722146496177

Training epoch-17 batch-137
Running loss of epoch-17 batch-137 = 0.0009185842936858535

Training epoch-17 batch-138
Running loss of epoch-17 batch-138 = 0.0007022907957434654

Training epoch-17 batch-139
Running loss of epoch-17 batch-139 = 0.0025524829979985952

Training epoch-17 batch-140
Running loss of epoch-17 batch-140 = 0.0027246782556176186

Training epoch-17 batch-141
Running loss of epoch-17 batch-141 = 0.0019515460589900613

Training epoch-17 batch-142
Running loss of epoch-17 batch-142 = 0.002260637003928423

Training epoch-17 batch-143
Running loss of epoch-17 batch-143 = 0.0009241969091817737

Training epoch-17 batch-144
Running loss of epoch-17 batch-144 = 0.0009086489444598556

Training epoch-17 batch-145
Running loss of epoch-17 batch-145 = 0.003206854220479727

Training epoch-17 batch-146
Running loss of epoch-17 batch-146 = 0.0015173250576481223

Training epoch-17 batch-147
Running loss of epoch-17 batch-147 = 0.0014431166928261518

Training epoch-17 batch-148
Running loss of epoch-17 batch-148 = 0.0015357373049482703

Training epoch-17 batch-149
Running loss of epoch-17 batch-149 = 0.001914951833896339

Training epoch-17 batch-150
Running loss of epoch-17 batch-150 = 0.0008401001105085015

Training epoch-17 batch-151
Running loss of epoch-17 batch-151 = 0.0020660676527768373

Training epoch-17 batch-152
Running loss of epoch-17 batch-152 = 0.0009878863347694278

Training epoch-17 batch-153
Running loss of epoch-17 batch-153 = 0.0017247224459424615

Training epoch-17 batch-154
Running loss of epoch-17 batch-154 = 0.002675512805581093

Training epoch-17 batch-155
Running loss of epoch-17 batch-155 = 0.0029297496657818556

Training epoch-17 batch-156
Running loss of epoch-17 batch-156 = 0.0022632889449596405

Training epoch-17 batch-157
Running loss of epoch-17 batch-157 = 0.002362646162509918

Finished training epoch-17.



Average train loss at epoch-17 = 0.0013602133724838496

Started Evaluation

Average val loss at epoch-17 = 0.9578686414453458

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 92.30 %
Accuracy for class setUp is: 84.26 %
Accuracy for class onCreate is: 86.46 %
Accuracy for class toString is: 84.64 %
Accuracy for class run is: 76.26 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 38.55 %
Accuracy for class get is: 54.87 %

Overall Accuracy = 79.39 %

Finished Evaluation



Started training epoch-18


Training epoch-18 batch-1
Running loss of epoch-18 batch-1 = 0.00242844270542264

Training epoch-18 batch-2
Running loss of epoch-18 batch-2 = 0.002053923439234495

Training epoch-18 batch-3
Running loss of epoch-18 batch-3 = 0.0023052357137203217

Training epoch-18 batch-4
Running loss of epoch-18 batch-4 = 0.0009834844386205077

Training epoch-18 batch-5
Running loss of epoch-18 batch-5 = 0.0014526692684739828

Training epoch-18 batch-6
Running loss of epoch-18 batch-6 = 0.0012853924417868257

Training epoch-18 batch-7
Running loss of epoch-18 batch-7 = 0.0006073832046240568

Training epoch-18 batch-8
Running loss of epoch-18 batch-8 = 0.001910966937430203

Training epoch-18 batch-9
Running loss of epoch-18 batch-9 = 0.0007404313655570149

Training epoch-18 batch-10
Running loss of epoch-18 batch-10 = 0.0004393687704578042

Training epoch-18 batch-11
Running loss of epoch-18 batch-11 = 0.0006307148141786456

Training epoch-18 batch-12
Running loss of epoch-18 batch-12 = 0.0006141852354630828

Training epoch-18 batch-13
Running loss of epoch-18 batch-13 = 0.0011143600568175316

Training epoch-18 batch-14
Running loss of epoch-18 batch-14 = 0.0007534191245213151

Training epoch-18 batch-15
Running loss of epoch-18 batch-15 = 0.0006991960108280182

Training epoch-18 batch-16
Running loss of epoch-18 batch-16 = 0.0006873062229715288

Training epoch-18 batch-17
Running loss of epoch-18 batch-17 = 0.0013715901877731085

Training epoch-18 batch-18
Running loss of epoch-18 batch-18 = 0.0004986292915418744

Training epoch-18 batch-19
Running loss of epoch-18 batch-19 = 0.0006312216864898801

Training epoch-18 batch-20
Running loss of epoch-18 batch-20 = 0.0013809112133458257

Training epoch-18 batch-21
Running loss of epoch-18 batch-21 = 0.0018834559014067054

Training epoch-18 batch-22
Running loss of epoch-18 batch-22 = 0.0009528577793389559

Training epoch-18 batch-23
Running loss of epoch-18 batch-23 = 0.001250500208698213

Training epoch-18 batch-24
Running loss of epoch-18 batch-24 = 0.0011890954338014126

Training epoch-18 batch-25
Running loss of epoch-18 batch-25 = 0.0019006188958883286

Training epoch-18 batch-26
Running loss of epoch-18 batch-26 = 0.0012517289724200964

Training epoch-18 batch-27
Running loss of epoch-18 batch-27 = 0.001151213189586997

Training epoch-18 batch-28
Running loss of epoch-18 batch-28 = 0.0018786992877721786

Training epoch-18 batch-29
Running loss of epoch-18 batch-29 = 0.0007667611353099346

Training epoch-18 batch-30
Running loss of epoch-18 batch-30 = 0.00022156001068651676

Training epoch-18 batch-31
Running loss of epoch-18 batch-31 = 0.0006050037918612361

Training epoch-18 batch-32
Running loss of epoch-18 batch-32 = 0.0008987578330561519

Training epoch-18 batch-33
Running loss of epoch-18 batch-33 = 0.0006194836460053921

Training epoch-18 batch-34
Running loss of epoch-18 batch-34 = 0.0018038398120552301

Training epoch-18 batch-35
Running loss of epoch-18 batch-35 = 0.001537255709990859

Training epoch-18 batch-36
Running loss of epoch-18 batch-36 = 0.001088913413695991

Training epoch-18 batch-37
Running loss of epoch-18 batch-37 = 0.0010582589311525226

Training epoch-18 batch-38
Running loss of epoch-18 batch-38 = 0.0013647651067003608

Training epoch-18 batch-39
Running loss of epoch-18 batch-39 = 0.001542561687529087

Training epoch-18 batch-40
Running loss of epoch-18 batch-40 = 0.0015741087263450027

Training epoch-18 batch-41
Running loss of epoch-18 batch-41 = 0.0008585446048527956

Training epoch-18 batch-42
Running loss of epoch-18 batch-42 = 0.0004570889286696911

Training epoch-18 batch-43
Running loss of epoch-18 batch-43 = 0.001219188212417066

Training epoch-18 batch-44
Running loss of epoch-18 batch-44 = 0.00038550596218556166

Training epoch-18 batch-45
Running loss of epoch-18 batch-45 = 0.00044183689169585705

Training epoch-18 batch-46
Running loss of epoch-18 batch-46 = 0.0008756951428949833

Training epoch-18 batch-47
Running loss of epoch-18 batch-47 = 0.0006590065313503146

Training epoch-18 batch-48
Running loss of epoch-18 batch-48 = 0.00014115776866674423

Training epoch-18 batch-49
Running loss of epoch-18 batch-49 = 0.0007589989108964801

Training epoch-18 batch-50
Running loss of epoch-18 batch-50 = 0.001606106641702354

Training epoch-18 batch-51
Running loss of epoch-18 batch-51 = 0.0004627685993909836

Training epoch-18 batch-52
Running loss of epoch-18 batch-52 = 0.000685908948071301

Training epoch-18 batch-53
Running loss of epoch-18 batch-53 = 0.0021681231446564198

Training epoch-18 batch-54
Running loss of epoch-18 batch-54 = 0.002194169210270047

Training epoch-18 batch-55
Running loss of epoch-18 batch-55 = 0.0012905288022011518

Training epoch-18 batch-56
Running loss of epoch-18 batch-56 = 0.0005368472775444388

Training epoch-18 batch-57
Running loss of epoch-18 batch-57 = 0.0012760545359924436

Training epoch-18 batch-58
Running loss of epoch-18 batch-58 = 0.0005653866101056337

Training epoch-18 batch-59
Running loss of epoch-18 batch-59 = 0.0007933821761980653

Training epoch-18 batch-60
Running loss of epoch-18 batch-60 = 0.0013511119177564979

Training epoch-18 batch-61
Running loss of epoch-18 batch-61 = 0.0009602392092347145

Training epoch-18 batch-62
Running loss of epoch-18 batch-62 = 0.0008778050541877747

Training epoch-18 batch-63
Running loss of epoch-18 batch-63 = 0.00021535682026296854

Training epoch-18 batch-64
Running loss of epoch-18 batch-64 = 0.001896724454127252

Training epoch-18 batch-65
Running loss of epoch-18 batch-65 = 0.0005456384969875216

Training epoch-18 batch-66
Running loss of epoch-18 batch-66 = 0.0011556677054613829

Training epoch-18 batch-67
Running loss of epoch-18 batch-67 = 0.0011165833566337824

Training epoch-18 batch-68
Running loss of epoch-18 batch-68 = 0.0016443138010799885

Training epoch-18 batch-69
Running loss of epoch-18 batch-69 = 0.0007010923000052571

Training epoch-18 batch-70
Running loss of epoch-18 batch-70 = 0.0005340843927115202

Training epoch-18 batch-71
Running loss of epoch-18 batch-71 = 0.0013762104790657759

Training epoch-18 batch-72
Running loss of epoch-18 batch-72 = 0.003134869271889329

Training epoch-18 batch-73
Running loss of epoch-18 batch-73 = 0.001104984083212912

Training epoch-18 batch-74
Running loss of epoch-18 batch-74 = 0.0004382057813927531

Training epoch-18 batch-75
Running loss of epoch-18 batch-75 = 0.0015465538017451763

Training epoch-18 batch-76
Running loss of epoch-18 batch-76 = 0.0012419435661286116

Training epoch-18 batch-77
Running loss of epoch-18 batch-77 = 0.0005847451975569129

Training epoch-18 batch-78
Running loss of epoch-18 batch-78 = 0.0006088453810662031

Training epoch-18 batch-79
Running loss of epoch-18 batch-79 = 0.0011630310909822583

Training epoch-18 batch-80
Running loss of epoch-18 batch-80 = 0.0014308658428490162

Training epoch-18 batch-81
Running loss of epoch-18 batch-81 = 0.0008367488626390696

Training epoch-18 batch-82
Running loss of epoch-18 batch-82 = 0.0005796037148684263

Training epoch-18 batch-83
Running loss of epoch-18 batch-83 = 0.0011304786894470453

Training epoch-18 batch-84
Running loss of epoch-18 batch-84 = 0.0015521456953138113

Training epoch-18 batch-85
Running loss of epoch-18 batch-85 = 0.002249513752758503

Training epoch-18 batch-86
Running loss of epoch-18 batch-86 = 0.00036179996095597744

Training epoch-18 batch-87
Running loss of epoch-18 batch-87 = 0.00023857841733843088

Training epoch-18 batch-88
Running loss of epoch-18 batch-88 = 0.0015125400386750698

Training epoch-18 batch-89
Running loss of epoch-18 batch-89 = 0.000683850608766079

Training epoch-18 batch-90
Running loss of epoch-18 batch-90 = 0.0005088866455480456

Training epoch-18 batch-91
Running loss of epoch-18 batch-91 = 0.00021886348258703947

Training epoch-18 batch-92
Running loss of epoch-18 batch-92 = 0.001515010604634881

Training epoch-18 batch-93
Running loss of epoch-18 batch-93 = 0.0018701065564528108

Training epoch-18 batch-94
Running loss of epoch-18 batch-94 = 0.0003961593611165881

Training epoch-18 batch-95
Running loss of epoch-18 batch-95 = 0.0009344443678855896

Training epoch-18 batch-96
Running loss of epoch-18 batch-96 = 0.0004658463876694441

Training epoch-18 batch-97
Running loss of epoch-18 batch-97 = 0.002280284184962511

Training epoch-18 batch-98
Running loss of epoch-18 batch-98 = 0.0007643613498657942

Training epoch-18 batch-99
Running loss of epoch-18 batch-99 = 0.0005229788366705179

Training epoch-18 batch-100
Running loss of epoch-18 batch-100 = 0.0013229893520474434

Training epoch-18 batch-101
Running loss of epoch-18 batch-101 = 0.0006452415836974978

Training epoch-18 batch-102
Running loss of epoch-18 batch-102 = 0.0001976554049178958

Training epoch-18 batch-103
Running loss of epoch-18 batch-103 = 0.0010873667197301984

Training epoch-18 batch-104
Running loss of epoch-18 batch-104 = 0.0023064599372446537

Training epoch-18 batch-105
Running loss of epoch-18 batch-105 = 0.0007146893767639995

Training epoch-18 batch-106
Running loss of epoch-18 batch-106 = 0.0005466453731060028

Training epoch-18 batch-107
Running loss of epoch-18 batch-107 = 0.001098879729397595

Training epoch-18 batch-108
Running loss of epoch-18 batch-108 = 0.0017191999359056354

Training epoch-18 batch-109
Running loss of epoch-18 batch-109 = 0.0006249444559216499

Training epoch-18 batch-110
Running loss of epoch-18 batch-110 = 0.001715010148473084

Training epoch-18 batch-111
Running loss of epoch-18 batch-111 = 0.0015445504104718566

Training epoch-18 batch-112
Running loss of epoch-18 batch-112 = 0.0011323826620355248

Training epoch-18 batch-113
Running loss of epoch-18 batch-113 = 0.001049594720825553

Training epoch-18 batch-114
Running loss of epoch-18 batch-114 = 0.0012186921667307615

Training epoch-18 batch-115
Running loss of epoch-18 batch-115 = 0.0024396656081080437

Training epoch-18 batch-116
Running loss of epoch-18 batch-116 = 0.0009508212096989155

Training epoch-18 batch-117
Running loss of epoch-18 batch-117 = 0.001598236383870244

Training epoch-18 batch-118
Running loss of epoch-18 batch-118 = 0.0008184830658137798

Training epoch-18 batch-119
Running loss of epoch-18 batch-119 = 0.0006610141135752201

Training epoch-18 batch-120
Running loss of epoch-18 batch-120 = 0.00029423984233289957

Training epoch-18 batch-121
Running loss of epoch-18 batch-121 = 0.0009401546558365226

Training epoch-18 batch-122
Running loss of epoch-18 batch-122 = 0.000903064850717783

Training epoch-18 batch-123
Running loss of epoch-18 batch-123 = 0.0007780654123052955

Training epoch-18 batch-124
Running loss of epoch-18 batch-124 = 0.0019927641842514277

Training epoch-18 batch-125
Running loss of epoch-18 batch-125 = 0.0010827587684616446

Training epoch-18 batch-126
Running loss of epoch-18 batch-126 = 0.001489481539465487

Training epoch-18 batch-127
Running loss of epoch-18 batch-127 = 0.0011775391176342964

Training epoch-18 batch-128
Running loss of epoch-18 batch-128 = 0.0022488164249807596

Training epoch-18 batch-129
Running loss of epoch-18 batch-129 = 0.0005832443712279201

Training epoch-18 batch-130
Running loss of epoch-18 batch-130 = 0.002068052301183343

Training epoch-18 batch-131
Running loss of epoch-18 batch-131 = 0.0013512399746105075

Training epoch-18 batch-132
Running loss of epoch-18 batch-132 = 0.0006837168475612998

Training epoch-18 batch-133
Running loss of epoch-18 batch-133 = 0.0017951291520148516

Training epoch-18 batch-134
Running loss of epoch-18 batch-134 = 0.000983107485808432

Training epoch-18 batch-135
Running loss of epoch-18 batch-135 = 0.001102740061469376

Training epoch-18 batch-136
Running loss of epoch-18 batch-136 = 0.0007766454946249723

Training epoch-18 batch-137
Running loss of epoch-18 batch-137 = 0.0005622467724606395

Training epoch-18 batch-138
Running loss of epoch-18 batch-138 = 0.0015030857175588608

Training epoch-18 batch-139
Running loss of epoch-18 batch-139 = 0.000525819486938417

Training epoch-18 batch-140
Running loss of epoch-18 batch-140 = 0.00152427121065557

Training epoch-18 batch-141
Running loss of epoch-18 batch-141 = 0.0010310770012438297

Training epoch-18 batch-142
Running loss of epoch-18 batch-142 = 0.0010190977482125163

Training epoch-18 batch-143
Running loss of epoch-18 batch-143 = 0.003182773245498538

Training epoch-18 batch-144
Running loss of epoch-18 batch-144 = 0.000857566948980093

Training epoch-18 batch-145
Running loss of epoch-18 batch-145 = 0.0007912622531875968

Training epoch-18 batch-146
Running loss of epoch-18 batch-146 = 0.0006487466162070632

Training epoch-18 batch-147
Running loss of epoch-18 batch-147 = 0.0014741659397259355

Training epoch-18 batch-148
Running loss of epoch-18 batch-148 = 0.0006680280202999711

Training epoch-18 batch-149
Running loss of epoch-18 batch-149 = 0.0016738901613280177

Training epoch-18 batch-150
Running loss of epoch-18 batch-150 = 0.0017607634654268622

Training epoch-18 batch-151
Running loss of epoch-18 batch-151 = 0.00044737360440194607

Training epoch-18 batch-152
Running loss of epoch-18 batch-152 = 0.0005633780965581536

Training epoch-18 batch-153
Running loss of epoch-18 batch-153 = 0.0014488778542727232

Training epoch-18 batch-154
Running loss of epoch-18 batch-154 = 0.001142899738624692

Training epoch-18 batch-155
Running loss of epoch-18 batch-155 = 0.0016564989928156137

Training epoch-18 batch-156
Running loss of epoch-18 batch-156 = 0.001970030600205064

Training epoch-18 batch-157
Running loss of epoch-18 batch-157 = 0.0015860199928283691

Finished training epoch-18.



Average train loss at epoch-18 = 0.0011215826261788606

Started Evaluation

Average val loss at epoch-18 = 0.9398155470388654

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 95.25 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 92.11 %
Accuracy for class toString is: 84.98 %
Accuracy for class run is: 70.78 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 44.39 %
Accuracy for class execute is: 48.59 %
Accuracy for class get is: 58.72 %

Overall Accuracy = 81.49 %

Finished Evaluation



Started training epoch-19


Training epoch-19 batch-1
Running loss of epoch-19 batch-1 = 0.0010563523974269629

Training epoch-19 batch-2
Running loss of epoch-19 batch-2 = 0.0005854551563970745

Training epoch-19 batch-3
Running loss of epoch-19 batch-3 = 0.0022857121657580137

Training epoch-19 batch-4
Running loss of epoch-19 batch-4 = 0.0024935551919043064

Training epoch-19 batch-5
Running loss of epoch-19 batch-5 = 0.0012649360578507185

Training epoch-19 batch-6
Running loss of epoch-19 batch-6 = 0.0013277503894641995

Training epoch-19 batch-7
Running loss of epoch-19 batch-7 = 0.00165813066996634

Training epoch-19 batch-8
Running loss of epoch-19 batch-8 = 0.0009903231402859092

Training epoch-19 batch-9
Running loss of epoch-19 batch-9 = 0.00043492414988577366

Training epoch-19 batch-10
Running loss of epoch-19 batch-10 = 0.0012092733522877097

Training epoch-19 batch-11
Running loss of epoch-19 batch-11 = 0.0012870911741629243

Training epoch-19 batch-12
Running loss of epoch-19 batch-12 = 0.0008353268494829535

Training epoch-19 batch-13
Running loss of epoch-19 batch-13 = 0.0006689347792416811

Training epoch-19 batch-14
Running loss of epoch-19 batch-14 = 0.0015380322001874447

Training epoch-19 batch-15
Running loss of epoch-19 batch-15 = 0.002866347786039114

Training epoch-19 batch-16
Running loss of epoch-19 batch-16 = 0.0020591828506439924

Training epoch-19 batch-17
Running loss of epoch-19 batch-17 = 0.0004574364284053445

Training epoch-19 batch-18
Running loss of epoch-19 batch-18 = 0.000941159320063889

Training epoch-19 batch-19
Running loss of epoch-19 batch-19 = 0.0005808786954730749

Training epoch-19 batch-20
Running loss of epoch-19 batch-20 = 0.0012282239040359855

Training epoch-19 batch-21
Running loss of epoch-19 batch-21 = 0.0007266753818839788

Training epoch-19 batch-22
Running loss of epoch-19 batch-22 = 0.0025180624797940254

Training epoch-19 batch-23
Running loss of epoch-19 batch-23 = 0.0005151277873665094

Training epoch-19 batch-24
Running loss of epoch-19 batch-24 = 0.0009870618814602494

Training epoch-19 batch-25
Running loss of epoch-19 batch-25 = 0.001959984190762043

Training epoch-19 batch-26
Running loss of epoch-19 batch-26 = 0.0006784854922443628

Training epoch-19 batch-27
Running loss of epoch-19 batch-27 = 0.0007792680989950895

Training epoch-19 batch-28
Running loss of epoch-19 batch-28 = 0.0005789123242720962

Training epoch-19 batch-29
Running loss of epoch-19 batch-29 = 0.0013616910437121987

Training epoch-19 batch-30
Running loss of epoch-19 batch-30 = 0.001017408911138773

Training epoch-19 batch-31
Running loss of epoch-19 batch-31 = 0.0011230793315917253

Training epoch-19 batch-32
Running loss of epoch-19 batch-32 = 0.0008177080890163779

Training epoch-19 batch-33
Running loss of epoch-19 batch-33 = 0.00062888755928725

Training epoch-19 batch-34
Running loss of epoch-19 batch-34 = 0.000515283434651792

Training epoch-19 batch-35
Running loss of epoch-19 batch-35 = 0.0013385388301685452

Training epoch-19 batch-36
Running loss of epoch-19 batch-36 = 0.0004053156590089202

Training epoch-19 batch-37
Running loss of epoch-19 batch-37 = 0.0018023635493591428

Training epoch-19 batch-38
Running loss of epoch-19 batch-38 = 0.00149923178832978

Training epoch-19 batch-39
Running loss of epoch-19 batch-39 = 0.0015872020740061998

Training epoch-19 batch-40
Running loss of epoch-19 batch-40 = 0.0010880406480282545

Training epoch-19 batch-41
Running loss of epoch-19 batch-41 = 0.00036352884490042925

Training epoch-19 batch-42
Running loss of epoch-19 batch-42 = 0.0017488719895482063

Training epoch-19 batch-43
Running loss of epoch-19 batch-43 = 0.001063040690496564

Training epoch-19 batch-44
Running loss of epoch-19 batch-44 = 0.0002525398740544915

Training epoch-19 batch-45
Running loss of epoch-19 batch-45 = 0.0015135331777855754

Training epoch-19 batch-46
Running loss of epoch-19 batch-46 = 0.0008843281539157033

Training epoch-19 batch-47
Running loss of epoch-19 batch-47 = 0.0009877533884719014

Training epoch-19 batch-48
Running loss of epoch-19 batch-48 = 0.001073142746463418

Training epoch-19 batch-49
Running loss of epoch-19 batch-49 = 0.001170314964838326

Training epoch-19 batch-50
Running loss of epoch-19 batch-50 = 0.0014908433659002185

Training epoch-19 batch-51
Running loss of epoch-19 batch-51 = 0.0005448140436783433

Training epoch-19 batch-52
Running loss of epoch-19 batch-52 = 0.00044985173735767603

Training epoch-19 batch-53
Running loss of epoch-19 batch-53 = 0.0014301131013780832

Training epoch-19 batch-54
Running loss of epoch-19 batch-54 = 0.0006118849851191044

Training epoch-19 batch-55
Running loss of epoch-19 batch-55 = 0.0011747234966605902

Training epoch-19 batch-56
Running loss of epoch-19 batch-56 = 0.0014768618857488036

Training epoch-19 batch-57
Running loss of epoch-19 batch-57 = 0.0016493252478539944

Training epoch-19 batch-58
Running loss of epoch-19 batch-58 = 0.000729938386939466

Training epoch-19 batch-59
Running loss of epoch-19 batch-59 = 0.0014254761626943946

Training epoch-19 batch-60
Running loss of epoch-19 batch-60 = 0.0010316730476915836

Training epoch-19 batch-61
Running loss of epoch-19 batch-61 = 0.0009674415923655033

Training epoch-19 batch-62
Running loss of epoch-19 batch-62 = 0.0019387316424399614

Training epoch-19 batch-63
Running loss of epoch-19 batch-63 = 0.0007772194221615791

Training epoch-19 batch-64
Running loss of epoch-19 batch-64 = 0.0004487473051995039

Training epoch-19 batch-65
Running loss of epoch-19 batch-65 = 0.001285803271457553

Training epoch-19 batch-66
Running loss of epoch-19 batch-66 = 0.0010600994573906064

Training epoch-19 batch-67
Running loss of epoch-19 batch-67 = 0.0023666450288146734

Training epoch-19 batch-68
Running loss of epoch-19 batch-68 = 0.00046000629663467407

Training epoch-19 batch-69
Running loss of epoch-19 batch-69 = 0.0007472273427993059

Training epoch-19 batch-70
Running loss of epoch-19 batch-70 = 0.001647741999477148

Training epoch-19 batch-71
Running loss of epoch-19 batch-71 = 0.0007671585772186518

Training epoch-19 batch-72
Running loss of epoch-19 batch-72 = 0.0003390287747606635

Training epoch-19 batch-73
Running loss of epoch-19 batch-73 = 0.0009571986738592386

Training epoch-19 batch-74
Running loss of epoch-19 batch-74 = 0.0016205694992095232

Training epoch-19 batch-75
Running loss of epoch-19 batch-75 = 0.0012991840485483408

Training epoch-19 batch-76
Running loss of epoch-19 batch-76 = 0.0008502145064994693

Training epoch-19 batch-77
Running loss of epoch-19 batch-77 = 0.0010141574312001467

Training epoch-19 batch-78
Running loss of epoch-19 batch-78 = 0.0006038490682840347

Training epoch-19 batch-79
Running loss of epoch-19 batch-79 = 0.00017793511506170034

Training epoch-19 batch-80
Running loss of epoch-19 batch-80 = 0.0005947024328634143

Training epoch-19 batch-81
Running loss of epoch-19 batch-81 = 0.00047577801160514355

Training epoch-19 batch-82
Running loss of epoch-19 batch-82 = 0.0012607838725671172

Training epoch-19 batch-83
Running loss of epoch-19 batch-83 = 0.0012087186332792044

Training epoch-19 batch-84
Running loss of epoch-19 batch-84 = 0.0006635644240304828

Training epoch-19 batch-85
Running loss of epoch-19 batch-85 = 0.00134764623362571

Training epoch-19 batch-86
Running loss of epoch-19 batch-86 = 0.0007030639098957181

Training epoch-19 batch-87
Running loss of epoch-19 batch-87 = 0.0012987337540835142

Training epoch-19 batch-88
Running loss of epoch-19 batch-88 = 0.0006786747835576534

Training epoch-19 batch-89
Running loss of epoch-19 batch-89 = 0.0007760908920317888

Training epoch-19 batch-90
Running loss of epoch-19 batch-90 = 0.00039056933019310236

Training epoch-19 batch-91
Running loss of epoch-19 batch-91 = 0.002225192729383707

Training epoch-19 batch-92
Running loss of epoch-19 batch-92 = 0.00044335878919810057

Training epoch-19 batch-93
Running loss of epoch-19 batch-93 = 0.0003602555952966213

Training epoch-19 batch-94
Running loss of epoch-19 batch-94 = 0.0006633243756368756

Training epoch-19 batch-95
Running loss of epoch-19 batch-95 = 0.0007904157973825932

Training epoch-19 batch-96
Running loss of epoch-19 batch-96 = 0.0019982398953288794

Training epoch-19 batch-97
Running loss of epoch-19 batch-97 = 0.0010899052722379565

Training epoch-19 batch-98
Running loss of epoch-19 batch-98 = 0.0009618049953132868

Training epoch-19 batch-99
Running loss of epoch-19 batch-99 = 0.0005005198763683438

Training epoch-19 batch-100
Running loss of epoch-19 batch-100 = 0.0017884670523926616

Training epoch-19 batch-101
Running loss of epoch-19 batch-101 = 0.0011749635450541973

Training epoch-19 batch-102
Running loss of epoch-19 batch-102 = 0.0007640409749001265

Training epoch-19 batch-103
Running loss of epoch-19 batch-103 = 0.0005286304512992501

Training epoch-19 batch-104
Running loss of epoch-19 batch-104 = 0.0010542267700657248

Training epoch-19 batch-105
Running loss of epoch-19 batch-105 = 0.0011284552747383714

Training epoch-19 batch-106
Running loss of epoch-19 batch-106 = 0.000601745443418622

Training epoch-19 batch-107
Running loss of epoch-19 batch-107 = 0.0007991316961124539

Training epoch-19 batch-108
Running loss of epoch-19 batch-108 = 0.0012590148253366351

Training epoch-19 batch-109
Running loss of epoch-19 batch-109 = 0.0005400775698944926

Training epoch-19 batch-110
Running loss of epoch-19 batch-110 = 0.0004132643807679415

Training epoch-19 batch-111
Running loss of epoch-19 batch-111 = 0.0007268855115398765

Training epoch-19 batch-112
Running loss of epoch-19 batch-112 = 0.0011876542121171951

Training epoch-19 batch-113
Running loss of epoch-19 batch-113 = 0.0006633421871811152

Training epoch-19 batch-114
Running loss of epoch-19 batch-114 = 0.0004269996425136924

Training epoch-19 batch-115
Running loss of epoch-19 batch-115 = 0.0006469897925853729

Training epoch-19 batch-116
Running loss of epoch-19 batch-116 = 0.0007706934120506048

Training epoch-19 batch-117
Running loss of epoch-19 batch-117 = 0.0008099476108327508

Training epoch-19 batch-118
Running loss of epoch-19 batch-118 = 0.00066514709033072

Training epoch-19 batch-119
Running loss of epoch-19 batch-119 = 0.00015693786554038525

Training epoch-19 batch-120
Running loss of epoch-19 batch-120 = 0.00022167491260915995

Training epoch-19 batch-121
Running loss of epoch-19 batch-121 = 0.0005356424953788519

Training epoch-19 batch-122
Running loss of epoch-19 batch-122 = 0.0013348667416721582

Training epoch-19 batch-123
Running loss of epoch-19 batch-123 = 0.00042457960080355406

Training epoch-19 batch-124
Running loss of epoch-19 batch-124 = 0.0006179006304591894

Training epoch-19 batch-125
Running loss of epoch-19 batch-125 = 0.0005768343107774854

Training epoch-19 batch-126
Running loss of epoch-19 batch-126 = 0.0002008866285905242

Training epoch-19 batch-127
Running loss of epoch-19 batch-127 = 0.001335293985903263

Training epoch-19 batch-128
Running loss of epoch-19 batch-128 = 0.0010394323617219925

Training epoch-19 batch-129
Running loss of epoch-19 batch-129 = 0.0007862296770326793

Training epoch-19 batch-130
Running loss of epoch-19 batch-130 = 0.0005633134860545397

Training epoch-19 batch-131
Running loss of epoch-19 batch-131 = 0.0003045832272619009

Training epoch-19 batch-132
Running loss of epoch-19 batch-132 = 0.0006751096807420254

Training epoch-19 batch-133
Running loss of epoch-19 batch-133 = 0.0015291123418137431

Training epoch-19 batch-134
Running loss of epoch-19 batch-134 = 0.00032964441925287247

Training epoch-19 batch-135
Running loss of epoch-19 batch-135 = 0.00042222323827445507

Training epoch-19 batch-136
Running loss of epoch-19 batch-136 = 0.0013192726764827967

Training epoch-19 batch-137
Running loss of epoch-19 batch-137 = 0.000727639882825315

Training epoch-19 batch-138
Running loss of epoch-19 batch-138 = 0.0001099796500056982

Training epoch-19 batch-139
Running loss of epoch-19 batch-139 = 0.0009964850032702088

Training epoch-19 batch-140
Running loss of epoch-19 batch-140 = 0.0015492684906348586

Training epoch-19 batch-141
Running loss of epoch-19 batch-141 = 0.0011037858203053474

Training epoch-19 batch-142
Running loss of epoch-19 batch-142 = 0.0021348081063479185

Training epoch-19 batch-143
Running loss of epoch-19 batch-143 = 0.001172168180346489

Training epoch-19 batch-144
Running loss of epoch-19 batch-144 = 0.0005893115885555744

Training epoch-19 batch-145
Running loss of epoch-19 batch-145 = 0.0002254302380606532

Training epoch-19 batch-146
Running loss of epoch-19 batch-146 = 0.000659656710922718

Training epoch-19 batch-147
Running loss of epoch-19 batch-147 = 0.0011541147250682116

Training epoch-19 batch-148
Running loss of epoch-19 batch-148 = 0.003214622614905238

Training epoch-19 batch-149
Running loss of epoch-19 batch-149 = 0.00041955721098929644

Training epoch-19 batch-150
Running loss of epoch-19 batch-150 = 0.00021708733402192593

Training epoch-19 batch-151
Running loss of epoch-19 batch-151 = 0.0005291752750054002

Training epoch-19 batch-152
Running loss of epoch-19 batch-152 = 0.0022051543928682804

Training epoch-19 batch-153
Running loss of epoch-19 batch-153 = 0.0004935218021273613

Training epoch-19 batch-154
Running loss of epoch-19 batch-154 = 0.0010439619654789567

Training epoch-19 batch-155
Running loss of epoch-19 batch-155 = 0.001561681623570621

Training epoch-19 batch-156
Running loss of epoch-19 batch-156 = 0.0008519059047102928

Training epoch-19 batch-157
Running loss of epoch-19 batch-157 = 0.0030553508549928665

Finished training epoch-19.



Average train loss at epoch-19 = 0.0010020901575684548

Started Evaluation

Average val loss at epoch-19 = 1.0611982160830131

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 95.25 %
Accuracy for class setUp is: 92.46 %
Accuracy for class onCreate is: 92.86 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 70.32 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 35.87 %
Accuracy for class execute is: 46.99 %
Accuracy for class get is: 54.36 %

Overall Accuracy = 80.75 %

Finished Evaluation



Started training epoch-20


Training epoch-20 batch-1
Running loss of epoch-20 batch-1 = 0.0004117413191124797

Training epoch-20 batch-2
Running loss of epoch-20 batch-2 = 0.001307915081270039

Training epoch-20 batch-3
Running loss of epoch-20 batch-3 = 0.0006831150967627764

Training epoch-20 batch-4
Running loss of epoch-20 batch-4 = 0.002701914170756936

Training epoch-20 batch-5
Running loss of epoch-20 batch-5 = 0.0017489661695435643

Training epoch-20 batch-6
Running loss of epoch-20 batch-6 = 0.0006107433000579476

Training epoch-20 batch-7
Running loss of epoch-20 batch-7 = 0.0004755266709253192

Training epoch-20 batch-8
Running loss of epoch-20 batch-8 = 0.0004317302955314517

Training epoch-20 batch-9
Running loss of epoch-20 batch-9 = 0.0016473287250846624

Training epoch-20 batch-10
Running loss of epoch-20 batch-10 = 0.0006236507324501872

Training epoch-20 batch-11
Running loss of epoch-20 batch-11 = 0.0007744297618046403

Training epoch-20 batch-12
Running loss of epoch-20 batch-12 = 0.00016759522259235382

Training epoch-20 batch-13
Running loss of epoch-20 batch-13 = 0.001715632388368249

Training epoch-20 batch-14
Running loss of epoch-20 batch-14 = 0.00025973981246352196

Training epoch-20 batch-15
Running loss of epoch-20 batch-15 = 0.0004140514647588134

Training epoch-20 batch-16
Running loss of epoch-20 batch-16 = 0.001004192978143692

Training epoch-20 batch-17
Running loss of epoch-20 batch-17 = 0.0009637799812480807

Training epoch-20 batch-18
Running loss of epoch-20 batch-18 = 0.0005608841311186552

Training epoch-20 batch-19
Running loss of epoch-20 batch-19 = 0.0006623957306146622

Training epoch-20 batch-20
Running loss of epoch-20 batch-20 = 0.0012433173833414912

Training epoch-20 batch-21
Running loss of epoch-20 batch-21 = 0.0004601757973432541

Training epoch-20 batch-22
Running loss of epoch-20 batch-22 = 0.000567117240279913

Training epoch-20 batch-23
Running loss of epoch-20 batch-23 = 0.0009133304702118039

Training epoch-20 batch-24
Running loss of epoch-20 batch-24 = 0.001060795970261097

Training epoch-20 batch-25
Running loss of epoch-20 batch-25 = 0.0003487097565084696

Training epoch-20 batch-26
Running loss of epoch-20 batch-26 = 0.0003064749762415886

Training epoch-20 batch-27
Running loss of epoch-20 batch-27 = 0.0002159962896257639

Training epoch-20 batch-28
Running loss of epoch-20 batch-28 = 0.0005330233834683895

Training epoch-20 batch-29
Running loss of epoch-20 batch-29 = 0.0016490156995132565

Training epoch-20 batch-30
Running loss of epoch-20 batch-30 = 0.0010766958585008979

Training epoch-20 batch-31
Running loss of epoch-20 batch-31 = 0.0011752878781408072

Training epoch-20 batch-32
Running loss of epoch-20 batch-32 = 0.000245335279032588

Training epoch-20 batch-33
Running loss of epoch-20 batch-33 = 0.000729147344827652

Training epoch-20 batch-34
Running loss of epoch-20 batch-34 = 0.0012110874522477388

Training epoch-20 batch-35
Running loss of epoch-20 batch-35 = 0.000571840675547719

Training epoch-20 batch-36
Running loss of epoch-20 batch-36 = 0.0006069452501833439

Training epoch-20 batch-37
Running loss of epoch-20 batch-37 = 0.000996309332549572

Training epoch-20 batch-38
Running loss of epoch-20 batch-38 = 0.0013901659986004233

Training epoch-20 batch-39
Running loss of epoch-20 batch-39 = 0.0013816689606755972

Training epoch-20 batch-40
Running loss of epoch-20 batch-40 = 0.0013703836593776941

Training epoch-20 batch-41
Running loss of epoch-20 batch-41 = 0.0012094706762582064

Training epoch-20 batch-42
Running loss of epoch-20 batch-42 = 0.0004303982714191079

Training epoch-20 batch-43
Running loss of epoch-20 batch-43 = 0.000361989950761199

Training epoch-20 batch-44
Running loss of epoch-20 batch-44 = 0.0004925911780446768

Training epoch-20 batch-45
Running loss of epoch-20 batch-45 = 0.0001099955989047885

Training epoch-20 batch-46
Running loss of epoch-20 batch-46 = 0.00019204476848244667

Training epoch-20 batch-47
Running loss of epoch-20 batch-47 = 0.0003872562665492296

Training epoch-20 batch-48
Running loss of epoch-20 batch-48 = 0.0008099726401269436

Training epoch-20 batch-49
Running loss of epoch-20 batch-49 = 0.0003888263599947095

Training epoch-20 batch-50
Running loss of epoch-20 batch-50 = 0.000662190024740994

Training epoch-20 batch-51
Running loss of epoch-20 batch-51 = 0.0003581632627174258

Training epoch-20 batch-52
Running loss of epoch-20 batch-52 = 0.0008365741232410073

Training epoch-20 batch-53
Running loss of epoch-20 batch-53 = 0.0002251294208690524

Training epoch-20 batch-54
Running loss of epoch-20 batch-54 = 0.00028687575832009315

Training epoch-20 batch-55
Running loss of epoch-20 batch-55 = 0.0004363661864772439

Training epoch-20 batch-56
Running loss of epoch-20 batch-56 = 0.00037420669104903936

Training epoch-20 batch-57
Running loss of epoch-20 batch-57 = 0.0008395871846005321

Training epoch-20 batch-58
Running loss of epoch-20 batch-58 = 0.00022541917860507965

Training epoch-20 batch-59
Running loss of epoch-20 batch-59 = 0.0008262343471869826

Training epoch-20 batch-60
Running loss of epoch-20 batch-60 = 0.0006463238969445229

Training epoch-20 batch-61
Running loss of epoch-20 batch-61 = 0.000692978035658598

Training epoch-20 batch-62
Running loss of epoch-20 batch-62 = 9.849236812442541e-05

Training epoch-20 batch-63
Running loss of epoch-20 batch-63 = 0.0012072097742930055

Training epoch-20 batch-64
Running loss of epoch-20 batch-64 = 0.0019122937228530645

Training epoch-20 batch-65
Running loss of epoch-20 batch-65 = 0.0006199990166351199

Training epoch-20 batch-66
Running loss of epoch-20 batch-66 = 0.00013809918891638517

Training epoch-20 batch-67
Running loss of epoch-20 batch-67 = 0.00030805193819105625

Training epoch-20 batch-68
Running loss of epoch-20 batch-68 = 0.0006281078094616532

Training epoch-20 batch-69
Running loss of epoch-20 batch-69 = 0.002063910709694028

Training epoch-20 batch-70
Running loss of epoch-20 batch-70 = 0.000599981751292944

Training epoch-20 batch-71
Running loss of epoch-20 batch-71 = 0.0007358069997280836

Training epoch-20 batch-72
Running loss of epoch-20 batch-72 = 0.0002097893739119172

Training epoch-20 batch-73
Running loss of epoch-20 batch-73 = 0.000875136349350214

Training epoch-20 batch-74
Running loss of epoch-20 batch-74 = 0.00021746114362031221

Training epoch-20 batch-75
Running loss of epoch-20 batch-75 = 0.0019644321873784065

Training epoch-20 batch-76
Running loss of epoch-20 batch-76 = 0.0017387553816661239

Training epoch-20 batch-77
Running loss of epoch-20 batch-77 = 0.0005230996757745743

Training epoch-20 batch-78
Running loss of epoch-20 batch-78 = 0.0005973103689029813

Training epoch-20 batch-79
Running loss of epoch-20 batch-79 = 0.0005279839970171452

Training epoch-20 batch-80
Running loss of epoch-20 batch-80 = 0.0005929998587816954

Training epoch-20 batch-81
Running loss of epoch-20 batch-81 = 0.000450019259005785

Training epoch-20 batch-82
Running loss of epoch-20 batch-82 = 0.0024165064096450806

Training epoch-20 batch-83
Running loss of epoch-20 batch-83 = 0.00026691926177591085

Training epoch-20 batch-84
Running loss of epoch-20 batch-84 = 0.0002678066957741976

Training epoch-20 batch-85
Running loss of epoch-20 batch-85 = 0.0010005647782236338

Training epoch-20 batch-86
Running loss of epoch-20 batch-86 = 0.0001466851681470871

Training epoch-20 batch-87
Running loss of epoch-20 batch-87 = 0.00041168148163706064

Training epoch-20 batch-88
Running loss of epoch-20 batch-88 = 0.0005397250643000007

Training epoch-20 batch-89
Running loss of epoch-20 batch-89 = 0.0008191304514184594

Training epoch-20 batch-90
Running loss of epoch-20 batch-90 = 0.0006375156808644533

Training epoch-20 batch-91
Running loss of epoch-20 batch-91 = 0.0010958370985463262

Training epoch-20 batch-92
Running loss of epoch-20 batch-92 = 0.0007619649404659867

Training epoch-20 batch-93
Running loss of epoch-20 batch-93 = 0.0006076049758121371

Training epoch-20 batch-94
Running loss of epoch-20 batch-94 = 0.0009445422329008579

Training epoch-20 batch-95
Running loss of epoch-20 batch-95 = 0.0004105401458218694

Training epoch-20 batch-96
Running loss of epoch-20 batch-96 = 0.000600184197537601

Training epoch-20 batch-97
Running loss of epoch-20 batch-97 = 0.0005058092065155506

Training epoch-20 batch-98
Running loss of epoch-20 batch-98 = 0.0015613808063790202

Training epoch-20 batch-99
Running loss of epoch-20 batch-99 = 0.00160885916557163

Training epoch-20 batch-100
Running loss of epoch-20 batch-100 = 0.0010331339435651898

Training epoch-20 batch-101
Running loss of epoch-20 batch-101 = 0.0007583452388644218

Training epoch-20 batch-102
Running loss of epoch-20 batch-102 = 0.00030498788692057133

Training epoch-20 batch-103
Running loss of epoch-20 batch-103 = 0.001896667992696166

Training epoch-20 batch-104
Running loss of epoch-20 batch-104 = 0.002106879372149706

Training epoch-20 batch-105
Running loss of epoch-20 batch-105 = 0.0005000202218070626

Training epoch-20 batch-106
Running loss of epoch-20 batch-106 = 0.0006365097360685468

Training epoch-20 batch-107
Running loss of epoch-20 batch-107 = 0.00033928826451301575

Training epoch-20 batch-108
Running loss of epoch-20 batch-108 = 0.0006782533600926399

Training epoch-20 batch-109
Running loss of epoch-20 batch-109 = 0.0017619015416130424

Training epoch-20 batch-110
Running loss of epoch-20 batch-110 = 0.0015820321859791875

Training epoch-20 batch-111
Running loss of epoch-20 batch-111 = 0.0003210505237802863

Training epoch-20 batch-112
Running loss of epoch-20 batch-112 = 0.001026284764520824

Training epoch-20 batch-113
Running loss of epoch-20 batch-113 = 0.0003276560455560684

Training epoch-20 batch-114
Running loss of epoch-20 batch-114 = 0.000751861312892288

Training epoch-20 batch-115
Running loss of epoch-20 batch-115 = 0.0010956398909911513

Training epoch-20 batch-116
Running loss of epoch-20 batch-116 = 0.00022254197392612696

Training epoch-20 batch-117
Running loss of epoch-20 batch-117 = 0.00023459794465452433

Training epoch-20 batch-118
Running loss of epoch-20 batch-118 = 0.0012741810642182827

Training epoch-20 batch-119
Running loss of epoch-20 batch-119 = 0.0011741521302610636

Training epoch-20 batch-120
Running loss of epoch-20 batch-120 = 0.0006596093298867345

Training epoch-20 batch-121
Running loss of epoch-20 batch-121 = 0.0007224449655041099

Training epoch-20 batch-122
Running loss of epoch-20 batch-122 = 0.0007048022234812379

Training epoch-20 batch-123
Running loss of epoch-20 batch-123 = 0.00024332792963832617

Training epoch-20 batch-124
Running loss of epoch-20 batch-124 = 0.0006867649499326944

Training epoch-20 batch-125
Running loss of epoch-20 batch-125 = 0.00046274217311292887

Training epoch-20 batch-126
Running loss of epoch-20 batch-126 = 0.0003317698137834668

Training epoch-20 batch-127
Running loss of epoch-20 batch-127 = 0.0001638863468542695

Training epoch-20 batch-128
Running loss of epoch-20 batch-128 = 0.0005563867744058371

Training epoch-20 batch-129
Running loss of epoch-20 batch-129 = 0.0005259057506918907

Training epoch-20 batch-130
Running loss of epoch-20 batch-130 = 0.0004591437755152583

Training epoch-20 batch-131
Running loss of epoch-20 batch-131 = 0.001955291721969843

Training epoch-20 batch-132
Running loss of epoch-20 batch-132 = 0.0013188062002882361

Training epoch-20 batch-133
Running loss of epoch-20 batch-133 = 0.002630494302138686

Training epoch-20 batch-134
Running loss of epoch-20 batch-134 = 0.0010046183597296476

Training epoch-20 batch-135
Running loss of epoch-20 batch-135 = 0.0018217377364635468

Training epoch-20 batch-136
Running loss of epoch-20 batch-136 = 0.0011899027740582824

Training epoch-20 batch-137
Running loss of epoch-20 batch-137 = 0.0005142606096342206

Training epoch-20 batch-138
Running loss of epoch-20 batch-138 = 0.0006907755741849542

Training epoch-20 batch-139
Running loss of epoch-20 batch-139 = 0.0007963546086102724

Training epoch-20 batch-140
Running loss of epoch-20 batch-140 = 0.0024183057248592377

Training epoch-20 batch-141
Running loss of epoch-20 batch-141 = 0.0016692873323336244

Training epoch-20 batch-142
Running loss of epoch-20 batch-142 = 0.0015242164954543114

Training epoch-20 batch-143
Running loss of epoch-20 batch-143 = 0.0017970148473978043

Training epoch-20 batch-144
Running loss of epoch-20 batch-144 = 0.00022401625756174326

Training epoch-20 batch-145
Running loss of epoch-20 batch-145 = 0.0007353147957473993

Training epoch-20 batch-146
Running loss of epoch-20 batch-146 = 0.0008617739658802748

Training epoch-20 batch-147
Running loss of epoch-20 batch-147 = 0.00039173546247184277

Training epoch-20 batch-148
Running loss of epoch-20 batch-148 = 0.0009599473560228944

Training epoch-20 batch-149
Running loss of epoch-20 batch-149 = 0.0014713900163769722

Training epoch-20 batch-150
Running loss of epoch-20 batch-150 = 0.0006649695569649339

Training epoch-20 batch-151
Running loss of epoch-20 batch-151 = 0.0011537487152963877

Training epoch-20 batch-152
Running loss of epoch-20 batch-152 = 0.0013307395856827497

Training epoch-20 batch-153
Running loss of epoch-20 batch-153 = 0.0012362378183752298

Training epoch-20 batch-154
Running loss of epoch-20 batch-154 = 0.0015154120046645403

Training epoch-20 batch-155
Running loss of epoch-20 batch-155 = 0.0007046579848974943

Training epoch-20 batch-156
Running loss of epoch-20 batch-156 = 0.0009337763767689466

Training epoch-20 batch-157
Running loss of epoch-20 batch-157 = 0.0008588209748268127

Finished training epoch-20.



Average train loss at epoch-20 = 0.0008441293504089117

Started Evaluation

Average val loss at epoch-20 = 1.07738381327918

Accuracy for classes:
Accuracy for class equals is: 94.06 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 87.85 %
Accuracy for class toString is: 84.30 %
Accuracy for class run is: 71.00 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 50.90 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 58.46 %

Overall Accuracy = 81.08 %

Finished Evaluation



Started training epoch-21


Training epoch-21 batch-1
Running loss of epoch-21 batch-1 = 0.000375878531485796

Training epoch-21 batch-2
Running loss of epoch-21 batch-2 = 0.001850662985816598

Training epoch-21 batch-3
Running loss of epoch-21 batch-3 = 0.0016339565627276897

Training epoch-21 batch-4
Running loss of epoch-21 batch-4 = 0.0006264882395043969

Training epoch-21 batch-5
Running loss of epoch-21 batch-5 = 0.00029449781868606806

Training epoch-21 batch-6
Running loss of epoch-21 batch-6 = 0.00020920857787132263

Training epoch-21 batch-7
Running loss of epoch-21 batch-7 = 0.002143744146451354

Training epoch-21 batch-8
Running loss of epoch-21 batch-8 = 0.0007548200665041804

Training epoch-21 batch-9
Running loss of epoch-21 batch-9 = 0.0005998326232656837

Training epoch-21 batch-10
Running loss of epoch-21 batch-10 = 0.0004473392618820071

Training epoch-21 batch-11
Running loss of epoch-21 batch-11 = 0.0007734384853392839

Training epoch-21 batch-12
Running loss of epoch-21 batch-12 = 0.0006298130610957742

Training epoch-21 batch-13
Running loss of epoch-21 batch-13 = 0.0008695694850757718

Training epoch-21 batch-14
Running loss of epoch-21 batch-14 = 0.0005894360365346074

Training epoch-21 batch-15
Running loss of epoch-21 batch-15 = 0.0010841414332389832

Training epoch-21 batch-16
Running loss of epoch-21 batch-16 = 0.0005072387866675854

Training epoch-21 batch-17
Running loss of epoch-21 batch-17 = 9.169545955955982e-05

Training epoch-21 batch-18
Running loss of epoch-21 batch-18 = 0.0014463987899944186

Training epoch-21 batch-19
Running loss of epoch-21 batch-19 = 0.0004491714062169194

Training epoch-21 batch-20
Running loss of epoch-21 batch-20 = 0.0007277958793565631

Training epoch-21 batch-21
Running loss of epoch-21 batch-21 = 0.00047891121357679367

Training epoch-21 batch-22
Running loss of epoch-21 batch-22 = 0.0003479154547676444

Training epoch-21 batch-23
Running loss of epoch-21 batch-23 = 0.00033623503986746073

Training epoch-21 batch-24
Running loss of epoch-21 batch-24 = 0.0012532195542007685

Training epoch-21 batch-25
Running loss of epoch-21 batch-25 = 0.0006664333632215858

Training epoch-21 batch-26
Running loss of epoch-21 batch-26 = 0.0006649236893281341

Training epoch-21 batch-27
Running loss of epoch-21 batch-27 = 0.0004147283034399152

Training epoch-21 batch-28
Running loss of epoch-21 batch-28 = 0.0002997510600835085

Training epoch-21 batch-29
Running loss of epoch-21 batch-29 = 0.0008311707060784101

Training epoch-21 batch-30
Running loss of epoch-21 batch-30 = 0.000428495230153203

Training epoch-21 batch-31
Running loss of epoch-21 batch-31 = 0.0002370515139773488

Training epoch-21 batch-32
Running loss of epoch-21 batch-32 = 0.00024060090072453022

Training epoch-21 batch-33
Running loss of epoch-21 batch-33 = 0.0006752872141078115

Training epoch-21 batch-34
Running loss of epoch-21 batch-34 = 0.00023037265054881573

Training epoch-21 batch-35
Running loss of epoch-21 batch-35 = 0.0002847270807251334

Training epoch-21 batch-36
Running loss of epoch-21 batch-36 = 0.00021850771736353636

Training epoch-21 batch-37
Running loss of epoch-21 batch-37 = 0.0010455200681462884

Training epoch-21 batch-38
Running loss of epoch-21 batch-38 = 0.00047628162428736687

Training epoch-21 batch-39
Running loss of epoch-21 batch-39 = 0.0004648116882890463

Training epoch-21 batch-40
Running loss of epoch-21 batch-40 = 0.00025655608624219894

Training epoch-21 batch-41
Running loss of epoch-21 batch-41 = 0.0010928106494247913

Training epoch-21 batch-42
Running loss of epoch-21 batch-42 = 0.0008387486450374126

Training epoch-21 batch-43
Running loss of epoch-21 batch-43 = 5.264661740511656e-05

Training epoch-21 batch-44
Running loss of epoch-21 batch-44 = 0.002012379001826048

Training epoch-21 batch-45
Running loss of epoch-21 batch-45 = 0.0004515406908467412

Training epoch-21 batch-46
Running loss of epoch-21 batch-46 = 0.0002577223349362612

Training epoch-21 batch-47
Running loss of epoch-21 batch-47 = 0.00019072415307164192

Training epoch-21 batch-48
Running loss of epoch-21 batch-48 = 0.0006325059803202748

Training epoch-21 batch-49
Running loss of epoch-21 batch-49 = 0.0015708591090515256

Training epoch-21 batch-50
Running loss of epoch-21 batch-50 = 0.00024731247685849667

Training epoch-21 batch-51
Running loss of epoch-21 batch-51 = 0.0011206628987565637

Training epoch-21 batch-52
Running loss of epoch-21 batch-52 = 0.00017592450603842735

Training epoch-21 batch-53
Running loss of epoch-21 batch-53 = 0.00019671942573040724

Training epoch-21 batch-54
Running loss of epoch-21 batch-54 = 0.0005205490160733461

Training epoch-21 batch-55
Running loss of epoch-21 batch-55 = 0.0006831223145127296

Training epoch-21 batch-56
Running loss of epoch-21 batch-56 = 0.0005247639492154121

Training epoch-21 batch-57
Running loss of epoch-21 batch-57 = 0.0009296988137066364

Training epoch-21 batch-58
Running loss of epoch-21 batch-58 = 0.00042283779475837946

Training epoch-21 batch-59
Running loss of epoch-21 batch-59 = 0.0003402046859264374

Training epoch-21 batch-60
Running loss of epoch-21 batch-60 = 0.0011929471511393785

Training epoch-21 batch-61
Running loss of epoch-21 batch-61 = 0.0013096120674163103

Training epoch-21 batch-62
Running loss of epoch-21 batch-62 = 0.0010513711022213101

Training epoch-21 batch-63
Running loss of epoch-21 batch-63 = 0.0003216102486476302

Training epoch-21 batch-64
Running loss of epoch-21 batch-64 = 0.0005579085554927588

Training epoch-21 batch-65
Running loss of epoch-21 batch-65 = 0.00021254713647067547

Training epoch-21 batch-66
Running loss of epoch-21 batch-66 = 0.0014660705346614122

Training epoch-21 batch-67
Running loss of epoch-21 batch-67 = 0.0009141579503193498

Training epoch-21 batch-68
Running loss of epoch-21 batch-68 = 0.0008563088485971093

Training epoch-21 batch-69
Running loss of epoch-21 batch-69 = 0.0006862448062747717

Training epoch-21 batch-70
Running loss of epoch-21 batch-70 = 0.0006753214402124286

Training epoch-21 batch-71
Running loss of epoch-21 batch-71 = 0.0008712079143151641

Training epoch-21 batch-72
Running loss of epoch-21 batch-72 = 0.0011210056254640222

Training epoch-21 batch-73
Running loss of epoch-21 batch-73 = 0.0010143232066184282

Training epoch-21 batch-74
Running loss of epoch-21 batch-74 = 0.00021451886277645826

Training epoch-21 batch-75
Running loss of epoch-21 batch-75 = 0.0012394528603181243

Training epoch-21 batch-76
Running loss of epoch-21 batch-76 = 0.0007471753051504493

Training epoch-21 batch-77
Running loss of epoch-21 batch-77 = 0.00021452736109495163

Training epoch-21 batch-78
Running loss of epoch-21 batch-78 = 0.00030863110441714525

Training epoch-21 batch-79
Running loss of epoch-21 batch-79 = 0.00043837307021021843

Training epoch-21 batch-80
Running loss of epoch-21 batch-80 = 0.0013113319873809814

Training epoch-21 batch-81
Running loss of epoch-21 batch-81 = 0.00036688661202788353

Training epoch-21 batch-82
Running loss of epoch-21 batch-82 = 0.0009558259043842554

Training epoch-21 batch-83
Running loss of epoch-21 batch-83 = 0.00038904615212231874

Training epoch-21 batch-84
Running loss of epoch-21 batch-84 = 0.0005387846613302827

Training epoch-21 batch-85
Running loss of epoch-21 batch-85 = 0.00023006054107099771

Training epoch-21 batch-86
Running loss of epoch-21 batch-86 = 0.0004091303562745452

Training epoch-21 batch-87
Running loss of epoch-21 batch-87 = 0.0004083048552274704

Training epoch-21 batch-88
Running loss of epoch-21 batch-88 = 0.000994940404780209

Training epoch-21 batch-89
Running loss of epoch-21 batch-89 = 0.00019524048548191786

Training epoch-21 batch-90
Running loss of epoch-21 batch-90 = 0.0002791411243379116

Training epoch-21 batch-91
Running loss of epoch-21 batch-91 = 0.0006142420461401343

Training epoch-21 batch-92
Running loss of epoch-21 batch-92 = 0.0016856446163728833

Training epoch-21 batch-93
Running loss of epoch-21 batch-93 = 0.0005647531943395734

Training epoch-21 batch-94
Running loss of epoch-21 batch-94 = 0.00043987587559968233

Training epoch-21 batch-95
Running loss of epoch-21 batch-95 = 0.0010382721666246653

Training epoch-21 batch-96
Running loss of epoch-21 batch-96 = 0.0004957524361088872

Training epoch-21 batch-97
Running loss of epoch-21 batch-97 = 0.000193594372831285

Training epoch-21 batch-98
Running loss of epoch-21 batch-98 = 0.0004471606225706637

Training epoch-21 batch-99
Running loss of epoch-21 batch-99 = 0.0003458542050793767

Training epoch-21 batch-100
Running loss of epoch-21 batch-100 = 0.0007623225683346391

Training epoch-21 batch-101
Running loss of epoch-21 batch-101 = 0.0005797658814117312

Training epoch-21 batch-102
Running loss of epoch-21 batch-102 = 0.0006252836901694536

Training epoch-21 batch-103
Running loss of epoch-21 batch-103 = 0.0005108944606035948

Training epoch-21 batch-104
Running loss of epoch-21 batch-104 = 0.001578980009071529

Training epoch-21 batch-105
Running loss of epoch-21 batch-105 = 0.00027947500348091125

Training epoch-21 batch-106
Running loss of epoch-21 batch-106 = 0.0003686923300847411

Training epoch-21 batch-107
Running loss of epoch-21 batch-107 = 0.000867732334882021

Training epoch-21 batch-108
Running loss of epoch-21 batch-108 = 0.0002493478823453188

Training epoch-21 batch-109
Running loss of epoch-21 batch-109 = 0.0002978801494464278

Training epoch-21 batch-110
Running loss of epoch-21 batch-110 = 0.0006209921557456255

Training epoch-21 batch-111
Running loss of epoch-21 batch-111 = 0.001340029644779861

Training epoch-21 batch-112
Running loss of epoch-21 batch-112 = 0.00019691209308803082

Training epoch-21 batch-113
Running loss of epoch-21 batch-113 = 0.0010897357715293765

Training epoch-21 batch-114
Running loss of epoch-21 batch-114 = 0.0010677261743694544

Training epoch-21 batch-115
Running loss of epoch-21 batch-115 = 0.0002627072390168905

Training epoch-21 batch-116
Running loss of epoch-21 batch-116 = 0.0005059952381998301

Training epoch-21 batch-117
Running loss of epoch-21 batch-117 = 0.0004234422231093049

Training epoch-21 batch-118
Running loss of epoch-21 batch-118 = 0.00024310965090990067

Training epoch-21 batch-119
Running loss of epoch-21 batch-119 = 0.0008361139334738255

Training epoch-21 batch-120
Running loss of epoch-21 batch-120 = 0.00018082838505506516

Training epoch-21 batch-121
Running loss of epoch-21 batch-121 = 0.000191541388630867

Training epoch-21 batch-122
Running loss of epoch-21 batch-122 = 0.0002776616020128131

Training epoch-21 batch-123
Running loss of epoch-21 batch-123 = 0.00037444487679749727

Training epoch-21 batch-124
Running loss of epoch-21 batch-124 = 0.0011736549204215407

Training epoch-21 batch-125
Running loss of epoch-21 batch-125 = 0.0001735782716423273

Training epoch-21 batch-126
Running loss of epoch-21 batch-126 = 0.0003781563136726618

Training epoch-21 batch-127
Running loss of epoch-21 batch-127 = 0.0008092874195426702

Training epoch-21 batch-128
Running loss of epoch-21 batch-128 = 0.0004173647612333298

Training epoch-21 batch-129
Running loss of epoch-21 batch-129 = 0.0009542073821648955

Training epoch-21 batch-130
Running loss of epoch-21 batch-130 = 0.0008764703525230289

Training epoch-21 batch-131
Running loss of epoch-21 batch-131 = 0.0009420318529009819

Training epoch-21 batch-132
Running loss of epoch-21 batch-132 = 0.00036604900378733873

Training epoch-21 batch-133
Running loss of epoch-21 batch-133 = 0.0001981606474146247

Training epoch-21 batch-134
Running loss of epoch-21 batch-134 = 0.00040287431329488754

Training epoch-21 batch-135
Running loss of epoch-21 batch-135 = 0.0008661518804728985

Training epoch-21 batch-136
Running loss of epoch-21 batch-136 = 0.0002721189521253109

Training epoch-21 batch-137
Running loss of epoch-21 batch-137 = 8.492055349051952e-05

Training epoch-21 batch-138
Running loss of epoch-21 batch-138 = 0.0006098493468016386

Training epoch-21 batch-139
Running loss of epoch-21 batch-139 = 0.0012588286772370338

Training epoch-21 batch-140
Running loss of epoch-21 batch-140 = 0.0008834943873807788

Training epoch-21 batch-141
Running loss of epoch-21 batch-141 = 0.0005314497975632548

Training epoch-21 batch-142
Running loss of epoch-21 batch-142 = 0.0007584951817989349

Training epoch-21 batch-143
Running loss of epoch-21 batch-143 = 0.0009625002858228981

Training epoch-21 batch-144
Running loss of epoch-21 batch-144 = 0.0004558810032904148

Training epoch-21 batch-145
Running loss of epoch-21 batch-145 = 0.0003605642123147845

Training epoch-21 batch-146
Running loss of epoch-21 batch-146 = 0.0003475057892501354

Training epoch-21 batch-147
Running loss of epoch-21 batch-147 = 0.00025468505918979645

Training epoch-21 batch-148
Running loss of epoch-21 batch-148 = 9.68538224697113e-05

Training epoch-21 batch-149
Running loss of epoch-21 batch-149 = 0.00034259085077792406

Training epoch-21 batch-150
Running loss of epoch-21 batch-150 = 0.0006251494633033872

Training epoch-21 batch-151
Running loss of epoch-21 batch-151 = 0.0005610178923234344

Training epoch-21 batch-152
Running loss of epoch-21 batch-152 = 0.0009225724497810006

Training epoch-21 batch-153
Running loss of epoch-21 batch-153 = 0.0006412718212231994

Training epoch-21 batch-154
Running loss of epoch-21 batch-154 = 0.0004672606009989977

Training epoch-21 batch-155
Running loss of epoch-21 batch-155 = 0.0004281628644093871

Training epoch-21 batch-156
Running loss of epoch-21 batch-156 = 0.0004524228861555457

Training epoch-21 batch-157
Running loss of epoch-21 batch-157 = 0.0010428503155708313

Finished training epoch-21.



Average train loss at epoch-21 = 0.0006297819450497628

Started Evaluation

Average val loss at epoch-21 = 1.122931655183062

Accuracy for classes:
Accuracy for class equals is: 94.39 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 87.87 %
Accuracy for class onCreate is: 87.42 %
Accuracy for class toString is: 85.32 %
Accuracy for class run is: 69.63 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 47.98 %
Accuracy for class execute is: 40.96 %
Accuracy for class get is: 68.72 %

Overall Accuracy = 80.85 %

Finished Evaluation



Started training epoch-22


Training epoch-22 batch-1
Running loss of epoch-22 batch-1 = 0.0005632988177239895

Training epoch-22 batch-2
Running loss of epoch-22 batch-2 = 0.0003176894970238209

Training epoch-22 batch-3
Running loss of epoch-22 batch-3 = 0.00013917800970375538

Training epoch-22 batch-4
Running loss of epoch-22 batch-4 = 0.0004345466149970889

Training epoch-22 batch-5
Running loss of epoch-22 batch-5 = 0.00020344427321106195

Training epoch-22 batch-6
Running loss of epoch-22 batch-6 = 0.00024326331913471222

Training epoch-22 batch-7
Running loss of epoch-22 batch-7 = 0.0009909552754834294

Training epoch-22 batch-8
Running loss of epoch-22 batch-8 = 0.0004957841010764241

Training epoch-22 batch-9
Running loss of epoch-22 batch-9 = 0.0005550852511078119

Training epoch-22 batch-10
Running loss of epoch-22 batch-10 = 6.963883060961962e-05

Training epoch-22 batch-11
Running loss of epoch-22 batch-11 = 0.00043860182631760836

Training epoch-22 batch-12
Running loss of epoch-22 batch-12 = 0.0016480344347655773

Training epoch-22 batch-13
Running loss of epoch-22 batch-13 = 0.0004184034187346697

Training epoch-22 batch-14
Running loss of epoch-22 batch-14 = 0.0002964991144835949

Training epoch-22 batch-15
Running loss of epoch-22 batch-15 = 0.00022121949587017298

Training epoch-22 batch-16
Running loss of epoch-22 batch-16 = 0.0003429038915783167

Training epoch-22 batch-17
Running loss of epoch-22 batch-17 = 0.00036489800550043583

Training epoch-22 batch-18
Running loss of epoch-22 batch-18 = 0.00030057993717491627

Training epoch-22 batch-19
Running loss of epoch-22 batch-19 = 0.0008369690040126443

Training epoch-22 batch-20
Running loss of epoch-22 batch-20 = 0.0005129792261868715

Training epoch-22 batch-21
Running loss of epoch-22 batch-21 = 0.0004755672998726368

Training epoch-22 batch-22
Running loss of epoch-22 batch-22 = 0.0008023103000596166

Training epoch-22 batch-23
Running loss of epoch-22 batch-23 = 0.0001639792462810874

Training epoch-22 batch-24
Running loss of epoch-22 batch-24 = 0.00030992680694907904

Training epoch-22 batch-25
Running loss of epoch-22 batch-25 = 0.00037843931932002306

Training epoch-22 batch-26
Running loss of epoch-22 batch-26 = 0.0001621544361114502

Training epoch-22 batch-27
Running loss of epoch-22 batch-27 = 0.0005604185862466693

Training epoch-22 batch-28
Running loss of epoch-22 batch-28 = 0.00024674530141055584

Training epoch-22 batch-29
Running loss of epoch-22 batch-29 = 0.001399616478011012

Training epoch-22 batch-30
Running loss of epoch-22 batch-30 = 0.00013603013940155506

Training epoch-22 batch-31
Running loss of epoch-22 batch-31 = 0.00024589174427092075

Training epoch-22 batch-32
Running loss of epoch-22 batch-32 = 7.415073923766613e-05

Training epoch-22 batch-33
Running loss of epoch-22 batch-33 = 0.0004952156450599432

Training epoch-22 batch-34
Running loss of epoch-22 batch-34 = 0.0005760209169238806

Training epoch-22 batch-35
Running loss of epoch-22 batch-35 = 0.0004823963390663266

Training epoch-22 batch-36
Running loss of epoch-22 batch-36 = 0.00045085593592375517

Training epoch-22 batch-37
Running loss of epoch-22 batch-37 = 7.05295242369175e-05

Training epoch-22 batch-38
Running loss of epoch-22 batch-38 = 0.0006511849351227283

Training epoch-22 batch-39
Running loss of epoch-22 batch-39 = 0.0002803170355036855

Training epoch-22 batch-40
Running loss of epoch-22 batch-40 = 0.0004793280968442559

Training epoch-22 batch-41
Running loss of epoch-22 batch-41 = 0.0002331892028450966

Training epoch-22 batch-42
Running loss of epoch-22 batch-42 = 0.0006456036935560405

Training epoch-22 batch-43
Running loss of epoch-22 batch-43 = 0.00022231414914131165

Training epoch-22 batch-44
Running loss of epoch-22 batch-44 = 0.0006820827256888151

Training epoch-22 batch-45
Running loss of epoch-22 batch-45 = 0.00024713936727494

Training epoch-22 batch-46
Running loss of epoch-22 batch-46 = 0.00023554894141852856

Training epoch-22 batch-47
Running loss of epoch-22 batch-47 = 0.00020286417566239834

Training epoch-22 batch-48
Running loss of epoch-22 batch-48 = 0.000229084980674088

Training epoch-22 batch-49
Running loss of epoch-22 batch-49 = 0.0002454177010804415

Training epoch-22 batch-50
Running loss of epoch-22 batch-50 = 0.00036037247627973557

Training epoch-22 batch-51
Running loss of epoch-22 batch-51 = 0.0006636254256591201

Training epoch-22 batch-52
Running loss of epoch-22 batch-52 = 0.0005391647573560476

Training epoch-22 batch-53
Running loss of epoch-22 batch-53 = 0.00014353147707879543

Training epoch-22 batch-54
Running loss of epoch-22 batch-54 = 0.0003781201085075736

Training epoch-22 batch-55
Running loss of epoch-22 batch-55 = 9.988911915570498e-05

Training epoch-22 batch-56
Running loss of epoch-22 batch-56 = 0.0006720599485561252

Training epoch-22 batch-57
Running loss of epoch-22 batch-57 = 0.0008845028933137655

Training epoch-22 batch-58
Running loss of epoch-22 batch-58 = 0.0006352127529680729

Training epoch-22 batch-59
Running loss of epoch-22 batch-59 = 0.0004377777222543955

Training epoch-22 batch-60
Running loss of epoch-22 batch-60 = 0.0008049978641793132

Training epoch-22 batch-61
Running loss of epoch-22 batch-61 = 0.00034522684291005135

Training epoch-22 batch-62
Running loss of epoch-22 batch-62 = 0.00041216739919036627

Training epoch-22 batch-63
Running loss of epoch-22 batch-63 = 9.997072629630566e-05

Training epoch-22 batch-64
Running loss of epoch-22 batch-64 = 0.0005907972808927298

Training epoch-22 batch-65
Running loss of epoch-22 batch-65 = 0.0003440332366153598

Training epoch-22 batch-66
Running loss of epoch-22 batch-66 = 0.0004901664797216654

Training epoch-22 batch-67
Running loss of epoch-22 batch-67 = 0.00026667199563235044

Training epoch-22 batch-68
Running loss of epoch-22 batch-68 = 0.00020787888206541538

Training epoch-22 batch-69
Running loss of epoch-22 batch-69 = 0.0005602807505056262

Training epoch-22 batch-70
Running loss of epoch-22 batch-70 = 0.00031328783370554447

Training epoch-22 batch-71
Running loss of epoch-22 batch-71 = 0.0004859836772084236

Training epoch-22 batch-72
Running loss of epoch-22 batch-72 = 0.00020799553021788597

Training epoch-22 batch-73
Running loss of epoch-22 batch-73 = 0.0005379053764045238

Training epoch-22 batch-74
Running loss of epoch-22 batch-74 = 0.00028683373238891363

Training epoch-22 batch-75
Running loss of epoch-22 batch-75 = 0.0009296472417190671

Training epoch-22 batch-76
Running loss of epoch-22 batch-76 = 0.0011869665468111634

Training epoch-22 batch-77
Running loss of epoch-22 batch-77 = 8.949963375926018e-05

Training epoch-22 batch-78
Running loss of epoch-22 batch-78 = 0.0010581003734841943

Training epoch-22 batch-79
Running loss of epoch-22 batch-79 = 0.0002942633582279086

Training epoch-22 batch-80
Running loss of epoch-22 batch-80 = 3.7264544516801834e-05

Training epoch-22 batch-81
Running loss of epoch-22 batch-81 = 0.0003082033945247531

Training epoch-22 batch-82
Running loss of epoch-22 batch-82 = 0.0002794298343360424

Training epoch-22 batch-83
Running loss of epoch-22 batch-83 = 0.0006395194213837385

Training epoch-22 batch-84
Running loss of epoch-22 batch-84 = 0.00024026399478316307

Training epoch-22 batch-85
Running loss of epoch-22 batch-85 = 0.00032761297188699245

Training epoch-22 batch-86
Running loss of epoch-22 batch-86 = 0.0003142512869089842

Training epoch-22 batch-87
Running loss of epoch-22 batch-87 = 0.0002362972591072321

Training epoch-22 batch-88
Running loss of epoch-22 batch-88 = 0.00014883652329444885

Training epoch-22 batch-89
Running loss of epoch-22 batch-89 = 0.0003114814171567559

Training epoch-22 batch-90
Running loss of epoch-22 batch-90 = 0.00035204202868044376

Training epoch-22 batch-91
Running loss of epoch-22 batch-91 = 0.00024259556084871292

Training epoch-22 batch-92
Running loss of epoch-22 batch-92 = 0.0005561817670240998

Training epoch-22 batch-93
Running loss of epoch-22 batch-93 = 0.0003479871666058898

Training epoch-22 batch-94
Running loss of epoch-22 batch-94 = 0.0002755667082965374

Training epoch-22 batch-95
Running loss of epoch-22 batch-95 = 0.0003979405155405402

Training epoch-22 batch-96
Running loss of epoch-22 batch-96 = 0.0003674352774396539

Training epoch-22 batch-97
Running loss of epoch-22 batch-97 = 0.00041591410990804434

Training epoch-22 batch-98
Running loss of epoch-22 batch-98 = 0.0003155553713440895

Training epoch-22 batch-99
Running loss of epoch-22 batch-99 = 0.0002784992102533579

Training epoch-22 batch-100
Running loss of epoch-22 batch-100 = 0.00023019115906208754

Training epoch-22 batch-101
Running loss of epoch-22 batch-101 = 0.001297307899221778

Training epoch-22 batch-102
Running loss of epoch-22 batch-102 = 0.00022691593039780855

Training epoch-22 batch-103
Running loss of epoch-22 batch-103 = 0.0014085652073845267

Training epoch-22 batch-104
Running loss of epoch-22 batch-104 = 0.00011389702558517456

Training epoch-22 batch-105
Running loss of epoch-22 batch-105 = 0.00023579644039273262

Training epoch-22 batch-106
Running loss of epoch-22 batch-106 = 0.00017157301772385836

Training epoch-22 batch-107
Running loss of epoch-22 batch-107 = 0.0005337572656571865

Training epoch-22 batch-108
Running loss of epoch-22 batch-108 = 0.0003183914814144373

Training epoch-22 batch-109
Running loss of epoch-22 batch-109 = 0.0005131594371050596

Training epoch-22 batch-110
Running loss of epoch-22 batch-110 = 0.0003464134642854333

Training epoch-22 batch-111
Running loss of epoch-22 batch-111 = 0.0006275787018239498

Training epoch-22 batch-112
Running loss of epoch-22 batch-112 = 0.0005124056478962302

Training epoch-22 batch-113
Running loss of epoch-22 batch-113 = 0.00011934340000152588

Training epoch-22 batch-114
Running loss of epoch-22 batch-114 = 0.0003293121699243784

Training epoch-22 batch-115
Running loss of epoch-22 batch-115 = 0.0003490305971354246

Training epoch-22 batch-116
Running loss of epoch-22 batch-116 = 0.00016451801639050245

Training epoch-22 batch-117
Running loss of epoch-22 batch-117 = 0.0003945686621591449

Training epoch-22 batch-118
Running loss of epoch-22 batch-118 = 0.00010422454215586185

Training epoch-22 batch-119
Running loss of epoch-22 batch-119 = 0.0005007007857784629

Training epoch-22 batch-120
Running loss of epoch-22 batch-120 = 0.00017850822769105434

Training epoch-22 batch-121
Running loss of epoch-22 batch-121 = 0.0005287426756694913

Training epoch-22 batch-122
Running loss of epoch-22 batch-122 = 0.00014742417261004448

Training epoch-22 batch-123
Running loss of epoch-22 batch-123 = 0.00038172444328665733

Training epoch-22 batch-124
Running loss of epoch-22 batch-124 = 0.0004542752867564559

Training epoch-22 batch-125
Running loss of epoch-22 batch-125 = 0.00033218658063560724

Training epoch-22 batch-126
Running loss of epoch-22 batch-126 = 0.0004982850514352322

Training epoch-22 batch-127
Running loss of epoch-22 batch-127 = 0.0002855097409337759

Training epoch-22 batch-128
Running loss of epoch-22 batch-128 = 0.00023316754959523678

Training epoch-22 batch-129
Running loss of epoch-22 batch-129 = 0.0006322732660919428

Training epoch-22 batch-130
Running loss of epoch-22 batch-130 = 0.0006145860534161329

Training epoch-22 batch-131
Running loss of epoch-22 batch-131 = 0.0008295192383229733

Training epoch-22 batch-132
Running loss of epoch-22 batch-132 = 0.00032450526487082243

Training epoch-22 batch-133
Running loss of epoch-22 batch-133 = 0.0006993133574724197

Training epoch-22 batch-134
Running loss of epoch-22 batch-134 = 0.0003234168980270624

Training epoch-22 batch-135
Running loss of epoch-22 batch-135 = 0.0010388129157945514

Training epoch-22 batch-136
Running loss of epoch-22 batch-136 = 0.00027728371787816286

Training epoch-22 batch-137
Running loss of epoch-22 batch-137 = 0.0002528371987864375

Training epoch-22 batch-138
Running loss of epoch-22 batch-138 = 0.000915260287001729

Training epoch-22 batch-139
Running loss of epoch-22 batch-139 = 0.00018298299983143806

Training epoch-22 batch-140
Running loss of epoch-22 batch-140 = 0.000276441453024745

Training epoch-22 batch-141
Running loss of epoch-22 batch-141 = 0.0006970013491809368

Training epoch-22 batch-142
Running loss of epoch-22 batch-142 = 0.0007699356647208333

Training epoch-22 batch-143
Running loss of epoch-22 batch-143 = 0.00014693010598421097

Training epoch-22 batch-144
Running loss of epoch-22 batch-144 = 0.00035673531237989664

Training epoch-22 batch-145
Running loss of epoch-22 batch-145 = 8.64777248352766e-05

Training epoch-22 batch-146
Running loss of epoch-22 batch-146 = 0.0001587049337103963

Training epoch-22 batch-147
Running loss of epoch-22 batch-147 = 0.0001765263732522726

Training epoch-22 batch-148
Running loss of epoch-22 batch-148 = 0.00032560527324676514

Training epoch-22 batch-149
Running loss of epoch-22 batch-149 = 0.00035242282319813967

Training epoch-22 batch-150
Running loss of epoch-22 batch-150 = 8.414429612457752e-05

Training epoch-22 batch-151
Running loss of epoch-22 batch-151 = 0.00028622837271541357

Training epoch-22 batch-152
Running loss of epoch-22 batch-152 = 0.00013270985800772905

Training epoch-22 batch-153
Running loss of epoch-22 batch-153 = 0.0001922341762110591

Training epoch-22 batch-154
Running loss of epoch-22 batch-154 = 0.00040198361966758966

Training epoch-22 batch-155
Running loss of epoch-22 batch-155 = 9.090360254049301e-05

Training epoch-22 batch-156
Running loss of epoch-22 batch-156 = 0.00015563389752060175

Training epoch-22 batch-157
Running loss of epoch-22 batch-157 = 0.0004782937467098236

Finished training epoch-22.



Average train loss at epoch-22 = 0.00040919400937855245

Started Evaluation

Average val loss at epoch-22 = 1.0779803823679206

Accuracy for classes:
Accuracy for class equals is: 94.72 %
Accuracy for class main is: 97.21 %
Accuracy for class setUp is: 90.98 %
Accuracy for class onCreate is: 89.98 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 63.24 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 55.16 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.93 %

Finished Evaluation



Started training epoch-23


Training epoch-23 batch-1
Running loss of epoch-23 batch-1 = 4.536809865385294e-05

Training epoch-23 batch-2
Running loss of epoch-23 batch-2 = 0.00018929538782685995

Training epoch-23 batch-3
Running loss of epoch-23 batch-3 = 8.714571595191956e-05

Training epoch-23 batch-4
Running loss of epoch-23 batch-4 = 0.00036871107295155525

Training epoch-23 batch-5
Running loss of epoch-23 batch-5 = 0.0002320308703929186

Training epoch-23 batch-6
Running loss of epoch-23 batch-6 = 0.000559023697860539

Training epoch-23 batch-7
Running loss of epoch-23 batch-7 = 0.0003142205532640219

Training epoch-23 batch-8
Running loss of epoch-23 batch-8 = 0.0003547028172761202

Training epoch-23 batch-9
Running loss of epoch-23 batch-9 = 0.0002492524217814207

Training epoch-23 batch-10
Running loss of epoch-23 batch-10 = 9.010382927954197e-05

Training epoch-23 batch-11
Running loss of epoch-23 batch-11 = 0.00021600816398859024

Training epoch-23 batch-12
Running loss of epoch-23 batch-12 = 0.0001497901976108551

Training epoch-23 batch-13
Running loss of epoch-23 batch-13 = 0.00010209484025835991

Training epoch-23 batch-14
Running loss of epoch-23 batch-14 = 0.0002657495206221938

Training epoch-23 batch-15
Running loss of epoch-23 batch-15 = 0.0008298947941511869

Training epoch-23 batch-16
Running loss of epoch-23 batch-16 = 0.00036347995046526194

Training epoch-23 batch-17
Running loss of epoch-23 batch-17 = 0.00033565331250429153

Training epoch-23 batch-18
Running loss of epoch-23 batch-18 = 0.0004347773501649499

Training epoch-23 batch-19
Running loss of epoch-23 batch-19 = 0.00025752035435289145

Training epoch-23 batch-20
Running loss of epoch-23 batch-20 = 0.00035360222682356834

Training epoch-23 batch-21
Running loss of epoch-23 batch-21 = 0.0004317392595112324

Training epoch-23 batch-22
Running loss of epoch-23 batch-22 = 0.00035021849907934666

Training epoch-23 batch-23
Running loss of epoch-23 batch-23 = 0.00026815570890903473

Training epoch-23 batch-24
Running loss of epoch-23 batch-24 = 0.000260696979239583

Training epoch-23 batch-25
Running loss of epoch-23 batch-25 = 0.0005459001986309886

Training epoch-23 batch-26
Running loss of epoch-23 batch-26 = 0.0007928051054477692

Training epoch-23 batch-27
Running loss of epoch-23 batch-27 = 0.00017850880976766348

Training epoch-23 batch-28
Running loss of epoch-23 batch-28 = 0.00013732025399804115

Training epoch-23 batch-29
Running loss of epoch-23 batch-29 = 0.00036925787571817636

Training epoch-23 batch-30
Running loss of epoch-23 batch-30 = 0.00026143179275095463

Training epoch-23 batch-31
Running loss of epoch-23 batch-31 = 0.0002168507780879736

Training epoch-23 batch-32
Running loss of epoch-23 batch-32 = 0.00019426504150032997

Training epoch-23 batch-33
Running loss of epoch-23 batch-33 = 0.000370905501767993

Training epoch-23 batch-34
Running loss of epoch-23 batch-34 = 0.0002083047293126583

Training epoch-23 batch-35
Running loss of epoch-23 batch-35 = 0.00011445314157754183

Training epoch-23 batch-36
Running loss of epoch-23 batch-36 = 0.00014570378698408604

Training epoch-23 batch-37
Running loss of epoch-23 batch-37 = 0.00035140919499099255

Training epoch-23 batch-38
Running loss of epoch-23 batch-38 = 0.00016596727073192596

Training epoch-23 batch-39
Running loss of epoch-23 batch-39 = 0.00025593838654458523

Training epoch-23 batch-40
Running loss of epoch-23 batch-40 = 0.00021894299425184727

Training epoch-23 batch-41
Running loss of epoch-23 batch-41 = 8.736946620047092e-05

Training epoch-23 batch-42
Running loss of epoch-23 batch-42 = 0.00025011482648551464

Training epoch-23 batch-43
Running loss of epoch-23 batch-43 = 0.00017439958173781633

Training epoch-23 batch-44
Running loss of epoch-23 batch-44 = 0.00012593704741448164

Training epoch-23 batch-45
Running loss of epoch-23 batch-45 = 0.0006413804367184639

Training epoch-23 batch-46
Running loss of epoch-23 batch-46 = 0.0005817059427499771

Training epoch-23 batch-47
Running loss of epoch-23 batch-47 = 5.6536286137998104e-05

Training epoch-23 batch-48
Running loss of epoch-23 batch-48 = 0.0001661311835050583

Training epoch-23 batch-49
Running loss of epoch-23 batch-49 = 5.0942180678248405e-05

Training epoch-23 batch-50
Running loss of epoch-23 batch-50 = 5.055125802755356e-05

Training epoch-23 batch-51
Running loss of epoch-23 batch-51 = 0.00041932822205126286

Training epoch-23 batch-52
Running loss of epoch-23 batch-52 = 0.00031762535218149424

Training epoch-23 batch-53
Running loss of epoch-23 batch-53 = 0.00010216725058853626

Training epoch-23 batch-54
Running loss of epoch-23 batch-54 = 0.00029981217812746763

Training epoch-23 batch-55
Running loss of epoch-23 batch-55 = 9.338650852441788e-05

Training epoch-23 batch-56
Running loss of epoch-23 batch-56 = 0.0011527640745043755

Training epoch-23 batch-57
Running loss of epoch-23 batch-57 = 0.0001779396552592516

Training epoch-23 batch-58
Running loss of epoch-23 batch-58 = 0.0003100017784163356

Training epoch-23 batch-59
Running loss of epoch-23 batch-59 = 0.0003883431199938059

Training epoch-23 batch-60
Running loss of epoch-23 batch-60 = 0.00019034708384424448

Training epoch-23 batch-61
Running loss of epoch-23 batch-61 = 0.00015260791406035423

Training epoch-23 batch-62
Running loss of epoch-23 batch-62 = 0.0001903411466628313

Training epoch-23 batch-63
Running loss of epoch-23 batch-63 = 0.00012640515342354774

Training epoch-23 batch-64
Running loss of epoch-23 batch-64 = 0.00011265184730291367

Training epoch-23 batch-65
Running loss of epoch-23 batch-65 = 0.00019819452427327633

Training epoch-23 batch-66
Running loss of epoch-23 batch-66 = 0.00016160798259079456

Training epoch-23 batch-67
Running loss of epoch-23 batch-67 = 0.0005400286754593253

Training epoch-23 batch-68
Running loss of epoch-23 batch-68 = 0.0002751038409769535

Training epoch-23 batch-69
Running loss of epoch-23 batch-69 = 0.0005838812794536352

Training epoch-23 batch-70
Running loss of epoch-23 batch-70 = 8.041446562856436e-05

Training epoch-23 batch-71
Running loss of epoch-23 batch-71 = 0.00029610609635710716

Training epoch-23 batch-72
Running loss of epoch-23 batch-72 = 0.00020847562700510025

Training epoch-23 batch-73
Running loss of epoch-23 batch-73 = 0.0007483760127797723

Training epoch-23 batch-74
Running loss of epoch-23 batch-74 = 0.0006760715041309595

Training epoch-23 batch-75
Running loss of epoch-23 batch-75 = 0.00035025179386138916

Training epoch-23 batch-76
Running loss of epoch-23 batch-76 = 0.00018707127310335636

Training epoch-23 batch-77
Running loss of epoch-23 batch-77 = 0.00017931510228663683

Training epoch-23 batch-78
Running loss of epoch-23 batch-78 = 4.5163556933403015e-05

Training epoch-23 batch-79
Running loss of epoch-23 batch-79 = 0.00031343603041023016

Training epoch-23 batch-80
Running loss of epoch-23 batch-80 = 0.00046929612290114164

Training epoch-23 batch-81
Running loss of epoch-23 batch-81 = 0.0003197392215952277

Training epoch-23 batch-82
Running loss of epoch-23 batch-82 = 0.0002543878508731723

Training epoch-23 batch-83
Running loss of epoch-23 batch-83 = 0.0001318533904850483

Training epoch-23 batch-84
Running loss of epoch-23 batch-84 = 5.787168629467487e-05

Training epoch-23 batch-85
Running loss of epoch-23 batch-85 = 0.0001471822615712881

Training epoch-23 batch-86
Running loss of epoch-23 batch-86 = 0.0005442671244964004

Training epoch-23 batch-87
Running loss of epoch-23 batch-87 = 0.00018139556050300598

Training epoch-23 batch-88
Running loss of epoch-23 batch-88 = 0.00019744876772165298

Training epoch-23 batch-89
Running loss of epoch-23 batch-89 = 0.0001303467433899641

Training epoch-23 batch-90
Running loss of epoch-23 batch-90 = 0.00017312774434685707

Training epoch-23 batch-91
Running loss of epoch-23 batch-91 = 6.572157144546509e-05

Training epoch-23 batch-92
Running loss of epoch-23 batch-92 = 0.0002457471564412117

Training epoch-23 batch-93
Running loss of epoch-23 batch-93 = 0.00033487530890852213

Training epoch-23 batch-94
Running loss of epoch-23 batch-94 = 0.00012206274550408125

Training epoch-23 batch-95
Running loss of epoch-23 batch-95 = 7.680058479309082e-05

Training epoch-23 batch-96
Running loss of epoch-23 batch-96 = 0.00021119520533829927

Training epoch-23 batch-97
Running loss of epoch-23 batch-97 = 0.00010771001689136028

Training epoch-23 batch-98
Running loss of epoch-23 batch-98 = 2.6452820748090744e-05

Training epoch-23 batch-99
Running loss of epoch-23 batch-99 = 0.0003667131531983614

Training epoch-23 batch-100
Running loss of epoch-23 batch-100 = 0.00039203756023198366

Training epoch-23 batch-101
Running loss of epoch-23 batch-101 = 0.0002299778861925006

Training epoch-23 batch-102
Running loss of epoch-23 batch-102 = 0.00020408607088029385

Training epoch-23 batch-103
Running loss of epoch-23 batch-103 = 0.00036544003523886204

Training epoch-23 batch-104
Running loss of epoch-23 batch-104 = 0.0004309583455324173

Training epoch-23 batch-105
Running loss of epoch-23 batch-105 = 0.0002140894066542387

Training epoch-23 batch-106
Running loss of epoch-23 batch-106 = 0.00029608607292175293

Training epoch-23 batch-107
Running loss of epoch-23 batch-107 = 0.00039708532858639956

Training epoch-23 batch-108
Running loss of epoch-23 batch-108 = 0.00021112069953233004

Training epoch-23 batch-109
Running loss of epoch-23 batch-109 = 0.00030694727320224047

Training epoch-23 batch-110
Running loss of epoch-23 batch-110 = 0.00038491981104016304

Training epoch-23 batch-111
Running loss of epoch-23 batch-111 = 0.000255211372859776

Training epoch-23 batch-112
Running loss of epoch-23 batch-112 = 6.758410017937422e-05

Training epoch-23 batch-113
Running loss of epoch-23 batch-113 = 0.00022988358978182077

Training epoch-23 batch-114
Running loss of epoch-23 batch-114 = 0.0002684452338144183

Training epoch-23 batch-115
Running loss of epoch-23 batch-115 = 0.00027474481612443924

Training epoch-23 batch-116
Running loss of epoch-23 batch-116 = 0.00012310175225138664

Training epoch-23 batch-117
Running loss of epoch-23 batch-117 = 3.829924389719963e-05

Training epoch-23 batch-118
Running loss of epoch-23 batch-118 = 0.0012366088340058923

Training epoch-23 batch-119
Running loss of epoch-23 batch-119 = 0.00014609028585255146

Training epoch-23 batch-120
Running loss of epoch-23 batch-120 = 0.00040475360583513975

Training epoch-23 batch-121
Running loss of epoch-23 batch-121 = 0.00023608282208442688

Training epoch-23 batch-122
Running loss of epoch-23 batch-122 = 0.00018964067567139864

Training epoch-23 batch-123
Running loss of epoch-23 batch-123 = 0.00019437202718108892

Training epoch-23 batch-124
Running loss of epoch-23 batch-124 = 0.00013191532343626022

Training epoch-23 batch-125
Running loss of epoch-23 batch-125 = 0.0004453962901607156

Training epoch-23 batch-126
Running loss of epoch-23 batch-126 = 0.00018143095076084137

Training epoch-23 batch-127
Running loss of epoch-23 batch-127 = 0.0006558967288583517

Training epoch-23 batch-128
Running loss of epoch-23 batch-128 = 0.00010485178790986538

Training epoch-23 batch-129
Running loss of epoch-23 batch-129 = 0.0001897645415738225

Training epoch-23 batch-130
Running loss of epoch-23 batch-130 = 0.00018998701125383377

Training epoch-23 batch-131
Running loss of epoch-23 batch-131 = 0.0009977186564356089

Training epoch-23 batch-132
Running loss of epoch-23 batch-132 = 0.0001763426698744297

Training epoch-23 batch-133
Running loss of epoch-23 batch-133 = 0.0002807428827509284

Training epoch-23 batch-134
Running loss of epoch-23 batch-134 = 0.00020441901870071888

Training epoch-23 batch-135
Running loss of epoch-23 batch-135 = 0.0001272477675229311

Training epoch-23 batch-136
Running loss of epoch-23 batch-136 = 0.00012853380758315325

Training epoch-23 batch-137
Running loss of epoch-23 batch-137 = 0.00010666227899491787

Training epoch-23 batch-138
Running loss of epoch-23 batch-138 = 0.00024497683625668287

Training epoch-23 batch-139
Running loss of epoch-23 batch-139 = 0.00010764133185148239

Training epoch-23 batch-140
Running loss of epoch-23 batch-140 = 0.00021558511070907116

Training epoch-23 batch-141
Running loss of epoch-23 batch-141 = 3.620586358010769e-05

Training epoch-23 batch-142
Running loss of epoch-23 batch-142 = 0.0001554763875901699

Training epoch-23 batch-143
Running loss of epoch-23 batch-143 = 0.00032563600689172745

Training epoch-23 batch-144
Running loss of epoch-23 batch-144 = 0.000270058517344296

Training epoch-23 batch-145
Running loss of epoch-23 batch-145 = 0.0001901548821479082

Training epoch-23 batch-146
Running loss of epoch-23 batch-146 = 0.0005234612617641687

Training epoch-23 batch-147
Running loss of epoch-23 batch-147 = 0.00016349239740520716

Training epoch-23 batch-148
Running loss of epoch-23 batch-148 = 0.0003830426139757037

Training epoch-23 batch-149
Running loss of epoch-23 batch-149 = 0.000560296350158751

Training epoch-23 batch-150
Running loss of epoch-23 batch-150 = 0.00016729487106204033

Training epoch-23 batch-151
Running loss of epoch-23 batch-151 = 0.00010387576185166836

Training epoch-23 batch-152
Running loss of epoch-23 batch-152 = 0.0003630035789683461

Training epoch-23 batch-153
Running loss of epoch-23 batch-153 = 0.00036320288199931383

Training epoch-23 batch-154
Running loss of epoch-23 batch-154 = 0.00035824254155158997

Training epoch-23 batch-155
Running loss of epoch-23 batch-155 = 0.0005391491577029228

Training epoch-23 batch-156
Running loss of epoch-23 batch-156 = 0.0002655693097040057

Training epoch-23 batch-157
Running loss of epoch-23 batch-157 = 0.0015492476522922516

Finished training epoch-23.



Average train loss at epoch-23 = 0.00027907735630869866

Started Evaluation

Average val loss at epoch-23 = 1.1705612069267055

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 95.57 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 88.59 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 61.19 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.69 %
Accuracy for class execute is: 57.43 %
Accuracy for class get is: 61.79 %

Overall Accuracy = 81.04 %

Finished Evaluation



Started training epoch-24


Training epoch-24 batch-1
Running loss of epoch-24 batch-1 = 0.0001014248700812459

Training epoch-24 batch-2
Running loss of epoch-24 batch-2 = 0.00021330348681658506

Training epoch-24 batch-3
Running loss of epoch-24 batch-3 = 0.00013804214540868998

Training epoch-24 batch-4
Running loss of epoch-24 batch-4 = 9.6180010586977e-05

Training epoch-24 batch-5
Running loss of epoch-24 batch-5 = 0.00012423365842550993

Training epoch-24 batch-6
Running loss of epoch-24 batch-6 = 0.0003628288395702839

Training epoch-24 batch-7
Running loss of epoch-24 batch-7 = 0.0006310651078820229

Training epoch-24 batch-8
Running loss of epoch-24 batch-8 = 0.00014013051986694336

Training epoch-24 batch-9
Running loss of epoch-24 batch-9 = 0.0007597733056172729

Training epoch-24 batch-10
Running loss of epoch-24 batch-10 = 0.00014234078116714954

Training epoch-24 batch-11
Running loss of epoch-24 batch-11 = 0.00012493913527578115

Training epoch-24 batch-12
Running loss of epoch-24 batch-12 = 5.918811075389385e-05

Training epoch-24 batch-13
Running loss of epoch-24 batch-13 = 0.00010730186477303505

Training epoch-24 batch-14
Running loss of epoch-24 batch-14 = 0.00020681985188275576

Training epoch-24 batch-15
Running loss of epoch-24 batch-15 = 0.00013419822789728642

Training epoch-24 batch-16
Running loss of epoch-24 batch-16 = 6.435555405914783e-05

Training epoch-24 batch-17
Running loss of epoch-24 batch-17 = 0.0002862639958038926

Training epoch-24 batch-18
Running loss of epoch-24 batch-18 = 0.00011023040860891342

Training epoch-24 batch-19
Running loss of epoch-24 batch-19 = 0.000308748334646225

Training epoch-24 batch-20
Running loss of epoch-24 batch-20 = 0.0003495849668979645

Training epoch-24 batch-21
Running loss of epoch-24 batch-21 = 0.00010869838297367096

Training epoch-24 batch-22
Running loss of epoch-24 batch-22 = 0.00024532468523830175

Training epoch-24 batch-23
Running loss of epoch-24 batch-23 = 6.162666250020266e-05

Training epoch-24 batch-24
Running loss of epoch-24 batch-24 = 0.0007685784948989749

Training epoch-24 batch-25
Running loss of epoch-24 batch-25 = 0.00010917941108345985

Training epoch-24 batch-26
Running loss of epoch-24 batch-26 = 2.4212989956140518e-05

Training epoch-24 batch-27
Running loss of epoch-24 batch-27 = 0.00013949163258075714

Training epoch-24 batch-28
Running loss of epoch-24 batch-28 = 0.00010706216562539339

Training epoch-24 batch-29
Running loss of epoch-24 batch-29 = 0.00011714652646332979

Training epoch-24 batch-30
Running loss of epoch-24 batch-30 = 6.663764361292124e-05

Training epoch-24 batch-31
Running loss of epoch-24 batch-31 = 0.000373073504306376

Training epoch-24 batch-32
Running loss of epoch-24 batch-32 = 0.00014138990081846714

Training epoch-24 batch-33
Running loss of epoch-24 batch-33 = 4.2374012991786e-05

Training epoch-24 batch-34
Running loss of epoch-24 batch-34 = 6.182328797876835e-05

Training epoch-24 batch-35
Running loss of epoch-24 batch-35 = 0.00013794470578432083

Training epoch-24 batch-36
Running loss of epoch-24 batch-36 = 8.914107456803322e-05

Training epoch-24 batch-37
Running loss of epoch-24 batch-37 = 6.653438322246075e-05

Training epoch-24 batch-38
Running loss of epoch-24 batch-38 = 0.00018183467909693718

Training epoch-24 batch-39
Running loss of epoch-24 batch-39 = 9.946362115442753e-05

Training epoch-24 batch-40
Running loss of epoch-24 batch-40 = 0.00013691349886357784

Training epoch-24 batch-41
Running loss of epoch-24 batch-41 = 8.362578228116035e-05

Training epoch-24 batch-42
Running loss of epoch-24 batch-42 = 9.311595931649208e-05

Training epoch-24 batch-43
Running loss of epoch-24 batch-43 = 0.00012914510443806648

Training epoch-24 batch-44
Running loss of epoch-24 batch-44 = 3.265193663537502e-05

Training epoch-24 batch-45
Running loss of epoch-24 batch-45 = 0.00010135537013411522

Training epoch-24 batch-46
Running loss of epoch-24 batch-46 = 0.00014320923946797848

Training epoch-24 batch-47
Running loss of epoch-24 batch-47 = 9.644206147640944e-05

Training epoch-24 batch-48
Running loss of epoch-24 batch-48 = 8.802604861557484e-05

Training epoch-24 batch-49
Running loss of epoch-24 batch-49 = 0.0012635714374482632

Training epoch-24 batch-50
Running loss of epoch-24 batch-50 = 0.00020905514247715473

Training epoch-24 batch-51
Running loss of epoch-24 batch-51 = 0.0004959871293976903

Training epoch-24 batch-52
Running loss of epoch-24 batch-52 = 0.0002564296592026949

Training epoch-24 batch-53
Running loss of epoch-24 batch-53 = 7.040589116513729e-05

Training epoch-24 batch-54
Running loss of epoch-24 batch-54 = 0.0001287319464609027

Training epoch-24 batch-55
Running loss of epoch-24 batch-55 = 8.603744208812714e-05

Training epoch-24 batch-56
Running loss of epoch-24 batch-56 = 0.0002032055053859949

Training epoch-24 batch-57
Running loss of epoch-24 batch-57 = 0.00011816923506557941

Training epoch-24 batch-58
Running loss of epoch-24 batch-58 = 0.00010446296073496342

Training epoch-24 batch-59
Running loss of epoch-24 batch-59 = 0.0001803925260901451

Training epoch-24 batch-60
Running loss of epoch-24 batch-60 = 9.336182847619057e-05

Training epoch-24 batch-61
Running loss of epoch-24 batch-61 = 9.382085409015417e-05

Training epoch-24 batch-62
Running loss of epoch-24 batch-62 = 0.0006942147156223655

Training epoch-24 batch-63
Running loss of epoch-24 batch-63 = 0.0001335714478045702

Training epoch-24 batch-64
Running loss of epoch-24 batch-64 = 2.4045119062066078e-05

Training epoch-24 batch-65
Running loss of epoch-24 batch-65 = 0.00014061923138797283

Training epoch-24 batch-66
Running loss of epoch-24 batch-66 = 0.0002763601951301098

Training epoch-24 batch-67
Running loss of epoch-24 batch-67 = 0.00018631014972925186

Training epoch-24 batch-68
Running loss of epoch-24 batch-68 = 0.00010761467274278402

Training epoch-24 batch-69
Running loss of epoch-24 batch-69 = 0.00018941913731396198

Training epoch-24 batch-70
Running loss of epoch-24 batch-70 = 0.0003819495905190706

Training epoch-24 batch-71
Running loss of epoch-24 batch-71 = 0.0003780107945203781

Training epoch-24 batch-72
Running loss of epoch-24 batch-72 = 0.00010158622171729803

Training epoch-24 batch-73
Running loss of epoch-24 batch-73 = 0.00016937684267759323

Training epoch-24 batch-74
Running loss of epoch-24 batch-74 = 0.00027391500771045685

Training epoch-24 batch-75
Running loss of epoch-24 batch-75 = 8.463836275041103e-05

Training epoch-24 batch-76
Running loss of epoch-24 batch-76 = 0.00020697189029306173

Training epoch-24 batch-77
Running loss of epoch-24 batch-77 = 0.00015417649410665035

Training epoch-24 batch-78
Running loss of epoch-24 batch-78 = 0.0006110274698585272

Training epoch-24 batch-79
Running loss of epoch-24 batch-79 = 0.00019053358118981123

Training epoch-24 batch-80
Running loss of epoch-24 batch-80 = 0.00021556206047534943

Training epoch-24 batch-81
Running loss of epoch-24 batch-81 = 5.7996716350317e-05

Training epoch-24 batch-82
Running loss of epoch-24 batch-82 = 0.0002453937195241451

Training epoch-24 batch-83
Running loss of epoch-24 batch-83 = 0.0002059011021628976

Training epoch-24 batch-84
Running loss of epoch-24 batch-84 = 0.000713686051312834

Training epoch-24 batch-85
Running loss of epoch-24 batch-85 = 0.00016398495063185692

Training epoch-24 batch-86
Running loss of epoch-24 batch-86 = 0.0004291730001568794

Training epoch-24 batch-87
Running loss of epoch-24 batch-87 = 0.00020450446754693985

Training epoch-24 batch-88
Running loss of epoch-24 batch-88 = 0.00022646249271929264

Training epoch-24 batch-89
Running loss of epoch-24 batch-89 = 0.00016229215543717146

Training epoch-24 batch-90
Running loss of epoch-24 batch-90 = 0.00015932053793221712

Training epoch-24 batch-91
Running loss of epoch-24 batch-91 = 0.00011828378774225712

Training epoch-24 batch-92
Running loss of epoch-24 batch-92 = 4.584621638059616e-05

Training epoch-24 batch-93
Running loss of epoch-24 batch-93 = 8.893478661775589e-05

Training epoch-24 batch-94
Running loss of epoch-24 batch-94 = 8.82099848240614e-05

Training epoch-24 batch-95
Running loss of epoch-24 batch-95 = 0.00011788192205131054

Training epoch-24 batch-96
Running loss of epoch-24 batch-96 = 0.0001429971307516098

Training epoch-24 batch-97
Running loss of epoch-24 batch-97 = 0.00026153947692364454

Training epoch-24 batch-98
Running loss of epoch-24 batch-98 = 9.557022713124752e-05

Training epoch-24 batch-99
Running loss of epoch-24 batch-99 = 0.00011473731137812138

Training epoch-24 batch-100
Running loss of epoch-24 batch-100 = 0.00019740208517760038

Training epoch-24 batch-101
Running loss of epoch-24 batch-101 = 2.6251189410686493e-05

Training epoch-24 batch-102
Running loss of epoch-24 batch-102 = 0.00012638629414141178

Training epoch-24 batch-103
Running loss of epoch-24 batch-103 = 0.00017350516282022

Training epoch-24 batch-104
Running loss of epoch-24 batch-104 = 7.162592373788357e-05

Training epoch-24 batch-105
Running loss of epoch-24 batch-105 = 0.00011479388922452927

Training epoch-24 batch-106
Running loss of epoch-24 batch-106 = 9.58514865487814e-05

Training epoch-24 batch-107
Running loss of epoch-24 batch-107 = 0.0002488417085260153

Training epoch-24 batch-108
Running loss of epoch-24 batch-108 = 0.0005495338700711727

Training epoch-24 batch-109
Running loss of epoch-24 batch-109 = 0.00017260806635022163

Training epoch-24 batch-110
Running loss of epoch-24 batch-110 = 0.00031171797309070826

Training epoch-24 batch-111
Running loss of epoch-24 batch-111 = 9.620888158679008e-05

Training epoch-24 batch-112
Running loss of epoch-24 batch-112 = 0.0003364493604749441

Training epoch-24 batch-113
Running loss of epoch-24 batch-113 = 0.00010566855780780315

Training epoch-24 batch-114
Running loss of epoch-24 batch-114 = 6.599095650017262e-05

Training epoch-24 batch-115
Running loss of epoch-24 batch-115 = 0.00016446714289486408

Training epoch-24 batch-116
Running loss of epoch-24 batch-116 = 0.00020566338207572699

Training epoch-24 batch-117
Running loss of epoch-24 batch-117 = 0.0006950142560526729

Training epoch-24 batch-118
Running loss of epoch-24 batch-118 = 4.675309173762798e-05

Training epoch-24 batch-119
Running loss of epoch-24 batch-119 = 0.0005728870164602995

Training epoch-24 batch-120
Running loss of epoch-24 batch-120 = 0.00017303484492003918

Training epoch-24 batch-121
Running loss of epoch-24 batch-121 = 9.806954767554998e-05

Training epoch-24 batch-122
Running loss of epoch-24 batch-122 = 0.00033774564508348703

Training epoch-24 batch-123
Running loss of epoch-24 batch-123 = 0.0005318343173712492

Training epoch-24 batch-124
Running loss of epoch-24 batch-124 = 0.0005772820441052318

Training epoch-24 batch-125
Running loss of epoch-24 batch-125 = 0.00012610689736902714

Training epoch-24 batch-126
Running loss of epoch-24 batch-126 = 0.00011979276314377785

Training epoch-24 batch-127
Running loss of epoch-24 batch-127 = 0.0002255080034956336

Training epoch-24 batch-128
Running loss of epoch-24 batch-128 = 0.00010473572183400393

Training epoch-24 batch-129
Running loss of epoch-24 batch-129 = 8.397642523050308e-05

Training epoch-24 batch-130
Running loss of epoch-24 batch-130 = 0.00022408843506127596

Training epoch-24 batch-131
Running loss of epoch-24 batch-131 = 0.000741176656447351

Training epoch-24 batch-132
Running loss of epoch-24 batch-132 = 7.054954767227173e-05

Training epoch-24 batch-133
Running loss of epoch-24 batch-133 = 0.0005439468659460545

Training epoch-24 batch-134
Running loss of epoch-24 batch-134 = 0.0003116906154900789

Training epoch-24 batch-135
Running loss of epoch-24 batch-135 = 0.00038733496330678463

Training epoch-24 batch-136
Running loss of epoch-24 batch-136 = 0.00012668408453464508

Training epoch-24 batch-137
Running loss of epoch-24 batch-137 = 8.269469253718853e-05

Training epoch-24 batch-138
Running loss of epoch-24 batch-138 = 0.0001114102778956294

Training epoch-24 batch-139
Running loss of epoch-24 batch-139 = 0.00010762806050479412

Training epoch-24 batch-140
Running loss of epoch-24 batch-140 = 0.00021869712509214878

Training epoch-24 batch-141
Running loss of epoch-24 batch-141 = 0.0001475742319598794

Training epoch-24 batch-142
Running loss of epoch-24 batch-142 = 0.00011165672913193703

Training epoch-24 batch-143
Running loss of epoch-24 batch-143 = 0.00018039147835224867

Training epoch-24 batch-144
Running loss of epoch-24 batch-144 = 0.00010618462692946196

Training epoch-24 batch-145
Running loss of epoch-24 batch-145 = 0.00017924979329109192

Training epoch-24 batch-146
Running loss of epoch-24 batch-146 = 0.0001622309209778905

Training epoch-24 batch-147
Running loss of epoch-24 batch-147 = 0.0001081656664609909

Training epoch-24 batch-148
Running loss of epoch-24 batch-148 = 0.00024756311904639006

Training epoch-24 batch-149
Running loss of epoch-24 batch-149 = 0.00039781781379133463

Training epoch-24 batch-150
Running loss of epoch-24 batch-150 = 0.0001718756975606084

Training epoch-24 batch-151
Running loss of epoch-24 batch-151 = 0.00015338766388595104

Training epoch-24 batch-152
Running loss of epoch-24 batch-152 = 0.000235862098634243

Training epoch-24 batch-153
Running loss of epoch-24 batch-153 = 0.0001773717813193798

Training epoch-24 batch-154
Running loss of epoch-24 batch-154 = 6.571679841727018e-05

Training epoch-24 batch-155
Running loss of epoch-24 batch-155 = 0.00010982831008732319

Training epoch-24 batch-156
Running loss of epoch-24 batch-156 = 8.981314022094011e-05

Training epoch-24 batch-157
Running loss of epoch-24 batch-157 = 0.0006545595824718475

Finished training epoch-24.



Average train loss at epoch-24 = 0.0002064842503517866

Started Evaluation

Average val loss at epoch-24 = 1.155637430418234

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 92.13 %
Accuracy for class onCreate is: 89.45 %
Accuracy for class toString is: 84.64 %
Accuracy for class run is: 61.19 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 50.60 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.74 %

Finished Evaluation



Started training epoch-25


Training epoch-25 batch-1
Running loss of epoch-25 batch-1 = 0.00028644269332289696

Training epoch-25 batch-2
Running loss of epoch-25 batch-2 = 0.0001129199517890811

Training epoch-25 batch-3
Running loss of epoch-25 batch-3 = 0.00013454025611281395

Training epoch-25 batch-4
Running loss of epoch-25 batch-4 = 0.0002750969724729657

Training epoch-25 batch-5
Running loss of epoch-25 batch-5 = 6.574112921953201e-05

Training epoch-25 batch-6
Running loss of epoch-25 batch-6 = 0.00013235374353826046

Training epoch-25 batch-7
Running loss of epoch-25 batch-7 = 0.00023338710889220238

Training epoch-25 batch-8
Running loss of epoch-25 batch-8 = 6.784661673009396e-05

Training epoch-25 batch-9
Running loss of epoch-25 batch-9 = 0.00010128598660230637

Training epoch-25 batch-10
Running loss of epoch-25 batch-10 = 2.872792538255453e-05

Training epoch-25 batch-11
Running loss of epoch-25 batch-11 = 0.00030550500378012657

Training epoch-25 batch-12
Running loss of epoch-25 batch-12 = 0.0004443945363163948

Training epoch-25 batch-13
Running loss of epoch-25 batch-13 = 0.00023756385780870914

Training epoch-25 batch-14
Running loss of epoch-25 batch-14 = 4.7215959057211876e-05

Training epoch-25 batch-15
Running loss of epoch-25 batch-15 = 0.000180685892701149

Training epoch-25 batch-16
Running loss of epoch-25 batch-16 = 7.895578164607286e-05

Training epoch-25 batch-17
Running loss of epoch-25 batch-17 = 8.554139640182257e-05

Training epoch-25 batch-18
Running loss of epoch-25 batch-18 = 0.00012896140106022358

Training epoch-25 batch-19
Running loss of epoch-25 batch-19 = 8.305837400257587e-05

Training epoch-25 batch-20
Running loss of epoch-25 batch-20 = 0.000691344728693366

Training epoch-25 batch-21
Running loss of epoch-25 batch-21 = 8.20644199848175e-05

Training epoch-25 batch-22
Running loss of epoch-25 batch-22 = 0.0003178187180310488

Training epoch-25 batch-23
Running loss of epoch-25 batch-23 = 0.00023519829846918583

Training epoch-25 batch-24
Running loss of epoch-25 batch-24 = 0.000925534637644887

Training epoch-25 batch-25
Running loss of epoch-25 batch-25 = 8.523557335138321e-05

Training epoch-25 batch-26
Running loss of epoch-25 batch-26 = 0.0007139942608773708

Training epoch-25 batch-27
Running loss of epoch-25 batch-27 = 0.00013001530896872282

Training epoch-25 batch-28
Running loss of epoch-25 batch-28 = 0.00036352500319480896

Training epoch-25 batch-29
Running loss of epoch-25 batch-29 = 0.0001995359780266881

Training epoch-25 batch-30
Running loss of epoch-25 batch-30 = 0.00017879600636661053

Training epoch-25 batch-31
Running loss of epoch-25 batch-31 = 7.65104778110981e-05

Training epoch-25 batch-32
Running loss of epoch-25 batch-32 = 0.00030543189495801926

Training epoch-25 batch-33
Running loss of epoch-25 batch-33 = 0.00031419098377227783

Training epoch-25 batch-34
Running loss of epoch-25 batch-34 = 6.035575643181801e-05

Training epoch-25 batch-35
Running loss of epoch-25 batch-35 = 0.00033053895458579063

Training epoch-25 batch-36
Running loss of epoch-25 batch-36 = 0.0002180878072977066

Training epoch-25 batch-37
Running loss of epoch-25 batch-37 = 0.00019581697415560484

Training epoch-25 batch-38
Running loss of epoch-25 batch-38 = 0.00013783283066004515

Training epoch-25 batch-39
Running loss of epoch-25 batch-39 = 0.00020358548499643803

Training epoch-25 batch-40
Running loss of epoch-25 batch-40 = 0.00017869321163743734

Training epoch-25 batch-41
Running loss of epoch-25 batch-41 = 6.0532474890351295e-05

Training epoch-25 batch-42
Running loss of epoch-25 batch-42 = 0.00028001959435641766

Training epoch-25 batch-43
Running loss of epoch-25 batch-43 = 7.898407056927681e-05

Training epoch-25 batch-44
Running loss of epoch-25 batch-44 = 0.00019063870422542095

Training epoch-25 batch-45
Running loss of epoch-25 batch-45 = 0.00011555175296962261

Training epoch-25 batch-46
Running loss of epoch-25 batch-46 = 0.00017547502648085356

Training epoch-25 batch-47
Running loss of epoch-25 batch-47 = 0.00011246220674365759

Training epoch-25 batch-48
Running loss of epoch-25 batch-48 = 2.9234797693789005e-05

Training epoch-25 batch-49
Running loss of epoch-25 batch-49 = 0.00034821731969714165

Training epoch-25 batch-50
Running loss of epoch-25 batch-50 = 0.0005009042797610164

Training epoch-25 batch-51
Running loss of epoch-25 batch-51 = 0.0003071889514103532

Training epoch-25 batch-52
Running loss of epoch-25 batch-52 = 0.00013601465616375208

Training epoch-25 batch-53
Running loss of epoch-25 batch-53 = 0.00015811831690371037

Training epoch-25 batch-54
Running loss of epoch-25 batch-54 = 3.4210504963994026e-05

Training epoch-25 batch-55
Running loss of epoch-25 batch-55 = 6.379245314747095e-05

Training epoch-25 batch-56
Running loss of epoch-25 batch-56 = 5.8921054005622864e-05

Training epoch-25 batch-57
Running loss of epoch-25 batch-57 = 0.0002576281549409032

Training epoch-25 batch-58
Running loss of epoch-25 batch-58 = 0.00036823004484176636

Training epoch-25 batch-59
Running loss of epoch-25 batch-59 = 6.904685869812965e-05

Training epoch-25 batch-60
Running loss of epoch-25 batch-60 = 0.00022064032964408398

Training epoch-25 batch-61
Running loss of epoch-25 batch-61 = 9.41343605518341e-05

Training epoch-25 batch-62
Running loss of epoch-25 batch-62 = 0.00025516864843666553

Training epoch-25 batch-63
Running loss of epoch-25 batch-63 = 0.000631851376965642

Training epoch-25 batch-64
Running loss of epoch-25 batch-64 = 0.00011545512825250626

Training epoch-25 batch-65
Running loss of epoch-25 batch-65 = 0.00010705203749239445

Training epoch-25 batch-66
Running loss of epoch-25 batch-66 = 8.213985711336136e-05

Training epoch-25 batch-67
Running loss of epoch-25 batch-67 = 0.00025506538804620504

Training epoch-25 batch-68
Running loss of epoch-25 batch-68 = 0.0002533765509724617

Training epoch-25 batch-69
Running loss of epoch-25 batch-69 = 0.00031817471608519554

Training epoch-25 batch-70
Running loss of epoch-25 batch-70 = 0.000179751543328166

Training epoch-25 batch-71
Running loss of epoch-25 batch-71 = 4.3178675696253777e-05

Training epoch-25 batch-72
Running loss of epoch-25 batch-72 = 0.0002006662543863058

Training epoch-25 batch-73
Running loss of epoch-25 batch-73 = 5.0880247727036476e-05

Training epoch-25 batch-74
Running loss of epoch-25 batch-74 = 8.645746856927872e-05

Training epoch-25 batch-75
Running loss of epoch-25 batch-75 = 0.00023757899180054665

Training epoch-25 batch-76
Running loss of epoch-25 batch-76 = 0.0003210724098607898

Training epoch-25 batch-77
Running loss of epoch-25 batch-77 = 0.000139854964800179

Training epoch-25 batch-78
Running loss of epoch-25 batch-78 = 2.1394691430032253e-05

Training epoch-25 batch-79
Running loss of epoch-25 batch-79 = 0.00021926092449575663

Training epoch-25 batch-80
Running loss of epoch-25 batch-80 = 3.9977021515369415e-05

Training epoch-25 batch-81
Running loss of epoch-25 batch-81 = 0.0002829639706760645

Training epoch-25 batch-82
Running loss of epoch-25 batch-82 = 0.0008637883001938462

Training epoch-25 batch-83
Running loss of epoch-25 batch-83 = 8.652475662529469e-05

Training epoch-25 batch-84
Running loss of epoch-25 batch-84 = 0.0001508214045315981

Training epoch-25 batch-85
Running loss of epoch-25 batch-85 = 7.418706081807613e-05

Training epoch-25 batch-86
Running loss of epoch-25 batch-86 = 0.00020652892999351025

Training epoch-25 batch-87
Running loss of epoch-25 batch-87 = 0.00012945663183927536

Training epoch-25 batch-88
Running loss of epoch-25 batch-88 = 6.178440526127815e-05

Training epoch-25 batch-89
Running loss of epoch-25 batch-89 = 0.000252585974521935

Training epoch-25 batch-90
Running loss of epoch-25 batch-90 = 9.7277807071805e-05

Training epoch-25 batch-91
Running loss of epoch-25 batch-91 = 4.026084206998348e-05

Training epoch-25 batch-92
Running loss of epoch-25 batch-92 = 0.00024397089146077633

Training epoch-25 batch-93
Running loss of epoch-25 batch-93 = 0.0005213720723986626

Training epoch-25 batch-94
Running loss of epoch-25 batch-94 = 0.00012623891234397888

Training epoch-25 batch-95
Running loss of epoch-25 batch-95 = 8.848670404404402e-05

Training epoch-25 batch-96
Running loss of epoch-25 batch-96 = 0.00034893443807959557

Training epoch-25 batch-97
Running loss of epoch-25 batch-97 = 6.939750164747238e-05

Training epoch-25 batch-98
Running loss of epoch-25 batch-98 = 0.00032303761690855026

Training epoch-25 batch-99
Running loss of epoch-25 batch-99 = 0.00017718144226819277

Training epoch-25 batch-100
Running loss of epoch-25 batch-100 = 0.00022741092834621668

Training epoch-25 batch-101
Running loss of epoch-25 batch-101 = 0.0003722939873114228

Training epoch-25 batch-102
Running loss of epoch-25 batch-102 = 0.0004148965235799551

Training epoch-25 batch-103
Running loss of epoch-25 batch-103 = 6.340933032333851e-05

Training epoch-25 batch-104
Running loss of epoch-25 batch-104 = 0.00016410520765930414

Training epoch-25 batch-105
Running loss of epoch-25 batch-105 = 0.0006029611686244607

Training epoch-25 batch-106
Running loss of epoch-25 batch-106 = 0.00020914559718221426

Training epoch-25 batch-107
Running loss of epoch-25 batch-107 = 8.088722825050354e-05

Training epoch-25 batch-108
Running loss of epoch-25 batch-108 = 9.032106027007103e-05

Training epoch-25 batch-109
Running loss of epoch-25 batch-109 = 6.474810652434826e-05

Training epoch-25 batch-110
Running loss of epoch-25 batch-110 = 2.513127401471138e-05

Training epoch-25 batch-111
Running loss of epoch-25 batch-111 = 0.00010483642108738422

Training epoch-25 batch-112
Running loss of epoch-25 batch-112 = 0.0002521100686863065

Training epoch-25 batch-113
Running loss of epoch-25 batch-113 = 0.0005018594674766064

Training epoch-25 batch-114
Running loss of epoch-25 batch-114 = 8.49191565066576e-05

Training epoch-25 batch-115
Running loss of epoch-25 batch-115 = 0.00012301129754632711

Training epoch-25 batch-116
Running loss of epoch-25 batch-116 = 0.0001730003859847784

Training epoch-25 batch-117
Running loss of epoch-25 batch-117 = 0.00014406000263988972

Training epoch-25 batch-118
Running loss of epoch-25 batch-118 = 0.00034452404361218214

Training epoch-25 batch-119
Running loss of epoch-25 batch-119 = 0.00016522803343832493

Training epoch-25 batch-120
Running loss of epoch-25 batch-120 = 0.00012052943930029869

Training epoch-25 batch-121
Running loss of epoch-25 batch-121 = 0.00012757699005305767

Training epoch-25 batch-122
Running loss of epoch-25 batch-122 = 8.013425394892693e-05

Training epoch-25 batch-123
Running loss of epoch-25 batch-123 = 0.00029242713935673237

Training epoch-25 batch-124
Running loss of epoch-25 batch-124 = 0.00039854622446000576

Training epoch-25 batch-125
Running loss of epoch-25 batch-125 = 5.4504722356796265e-05

Training epoch-25 batch-126
Running loss of epoch-25 batch-126 = 0.00023263017646968365

Training epoch-25 batch-127
Running loss of epoch-25 batch-127 = 0.00010366924107074738

Training epoch-25 batch-128
Running loss of epoch-25 batch-128 = 0.00029341899789869785

Training epoch-25 batch-129
Running loss of epoch-25 batch-129 = 0.00019718310795724392

Training epoch-25 batch-130
Running loss of epoch-25 batch-130 = 0.0002444254932925105

Training epoch-25 batch-131
Running loss of epoch-25 batch-131 = 0.0004014321602880955

Training epoch-25 batch-132
Running loss of epoch-25 batch-132 = 4.6711647883057594e-05

Training epoch-25 batch-133
Running loss of epoch-25 batch-133 = 0.000254311366006732

Training epoch-25 batch-134
Running loss of epoch-25 batch-134 = 5.9920595958828926e-05

Training epoch-25 batch-135
Running loss of epoch-25 batch-135 = 0.0009004207095131278

Training epoch-25 batch-136
Running loss of epoch-25 batch-136 = 0.00017807947006076574

Training epoch-25 batch-137
Running loss of epoch-25 batch-137 = 0.00023150071501731873

Training epoch-25 batch-138
Running loss of epoch-25 batch-138 = 4.627159796655178e-05

Training epoch-25 batch-139
Running loss of epoch-25 batch-139 = 0.0002294951118528843

Training epoch-25 batch-140
Running loss of epoch-25 batch-140 = 0.00047582644037902355

Training epoch-25 batch-141
Running loss of epoch-25 batch-141 = 6.37935008853674e-05

Training epoch-25 batch-142
Running loss of epoch-25 batch-142 = 7.878453470766544e-05

Training epoch-25 batch-143
Running loss of epoch-25 batch-143 = 6.149732507765293e-05

Training epoch-25 batch-144
Running loss of epoch-25 batch-144 = 0.0002805751282721758

Training epoch-25 batch-145
Running loss of epoch-25 batch-145 = 0.0005170825170353055

Training epoch-25 batch-146
Running loss of epoch-25 batch-146 = 2.765585668385029e-05

Training epoch-25 batch-147
Running loss of epoch-25 batch-147 = 0.0002263028873130679

Training epoch-25 batch-148
Running loss of epoch-25 batch-148 = 0.0003486613277345896

Training epoch-25 batch-149
Running loss of epoch-25 batch-149 = 3.893882967531681e-05

Training epoch-25 batch-150
Running loss of epoch-25 batch-150 = 0.0004304456524550915

Training epoch-25 batch-151
Running loss of epoch-25 batch-151 = 9.20493621379137e-05

Training epoch-25 batch-152
Running loss of epoch-25 batch-152 = 0.00017030350863933563

Training epoch-25 batch-153
Running loss of epoch-25 batch-153 = 0.0002979743294417858

Training epoch-25 batch-154
Running loss of epoch-25 batch-154 = 0.0002748388797044754

Training epoch-25 batch-155
Running loss of epoch-25 batch-155 = 0.0006163271609693766

Training epoch-25 batch-156
Running loss of epoch-25 batch-156 = 0.0002574487589299679

Training epoch-25 batch-157
Running loss of epoch-25 batch-157 = 0.001750735566020012

Finished training epoch-25.



Average train loss at epoch-25 = 0.0002159083053469658

Started Evaluation

Average val loss at epoch-25 = 1.2570442971810305

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 95.08 %
Accuracy for class setUp is: 89.34 %
Accuracy for class onCreate is: 89.34 %
Accuracy for class toString is: 86.01 %
Accuracy for class run is: 66.44 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 49.10 %
Accuracy for class execute is: 55.02 %
Accuracy for class get is: 57.44 %

Overall Accuracy = 81.02 %

Finished Evaluation



Started training epoch-26


Training epoch-26 batch-1
Running loss of epoch-26 batch-1 = 0.0006895745173096657

Training epoch-26 batch-2
Running loss of epoch-26 batch-2 = 4.2022671550512314e-05

Training epoch-26 batch-3
Running loss of epoch-26 batch-3 = 0.0001811998663470149

Training epoch-26 batch-4
Running loss of epoch-26 batch-4 = 5.079037509858608e-05

Training epoch-26 batch-5
Running loss of epoch-26 batch-5 = 4.4530490413308144e-05

Training epoch-26 batch-6
Running loss of epoch-26 batch-6 = 0.0001932221930474043

Training epoch-26 batch-7
Running loss of epoch-26 batch-7 = 0.00010515004396438599

Training epoch-26 batch-8
Running loss of epoch-26 batch-8 = 6.703846156597137e-05

Training epoch-26 batch-9
Running loss of epoch-26 batch-9 = 0.00011994084343314171

Training epoch-26 batch-10
Running loss of epoch-26 batch-10 = 0.0004133497131988406

Training epoch-26 batch-11
Running loss of epoch-26 batch-11 = 8.95732082426548e-05

Training epoch-26 batch-12
Running loss of epoch-26 batch-12 = 3.607338294386864e-05

Training epoch-26 batch-13
Running loss of epoch-26 batch-13 = 0.00011628807988017797

Training epoch-26 batch-14
Running loss of epoch-26 batch-14 = 0.00020997761748731136

Training epoch-26 batch-15
Running loss of epoch-26 batch-15 = 0.00010781432501971722

Training epoch-26 batch-16
Running loss of epoch-26 batch-16 = 5.183031316846609e-05

Training epoch-26 batch-17
Running loss of epoch-26 batch-17 = 0.00031864887569099665

Training epoch-26 batch-18
Running loss of epoch-26 batch-18 = 0.00025666889268904924

Training epoch-26 batch-19
Running loss of epoch-26 batch-19 = 0.00011994130909442902

Training epoch-26 batch-20
Running loss of epoch-26 batch-20 = 0.00012134411372244358

Training epoch-26 batch-21
Running loss of epoch-26 batch-21 = 5.2270712330937386e-05

Training epoch-26 batch-22
Running loss of epoch-26 batch-22 = 8.004321716725826e-05

Training epoch-26 batch-23
Running loss of epoch-26 batch-23 = 9.551318362355232e-05

Training epoch-26 batch-24
Running loss of epoch-26 batch-24 = 5.577236879616976e-05

Training epoch-26 batch-25
Running loss of epoch-26 batch-25 = 0.0004775701090693474

Training epoch-26 batch-26
Running loss of epoch-26 batch-26 = 0.00011135777458548546

Training epoch-26 batch-27
Running loss of epoch-26 batch-27 = 4.575541242957115e-05

Training epoch-26 batch-28
Running loss of epoch-26 batch-28 = 6.546941585838795e-05

Training epoch-26 batch-29
Running loss of epoch-26 batch-29 = 0.00012720306403934956

Training epoch-26 batch-30
Running loss of epoch-26 batch-30 = 0.0002638143487274647

Training epoch-26 batch-31
Running loss of epoch-26 batch-31 = 0.0002577367704361677

Training epoch-26 batch-32
Running loss of epoch-26 batch-32 = 6.932881660759449e-05

Training epoch-26 batch-33
Running loss of epoch-26 batch-33 = 2.3382483050227165e-05

Training epoch-26 batch-34
Running loss of epoch-26 batch-34 = 0.00011061166878789663

Training epoch-26 batch-35
Running loss of epoch-26 batch-35 = 0.00010837591253221035

Training epoch-26 batch-36
Running loss of epoch-26 batch-36 = 6.647466216236353e-05

Training epoch-26 batch-37
Running loss of epoch-26 batch-37 = 7.218681275844574e-05

Training epoch-26 batch-38
Running loss of epoch-26 batch-38 = 0.00021692865993827581

Training epoch-26 batch-39
Running loss of epoch-26 batch-39 = 3.204448148608208e-05

Training epoch-26 batch-40
Running loss of epoch-26 batch-40 = 0.00014891615137457848

Training epoch-26 batch-41
Running loss of epoch-26 batch-41 = 9.465869516134262e-05

Training epoch-26 batch-42
Running loss of epoch-26 batch-42 = 0.000209550722502172

Training epoch-26 batch-43
Running loss of epoch-26 batch-43 = 0.00012265541590750217

Training epoch-26 batch-44
Running loss of epoch-26 batch-44 = 0.000300074927508831

Training epoch-26 batch-45
Running loss of epoch-26 batch-45 = 5.673652049154043e-05

Training epoch-26 batch-46
Running loss of epoch-26 batch-46 = 0.00021021266002207994

Training epoch-26 batch-47
Running loss of epoch-26 batch-47 = 0.00016774144023656845

Training epoch-26 batch-48
Running loss of epoch-26 batch-48 = 5.5323936976492405e-05

Training epoch-26 batch-49
Running loss of epoch-26 batch-49 = 6.0224905610084534e-05

Training epoch-26 batch-50
Running loss of epoch-26 batch-50 = 0.00014899414964020252

Training epoch-26 batch-51
Running loss of epoch-26 batch-51 = 0.00016539101488888264

Training epoch-26 batch-52
Running loss of epoch-26 batch-52 = 8.470891043543816e-05

Training epoch-26 batch-53
Running loss of epoch-26 batch-53 = 7.412699051201344e-05

Training epoch-26 batch-54
Running loss of epoch-26 batch-54 = 0.00010998104698956013

Training epoch-26 batch-55
Running loss of epoch-26 batch-55 = 7.000367622822523e-05

Training epoch-26 batch-56
Running loss of epoch-26 batch-56 = 0.0003328523598611355

Training epoch-26 batch-57
Running loss of epoch-26 batch-57 = 0.00042876251973211765

Training epoch-26 batch-58
Running loss of epoch-26 batch-58 = 0.00017387967091053724

Training epoch-26 batch-59
Running loss of epoch-26 batch-59 = 0.00011719251051545143

Training epoch-26 batch-60
Running loss of epoch-26 batch-60 = 0.00011801440268754959

Training epoch-26 batch-61
Running loss of epoch-26 batch-61 = 2.8231414034962654e-05

Training epoch-26 batch-62
Running loss of epoch-26 batch-62 = 5.897576920688152e-05

Training epoch-26 batch-63
Running loss of epoch-26 batch-63 = 8.257548324763775e-05

Training epoch-26 batch-64
Running loss of epoch-26 batch-64 = 9.166938252747059e-05

Training epoch-26 batch-65
Running loss of epoch-26 batch-65 = 0.00010655040387064219

Training epoch-26 batch-66
Running loss of epoch-26 batch-66 = 0.0002612255048006773

Training epoch-26 batch-67
Running loss of epoch-26 batch-67 = 7.218052633106709e-05

Training epoch-26 batch-68
Running loss of epoch-26 batch-68 = 0.00013438647147268057

Training epoch-26 batch-69
Running loss of epoch-26 batch-69 = 0.00012321327812969685

Training epoch-26 batch-70
Running loss of epoch-26 batch-70 = 0.0004483129596337676

Training epoch-26 batch-71
Running loss of epoch-26 batch-71 = 0.00023755058646202087

Training epoch-26 batch-72
Running loss of epoch-26 batch-72 = 0.00027322396636009216

Training epoch-26 batch-73
Running loss of epoch-26 batch-73 = 0.00010798173025250435

Training epoch-26 batch-74
Running loss of epoch-26 batch-74 = 0.0001380115281790495

Training epoch-26 batch-75
Running loss of epoch-26 batch-75 = 5.052727647125721e-05

Training epoch-26 batch-76
Running loss of epoch-26 batch-76 = 0.0002727530663833022

Training epoch-26 batch-77
Running loss of epoch-26 batch-77 = 5.777529440820217e-05

Training epoch-26 batch-78
Running loss of epoch-26 batch-78 = 6.936094723641872e-05

Training epoch-26 batch-79
Running loss of epoch-26 batch-79 = 0.00010526273399591446

Training epoch-26 batch-80
Running loss of epoch-26 batch-80 = 5.1659299060702324e-05

Training epoch-26 batch-81
Running loss of epoch-26 batch-81 = 4.001404158771038e-05

Training epoch-26 batch-82
Running loss of epoch-26 batch-82 = 0.0009333777707070112

Training epoch-26 batch-83
Running loss of epoch-26 batch-83 = 0.0001255954848602414

Training epoch-26 batch-84
Running loss of epoch-26 batch-84 = 7.389509119093418e-05

Training epoch-26 batch-85
Running loss of epoch-26 batch-85 = 0.00012530572712421417

Training epoch-26 batch-86
Running loss of epoch-26 batch-86 = 0.00010298914276063442

Training epoch-26 batch-87
Running loss of epoch-26 batch-87 = 4.815380088984966e-05

Training epoch-26 batch-88
Running loss of epoch-26 batch-88 = 0.00028448179364204407

Training epoch-26 batch-89
Running loss of epoch-26 batch-89 = 0.00039241311606019735

Training epoch-26 batch-90
Running loss of epoch-26 batch-90 = 0.00010430184192955494

Training epoch-26 batch-91
Running loss of epoch-26 batch-91 = 0.00022227142471820116

Training epoch-26 batch-92
Running loss of epoch-26 batch-92 = 5.5388547480106354e-05

Training epoch-26 batch-93
Running loss of epoch-26 batch-93 = 0.00011037057265639305

Training epoch-26 batch-94
Running loss of epoch-26 batch-94 = 0.00010230368934571743

Training epoch-26 batch-95
Running loss of epoch-26 batch-95 = 0.00029479898512363434

Training epoch-26 batch-96
Running loss of epoch-26 batch-96 = 0.00011213927064090967

Training epoch-26 batch-97
Running loss of epoch-26 batch-97 = 0.0004781126044690609

Training epoch-26 batch-98
Running loss of epoch-26 batch-98 = 0.0002049066824838519

Training epoch-26 batch-99
Running loss of epoch-26 batch-99 = 9.82221681624651e-05

Training epoch-26 batch-100
Running loss of epoch-26 batch-100 = 5.720788612961769e-05

Training epoch-26 batch-101
Running loss of epoch-26 batch-101 = 7.607182487845421e-05

Training epoch-26 batch-102
Running loss of epoch-26 batch-102 = 0.00016669323667883873

Training epoch-26 batch-103
Running loss of epoch-26 batch-103 = 8.156197145581245e-05

Training epoch-26 batch-104
Running loss of epoch-26 batch-104 = 9.526079520583153e-05

Training epoch-26 batch-105
Running loss of epoch-26 batch-105 = 9.649991989135742e-05

Training epoch-26 batch-106
Running loss of epoch-26 batch-106 = 0.00015522935427725315

Training epoch-26 batch-107
Running loss of epoch-26 batch-107 = 8.362368680536747e-05

Training epoch-26 batch-108
Running loss of epoch-26 batch-108 = 0.00011786911636590958

Training epoch-26 batch-109
Running loss of epoch-26 batch-109 = 0.00021195132285356522

Training epoch-26 batch-110
Running loss of epoch-26 batch-110 = 0.00022320589050650597

Training epoch-26 batch-111
Running loss of epoch-26 batch-111 = 0.00016530253924429417

Training epoch-26 batch-112
Running loss of epoch-26 batch-112 = 5.581486038863659e-05

Training epoch-26 batch-113
Running loss of epoch-26 batch-113 = 0.0001319290604442358

Training epoch-26 batch-114
Running loss of epoch-26 batch-114 = 9.422143921256065e-05

Training epoch-26 batch-115
Running loss of epoch-26 batch-115 = 7.651001214981079e-05

Training epoch-26 batch-116
Running loss of epoch-26 batch-116 = 3.25243454426527e-05

Training epoch-26 batch-117
Running loss of epoch-26 batch-117 = 7.517845369875431e-05

Training epoch-26 batch-118
Running loss of epoch-26 batch-118 = 0.0003447218332439661

Training epoch-26 batch-119
Running loss of epoch-26 batch-119 = 9.651039727032185e-05

Training epoch-26 batch-120
Running loss of epoch-26 batch-120 = 0.00010361999738961458

Training epoch-26 batch-121
Running loss of epoch-26 batch-121 = 7.234280928969383e-05

Training epoch-26 batch-122
Running loss of epoch-26 batch-122 = 0.00020524731371551752

Training epoch-26 batch-123
Running loss of epoch-26 batch-123 = 8.262088522315025e-05

Training epoch-26 batch-124
Running loss of epoch-26 batch-124 = 6.0893362388014793e-05

Training epoch-26 batch-125
Running loss of epoch-26 batch-125 = 3.77981923520565e-05

Training epoch-26 batch-126
Running loss of epoch-26 batch-126 = 0.0002645346103236079

Training epoch-26 batch-127
Running loss of epoch-26 batch-127 = 0.00017323740758001804

Training epoch-26 batch-128
Running loss of epoch-26 batch-128 = 9.539606980979443e-05

Training epoch-26 batch-129
Running loss of epoch-26 batch-129 = 7.581966929137707e-05

Training epoch-26 batch-130
Running loss of epoch-26 batch-130 = 0.00016937986947596073

Training epoch-26 batch-131
Running loss of epoch-26 batch-131 = 4.605739377439022e-05

Training epoch-26 batch-132
Running loss of epoch-26 batch-132 = 0.00013457611203193665

Training epoch-26 batch-133
Running loss of epoch-26 batch-133 = 0.00010492396540939808

Training epoch-26 batch-134
Running loss of epoch-26 batch-134 = 9.490502998232841e-05

Training epoch-26 batch-135
Running loss of epoch-26 batch-135 = 0.00015459535643458366

Training epoch-26 batch-136
Running loss of epoch-26 batch-136 = 6.354972720146179e-05

Training epoch-26 batch-137
Running loss of epoch-26 batch-137 = 0.00024416809901595116

Training epoch-26 batch-138
Running loss of epoch-26 batch-138 = 4.269205965101719e-05

Training epoch-26 batch-139
Running loss of epoch-26 batch-139 = 6.911600939929485e-05

Training epoch-26 batch-140
Running loss of epoch-26 batch-140 = 5.6875403970479965e-05

Training epoch-26 batch-141
Running loss of epoch-26 batch-141 = 0.0005078110843896866

Training epoch-26 batch-142
Running loss of epoch-26 batch-142 = 4.688999615609646e-05

Training epoch-26 batch-143
Running loss of epoch-26 batch-143 = 5.1237642765045166e-05

Training epoch-26 batch-144
Running loss of epoch-26 batch-144 = 0.00012307544238865376

Training epoch-26 batch-145
Running loss of epoch-26 batch-145 = 3.8317637518048286e-05

Training epoch-26 batch-146
Running loss of epoch-26 batch-146 = 0.00021798990201205015

Training epoch-26 batch-147
Running loss of epoch-26 batch-147 = 0.00014485616702586412

Training epoch-26 batch-148
Running loss of epoch-26 batch-148 = 0.0001486780820414424

Training epoch-26 batch-149
Running loss of epoch-26 batch-149 = 0.000109449727460742

Training epoch-26 batch-150
Running loss of epoch-26 batch-150 = 0.0001575013156980276

Training epoch-26 batch-151
Running loss of epoch-26 batch-151 = 6.5632164478302e-05

Training epoch-26 batch-152
Running loss of epoch-26 batch-152 = 0.00022453605197370052

Training epoch-26 batch-153
Running loss of epoch-26 batch-153 = 5.198176950216293e-05

Training epoch-26 batch-154
Running loss of epoch-26 batch-154 = 0.00014466466382145882

Training epoch-26 batch-155
Running loss of epoch-26 batch-155 = 3.23946587741375e-05

Training epoch-26 batch-156
Running loss of epoch-26 batch-156 = 3.097834996879101e-05

Training epoch-26 batch-157
Running loss of epoch-26 batch-157 = 0.00041020289063453674

Finished training epoch-26.



Average train loss at epoch-26 = 0.00014455087259411813

Started Evaluation

Average val loss at epoch-26 = 1.3113802106236885

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 90.62 %
Accuracy for class toString is: 84.30 %
Accuracy for class run is: 66.44 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 44.62 %
Accuracy for class execute is: 56.22 %
Accuracy for class get is: 59.49 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-27


Training epoch-27 batch-1
Running loss of epoch-27 batch-1 = 3.434508107602596e-05

Training epoch-27 batch-2
Running loss of epoch-27 batch-2 = 3.900798037648201e-05

Training epoch-27 batch-3
Running loss of epoch-27 batch-3 = 0.0001255064271390438

Training epoch-27 batch-4
Running loss of epoch-27 batch-4 = 9.024981409311295e-05

Training epoch-27 batch-5
Running loss of epoch-27 batch-5 = 4.8338668420910835e-05

Training epoch-27 batch-6
Running loss of epoch-27 batch-6 = 9.038508869707584e-05

Training epoch-27 batch-7
Running loss of epoch-27 batch-7 = 0.0002286817179992795

Training epoch-27 batch-8
Running loss of epoch-27 batch-8 = 6.746710278093815e-05

Training epoch-27 batch-9
Running loss of epoch-27 batch-9 = 0.00010168307926505804

Training epoch-27 batch-10
Running loss of epoch-27 batch-10 = 7.098354399204254e-05

Training epoch-27 batch-11
Running loss of epoch-27 batch-11 = 0.00011807179544121027

Training epoch-27 batch-12
Running loss of epoch-27 batch-12 = 0.0002612452954053879

Training epoch-27 batch-13
Running loss of epoch-27 batch-13 = 6.18472695350647e-05

Training epoch-27 batch-14
Running loss of epoch-27 batch-14 = 8.82718013599515e-05

Training epoch-27 batch-15
Running loss of epoch-27 batch-15 = 3.1196512281894684e-05

Training epoch-27 batch-16
Running loss of epoch-27 batch-16 = 0.00020597362890839577

Training epoch-27 batch-17
Running loss of epoch-27 batch-17 = 1.1964235454797745e-05

Training epoch-27 batch-18
Running loss of epoch-27 batch-18 = 5.551031790673733e-05

Training epoch-27 batch-19
Running loss of epoch-27 batch-19 = 5.5563170462846756e-05

Training epoch-27 batch-20
Running loss of epoch-27 batch-20 = 4.101055674254894e-05

Training epoch-27 batch-21
Running loss of epoch-27 batch-21 = 0.00010263966396450996

Training epoch-27 batch-22
Running loss of epoch-27 batch-22 = 3.8506463170051575e-05

Training epoch-27 batch-23
Running loss of epoch-27 batch-23 = 0.0003747805021703243

Training epoch-27 batch-24
Running loss of epoch-27 batch-24 = 0.00042380625382065773

Training epoch-27 batch-25
Running loss of epoch-27 batch-25 = 9.123515337705612e-05

Training epoch-27 batch-26
Running loss of epoch-27 batch-26 = 9.132176637649536e-05

Training epoch-27 batch-27
Running loss of epoch-27 batch-27 = 5.054892972111702e-05

Training epoch-27 batch-28
Running loss of epoch-27 batch-28 = 0.00013635249342769384

Training epoch-27 batch-29
Running loss of epoch-27 batch-29 = 0.00010567391291260719

Training epoch-27 batch-30
Running loss of epoch-27 batch-30 = 5.127512849867344e-05

Training epoch-27 batch-31
Running loss of epoch-27 batch-31 = 4.2521627619862556e-05

Training epoch-27 batch-32
Running loss of epoch-27 batch-32 = 0.0001189142931252718

Training epoch-27 batch-33
Running loss of epoch-27 batch-33 = 0.0001062414376065135

Training epoch-27 batch-34
Running loss of epoch-27 batch-34 = 6.848876364529133e-05

Training epoch-27 batch-35
Running loss of epoch-27 batch-35 = 0.00018353434279561043

Training epoch-27 batch-36
Running loss of epoch-27 batch-36 = 0.00017689494416117668

Training epoch-27 batch-37
Running loss of epoch-27 batch-37 = 0.0001410655677318573

Training epoch-27 batch-38
Running loss of epoch-27 batch-38 = 8.969311602413654e-05

Training epoch-27 batch-39
Running loss of epoch-27 batch-39 = 8.742697536945343e-05

Training epoch-27 batch-40
Running loss of epoch-27 batch-40 = 8.723873179405928e-05

Training epoch-27 batch-41
Running loss of epoch-27 batch-41 = 3.820890560746193e-05

Training epoch-27 batch-42
Running loss of epoch-27 batch-42 = 9.794509969651699e-05

Training epoch-27 batch-43
Running loss of epoch-27 batch-43 = 5.356082692742348e-05

Training epoch-27 batch-44
Running loss of epoch-27 batch-44 = 0.0001670552883297205

Training epoch-27 batch-45
Running loss of epoch-27 batch-45 = 7.265107706189156e-05

Training epoch-27 batch-46
Running loss of epoch-27 batch-46 = 3.721844404935837e-05

Training epoch-27 batch-47
Running loss of epoch-27 batch-47 = 0.00027385144494473934

Training epoch-27 batch-48
Running loss of epoch-27 batch-48 = 4.2265746742486954e-05

Training epoch-27 batch-49
Running loss of epoch-27 batch-49 = 7.002369966357946e-05

Training epoch-27 batch-50
Running loss of epoch-27 batch-50 = 7.204653229564428e-05

Training epoch-27 batch-51
Running loss of epoch-27 batch-51 = 8.709460962563753e-05

Training epoch-27 batch-52
Running loss of epoch-27 batch-52 = 4.583713598549366e-05

Training epoch-27 batch-53
Running loss of epoch-27 batch-53 = 6.384728476405144e-05

Training epoch-27 batch-54
Running loss of epoch-27 batch-54 = 0.00015875417739152908

Training epoch-27 batch-55
Running loss of epoch-27 batch-55 = 4.642875865101814e-05

Training epoch-27 batch-56
Running loss of epoch-27 batch-56 = 0.0003258553333580494

Training epoch-27 batch-57
Running loss of epoch-27 batch-57 = 8.003786206245422e-05

Training epoch-27 batch-58
Running loss of epoch-27 batch-58 = 0.00014309457037597895

Training epoch-27 batch-59
Running loss of epoch-27 batch-59 = 0.00011132657527923584

Training epoch-27 batch-60
Running loss of epoch-27 batch-60 = 0.00027790991589426994

Training epoch-27 batch-61
Running loss of epoch-27 batch-61 = 8.893921039998531e-05

Training epoch-27 batch-62
Running loss of epoch-27 batch-62 = 0.0001627106685191393

Training epoch-27 batch-63
Running loss of epoch-27 batch-63 = 6.425206083804369e-05

Training epoch-27 batch-64
Running loss of epoch-27 batch-64 = 0.00015538535080850124

Training epoch-27 batch-65
Running loss of epoch-27 batch-65 = 0.00012549827806651592

Training epoch-27 batch-66
Running loss of epoch-27 batch-66 = 7.594493217766285e-05

Training epoch-27 batch-67
Running loss of epoch-27 batch-67 = 5.4227421060204506e-05

Training epoch-27 batch-68
Running loss of epoch-27 batch-68 = 0.00013232696801424026

Training epoch-27 batch-69
Running loss of epoch-27 batch-69 = 8.517387323081493e-05

Training epoch-27 batch-70
Running loss of epoch-27 batch-70 = 1.9902363419532776e-05

Training epoch-27 batch-71
Running loss of epoch-27 batch-71 = 0.00015559745952486992

Training epoch-27 batch-72
Running loss of epoch-27 batch-72 = 6.020721048116684e-05

Training epoch-27 batch-73
Running loss of epoch-27 batch-73 = 0.00011772476136684418

Training epoch-27 batch-74
Running loss of epoch-27 batch-74 = 5.3440104238688946e-05

Training epoch-27 batch-75
Running loss of epoch-27 batch-75 = 4.278006963431835e-05

Training epoch-27 batch-76
Running loss of epoch-27 batch-76 = 8.068140596151352e-05

Training epoch-27 batch-77
Running loss of epoch-27 batch-77 = 0.0003033246612176299

Training epoch-27 batch-78
Running loss of epoch-27 batch-78 = 2.5742221623659134e-05

Training epoch-27 batch-79
Running loss of epoch-27 batch-79 = 2.554641105234623e-05

Training epoch-27 batch-80
Running loss of epoch-27 batch-80 = 6.43834937363863e-05

Training epoch-27 batch-81
Running loss of epoch-27 batch-81 = 4.6929228119552135e-05

Training epoch-27 batch-82
Running loss of epoch-27 batch-82 = 6.698269862681627e-05

Training epoch-27 batch-83
Running loss of epoch-27 batch-83 = 0.00013219926040619612

Training epoch-27 batch-84
Running loss of epoch-27 batch-84 = 0.0001530214212834835

Training epoch-27 batch-85
Running loss of epoch-27 batch-85 = 9.378907270729542e-05

Training epoch-27 batch-86
Running loss of epoch-27 batch-86 = 0.00010652909986674786

Training epoch-27 batch-87
Running loss of epoch-27 batch-87 = 6.127939559519291e-05

Training epoch-27 batch-88
Running loss of epoch-27 batch-88 = 4.680571146309376e-05

Training epoch-27 batch-89
Running loss of epoch-27 batch-89 = 0.0002913466887548566

Training epoch-27 batch-90
Running loss of epoch-27 batch-90 = 5.254428833723068e-05

Training epoch-27 batch-91
Running loss of epoch-27 batch-91 = 4.4432468712329865e-05

Training epoch-27 batch-92
Running loss of epoch-27 batch-92 = 0.00011223845649510622

Training epoch-27 batch-93
Running loss of epoch-27 batch-93 = 8.003576658666134e-05

Training epoch-27 batch-94
Running loss of epoch-27 batch-94 = 5.443533882498741e-05

Training epoch-27 batch-95
Running loss of epoch-27 batch-95 = 8.146138861775398e-05

Training epoch-27 batch-96
Running loss of epoch-27 batch-96 = 6.282678805291653e-05

Training epoch-27 batch-97
Running loss of epoch-27 batch-97 = 0.00010562548413872719

Training epoch-27 batch-98
Running loss of epoch-27 batch-98 = 4.667160101234913e-05

Training epoch-27 batch-99
Running loss of epoch-27 batch-99 = 9.71697736531496e-05

Training epoch-27 batch-100
Running loss of epoch-27 batch-100 = 6.571738049387932e-05

Training epoch-27 batch-101
Running loss of epoch-27 batch-101 = 2.9126298613846302e-05

Training epoch-27 batch-102
Running loss of epoch-27 batch-102 = 0.00011414347682148218

Training epoch-27 batch-103
Running loss of epoch-27 batch-103 = 6.793113425374031e-05

Training epoch-27 batch-104
Running loss of epoch-27 batch-104 = 4.95340209454298e-05

Training epoch-27 batch-105
Running loss of epoch-27 batch-105 = 6.028241477906704e-05

Training epoch-27 batch-106
Running loss of epoch-27 batch-106 = 0.00022426946088671684

Training epoch-27 batch-107
Running loss of epoch-27 batch-107 = 7.069390267133713e-05

Training epoch-27 batch-108
Running loss of epoch-27 batch-108 = 2.2051390260457993e-05

Training epoch-27 batch-109
Running loss of epoch-27 batch-109 = 0.00010379101149737835

Training epoch-27 batch-110
Running loss of epoch-27 batch-110 = 1.3159122318029404e-05

Training epoch-27 batch-111
Running loss of epoch-27 batch-111 = 4.3632928282022476e-05

Training epoch-27 batch-112
Running loss of epoch-27 batch-112 = 2.5471323169767857e-05

Training epoch-27 batch-113
Running loss of epoch-27 batch-113 = 6.145844236016273e-05

Training epoch-27 batch-114
Running loss of epoch-27 batch-114 = 0.0006443840684369206

Training epoch-27 batch-115
Running loss of epoch-27 batch-115 = 0.00011529144831001759

Training epoch-27 batch-116
Running loss of epoch-27 batch-116 = 3.642146475613117e-05

Training epoch-27 batch-117
Running loss of epoch-27 batch-117 = 6.797630339860916e-05

Training epoch-27 batch-118
Running loss of epoch-27 batch-118 = 0.00011526455637067556

Training epoch-27 batch-119
Running loss of epoch-27 batch-119 = 0.000279904343187809

Training epoch-27 batch-120
Running loss of epoch-27 batch-120 = 5.2955118007957935e-05

Training epoch-27 batch-121
Running loss of epoch-27 batch-121 = 8.96393321454525e-05

Training epoch-27 batch-122
Running loss of epoch-27 batch-122 = 0.00013152044266462326

Training epoch-27 batch-123
Running loss of epoch-27 batch-123 = 7.71981431171298e-05

Training epoch-27 batch-124
Running loss of epoch-27 batch-124 = 7.109390571713448e-05

Training epoch-27 batch-125
Running loss of epoch-27 batch-125 = 4.0201470255851746e-05

Training epoch-27 batch-126
Running loss of epoch-27 batch-126 = 6.458151619881392e-05

Training epoch-27 batch-127
Running loss of epoch-27 batch-127 = 0.00010410090908408165

Training epoch-27 batch-128
Running loss of epoch-27 batch-128 = 1.8786638975143433e-05

Training epoch-27 batch-129
Running loss of epoch-27 batch-129 = 3.980263136327267e-05

Training epoch-27 batch-130
Running loss of epoch-27 batch-130 = 6.345624569803476e-05

Training epoch-27 batch-131
Running loss of epoch-27 batch-131 = 5.223648622632027e-05

Training epoch-27 batch-132
Running loss of epoch-27 batch-132 = 7.40287359803915e-05

Training epoch-27 batch-133
Running loss of epoch-27 batch-133 = 8.032482583075762e-05

Training epoch-27 batch-134
Running loss of epoch-27 batch-134 = 0.00010172335896641016

Training epoch-27 batch-135
Running loss of epoch-27 batch-135 = 0.00020206801127642393

Training epoch-27 batch-136
Running loss of epoch-27 batch-136 = 0.00010407203808426857

Training epoch-27 batch-137
Running loss of epoch-27 batch-137 = 4.304945468902588e-05

Training epoch-27 batch-138
Running loss of epoch-27 batch-138 = 4.4653890654444695e-05

Training epoch-27 batch-139
Running loss of epoch-27 batch-139 = 1.881527714431286e-05

Training epoch-27 batch-140
Running loss of epoch-27 batch-140 = 0.0002964235609397292

Training epoch-27 batch-141
Running loss of epoch-27 batch-141 = 2.5258399546146393e-05

Training epoch-27 batch-142
Running loss of epoch-27 batch-142 = 0.00030055141542106867

Training epoch-27 batch-143
Running loss of epoch-27 batch-143 = 2.0944513380527496e-05

Training epoch-27 batch-144
Running loss of epoch-27 batch-144 = 0.00010346272028982639

Training epoch-27 batch-145
Running loss of epoch-27 batch-145 = 0.00046692206524312496

Training epoch-27 batch-146
Running loss of epoch-27 batch-146 = 0.0002581028966233134

Training epoch-27 batch-147
Running loss of epoch-27 batch-147 = 0.00016409414820373058

Training epoch-27 batch-148
Running loss of epoch-27 batch-148 = 0.0001354312989860773

Training epoch-27 batch-149
Running loss of epoch-27 batch-149 = 4.056654870510101e-05

Training epoch-27 batch-150
Running loss of epoch-27 batch-150 = 5.5799027904868126e-05

Training epoch-27 batch-151
Running loss of epoch-27 batch-151 = 0.00034016824793070555

Training epoch-27 batch-152
Running loss of epoch-27 batch-152 = 0.00023078895173966885

Training epoch-27 batch-153
Running loss of epoch-27 batch-153 = 0.00023205531761050224

Training epoch-27 batch-154
Running loss of epoch-27 batch-154 = 0.00024043768644332886

Training epoch-27 batch-155
Running loss of epoch-27 batch-155 = 6.622052751481533e-05

Training epoch-27 batch-156
Running loss of epoch-27 batch-156 = 7.983553223311901e-05

Training epoch-27 batch-157
Running loss of epoch-27 batch-157 = 4.040822386741638e-05

Finished training epoch-27.



Average train loss at epoch-27 = 0.00010907242968678474

Started Evaluation

Average val loss at epoch-27 = 1.2856719870736637

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 90.09 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 49.78 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 64.36 %

Overall Accuracy = 81.47 %

Finished Evaluation



Started training epoch-28


Training epoch-28 batch-1
Running loss of epoch-28 batch-1 = 0.00010439078323543072

Training epoch-28 batch-2
Running loss of epoch-28 batch-2 = 4.3152133002877235e-05

Training epoch-28 batch-3
Running loss of epoch-28 batch-3 = 2.8028851374983788e-05

Training epoch-28 batch-4
Running loss of epoch-28 batch-4 = 2.181599847972393e-05

Training epoch-28 batch-5
Running loss of epoch-28 batch-5 = 9.842077270150185e-05

Training epoch-28 batch-6
Running loss of epoch-28 batch-6 = 0.00033705937676131725

Training epoch-28 batch-7
Running loss of epoch-28 batch-7 = 4.434725269675255e-05

Training epoch-28 batch-8
Running loss of epoch-28 batch-8 = 7.47418962419033e-05

Training epoch-28 batch-9
Running loss of epoch-28 batch-9 = 7.499288767576218e-05

Training epoch-28 batch-10
Running loss of epoch-28 batch-10 = 5.314359441399574e-05

Training epoch-28 batch-11
Running loss of epoch-28 batch-11 = 8.511636406183243e-05

Training epoch-28 batch-12
Running loss of epoch-28 batch-12 = 6.0088932514190674e-05

Training epoch-28 batch-13
Running loss of epoch-28 batch-13 = 4.149950109422207e-05

Training epoch-28 batch-14
Running loss of epoch-28 batch-14 = 5.608820356428623e-05

Training epoch-28 batch-15
Running loss of epoch-28 batch-15 = 2.8979498893022537e-05

Training epoch-28 batch-16
Running loss of epoch-28 batch-16 = 0.00018474948592483997

Training epoch-28 batch-17
Running loss of epoch-28 batch-17 = 0.00012395833618938923

Training epoch-28 batch-18
Running loss of epoch-28 batch-18 = 0.00016530952416360378

Training epoch-28 batch-19
Running loss of epoch-28 batch-19 = 0.0003740767715498805

Training epoch-28 batch-20
Running loss of epoch-28 batch-20 = 5.3508905693888664e-05

Training epoch-28 batch-21
Running loss of epoch-28 batch-21 = 5.2334973588585854e-05

Training epoch-28 batch-22
Running loss of epoch-28 batch-22 = 0.00011912989430129528

Training epoch-28 batch-23
Running loss of epoch-28 batch-23 = 6.649340502917767e-05

Training epoch-28 batch-24
Running loss of epoch-28 batch-24 = 3.979308530688286e-05

Training epoch-28 batch-25
Running loss of epoch-28 batch-25 = 0.0001459664199501276

Training epoch-28 batch-26
Running loss of epoch-28 batch-26 = 0.00012707780115306377

Training epoch-28 batch-27
Running loss of epoch-28 batch-27 = 8.676736615598202e-05

Training epoch-28 batch-28
Running loss of epoch-28 batch-28 = 6.687827408313751e-05

Training epoch-28 batch-29
Running loss of epoch-28 batch-29 = 3.5318778827786446e-05

Training epoch-28 batch-30
Running loss of epoch-28 batch-30 = 0.00019798788707703352

Training epoch-28 batch-31
Running loss of epoch-28 batch-31 = 3.454764373600483e-05

Training epoch-28 batch-32
Running loss of epoch-28 batch-32 = 8.763477671891451e-05

Training epoch-28 batch-33
Running loss of epoch-28 batch-33 = 7.759244181215763e-05

Training epoch-28 batch-34
Running loss of epoch-28 batch-34 = 6.440747529268265e-05

Training epoch-28 batch-35
Running loss of epoch-28 batch-35 = 3.802403807640076e-05

Training epoch-28 batch-36
Running loss of epoch-28 batch-36 = 5.124497693032026e-05

Training epoch-28 batch-37
Running loss of epoch-28 batch-37 = 5.191308446228504e-05

Training epoch-28 batch-38
Running loss of epoch-28 batch-38 = 4.7017354518175125e-05

Training epoch-28 batch-39
Running loss of epoch-28 batch-39 = 1.6548670828342438e-05

Training epoch-28 batch-40
Running loss of epoch-28 batch-40 = 2.3375730961561203e-05

Training epoch-28 batch-41
Running loss of epoch-28 batch-41 = 9.180628694593906e-05

Training epoch-28 batch-42
Running loss of epoch-28 batch-42 = 6.0145044699311256e-05

Training epoch-28 batch-43
Running loss of epoch-28 batch-43 = 5.4987380281090736e-05

Training epoch-28 batch-44
Running loss of epoch-28 batch-44 = 0.0001076145563274622

Training epoch-28 batch-45
Running loss of epoch-28 batch-45 = 0.00022612675093114376

Training epoch-28 batch-46
Running loss of epoch-28 batch-46 = 2.7237925678491592e-05

Training epoch-28 batch-47
Running loss of epoch-28 batch-47 = 6.960681639611721e-05

Training epoch-28 batch-48
Running loss of epoch-28 batch-48 = 0.00011843070387840271

Training epoch-28 batch-49
Running loss of epoch-28 batch-49 = 5.6128017604351044e-05

Training epoch-28 batch-50
Running loss of epoch-28 batch-50 = 5.241832695901394e-05

Training epoch-28 batch-51
Running loss of epoch-28 batch-51 = 4.335748963057995e-05

Training epoch-28 batch-52
Running loss of epoch-28 batch-52 = 9.853183291852474e-05

Training epoch-28 batch-53
Running loss of epoch-28 batch-53 = 4.2294152081012726e-05

Training epoch-28 batch-54
Running loss of epoch-28 batch-54 = 2.6771798729896545e-05

Training epoch-28 batch-55
Running loss of epoch-28 batch-55 = 1.7893267795443535e-05

Training epoch-28 batch-56
Running loss of epoch-28 batch-56 = 3.810075577348471e-05

Training epoch-28 batch-57
Running loss of epoch-28 batch-57 = 0.0002588285133242607

Training epoch-28 batch-58
Running loss of epoch-28 batch-58 = 4.706718027591705e-05

Training epoch-28 batch-59
Running loss of epoch-28 batch-59 = 5.8088917285203934e-05

Training epoch-28 batch-60
Running loss of epoch-28 batch-60 = 3.402656875550747e-05

Training epoch-28 batch-61
Running loss of epoch-28 batch-61 = 4.816462751477957e-05

Training epoch-28 batch-62
Running loss of epoch-28 batch-62 = 5.219422746449709e-05

Training epoch-28 batch-63
Running loss of epoch-28 batch-63 = 8.740671910345554e-05

Training epoch-28 batch-64
Running loss of epoch-28 batch-64 = 0.0002699007745832205

Training epoch-28 batch-65
Running loss of epoch-28 batch-65 = 9.701272938400507e-05

Training epoch-28 batch-66
Running loss of epoch-28 batch-66 = 5.970743950456381e-05

Training epoch-28 batch-67
Running loss of epoch-28 batch-67 = 4.017772153019905e-05

Training epoch-28 batch-68
Running loss of epoch-28 batch-68 = 0.00010513490997254848

Training epoch-28 batch-69
Running loss of epoch-28 batch-69 = 8.172355592250824e-06

Training epoch-28 batch-70
Running loss of epoch-28 batch-70 = 9.846058674156666e-05

Training epoch-28 batch-71
Running loss of epoch-28 batch-71 = 4.591757897287607e-05

Training epoch-28 batch-72
Running loss of epoch-28 batch-72 = 4.498381167650223e-05

Training epoch-28 batch-73
Running loss of epoch-28 batch-73 = 3.800122067332268e-05

Training epoch-28 batch-74
Running loss of epoch-28 batch-74 = 7.905054371803999e-05

Training epoch-28 batch-75
Running loss of epoch-28 batch-75 = 0.00012481678277254105

Training epoch-28 batch-76
Running loss of epoch-28 batch-76 = 0.00016126129776239395

Training epoch-28 batch-77
Running loss of epoch-28 batch-77 = 0.00010548799764364958

Training epoch-28 batch-78
Running loss of epoch-28 batch-78 = 5.864084232598543e-05

Training epoch-28 batch-79
Running loss of epoch-28 batch-79 = 5.3242314606904984e-05

Training epoch-28 batch-80
Running loss of epoch-28 batch-80 = 0.00014296628069132566

Training epoch-28 batch-81
Running loss of epoch-28 batch-81 = 4.017096944153309e-05

Training epoch-28 batch-82
Running loss of epoch-28 batch-82 = 0.0001018389593809843

Training epoch-28 batch-83
Running loss of epoch-28 batch-83 = 5.223741754889488e-05

Training epoch-28 batch-84
Running loss of epoch-28 batch-84 = 2.6058172807097435e-05

Training epoch-28 batch-85
Running loss of epoch-28 batch-85 = 1.6985926777124405e-05

Training epoch-28 batch-86
Running loss of epoch-28 batch-86 = 8.28942283987999e-05

Training epoch-28 batch-87
Running loss of epoch-28 batch-87 = 4.3883686885237694e-05

Training epoch-28 batch-88
Running loss of epoch-28 batch-88 = 6.454065442085266e-05

Training epoch-28 batch-89
Running loss of epoch-28 batch-89 = 4.745437763631344e-05

Training epoch-28 batch-90
Running loss of epoch-28 batch-90 = 5.8303819969296455e-05

Training epoch-28 batch-91
Running loss of epoch-28 batch-91 = 0.00015240930952131748

Training epoch-28 batch-92
Running loss of epoch-28 batch-92 = 6.732577458024025e-05

Training epoch-28 batch-93
Running loss of epoch-28 batch-93 = 2.2695166990160942e-05

Training epoch-28 batch-94
Running loss of epoch-28 batch-94 = 4.4562388211488724e-05

Training epoch-28 batch-95
Running loss of epoch-28 batch-95 = 1.6384292393922806e-05

Training epoch-28 batch-96
Running loss of epoch-28 batch-96 = 0.00011978158727288246

Training epoch-28 batch-97
Running loss of epoch-28 batch-97 = 8.780253119766712e-05

Training epoch-28 batch-98
Running loss of epoch-28 batch-98 = 2.774852328002453e-05

Training epoch-28 batch-99
Running loss of epoch-28 batch-99 = 0.00019687600433826447

Training epoch-28 batch-100
Running loss of epoch-28 batch-100 = 0.00010431278496980667

Training epoch-28 batch-101
Running loss of epoch-28 batch-101 = 5.547981709241867e-05

Training epoch-28 batch-102
Running loss of epoch-28 batch-102 = 3.329396713525057e-05

Training epoch-28 batch-103
Running loss of epoch-28 batch-103 = 3.094109706580639e-05

Training epoch-28 batch-104
Running loss of epoch-28 batch-104 = 4.731304943561554e-05

Training epoch-28 batch-105
Running loss of epoch-28 batch-105 = 2.9374146834015846e-05

Training epoch-28 batch-106
Running loss of epoch-28 batch-106 = 9.054201655089855e-05

Training epoch-28 batch-107
Running loss of epoch-28 batch-107 = 6.667640991508961e-05

Training epoch-28 batch-108
Running loss of epoch-28 batch-108 = 0.00014640344306826591

Training epoch-28 batch-109
Running loss of epoch-28 batch-109 = 4.0552811697125435e-05

Training epoch-28 batch-110
Running loss of epoch-28 batch-110 = 6.805639714002609e-05

Training epoch-28 batch-111
Running loss of epoch-28 batch-111 = 7.696426473557949e-05

Training epoch-28 batch-112
Running loss of epoch-28 batch-112 = 4.55386471003294e-05

Training epoch-28 batch-113
Running loss of epoch-28 batch-113 = 0.00014558457769453526

Training epoch-28 batch-114
Running loss of epoch-28 batch-114 = 9.21487808227539e-05

Training epoch-28 batch-115
Running loss of epoch-28 batch-115 = 4.6693021431565285e-05

Training epoch-28 batch-116
Running loss of epoch-28 batch-116 = 0.0003396726679056883

Training epoch-28 batch-117
Running loss of epoch-28 batch-117 = 0.0002428495790809393

Training epoch-28 batch-118
Running loss of epoch-28 batch-118 = 3.789621405303478e-05

Training epoch-28 batch-119
Running loss of epoch-28 batch-119 = 4.690582863986492e-05

Training epoch-28 batch-120
Running loss of epoch-28 batch-120 = 5.85352536290884e-05

Training epoch-28 batch-121
Running loss of epoch-28 batch-121 = 7.569510489702225e-05

Training epoch-28 batch-122
Running loss of epoch-28 batch-122 = 0.00011601042933762074

Training epoch-28 batch-123
Running loss of epoch-28 batch-123 = 0.00016313162632286549

Training epoch-28 batch-124
Running loss of epoch-28 batch-124 = 0.00033481279388070107

Training epoch-28 batch-125
Running loss of epoch-28 batch-125 = 2.879137173295021e-05

Training epoch-28 batch-126
Running loss of epoch-28 batch-126 = 0.00010533106978982687

Training epoch-28 batch-127
Running loss of epoch-28 batch-127 = 4.714145325124264e-05

Training epoch-28 batch-128
Running loss of epoch-28 batch-128 = 3.6348821595311165e-05

Training epoch-28 batch-129
Running loss of epoch-28 batch-129 = 0.00015273503959178925

Training epoch-28 batch-130
Running loss of epoch-28 batch-130 = 8.079712279140949e-05

Training epoch-28 batch-131
Running loss of epoch-28 batch-131 = 0.00010066642425954342

Training epoch-28 batch-132
Running loss of epoch-28 batch-132 = 9.984313510358334e-05

Training epoch-28 batch-133
Running loss of epoch-28 batch-133 = 6.233947351574898e-05

Training epoch-28 batch-134
Running loss of epoch-28 batch-134 = 2.007954753935337e-05

Training epoch-28 batch-135
Running loss of epoch-28 batch-135 = 7.001892663538456e-05

Training epoch-28 batch-136
Running loss of epoch-28 batch-136 = 0.00015594647265970707

Training epoch-28 batch-137
Running loss of epoch-28 batch-137 = 3.231223672628403e-05

Training epoch-28 batch-138
Running loss of epoch-28 batch-138 = 5.1158247515559196e-05

Training epoch-28 batch-139
Running loss of epoch-28 batch-139 = 0.00015732739120721817

Training epoch-28 batch-140
Running loss of epoch-28 batch-140 = 0.00014872313477098942

Training epoch-28 batch-141
Running loss of epoch-28 batch-141 = 5.093170329928398e-05

Training epoch-28 batch-142
Running loss of epoch-28 batch-142 = 9.50428657233715e-05

Training epoch-28 batch-143
Running loss of epoch-28 batch-143 = 8.562998846173286e-05

Training epoch-28 batch-144
Running loss of epoch-28 batch-144 = 0.0001568214502185583

Training epoch-28 batch-145
Running loss of epoch-28 batch-145 = 6.715417839586735e-05

Training epoch-28 batch-146
Running loss of epoch-28 batch-146 = 6.455648690462112e-05

Training epoch-28 batch-147
Running loss of epoch-28 batch-147 = 8.169910870492458e-05

Training epoch-28 batch-148
Running loss of epoch-28 batch-148 = 5.224463529884815e-05

Training epoch-28 batch-149
Running loss of epoch-28 batch-149 = 7.717916741967201e-05

Training epoch-28 batch-150
Running loss of epoch-28 batch-150 = 4.896009340882301e-05

Training epoch-28 batch-151
Running loss of epoch-28 batch-151 = 8.225650526583195e-05

Training epoch-28 batch-152
Running loss of epoch-28 batch-152 = 6.271363236010075e-05

Training epoch-28 batch-153
Running loss of epoch-28 batch-153 = 3.9120553992688656e-05

Training epoch-28 batch-154
Running loss of epoch-28 batch-154 = 0.0003677812637761235

Training epoch-28 batch-155
Running loss of epoch-28 batch-155 = 7.875612936913967e-05

Training epoch-28 batch-156
Running loss of epoch-28 batch-156 = 0.00011151493526995182

Training epoch-28 batch-157
Running loss of epoch-28 batch-157 = 0.0013413690030574799

Finished training epoch-28.



Average train loss at epoch-28 = 8.797625303268432e-05

Started Evaluation

Average val loss at epoch-28 = 1.3800630965824057

Accuracy for classes:
Accuracy for class equals is: 94.55 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 89.02 %
Accuracy for class onCreate is: 89.34 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 63.01 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 46.19 %
Accuracy for class execute is: 59.84 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 80.87 %

Finished Evaluation



Started training epoch-29


Training epoch-29 batch-1
Running loss of epoch-29 batch-1 = 0.00031345500610768795

Training epoch-29 batch-2
Running loss of epoch-29 batch-2 = 5.370820872485638e-05

Training epoch-29 batch-3
Running loss of epoch-29 batch-3 = 3.562658093869686e-05

Training epoch-29 batch-4
Running loss of epoch-29 batch-4 = 6.42391387373209e-05

Training epoch-29 batch-5
Running loss of epoch-29 batch-5 = 0.0001183398999273777

Training epoch-29 batch-6
Running loss of epoch-29 batch-6 = 5.371612496674061e-05

Training epoch-29 batch-7
Running loss of epoch-29 batch-7 = 4.265527240931988e-05

Training epoch-29 batch-8
Running loss of epoch-29 batch-8 = 7.206154987215996e-05

Training epoch-29 batch-9
Running loss of epoch-29 batch-9 = 4.756334237754345e-05

Training epoch-29 batch-10
Running loss of epoch-29 batch-10 = 6.416300311684608e-05

Training epoch-29 batch-11
Running loss of epoch-29 batch-11 = 2.681650221347809e-05

Training epoch-29 batch-12
Running loss of epoch-29 batch-12 = 0.00012888899073004723

Training epoch-29 batch-13
Running loss of epoch-29 batch-13 = 4.733121022582054e-05

Training epoch-29 batch-14
Running loss of epoch-29 batch-14 = 2.497970126569271e-05

Training epoch-29 batch-15
Running loss of epoch-29 batch-15 = 0.00013709021732211113

Training epoch-29 batch-16
Running loss of epoch-29 batch-16 = 6.448267959058285e-05

Training epoch-29 batch-17
Running loss of epoch-29 batch-17 = 0.00049265765119344

Training epoch-29 batch-18
Running loss of epoch-29 batch-18 = 4.981784150004387e-05

Training epoch-29 batch-19
Running loss of epoch-29 batch-19 = 1.5731900930404663e-05

Training epoch-29 batch-20
Running loss of epoch-29 batch-20 = 5.585025064647198e-05

Training epoch-29 batch-21
Running loss of epoch-29 batch-21 = 2.7621164917945862e-05

Training epoch-29 batch-22
Running loss of epoch-29 batch-22 = 6.66941050440073e-05

Training epoch-29 batch-23
Running loss of epoch-29 batch-23 = 6.648828275501728e-05

Training epoch-29 batch-24
Running loss of epoch-29 batch-24 = 7.731467485427856e-05

Training epoch-29 batch-25
Running loss of epoch-29 batch-25 = 8.189049549400806e-05

Training epoch-29 batch-26
Running loss of epoch-29 batch-26 = 0.00012556579895317554

Training epoch-29 batch-27
Running loss of epoch-29 batch-27 = 0.00014339876361191273

Training epoch-29 batch-28
Running loss of epoch-29 batch-28 = 2.5108223780989647e-05

Training epoch-29 batch-29
Running loss of epoch-29 batch-29 = 4.687625914812088e-05

Training epoch-29 batch-30
Running loss of epoch-29 batch-30 = 6.721541285514832e-05

Training epoch-29 batch-31
Running loss of epoch-29 batch-31 = 0.00014859228394925594

Training epoch-29 batch-32
Running loss of epoch-29 batch-32 = 0.0002424099948257208

Training epoch-29 batch-33
Running loss of epoch-29 batch-33 = 5.495455116033554e-05

Training epoch-29 batch-34
Running loss of epoch-29 batch-34 = 2.7983449399471283e-05

Training epoch-29 batch-35
Running loss of epoch-29 batch-35 = 5.8974954299628735e-05

Training epoch-29 batch-36
Running loss of epoch-29 batch-36 = 2.2099819034337997e-05

Training epoch-29 batch-37
Running loss of epoch-29 batch-37 = 6.780237890779972e-05

Training epoch-29 batch-38
Running loss of epoch-29 batch-38 = 7.056305184960365e-05

Training epoch-29 batch-39
Running loss of epoch-29 batch-39 = 5.422648973762989e-05

Training epoch-29 batch-40
Running loss of epoch-29 batch-40 = 0.00017578888218849897

Training epoch-29 batch-41
Running loss of epoch-29 batch-41 = 5.9104058891534805e-05

Training epoch-29 batch-42
Running loss of epoch-29 batch-42 = 5.00271562486887e-05

Training epoch-29 batch-43
Running loss of epoch-29 batch-43 = 9.76927112787962e-05

Training epoch-29 batch-44
Running loss of epoch-29 batch-44 = 2.3605069145560265e-05

Training epoch-29 batch-45
Running loss of epoch-29 batch-45 = 2.8252601623535156e-05

Training epoch-29 batch-46
Running loss of epoch-29 batch-46 = 9.44710336625576e-06

Training epoch-29 batch-47
Running loss of epoch-29 batch-47 = 0.00011111656203866005

Training epoch-29 batch-48
Running loss of epoch-29 batch-48 = 3.5232165828347206e-05

Training epoch-29 batch-49
Running loss of epoch-29 batch-49 = 2.5056535378098488e-05

Training epoch-29 batch-50
Running loss of epoch-29 batch-50 = 2.5582266971468925e-05

Training epoch-29 batch-51
Running loss of epoch-29 batch-51 = 5.8185774832963943e-05

Training epoch-29 batch-52
Running loss of epoch-29 batch-52 = 1.6511650756001472e-05

Training epoch-29 batch-53
Running loss of epoch-29 batch-53 = 3.7860823795199394e-05

Training epoch-29 batch-54
Running loss of epoch-29 batch-54 = 5.1975250244140625e-05

Training epoch-29 batch-55
Running loss of epoch-29 batch-55 = 7.578171789646149e-05

Training epoch-29 batch-56
Running loss of epoch-29 batch-56 = 0.00011125975288450718

Training epoch-29 batch-57
Running loss of epoch-29 batch-57 = 8.32520890980959e-05

Training epoch-29 batch-58
Running loss of epoch-29 batch-58 = 0.00012247427366673946

Training epoch-29 batch-59
Running loss of epoch-29 batch-59 = 0.0001469319686293602

Training epoch-29 batch-60
Running loss of epoch-29 batch-60 = 7.501640357077122e-05

Training epoch-29 batch-61
Running loss of epoch-29 batch-61 = 6.930669769644737e-05

Training epoch-29 batch-62
Running loss of epoch-29 batch-62 = 0.00010201451368629932

Training epoch-29 batch-63
Running loss of epoch-29 batch-63 = 0.00046038953587412834

Training epoch-29 batch-64
Running loss of epoch-29 batch-64 = 8.868006989359856e-05

Training epoch-29 batch-65
Running loss of epoch-29 batch-65 = 0.0001624082215130329

Training epoch-29 batch-66
Running loss of epoch-29 batch-66 = 2.7847709134221077e-05

Training epoch-29 batch-67
Running loss of epoch-29 batch-67 = 2.2820429876446724e-05

Training epoch-29 batch-68
Running loss of epoch-29 batch-68 = 8.365255780518055e-05

Training epoch-29 batch-69
Running loss of epoch-29 batch-69 = 2.9634451493620872e-05

Training epoch-29 batch-70
Running loss of epoch-29 batch-70 = 3.2704323530197144e-05

Training epoch-29 batch-71
Running loss of epoch-29 batch-71 = 3.23164276778698e-05

Training epoch-29 batch-72
Running loss of epoch-29 batch-72 = 3.8755591958761215e-05

Training epoch-29 batch-73
Running loss of epoch-29 batch-73 = 3.4052180126309395e-05

Training epoch-29 batch-74
Running loss of epoch-29 batch-74 = 1.3618730008602142e-05

Training epoch-29 batch-75
Running loss of epoch-29 batch-75 = 4.085688851773739e-05

Training epoch-29 batch-76
Running loss of epoch-29 batch-76 = 0.0003604339435696602

Training epoch-29 batch-77
Running loss of epoch-29 batch-77 = 9.544938802719116e-05

Training epoch-29 batch-78
Running loss of epoch-29 batch-78 = 0.0002910969778895378

Training epoch-29 batch-79
Running loss of epoch-29 batch-79 = 5.996529944241047e-05

Training epoch-29 batch-80
Running loss of epoch-29 batch-80 = 4.909210838377476e-05

Training epoch-29 batch-81
Running loss of epoch-29 batch-81 = 4.7274399548769e-05

Training epoch-29 batch-82
Running loss of epoch-29 batch-82 = 2.552056685090065e-05

Training epoch-29 batch-83
Running loss of epoch-29 batch-83 = 6.125355139374733e-05

Training epoch-29 batch-84
Running loss of epoch-29 batch-84 = 5.532801151275635e-05

Training epoch-29 batch-85
Running loss of epoch-29 batch-85 = 0.00016581080853939056

Training epoch-29 batch-86
Running loss of epoch-29 batch-86 = 8.921511471271515e-05

Training epoch-29 batch-87
Running loss of epoch-29 batch-87 = 4.3089850805699825e-05

Training epoch-29 batch-88
Running loss of epoch-29 batch-88 = 6.97956420481205e-05

Training epoch-29 batch-89
Running loss of epoch-29 batch-89 = 6.796629168093204e-05

Training epoch-29 batch-90
Running loss of epoch-29 batch-90 = 2.4404842406511307e-05

Training epoch-29 batch-91
Running loss of epoch-29 batch-91 = 2.8707552701234818e-05

Training epoch-29 batch-92
Running loss of epoch-29 batch-92 = 0.0002746962709352374

Training epoch-29 batch-93
Running loss of epoch-29 batch-93 = 4.11763321608305e-05

Training epoch-29 batch-94
Running loss of epoch-29 batch-94 = 7.406005170196295e-05

Training epoch-29 batch-95
Running loss of epoch-29 batch-95 = 9.05205961316824e-05

Training epoch-29 batch-96
Running loss of epoch-29 batch-96 = 3.407732583582401e-05

Training epoch-29 batch-97
Running loss of epoch-29 batch-97 = 8.851964958012104e-05

Training epoch-29 batch-98
Running loss of epoch-29 batch-98 = 2.9841437935829163e-05

Training epoch-29 batch-99
Running loss of epoch-29 batch-99 = 5.7009514421224594e-05

Training epoch-29 batch-100
Running loss of epoch-29 batch-100 = 7.379043381661177e-05

Training epoch-29 batch-101
Running loss of epoch-29 batch-101 = 8.243951015174389e-05

Training epoch-29 batch-102
Running loss of epoch-29 batch-102 = 6.0185324400663376e-05

Training epoch-29 batch-103
Running loss of epoch-29 batch-103 = 0.00012770690955221653

Training epoch-29 batch-104
Running loss of epoch-29 batch-104 = 9.728269651532173e-05

Training epoch-29 batch-105
Running loss of epoch-29 batch-105 = 5.6348275393247604e-05

Training epoch-29 batch-106
Running loss of epoch-29 batch-106 = 2.6443740352988243e-05

Training epoch-29 batch-107
Running loss of epoch-29 batch-107 = 2.5132670998573303e-05

Training epoch-29 batch-108
Running loss of epoch-29 batch-108 = 9.208172559738159e-05

Training epoch-29 batch-109
Running loss of epoch-29 batch-109 = 3.4780241549015045e-05

Training epoch-29 batch-110
Running loss of epoch-29 batch-110 = 5.049852188676596e-05

Training epoch-29 batch-111
Running loss of epoch-29 batch-111 = 0.00015832600183784962

Training epoch-29 batch-112
Running loss of epoch-29 batch-112 = 4.2482512071728706e-05

Training epoch-29 batch-113
Running loss of epoch-29 batch-113 = 3.94224189221859e-05

Training epoch-29 batch-114
Running loss of epoch-29 batch-114 = 6.467243656516075e-05

Training epoch-29 batch-115
Running loss of epoch-29 batch-115 = 5.132821388542652e-05

Training epoch-29 batch-116
Running loss of epoch-29 batch-116 = 1.4618504792451859e-05

Training epoch-29 batch-117
Running loss of epoch-29 batch-117 = 7.324363104999065e-05

Training epoch-29 batch-118
Running loss of epoch-29 batch-118 = 0.00021253456361591816

Training epoch-29 batch-119
Running loss of epoch-29 batch-119 = 5.719810724258423e-05

Training epoch-29 batch-120
Running loss of epoch-29 batch-120 = 0.0001274335663765669

Training epoch-29 batch-121
Running loss of epoch-29 batch-121 = 0.00016785517800599337

Training epoch-29 batch-122
Running loss of epoch-29 batch-122 = 4.5268796384334564e-05

Training epoch-29 batch-123
Running loss of epoch-29 batch-123 = 0.00011000991798937321

Training epoch-29 batch-124
Running loss of epoch-29 batch-124 = 2.7947360649704933e-05

Training epoch-29 batch-125
Running loss of epoch-29 batch-125 = 0.00019037630409002304

Training epoch-29 batch-126
Running loss of epoch-29 batch-126 = 5.3263502195477486e-05

Training epoch-29 batch-127
Running loss of epoch-29 batch-127 = 4.4144224375486374e-05

Training epoch-29 batch-128
Running loss of epoch-29 batch-128 = 0.00017735897563397884

Training epoch-29 batch-129
Running loss of epoch-29 batch-129 = 3.678887151181698e-05

Training epoch-29 batch-130
Running loss of epoch-29 batch-130 = 0.00010769790969789028

Training epoch-29 batch-131
Running loss of epoch-29 batch-131 = 3.5757897421717644e-05

Training epoch-29 batch-132
Running loss of epoch-29 batch-132 = 0.00013254943769425154

Training epoch-29 batch-133
Running loss of epoch-29 batch-133 = 2.4286098778247833e-05

Training epoch-29 batch-134
Running loss of epoch-29 batch-134 = 0.00011636840645223856

Training epoch-29 batch-135
Running loss of epoch-29 batch-135 = 5.5921729654073715e-05

Training epoch-29 batch-136
Running loss of epoch-29 batch-136 = 6.325612775981426e-05

Training epoch-29 batch-137
Running loss of epoch-29 batch-137 = 3.641773946583271e-05

Training epoch-29 batch-138
Running loss of epoch-29 batch-138 = 3.7574442103505135e-05

Training epoch-29 batch-139
Running loss of epoch-29 batch-139 = 6.598769687116146e-05

Training epoch-29 batch-140
Running loss of epoch-29 batch-140 = 6.397184915840626e-05

Training epoch-29 batch-141
Running loss of epoch-29 batch-141 = 9.479024447500706e-05

Training epoch-29 batch-142
Running loss of epoch-29 batch-142 = 7.23863486200571e-05

Training epoch-29 batch-143
Running loss of epoch-29 batch-143 = 0.0001370209502056241

Training epoch-29 batch-144
Running loss of epoch-29 batch-144 = 1.4734454452991486e-05

Training epoch-29 batch-145
Running loss of epoch-29 batch-145 = 0.00012197904288768768

Training epoch-29 batch-146
Running loss of epoch-29 batch-146 = 3.9753271266818047e-05

Training epoch-29 batch-147
Running loss of epoch-29 batch-147 = 0.00017084041610360146

Training epoch-29 batch-148
Running loss of epoch-29 batch-148 = 0.00024401070550084114

Training epoch-29 batch-149
Running loss of epoch-29 batch-149 = 3.45488078892231e-05

Training epoch-29 batch-150
Running loss of epoch-29 batch-150 = 3.594160079956055e-05

Training epoch-29 batch-151
Running loss of epoch-29 batch-151 = 7.047806866466999e-05

Training epoch-29 batch-152
Running loss of epoch-29 batch-152 = 4.598172381520271e-05

Training epoch-29 batch-153
Running loss of epoch-29 batch-153 = 7.816357538104057e-06

Training epoch-29 batch-154
Running loss of epoch-29 batch-154 = 6.209523417055607e-05

Training epoch-29 batch-155
Running loss of epoch-29 batch-155 = 2.862687688320875e-05

Training epoch-29 batch-156
Running loss of epoch-29 batch-156 = 4.020356573164463e-05

Training epoch-29 batch-157
Running loss of epoch-29 batch-157 = 6.838887929916382e-05

Finished training epoch-29.



Average train loss at epoch-29 = 8.107220008969306e-05

Started Evaluation

Average val loss at epoch-29 = 1.3707986757265787

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.69 %
Accuracy for class onCreate is: 89.77 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 59.82 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 53.82 %
Accuracy for class get is: 65.13 %

Overall Accuracy = 81.08 %

Finished Evaluation



Started training epoch-30


Training epoch-30 batch-1
Running loss of epoch-30 batch-1 = 1.619989052414894e-05

Training epoch-30 batch-2
Running loss of epoch-30 batch-2 = 4.4577522203326225e-05

Training epoch-30 batch-3
Running loss of epoch-30 batch-3 = 5.577458068728447e-05

Training epoch-30 batch-4
Running loss of epoch-30 batch-4 = 2.7913833037018776e-05

Training epoch-30 batch-5
Running loss of epoch-30 batch-5 = 3.231526352465153e-05

Training epoch-30 batch-6
Running loss of epoch-30 batch-6 = 7.952877786010504e-05

Training epoch-30 batch-7
Running loss of epoch-30 batch-7 = 7.470347918570042e-05

Training epoch-30 batch-8
Running loss of epoch-30 batch-8 = 0.00011708203237503767

Training epoch-30 batch-9
Running loss of epoch-30 batch-9 = 0.0001279799034819007

Training epoch-30 batch-10
Running loss of epoch-30 batch-10 = 4.524551331996918e-05

Training epoch-30 batch-11
Running loss of epoch-30 batch-11 = 4.4065178371965885e-05

Training epoch-30 batch-12
Running loss of epoch-30 batch-12 = 1.6634585335850716e-05

Training epoch-30 batch-13
Running loss of epoch-30 batch-13 = 1.2712320312857628e-05

Training epoch-30 batch-14
Running loss of epoch-30 batch-14 = 2.545071765780449e-05

Training epoch-30 batch-15
Running loss of epoch-30 batch-15 = 5.182693712413311e-05

Training epoch-30 batch-16
Running loss of epoch-30 batch-16 = 0.0003417728003114462

Training epoch-30 batch-17
Running loss of epoch-30 batch-17 = 4.274374805390835e-05

Training epoch-30 batch-18
Running loss of epoch-30 batch-18 = 5.275104194879532e-05

Training epoch-30 batch-19
Running loss of epoch-30 batch-19 = 4.6136556193232536e-05

Training epoch-30 batch-20
Running loss of epoch-30 batch-20 = 2.9388582333922386e-05

Training epoch-30 batch-21
Running loss of epoch-30 batch-21 = 3.703869879245758e-05

Training epoch-30 batch-22
Running loss of epoch-30 batch-22 = 3.103981725871563e-05

Training epoch-30 batch-23
Running loss of epoch-30 batch-23 = 2.9968563467264175e-05

Training epoch-30 batch-24
Running loss of epoch-30 batch-24 = 3.616220783442259e-05

Training epoch-30 batch-25
Running loss of epoch-30 batch-25 = 7.34417699277401e-05

Training epoch-30 batch-26
Running loss of epoch-30 batch-26 = 6.801215931773186e-05

Training epoch-30 batch-27
Running loss of epoch-30 batch-27 = 8.600472938269377e-05

Training epoch-30 batch-28
Running loss of epoch-30 batch-28 = 0.0001273532398045063

Training epoch-30 batch-29
Running loss of epoch-30 batch-29 = 7.439963519573212e-05

Training epoch-30 batch-30
Running loss of epoch-30 batch-30 = 0.00019130506552755833

Training epoch-30 batch-31
Running loss of epoch-30 batch-31 = 4.6219443902373314e-05

Training epoch-30 batch-32
Running loss of epoch-30 batch-32 = 3.6268262192606926e-05

Training epoch-30 batch-33
Running loss of epoch-30 batch-33 = 3.2339245080947876e-05

Training epoch-30 batch-34
Running loss of epoch-30 batch-34 = 3.2380688935518265e-05

Training epoch-30 batch-35
Running loss of epoch-30 batch-35 = 3.540702164173126e-05

Training epoch-30 batch-36
Running loss of epoch-30 batch-36 = 3.112573176622391e-05

Training epoch-30 batch-37
Running loss of epoch-30 batch-37 = 2.5723129510879517e-05

Training epoch-30 batch-38
Running loss of epoch-30 batch-38 = 5.910429172217846e-05

Training epoch-30 batch-39
Running loss of epoch-30 batch-39 = 1.223827712237835e-05

Training epoch-30 batch-40
Running loss of epoch-30 batch-40 = 3.0092895030975342e-05

Training epoch-30 batch-41
Running loss of epoch-30 batch-41 = 4.325248301029205e-05

Training epoch-30 batch-42
Running loss of epoch-30 batch-42 = 3.7866877391934395e-05

Training epoch-30 batch-43
Running loss of epoch-30 batch-43 = 7.670442573726177e-05

Training epoch-30 batch-44
Running loss of epoch-30 batch-44 = 0.00020829983986914158

Training epoch-30 batch-45
Running loss of epoch-30 batch-45 = 0.00030002999119460583

Training epoch-30 batch-46
Running loss of epoch-30 batch-46 = 3.21331899613142e-05

Training epoch-30 batch-47
Running loss of epoch-30 batch-47 = 0.0001816506264731288

Training epoch-30 batch-48
Running loss of epoch-30 batch-48 = 0.0001271932851523161

Training epoch-30 batch-49
Running loss of epoch-30 batch-49 = 3.077322617173195e-05

Training epoch-30 batch-50
Running loss of epoch-30 batch-50 = 0.00016733910888433456

Training epoch-30 batch-51
Running loss of epoch-30 batch-51 = 7.261894643306732e-05

Training epoch-30 batch-52
Running loss of epoch-30 batch-52 = 3.316276706755161e-05

Training epoch-30 batch-53
Running loss of epoch-30 batch-53 = 1.6798265278339386e-05

Training epoch-30 batch-54
Running loss of epoch-30 batch-54 = 8.167955093085766e-05

Training epoch-30 batch-55
Running loss of epoch-30 batch-55 = 0.00011508644092828035

Training epoch-30 batch-56
Running loss of epoch-30 batch-56 = 3.592553548514843e-05

Training epoch-30 batch-57
Running loss of epoch-30 batch-57 = 0.0002921777777373791

Training epoch-30 batch-58
Running loss of epoch-30 batch-58 = 8.583231829106808e-05

Training epoch-30 batch-59
Running loss of epoch-30 batch-59 = 6.426265463232994e-05

Training epoch-30 batch-60
Running loss of epoch-30 batch-60 = 3.6017270758748055e-05

Training epoch-30 batch-61
Running loss of epoch-30 batch-61 = 0.00013406877405941486

Training epoch-30 batch-62
Running loss of epoch-30 batch-62 = 3.492599353194237e-05

Training epoch-30 batch-63
Running loss of epoch-30 batch-63 = 3.8226135075092316e-05

Training epoch-30 batch-64
Running loss of epoch-30 batch-64 = 0.00012929504737257957

Training epoch-30 batch-65
Running loss of epoch-30 batch-65 = 7.50712351873517e-05

Training epoch-30 batch-66
Running loss of epoch-30 batch-66 = 3.2679177820682526e-05

Training epoch-30 batch-67
Running loss of epoch-30 batch-67 = 6.850366480648518e-05

Training epoch-30 batch-68
Running loss of epoch-30 batch-68 = 0.00014535710215568542

Training epoch-30 batch-69
Running loss of epoch-30 batch-69 = 0.00019808392971754074

Training epoch-30 batch-70
Running loss of epoch-30 batch-70 = 5.492568016052246e-05

Training epoch-30 batch-71
Running loss of epoch-30 batch-71 = 3.1946925446391106e-05

Training epoch-30 batch-72
Running loss of epoch-30 batch-72 = 3.1403033062815666e-05

Training epoch-30 batch-73
Running loss of epoch-30 batch-73 = 2.8922338970005512e-05

Training epoch-30 batch-74
Running loss of epoch-30 batch-74 = 5.986238829791546e-05

Training epoch-30 batch-75
Running loss of epoch-30 batch-75 = 0.00021370919421315193

Training epoch-30 batch-76
Running loss of epoch-30 batch-76 = 5.990080535411835e-05

Training epoch-30 batch-77
Running loss of epoch-30 batch-77 = 5.42572233825922e-05

Training epoch-30 batch-78
Running loss of epoch-30 batch-78 = 2.2447435185313225e-05

Training epoch-30 batch-79
Running loss of epoch-30 batch-79 = 0.00017663580365478992

Training epoch-30 batch-80
Running loss of epoch-30 batch-80 = 3.1557981856167316e-05

Training epoch-30 batch-81
Running loss of epoch-30 batch-81 = 8.422869723290205e-05

Training epoch-30 batch-82
Running loss of epoch-30 batch-82 = 9.203015360981226e-05

Training epoch-30 batch-83
Running loss of epoch-30 batch-83 = 1.8780352547764778e-05

Training epoch-30 batch-84
Running loss of epoch-30 batch-84 = 2.179853618144989e-05

Training epoch-30 batch-85
Running loss of epoch-30 batch-85 = 5.522416904568672e-05

Training epoch-30 batch-86
Running loss of epoch-30 batch-86 = 2.116430550813675e-05

Training epoch-30 batch-87
Running loss of epoch-30 batch-87 = 4.057399928569794e-05

Training epoch-30 batch-88
Running loss of epoch-30 batch-88 = 8.303672075271606e-05

Training epoch-30 batch-89
Running loss of epoch-30 batch-89 = 4.8061367124319077e-05

Training epoch-30 batch-90
Running loss of epoch-30 batch-90 = 3.1028641387820244e-05

Training epoch-30 batch-91
Running loss of epoch-30 batch-91 = 4.411069676280022e-05

Training epoch-30 batch-92
Running loss of epoch-30 batch-92 = 3.9385282434523106e-05

Training epoch-30 batch-93
Running loss of epoch-30 batch-93 = 0.0001156852813437581

Training epoch-30 batch-94
Running loss of epoch-30 batch-94 = 2.8721056878566742e-05

Training epoch-30 batch-95
Running loss of epoch-30 batch-95 = 7.554376497864723e-05

Training epoch-30 batch-96
Running loss of epoch-30 batch-96 = 2.4800654500722885e-05

Training epoch-30 batch-97
Running loss of epoch-30 batch-97 = 1.0365387424826622e-05

Training epoch-30 batch-98
Running loss of epoch-30 batch-98 = 9.130022954195738e-05

Training epoch-30 batch-99
Running loss of epoch-30 batch-99 = 5.130656063556671e-05

Training epoch-30 batch-100
Running loss of epoch-30 batch-100 = 2.398015931248665e-05

Training epoch-30 batch-101
Running loss of epoch-30 batch-101 = 3.9744190871715546e-06

Training epoch-30 batch-102
Running loss of epoch-30 batch-102 = 5.2214134484529495e-05

Training epoch-30 batch-103
Running loss of epoch-30 batch-103 = 0.00012628920376300812

Training epoch-30 batch-104
Running loss of epoch-30 batch-104 = 0.00011419574730098248

Training epoch-30 batch-105
Running loss of epoch-30 batch-105 = 0.00011911964975297451

Training epoch-30 batch-106
Running loss of epoch-30 batch-106 = 9.895209223031998e-05

Training epoch-30 batch-107
Running loss of epoch-30 batch-107 = 0.0002241694601252675

Training epoch-30 batch-108
Running loss of epoch-30 batch-108 = 4.1912542656064034e-05

Training epoch-30 batch-109
Running loss of epoch-30 batch-109 = 7.067341357469559e-06

Training epoch-30 batch-110
Running loss of epoch-30 batch-110 = 9.56293661147356e-05

Training epoch-30 batch-111
Running loss of epoch-30 batch-111 = 4.01507131755352e-05

Training epoch-30 batch-112
Running loss of epoch-30 batch-112 = 1.5160301700234413e-05

Training epoch-30 batch-113
Running loss of epoch-30 batch-113 = 5.81860076636076e-05

Training epoch-30 batch-114
Running loss of epoch-30 batch-114 = 7.874495349824429e-05

Training epoch-30 batch-115
Running loss of epoch-30 batch-115 = 8.930312469601631e-05

Training epoch-30 batch-116
Running loss of epoch-30 batch-116 = 0.00017372309230268002

Training epoch-30 batch-117
Running loss of epoch-30 batch-117 = 3.4022144973278046e-05

Training epoch-30 batch-118
Running loss of epoch-30 batch-118 = 5.9876823797822e-05

Training epoch-30 batch-119
Running loss of epoch-30 batch-119 = 4.464574158191681e-05

Training epoch-30 batch-120
Running loss of epoch-30 batch-120 = 2.402416430413723e-05

Training epoch-30 batch-121
Running loss of epoch-30 batch-121 = 3.3581163734197617e-05

Training epoch-30 batch-122
Running loss of epoch-30 batch-122 = 3.2438430935144424e-05

Training epoch-30 batch-123
Running loss of epoch-30 batch-123 = 6.13087322562933e-05

Training epoch-30 batch-124
Running loss of epoch-30 batch-124 = 2.5801127776503563e-05

Training epoch-30 batch-125
Running loss of epoch-30 batch-125 = 4.679197445511818e-05

Training epoch-30 batch-126
Running loss of epoch-30 batch-126 = 0.00014647655189037323

Training epoch-30 batch-127
Running loss of epoch-30 batch-127 = 0.00024040206335484982

Training epoch-30 batch-128
Running loss of epoch-30 batch-128 = 2.1447427570819855e-05

Training epoch-30 batch-129
Running loss of epoch-30 batch-129 = 2.501765266060829e-05

Training epoch-30 batch-130
Running loss of epoch-30 batch-130 = 5.497247911989689e-05

Training epoch-30 batch-131
Running loss of epoch-30 batch-131 = 0.00012750551104545593

Training epoch-30 batch-132
Running loss of epoch-30 batch-132 = 4.409532994031906e-05

Training epoch-30 batch-133
Running loss of epoch-30 batch-133 = 4.9350433982908726e-05

Training epoch-30 batch-134
Running loss of epoch-30 batch-134 = 4.091695882380009e-05

Training epoch-30 batch-135
Running loss of epoch-30 batch-135 = 0.00011015520431101322

Training epoch-30 batch-136
Running loss of epoch-30 batch-136 = 3.0316878110170364e-05

Training epoch-30 batch-137
Running loss of epoch-30 batch-137 = 6.477092392742634e-05

Training epoch-30 batch-138
Running loss of epoch-30 batch-138 = 2.778111957013607e-05

Training epoch-30 batch-139
Running loss of epoch-30 batch-139 = 8.116592653095722e-05

Training epoch-30 batch-140
Running loss of epoch-30 batch-140 = 3.767781890928745e-05

Training epoch-30 batch-141
Running loss of epoch-30 batch-141 = 4.4438522309064865e-05

Training epoch-30 batch-142
Running loss of epoch-30 batch-142 = 0.00018448429182171822

Training epoch-30 batch-143
Running loss of epoch-30 batch-143 = 4.575774073600769e-05

Training epoch-30 batch-144
Running loss of epoch-30 batch-144 = 2.2022053599357605e-05

Training epoch-30 batch-145
Running loss of epoch-30 batch-145 = 3.80929559469223e-05

Training epoch-30 batch-146
Running loss of epoch-30 batch-146 = 7.357355207204819e-05

Training epoch-30 batch-147
Running loss of epoch-30 batch-147 = 2.5641638785600662e-05

Training epoch-30 batch-148
Running loss of epoch-30 batch-148 = 2.975529059767723e-05

Training epoch-30 batch-149
Running loss of epoch-30 batch-149 = 2.5421613827347755e-05

Training epoch-30 batch-150
Running loss of epoch-30 batch-150 = 1.3041077181696892e-05

Training epoch-30 batch-151
Running loss of epoch-30 batch-151 = 0.00015930994413793087

Training epoch-30 batch-152
Running loss of epoch-30 batch-152 = 2.5376444682478905e-05

Training epoch-30 batch-153
Running loss of epoch-30 batch-153 = 1.662643626332283e-05

Training epoch-30 batch-154
Running loss of epoch-30 batch-154 = 0.0001548193395137787

Training epoch-30 batch-155
Running loss of epoch-30 batch-155 = 2.9658665880560875e-05

Training epoch-30 batch-156
Running loss of epoch-30 batch-156 = 6.531761027872562e-05

Training epoch-30 batch-157
Running loss of epoch-30 batch-157 = 0.00011362507939338684

Finished training epoch-30.



Average train loss at epoch-30 = 6.929418742656708e-05

Started Evaluation

Average val loss at epoch-30 = 1.3605506660777864

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.64 %
Accuracy for class onCreate is: 88.91 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 63.01 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.45 %

Finished Evaluation



Started training epoch-31


Training epoch-31 batch-1
Running loss of epoch-31 batch-1 = 0.00012343097478151321

Training epoch-31 batch-2
Running loss of epoch-31 batch-2 = 4.506739787757397e-05

Training epoch-31 batch-3
Running loss of epoch-31 batch-3 = 8.036382496356964e-06

Training epoch-31 batch-4
Running loss of epoch-31 batch-4 = 2.2570369765162468e-05

Training epoch-31 batch-5
Running loss of epoch-31 batch-5 = 2.7348054572939873e-05

Training epoch-31 batch-6
Running loss of epoch-31 batch-6 = 5.426118150353432e-05

Training epoch-31 batch-7
Running loss of epoch-31 batch-7 = 8.213170804083347e-05

Training epoch-31 batch-8
Running loss of epoch-31 batch-8 = 3.3104559406638145e-05

Training epoch-31 batch-9
Running loss of epoch-31 batch-9 = 1.0925112292170525e-05

Training epoch-31 batch-10
Running loss of epoch-31 batch-10 = 7.29486346244812e-05

Training epoch-31 batch-11
Running loss of epoch-31 batch-11 = 3.2537151128053665e-05

Training epoch-31 batch-12
Running loss of epoch-31 batch-12 = 2.873106859624386e-05

Training epoch-31 batch-13
Running loss of epoch-31 batch-13 = 8.237757720053196e-05

Training epoch-31 batch-14
Running loss of epoch-31 batch-14 = 0.00015827035531401634

Training epoch-31 batch-15
Running loss of epoch-31 batch-15 = 3.560504410415888e-05

Training epoch-31 batch-16
Running loss of epoch-31 batch-16 = 3.117159940302372e-05

Training epoch-31 batch-17
Running loss of epoch-31 batch-17 = 7.977639324963093e-05

Training epoch-31 batch-18
Running loss of epoch-31 batch-18 = 5.80779742449522e-05

Training epoch-31 batch-19
Running loss of epoch-31 batch-19 = 5.131727084517479e-05

Training epoch-31 batch-20
Running loss of epoch-31 batch-20 = 1.8218299373984337e-05

Training epoch-31 batch-21
Running loss of epoch-31 batch-21 = 5.4012518376111984e-05

Training epoch-31 batch-22
Running loss of epoch-31 batch-22 = 5.984073504805565e-05

Training epoch-31 batch-23
Running loss of epoch-31 batch-23 = 3.1274277716875076e-05

Training epoch-31 batch-24
Running loss of epoch-31 batch-24 = 2.1035317331552505e-05

Training epoch-31 batch-25
Running loss of epoch-31 batch-25 = 7.16822687536478e-05

Training epoch-31 batch-26
Running loss of epoch-31 batch-26 = 3.412901423871517e-05

Training epoch-31 batch-27
Running loss of epoch-31 batch-27 = 1.814030110836029e-05

Training epoch-31 batch-28
Running loss of epoch-31 batch-28 = 2.115289680659771e-05

Training epoch-31 batch-29
Running loss of epoch-31 batch-29 = 5.164765752851963e-05

Training epoch-31 batch-30
Running loss of epoch-31 batch-30 = 4.572095349431038e-05

Training epoch-31 batch-31
Running loss of epoch-31 batch-31 = 9.956955909729004e-05

Training epoch-31 batch-32
Running loss of epoch-31 batch-32 = 1.7438665963709354e-05

Training epoch-31 batch-33
Running loss of epoch-31 batch-33 = 2.9438757337629795e-05

Training epoch-31 batch-34
Running loss of epoch-31 batch-34 = 5.748332478106022e-05

Training epoch-31 batch-35
Running loss of epoch-31 batch-35 = 3.0641211196780205e-05

Training epoch-31 batch-36
Running loss of epoch-31 batch-36 = 3.511505201458931e-05

Training epoch-31 batch-37
Running loss of epoch-31 batch-37 = 2.3109139874577522e-05

Training epoch-31 batch-38
Running loss of epoch-31 batch-38 = 1.6265781596302986e-05

Training epoch-31 batch-39
Running loss of epoch-31 batch-39 = 6.277370266616344e-05

Training epoch-31 batch-40
Running loss of epoch-31 batch-40 = 1.550489105284214e-05

Training epoch-31 batch-41
Running loss of epoch-31 batch-41 = 4.768674261868e-05

Training epoch-31 batch-42
Running loss of epoch-31 batch-42 = 1.6896752640604973e-05

Training epoch-31 batch-43
Running loss of epoch-31 batch-43 = 0.0003008302301168442

Training epoch-31 batch-44
Running loss of epoch-31 batch-44 = 2.8780894353985786e-05

Training epoch-31 batch-45
Running loss of epoch-31 batch-45 = 3.393343649804592e-05

Training epoch-31 batch-46
Running loss of epoch-31 batch-46 = 7.203686982393265e-05

Training epoch-31 batch-47
Running loss of epoch-31 batch-47 = 2.5590648874640465e-05

Training epoch-31 batch-48
Running loss of epoch-31 batch-48 = 5.015893839299679e-05

Training epoch-31 batch-49
Running loss of epoch-31 batch-49 = 7.485621608793736e-05

Training epoch-31 batch-50
Running loss of epoch-31 batch-50 = 3.825873136520386e-05

Training epoch-31 batch-51
Running loss of epoch-31 batch-51 = 2.753199078142643e-05

Training epoch-31 batch-52
Running loss of epoch-31 batch-52 = 3.0049355700612068e-05

Training epoch-31 batch-53
Running loss of epoch-31 batch-53 = 6.848212797194719e-05

Training epoch-31 batch-54
Running loss of epoch-31 batch-54 = 3.787688910961151e-05

Training epoch-31 batch-55
Running loss of epoch-31 batch-55 = 0.00018568814266473055

Training epoch-31 batch-56
Running loss of epoch-31 batch-56 = 1.9805273041129112e-05

Training epoch-31 batch-57
Running loss of epoch-31 batch-57 = 1.61794014275074e-05

Training epoch-31 batch-58
Running loss of epoch-31 batch-58 = 3.539235331118107e-05

Training epoch-31 batch-59
Running loss of epoch-31 batch-59 = 2.7797650545835495e-05

Training epoch-31 batch-60
Running loss of epoch-31 batch-60 = 1.1476920917630196e-05

Training epoch-31 batch-61
Running loss of epoch-31 batch-61 = 2.8702663257718086e-05

Training epoch-31 batch-62
Running loss of epoch-31 batch-62 = 3.935163840651512e-05

Training epoch-31 batch-63
Running loss of epoch-31 batch-63 = 4.5548658818006516e-05

Training epoch-31 batch-64
Running loss of epoch-31 batch-64 = 3.378349356353283e-05

Training epoch-31 batch-65
Running loss of epoch-31 batch-65 = 3.4598750062286854e-05

Training epoch-31 batch-66
Running loss of epoch-31 batch-66 = 2.256303559988737e-05

Training epoch-31 batch-67
Running loss of epoch-31 batch-67 = 9.597092866897583e-05

Training epoch-31 batch-68
Running loss of epoch-31 batch-68 = 3.230036236345768e-05

Training epoch-31 batch-69
Running loss of epoch-31 batch-69 = 4.834216088056564e-05

Training epoch-31 batch-70
Running loss of epoch-31 batch-70 = 6.684358231723309e-05

Training epoch-31 batch-71
Running loss of epoch-31 batch-71 = 4.48017381131649e-05

Training epoch-31 batch-72
Running loss of epoch-31 batch-72 = 0.00022268062457442284

Training epoch-31 batch-73
Running loss of epoch-31 batch-73 = 0.0001176302321255207

Training epoch-31 batch-74
Running loss of epoch-31 batch-74 = 3.5103876143693924e-05

Training epoch-31 batch-75
Running loss of epoch-31 batch-75 = 3.3067772164940834e-05

Training epoch-31 batch-76
Running loss of epoch-31 batch-76 = 4.634121432900429e-05

Training epoch-31 batch-77
Running loss of epoch-31 batch-77 = 6.386591121554375e-05

Training epoch-31 batch-78
Running loss of epoch-31 batch-78 = 5.0058355554938316e-05

Training epoch-31 batch-79
Running loss of epoch-31 batch-79 = 4.619383253157139e-05

Training epoch-31 batch-80
Running loss of epoch-31 batch-80 = 1.7898855730891228e-05

Training epoch-31 batch-81
Running loss of epoch-31 batch-81 = 5.6386226788163185e-05

Training epoch-31 batch-82
Running loss of epoch-31 batch-82 = 4.6170782297849655e-05

Training epoch-31 batch-83
Running loss of epoch-31 batch-83 = 6.52677845209837e-05

Training epoch-31 batch-84
Running loss of epoch-31 batch-84 = 2.1731946617364883e-05

Training epoch-31 batch-85
Running loss of epoch-31 batch-85 = 0.0001323737669736147

Training epoch-31 batch-86
Running loss of epoch-31 batch-86 = 3.0084745958447456e-05

Training epoch-31 batch-87
Running loss of epoch-31 batch-87 = 7.663527503609657e-05

Training epoch-31 batch-88
Running loss of epoch-31 batch-88 = 3.6060577258467674e-05

Training epoch-31 batch-89
Running loss of epoch-31 batch-89 = 4.702247679233551e-05

Training epoch-31 batch-90
Running loss of epoch-31 batch-90 = 2.5932909920811653e-05

Training epoch-31 batch-91
Running loss of epoch-31 batch-91 = 6.269081495702267e-05

Training epoch-31 batch-92
Running loss of epoch-31 batch-92 = 3.633368760347366e-05

Training epoch-31 batch-93
Running loss of epoch-31 batch-93 = 0.00017303717322647572

Training epoch-31 batch-94
Running loss of epoch-31 batch-94 = 3.0314084142446518e-05

Training epoch-31 batch-95
Running loss of epoch-31 batch-95 = 0.00011595420073717833

Training epoch-31 batch-96
Running loss of epoch-31 batch-96 = 3.461586311459541e-05

Training epoch-31 batch-97
Running loss of epoch-31 batch-97 = 3.194971941411495e-05

Training epoch-31 batch-98
Running loss of epoch-31 batch-98 = 1.9711675122380257e-05

Training epoch-31 batch-99
Running loss of epoch-31 batch-99 = 1.6141682863235474e-05

Training epoch-31 batch-100
Running loss of epoch-31 batch-100 = 0.00010278169065713882

Training epoch-31 batch-101
Running loss of epoch-31 batch-101 = 6.868760101497173e-05

Training epoch-31 batch-102
Running loss of epoch-31 batch-102 = 5.610776133835316e-05

Training epoch-31 batch-103
Running loss of epoch-31 batch-103 = 3.415043465793133e-05

Training epoch-31 batch-104
Running loss of epoch-31 batch-104 = 4.726136103272438e-05

Training epoch-31 batch-105
Running loss of epoch-31 batch-105 = 5.044299177825451e-05

Training epoch-31 batch-106
Running loss of epoch-31 batch-106 = 6.87695574015379e-05

Training epoch-31 batch-107
Running loss of epoch-31 batch-107 = 4.455121234059334e-05

Training epoch-31 batch-108
Running loss of epoch-31 batch-108 = 3.425590693950653e-05

Training epoch-31 batch-109
Running loss of epoch-31 batch-109 = 3.6721350625157356e-05

Training epoch-31 batch-110
Running loss of epoch-31 batch-110 = 3.455253317952156e-05

Training epoch-31 batch-111
Running loss of epoch-31 batch-111 = 2.296827733516693e-05

Training epoch-31 batch-112
Running loss of epoch-31 batch-112 = 2.6444904506206512e-05

Training epoch-31 batch-113
Running loss of epoch-31 batch-113 = 0.000177780631929636

Training epoch-31 batch-114
Running loss of epoch-31 batch-114 = 1.9647181034088135e-05

Training epoch-31 batch-115
Running loss of epoch-31 batch-115 = 4.1812658309936523e-05

Training epoch-31 batch-116
Running loss of epoch-31 batch-116 = 5.4172356612980366e-05

Training epoch-31 batch-117
Running loss of epoch-31 batch-117 = 2.3647677153348923e-05

Training epoch-31 batch-118
Running loss of epoch-31 batch-118 = 8.704990614205599e-05

Training epoch-31 batch-119
Running loss of epoch-31 batch-119 = 3.9762817323207855e-05

Training epoch-31 batch-120
Running loss of epoch-31 batch-120 = 0.0001859853509813547

Training epoch-31 batch-121
Running loss of epoch-31 batch-121 = 2.3568514734506607e-05

Training epoch-31 batch-122
Running loss of epoch-31 batch-122 = 3.4211669117212296e-05

Training epoch-31 batch-123
Running loss of epoch-31 batch-123 = 1.9731232896447182e-05

Training epoch-31 batch-124
Running loss of epoch-31 batch-124 = 2.0484207198023796e-05

Training epoch-31 batch-125
Running loss of epoch-31 batch-125 = 6.993277929723263e-05

Training epoch-31 batch-126
Running loss of epoch-31 batch-126 = 1.7553800716996193e-05

Training epoch-31 batch-127
Running loss of epoch-31 batch-127 = 1.8449965864419937e-05

Training epoch-31 batch-128
Running loss of epoch-31 batch-128 = 7.104105316102505e-05

Training epoch-31 batch-129
Running loss of epoch-31 batch-129 = 0.00027914252132177353

Training epoch-31 batch-130
Running loss of epoch-31 batch-130 = 2.6902416720986366e-05

Training epoch-31 batch-131
Running loss of epoch-31 batch-131 = 3.7888530641794205e-05

Training epoch-31 batch-132
Running loss of epoch-31 batch-132 = 2.035871148109436e-05

Training epoch-31 batch-133
Running loss of epoch-31 batch-133 = 6.008136551827192e-05

Training epoch-31 batch-134
Running loss of epoch-31 batch-134 = 7.899338379502296e-05

Training epoch-31 batch-135
Running loss of epoch-31 batch-135 = 4.323828034102917e-05

Training epoch-31 batch-136
Running loss of epoch-31 batch-136 = 9.76762967184186e-05

Training epoch-31 batch-137
Running loss of epoch-31 batch-137 = 2.307537943124771e-05

Training epoch-31 batch-138
Running loss of epoch-31 batch-138 = 0.00016779673751443624

Training epoch-31 batch-139
Running loss of epoch-31 batch-139 = 4.936777986586094e-05

Training epoch-31 batch-140
Running loss of epoch-31 batch-140 = 3.2632146030664444e-05

Training epoch-31 batch-141
Running loss of epoch-31 batch-141 = 3.0156224966049194e-05

Training epoch-31 batch-142
Running loss of epoch-31 batch-142 = 4.542502574622631e-05

Training epoch-31 batch-143
Running loss of epoch-31 batch-143 = 2.225581556558609e-05

Training epoch-31 batch-144
Running loss of epoch-31 batch-144 = 5.613756366074085e-05

Training epoch-31 batch-145
Running loss of epoch-31 batch-145 = 6.509269587695599e-05

Training epoch-31 batch-146
Running loss of epoch-31 batch-146 = 3.219477366656065e-05

Training epoch-31 batch-147
Running loss of epoch-31 batch-147 = 4.492257721722126e-05

Training epoch-31 batch-148
Running loss of epoch-31 batch-148 = 4.741759039461613e-05

Training epoch-31 batch-149
Running loss of epoch-31 batch-149 = 9.094434790313244e-05

Training epoch-31 batch-150
Running loss of epoch-31 batch-150 = 7.943203672766685e-05

Training epoch-31 batch-151
Running loss of epoch-31 batch-151 = 4.060287028551102e-05

Training epoch-31 batch-152
Running loss of epoch-31 batch-152 = 0.00022996612824499607

Training epoch-31 batch-153
Running loss of epoch-31 batch-153 = 3.03671695291996e-05

Training epoch-31 batch-154
Running loss of epoch-31 batch-154 = 3.58126126229763e-05

Training epoch-31 batch-155
Running loss of epoch-31 batch-155 = 2.7379021048545837e-05

Training epoch-31 batch-156
Running loss of epoch-31 batch-156 = 4.30478248745203e-05

Training epoch-31 batch-157
Running loss of epoch-31 batch-157 = 0.0002100132405757904

Finished training epoch-31.



Average train loss at epoch-31 = 5.509500354528427e-05

Started Evaluation

Average val loss at epoch-31 = 1.3820757451655339

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 87.54 %
Accuracy for class onCreate is: 88.38 %
Accuracy for class toString is: 84.30 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 62.31 %

Overall Accuracy = 80.96 %

Finished Evaluation



Started training epoch-32


Training epoch-32 batch-1
Running loss of epoch-32 batch-1 = 3.5229139029979706e-05

Training epoch-32 batch-2
Running loss of epoch-32 batch-2 = 9.686686098575592e-05

Training epoch-32 batch-3
Running loss of epoch-32 batch-3 = 9.341188706457615e-05

Training epoch-32 batch-4
Running loss of epoch-32 batch-4 = 7.774541154503822e-05

Training epoch-32 batch-5
Running loss of epoch-32 batch-5 = 2.162461169064045e-05

Training epoch-32 batch-6
Running loss of epoch-32 batch-6 = 4.598568193614483e-05

Training epoch-32 batch-7
Running loss of epoch-32 batch-7 = 7.551908493041992e-05

Training epoch-32 batch-8
Running loss of epoch-32 batch-8 = 3.0578114092350006e-05

Training epoch-32 batch-9
Running loss of epoch-32 batch-9 = 2.4505890905857086e-05

Training epoch-32 batch-10
Running loss of epoch-32 batch-10 = 2.9652612283825874e-05

Training epoch-32 batch-11
Running loss of epoch-32 batch-11 = 3.531924448907375e-05

Training epoch-32 batch-12
Running loss of epoch-32 batch-12 = 0.00010854087304323912

Training epoch-32 batch-13
Running loss of epoch-32 batch-13 = 3.518303856253624e-05

Training epoch-32 batch-14
Running loss of epoch-32 batch-14 = 6.148102693259716e-05

Training epoch-32 batch-15
Running loss of epoch-32 batch-15 = 4.4956570491194725e-05

Training epoch-32 batch-16
Running loss of epoch-32 batch-16 = 3.149756230413914e-05

Training epoch-32 batch-17
Running loss of epoch-32 batch-17 = 1.5569385141134262e-05

Training epoch-32 batch-18
Running loss of epoch-32 batch-18 = 3.064097836613655e-05

Training epoch-32 batch-19
Running loss of epoch-32 batch-19 = 4.6408502385020256e-05

Training epoch-32 batch-20
Running loss of epoch-32 batch-20 = 8.826330304145813e-05

Training epoch-32 batch-21
Running loss of epoch-32 batch-21 = 8.643604815006256e-06

Training epoch-32 batch-22
Running loss of epoch-32 batch-22 = 3.6363257095217705e-05

Training epoch-32 batch-23
Running loss of epoch-32 batch-23 = 3.0556926503777504e-05

Training epoch-32 batch-24
Running loss of epoch-32 batch-24 = 3.2272422686219215e-05

Training epoch-32 batch-25
Running loss of epoch-32 batch-25 = 0.00012268242426216602

Training epoch-32 batch-26
Running loss of epoch-32 batch-26 = 1.3730721548199654e-05

Training epoch-32 batch-27
Running loss of epoch-32 batch-27 = 5.101505666971207e-05

Training epoch-32 batch-28
Running loss of epoch-32 batch-28 = 3.7470366805791855e-05

Training epoch-32 batch-29
Running loss of epoch-32 batch-29 = 6.348825991153717e-06

Training epoch-32 batch-30
Running loss of epoch-32 batch-30 = 1.6031786799430847e-05

Training epoch-32 batch-31
Running loss of epoch-32 batch-31 = 3.879843279719353e-05

Training epoch-32 batch-32
Running loss of epoch-32 batch-32 = 0.00010826322250068188

Training epoch-32 batch-33
Running loss of epoch-32 batch-33 = 7.189414463937283e-05

Training epoch-32 batch-34
Running loss of epoch-32 batch-34 = 4.6568457037210464e-05

Training epoch-32 batch-35
Running loss of epoch-32 batch-35 = 5.316035822033882e-05

Training epoch-32 batch-36
Running loss of epoch-32 batch-36 = 1.838267780840397e-05

Training epoch-32 batch-37
Running loss of epoch-32 batch-37 = 2.6742462068796158e-05

Training epoch-32 batch-38
Running loss of epoch-32 batch-38 = 3.274064511060715e-05

Training epoch-32 batch-39
Running loss of epoch-32 batch-39 = 0.00014081411063671112

Training epoch-32 batch-40
Running loss of epoch-32 batch-40 = 2.011680044233799e-05

Training epoch-32 batch-41
Running loss of epoch-32 batch-41 = 7.599196396768093e-05

Training epoch-32 batch-42
Running loss of epoch-32 batch-42 = 4.701386205852032e-05

Training epoch-32 batch-43
Running loss of epoch-32 batch-43 = 1.8505845218896866e-05

Training epoch-32 batch-44
Running loss of epoch-32 batch-44 = 6.386684253811836e-05

Training epoch-32 batch-45
Running loss of epoch-32 batch-45 = 4.09840140491724e-05

Training epoch-32 batch-46
Running loss of epoch-32 batch-46 = 1.1588912457227707e-05

Training epoch-32 batch-47
Running loss of epoch-32 batch-47 = 2.2879336029291153e-05

Training epoch-32 batch-48
Running loss of epoch-32 batch-48 = 6.654579192399979e-05

Training epoch-32 batch-49
Running loss of epoch-32 batch-49 = 5.730031989514828e-05

Training epoch-32 batch-50
Running loss of epoch-32 batch-50 = 0.00012681144289672375

Training epoch-32 batch-51
Running loss of epoch-32 batch-51 = 1.772260293364525e-05

Training epoch-32 batch-52
Running loss of epoch-32 batch-52 = 2.1772459149360657e-05

Training epoch-32 batch-53
Running loss of epoch-32 batch-53 = 3.457348793745041e-05

Training epoch-32 batch-54
Running loss of epoch-32 batch-54 = 4.568416625261307e-05

Training epoch-32 batch-55
Running loss of epoch-32 batch-55 = 1.1570518836379051e-05

Training epoch-32 batch-56
Running loss of epoch-32 batch-56 = 4.0002865716814995e-05

Training epoch-32 batch-57
Running loss of epoch-32 batch-57 = 2.1914485841989517e-05

Training epoch-32 batch-58
Running loss of epoch-32 batch-58 = 0.00013744155876338482

Training epoch-32 batch-59
Running loss of epoch-32 batch-59 = 1.6189878806471825e-05

Training epoch-32 batch-60
Running loss of epoch-32 batch-60 = 7.02710822224617e-05

Training epoch-32 batch-61
Running loss of epoch-32 batch-61 = 3.866665065288544e-05

Training epoch-32 batch-62
Running loss of epoch-32 batch-62 = 3.521237522363663e-05

Training epoch-32 batch-63
Running loss of epoch-32 batch-63 = 1.747184433043003e-05

Training epoch-32 batch-64
Running loss of epoch-32 batch-64 = 2.1367333829402924e-05

Training epoch-32 batch-65
Running loss of epoch-32 batch-65 = 9.12277027964592e-05

Training epoch-32 batch-66
Running loss of epoch-32 batch-66 = 1.858314499258995e-05

Training epoch-32 batch-67
Running loss of epoch-32 batch-67 = 2.236291766166687e-05

Training epoch-32 batch-68
Running loss of epoch-32 batch-68 = 4.0158629417419434e-05

Training epoch-32 batch-69
Running loss of epoch-32 batch-69 = 2.1105632185935974e-05

Training epoch-32 batch-70
Running loss of epoch-32 batch-70 = 4.6426779590547085e-05

Training epoch-32 batch-71
Running loss of epoch-32 batch-71 = 2.0971754565835e-05

Training epoch-32 batch-72
Running loss of epoch-32 batch-72 = 6.167497485876083e-05

Training epoch-32 batch-73
Running loss of epoch-32 batch-73 = 6.712810136377811e-05

Training epoch-32 batch-74
Running loss of epoch-32 batch-74 = 2.9296614229679108e-05

Training epoch-32 batch-75
Running loss of epoch-32 batch-75 = 3.718864172697067e-05

Training epoch-32 batch-76
Running loss of epoch-32 batch-76 = 4.1906023398041725e-05

Training epoch-32 batch-77
Running loss of epoch-32 batch-77 = 1.2194039300084114e-05

Training epoch-32 batch-78
Running loss of epoch-32 batch-78 = 2.7320580556988716e-05

Training epoch-32 batch-79
Running loss of epoch-32 batch-79 = 4.231068305671215e-05

Training epoch-32 batch-80
Running loss of epoch-32 batch-80 = 2.0913779735565186e-05

Training epoch-32 batch-81
Running loss of epoch-32 batch-81 = 2.368958666920662e-05

Training epoch-32 batch-82
Running loss of epoch-32 batch-82 = 4.6244123950600624e-05

Training epoch-32 batch-83
Running loss of epoch-32 batch-83 = 3.0272407457232475e-05

Training epoch-32 batch-84
Running loss of epoch-32 batch-84 = 6.158964242786169e-05

Training epoch-32 batch-85
Running loss of epoch-32 batch-85 = 1.5337485820055008e-05

Training epoch-32 batch-86
Running loss of epoch-32 batch-86 = 2.1073035895824432e-05

Training epoch-32 batch-87
Running loss of epoch-32 batch-87 = 5.078408867120743e-05

Training epoch-32 batch-88
Running loss of epoch-32 batch-88 = 9.159219916909933e-05

Training epoch-32 batch-89
Running loss of epoch-32 batch-89 = 3.572774585336447e-05

Training epoch-32 batch-90
Running loss of epoch-32 batch-90 = 2.5027431547641754e-05

Training epoch-32 batch-91
Running loss of epoch-32 batch-91 = 2.112472429871559e-05

Training epoch-32 batch-92
Running loss of epoch-32 batch-92 = 4.971330054104328e-05

Training epoch-32 batch-93
Running loss of epoch-32 batch-93 = 0.00010087969712913036

Training epoch-32 batch-94
Running loss of epoch-32 batch-94 = 1.3129785656929016e-05

Training epoch-32 batch-95
Running loss of epoch-32 batch-95 = 3.368081524968147e-05

Training epoch-32 batch-96
Running loss of epoch-32 batch-96 = 1.369137316942215e-05

Training epoch-32 batch-97
Running loss of epoch-32 batch-97 = 5.673663690686226e-05

Training epoch-32 batch-98
Running loss of epoch-32 batch-98 = 6.499676965177059e-05

Training epoch-32 batch-99
Running loss of epoch-32 batch-99 = 8.118152618408203e-05

Training epoch-32 batch-100
Running loss of epoch-32 batch-100 = 2.9130373150110245e-05

Training epoch-32 batch-101
Running loss of epoch-32 batch-101 = 1.6330042853951454e-05

Training epoch-32 batch-102
Running loss of epoch-32 batch-102 = 5.533243529498577e-05

Training epoch-32 batch-103
Running loss of epoch-32 batch-103 = 2.8002075850963593e-05

Training epoch-32 batch-104
Running loss of epoch-32 batch-104 = 0.00013876042794436216

Training epoch-32 batch-105
Running loss of epoch-32 batch-105 = 5.806144326925278e-05

Training epoch-32 batch-106
Running loss of epoch-32 batch-106 = 2.2137537598609924e-05

Training epoch-32 batch-107
Running loss of epoch-32 batch-107 = 6.394670344889164e-05

Training epoch-32 batch-108
Running loss of epoch-32 batch-108 = 4.135444760322571e-05

Training epoch-32 batch-109
Running loss of epoch-32 batch-109 = 4.9208058044314384e-05

Training epoch-32 batch-110
Running loss of epoch-32 batch-110 = 2.350006252527237e-05

Training epoch-32 batch-111
Running loss of epoch-32 batch-111 = 5.9835147112607956e-05

Training epoch-32 batch-112
Running loss of epoch-32 batch-112 = 3.405963070690632e-05

Training epoch-32 batch-113
Running loss of epoch-32 batch-113 = 2.9688933864235878e-05

Training epoch-32 batch-114
Running loss of epoch-32 batch-114 = 2.442672848701477e-05

Training epoch-32 batch-115
Running loss of epoch-32 batch-115 = 2.552662044763565e-05

Training epoch-32 batch-116
Running loss of epoch-32 batch-116 = 1.2356322258710861e-05

Training epoch-32 batch-117
Running loss of epoch-32 batch-117 = 4.2211031541228294e-05

Training epoch-32 batch-118
Running loss of epoch-32 batch-118 = 2.5005312636494637e-05

Training epoch-32 batch-119
Running loss of epoch-32 batch-119 = 2.6288209483027458e-05

Training epoch-32 batch-120
Running loss of epoch-32 batch-120 = 7.78494868427515e-05

Training epoch-32 batch-121
Running loss of epoch-32 batch-121 = 4.381849430501461e-05

Training epoch-32 batch-122
Running loss of epoch-32 batch-122 = 2.508074976503849e-05

Training epoch-32 batch-123
Running loss of epoch-32 batch-123 = 7.431372068822384e-05

Training epoch-32 batch-124
Running loss of epoch-32 batch-124 = 2.7737929485738277e-05

Training epoch-32 batch-125
Running loss of epoch-32 batch-125 = 2.9065879061818123e-05

Training epoch-32 batch-126
Running loss of epoch-32 batch-126 = 0.00024368439335376024

Training epoch-32 batch-127
Running loss of epoch-32 batch-127 = 3.4509459510445595e-05

Training epoch-32 batch-128
Running loss of epoch-32 batch-128 = 3.211642615497112e-05

Training epoch-32 batch-129
Running loss of epoch-32 batch-129 = 1.6030389815568924e-05

Training epoch-32 batch-130
Running loss of epoch-32 batch-130 = 4.251382779330015e-05

Training epoch-32 batch-131
Running loss of epoch-32 batch-131 = 0.00021074747201055288

Training epoch-32 batch-132
Running loss of epoch-32 batch-132 = 3.052013926208019e-05

Training epoch-32 batch-133
Running loss of epoch-32 batch-133 = 4.729581996798515e-05

Training epoch-32 batch-134
Running loss of epoch-32 batch-134 = 7.385830394923687e-05

Training epoch-32 batch-135
Running loss of epoch-32 batch-135 = 3.8421712815761566e-05

Training epoch-32 batch-136
Running loss of epoch-32 batch-136 = 4.0302518755197525e-05

Training epoch-32 batch-137
Running loss of epoch-32 batch-137 = 3.402354195713997e-05

Training epoch-32 batch-138
Running loss of epoch-32 batch-138 = 2.3334752768278122e-05

Training epoch-32 batch-139
Running loss of epoch-32 batch-139 = 4.325108602643013e-05

Training epoch-32 batch-140
Running loss of epoch-32 batch-140 = 3.3008866012096405e-05

Training epoch-32 batch-141
Running loss of epoch-32 batch-141 = 4.2839208617806435e-05

Training epoch-32 batch-142
Running loss of epoch-32 batch-142 = 9.016599506139755e-06

Training epoch-32 batch-143
Running loss of epoch-32 batch-143 = 5.8901961892843246e-05

Training epoch-32 batch-144
Running loss of epoch-32 batch-144 = 6.102025508880615e-05

Training epoch-32 batch-145
Running loss of epoch-32 batch-145 = 0.0002848769072443247

Training epoch-32 batch-146
Running loss of epoch-32 batch-146 = 2.003926783800125e-05

Training epoch-32 batch-147
Running loss of epoch-32 batch-147 = 3.502541221678257e-05

Training epoch-32 batch-148
Running loss of epoch-32 batch-148 = 1.4777062460780144e-05

Training epoch-32 batch-149
Running loss of epoch-32 batch-149 = 4.402664490044117e-05

Training epoch-32 batch-150
Running loss of epoch-32 batch-150 = 8.015427738428116e-06

Training epoch-32 batch-151
Running loss of epoch-32 batch-151 = 1.501268707215786e-05

Training epoch-32 batch-152
Running loss of epoch-32 batch-152 = 3.6262208595871925e-05

Training epoch-32 batch-153
Running loss of epoch-32 batch-153 = 3.751600161194801e-05

Training epoch-32 batch-154
Running loss of epoch-32 batch-154 = 4.1683437302708626e-05

Training epoch-32 batch-155
Running loss of epoch-32 batch-155 = 2.1502142772078514e-05

Training epoch-32 batch-156
Running loss of epoch-32 batch-156 = 2.0374776795506477e-05

Training epoch-32 batch-157
Running loss of epoch-32 batch-157 = 0.00012195110321044922

Finished training epoch-32.



Average train loss at epoch-32 = 4.638843387365341e-05

Started Evaluation

Average val loss at epoch-32 = 1.4047038336493283

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 81.91 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 50.00 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 64.87 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-33


Training epoch-33 batch-1
Running loss of epoch-33 batch-1 = 3.130384720861912e-05

Training epoch-33 batch-2
Running loss of epoch-33 batch-2 = 3.77212418243289e-05

Training epoch-33 batch-3
Running loss of epoch-33 batch-3 = 2.5245361030101776e-05

Training epoch-33 batch-4
Running loss of epoch-33 batch-4 = 2.87485308945179e-05

Training epoch-33 batch-5
Running loss of epoch-33 batch-5 = 2.9716407880187035e-05

Training epoch-33 batch-6
Running loss of epoch-33 batch-6 = 3.7297606468200684e-05

Training epoch-33 batch-7
Running loss of epoch-33 batch-7 = 2.531125210225582e-05

Training epoch-33 batch-8
Running loss of epoch-33 batch-8 = 3.3774180337786674e-05

Training epoch-33 batch-9
Running loss of epoch-33 batch-9 = 1.9846251234412193e-05

Training epoch-33 batch-10
Running loss of epoch-33 batch-10 = 3.745523281395435e-05

Training epoch-33 batch-11
Running loss of epoch-33 batch-11 = 1.3054581359028816e-05

Training epoch-33 batch-12
Running loss of epoch-33 batch-12 = 2.559693530201912e-05

Training epoch-33 batch-13
Running loss of epoch-33 batch-13 = 2.9178103432059288e-05

Training epoch-33 batch-14
Running loss of epoch-33 batch-14 = 4.121428355574608e-05

Training epoch-33 batch-15
Running loss of epoch-33 batch-15 = 1.6113044694066048e-05

Training epoch-33 batch-16
Running loss of epoch-33 batch-16 = 2.0860228687524796e-05

Training epoch-33 batch-17
Running loss of epoch-33 batch-17 = 4.306505434215069e-05

Training epoch-33 batch-18
Running loss of epoch-33 batch-18 = 2.1088402718305588e-05

Training epoch-33 batch-19
Running loss of epoch-33 batch-19 = 2.516736276447773e-05

Training epoch-33 batch-20
Running loss of epoch-33 batch-20 = 1.4045042917132378e-05

Training epoch-33 batch-21
Running loss of epoch-33 batch-21 = 4.85952477902174e-05

Training epoch-33 batch-22
Running loss of epoch-33 batch-22 = 2.2505177184939384e-05

Training epoch-33 batch-23
Running loss of epoch-33 batch-23 = 2.1903542801737785e-05

Training epoch-33 batch-24
Running loss of epoch-33 batch-24 = 3.2521551474928856e-05

Training epoch-33 batch-25
Running loss of epoch-33 batch-25 = 1.4570308849215508e-05

Training epoch-33 batch-26
Running loss of epoch-33 batch-26 = 2.3239757865667343e-05

Training epoch-33 batch-27
Running loss of epoch-33 batch-27 = 5.163508467376232e-05

Training epoch-33 batch-28
Running loss of epoch-33 batch-28 = 9.513774421066046e-05

Training epoch-33 batch-29
Running loss of epoch-33 batch-29 = 3.713974729180336e-05

Training epoch-33 batch-30
Running loss of epoch-33 batch-30 = 0.00019348948262631893

Training epoch-33 batch-31
Running loss of epoch-33 batch-31 = 4.2453641071915627e-05

Training epoch-33 batch-32
Running loss of epoch-33 batch-32 = 6.837630644440651e-05

Training epoch-33 batch-33
Running loss of epoch-33 batch-33 = 1.8770573660731316e-05

Training epoch-33 batch-34
Running loss of epoch-33 batch-34 = 4.25002072006464e-05

Training epoch-33 batch-35
Running loss of epoch-33 batch-35 = 1.1348864063620567e-05

Training epoch-33 batch-36
Running loss of epoch-33 batch-36 = 1.480989158153534e-05

Training epoch-33 batch-37
Running loss of epoch-33 batch-37 = 1.7976155504584312e-05

Training epoch-33 batch-38
Running loss of epoch-33 batch-38 = 3.7546735256910324e-05

Training epoch-33 batch-39
Running loss of epoch-33 batch-39 = 2.643209882080555e-05

Training epoch-33 batch-40
Running loss of epoch-33 batch-40 = 3.374763764441013e-05

Training epoch-33 batch-41
Running loss of epoch-33 batch-41 = 4.772446118295193e-05

Training epoch-33 batch-42
Running loss of epoch-33 batch-42 = 1.0433141142129898e-05

Training epoch-33 batch-43
Running loss of epoch-33 batch-43 = 6.354972720146179e-05

Training epoch-33 batch-44
Running loss of epoch-33 batch-44 = 1.380033791065216e-05

Training epoch-33 batch-45
Running loss of epoch-33 batch-45 = 3.116251900792122e-05

Training epoch-33 batch-46
Running loss of epoch-33 batch-46 = 9.238254278898239e-06

Training epoch-33 batch-47
Running loss of epoch-33 batch-47 = 2.0960811525583267e-05

Training epoch-33 batch-48
Running loss of epoch-33 batch-48 = 5.734781734645367e-05

Training epoch-33 batch-49
Running loss of epoch-33 batch-49 = 1.0251998901367188e-05

Training epoch-33 batch-50
Running loss of epoch-33 batch-50 = 3.594462759792805e-05

Training epoch-33 batch-51
Running loss of epoch-33 batch-51 = 2.661510370671749e-05

Training epoch-33 batch-52
Running loss of epoch-33 batch-52 = 4.484248347580433e-05

Training epoch-33 batch-53
Running loss of epoch-33 batch-53 = 4.897685721516609e-05

Training epoch-33 batch-54
Running loss of epoch-33 batch-54 = 0.00014827982522547245

Training epoch-33 batch-55
Running loss of epoch-33 batch-55 = 3.179069608449936e-05

Training epoch-33 batch-56
Running loss of epoch-33 batch-56 = 4.349113442003727e-05

Training epoch-33 batch-57
Running loss of epoch-33 batch-57 = 2.669263631105423e-05

Training epoch-33 batch-58
Running loss of epoch-33 batch-58 = 2.2674212232232094e-05

Training epoch-33 batch-59
Running loss of epoch-33 batch-59 = 4.125526174902916e-05

Training epoch-33 batch-60
Running loss of epoch-33 batch-60 = 7.736287079751492e-05

Training epoch-33 batch-61
Running loss of epoch-33 batch-61 = 2.0765000954270363e-05

Training epoch-33 batch-62
Running loss of epoch-33 batch-62 = 4.406040534377098e-05

Training epoch-33 batch-63
Running loss of epoch-33 batch-63 = 5.722115747630596e-05

Training epoch-33 batch-64
Running loss of epoch-33 batch-64 = 2.2180378437042236e-05

Training epoch-33 batch-65
Running loss of epoch-33 batch-65 = 3.939657472074032e-05

Training epoch-33 batch-66
Running loss of epoch-33 batch-66 = 4.2713480070233345e-05

Training epoch-33 batch-67
Running loss of epoch-33 batch-67 = 3.1286850571632385e-05

Training epoch-33 batch-68
Running loss of epoch-33 batch-68 = 1.4568213373422623e-05

Training epoch-33 batch-69
Running loss of epoch-33 batch-69 = 3.961496986448765e-05

Training epoch-33 batch-70
Running loss of epoch-33 batch-70 = 3.8368627429008484e-05

Training epoch-33 batch-71
Running loss of epoch-33 batch-71 = 2.634432166814804e-05

Training epoch-33 batch-72
Running loss of epoch-33 batch-72 = 3.2553449273109436e-05

Training epoch-33 batch-73
Running loss of epoch-33 batch-73 = 1.9328435882925987e-05

Training epoch-33 batch-74
Running loss of epoch-33 batch-74 = 2.242252230644226e-05

Training epoch-33 batch-75
Running loss of epoch-33 batch-75 = 5.789659917354584e-05

Training epoch-33 batch-76
Running loss of epoch-33 batch-76 = 4.2811501771211624e-05

Training epoch-33 batch-77
Running loss of epoch-33 batch-77 = 2.5487272068858147e-05

Training epoch-33 batch-78
Running loss of epoch-33 batch-78 = 8.810218423604965e-05

Training epoch-33 batch-79
Running loss of epoch-33 batch-79 = 3.9950013160705566e-05

Training epoch-33 batch-80
Running loss of epoch-33 batch-80 = 1.256261020898819e-05

Training epoch-33 batch-81
Running loss of epoch-33 batch-81 = 1.5170546248555183e-05

Training epoch-33 batch-82
Running loss of epoch-33 batch-82 = 4.0142564103007317e-05

Training epoch-33 batch-83
Running loss of epoch-33 batch-83 = 2.306513488292694e-05

Training epoch-33 batch-84
Running loss of epoch-33 batch-84 = 4.474283196032047e-05

Training epoch-33 batch-85
Running loss of epoch-33 batch-85 = 2.3820437490940094e-05

Training epoch-33 batch-86
Running loss of epoch-33 batch-86 = 0.00013961945660412312

Training epoch-33 batch-87
Running loss of epoch-33 batch-87 = 1.4164485037326813e-05

Training epoch-33 batch-88
Running loss of epoch-33 batch-88 = 1.796637661755085e-05

Training epoch-33 batch-89
Running loss of epoch-33 batch-89 = 3.980030305683613e-05

Training epoch-33 batch-90
Running loss of epoch-33 batch-90 = 1.1238735169172287e-05

Training epoch-33 batch-91
Running loss of epoch-33 batch-91 = 1.626601442694664e-05

Training epoch-33 batch-92
Running loss of epoch-33 batch-92 = 1.2665987014770508e-05

Training epoch-33 batch-93
Running loss of epoch-33 batch-93 = 5.636829882860184e-05

Training epoch-33 batch-94
Running loss of epoch-33 batch-94 = 2.414267510175705e-05

Training epoch-33 batch-95
Running loss of epoch-33 batch-95 = 1.936359331011772e-05

Training epoch-33 batch-96
Running loss of epoch-33 batch-96 = 5.568261258304119e-05

Training epoch-33 batch-97
Running loss of epoch-33 batch-97 = 0.00010782433673739433

Training epoch-33 batch-98
Running loss of epoch-33 batch-98 = 2.796039916574955e-05

Training epoch-33 batch-99
Running loss of epoch-33 batch-99 = 2.859509550035e-05

Training epoch-33 batch-100
Running loss of epoch-33 batch-100 = 4.187808372080326e-05

Training epoch-33 batch-101
Running loss of epoch-33 batch-101 = 2.07521952688694e-05

Training epoch-33 batch-102
Running loss of epoch-33 batch-102 = 6.837258115410805e-05

Training epoch-33 batch-103
Running loss of epoch-33 batch-103 = 3.146589733660221e-05

Training epoch-33 batch-104
Running loss of epoch-33 batch-104 = 3.353343345224857e-05

Training epoch-33 batch-105
Running loss of epoch-33 batch-105 = 0.00013000238686800003

Training epoch-33 batch-106
Running loss of epoch-33 batch-106 = 8.291983976960182e-05

Training epoch-33 batch-107
Running loss of epoch-33 batch-107 = 2.5494256988167763e-05

Training epoch-33 batch-108
Running loss of epoch-33 batch-108 = 2.6244204491376877e-05

Training epoch-33 batch-109
Running loss of epoch-33 batch-109 = 8.974364027380943e-05

Training epoch-33 batch-110
Running loss of epoch-33 batch-110 = 5.944538861513138e-05

Training epoch-33 batch-111
Running loss of epoch-33 batch-111 = 1.6280217096209526e-05

Training epoch-33 batch-112
Running loss of epoch-33 batch-112 = 1.645064912736416e-05

Training epoch-33 batch-113
Running loss of epoch-33 batch-113 = 4.8004789277911186e-05

Training epoch-33 batch-114
Running loss of epoch-33 batch-114 = 3.4135766327381134e-05

Training epoch-33 batch-115
Running loss of epoch-33 batch-115 = 2.167932689189911e-05

Training epoch-33 batch-116
Running loss of epoch-33 batch-116 = 3.1320611014962196e-05

Training epoch-33 batch-117
Running loss of epoch-33 batch-117 = 7.736659608781338e-05

Training epoch-33 batch-118
Running loss of epoch-33 batch-118 = 3.080244641751051e-05

Training epoch-33 batch-119
Running loss of epoch-33 batch-119 = 2.0794104784727097e-05

Training epoch-33 batch-120
Running loss of epoch-33 batch-120 = 3.640097565948963e-05

Training epoch-33 batch-121
Running loss of epoch-33 batch-121 = 2.2321008145809174e-05

Training epoch-33 batch-122
Running loss of epoch-33 batch-122 = 1.5251338481903076e-05

Training epoch-33 batch-123
Running loss of epoch-33 batch-123 = 2.6857014745473862e-05

Training epoch-33 batch-124
Running loss of epoch-33 batch-124 = 4.2493222281336784e-05

Training epoch-33 batch-125
Running loss of epoch-33 batch-125 = 4.835333675146103e-05

Training epoch-33 batch-126
Running loss of epoch-33 batch-126 = 1.0974239557981491e-05

Training epoch-33 batch-127
Running loss of epoch-33 batch-127 = 1.8470920622348785e-05

Training epoch-33 batch-128
Running loss of epoch-33 batch-128 = 4.244362935423851e-05

Training epoch-33 batch-129
Running loss of epoch-33 batch-129 = 4.184315912425518e-05

Training epoch-33 batch-130
Running loss of epoch-33 batch-130 = 5.355454050004482e-05

Training epoch-33 batch-131
Running loss of epoch-33 batch-131 = 3.5416800528764725e-05

Training epoch-33 batch-132
Running loss of epoch-33 batch-132 = 2.9565533623099327e-05

Training epoch-33 batch-133
Running loss of epoch-33 batch-133 = 1.0239891707897186e-05

Training epoch-33 batch-134
Running loss of epoch-33 batch-134 = 6.874511018395424e-05

Training epoch-33 batch-135
Running loss of epoch-33 batch-135 = 2.0927749574184418e-05

Training epoch-33 batch-136
Running loss of epoch-33 batch-136 = 3.524450585246086e-05

Training epoch-33 batch-137
Running loss of epoch-33 batch-137 = 9.111221879720688e-05

Training epoch-33 batch-138
Running loss of epoch-33 batch-138 = 2.470402978360653e-05

Training epoch-33 batch-139
Running loss of epoch-33 batch-139 = 3.455416299402714e-05

Training epoch-33 batch-140
Running loss of epoch-33 batch-140 = 3.259815275669098e-05

Training epoch-33 batch-141
Running loss of epoch-33 batch-141 = 1.0933028534054756e-05

Training epoch-33 batch-142
Running loss of epoch-33 batch-142 = 5.7669589295983315e-05

Training epoch-33 batch-143
Running loss of epoch-33 batch-143 = 3.351084887981415e-05

Training epoch-33 batch-144
Running loss of epoch-33 batch-144 = 2.218247391283512e-05

Training epoch-33 batch-145
Running loss of epoch-33 batch-145 = 4.188413731753826e-05

Training epoch-33 batch-146
Running loss of epoch-33 batch-146 = 6.525579374283552e-05

Training epoch-33 batch-147
Running loss of epoch-33 batch-147 = 7.965485565364361e-05

Training epoch-33 batch-148
Running loss of epoch-33 batch-148 = 8.00533453002572e-05

Training epoch-33 batch-149
Running loss of epoch-33 batch-149 = 8.533475920557976e-06

Training epoch-33 batch-150
Running loss of epoch-33 batch-150 = 1.1990312486886978e-05

Training epoch-33 batch-151
Running loss of epoch-33 batch-151 = 1.8466729670763016e-05

Training epoch-33 batch-152
Running loss of epoch-33 batch-152 = 1.3424549251794815e-05

Training epoch-33 batch-153
Running loss of epoch-33 batch-153 = 2.698996104300022e-05

Training epoch-33 batch-154
Running loss of epoch-33 batch-154 = 7.949071004986763e-06

Training epoch-33 batch-155
Running loss of epoch-33 batch-155 = 2.8462964110076427e-05

Training epoch-33 batch-156
Running loss of epoch-33 batch-156 = 0.0003219554200768471

Training epoch-33 batch-157
Running loss of epoch-33 batch-157 = 0.00020407140254974365

Finished training epoch-33.



Average train loss at epoch-33 = 3.960207551717758e-05

Started Evaluation

Average val loss at epoch-33 = 1.4480627275918323

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 50.22 %
Accuracy for class execute is: 53.82 %
Accuracy for class get is: 62.56 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-34


Training epoch-34 batch-1
Running loss of epoch-34 batch-1 = 4.314328543841839e-05

Training epoch-34 batch-2
Running loss of epoch-34 batch-2 = 8.123926818370819e-06

Training epoch-34 batch-3
Running loss of epoch-34 batch-3 = 1.3122102245688438e-05

Training epoch-34 batch-4
Running loss of epoch-34 batch-4 = 2.8892653062939644e-05

Training epoch-34 batch-5
Running loss of epoch-34 batch-5 = 9.010476060211658e-05

Training epoch-34 batch-6
Running loss of epoch-34 batch-6 = 4.535680636763573e-05

Training epoch-34 batch-7
Running loss of epoch-34 batch-7 = 2.819439396262169e-05

Training epoch-34 batch-8
Running loss of epoch-34 batch-8 = 4.3630367144942284e-05

Training epoch-34 batch-9
Running loss of epoch-34 batch-9 = 6.785965524613857e-05

Training epoch-34 batch-10
Running loss of epoch-34 batch-10 = 3.195204772055149e-05

Training epoch-34 batch-11
Running loss of epoch-34 batch-11 = 6.135436706244946e-05

Training epoch-34 batch-12
Running loss of epoch-34 batch-12 = 3.912276588380337e-05

Training epoch-34 batch-13
Running loss of epoch-34 batch-13 = 1.8605729565024376e-05

Training epoch-34 batch-14
Running loss of epoch-34 batch-14 = 2.2394582629203796e-05

Training epoch-34 batch-15
Running loss of epoch-34 batch-15 = 2.941349521279335e-05

Training epoch-34 batch-16
Running loss of epoch-34 batch-16 = 4.0468061342835426e-05

Training epoch-34 batch-17
Running loss of epoch-34 batch-17 = 2.4046050384640694e-05

Training epoch-34 batch-18
Running loss of epoch-34 batch-18 = 4.074745811522007e-05

Training epoch-34 batch-19
Running loss of epoch-34 batch-19 = 7.700873538851738e-06

Training epoch-34 batch-20
Running loss of epoch-34 batch-20 = 2.7799978852272034e-05

Training epoch-34 batch-21
Running loss of epoch-34 batch-21 = 3.98766715079546e-05

Training epoch-34 batch-22
Running loss of epoch-34 batch-22 = 3.0818628147244453e-05

Training epoch-34 batch-23
Running loss of epoch-34 batch-23 = 1.1459924280643463e-05

Training epoch-34 batch-24
Running loss of epoch-34 batch-24 = 1.6636447980999947e-05

Training epoch-34 batch-25
Running loss of epoch-34 batch-25 = 3.5042641684412956e-05

Training epoch-34 batch-26
Running loss of epoch-34 batch-26 = 1.7934711650013924e-05

Training epoch-34 batch-27
Running loss of epoch-34 batch-27 = 2.4616951122879982e-05

Training epoch-34 batch-28
Running loss of epoch-34 batch-28 = 1.0379357263445854e-05

Training epoch-34 batch-29
Running loss of epoch-34 batch-29 = 1.9406666979193687e-05

Training epoch-34 batch-30
Running loss of epoch-34 batch-30 = 3.809155896306038e-05

Training epoch-34 batch-31
Running loss of epoch-34 batch-31 = 2.551823854446411e-05

Training epoch-34 batch-32
Running loss of epoch-34 batch-32 = 2.6008114218711853e-05

Training epoch-34 batch-33
Running loss of epoch-34 batch-33 = 2.1318206563591957e-05

Training epoch-34 batch-34
Running loss of epoch-34 batch-34 = 5.9425365179777145e-06

Training epoch-34 batch-35
Running loss of epoch-34 batch-35 = 1.1285534128546715e-05

Training epoch-34 batch-36
Running loss of epoch-34 batch-36 = 2.953922376036644e-05

Training epoch-34 batch-37
Running loss of epoch-34 batch-37 = 2.839113585650921e-05

Training epoch-34 batch-38
Running loss of epoch-34 batch-38 = 2.6602763682603836e-05

Training epoch-34 batch-39
Running loss of epoch-34 batch-39 = 3.97007679566741e-05

Training epoch-34 batch-40
Running loss of epoch-34 batch-40 = 6.234738975763321e-05

Training epoch-34 batch-41
Running loss of epoch-34 batch-41 = 2.4283071979880333e-05

Training epoch-34 batch-42
Running loss of epoch-34 batch-42 = 8.024880662560463e-05

Training epoch-34 batch-43
Running loss of epoch-34 batch-43 = 4.6721892431378365e-05

Training epoch-34 batch-44
Running loss of epoch-34 batch-44 = 5.083344876766205e-05

Training epoch-34 batch-45
Running loss of epoch-34 batch-45 = 3.131851553916931e-05

Training epoch-34 batch-46
Running loss of epoch-34 batch-46 = 1.680571585893631e-05

Training epoch-34 batch-47
Running loss of epoch-34 batch-47 = 3.929901868104935e-05

Training epoch-34 batch-48
Running loss of epoch-34 batch-48 = 2.1047191694378853e-05

Training epoch-34 batch-49
Running loss of epoch-34 batch-49 = 2.430821768939495e-05

Training epoch-34 batch-50
Running loss of epoch-34 batch-50 = 4.110950976610184e-05

Training epoch-34 batch-51
Running loss of epoch-34 batch-51 = 2.6921508833765984e-05

Training epoch-34 batch-52
Running loss of epoch-34 batch-52 = 7.888977415859699e-05

Training epoch-34 batch-53
Running loss of epoch-34 batch-53 = 2.3023691028356552e-05

Training epoch-34 batch-54
Running loss of epoch-34 batch-54 = 3.991182893514633e-06

Training epoch-34 batch-55
Running loss of epoch-34 batch-55 = 3.9465026929974556e-05

Training epoch-34 batch-56
Running loss of epoch-34 batch-56 = 1.4709075912833214e-05

Training epoch-34 batch-57
Running loss of epoch-34 batch-57 = 5.643046461045742e-05

Training epoch-34 batch-58
Running loss of epoch-34 batch-58 = 8.571380749344826e-05

Training epoch-34 batch-59
Running loss of epoch-34 batch-59 = 3.6194222047924995e-05

Training epoch-34 batch-60
Running loss of epoch-34 batch-60 = 4.9852184019982815e-05

Training epoch-34 batch-61
Running loss of epoch-34 batch-61 = 1.8640654161572456e-05

Training epoch-34 batch-62
Running loss of epoch-34 batch-62 = 1.6016187146306038e-05

Training epoch-34 batch-63
Running loss of epoch-34 batch-63 = 9.487918578088284e-05

Training epoch-34 batch-64
Running loss of epoch-34 batch-64 = 3.433879464864731e-05

Training epoch-34 batch-65
Running loss of epoch-34 batch-65 = 2.9876362532377243e-05

Training epoch-34 batch-66
Running loss of epoch-34 batch-66 = 3.9511825889348984e-05

Training epoch-34 batch-67
Running loss of epoch-34 batch-67 = 3.0888011679053307e-05

Training epoch-34 batch-68
Running loss of epoch-34 batch-68 = 4.573469050228596e-05

Training epoch-34 batch-69
Running loss of epoch-34 batch-69 = 3.384146839380264e-05

Training epoch-34 batch-70
Running loss of epoch-34 batch-70 = 4.9358466640114784e-05

Training epoch-34 batch-71
Running loss of epoch-34 batch-71 = 2.9102899134159088e-05

Training epoch-34 batch-72
Running loss of epoch-34 batch-72 = 0.00010378495790064335

Training epoch-34 batch-73
Running loss of epoch-34 batch-73 = 3.570551052689552e-05

Training epoch-34 batch-74
Running loss of epoch-34 batch-74 = 1.549883745610714e-05

Training epoch-34 batch-75
Running loss of epoch-34 batch-75 = 3.554695285856724e-05

Training epoch-34 batch-76
Running loss of epoch-34 batch-76 = 7.297878619283438e-05

Training epoch-34 batch-77
Running loss of epoch-34 batch-77 = 1.5878351405262947e-05

Training epoch-34 batch-78
Running loss of epoch-34 batch-78 = 2.053147181868553e-05

Training epoch-34 batch-79
Running loss of epoch-34 batch-79 = 3.205123357474804e-05

Training epoch-34 batch-80
Running loss of epoch-34 batch-80 = 1.4679506421089172e-05

Training epoch-34 batch-81
Running loss of epoch-34 batch-81 = 3.340491093695164e-05

Training epoch-34 batch-82
Running loss of epoch-34 batch-82 = 7.425213698297739e-05

Training epoch-34 batch-83
Running loss of epoch-34 batch-83 = 1.8887920305132866e-05

Training epoch-34 batch-84
Running loss of epoch-34 batch-84 = 3.4864991903305054e-05

Training epoch-34 batch-85
Running loss of epoch-34 batch-85 = 4.7157518565654755e-05

Training epoch-34 batch-86
Running loss of epoch-34 batch-86 = 7.39656388759613e-06

Training epoch-34 batch-87
Running loss of epoch-34 batch-87 = 3.555021248757839e-05

Training epoch-34 batch-88
Running loss of epoch-34 batch-88 = 2.0356616005301476e-05

Training epoch-34 batch-89
Running loss of epoch-34 batch-89 = 1.5700003132224083e-05

Training epoch-34 batch-90
Running loss of epoch-34 batch-90 = 2.193520776927471e-05

Training epoch-34 batch-91
Running loss of epoch-34 batch-91 = 4.262290894985199e-05

Training epoch-34 batch-92
Running loss of epoch-34 batch-92 = 5.0672562792897224e-05

Training epoch-34 batch-93
Running loss of epoch-34 batch-93 = 2.011074684560299e-05

Training epoch-34 batch-94
Running loss of epoch-34 batch-94 = 2.5046057999134064e-05

Training epoch-34 batch-95
Running loss of epoch-34 batch-95 = 1.186109147965908e-05

Training epoch-34 batch-96
Running loss of epoch-34 batch-96 = 2.0863953977823257e-05

Training epoch-34 batch-97
Running loss of epoch-34 batch-97 = 1.6624340787529945e-05

Training epoch-34 batch-98
Running loss of epoch-34 batch-98 = 5.6810444220900536e-05

Training epoch-34 batch-99
Running loss of epoch-34 batch-99 = 1.3440148904919624e-05

Training epoch-34 batch-100
Running loss of epoch-34 batch-100 = 2.1892599761486053e-05

Training epoch-34 batch-101
Running loss of epoch-34 batch-101 = 0.00011800485663115978

Training epoch-34 batch-102
Running loss of epoch-34 batch-102 = 2.2872351109981537e-05

Training epoch-34 batch-103
Running loss of epoch-34 batch-103 = 4.068238195031881e-05

Training epoch-34 batch-104
Running loss of epoch-34 batch-104 = 1.4525838196277618e-05

Training epoch-34 batch-105
Running loss of epoch-34 batch-105 = 5.9409067034721375e-05

Training epoch-34 batch-106
Running loss of epoch-34 batch-106 = 2.3155473172664642e-05

Training epoch-34 batch-107
Running loss of epoch-34 batch-107 = 5.126860924065113e-05

Training epoch-34 batch-108
Running loss of epoch-34 batch-108 = 3.2150186598300934e-05

Training epoch-34 batch-109
Running loss of epoch-34 batch-109 = 1.1851545423269272e-05

Training epoch-34 batch-110
Running loss of epoch-34 batch-110 = 2.1396903321146965e-05

Training epoch-34 batch-111
Running loss of epoch-34 batch-111 = 2.8315698727965355e-05

Training epoch-34 batch-112
Running loss of epoch-34 batch-112 = 2.100132405757904e-05

Training epoch-34 batch-113
Running loss of epoch-34 batch-113 = 4.845275543630123e-05

Training epoch-34 batch-114
Running loss of epoch-34 batch-114 = 0.00012461538426578045

Training epoch-34 batch-115
Running loss of epoch-34 batch-115 = 2.3058615624904633e-05

Training epoch-34 batch-116
Running loss of epoch-34 batch-116 = 5.622813478112221e-05

Training epoch-34 batch-117
Running loss of epoch-34 batch-117 = 1.6959384083747864e-05

Training epoch-34 batch-118
Running loss of epoch-34 batch-118 = 2.7906382456421852e-05

Training epoch-34 batch-119
Running loss of epoch-34 batch-119 = 3.222562372684479e-05

Training epoch-34 batch-120
Running loss of epoch-34 batch-120 = 5.847681313753128e-05

Training epoch-34 batch-121
Running loss of epoch-34 batch-121 = 1.6371719539165497e-05

Training epoch-34 batch-122
Running loss of epoch-34 batch-122 = 2.8615817427635193e-05

Training epoch-34 batch-123
Running loss of epoch-34 batch-123 = 3.0977651476860046e-05

Training epoch-34 batch-124
Running loss of epoch-34 batch-124 = 2.4155713617801666e-05

Training epoch-34 batch-125
Running loss of epoch-34 batch-125 = 3.578886389732361e-05

Training epoch-34 batch-126
Running loss of epoch-34 batch-126 = 6.433925591409206e-05

Training epoch-34 batch-127
Running loss of epoch-34 batch-127 = 4.08245250582695e-05

Training epoch-34 batch-128
Running loss of epoch-34 batch-128 = 3.249035216867924e-05

Training epoch-34 batch-129
Running loss of epoch-34 batch-129 = 1.791166141629219e-05

Training epoch-34 batch-130
Running loss of epoch-34 batch-130 = 3.267964348196983e-05

Training epoch-34 batch-131
Running loss of epoch-34 batch-131 = 0.0002266969531774521

Training epoch-34 batch-132
Running loss of epoch-34 batch-132 = 7.251277565956116e-06

Training epoch-34 batch-133
Running loss of epoch-34 batch-133 = 3.7656864151358604e-05

Training epoch-34 batch-134
Running loss of epoch-34 batch-134 = 4.823622293770313e-05

Training epoch-34 batch-135
Running loss of epoch-34 batch-135 = 1.8871738575398922e-05

Training epoch-34 batch-136
Running loss of epoch-34 batch-136 = 3.108917735517025e-05

Training epoch-34 batch-137
Running loss of epoch-34 batch-137 = 1.620640978217125e-05

Training epoch-34 batch-138
Running loss of epoch-34 batch-138 = 2.5392742827534676e-05

Training epoch-34 batch-139
Running loss of epoch-34 batch-139 = 6.995140574872494e-05

Training epoch-34 batch-140
Running loss of epoch-34 batch-140 = 8.568493649363518e-05

Training epoch-34 batch-141
Running loss of epoch-34 batch-141 = 9.701529052108526e-05

Training epoch-34 batch-142
Running loss of epoch-34 batch-142 = 1.0047107934951782e-05

Training epoch-34 batch-143
Running loss of epoch-34 batch-143 = 4.0111131966114044e-05

Training epoch-34 batch-144
Running loss of epoch-34 batch-144 = 3.336183726787567e-05

Training epoch-34 batch-145
Running loss of epoch-34 batch-145 = 7.344153709709644e-05

Training epoch-34 batch-146
Running loss of epoch-34 batch-146 = 2.4539651349186897e-05

Training epoch-34 batch-147
Running loss of epoch-34 batch-147 = 4.9484893679618835e-05

Training epoch-34 batch-148
Running loss of epoch-34 batch-148 = 1.8442049622535706e-05

Training epoch-34 batch-149
Running loss of epoch-34 batch-149 = 7.360940799117088e-06

Training epoch-34 batch-150
Running loss of epoch-34 batch-150 = 8.406536653637886e-05

Training epoch-34 batch-151
Running loss of epoch-34 batch-151 = 1.77859328687191e-05

Training epoch-34 batch-152
Running loss of epoch-34 batch-152 = 2.1210405975580215e-05

Training epoch-34 batch-153
Running loss of epoch-34 batch-153 = 3.688083961606026e-05

Training epoch-34 batch-154
Running loss of epoch-34 batch-154 = 2.5011831894516945e-05

Training epoch-34 batch-155
Running loss of epoch-34 batch-155 = 8.457619696855545e-05

Training epoch-34 batch-156
Running loss of epoch-34 batch-156 = 2.4976441636681557e-05

Training epoch-34 batch-157
Running loss of epoch-34 batch-157 = 2.9187649488449097e-05

Finished training epoch-34.



Average train loss at epoch-34 = 3.6875333636999133e-05

Started Evaluation

Average val loss at epoch-34 = 1.45173242283558

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 90.98 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 66.44 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 48.21 %
Accuracy for class execute is: 49.80 %
Accuracy for class get is: 64.87 %

Overall Accuracy = 81.35 %

Finished Evaluation



Started training epoch-35


Training epoch-35 batch-1
Running loss of epoch-35 batch-1 = 2.1395273506641388e-05

Training epoch-35 batch-2
Running loss of epoch-35 batch-2 = 1.6069505363702774e-05

Training epoch-35 batch-3
Running loss of epoch-35 batch-3 = 6.384565494954586e-05

Training epoch-35 batch-4
Running loss of epoch-35 batch-4 = 2.9803486540913582e-05

Training epoch-35 batch-5
Running loss of epoch-35 batch-5 = 4.928046837449074e-05

Training epoch-35 batch-6
Running loss of epoch-35 batch-6 = 1.0307179763913155e-05

Training epoch-35 batch-7
Running loss of epoch-35 batch-7 = 1.7662765458226204e-05

Training epoch-35 batch-8
Running loss of epoch-35 batch-8 = 2.8409063816070557e-05

Training epoch-35 batch-9
Running loss of epoch-35 batch-9 = 2.8084497898817062e-05

Training epoch-35 batch-10
Running loss of epoch-35 batch-10 = 1.2041768059134483e-05

Training epoch-35 batch-11
Running loss of epoch-35 batch-11 = 1.8531223759055138e-05

Training epoch-35 batch-12
Running loss of epoch-35 batch-12 = 7.86732416599989e-05

Training epoch-35 batch-13
Running loss of epoch-35 batch-13 = 1.6743317246437073e-05

Training epoch-35 batch-14
Running loss of epoch-35 batch-14 = 1.736101694405079e-05

Training epoch-35 batch-15
Running loss of epoch-35 batch-15 = 4.794646520167589e-05

Training epoch-35 batch-16
Running loss of epoch-35 batch-16 = 4.4538406655192375e-05

Training epoch-35 batch-17
Running loss of epoch-35 batch-17 = 2.9426533728837967e-05

Training epoch-35 batch-18
Running loss of epoch-35 batch-18 = 5.955807864665985e-06

Training epoch-35 batch-19
Running loss of epoch-35 batch-19 = 9.818235412240028e-06

Training epoch-35 batch-20
Running loss of epoch-35 batch-20 = 1.4805234968662262e-05

Training epoch-35 batch-21
Running loss of epoch-35 batch-21 = 2.656690776348114e-05

Training epoch-35 batch-22
Running loss of epoch-35 batch-22 = 2.5489134714007378e-05

Training epoch-35 batch-23
Running loss of epoch-35 batch-23 = 3.212038427591324e-05

Training epoch-35 batch-24
Running loss of epoch-35 batch-24 = 1.657218672335148e-05

Training epoch-35 batch-25
Running loss of epoch-35 batch-25 = 1.8118880689144135e-05

Training epoch-35 batch-26
Running loss of epoch-35 batch-26 = 3.907759673893452e-05

Training epoch-35 batch-27
Running loss of epoch-35 batch-27 = 3.3680349588394165e-05

Training epoch-35 batch-28
Running loss of epoch-35 batch-28 = 2.730335108935833e-05

Training epoch-35 batch-29
Running loss of epoch-35 batch-29 = 2.57131177932024e-05

Training epoch-35 batch-30
Running loss of epoch-35 batch-30 = 5.9962738305330276e-05

Training epoch-35 batch-31
Running loss of epoch-35 batch-31 = 1.0844552889466286e-05

Training epoch-35 batch-32
Running loss of epoch-35 batch-32 = 3.144890069961548e-05

Training epoch-35 batch-33
Running loss of epoch-35 batch-33 = 4.596752114593983e-05

Training epoch-35 batch-34
Running loss of epoch-35 batch-34 = 6.597721949219704e-05

Training epoch-35 batch-35
Running loss of epoch-35 batch-35 = 1.889723353087902e-05

Training epoch-35 batch-36
Running loss of epoch-35 batch-36 = 3.523658961057663e-06

Training epoch-35 batch-37
Running loss of epoch-35 batch-37 = 3.4015392884612083e-05

Training epoch-35 batch-38
Running loss of epoch-35 batch-38 = 3.0632130801677704e-05

Training epoch-35 batch-39
Running loss of epoch-35 batch-39 = 2.0182225853204727e-05

Training epoch-35 batch-40
Running loss of epoch-35 batch-40 = 3.239908255636692e-05

Training epoch-35 batch-41
Running loss of epoch-35 batch-41 = 4.5191613025963306e-05

Training epoch-35 batch-42
Running loss of epoch-35 batch-42 = 2.1380838006734848e-05

Training epoch-35 batch-43
Running loss of epoch-35 batch-43 = 2.575572580099106e-05

Training epoch-35 batch-44
Running loss of epoch-35 batch-44 = 3.3809803426265717e-05

Training epoch-35 batch-45
Running loss of epoch-35 batch-45 = 1.723645254969597e-05

Training epoch-35 batch-46
Running loss of epoch-35 batch-46 = 4.86013013869524e-05

Training epoch-35 batch-47
Running loss of epoch-35 batch-47 = 9.916024282574654e-06

Training epoch-35 batch-48
Running loss of epoch-35 batch-48 = 2.4919863790273666e-05

Training epoch-35 batch-49
Running loss of epoch-35 batch-49 = 2.1354760974645615e-05

Training epoch-35 batch-50
Running loss of epoch-35 batch-50 = 2.2630207240581512e-05

Training epoch-35 batch-51
Running loss of epoch-35 batch-51 = 1.373235136270523e-05

Training epoch-35 batch-52
Running loss of epoch-35 batch-52 = 3.0833762139081955e-05

Training epoch-35 batch-53
Running loss of epoch-35 batch-53 = 8.40378925204277e-06

Training epoch-35 batch-54
Running loss of epoch-35 batch-54 = 9.984476491808891e-06

Training epoch-35 batch-55
Running loss of epoch-35 batch-55 = 1.8524006009101868e-05

Training epoch-35 batch-56
Running loss of epoch-35 batch-56 = 1.1184951290488243e-05

Training epoch-35 batch-57
Running loss of epoch-35 batch-57 = 0.00010057108011096716

Training epoch-35 batch-58
Running loss of epoch-35 batch-58 = 4.19106800109148e-05

Training epoch-35 batch-59
Running loss of epoch-35 batch-59 = 7.859221659600735e-05

Training epoch-35 batch-60
Running loss of epoch-35 batch-60 = 2.216966822743416e-05

Training epoch-35 batch-61
Running loss of epoch-35 batch-61 = 3.0209077522158623e-05

Training epoch-35 batch-62
Running loss of epoch-35 batch-62 = 1.7198268324136734e-05

Training epoch-35 batch-63
Running loss of epoch-35 batch-63 = 4.741223528981209e-05

Training epoch-35 batch-64
Running loss of epoch-35 batch-64 = 1.1808471754193306e-05

Training epoch-35 batch-65
Running loss of epoch-35 batch-65 = 3.460189327597618e-05

Training epoch-35 batch-66
Running loss of epoch-35 batch-66 = 1.3743061572313309e-05

Training epoch-35 batch-67
Running loss of epoch-35 batch-67 = 1.8728431314229965e-05

Training epoch-35 batch-68
Running loss of epoch-35 batch-68 = 1.3531884178519249e-05

Training epoch-35 batch-69
Running loss of epoch-35 batch-69 = 4.150089807808399e-05

Training epoch-35 batch-70
Running loss of epoch-35 batch-70 = 1.680641435086727e-05

Training epoch-35 batch-71
Running loss of epoch-35 batch-71 = 2.730567939579487e-05

Training epoch-35 batch-72
Running loss of epoch-35 batch-72 = 5.6874705478549004e-05

Training epoch-35 batch-73
Running loss of epoch-35 batch-73 = 2.0105857402086258e-05

Training epoch-35 batch-74
Running loss of epoch-35 batch-74 = 7.923319935798645e-05

Training epoch-35 batch-75
Running loss of epoch-35 batch-75 = 1.2697651982307434e-05

Training epoch-35 batch-76
Running loss of epoch-35 batch-76 = 2.582860179245472e-05

Training epoch-35 batch-77
Running loss of epoch-35 batch-77 = 2.833339385688305e-05

Training epoch-35 batch-78
Running loss of epoch-35 batch-78 = 3.382493741810322e-05

Training epoch-35 batch-79
Running loss of epoch-35 batch-79 = 1.1402647942304611e-05

Training epoch-35 batch-80
Running loss of epoch-35 batch-80 = 1.6285572201013565e-05

Training epoch-35 batch-81
Running loss of epoch-35 batch-81 = 2.317572943866253e-05

Training epoch-35 batch-82
Running loss of epoch-35 batch-82 = 1.9302591681480408e-05

Training epoch-35 batch-83
Running loss of epoch-35 batch-83 = 5.34416176378727e-06

Training epoch-35 batch-84
Running loss of epoch-35 batch-84 = 4.7262292355298996e-05

Training epoch-35 batch-85
Running loss of epoch-35 batch-85 = 4.054117016494274e-05

Training epoch-35 batch-86
Running loss of epoch-35 batch-86 = 1.4326302334666252e-05

Training epoch-35 batch-87
Running loss of epoch-35 batch-87 = 4.147854633629322e-05

Training epoch-35 batch-88
Running loss of epoch-35 batch-88 = 9.829644113779068e-06

Training epoch-35 batch-89
Running loss of epoch-35 batch-89 = 1.7269747331738472e-05

Training epoch-35 batch-90
Running loss of epoch-35 batch-90 = 2.209027297794819e-05

Training epoch-35 batch-91
Running loss of epoch-35 batch-91 = 4.3036649003624916e-05

Training epoch-35 batch-92
Running loss of epoch-35 batch-92 = 1.0994262993335724e-05

Training epoch-35 batch-93
Running loss of epoch-35 batch-93 = 2.1749408915638924e-05

Training epoch-35 batch-94
Running loss of epoch-35 batch-94 = 0.0001261378638446331

Training epoch-35 batch-95
Running loss of epoch-35 batch-95 = 8.022645488381386e-06

Training epoch-35 batch-96
Running loss of epoch-35 batch-96 = 2.5208573788404465e-05

Training epoch-35 batch-97
Running loss of epoch-35 batch-97 = 1.3314886018633842e-05

Training epoch-35 batch-98
Running loss of epoch-35 batch-98 = 2.950918860733509e-05

Training epoch-35 batch-99
Running loss of epoch-35 batch-99 = 2.2883759811520576e-05

Training epoch-35 batch-100
Running loss of epoch-35 batch-100 = 3.623310476541519e-05

Training epoch-35 batch-101
Running loss of epoch-35 batch-101 = 4.485039971768856e-05

Training epoch-35 batch-102
Running loss of epoch-35 batch-102 = 1.2189848348498344e-05

Training epoch-35 batch-103
Running loss of epoch-35 batch-103 = 1.0799150913953781e-05

Training epoch-35 batch-104
Running loss of epoch-35 batch-104 = 2.0159408450126648e-05

Training epoch-35 batch-105
Running loss of epoch-35 batch-105 = 3.7055229768157005e-05

Training epoch-35 batch-106
Running loss of epoch-35 batch-106 = 1.77829060703516e-05

Training epoch-35 batch-107
Running loss of epoch-35 batch-107 = 2.9621878638863564e-05

Training epoch-35 batch-108
Running loss of epoch-35 batch-108 = 2.0021572709083557e-05

Training epoch-35 batch-109
Running loss of epoch-35 batch-109 = 4.1961902752518654e-05

Training epoch-35 batch-110
Running loss of epoch-35 batch-110 = 0.0001103703398257494

Training epoch-35 batch-111
Running loss of epoch-35 batch-111 = 1.5216413885354996e-05

Training epoch-35 batch-112
Running loss of epoch-35 batch-112 = 1.0151881724596024e-05

Training epoch-35 batch-113
Running loss of epoch-35 batch-113 = 0.0002632937394082546

Training epoch-35 batch-114
Running loss of epoch-35 batch-114 = 9.507755748927593e-05

Training epoch-35 batch-115
Running loss of epoch-35 batch-115 = 4.242081195116043e-05

Training epoch-35 batch-116
Running loss of epoch-35 batch-116 = 4.098634235560894e-05

Training epoch-35 batch-117
Running loss of epoch-35 batch-117 = 8.90018418431282e-06

Training epoch-35 batch-118
Running loss of epoch-35 batch-118 = 3.165728412568569e-05

Training epoch-35 batch-119
Running loss of epoch-35 batch-119 = 3.07795125991106e-05

Training epoch-35 batch-120
Running loss of epoch-35 batch-120 = 3.0800700187683105e-05

Training epoch-35 batch-121
Running loss of epoch-35 batch-121 = 2.6389723643660545e-05

Training epoch-35 batch-122
Running loss of epoch-35 batch-122 = 0.0003146325470879674

Training epoch-35 batch-123
Running loss of epoch-35 batch-123 = 7.382035255432129e-05

Training epoch-35 batch-124
Running loss of epoch-35 batch-124 = 6.149173714220524e-05

Training epoch-35 batch-125
Running loss of epoch-35 batch-125 = 4.454306326806545e-05

Training epoch-35 batch-126
Running loss of epoch-35 batch-126 = 5.650264210999012e-05

Training epoch-35 batch-127
Running loss of epoch-35 batch-127 = 1.26685481518507e-05

Training epoch-35 batch-128
Running loss of epoch-35 batch-128 = 1.2940727174282074e-05

Training epoch-35 batch-129
Running loss of epoch-35 batch-129 = 1.7760787159204483e-05

Training epoch-35 batch-130
Running loss of epoch-35 batch-130 = 1.444225199520588e-05

Training epoch-35 batch-131
Running loss of epoch-35 batch-131 = 1.105363480746746e-05

Training epoch-35 batch-132
Running loss of epoch-35 batch-132 = 4.0489016100764275e-05

Training epoch-35 batch-133
Running loss of epoch-35 batch-133 = 3.098789602518082e-05

Training epoch-35 batch-134
Running loss of epoch-35 batch-134 = 2.454454079270363e-05

Training epoch-35 batch-135
Running loss of epoch-35 batch-135 = 1.8969643861055374e-05

Training epoch-35 batch-136
Running loss of epoch-35 batch-136 = 1.3419194146990776e-05

Training epoch-35 batch-137
Running loss of epoch-35 batch-137 = 2.1761981770396233e-05

Training epoch-35 batch-138
Running loss of epoch-35 batch-138 = 1.0351883247494698e-05

Training epoch-35 batch-139
Running loss of epoch-35 batch-139 = 1.0365154594182968e-05

Training epoch-35 batch-140
Running loss of epoch-35 batch-140 = 2.850731834769249e-05

Training epoch-35 batch-141
Running loss of epoch-35 batch-141 = 3.480585291981697e-05

Training epoch-35 batch-142
Running loss of epoch-35 batch-142 = 3.0274270102381706e-05

Training epoch-35 batch-143
Running loss of epoch-35 batch-143 = 3.0701979994773865e-05

Training epoch-35 batch-144
Running loss of epoch-35 batch-144 = 1.4919554814696312e-05

Training epoch-35 batch-145
Running loss of epoch-35 batch-145 = 3.062048926949501e-05

Training epoch-35 batch-146
Running loss of epoch-35 batch-146 = 1.5549827367067337e-05

Training epoch-35 batch-147
Running loss of epoch-35 batch-147 = 5.301949568092823e-05

Training epoch-35 batch-148
Running loss of epoch-35 batch-148 = 3.196648322045803e-05

Training epoch-35 batch-149
Running loss of epoch-35 batch-149 = 2.5394372642040253e-05

Training epoch-35 batch-150
Running loss of epoch-35 batch-150 = 3.2549258321523666e-05

Training epoch-35 batch-151
Running loss of epoch-35 batch-151 = 4.474981687963009e-05

Training epoch-35 batch-152
Running loss of epoch-35 batch-152 = 3.710831515491009e-05

Training epoch-35 batch-153
Running loss of epoch-35 batch-153 = 5.380064249038696e-05

Training epoch-35 batch-154
Running loss of epoch-35 batch-154 = 2.9424205422401428e-05

Training epoch-35 batch-155
Running loss of epoch-35 batch-155 = 2.6652123779058456e-05

Training epoch-35 batch-156
Running loss of epoch-35 batch-156 = 2.798996865749359e-05

Training epoch-35 batch-157
Running loss of epoch-35 batch-157 = 3.758817911148071e-05

Finished training epoch-35.



Average train loss at epoch-35 = 3.321409523487091e-05

Started Evaluation

Average val loss at epoch-35 = 1.4881849382299404

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 89.45 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 49.55 %
Accuracy for class execute is: 53.41 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.33 %

Finished Evaluation



Started training epoch-36


Training epoch-36 batch-1
Running loss of epoch-36 batch-1 = 1.2499745935201645e-05

Training epoch-36 batch-2
Running loss of epoch-36 batch-2 = 1.5581143088638783e-05

Training epoch-36 batch-3
Running loss of epoch-36 batch-3 = 1.5250174328684807e-05

Training epoch-36 batch-4
Running loss of epoch-36 batch-4 = 1.880992203950882e-05

Training epoch-36 batch-5
Running loss of epoch-36 batch-5 = 2.6653986424207687e-05

Training epoch-36 batch-6
Running loss of epoch-36 batch-6 = 7.403315976262093e-06

Training epoch-36 batch-7
Running loss of epoch-36 batch-7 = 2.6523368433117867e-05

Training epoch-36 batch-8
Running loss of epoch-36 batch-8 = 3.02768312394619e-05

Training epoch-36 batch-9
Running loss of epoch-36 batch-9 = 5.6889839470386505e-06

Training epoch-36 batch-10
Running loss of epoch-36 batch-10 = 4.384201020002365e-05

Training epoch-36 batch-11
Running loss of epoch-36 batch-11 = 2.015708014369011e-05

Training epoch-36 batch-12
Running loss of epoch-36 batch-12 = 8.174683898687363e-06

Training epoch-36 batch-13
Running loss of epoch-36 batch-13 = 3.157765604555607e-05

Training epoch-36 batch-14
Running loss of epoch-36 batch-14 = 5.2091898396611214e-05

Training epoch-36 batch-15
Running loss of epoch-36 batch-15 = 5.6087737902998924e-05

Training epoch-36 batch-16
Running loss of epoch-36 batch-16 = 1.5596160665154457e-05

Training epoch-36 batch-17
Running loss of epoch-36 batch-17 = 3.042072057723999e-05

Training epoch-36 batch-18
Running loss of epoch-36 batch-18 = 3.872811794281006e-05

Training epoch-36 batch-19
Running loss of epoch-36 batch-19 = 4.398683086037636e-05

Training epoch-36 batch-20
Running loss of epoch-36 batch-20 = 2.9880786314606667e-05

Training epoch-36 batch-21
Running loss of epoch-36 batch-21 = 1.1700903996825218e-05

Training epoch-36 batch-22
Running loss of epoch-36 batch-22 = 2.559309359639883e-05

Training epoch-36 batch-23
Running loss of epoch-36 batch-23 = 6.31196890026331e-05

Training epoch-36 batch-24
Running loss of epoch-36 batch-24 = 2.9321759939193726e-05

Training epoch-36 batch-25
Running loss of epoch-36 batch-25 = 3.7190504372119904e-05

Training epoch-36 batch-26
Running loss of epoch-36 batch-26 = 1.2042466551065445e-05

Training epoch-36 batch-27
Running loss of epoch-36 batch-27 = 1.1126743629574776e-05

Training epoch-36 batch-28
Running loss of epoch-36 batch-28 = 2.032681368291378e-05

Training epoch-36 batch-29
Running loss of epoch-36 batch-29 = 3.0094292014837265e-05

Training epoch-36 batch-30
Running loss of epoch-36 batch-30 = 4.138587974011898e-05

Training epoch-36 batch-31
Running loss of epoch-36 batch-31 = 4.6064844354987144e-05

Training epoch-36 batch-32
Running loss of epoch-36 batch-32 = 1.837196759879589e-05

Training epoch-36 batch-33
Running loss of epoch-36 batch-33 = 5.006534047424793e-05

Training epoch-36 batch-34
Running loss of epoch-36 batch-34 = 4.0409620851278305e-05

Training epoch-36 batch-35
Running loss of epoch-36 batch-35 = 3.927387297153473e-06

Training epoch-36 batch-36
Running loss of epoch-36 batch-36 = 1.6221776604652405e-05

Training epoch-36 batch-37
Running loss of epoch-36 batch-37 = 3.0995113775134087e-05

Training epoch-36 batch-38
Running loss of epoch-36 batch-38 = 1.2831995263695717e-05

Training epoch-36 batch-39
Running loss of epoch-36 batch-39 = 1.252233050763607e-05

Training epoch-36 batch-40
Running loss of epoch-36 batch-40 = 1.9889790564775467e-05

Training epoch-36 batch-41
Running loss of epoch-36 batch-41 = 3.846967592835426e-05

Training epoch-36 batch-42
Running loss of epoch-36 batch-42 = 1.3076234608888626e-05

Training epoch-36 batch-43
Running loss of epoch-36 batch-43 = 1.3618730008602142e-05

Training epoch-36 batch-44
Running loss of epoch-36 batch-44 = 1.9831815734505653e-05

Training epoch-36 batch-45
Running loss of epoch-36 batch-45 = 2.4320092052221298e-05

Training epoch-36 batch-46
Running loss of epoch-36 batch-46 = 1.5058554708957672e-05

Training epoch-36 batch-47
Running loss of epoch-36 batch-47 = 9.671016596257687e-05

Training epoch-36 batch-48
Running loss of epoch-36 batch-48 = 1.3886252418160439e-05

Training epoch-36 batch-49
Running loss of epoch-36 batch-49 = 2.1832995116710663e-05

Training epoch-36 batch-50
Running loss of epoch-36 batch-50 = 8.554081432521343e-05

Training epoch-36 batch-51
Running loss of epoch-36 batch-51 = 5.424092523753643e-05

Training epoch-36 batch-52
Running loss of epoch-36 batch-52 = 6.774440407752991e-06

Training epoch-36 batch-53
Running loss of epoch-36 batch-53 = 7.167458534240723e-05

Training epoch-36 batch-54
Running loss of epoch-36 batch-54 = 9.573996067047119e-06

Training epoch-36 batch-55
Running loss of epoch-36 batch-55 = 2.454034984111786e-05

Training epoch-36 batch-56
Running loss of epoch-36 batch-56 = 2.7051661163568497e-05

Training epoch-36 batch-57
Running loss of epoch-36 batch-57 = 8.35862010717392e-06

Training epoch-36 batch-58
Running loss of epoch-36 batch-58 = 2.369075082242489e-05

Training epoch-36 batch-59
Running loss of epoch-36 batch-59 = 1.760409213602543e-05

Training epoch-36 batch-60
Running loss of epoch-36 batch-60 = 2.0524952560663223e-05

Training epoch-36 batch-61
Running loss of epoch-36 batch-61 = 4.784739576280117e-05

Training epoch-36 batch-62
Running loss of epoch-36 batch-62 = 1.966068521142006e-05

Training epoch-36 batch-63
Running loss of epoch-36 batch-63 = 2.4802284315228462e-05

Training epoch-36 batch-64
Running loss of epoch-36 batch-64 = 2.409936860203743e-05

Training epoch-36 batch-65
Running loss of epoch-36 batch-65 = 3.387872129678726e-05

Training epoch-36 batch-66
Running loss of epoch-36 batch-66 = 1.737847924232483e-05

Training epoch-36 batch-67
Running loss of epoch-36 batch-67 = 1.8780818209052086e-05

Training epoch-36 batch-68
Running loss of epoch-36 batch-68 = 1.2416159734129906e-05

Training epoch-36 batch-69
Running loss of epoch-36 batch-69 = 0.00014091876801103354

Training epoch-36 batch-70
Running loss of epoch-36 batch-70 = 1.8259044736623764e-05

Training epoch-36 batch-71
Running loss of epoch-36 batch-71 = 2.386770211160183e-05

Training epoch-36 batch-72
Running loss of epoch-36 batch-72 = 2.5891931727528572e-05

Training epoch-36 batch-73
Running loss of epoch-36 batch-73 = 2.0891427993774414e-05

Training epoch-36 batch-74
Running loss of epoch-36 batch-74 = 2.7542700991034508e-05

Training epoch-36 batch-75
Running loss of epoch-36 batch-75 = 9.530223906040192e-06

Training epoch-36 batch-76
Running loss of epoch-36 batch-76 = 2.748798578977585e-05

Training epoch-36 batch-77
Running loss of epoch-36 batch-77 = 0.00012910575605928898

Training epoch-36 batch-78
Running loss of epoch-36 batch-78 = 3.051036037504673e-05

Training epoch-36 batch-79
Running loss of epoch-36 batch-79 = 1.4203134924173355e-05

Training epoch-36 batch-80
Running loss of epoch-36 batch-80 = 1.5724217519164085e-05

Training epoch-36 batch-81
Running loss of epoch-36 batch-81 = 4.6379631385207176e-05

Training epoch-36 batch-82
Running loss of epoch-36 batch-82 = 1.234305091202259e-05

Training epoch-36 batch-83
Running loss of epoch-36 batch-83 = 1.103617250919342e-05

Training epoch-36 batch-84
Running loss of epoch-36 batch-84 = 1.3524200767278671e-05

Training epoch-36 batch-85
Running loss of epoch-36 batch-85 = 1.8094666302204132e-05

Training epoch-36 batch-86
Running loss of epoch-36 batch-86 = 1.18291936814785e-05

Training epoch-36 batch-87
Running loss of epoch-36 batch-87 = 2.228771336376667e-05

Training epoch-36 batch-88
Running loss of epoch-36 batch-88 = 7.861224003136158e-05

Training epoch-36 batch-89
Running loss of epoch-36 batch-89 = 3.2635871320962906e-05

Training epoch-36 batch-90
Running loss of epoch-36 batch-90 = 2.502650022506714e-05

Training epoch-36 batch-91
Running loss of epoch-36 batch-91 = 4.433770664036274e-05

Training epoch-36 batch-92
Running loss of epoch-36 batch-92 = 5.0975242629647255e-05

Training epoch-36 batch-93
Running loss of epoch-36 batch-93 = 6.017275154590607e-06

Training epoch-36 batch-94
Running loss of epoch-36 batch-94 = 1.4043180271983147e-05

Training epoch-36 batch-95
Running loss of epoch-36 batch-95 = 1.9540544599294662e-05

Training epoch-36 batch-96
Running loss of epoch-36 batch-96 = 7.5856223702430725e-06

Training epoch-36 batch-97
Running loss of epoch-36 batch-97 = 5.36404550075531e-05

Training epoch-36 batch-98
Running loss of epoch-36 batch-98 = 2.0363135263323784e-05

Training epoch-36 batch-99
Running loss of epoch-36 batch-99 = 1.4792894944548607e-05

Training epoch-36 batch-100
Running loss of epoch-36 batch-100 = 2.588098868727684e-05

Training epoch-36 batch-101
Running loss of epoch-36 batch-101 = 2.391776069998741e-05

Training epoch-36 batch-102
Running loss of epoch-36 batch-102 = 2.0003877580165863e-05

Training epoch-36 batch-103
Running loss of epoch-36 batch-103 = 3.100710455328226e-05

Training epoch-36 batch-104
Running loss of epoch-36 batch-104 = 2.5920337066054344e-05

Training epoch-36 batch-105
Running loss of epoch-36 batch-105 = 5.399598740041256e-05

Training epoch-36 batch-106
Running loss of epoch-36 batch-106 = 3.518746234476566e-05

Training epoch-36 batch-107
Running loss of epoch-36 batch-107 = 8.909485768526793e-05

Training epoch-36 batch-108
Running loss of epoch-36 batch-108 = 9.392620995640755e-06

Training epoch-36 batch-109
Running loss of epoch-36 batch-109 = 3.552716225385666e-05

Training epoch-36 batch-110
Running loss of epoch-36 batch-110 = 2.4845590814948082e-05

Training epoch-36 batch-111
Running loss of epoch-36 batch-111 = 9.714392945170403e-06

Training epoch-36 batch-112
Running loss of epoch-36 batch-112 = 2.968544140458107e-05

Training epoch-36 batch-113
Running loss of epoch-36 batch-113 = 2.112402580678463e-05

Training epoch-36 batch-114
Running loss of epoch-36 batch-114 = 1.6043661162257195e-05

Training epoch-36 batch-115
Running loss of epoch-36 batch-115 = 1.2466218322515488e-05

Training epoch-36 batch-116
Running loss of epoch-36 batch-116 = 1.3849232345819473e-05

Training epoch-36 batch-117
Running loss of epoch-36 batch-117 = 5.9623271226882935e-06

Training epoch-36 batch-118
Running loss of epoch-36 batch-118 = 6.379233673214912e-05

Training epoch-36 batch-119
Running loss of epoch-36 batch-119 = 6.557628512382507e-05

Training epoch-36 batch-120
Running loss of epoch-36 batch-120 = 3.110594116151333e-05

Training epoch-36 batch-121
Running loss of epoch-36 batch-121 = 3.1513627618551254e-05

Training epoch-36 batch-122
Running loss of epoch-36 batch-122 = 1.535378396511078e-05

Training epoch-36 batch-123
Running loss of epoch-36 batch-123 = 3.203633241355419e-05

Training epoch-36 batch-124
Running loss of epoch-36 batch-124 = 3.338535316288471e-05

Training epoch-36 batch-125
Running loss of epoch-36 batch-125 = 9.226379916071892e-06

Training epoch-36 batch-126
Running loss of epoch-36 batch-126 = 1.678941771388054e-05

Training epoch-36 batch-127
Running loss of epoch-36 batch-127 = 1.0395655408501625e-05

Training epoch-36 batch-128
Running loss of epoch-36 batch-128 = 3.335112705826759e-05

Training epoch-36 batch-129
Running loss of epoch-36 batch-129 = 8.273904677480459e-05

Training epoch-36 batch-130
Running loss of epoch-36 batch-130 = 1.0495306923985481e-05

Training epoch-36 batch-131
Running loss of epoch-36 batch-131 = 1.8502119928598404e-05

Training epoch-36 batch-132
Running loss of epoch-36 batch-132 = 7.383991032838821e-05

Training epoch-36 batch-133
Running loss of epoch-36 batch-133 = 3.3133430406451225e-05

Training epoch-36 batch-134
Running loss of epoch-36 batch-134 = 4.260987043380737e-05

Training epoch-36 batch-135
Running loss of epoch-36 batch-135 = 3.663962706923485e-05

Training epoch-36 batch-136
Running loss of epoch-36 batch-136 = 4.61511081084609e-05

Training epoch-36 batch-137
Running loss of epoch-36 batch-137 = 3.0488474294543266e-05

Training epoch-36 batch-138
Running loss of epoch-36 batch-138 = 8.178409188985825e-06

Training epoch-36 batch-139
Running loss of epoch-36 batch-139 = 1.4955876395106316e-05

Training epoch-36 batch-140
Running loss of epoch-36 batch-140 = 0.0001405421644449234

Training epoch-36 batch-141
Running loss of epoch-36 batch-141 = 1.655425876379013e-05

Training epoch-36 batch-142
Running loss of epoch-36 batch-142 = 2.8827926144003868e-05

Training epoch-36 batch-143
Running loss of epoch-36 batch-143 = 2.0796549506485462e-05

Training epoch-36 batch-144
Running loss of epoch-36 batch-144 = 2.8569018468260765e-05

Training epoch-36 batch-145
Running loss of epoch-36 batch-145 = 3.0481256544589996e-05

Training epoch-36 batch-146
Running loss of epoch-36 batch-146 = 3.332365304231644e-05

Training epoch-36 batch-147
Running loss of epoch-36 batch-147 = 2.1757790818810463e-05

Training epoch-36 batch-148
Running loss of epoch-36 batch-148 = 2.2891908884048462e-05

Training epoch-36 batch-149
Running loss of epoch-36 batch-149 = 2.489541657269001e-05

Training epoch-36 batch-150
Running loss of epoch-36 batch-150 = 3.6129727959632874e-05

Training epoch-36 batch-151
Running loss of epoch-36 batch-151 = 1.634843647480011e-05

Training epoch-36 batch-152
Running loss of epoch-36 batch-152 = 2.028234302997589e-05

Training epoch-36 batch-153
Running loss of epoch-36 batch-153 = 6.06500543653965e-06

Training epoch-36 batch-154
Running loss of epoch-36 batch-154 = 4.1093211621046066e-05

Training epoch-36 batch-155
Running loss of epoch-36 batch-155 = 4.815496504306793e-05

Training epoch-36 batch-156
Running loss of epoch-36 batch-156 = 3.5490607842803e-05

Training epoch-36 batch-157
Running loss of epoch-36 batch-157 = 0.00013839080929756165

Finished training epoch-36.



Average train loss at epoch-36 = 2.998627871274948e-05

Started Evaluation

Average val loss at epoch-36 = 1.4962911008759587

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 88.38 %
Accuracy for class toString is: 81.91 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 53.01 %
Accuracy for class get is: 65.13 %

Overall Accuracy = 81.00 %

Finished Evaluation



Started training epoch-37


Training epoch-37 batch-1
Running loss of epoch-37 batch-1 = 1.2868549674749374e-06

Training epoch-37 batch-2
Running loss of epoch-37 batch-2 = 1.4350749552249908e-05

Training epoch-37 batch-3
Running loss of epoch-37 batch-3 = 2.5715911760926247e-05

Training epoch-37 batch-4
Running loss of epoch-37 batch-4 = 6.790738552808762e-06

Training epoch-37 batch-5
Running loss of epoch-37 batch-5 = 9.433389641344547e-05

Training epoch-37 batch-6
Running loss of epoch-37 batch-6 = 1.6165664419531822e-05

Training epoch-37 batch-7
Running loss of epoch-37 batch-7 = 2.9558781534433365e-05

Training epoch-37 batch-8
Running loss of epoch-37 batch-8 = 1.586880534887314e-05

Training epoch-37 batch-9
Running loss of epoch-37 batch-9 = 3.279256634414196e-05

Training epoch-37 batch-10
Running loss of epoch-37 batch-10 = 1.4067860320210457e-05

Training epoch-37 batch-11
Running loss of epoch-37 batch-11 = 1.6185687854886055e-05

Training epoch-37 batch-12
Running loss of epoch-37 batch-12 = 6.732530891895294e-06

Training epoch-37 batch-13
Running loss of epoch-37 batch-13 = 1.8950551748275757e-05

Training epoch-37 batch-14
Running loss of epoch-37 batch-14 = 2.313242293894291e-05

Training epoch-37 batch-15
Running loss of epoch-37 batch-15 = 1.1903000995516777e-05

Training epoch-37 batch-16
Running loss of epoch-37 batch-16 = 1.3884622603654861e-05

Training epoch-37 batch-17
Running loss of epoch-37 batch-17 = 1.2421514838933945e-05

Training epoch-37 batch-18
Running loss of epoch-37 batch-18 = 3.4052878618240356e-05

Training epoch-37 batch-19
Running loss of epoch-37 batch-19 = 1.3730023056268692e-05

Training epoch-37 batch-20
Running loss of epoch-37 batch-20 = 3.154785372316837e-05

Training epoch-37 batch-21
Running loss of epoch-37 batch-21 = 1.3955403119325638e-05

Training epoch-37 batch-22
Running loss of epoch-37 batch-22 = 7.593038026243448e-05

Training epoch-37 batch-23
Running loss of epoch-37 batch-23 = 2.257712185382843e-05

Training epoch-37 batch-24
Running loss of epoch-37 batch-24 = 4.282617010176182e-05

Training epoch-37 batch-25
Running loss of epoch-37 batch-25 = 2.0378967747092247e-05

Training epoch-37 batch-26
Running loss of epoch-37 batch-26 = 2.7973204851150513e-05

Training epoch-37 batch-27
Running loss of epoch-37 batch-27 = 2.3538479581475258e-05

Training epoch-37 batch-28
Running loss of epoch-37 batch-28 = 1.2644333764910698e-05

Training epoch-37 batch-29
Running loss of epoch-37 batch-29 = 3.1669856980443e-05

Training epoch-37 batch-30
Running loss of epoch-37 batch-30 = 2.2834166884422302e-05

Training epoch-37 batch-31
Running loss of epoch-37 batch-31 = 2.7866801247000694e-05

Training epoch-37 batch-32
Running loss of epoch-37 batch-32 = 2.86733265966177e-05

Training epoch-37 batch-33
Running loss of epoch-37 batch-33 = 1.70127023011446e-05

Training epoch-37 batch-34
Running loss of epoch-37 batch-34 = 1.382734626531601e-05

Training epoch-37 batch-35
Running loss of epoch-37 batch-35 = 7.479451596736908e-06

Training epoch-37 batch-36
Running loss of epoch-37 batch-36 = 2.18729255720973e-05

Training epoch-37 batch-37
Running loss of epoch-37 batch-37 = 7.647555321455002e-06

Training epoch-37 batch-38
Running loss of epoch-37 batch-38 = 6.956933066248894e-05

Training epoch-37 batch-39
Running loss of epoch-37 batch-39 = 2.3846281692385674e-05

Training epoch-37 batch-40
Running loss of epoch-37 batch-40 = 3.955094143748283e-06

Training epoch-37 batch-41
Running loss of epoch-37 batch-41 = 1.891050487756729e-05

Training epoch-37 batch-42
Running loss of epoch-37 batch-42 = 2.1927058696746826e-05

Training epoch-37 batch-43
Running loss of epoch-37 batch-43 = 3.799423575401306e-05

Training epoch-37 batch-44
Running loss of epoch-37 batch-44 = 1.7936574295163155e-05

Training epoch-37 batch-45
Running loss of epoch-37 batch-45 = 1.0916730388998985e-05

Training epoch-37 batch-46
Running loss of epoch-37 batch-46 = 2.437736839056015e-05

Training epoch-37 batch-47
Running loss of epoch-37 batch-47 = 1.2005213648080826e-05

Training epoch-37 batch-48
Running loss of epoch-37 batch-48 = 2.4274922907352448e-05

Training epoch-37 batch-49
Running loss of epoch-37 batch-49 = 3.5540200769901276e-05

Training epoch-37 batch-50
Running loss of epoch-37 batch-50 = 3.1391624361276627e-05

Training epoch-37 batch-51
Running loss of epoch-37 batch-51 = 1.5159836038947105e-05

Training epoch-37 batch-52
Running loss of epoch-37 batch-52 = 5.1130540668964386e-05

Training epoch-37 batch-53
Running loss of epoch-37 batch-53 = 2.095499075949192e-05

Training epoch-37 batch-54
Running loss of epoch-37 batch-54 = 1.5675323083996773e-05

Training epoch-37 batch-55
Running loss of epoch-37 batch-55 = 8.583534508943558e-06

Training epoch-37 batch-56
Running loss of epoch-37 batch-56 = 1.5315134078264236e-05

Training epoch-37 batch-57
Running loss of epoch-37 batch-57 = 3.0659139156341553e-06

Training epoch-37 batch-58
Running loss of epoch-37 batch-58 = 1.0599149391055107e-05

Training epoch-37 batch-59
Running loss of epoch-37 batch-59 = 3.249780274927616e-05

Training epoch-37 batch-60
Running loss of epoch-37 batch-60 = 1.1430121958255768e-05

Training epoch-37 batch-61
Running loss of epoch-37 batch-61 = 1.447182148694992e-05

Training epoch-37 batch-62
Running loss of epoch-37 batch-62 = 5.584338214248419e-05

Training epoch-37 batch-63
Running loss of epoch-37 batch-63 = 1.0179588571190834e-05

Training epoch-37 batch-64
Running loss of epoch-37 batch-64 = 1.8652062863111496e-05

Training epoch-37 batch-65
Running loss of epoch-37 batch-65 = 1.845741644501686e-05

Training epoch-37 batch-66
Running loss of epoch-37 batch-66 = 1.8481630831956863e-05

Training epoch-37 batch-67
Running loss of epoch-37 batch-67 = 4.712538793683052e-05

Training epoch-37 batch-68
Running loss of epoch-37 batch-68 = 4.010740667581558e-05

Training epoch-37 batch-69
Running loss of epoch-37 batch-69 = 5.1201554015278816e-05

Training epoch-37 batch-70
Running loss of epoch-37 batch-70 = 4.691607318818569e-05

Training epoch-37 batch-71
Running loss of epoch-37 batch-71 = 1.436891034245491e-05

Training epoch-37 batch-72
Running loss of epoch-37 batch-72 = 1.4693941920995712e-05

Training epoch-37 batch-73
Running loss of epoch-37 batch-73 = 7.977942004799843e-06

Training epoch-37 batch-74
Running loss of epoch-37 batch-74 = 3.0030030757188797e-05

Training epoch-37 batch-75
Running loss of epoch-37 batch-75 = 4.2475294321775436e-05

Training epoch-37 batch-76
Running loss of epoch-37 batch-76 = 3.1233299523591995e-05

Training epoch-37 batch-77
Running loss of epoch-37 batch-77 = 9.918352589011192e-06

Training epoch-37 batch-78
Running loss of epoch-37 batch-78 = 1.056562177836895e-05

Training epoch-37 batch-79
Running loss of epoch-37 batch-79 = 2.971244975924492e-05

Training epoch-37 batch-80
Running loss of epoch-37 batch-80 = 9.307870641350746e-06

Training epoch-37 batch-81
Running loss of epoch-37 batch-81 = 2.144649624824524e-05

Training epoch-37 batch-82
Running loss of epoch-37 batch-82 = 2.055126242339611e-05

Training epoch-37 batch-83
Running loss of epoch-37 batch-83 = 2.5562243536114693e-05

Training epoch-37 batch-84
Running loss of epoch-37 batch-84 = 2.8684968128800392e-05

Training epoch-37 batch-85
Running loss of epoch-37 batch-85 = 2.821674570441246e-06

Training epoch-37 batch-86
Running loss of epoch-37 batch-86 = 1.8459977582097054e-05

Training epoch-37 batch-87
Running loss of epoch-37 batch-87 = 7.756054401397705e-06

Training epoch-37 batch-88
Running loss of epoch-37 batch-88 = 2.4797627702355385e-05

Training epoch-37 batch-89
Running loss of epoch-37 batch-89 = 7.966742850840092e-05

Training epoch-37 batch-90
Running loss of epoch-37 batch-90 = 3.5381410270929337e-05

Training epoch-37 batch-91
Running loss of epoch-37 batch-91 = 4.219985567033291e-05

Training epoch-37 batch-92
Running loss of epoch-37 batch-92 = 1.5671364963054657e-05

Training epoch-37 batch-93
Running loss of epoch-37 batch-93 = 4.4230371713638306e-05

Training epoch-37 batch-94
Running loss of epoch-37 batch-94 = 2.1996907889842987e-05

Training epoch-37 batch-95
Running loss of epoch-37 batch-95 = 4.098168574273586e-05

Training epoch-37 batch-96
Running loss of epoch-37 batch-96 = 2.6802532374858856e-05

Training epoch-37 batch-97
Running loss of epoch-37 batch-97 = 3.917957656085491e-05

Training epoch-37 batch-98
Running loss of epoch-37 batch-98 = 3.9506470784544945e-05

Training epoch-37 batch-99
Running loss of epoch-37 batch-99 = 2.000364474952221e-05

Training epoch-37 batch-100
Running loss of epoch-37 batch-100 = 6.097601726651192e-06

Training epoch-37 batch-101
Running loss of epoch-37 batch-101 = 1.8762541003525257e-05

Training epoch-37 batch-102
Running loss of epoch-37 batch-102 = 1.6866950318217278e-05

Training epoch-37 batch-103
Running loss of epoch-37 batch-103 = 1.6087666153907776e-05

Training epoch-37 batch-104
Running loss of epoch-37 batch-104 = 1.1305324733257294e-05

Training epoch-37 batch-105
Running loss of epoch-37 batch-105 = 1.3540266081690788e-05

Training epoch-37 batch-106
Running loss of epoch-37 batch-106 = 1.776008866727352e-05

Training epoch-37 batch-107
Running loss of epoch-37 batch-107 = 1.472211442887783e-05

Training epoch-37 batch-108
Running loss of epoch-37 batch-108 = 1.538521610200405e-05

Training epoch-37 batch-109
Running loss of epoch-37 batch-109 = 2.9925024136900902e-05

Training epoch-37 batch-110
Running loss of epoch-37 batch-110 = 3.166333772242069e-05

Training epoch-37 batch-111
Running loss of epoch-37 batch-111 = 1.6125617548823357e-05

Training epoch-37 batch-112
Running loss of epoch-37 batch-112 = 7.634516805410385e-06

Training epoch-37 batch-113
Running loss of epoch-37 batch-113 = 1.6632024198770523e-05

Training epoch-37 batch-114
Running loss of epoch-37 batch-114 = 2.6179011911153793e-05

Training epoch-37 batch-115
Running loss of epoch-37 batch-115 = 4.781736060976982e-05

Training epoch-37 batch-116
Running loss of epoch-37 batch-116 = 1.0787742212414742e-05

Training epoch-37 batch-117
Running loss of epoch-37 batch-117 = 1.765345223248005e-05

Training epoch-37 batch-118
Running loss of epoch-37 batch-118 = 1.4076242223381996e-05

Training epoch-37 batch-119
Running loss of epoch-37 batch-119 = 1.2863194569945335e-05

Training epoch-37 batch-120
Running loss of epoch-37 batch-120 = 2.3011351004242897e-05

Training epoch-37 batch-121
Running loss of epoch-37 batch-121 = 1.0242220014333725e-05

Training epoch-37 batch-122
Running loss of epoch-37 batch-122 = 1.1715572327375412e-05

Training epoch-37 batch-123
Running loss of epoch-37 batch-123 = 4.951655864715576e-05

Training epoch-37 batch-124
Running loss of epoch-37 batch-124 = 1.0767718777060509e-05

Training epoch-37 batch-125
Running loss of epoch-37 batch-125 = 2.5322427973151207e-05

Training epoch-37 batch-126
Running loss of epoch-37 batch-126 = 4.3868087232112885e-05

Training epoch-37 batch-127
Running loss of epoch-37 batch-127 = 0.0001299192663282156

Training epoch-37 batch-128
Running loss of epoch-37 batch-128 = 1.9427621737122536e-05

Training epoch-37 batch-129
Running loss of epoch-37 batch-129 = 2.5635818019509315e-05

Training epoch-37 batch-130
Running loss of epoch-37 batch-130 = 2.4818815290927887e-05

Training epoch-37 batch-131
Running loss of epoch-37 batch-131 = 3.7016114220023155e-05

Training epoch-37 batch-132
Running loss of epoch-37 batch-132 = 1.7138198018074036e-05

Training epoch-37 batch-133
Running loss of epoch-37 batch-133 = 1.787557266652584e-05

Training epoch-37 batch-134
Running loss of epoch-37 batch-134 = 1.795007847249508e-05

Training epoch-37 batch-135
Running loss of epoch-37 batch-135 = 2.9982533305883408e-05

Training epoch-37 batch-136
Running loss of epoch-37 batch-136 = 3.869831562042236e-05

Training epoch-37 batch-137
Running loss of epoch-37 batch-137 = 4.107528366148472e-05

Training epoch-37 batch-138
Running loss of epoch-37 batch-138 = 3.414088860154152e-05

Training epoch-37 batch-139
Running loss of epoch-37 batch-139 = 1.597544178366661e-05

Training epoch-37 batch-140
Running loss of epoch-37 batch-140 = 6.477115675806999e-06

Training epoch-37 batch-141
Running loss of epoch-37 batch-141 = 1.5227589756250381e-05

Training epoch-37 batch-142
Running loss of epoch-37 batch-142 = 4.163989797234535e-05

Training epoch-37 batch-143
Running loss of epoch-37 batch-143 = 2.69466545432806e-05

Training epoch-37 batch-144
Running loss of epoch-37 batch-144 = 2.199341543018818e-05

Training epoch-37 batch-145
Running loss of epoch-37 batch-145 = 1.1340947821736336e-05

Training epoch-37 batch-146
Running loss of epoch-37 batch-146 = 3.653066232800484e-05

Training epoch-37 batch-147
Running loss of epoch-37 batch-147 = 1.7229001969099045e-05

Training epoch-37 batch-148
Running loss of epoch-37 batch-148 = 0.0001602336997166276

Training epoch-37 batch-149
Running loss of epoch-37 batch-149 = 9.16915014386177e-05

Training epoch-37 batch-150
Running loss of epoch-37 batch-150 = 1.5229685232043266e-05

Training epoch-37 batch-151
Running loss of epoch-37 batch-151 = 7.997220382094383e-05

Training epoch-37 batch-152
Running loss of epoch-37 batch-152 = 2.1251849830150604e-05

Training epoch-37 batch-153
Running loss of epoch-37 batch-153 = 1.7336802557110786e-05

Training epoch-37 batch-154
Running loss of epoch-37 batch-154 = 1.2033386155962944e-05

Training epoch-37 batch-155
Running loss of epoch-37 batch-155 = 1.3716984540224075e-05

Training epoch-37 batch-156
Running loss of epoch-37 batch-156 = 1.595309004187584e-05

Training epoch-37 batch-157
Running loss of epoch-37 batch-157 = 0.00014065206050872803

Finished training epoch-37.



Average train loss at epoch-37 = 2.6064709573984148e-05

Started Evaluation

Average val loss at epoch-37 = 1.4716799208667728

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 64.36 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-38


Training epoch-38 batch-1
Running loss of epoch-38 batch-1 = 7.0945825427770615e-06

Training epoch-38 batch-2
Running loss of epoch-38 batch-2 = 3.5975128412246704e-05

Training epoch-38 batch-3
Running loss of epoch-38 batch-3 = 4.859478212893009e-05

Training epoch-38 batch-4
Running loss of epoch-38 batch-4 = 1.4840858057141304e-05

Training epoch-38 batch-5
Running loss of epoch-38 batch-5 = 2.4181557819247246e-05

Training epoch-38 batch-6
Running loss of epoch-38 batch-6 = 2.6094261556863785e-05

Training epoch-38 batch-7
Running loss of epoch-38 batch-7 = 1.3052951544523239e-05

Training epoch-38 batch-8
Running loss of epoch-38 batch-8 = 1.9862549379467964e-05

Training epoch-38 batch-9
Running loss of epoch-38 batch-9 = 1.1578202247619629e-05

Training epoch-38 batch-10
Running loss of epoch-38 batch-10 = 3.300374373793602e-05

Training epoch-38 batch-11
Running loss of epoch-38 batch-11 = 9.104842320084572e-06

Training epoch-38 batch-12
Running loss of epoch-38 batch-12 = 2.4376437067985535e-05

Training epoch-38 batch-13
Running loss of epoch-38 batch-13 = 2.1102605387568474e-05

Training epoch-38 batch-14
Running loss of epoch-38 batch-14 = 1.272629015147686e-05

Training epoch-38 batch-15
Running loss of epoch-38 batch-15 = 1.1542579159140587e-05

Training epoch-38 batch-16
Running loss of epoch-38 batch-16 = 3.083841875195503e-05

Training epoch-38 batch-17
Running loss of epoch-38 batch-17 = 1.7449725419282913e-05

Training epoch-38 batch-18
Running loss of epoch-38 batch-18 = 3.249594010412693e-05

Training epoch-38 batch-19
Running loss of epoch-38 batch-19 = 3.829807974398136e-05

Training epoch-38 batch-20
Running loss of epoch-38 batch-20 = 2.2972701117396355e-05

Training epoch-38 batch-21
Running loss of epoch-38 batch-21 = 3.9147911593317986e-05

Training epoch-38 batch-22
Running loss of epoch-38 batch-22 = 1.0106014087796211e-05

Training epoch-38 batch-23
Running loss of epoch-38 batch-23 = 1.9256724044680595e-05

Training epoch-38 batch-24
Running loss of epoch-38 batch-24 = 1.1553755030035973e-05

Training epoch-38 batch-25
Running loss of epoch-38 batch-25 = 2.5327783077955246e-05

Training epoch-38 batch-26
Running loss of epoch-38 batch-26 = 1.2606382369995117e-05

Training epoch-38 batch-27
Running loss of epoch-38 batch-27 = 2.014636993408203e-05

Training epoch-38 batch-28
Running loss of epoch-38 batch-28 = 1.6032718122005463e-05

Training epoch-38 batch-29
Running loss of epoch-38 batch-29 = 1.3168901205062866e-05

Training epoch-38 batch-30
Running loss of epoch-38 batch-30 = 1.3156095519661903e-05

Training epoch-38 batch-31
Running loss of epoch-38 batch-31 = 3.165402449667454e-05

Training epoch-38 batch-32
Running loss of epoch-38 batch-32 = 4.619825631380081e-06

Training epoch-38 batch-33
Running loss of epoch-38 batch-33 = 8.203135803341866e-05

Training epoch-38 batch-34
Running loss of epoch-38 batch-34 = 2.6811379939317703e-05

Training epoch-38 batch-35
Running loss of epoch-38 batch-35 = 8.139759302139282e-06

Training epoch-38 batch-36
Running loss of epoch-38 batch-36 = 2.581719309091568e-05

Training epoch-38 batch-37
Running loss of epoch-38 batch-37 = 9.501469321548939e-05

Training epoch-38 batch-38
Running loss of epoch-38 batch-38 = 9.646639227867126e-06

Training epoch-38 batch-39
Running loss of epoch-38 batch-39 = 1.83875672519207e-05

Training epoch-38 batch-40
Running loss of epoch-38 batch-40 = 1.5029683709144592e-05

Training epoch-38 batch-41
Running loss of epoch-38 batch-41 = 1.376960426568985e-05

Training epoch-38 batch-42
Running loss of epoch-38 batch-42 = 3.0205585062503815e-05

Training epoch-38 batch-43
Running loss of epoch-38 batch-43 = 1.2333737686276436e-05

Training epoch-38 batch-44
Running loss of epoch-38 batch-44 = 9.414739906787872e-06

Training epoch-38 batch-45
Running loss of epoch-38 batch-45 = 1.2730713933706284e-05

Training epoch-38 batch-46
Running loss of epoch-38 batch-46 = 2.3004133254289627e-05

Training epoch-38 batch-47
Running loss of epoch-38 batch-47 = 1.5491852536797523e-05

Training epoch-38 batch-48
Running loss of epoch-38 batch-48 = 4.127458669245243e-05

Training epoch-38 batch-49
Running loss of epoch-38 batch-49 = 1.9526341930031776e-05

Training epoch-38 batch-50
Running loss of epoch-38 batch-50 = 3.471248783171177e-05

Training epoch-38 batch-51
Running loss of epoch-38 batch-51 = 2.2493768483400345e-05

Training epoch-38 batch-52
Running loss of epoch-38 batch-52 = 9.928829967975616e-06

Training epoch-38 batch-53
Running loss of epoch-38 batch-53 = 1.2274831533432007e-05

Training epoch-38 batch-54
Running loss of epoch-38 batch-54 = 2.8003007173538208e-05

Training epoch-38 batch-55
Running loss of epoch-38 batch-55 = 5.898997187614441e-06

Training epoch-38 batch-56
Running loss of epoch-38 batch-56 = 5.3972238674759865e-05

Training epoch-38 batch-57
Running loss of epoch-38 batch-57 = 1.718895509839058e-05

Training epoch-38 batch-58
Running loss of epoch-38 batch-58 = 1.9090017303824425e-05

Training epoch-38 batch-59
Running loss of epoch-38 batch-59 = 9.997747838497162e-06

Training epoch-38 batch-60
Running loss of epoch-38 batch-60 = 5.315989255905151e-06

Training epoch-38 batch-61
Running loss of epoch-38 batch-61 = 2.4644657969474792e-05

Training epoch-38 batch-62
Running loss of epoch-38 batch-62 = 2.4237437173724174e-05

Training epoch-38 batch-63
Running loss of epoch-38 batch-63 = 2.3603439331054688e-05

Training epoch-38 batch-64
Running loss of epoch-38 batch-64 = 2.7457019314169884e-05

Training epoch-38 batch-65
Running loss of epoch-38 batch-65 = 1.7795246094465256e-05

Training epoch-38 batch-66
Running loss of epoch-38 batch-66 = 2.2484920918941498e-05

Training epoch-38 batch-67
Running loss of epoch-38 batch-67 = 2.6347581297159195e-05

Training epoch-38 batch-68
Running loss of epoch-38 batch-68 = 2.3307278752326965e-05

Training epoch-38 batch-69
Running loss of epoch-38 batch-69 = 1.153070479631424e-05

Training epoch-38 batch-70
Running loss of epoch-38 batch-70 = 2.5603221729397774e-05

Training epoch-38 batch-71
Running loss of epoch-38 batch-71 = 1.3489741832017899e-05

Training epoch-38 batch-72
Running loss of epoch-38 batch-72 = 9.688618592917919e-05

Training epoch-38 batch-73
Running loss of epoch-38 batch-73 = 1.6745412722229958e-05

Training epoch-38 batch-74
Running loss of epoch-38 batch-74 = 2.3488420993089676e-05

Training epoch-38 batch-75
Running loss of epoch-38 batch-75 = 3.4192344173789024e-05

Training epoch-38 batch-76
Running loss of epoch-38 batch-76 = 6.480677984654903e-05

Training epoch-38 batch-77
Running loss of epoch-38 batch-77 = 1.3733282685279846e-05

Training epoch-38 batch-78
Running loss of epoch-38 batch-78 = 1.0836636647582054e-05

Training epoch-38 batch-79
Running loss of epoch-38 batch-79 = 1.2148404493927956e-05

Training epoch-38 batch-80
Running loss of epoch-38 batch-80 = 1.960480585694313e-05

Training epoch-38 batch-81
Running loss of epoch-38 batch-81 = 4.099425859749317e-05

Training epoch-38 batch-82
Running loss of epoch-38 batch-82 = 7.370254024863243e-06

Training epoch-38 batch-83
Running loss of epoch-38 batch-83 = 9.286287240684032e-05

Training epoch-38 batch-84
Running loss of epoch-38 batch-84 = 5.056173540651798e-05

Training epoch-38 batch-85
Running loss of epoch-38 batch-85 = 6.273388862609863e-06

Training epoch-38 batch-86
Running loss of epoch-38 batch-86 = 1.7660902813076973e-05

Training epoch-38 batch-87
Running loss of epoch-38 batch-87 = 1.7280923202633858e-05

Training epoch-38 batch-88
Running loss of epoch-38 batch-88 = 8.787494152784348e-06

Training epoch-38 batch-89
Running loss of epoch-38 batch-89 = 8.418690413236618e-06

Training epoch-38 batch-90
Running loss of epoch-38 batch-90 = 3.065401688218117e-05

Training epoch-38 batch-91
Running loss of epoch-38 batch-91 = 6.292760372161865e-05

Training epoch-38 batch-92
Running loss of epoch-38 batch-92 = 2.245791256427765e-05

Training epoch-38 batch-93
Running loss of epoch-38 batch-93 = 2.8169481083750725e-05

Training epoch-38 batch-94
Running loss of epoch-38 batch-94 = 1.7268583178520203e-05

Training epoch-38 batch-95
Running loss of epoch-38 batch-95 = 1.3465993106365204e-05

Training epoch-38 batch-96
Running loss of epoch-38 batch-96 = 1.2709060683846474e-05

Training epoch-38 batch-97
Running loss of epoch-38 batch-97 = 2.3838598281145096e-05

Training epoch-38 batch-98
Running loss of epoch-38 batch-98 = 1.0823365300893784e-05

Training epoch-38 batch-99
Running loss of epoch-38 batch-99 = 2.0852312445640564e-05

Training epoch-38 batch-100
Running loss of epoch-38 batch-100 = 2.2653955966234207e-05

Training epoch-38 batch-101
Running loss of epoch-38 batch-101 = 3.6073848605155945e-05

Training epoch-38 batch-102
Running loss of epoch-38 batch-102 = 9.499490261077881e-06

Training epoch-38 batch-103
Running loss of epoch-38 batch-103 = 1.3776589184999466e-05

Training epoch-38 batch-104
Running loss of epoch-38 batch-104 = 1.902063377201557e-05

Training epoch-38 batch-105
Running loss of epoch-38 batch-105 = 1.605600118637085e-05

Training epoch-38 batch-106
Running loss of epoch-38 batch-106 = 8.262228220701218e-06

Training epoch-38 batch-107
Running loss of epoch-38 batch-107 = 2.088281325995922e-05

Training epoch-38 batch-108
Running loss of epoch-38 batch-108 = 8.296454325318336e-06

Training epoch-38 batch-109
Running loss of epoch-38 batch-109 = 6.287824362516403e-06

Training epoch-38 batch-110
Running loss of epoch-38 batch-110 = 2.779858186841011e-05

Training epoch-38 batch-111
Running loss of epoch-38 batch-111 = 3.80207784473896e-05

Training epoch-38 batch-112
Running loss of epoch-38 batch-112 = 4.1573541238904e-05

Training epoch-38 batch-113
Running loss of epoch-38 batch-113 = 1.3557961210608482e-05

Training epoch-38 batch-114
Running loss of epoch-38 batch-114 = 3.652041777968407e-05

Training epoch-38 batch-115
Running loss of epoch-38 batch-115 = 2.1806219592690468e-05

Training epoch-38 batch-116
Running loss of epoch-38 batch-116 = 2.0731007680296898e-05

Training epoch-38 batch-117
Running loss of epoch-38 batch-117 = 2.838345244526863e-05

Training epoch-38 batch-118
Running loss of epoch-38 batch-118 = 0.00016438006423413754

Training epoch-38 batch-119
Running loss of epoch-38 batch-119 = 1.4232704415917397e-05

Training epoch-38 batch-120
Running loss of epoch-38 batch-120 = 2.1073268726468086e-05

Training epoch-38 batch-121
Running loss of epoch-38 batch-121 = 1.1767493560910225e-05

Training epoch-38 batch-122
Running loss of epoch-38 batch-122 = 4.519219510257244e-05

Training epoch-38 batch-123
Running loss of epoch-38 batch-123 = 1.4291377738118172e-05

Training epoch-38 batch-124
Running loss of epoch-38 batch-124 = 1.1892523616552353e-05

Training epoch-38 batch-125
Running loss of epoch-38 batch-125 = 1.8482329323887825e-05

Training epoch-38 batch-126
Running loss of epoch-38 batch-126 = 7.141148671507835e-06

Training epoch-38 batch-127
Running loss of epoch-38 batch-127 = 1.3781711459159851e-05

Training epoch-38 batch-128
Running loss of epoch-38 batch-128 = 2.0517036318778992e-05

Training epoch-38 batch-129
Running loss of epoch-38 batch-129 = 3.6769197322428226e-05

Training epoch-38 batch-130
Running loss of epoch-38 batch-130 = 1.1494383215904236e-05

Training epoch-38 batch-131
Running loss of epoch-38 batch-131 = 6.976071745157242e-06

Training epoch-38 batch-132
Running loss of epoch-38 batch-132 = 3.2495008781552315e-05

Training epoch-38 batch-133
Running loss of epoch-38 batch-133 = 1.1358875781297684e-05

Training epoch-38 batch-134
Running loss of epoch-38 batch-134 = 1.3346085324883461e-05

Training epoch-38 batch-135
Running loss of epoch-38 batch-135 = 9.886990301311016e-05

Training epoch-38 batch-136
Running loss of epoch-38 batch-136 = 1.0817078873515129e-05

Training epoch-38 batch-137
Running loss of epoch-38 batch-137 = 2.524442970752716e-05

Training epoch-38 batch-138
Running loss of epoch-38 batch-138 = 1.2278323993086815e-05

Training epoch-38 batch-139
Running loss of epoch-38 batch-139 = 4.739663563668728e-05

Training epoch-38 batch-140
Running loss of epoch-38 batch-140 = 1.4354242011904716e-05

Training epoch-38 batch-141
Running loss of epoch-38 batch-141 = 1.0384246706962585e-05

Training epoch-38 batch-142
Running loss of epoch-38 batch-142 = 5.863828118890524e-05

Training epoch-38 batch-143
Running loss of epoch-38 batch-143 = 2.1557207219302654e-05

Training epoch-38 batch-144
Running loss of epoch-38 batch-144 = 3.095902502536774e-05

Training epoch-38 batch-145
Running loss of epoch-38 batch-145 = 3.1253090128302574e-05

Training epoch-38 batch-146
Running loss of epoch-38 batch-146 = 2.12064478546381e-05

Training epoch-38 batch-147
Running loss of epoch-38 batch-147 = 1.7317011952400208e-05

Training epoch-38 batch-148
Running loss of epoch-38 batch-148 = 2.6554102078080177e-05

Training epoch-38 batch-149
Running loss of epoch-38 batch-149 = 2.7262140065431595e-06

Training epoch-38 batch-150
Running loss of epoch-38 batch-150 = 9.40720783546567e-05

Training epoch-38 batch-151
Running loss of epoch-38 batch-151 = 7.81485578045249e-05

Training epoch-38 batch-152
Running loss of epoch-38 batch-152 = 1.3134442269802094e-05

Training epoch-38 batch-153
Running loss of epoch-38 batch-153 = 8.340226486325264e-06

Training epoch-38 batch-154
Running loss of epoch-38 batch-154 = 1.2433622032403946e-05

Training epoch-38 batch-155
Running loss of epoch-38 batch-155 = 2.4624168872833252e-05

Training epoch-38 batch-156
Running loss of epoch-38 batch-156 = 8.106930181384087e-06

Training epoch-38 batch-157
Running loss of epoch-38 batch-157 = 0.00016732141375541687

Finished training epoch-38.



Average train loss at epoch-38 = 2.5154752284288405e-05

Started Evaluation

Average val loss at epoch-38 = 1.501357538965402

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 95.90 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 88.91 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 62.56 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-39


Training epoch-39 batch-1
Running loss of epoch-39 batch-1 = 2.5081215426325798e-05

Training epoch-39 batch-2
Running loss of epoch-39 batch-2 = 1.2348173186182976e-05

Training epoch-39 batch-3
Running loss of epoch-39 batch-3 = 4.071393050253391e-05

Training epoch-39 batch-4
Running loss of epoch-39 batch-4 = 3.815162926912308e-05

Training epoch-39 batch-5
Running loss of epoch-39 batch-5 = 1.582247205078602e-05

Training epoch-39 batch-6
Running loss of epoch-39 batch-6 = 9.3863345682621e-06

Training epoch-39 batch-7
Running loss of epoch-39 batch-7 = 2.2660940885543823e-05

Training epoch-39 batch-8
Running loss of epoch-39 batch-8 = 1.820409670472145e-05

Training epoch-39 batch-9
Running loss of epoch-39 batch-9 = 1.299520954489708e-05

Training epoch-39 batch-10
Running loss of epoch-39 batch-10 = 1.2230826541781425e-05

Training epoch-39 batch-11
Running loss of epoch-39 batch-11 = 2.050306648015976e-05

Training epoch-39 batch-12
Running loss of epoch-39 batch-12 = 1.9141240045428276e-05

Training epoch-39 batch-13
Running loss of epoch-39 batch-13 = 3.107590600848198e-06

Training epoch-39 batch-14
Running loss of epoch-39 batch-14 = 1.550186425447464e-05

Training epoch-39 batch-15
Running loss of epoch-39 batch-15 = 2.7101486921310425e-05

Training epoch-39 batch-16
Running loss of epoch-39 batch-16 = 1.1813594028353691e-05

Training epoch-39 batch-17
Running loss of epoch-39 batch-17 = 1.2715812772512436e-05

Training epoch-39 batch-18
Running loss of epoch-39 batch-18 = 1.875939778983593e-05

Training epoch-39 batch-19
Running loss of epoch-39 batch-19 = 2.0975712686777115e-05

Training epoch-39 batch-20
Running loss of epoch-39 batch-20 = 8.999835699796677e-06

Training epoch-39 batch-21
Running loss of epoch-39 batch-21 = 1.9621336832642555e-05

Training epoch-39 batch-22
Running loss of epoch-39 batch-22 = 6.902730092406273e-06

Training epoch-39 batch-23
Running loss of epoch-39 batch-23 = 8.505769073963165e-06

Training epoch-39 batch-24
Running loss of epoch-39 batch-24 = 1.6565434634685516e-05

Training epoch-39 batch-25
Running loss of epoch-39 batch-25 = 3.581470809876919e-05

Training epoch-39 batch-26
Running loss of epoch-39 batch-26 = 1.902645453810692e-05

Training epoch-39 batch-27
Running loss of epoch-39 batch-27 = 1.3320939615368843e-05

Training epoch-39 batch-28
Running loss of epoch-39 batch-28 = 2.697971649467945e-05

Training epoch-39 batch-29
Running loss of epoch-39 batch-29 = 4.87244687974453e-06

Training epoch-39 batch-30
Running loss of epoch-39 batch-30 = 1.776660792529583e-05

Training epoch-39 batch-31
Running loss of epoch-39 batch-31 = 6.262934766709805e-05

Training epoch-39 batch-32
Running loss of epoch-39 batch-32 = 1.2886477634310722e-05

Training epoch-39 batch-33
Running loss of epoch-39 batch-33 = 1.6666017472743988e-05

Training epoch-39 batch-34
Running loss of epoch-39 batch-34 = 1.8788501620292664e-05

Training epoch-39 batch-35
Running loss of epoch-39 batch-35 = 1.8897932022809982e-05

Training epoch-39 batch-36
Running loss of epoch-39 batch-36 = 8.317641913890839e-06

Training epoch-39 batch-37
Running loss of epoch-39 batch-37 = 9.03732143342495e-06

Training epoch-39 batch-38
Running loss of epoch-39 batch-38 = 1.6630394384264946e-05

Training epoch-39 batch-39
Running loss of epoch-39 batch-39 = 6.67814165353775e-05

Training epoch-39 batch-40
Running loss of epoch-39 batch-40 = 1.1370750144124031e-05

Training epoch-39 batch-41
Running loss of epoch-39 batch-41 = 8.502276614308357e-06

Training epoch-39 batch-42
Running loss of epoch-39 batch-42 = 1.8050195649266243e-05

Training epoch-39 batch-43
Running loss of epoch-39 batch-43 = 1.3217329978942871e-05

Training epoch-39 batch-44
Running loss of epoch-39 batch-44 = 1.4348886907100677e-05

Training epoch-39 batch-45
Running loss of epoch-39 batch-45 = 5.172449164092541e-05

Training epoch-39 batch-46
Running loss of epoch-39 batch-46 = 1.2728618457913399e-05

Training epoch-39 batch-47
Running loss of epoch-39 batch-47 = 1.9273953512310982e-05

Training epoch-39 batch-48
Running loss of epoch-39 batch-48 = 8.103437721729279e-06

Training epoch-39 batch-49
Running loss of epoch-39 batch-49 = 2.079014666378498e-05

Training epoch-39 batch-50
Running loss of epoch-39 batch-50 = 2.824678085744381e-05

Training epoch-39 batch-51
Running loss of epoch-39 batch-51 = 3.000372089445591e-05

Training epoch-39 batch-52
Running loss of epoch-39 batch-52 = 7.330672815442085e-06

Training epoch-39 batch-53
Running loss of epoch-39 batch-53 = 2.493755891919136e-05

Training epoch-39 batch-54
Running loss of epoch-39 batch-54 = 2.1160347387194633e-05

Training epoch-39 batch-55
Running loss of epoch-39 batch-55 = 5.487934686243534e-05

Training epoch-39 batch-56
Running loss of epoch-39 batch-56 = 1.1973781511187553e-05

Training epoch-39 batch-57
Running loss of epoch-39 batch-57 = 1.9718194380402565e-05

Training epoch-39 batch-58
Running loss of epoch-39 batch-58 = 1.028226688504219e-05

Training epoch-39 batch-59
Running loss of epoch-39 batch-59 = 2.3141968995332718e-05

Training epoch-39 batch-60
Running loss of epoch-39 batch-60 = 1.0863412171602249e-05

Training epoch-39 batch-61
Running loss of epoch-39 batch-61 = 3.734254278242588e-05

Training epoch-39 batch-62
Running loss of epoch-39 batch-62 = 1.4543533325195312e-05

Training epoch-39 batch-63
Running loss of epoch-39 batch-63 = 1.9821105524897575e-05

Training epoch-39 batch-64
Running loss of epoch-39 batch-64 = 1.8967315554618835e-05

Training epoch-39 batch-65
Running loss of epoch-39 batch-65 = 8.738366886973381e-05

Training epoch-39 batch-66
Running loss of epoch-39 batch-66 = 3.1273579224944115e-05

Training epoch-39 batch-67
Running loss of epoch-39 batch-67 = 1.0673189535737038e-05

Training epoch-39 batch-68
Running loss of epoch-39 batch-68 = 5.474314093589783e-06

Training epoch-39 batch-69
Running loss of epoch-39 batch-69 = 2.0943814888596535e-05

Training epoch-39 batch-70
Running loss of epoch-39 batch-70 = 3.289780579507351e-05

Training epoch-39 batch-71
Running loss of epoch-39 batch-71 = 1.9072555005550385e-05

Training epoch-39 batch-72
Running loss of epoch-39 batch-72 = 6.895512342453003e-06

Training epoch-39 batch-73
Running loss of epoch-39 batch-73 = 3.376300446689129e-05

Training epoch-39 batch-74
Running loss of epoch-39 batch-74 = 1.630396582186222e-05

Training epoch-39 batch-75
Running loss of epoch-39 batch-75 = 2.267817035317421e-05

Training epoch-39 batch-76
Running loss of epoch-39 batch-76 = 2.126372419297695e-05

Training epoch-39 batch-77
Running loss of epoch-39 batch-77 = 8.803559467196465e-06

Training epoch-39 batch-78
Running loss of epoch-39 batch-78 = 2.766028046607971e-06

Training epoch-39 batch-79
Running loss of epoch-39 batch-79 = 8.572125807404518e-06

Training epoch-39 batch-80
Running loss of epoch-39 batch-80 = 9.192386642098427e-06

Training epoch-39 batch-81
Running loss of epoch-39 batch-81 = 1.7903978005051613e-05

Training epoch-39 batch-82
Running loss of epoch-39 batch-82 = 1.246575266122818e-05

Training epoch-39 batch-83
Running loss of epoch-39 batch-83 = 7.714610546827316e-06

Training epoch-39 batch-84
Running loss of epoch-39 batch-84 = 2.795853652060032e-05

Training epoch-39 batch-85
Running loss of epoch-39 batch-85 = 2.024439163506031e-05

Training epoch-39 batch-86
Running loss of epoch-39 batch-86 = 2.3284228518605232e-05

Training epoch-39 batch-87
Running loss of epoch-39 batch-87 = 3.649666905403137e-05

Training epoch-39 batch-88
Running loss of epoch-39 batch-88 = 2.038036473095417e-05

Training epoch-39 batch-89
Running loss of epoch-39 batch-89 = 1.7573824152350426e-05

Training epoch-39 batch-90
Running loss of epoch-39 batch-90 = 1.4937017112970352e-05

Training epoch-39 batch-91
Running loss of epoch-39 batch-91 = 2.2659776732325554e-05

Training epoch-39 batch-92
Running loss of epoch-39 batch-92 = 1.7768004909157753e-05

Training epoch-39 batch-93
Running loss of epoch-39 batch-93 = 2.5839312002062798e-05

Training epoch-39 batch-94
Running loss of epoch-39 batch-94 = 1.5857629477977753e-05

Training epoch-39 batch-95
Running loss of epoch-39 batch-95 = 3.621762152761221e-05

Training epoch-39 batch-96
Running loss of epoch-39 batch-96 = 1.7213402315974236e-05

Training epoch-39 batch-97
Running loss of epoch-39 batch-97 = 8.048885501921177e-05

Training epoch-39 batch-98
Running loss of epoch-39 batch-98 = 4.476145841181278e-05

Training epoch-39 batch-99
Running loss of epoch-39 batch-99 = 1.185503788292408e-05

Training epoch-39 batch-100
Running loss of epoch-39 batch-100 = 2.2957567125558853e-05

Training epoch-39 batch-101
Running loss of epoch-39 batch-101 = 9.955605491995811e-06

Training epoch-39 batch-102
Running loss of epoch-39 batch-102 = 2.3332424461841583e-05

Training epoch-39 batch-103
Running loss of epoch-39 batch-103 = 1.596403308212757e-05

Training epoch-39 batch-104
Running loss of epoch-39 batch-104 = 1.4461809769272804e-05

Training epoch-39 batch-105
Running loss of epoch-39 batch-105 = 4.7968002036213875e-05

Training epoch-39 batch-106
Running loss of epoch-39 batch-106 = 1.0431976988911629e-05

Training epoch-39 batch-107
Running loss of epoch-39 batch-107 = 1.2631062418222427e-05

Training epoch-39 batch-108
Running loss of epoch-39 batch-108 = 1.583946868777275e-05

Training epoch-39 batch-109
Running loss of epoch-39 batch-109 = 2.0076986402273178e-05

Training epoch-39 batch-110
Running loss of epoch-39 batch-110 = 2.2433465346693993e-05

Training epoch-39 batch-111
Running loss of epoch-39 batch-111 = 2.9686372727155685e-05

Training epoch-39 batch-112
Running loss of epoch-39 batch-112 = 1.183641143143177e-05

Training epoch-39 batch-113
Running loss of epoch-39 batch-113 = 1.4793593436479568e-05

Training epoch-39 batch-114
Running loss of epoch-39 batch-114 = 5.5233947932720184e-05

Training epoch-39 batch-115
Running loss of epoch-39 batch-115 = 1.0356772691011429e-05

Training epoch-39 batch-116
Running loss of epoch-39 batch-116 = 1.5193363651633263e-05

Training epoch-39 batch-117
Running loss of epoch-39 batch-117 = 1.6697915270924568e-05

Training epoch-39 batch-118
Running loss of epoch-39 batch-118 = 1.6265781596302986e-05

Training epoch-39 batch-119
Running loss of epoch-39 batch-119 = 1.5524448826909065e-05

Training epoch-39 batch-120
Running loss of epoch-39 batch-120 = 1.3981945812702179e-05

Training epoch-39 batch-121
Running loss of epoch-39 batch-121 = 3.3943215385079384e-05

Training epoch-39 batch-122
Running loss of epoch-39 batch-122 = 2.867705188691616e-05

Training epoch-39 batch-123
Running loss of epoch-39 batch-123 = 1.5311874449253082e-05

Training epoch-39 batch-124
Running loss of epoch-39 batch-124 = 2.05927062779665e-05

Training epoch-39 batch-125
Running loss of epoch-39 batch-125 = 3.497977741062641e-05

Training epoch-39 batch-126
Running loss of epoch-39 batch-126 = 5.302857607603073e-05

Training epoch-39 batch-127
Running loss of epoch-39 batch-127 = 3.391411155462265e-06

Training epoch-39 batch-128
Running loss of epoch-39 batch-128 = 9.257346391677856e-06

Training epoch-39 batch-129
Running loss of epoch-39 batch-129 = 2.3035332560539246e-05

Training epoch-39 batch-130
Running loss of epoch-39 batch-130 = 1.0988675057888031e-05

Training epoch-39 batch-131
Running loss of epoch-39 batch-131 = 6.045447662472725e-06

Training epoch-39 batch-132
Running loss of epoch-39 batch-132 = 1.3491138815879822e-05

Training epoch-39 batch-133
Running loss of epoch-39 batch-133 = 4.499475471675396e-05

Training epoch-39 batch-134
Running loss of epoch-39 batch-134 = 2.3931032046675682e-05

Training epoch-39 batch-135
Running loss of epoch-39 batch-135 = 8.43261368572712e-05

Training epoch-39 batch-136
Running loss of epoch-39 batch-136 = 7.454771548509598e-06

Training epoch-39 batch-137
Running loss of epoch-39 batch-137 = 1.421361230313778e-05

Training epoch-39 batch-138
Running loss of epoch-39 batch-138 = 3.194902092218399e-06

Training epoch-39 batch-139
Running loss of epoch-39 batch-139 = 1.568184234201908e-05

Training epoch-39 batch-140
Running loss of epoch-39 batch-140 = 4.6860892325639725e-05

Training epoch-39 batch-141
Running loss of epoch-39 batch-141 = 2.9562506824731827e-05

Training epoch-39 batch-142
Running loss of epoch-39 batch-142 = 9.053363464772701e-05

Training epoch-39 batch-143
Running loss of epoch-39 batch-143 = 2.5995774194598198e-05

Training epoch-39 batch-144
Running loss of epoch-39 batch-144 = 6.67779240757227e-05

Training epoch-39 batch-145
Running loss of epoch-39 batch-145 = 3.58065590262413e-05

Training epoch-39 batch-146
Running loss of epoch-39 batch-146 = 9.116251021623611e-06

Training epoch-39 batch-147
Running loss of epoch-39 batch-147 = 9.780051186680794e-06

Training epoch-39 batch-148
Running loss of epoch-39 batch-148 = 3.596767783164978e-06

Training epoch-39 batch-149
Running loss of epoch-39 batch-149 = 1.7836689949035645e-05

Training epoch-39 batch-150
Running loss of epoch-39 batch-150 = 2.6586931198835373e-05

Training epoch-39 batch-151
Running loss of epoch-39 batch-151 = 1.528835855424404e-05

Training epoch-39 batch-152
Running loss of epoch-39 batch-152 = 2.9764138162136078e-05

Training epoch-39 batch-153
Running loss of epoch-39 batch-153 = 1.7852173186838627e-05

Training epoch-39 batch-154
Running loss of epoch-39 batch-154 = 4.532560706138611e-05

Training epoch-39 batch-155
Running loss of epoch-39 batch-155 = 1.9242288544774055e-05

Training epoch-39 batch-156
Running loss of epoch-39 batch-156 = 2.034544013440609e-05

Training epoch-39 batch-157
Running loss of epoch-39 batch-157 = 0.0004345439374446869

Finished training epoch-39.



Average train loss at epoch-39 = 2.2836458683013917e-05

Started Evaluation

Average val loss at epoch-39 = 1.5302077791582835

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 87.87 %
Accuracy for class onCreate is: 89.23 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 63.70 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.59 %
Accuracy for class execute is: 52.61 %
Accuracy for class get is: 64.10 %

Overall Accuracy = 81.25 %

Finished Evaluation



Started training epoch-40


Training epoch-40 batch-1
Running loss of epoch-40 batch-1 = 3.6228448152542114e-05

Training epoch-40 batch-2
Running loss of epoch-40 batch-2 = 9.851297363638878e-06

Training epoch-40 batch-3
Running loss of epoch-40 batch-3 = 1.777266152203083e-05

Training epoch-40 batch-4
Running loss of epoch-40 batch-4 = 8.355127647519112e-06

Training epoch-40 batch-5
Running loss of epoch-40 batch-5 = 1.1355150490999222e-05

Training epoch-40 batch-6
Running loss of epoch-40 batch-6 = 4.4551678001880646e-05

Training epoch-40 batch-7
Running loss of epoch-40 batch-7 = 6.866734474897385e-05

Training epoch-40 batch-8
Running loss of epoch-40 batch-8 = 1.6677426174283028e-05

Training epoch-40 batch-9
Running loss of epoch-40 batch-9 = 4.298565909266472e-05

Training epoch-40 batch-10
Running loss of epoch-40 batch-10 = 2.7091009542346e-05

Training epoch-40 batch-11
Running loss of epoch-40 batch-11 = 3.8162339478731155e-05

Training epoch-40 batch-12
Running loss of epoch-40 batch-12 = 1.634354703128338e-05

Training epoch-40 batch-13
Running loss of epoch-40 batch-13 = 1.2635486200451851e-05

Training epoch-40 batch-14
Running loss of epoch-40 batch-14 = 1.4376593753695488e-05

Training epoch-40 batch-15
Running loss of epoch-40 batch-15 = 9.360723197460175e-06

Training epoch-40 batch-16
Running loss of epoch-40 batch-16 = 1.5347497537732124e-05

Training epoch-40 batch-17
Running loss of epoch-40 batch-17 = 1.0604271665215492e-05

Training epoch-40 batch-18
Running loss of epoch-40 batch-18 = 2.09116842597723e-05

Training epoch-40 batch-19
Running loss of epoch-40 batch-19 = 1.9061146304011345e-05

Training epoch-40 batch-20
Running loss of epoch-40 batch-20 = 1.0685296729207039e-05

Training epoch-40 batch-21
Running loss of epoch-40 batch-21 = 1.4042248949408531e-05

Training epoch-40 batch-22
Running loss of epoch-40 batch-22 = 1.765834167599678e-05

Training epoch-40 batch-23
Running loss of epoch-40 batch-23 = 3.07431910187006e-05

Training epoch-40 batch-24
Running loss of epoch-40 batch-24 = 2.0670238882303238e-05

Training epoch-40 batch-25
Running loss of epoch-40 batch-25 = 3.068731166422367e-05

Training epoch-40 batch-26
Running loss of epoch-40 batch-26 = 8.667097426950932e-05

Training epoch-40 batch-27
Running loss of epoch-40 batch-27 = 6.532762199640274e-06

Training epoch-40 batch-28
Running loss of epoch-40 batch-28 = 1.247297041118145e-05

Training epoch-40 batch-29
Running loss of epoch-40 batch-29 = 2.8916634619235992e-05

Training epoch-40 batch-30
Running loss of epoch-40 batch-30 = 1.925998367369175e-05

Training epoch-40 batch-31
Running loss of epoch-40 batch-31 = 2.4606706574559212e-05

Training epoch-40 batch-32
Running loss of epoch-40 batch-32 = 1.4994526281952858e-05

Training epoch-40 batch-33
Running loss of epoch-40 batch-33 = 9.196577593684196e-06

Training epoch-40 batch-34
Running loss of epoch-40 batch-34 = 2.4159671738743782e-05

Training epoch-40 batch-35
Running loss of epoch-40 batch-35 = 9.454553946852684e-06

Training epoch-40 batch-36
Running loss of epoch-40 batch-36 = 3.301398828625679e-05

Training epoch-40 batch-37
Running loss of epoch-40 batch-37 = 1.395377330482006e-05

Training epoch-40 batch-38
Running loss of epoch-40 batch-38 = 3.0549708753824234e-06

Training epoch-40 batch-39
Running loss of epoch-40 batch-39 = 6.00120984017849e-06

Training epoch-40 batch-40
Running loss of epoch-40 batch-40 = 1.4341669157147408e-05

Training epoch-40 batch-41
Running loss of epoch-40 batch-41 = 2.374313771724701e-05

Training epoch-40 batch-42
Running loss of epoch-40 batch-42 = 9.217299520969391e-06

Training epoch-40 batch-43
Running loss of epoch-40 batch-43 = 1.3636425137519836e-05

Training epoch-40 batch-44
Running loss of epoch-40 batch-44 = 1.4906283468008041e-05

Training epoch-40 batch-45
Running loss of epoch-40 batch-45 = 1.2619653716683388e-05

Training epoch-40 batch-46
Running loss of epoch-40 batch-46 = 1.2191012501716614e-05

Training epoch-40 batch-47
Running loss of epoch-40 batch-47 = 7.338204886764288e-05

Training epoch-40 batch-48
Running loss of epoch-40 batch-48 = 2.509285695850849e-05

Training epoch-40 batch-49
Running loss of epoch-40 batch-49 = 4.666624590754509e-06

Training epoch-40 batch-50
Running loss of epoch-40 batch-50 = 1.9130762666463852e-05

Training epoch-40 batch-51
Running loss of epoch-40 batch-51 = 1.0102987289428711e-05

Training epoch-40 batch-52
Running loss of epoch-40 batch-52 = 1.6871606931090355e-05

Training epoch-40 batch-53
Running loss of epoch-40 batch-53 = 2.166791819036007e-05

Training epoch-40 batch-54
Running loss of epoch-40 batch-54 = 2.1833693608641624e-05

Training epoch-40 batch-55
Running loss of epoch-40 batch-55 = 1.971609890460968e-05

Training epoch-40 batch-56
Running loss of epoch-40 batch-56 = 3.412226215004921e-05

Training epoch-40 batch-57
Running loss of epoch-40 batch-57 = 2.600555308163166e-05

Training epoch-40 batch-58
Running loss of epoch-40 batch-58 = 2.6990193873643875e-05

Training epoch-40 batch-59
Running loss of epoch-40 batch-59 = 0.00015704170800745487

Training epoch-40 batch-60
Running loss of epoch-40 batch-60 = 1.7280690371990204e-05

Training epoch-40 batch-61
Running loss of epoch-40 batch-61 = 9.183771908283234e-06

Training epoch-40 batch-62
Running loss of epoch-40 batch-62 = 1.4440156519412994e-05

Training epoch-40 batch-63
Running loss of epoch-40 batch-63 = 1.5422236174345016e-05

Training epoch-40 batch-64
Running loss of epoch-40 batch-64 = 1.4760764315724373e-05

Training epoch-40 batch-65
Running loss of epoch-40 batch-65 = 1.8998747691512108e-05

Training epoch-40 batch-66
Running loss of epoch-40 batch-66 = 2.043996937572956e-05

Training epoch-40 batch-67
Running loss of epoch-40 batch-67 = 2.746214158833027e-05

Training epoch-40 batch-68
Running loss of epoch-40 batch-68 = 1.6880687326192856e-05

Training epoch-40 batch-69
Running loss of epoch-40 batch-69 = 3.5857316106557846e-05

Training epoch-40 batch-70
Running loss of epoch-40 batch-70 = 3.169616684317589e-05

Training epoch-40 batch-71
Running loss of epoch-40 batch-71 = 2.5799963623285294e-05

Training epoch-40 batch-72
Running loss of epoch-40 batch-72 = 2.327817492187023e-05

Training epoch-40 batch-73
Running loss of epoch-40 batch-73 = 1.5520025044679642e-05

Training epoch-40 batch-74
Running loss of epoch-40 batch-74 = 1.0102754458785057e-05

Training epoch-40 batch-75
Running loss of epoch-40 batch-75 = 3.300677053630352e-05

Training epoch-40 batch-76
Running loss of epoch-40 batch-76 = 1.867208629846573e-05

Training epoch-40 batch-77
Running loss of epoch-40 batch-77 = 2.9470771551132202e-05

Training epoch-40 batch-78
Running loss of epoch-40 batch-78 = 1.8919352442026138e-05

Training epoch-40 batch-79
Running loss of epoch-40 batch-79 = 3.1543662771582603e-05

Training epoch-40 batch-80
Running loss of epoch-40 batch-80 = 1.750839874148369e-05

Training epoch-40 batch-81
Running loss of epoch-40 batch-81 = 9.382609277963638e-06

Training epoch-40 batch-82
Running loss of epoch-40 batch-82 = 1.9952887669205666e-05

Training epoch-40 batch-83
Running loss of epoch-40 batch-83 = 2.0254869014024734e-05

Training epoch-40 batch-84
Running loss of epoch-40 batch-84 = 1.423596404492855e-05

Training epoch-40 batch-85
Running loss of epoch-40 batch-85 = 1.4814315363764763e-05

Training epoch-40 batch-86
Running loss of epoch-40 batch-86 = 1.7205718904733658e-05

Training epoch-40 batch-87
Running loss of epoch-40 batch-87 = 4.289252683520317e-05

Training epoch-40 batch-88
Running loss of epoch-40 batch-88 = 3.795907832682133e-05

Training epoch-40 batch-89
Running loss of epoch-40 batch-89 = 1.5170546248555183e-05

Training epoch-40 batch-90
Running loss of epoch-40 batch-90 = 7.43999844416976e-05

Training epoch-40 batch-91
Running loss of epoch-40 batch-91 = 2.690800465643406e-05

Training epoch-40 batch-92
Running loss of epoch-40 batch-92 = 2.5663990527391434e-05

Training epoch-40 batch-93
Running loss of epoch-40 batch-93 = 1.4255056157708168e-05

Training epoch-40 batch-94
Running loss of epoch-40 batch-94 = 6.756489165127277e-05

Training epoch-40 batch-95
Running loss of epoch-40 batch-95 = 2.1790852770209312e-05

Training epoch-40 batch-96
Running loss of epoch-40 batch-96 = 1.3046665117144585e-05

Training epoch-40 batch-97
Running loss of epoch-40 batch-97 = 1.5594763681292534e-05

Training epoch-40 batch-98
Running loss of epoch-40 batch-98 = 1.4505349099636078e-05

Training epoch-40 batch-99
Running loss of epoch-40 batch-99 = 1.6375211998820305e-05

Training epoch-40 batch-100
Running loss of epoch-40 batch-100 = 8.799601346254349e-06

Training epoch-40 batch-101
Running loss of epoch-40 batch-101 = 1.1613825336098671e-05

Training epoch-40 batch-102
Running loss of epoch-40 batch-102 = 1.0729767382144928e-05

Training epoch-40 batch-103
Running loss of epoch-40 batch-103 = 2.5464920327067375e-05

Training epoch-40 batch-104
Running loss of epoch-40 batch-104 = 2.953503280878067e-05

Training epoch-40 batch-105
Running loss of epoch-40 batch-105 = 1.6073230654001236e-05

Training epoch-40 batch-106
Running loss of epoch-40 batch-106 = 5.870009772479534e-05

Training epoch-40 batch-107
Running loss of epoch-40 batch-107 = 1.969141885638237e-05

Training epoch-40 batch-108
Running loss of epoch-40 batch-108 = 1.9496306777000427e-05

Training epoch-40 batch-109
Running loss of epoch-40 batch-109 = 2.8166454285383224e-05

Training epoch-40 batch-110
Running loss of epoch-40 batch-110 = 8.337665349245071e-06

Training epoch-40 batch-111
Running loss of epoch-40 batch-111 = 1.789000816643238e-05

Training epoch-40 batch-112
Running loss of epoch-40 batch-112 = 1.3479730114340782e-05

Training epoch-40 batch-113
Running loss of epoch-40 batch-113 = 8.290866389870644e-06

Training epoch-40 batch-114
Running loss of epoch-40 batch-114 = 2.631847746670246e-05

Training epoch-40 batch-115
Running loss of epoch-40 batch-115 = 7.124617695808411e-06

Training epoch-40 batch-116
Running loss of epoch-40 batch-116 = 4.6996865421533585e-05

Training epoch-40 batch-117
Running loss of epoch-40 batch-117 = 2.3962929844856262e-05

Training epoch-40 batch-118
Running loss of epoch-40 batch-118 = 1.4136312529444695e-05

Training epoch-40 batch-119
Running loss of epoch-40 batch-119 = 6.216345354914665e-06

Training epoch-40 batch-120
Running loss of epoch-40 batch-120 = 1.673470251262188e-05

Training epoch-40 batch-121
Running loss of epoch-40 batch-121 = 2.9496382921934128e-05

Training epoch-40 batch-122
Running loss of epoch-40 batch-122 = 1.2335600331425667e-05

Training epoch-40 batch-123
Running loss of epoch-40 batch-123 = 8.892500773072243e-06

Training epoch-40 batch-124
Running loss of epoch-40 batch-124 = 1.1271098628640175e-05

Training epoch-40 batch-125
Running loss of epoch-40 batch-125 = 1.093745231628418e-05

Training epoch-40 batch-126
Running loss of epoch-40 batch-126 = 1.109810546040535e-05

Training epoch-40 batch-127
Running loss of epoch-40 batch-127 = 2.337922342121601e-05

Training epoch-40 batch-128
Running loss of epoch-40 batch-128 = 1.3786368072032928e-05

Training epoch-40 batch-129
Running loss of epoch-40 batch-129 = 2.176361158490181e-05

Training epoch-40 batch-130
Running loss of epoch-40 batch-130 = 1.4804070815443993e-05

Training epoch-40 batch-131
Running loss of epoch-40 batch-131 = 2.2113323211669922e-05

Training epoch-40 batch-132
Running loss of epoch-40 batch-132 = 2.8569484129548073e-05

Training epoch-40 batch-133
Running loss of epoch-40 batch-133 = 2.3548724129796028e-05

Training epoch-40 batch-134
Running loss of epoch-40 batch-134 = 1.5963101759552956e-05

Training epoch-40 batch-135
Running loss of epoch-40 batch-135 = 6.905221380293369e-05

Training epoch-40 batch-136
Running loss of epoch-40 batch-136 = 2.6958296075463295e-05

Training epoch-40 batch-137
Running loss of epoch-40 batch-137 = 6.416672840714455e-05

Training epoch-40 batch-138
Running loss of epoch-40 batch-138 = 1.0491814464330673e-05

Training epoch-40 batch-139
Running loss of epoch-40 batch-139 = 8.152332156896591e-06

Training epoch-40 batch-140
Running loss of epoch-40 batch-140 = 1.6377773135900497e-05

Training epoch-40 batch-141
Running loss of epoch-40 batch-141 = 1.953612081706524e-05

Training epoch-40 batch-142
Running loss of epoch-40 batch-142 = 2.2144289687275887e-05

Training epoch-40 batch-143
Running loss of epoch-40 batch-143 = 9.498000144958496e-05

Training epoch-40 batch-144
Running loss of epoch-40 batch-144 = 3.494229167699814e-05

Training epoch-40 batch-145
Running loss of epoch-40 batch-145 = 4.348810762166977e-06

Training epoch-40 batch-146
Running loss of epoch-40 batch-146 = 2.5506597012281418e-05

Training epoch-40 batch-147
Running loss of epoch-40 batch-147 = 1.8584774807095528e-05

Training epoch-40 batch-148
Running loss of epoch-40 batch-148 = 1.6924692317843437e-05

Training epoch-40 batch-149
Running loss of epoch-40 batch-149 = 1.7301645129919052e-05

Training epoch-40 batch-150
Running loss of epoch-40 batch-150 = 1.1178315617144108e-05

Training epoch-40 batch-151
Running loss of epoch-40 batch-151 = 1.5188008546829224e-05

Training epoch-40 batch-152
Running loss of epoch-40 batch-152 = 1.5590107068419456e-05

Training epoch-40 batch-153
Running loss of epoch-40 batch-153 = 2.158619463443756e-05

Training epoch-40 batch-154
Running loss of epoch-40 batch-154 = 8.647330105304718e-06

Training epoch-40 batch-155
Running loss of epoch-40 batch-155 = 1.4519551768898964e-05

Training epoch-40 batch-156
Running loss of epoch-40 batch-156 = 8.726958185434341e-06

Training epoch-40 batch-157
Running loss of epoch-40 batch-157 = 2.2977590560913086e-05

Finished training epoch-40.



Average train loss at epoch-40 = 2.277747318148613e-05

Started Evaluation

Average val loss at epoch-40 = 1.5307783313127354

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.69 %
Accuracy for class onCreate is: 89.23 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 64.36 %

Overall Accuracy = 81.33 %

Finished Evaluation



Started training epoch-41


Training epoch-41 batch-1
Running loss of epoch-41 batch-1 = 1.1417316272854805e-05

Training epoch-41 batch-2
Running loss of epoch-41 batch-2 = 3.481796011328697e-05

Training epoch-41 batch-3
Running loss of epoch-41 batch-3 = 1.7348909750580788e-05

Training epoch-41 batch-4
Running loss of epoch-41 batch-4 = 1.5527941286563873e-05

Training epoch-41 batch-5
Running loss of epoch-41 batch-5 = 1.0611489415168762e-05

Training epoch-41 batch-6
Running loss of epoch-41 batch-6 = 2.130446955561638e-05

Training epoch-41 batch-7
Running loss of epoch-41 batch-7 = 2.0890962332487106e-05

Training epoch-41 batch-8
Running loss of epoch-41 batch-8 = 1.4624791219830513e-05

Training epoch-41 batch-9
Running loss of epoch-41 batch-9 = 6.253505125641823e-05

Training epoch-41 batch-10
Running loss of epoch-41 batch-10 = 2.0939158275723457e-05

Training epoch-41 batch-11
Running loss of epoch-41 batch-11 = 1.5517231076955795e-05

Training epoch-41 batch-12
Running loss of epoch-41 batch-12 = 1.4639226719737053e-05

Training epoch-41 batch-13
Running loss of epoch-41 batch-13 = 1.0537216439843178e-05

Training epoch-41 batch-14
Running loss of epoch-41 batch-14 = 2.5224871933460236e-05

Training epoch-41 batch-15
Running loss of epoch-41 batch-15 = 1.5040859580039978e-05

Training epoch-41 batch-16
Running loss of epoch-41 batch-16 = 9.223120287060738e-06

Training epoch-41 batch-17
Running loss of epoch-41 batch-17 = 1.8582912161946297e-05

Training epoch-41 batch-18
Running loss of epoch-41 batch-18 = 2.8611626476049423e-05

Training epoch-41 batch-19
Running loss of epoch-41 batch-19 = 1.3915356248617172e-05

Training epoch-41 batch-20
Running loss of epoch-41 batch-20 = 2.6542693376541138e-05

Training epoch-41 batch-21
Running loss of epoch-41 batch-21 = 6.611226126551628e-06

Training epoch-41 batch-22
Running loss of epoch-41 batch-22 = 1.1180760338902473e-05

Training epoch-41 batch-23
Running loss of epoch-41 batch-23 = 3.111641854047775e-05

Training epoch-41 batch-24
Running loss of epoch-41 batch-24 = 1.0315096005797386e-05

Training epoch-41 batch-25
Running loss of epoch-41 batch-25 = 2.2189458832144737e-05

Training epoch-41 batch-26
Running loss of epoch-41 batch-26 = 1.2673204764723778e-05

Training epoch-41 batch-27
Running loss of epoch-41 batch-27 = 4.7711655497550964e-06

Training epoch-41 batch-28
Running loss of epoch-41 batch-28 = 9.7593292593956e-06

Training epoch-41 batch-29
Running loss of epoch-41 batch-29 = 2.2379448637366295e-05

Training epoch-41 batch-30
Running loss of epoch-41 batch-30 = 1.964438706636429e-05

Training epoch-41 batch-31
Running loss of epoch-41 batch-31 = 1.4367746189236641e-05

Training epoch-41 batch-32
Running loss of epoch-41 batch-32 = 4.682224243879318e-06

Training epoch-41 batch-33
Running loss of epoch-41 batch-33 = 1.785624772310257e-05

Training epoch-41 batch-34
Running loss of epoch-41 batch-34 = 1.5105819329619408e-05

Training epoch-41 batch-35
Running loss of epoch-41 batch-35 = 5.38660679012537e-05

Training epoch-41 batch-36
Running loss of epoch-41 batch-36 = 4.150671884417534e-06

Training epoch-41 batch-37
Running loss of epoch-41 batch-37 = 2.4289824068546295e-05

Training epoch-41 batch-38
Running loss of epoch-41 batch-38 = 1.2927455827593803e-05

Training epoch-41 batch-39
Running loss of epoch-41 batch-39 = 9.899027645587921e-06

Training epoch-41 batch-40
Running loss of epoch-41 batch-40 = 2.5415094569325447e-05

Training epoch-41 batch-41
Running loss of epoch-41 batch-41 = 5.065381992608309e-05

Training epoch-41 batch-42
Running loss of epoch-41 batch-42 = 1.9977102056145668e-05

Training epoch-41 batch-43
Running loss of epoch-41 batch-43 = 2.101040445268154e-05

Training epoch-41 batch-44
Running loss of epoch-41 batch-44 = 1.4789169654250145e-05

Training epoch-41 batch-45
Running loss of epoch-41 batch-45 = 1.5119090676307678e-05

Training epoch-41 batch-46
Running loss of epoch-41 batch-46 = 1.893332228064537e-05

Training epoch-41 batch-47
Running loss of epoch-41 batch-47 = 1.275399699807167e-05

Training epoch-41 batch-48
Running loss of epoch-41 batch-48 = 1.4124670997262001e-05

Training epoch-41 batch-49
Running loss of epoch-41 batch-49 = 2.0064879208803177e-05

Training epoch-41 batch-50
Running loss of epoch-41 batch-50 = 9.792391210794449e-06

Training epoch-41 batch-51
Running loss of epoch-41 batch-51 = 1.2091128155589104e-05

Training epoch-41 batch-52
Running loss of epoch-41 batch-52 = 1.6993843019008636e-05

Training epoch-41 batch-53
Running loss of epoch-41 batch-53 = 2.9299873858690262e-05

Training epoch-41 batch-54
Running loss of epoch-41 batch-54 = 3.5336706787347794e-06

Training epoch-41 batch-55
Running loss of epoch-41 batch-55 = 5.2370596677064896e-06

Training epoch-41 batch-56
Running loss of epoch-41 batch-56 = 1.5161698684096336e-05

Training epoch-41 batch-57
Running loss of epoch-41 batch-57 = 1.9322149455547333e-05

Training epoch-41 batch-58
Running loss of epoch-41 batch-58 = 1.31949782371521e-05

Training epoch-41 batch-59
Running loss of epoch-41 batch-59 = 1.37395691126585e-05

Training epoch-41 batch-60
Running loss of epoch-41 batch-60 = 1.1728610843420029e-05

Training epoch-41 batch-61
Running loss of epoch-41 batch-61 = 2.2635795176029205e-05

Training epoch-41 batch-62
Running loss of epoch-41 batch-62 = 4.761223681271076e-05

Training epoch-41 batch-63
Running loss of epoch-41 batch-63 = 1.8650200217962265e-05

Training epoch-41 batch-64
Running loss of epoch-41 batch-64 = 9.587965905666351e-06

Training epoch-41 batch-65
Running loss of epoch-41 batch-65 = 8.284114301204681e-06

Training epoch-41 batch-66
Running loss of epoch-41 batch-66 = 2.1293992176651955e-05

Training epoch-41 batch-67
Running loss of epoch-41 batch-67 = 1.0612187907099724e-05

Training epoch-41 batch-68
Running loss of epoch-41 batch-68 = 9.696464985609055e-06

Training epoch-41 batch-69
Running loss of epoch-41 batch-69 = 1.1906959116458893e-05

Training epoch-41 batch-70
Running loss of epoch-41 batch-70 = 1.035863533616066e-05

Training epoch-41 batch-71
Running loss of epoch-41 batch-71 = 1.848023384809494e-05

Training epoch-41 batch-72
Running loss of epoch-41 batch-72 = 1.4178920537233353e-05

Training epoch-41 batch-73
Running loss of epoch-41 batch-73 = 8.916249498724937e-06

Training epoch-41 batch-74
Running loss of epoch-41 batch-74 = 2.8409762308001518e-05

Training epoch-41 batch-75
Running loss of epoch-41 batch-75 = 9.027810301631689e-05

Training epoch-41 batch-76
Running loss of epoch-41 batch-76 = 1.0074581950902939e-05

Training epoch-41 batch-77
Running loss of epoch-41 batch-77 = 2.587796188890934e-05

Training epoch-41 batch-78
Running loss of epoch-41 batch-78 = 2.460833638906479e-05

Training epoch-41 batch-79
Running loss of epoch-41 batch-79 = 3.308453597128391e-05

Training epoch-41 batch-80
Running loss of epoch-41 batch-80 = 1.5655066817998886e-05

Training epoch-41 batch-81
Running loss of epoch-41 batch-81 = 2.041039988398552e-05

Training epoch-41 batch-82
Running loss of epoch-41 batch-82 = 2.8097303584218025e-05

Training epoch-41 batch-83
Running loss of epoch-41 batch-83 = 1.8454622477293015e-05

Training epoch-41 batch-84
Running loss of epoch-41 batch-84 = 6.748363375663757e-06

Training epoch-41 batch-85
Running loss of epoch-41 batch-85 = 1.6407575458288193e-05

Training epoch-41 batch-86
Running loss of epoch-41 batch-86 = 4.730187356472015e-06

Training epoch-41 batch-87
Running loss of epoch-41 batch-87 = 2.5766436010599136e-05

Training epoch-41 batch-88
Running loss of epoch-41 batch-88 = 1.4398712664842606e-05

Training epoch-41 batch-89
Running loss of epoch-41 batch-89 = 7.041497156023979e-06

Training epoch-41 batch-90
Running loss of epoch-41 batch-90 = 8.385395631194115e-06

Training epoch-41 batch-91
Running loss of epoch-41 batch-91 = 7.163500413298607e-06

Training epoch-41 batch-92
Running loss of epoch-41 batch-92 = 1.328461803495884e-05

Training epoch-41 batch-93
Running loss of epoch-41 batch-93 = 2.1900516003370285e-05

Training epoch-41 batch-94
Running loss of epoch-41 batch-94 = 6.415415555238724e-06

Training epoch-41 batch-95
Running loss of epoch-41 batch-95 = 1.7459504306316376e-05

Training epoch-41 batch-96
Running loss of epoch-41 batch-96 = 6.745802238583565e-06

Training epoch-41 batch-97
Running loss of epoch-41 batch-97 = 4.066620022058487e-06

Training epoch-41 batch-98
Running loss of epoch-41 batch-98 = 1.4580786228179932e-05

Training epoch-41 batch-99
Running loss of epoch-41 batch-99 = 1.8568476662039757e-05

Training epoch-41 batch-100
Running loss of epoch-41 batch-100 = 2.716435119509697e-05

Training epoch-41 batch-101
Running loss of epoch-41 batch-101 = 1.2926524505019188e-05

Training epoch-41 batch-102
Running loss of epoch-41 batch-102 = 4.840083420276642e-06

Training epoch-41 batch-103
Running loss of epoch-41 batch-103 = 7.142079994082451e-06

Training epoch-41 batch-104
Running loss of epoch-41 batch-104 = 3.3748801797628403e-06

Training epoch-41 batch-105
Running loss of epoch-41 batch-105 = 5.048932507634163e-06

Training epoch-41 batch-106
Running loss of epoch-41 batch-106 = 1.1820346117019653e-05

Training epoch-41 batch-107
Running loss of epoch-41 batch-107 = 1.4193123206496239e-05

Training epoch-41 batch-108
Running loss of epoch-41 batch-108 = 6.556045264005661e-06

Training epoch-41 batch-109
Running loss of epoch-41 batch-109 = 1.5787780284881592e-05

Training epoch-41 batch-110
Running loss of epoch-41 batch-110 = 2.467283047735691e-05

Training epoch-41 batch-111
Running loss of epoch-41 batch-111 = 1.1931871995329857e-05

Training epoch-41 batch-112
Running loss of epoch-41 batch-112 = 8.729984983801842e-06

Training epoch-41 batch-113
Running loss of epoch-41 batch-113 = 7.83032737672329e-06

Training epoch-41 batch-114
Running loss of epoch-41 batch-114 = 2.2723572328686714e-05

Training epoch-41 batch-115
Running loss of epoch-41 batch-115 = 1.2415461242198944e-05

Training epoch-41 batch-116
Running loss of epoch-41 batch-116 = 1.4810357242822647e-05

Training epoch-41 batch-117
Running loss of epoch-41 batch-117 = 1.4859717339277267e-05

Training epoch-41 batch-118
Running loss of epoch-41 batch-118 = 9.803567081689835e-06

Training epoch-41 batch-119
Running loss of epoch-41 batch-119 = 2.5279587134718895e-05

Training epoch-41 batch-120
Running loss of epoch-41 batch-120 = 2.960185520350933e-05

Training epoch-41 batch-121
Running loss of epoch-41 batch-121 = 9.478069841861725e-06

Training epoch-41 batch-122
Running loss of epoch-41 batch-122 = 7.092161104083061e-05

Training epoch-41 batch-123
Running loss of epoch-41 batch-123 = 1.8996186554431915e-05

Training epoch-41 batch-124
Running loss of epoch-41 batch-124 = 1.4862976968288422e-05

Training epoch-41 batch-125
Running loss of epoch-41 batch-125 = 4.30642394348979e-05

Training epoch-41 batch-126
Running loss of epoch-41 batch-126 = 8.028466254472733e-06

Training epoch-41 batch-127
Running loss of epoch-41 batch-127 = 7.950677536427975e-05

Training epoch-41 batch-128
Running loss of epoch-41 batch-128 = 1.5283003449440002e-05

Training epoch-41 batch-129
Running loss of epoch-41 batch-129 = 1.5614088624715805e-05

Training epoch-41 batch-130
Running loss of epoch-41 batch-130 = 8.200760930776596e-06

Training epoch-41 batch-131
Running loss of epoch-41 batch-131 = 1.7887447029352188e-05

Training epoch-41 batch-132
Running loss of epoch-41 batch-132 = 2.3677712306380272e-05

Training epoch-41 batch-133
Running loss of epoch-41 batch-133 = 1.3154814951121807e-05

Training epoch-41 batch-134
Running loss of epoch-41 batch-134 = 1.5844590961933136e-05

Training epoch-41 batch-135
Running loss of epoch-41 batch-135 = 1.141219399869442e-05

Training epoch-41 batch-136
Running loss of epoch-41 batch-136 = 1.3810582458972931e-05

Training epoch-41 batch-137
Running loss of epoch-41 batch-137 = 7.324432954192162e-05

Training epoch-41 batch-138
Running loss of epoch-41 batch-138 = 8.032284677028656e-05

Training epoch-41 batch-139
Running loss of epoch-41 batch-139 = 7.663457654416561e-05

Training epoch-41 batch-140
Running loss of epoch-41 batch-140 = 1.0102055966854095e-05

Training epoch-41 batch-141
Running loss of epoch-41 batch-141 = 1.704948954284191e-05

Training epoch-41 batch-142
Running loss of epoch-41 batch-142 = 6.604474037885666e-06

Training epoch-41 batch-143
Running loss of epoch-41 batch-143 = 9.133713319897652e-06

Training epoch-41 batch-144
Running loss of epoch-41 batch-144 = 8.34604725241661e-06

Training epoch-41 batch-145
Running loss of epoch-41 batch-145 = 3.3938093110919e-05

Training epoch-41 batch-146
Running loss of epoch-41 batch-146 = 1.0802876204252243e-05

Training epoch-41 batch-147
Running loss of epoch-41 batch-147 = 2.770521678030491e-05

Training epoch-41 batch-148
Running loss of epoch-41 batch-148 = 2.191169187426567e-05

Training epoch-41 batch-149
Running loss of epoch-41 batch-149 = 2.0383857190608978e-05

Training epoch-41 batch-150
Running loss of epoch-41 batch-150 = 3.08244489133358e-05

Training epoch-41 batch-151
Running loss of epoch-41 batch-151 = 3.618141636252403e-05

Training epoch-41 batch-152
Running loss of epoch-41 batch-152 = 2.3125438019633293e-05

Training epoch-41 batch-153
Running loss of epoch-41 batch-153 = 3.482308238744736e-05

Training epoch-41 batch-154
Running loss of epoch-41 batch-154 = 0.00013656425289809704

Training epoch-41 batch-155
Running loss of epoch-41 batch-155 = 5.574431270360947e-06

Training epoch-41 batch-156
Running loss of epoch-41 batch-156 = 1.2625940144062042e-05

Training epoch-41 batch-157
Running loss of epoch-41 batch-157 = 3.293156623840332e-05

Finished training epoch-41.



Average train loss at epoch-41 = 2.006266415119171e-05

Started Evaluation

Average val loss at epoch-41 = 1.5566533475760085

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 88.81 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 65.53 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 52.61 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.18 %

Finished Evaluation



Started training epoch-42


Training epoch-42 batch-1
Running loss of epoch-42 batch-1 = 7.749535143375397e-06

Training epoch-42 batch-2
Running loss of epoch-42 batch-2 = 1.30506232380867e-05

Training epoch-42 batch-3
Running loss of epoch-42 batch-3 = 1.0900897905230522e-05

Training epoch-42 batch-4
Running loss of epoch-42 batch-4 = 3.4714117646217346e-05

Training epoch-42 batch-5
Running loss of epoch-42 batch-5 = 2.11494043469429e-05

Training epoch-42 batch-6
Running loss of epoch-42 batch-6 = 1.3239448890089989e-05

Training epoch-42 batch-7
Running loss of epoch-42 batch-7 = 1.2380536645650864e-05

Training epoch-42 batch-8
Running loss of epoch-42 batch-8 = 1.7354963347315788e-05

Training epoch-42 batch-9
Running loss of epoch-42 batch-9 = 5.8407895267009735e-06

Training epoch-42 batch-10
Running loss of epoch-42 batch-10 = 6.751157343387604e-06

Training epoch-42 batch-11
Running loss of epoch-42 batch-11 = 1.0056653991341591e-05

Training epoch-42 batch-12
Running loss of epoch-42 batch-12 = 1.3878103345632553e-05

Training epoch-42 batch-13
Running loss of epoch-42 batch-13 = 2.2051390260457993e-05

Training epoch-42 batch-14
Running loss of epoch-42 batch-14 = 9.349314495921135e-06

Training epoch-42 batch-15
Running loss of epoch-42 batch-15 = 4.118075594305992e-06

Training epoch-42 batch-16
Running loss of epoch-42 batch-16 = 2.0448118448257446e-05

Training epoch-42 batch-17
Running loss of epoch-42 batch-17 = 7.79610127210617e-06

Training epoch-42 batch-18
Running loss of epoch-42 batch-18 = 2.508983016014099e-05

Training epoch-42 batch-19
Running loss of epoch-42 batch-19 = 3.1861476600170135e-05

Training epoch-42 batch-20
Running loss of epoch-42 batch-20 = 7.260357961058617e-06

Training epoch-42 batch-21
Running loss of epoch-42 batch-21 = 1.6797799617052078e-05

Training epoch-42 batch-22
Running loss of epoch-42 batch-22 = 3.8959551602602005e-05

Training epoch-42 batch-23
Running loss of epoch-42 batch-23 = 1.5492085367441177e-05

Training epoch-42 batch-24
Running loss of epoch-42 batch-24 = 1.714145764708519e-05

Training epoch-42 batch-25
Running loss of epoch-42 batch-25 = 1.0007061064243317e-05

Training epoch-42 batch-26
Running loss of epoch-42 batch-26 = 1.2560980394482613e-05

Training epoch-42 batch-27
Running loss of epoch-42 batch-27 = 1.1981930583715439e-05

Training epoch-42 batch-28
Running loss of epoch-42 batch-28 = 9.935349225997925e-06

Training epoch-42 batch-29
Running loss of epoch-42 batch-29 = 1.2131407856941223e-05

Training epoch-42 batch-30
Running loss of epoch-42 batch-30 = 1.4213030226528645e-05

Training epoch-42 batch-31
Running loss of epoch-42 batch-31 = 1.2628268450498581e-05

Training epoch-42 batch-32
Running loss of epoch-42 batch-32 = 1.717032864689827e-05

Training epoch-42 batch-33
Running loss of epoch-42 batch-33 = 3.263913094997406e-05

Training epoch-42 batch-34
Running loss of epoch-42 batch-34 = 5.34416176378727e-06

Training epoch-42 batch-35
Running loss of epoch-42 batch-35 = 1.4557735994458199e-05

Training epoch-42 batch-36
Running loss of epoch-42 batch-36 = 1.5786150470376015e-05

Training epoch-42 batch-37
Running loss of epoch-42 batch-37 = 1.7733080312609673e-05

Training epoch-42 batch-38
Running loss of epoch-42 batch-38 = 1.9681639969348907e-05

Training epoch-42 batch-39
Running loss of epoch-42 batch-39 = 4.200986586511135e-05

Training epoch-42 batch-40
Running loss of epoch-42 batch-40 = 5.462905392050743e-06

Training epoch-42 batch-41
Running loss of epoch-42 batch-41 = 1.2904172763228416e-05

Training epoch-42 batch-42
Running loss of epoch-42 batch-42 = 1.2565869837999344e-05

Training epoch-42 batch-43
Running loss of epoch-42 batch-43 = 0.00011396734043955803

Training epoch-42 batch-44
Running loss of epoch-42 batch-44 = 1.9724247977137566e-05

Training epoch-42 batch-45
Running loss of epoch-42 batch-45 = 1.8582213670015335e-05

Training epoch-42 batch-46
Running loss of epoch-42 batch-46 = 1.5751691535115242e-05

Training epoch-42 batch-47
Running loss of epoch-42 batch-47 = 9.993789717555046e-06

Training epoch-42 batch-48
Running loss of epoch-42 batch-48 = 8.566305041313171e-06

Training epoch-42 batch-49
Running loss of epoch-42 batch-49 = 2.9287301003932953e-05

Training epoch-42 batch-50
Running loss of epoch-42 batch-50 = 7.438007742166519e-06

Training epoch-42 batch-51
Running loss of epoch-42 batch-51 = 1.5718862414360046e-05

Training epoch-42 batch-52
Running loss of epoch-42 batch-52 = 2.5719404220581055e-05

Training epoch-42 batch-53
Running loss of epoch-42 batch-53 = 7.736030966043472e-06

Training epoch-42 batch-54
Running loss of epoch-42 batch-54 = 1.4285324141383171e-05

Training epoch-42 batch-55
Running loss of epoch-42 batch-55 = 1.6503967344760895e-05

Training epoch-42 batch-56
Running loss of epoch-42 batch-56 = 1.1883443221449852e-05

Training epoch-42 batch-57
Running loss of epoch-42 batch-57 = 1.742434687912464e-05

Training epoch-42 batch-58
Running loss of epoch-42 batch-58 = 6.9784000515937805e-06

Training epoch-42 batch-59
Running loss of epoch-42 batch-59 = 1.5669967979192734e-05

Training epoch-42 batch-60
Running loss of epoch-42 batch-60 = 2.210307866334915e-05

Training epoch-42 batch-61
Running loss of epoch-42 batch-61 = 5.421694368124008e-06

Training epoch-42 batch-62
Running loss of epoch-42 batch-62 = 1.4532124623656273e-05

Training epoch-42 batch-63
Running loss of epoch-42 batch-63 = 2.5010202080011368e-05

Training epoch-42 batch-64
Running loss of epoch-42 batch-64 = 2.2196676582098007e-05

Training epoch-42 batch-65
Running loss of epoch-42 batch-65 = 1.3669021427631378e-05

Training epoch-42 batch-66
Running loss of epoch-42 batch-66 = 2.0316801965236664e-05

Training epoch-42 batch-67
Running loss of epoch-42 batch-67 = 1.0707415640354156e-05

Training epoch-42 batch-68
Running loss of epoch-42 batch-68 = 2.8833746910095215e-06

Training epoch-42 batch-69
Running loss of epoch-42 batch-69 = 6.170244887471199e-06

Training epoch-42 batch-70
Running loss of epoch-42 batch-70 = 2.079550176858902e-05

Training epoch-42 batch-71
Running loss of epoch-42 batch-71 = 1.1200550943613052e-05

Training epoch-42 batch-72
Running loss of epoch-42 batch-72 = 1.727440394461155e-05

Training epoch-42 batch-73
Running loss of epoch-42 batch-73 = 1.2651318684220314e-05

Training epoch-42 batch-74
Running loss of epoch-42 batch-74 = 1.3326993212103844e-05

Training epoch-42 batch-75
Running loss of epoch-42 batch-75 = 1.870770938694477e-05

Training epoch-42 batch-76
Running loss of epoch-42 batch-76 = 2.504047006368637e-05

Training epoch-42 batch-77
Running loss of epoch-42 batch-77 = 1.112697646021843e-05

Training epoch-42 batch-78
Running loss of epoch-42 batch-78 = 1.3699056580662727e-05

Training epoch-42 batch-79
Running loss of epoch-42 batch-79 = 2.0247185602784157e-05

Training epoch-42 batch-80
Running loss of epoch-42 batch-80 = 2.1281884983181953e-05

Training epoch-42 batch-81
Running loss of epoch-42 batch-81 = 5.923211574554443e-06

Training epoch-42 batch-82
Running loss of epoch-42 batch-82 = 8.966773748397827e-06

Training epoch-42 batch-83
Running loss of epoch-42 batch-83 = 1.937500201165676e-05

Training epoch-42 batch-84
Running loss of epoch-42 batch-84 = 1.0743271559476852e-05

Training epoch-42 batch-85
Running loss of epoch-42 batch-85 = 1.8971506506204605e-05

Training epoch-42 batch-86
Running loss of epoch-42 batch-86 = 1.5275203622877598e-05

Training epoch-42 batch-87
Running loss of epoch-42 batch-87 = 1.1935364454984665e-05

Training epoch-42 batch-88
Running loss of epoch-42 batch-88 = 1.3848766684532166e-05

Training epoch-42 batch-89
Running loss of epoch-42 batch-89 = 3.053806722164154e-05

Training epoch-42 batch-90
Running loss of epoch-42 batch-90 = 2.3996224626898766e-05

Training epoch-42 batch-91
Running loss of epoch-42 batch-91 = 1.9883504137396812e-05

Training epoch-42 batch-92
Running loss of epoch-42 batch-92 = 2.212636172771454e-05

Training epoch-42 batch-93
Running loss of epoch-42 batch-93 = 1.3282988220453262e-05

Training epoch-42 batch-94
Running loss of epoch-42 batch-94 = 8.476199582219124e-06

Training epoch-42 batch-95
Running loss of epoch-42 batch-95 = 3.076787106692791e-05

Training epoch-42 batch-96
Running loss of epoch-42 batch-96 = 3.8398196920752525e-05

Training epoch-42 batch-97
Running loss of epoch-42 batch-97 = 4.9814581871032715e-05

Training epoch-42 batch-98
Running loss of epoch-42 batch-98 = 3.800657577812672e-05

Training epoch-42 batch-99
Running loss of epoch-42 batch-99 = 4.1709281504154205e-06

Training epoch-42 batch-100
Running loss of epoch-42 batch-100 = 2.4885404855012894e-05

Training epoch-42 batch-101
Running loss of epoch-42 batch-101 = 1.3601267710328102e-05

Training epoch-42 batch-102
Running loss of epoch-42 batch-102 = 2.301880158483982e-05

Training epoch-42 batch-103
Running loss of epoch-42 batch-103 = 3.0479859560728073e-05

Training epoch-42 batch-104
Running loss of epoch-42 batch-104 = 1.1810800060629845e-05

Training epoch-42 batch-105
Running loss of epoch-42 batch-105 = 3.154901787638664e-05

Training epoch-42 batch-106
Running loss of epoch-42 batch-106 = 3.364402800798416e-06

Training epoch-42 batch-107
Running loss of epoch-42 batch-107 = 1.9152648746967316e-05

Training epoch-42 batch-108
Running loss of epoch-42 batch-108 = 1.668184995651245e-05

Training epoch-42 batch-109
Running loss of epoch-42 batch-109 = 2.1602027118206024e-05

Training epoch-42 batch-110
Running loss of epoch-42 batch-110 = 1.112883910536766e-05

Training epoch-42 batch-111
Running loss of epoch-42 batch-111 = 1.692073419690132e-05

Training epoch-42 batch-112
Running loss of epoch-42 batch-112 = 6.419606506824493e-06

Training epoch-42 batch-113
Running loss of epoch-42 batch-113 = 3.073364496231079e-06

Training epoch-42 batch-114
Running loss of epoch-42 batch-114 = 3.965524956583977e-05

Training epoch-42 batch-115
Running loss of epoch-42 batch-115 = 1.2324424460530281e-05

Training epoch-42 batch-116
Running loss of epoch-42 batch-116 = 3.6951154470443726e-05

Training epoch-42 batch-117
Running loss of epoch-42 batch-117 = 1.649465411901474e-05

Training epoch-42 batch-118
Running loss of epoch-42 batch-118 = 7.927650585770607e-06

Training epoch-42 batch-119
Running loss of epoch-42 batch-119 = 1.598801463842392e-05

Training epoch-42 batch-120
Running loss of epoch-42 batch-120 = 1.873960718512535e-05

Training epoch-42 batch-121
Running loss of epoch-42 batch-121 = 1.3715587556362152e-05

Training epoch-42 batch-122
Running loss of epoch-42 batch-122 = 1.8971040844917297e-05

Training epoch-42 batch-123
Running loss of epoch-42 batch-123 = 1.4638528227806091e-05

Training epoch-42 batch-124
Running loss of epoch-42 batch-124 = 2.1272804588079453e-05

Training epoch-42 batch-125
Running loss of epoch-42 batch-125 = 7.702736184000969e-06

Training epoch-42 batch-126
Running loss of epoch-42 batch-126 = 9.473646059632301e-06

Training epoch-42 batch-127
Running loss of epoch-42 batch-127 = 6.67572021484375e-06

Training epoch-42 batch-128
Running loss of epoch-42 batch-128 = 3.717094659805298e-05

Training epoch-42 batch-129
Running loss of epoch-42 batch-129 = 1.0904623195528984e-05

Training epoch-42 batch-130
Running loss of epoch-42 batch-130 = 1.5182653442025185e-05

Training epoch-42 batch-131
Running loss of epoch-42 batch-131 = 6.20547216385603e-05

Training epoch-42 batch-132
Running loss of epoch-42 batch-132 = 1.4691846445202827e-05

Training epoch-42 batch-133
Running loss of epoch-42 batch-133 = 1.754635013639927e-05

Training epoch-42 batch-134
Running loss of epoch-42 batch-134 = 1.3575190678238869e-05

Training epoch-42 batch-135
Running loss of epoch-42 batch-135 = 8.559320122003555e-06

Training epoch-42 batch-136
Running loss of epoch-42 batch-136 = 2.3086555302143097e-05

Training epoch-42 batch-137
Running loss of epoch-42 batch-137 = 1.7751473933458328e-05

Training epoch-42 batch-138
Running loss of epoch-42 batch-138 = 1.635868102312088e-05

Training epoch-42 batch-139
Running loss of epoch-42 batch-139 = 1.1223368346691132e-05

Training epoch-42 batch-140
Running loss of epoch-42 batch-140 = 6.121397018432617e-05

Training epoch-42 batch-141
Running loss of epoch-42 batch-141 = 2.0527979359030724e-05

Training epoch-42 batch-142
Running loss of epoch-42 batch-142 = 1.7255544662475586e-05

Training epoch-42 batch-143
Running loss of epoch-42 batch-143 = 8.120667189359665e-06

Training epoch-42 batch-144
Running loss of epoch-42 batch-144 = 1.9633909687399864e-05

Training epoch-42 batch-145
Running loss of epoch-42 batch-145 = 1.0997522622346878e-05

Training epoch-42 batch-146
Running loss of epoch-42 batch-146 = 2.0782463252544403e-05

Training epoch-42 batch-147
Running loss of epoch-42 batch-147 = 1.2585893273353577e-05

Training epoch-42 batch-148
Running loss of epoch-42 batch-148 = 1.0488322004675865e-05

Training epoch-42 batch-149
Running loss of epoch-42 batch-149 = 2.5110552087426186e-05

Training epoch-42 batch-150
Running loss of epoch-42 batch-150 = 1.5063444152474403e-05

Training epoch-42 batch-151
Running loss of epoch-42 batch-151 = 0.00010873645078390837

Training epoch-42 batch-152
Running loss of epoch-42 batch-152 = 2.7550850063562393e-06

Training epoch-42 batch-153
Running loss of epoch-42 batch-153 = 1.805671490728855e-05

Training epoch-42 batch-154
Running loss of epoch-42 batch-154 = 1.511257141828537e-05

Training epoch-42 batch-155
Running loss of epoch-42 batch-155 = 2.891290932893753e-05

Training epoch-42 batch-156
Running loss of epoch-42 batch-156 = 4.720874130725861e-06

Training epoch-42 batch-157
Running loss of epoch-42 batch-157 = 6.959587335586548e-05

Finished training epoch-42.



Average train loss at epoch-42 = 1.8372081965208055e-05

Started Evaluation

Average val loss at epoch-42 = 1.5584325663681369

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.55 %
Accuracy for class toString is: 81.91 %
Accuracy for class run is: 63.01 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 64.62 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-43


Training epoch-43 batch-1
Running loss of epoch-43 batch-1 = 6.072688847780228e-06

Training epoch-43 batch-2
Running loss of epoch-43 batch-2 = 1.1730706319212914e-05

Training epoch-43 batch-3
Running loss of epoch-43 batch-3 = 1.473654992878437e-05

Training epoch-43 batch-4
Running loss of epoch-43 batch-4 = 1.5327241271734238e-05

Training epoch-43 batch-5
Running loss of epoch-43 batch-5 = 1.941877417266369e-05

Training epoch-43 batch-6
Running loss of epoch-43 batch-6 = 2.220261376351118e-05

Training epoch-43 batch-7
Running loss of epoch-43 batch-7 = 1.1378433555364609e-05

Training epoch-43 batch-8
Running loss of epoch-43 batch-8 = 1.5979399904608727e-05

Training epoch-43 batch-9
Running loss of epoch-43 batch-9 = 1.7802929505705833e-05

Training epoch-43 batch-10
Running loss of epoch-43 batch-10 = 1.6505597159266472e-05

Training epoch-43 batch-11
Running loss of epoch-43 batch-11 = 9.71485860645771e-06

Training epoch-43 batch-12
Running loss of epoch-43 batch-12 = 1.6720499843358994e-05

Training epoch-43 batch-13
Running loss of epoch-43 batch-13 = 1.63207296282053e-05

Training epoch-43 batch-14
Running loss of epoch-43 batch-14 = 8.592149242758751e-06

Training epoch-43 batch-15
Running loss of epoch-43 batch-15 = 1.956266351044178e-05

Training epoch-43 batch-16
Running loss of epoch-43 batch-16 = 3.3856136724352837e-05

Training epoch-43 batch-17
Running loss of epoch-43 batch-17 = 9.508570656180382e-06

Training epoch-43 batch-18
Running loss of epoch-43 batch-18 = 1.1324649676680565e-05

Training epoch-43 batch-19
Running loss of epoch-43 batch-19 = 1.272326335310936e-05

Training epoch-43 batch-20
Running loss of epoch-43 batch-20 = 3.123236820101738e-05

Training epoch-43 batch-21
Running loss of epoch-43 batch-21 = 2.048327587544918e-05

Training epoch-43 batch-22
Running loss of epoch-43 batch-22 = 6.70403242111206e-05

Training epoch-43 batch-23
Running loss of epoch-43 batch-23 = 1.5326309949159622e-05

Training epoch-43 batch-24
Running loss of epoch-43 batch-24 = 7.688300684094429e-06

Training epoch-43 batch-25
Running loss of epoch-43 batch-25 = 6.764894351363182e-06

Training epoch-43 batch-26
Running loss of epoch-43 batch-26 = 8.11973586678505e-06

Training epoch-43 batch-27
Running loss of epoch-43 batch-27 = 1.1386116966605186e-05

Training epoch-43 batch-28
Running loss of epoch-43 batch-28 = 5.386071279644966e-06

Training epoch-43 batch-29
Running loss of epoch-43 batch-29 = 1.020217314362526e-05

Training epoch-43 batch-30
Running loss of epoch-43 batch-30 = 1.393444836139679e-05

Training epoch-43 batch-31
Running loss of epoch-43 batch-31 = 1.8484191969037056e-05

Training epoch-43 batch-32
Running loss of epoch-43 batch-32 = 7.751397788524628e-06

Training epoch-43 batch-33
Running loss of epoch-43 batch-33 = 1.3324664905667305e-05

Training epoch-43 batch-34
Running loss of epoch-43 batch-34 = 1.096213236451149e-05

Training epoch-43 batch-35
Running loss of epoch-43 batch-35 = 2.756691537797451e-05

Training epoch-43 batch-36
Running loss of epoch-43 batch-36 = 2.508610486984253e-05

Training epoch-43 batch-37
Running loss of epoch-43 batch-37 = 2.1747546270489693e-05

Training epoch-43 batch-38
Running loss of epoch-43 batch-38 = 2.1845335140824318e-05

Training epoch-43 batch-39
Running loss of epoch-43 batch-39 = 1.1731870472431183e-05

Training epoch-43 batch-40
Running loss of epoch-43 batch-40 = 1.6401056200265884e-05

Training epoch-43 batch-41
Running loss of epoch-43 batch-41 = 2.1637650206685066e-05

Training epoch-43 batch-42
Running loss of epoch-43 batch-42 = 1.7035286873579025e-05

Training epoch-43 batch-43
Running loss of epoch-43 batch-43 = 3.9125094190239906e-05

Training epoch-43 batch-44
Running loss of epoch-43 batch-44 = 1.4843419194221497e-05

Training epoch-43 batch-45
Running loss of epoch-43 batch-45 = 4.346715286374092e-06

Training epoch-43 batch-46
Running loss of epoch-43 batch-46 = 1.2369593605399132e-05

Training epoch-43 batch-47
Running loss of epoch-43 batch-47 = 1.1961441487073898e-05

Training epoch-43 batch-48
Running loss of epoch-43 batch-48 = 1.4341669157147408e-05

Training epoch-43 batch-49
Running loss of epoch-43 batch-49 = 7.4803829193115234e-06

Training epoch-43 batch-50
Running loss of epoch-43 batch-50 = 1.675216481089592e-05

Training epoch-43 batch-51
Running loss of epoch-43 batch-51 = 5.193287506699562e-06

Training epoch-43 batch-52
Running loss of epoch-43 batch-52 = 8.906470611691475e-06

Training epoch-43 batch-53
Running loss of epoch-43 batch-53 = 2.038874663412571e-05

Training epoch-43 batch-54
Running loss of epoch-43 batch-54 = 1.863786019384861e-05

Training epoch-43 batch-55
Running loss of epoch-43 batch-55 = 7.085734978318214e-06

Training epoch-43 batch-56
Running loss of epoch-43 batch-56 = 1.1416152119636536e-05

Training epoch-43 batch-57
Running loss of epoch-43 batch-57 = 1.2937933206558228e-05

Training epoch-43 batch-58
Running loss of epoch-43 batch-58 = 6.7944638431072235e-06

Training epoch-43 batch-59
Running loss of epoch-43 batch-59 = 3.347126767039299e-05

Training epoch-43 batch-60
Running loss of epoch-43 batch-60 = 9.630806744098663e-06

Training epoch-43 batch-61
Running loss of epoch-43 batch-61 = 1.2863893061876297e-05

Training epoch-43 batch-62
Running loss of epoch-43 batch-62 = 3.0491501092910767e-06

Training epoch-43 batch-63
Running loss of epoch-43 batch-63 = 8.524162694811821e-06

Training epoch-43 batch-64
Running loss of epoch-43 batch-64 = 1.6513047739863396e-05

Training epoch-43 batch-65
Running loss of epoch-43 batch-65 = 6.311573088169098e-06

Training epoch-43 batch-66
Running loss of epoch-43 batch-66 = 9.41217876970768e-06

Training epoch-43 batch-67
Running loss of epoch-43 batch-67 = 1.5740515664219856e-05

Training epoch-43 batch-68
Running loss of epoch-43 batch-68 = 1.6757985576987267e-05

Training epoch-43 batch-69
Running loss of epoch-43 batch-69 = 4.691071808338165e-05

Training epoch-43 batch-70
Running loss of epoch-43 batch-70 = 3.983848728239536e-05

Training epoch-43 batch-71
Running loss of epoch-43 batch-71 = 1.0563060641288757e-05

Training epoch-43 batch-72
Running loss of epoch-43 batch-72 = 6.634742021560669e-06

Training epoch-43 batch-73
Running loss of epoch-43 batch-73 = 2.416013740003109e-05

Training epoch-43 batch-74
Running loss of epoch-43 batch-74 = 4.5347725972533226e-05

Training epoch-43 batch-75
Running loss of epoch-43 batch-75 = 4.660105332732201e-06

Training epoch-43 batch-76
Running loss of epoch-43 batch-76 = 1.143990084528923e-05

Training epoch-43 batch-77
Running loss of epoch-43 batch-77 = 1.1028489097952843e-05

Training epoch-43 batch-78
Running loss of epoch-43 batch-78 = 1.415167935192585e-05

Training epoch-43 batch-79
Running loss of epoch-43 batch-79 = 1.954217441380024e-05

Training epoch-43 batch-80
Running loss of epoch-43 batch-80 = 2.1827174350619316e-05

Training epoch-43 batch-81
Running loss of epoch-43 batch-81 = 1.0021962225437164e-05

Training epoch-43 batch-82
Running loss of epoch-43 batch-82 = 1.8629245460033417e-05

Training epoch-43 batch-83
Running loss of epoch-43 batch-83 = 2.2735213860869408e-05

Training epoch-43 batch-84
Running loss of epoch-43 batch-84 = 1.3020704500377178e-05

Training epoch-43 batch-85
Running loss of epoch-43 batch-85 = 9.303679689764977e-06

Training epoch-43 batch-86
Running loss of epoch-43 batch-86 = 9.731389582157135e-06

Training epoch-43 batch-87
Running loss of epoch-43 batch-87 = 5.3088879212737083e-05

Training epoch-43 batch-88
Running loss of epoch-43 batch-88 = 1.695123501121998e-05

Training epoch-43 batch-89
Running loss of epoch-43 batch-89 = 6.035901606082916e-06

Training epoch-43 batch-90
Running loss of epoch-43 batch-90 = 1.3065757229924202e-05

Training epoch-43 batch-91
Running loss of epoch-43 batch-91 = 6.400467827916145e-05

Training epoch-43 batch-92
Running loss of epoch-43 batch-92 = 4.128366708755493e-05

Training epoch-43 batch-93
Running loss of epoch-43 batch-93 = 1.2933742254972458e-05

Training epoch-43 batch-94
Running loss of epoch-43 batch-94 = 1.0510440915822983e-05

Training epoch-43 batch-95
Running loss of epoch-43 batch-95 = 8.42241570353508e-06

Training epoch-43 batch-96
Running loss of epoch-43 batch-96 = 1.3292301446199417e-05

Training epoch-43 batch-97
Running loss of epoch-43 batch-97 = 1.2062839232385159e-05

Training epoch-43 batch-98
Running loss of epoch-43 batch-98 = 1.49549450725317e-05

Training epoch-43 batch-99
Running loss of epoch-43 batch-99 = 1.4653895050287247e-05

Training epoch-43 batch-100
Running loss of epoch-43 batch-100 = 1.3773562386631966e-05

Training epoch-43 batch-101
Running loss of epoch-43 batch-101 = 7.356749847531319e-06

Training epoch-43 batch-102
Running loss of epoch-43 batch-102 = 9.741168469190598e-06

Training epoch-43 batch-103
Running loss of epoch-43 batch-103 = 1.6686273738741875e-05

Training epoch-43 batch-104
Running loss of epoch-43 batch-104 = 3.898516297340393e-05

Training epoch-43 batch-105
Running loss of epoch-43 batch-105 = 3.652111627161503e-05

Training epoch-43 batch-106
Running loss of epoch-43 batch-106 = 9.974930435419083e-06

Training epoch-43 batch-107
Running loss of epoch-43 batch-107 = 1.5539349988102913e-05

Training epoch-43 batch-108
Running loss of epoch-43 batch-108 = 1.0130461305379868e-05

Training epoch-43 batch-109
Running loss of epoch-43 batch-109 = 7.411930710077286e-06

Training epoch-43 batch-110
Running loss of epoch-43 batch-110 = 2.5762245059013367e-05

Training epoch-43 batch-111
Running loss of epoch-43 batch-111 = 4.255212843418121e-06

Training epoch-43 batch-112
Running loss of epoch-43 batch-112 = 1.3508368283510208e-05

Training epoch-43 batch-113
Running loss of epoch-43 batch-113 = 3.5185134038329124e-05

Training epoch-43 batch-114
Running loss of epoch-43 batch-114 = 2.494547516107559e-06

Training epoch-43 batch-115
Running loss of epoch-43 batch-115 = 1.971074379980564e-05

Training epoch-43 batch-116
Running loss of epoch-43 batch-116 = 1.3692304491996765e-05

Training epoch-43 batch-117
Running loss of epoch-43 batch-117 = 4.414469003677368e-06

Training epoch-43 batch-118
Running loss of epoch-43 batch-118 = 1.5448778867721558e-05

Training epoch-43 batch-119
Running loss of epoch-43 batch-119 = 2.4452339857816696e-05

Training epoch-43 batch-120
Running loss of epoch-43 batch-120 = 1.3753073289990425e-05

Training epoch-43 batch-121
Running loss of epoch-43 batch-121 = 2.3517059162259102e-05

Training epoch-43 batch-122
Running loss of epoch-43 batch-122 = 1.0560033842921257e-05

Training epoch-43 batch-123
Running loss of epoch-43 batch-123 = 6.141513586044312e-05

Training epoch-43 batch-124
Running loss of epoch-43 batch-124 = 9.675510227680206e-06

Training epoch-43 batch-125
Running loss of epoch-43 batch-125 = 1.770886592566967e-05

Training epoch-43 batch-126
Running loss of epoch-43 batch-126 = 1.2057367712259293e-05

Training epoch-43 batch-127
Running loss of epoch-43 batch-127 = 2.118293195962906e-05

Training epoch-43 batch-128
Running loss of epoch-43 batch-128 = 8.019385859370232e-06

Training epoch-43 batch-129
Running loss of epoch-43 batch-129 = 1.832260750234127e-05

Training epoch-43 batch-130
Running loss of epoch-43 batch-130 = 1.3559358194470406e-05

Training epoch-43 batch-131
Running loss of epoch-43 batch-131 = 1.9973376765847206e-05

Training epoch-43 batch-132
Running loss of epoch-43 batch-132 = 6.847316399216652e-06

Training epoch-43 batch-133
Running loss of epoch-43 batch-133 = 4.635890945792198e-05

Training epoch-43 batch-134
Running loss of epoch-43 batch-134 = 9.584007784724236e-06

Training epoch-43 batch-135
Running loss of epoch-43 batch-135 = 2.149236388504505e-05

Training epoch-43 batch-136
Running loss of epoch-43 batch-136 = 2.196640707552433e-05

Training epoch-43 batch-137
Running loss of epoch-43 batch-137 = 8.652452379465103e-06

Training epoch-43 batch-138
Running loss of epoch-43 batch-138 = 1.4651333913207054e-05

Training epoch-43 batch-139
Running loss of epoch-43 batch-139 = 7.450394332408905e-05

Training epoch-43 batch-140
Running loss of epoch-43 batch-140 = 4.936475306749344e-06

Training epoch-43 batch-141
Running loss of epoch-43 batch-141 = 2.26444099098444e-05

Training epoch-43 batch-142
Running loss of epoch-43 batch-142 = 1.0589836165308952e-05

Training epoch-43 batch-143
Running loss of epoch-43 batch-143 = 1.2317905202507973e-05

Training epoch-43 batch-144
Running loss of epoch-43 batch-144 = 1.1085299775004387e-05

Training epoch-43 batch-145
Running loss of epoch-43 batch-145 = 1.507229171693325e-05

Training epoch-43 batch-146
Running loss of epoch-43 batch-146 = 8.319737389683723e-06

Training epoch-43 batch-147
Running loss of epoch-43 batch-147 = 1.2505566701292992e-05

Training epoch-43 batch-148
Running loss of epoch-43 batch-148 = 1.6208505257964134e-05

Training epoch-43 batch-149
Running loss of epoch-43 batch-149 = 1.2411270290613174e-05

Training epoch-43 batch-150
Running loss of epoch-43 batch-150 = 2.355477772653103e-05

Training epoch-43 batch-151
Running loss of epoch-43 batch-151 = 1.5990575775504112e-05

Training epoch-43 batch-152
Running loss of epoch-43 batch-152 = 2.473779022693634e-05

Training epoch-43 batch-153
Running loss of epoch-43 batch-153 = 5.875714123249054e-06

Training epoch-43 batch-154
Running loss of epoch-43 batch-154 = 9.774696081876755e-06

Training epoch-43 batch-155
Running loss of epoch-43 batch-155 = 5.305511876940727e-06

Training epoch-43 batch-156
Running loss of epoch-43 batch-156 = 1.2441305443644524e-05

Training epoch-43 batch-157
Running loss of epoch-43 batch-157 = 1.481175422668457e-05

Finished training epoch-43.



Average train loss at epoch-43 = 1.6976643353700637e-05

Started Evaluation

Average val loss at epoch-43 = 1.5667070628035162

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.34 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-44


Training epoch-44 batch-1
Running loss of epoch-44 batch-1 = 1.1428724974393845e-05

Training epoch-44 batch-2
Running loss of epoch-44 batch-2 = 1.0838033631443977e-05

Training epoch-44 batch-3
Running loss of epoch-44 batch-3 = 1.084059476852417e-05

Training epoch-44 batch-4
Running loss of epoch-44 batch-4 = 2.6279129087924957e-05

Training epoch-44 batch-5
Running loss of epoch-44 batch-5 = 1.404574140906334e-05

Training epoch-44 batch-6
Running loss of epoch-44 batch-6 = 1.204153522849083e-05

Training epoch-44 batch-7
Running loss of epoch-44 batch-7 = 2.1344749256968498e-05

Training epoch-44 batch-8
Running loss of epoch-44 batch-8 = 7.700175046920776e-06

Training epoch-44 batch-9
Running loss of epoch-44 batch-9 = 8.035218343138695e-06

Training epoch-44 batch-10
Running loss of epoch-44 batch-10 = 7.606111466884613e-06

Training epoch-44 batch-11
Running loss of epoch-44 batch-11 = 1.9068364053964615e-05

Training epoch-44 batch-12
Running loss of epoch-44 batch-12 = 8.389120921492577e-06

Training epoch-44 batch-13
Running loss of epoch-44 batch-13 = 1.7810147255659103e-05

Training epoch-44 batch-14
Running loss of epoch-44 batch-14 = 1.2213829904794693e-05

Training epoch-44 batch-15
Running loss of epoch-44 batch-15 = 1.1310912668704987e-05

Training epoch-44 batch-16
Running loss of epoch-44 batch-16 = 7.538357749581337e-06

Training epoch-44 batch-17
Running loss of epoch-44 batch-17 = 1.6979407519102097e-05

Training epoch-44 batch-18
Running loss of epoch-44 batch-18 = 1.1457130312919617e-05

Training epoch-44 batch-19
Running loss of epoch-44 batch-19 = 4.135072231292725e-06

Training epoch-44 batch-20
Running loss of epoch-44 batch-20 = 3.399117849767208e-05

Training epoch-44 batch-21
Running loss of epoch-44 batch-21 = 2.1916581317782402e-05

Training epoch-44 batch-22
Running loss of epoch-44 batch-22 = 2.6181107386946678e-05

Training epoch-44 batch-23
Running loss of epoch-44 batch-23 = 2.4922890588641167e-05

Training epoch-44 batch-24
Running loss of epoch-44 batch-24 = 3.51746566593647e-05

Training epoch-44 batch-25
Running loss of epoch-44 batch-25 = 5.310843698680401e-05

Training epoch-44 batch-26
Running loss of epoch-44 batch-26 = 7.746275514364243e-06

Training epoch-44 batch-27
Running loss of epoch-44 batch-27 = 8.712289854884148e-06

Training epoch-44 batch-28
Running loss of epoch-44 batch-28 = 2.154032699763775e-05

Training epoch-44 batch-29
Running loss of epoch-44 batch-29 = 5.595153197646141e-06

Training epoch-44 batch-30
Running loss of epoch-44 batch-30 = 7.88387842476368e-06

Training epoch-44 batch-31
Running loss of epoch-44 batch-31 = 9.38447192311287e-06

Training epoch-44 batch-32
Running loss of epoch-44 batch-32 = 2.484675496816635e-05

Training epoch-44 batch-33
Running loss of epoch-44 batch-33 = 2.1359883248806e-05

Training epoch-44 batch-34
Running loss of epoch-44 batch-34 = 8.31647776067257e-06

Training epoch-44 batch-35
Running loss of epoch-44 batch-35 = 2.292916178703308e-05

Training epoch-44 batch-36
Running loss of epoch-44 batch-36 = 3.172573633491993e-05

Training epoch-44 batch-37
Running loss of epoch-44 batch-37 = 7.832655683159828e-06

Training epoch-44 batch-38
Running loss of epoch-44 batch-38 = 1.605111174285412e-05

Training epoch-44 batch-39
Running loss of epoch-44 batch-39 = 1.0150717571377754e-05

Training epoch-44 batch-40
Running loss of epoch-44 batch-40 = 9.757699444890022e-06

Training epoch-44 batch-41
Running loss of epoch-44 batch-41 = 1.970049925148487e-05

Training epoch-44 batch-42
Running loss of epoch-44 batch-42 = 9.162118658423424e-06

Training epoch-44 batch-43
Running loss of epoch-44 batch-43 = 1.2874137610197067e-05

Training epoch-44 batch-44
Running loss of epoch-44 batch-44 = 1.6219448298215866e-05

Training epoch-44 batch-45
Running loss of epoch-44 batch-45 = 1.3068318367004395e-05

Training epoch-44 batch-46
Running loss of epoch-44 batch-46 = 2.535339444875717e-05

Training epoch-44 batch-47
Running loss of epoch-44 batch-47 = 1.9700033590197563e-05

Training epoch-44 batch-48
Running loss of epoch-44 batch-48 = 1.7769401893019676e-05

Training epoch-44 batch-49
Running loss of epoch-44 batch-49 = 1.4738412573933601e-05

Training epoch-44 batch-50
Running loss of epoch-44 batch-50 = 6.689690053462982e-06

Training epoch-44 batch-51
Running loss of epoch-44 batch-51 = 1.3952609151601791e-05

Training epoch-44 batch-52
Running loss of epoch-44 batch-52 = 6.325077265501022e-06

Training epoch-44 batch-53
Running loss of epoch-44 batch-53 = 1.0914867743849754e-05

Training epoch-44 batch-54
Running loss of epoch-44 batch-54 = 9.774928912520409e-06

Training epoch-44 batch-55
Running loss of epoch-44 batch-55 = 8.410308510065079e-06

Training epoch-44 batch-56
Running loss of epoch-44 batch-56 = 1.8445774912834167e-05

Training epoch-44 batch-57
Running loss of epoch-44 batch-57 = 1.0515563189983368e-05

Training epoch-44 batch-58
Running loss of epoch-44 batch-58 = 1.8933089450001717e-05

Training epoch-44 batch-59
Running loss of epoch-44 batch-59 = 1.1274591088294983e-05

Training epoch-44 batch-60
Running loss of epoch-44 batch-60 = 1.5869038179516792e-05

Training epoch-44 batch-61
Running loss of epoch-44 batch-61 = 1.8996652215719223e-05

Training epoch-44 batch-62
Running loss of epoch-44 batch-62 = 1.1963071301579475e-05

Training epoch-44 batch-63
Running loss of epoch-44 batch-63 = 1.2803124263882637e-05

Training epoch-44 batch-64
Running loss of epoch-44 batch-64 = 9.0824905782938e-06

Training epoch-44 batch-65
Running loss of epoch-44 batch-65 = 2.2703316062688828e-05

Training epoch-44 batch-66
Running loss of epoch-44 batch-66 = 1.6231555491685867e-05

Training epoch-44 batch-67
Running loss of epoch-44 batch-67 = 1.1605443432927132e-05

Training epoch-44 batch-68
Running loss of epoch-44 batch-68 = 2.2411812096834183e-05

Training epoch-44 batch-69
Running loss of epoch-44 batch-69 = 1.4586374163627625e-05

Training epoch-44 batch-70
Running loss of epoch-44 batch-70 = 1.8831342458724976e-05

Training epoch-44 batch-71
Running loss of epoch-44 batch-71 = 2.1922169253230095e-05

Training epoch-44 batch-72
Running loss of epoch-44 batch-72 = 6.244052201509476e-06

Training epoch-44 batch-73
Running loss of epoch-44 batch-73 = 1.3673678040504456e-05

Training epoch-44 batch-74
Running loss of epoch-44 batch-74 = 4.4948188588023186e-05

Training epoch-44 batch-75
Running loss of epoch-44 batch-75 = 1.3665296137332916e-05

Training epoch-44 batch-76
Running loss of epoch-44 batch-76 = 6.5891072154045105e-06

Training epoch-44 batch-77
Running loss of epoch-44 batch-77 = 1.1243857443332672e-05

Training epoch-44 batch-78
Running loss of epoch-44 batch-78 = 1.454097218811512e-05

Training epoch-44 batch-79
Running loss of epoch-44 batch-79 = 9.910669177770615e-06

Training epoch-44 batch-80
Running loss of epoch-44 batch-80 = 1.8685590475797653e-05

Training epoch-44 batch-81
Running loss of epoch-44 batch-81 = 2.3068394511938095e-05

Training epoch-44 batch-82
Running loss of epoch-44 batch-82 = 1.2252014130353928e-05

Training epoch-44 batch-83
Running loss of epoch-44 batch-83 = 1.155422069132328e-05

Training epoch-44 batch-84
Running loss of epoch-44 batch-84 = 5.2541494369506836e-05

Training epoch-44 batch-85
Running loss of epoch-44 batch-85 = 3.0794180929660797e-06

Training epoch-44 batch-86
Running loss of epoch-44 batch-86 = 4.576356150209904e-05

Training epoch-44 batch-87
Running loss of epoch-44 batch-87 = 1.0212184861302376e-05

Training epoch-44 batch-88
Running loss of epoch-44 batch-88 = 4.200870171189308e-05

Training epoch-44 batch-89
Running loss of epoch-44 batch-89 = 6.671063601970673e-06

Training epoch-44 batch-90
Running loss of epoch-44 batch-90 = 1.1885305866599083e-05

Training epoch-44 batch-91
Running loss of epoch-44 batch-91 = 1.4639459550380707e-05

Training epoch-44 batch-92
Running loss of epoch-44 batch-92 = 8.090399205684662e-06

Training epoch-44 batch-93
Running loss of epoch-44 batch-93 = 1.3615470379590988e-05

Training epoch-44 batch-94
Running loss of epoch-44 batch-94 = 1.0353978723287582e-05

Training epoch-44 batch-95
Running loss of epoch-44 batch-95 = 2.625398337841034e-06

Training epoch-44 batch-96
Running loss of epoch-44 batch-96 = 5.362159572541714e-05

Training epoch-44 batch-97
Running loss of epoch-44 batch-97 = 1.49488914757967e-05

Training epoch-44 batch-98
Running loss of epoch-44 batch-98 = 2.1570362150669098e-05

Training epoch-44 batch-99
Running loss of epoch-44 batch-99 = 1.7328187823295593e-05

Training epoch-44 batch-100
Running loss of epoch-44 batch-100 = 6.345333531498909e-06

Training epoch-44 batch-101
Running loss of epoch-44 batch-101 = 7.74790532886982e-06

Training epoch-44 batch-102
Running loss of epoch-44 batch-102 = 2.202647738158703e-05

Training epoch-44 batch-103
Running loss of epoch-44 batch-103 = 1.656799577176571e-05

Training epoch-44 batch-104
Running loss of epoch-44 batch-104 = 2.1701445803046227e-05

Training epoch-44 batch-105
Running loss of epoch-44 batch-105 = 1.8115155398845673e-05

Training epoch-44 batch-106
Running loss of epoch-44 batch-106 = 4.54997643828392e-06

Training epoch-44 batch-107
Running loss of epoch-44 batch-107 = 7.88271427154541e-06

Training epoch-44 batch-108
Running loss of epoch-44 batch-108 = 5.786307156085968e-06

Training epoch-44 batch-109
Running loss of epoch-44 batch-109 = 3.437977284193039e-06

Training epoch-44 batch-110
Running loss of epoch-44 batch-110 = 1.1302530765533447e-05

Training epoch-44 batch-111
Running loss of epoch-44 batch-111 = 1.1057592928409576e-05

Training epoch-44 batch-112
Running loss of epoch-44 batch-112 = 7.787486538290977e-06

Training epoch-44 batch-113
Running loss of epoch-44 batch-113 = 2.494780346751213e-05

Training epoch-44 batch-114
Running loss of epoch-44 batch-114 = 9.401235729455948e-06

Training epoch-44 batch-115
Running loss of epoch-44 batch-115 = 5.336245521903038e-06

Training epoch-44 batch-116
Running loss of epoch-44 batch-116 = 1.3174256309866905e-05

Training epoch-44 batch-117
Running loss of epoch-44 batch-117 = 8.520670235157013e-06

Training epoch-44 batch-118
Running loss of epoch-44 batch-118 = 1.1194730177521706e-05

Training epoch-44 batch-119
Running loss of epoch-44 batch-119 = 8.186791092157364e-06

Training epoch-44 batch-120
Running loss of epoch-44 batch-120 = 3.582332283258438e-06

Training epoch-44 batch-121
Running loss of epoch-44 batch-121 = 1.972098834812641e-05

Training epoch-44 batch-122
Running loss of epoch-44 batch-122 = 2.8247013688087463e-05

Training epoch-44 batch-123
Running loss of epoch-44 batch-123 = 1.0973308235406876e-05

Training epoch-44 batch-124
Running loss of epoch-44 batch-124 = 2.1596672013401985e-05

Training epoch-44 batch-125
Running loss of epoch-44 batch-125 = 8.901813998818398e-06

Training epoch-44 batch-126
Running loss of epoch-44 batch-126 = 1.7231563106179237e-05

Training epoch-44 batch-127
Running loss of epoch-44 batch-127 = 4.78189904242754e-05

Training epoch-44 batch-128
Running loss of epoch-44 batch-128 = 1.6645994037389755e-05

Training epoch-44 batch-129
Running loss of epoch-44 batch-129 = 1.5630852431058884e-05

Training epoch-44 batch-130
Running loss of epoch-44 batch-130 = 1.226295717060566e-05

Training epoch-44 batch-131
Running loss of epoch-44 batch-131 = 1.321663148701191e-05

Training epoch-44 batch-132
Running loss of epoch-44 batch-132 = 5.935085937380791e-06

Training epoch-44 batch-133
Running loss of epoch-44 batch-133 = 4.695029929280281e-06

Training epoch-44 batch-134
Running loss of epoch-44 batch-134 = 1.2434320524334908e-05

Training epoch-44 batch-135
Running loss of epoch-44 batch-135 = 4.7219800762832165e-05

Training epoch-44 batch-136
Running loss of epoch-44 batch-136 = 1.314864493906498e-05

Training epoch-44 batch-137
Running loss of epoch-44 batch-137 = 7.503200322389603e-06

Training epoch-44 batch-138
Running loss of epoch-44 batch-138 = 1.0160263627767563e-05

Training epoch-44 batch-139
Running loss of epoch-44 batch-139 = 1.1010328307747841e-05

Training epoch-44 batch-140
Running loss of epoch-44 batch-140 = 3.5017728805541992e-06

Training epoch-44 batch-141
Running loss of epoch-44 batch-141 = 5.846377462148666e-06

Training epoch-44 batch-142
Running loss of epoch-44 batch-142 = 2.6416731998324394e-05

Training epoch-44 batch-143
Running loss of epoch-44 batch-143 = 6.6410284489393234e-06

Training epoch-44 batch-144
Running loss of epoch-44 batch-144 = 1.1642230674624443e-05

Training epoch-44 batch-145
Running loss of epoch-44 batch-145 = 6.739620584994555e-05

Training epoch-44 batch-146
Running loss of epoch-44 batch-146 = 1.2353644706308842e-05

Training epoch-44 batch-147
Running loss of epoch-44 batch-147 = 4.073185846209526e-05

Training epoch-44 batch-148
Running loss of epoch-44 batch-148 = 1.3020355254411697e-05

Training epoch-44 batch-149
Running loss of epoch-44 batch-149 = 3.95728275179863e-05

Training epoch-44 batch-150
Running loss of epoch-44 batch-150 = 6.96210190653801e-06

Training epoch-44 batch-151
Running loss of epoch-44 batch-151 = 1.70879065990448e-05

Training epoch-44 batch-152
Running loss of epoch-44 batch-152 = 1.3998011127114296e-05

Training epoch-44 batch-153
Running loss of epoch-44 batch-153 = 1.8321210518479347e-05

Training epoch-44 batch-154
Running loss of epoch-44 batch-154 = 1.3263197615742683e-05

Training epoch-44 batch-155
Running loss of epoch-44 batch-155 = 1.583341509103775e-05

Training epoch-44 batch-156
Running loss of epoch-44 batch-156 = 1.61582138389349e-05

Training epoch-44 batch-157
Running loss of epoch-44 batch-157 = 1.0229647159576416e-05

Finished training epoch-44.



Average train loss at epoch-44 = 1.6055429726839065e-05

Started Evaluation

Average val loss at epoch-44 = 1.5970227287971583

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 87.70 %
Accuracy for class onCreate is: 88.70 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 53.82 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.02 %

Finished Evaluation



Started training epoch-45


Training epoch-45 batch-1
Running loss of epoch-45 batch-1 = 1.559336669743061e-05

Training epoch-45 batch-2
Running loss of epoch-45 batch-2 = 1.628836616873741e-05

Training epoch-45 batch-3
Running loss of epoch-45 batch-3 = 1.209997572004795e-05

Training epoch-45 batch-4
Running loss of epoch-45 batch-4 = 2.3934990167617798e-05

Training epoch-45 batch-5
Running loss of epoch-45 batch-5 = 1.2093456462025642e-05

Training epoch-45 batch-6
Running loss of epoch-45 batch-6 = 5.2226707339286804e-05

Training epoch-45 batch-7
Running loss of epoch-45 batch-7 = 2.0478619262576103e-05

Training epoch-45 batch-8
Running loss of epoch-45 batch-8 = 1.4735152944922447e-05

Training epoch-45 batch-9
Running loss of epoch-45 batch-9 = 1.3223616406321526e-05

Training epoch-45 batch-10
Running loss of epoch-45 batch-10 = 1.0768882930278778e-05

Training epoch-45 batch-11
Running loss of epoch-45 batch-11 = 1.044105738401413e-05

Training epoch-45 batch-12
Running loss of epoch-45 batch-12 = 6.450340151786804e-06

Training epoch-45 batch-13
Running loss of epoch-45 batch-13 = 1.3946788385510445e-05

Training epoch-45 batch-14
Running loss of epoch-45 batch-14 = 3.23017593473196e-05

Training epoch-45 batch-15
Running loss of epoch-45 batch-15 = 2.7991365641355515e-05

Training epoch-45 batch-16
Running loss of epoch-45 batch-16 = 2.7219532057642937e-05

Training epoch-45 batch-17
Running loss of epoch-45 batch-17 = 1.3576354831457138e-05

Training epoch-45 batch-18
Running loss of epoch-45 batch-18 = 6.770482286810875e-06

Training epoch-45 batch-19
Running loss of epoch-45 batch-19 = 1.1699274182319641e-05

Training epoch-45 batch-20
Running loss of epoch-45 batch-20 = 1.3181474059820175e-05

Training epoch-45 batch-21
Running loss of epoch-45 batch-21 = 9.685521945357323e-06

Training epoch-45 batch-22
Running loss of epoch-45 batch-22 = 1.2695789337158203e-05

Training epoch-45 batch-23
Running loss of epoch-45 batch-23 = 1.1944910511374474e-05

Training epoch-45 batch-24
Running loss of epoch-45 batch-24 = 1.1653173714876175e-05

Training epoch-45 batch-25
Running loss of epoch-45 batch-25 = 1.816730946302414e-05

Training epoch-45 batch-26
Running loss of epoch-45 batch-26 = 1.8271617591381073e-05

Training epoch-45 batch-27
Running loss of epoch-45 batch-27 = 3.842147998511791e-05

Training epoch-45 batch-28
Running loss of epoch-45 batch-28 = 1.329067163169384e-05

Training epoch-45 batch-29
Running loss of epoch-45 batch-29 = 3.3570919185876846e-05

Training epoch-45 batch-30
Running loss of epoch-45 batch-30 = 1.3832934200763702e-05

Training epoch-45 batch-31
Running loss of epoch-45 batch-31 = 1.0502059012651443e-05

Training epoch-45 batch-32
Running loss of epoch-45 batch-32 = 1.1706026270985603e-05

Training epoch-45 batch-33
Running loss of epoch-45 batch-33 = 6.424263119697571e-06

Training epoch-45 batch-34
Running loss of epoch-45 batch-34 = 4.94346022605896e-06

Training epoch-45 batch-35
Running loss of epoch-45 batch-35 = 1.0673888027668e-05

Training epoch-45 batch-36
Running loss of epoch-45 batch-36 = 7.726019248366356e-06

Training epoch-45 batch-37
Running loss of epoch-45 batch-37 = 2.3368746042251587e-05

Training epoch-45 batch-38
Running loss of epoch-45 batch-38 = 1.9020168110728264e-05

Training epoch-45 batch-39
Running loss of epoch-45 batch-39 = 1.4173565432429314e-05

Training epoch-45 batch-40
Running loss of epoch-45 batch-40 = 9.731855243444443e-06

Training epoch-45 batch-41
Running loss of epoch-45 batch-41 = 1.5660887584090233e-05

Training epoch-45 batch-42
Running loss of epoch-45 batch-42 = 8.796807378530502e-06

Training epoch-45 batch-43
Running loss of epoch-45 batch-43 = 3.982335329055786e-05

Training epoch-45 batch-44
Running loss of epoch-45 batch-44 = 2.790032885968685e-05

Training epoch-45 batch-45
Running loss of epoch-45 batch-45 = 9.43383201956749e-06

Training epoch-45 batch-46
Running loss of epoch-45 batch-46 = 1.3838289305567741e-05

Training epoch-45 batch-47
Running loss of epoch-45 batch-47 = 7.602619007229805e-06

Training epoch-45 batch-48
Running loss of epoch-45 batch-48 = 1.7466256394982338e-05

Training epoch-45 batch-49
Running loss of epoch-45 batch-49 = 9.495997801423073e-06

Training epoch-45 batch-50
Running loss of epoch-45 batch-50 = 4.695262759923935e-06

Training epoch-45 batch-51
Running loss of epoch-45 batch-51 = 2.175336703658104e-06

Training epoch-45 batch-52
Running loss of epoch-45 batch-52 = 9.83034260571003e-06

Training epoch-45 batch-53
Running loss of epoch-45 batch-53 = 1.9237631931900978e-05

Training epoch-45 batch-54
Running loss of epoch-45 batch-54 = 5.610985681414604e-06

Training epoch-45 batch-55
Running loss of epoch-45 batch-55 = 9.241001680493355e-05

Training epoch-45 batch-56
Running loss of epoch-45 batch-56 = 2.18353234231472e-05

Training epoch-45 batch-57
Running loss of epoch-45 batch-57 = 3.5185134038329124e-05

Training epoch-45 batch-58
Running loss of epoch-45 batch-58 = 9.394483640789986e-06

Training epoch-45 batch-59
Running loss of epoch-45 batch-59 = 9.36165452003479e-06

Training epoch-45 batch-60
Running loss of epoch-45 batch-60 = 1.5402445569634438e-05

Training epoch-45 batch-61
Running loss of epoch-45 batch-61 = 1.0485760867595673e-05

Training epoch-45 batch-62
Running loss of epoch-45 batch-62 = 1.0305782780051231e-05

Training epoch-45 batch-63
Running loss of epoch-45 batch-63 = 3.095320425927639e-05

Training epoch-45 batch-64
Running loss of epoch-45 batch-64 = 7.332535460591316e-06

Training epoch-45 batch-65
Running loss of epoch-45 batch-65 = 3.33904754370451e-05

Training epoch-45 batch-66
Running loss of epoch-45 batch-66 = 6.220536306500435e-06

Training epoch-45 batch-67
Running loss of epoch-45 batch-67 = 1.7540063709020615e-05

Training epoch-45 batch-68
Running loss of epoch-45 batch-68 = 1.69975683093071e-05

Training epoch-45 batch-69
Running loss of epoch-45 batch-69 = 1.5273690223693848e-05

Training epoch-45 batch-70
Running loss of epoch-45 batch-70 = 6.250571459531784e-06

Training epoch-45 batch-71
Running loss of epoch-45 batch-71 = 1.081288792192936e-05

Training epoch-45 batch-72
Running loss of epoch-45 batch-72 = 8.1679318100214e-06

Training epoch-45 batch-73
Running loss of epoch-45 batch-73 = 2.817460335791111e-05

Training epoch-45 batch-74
Running loss of epoch-45 batch-74 = 3.253575414419174e-05

Training epoch-45 batch-75
Running loss of epoch-45 batch-75 = 4.580942913889885e-06

Training epoch-45 batch-76
Running loss of epoch-45 batch-76 = 4.37605194747448e-06

Training epoch-45 batch-77
Running loss of epoch-45 batch-77 = 2.1314946934580803e-05

Training epoch-45 batch-78
Running loss of epoch-45 batch-78 = 1.4156801626086235e-05

Training epoch-45 batch-79
Running loss of epoch-45 batch-79 = 2.0807841792702675e-05

Training epoch-45 batch-80
Running loss of epoch-45 batch-80 = 3.936770372092724e-05

Training epoch-45 batch-81
Running loss of epoch-45 batch-81 = 1.3283686712384224e-05

Training epoch-45 batch-82
Running loss of epoch-45 batch-82 = 1.1861324310302734e-05

Training epoch-45 batch-83
Running loss of epoch-45 batch-83 = 8.909730240702629e-06

Training epoch-45 batch-84
Running loss of epoch-45 batch-84 = 6.452202796936035e-06

Training epoch-45 batch-85
Running loss of epoch-45 batch-85 = 1.578708179295063e-05

Training epoch-45 batch-86
Running loss of epoch-45 batch-86 = 5.7406723499298096e-06

Training epoch-45 batch-87
Running loss of epoch-45 batch-87 = 1.6474397853016853e-05

Training epoch-45 batch-88
Running loss of epoch-45 batch-88 = 1.3705343008041382e-05

Training epoch-45 batch-89
Running loss of epoch-45 batch-89 = 3.142119385302067e-05

Training epoch-45 batch-90
Running loss of epoch-45 batch-90 = 1.396099105477333e-05

Training epoch-45 batch-91
Running loss of epoch-45 batch-91 = 8.452218025922775e-06

Training epoch-45 batch-92
Running loss of epoch-45 batch-92 = 5.605630576610565e-06

Training epoch-45 batch-93
Running loss of epoch-45 batch-93 = 3.3532269299030304e-06

Training epoch-45 batch-94
Running loss of epoch-45 batch-94 = 7.956288754940033e-06

Training epoch-45 batch-95
Running loss of epoch-45 batch-95 = 2.0634615793824196e-05

Training epoch-45 batch-96
Running loss of epoch-45 batch-96 = 5.726702511310577e-06

Training epoch-45 batch-97
Running loss of epoch-45 batch-97 = 1.2734206393361092e-05

Training epoch-45 batch-98
Running loss of epoch-45 batch-98 = 1.1859927326440811e-05

Training epoch-45 batch-99
Running loss of epoch-45 batch-99 = 9.627314284443855e-06

Training epoch-45 batch-100
Running loss of epoch-45 batch-100 = 1.5008728951215744e-05

Training epoch-45 batch-101
Running loss of epoch-45 batch-101 = 1.745857298374176e-05

Training epoch-45 batch-102
Running loss of epoch-45 batch-102 = 1.6893260180950165e-05

Training epoch-45 batch-103
Running loss of epoch-45 batch-103 = 2.0263250917196274e-05

Training epoch-45 batch-104
Running loss of epoch-45 batch-104 = 1.2140953913331032e-05

Training epoch-45 batch-105
Running loss of epoch-45 batch-105 = 1.0794494301080704e-05

Training epoch-45 batch-106
Running loss of epoch-45 batch-106 = 1.6515608876943588e-05

Training epoch-45 batch-107
Running loss of epoch-45 batch-107 = 3.6158598959445953e-06

Training epoch-45 batch-108
Running loss of epoch-45 batch-108 = 6.229616701602936e-06

Training epoch-45 batch-109
Running loss of epoch-45 batch-109 = 8.437316864728928e-06

Training epoch-45 batch-110
Running loss of epoch-45 batch-110 = 2.345326356589794e-05

Training epoch-45 batch-111
Running loss of epoch-45 batch-111 = 9.788665920495987e-06

Training epoch-45 batch-112
Running loss of epoch-45 batch-112 = 1.0137911885976791e-05

Training epoch-45 batch-113
Running loss of epoch-45 batch-113 = 1.531769521534443e-05

Training epoch-45 batch-114
Running loss of epoch-45 batch-114 = 1.2592645362019539e-05

Training epoch-45 batch-115
Running loss of epoch-45 batch-115 = 1.2611737474799156e-05

Training epoch-45 batch-116
Running loss of epoch-45 batch-116 = 1.171906478703022e-05

Training epoch-45 batch-117
Running loss of epoch-45 batch-117 = 2.7295434847474098e-05

Training epoch-45 batch-118
Running loss of epoch-45 batch-118 = 3.620050847530365e-06

Training epoch-45 batch-119
Running loss of epoch-45 batch-119 = 6.652437150478363e-06

Training epoch-45 batch-120
Running loss of epoch-45 batch-120 = 8.112983778119087e-06

Training epoch-45 batch-121
Running loss of epoch-45 batch-121 = 1.0756077244877815e-05

Training epoch-45 batch-122
Running loss of epoch-45 batch-122 = 1.378101296722889e-05

Training epoch-45 batch-123
Running loss of epoch-45 batch-123 = 8.311355486512184e-06

Training epoch-45 batch-124
Running loss of epoch-45 batch-124 = 2.180761657655239e-05

Training epoch-45 batch-125
Running loss of epoch-45 batch-125 = 8.153030648827553e-06

Training epoch-45 batch-126
Running loss of epoch-45 batch-126 = 6.159301847219467e-06

Training epoch-45 batch-127
Running loss of epoch-45 batch-127 = 8.63964669406414e-06

Training epoch-45 batch-128
Running loss of epoch-45 batch-128 = 2.652930561453104e-05

Training epoch-45 batch-129
Running loss of epoch-45 batch-129 = 1.425878144800663e-05

Training epoch-45 batch-130
Running loss of epoch-45 batch-130 = 8.041737601161003e-06

Training epoch-45 batch-131
Running loss of epoch-45 batch-131 = 8.622417226433754e-06

Training epoch-45 batch-132
Running loss of epoch-45 batch-132 = 7.309950888156891e-06

Training epoch-45 batch-133
Running loss of epoch-45 batch-133 = 7.500872015953064e-06

Training epoch-45 batch-134
Running loss of epoch-45 batch-134 = 5.331588909029961e-06

Training epoch-45 batch-135
Running loss of epoch-45 batch-135 = 4.544388502836227e-06

Training epoch-45 batch-136
Running loss of epoch-45 batch-136 = 7.3427800089120865e-06

Training epoch-45 batch-137
Running loss of epoch-45 batch-137 = 1.766858622431755e-05

Training epoch-45 batch-138
Running loss of epoch-45 batch-138 = 2.701161429286003e-05

Training epoch-45 batch-139
Running loss of epoch-45 batch-139 = 9.25874337553978e-06

Training epoch-45 batch-140
Running loss of epoch-45 batch-140 = 9.190523996949196e-06

Training epoch-45 batch-141
Running loss of epoch-45 batch-141 = 1.0835938155651093e-05

Training epoch-45 batch-142
Running loss of epoch-45 batch-142 = 1.2753764167428017e-05

Training epoch-45 batch-143
Running loss of epoch-45 batch-143 = 5.208887159824371e-06

Training epoch-45 batch-144
Running loss of epoch-45 batch-144 = 1.629721373319626e-05

Training epoch-45 batch-145
Running loss of epoch-45 batch-145 = 3.030826337635517e-05

Training epoch-45 batch-146
Running loss of epoch-45 batch-146 = 5.666632205247879e-06

Training epoch-45 batch-147
Running loss of epoch-45 batch-147 = 7.294118404388428e-06

Training epoch-45 batch-148
Running loss of epoch-45 batch-148 = 1.7305603250861168e-05

Training epoch-45 batch-149
Running loss of epoch-45 batch-149 = 1.1689495295286179e-05

Training epoch-45 batch-150
Running loss of epoch-45 batch-150 = 7.836613804101944e-06

Training epoch-45 batch-151
Running loss of epoch-45 batch-151 = 1.6295351088047028e-05

Training epoch-45 batch-152
Running loss of epoch-45 batch-152 = 3.176205791532993e-05

Training epoch-45 batch-153
Running loss of epoch-45 batch-153 = 9.356532245874405e-06

Training epoch-45 batch-154
Running loss of epoch-45 batch-154 = 1.8280232325196266e-05

Training epoch-45 batch-155
Running loss of epoch-45 batch-155 = 6.401911377906799e-06

Training epoch-45 batch-156
Running loss of epoch-45 batch-156 = 6.30388967692852e-06

Training epoch-45 batch-157
Running loss of epoch-45 batch-157 = 9.626150131225586e-06

Finished training epoch-45.



Average train loss at epoch-45 = 1.4767726510763168e-05

Started Evaluation

Average val loss at epoch-45 = 1.5767188568735175

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.34 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.91 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 64.10 %

Overall Accuracy = 81.43 %

Finished Evaluation



Started training epoch-46


Training epoch-46 batch-1
Running loss of epoch-46 batch-1 = 8.565140888094902e-06

Training epoch-46 batch-2
Running loss of epoch-46 batch-2 = 1.0548392310738564e-05

Training epoch-46 batch-3
Running loss of epoch-46 batch-3 = 1.0814983397722244e-05

Training epoch-46 batch-4
Running loss of epoch-46 batch-4 = 2.357643097639084e-06

Training epoch-46 batch-5
Running loss of epoch-46 batch-5 = 4.3495092540979385e-05

Training epoch-46 batch-6
Running loss of epoch-46 batch-6 = 3.975583240389824e-06

Training epoch-46 batch-7
Running loss of epoch-46 batch-7 = 4.127225838601589e-05

Training epoch-46 batch-8
Running loss of epoch-46 batch-8 = 1.42750795930624e-05

Training epoch-46 batch-9
Running loss of epoch-46 batch-9 = 8.928822353482246e-06

Training epoch-46 batch-10
Running loss of epoch-46 batch-10 = 1.8400372937321663e-05

Training epoch-46 batch-11
Running loss of epoch-46 batch-11 = 5.77140599489212e-06

Training epoch-46 batch-12
Running loss of epoch-46 batch-12 = 3.9137667044997215e-05

Training epoch-46 batch-13
Running loss of epoch-46 batch-13 = 1.6690930351614952e-05

Training epoch-46 batch-14
Running loss of epoch-46 batch-14 = 1.8761958926916122e-05

Training epoch-46 batch-15
Running loss of epoch-46 batch-15 = 1.605786383152008e-05

Training epoch-46 batch-16
Running loss of epoch-46 batch-16 = 8.19074921309948e-06

Training epoch-46 batch-17
Running loss of epoch-46 batch-17 = 1.1145370081067085e-05

Training epoch-46 batch-18
Running loss of epoch-46 batch-18 = 1.5130965039134026e-05

Training epoch-46 batch-19
Running loss of epoch-46 batch-19 = 1.0136747732758522e-05

Training epoch-46 batch-20
Running loss of epoch-46 batch-20 = 1.6583362594246864e-05

Training epoch-46 batch-21
Running loss of epoch-46 batch-21 = 7.431255653500557e-06

Training epoch-46 batch-22
Running loss of epoch-46 batch-22 = 1.12906564027071e-05

Training epoch-46 batch-23
Running loss of epoch-46 batch-23 = 9.25152562558651e-06

Training epoch-46 batch-24
Running loss of epoch-46 batch-24 = 2.8287293389439583e-05

Training epoch-46 batch-25
Running loss of epoch-46 batch-25 = 1.0716961696743965e-05

Training epoch-46 batch-26
Running loss of epoch-46 batch-26 = 3.745965659618378e-05

Training epoch-46 batch-27
Running loss of epoch-46 batch-27 = 7.378635928034782e-06

Training epoch-46 batch-28
Running loss of epoch-46 batch-28 = 1.3971701264381409e-05

Training epoch-46 batch-29
Running loss of epoch-46 batch-29 = 8.071539923548698e-06

Training epoch-46 batch-30
Running loss of epoch-46 batch-30 = 9.532086551189423e-06

Training epoch-46 batch-31
Running loss of epoch-46 batch-31 = 1.2517441064119339e-05

Training epoch-46 batch-32
Running loss of epoch-46 batch-32 = 1.0088784620165825e-05

Training epoch-46 batch-33
Running loss of epoch-46 batch-33 = 1.530745066702366e-05

Training epoch-46 batch-34
Running loss of epoch-46 batch-34 = 1.0924180969595909e-05

Training epoch-46 batch-35
Running loss of epoch-46 batch-35 = 7.695518434047699e-06

Training epoch-46 batch-36
Running loss of epoch-46 batch-36 = 3.036484122276306e-05

Training epoch-46 batch-37
Running loss of epoch-46 batch-37 = 1.6802223399281502e-05

Training epoch-46 batch-38
Running loss of epoch-46 batch-38 = 1.2289034202694893e-05

Training epoch-46 batch-39
Running loss of epoch-46 batch-39 = 9.489711374044418e-06

Training epoch-46 batch-40
Running loss of epoch-46 batch-40 = 3.352854400873184e-05

Training epoch-46 batch-41
Running loss of epoch-46 batch-41 = 1.1231983080506325e-05

Training epoch-46 batch-42
Running loss of epoch-46 batch-42 = 1.8439488485455513e-05

Training epoch-46 batch-43
Running loss of epoch-46 batch-43 = 6.099231541156769e-06

Training epoch-46 batch-44
Running loss of epoch-46 batch-44 = 3.360910341143608e-06

Training epoch-46 batch-45
Running loss of epoch-46 batch-45 = 1.723901368677616e-05

Training epoch-46 batch-46
Running loss of epoch-46 batch-46 = 8.721835911273956e-06

Training epoch-46 batch-47
Running loss of epoch-46 batch-47 = 1.0866904631257057e-05

Training epoch-46 batch-48
Running loss of epoch-46 batch-48 = 1.246691681444645e-05

Training epoch-46 batch-49
Running loss of epoch-46 batch-49 = 2.0457664504647255e-05

Training epoch-46 batch-50
Running loss of epoch-46 batch-50 = 6.954651325941086e-06

Training epoch-46 batch-51
Running loss of epoch-46 batch-51 = 6.8747904151678085e-06

Training epoch-46 batch-52
Running loss of epoch-46 batch-52 = 6.8142544478178024e-06

Training epoch-46 batch-53
Running loss of epoch-46 batch-53 = 2.4727778509259224e-05

Training epoch-46 batch-54
Running loss of epoch-46 batch-54 = 1.0530231520533562e-05

Training epoch-46 batch-55
Running loss of epoch-46 batch-55 = 1.6614096239209175e-05

Training epoch-46 batch-56
Running loss of epoch-46 batch-56 = 2.747727558016777e-05

Training epoch-46 batch-57
Running loss of epoch-46 batch-57 = 1.1665746569633484e-05

Training epoch-46 batch-58
Running loss of epoch-46 batch-58 = 1.0470394045114517e-05

Training epoch-46 batch-59
Running loss of epoch-46 batch-59 = 1.1254101991653442e-05

Training epoch-46 batch-60
Running loss of epoch-46 batch-60 = 5.802838131785393e-06

Training epoch-46 batch-61
Running loss of epoch-46 batch-61 = 1.0497402399778366e-05

Training epoch-46 batch-62
Running loss of epoch-46 batch-62 = 1.698569394648075e-05

Training epoch-46 batch-63
Running loss of epoch-46 batch-63 = 1.2681586667895317e-05

Training epoch-46 batch-64
Running loss of epoch-46 batch-64 = 1.153675839304924e-05

Training epoch-46 batch-65
Running loss of epoch-46 batch-65 = 1.3184384442865849e-05

Training epoch-46 batch-66
Running loss of epoch-46 batch-66 = 6.938818842172623e-06

Training epoch-46 batch-67
Running loss of epoch-46 batch-67 = 8.05477611720562e-06

Training epoch-46 batch-68
Running loss of epoch-46 batch-68 = 1.2279953807592392e-05

Training epoch-46 batch-69
Running loss of epoch-46 batch-69 = 1.719733700156212e-05

Training epoch-46 batch-70
Running loss of epoch-46 batch-70 = 5.013076588511467e-06

Training epoch-46 batch-71
Running loss of epoch-46 batch-71 = 8.975854143500328e-06

Training epoch-46 batch-72
Running loss of epoch-46 batch-72 = 6.844755262136459e-06

Training epoch-46 batch-73
Running loss of epoch-46 batch-73 = 7.730908691883087e-06

Training epoch-46 batch-74
Running loss of epoch-46 batch-74 = 8.562812581658363e-06

Training epoch-46 batch-75
Running loss of epoch-46 batch-75 = 1.5922822058200836e-05

Training epoch-46 batch-76
Running loss of epoch-46 batch-76 = 1.1178432032465935e-05

Training epoch-46 batch-77
Running loss of epoch-46 batch-77 = 1.3603363186120987e-05

Training epoch-46 batch-78
Running loss of epoch-46 batch-78 = 1.6538891941308975e-05

Training epoch-46 batch-79
Running loss of epoch-46 batch-79 = 4.05777245759964e-06

Training epoch-46 batch-80
Running loss of epoch-46 batch-80 = 1.4004763215780258e-05

Training epoch-46 batch-81
Running loss of epoch-46 batch-81 = 2.7563190087676048e-05

Training epoch-46 batch-82
Running loss of epoch-46 batch-82 = 1.7178012058138847e-05

Training epoch-46 batch-83
Running loss of epoch-46 batch-83 = 1.604529097676277e-05

Training epoch-46 batch-84
Running loss of epoch-46 batch-84 = 2.3310072720050812e-05

Training epoch-46 batch-85
Running loss of epoch-46 batch-85 = 1.553143374621868e-05

Training epoch-46 batch-86
Running loss of epoch-46 batch-86 = 1.3658776879310608e-05

Training epoch-46 batch-87
Running loss of epoch-46 batch-87 = 7.637077942490578e-06

Training epoch-46 batch-88
Running loss of epoch-46 batch-88 = 8.243368938565254e-06

Training epoch-46 batch-89
Running loss of epoch-46 batch-89 = 1.187971793115139e-05

Training epoch-46 batch-90
Running loss of epoch-46 batch-90 = 1.3208482414484024e-05

Training epoch-46 batch-91
Running loss of epoch-46 batch-91 = 2.0511914044618607e-05

Training epoch-46 batch-92
Running loss of epoch-46 batch-92 = 3.350432962179184e-06

Training epoch-46 batch-93
Running loss of epoch-46 batch-93 = 2.8855865821242332e-05

Training epoch-46 batch-94
Running loss of epoch-46 batch-94 = 1.164502464234829e-05

Training epoch-46 batch-95
Running loss of epoch-46 batch-95 = 2.362160012125969e-05

Training epoch-46 batch-96
Running loss of epoch-46 batch-96 = 1.1109281331300735e-05

Training epoch-46 batch-97
Running loss of epoch-46 batch-97 = 8.36770050227642e-06

Training epoch-46 batch-98
Running loss of epoch-46 batch-98 = 1.1996133252978325e-05

Training epoch-46 batch-99
Running loss of epoch-46 batch-99 = 2.330332063138485e-05

Training epoch-46 batch-100
Running loss of epoch-46 batch-100 = 1.3865530490875244e-05

Training epoch-46 batch-101
Running loss of epoch-46 batch-101 = 7.712049409747124e-06

Training epoch-46 batch-102
Running loss of epoch-46 batch-102 = 8.168630301952362e-06

Training epoch-46 batch-103
Running loss of epoch-46 batch-103 = 5.593523383140564e-06

Training epoch-46 batch-104
Running loss of epoch-46 batch-104 = 5.0533562898635864e-06

Training epoch-46 batch-105
Running loss of epoch-46 batch-105 = 2.5973888114094734e-05

Training epoch-46 batch-106
Running loss of epoch-46 batch-106 = 1.8691644072532654e-05

Training epoch-46 batch-107
Running loss of epoch-46 batch-107 = 8.504372090101242e-06

Training epoch-46 batch-108
Running loss of epoch-46 batch-108 = 7.243361324071884e-06

Training epoch-46 batch-109
Running loss of epoch-46 batch-109 = 2.701510675251484e-05

Training epoch-46 batch-110
Running loss of epoch-46 batch-110 = 4.2531173676252365e-06

Training epoch-46 batch-111
Running loss of epoch-46 batch-111 = 1.588999293744564e-05

Training epoch-46 batch-112
Running loss of epoch-46 batch-112 = 2.6240013539791107e-06

Training epoch-46 batch-113
Running loss of epoch-46 batch-113 = 6.525078788399696e-06

Training epoch-46 batch-114
Running loss of epoch-46 batch-114 = 1.5936559066176414e-05

Training epoch-46 batch-115
Running loss of epoch-46 batch-115 = 1.2584030628204346e-05

Training epoch-46 batch-116
Running loss of epoch-46 batch-116 = 1.1751661077141762e-05

Training epoch-46 batch-117
Running loss of epoch-46 batch-117 = 2.1475134417414665e-05

Training epoch-46 batch-118
Running loss of epoch-46 batch-118 = 4.3548643589019775e-06

Training epoch-46 batch-119
Running loss of epoch-46 batch-119 = 1.2873206287622452e-05

Training epoch-46 batch-120
Running loss of epoch-46 batch-120 = 5.013309419155121e-06

Training epoch-46 batch-121
Running loss of epoch-46 batch-121 = 5.3890980780124664e-06

Training epoch-46 batch-122
Running loss of epoch-46 batch-122 = 1.8151476979255676e-05

Training epoch-46 batch-123
Running loss of epoch-46 batch-123 = 7.903529331088066e-05

Training epoch-46 batch-124
Running loss of epoch-46 batch-124 = 1.2711388990283012e-05

Training epoch-46 batch-125
Running loss of epoch-46 batch-125 = 1.9704224541783333e-05

Training epoch-46 batch-126
Running loss of epoch-46 batch-126 = 3.501167520880699e-05

Training epoch-46 batch-127
Running loss of epoch-46 batch-127 = 7.494818419218063e-06

Training epoch-46 batch-128
Running loss of epoch-46 batch-128 = 1.176388468593359e-05

Training epoch-46 batch-129
Running loss of epoch-46 batch-129 = 7.300404831767082e-06

Training epoch-46 batch-130
Running loss of epoch-46 batch-130 = 5.503417924046516e-06

Training epoch-46 batch-131
Running loss of epoch-46 batch-131 = 8.986331522464752e-06

Training epoch-46 batch-132
Running loss of epoch-46 batch-132 = 1.3942597433924675e-05

Training epoch-46 batch-133
Running loss of epoch-46 batch-133 = 1.3973098248243332e-05

Training epoch-46 batch-134
Running loss of epoch-46 batch-134 = 6.3425395637750626e-06

Training epoch-46 batch-135
Running loss of epoch-46 batch-135 = 1.871841959655285e-05

Training epoch-46 batch-136
Running loss of epoch-46 batch-136 = 2.3897038772702217e-05

Training epoch-46 batch-137
Running loss of epoch-46 batch-137 = 8.571892976760864e-06

Training epoch-46 batch-138
Running loss of epoch-46 batch-138 = 9.924173355102539e-06

Training epoch-46 batch-139
Running loss of epoch-46 batch-139 = 3.080582246184349e-06

Training epoch-46 batch-140
Running loss of epoch-46 batch-140 = 1.2217322364449501e-05

Training epoch-46 batch-141
Running loss of epoch-46 batch-141 = 2.3961998522281647e-05

Training epoch-46 batch-142
Running loss of epoch-46 batch-142 = 1.2023374438285828e-05

Training epoch-46 batch-143
Running loss of epoch-46 batch-143 = 2.7599046006798744e-05

Training epoch-46 batch-144
Running loss of epoch-46 batch-144 = 8.3122868090868e-06

Training epoch-46 batch-145
Running loss of epoch-46 batch-145 = 6.3301995396614075e-06

Training epoch-46 batch-146
Running loss of epoch-46 batch-146 = 2.698507159948349e-06

Training epoch-46 batch-147
Running loss of epoch-46 batch-147 = 1.1045951396226883e-05

Training epoch-46 batch-148
Running loss of epoch-46 batch-148 = 1.4910008758306503e-05

Training epoch-46 batch-149
Running loss of epoch-46 batch-149 = 2.381298691034317e-05

Training epoch-46 batch-150
Running loss of epoch-46 batch-150 = 5.831243470311165e-06

Training epoch-46 batch-151
Running loss of epoch-46 batch-151 = 1.3439450412988663e-05

Training epoch-46 batch-152
Running loss of epoch-46 batch-152 = 4.76716086268425e-05

Training epoch-46 batch-153
Running loss of epoch-46 batch-153 = 3.277091309428215e-06

Training epoch-46 batch-154
Running loss of epoch-46 batch-154 = 1.3105804100632668e-05

Training epoch-46 batch-155
Running loss of epoch-46 batch-155 = 6.255460903048515e-06

Training epoch-46 batch-156
Running loss of epoch-46 batch-156 = 1.4825724065303802e-05

Training epoch-46 batch-157
Running loss of epoch-46 batch-157 = 0.00020240992307662964

Finished training epoch-46.



Average train loss at epoch-46 = 1.4323467016220092e-05

Started Evaluation

Average val loss at epoch-46 = 1.6012100696947449

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.12 %

Finished Evaluation



Started training epoch-47


Training epoch-47 batch-1
Running loss of epoch-47 batch-1 = 9.27504152059555e-06

Training epoch-47 batch-2
Running loss of epoch-47 batch-2 = 1.4060642570257187e-05

Training epoch-47 batch-3
Running loss of epoch-47 batch-3 = 1.413840800523758e-05

Training epoch-47 batch-4
Running loss of epoch-47 batch-4 = 3.044120967388153e-05

Training epoch-47 batch-5
Running loss of epoch-47 batch-5 = 1.126900315284729e-05

Training epoch-47 batch-6
Running loss of epoch-47 batch-6 = 1.7527490854263306e-05

Training epoch-47 batch-7
Running loss of epoch-47 batch-7 = 1.114862971007824e-05

Training epoch-47 batch-8
Running loss of epoch-47 batch-8 = 1.9770115613937378e-05

Training epoch-47 batch-9
Running loss of epoch-47 batch-9 = 1.807278022170067e-05

Training epoch-47 batch-10
Running loss of epoch-47 batch-10 = 8.04034061729908e-06

Training epoch-47 batch-11
Running loss of epoch-47 batch-11 = 3.841239959001541e-06

Training epoch-47 batch-12
Running loss of epoch-47 batch-12 = 9.413110092282295e-06

Training epoch-47 batch-13
Running loss of epoch-47 batch-13 = 1.1082040145993233e-05

Training epoch-47 batch-14
Running loss of epoch-47 batch-14 = 1.01444311439991e-05

Training epoch-47 batch-15
Running loss of epoch-47 batch-15 = 1.115887425839901e-05

Training epoch-47 batch-16
Running loss of epoch-47 batch-16 = 2.5412533432245255e-05

Training epoch-47 batch-17
Running loss of epoch-47 batch-17 = 1.0817544534802437e-05

Training epoch-47 batch-18
Running loss of epoch-47 batch-18 = 2.846820279955864e-05

Training epoch-47 batch-19
Running loss of epoch-47 batch-19 = 5.524139851331711e-06

Training epoch-47 batch-20
Running loss of epoch-47 batch-20 = 3.3299438655376434e-06

Training epoch-47 batch-21
Running loss of epoch-47 batch-21 = 1.0740011930465698e-05

Training epoch-47 batch-22
Running loss of epoch-47 batch-22 = 1.965463161468506e-05

Training epoch-47 batch-23
Running loss of epoch-47 batch-23 = 4.180241376161575e-06

Training epoch-47 batch-24
Running loss of epoch-47 batch-24 = 2.693082205951214e-05

Training epoch-47 batch-25
Running loss of epoch-47 batch-25 = 1.0216142982244492e-05

Training epoch-47 batch-26
Running loss of epoch-47 batch-26 = 1.992727629840374e-05

Training epoch-47 batch-27
Running loss of epoch-47 batch-27 = 5.1972921937704086e-05

Training epoch-47 batch-28
Running loss of epoch-47 batch-28 = 1.3408483937382698e-05

Training epoch-47 batch-29
Running loss of epoch-47 batch-29 = 8.47899354994297e-06

Training epoch-47 batch-30
Running loss of epoch-47 batch-30 = 3.2578129321336746e-05

Training epoch-47 batch-31
Running loss of epoch-47 batch-31 = 1.365644857287407e-05

Training epoch-47 batch-32
Running loss of epoch-47 batch-32 = 2.5372719392180443e-05

Training epoch-47 batch-33
Running loss of epoch-47 batch-33 = 8.949311450123787e-06

Training epoch-47 batch-34
Running loss of epoch-47 batch-34 = 2.8237467631697655e-05

Training epoch-47 batch-35
Running loss of epoch-47 batch-35 = 8.318806067109108e-06

Training epoch-47 batch-36
Running loss of epoch-47 batch-36 = 6.724381819367409e-06

Training epoch-47 batch-37
Running loss of epoch-47 batch-37 = 1.5892786905169487e-05

Training epoch-47 batch-38
Running loss of epoch-47 batch-38 = 5.551846697926521e-06

Training epoch-47 batch-39
Running loss of epoch-47 batch-39 = 9.978190064430237e-06

Training epoch-47 batch-40
Running loss of epoch-47 batch-40 = 4.906323738396168e-05

Training epoch-47 batch-41
Running loss of epoch-47 batch-41 = 1.3153767213225365e-05

Training epoch-47 batch-42
Running loss of epoch-47 batch-42 = 9.72626730799675e-06

Training epoch-47 batch-43
Running loss of epoch-47 batch-43 = 1.7948215827345848e-05

Training epoch-47 batch-44
Running loss of epoch-47 batch-44 = 6.139278411865234e-06

Training epoch-47 batch-45
Running loss of epoch-47 batch-45 = 1.1897878721356392e-05

Training epoch-47 batch-46
Running loss of epoch-47 batch-46 = 1.884251832962036e-05

Training epoch-47 batch-47
Running loss of epoch-47 batch-47 = 7.689697667956352e-06

Training epoch-47 batch-48
Running loss of epoch-47 batch-48 = 6.209127604961395e-06

Training epoch-47 batch-49
Running loss of epoch-47 batch-49 = 1.1553987860679626e-05

Training epoch-47 batch-50
Running loss of epoch-47 batch-50 = 9.952578693628311e-06

Training epoch-47 batch-51
Running loss of epoch-47 batch-51 = 9.088078513741493e-06

Training epoch-47 batch-52
Running loss of epoch-47 batch-52 = 8.116941899061203e-06

Training epoch-47 batch-53
Running loss of epoch-47 batch-53 = 1.6242265701293945e-05

Training epoch-47 batch-54
Running loss of epoch-47 batch-54 = 1.3954704627394676e-05

Training epoch-47 batch-55
Running loss of epoch-47 batch-55 = 1.4665070921182632e-05

Training epoch-47 batch-56
Running loss of epoch-47 batch-56 = 1.880084164440632e-05

Training epoch-47 batch-57
Running loss of epoch-47 batch-57 = 1.6217119991779327e-05

Training epoch-47 batch-58
Running loss of epoch-47 batch-58 = 3.147544339299202e-05

Training epoch-47 batch-59
Running loss of epoch-47 batch-59 = 8.275499567389488e-06

Training epoch-47 batch-60
Running loss of epoch-47 batch-60 = 9.09389927983284e-06

Training epoch-47 batch-61
Running loss of epoch-47 batch-61 = 8.408213034272194e-06

Training epoch-47 batch-62
Running loss of epoch-47 batch-62 = 7.2051770985126495e-06

Training epoch-47 batch-63
Running loss of epoch-47 batch-63 = 3.5913195461034775e-05

Training epoch-47 batch-64
Running loss of epoch-47 batch-64 = 4.541361704468727e-06

Training epoch-47 batch-65
Running loss of epoch-47 batch-65 = 5.8871228247880936e-06

Training epoch-47 batch-66
Running loss of epoch-47 batch-66 = 9.966781362891197e-06

Training epoch-47 batch-67
Running loss of epoch-47 batch-67 = 1.616077497601509e-05

Training epoch-47 batch-68
Running loss of epoch-47 batch-68 = 2.76865903288126e-05

Training epoch-47 batch-69
Running loss of epoch-47 batch-69 = 1.1635711416602135e-05

Training epoch-47 batch-70
Running loss of epoch-47 batch-70 = 1.3869022950530052e-05

Training epoch-47 batch-71
Running loss of epoch-47 batch-71 = 1.7739366739988327e-05

Training epoch-47 batch-72
Running loss of epoch-47 batch-72 = 1.1905794963240623e-05

Training epoch-47 batch-73
Running loss of epoch-47 batch-73 = 1.1000782251358032e-05

Training epoch-47 batch-74
Running loss of epoch-47 batch-74 = 1.6172882169485092e-05

Training epoch-47 batch-75
Running loss of epoch-47 batch-75 = 3.8044527173042297e-06

Training epoch-47 batch-76
Running loss of epoch-47 batch-76 = 8.42963345348835e-06

Training epoch-47 batch-77
Running loss of epoch-47 batch-77 = 8.165137842297554e-06

Training epoch-47 batch-78
Running loss of epoch-47 batch-78 = 1.4488818123936653e-05

Training epoch-47 batch-79
Running loss of epoch-47 batch-79 = 1.1117197573184967e-05

Training epoch-47 batch-80
Running loss of epoch-47 batch-80 = 5.852431058883667e-06

Training epoch-47 batch-81
Running loss of epoch-47 batch-81 = 1.1245254427194595e-05

Training epoch-47 batch-82
Running loss of epoch-47 batch-82 = 1.6114208847284317e-05

Training epoch-47 batch-83
Running loss of epoch-47 batch-83 = 8.349772542715073e-06

Training epoch-47 batch-84
Running loss of epoch-47 batch-84 = 9.430572390556335e-06

Training epoch-47 batch-85
Running loss of epoch-47 batch-85 = 1.8285121768712997e-05

Training epoch-47 batch-86
Running loss of epoch-47 batch-86 = 2.1426938474178314e-05

Training epoch-47 batch-87
Running loss of epoch-47 batch-87 = 6.1034224927425385e-06

Training epoch-47 batch-88
Running loss of epoch-47 batch-88 = 1.899106428027153e-05

Training epoch-47 batch-89
Running loss of epoch-47 batch-89 = 5.369773134589195e-06

Training epoch-47 batch-90
Running loss of epoch-47 batch-90 = 2.5062821805477142e-05

Training epoch-47 batch-91
Running loss of epoch-47 batch-91 = 5.620997399091721e-06

Training epoch-47 batch-92
Running loss of epoch-47 batch-92 = 8.598202839493752e-06

Training epoch-47 batch-93
Running loss of epoch-47 batch-93 = 3.616092726588249e-06

Training epoch-47 batch-94
Running loss of epoch-47 batch-94 = 7.921131327748299e-06

Training epoch-47 batch-95
Running loss of epoch-47 batch-95 = 7.821246981620789e-06

Training epoch-47 batch-96
Running loss of epoch-47 batch-96 = 1.6778241842985153e-05

Training epoch-47 batch-97
Running loss of epoch-47 batch-97 = 1.0187970474362373e-05

Training epoch-47 batch-98
Running loss of epoch-47 batch-98 = 2.7626752853393555e-05

Training epoch-47 batch-99
Running loss of epoch-47 batch-99 = 4.6745408326387405e-06

Training epoch-47 batch-100
Running loss of epoch-47 batch-100 = 4.61074523627758e-06

Training epoch-47 batch-101
Running loss of epoch-47 batch-101 = 1.6680685803294182e-05

Training epoch-47 batch-102
Running loss of epoch-47 batch-102 = 2.0009465515613556e-05

Training epoch-47 batch-103
Running loss of epoch-47 batch-103 = 2.1488871425390244e-05

Training epoch-47 batch-104
Running loss of epoch-47 batch-104 = 9.493203833699226e-06

Training epoch-47 batch-105
Running loss of epoch-47 batch-105 = 1.0554911568760872e-05

Training epoch-47 batch-106
Running loss of epoch-47 batch-106 = 1.0412652045488358e-05

Training epoch-47 batch-107
Running loss of epoch-47 batch-107 = 1.3516517356038094e-05

Training epoch-47 batch-108
Running loss of epoch-47 batch-108 = 1.1044787243008614e-05

Training epoch-47 batch-109
Running loss of epoch-47 batch-109 = 8.574919775128365e-06

Training epoch-47 batch-110
Running loss of epoch-47 batch-110 = 1.0290881618857384e-05

Training epoch-47 batch-111
Running loss of epoch-47 batch-111 = 6.117159500718117e-06

Training epoch-47 batch-112
Running loss of epoch-47 batch-112 = 1.790374517440796e-05

Training epoch-47 batch-113
Running loss of epoch-47 batch-113 = 9.245472028851509e-06

Training epoch-47 batch-114
Running loss of epoch-47 batch-114 = 1.135258935391903e-05

Training epoch-47 batch-115
Running loss of epoch-47 batch-115 = 6.448710337281227e-06

Training epoch-47 batch-116
Running loss of epoch-47 batch-116 = 5.060108378529549e-06

Training epoch-47 batch-117
Running loss of epoch-47 batch-117 = 6.372341886162758e-06

Training epoch-47 batch-118
Running loss of epoch-47 batch-118 = 4.016561433672905e-06

Training epoch-47 batch-119
Running loss of epoch-47 batch-119 = 1.06107909232378e-05

Training epoch-47 batch-120
Running loss of epoch-47 batch-120 = 5.18234446644783e-06

Training epoch-47 batch-121
Running loss of epoch-47 batch-121 = 1.035863533616066e-05

Training epoch-47 batch-122
Running loss of epoch-47 batch-122 = 1.729605719447136e-05

Training epoch-47 batch-123
Running loss of epoch-47 batch-123 = 4.3101608753204346e-06

Training epoch-47 batch-124
Running loss of epoch-47 batch-124 = 7.638009265065193e-06

Training epoch-47 batch-125
Running loss of epoch-47 batch-125 = 3.692903555929661e-05

Training epoch-47 batch-126
Running loss of epoch-47 batch-126 = 8.545350283384323e-06

Training epoch-47 batch-127
Running loss of epoch-47 batch-127 = 7.517170161008835e-06

Training epoch-47 batch-128
Running loss of epoch-47 batch-128 = 8.295523002743721e-06

Training epoch-47 batch-129
Running loss of epoch-47 batch-129 = 5.1908427849411964e-05

Training epoch-47 batch-130
Running loss of epoch-47 batch-130 = 3.5860575735569e-06

Training epoch-47 batch-131
Running loss of epoch-47 batch-131 = 6.9213565438985825e-06

Training epoch-47 batch-132
Running loss of epoch-47 batch-132 = 9.283889085054398e-06

Training epoch-47 batch-133
Running loss of epoch-47 batch-133 = 1.605902798473835e-05

Training epoch-47 batch-134
Running loss of epoch-47 batch-134 = 1.5139114111661911e-05

Training epoch-47 batch-135
Running loss of epoch-47 batch-135 = 6.654299795627594e-06

Training epoch-47 batch-136
Running loss of epoch-47 batch-136 = 8.459435775876045e-06

Training epoch-47 batch-137
Running loss of epoch-47 batch-137 = 2.5819288566708565e-05

Training epoch-47 batch-138
Running loss of epoch-47 batch-138 = 1.1516967788338661e-05

Training epoch-47 batch-139
Running loss of epoch-47 batch-139 = 2.229376696050167e-05

Training epoch-47 batch-140
Running loss of epoch-47 batch-140 = 3.93204391002655e-06

Training epoch-47 batch-141
Running loss of epoch-47 batch-141 = 1.4062970876693726e-05

Training epoch-47 batch-142
Running loss of epoch-47 batch-142 = 1.086527481675148e-05

Training epoch-47 batch-143
Running loss of epoch-47 batch-143 = 9.429408237338066e-06

Training epoch-47 batch-144
Running loss of epoch-47 batch-144 = 1.164502464234829e-05

Training epoch-47 batch-145
Running loss of epoch-47 batch-145 = 1.7255544662475586e-05

Training epoch-47 batch-146
Running loss of epoch-47 batch-146 = 9.975163266062737e-06

Training epoch-47 batch-147
Running loss of epoch-47 batch-147 = 4.5468565076589584e-05

Training epoch-47 batch-148
Running loss of epoch-47 batch-148 = 1.7663463950157166e-05

Training epoch-47 batch-149
Running loss of epoch-47 batch-149 = 6.783986464142799e-06

Training epoch-47 batch-150
Running loss of epoch-47 batch-150 = 1.1265045031905174e-05

Training epoch-47 batch-151
Running loss of epoch-47 batch-151 = 6.125075742602348e-06

Training epoch-47 batch-152
Running loss of epoch-47 batch-152 = 1.759221777319908e-05

Training epoch-47 batch-153
Running loss of epoch-47 batch-153 = 2.589356154203415e-05

Training epoch-47 batch-154
Running loss of epoch-47 batch-154 = 6.678979843854904e-06

Training epoch-47 batch-155
Running loss of epoch-47 batch-155 = 1.7095590010285378e-05

Training epoch-47 batch-156
Running loss of epoch-47 batch-156 = 1.65523961186409e-05

Training epoch-47 batch-157
Running loss of epoch-47 batch-157 = 9.746477007865906e-05

Finished training epoch-47.



Average train loss at epoch-47 = 1.379299759864807e-05

Started Evaluation

Average val loss at epoch-47 = 1.595881075573091

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 89.18 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 65.98 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 64.87 %

Overall Accuracy = 81.43 %

Finished Evaluation



Started training epoch-48


Training epoch-48 batch-1
Running loss of epoch-48 batch-1 = 1.7732614651322365e-05

Training epoch-48 batch-2
Running loss of epoch-48 batch-2 = 5.954178050160408e-06

Training epoch-48 batch-3
Running loss of epoch-48 batch-3 = 1.358729787170887e-05

Training epoch-48 batch-4
Running loss of epoch-48 batch-4 = 3.919936716556549e-06

Training epoch-48 batch-5
Running loss of epoch-48 batch-5 = 1.2032454833388329e-05

Training epoch-48 batch-6
Running loss of epoch-48 batch-6 = 1.0741408914327621e-05

Training epoch-48 batch-7
Running loss of epoch-48 batch-7 = 9.26177017390728e-06

Training epoch-48 batch-8
Running loss of epoch-48 batch-8 = 2.5485176593065262e-05

Training epoch-48 batch-9
Running loss of epoch-48 batch-9 = 7.751863449811935e-06

Training epoch-48 batch-10
Running loss of epoch-48 batch-10 = 1.4042714610695839e-05

Training epoch-48 batch-11
Running loss of epoch-48 batch-11 = 4.5353081077337265e-06

Training epoch-48 batch-12
Running loss of epoch-48 batch-12 = 3.0940864235162735e-06

Training epoch-48 batch-13
Running loss of epoch-48 batch-13 = 2.2460706532001495e-05

Training epoch-48 batch-14
Running loss of epoch-48 batch-14 = 1.8396880477666855e-05

Training epoch-48 batch-15
Running loss of epoch-48 batch-15 = 7.64988362789154e-06

Training epoch-48 batch-16
Running loss of epoch-48 batch-16 = 9.085983037948608e-06

Training epoch-48 batch-17
Running loss of epoch-48 batch-17 = 4.584901034832001e-06

Training epoch-48 batch-18
Running loss of epoch-48 batch-18 = 8.20215791463852e-06

Training epoch-48 batch-19
Running loss of epoch-48 batch-19 = 6.06500543653965e-06

Training epoch-48 batch-20
Running loss of epoch-48 batch-20 = 1.662108115851879e-05

Training epoch-48 batch-21
Running loss of epoch-48 batch-21 = 8.134637027978897e-06

Training epoch-48 batch-22
Running loss of epoch-48 batch-22 = 3.1238654628396034e-05

Training epoch-48 batch-23
Running loss of epoch-48 batch-23 = 3.998167812824249e-06

Training epoch-48 batch-24
Running loss of epoch-48 batch-24 = 1.2932578101754189e-05

Training epoch-48 batch-25
Running loss of epoch-48 batch-25 = 1.8343795090913773e-05

Training epoch-48 batch-26
Running loss of epoch-48 batch-26 = 8.850358426570892e-06

Training epoch-48 batch-27
Running loss of epoch-48 batch-27 = 4.01865690946579e-06

Training epoch-48 batch-28
Running loss of epoch-48 batch-28 = 2.017570659518242e-05

Training epoch-48 batch-29
Running loss of epoch-48 batch-29 = 2.3025786504149437e-05

Training epoch-48 batch-30
Running loss of epoch-48 batch-30 = 1.3562152162194252e-05

Training epoch-48 batch-31
Running loss of epoch-48 batch-31 = 9.626848623156548e-06

Training epoch-48 batch-32
Running loss of epoch-48 batch-32 = 7.685041055083275e-06

Training epoch-48 batch-33
Running loss of epoch-48 batch-33 = 5.6105200201272964e-06

Training epoch-48 batch-34
Running loss of epoch-48 batch-34 = 1.0659685358405113e-05

Training epoch-48 batch-35
Running loss of epoch-48 batch-35 = 4.209112375974655e-06

Training epoch-48 batch-36
Running loss of epoch-48 batch-36 = 8.146744221448898e-06

Training epoch-48 batch-37
Running loss of epoch-48 batch-37 = 2.30537261813879e-05

Training epoch-48 batch-38
Running loss of epoch-48 batch-38 = 4.773028194904327e-06

Training epoch-48 batch-39
Running loss of epoch-48 batch-39 = 1.3935379683971405e-05

Training epoch-48 batch-40
Running loss of epoch-48 batch-40 = 1.790258102118969e-05

Training epoch-48 batch-41
Running loss of epoch-48 batch-41 = 8.153729140758514e-06

Training epoch-48 batch-42
Running loss of epoch-48 batch-42 = 8.562114089727402e-06

Training epoch-48 batch-43
Running loss of epoch-48 batch-43 = 1.647230237722397e-05

Training epoch-48 batch-44
Running loss of epoch-48 batch-44 = 7.957220077514648e-06

Training epoch-48 batch-45
Running loss of epoch-48 batch-45 = 7.79307447373867e-06

Training epoch-48 batch-46
Running loss of epoch-48 batch-46 = 8.821487426757812e-06

Training epoch-48 batch-47
Running loss of epoch-48 batch-47 = 8.921138942241669e-06

Training epoch-48 batch-48
Running loss of epoch-48 batch-48 = 1.2253178283572197e-05

Training epoch-48 batch-49
Running loss of epoch-48 batch-49 = 1.8096528947353363e-05

Training epoch-48 batch-50
Running loss of epoch-48 batch-50 = 3.3064279705286026e-06

Training epoch-48 batch-51
Running loss of epoch-48 batch-51 = 1.4783814549446106e-05

Training epoch-48 batch-52
Running loss of epoch-48 batch-52 = 1.6099074855446815e-05

Training epoch-48 batch-53
Running loss of epoch-48 batch-53 = 9.356997907161713e-06

Training epoch-48 batch-54
Running loss of epoch-48 batch-54 = 1.4187069609761238e-05

Training epoch-48 batch-55
Running loss of epoch-48 batch-55 = 7.92182981967926e-06

Training epoch-48 batch-56
Running loss of epoch-48 batch-56 = 1.2575183063745499e-05

Training epoch-48 batch-57
Running loss of epoch-48 batch-57 = 3.062887117266655e-06

Training epoch-48 batch-58
Running loss of epoch-48 batch-58 = 1.538521610200405e-05

Training epoch-48 batch-59
Running loss of epoch-48 batch-59 = 2.4675391614437103e-05

Training epoch-48 batch-60
Running loss of epoch-48 batch-60 = 1.366250216960907e-05

Training epoch-48 batch-61
Running loss of epoch-48 batch-61 = 5.370238795876503e-06

Training epoch-48 batch-62
Running loss of epoch-48 batch-62 = 5.526235327124596e-06

Training epoch-48 batch-63
Running loss of epoch-48 batch-63 = 7.887836545705795e-06

Training epoch-48 batch-64
Running loss of epoch-48 batch-64 = 9.103678166866302e-06

Training epoch-48 batch-65
Running loss of epoch-48 batch-65 = 3.1562522053718567e-06

Training epoch-48 batch-66
Running loss of epoch-48 batch-66 = 1.4959834516048431e-05

Training epoch-48 batch-67
Running loss of epoch-48 batch-67 = 7.061054930090904e-06

Training epoch-48 batch-68
Running loss of epoch-48 batch-68 = 6.155576556921005e-06

Training epoch-48 batch-69
Running loss of epoch-48 batch-69 = 6.095273420214653e-06

Training epoch-48 batch-70
Running loss of epoch-48 batch-70 = 1.8338439986109734e-05

Training epoch-48 batch-71
Running loss of epoch-48 batch-71 = 1.6523292288184166e-05

Training epoch-48 batch-72
Running loss of epoch-48 batch-72 = 2.05987598747015e-05

Training epoch-48 batch-73
Running loss of epoch-48 batch-73 = 4.8924703150987625e-06

Training epoch-48 batch-74
Running loss of epoch-48 batch-74 = 8.587492629885674e-06

Training epoch-48 batch-75
Running loss of epoch-48 batch-75 = 1.1788913980126381e-05

Training epoch-48 batch-76
Running loss of epoch-48 batch-76 = 4.004454240202904e-06

Training epoch-48 batch-77
Running loss of epoch-48 batch-77 = 4.526181146502495e-05

Training epoch-48 batch-78
Running loss of epoch-48 batch-78 = 8.994713425636292e-06

Training epoch-48 batch-79
Running loss of epoch-48 batch-79 = 1.4218734577298164e-05

Training epoch-48 batch-80
Running loss of epoch-48 batch-80 = 6.8587251007556915e-06

Training epoch-48 batch-81
Running loss of epoch-48 batch-81 = 9.287381544709206e-06

Training epoch-48 batch-82
Running loss of epoch-48 batch-82 = 1.1577969416975975e-05

Training epoch-48 batch-83
Running loss of epoch-48 batch-83 = 1.180870458483696e-05

Training epoch-48 batch-84
Running loss of epoch-48 batch-84 = 1.5455763787031174e-05

Training epoch-48 batch-85
Running loss of epoch-48 batch-85 = 4.7869980335235596e-06

Training epoch-48 batch-86
Running loss of epoch-48 batch-86 = 1.2591015547513962e-05

Training epoch-48 batch-87
Running loss of epoch-48 batch-87 = 9.934883564710617e-06

Training epoch-48 batch-88
Running loss of epoch-48 batch-88 = 1.6274163499474525e-05

Training epoch-48 batch-89
Running loss of epoch-48 batch-89 = 8.594710379838943e-06

Training epoch-48 batch-90
Running loss of epoch-48 batch-90 = 1.902901567518711e-05

Training epoch-48 batch-91
Running loss of epoch-48 batch-91 = 9.438255801796913e-06

Training epoch-48 batch-92
Running loss of epoch-48 batch-92 = 1.1629890650510788e-05

Training epoch-48 batch-93
Running loss of epoch-48 batch-93 = 8.650356903672218e-06

Training epoch-48 batch-94
Running loss of epoch-48 batch-94 = 1.2144679203629494e-05

Training epoch-48 batch-95
Running loss of epoch-48 batch-95 = 2.1161744371056557e-05

Training epoch-48 batch-96
Running loss of epoch-48 batch-96 = 1.4436198398470879e-05

Training epoch-48 batch-97
Running loss of epoch-48 batch-97 = 1.3588927686214447e-05

Training epoch-48 batch-98
Running loss of epoch-48 batch-98 = 5.068257451057434e-06

Training epoch-48 batch-99
Running loss of epoch-48 batch-99 = 1.2329313904047012e-05

Training epoch-48 batch-100
Running loss of epoch-48 batch-100 = 6.831018254160881e-06

Training epoch-48 batch-101
Running loss of epoch-48 batch-101 = 1.043686643242836e-05

Training epoch-48 batch-102
Running loss of epoch-48 batch-102 = 6.593530997633934e-06

Training epoch-48 batch-103
Running loss of epoch-48 batch-103 = 7.979106158018112e-06

Training epoch-48 batch-104
Running loss of epoch-48 batch-104 = 5.0229718908667564e-05

Training epoch-48 batch-105
Running loss of epoch-48 batch-105 = 3.404216840863228e-05

Training epoch-48 batch-106
Running loss of epoch-48 batch-106 = 1.3606157153844833e-05

Training epoch-48 batch-107
Running loss of epoch-48 batch-107 = 7.684342563152313e-06

Training epoch-48 batch-108
Running loss of epoch-48 batch-108 = 2.475827932357788e-05

Training epoch-48 batch-109
Running loss of epoch-48 batch-109 = 1.611560583114624e-05

Training epoch-48 batch-110
Running loss of epoch-48 batch-110 = 7.177935913205147e-06

Training epoch-48 batch-111
Running loss of epoch-48 batch-111 = 1.2738164514303207e-05

Training epoch-48 batch-112
Running loss of epoch-48 batch-112 = 6.7516230046749115e-06

Training epoch-48 batch-113
Running loss of epoch-48 batch-113 = 9.561190381646156e-06

Training epoch-48 batch-114
Running loss of epoch-48 batch-114 = 9.14628617465496e-06

Training epoch-48 batch-115
Running loss of epoch-48 batch-115 = 3.3136457204818726e-06

Training epoch-48 batch-116
Running loss of epoch-48 batch-116 = 6.3963234424591064e-06

Training epoch-48 batch-117
Running loss of epoch-48 batch-117 = 4.4365180656313896e-05

Training epoch-48 batch-118
Running loss of epoch-48 batch-118 = 3.343704156577587e-05

Training epoch-48 batch-119
Running loss of epoch-48 batch-119 = 7.0945825427770615e-06

Training epoch-48 batch-120
Running loss of epoch-48 batch-120 = 1.1946074664592743e-05

Training epoch-48 batch-121
Running loss of epoch-48 batch-121 = 2.1964777261018753e-05

Training epoch-48 batch-122
Running loss of epoch-48 batch-122 = 1.1748168617486954e-05

Training epoch-48 batch-123
Running loss of epoch-48 batch-123 = 7.673865184187889e-06

Training epoch-48 batch-124
Running loss of epoch-48 batch-124 = 8.326489478349686e-06

Training epoch-48 batch-125
Running loss of epoch-48 batch-125 = 5.3264666348695755e-06

Training epoch-48 batch-126
Running loss of epoch-48 batch-126 = 1.5418045222759247e-05

Training epoch-48 batch-127
Running loss of epoch-48 batch-127 = 4.710163921117783e-06

Training epoch-48 batch-128
Running loss of epoch-48 batch-128 = 1.6332371160387993e-05

Training epoch-48 batch-129
Running loss of epoch-48 batch-129 = 3.9821257814764977e-05

Training epoch-48 batch-130
Running loss of epoch-48 batch-130 = 5.036825314164162e-06

Training epoch-48 batch-131
Running loss of epoch-48 batch-131 = 1.5120254829525948e-05

Training epoch-48 batch-132
Running loss of epoch-48 batch-132 = 1.0702526196837425e-05

Training epoch-48 batch-133
Running loss of epoch-48 batch-133 = 2.229074016213417e-05

Training epoch-48 batch-134
Running loss of epoch-48 batch-134 = 2.167164348065853e-05

Training epoch-48 batch-135
Running loss of epoch-48 batch-135 = 5.403067916631699e-06

Training epoch-48 batch-136
Running loss of epoch-48 batch-136 = 1.7068348824977875e-05

Training epoch-48 batch-137
Running loss of epoch-48 batch-137 = 3.462820313870907e-05

Training epoch-48 batch-138
Running loss of epoch-48 batch-138 = 3.044470213353634e-05

Training epoch-48 batch-139
Running loss of epoch-48 batch-139 = 1.0173069313168526e-05

Training epoch-48 batch-140
Running loss of epoch-48 batch-140 = 3.865920007228851e-06

Training epoch-48 batch-141
Running loss of epoch-48 batch-141 = 5.080364644527435e-06

Training epoch-48 batch-142
Running loss of epoch-48 batch-142 = 6.0289166867733e-06

Training epoch-48 batch-143
Running loss of epoch-48 batch-143 = 1.4232238754630089e-05

Training epoch-48 batch-144
Running loss of epoch-48 batch-144 = 9.004492312669754e-06

Training epoch-48 batch-145
Running loss of epoch-48 batch-145 = 2.8158538043498993e-06

Training epoch-48 batch-146
Running loss of epoch-48 batch-146 = 9.806826710700989e-06

Training epoch-48 batch-147
Running loss of epoch-48 batch-147 = 2.0525185391306877e-05

Training epoch-48 batch-148
Running loss of epoch-48 batch-148 = 2.327410038560629e-05

Training epoch-48 batch-149
Running loss of epoch-48 batch-149 = 1.4834338799118996e-05

Training epoch-48 batch-150
Running loss of epoch-48 batch-150 = 1.4225021004676819e-05

Training epoch-48 batch-151
Running loss of epoch-48 batch-151 = 1.2338627129793167e-05

Training epoch-48 batch-152
Running loss of epoch-48 batch-152 = 9.03918407857418e-06

Training epoch-48 batch-153
Running loss of epoch-48 batch-153 = 5.124136805534363e-06

Training epoch-48 batch-154
Running loss of epoch-48 batch-154 = 1.2717209756374359e-05

Training epoch-48 batch-155
Running loss of epoch-48 batch-155 = 1.4718389138579369e-05

Training epoch-48 batch-156
Running loss of epoch-48 batch-156 = 2.0996667444705963e-05

Training epoch-48 batch-157
Running loss of epoch-48 batch-157 = 2.216920256614685e-05

Finished training epoch-48.



Average train loss at epoch-48 = 1.2765077501535416e-05

Started Evaluation

Average val loss at epoch-48 = 1.6204230757667653

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.23 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 62.31 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-49


Training epoch-49 batch-1
Running loss of epoch-49 batch-1 = 1.2038042768836021e-05

Training epoch-49 batch-2
Running loss of epoch-49 batch-2 = 9.582377970218658e-06

Training epoch-49 batch-3
Running loss of epoch-49 batch-3 = 7.250579074025154e-06

Training epoch-49 batch-4
Running loss of epoch-49 batch-4 = 6.158137694001198e-06

Training epoch-49 batch-5
Running loss of epoch-49 batch-5 = 3.560911864042282e-06

Training epoch-49 batch-6
Running loss of epoch-49 batch-6 = 6.4123887568712234e-06

Training epoch-49 batch-7
Running loss of epoch-49 batch-7 = 9.389827027916908e-06

Training epoch-49 batch-8
Running loss of epoch-49 batch-8 = 5.5264681577682495e-06

Training epoch-49 batch-9
Running loss of epoch-49 batch-9 = 6.395857781171799e-06

Training epoch-49 batch-10
Running loss of epoch-49 batch-10 = 1.55686866492033e-05

Training epoch-49 batch-11
Running loss of epoch-49 batch-11 = 1.4490680769085884e-05

Training epoch-49 batch-12
Running loss of epoch-49 batch-12 = 1.2403354048728943e-05

Training epoch-49 batch-13
Running loss of epoch-49 batch-13 = 1.4695455320179462e-05

Training epoch-49 batch-14
Running loss of epoch-49 batch-14 = 5.668029189109802e-06

Training epoch-49 batch-15
Running loss of epoch-49 batch-15 = 1.4631310477852821e-05

Training epoch-49 batch-16
Running loss of epoch-49 batch-16 = 1.0403338819742203e-05

Training epoch-49 batch-17
Running loss of epoch-49 batch-17 = 6.0247257351875305e-06

Training epoch-49 batch-18
Running loss of epoch-49 batch-18 = 5.607493221759796e-06

Training epoch-49 batch-19
Running loss of epoch-49 batch-19 = 3.994908183813095e-05

Training epoch-49 batch-20
Running loss of epoch-49 batch-20 = 4.1215680539608e-06

Training epoch-49 batch-21
Running loss of epoch-49 batch-21 = 1.1152820661664009e-05

Training epoch-49 batch-22
Running loss of epoch-49 batch-22 = 9.73767600953579e-06

Training epoch-49 batch-23
Running loss of epoch-49 batch-23 = 1.0770745575428009e-05

Training epoch-49 batch-24
Running loss of epoch-49 batch-24 = 1.881713978946209e-05

Training epoch-49 batch-25
Running loss of epoch-49 batch-25 = 6.0372985899448395e-06

Training epoch-49 batch-26
Running loss of epoch-49 batch-26 = 4.282686859369278e-06

Training epoch-49 batch-27
Running loss of epoch-49 batch-27 = 5.240319296717644e-06

Training epoch-49 batch-28
Running loss of epoch-49 batch-28 = 8.978648111224174e-06

Training epoch-49 batch-29
Running loss of epoch-49 batch-29 = 1.2105097994208336e-05

Training epoch-49 batch-30
Running loss of epoch-49 batch-30 = 1.1087628081440926e-05

Training epoch-49 batch-31
Running loss of epoch-49 batch-31 = 1.7462065443396568e-05

Training epoch-49 batch-32
Running loss of epoch-49 batch-32 = 5.817040801048279e-06

Training epoch-49 batch-33
Running loss of epoch-49 batch-33 = 9.843148291110992e-06

Training epoch-49 batch-34
Running loss of epoch-49 batch-34 = 4.5655062422156334e-05

Training epoch-49 batch-35
Running loss of epoch-49 batch-35 = 1.6517937183380127e-05

Training epoch-49 batch-36
Running loss of epoch-49 batch-36 = 4.916451871395111e-06

Training epoch-49 batch-37
Running loss of epoch-49 batch-37 = 1.7134007066488266e-05

Training epoch-49 batch-38
Running loss of epoch-49 batch-38 = 5.785143002867699e-06

Training epoch-49 batch-39
Running loss of epoch-49 batch-39 = 1.6882549971342087e-05

Training epoch-49 batch-40
Running loss of epoch-49 batch-40 = 1.1638738214969635e-05

Training epoch-49 batch-41
Running loss of epoch-49 batch-41 = 3.773951902985573e-06

Training epoch-49 batch-42
Running loss of epoch-49 batch-42 = 7.757917046546936e-06

Training epoch-49 batch-43
Running loss of epoch-49 batch-43 = 8.298316970467567e-06

Training epoch-49 batch-44
Running loss of epoch-49 batch-44 = 3.915745764970779e-06

Training epoch-49 batch-45
Running loss of epoch-49 batch-45 = 3.87965701520443e-06

Training epoch-49 batch-46
Running loss of epoch-49 batch-46 = 4.991888999938965e-06

Training epoch-49 batch-47
Running loss of epoch-49 batch-47 = 1.2172386050224304e-05

Training epoch-49 batch-48
Running loss of epoch-49 batch-48 = 3.44316940754652e-05

Training epoch-49 batch-49
Running loss of epoch-49 batch-49 = 1.3696961104869843e-05

Training epoch-49 batch-50
Running loss of epoch-49 batch-50 = 4.14741225540638e-06

Training epoch-49 batch-51
Running loss of epoch-49 batch-51 = 2.0633451640605927e-06

Training epoch-49 batch-52
Running loss of epoch-49 batch-52 = 2.6419293135404587e-06

Training epoch-49 batch-53
Running loss of epoch-49 batch-53 = 1.3481127098202705e-05

Training epoch-49 batch-54
Running loss of epoch-49 batch-54 = 6.473157554864883e-06

Training epoch-49 batch-55
Running loss of epoch-49 batch-55 = 8.958159014582634e-06

Training epoch-49 batch-56
Running loss of epoch-49 batch-56 = 6.6356733441352844e-06

Training epoch-49 batch-57
Running loss of epoch-49 batch-57 = 8.627073839306831e-06

Training epoch-49 batch-58
Running loss of epoch-49 batch-58 = 6.519956514239311e-06

Training epoch-49 batch-59
Running loss of epoch-49 batch-59 = 7.478287443518639e-06

Training epoch-49 batch-60
Running loss of epoch-49 batch-60 = 9.510433301329613e-06

Training epoch-49 batch-61
Running loss of epoch-49 batch-61 = 1.543387770652771e-05

Training epoch-49 batch-62
Running loss of epoch-49 batch-62 = 8.214963600039482e-06

Training epoch-49 batch-63
Running loss of epoch-49 batch-63 = 1.2674368917942047e-05

Training epoch-49 batch-64
Running loss of epoch-49 batch-64 = 2.117827534675598e-05

Training epoch-49 batch-65
Running loss of epoch-49 batch-65 = 1.367391087114811e-05

Training epoch-49 batch-66
Running loss of epoch-49 batch-66 = 1.917569898068905e-05

Training epoch-49 batch-67
Running loss of epoch-49 batch-67 = 5.5104028433561325e-06

Training epoch-49 batch-68
Running loss of epoch-49 batch-68 = 4.352536052465439e-06

Training epoch-49 batch-69
Running loss of epoch-49 batch-69 = 1.3986602425575256e-05

Training epoch-49 batch-70
Running loss of epoch-49 batch-70 = 5.867099389433861e-06

Training epoch-49 batch-71
Running loss of epoch-49 batch-71 = 5.0584785640239716e-06

Training epoch-49 batch-72
Running loss of epoch-49 batch-72 = 9.033596143126488e-06

Training epoch-49 batch-73
Running loss of epoch-49 batch-73 = 2.165534533560276e-05

Training epoch-49 batch-74
Running loss of epoch-49 batch-74 = 7.2231050580739975e-06

Training epoch-49 batch-75
Running loss of epoch-49 batch-75 = 1.256261020898819e-05

Training epoch-49 batch-76
Running loss of epoch-49 batch-76 = 9.848736226558685e-06

Training epoch-49 batch-77
Running loss of epoch-49 batch-77 = 7.5516290962696075e-06

Training epoch-49 batch-78
Running loss of epoch-49 batch-78 = 9.776325896382332e-06

Training epoch-49 batch-79
Running loss of epoch-49 batch-79 = 1.043383963406086e-05

Training epoch-49 batch-80
Running loss of epoch-49 batch-80 = 1.0408461093902588e-05

Training epoch-49 batch-81
Running loss of epoch-49 batch-81 = 4.708301275968552e-06

Training epoch-49 batch-82
Running loss of epoch-49 batch-82 = 6.670830771327019e-06

Training epoch-49 batch-83
Running loss of epoch-49 batch-83 = 1.6382895410060883e-05

Training epoch-49 batch-84
Running loss of epoch-49 batch-84 = 2.0771054551005363e-05

Training epoch-49 batch-85
Running loss of epoch-49 batch-85 = 8.813338354229927e-06

Training epoch-49 batch-86
Running loss of epoch-49 batch-86 = 1.686881296336651e-05

Training epoch-49 batch-87
Running loss of epoch-49 batch-87 = 2.122577279806137e-05

Training epoch-49 batch-88
Running loss of epoch-49 batch-88 = 1.3107433915138245e-05

Training epoch-49 batch-89
Running loss of epoch-49 batch-89 = 2.469797618687153e-05

Training epoch-49 batch-90
Running loss of epoch-49 batch-90 = 1.2077158316969872e-05

Training epoch-49 batch-91
Running loss of epoch-49 batch-91 = 5.932757630944252e-06

Training epoch-49 batch-92
Running loss of epoch-49 batch-92 = 1.3364478945732117e-05

Training epoch-49 batch-93
Running loss of epoch-49 batch-93 = 3.7490855902433395e-05

Training epoch-49 batch-94
Running loss of epoch-49 batch-94 = 1.4540506526827812e-05

Training epoch-49 batch-95
Running loss of epoch-49 batch-95 = 8.629867807030678e-06

Training epoch-49 batch-96
Running loss of epoch-49 batch-96 = 1.2039439752697945e-05

Training epoch-49 batch-97
Running loss of epoch-49 batch-97 = 5.113659426569939e-06

Training epoch-49 batch-98
Running loss of epoch-49 batch-98 = 1.0414747521281242e-05

Training epoch-49 batch-99
Running loss of epoch-49 batch-99 = 1.0815681889653206e-05

Training epoch-49 batch-100
Running loss of epoch-49 batch-100 = 4.2882515117526054e-05

Training epoch-49 batch-101
Running loss of epoch-49 batch-101 = 1.478707417845726e-05

Training epoch-49 batch-102
Running loss of epoch-49 batch-102 = 9.302631951868534e-06

Training epoch-49 batch-103
Running loss of epoch-49 batch-103 = 1.660827547311783e-05

Training epoch-49 batch-104
Running loss of epoch-49 batch-104 = 1.639360561966896e-05

Training epoch-49 batch-105
Running loss of epoch-49 batch-105 = 4.6817585825920105e-06

Training epoch-49 batch-106
Running loss of epoch-49 batch-106 = 5.375593900680542e-06

Training epoch-49 batch-107
Running loss of epoch-49 batch-107 = 4.7816429287195206e-06

Training epoch-49 batch-108
Running loss of epoch-49 batch-108 = 5.014706403017044e-06

Training epoch-49 batch-109
Running loss of epoch-49 batch-109 = 5.71901910007e-06

Training epoch-49 batch-110
Running loss of epoch-49 batch-110 = 1.240614801645279e-05

Training epoch-49 batch-111
Running loss of epoch-49 batch-111 = 3.710226155817509e-05

Training epoch-49 batch-112
Running loss of epoch-49 batch-112 = 7.191207259893417e-06

Training epoch-49 batch-113
Running loss of epoch-49 batch-113 = 2.6148278266191483e-05

Training epoch-49 batch-114
Running loss of epoch-49 batch-114 = 9.875278919935226e-06

Training epoch-49 batch-115
Running loss of epoch-49 batch-115 = 6.4158812165260315e-06

Training epoch-49 batch-116
Running loss of epoch-49 batch-116 = 1.081777736544609e-05

Training epoch-49 batch-117
Running loss of epoch-49 batch-117 = 2.8093578293919563e-05

Training epoch-49 batch-118
Running loss of epoch-49 batch-118 = 9.86829400062561e-06

Training epoch-49 batch-119
Running loss of epoch-49 batch-119 = 6.718095391988754e-06

Training epoch-49 batch-120
Running loss of epoch-49 batch-120 = 9.818002581596375e-06

Training epoch-49 batch-121
Running loss of epoch-49 batch-121 = 1.722807064652443e-05

Training epoch-49 batch-122
Running loss of epoch-49 batch-122 = 8.275732398033142e-06

Training epoch-49 batch-123
Running loss of epoch-49 batch-123 = 1.36552844196558e-05

Training epoch-49 batch-124
Running loss of epoch-49 batch-124 = 3.4283846616744995e-05

Training epoch-49 batch-125
Running loss of epoch-49 batch-125 = 6.4391642808914185e-06

Training epoch-49 batch-126
Running loss of epoch-49 batch-126 = 2.810428850352764e-05

Training epoch-49 batch-127
Running loss of epoch-49 batch-127 = 7.647089660167694e-06

Training epoch-49 batch-128
Running loss of epoch-49 batch-128 = 9.39168967306614e-06

Training epoch-49 batch-129
Running loss of epoch-49 batch-129 = 6.030779331922531e-06

Training epoch-49 batch-130
Running loss of epoch-49 batch-130 = 1.6723526641726494e-05

Training epoch-49 batch-131
Running loss of epoch-49 batch-131 = 1.8140999600291252e-05

Training epoch-49 batch-132
Running loss of epoch-49 batch-132 = 2.971617504954338e-06

Training epoch-49 batch-133
Running loss of epoch-49 batch-133 = 9.297393262386322e-06

Training epoch-49 batch-134
Running loss of epoch-49 batch-134 = 3.208266571164131e-05

Training epoch-49 batch-135
Running loss of epoch-49 batch-135 = 1.2051081284880638e-05

Training epoch-49 batch-136
Running loss of epoch-49 batch-136 = 7.5392890721559525e-06

Training epoch-49 batch-137
Running loss of epoch-49 batch-137 = 1.952424645423889e-05

Training epoch-49 batch-138
Running loss of epoch-49 batch-138 = 8.250819519162178e-06

Training epoch-49 batch-139
Running loss of epoch-49 batch-139 = 7.593771442770958e-06

Training epoch-49 batch-140
Running loss of epoch-49 batch-140 = 3.8386788219213486e-06

Training epoch-49 batch-141
Running loss of epoch-49 batch-141 = 1.5505822375416756e-05

Training epoch-49 batch-142
Running loss of epoch-49 batch-142 = 4.416704177856445e-05

Training epoch-49 batch-143
Running loss of epoch-49 batch-143 = 5.061039701104164e-06

Training epoch-49 batch-144
Running loss of epoch-49 batch-144 = 8.22148285806179e-06

Training epoch-49 batch-145
Running loss of epoch-49 batch-145 = 9.247567504644394e-06

Training epoch-49 batch-146
Running loss of epoch-49 batch-146 = 1.0098563507199287e-05

Training epoch-49 batch-147
Running loss of epoch-49 batch-147 = 1.1713244020938873e-05

Training epoch-49 batch-148
Running loss of epoch-49 batch-148 = 1.0267598554491997e-05

Training epoch-49 batch-149
Running loss of epoch-49 batch-149 = 6.0782767832279205e-06

Training epoch-49 batch-150
Running loss of epoch-49 batch-150 = 1.4835270121693611e-05

Training epoch-49 batch-151
Running loss of epoch-49 batch-151 = 1.4760531485080719e-05

Training epoch-49 batch-152
Running loss of epoch-49 batch-152 = 2.6838388293981552e-06

Training epoch-49 batch-153
Running loss of epoch-49 batch-153 = 1.0866206139326096e-05

Training epoch-49 batch-154
Running loss of epoch-49 batch-154 = 2.0930776372551918e-05

Training epoch-49 batch-155
Running loss of epoch-49 batch-155 = 1.1921161785721779e-05

Training epoch-49 batch-156
Running loss of epoch-49 batch-156 = 1.3440614566206932e-05

Training epoch-49 batch-157
Running loss of epoch-49 batch-157 = 6.907060742378235e-05

Finished training epoch-49.



Average train loss at epoch-49 = 1.2176764011383057e-05

Started Evaluation

Average val loss at epoch-49 = 1.6349337375714987

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 88.70 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 64.62 %

Overall Accuracy = 81.08 %

Finished Evaluation



Started training epoch-50


Training epoch-50 batch-1
Running loss of epoch-50 batch-1 = 1.780875027179718e-05

Training epoch-50 batch-2
Running loss of epoch-50 batch-2 = 5.552778020501137e-06

Training epoch-50 batch-3
Running loss of epoch-50 batch-3 = 7.147202268242836e-06

Training epoch-50 batch-4
Running loss of epoch-50 batch-4 = 1.1079944670200348e-05

Training epoch-50 batch-5
Running loss of epoch-50 batch-5 = 1.4980323612689972e-05

Training epoch-50 batch-6
Running loss of epoch-50 batch-6 = 1.757522113621235e-05

Training epoch-50 batch-7
Running loss of epoch-50 batch-7 = 9.581213817000389e-06

Training epoch-50 batch-8
Running loss of epoch-50 batch-8 = 5.5623240768909454e-06

Training epoch-50 batch-9
Running loss of epoch-50 batch-9 = 4.6622008085250854e-06

Training epoch-50 batch-10
Running loss of epoch-50 batch-10 = 7.442431524395943e-06

Training epoch-50 batch-11
Running loss of epoch-50 batch-11 = 1.3228040188550949e-05

Training epoch-50 batch-12
Running loss of epoch-50 batch-12 = 9.298091754317284e-06

Training epoch-50 batch-13
Running loss of epoch-50 batch-13 = 1.5376601368188858e-05

Training epoch-50 batch-14
Running loss of epoch-50 batch-14 = 1.2168893590569496e-05

Training epoch-50 batch-15
Running loss of epoch-50 batch-15 = 9.51089896261692e-06

Training epoch-50 batch-16
Running loss of epoch-50 batch-16 = 4.883855581283569e-06

Training epoch-50 batch-17
Running loss of epoch-50 batch-17 = 5.900394171476364e-06

Training epoch-50 batch-18
Running loss of epoch-50 batch-18 = 7.355818524956703e-06

Training epoch-50 batch-19
Running loss of epoch-50 batch-19 = 9.29250381886959e-06

Training epoch-50 batch-20
Running loss of epoch-50 batch-20 = 4.835193976759911e-06

Training epoch-50 batch-21
Running loss of epoch-50 batch-21 = 8.683418855071068e-06

Training epoch-50 batch-22
Running loss of epoch-50 batch-22 = 6.889691576361656e-06

Training epoch-50 batch-23
Running loss of epoch-50 batch-23 = 4.81470488011837e-06

Training epoch-50 batch-24
Running loss of epoch-50 batch-24 = 1.1963769793510437e-05

Training epoch-50 batch-25
Running loss of epoch-50 batch-25 = 1.1279014870524406e-05

Training epoch-50 batch-26
Running loss of epoch-50 batch-26 = 2.8510810807347298e-05

Training epoch-50 batch-27
Running loss of epoch-50 batch-27 = 1.5567755326628685e-05

Training epoch-50 batch-28
Running loss of epoch-50 batch-28 = 5.129259079694748e-06

Training epoch-50 batch-29
Running loss of epoch-50 batch-29 = 8.56141559779644e-06

Training epoch-50 batch-30
Running loss of epoch-50 batch-30 = 5.088048055768013e-06

Training epoch-50 batch-31
Running loss of epoch-50 batch-31 = 9.576324373483658e-06

Training epoch-50 batch-32
Running loss of epoch-50 batch-32 = 1.3423850759863853e-05

Training epoch-50 batch-33
Running loss of epoch-50 batch-33 = 1.3711629435420036e-05

Training epoch-50 batch-34
Running loss of epoch-50 batch-34 = 6.904127076268196e-06

Training epoch-50 batch-35
Running loss of epoch-50 batch-35 = 2.6716850697994232e-05

Training epoch-50 batch-36
Running loss of epoch-50 batch-36 = 4.7853682190179825e-06

Training epoch-50 batch-37
Running loss of epoch-50 batch-37 = 1.1557945981621742e-05

Training epoch-50 batch-38
Running loss of epoch-50 batch-38 = 7.014954462647438e-06

Training epoch-50 batch-39
Running loss of epoch-50 batch-39 = 8.88807699084282e-06

Training epoch-50 batch-40
Running loss of epoch-50 batch-40 = 1.2026401236653328e-05

Training epoch-50 batch-41
Running loss of epoch-50 batch-41 = 4.644738510251045e-06

Training epoch-50 batch-42
Running loss of epoch-50 batch-42 = 1.0705320164561272e-05

Training epoch-50 batch-43
Running loss of epoch-50 batch-43 = 1.5766127035021782e-05

Training epoch-50 batch-44
Running loss of epoch-50 batch-44 = 8.127186447381973e-06

Training epoch-50 batch-45
Running loss of epoch-50 batch-45 = 2.4046516045928e-05

Training epoch-50 batch-46
Running loss of epoch-50 batch-46 = 7.338356226682663e-06

Training epoch-50 batch-47
Running loss of epoch-50 batch-47 = 7.5749121606349945e-06

Training epoch-50 batch-48
Running loss of epoch-50 batch-48 = 4.64683398604393e-06

Training epoch-50 batch-49
Running loss of epoch-50 batch-49 = 1.1918134987354279e-05

Training epoch-50 batch-50
Running loss of epoch-50 batch-50 = 9.320210665464401e-06

Training epoch-50 batch-51
Running loss of epoch-50 batch-51 = 1.3382639735937119e-05

Training epoch-50 batch-52
Running loss of epoch-50 batch-52 = 1.2185657396912575e-05

Training epoch-50 batch-53
Running loss of epoch-50 batch-53 = 9.577721357345581e-06

Training epoch-50 batch-54
Running loss of epoch-50 batch-54 = 7.527414709329605e-06

Training epoch-50 batch-55
Running loss of epoch-50 batch-55 = 2.364278770983219e-05

Training epoch-50 batch-56
Running loss of epoch-50 batch-56 = 2.314569428563118e-06

Training epoch-50 batch-57
Running loss of epoch-50 batch-57 = 2.0225299522280693e-05

Training epoch-50 batch-58
Running loss of epoch-50 batch-58 = 7.392372936010361e-06

Training epoch-50 batch-59
Running loss of epoch-50 batch-59 = 1.0073650628328323e-05

Training epoch-50 batch-60
Running loss of epoch-50 batch-60 = 1.371721737086773e-05

Training epoch-50 batch-61
Running loss of epoch-50 batch-61 = 5.241483449935913e-06

Training epoch-50 batch-62
Running loss of epoch-50 batch-62 = 7.709488272666931e-06

Training epoch-50 batch-63
Running loss of epoch-50 batch-63 = 6.397254765033722e-06

Training epoch-50 batch-64
Running loss of epoch-50 batch-64 = 2.0369887351989746e-05

Training epoch-50 batch-65
Running loss of epoch-50 batch-65 = 2.0264415070414543e-05

Training epoch-50 batch-66
Running loss of epoch-50 batch-66 = 1.6132835298776627e-05

Training epoch-50 batch-67
Running loss of epoch-50 batch-67 = 2.155848778784275e-05

Training epoch-50 batch-68
Running loss of epoch-50 batch-68 = 6.538582965731621e-06

Training epoch-50 batch-69
Running loss of epoch-50 batch-69 = 1.0388903319835663e-05

Training epoch-50 batch-70
Running loss of epoch-50 batch-70 = 5.190959200263023e-06

Training epoch-50 batch-71
Running loss of epoch-50 batch-71 = 1.122383400797844e-05

Training epoch-50 batch-72
Running loss of epoch-50 batch-72 = 9.90321859717369e-06

Training epoch-50 batch-73
Running loss of epoch-50 batch-73 = 2.825097180902958e-05

Training epoch-50 batch-74
Running loss of epoch-50 batch-74 = 2.5540124624967575e-05

Training epoch-50 batch-75
Running loss of epoch-50 batch-75 = 2.200016751885414e-06

Training epoch-50 batch-76
Running loss of epoch-50 batch-76 = 2.6500900276005268e-05

Training epoch-50 batch-77
Running loss of epoch-50 batch-77 = 1.4772871509194374e-05

Training epoch-50 batch-78
Running loss of epoch-50 batch-78 = 1.7870217561721802e-05

Training epoch-50 batch-79
Running loss of epoch-50 batch-79 = 2.559577114880085e-05

Training epoch-50 batch-80
Running loss of epoch-50 batch-80 = 6.949529051780701e-06

Training epoch-50 batch-81
Running loss of epoch-50 batch-81 = 5.153007805347443e-06

Training epoch-50 batch-82
Running loss of epoch-50 batch-82 = 1.3003358617424965e-05

Training epoch-50 batch-83
Running loss of epoch-50 batch-83 = 8.915551006793976e-06

Training epoch-50 batch-84
Running loss of epoch-50 batch-84 = 6.45010732114315e-06

Training epoch-50 batch-85
Running loss of epoch-50 batch-85 = 8.501578122377396e-06

Training epoch-50 batch-86
Running loss of epoch-50 batch-86 = 8.288305252790451e-06

Training epoch-50 batch-87
Running loss of epoch-50 batch-87 = 1.1013587936758995e-05

Training epoch-50 batch-88
Running loss of epoch-50 batch-88 = 7.961643859744072e-06

Training epoch-50 batch-89
Running loss of epoch-50 batch-89 = 4.5634107664227486e-05

Training epoch-50 batch-90
Running loss of epoch-50 batch-90 = 1.0247575119137764e-05

Training epoch-50 batch-91
Running loss of epoch-50 batch-91 = 3.5884790122509e-05

Training epoch-50 batch-92
Running loss of epoch-50 batch-92 = 7.4070412665605545e-06

Training epoch-50 batch-93
Running loss of epoch-50 batch-93 = 8.282717317342758e-06

Training epoch-50 batch-94
Running loss of epoch-50 batch-94 = 4.9229711294174194e-06

Training epoch-50 batch-95
Running loss of epoch-50 batch-95 = 3.9013102650642395e-06

Training epoch-50 batch-96
Running loss of epoch-50 batch-96 = 1.1692987754940987e-05

Training epoch-50 batch-97
Running loss of epoch-50 batch-97 = 1.3167038559913635e-05

Training epoch-50 batch-98
Running loss of epoch-50 batch-98 = 3.2070092856884003e-06

Training epoch-50 batch-99
Running loss of epoch-50 batch-99 = 1.3645738363265991e-05

Training epoch-50 batch-100
Running loss of epoch-50 batch-100 = 6.21424987912178e-06

Training epoch-50 batch-101
Running loss of epoch-50 batch-101 = 4.784669727087021e-06

Training epoch-50 batch-102
Running loss of epoch-50 batch-102 = 4.302710294723511e-06

Training epoch-50 batch-103
Running loss of epoch-50 batch-103 = 5.968846380710602e-06

Training epoch-50 batch-104
Running loss of epoch-50 batch-104 = 2.153916284441948e-06

Training epoch-50 batch-105
Running loss of epoch-50 batch-105 = 8.379342034459114e-06

Training epoch-50 batch-106
Running loss of epoch-50 batch-106 = 1.5084398910403252e-05

Training epoch-50 batch-107
Running loss of epoch-50 batch-107 = 1.0617543011903763e-05

Training epoch-50 batch-108
Running loss of epoch-50 batch-108 = 2.3222528398036957e-06

Training epoch-50 batch-109
Running loss of epoch-50 batch-109 = 2.559274435043335e-06

Training epoch-50 batch-110
Running loss of epoch-50 batch-110 = 1.9498402252793312e-05

Training epoch-50 batch-111
Running loss of epoch-50 batch-111 = 1.77889596670866e-05

Training epoch-50 batch-112
Running loss of epoch-50 batch-112 = 3.2679177820682526e-05

Training epoch-50 batch-113
Running loss of epoch-50 batch-113 = 3.513181582093239e-06

Training epoch-50 batch-114
Running loss of epoch-50 batch-114 = 4.854751750826836e-06

Training epoch-50 batch-115
Running loss of epoch-50 batch-115 = 7.678521797060966e-06

Training epoch-50 batch-116
Running loss of epoch-50 batch-116 = 9.130220860242844e-06

Training epoch-50 batch-117
Running loss of epoch-50 batch-117 = 9.286217391490936e-06

Training epoch-50 batch-118
Running loss of epoch-50 batch-118 = 2.596992999315262e-06

Training epoch-50 batch-119
Running loss of epoch-50 batch-119 = 1.3979850336909294e-05

Training epoch-50 batch-120
Running loss of epoch-50 batch-120 = 5.068955942988396e-06

Training epoch-50 batch-121
Running loss of epoch-50 batch-121 = 1.4086253941059113e-05

Training epoch-50 batch-122
Running loss of epoch-50 batch-122 = 7.610069587826729e-06

Training epoch-50 batch-123
Running loss of epoch-50 batch-123 = 7.668975740671158e-06

Training epoch-50 batch-124
Running loss of epoch-50 batch-124 = 6.44405372440815e-06

Training epoch-50 batch-125
Running loss of epoch-50 batch-125 = 8.352566510438919e-06

Training epoch-50 batch-126
Running loss of epoch-50 batch-126 = 4.50480729341507e-06

Training epoch-50 batch-127
Running loss of epoch-50 batch-127 = 3.8144178688526154e-05

Training epoch-50 batch-128
Running loss of epoch-50 batch-128 = 1.0606367141008377e-05

Training epoch-50 batch-129
Running loss of epoch-50 batch-129 = 5.5818818509578705e-06

Training epoch-50 batch-130
Running loss of epoch-50 batch-130 = 1.0926276445388794e-05

Training epoch-50 batch-131
Running loss of epoch-50 batch-131 = 1.4747725799679756e-05

Training epoch-50 batch-132
Running loss of epoch-50 batch-132 = 1.6026198863983154e-05

Training epoch-50 batch-133
Running loss of epoch-50 batch-133 = 2.264208160340786e-05

Training epoch-50 batch-134
Running loss of epoch-50 batch-134 = 1.2138858437538147e-05

Training epoch-50 batch-135
Running loss of epoch-50 batch-135 = 1.164805144071579e-05

Training epoch-50 batch-136
Running loss of epoch-50 batch-136 = 8.760020136833191e-06

Training epoch-50 batch-137
Running loss of epoch-50 batch-137 = 4.554307088255882e-05

Training epoch-50 batch-138
Running loss of epoch-50 batch-138 = 1.3594748452305794e-05

Training epoch-50 batch-139
Running loss of epoch-50 batch-139 = 8.118338882923126e-06

Training epoch-50 batch-140
Running loss of epoch-50 batch-140 = 1.1955853551626205e-05

Training epoch-50 batch-141
Running loss of epoch-50 batch-141 = 3.0007213354110718e-06

Training epoch-50 batch-142
Running loss of epoch-50 batch-142 = 5.340902134776115e-06

Training epoch-50 batch-143
Running loss of epoch-50 batch-143 = 4.1262246668338776e-06

Training epoch-50 batch-144
Running loss of epoch-50 batch-144 = 6.727408617734909e-06

Training epoch-50 batch-145
Running loss of epoch-50 batch-145 = 1.3764481991529465e-05

Training epoch-50 batch-146
Running loss of epoch-50 batch-146 = 1.512700691819191e-05

Training epoch-50 batch-147
Running loss of epoch-50 batch-147 = 4.320242442190647e-05

Training epoch-50 batch-148
Running loss of epoch-50 batch-148 = 5.631474778056145e-06

Training epoch-50 batch-149
Running loss of epoch-50 batch-149 = 2.0927749574184418e-05

Training epoch-50 batch-150
Running loss of epoch-50 batch-150 = 1.3464828953146935e-05

Training epoch-50 batch-151
Running loss of epoch-50 batch-151 = 9.142328053712845e-06

Training epoch-50 batch-152
Running loss of epoch-50 batch-152 = 1.4964723959565163e-05

Training epoch-50 batch-153
Running loss of epoch-50 batch-153 = 5.967449396848679e-06

Training epoch-50 batch-154
Running loss of epoch-50 batch-154 = 1.1858996003866196e-05

Training epoch-50 batch-155
Running loss of epoch-50 batch-155 = 1.298566348850727e-05

Training epoch-50 batch-156
Running loss of epoch-50 batch-156 = 8.850591257214546e-06

Training epoch-50 batch-157
Running loss of epoch-50 batch-157 = 1.3317912817001343e-05

Finished training epoch-50.



Average train loss at epoch-50 = 1.1650042980909347e-05

Started Evaluation

Average val loss at epoch-50 = 1.6324098635407662

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 87.87 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.91 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-51


Training epoch-51 batch-1
Running loss of epoch-51 batch-1 = 9.47713851928711e-06

Training epoch-51 batch-2
Running loss of epoch-51 batch-2 = 6.6193751990795135e-06

Training epoch-51 batch-3
Running loss of epoch-51 batch-3 = 1.0417774319648743e-05

Training epoch-51 batch-4
Running loss of epoch-51 batch-4 = 4.506204277276993e-06

Training epoch-51 batch-5
Running loss of epoch-51 batch-5 = 6.5336935222148895e-06

Training epoch-51 batch-6
Running loss of epoch-51 batch-6 = 4.132045432925224e-06

Training epoch-51 batch-7
Running loss of epoch-51 batch-7 = 1.7305603250861168e-05

Training epoch-51 batch-8
Running loss of epoch-51 batch-8 = 5.421461537480354e-06

Training epoch-51 batch-9
Running loss of epoch-51 batch-9 = 8.831499144434929e-06

Training epoch-51 batch-10
Running loss of epoch-51 batch-10 = 2.203509211540222e-05

Training epoch-51 batch-11
Running loss of epoch-51 batch-11 = 1.095072366297245e-05

Training epoch-51 batch-12
Running loss of epoch-51 batch-12 = 6.929505616426468e-06

Training epoch-51 batch-13
Running loss of epoch-51 batch-13 = 1.0221032425761223e-05

Training epoch-51 batch-14
Running loss of epoch-51 batch-14 = 1.0399846360087395e-05

Training epoch-51 batch-15
Running loss of epoch-51 batch-15 = 1.0766321793198586e-05

Training epoch-51 batch-16
Running loss of epoch-51 batch-16 = 5.6372955441474915e-06

Training epoch-51 batch-17
Running loss of epoch-51 batch-17 = 6.062444299459457e-06

Training epoch-51 batch-18
Running loss of epoch-51 batch-18 = 1.4686724171042442e-05

Training epoch-51 batch-19
Running loss of epoch-51 batch-19 = 8.911360055208206e-06

Training epoch-51 batch-20
Running loss of epoch-51 batch-20 = 6.68293796479702e-06

Training epoch-51 batch-21
Running loss of epoch-51 batch-21 = 4.307832568883896e-06

Training epoch-51 batch-22
Running loss of epoch-51 batch-22 = 6.16488978266716e-06

Training epoch-51 batch-23
Running loss of epoch-51 batch-23 = 5.720648914575577e-06

Training epoch-51 batch-24
Running loss of epoch-51 batch-24 = 5.836598575115204e-06

Training epoch-51 batch-25
Running loss of epoch-51 batch-25 = 3.0852388590574265e-06

Training epoch-51 batch-26
Running loss of epoch-51 batch-26 = 1.1500203981995583e-05

Training epoch-51 batch-27
Running loss of epoch-51 batch-27 = 9.945593774318695e-06

Training epoch-51 batch-28
Running loss of epoch-51 batch-28 = 6.056157872080803e-06

Training epoch-51 batch-29
Running loss of epoch-51 batch-29 = 7.808441296219826e-06

Training epoch-51 batch-30
Running loss of epoch-51 batch-30 = 1.4497200027108192e-05

Training epoch-51 batch-31
Running loss of epoch-51 batch-31 = 1.0430347174406052e-05

Training epoch-51 batch-32
Running loss of epoch-51 batch-32 = 5.6480057537555695e-06

Training epoch-51 batch-33
Running loss of epoch-51 batch-33 = 1.3114186003804207e-05

Training epoch-51 batch-34
Running loss of epoch-51 batch-34 = 5.4514966905117035e-06

Training epoch-51 batch-35
Running loss of epoch-51 batch-35 = 5.924608558416367e-06

Training epoch-51 batch-36
Running loss of epoch-51 batch-36 = 7.114955224096775e-06

Training epoch-51 batch-37
Running loss of epoch-51 batch-37 = 8.449424058198929e-06

Training epoch-51 batch-38
Running loss of epoch-51 batch-38 = 9.533949196338654e-06

Training epoch-51 batch-39
Running loss of epoch-51 batch-39 = 9.950948879122734e-06

Training epoch-51 batch-40
Running loss of epoch-51 batch-40 = 9.242678061127663e-06

Training epoch-51 batch-41
Running loss of epoch-51 batch-41 = 4.200497642159462e-06

Training epoch-51 batch-42
Running loss of epoch-51 batch-42 = 2.5572720915079117e-05

Training epoch-51 batch-43
Running loss of epoch-51 batch-43 = 1.554167829453945e-05

Training epoch-51 batch-44
Running loss of epoch-51 batch-44 = 7.71484337747097e-06

Training epoch-51 batch-45
Running loss of epoch-51 batch-45 = 1.608370803296566e-05

Training epoch-51 batch-46
Running loss of epoch-51 batch-46 = 5.24008646607399e-06

Training epoch-51 batch-47
Running loss of epoch-51 batch-47 = 3.9173755794763565e-06

Training epoch-51 batch-48
Running loss of epoch-51 batch-48 = 1.072208397090435e-05

Training epoch-51 batch-49
Running loss of epoch-51 batch-49 = 8.973991498351097e-06

Training epoch-51 batch-50
Running loss of epoch-51 batch-50 = 3.2151583582162857e-06

Training epoch-51 batch-51
Running loss of epoch-51 batch-51 = 1.1631753295660019e-05

Training epoch-51 batch-52
Running loss of epoch-51 batch-52 = 1.0657589882612228e-05

Training epoch-51 batch-53
Running loss of epoch-51 batch-53 = 1.1991709470748901e-05

Training epoch-51 batch-54
Running loss of epoch-51 batch-54 = 3.5350676625967026e-06

Training epoch-51 batch-55
Running loss of epoch-51 batch-55 = 5.562789738178253e-06

Training epoch-51 batch-56
Running loss of epoch-51 batch-56 = 9.170034900307655e-06

Training epoch-51 batch-57
Running loss of epoch-51 batch-57 = 1.4517689123749733e-05

Training epoch-51 batch-58
Running loss of epoch-51 batch-58 = 2.2041145712137222e-05

Training epoch-51 batch-59
Running loss of epoch-51 batch-59 = 1.3673445209860802e-05

Training epoch-51 batch-60
Running loss of epoch-51 batch-60 = 1.4154007658362389e-05

Training epoch-51 batch-61
Running loss of epoch-51 batch-61 = 1.3574725016951561e-05

Training epoch-51 batch-62
Running loss of epoch-51 batch-62 = 9.066425263881683e-06

Training epoch-51 batch-63
Running loss of epoch-51 batch-63 = 2.0246487110853195e-05

Training epoch-51 batch-64
Running loss of epoch-51 batch-64 = 5.854060873389244e-06

Training epoch-51 batch-65
Running loss of epoch-51 batch-65 = 7.801922038197517e-06

Training epoch-51 batch-66
Running loss of epoch-51 batch-66 = 1.0074581950902939e-05

Training epoch-51 batch-67
Running loss of epoch-51 batch-67 = 7.211463525891304e-06

Training epoch-51 batch-68
Running loss of epoch-51 batch-68 = 5.024950951337814e-06

Training epoch-51 batch-69
Running loss of epoch-51 batch-69 = 1.1658994480967522e-05

Training epoch-51 batch-70
Running loss of epoch-51 batch-70 = 1.4456454664468765e-05

Training epoch-51 batch-71
Running loss of epoch-51 batch-71 = 8.35140235722065e-06

Training epoch-51 batch-72
Running loss of epoch-51 batch-72 = 5.478737875819206e-06

Training epoch-51 batch-73
Running loss of epoch-51 batch-73 = 1.0322080925107002e-05

Training epoch-51 batch-74
Running loss of epoch-51 batch-74 = 4.109460860490799e-06

Training epoch-51 batch-75
Running loss of epoch-51 batch-75 = 7.477123290300369e-06

Training epoch-51 batch-76
Running loss of epoch-51 batch-76 = 7.902039214968681e-06

Training epoch-51 batch-77
Running loss of epoch-51 batch-77 = 4.505738615989685e-06

Training epoch-51 batch-78
Running loss of epoch-51 batch-78 = 4.980480298399925e-06

Training epoch-51 batch-79
Running loss of epoch-51 batch-79 = 9.834766387939453e-06

Training epoch-51 batch-80
Running loss of epoch-51 batch-80 = 1.100543886423111e-05

Training epoch-51 batch-81
Running loss of epoch-51 batch-81 = 6.505986675620079e-06

Training epoch-51 batch-82
Running loss of epoch-51 batch-82 = 8.821720257401466e-06

Training epoch-51 batch-83
Running loss of epoch-51 batch-83 = 1.810723915696144e-06

Training epoch-51 batch-84
Running loss of epoch-51 batch-84 = 4.798499867320061e-05

Training epoch-51 batch-85
Running loss of epoch-51 batch-85 = 9.600073099136353e-06

Training epoch-51 batch-86
Running loss of epoch-51 batch-86 = 1.7818529158830643e-05

Training epoch-51 batch-87
Running loss of epoch-51 batch-87 = 1.0196119546890259e-05

Training epoch-51 batch-88
Running loss of epoch-51 batch-88 = 7.016351446509361e-06

Training epoch-51 batch-89
Running loss of epoch-51 batch-89 = 5.0780363380908966e-05

Training epoch-51 batch-90
Running loss of epoch-51 batch-90 = 7.739756256341934e-06

Training epoch-51 batch-91
Running loss of epoch-51 batch-91 = 2.126605249941349e-05

Training epoch-51 batch-92
Running loss of epoch-51 batch-92 = 1.0580988600850105e-05

Training epoch-51 batch-93
Running loss of epoch-51 batch-93 = 7.113441824913025e-06

Training epoch-51 batch-94
Running loss of epoch-51 batch-94 = 1.764087937772274e-05

Training epoch-51 batch-95
Running loss of epoch-51 batch-95 = 1.0159099474549294e-05

Training epoch-51 batch-96
Running loss of epoch-51 batch-96 = 2.5728950276970863e-05

Training epoch-51 batch-97
Running loss of epoch-51 batch-97 = 1.1936994269490242e-05

Training epoch-51 batch-98
Running loss of epoch-51 batch-98 = 1.537241041660309e-05

Training epoch-51 batch-99
Running loss of epoch-51 batch-99 = 1.0379590094089508e-05

Training epoch-51 batch-100
Running loss of epoch-51 batch-100 = 1.6045058146119118e-05

Training epoch-51 batch-101
Running loss of epoch-51 batch-101 = 1.2868549674749374e-05

Training epoch-51 batch-102
Running loss of epoch-51 batch-102 = 7.468275725841522e-06

Training epoch-51 batch-103
Running loss of epoch-51 batch-103 = 9.365146979689598e-06

Training epoch-51 batch-104
Running loss of epoch-51 batch-104 = 9.56677831709385e-06

Training epoch-51 batch-105
Running loss of epoch-51 batch-105 = 1.0994961485266685e-05

Training epoch-51 batch-106
Running loss of epoch-51 batch-106 = 1.345248892903328e-05

Training epoch-51 batch-107
Running loss of epoch-51 batch-107 = 5.488749593496323e-06

Training epoch-51 batch-108
Running loss of epoch-51 batch-108 = 1.1774012818932533e-05

Training epoch-51 batch-109
Running loss of epoch-51 batch-109 = 5.363253876566887e-06

Training epoch-51 batch-110
Running loss of epoch-51 batch-110 = 9.673647582530975e-06

Training epoch-51 batch-111
Running loss of epoch-51 batch-111 = 7.73346982896328e-06

Training epoch-51 batch-112
Running loss of epoch-51 batch-112 = 1.1418713256716728e-05

Training epoch-51 batch-113
Running loss of epoch-51 batch-113 = 9.717419743537903e-06

Training epoch-51 batch-114
Running loss of epoch-51 batch-114 = 9.255018085241318e-06

Training epoch-51 batch-115
Running loss of epoch-51 batch-115 = 2.8857961297035217e-05

Training epoch-51 batch-116
Running loss of epoch-51 batch-116 = 1.5871133655309677e-05

Training epoch-51 batch-117
Running loss of epoch-51 batch-117 = 1.101987436413765e-05

Training epoch-51 batch-118
Running loss of epoch-51 batch-118 = 5.812849849462509e-06

Training epoch-51 batch-119
Running loss of epoch-51 batch-119 = 6.0659367591142654e-06

Training epoch-51 batch-120
Running loss of epoch-51 batch-120 = 1.6974518075585365e-05

Training epoch-51 batch-121
Running loss of epoch-51 batch-121 = 1.7266487702727318e-05

Training epoch-51 batch-122
Running loss of epoch-51 batch-122 = 1.5230849385261536e-05

Training epoch-51 batch-123
Running loss of epoch-51 batch-123 = 4.991888999938965e-06

Training epoch-51 batch-124
Running loss of epoch-51 batch-124 = 9.648967534303665e-06

Training epoch-51 batch-125
Running loss of epoch-51 batch-125 = 5.460809916257858e-06

Training epoch-51 batch-126
Running loss of epoch-51 batch-126 = 8.794013410806656e-06

Training epoch-51 batch-127
Running loss of epoch-51 batch-127 = 6.194692105054855e-06

Training epoch-51 batch-128
Running loss of epoch-51 batch-128 = 4.281988367438316e-06

Training epoch-51 batch-129
Running loss of epoch-51 batch-129 = 1.5722354874014854e-05

Training epoch-51 batch-130
Running loss of epoch-51 batch-130 = 1.3867625966668129e-05

Training epoch-51 batch-131
Running loss of epoch-51 batch-131 = 1.1892756447196007e-05

Training epoch-51 batch-132
Running loss of epoch-51 batch-132 = 1.910841092467308e-06

Training epoch-51 batch-133
Running loss of epoch-51 batch-133 = 9.283889085054398e-06

Training epoch-51 batch-134
Running loss of epoch-51 batch-134 = 8.634291589260101e-06

Training epoch-51 batch-135
Running loss of epoch-51 batch-135 = 7.554423063993454e-06

Training epoch-51 batch-136
Running loss of epoch-51 batch-136 = 1.2098345905542374e-05

Training epoch-51 batch-137
Running loss of epoch-51 batch-137 = 2.7644680812954903e-05

Training epoch-51 batch-138
Running loss of epoch-51 batch-138 = 5.688052624464035e-06

Training epoch-51 batch-139
Running loss of epoch-51 batch-139 = 8.48621129989624e-06

Training epoch-51 batch-140
Running loss of epoch-51 batch-140 = 8.690403774380684e-06

Training epoch-51 batch-141
Running loss of epoch-51 batch-141 = 1.96029432117939e-05

Training epoch-51 batch-142
Running loss of epoch-51 batch-142 = 1.1854572221636772e-05

Training epoch-51 batch-143
Running loss of epoch-51 batch-143 = 6.941612809896469e-06

Training epoch-51 batch-144
Running loss of epoch-51 batch-144 = 5.976762622594833e-06

Training epoch-51 batch-145
Running loss of epoch-51 batch-145 = 4.5655760914087296e-05

Training epoch-51 batch-146
Running loss of epoch-51 batch-146 = 4.384201020002365e-06

Training epoch-51 batch-147
Running loss of epoch-51 batch-147 = 5.282042548060417e-05

Training epoch-51 batch-148
Running loss of epoch-51 batch-148 = 1.3967743143439293e-05

Training epoch-51 batch-149
Running loss of epoch-51 batch-149 = 4.8282090574502945e-06

Training epoch-51 batch-150
Running loss of epoch-51 batch-150 = 2.1218787878751755e-05

Training epoch-51 batch-151
Running loss of epoch-51 batch-151 = 8.399365469813347e-06

Training epoch-51 batch-152
Running loss of epoch-51 batch-152 = 4.124827682971954e-06

Training epoch-51 batch-153
Running loss of epoch-51 batch-153 = 2.514082007110119e-05

Training epoch-51 batch-154
Running loss of epoch-51 batch-154 = 4.815403372049332e-06

Training epoch-51 batch-155
Running loss of epoch-51 batch-155 = 1.4696503058075905e-05

Training epoch-51 batch-156
Running loss of epoch-51 batch-156 = 5.1674433052539825e-06

Training epoch-51 batch-157
Running loss of epoch-51 batch-157 = 1.4718621969223022e-05

Finished training epoch-51.



Average train loss at epoch-51 = 1.1092644184827805e-05

Started Evaluation

Average val loss at epoch-51 = 1.6279598501946357

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 89.18 %
Accuracy for class onCreate is: 89.34 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 66.44 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.51 %

Finished Evaluation



Started training epoch-52


Training epoch-52 batch-1
Running loss of epoch-52 batch-1 = 1.142127439379692e-05

Training epoch-52 batch-2
Running loss of epoch-52 batch-2 = 5.669193342328072e-06

Training epoch-52 batch-3
Running loss of epoch-52 batch-3 = 7.4338167905807495e-06

Training epoch-52 batch-4
Running loss of epoch-52 batch-4 = 1.585995778441429e-05

Training epoch-52 batch-5
Running loss of epoch-52 batch-5 = 1.238449476659298e-05

Training epoch-52 batch-6
Running loss of epoch-52 batch-6 = 5.526002496480942e-06

Training epoch-52 batch-7
Running loss of epoch-52 batch-7 = 4.714145325124264e-05

Training epoch-52 batch-8
Running loss of epoch-52 batch-8 = 6.084563210606575e-06

Training epoch-52 batch-9
Running loss of epoch-52 batch-9 = 3.7547084502875805e-05

Training epoch-52 batch-10
Running loss of epoch-52 batch-10 = 7.994240149855614e-06

Training epoch-52 batch-11
Running loss of epoch-52 batch-11 = 8.127884939312935e-06

Training epoch-52 batch-12
Running loss of epoch-52 batch-12 = 1.199822872877121e-05

Training epoch-52 batch-13
Running loss of epoch-52 batch-13 = 5.535781383514404e-06

Training epoch-52 batch-14
Running loss of epoch-52 batch-14 = 5.827285349369049e-06

Training epoch-52 batch-15
Running loss of epoch-52 batch-15 = 4.148576408624649e-06

Training epoch-52 batch-16
Running loss of epoch-52 batch-16 = 9.841984137892723e-06

Training epoch-52 batch-17
Running loss of epoch-52 batch-17 = 9.431969374418259e-06

Training epoch-52 batch-18
Running loss of epoch-52 batch-18 = 1.310836523771286e-05

Training epoch-52 batch-19
Running loss of epoch-52 batch-19 = 4.967208951711655e-06

Training epoch-52 batch-20
Running loss of epoch-52 batch-20 = 1.2871576473116875e-05

Training epoch-52 batch-21
Running loss of epoch-52 batch-21 = 6.258022040128708e-06

Training epoch-52 batch-22
Running loss of epoch-52 batch-22 = 5.336944013834e-06

Training epoch-52 batch-23
Running loss of epoch-52 batch-23 = 1.1841300874948502e-05

Training epoch-52 batch-24
Running loss of epoch-52 batch-24 = 5.923677235841751e-06

Training epoch-52 batch-25
Running loss of epoch-52 batch-25 = 2.641696482896805e-06

Training epoch-52 batch-26
Running loss of epoch-52 batch-26 = 5.600042641162872e-06

Training epoch-52 batch-27
Running loss of epoch-52 batch-27 = 2.5960616767406464e-06

Training epoch-52 batch-28
Running loss of epoch-52 batch-28 = 6.549060344696045e-06

Training epoch-52 batch-29
Running loss of epoch-52 batch-29 = 7.507391273975372e-06

Training epoch-52 batch-30
Running loss of epoch-52 batch-30 = 1.2340256944298744e-05

Training epoch-52 batch-31
Running loss of epoch-52 batch-31 = 1.1304626241326332e-05

Training epoch-52 batch-32
Running loss of epoch-52 batch-32 = 7.4713025242090225e-06

Training epoch-52 batch-33
Running loss of epoch-52 batch-33 = 6.882240995764732e-06

Training epoch-52 batch-34
Running loss of epoch-52 batch-34 = 5.191657692193985e-06

Training epoch-52 batch-35
Running loss of epoch-52 batch-35 = 1.8619699403643608e-05

Training epoch-52 batch-36
Running loss of epoch-52 batch-36 = 4.139263182878494e-06

Training epoch-52 batch-37
Running loss of epoch-52 batch-37 = 9.443610906600952e-06

Training epoch-52 batch-38
Running loss of epoch-52 batch-38 = 3.943685442209244e-06

Training epoch-52 batch-39
Running loss of epoch-52 batch-39 = 7.817987352609634e-06

Training epoch-52 batch-40
Running loss of epoch-52 batch-40 = 3.132689744234085e-05

Training epoch-52 batch-41
Running loss of epoch-52 batch-41 = 1.143687404692173e-05

Training epoch-52 batch-42
Running loss of epoch-52 batch-42 = 5.454523488879204e-06

Training epoch-52 batch-43
Running loss of epoch-52 batch-43 = 4.638219252228737e-06

Training epoch-52 batch-44
Running loss of epoch-52 batch-44 = 6.625428795814514e-06

Training epoch-52 batch-45
Running loss of epoch-52 batch-45 = 7.588416337966919e-06

Training epoch-52 batch-46
Running loss of epoch-52 batch-46 = 5.810987204313278e-06

Training epoch-52 batch-47
Running loss of epoch-52 batch-47 = 1.3024779036641121e-05

Training epoch-52 batch-48
Running loss of epoch-52 batch-48 = 5.726004019379616e-06

Training epoch-52 batch-49
Running loss of epoch-52 batch-49 = 8.899485692381859e-06

Training epoch-52 batch-50
Running loss of epoch-52 batch-50 = 7.2445254772901535e-06

Training epoch-52 batch-51
Running loss of epoch-52 batch-51 = 9.11671668291092e-06

Training epoch-52 batch-52
Running loss of epoch-52 batch-52 = 3.161514177918434e-05

Training epoch-52 batch-53
Running loss of epoch-52 batch-53 = 2.4489127099514008e-06

Training epoch-52 batch-54
Running loss of epoch-52 batch-54 = 2.4731503799557686e-05

Training epoch-52 batch-55
Running loss of epoch-52 batch-55 = 6.047775968909264e-06

Training epoch-52 batch-56
Running loss of epoch-52 batch-56 = 5.17512671649456e-06

Training epoch-52 batch-57
Running loss of epoch-52 batch-57 = 7.398659363389015e-06

Training epoch-52 batch-58
Running loss of epoch-52 batch-58 = 2.7354341000318527e-05

Training epoch-52 batch-59
Running loss of epoch-52 batch-59 = 4.532979801297188e-06

Training epoch-52 batch-60
Running loss of epoch-52 batch-60 = 1.5245052054524422e-05

Training epoch-52 batch-61
Running loss of epoch-52 batch-61 = 9.057344868779182e-06

Training epoch-52 batch-62
Running loss of epoch-52 batch-62 = 3.6712735891342163e-06

Training epoch-52 batch-63
Running loss of epoch-52 batch-63 = 8.84360633790493e-06

Training epoch-52 batch-64
Running loss of epoch-52 batch-64 = 1.5349825844168663e-05

Training epoch-52 batch-65
Running loss of epoch-52 batch-65 = 3.1850533559918404e-05

Training epoch-52 batch-66
Running loss of epoch-52 batch-66 = 2.7562491595745087e-06

Training epoch-52 batch-67
Running loss of epoch-52 batch-67 = 3.4614931792020798e-06

Training epoch-52 batch-68
Running loss of epoch-52 batch-68 = 3.1928066164255142e-06

Training epoch-52 batch-69
Running loss of epoch-52 batch-69 = 1.3229204341769218e-05

Training epoch-52 batch-70
Running loss of epoch-52 batch-70 = 5.9211160987615585e-06

Training epoch-52 batch-71
Running loss of epoch-52 batch-71 = 9.658746421337128e-06

Training epoch-52 batch-72
Running loss of epoch-52 batch-72 = 1.6883481293916702e-05

Training epoch-52 batch-73
Running loss of epoch-52 batch-73 = 1.3976125046610832e-05

Training epoch-52 batch-74
Running loss of epoch-52 batch-74 = 4.913657903671265e-06

Training epoch-52 batch-75
Running loss of epoch-52 batch-75 = 3.537628799676895e-06

Training epoch-52 batch-76
Running loss of epoch-52 batch-76 = 9.52836126089096e-06

Training epoch-52 batch-77
Running loss of epoch-52 batch-77 = 5.2801333367824554e-06

Training epoch-52 batch-78
Running loss of epoch-52 batch-78 = 1.649279147386551e-05

Training epoch-52 batch-79
Running loss of epoch-52 batch-79 = 1.360359601676464e-05

Training epoch-52 batch-80
Running loss of epoch-52 batch-80 = 1.1560507118701935e-05

Training epoch-52 batch-81
Running loss of epoch-52 batch-81 = 1.1463649570941925e-05

Training epoch-52 batch-82
Running loss of epoch-52 batch-82 = 1.6932841390371323e-05

Training epoch-52 batch-83
Running loss of epoch-52 batch-83 = 1.727021299302578e-05

Training epoch-52 batch-84
Running loss of epoch-52 batch-84 = 1.986045390367508e-05

Training epoch-52 batch-85
Running loss of epoch-52 batch-85 = 1.533934846520424e-05

Training epoch-52 batch-86
Running loss of epoch-52 batch-86 = 1.821131445467472e-05

Training epoch-52 batch-87
Running loss of epoch-52 batch-87 = 6.5159983932971954e-06

Training epoch-52 batch-88
Running loss of epoch-52 batch-88 = 6.364425644278526e-06

Training epoch-52 batch-89
Running loss of epoch-52 batch-89 = 1.2391945347189903e-05

Training epoch-52 batch-90
Running loss of epoch-52 batch-90 = 5.5176205933094025e-06

Training epoch-52 batch-91
Running loss of epoch-52 batch-91 = 9.79006290435791e-06

Training epoch-52 batch-92
Running loss of epoch-52 batch-92 = 1.2903474271297455e-05

Training epoch-52 batch-93
Running loss of epoch-52 batch-93 = 6.688758730888367e-06

Training epoch-52 batch-94
Running loss of epoch-52 batch-94 = 5.5460259318351746e-06

Training epoch-52 batch-95
Running loss of epoch-52 batch-95 = 1.0371441021561623e-05

Training epoch-52 batch-96
Running loss of epoch-52 batch-96 = 4.650093615055084e-06

Training epoch-52 batch-97
Running loss of epoch-52 batch-97 = 1.0262476280331612e-05

Training epoch-52 batch-98
Running loss of epoch-52 batch-98 = 2.6000896468758583e-05

Training epoch-52 batch-99
Running loss of epoch-52 batch-99 = 4.089437425136566e-06

Training epoch-52 batch-100
Running loss of epoch-52 batch-100 = 1.088390126824379e-05

Training epoch-52 batch-101
Running loss of epoch-52 batch-101 = 6.171409040689468e-06

Training epoch-52 batch-102
Running loss of epoch-52 batch-102 = 3.4207478165626526e-06

Training epoch-52 batch-103
Running loss of epoch-52 batch-103 = 1.3562384992837906e-05

Training epoch-52 batch-104
Running loss of epoch-52 batch-104 = 1.3794051483273506e-05

Training epoch-52 batch-105
Running loss of epoch-52 batch-105 = 1.261616125702858e-05

Training epoch-52 batch-106
Running loss of epoch-52 batch-106 = 1.4342833310365677e-05

Training epoch-52 batch-107
Running loss of epoch-52 batch-107 = 7.0999376475811005e-06

Training epoch-52 batch-108
Running loss of epoch-52 batch-108 = 6.552087143063545e-06

Training epoch-52 batch-109
Running loss of epoch-52 batch-109 = 5.352310836315155e-06

Training epoch-52 batch-110
Running loss of epoch-52 batch-110 = 6.220303475856781e-06

Training epoch-52 batch-111
Running loss of epoch-52 batch-111 = 5.364185199141502e-06

Training epoch-52 batch-112
Running loss of epoch-52 batch-112 = 1.6903039067983627e-05

Training epoch-52 batch-113
Running loss of epoch-52 batch-113 = 2.01414804905653e-05

Training epoch-52 batch-114
Running loss of epoch-52 batch-114 = 2.251635305583477e-05

Training epoch-52 batch-115
Running loss of epoch-52 batch-115 = 2.889428287744522e-06

Training epoch-52 batch-116
Running loss of epoch-52 batch-116 = 2.0747771486639977e-05

Training epoch-52 batch-117
Running loss of epoch-52 batch-117 = 4.241243004798889e-06

Training epoch-52 batch-118
Running loss of epoch-52 batch-118 = 1.4692079275846481e-05

Training epoch-52 batch-119
Running loss of epoch-52 batch-119 = 1.011136919260025e-05

Training epoch-52 batch-120
Running loss of epoch-52 batch-120 = 1.2998934835195541e-05

Training epoch-52 batch-121
Running loss of epoch-52 batch-121 = 1.1053984053432941e-05

Training epoch-52 batch-122
Running loss of epoch-52 batch-122 = 3.352062776684761e-06

Training epoch-52 batch-123
Running loss of epoch-52 batch-123 = 3.477348946034908e-05

Training epoch-52 batch-124
Running loss of epoch-52 batch-124 = 3.230338916182518e-05

Training epoch-52 batch-125
Running loss of epoch-52 batch-125 = 8.230097591876984e-06

Training epoch-52 batch-126
Running loss of epoch-52 batch-126 = 1.0074581950902939e-05

Training epoch-52 batch-127
Running loss of epoch-52 batch-127 = 8.44104215502739e-06

Training epoch-52 batch-128
Running loss of epoch-52 batch-128 = 6.341841071844101e-06

Training epoch-52 batch-129
Running loss of epoch-52 batch-129 = 9.373528882861137e-06

Training epoch-52 batch-130
Running loss of epoch-52 batch-130 = 9.45734791457653e-06

Training epoch-52 batch-131
Running loss of epoch-52 batch-131 = 1.5463214367628098e-05

Training epoch-52 batch-132
Running loss of epoch-52 batch-132 = 1.3580545783042908e-05

Training epoch-52 batch-133
Running loss of epoch-52 batch-133 = 6.959307938814163e-06

Training epoch-52 batch-134
Running loss of epoch-52 batch-134 = 6.410060450434685e-06

Training epoch-52 batch-135
Running loss of epoch-52 batch-135 = 1.3274373486638069e-05

Training epoch-52 batch-136
Running loss of epoch-52 batch-136 = 1.599593088030815e-05

Training epoch-52 batch-137
Running loss of epoch-52 batch-137 = 1.1579599231481552e-05

Training epoch-52 batch-138
Running loss of epoch-52 batch-138 = 4.953006282448769e-06

Training epoch-52 batch-139
Running loss of epoch-52 batch-139 = 9.974464774131775e-06

Training epoch-52 batch-140
Running loss of epoch-52 batch-140 = 4.454748705029488e-06

Training epoch-52 batch-141
Running loss of epoch-52 batch-141 = 7.518799975514412e-06

Training epoch-52 batch-142
Running loss of epoch-52 batch-142 = 1.0184245184063911e-05

Training epoch-52 batch-143
Running loss of epoch-52 batch-143 = 8.969102054834366e-06

Training epoch-52 batch-144
Running loss of epoch-52 batch-144 = 3.0868686735630035e-05

Training epoch-52 batch-145
Running loss of epoch-52 batch-145 = 1.4040153473615646e-05

Training epoch-52 batch-146
Running loss of epoch-52 batch-146 = 7.588416337966919e-06

Training epoch-52 batch-147
Running loss of epoch-52 batch-147 = 6.924616172909737e-06

Training epoch-52 batch-148
Running loss of epoch-52 batch-148 = 9.220326319336891e-06

Training epoch-52 batch-149
Running loss of epoch-52 batch-149 = 1.0434305295348167e-05

Training epoch-52 batch-150
Running loss of epoch-52 batch-150 = 6.4997002482414246e-06

Training epoch-52 batch-151
Running loss of epoch-52 batch-151 = 4.926230758428574e-06

Training epoch-52 batch-152
Running loss of epoch-52 batch-152 = 2.5269342586398125e-05

Training epoch-52 batch-153
Running loss of epoch-52 batch-153 = 1.3453885912895203e-05

Training epoch-52 batch-154
Running loss of epoch-52 batch-154 = 1.0062474757432938e-05

Training epoch-52 batch-155
Running loss of epoch-52 batch-155 = 6.179790943861008e-06

Training epoch-52 batch-156
Running loss of epoch-52 batch-156 = 1.9137980416417122e-05

Training epoch-52 batch-157
Running loss of epoch-52 batch-157 = 3.773719072341919e-05

Finished training epoch-52.



Average train loss at epoch-52 = 1.0982987284660339e-05

Started Evaluation

Average val loss at epoch-52 = 1.6486676984312292

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 89.02 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 64.16 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.69 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 64.10 %

Overall Accuracy = 81.29 %

Finished Evaluation



Started training epoch-53


Training epoch-53 batch-1
Running loss of epoch-53 batch-1 = 6.17210753262043e-06

Training epoch-53 batch-2
Running loss of epoch-53 batch-2 = 4.339031875133514e-06

Training epoch-53 batch-3
Running loss of epoch-53 batch-3 = 3.4943222999572754e-06

Training epoch-53 batch-4
Running loss of epoch-53 batch-4 = 1.6067875549197197e-05

Training epoch-53 batch-5
Running loss of epoch-53 batch-5 = 2.3513799533247948e-05

Training epoch-53 batch-6
Running loss of epoch-53 batch-6 = 1.2887874618172646e-05

Training epoch-53 batch-7
Running loss of epoch-53 batch-7 = 1.397356390953064e-05

Training epoch-53 batch-8
Running loss of epoch-53 batch-8 = 6.473623216152191e-06

Training epoch-53 batch-9
Running loss of epoch-53 batch-9 = 5.259411409497261e-06

Training epoch-53 batch-10
Running loss of epoch-53 batch-10 = 9.24011692404747e-06

Training epoch-53 batch-11
Running loss of epoch-53 batch-11 = 6.090616807341576e-06

Training epoch-53 batch-12
Running loss of epoch-53 batch-12 = 4.6014320105314255e-06

Training epoch-53 batch-13
Running loss of epoch-53 batch-13 = 1.1954223737120628e-05

Training epoch-53 batch-14
Running loss of epoch-53 batch-14 = 9.159324690699577e-06

Training epoch-53 batch-15
Running loss of epoch-53 batch-15 = 7.057329639792442e-06

Training epoch-53 batch-16
Running loss of epoch-53 batch-16 = 9.612413123250008e-06

Training epoch-53 batch-17
Running loss of epoch-53 batch-17 = 1.737242564558983e-05

Training epoch-53 batch-18
Running loss of epoch-53 batch-18 = 9.224051609635353e-06

Training epoch-53 batch-19
Running loss of epoch-53 batch-19 = 1.2369127944111824e-05

Training epoch-53 batch-20
Running loss of epoch-53 batch-20 = 1.0951422154903412e-05

Training epoch-53 batch-21
Running loss of epoch-53 batch-21 = 1.1151190847158432e-05

Training epoch-53 batch-22
Running loss of epoch-53 batch-22 = 7.870839908719063e-06

Training epoch-53 batch-23
Running loss of epoch-53 batch-23 = 6.7229848355054855e-06

Training epoch-53 batch-24
Running loss of epoch-53 batch-24 = 8.05174931883812e-06

Training epoch-53 batch-25
Running loss of epoch-53 batch-25 = 9.270617738366127e-06

Training epoch-53 batch-26
Running loss of epoch-53 batch-26 = 1.2244097888469696e-05

Training epoch-53 batch-27
Running loss of epoch-53 batch-27 = 1.1367024853825569e-05

Training epoch-53 batch-28
Running loss of epoch-53 batch-28 = 1.9717030227184296e-05

Training epoch-53 batch-29
Running loss of epoch-53 batch-29 = 7.112976163625717e-06

Training epoch-53 batch-30
Running loss of epoch-53 batch-30 = 6.754184141755104e-06

Training epoch-53 batch-31
Running loss of epoch-53 batch-31 = 7.722293958067894e-06

Training epoch-53 batch-32
Running loss of epoch-53 batch-32 = 4.5995693653821945e-06

Training epoch-53 batch-33
Running loss of epoch-53 batch-33 = 1.2724194675683975e-05

Training epoch-53 batch-34
Running loss of epoch-53 batch-34 = 9.403564035892487e-06

Training epoch-53 batch-35
Running loss of epoch-53 batch-35 = 9.279698133468628e-06

Training epoch-53 batch-36
Running loss of epoch-53 batch-36 = 5.186069756746292e-06

Training epoch-53 batch-37
Running loss of epoch-53 batch-37 = 5.177687853574753e-06

Training epoch-53 batch-38
Running loss of epoch-53 batch-38 = 2.551102079451084e-05

Training epoch-53 batch-39
Running loss of epoch-53 batch-39 = 9.387033060193062e-06

Training epoch-53 batch-40
Running loss of epoch-53 batch-40 = 6.824498996138573e-06

Training epoch-53 batch-41
Running loss of epoch-53 batch-41 = 6.343238055706024e-06

Training epoch-53 batch-42
Running loss of epoch-53 batch-42 = 7.844530045986176e-06

Training epoch-53 batch-43
Running loss of epoch-53 batch-43 = 7.707392796874046e-06

Training epoch-53 batch-44
Running loss of epoch-53 batch-44 = 1.4657620340585709e-05

Training epoch-53 batch-45
Running loss of epoch-53 batch-45 = 7.939990609884262e-06

Training epoch-53 batch-46
Running loss of epoch-53 batch-46 = 5.995156243443489e-06

Training epoch-53 batch-47
Running loss of epoch-53 batch-47 = 1.5656230971217155e-05

Training epoch-53 batch-48
Running loss of epoch-53 batch-48 = 9.60472971200943e-06

Training epoch-53 batch-49
Running loss of epoch-53 batch-49 = 1.032557338476181e-05

Training epoch-53 batch-50
Running loss of epoch-53 batch-50 = 9.90554690361023e-06

Training epoch-53 batch-51
Running loss of epoch-53 batch-51 = 9.708106517791748e-06

Training epoch-53 batch-52
Running loss of epoch-53 batch-52 = 1.010834239423275e-05

Training epoch-53 batch-53
Running loss of epoch-53 batch-53 = 1.2881821021437645e-05

Training epoch-53 batch-54
Running loss of epoch-53 batch-54 = 1.283339224755764e-05

Training epoch-53 batch-55
Running loss of epoch-53 batch-55 = 1.480034552514553e-05

Training epoch-53 batch-56
Running loss of epoch-53 batch-56 = 8.585629984736443e-06

Training epoch-53 batch-57
Running loss of epoch-53 batch-57 = 1.2342119589447975e-05

Training epoch-53 batch-58
Running loss of epoch-53 batch-58 = 1.0357238352298737e-05

Training epoch-53 batch-59
Running loss of epoch-53 batch-59 = 4.516215994954109e-06

Training epoch-53 batch-60
Running loss of epoch-53 batch-60 = 8.292729035019875e-06

Training epoch-53 batch-61
Running loss of epoch-53 batch-61 = 8.808914572000504e-06

Training epoch-53 batch-62
Running loss of epoch-53 batch-62 = 1.2875301763415337e-05

Training epoch-53 batch-63
Running loss of epoch-53 batch-63 = 6.413552910089493e-06

Training epoch-53 batch-64
Running loss of epoch-53 batch-64 = 3.03611159324646e-06

Training epoch-53 batch-65
Running loss of epoch-53 batch-65 = 9.119976311922073e-06

Training epoch-53 batch-66
Running loss of epoch-53 batch-66 = 3.580469638109207e-06

Training epoch-53 batch-67
Running loss of epoch-53 batch-67 = 1.5707453712821007e-05

Training epoch-53 batch-68
Running loss of epoch-53 batch-68 = 6.3551124185323715e-06

Training epoch-53 batch-69
Running loss of epoch-53 batch-69 = 3.0455412343144417e-05

Training epoch-53 batch-70
Running loss of epoch-53 batch-70 = 4.208879545331001e-06

Training epoch-53 batch-71
Running loss of epoch-53 batch-71 = 9.860377758741379e-06

Training epoch-53 batch-72
Running loss of epoch-53 batch-72 = 7.289927452802658e-06

Training epoch-53 batch-73
Running loss of epoch-53 batch-73 = 1.1270865797996521e-05

Training epoch-53 batch-74
Running loss of epoch-53 batch-74 = 1.0544434189796448e-05

Training epoch-53 batch-75
Running loss of epoch-53 batch-75 = 2.2044405341148376e-06

Training epoch-53 batch-76
Running loss of epoch-53 batch-76 = 1.2238975614309311e-05

Training epoch-53 batch-77
Running loss of epoch-53 batch-77 = 7.247319445014e-06

Training epoch-53 batch-78
Running loss of epoch-53 batch-78 = 1.0786112397909164e-05

Training epoch-53 batch-79
Running loss of epoch-53 batch-79 = 1.5162164345383644e-05

Training epoch-53 batch-80
Running loss of epoch-53 batch-80 = 2.7043279260396957e-06

Training epoch-53 batch-81
Running loss of epoch-53 batch-81 = 4.138564690947533e-06

Training epoch-53 batch-82
Running loss of epoch-53 batch-82 = 9.485287591814995e-06

Training epoch-53 batch-83
Running loss of epoch-53 batch-83 = 4.987930878996849e-06

Training epoch-53 batch-84
Running loss of epoch-53 batch-84 = 1.7249025404453278e-05

Training epoch-53 batch-85
Running loss of epoch-53 batch-85 = 1.6077188774943352e-05

Training epoch-53 batch-86
Running loss of epoch-53 batch-86 = 1.67209655046463e-05

Training epoch-53 batch-87
Running loss of epoch-53 batch-87 = 1.1770753189921379e-05

Training epoch-53 batch-88
Running loss of epoch-53 batch-88 = 7.247086614370346e-06

Training epoch-53 batch-89
Running loss of epoch-53 batch-89 = 7.212487980723381e-05

Training epoch-53 batch-90
Running loss of epoch-53 batch-90 = 8.168164640665054e-06

Training epoch-53 batch-91
Running loss of epoch-53 batch-91 = 9.045703336596489e-06

Training epoch-53 batch-92
Running loss of epoch-53 batch-92 = 1.202896237373352e-05

Training epoch-53 batch-93
Running loss of epoch-53 batch-93 = 5.702720955014229e-06

Training epoch-53 batch-94
Running loss of epoch-53 batch-94 = 7.670372724533081e-06

Training epoch-53 batch-95
Running loss of epoch-53 batch-95 = 6.094807758927345e-06

Training epoch-53 batch-96
Running loss of epoch-53 batch-96 = 1.7251353710889816e-05

Training epoch-53 batch-97
Running loss of epoch-53 batch-97 = 9.556533768773079e-06

Training epoch-53 batch-98
Running loss of epoch-53 batch-98 = 5.297595635056496e-06

Training epoch-53 batch-99
Running loss of epoch-53 batch-99 = 7.193302735686302e-06

Training epoch-53 batch-100
Running loss of epoch-53 batch-100 = 7.594004273414612e-06

Training epoch-53 batch-101
Running loss of epoch-53 batch-101 = 5.402602255344391e-06

Training epoch-53 batch-102
Running loss of epoch-53 batch-102 = 1.1167488992214203e-05

Training epoch-53 batch-103
Running loss of epoch-53 batch-103 = 4.217959940433502e-06

Training epoch-53 batch-104
Running loss of epoch-53 batch-104 = 1.405365765094757e-05

Training epoch-53 batch-105
Running loss of epoch-53 batch-105 = 2.9620714485645294e-06

Training epoch-53 batch-106
Running loss of epoch-53 batch-106 = 4.9015507102012634e-06

Training epoch-53 batch-107
Running loss of epoch-53 batch-107 = 1.984252594411373e-05

Training epoch-53 batch-108
Running loss of epoch-53 batch-108 = 9.251059964299202e-06

Training epoch-53 batch-109
Running loss of epoch-53 batch-109 = 1.719919964671135e-05

Training epoch-53 batch-110
Running loss of epoch-53 batch-110 = 8.118338882923126e-06

Training epoch-53 batch-111
Running loss of epoch-53 batch-111 = 1.140497624874115e-05

Training epoch-53 batch-112
Running loss of epoch-53 batch-112 = 6.461283192038536e-06

Training epoch-53 batch-113
Running loss of epoch-53 batch-113 = 7.2142574936151505e-06

Training epoch-53 batch-114
Running loss of epoch-53 batch-114 = 1.1299271136522293e-05

Training epoch-53 batch-115
Running loss of epoch-53 batch-115 = 2.4985289201140404e-05

Training epoch-53 batch-116
Running loss of epoch-53 batch-116 = 1.1670868843793869e-05

Training epoch-53 batch-117
Running loss of epoch-53 batch-117 = 7.844297215342522e-06

Training epoch-53 batch-118
Running loss of epoch-53 batch-118 = 4.9371737986803055e-06

Training epoch-53 batch-119
Running loss of epoch-53 batch-119 = 9.693321771919727e-06

Training epoch-53 batch-120
Running loss of epoch-53 batch-120 = 6.35022297501564e-06

Training epoch-53 batch-121
Running loss of epoch-53 batch-121 = 4.716450348496437e-06

Training epoch-53 batch-122
Running loss of epoch-53 batch-122 = 2.614804543554783e-05

Training epoch-53 batch-123
Running loss of epoch-53 batch-123 = 1.286529004573822e-05

Training epoch-53 batch-124
Running loss of epoch-53 batch-124 = 3.949273377656937e-06

Training epoch-53 batch-125
Running loss of epoch-53 batch-125 = 1.177145168185234e-05

Training epoch-53 batch-126
Running loss of epoch-53 batch-126 = 6.848946213722229e-06

Training epoch-53 batch-127
Running loss of epoch-53 batch-127 = 9.600305929780006e-06

Training epoch-53 batch-128
Running loss of epoch-53 batch-128 = 2.078479155898094e-06

Training epoch-53 batch-129
Running loss of epoch-53 batch-129 = 1.4764023944735527e-05

Training epoch-53 batch-130
Running loss of epoch-53 batch-130 = 6.307382136583328e-06

Training epoch-53 batch-131
Running loss of epoch-53 batch-131 = 4.2943283915519714e-06

Training epoch-53 batch-132
Running loss of epoch-53 batch-132 = 6.0372985899448395e-06

Training epoch-53 batch-133
Running loss of epoch-53 batch-133 = 2.270773984491825e-05

Training epoch-53 batch-134
Running loss of epoch-53 batch-134 = 9.872019290924072e-06

Training epoch-53 batch-135
Running loss of epoch-53 batch-135 = 6.927410140633583e-06

Training epoch-53 batch-136
Running loss of epoch-53 batch-136 = 4.815869033336639e-06

Training epoch-53 batch-137
Running loss of epoch-53 batch-137 = 6.241723895072937e-06

Training epoch-53 batch-138
Running loss of epoch-53 batch-138 = 2.3746397346258163e-06

Training epoch-53 batch-139
Running loss of epoch-53 batch-139 = 8.369330316781998e-06

Training epoch-53 batch-140
Running loss of epoch-53 batch-140 = 2.9912451282143593e-05

Training epoch-53 batch-141
Running loss of epoch-53 batch-141 = 7.112743332982063e-06

Training epoch-53 batch-142
Running loss of epoch-53 batch-142 = 1.4530261978507042e-05

Training epoch-53 batch-143
Running loss of epoch-53 batch-143 = 2.1285610273480415e-05

Training epoch-53 batch-144
Running loss of epoch-53 batch-144 = 1.3296026736497879e-05

Training epoch-53 batch-145
Running loss of epoch-53 batch-145 = 1.3498123735189438e-05

Training epoch-53 batch-146
Running loss of epoch-53 batch-146 = 9.909737855195999e-06

Training epoch-53 batch-147
Running loss of epoch-53 batch-147 = 5.359062924981117e-06

Training epoch-53 batch-148
Running loss of epoch-53 batch-148 = 6.909947842359543e-06

Training epoch-53 batch-149
Running loss of epoch-53 batch-149 = 7.953960448503494e-06

Training epoch-53 batch-150
Running loss of epoch-53 batch-150 = 5.7211145758628845e-06

Training epoch-53 batch-151
Running loss of epoch-53 batch-151 = 8.017057552933693e-06

Training epoch-53 batch-152
Running loss of epoch-53 batch-152 = 3.2014213502407074e-06

Training epoch-53 batch-153
Running loss of epoch-53 batch-153 = 1.3155164197087288e-05

Training epoch-53 batch-154
Running loss of epoch-53 batch-154 = 8.433591574430466e-06

Training epoch-53 batch-155
Running loss of epoch-53 batch-155 = 7.0156529545784e-06

Training epoch-53 batch-156
Running loss of epoch-53 batch-156 = 5.277339369058609e-06

Training epoch-53 batch-157
Running loss of epoch-53 batch-157 = 3.6656856536865234e-06

Finished training epoch-53.



Average train loss at epoch-53 = 1.0193579643964768e-05

Started Evaluation

Average val loss at epoch-53 = 1.6712852507628317

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 88.70 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 52.61 %
Accuracy for class get is: 62.31 %

Overall Accuracy = 81.08 %

Finished Evaluation



Started training epoch-54


Training epoch-54 batch-1
Running loss of epoch-54 batch-1 = 6.357673555612564e-06

Training epoch-54 batch-2
Running loss of epoch-54 batch-2 = 5.138805136084557e-06

Training epoch-54 batch-3
Running loss of epoch-54 batch-3 = 4.071276634931564e-05

Training epoch-54 batch-4
Running loss of epoch-54 batch-4 = 7.443828508257866e-06

Training epoch-54 batch-5
Running loss of epoch-54 batch-5 = 1.4787772670388222e-05

Training epoch-54 batch-6
Running loss of epoch-54 batch-6 = 5.2677933126688e-06

Training epoch-54 batch-7
Running loss of epoch-54 batch-7 = 7.54697248339653e-06

Training epoch-54 batch-8
Running loss of epoch-54 batch-8 = 1.0216142982244492e-05

Training epoch-54 batch-9
Running loss of epoch-54 batch-9 = 2.3895641788840294e-05

Training epoch-54 batch-10
Running loss of epoch-54 batch-10 = 1.9264640286564827e-05

Training epoch-54 batch-11
Running loss of epoch-54 batch-11 = 5.1283277571201324e-06

Training epoch-54 batch-12
Running loss of epoch-54 batch-12 = 1.5870435163378716e-05

Training epoch-54 batch-13
Running loss of epoch-54 batch-13 = 1.2849457561969757e-05

Training epoch-54 batch-14
Running loss of epoch-54 batch-14 = 3.47895547747612e-06

Training epoch-54 batch-15
Running loss of epoch-54 batch-15 = 6.732530891895294e-06

Training epoch-54 batch-16
Running loss of epoch-54 batch-16 = 4.646601155400276e-06

Training epoch-54 batch-17
Running loss of epoch-54 batch-17 = 7.847091183066368e-06

Training epoch-54 batch-18
Running loss of epoch-54 batch-18 = 9.142095223069191e-06

Training epoch-54 batch-19
Running loss of epoch-54 batch-19 = 1.1180993169546127e-05

Training epoch-54 batch-20
Running loss of epoch-54 batch-20 = 1.0776566341519356e-05

Training epoch-54 batch-21
Running loss of epoch-54 batch-21 = 3.252178430557251e-06

Training epoch-54 batch-22
Running loss of epoch-54 batch-22 = 1.4809193089604378e-05

Training epoch-54 batch-23
Running loss of epoch-54 batch-23 = 7.386785000562668e-06

Training epoch-54 batch-24
Running loss of epoch-54 batch-24 = 7.051276043057442e-06

Training epoch-54 batch-25
Running loss of epoch-54 batch-25 = 9.001465514302254e-06

Training epoch-54 batch-26
Running loss of epoch-54 batch-26 = 3.0989758670330048e-06

Training epoch-54 batch-27
Running loss of epoch-54 batch-27 = 1.1724419891834259e-05

Training epoch-54 batch-28
Running loss of epoch-54 batch-28 = 3.7422869354486465e-06

Training epoch-54 batch-29
Running loss of epoch-54 batch-29 = 7.724622264504433e-06

Training epoch-54 batch-30
Running loss of epoch-54 batch-30 = 1.3974960893392563e-05

Training epoch-54 batch-31
Running loss of epoch-54 batch-31 = 9.919283911585808e-06

Training epoch-54 batch-32
Running loss of epoch-54 batch-32 = 9.113224223256111e-06

Training epoch-54 batch-33
Running loss of epoch-54 batch-33 = 1.4197314158082008e-05

Training epoch-54 batch-34
Running loss of epoch-54 batch-34 = 5.871988832950592e-06

Training epoch-54 batch-35
Running loss of epoch-54 batch-35 = 4.311790689826012e-06

Training epoch-54 batch-36
Running loss of epoch-54 batch-36 = 7.39051029086113e-06

Training epoch-54 batch-37
Running loss of epoch-54 batch-37 = 7.668044418096542e-06

Training epoch-54 batch-38
Running loss of epoch-54 batch-38 = 2.668472006917e-06

Training epoch-54 batch-39
Running loss of epoch-54 batch-39 = 7.705530151724815e-06

Training epoch-54 batch-40
Running loss of epoch-54 batch-40 = 6.52531161904335e-06

Training epoch-54 batch-41
Running loss of epoch-54 batch-41 = 5.6438148021698e-06

Training epoch-54 batch-42
Running loss of epoch-54 batch-42 = 6.238231435418129e-06

Training epoch-54 batch-43
Running loss of epoch-54 batch-43 = 6.134388968348503e-06

Training epoch-54 batch-44
Running loss of epoch-54 batch-44 = 1.6559381037950516e-05

Training epoch-54 batch-45
Running loss of epoch-54 batch-45 = 1.2784963473677635e-05

Training epoch-54 batch-46
Running loss of epoch-54 batch-46 = 3.271503373980522e-06

Training epoch-54 batch-47
Running loss of epoch-54 batch-47 = 2.2114254534244537e-06

Training epoch-54 batch-48
Running loss of epoch-54 batch-48 = 1.1930707842111588e-05

Training epoch-54 batch-49
Running loss of epoch-54 batch-49 = 2.3486092686653137e-05

Training epoch-54 batch-50
Running loss of epoch-54 batch-50 = 3.5888515412807465e-06

Training epoch-54 batch-51
Running loss of epoch-54 batch-51 = 4.3031759560108185e-06

Training epoch-54 batch-52
Running loss of epoch-54 batch-52 = 8.73231329023838e-06

Training epoch-54 batch-53
Running loss of epoch-54 batch-53 = 9.080162271857262e-06

Training epoch-54 batch-54
Running loss of epoch-54 batch-54 = 1.230812631547451e-05

Training epoch-54 batch-55
Running loss of epoch-54 batch-55 = 3.344763536006212e-05

Training epoch-54 batch-56
Running loss of epoch-54 batch-56 = 1.2942356988787651e-05

Training epoch-54 batch-57
Running loss of epoch-54 batch-57 = 4.952307790517807e-06

Training epoch-54 batch-58
Running loss of epoch-54 batch-58 = 6.183981895446777e-06

Training epoch-54 batch-59
Running loss of epoch-54 batch-59 = 2.1438347175717354e-05

Training epoch-54 batch-60
Running loss of epoch-54 batch-60 = 1.2894859537482262e-05

Training epoch-54 batch-61
Running loss of epoch-54 batch-61 = 7.0338137447834015e-06

Training epoch-54 batch-62
Running loss of epoch-54 batch-62 = 1.3919780030846596e-05

Training epoch-54 batch-63
Running loss of epoch-54 batch-63 = 4.799338057637215e-06

Training epoch-54 batch-64
Running loss of epoch-54 batch-64 = 4.458613693714142e-05

Training epoch-54 batch-65
Running loss of epoch-54 batch-65 = 6.950227543711662e-06

Training epoch-54 batch-66
Running loss of epoch-54 batch-66 = 2.4817418307065964e-06

Training epoch-54 batch-67
Running loss of epoch-54 batch-67 = 9.737443178892136e-06

Training epoch-54 batch-68
Running loss of epoch-54 batch-68 = 1.2150965631008148e-05

Training epoch-54 batch-69
Running loss of epoch-54 batch-69 = 2.521788701415062e-06

Training epoch-54 batch-70
Running loss of epoch-54 batch-70 = 1.6737263649702072e-05

Training epoch-54 batch-71
Running loss of epoch-54 batch-71 = 1.2000324204564095e-05

Training epoch-54 batch-72
Running loss of epoch-54 batch-72 = 4.69735823571682e-06

Training epoch-54 batch-73
Running loss of epoch-54 batch-73 = 8.465489372611046e-06

Training epoch-54 batch-74
Running loss of epoch-54 batch-74 = 6.449175998568535e-06

Training epoch-54 batch-75
Running loss of epoch-54 batch-75 = 5.7194847613573074e-06

Training epoch-54 batch-76
Running loss of epoch-54 batch-76 = 1.1754222214221954e-05

Training epoch-54 batch-77
Running loss of epoch-54 batch-77 = 1.3273674994707108e-05

Training epoch-54 batch-78
Running loss of epoch-54 batch-78 = 6.932765245437622e-06

Training epoch-54 batch-79
Running loss of epoch-54 batch-79 = 1.99067872017622e-05

Training epoch-54 batch-80
Running loss of epoch-54 batch-80 = 5.479436367750168e-06

Training epoch-54 batch-81
Running loss of epoch-54 batch-81 = 6.932998076081276e-06

Training epoch-54 batch-82
Running loss of epoch-54 batch-82 = 7.123686373233795e-06

Training epoch-54 batch-83
Running loss of epoch-54 batch-83 = 9.462470188736916e-06

Training epoch-54 batch-84
Running loss of epoch-54 batch-84 = 3.7783756852149963e-06

Training epoch-54 batch-85
Running loss of epoch-54 batch-85 = 9.438721463084221e-06

Training epoch-54 batch-86
Running loss of epoch-54 batch-86 = 1.1428957805037498e-05

Training epoch-54 batch-87
Running loss of epoch-54 batch-87 = 1.0483432561159134e-05

Training epoch-54 batch-88
Running loss of epoch-54 batch-88 = 7.874099537730217e-06

Training epoch-54 batch-89
Running loss of epoch-54 batch-89 = 9.890180081129074e-06

Training epoch-54 batch-90
Running loss of epoch-54 batch-90 = 3.3713877201080322e-06

Training epoch-54 batch-91
Running loss of epoch-54 batch-91 = 1.7664860934019089e-06

Training epoch-54 batch-92
Running loss of epoch-54 batch-92 = 7.133232429623604e-06

Training epoch-54 batch-93
Running loss of epoch-54 batch-93 = 8.126022294163704e-06

Training epoch-54 batch-94
Running loss of epoch-54 batch-94 = 2.974853850901127e-05

Training epoch-54 batch-95
Running loss of epoch-54 batch-95 = 6.57280907034874e-06

Training epoch-54 batch-96
Running loss of epoch-54 batch-96 = 2.5192275643348694e-06

Training epoch-54 batch-97
Running loss of epoch-54 batch-97 = 2.6400433853268623e-05

Training epoch-54 batch-98
Running loss of epoch-54 batch-98 = 7.797731086611748e-06

Training epoch-54 batch-99
Running loss of epoch-54 batch-99 = 5.256151780486107e-06

Training epoch-54 batch-100
Running loss of epoch-54 batch-100 = 6.683170795440674e-06

Training epoch-54 batch-101
Running loss of epoch-54 batch-101 = 1.0621501132845879e-05

Training epoch-54 batch-102
Running loss of epoch-54 batch-102 = 1.184060238301754e-05

Training epoch-54 batch-103
Running loss of epoch-54 batch-103 = 1.3545854017138481e-05

Training epoch-54 batch-104
Running loss of epoch-54 batch-104 = 1.9722850993275642e-05

Training epoch-54 batch-105
Running loss of epoch-54 batch-105 = 2.0093051716685295e-05

Training epoch-54 batch-106
Running loss of epoch-54 batch-106 = 3.507593646645546e-06

Training epoch-54 batch-107
Running loss of epoch-54 batch-107 = 9.622657671570778e-06

Training epoch-54 batch-108
Running loss of epoch-54 batch-108 = 1.0603107511997223e-05

Training epoch-54 batch-109
Running loss of epoch-54 batch-109 = 7.780501618981361e-06

Training epoch-54 batch-110
Running loss of epoch-54 batch-110 = 3.0854716897010803e-06

Training epoch-54 batch-111
Running loss of epoch-54 batch-111 = 1.3799872249364853e-05

Training epoch-54 batch-112
Running loss of epoch-54 batch-112 = 1.0461779311299324e-05

Training epoch-54 batch-113
Running loss of epoch-54 batch-113 = 8.16071406006813e-06

Training epoch-54 batch-114
Running loss of epoch-54 batch-114 = 7.788185030221939e-06

Training epoch-54 batch-115
Running loss of epoch-54 batch-115 = 6.217043846845627e-06

Training epoch-54 batch-116
Running loss of epoch-54 batch-116 = 6.106682121753693e-06

Training epoch-54 batch-117
Running loss of epoch-54 batch-117 = 1.6784761101007462e-06

Training epoch-54 batch-118
Running loss of epoch-54 batch-118 = 6.860820576548576e-06

Training epoch-54 batch-119
Running loss of epoch-54 batch-119 = 8.356990292668343e-06

Training epoch-54 batch-120
Running loss of epoch-54 batch-120 = 7.202150300145149e-06

Training epoch-54 batch-121
Running loss of epoch-54 batch-121 = 1.1232448741793633e-05

Training epoch-54 batch-122
Running loss of epoch-54 batch-122 = 7.68899917602539e-06

Training epoch-54 batch-123
Running loss of epoch-54 batch-123 = 1.0547693818807602e-05

Training epoch-54 batch-124
Running loss of epoch-54 batch-124 = 1.302175223827362e-05

Training epoch-54 batch-125
Running loss of epoch-54 batch-125 = 4.2852479964494705e-06

Training epoch-54 batch-126
Running loss of epoch-54 batch-126 = 3.2850075513124466e-06

Training epoch-54 batch-127
Running loss of epoch-54 batch-127 = 4.791188985109329e-06

Training epoch-54 batch-128
Running loss of epoch-54 batch-128 = 6.7015644162893295e-06

Training epoch-54 batch-129
Running loss of epoch-54 batch-129 = 8.34418460726738e-06

Training epoch-54 batch-130
Running loss of epoch-54 batch-130 = 1.2350967153906822e-05

Training epoch-54 batch-131
Running loss of epoch-54 batch-131 = 1.570815220475197e-05

Training epoch-54 batch-132
Running loss of epoch-54 batch-132 = 4.683155566453934e-06

Training epoch-54 batch-133
Running loss of epoch-54 batch-133 = 6.14020973443985e-06

Training epoch-54 batch-134
Running loss of epoch-54 batch-134 = 8.669914677739143e-06

Training epoch-54 batch-135
Running loss of epoch-54 batch-135 = 1.6793375834822655e-05

Training epoch-54 batch-136
Running loss of epoch-54 batch-136 = 8.107395842671394e-06

Training epoch-54 batch-137
Running loss of epoch-54 batch-137 = 7.771886885166168e-06

Training epoch-54 batch-138
Running loss of epoch-54 batch-138 = 2.9236776754260063e-05

Training epoch-54 batch-139
Running loss of epoch-54 batch-139 = 3.437511622905731e-06

Training epoch-54 batch-140
Running loss of epoch-54 batch-140 = 5.820766091346741e-06

Training epoch-54 batch-141
Running loss of epoch-54 batch-141 = 4.765111953020096e-06

Training epoch-54 batch-142
Running loss of epoch-54 batch-142 = 1.1801952496170998e-05

Training epoch-54 batch-143
Running loss of epoch-54 batch-143 = 1.2161675840616226e-05

Training epoch-54 batch-144
Running loss of epoch-54 batch-144 = 1.516309566795826e-05

Training epoch-54 batch-145
Running loss of epoch-54 batch-145 = 7.006106898188591e-06

Training epoch-54 batch-146
Running loss of epoch-54 batch-146 = 9.765848517417908e-06

Training epoch-54 batch-147
Running loss of epoch-54 batch-147 = 5.349516868591309e-06

Training epoch-54 batch-148
Running loss of epoch-54 batch-148 = 1.6970792785286903e-05

Training epoch-54 batch-149
Running loss of epoch-54 batch-149 = 7.1802642196416855e-06

Training epoch-54 batch-150
Running loss of epoch-54 batch-150 = 1.2280303053557873e-05

Training epoch-54 batch-151
Running loss of epoch-54 batch-151 = 9.902752935886383e-06

Training epoch-54 batch-152
Running loss of epoch-54 batch-152 = 1.2189848348498344e-05

Training epoch-54 batch-153
Running loss of epoch-54 batch-153 = 8.181901648640633e-06

Training epoch-54 batch-154
Running loss of epoch-54 batch-154 = 1.1298339813947678e-05

Training epoch-54 batch-155
Running loss of epoch-54 batch-155 = 1.0375166311860085e-05

Training epoch-54 batch-156
Running loss of epoch-54 batch-156 = 6.50365836918354e-06

Training epoch-54 batch-157
Running loss of epoch-54 batch-157 = 6.861984729766846e-05

Finished training epoch-54.



Average train loss at epoch-54 = 9.949862957000733e-06

Started Evaluation

Average val loss at epoch-54 = 1.6657958395871646

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.34 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 63.47 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 52.91 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-55


Training epoch-55 batch-1
Running loss of epoch-55 batch-1 = 1.991167664527893e-05

Training epoch-55 batch-2
Running loss of epoch-55 batch-2 = 1.2942356988787651e-05

Training epoch-55 batch-3
Running loss of epoch-55 batch-3 = 6.295274943113327e-06

Training epoch-55 batch-4
Running loss of epoch-55 batch-4 = 3.593042492866516e-06

Training epoch-55 batch-5
Running loss of epoch-55 batch-5 = 3.7471530959010124e-05

Training epoch-55 batch-6
Running loss of epoch-55 batch-6 = 5.334150046110153e-06

Training epoch-55 batch-7
Running loss of epoch-55 batch-7 = 1.399102620780468e-05

Training epoch-55 batch-8
Running loss of epoch-55 batch-8 = 1.0197050869464874e-05

Training epoch-55 batch-9
Running loss of epoch-55 batch-9 = 1.1641765013337135e-05

Training epoch-55 batch-10
Running loss of epoch-55 batch-10 = 1.1482276022434235e-05

Training epoch-55 batch-11
Running loss of epoch-55 batch-11 = 1.3165641576051712e-05

Training epoch-55 batch-12
Running loss of epoch-55 batch-12 = 8.244998753070831e-06

Training epoch-55 batch-13
Running loss of epoch-55 batch-13 = 5.067791789770126e-06

Training epoch-55 batch-14
Running loss of epoch-55 batch-14 = 2.590380609035492e-05

Training epoch-55 batch-15
Running loss of epoch-55 batch-15 = 6.0070306062698364e-06

Training epoch-55 batch-16
Running loss of epoch-55 batch-16 = 3.3649150282144547e-05

Training epoch-55 batch-17
Running loss of epoch-55 batch-17 = 4.08547930419445e-06

Training epoch-55 batch-18
Running loss of epoch-55 batch-18 = 8.875271305441856e-06

Training epoch-55 batch-19
Running loss of epoch-55 batch-19 = 2.65615526586771e-05

Training epoch-55 batch-20
Running loss of epoch-55 batch-20 = 6.828922778367996e-06

Training epoch-55 batch-21
Running loss of epoch-55 batch-21 = 5.159061402082443e-06

Training epoch-55 batch-22
Running loss of epoch-55 batch-22 = 4.433095455169678e-06

Training epoch-55 batch-23
Running loss of epoch-55 batch-23 = 6.583984941244125e-06

Training epoch-55 batch-24
Running loss of epoch-55 batch-24 = 3.793695941567421e-05

Training epoch-55 batch-25
Running loss of epoch-55 batch-25 = 6.145332008600235e-06

Training epoch-55 batch-26
Running loss of epoch-55 batch-26 = 6.845686584711075e-06

Training epoch-55 batch-27
Running loss of epoch-55 batch-27 = 1.599593088030815e-05

Training epoch-55 batch-28
Running loss of epoch-55 batch-28 = 6.596092134714127e-06

Training epoch-55 batch-29
Running loss of epoch-55 batch-29 = 2.1489104256033897e-05

Training epoch-55 batch-30
Running loss of epoch-55 batch-30 = 3.534136340022087e-06

Training epoch-55 batch-31
Running loss of epoch-55 batch-31 = 1.1323252692818642e-05

Training epoch-55 batch-32
Running loss of epoch-55 batch-32 = 3.0740629881620407e-06

Training epoch-55 batch-33
Running loss of epoch-55 batch-33 = 6.3641928136348724e-06

Training epoch-55 batch-34
Running loss of epoch-55 batch-34 = 6.9749075919389725e-06

Training epoch-55 batch-35
Running loss of epoch-55 batch-35 = 7.187016308307648e-06

Training epoch-55 batch-36
Running loss of epoch-55 batch-36 = 6.400514394044876e-06

Training epoch-55 batch-37
Running loss of epoch-55 batch-37 = 1.4521880075335503e-05

Training epoch-55 batch-38
Running loss of epoch-55 batch-38 = 1.7148442566394806e-05

Training epoch-55 batch-39
Running loss of epoch-55 batch-39 = 1.0088318958878517e-05

Training epoch-55 batch-40
Running loss of epoch-55 batch-40 = 9.520212188363075e-06

Training epoch-55 batch-41
Running loss of epoch-55 batch-41 = 4.769070073962212e-06

Training epoch-55 batch-42
Running loss of epoch-55 batch-42 = 1.1478317901492119e-05

Training epoch-55 batch-43
Running loss of epoch-55 batch-43 = 1.4885794371366501e-05

Training epoch-55 batch-44
Running loss of epoch-55 batch-44 = 2.3867469280958176e-06

Training epoch-55 batch-45
Running loss of epoch-55 batch-45 = 6.016809493303299e-06

Training epoch-55 batch-46
Running loss of epoch-55 batch-46 = 5.1336828619241714e-06

Training epoch-55 batch-47
Running loss of epoch-55 batch-47 = 3.715045750141144e-06

Training epoch-55 batch-48
Running loss of epoch-55 batch-48 = 1.844647340476513e-05

Training epoch-55 batch-49
Running loss of epoch-55 batch-49 = 1.157890073955059e-05

Training epoch-55 batch-50
Running loss of epoch-55 batch-50 = 4.311325028538704e-06

Training epoch-55 batch-51
Running loss of epoch-55 batch-51 = 5.1658134907484055e-06

Training epoch-55 batch-52
Running loss of epoch-55 batch-52 = 7.271533831954002e-06

Training epoch-55 batch-53
Running loss of epoch-55 batch-53 = 3.1795352697372437e-06

Training epoch-55 batch-54
Running loss of epoch-55 batch-54 = 5.403067916631699e-06

Training epoch-55 batch-55
Running loss of epoch-55 batch-55 = 1.1250609531998634e-05

Training epoch-55 batch-56
Running loss of epoch-55 batch-56 = 3.4740660339593887e-06

Training epoch-55 batch-57
Running loss of epoch-55 batch-57 = 7.231719791889191e-06

Training epoch-55 batch-58
Running loss of epoch-55 batch-58 = 1.288275234401226e-05

Training epoch-55 batch-59
Running loss of epoch-55 batch-59 = 5.222158506512642e-06

Training epoch-55 batch-60
Running loss of epoch-55 batch-60 = 9.258044883608818e-06

Training epoch-55 batch-61
Running loss of epoch-55 batch-61 = 3.974186256527901e-06

Training epoch-55 batch-62
Running loss of epoch-55 batch-62 = 1.0317424312233925e-05

Training epoch-55 batch-63
Running loss of epoch-55 batch-63 = 6.307614967226982e-06

Training epoch-55 batch-64
Running loss of epoch-55 batch-64 = 9.740469977259636e-06

Training epoch-55 batch-65
Running loss of epoch-55 batch-65 = 9.234994649887085e-06

Training epoch-55 batch-66
Running loss of epoch-55 batch-66 = 1.0231509804725647e-05

Training epoch-55 batch-67
Running loss of epoch-55 batch-67 = 8.39470885694027e-06

Training epoch-55 batch-68
Running loss of epoch-55 batch-68 = 6.751855835318565e-06

Training epoch-55 batch-69
Running loss of epoch-55 batch-69 = 4.15765680372715e-06

Training epoch-55 batch-70
Running loss of epoch-55 batch-70 = 1.452234573662281e-05

Training epoch-55 batch-71
Running loss of epoch-55 batch-71 = 5.548819899559021e-06

Training epoch-55 batch-72
Running loss of epoch-55 batch-72 = 1.0101590305566788e-05

Training epoch-55 batch-73
Running loss of epoch-55 batch-73 = 4.473375156521797e-06

Training epoch-55 batch-74
Running loss of epoch-55 batch-74 = 3.0046794563531876e-06

Training epoch-55 batch-75
Running loss of epoch-55 batch-75 = 4.8889778554439545e-06

Training epoch-55 batch-76
Running loss of epoch-55 batch-76 = 1.0271789506077766e-05

Training epoch-55 batch-77
Running loss of epoch-55 batch-77 = 3.937631845474243e-06

Training epoch-55 batch-78
Running loss of epoch-55 batch-78 = 5.377223715186119e-06

Training epoch-55 batch-79
Running loss of epoch-55 batch-79 = 4.301546141505241e-06

Training epoch-55 batch-80
Running loss of epoch-55 batch-80 = 5.261506885290146e-06

Training epoch-55 batch-81
Running loss of epoch-55 batch-81 = 6.484100595116615e-06

Training epoch-55 batch-82
Running loss of epoch-55 batch-82 = 7.35442154109478e-06

Training epoch-55 batch-83
Running loss of epoch-55 batch-83 = 1.667160540819168e-05

Training epoch-55 batch-84
Running loss of epoch-55 batch-84 = 1.0874355211853981e-05

Training epoch-55 batch-85
Running loss of epoch-55 batch-85 = 1.3216864317655563e-05

Training epoch-55 batch-86
Running loss of epoch-55 batch-86 = 4.542991518974304e-06

Training epoch-55 batch-87
Running loss of epoch-55 batch-87 = 5.6694261729717255e-06

Training epoch-55 batch-88
Running loss of epoch-55 batch-88 = 1.1659227311611176e-05

Training epoch-55 batch-89
Running loss of epoch-55 batch-89 = 9.19075682759285e-06

Training epoch-55 batch-90
Running loss of epoch-55 batch-90 = 3.870343789458275e-06

Training epoch-55 batch-91
Running loss of epoch-55 batch-91 = 1.2821285054087639e-05

Training epoch-55 batch-92
Running loss of epoch-55 batch-92 = 4.585599526762962e-06

Training epoch-55 batch-93
Running loss of epoch-55 batch-93 = 1.1108117178082466e-05

Training epoch-55 batch-94
Running loss of epoch-55 batch-94 = 3.3115502446889877e-06

Training epoch-55 batch-95
Running loss of epoch-55 batch-95 = 6.174203008413315e-06

Training epoch-55 batch-96
Running loss of epoch-55 batch-96 = 8.362345397472382e-06

Training epoch-55 batch-97
Running loss of epoch-55 batch-97 = 5.019595846533775e-06

Training epoch-55 batch-98
Running loss of epoch-55 batch-98 = 1.164805144071579e-05

Training epoch-55 batch-99
Running loss of epoch-55 batch-99 = 2.3960601538419724e-06

Training epoch-55 batch-100
Running loss of epoch-55 batch-100 = 2.716854214668274e-05

Training epoch-55 batch-101
Running loss of epoch-55 batch-101 = 8.305767551064491e-06

Training epoch-55 batch-102
Running loss of epoch-55 batch-102 = 9.174691513180733e-06

Training epoch-55 batch-103
Running loss of epoch-55 batch-103 = 6.252899765968323e-06

Training epoch-55 batch-104
Running loss of epoch-55 batch-104 = 3.7883874028921127e-06

Training epoch-55 batch-105
Running loss of epoch-55 batch-105 = 6.0335732996463776e-06

Training epoch-55 batch-106
Running loss of epoch-55 batch-106 = 7.952097803354263e-06

Training epoch-55 batch-107
Running loss of epoch-55 batch-107 = 1.1865515261888504e-05

Training epoch-55 batch-108
Running loss of epoch-55 batch-108 = 1.0976102203130722e-05

Training epoch-55 batch-109
Running loss of epoch-55 batch-109 = 4.364177584648132e-06

Training epoch-55 batch-110
Running loss of epoch-55 batch-110 = 2.703862264752388e-06

Training epoch-55 batch-111
Running loss of epoch-55 batch-111 = 7.659196853637695e-06

Training epoch-55 batch-112
Running loss of epoch-55 batch-112 = 3.132270649075508e-06

Training epoch-55 batch-113
Running loss of epoch-55 batch-113 = 1.3503246009349823e-05

Training epoch-55 batch-114
Running loss of epoch-55 batch-114 = 1.024431549012661e-05

Training epoch-55 batch-115
Running loss of epoch-55 batch-115 = 8.508563041687012e-06

Training epoch-55 batch-116
Running loss of epoch-55 batch-116 = 1.4737015590071678e-05

Training epoch-55 batch-117
Running loss of epoch-55 batch-117 = 4.492700099945068e-06

Training epoch-55 batch-118
Running loss of epoch-55 batch-118 = 4.004687070846558e-06

Training epoch-55 batch-119
Running loss of epoch-55 batch-119 = 6.349291652441025e-06

Training epoch-55 batch-120
Running loss of epoch-55 batch-120 = 4.165107384324074e-06

Training epoch-55 batch-121
Running loss of epoch-55 batch-121 = 9.310198947787285e-06

Training epoch-55 batch-122
Running loss of epoch-55 batch-122 = 1.8650200217962265e-05

Training epoch-55 batch-123
Running loss of epoch-55 batch-123 = 6.709014996886253e-06

Training epoch-55 batch-124
Running loss of epoch-55 batch-124 = 1.1265743523836136e-05

Training epoch-55 batch-125
Running loss of epoch-55 batch-125 = 1.298985444009304e-05

Training epoch-55 batch-126
Running loss of epoch-55 batch-126 = 6.556045264005661e-06

Training epoch-55 batch-127
Running loss of epoch-55 batch-127 = 5.53252175450325e-06

Training epoch-55 batch-128
Running loss of epoch-55 batch-128 = 1.4433171600103378e-05

Training epoch-55 batch-129
Running loss of epoch-55 batch-129 = 1.1845026165246964e-05

Training epoch-55 batch-130
Running loss of epoch-55 batch-130 = 5.983281880617142e-06

Training epoch-55 batch-131
Running loss of epoch-55 batch-131 = 6.676884368062019e-06

Training epoch-55 batch-132
Running loss of epoch-55 batch-132 = 7.812166586518288e-06

Training epoch-55 batch-133
Running loss of epoch-55 batch-133 = 6.434274837374687e-06

Training epoch-55 batch-134
Running loss of epoch-55 batch-134 = 1.0272953659296036e-05

Training epoch-55 batch-135
Running loss of epoch-55 batch-135 = 1.3363780453801155e-05

Training epoch-55 batch-136
Running loss of epoch-55 batch-136 = 7.296912372112274e-06

Training epoch-55 batch-137
Running loss of epoch-55 batch-137 = 3.14381904900074e-05

Training epoch-55 batch-138
Running loss of epoch-55 batch-138 = 1.8254388123750687e-05

Training epoch-55 batch-139
Running loss of epoch-55 batch-139 = 1.285388134419918e-05

Training epoch-55 batch-140
Running loss of epoch-55 batch-140 = 3.721565008163452e-06

Training epoch-55 batch-141
Running loss of epoch-55 batch-141 = 7.678056135773659e-06

Training epoch-55 batch-142
Running loss of epoch-55 batch-142 = 1.0408228263258934e-05

Training epoch-55 batch-143
Running loss of epoch-55 batch-143 = 8.951639756560326e-06

Training epoch-55 batch-144
Running loss of epoch-55 batch-144 = 2.5857705622911453e-05

Training epoch-55 batch-145
Running loss of epoch-55 batch-145 = 7.771188393235207e-06

Training epoch-55 batch-146
Running loss of epoch-55 batch-146 = 9.738374501466751e-06

Training epoch-55 batch-147
Running loss of epoch-55 batch-147 = 6.434507668018341e-06

Training epoch-55 batch-148
Running loss of epoch-55 batch-148 = 8.689472451806068e-06

Training epoch-55 batch-149
Running loss of epoch-55 batch-149 = 2.1837186068296432e-06

Training epoch-55 batch-150
Running loss of epoch-55 batch-150 = 5.1925890147686005e-06

Training epoch-55 batch-151
Running loss of epoch-55 batch-151 = 3.6833807826042175e-06

Training epoch-55 batch-152
Running loss of epoch-55 batch-152 = 5.475245416164398e-06

Training epoch-55 batch-153
Running loss of epoch-55 batch-153 = 2.606934867799282e-05

Training epoch-55 batch-154
Running loss of epoch-55 batch-154 = 5.59631735086441e-06

Training epoch-55 batch-155
Running loss of epoch-55 batch-155 = 1.1210795491933823e-05

Training epoch-55 batch-156
Running loss of epoch-55 batch-156 = 8.2370825111866e-06

Training epoch-55 batch-157
Running loss of epoch-55 batch-157 = 4.026293754577637e-05

Finished training epoch-55.



Average train loss at epoch-55 = 9.519486129283905e-06

Started Evaluation

Average val loss at epoch-55 = 1.6675599494846431

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.36 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-56


Training epoch-56 batch-1
Running loss of epoch-56 batch-1 = 7.2356779128313065e-06

Training epoch-56 batch-2
Running loss of epoch-56 batch-2 = 3.0153896659612656e-06

Training epoch-56 batch-3
Running loss of epoch-56 batch-3 = 3.304099664092064e-06

Training epoch-56 batch-4
Running loss of epoch-56 batch-4 = 2.9203947633504868e-06

Training epoch-56 batch-5
Running loss of epoch-56 batch-5 = 5.091074854135513e-06

Training epoch-56 batch-6
Running loss of epoch-56 batch-6 = 6.5837521106004715e-06

Training epoch-56 batch-7
Running loss of epoch-56 batch-7 = 7.142545655369759e-06

Training epoch-56 batch-8
Running loss of epoch-56 batch-8 = 3.4694094210863113e-06

Training epoch-56 batch-9
Running loss of epoch-56 batch-9 = 4.180241376161575e-06

Training epoch-56 batch-10
Running loss of epoch-56 batch-10 = 9.814044460654259e-06

Training epoch-56 batch-11
Running loss of epoch-56 batch-11 = 1.8063001334667206e-06

Training epoch-56 batch-12
Running loss of epoch-56 batch-12 = 4.444969817996025e-06

Training epoch-56 batch-13
Running loss of epoch-56 batch-13 = 6.705755367875099e-06

Training epoch-56 batch-14
Running loss of epoch-56 batch-14 = 7.634749636054039e-06

Training epoch-56 batch-15
Running loss of epoch-56 batch-15 = 8.125090971589088e-06

Training epoch-56 batch-16
Running loss of epoch-56 batch-16 = 4.3208710849285126e-06

Training epoch-56 batch-17
Running loss of epoch-56 batch-17 = 5.084322765469551e-06

Training epoch-56 batch-18
Running loss of epoch-56 batch-18 = 8.215196430683136e-06

Training epoch-56 batch-19
Running loss of epoch-56 batch-19 = 1.3344455510377884e-05

Training epoch-56 batch-20
Running loss of epoch-56 batch-20 = 6.567453965544701e-06

Training epoch-56 batch-21
Running loss of epoch-56 batch-21 = 1.0190065950155258e-05

Training epoch-56 batch-22
Running loss of epoch-56 batch-22 = 2.4230685085058212e-06

Training epoch-56 batch-23
Running loss of epoch-56 batch-23 = 3.1276140362024307e-06

Training epoch-56 batch-24
Running loss of epoch-56 batch-24 = 3.4817494451999664e-06

Training epoch-56 batch-25
Running loss of epoch-56 batch-25 = 9.299023076891899e-06

Training epoch-56 batch-26
Running loss of epoch-56 batch-26 = 5.532056093215942e-06

Training epoch-56 batch-27
Running loss of epoch-56 batch-27 = 9.377719834446907e-06

Training epoch-56 batch-28
Running loss of epoch-56 batch-28 = 8.390052244067192e-06

Training epoch-56 batch-29
Running loss of epoch-56 batch-29 = 5.705747753381729e-06

Training epoch-56 batch-30
Running loss of epoch-56 batch-30 = 3.7278514355421066e-06

Training epoch-56 batch-31
Running loss of epoch-56 batch-31 = 9.984010830521584e-06

Training epoch-56 batch-32
Running loss of epoch-56 batch-32 = 6.215181201696396e-06

Training epoch-56 batch-33
Running loss of epoch-56 batch-33 = 8.160946890711784e-06

Training epoch-56 batch-34
Running loss of epoch-56 batch-34 = 1.6325386241078377e-05

Training epoch-56 batch-35
Running loss of epoch-56 batch-35 = 4.841946065425873e-06

Training epoch-56 batch-36
Running loss of epoch-56 batch-36 = 6.435206159949303e-06

Training epoch-56 batch-37
Running loss of epoch-56 batch-37 = 2.3744069039821625e-06

Training epoch-56 batch-38
Running loss of epoch-56 batch-38 = 7.358146831393242e-06

Training epoch-56 batch-39
Running loss of epoch-56 batch-39 = 4.971632733941078e-06

Training epoch-56 batch-40
Running loss of epoch-56 batch-40 = 2.696877345442772e-06

Training epoch-56 batch-41
Running loss of epoch-56 batch-41 = 7.159775123000145e-06

Training epoch-56 batch-42
Running loss of epoch-56 batch-42 = 7.911119610071182e-06

Training epoch-56 batch-43
Running loss of epoch-56 batch-43 = 1.0938383638858795e-05

Training epoch-56 batch-44
Running loss of epoch-56 batch-44 = 7.434980943799019e-06

Training epoch-56 batch-45
Running loss of epoch-56 batch-45 = 2.6605557650327682e-06

Training epoch-56 batch-46
Running loss of epoch-56 batch-46 = 1.5363330021500587e-05

Training epoch-56 batch-47
Running loss of epoch-56 batch-47 = 2.9795803129673004e-05

Training epoch-56 batch-48
Running loss of epoch-56 batch-48 = 7.444294169545174e-06

Training epoch-56 batch-49
Running loss of epoch-56 batch-49 = 8.325325325131416e-06

Training epoch-56 batch-50
Running loss of epoch-56 batch-50 = 7.772818207740784e-06

Training epoch-56 batch-51
Running loss of epoch-56 batch-51 = 1.1678086593747139e-05

Training epoch-56 batch-52
Running loss of epoch-56 batch-52 = 2.764398232102394e-06

Training epoch-56 batch-53
Running loss of epoch-56 batch-53 = 6.357673555612564e-06

Training epoch-56 batch-54
Running loss of epoch-56 batch-54 = 6.501330062747002e-06

Training epoch-56 batch-55
Running loss of epoch-56 batch-55 = 1.275935210287571e-05

Training epoch-56 batch-56
Running loss of epoch-56 batch-56 = 7.114838808774948e-06

Training epoch-56 batch-57
Running loss of epoch-56 batch-57 = 7.288530468940735e-06

Training epoch-56 batch-58
Running loss of epoch-56 batch-58 = 5.282461643218994e-06

Training epoch-56 batch-59
Running loss of epoch-56 batch-59 = 9.116483852267265e-06

Training epoch-56 batch-60
Running loss of epoch-56 batch-60 = 2.491055056452751e-05

Training epoch-56 batch-61
Running loss of epoch-56 batch-61 = 1.0890886187553406e-05

Training epoch-56 batch-62
Running loss of epoch-56 batch-62 = 9.330688044428825e-06

Training epoch-56 batch-63
Running loss of epoch-56 batch-63 = 2.316967584192753e-05

Training epoch-56 batch-64
Running loss of epoch-56 batch-64 = 1.3334676623344421e-05

Training epoch-56 batch-65
Running loss of epoch-56 batch-65 = 8.677598088979721e-06

Training epoch-56 batch-66
Running loss of epoch-56 batch-66 = 6.468500941991806e-06

Training epoch-56 batch-67
Running loss of epoch-56 batch-67 = 4.372093826532364e-06

Training epoch-56 batch-68
Running loss of epoch-56 batch-68 = 1.4120712876319885e-05

Training epoch-56 batch-69
Running loss of epoch-56 batch-69 = 1.8015969544649124e-05

Training epoch-56 batch-70
Running loss of epoch-56 batch-70 = 1.0478310286998749e-05

Training epoch-56 batch-71
Running loss of epoch-56 batch-71 = 5.156034603714943e-06

Training epoch-56 batch-72
Running loss of epoch-56 batch-72 = 1.3316981494426727e-05

Training epoch-56 batch-73
Running loss of epoch-56 batch-73 = 1.0826857760548592e-05

Training epoch-56 batch-74
Running loss of epoch-56 batch-74 = 8.696690201759338e-06

Training epoch-56 batch-75
Running loss of epoch-56 batch-75 = 1.3896729797124863e-05

Training epoch-56 batch-76
Running loss of epoch-56 batch-76 = 6.84056431055069e-06

Training epoch-56 batch-77
Running loss of epoch-56 batch-77 = 8.588656783103943e-06

Training epoch-56 batch-78
Running loss of epoch-56 batch-78 = 9.455950930714607e-06

Training epoch-56 batch-79
Running loss of epoch-56 batch-79 = 8.994247764348984e-06

Training epoch-56 batch-80
Running loss of epoch-56 batch-80 = 1.2857373803853989e-05

Training epoch-56 batch-81
Running loss of epoch-56 batch-81 = 5.022622644901276e-06

Training epoch-56 batch-82
Running loss of epoch-56 batch-82 = 4.670582711696625e-06

Training epoch-56 batch-83
Running loss of epoch-56 batch-83 = 1.199287362396717e-05

Training epoch-56 batch-84
Running loss of epoch-56 batch-84 = 7.241731509566307e-06

Training epoch-56 batch-85
Running loss of epoch-56 batch-85 = 1.0282034054398537e-05

Training epoch-56 batch-86
Running loss of epoch-56 batch-86 = 6.528804078698158e-06

Training epoch-56 batch-87
Running loss of epoch-56 batch-87 = 1.2225937098264694e-06

Training epoch-56 batch-88
Running loss of epoch-56 batch-88 = 2.7408823370933533e-06

Training epoch-56 batch-89
Running loss of epoch-56 batch-89 = 1.5784986317157745e-05

Training epoch-56 batch-90
Running loss of epoch-56 batch-90 = 2.8756912797689438e-06

Training epoch-56 batch-91
Running loss of epoch-56 batch-91 = 1.2210337445139885e-05

Training epoch-56 batch-92
Running loss of epoch-56 batch-92 = 4.047667607665062e-05

Training epoch-56 batch-93
Running loss of epoch-56 batch-93 = 1.3546319678425789e-05

Training epoch-56 batch-94
Running loss of epoch-56 batch-94 = 6.9497618824243546e-06

Training epoch-56 batch-95
Running loss of epoch-56 batch-95 = 8.902046829462051e-06

Training epoch-56 batch-96
Running loss of epoch-56 batch-96 = 8.356757462024689e-06

Training epoch-56 batch-97
Running loss of epoch-56 batch-97 = 7.86222517490387e-06

Training epoch-56 batch-98
Running loss of epoch-56 batch-98 = 7.956055924296379e-06

Training epoch-56 batch-99
Running loss of epoch-56 batch-99 = 2.4112872779369354e-05

Training epoch-56 batch-100
Running loss of epoch-56 batch-100 = 4.877801984548569e-06

Training epoch-56 batch-101
Running loss of epoch-56 batch-101 = 6.460817530751228e-06

Training epoch-56 batch-102
Running loss of epoch-56 batch-102 = 9.75211150944233e-06

Training epoch-56 batch-103
Running loss of epoch-56 batch-103 = 6.632646545767784e-06

Training epoch-56 batch-104
Running loss of epoch-56 batch-104 = 1.8815509974956512e-05

Training epoch-56 batch-105
Running loss of epoch-56 batch-105 = 7.224502041935921e-06

Training epoch-56 batch-106
Running loss of epoch-56 batch-106 = 1.2386823073029518e-05

Training epoch-56 batch-107
Running loss of epoch-56 batch-107 = 2.2658612579107285e-05

Training epoch-56 batch-108
Running loss of epoch-56 batch-108 = 9.89832915365696e-06

Training epoch-56 batch-109
Running loss of epoch-56 batch-109 = 4.963250830769539e-06

Training epoch-56 batch-110
Running loss of epoch-56 batch-110 = 5.950918421149254e-06

Training epoch-56 batch-111
Running loss of epoch-56 batch-111 = 3.9690639823675156e-06

Training epoch-56 batch-112
Running loss of epoch-56 batch-112 = 1.0260380804538727e-05

Training epoch-56 batch-113
Running loss of epoch-56 batch-113 = 6.991904228925705e-06

Training epoch-56 batch-114
Running loss of epoch-56 batch-114 = 1.185084693133831e-05

Training epoch-56 batch-115
Running loss of epoch-56 batch-115 = 1.3883225619792938e-05

Training epoch-56 batch-116
Running loss of epoch-56 batch-116 = 7.058959454298019e-06

Training epoch-56 batch-117
Running loss of epoch-56 batch-117 = 2.6207417249679565e-06

Training epoch-56 batch-118
Running loss of epoch-56 batch-118 = 1.0783318430185318e-05

Training epoch-56 batch-119
Running loss of epoch-56 batch-119 = 8.38516280055046e-06

Training epoch-56 batch-120
Running loss of epoch-56 batch-120 = 2.440996468067169e-06

Training epoch-56 batch-121
Running loss of epoch-56 batch-121 = 7.080379873514175e-06

Training epoch-56 batch-122
Running loss of epoch-56 batch-122 = 8.321134373545647e-06

Training epoch-56 batch-123
Running loss of epoch-56 batch-123 = 2.154591493308544e-05

Training epoch-56 batch-124
Running loss of epoch-56 batch-124 = 1.1670636013150215e-05

Training epoch-56 batch-125
Running loss of epoch-56 batch-125 = 9.50181856751442e-06

Training epoch-56 batch-126
Running loss of epoch-56 batch-126 = 1.2173783034086227e-05

Training epoch-56 batch-127
Running loss of epoch-56 batch-127 = 6.656395271420479e-06

Training epoch-56 batch-128
Running loss of epoch-56 batch-128 = 9.172828868031502e-06

Training epoch-56 batch-129
Running loss of epoch-56 batch-129 = 8.251052349805832e-06

Training epoch-56 batch-130
Running loss of epoch-56 batch-130 = 1.016911119222641e-05

Training epoch-56 batch-131
Running loss of epoch-56 batch-131 = 5.2496325224637985e-06

Training epoch-56 batch-132
Running loss of epoch-56 batch-132 = 1.0219402611255646e-05

Training epoch-56 batch-133
Running loss of epoch-56 batch-133 = 9.302981197834015e-06

Training epoch-56 batch-134
Running loss of epoch-56 batch-134 = 8.424976840615273e-06

Training epoch-56 batch-135
Running loss of epoch-56 batch-135 = 9.754206985235214e-06

Training epoch-56 batch-136
Running loss of epoch-56 batch-136 = 3.003072924911976e-05

Training epoch-56 batch-137
Running loss of epoch-56 batch-137 = 1.034536398947239e-05

Training epoch-56 batch-138
Running loss of epoch-56 batch-138 = 8.00727866590023e-06

Training epoch-56 batch-139
Running loss of epoch-56 batch-139 = 9.204726666212082e-06

Training epoch-56 batch-140
Running loss of epoch-56 batch-140 = 1.615704968571663e-05

Training epoch-56 batch-141
Running loss of epoch-56 batch-141 = 6.8033114075660706e-06

Training epoch-56 batch-142
Running loss of epoch-56 batch-142 = 4.837987944483757e-06

Training epoch-56 batch-143
Running loss of epoch-56 batch-143 = 8.163275197148323e-06

Training epoch-56 batch-144
Running loss of epoch-56 batch-144 = 6.7623332142829895e-06

Training epoch-56 batch-145
Running loss of epoch-56 batch-145 = 6.075948476791382e-06

Training epoch-56 batch-146
Running loss of epoch-56 batch-146 = 1.1858530342578888e-05

Training epoch-56 batch-147
Running loss of epoch-56 batch-147 = 4.779547452926636e-06

Training epoch-56 batch-148
Running loss of epoch-56 batch-148 = 2.5724060833454132e-05

Training epoch-56 batch-149
Running loss of epoch-56 batch-149 = 1.2810807675123215e-05

Training epoch-56 batch-150
Running loss of epoch-56 batch-150 = 1.2881355360150337e-05

Training epoch-56 batch-151
Running loss of epoch-56 batch-151 = 5.331588909029961e-06

Training epoch-56 batch-152
Running loss of epoch-56 batch-152 = 5.822163075208664e-06

Training epoch-56 batch-153
Running loss of epoch-56 batch-153 = 7.126014679670334e-06

Training epoch-56 batch-154
Running loss of epoch-56 batch-154 = 4.774192348122597e-06

Training epoch-56 batch-155
Running loss of epoch-56 batch-155 = 1.582619734108448e-05

Training epoch-56 batch-156
Running loss of epoch-56 batch-156 = 5.210051313042641e-06

Training epoch-56 batch-157
Running loss of epoch-56 batch-157 = 2.8152018785476685e-05

Finished training epoch-56.



Average train loss at epoch-56 = 9.122306108474731e-06

Started Evaluation

Average val loss at epoch-56 = 1.6755491787513677

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.36 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.53 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.12 %
Accuracy for class execute is: 51.00 %
Accuracy for class get is: 64.36 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-57


Training epoch-57 batch-1
Running loss of epoch-57 batch-1 = 9.188894182443619e-06

Training epoch-57 batch-2
Running loss of epoch-57 batch-2 = 1.7995014786720276e-05

Training epoch-57 batch-3
Running loss of epoch-57 batch-3 = 4.801200702786446e-06

Training epoch-57 batch-4
Running loss of epoch-57 batch-4 = 1.1251075193285942e-05

Training epoch-57 batch-5
Running loss of epoch-57 batch-5 = 6.326008588075638e-06

Training epoch-57 batch-6
Running loss of epoch-57 batch-6 = 3.6300625652074814e-06

Training epoch-57 batch-7
Running loss of epoch-57 batch-7 = 1.200730912387371e-05

Training epoch-57 batch-8
Running loss of epoch-57 batch-8 = 7.286667823791504e-06

Training epoch-57 batch-9
Running loss of epoch-57 batch-9 = 6.1444006860256195e-06

Training epoch-57 batch-10
Running loss of epoch-57 batch-10 = 6.507383659482002e-06

Training epoch-57 batch-11
Running loss of epoch-57 batch-11 = 7.408438250422478e-06

Training epoch-57 batch-12
Running loss of epoch-57 batch-12 = 6.89411535859108e-06

Training epoch-57 batch-13
Running loss of epoch-57 batch-13 = 2.7266796678304672e-06

Training epoch-57 batch-14
Running loss of epoch-57 batch-14 = 9.485986083745956e-06

Training epoch-57 batch-15
Running loss of epoch-57 batch-15 = 1.2553064152598381e-05

Training epoch-57 batch-16
Running loss of epoch-57 batch-16 = 5.6675635278224945e-06

Training epoch-57 batch-17
Running loss of epoch-57 batch-17 = 6.316229701042175e-06

Training epoch-57 batch-18
Running loss of epoch-57 batch-18 = 7.2442926466465e-06

Training epoch-57 batch-19
Running loss of epoch-57 batch-19 = 7.918570190668106e-06

Training epoch-57 batch-20
Running loss of epoch-57 batch-20 = 8.388655260205269e-06

Training epoch-57 batch-21
Running loss of epoch-57 batch-21 = 6.984686478972435e-06

Training epoch-57 batch-22
Running loss of epoch-57 batch-22 = 7.861293852329254e-06

Training epoch-57 batch-23
Running loss of epoch-57 batch-23 = 2.805376425385475e-06

Training epoch-57 batch-24
Running loss of epoch-57 batch-24 = 9.90857370197773e-06

Training epoch-57 batch-25
Running loss of epoch-57 batch-25 = 5.485257133841515e-06

Training epoch-57 batch-26
Running loss of epoch-57 batch-26 = 2.0781299099326134e-05

Training epoch-57 batch-27
Running loss of epoch-57 batch-27 = 1.4000339433550835e-05

Training epoch-57 batch-28
Running loss of epoch-57 batch-28 = 2.2520078346133232e-05

Training epoch-57 batch-29
Running loss of epoch-57 batch-29 = 9.497161954641342e-06

Training epoch-57 batch-30
Running loss of epoch-57 batch-30 = 5.3746625781059265e-06

Training epoch-57 batch-31
Running loss of epoch-57 batch-31 = 7.738592103123665e-06

Training epoch-57 batch-32
Running loss of epoch-57 batch-32 = 1.7850659787654877e-05

Training epoch-57 batch-33
Running loss of epoch-57 batch-33 = 6.132526323199272e-06

Training epoch-57 batch-34
Running loss of epoch-57 batch-34 = 7.409835234284401e-06

Training epoch-57 batch-35
Running loss of epoch-57 batch-35 = 1.4115823432803154e-05

Training epoch-57 batch-36
Running loss of epoch-57 batch-36 = 2.530403435230255e-06

Training epoch-57 batch-37
Running loss of epoch-57 batch-37 = 1.27407256513834e-05

Training epoch-57 batch-38
Running loss of epoch-57 batch-38 = 7.87968747317791e-06

Training epoch-57 batch-39
Running loss of epoch-57 batch-39 = 4.873843863606453e-06

Training epoch-57 batch-40
Running loss of epoch-57 batch-40 = 1.0390067473053932e-05

Training epoch-57 batch-41
Running loss of epoch-57 batch-41 = 5.140900611877441e-06

Training epoch-57 batch-42
Running loss of epoch-57 batch-42 = 4.627276211977005e-06

Training epoch-57 batch-43
Running loss of epoch-57 batch-43 = 1.650117337703705e-05

Training epoch-57 batch-44
Running loss of epoch-57 batch-44 = 3.4172553569078445e-06

Training epoch-57 batch-45
Running loss of epoch-57 batch-45 = 2.4959444999694824e-06

Training epoch-57 batch-46
Running loss of epoch-57 batch-46 = 1.277262344956398e-05

Training epoch-57 batch-47
Running loss of epoch-57 batch-47 = 7.856171578168869e-06

Training epoch-57 batch-48
Running loss of epoch-57 batch-48 = 4.8354268074035645e-06

Training epoch-57 batch-49
Running loss of epoch-57 batch-49 = 2.5250250473618507e-05

Training epoch-57 batch-50
Running loss of epoch-57 batch-50 = 3.469642251729965e-06

Training epoch-57 batch-51
Running loss of epoch-57 batch-51 = 5.734385922551155e-06

Training epoch-57 batch-52
Running loss of epoch-57 batch-52 = 8.283182978630066e-06

Training epoch-57 batch-53
Running loss of epoch-57 batch-53 = 2.391636371612549e-06

Training epoch-57 batch-54
Running loss of epoch-57 batch-54 = 4.637986421585083e-06

Training epoch-57 batch-55
Running loss of epoch-57 batch-55 = 5.461275577545166e-06

Training epoch-57 batch-56
Running loss of epoch-57 batch-56 = 8.829403668642044e-06

Training epoch-57 batch-57
Running loss of epoch-57 batch-57 = 4.100380465388298e-06

Training epoch-57 batch-58
Running loss of epoch-57 batch-58 = 1.3654585927724838e-05

Training epoch-57 batch-59
Running loss of epoch-57 batch-59 = 1.0894611477851868e-05

Training epoch-57 batch-60
Running loss of epoch-57 batch-60 = 4.1639432311058044e-06

Training epoch-57 batch-61
Running loss of epoch-57 batch-61 = 1.0020332410931587e-05

Training epoch-57 batch-62
Running loss of epoch-57 batch-62 = 1.1133961379528046e-05

Training epoch-57 batch-63
Running loss of epoch-57 batch-63 = 5.150213837623596e-06

Training epoch-57 batch-64
Running loss of epoch-57 batch-64 = 1.2511387467384338e-05

Training epoch-57 batch-65
Running loss of epoch-57 batch-65 = 1.576053909957409e-05

Training epoch-57 batch-66
Running loss of epoch-57 batch-66 = 9.909970685839653e-06

Training epoch-57 batch-67
Running loss of epoch-57 batch-67 = 7.5749121606349945e-06

Training epoch-57 batch-68
Running loss of epoch-57 batch-68 = 1.54243316501379e-05

Training epoch-57 batch-69
Running loss of epoch-57 batch-69 = 9.387033060193062e-06

Training epoch-57 batch-70
Running loss of epoch-57 batch-70 = 6.978167220950127e-06

Training epoch-57 batch-71
Running loss of epoch-57 batch-71 = 4.219356924295425e-06

Training epoch-57 batch-72
Running loss of epoch-57 batch-72 = 7.341615855693817e-06

Training epoch-57 batch-73
Running loss of epoch-57 batch-73 = 7.744180038571358e-06

Training epoch-57 batch-74
Running loss of epoch-57 batch-74 = 6.191432476043701e-06

Training epoch-57 batch-75
Running loss of epoch-57 batch-75 = 5.020759999752045e-06

Training epoch-57 batch-76
Running loss of epoch-57 batch-76 = 5.3765252232551575e-06

Training epoch-57 batch-77
Running loss of epoch-57 batch-77 = 8.076895028352737e-06

Training epoch-57 batch-78
Running loss of epoch-57 batch-78 = 3.8142316043376923e-06

Training epoch-57 batch-79
Running loss of epoch-57 batch-79 = 2.0043691620230675e-05

Training epoch-57 batch-80
Running loss of epoch-57 batch-80 = 5.219597369432449e-06

Training epoch-57 batch-81
Running loss of epoch-57 batch-81 = 8.938368409872055e-06

Training epoch-57 batch-82
Running loss of epoch-57 batch-82 = 4.618894308805466e-06

Training epoch-57 batch-83
Running loss of epoch-57 batch-83 = 9.113224223256111e-06

Training epoch-57 batch-84
Running loss of epoch-57 batch-84 = 8.946051821112633e-06

Training epoch-57 batch-85
Running loss of epoch-57 batch-85 = 6.861984729766846e-06

Training epoch-57 batch-86
Running loss of epoch-57 batch-86 = 4.973495379090309e-06

Training epoch-57 batch-87
Running loss of epoch-57 batch-87 = 5.5783893913030624e-06

Training epoch-57 batch-88
Running loss of epoch-57 batch-88 = 5.038455128669739e-06

Training epoch-57 batch-89
Running loss of epoch-57 batch-89 = 8.193077519536018e-06

Training epoch-57 batch-90
Running loss of epoch-57 batch-90 = 3.2801181077957153e-06

Training epoch-57 batch-91
Running loss of epoch-57 batch-91 = 5.454523488879204e-06

Training epoch-57 batch-92
Running loss of epoch-57 batch-92 = 6.237765774130821e-06

Training epoch-57 batch-93
Running loss of epoch-57 batch-93 = 1.5535857528448105e-05

Training epoch-57 batch-94
Running loss of epoch-57 batch-94 = 7.897848263382912e-06

Training epoch-57 batch-95
Running loss of epoch-57 batch-95 = 4.236586391925812e-06

Training epoch-57 batch-96
Running loss of epoch-57 batch-96 = 7.481547072529793e-06

Training epoch-57 batch-97
Running loss of epoch-57 batch-97 = 5.048932507634163e-06

Training epoch-57 batch-98
Running loss of epoch-57 batch-98 = 5.813781172037125e-06

Training epoch-57 batch-99
Running loss of epoch-57 batch-99 = 1.434003934264183e-05

Training epoch-57 batch-100
Running loss of epoch-57 batch-100 = 3.8046855479478836e-06

Training epoch-57 batch-101
Running loss of epoch-57 batch-101 = 4.241243004798889e-06

Training epoch-57 batch-102
Running loss of epoch-57 batch-102 = 4.819594323635101e-06

Training epoch-57 batch-103
Running loss of epoch-57 batch-103 = 1.9408762454986572e-06

Training epoch-57 batch-104
Running loss of epoch-57 batch-104 = 9.68598760664463e-06

Training epoch-57 batch-105
Running loss of epoch-57 batch-105 = 4.4729094952344894e-06

Training epoch-57 batch-106
Running loss of epoch-57 batch-106 = 2.698274329304695e-06

Training epoch-57 batch-107
Running loss of epoch-57 batch-107 = 8.241739124059677e-06

Training epoch-57 batch-108
Running loss of epoch-57 batch-108 = 6.716931238770485e-06

Training epoch-57 batch-109
Running loss of epoch-57 batch-109 = 6.81169331073761e-06

Training epoch-57 batch-110
Running loss of epoch-57 batch-110 = 1.2882985174655914e-05

Training epoch-57 batch-111
Running loss of epoch-57 batch-111 = 1.1531170457601547e-05

Training epoch-57 batch-112
Running loss of epoch-57 batch-112 = 5.779555067420006e-06

Training epoch-57 batch-113
Running loss of epoch-57 batch-113 = 4.15346585214138e-06

Training epoch-57 batch-114
Running loss of epoch-57 batch-114 = 6.560236215591431e-06

Training epoch-57 batch-115
Running loss of epoch-57 batch-115 = 5.331356078386307e-06

Training epoch-57 batch-116
Running loss of epoch-57 batch-116 = 8.276430889964104e-06

Training epoch-57 batch-117
Running loss of epoch-57 batch-117 = 7.86525197327137e-06

Training epoch-57 batch-118
Running loss of epoch-57 batch-118 = 7.377238944172859e-06

Training epoch-57 batch-119
Running loss of epoch-57 batch-119 = 8.283182978630066e-06

Training epoch-57 batch-120
Running loss of epoch-57 batch-120 = 3.607221879065037e-05

Training epoch-57 batch-121
Running loss of epoch-57 batch-121 = 1.4471355825662613e-05

Training epoch-57 batch-122
Running loss of epoch-57 batch-122 = 2.6104971766471863e-06

Training epoch-57 batch-123
Running loss of epoch-57 batch-123 = 5.764886736869812e-06

Training epoch-57 batch-124
Running loss of epoch-57 batch-124 = 5.16185536980629e-06

Training epoch-57 batch-125
Running loss of epoch-57 batch-125 = 2.517271786928177e-05

Training epoch-57 batch-126
Running loss of epoch-57 batch-126 = 4.282686859369278e-06

Training epoch-57 batch-127
Running loss of epoch-57 batch-127 = 4.4084154069423676e-06

Training epoch-57 batch-128
Running loss of epoch-57 batch-128 = 4.677334800362587e-06

Training epoch-57 batch-129
Running loss of epoch-57 batch-129 = 6.443122401833534e-06

Training epoch-57 batch-130
Running loss of epoch-57 batch-130 = 1.9066501408815384e-06

Training epoch-57 batch-131
Running loss of epoch-57 batch-131 = 8.926726877689362e-06

Training epoch-57 batch-132
Running loss of epoch-57 batch-132 = 1.2283911928534508e-05

Training epoch-57 batch-133
Running loss of epoch-57 batch-133 = 4.468951374292374e-06

Training epoch-57 batch-134
Running loss of epoch-57 batch-134 = 5.755806341767311e-06

Training epoch-57 batch-135
Running loss of epoch-57 batch-135 = 1.5887897461652756e-05

Training epoch-57 batch-136
Running loss of epoch-57 batch-136 = 1.0224059224128723e-05

Training epoch-57 batch-137
Running loss of epoch-57 batch-137 = 7.914379239082336e-06

Training epoch-57 batch-138
Running loss of epoch-57 batch-138 = 4.923203960061073e-06

Training epoch-57 batch-139
Running loss of epoch-57 batch-139 = 7.032649591565132e-06

Training epoch-57 batch-140
Running loss of epoch-57 batch-140 = 2.47324351221323e-05

Training epoch-57 batch-141
Running loss of epoch-57 batch-141 = 3.519700840115547e-06

Training epoch-57 batch-142
Running loss of epoch-57 batch-142 = 2.4551991373300552e-06

Training epoch-57 batch-143
Running loss of epoch-57 batch-143 = 8.113915100693703e-06

Training epoch-57 batch-144
Running loss of epoch-57 batch-144 = 8.056173101067543e-06

Training epoch-57 batch-145
Running loss of epoch-57 batch-145 = 1.5568220987915993e-05

Training epoch-57 batch-146
Running loss of epoch-57 batch-146 = 1.0163988918066025e-05

Training epoch-57 batch-147
Running loss of epoch-57 batch-147 = 3.961846232414246e-06

Training epoch-57 batch-148
Running loss of epoch-57 batch-148 = 3.5190489143133163e-05

Training epoch-57 batch-149
Running loss of epoch-57 batch-149 = 5.298061296343803e-06

Training epoch-57 batch-150
Running loss of epoch-57 batch-150 = 2.5562942028045654e-05

Training epoch-57 batch-151
Running loss of epoch-57 batch-151 = 5.646375939249992e-06

Training epoch-57 batch-152
Running loss of epoch-57 batch-152 = 8.601928129792213e-06

Training epoch-57 batch-153
Running loss of epoch-57 batch-153 = 1.2622447684407234e-05

Training epoch-57 batch-154
Running loss of epoch-57 batch-154 = 6.549293175339699e-06

Training epoch-57 batch-155
Running loss of epoch-57 batch-155 = 2.3369677364826202e-05

Training epoch-57 batch-156
Running loss of epoch-57 batch-156 = 8.890405297279358e-06

Training epoch-57 batch-157
Running loss of epoch-57 batch-157 = 3.071874380111694e-05

Finished training epoch-57.



Average train loss at epoch-57 = 8.71344357728958e-06

Started Evaluation

Average val loss at epoch-57 = 1.6852698001613347

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 89.02 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.27 %

Finished Evaluation



Started training epoch-58


Training epoch-58 batch-1
Running loss of epoch-58 batch-1 = 1.530442386865616e-05

Training epoch-58 batch-2
Running loss of epoch-58 batch-2 = 7.812399417161942e-06

Training epoch-58 batch-3
Running loss of epoch-58 batch-3 = 2.674991264939308e-06

Training epoch-58 batch-4
Running loss of epoch-58 batch-4 = 8.891220204532146e-06

Training epoch-58 batch-5
Running loss of epoch-58 batch-5 = 5.783280357718468e-06

Training epoch-58 batch-6
Running loss of epoch-58 batch-6 = 1.5469267964363098e-06

Training epoch-58 batch-7
Running loss of epoch-58 batch-7 = 3.388151526451111e-06

Training epoch-58 batch-8
Running loss of epoch-58 batch-8 = 5.994690582156181e-06

Training epoch-58 batch-9
Running loss of epoch-58 batch-9 = 8.014263585209846e-06

Training epoch-58 batch-10
Running loss of epoch-58 batch-10 = 7.033580914139748e-06

Training epoch-58 batch-11
Running loss of epoch-58 batch-11 = 3.7564896047115326e-06

Training epoch-58 batch-12
Running loss of epoch-58 batch-12 = 1.8527265638113022e-05

Training epoch-58 batch-13
Running loss of epoch-58 batch-13 = 1.0645715519785881e-05

Training epoch-58 batch-14
Running loss of epoch-58 batch-14 = 9.102746844291687e-06

Training epoch-58 batch-15
Running loss of epoch-58 batch-15 = 9.362120181322098e-06

Training epoch-58 batch-16
Running loss of epoch-58 batch-16 = 5.04637137055397e-06

Training epoch-58 batch-17
Running loss of epoch-58 batch-17 = 6.85802660882473e-06

Training epoch-58 batch-18
Running loss of epoch-58 batch-18 = 5.747657269239426e-06

Training epoch-58 batch-19
Running loss of epoch-58 batch-19 = 5.2622053772211075e-06

Training epoch-58 batch-20
Running loss of epoch-58 batch-20 = 1.2509757652878761e-05

Training epoch-58 batch-21
Running loss of epoch-58 batch-21 = 7.486436516046524e-06

Training epoch-58 batch-22
Running loss of epoch-58 batch-22 = 2.5364570319652557e-06

Training epoch-58 batch-23
Running loss of epoch-58 batch-23 = 9.168870747089386e-06

Training epoch-58 batch-24
Running loss of epoch-58 batch-24 = 4.899920895695686e-06

Training epoch-58 batch-25
Running loss of epoch-58 batch-25 = 6.833579391241074e-06

Training epoch-58 batch-26
Running loss of epoch-58 batch-26 = 5.473149940371513e-06

Training epoch-58 batch-27
Running loss of epoch-58 batch-27 = 3.49339097738266e-06

Training epoch-58 batch-28
Running loss of epoch-58 batch-28 = 6.387475878000259e-06

Training epoch-58 batch-29
Running loss of epoch-58 batch-29 = 8.371658623218536e-06

Training epoch-58 batch-30
Running loss of epoch-58 batch-30 = 8.749542757868767e-06

Training epoch-58 batch-31
Running loss of epoch-58 batch-31 = 9.621726348996162e-06

Training epoch-58 batch-32
Running loss of epoch-58 batch-32 = 1.3183802366256714e-05

Training epoch-58 batch-33
Running loss of epoch-58 batch-33 = 8.407514542341232e-06

Training epoch-58 batch-34
Running loss of epoch-58 batch-34 = 1.7605023458600044e-05

Training epoch-58 batch-35
Running loss of epoch-58 batch-35 = 6.302958354353905e-06

Training epoch-58 batch-36
Running loss of epoch-58 batch-36 = 7.611699402332306e-06

Training epoch-58 batch-37
Running loss of epoch-58 batch-37 = 5.5390410125255585e-06

Training epoch-58 batch-38
Running loss of epoch-58 batch-38 = 6.396789103746414e-06

Training epoch-58 batch-39
Running loss of epoch-58 batch-39 = 3.1213276088237762e-06

Training epoch-58 batch-40
Running loss of epoch-58 batch-40 = 4.150206223130226e-06

Training epoch-58 batch-41
Running loss of epoch-58 batch-41 = 7.400987669825554e-06

Training epoch-58 batch-42
Running loss of epoch-58 batch-42 = 7.117399945855141e-06

Training epoch-58 batch-43
Running loss of epoch-58 batch-43 = 3.6850105971097946e-06

Training epoch-58 batch-44
Running loss of epoch-58 batch-44 = 7.364898920059204e-06

Training epoch-58 batch-45
Running loss of epoch-58 batch-45 = 6.377696990966797e-06

Training epoch-58 batch-46
Running loss of epoch-58 batch-46 = 7.443595677614212e-06

Training epoch-58 batch-47
Running loss of epoch-58 batch-47 = 1.0700663551688194e-05

Training epoch-58 batch-48
Running loss of epoch-58 batch-48 = 7.471069693565369e-06

Training epoch-58 batch-49
Running loss of epoch-58 batch-49 = 6.980495527386665e-06

Training epoch-58 batch-50
Running loss of epoch-58 batch-50 = 3.7052668631076813e-06

Training epoch-58 batch-51
Running loss of epoch-58 batch-51 = 1.1921161785721779e-05

Training epoch-58 batch-52
Running loss of epoch-58 batch-52 = 8.467119187116623e-06

Training epoch-58 batch-53
Running loss of epoch-58 batch-53 = 2.05356627702713e-06

Training epoch-58 batch-54
Running loss of epoch-58 batch-54 = 3.5169068723917007e-06

Training epoch-58 batch-55
Running loss of epoch-58 batch-55 = 5.2014365792274475e-06

Training epoch-58 batch-56
Running loss of epoch-58 batch-56 = 4.031229764223099e-06

Training epoch-58 batch-57
Running loss of epoch-58 batch-57 = 1.1615455150604248e-05

Training epoch-58 batch-58
Running loss of epoch-58 batch-58 = 1.740013249218464e-05

Training epoch-58 batch-59
Running loss of epoch-58 batch-59 = 1.8926337361335754e-05

Training epoch-58 batch-60
Running loss of epoch-58 batch-60 = 1.4197081327438354e-05

Training epoch-58 batch-61
Running loss of epoch-58 batch-61 = 5.179783329367638e-06

Training epoch-58 batch-62
Running loss of epoch-58 batch-62 = 5.015404894948006e-06

Training epoch-58 batch-63
Running loss of epoch-58 batch-63 = 4.868488758802414e-06

Training epoch-58 batch-64
Running loss of epoch-58 batch-64 = 4.579313099384308e-06

Training epoch-58 batch-65
Running loss of epoch-58 batch-65 = 1.6924459487199783e-05

Training epoch-58 batch-66
Running loss of epoch-58 batch-66 = 1.3874843716621399e-05

Training epoch-58 batch-67
Running loss of epoch-58 batch-67 = 6.694113835692406e-06

Training epoch-58 batch-68
Running loss of epoch-58 batch-68 = 4.091998562216759e-06

Training epoch-58 batch-69
Running loss of epoch-58 batch-69 = 1.2380536645650864e-05

Training epoch-58 batch-70
Running loss of epoch-58 batch-70 = 7.555820047855377e-06

Training epoch-58 batch-71
Running loss of epoch-58 batch-71 = 7.549533620476723e-06

Training epoch-58 batch-72
Running loss of epoch-58 batch-72 = 2.1741958335042e-05

Training epoch-58 batch-73
Running loss of epoch-58 batch-73 = 1.1942349374294281e-05

Training epoch-58 batch-74
Running loss of epoch-58 batch-74 = 4.712259396910667e-06

Training epoch-58 batch-75
Running loss of epoch-58 batch-75 = 6.3695479184389114e-06

Training epoch-58 batch-76
Running loss of epoch-58 batch-76 = 7.509719580411911e-06

Training epoch-58 batch-77
Running loss of epoch-58 batch-77 = 2.0234379917383194e-05

Training epoch-58 batch-78
Running loss of epoch-58 batch-78 = 3.897352144122124e-06

Training epoch-58 batch-79
Running loss of epoch-58 batch-79 = 1.0245712473988533e-05

Training epoch-58 batch-80
Running loss of epoch-58 batch-80 = 3.772089257836342e-06

Training epoch-58 batch-81
Running loss of epoch-58 batch-81 = 1.2559350579977036e-05

Training epoch-58 batch-82
Running loss of epoch-58 batch-82 = 9.529292583465576e-06

Training epoch-58 batch-83
Running loss of epoch-58 batch-83 = 4.034489393234253e-06

Training epoch-58 batch-84
Running loss of epoch-58 batch-84 = 6.037997081875801e-06

Training epoch-58 batch-85
Running loss of epoch-58 batch-85 = 3.2263342291116714e-06

Training epoch-58 batch-86
Running loss of epoch-58 batch-86 = 1.108669675886631e-05

Training epoch-58 batch-87
Running loss of epoch-58 batch-87 = 7.3283445090055466e-06

Training epoch-58 batch-88
Running loss of epoch-58 batch-88 = 1.682434231042862e-06

Training epoch-58 batch-89
Running loss of epoch-58 batch-89 = 4.8906076699495316e-06

Training epoch-58 batch-90
Running loss of epoch-58 batch-90 = 8.860370144248009e-06

Training epoch-58 batch-91
Running loss of epoch-58 batch-91 = 4.655681550502777e-06

Training epoch-58 batch-92
Running loss of epoch-58 batch-92 = 9.941169992089272e-06

Training epoch-58 batch-93
Running loss of epoch-58 batch-93 = 7.353257387876511e-06

Training epoch-58 batch-94
Running loss of epoch-58 batch-94 = 4.319939762353897e-06

Training epoch-58 batch-95
Running loss of epoch-58 batch-95 = 7.085967808961868e-06

Training epoch-58 batch-96
Running loss of epoch-58 batch-96 = 3.632623702287674e-06

Training epoch-58 batch-97
Running loss of epoch-58 batch-97 = 7.91577622294426e-06

Training epoch-58 batch-98
Running loss of epoch-58 batch-98 = 2.3832544684410095e-05

Training epoch-58 batch-99
Running loss of epoch-58 batch-99 = 4.593981429934502e-06

Training epoch-58 batch-100
Running loss of epoch-58 batch-100 = 7.927417755126953e-06

Training epoch-58 batch-101
Running loss of epoch-58 batch-101 = 4.293164238333702e-06

Training epoch-58 batch-102
Running loss of epoch-58 batch-102 = 7.882015779614449e-06

Training epoch-58 batch-103
Running loss of epoch-58 batch-103 = 8.433358743786812e-06

Training epoch-58 batch-104
Running loss of epoch-58 batch-104 = 5.103647708892822e-06

Training epoch-58 batch-105
Running loss of epoch-58 batch-105 = 1.0727206245064735e-05

Training epoch-58 batch-106
Running loss of epoch-58 batch-106 = 6.4445193856954575e-06

Training epoch-58 batch-107
Running loss of epoch-58 batch-107 = 8.29063355922699e-06

Training epoch-58 batch-108
Running loss of epoch-58 batch-108 = 4.1120219975709915e-06

Training epoch-58 batch-109
Running loss of epoch-58 batch-109 = 1.993938349187374e-05

Training epoch-58 batch-110
Running loss of epoch-58 batch-110 = 5.2319373935461044e-06

Training epoch-58 batch-111
Running loss of epoch-58 batch-111 = 4.067784175276756e-06

Training epoch-58 batch-112
Running loss of epoch-58 batch-112 = 1.5790807083249092e-05

Training epoch-58 batch-113
Running loss of epoch-58 batch-113 = 7.884344086050987e-06

Training epoch-58 batch-114
Running loss of epoch-58 batch-114 = 4.003290086984634e-06

Training epoch-58 batch-115
Running loss of epoch-58 batch-115 = 8.46758484840393e-06

Training epoch-58 batch-116
Running loss of epoch-58 batch-116 = 2.976739779114723e-06

Training epoch-58 batch-117
Running loss of epoch-58 batch-117 = 7.453840225934982e-06

Training epoch-58 batch-118
Running loss of epoch-58 batch-118 = 1.5716301277279854e-05

Training epoch-58 batch-119
Running loss of epoch-58 batch-119 = 2.102414146065712e-05

Training epoch-58 batch-120
Running loss of epoch-58 batch-120 = 8.304603397846222e-06

Training epoch-58 batch-121
Running loss of epoch-58 batch-121 = 7.559778168797493e-06

Training epoch-58 batch-122
Running loss of epoch-58 batch-122 = 8.540460839867592e-06

Training epoch-58 batch-123
Running loss of epoch-58 batch-123 = 5.9746671468019485e-06

Training epoch-58 batch-124
Running loss of epoch-58 batch-124 = 5.683163180947304e-06

Training epoch-58 batch-125
Running loss of epoch-58 batch-125 = 1.546344719827175e-05

Training epoch-58 batch-126
Running loss of epoch-58 batch-126 = 8.399132639169693e-06

Training epoch-58 batch-127
Running loss of epoch-58 batch-127 = 3.837747499346733e-06

Training epoch-58 batch-128
Running loss of epoch-58 batch-128 = 9.772367775440216e-06

Training epoch-58 batch-129
Running loss of epoch-58 batch-129 = 1.208484172821045e-05

Training epoch-58 batch-130
Running loss of epoch-58 batch-130 = 3.3339019864797592e-06

Training epoch-58 batch-131
Running loss of epoch-58 batch-131 = 4.415865987539291e-06

Training epoch-58 batch-132
Running loss of epoch-58 batch-132 = 1.0468065738677979e-05

Training epoch-58 batch-133
Running loss of epoch-58 batch-133 = 8.207978680729866e-06

Training epoch-58 batch-134
Running loss of epoch-58 batch-134 = 3.909412771463394e-05

Training epoch-58 batch-135
Running loss of epoch-58 batch-135 = 6.105750799179077e-06

Training epoch-58 batch-136
Running loss of epoch-58 batch-136 = 4.7762878239154816e-06

Training epoch-58 batch-137
Running loss of epoch-58 batch-137 = 4.68292273581028e-06

Training epoch-58 batch-138
Running loss of epoch-58 batch-138 = 3.824010491371155e-06

Training epoch-58 batch-139
Running loss of epoch-58 batch-139 = 8.488772436976433e-06

Training epoch-58 batch-140
Running loss of epoch-58 batch-140 = 4.023313522338867e-06

Training epoch-58 batch-141
Running loss of epoch-58 batch-141 = 1.985207200050354e-05

Training epoch-58 batch-142
Running loss of epoch-58 batch-142 = 7.867347449064255e-06

Training epoch-58 batch-143
Running loss of epoch-58 batch-143 = 1.1397060006856918e-05

Training epoch-58 batch-144
Running loss of epoch-58 batch-144 = 8.191214874386787e-06

Training epoch-58 batch-145
Running loss of epoch-58 batch-145 = 3.8937898352742195e-05

Training epoch-58 batch-146
Running loss of epoch-58 batch-146 = 5.16907311975956e-06

Training epoch-58 batch-147
Running loss of epoch-58 batch-147 = 2.6454217731952667e-06

Training epoch-58 batch-148
Running loss of epoch-58 batch-148 = 8.710194379091263e-06

Training epoch-58 batch-149
Running loss of epoch-58 batch-149 = 1.2112781405448914e-05

Training epoch-58 batch-150
Running loss of epoch-58 batch-150 = 7.92904756963253e-06

Training epoch-58 batch-151
Running loss of epoch-58 batch-151 = 1.292000524699688e-05

Training epoch-58 batch-152
Running loss of epoch-58 batch-152 = 6.46151602268219e-06

Training epoch-58 batch-153
Running loss of epoch-58 batch-153 = 2.9296381399035454e-05

Training epoch-58 batch-154
Running loss of epoch-58 batch-154 = 8.261296898126602e-06

Training epoch-58 batch-155
Running loss of epoch-58 batch-155 = 6.102956831455231e-06

Training epoch-58 batch-156
Running loss of epoch-58 batch-156 = 4.743924364447594e-06

Training epoch-58 batch-157
Running loss of epoch-58 batch-157 = 1.015886664390564e-05

Finished training epoch-58.



Average train loss at epoch-58 = 8.562151342630387e-06

Started Evaluation

Average val loss at epoch-58 = 1.6925018184422422

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 89.02 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.18 %

Finished Evaluation



Started training epoch-59


Training epoch-59 batch-1
Running loss of epoch-59 batch-1 = 1.3481592759490013e-05

Training epoch-59 batch-2
Running loss of epoch-59 batch-2 = 1.1502066627144814e-05

Training epoch-59 batch-3
Running loss of epoch-59 batch-3 = 1.0673189535737038e-05

Training epoch-59 batch-4
Running loss of epoch-59 batch-4 = 8.779345080256462e-06

Training epoch-59 batch-5
Running loss of epoch-59 batch-5 = 5.287816748023033e-06

Training epoch-59 batch-6
Running loss of epoch-59 batch-6 = 3.255670890212059e-06

Training epoch-59 batch-7
Running loss of epoch-59 batch-7 = 5.549518391489983e-06

Training epoch-59 batch-8
Running loss of epoch-59 batch-8 = 9.401468560099602e-06

Training epoch-59 batch-9
Running loss of epoch-59 batch-9 = 1.400243490934372e-05

Training epoch-59 batch-10
Running loss of epoch-59 batch-10 = 6.583286449313164e-06

Training epoch-59 batch-11
Running loss of epoch-59 batch-11 = 8.618691936135292e-06

Training epoch-59 batch-12
Running loss of epoch-59 batch-12 = 2.800021320581436e-06

Training epoch-59 batch-13
Running loss of epoch-59 batch-13 = 1.1093448847532272e-05

Training epoch-59 batch-14
Running loss of epoch-59 batch-14 = 4.064524546265602e-06

Training epoch-59 batch-15
Running loss of epoch-59 batch-15 = 4.800967872142792e-06

Training epoch-59 batch-16
Running loss of epoch-59 batch-16 = 1.2542586773633957e-05

Training epoch-59 batch-17
Running loss of epoch-59 batch-17 = 2.079969272017479e-05

Training epoch-59 batch-18
Running loss of epoch-59 batch-18 = 4.764646291732788e-06

Training epoch-59 batch-19
Running loss of epoch-59 batch-19 = 4.683854058384895e-06

Training epoch-59 batch-20
Running loss of epoch-59 batch-20 = 7.956288754940033e-06

Training epoch-59 batch-21
Running loss of epoch-59 batch-21 = 2.3836269974708557e-05

Training epoch-59 batch-22
Running loss of epoch-59 batch-22 = 2.9134098440408707e-06

Training epoch-59 batch-23
Running loss of epoch-59 batch-23 = 2.3347092792391777e-05

Training epoch-59 batch-24
Running loss of epoch-59 batch-24 = 3.7741847336292267e-06

Training epoch-59 batch-25
Running loss of epoch-59 batch-25 = 7.472466677427292e-06

Training epoch-59 batch-26
Running loss of epoch-59 batch-26 = 4.970002919435501e-06

Training epoch-59 batch-27
Running loss of epoch-59 batch-27 = 1.3254466466605663e-05

Training epoch-59 batch-28
Running loss of epoch-59 batch-28 = 1.2564240023493767e-05

Training epoch-59 batch-29
Running loss of epoch-59 batch-29 = 4.773493856191635e-06

Training epoch-59 batch-30
Running loss of epoch-59 batch-30 = 6.771646440029144e-06

Training epoch-59 batch-31
Running loss of epoch-59 batch-31 = 5.402835085988045e-06

Training epoch-59 batch-32
Running loss of epoch-59 batch-32 = 4.952540621161461e-06

Training epoch-59 batch-33
Running loss of epoch-59 batch-33 = 6.430782377719879e-06

Training epoch-59 batch-34
Running loss of epoch-59 batch-34 = 1.0377028957009315e-05

Training epoch-59 batch-35
Running loss of epoch-59 batch-35 = 1.1553755030035973e-05

Training epoch-59 batch-36
Running loss of epoch-59 batch-36 = 6.082933396100998e-06

Training epoch-59 batch-37
Running loss of epoch-59 batch-37 = 4.332978278398514e-06

Training epoch-59 batch-38
Running loss of epoch-59 batch-38 = 7.391441613435745e-06

Training epoch-59 batch-39
Running loss of epoch-59 batch-39 = 3.4924596548080444e-06

Training epoch-59 batch-40
Running loss of epoch-59 batch-40 = 1.1852709576487541e-05

Training epoch-59 batch-41
Running loss of epoch-59 batch-41 = 9.662006050348282e-06

Training epoch-59 batch-42
Running loss of epoch-59 batch-42 = 8.825911208987236e-06

Training epoch-59 batch-43
Running loss of epoch-59 batch-43 = 5.15766441822052e-06

Training epoch-59 batch-44
Running loss of epoch-59 batch-44 = 5.054520443081856e-06

Training epoch-59 batch-45
Running loss of epoch-59 batch-45 = 8.704140782356262e-06

Training epoch-59 batch-46
Running loss of epoch-59 batch-46 = 3.8191210478544235e-06

Training epoch-59 batch-47
Running loss of epoch-59 batch-47 = 5.181180313229561e-06

Training epoch-59 batch-48
Running loss of epoch-59 batch-48 = 1.2492528185248375e-05

Training epoch-59 batch-49
Running loss of epoch-59 batch-49 = 3.845430910587311e-06

Training epoch-59 batch-50
Running loss of epoch-59 batch-50 = 4.0458980947732925e-06

Training epoch-59 batch-51
Running loss of epoch-59 batch-51 = 6.805639714002609e-06

Training epoch-59 batch-52
Running loss of epoch-59 batch-52 = 4.071742296218872e-06

Training epoch-59 batch-53
Running loss of epoch-59 batch-53 = 5.3532421588897705e-06

Training epoch-59 batch-54
Running loss of epoch-59 batch-54 = 5.576293915510178e-06

Training epoch-59 batch-55
Running loss of epoch-59 batch-55 = 7.121125236153603e-06

Training epoch-59 batch-56
Running loss of epoch-59 batch-56 = 1.168833114206791e-05

Training epoch-59 batch-57
Running loss of epoch-59 batch-57 = 6.3194893300533295e-06

Training epoch-59 batch-58
Running loss of epoch-59 batch-58 = 6.428221240639687e-06

Training epoch-59 batch-59
Running loss of epoch-59 batch-59 = 9.570736438035965e-06

Training epoch-59 batch-60
Running loss of epoch-59 batch-60 = 8.790288120508194e-06

Training epoch-59 batch-61
Running loss of epoch-59 batch-61 = 3.1811650842428207e-06

Training epoch-59 batch-62
Running loss of epoch-59 batch-62 = 1.1184019967913628e-05

Training epoch-59 batch-63
Running loss of epoch-59 batch-63 = 5.763256922364235e-06

Training epoch-59 batch-64
Running loss of epoch-59 batch-64 = 3.87127511203289e-06

Training epoch-59 batch-65
Running loss of epoch-59 batch-65 = 9.852927178144455e-06

Training epoch-59 batch-66
Running loss of epoch-59 batch-66 = 1.2584961950778961e-05

Training epoch-59 batch-67
Running loss of epoch-59 batch-67 = 9.053153917193413e-06

Training epoch-59 batch-68
Running loss of epoch-59 batch-68 = 2.7481000870466232e-06

Training epoch-59 batch-69
Running loss of epoch-59 batch-69 = 3.830529749393463e-06

Training epoch-59 batch-70
Running loss of epoch-59 batch-70 = 4.813773557543755e-06

Training epoch-59 batch-71
Running loss of epoch-59 batch-71 = 3.552529960870743e-06

Training epoch-59 batch-72
Running loss of epoch-59 batch-72 = 7.61798582971096e-06

Training epoch-59 batch-73
Running loss of epoch-59 batch-73 = 7.499009370803833e-06

Training epoch-59 batch-74
Running loss of epoch-59 batch-74 = 3.213132731616497e-05

Training epoch-59 batch-75
Running loss of epoch-59 batch-75 = 2.364395186305046e-06

Training epoch-59 batch-76
Running loss of epoch-59 batch-76 = 1.230509951710701e-05

Training epoch-59 batch-77
Running loss of epoch-59 batch-77 = 1.149647869169712e-05

Training epoch-59 batch-78
Running loss of epoch-59 batch-78 = 2.1704239770770073e-05

Training epoch-59 batch-79
Running loss of epoch-59 batch-79 = 1.0126736015081406e-05

Training epoch-59 batch-80
Running loss of epoch-59 batch-80 = 1.0879943147301674e-05

Training epoch-59 batch-81
Running loss of epoch-59 batch-81 = 6.520887836813927e-06

Training epoch-59 batch-82
Running loss of epoch-59 batch-82 = 1.4548888429999352e-05

Training epoch-59 batch-83
Running loss of epoch-59 batch-83 = 6.8682711571455e-06

Training epoch-59 batch-84
Running loss of epoch-59 batch-84 = 1.2691598385572433e-06

Training epoch-59 batch-85
Running loss of epoch-59 batch-85 = 1.013185828924179e-05

Training epoch-59 batch-86
Running loss of epoch-59 batch-86 = 4.893168807029724e-06

Training epoch-59 batch-87
Running loss of epoch-59 batch-87 = 9.067822247743607e-06

Training epoch-59 batch-88
Running loss of epoch-59 batch-88 = 5.287583917379379e-06

Training epoch-59 batch-89
Running loss of epoch-59 batch-89 = 5.115056410431862e-06

Training epoch-59 batch-90
Running loss of epoch-59 batch-90 = 3.725290298461914e-06

Training epoch-59 batch-91
Running loss of epoch-59 batch-91 = 8.607050403952599e-06

Training epoch-59 batch-92
Running loss of epoch-59 batch-92 = 1.2401258572936058e-05

Training epoch-59 batch-93
Running loss of epoch-59 batch-93 = 2.1988525986671448e-06

Training epoch-59 batch-94
Running loss of epoch-59 batch-94 = 8.279457688331604e-06

Training epoch-59 batch-95
Running loss of epoch-59 batch-95 = 4.66126948595047e-06

Training epoch-59 batch-96
Running loss of epoch-59 batch-96 = 6.018439307808876e-06

Training epoch-59 batch-97
Running loss of epoch-59 batch-97 = 6.137881428003311e-06

Training epoch-59 batch-98
Running loss of epoch-59 batch-98 = 4.320405423641205e-06

Training epoch-59 batch-99
Running loss of epoch-59 batch-99 = 6.920192390680313e-06

Training epoch-59 batch-100
Running loss of epoch-59 batch-100 = 6.4533669501543045e-06

Training epoch-59 batch-101
Running loss of epoch-59 batch-101 = 6.394926458597183e-06

Training epoch-59 batch-102
Running loss of epoch-59 batch-102 = 8.594011887907982e-06

Training epoch-59 batch-103
Running loss of epoch-59 batch-103 = 5.114125087857246e-06

Training epoch-59 batch-104
Running loss of epoch-59 batch-104 = 4.479195922613144e-06

Training epoch-59 batch-105
Running loss of epoch-59 batch-105 = 5.2140094339847565e-06

Training epoch-59 batch-106
Running loss of epoch-59 batch-106 = 1.5756580978631973e-05

Training epoch-59 batch-107
Running loss of epoch-59 batch-107 = 2.67871655523777e-06

Training epoch-59 batch-108
Running loss of epoch-59 batch-108 = 8.413335308432579e-06

Training epoch-59 batch-109
Running loss of epoch-59 batch-109 = 6.8338122218847275e-06

Training epoch-59 batch-110
Running loss of epoch-59 batch-110 = 2.0772451534867287e-05

Training epoch-59 batch-111
Running loss of epoch-59 batch-111 = 6.205402314662933e-06

Training epoch-59 batch-112
Running loss of epoch-59 batch-112 = 5.237758159637451e-06

Training epoch-59 batch-113
Running loss of epoch-59 batch-113 = 2.591637894511223e-06

Training epoch-59 batch-114
Running loss of epoch-59 batch-114 = 8.453615009784698e-06

Training epoch-59 batch-115
Running loss of epoch-59 batch-115 = 1.0855961591005325e-05

Training epoch-59 batch-116
Running loss of epoch-59 batch-116 = 6.511341780424118e-06

Training epoch-59 batch-117
Running loss of epoch-59 batch-117 = 5.83939254283905e-06

Training epoch-59 batch-118
Running loss of epoch-59 batch-118 = 4.289206117391586e-06

Training epoch-59 batch-119
Running loss of epoch-59 batch-119 = 7.502036169171333e-06

Training epoch-59 batch-120
Running loss of epoch-59 batch-120 = 5.814246833324432e-06

Training epoch-59 batch-121
Running loss of epoch-59 batch-121 = 1.4080200344324112e-05

Training epoch-59 batch-122
Running loss of epoch-59 batch-122 = 5.159061402082443e-06

Training epoch-59 batch-123
Running loss of epoch-59 batch-123 = 9.813113138079643e-06

Training epoch-59 batch-124
Running loss of epoch-59 batch-124 = 2.273591235280037e-06

Training epoch-59 batch-125
Running loss of epoch-59 batch-125 = 1.1356081813573837e-05

Training epoch-59 batch-126
Running loss of epoch-59 batch-126 = 8.150935173034668e-06

Training epoch-59 batch-127
Running loss of epoch-59 batch-127 = 1.1052237823605537e-05

Training epoch-59 batch-128
Running loss of epoch-59 batch-128 = 6.51204027235508e-06

Training epoch-59 batch-129
Running loss of epoch-59 batch-129 = 2.164160832762718e-06

Training epoch-59 batch-130
Running loss of epoch-59 batch-130 = 6.579793989658356e-06

Training epoch-59 batch-131
Running loss of epoch-59 batch-131 = 5.558831617236137e-06

Training epoch-59 batch-132
Running loss of epoch-59 batch-132 = 6.537884473800659e-06

Training epoch-59 batch-133
Running loss of epoch-59 batch-133 = 5.7176221162080765e-06

Training epoch-59 batch-134
Running loss of epoch-59 batch-134 = 6.047775968909264e-06

Training epoch-59 batch-135
Running loss of epoch-59 batch-135 = 3.1667761504650116e-05

Training epoch-59 batch-136
Running loss of epoch-59 batch-136 = 1.3776589184999466e-05

Training epoch-59 batch-137
Running loss of epoch-59 batch-137 = 1.2852717190980911e-05

Training epoch-59 batch-138
Running loss of epoch-59 batch-138 = 3.414927050471306e-06

Training epoch-59 batch-139
Running loss of epoch-59 batch-139 = 2.2410182282328606e-05

Training epoch-59 batch-140
Running loss of epoch-59 batch-140 = 9.46875661611557e-06

Training epoch-59 batch-141
Running loss of epoch-59 batch-141 = 7.416354492306709e-06

Training epoch-59 batch-142
Running loss of epoch-59 batch-142 = 1.0377494618296623e-05

Training epoch-59 batch-143
Running loss of epoch-59 batch-143 = 3.9960723370313644e-06

Training epoch-59 batch-144
Running loss of epoch-59 batch-144 = 7.963506504893303e-06

Training epoch-59 batch-145
Running loss of epoch-59 batch-145 = 8.783070370554924e-06

Training epoch-59 batch-146
Running loss of epoch-59 batch-146 = 4.705507308244705e-06

Training epoch-59 batch-147
Running loss of epoch-59 batch-147 = 1.7101410776376724e-06

Training epoch-59 batch-148
Running loss of epoch-59 batch-148 = 7.425202056765556e-06

Training epoch-59 batch-149
Running loss of epoch-59 batch-149 = 7.161172106862068e-06

Training epoch-59 batch-150
Running loss of epoch-59 batch-150 = 1.1040130630135536e-05

Training epoch-59 batch-151
Running loss of epoch-59 batch-151 = 6.923452019691467e-06

Training epoch-59 batch-152
Running loss of epoch-59 batch-152 = 6.615417078137398e-06

Training epoch-59 batch-153
Running loss of epoch-59 batch-153 = 9.541399776935577e-06

Training epoch-59 batch-154
Running loss of epoch-59 batch-154 = 4.233093932271004e-06

Training epoch-59 batch-155
Running loss of epoch-59 batch-155 = 2.0364299416542053e-05

Training epoch-59 batch-156
Running loss of epoch-59 batch-156 = 4.166038706898689e-06

Training epoch-59 batch-157
Running loss of epoch-59 batch-157 = 1.683831214904785e-05

Finished training epoch-59.



Average train loss at epoch-59 = 8.150184899568558e-06

Started Evaluation

Average val loss at epoch-59 = 1.6934349443513952

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-60


Training epoch-60 batch-1
Running loss of epoch-60 batch-1 = 1.8710969015955925e-05

Training epoch-60 batch-2
Running loss of epoch-60 batch-2 = 8.506001904606819e-06

Training epoch-60 batch-3
Running loss of epoch-60 batch-3 = 7.034279406070709e-06

Training epoch-60 batch-4
Running loss of epoch-60 batch-4 = 4.604458808898926e-06

Training epoch-60 batch-5
Running loss of epoch-60 batch-5 = 1.3558892533183098e-05

Training epoch-60 batch-6
Running loss of epoch-60 batch-6 = 3.156019374728203e-06

Training epoch-60 batch-7
Running loss of epoch-60 batch-7 = 8.286675438284874e-06

Training epoch-60 batch-8
Running loss of epoch-60 batch-8 = 5.334382876753807e-06

Training epoch-60 batch-9
Running loss of epoch-60 batch-9 = 1.9746599718928337e-05

Training epoch-60 batch-10
Running loss of epoch-60 batch-10 = 8.999835699796677e-06

Training epoch-60 batch-11
Running loss of epoch-60 batch-11 = 4.358822479844093e-06

Training epoch-60 batch-12
Running loss of epoch-60 batch-12 = 7.0186797529459e-06

Training epoch-60 batch-13
Running loss of epoch-60 batch-13 = 4.959059879183769e-06

Training epoch-60 batch-14
Running loss of epoch-60 batch-14 = 2.4234410375356674e-05

Training epoch-60 batch-15
Running loss of epoch-60 batch-15 = 6.230548024177551e-06

Training epoch-60 batch-16
Running loss of epoch-60 batch-16 = 7.751630619168282e-06

Training epoch-60 batch-17
Running loss of epoch-60 batch-17 = 1.1222902685403824e-05

Training epoch-60 batch-18
Running loss of epoch-60 batch-18 = 8.66735354065895e-06

Training epoch-60 batch-19
Running loss of epoch-60 batch-19 = 2.7567148208618164e-06

Training epoch-60 batch-20
Running loss of epoch-60 batch-20 = 6.279908120632172e-06

Training epoch-60 batch-21
Running loss of epoch-60 batch-21 = 8.892733603715897e-06

Training epoch-60 batch-22
Running loss of epoch-60 batch-22 = 1.6297679394483566e-05

Training epoch-60 batch-23
Running loss of epoch-60 batch-23 = 6.851973012089729e-06

Training epoch-60 batch-24
Running loss of epoch-60 batch-24 = 2.921139821410179e-05

Training epoch-60 batch-25
Running loss of epoch-60 batch-25 = 3.536464646458626e-06

Training epoch-60 batch-26
Running loss of epoch-60 batch-26 = 1.2195436283946037e-05

Training epoch-60 batch-27
Running loss of epoch-60 batch-27 = 6.799586117267609e-06

Training epoch-60 batch-28
Running loss of epoch-60 batch-28 = 2.3613683879375458e-06

Training epoch-60 batch-29
Running loss of epoch-60 batch-29 = 3.605382516980171e-06

Training epoch-60 batch-30
Running loss of epoch-60 batch-30 = 4.7388020902872086e-06

Training epoch-60 batch-31
Running loss of epoch-60 batch-31 = 5.373731255531311e-06

Training epoch-60 batch-32
Running loss of epoch-60 batch-32 = 7.5837597250938416e-06

Training epoch-60 batch-33
Running loss of epoch-60 batch-33 = 2.783956006169319e-06

Training epoch-60 batch-34
Running loss of epoch-60 batch-34 = 5.3069088608026505e-06

Training epoch-60 batch-35
Running loss of epoch-60 batch-35 = 1.1441530659794807e-05

Training epoch-60 batch-36
Running loss of epoch-60 batch-36 = 1.0721385478973389e-05

Training epoch-60 batch-37
Running loss of epoch-60 batch-37 = 1.3346085324883461e-05

Training epoch-60 batch-38
Running loss of epoch-60 batch-38 = 7.217982783913612e-06

Training epoch-60 batch-39
Running loss of epoch-60 batch-39 = 7.874332368373871e-06

Training epoch-60 batch-40
Running loss of epoch-60 batch-40 = 7.598428055644035e-06

Training epoch-60 batch-41
Running loss of epoch-60 batch-41 = 7.4910931289196014e-06

Training epoch-60 batch-42
Running loss of epoch-60 batch-42 = 2.701766788959503e-06

Training epoch-60 batch-43
Running loss of epoch-60 batch-43 = 1.2610573321580887e-05

Training epoch-60 batch-44
Running loss of epoch-60 batch-44 = 1.4496035873889923e-05

Training epoch-60 batch-45
Running loss of epoch-60 batch-45 = 4.334142431616783e-06

Training epoch-60 batch-46
Running loss of epoch-60 batch-46 = 1.3598473742604256e-05

Training epoch-60 batch-47
Running loss of epoch-60 batch-47 = 6.619258783757687e-06

Training epoch-60 batch-48
Running loss of epoch-60 batch-48 = 2.5096815079450607e-06

Training epoch-60 batch-49
Running loss of epoch-60 batch-49 = 8.677830919623375e-06

Training epoch-60 batch-50
Running loss of epoch-60 batch-50 = 2.8132926672697067e-06

Training epoch-60 batch-51
Running loss of epoch-60 batch-51 = 5.559064447879791e-06

Training epoch-60 batch-52
Running loss of epoch-60 batch-52 = 5.881534889340401e-06

Training epoch-60 batch-53
Running loss of epoch-60 batch-53 = 5.8212317526340485e-06

Training epoch-60 batch-54
Running loss of epoch-60 batch-54 = 4.000496119260788e-06

Training epoch-60 batch-55
Running loss of epoch-60 batch-55 = 4.2531173676252365e-06

Training epoch-60 batch-56
Running loss of epoch-60 batch-56 = 1.0756775736808777e-05

Training epoch-60 batch-57
Running loss of epoch-60 batch-57 = 5.156267434358597e-06

Training epoch-60 batch-58
Running loss of epoch-60 batch-58 = 7.2498805820941925e-06

Training epoch-60 batch-59
Running loss of epoch-60 batch-59 = 6.124610081315041e-06

Training epoch-60 batch-60
Running loss of epoch-60 batch-60 = 8.565839380025864e-06

Training epoch-60 batch-61
Running loss of epoch-60 batch-61 = 5.343463271856308e-06

Training epoch-60 batch-62
Running loss of epoch-60 batch-62 = 8.614268153905869e-06

Training epoch-60 batch-63
Running loss of epoch-60 batch-63 = 7.5446441769599915e-06

Training epoch-60 batch-64
Running loss of epoch-60 batch-64 = 4.97535802423954e-06

Training epoch-60 batch-65
Running loss of epoch-60 batch-65 = 8.156290277838707e-06

Training epoch-60 batch-66
Running loss of epoch-60 batch-66 = 1.0403338819742203e-05

Training epoch-60 batch-67
Running loss of epoch-60 batch-67 = 4.590954631567001e-06

Training epoch-60 batch-68
Running loss of epoch-60 batch-68 = 2.6796478778123856e-06

Training epoch-60 batch-69
Running loss of epoch-60 batch-69 = 6.6659413278102875e-06

Training epoch-60 batch-70
Running loss of epoch-60 batch-70 = 7.74674117565155e-06

Training epoch-60 batch-71
Running loss of epoch-60 batch-71 = 2.555781975388527e-06

Training epoch-60 batch-72
Running loss of epoch-60 batch-72 = 5.048234015703201e-06

Training epoch-60 batch-73
Running loss of epoch-60 batch-73 = 5.357665941119194e-06

Training epoch-60 batch-74
Running loss of epoch-60 batch-74 = 6.529968231916428e-06

Training epoch-60 batch-75
Running loss of epoch-60 batch-75 = 1.4333054423332214e-06

Training epoch-60 batch-76
Running loss of epoch-60 batch-76 = 6.492482498288155e-06

Training epoch-60 batch-77
Running loss of epoch-60 batch-77 = 9.932555258274078e-07

Training epoch-60 batch-78
Running loss of epoch-60 batch-78 = 2.0007137209177017e-06

Training epoch-60 batch-79
Running loss of epoch-60 batch-79 = 3.949040547013283e-06

Training epoch-60 batch-80
Running loss of epoch-60 batch-80 = 1.8886523321270943e-05

Training epoch-60 batch-81
Running loss of epoch-60 batch-81 = 1.5744008123874664e-06

Training epoch-60 batch-82
Running loss of epoch-60 batch-82 = 6.501330062747002e-06

Training epoch-60 batch-83
Running loss of epoch-60 batch-83 = 4.5245978981256485e-06

Training epoch-60 batch-84
Running loss of epoch-60 batch-84 = 1.893448643386364e-05

Training epoch-60 batch-85
Running loss of epoch-60 batch-85 = 8.748378604650497e-06

Training epoch-60 batch-86
Running loss of epoch-60 batch-86 = 5.392823368310928e-06

Training epoch-60 batch-87
Running loss of epoch-60 batch-87 = 4.691071808338165e-06

Training epoch-60 batch-88
Running loss of epoch-60 batch-88 = 3.833789378404617e-06

Training epoch-60 batch-89
Running loss of epoch-60 batch-89 = 1.5962868928909302e-06

Training epoch-60 batch-90
Running loss of epoch-60 batch-90 = 9.873416274785995e-06

Training epoch-60 batch-91
Running loss of epoch-60 batch-91 = 2.0266510546207428e-05

Training epoch-60 batch-92
Running loss of epoch-60 batch-92 = 3.5963021218776703e-06

Training epoch-60 batch-93
Running loss of epoch-60 batch-93 = 4.971632733941078e-06

Training epoch-60 batch-94
Running loss of epoch-60 batch-94 = 7.642898708581924e-06

Training epoch-60 batch-95
Running loss of epoch-60 batch-95 = 1.0640360414981842e-05

Training epoch-60 batch-96
Running loss of epoch-60 batch-96 = 1.7625512555241585e-05

Training epoch-60 batch-97
Running loss of epoch-60 batch-97 = 1.6301870346069336e-05

Training epoch-60 batch-98
Running loss of epoch-60 batch-98 = 1.1719530448317528e-05

Training epoch-60 batch-99
Running loss of epoch-60 batch-99 = 1.0437564924359322e-05

Training epoch-60 batch-100
Running loss of epoch-60 batch-100 = 4.8906076699495316e-06

Training epoch-60 batch-101
Running loss of epoch-60 batch-101 = 2.355314791202545e-06

Training epoch-60 batch-102
Running loss of epoch-60 batch-102 = 7.381662726402283e-06

Training epoch-60 batch-103
Running loss of epoch-60 batch-103 = 8.656643331050873e-06

Training epoch-60 batch-104
Running loss of epoch-60 batch-104 = 9.812414646148682e-06

Training epoch-60 batch-105
Running loss of epoch-60 batch-105 = 8.799368515610695e-06

Training epoch-60 batch-106
Running loss of epoch-60 batch-106 = 7.5588468462228775e-06

Training epoch-60 batch-107
Running loss of epoch-60 batch-107 = 9.266892448067665e-06

Training epoch-60 batch-108
Running loss of epoch-60 batch-108 = 6.766989827156067e-06

Training epoch-60 batch-109
Running loss of epoch-60 batch-109 = 1.41968484967947e-05

Training epoch-60 batch-110
Running loss of epoch-60 batch-110 = 1.1977506801486015e-05

Training epoch-60 batch-111
Running loss of epoch-60 batch-111 = 1.2867851182818413e-05

Training epoch-60 batch-112
Running loss of epoch-60 batch-112 = 8.424045518040657e-06

Training epoch-60 batch-113
Running loss of epoch-60 batch-113 = 4.984904080629349e-06

Training epoch-60 batch-114
Running loss of epoch-60 batch-114 = 1.494772732257843e-06

Training epoch-60 batch-115
Running loss of epoch-60 batch-115 = 1.3188691809773445e-05

Training epoch-60 batch-116
Running loss of epoch-60 batch-116 = 4.37186099588871e-06

Training epoch-60 batch-117
Running loss of epoch-60 batch-117 = 4.932982847094536e-06

Training epoch-60 batch-118
Running loss of epoch-60 batch-118 = 9.322771802544594e-06

Training epoch-60 batch-119
Running loss of epoch-60 batch-119 = 7.774913683533669e-06

Training epoch-60 batch-120
Running loss of epoch-60 batch-120 = 1.6651581972837448e-05

Training epoch-60 batch-121
Running loss of epoch-60 batch-121 = 5.967216566205025e-06

Training epoch-60 batch-122
Running loss of epoch-60 batch-122 = 1.2430362403392792e-05

Training epoch-60 batch-123
Running loss of epoch-60 batch-123 = 1.003616489470005e-05

Training epoch-60 batch-124
Running loss of epoch-60 batch-124 = 8.832663297653198e-06

Training epoch-60 batch-125
Running loss of epoch-60 batch-125 = 3.660796210169792e-06

Training epoch-60 batch-126
Running loss of epoch-60 batch-126 = 9.457813575863838e-06

Training epoch-60 batch-127
Running loss of epoch-60 batch-127 = 5.698530003428459e-06

Training epoch-60 batch-128
Running loss of epoch-60 batch-128 = 1.219380646944046e-05

Training epoch-60 batch-129
Running loss of epoch-60 batch-129 = 8.711591362953186e-06

Training epoch-60 batch-130
Running loss of epoch-60 batch-130 = 9.514624252915382e-06

Training epoch-60 batch-131
Running loss of epoch-60 batch-131 = 2.698739990592003e-06

Training epoch-60 batch-132
Running loss of epoch-60 batch-132 = 5.204463377594948e-06

Training epoch-60 batch-133
Running loss of epoch-60 batch-133 = 1.9355211406946182e-05

Training epoch-60 batch-134
Running loss of epoch-60 batch-134 = 1.2901145964860916e-05

Training epoch-60 batch-135
Running loss of epoch-60 batch-135 = 6.151385605335236e-06

Training epoch-60 batch-136
Running loss of epoch-60 batch-136 = 1.086830161511898e-05

Training epoch-60 batch-137
Running loss of epoch-60 batch-137 = 1.6025733202695847e-06

Training epoch-60 batch-138
Running loss of epoch-60 batch-138 = 9.3833077698946e-06

Training epoch-60 batch-139
Running loss of epoch-60 batch-139 = 5.367211997509003e-06

Training epoch-60 batch-140
Running loss of epoch-60 batch-140 = 3.7977006286382675e-06

Training epoch-60 batch-141
Running loss of epoch-60 batch-141 = 3.3963005989789963e-06

Training epoch-60 batch-142
Running loss of epoch-60 batch-142 = 5.610054358839989e-06

Training epoch-60 batch-143
Running loss of epoch-60 batch-143 = 3.085937350988388e-06

Training epoch-60 batch-144
Running loss of epoch-60 batch-144 = 2.539483830332756e-06

Training epoch-60 batch-145
Running loss of epoch-60 batch-145 = 1.7541460692882538e-06

Training epoch-60 batch-146
Running loss of epoch-60 batch-146 = 7.5409188866615295e-06

Training epoch-60 batch-147
Running loss of epoch-60 batch-147 = 3.755791112780571e-06

Training epoch-60 batch-148
Running loss of epoch-60 batch-148 = 8.162343874573708e-06

Training epoch-60 batch-149
Running loss of epoch-60 batch-149 = 7.07223080098629e-06

Training epoch-60 batch-150
Running loss of epoch-60 batch-150 = 1.1399155482649803e-05

Training epoch-60 batch-151
Running loss of epoch-60 batch-151 = 7.220078259706497e-06

Training epoch-60 batch-152
Running loss of epoch-60 batch-152 = 5.089212208986282e-06

Training epoch-60 batch-153
Running loss of epoch-60 batch-153 = 1.3076234608888626e-05

Training epoch-60 batch-154
Running loss of epoch-60 batch-154 = 4.8798974603414536e-06

Training epoch-60 batch-155
Running loss of epoch-60 batch-155 = 1.0210322216153145e-05

Training epoch-60 batch-156
Running loss of epoch-60 batch-156 = 7.861526682972908e-06

Training epoch-60 batch-157
Running loss of epoch-60 batch-157 = 4.734843969345093e-05

Finished training epoch-60.



Average train loss at epoch-60 = 7.918209582567216e-06

Started Evaluation

Average val loss at epoch-60 = 1.705295115733414

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 65.53 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.14 %

Finished Evaluation



Started training epoch-61


Training epoch-61 batch-1
Running loss of epoch-61 batch-1 = 5.202600732445717e-06

Training epoch-61 batch-2
Running loss of epoch-61 batch-2 = 2.6440247893333435e-05

Training epoch-61 batch-3
Running loss of epoch-61 batch-3 = 1.2481119483709335e-05

Training epoch-61 batch-4
Running loss of epoch-61 batch-4 = 6.702030077576637e-06

Training epoch-61 batch-5
Running loss of epoch-61 batch-5 = 3.378605470061302e-06

Training epoch-61 batch-6
Running loss of epoch-61 batch-6 = 1.498265191912651e-06

Training epoch-61 batch-7
Running loss of epoch-61 batch-7 = 6.247078999876976e-06

Training epoch-61 batch-8
Running loss of epoch-61 batch-8 = 1.016189344227314e-05

Training epoch-61 batch-9
Running loss of epoch-61 batch-9 = 1.574493944644928e-05

Training epoch-61 batch-10
Running loss of epoch-61 batch-10 = 4.695728421211243e-06

Training epoch-61 batch-11
Running loss of epoch-61 batch-11 = 5.05778007209301e-06

Training epoch-61 batch-12
Running loss of epoch-61 batch-12 = 1.2729549780488014e-05

Training epoch-61 batch-13
Running loss of epoch-61 batch-13 = 6.8354420363903046e-06

Training epoch-61 batch-14
Running loss of epoch-61 batch-14 = 5.0480011850595474e-06

Training epoch-61 batch-15
Running loss of epoch-61 batch-15 = 5.952781066298485e-06

Training epoch-61 batch-16
Running loss of epoch-61 batch-16 = 8.339295163750648e-06

Training epoch-61 batch-17
Running loss of epoch-61 batch-17 = 5.278037860989571e-06

Training epoch-61 batch-18
Running loss of epoch-61 batch-18 = 5.6889839470386505e-06

Training epoch-61 batch-19
Running loss of epoch-61 batch-19 = 7.522059604525566e-06

Training epoch-61 batch-20
Running loss of epoch-61 batch-20 = 1.5242723748087883e-05

Training epoch-61 batch-21
Running loss of epoch-61 batch-21 = 5.853129550814629e-06

Training epoch-61 batch-22
Running loss of epoch-61 batch-22 = 6.072688847780228e-06

Training epoch-61 batch-23
Running loss of epoch-61 batch-23 = 1.5443656593561172e-05

Training epoch-61 batch-24
Running loss of epoch-61 batch-24 = 2.4512410163879395e-06

Training epoch-61 batch-25
Running loss of epoch-61 batch-25 = 8.768867701292038e-06

Training epoch-61 batch-26
Running loss of epoch-61 batch-26 = 1.3443641364574432e-05

Training epoch-61 batch-27
Running loss of epoch-61 batch-27 = 1.3042241334915161e-05

Training epoch-61 batch-28
Running loss of epoch-61 batch-28 = 4.764879122376442e-06

Training epoch-61 batch-29
Running loss of epoch-61 batch-29 = 1.0577496141195297e-05

Training epoch-61 batch-30
Running loss of epoch-61 batch-30 = 6.437534466385841e-06

Training epoch-61 batch-31
Running loss of epoch-61 batch-31 = 8.959788829088211e-06

Training epoch-61 batch-32
Running loss of epoch-61 batch-32 = 7.912050932645798e-06

Training epoch-61 batch-33
Running loss of epoch-61 batch-33 = 3.738095983862877e-06

Training epoch-61 batch-34
Running loss of epoch-61 batch-34 = 4.878966137766838e-06

Training epoch-61 batch-35
Running loss of epoch-61 batch-35 = 1.1919648386538029e-05

Training epoch-61 batch-36
Running loss of epoch-61 batch-36 = 5.755340680480003e-06

Training epoch-61 batch-37
Running loss of epoch-61 batch-37 = 1.1674361303448677e-05

Training epoch-61 batch-38
Running loss of epoch-61 batch-38 = 9.586336091160774e-06

Training epoch-61 batch-39
Running loss of epoch-61 batch-39 = 2.9886141419410706e-06

Training epoch-61 batch-40
Running loss of epoch-61 batch-40 = 5.939975380897522e-06

Training epoch-61 batch-41
Running loss of epoch-61 batch-41 = 4.226807504892349e-06

Training epoch-61 batch-42
Running loss of epoch-61 batch-42 = 1.5093479305505753e-05

Training epoch-61 batch-43
Running loss of epoch-61 batch-43 = 4.738569259643555e-06

Training epoch-61 batch-44
Running loss of epoch-61 batch-44 = 8.821254596114159e-06

Training epoch-61 batch-45
Running loss of epoch-61 batch-45 = 4.7674402594566345e-06

Training epoch-61 batch-46
Running loss of epoch-61 batch-46 = 6.433110684156418e-07

Training epoch-61 batch-47
Running loss of epoch-61 batch-47 = 5.581416189670563e-06

Training epoch-61 batch-48
Running loss of epoch-61 batch-48 = 3.0731316655874252e-06

Training epoch-61 batch-49
Running loss of epoch-61 batch-49 = 8.376315236091614e-06

Training epoch-61 batch-50
Running loss of epoch-61 batch-50 = 1.0576099157333374e-05

Training epoch-61 batch-51
Running loss of epoch-61 batch-51 = 1.0067364200949669e-05

Training epoch-61 batch-52
Running loss of epoch-61 batch-52 = 7.04755075275898e-06

Training epoch-61 batch-53
Running loss of epoch-61 batch-53 = 5.568843334913254e-06

Training epoch-61 batch-54
Running loss of epoch-61 batch-54 = 7.481081411242485e-06

Training epoch-61 batch-55
Running loss of epoch-61 batch-55 = 5.924375727772713e-06

Training epoch-61 batch-56
Running loss of epoch-61 batch-56 = 2.3744069039821625e-06

Training epoch-61 batch-57
Running loss of epoch-61 batch-57 = 5.1895622164011e-06

Training epoch-61 batch-58
Running loss of epoch-61 batch-58 = 1.7210841178894043e-06

Training epoch-61 batch-59
Running loss of epoch-61 batch-59 = 7.3660630732774734e-06

Training epoch-61 batch-60
Running loss of epoch-61 batch-60 = 1.300009898841381e-05

Training epoch-61 batch-61
Running loss of epoch-61 batch-61 = 1.5558209270238876e-05

Training epoch-61 batch-62
Running loss of epoch-61 batch-62 = 2.042856067419052e-06

Training epoch-61 batch-63
Running loss of epoch-61 batch-63 = 2.1220184862613678e-05

Training epoch-61 batch-64
Running loss of epoch-61 batch-64 = 7.05360434949398e-06

Training epoch-61 batch-65
Running loss of epoch-61 batch-65 = 3.5159755498170853e-06

Training epoch-61 batch-66
Running loss of epoch-61 batch-66 = 8.063623681664467e-06

Training epoch-61 batch-67
Running loss of epoch-61 batch-67 = 5.299225449562073e-06

Training epoch-61 batch-68
Running loss of epoch-61 batch-68 = 6.773276254534721e-06

Training epoch-61 batch-69
Running loss of epoch-61 batch-69 = 6.234971806406975e-06

Training epoch-61 batch-70
Running loss of epoch-61 batch-70 = 1.7111655324697495e-05

Training epoch-61 batch-71
Running loss of epoch-61 batch-71 = 6.213784217834473e-06

Training epoch-61 batch-72
Running loss of epoch-61 batch-72 = 7.407739758491516e-06

Training epoch-61 batch-73
Running loss of epoch-61 batch-73 = 5.9711746871471405e-06

Training epoch-61 batch-74
Running loss of epoch-61 batch-74 = 8.105998858809471e-06

Training epoch-61 batch-75
Running loss of epoch-61 batch-75 = 5.010049790143967e-06

Training epoch-61 batch-76
Running loss of epoch-61 batch-76 = 2.771848812699318e-06

Training epoch-61 batch-77
Running loss of epoch-61 batch-77 = 6.603775545954704e-06

Training epoch-61 batch-78
Running loss of epoch-61 batch-78 = 2.3078173398971558e-06

Training epoch-61 batch-79
Running loss of epoch-61 batch-79 = 6.0230959206819534e-06

Training epoch-61 batch-80
Running loss of epoch-61 batch-80 = 9.84175130724907e-06

Training epoch-61 batch-81
Running loss of epoch-61 batch-81 = 6.991438567638397e-06

Training epoch-61 batch-82
Running loss of epoch-61 batch-82 = 4.561617970466614e-06

Training epoch-61 batch-83
Running loss of epoch-61 batch-83 = 5.3676776587963104e-06

Training epoch-61 batch-84
Running loss of epoch-61 batch-84 = 1.0988209396600723e-05

Training epoch-61 batch-85
Running loss of epoch-61 batch-85 = 2.241227775812149e-06

Training epoch-61 batch-86
Running loss of epoch-61 batch-86 = 7.91763886809349e-06

Training epoch-61 batch-87
Running loss of epoch-61 batch-87 = 4.25451435148716e-06

Training epoch-61 batch-88
Running loss of epoch-61 batch-88 = 1.699640415608883e-05

Training epoch-61 batch-89
Running loss of epoch-61 batch-89 = 9.346986189484596e-06

Training epoch-61 batch-90
Running loss of epoch-61 batch-90 = 4.86103817820549e-06

Training epoch-61 batch-91
Running loss of epoch-61 batch-91 = 5.056383088231087e-06

Training epoch-61 batch-92
Running loss of epoch-61 batch-92 = 4.870817065238953e-06

Training epoch-61 batch-93
Running loss of epoch-61 batch-93 = 4.058936610817909e-06

Training epoch-61 batch-94
Running loss of epoch-61 batch-94 = 2.7257483452558517e-06

Training epoch-61 batch-95
Running loss of epoch-61 batch-95 = 8.224742487072945e-06

Training epoch-61 batch-96
Running loss of epoch-61 batch-96 = 2.646585926413536e-06

Training epoch-61 batch-97
Running loss of epoch-61 batch-97 = 4.9818772822618484e-06

Training epoch-61 batch-98
Running loss of epoch-61 batch-98 = 7.33579508960247e-06

Training epoch-61 batch-99
Running loss of epoch-61 batch-99 = 1.55717134475708e-05

Training epoch-61 batch-100
Running loss of epoch-61 batch-100 = 1.1777505278587341e-05

Training epoch-61 batch-101
Running loss of epoch-61 batch-101 = 6.291083991527557e-06

Training epoch-61 batch-102
Running loss of epoch-61 batch-102 = 2.4872133508324623e-05

Training epoch-61 batch-103
Running loss of epoch-61 batch-103 = 1.045013777911663e-05

Training epoch-61 batch-104
Running loss of epoch-61 batch-104 = 1.1459458619356155e-05

Training epoch-61 batch-105
Running loss of epoch-61 batch-105 = 1.475517638027668e-05

Training epoch-61 batch-106
Running loss of epoch-61 batch-106 = 4.042871296405792e-06

Training epoch-61 batch-107
Running loss of epoch-61 batch-107 = 9.857118129730225e-06

Training epoch-61 batch-108
Running loss of epoch-61 batch-108 = 3.7064310163259506e-06

Training epoch-61 batch-109
Running loss of epoch-61 batch-109 = 6.627524271607399e-06

Training epoch-61 batch-110
Running loss of epoch-61 batch-110 = 7.620081305503845e-06

Training epoch-61 batch-111
Running loss of epoch-61 batch-111 = 1.0387739166617393e-05

Training epoch-61 batch-112
Running loss of epoch-61 batch-112 = 8.956994861364365e-07

Training epoch-61 batch-113
Running loss of epoch-61 batch-113 = 5.547655746340752e-06

Training epoch-61 batch-114
Running loss of epoch-61 batch-114 = 6.766989827156067e-06

Training epoch-61 batch-115
Running loss of epoch-61 batch-115 = 5.562789738178253e-06

Training epoch-61 batch-116
Running loss of epoch-61 batch-116 = 3.730412572622299e-06

Training epoch-61 batch-117
Running loss of epoch-61 batch-117 = 4.165340214967728e-06

Training epoch-61 batch-118
Running loss of epoch-61 batch-118 = 3.6533456295728683e-06

Training epoch-61 batch-119
Running loss of epoch-61 batch-119 = 9.405892342329025e-06

Training epoch-61 batch-120
Running loss of epoch-61 batch-120 = 4.760921001434326e-06

Training epoch-61 batch-121
Running loss of epoch-61 batch-121 = 1.1434312909841537e-05

Training epoch-61 batch-122
Running loss of epoch-61 batch-122 = 7.0217065513134e-06

Training epoch-61 batch-123
Running loss of epoch-61 batch-123 = 7.145572453737259e-06

Training epoch-61 batch-124
Running loss of epoch-61 batch-124 = 5.80749474465847e-06

Training epoch-61 batch-125
Running loss of epoch-61 batch-125 = 5.440087988972664e-06

Training epoch-61 batch-126
Running loss of epoch-61 batch-126 = 8.214730769395828e-06

Training epoch-61 batch-127
Running loss of epoch-61 batch-127 = 9.283889085054398e-06

Training epoch-61 batch-128
Running loss of epoch-61 batch-128 = 1.1378899216651917e-05

Training epoch-61 batch-129
Running loss of epoch-61 batch-129 = 3.6281999200582504e-06

Training epoch-61 batch-130
Running loss of epoch-61 batch-130 = 1.0322779417037964e-05

Training epoch-61 batch-131
Running loss of epoch-61 batch-131 = 5.081528797745705e-06

Training epoch-61 batch-132
Running loss of epoch-61 batch-132 = 1.0604038834571838e-05

Training epoch-61 batch-133
Running loss of epoch-61 batch-133 = 1.3988930732011795e-05

Training epoch-61 batch-134
Running loss of epoch-61 batch-134 = 1.1760042980313301e-05

Training epoch-61 batch-135
Running loss of epoch-61 batch-135 = 5.525536835193634e-06

Training epoch-61 batch-136
Running loss of epoch-61 batch-136 = 1.2621749192476273e-06

Training epoch-61 batch-137
Running loss of epoch-61 batch-137 = 2.2186432033777237e-06

Training epoch-61 batch-138
Running loss of epoch-61 batch-138 = 9.67388041317463e-06

Training epoch-61 batch-139
Running loss of epoch-61 batch-139 = 4.610046744346619e-06

Training epoch-61 batch-140
Running loss of epoch-61 batch-140 = 1.494656316936016e-05

Training epoch-61 batch-141
Running loss of epoch-61 batch-141 = 6.394926458597183e-06

Training epoch-61 batch-142
Running loss of epoch-61 batch-142 = 1.017213799059391e-05

Training epoch-61 batch-143
Running loss of epoch-61 batch-143 = 6.084097549319267e-06

Training epoch-61 batch-144
Running loss of epoch-61 batch-144 = 1.6987323760986328e-06

Training epoch-61 batch-145
Running loss of epoch-61 batch-145 = 4.358356818556786e-06

Training epoch-61 batch-146
Running loss of epoch-61 batch-146 = 5.926704034209251e-06

Training epoch-61 batch-147
Running loss of epoch-61 batch-147 = 6.627524271607399e-06

Training epoch-61 batch-148
Running loss of epoch-61 batch-148 = 5.4247211664915085e-06

Training epoch-61 batch-149
Running loss of epoch-61 batch-149 = 1.132860779762268e-05

Training epoch-61 batch-150
Running loss of epoch-61 batch-150 = 8.671777322888374e-06

Training epoch-61 batch-151
Running loss of epoch-61 batch-151 = 9.26060602068901e-06

Training epoch-61 batch-152
Running loss of epoch-61 batch-152 = 9.555835276842117e-06

Training epoch-61 batch-153
Running loss of epoch-61 batch-153 = 7.411930710077286e-06

Training epoch-61 batch-154
Running loss of epoch-61 batch-154 = 8.897623047232628e-06

Training epoch-61 batch-155
Running loss of epoch-61 batch-155 = 8.672010153532028e-06

Training epoch-61 batch-156
Running loss of epoch-61 batch-156 = 3.406079486012459e-06

Training epoch-61 batch-157
Running loss of epoch-61 batch-157 = 1.0240823030471802e-05

Finished training epoch-61.



Average train loss at epoch-61 = 7.592711597681046e-06

Started Evaluation

Average val loss at epoch-61 = 1.7182563570274478

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.23 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.16 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.69 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-62


Training epoch-62 batch-1
Running loss of epoch-62 batch-1 = 1.4157500118017197e-05

Training epoch-62 batch-2
Running loss of epoch-62 batch-2 = 7.24010169506073e-06

Training epoch-62 batch-3
Running loss of epoch-62 batch-3 = 9.306007996201515e-06

Training epoch-62 batch-4
Running loss of epoch-62 batch-4 = 1.5070196241140366e-05

Training epoch-62 batch-5
Running loss of epoch-62 batch-5 = 3.2850075513124466e-06

Training epoch-62 batch-6
Running loss of epoch-62 batch-6 = 7.242895662784576e-06

Training epoch-62 batch-7
Running loss of epoch-62 batch-7 = 3.919471055269241e-06

Training epoch-62 batch-8
Running loss of epoch-62 batch-8 = 6.091082468628883e-06

Training epoch-62 batch-9
Running loss of epoch-62 batch-9 = 5.173031240701675e-06

Training epoch-62 batch-10
Running loss of epoch-62 batch-10 = 7.6263677328825e-06

Training epoch-62 batch-11
Running loss of epoch-62 batch-11 = 6.963731721043587e-06

Training epoch-62 batch-12
Running loss of epoch-62 batch-12 = 3.885012120008469e-06

Training epoch-62 batch-13
Running loss of epoch-62 batch-13 = 4.573492333292961e-06

Training epoch-62 batch-14
Running loss of epoch-62 batch-14 = 1.2640375643968582e-05

Training epoch-62 batch-15
Running loss of epoch-62 batch-15 = 8.879927918314934e-06

Training epoch-62 batch-16
Running loss of epoch-62 batch-16 = 4.82611358165741e-06

Training epoch-62 batch-17
Running loss of epoch-62 batch-17 = 4.285015165805817e-06

Training epoch-62 batch-18
Running loss of epoch-62 batch-18 = 1.116073690354824e-05

Training epoch-62 batch-19
Running loss of epoch-62 batch-19 = 6.678048521280289e-06

Training epoch-62 batch-20
Running loss of epoch-62 batch-20 = 6.485963240265846e-06

Training epoch-62 batch-21
Running loss of epoch-62 batch-21 = 4.584435373544693e-06

Training epoch-62 batch-22
Running loss of epoch-62 batch-22 = 8.181435987353325e-06

Training epoch-62 batch-23
Running loss of epoch-62 batch-23 = 3.9031729102134705e-06

Training epoch-62 batch-24
Running loss of epoch-62 batch-24 = 4.877336323261261e-06

Training epoch-62 batch-25
Running loss of epoch-62 batch-25 = 4.28222119808197e-06

Training epoch-62 batch-26
Running loss of epoch-62 batch-26 = 7.534632459282875e-06

Training epoch-62 batch-27
Running loss of epoch-62 batch-27 = 5.966052412986755e-06

Training epoch-62 batch-28
Running loss of epoch-62 batch-28 = 1.1757249012589455e-05

Training epoch-62 batch-29
Running loss of epoch-62 batch-29 = 2.271495759487152e-06

Training epoch-62 batch-30
Running loss of epoch-62 batch-30 = 5.8407895267009735e-06

Training epoch-62 batch-31
Running loss of epoch-62 batch-31 = 6.477115675806999e-06

Training epoch-62 batch-32
Running loss of epoch-62 batch-32 = 1.7268816009163857e-05

Training epoch-62 batch-33
Running loss of epoch-62 batch-33 = 1.00722536444664e-05

Training epoch-62 batch-34
Running loss of epoch-62 batch-34 = 1.2599397450685501e-05

Training epoch-62 batch-35
Running loss of epoch-62 batch-35 = 1.44254881888628e-05

Training epoch-62 batch-36
Running loss of epoch-62 batch-36 = 2.2230669856071472e-06

Training epoch-62 batch-37
Running loss of epoch-62 batch-37 = 1.1289026588201523e-05

Training epoch-62 batch-38
Running loss of epoch-62 batch-38 = 1.0748859494924545e-05

Training epoch-62 batch-39
Running loss of epoch-62 batch-39 = 5.153007805347443e-06

Training epoch-62 batch-40
Running loss of epoch-62 batch-40 = 6.836606189608574e-06

Training epoch-62 batch-41
Running loss of epoch-62 batch-41 = 7.758615538477898e-06

Training epoch-62 batch-42
Running loss of epoch-62 batch-42 = 5.902955308556557e-06

Training epoch-62 batch-43
Running loss of epoch-62 batch-43 = 3.5045668482780457e-06

Training epoch-62 batch-44
Running loss of epoch-62 batch-44 = 5.852663889527321e-06

Training epoch-62 batch-45
Running loss of epoch-62 batch-45 = 7.422175258398056e-06

Training epoch-62 batch-46
Running loss of epoch-62 batch-46 = 5.1604583859443665e-06

Training epoch-62 batch-47
Running loss of epoch-62 batch-47 = 5.055218935012817e-06

Training epoch-62 batch-48
Running loss of epoch-62 batch-48 = 1.1133728548884392e-05

Training epoch-62 batch-49
Running loss of epoch-62 batch-49 = 2.1193409338593483e-05

Training epoch-62 batch-50
Running loss of epoch-62 batch-50 = 3.468012437224388e-06

Training epoch-62 batch-51
Running loss of epoch-62 batch-51 = 3.92901711165905e-06

Training epoch-62 batch-52
Running loss of epoch-62 batch-52 = 9.592855349183083e-06

Training epoch-62 batch-53
Running loss of epoch-62 batch-53 = 6.269663572311401e-06

Training epoch-62 batch-54
Running loss of epoch-62 batch-54 = 8.846400305628777e-06

Training epoch-62 batch-55
Running loss of epoch-62 batch-55 = 3.133900463581085e-06

Training epoch-62 batch-56
Running loss of epoch-62 batch-56 = 8.622417226433754e-06

Training epoch-62 batch-57
Running loss of epoch-62 batch-57 = 1.2293457984924316e-05

Training epoch-62 batch-58
Running loss of epoch-62 batch-58 = 9.32253897190094e-06

Training epoch-62 batch-59
Running loss of epoch-62 batch-59 = 5.604000762104988e-06

Training epoch-62 batch-60
Running loss of epoch-62 batch-60 = 1.063547097146511e-05

Training epoch-62 batch-61
Running loss of epoch-62 batch-61 = 3.470340743660927e-06

Training epoch-62 batch-62
Running loss of epoch-62 batch-62 = 8.685514330863953e-06

Training epoch-62 batch-63
Running loss of epoch-62 batch-63 = 3.2032839953899384e-06

Training epoch-62 batch-64
Running loss of epoch-62 batch-64 = 6.442191079258919e-06

Training epoch-62 batch-65
Running loss of epoch-62 batch-65 = 4.714587703347206e-06

Training epoch-62 batch-66
Running loss of epoch-62 batch-66 = 6.765127182006836e-06

Training epoch-62 batch-67
Running loss of epoch-62 batch-67 = 5.362788215279579e-06

Training epoch-62 batch-68
Running loss of epoch-62 batch-68 = 1.0204501450061798e-05

Training epoch-62 batch-69
Running loss of epoch-62 batch-69 = 2.423301339149475e-06

Training epoch-62 batch-70
Running loss of epoch-62 batch-70 = 1.445971429347992e-05

Training epoch-62 batch-71
Running loss of epoch-62 batch-71 = 3.041233867406845e-06

Training epoch-62 batch-72
Running loss of epoch-62 batch-72 = 2.96812504529953e-05

Training epoch-62 batch-73
Running loss of epoch-62 batch-73 = 5.095498636364937e-06

Training epoch-62 batch-74
Running loss of epoch-62 batch-74 = 2.9667280614376068e-05

Training epoch-62 batch-75
Running loss of epoch-62 batch-75 = 6.877118721604347e-06

Training epoch-62 batch-76
Running loss of epoch-62 batch-76 = 3.2640527933835983e-06

Training epoch-62 batch-77
Running loss of epoch-62 batch-77 = 6.465241312980652e-06

Training epoch-62 batch-78
Running loss of epoch-62 batch-78 = 2.157408744096756e-06

Training epoch-62 batch-79
Running loss of epoch-62 batch-79 = 1.072511076927185e-05

Training epoch-62 batch-80
Running loss of epoch-62 batch-80 = 4.61796298623085e-06

Training epoch-62 batch-81
Running loss of epoch-62 batch-81 = 5.909474566578865e-06

Training epoch-62 batch-82
Running loss of epoch-62 batch-82 = 1.1130468919873238e-05

Training epoch-62 batch-83
Running loss of epoch-62 batch-83 = 5.165114998817444e-06

Training epoch-62 batch-84
Running loss of epoch-62 batch-84 = 2.830522134900093e-06

Training epoch-62 batch-85
Running loss of epoch-62 batch-85 = 7.193535566329956e-06

Training epoch-62 batch-86
Running loss of epoch-62 batch-86 = 6.533460691571236e-06

Training epoch-62 batch-87
Running loss of epoch-62 batch-87 = 5.580950528383255e-06

Training epoch-62 batch-88
Running loss of epoch-62 batch-88 = 3.4277327358722687e-06

Training epoch-62 batch-89
Running loss of epoch-62 batch-89 = 3.7045683711767197e-06

Training epoch-62 batch-90
Running loss of epoch-62 batch-90 = 1.3444339856505394e-05

Training epoch-62 batch-91
Running loss of epoch-62 batch-91 = 4.987930878996849e-06

Training epoch-62 batch-92
Running loss of epoch-62 batch-92 = 5.1120296120643616e-06

Training epoch-62 batch-93
Running loss of epoch-62 batch-93 = 8.807051926851273e-06

Training epoch-62 batch-94
Running loss of epoch-62 batch-94 = 2.0514708012342453e-06

Training epoch-62 batch-95
Running loss of epoch-62 batch-95 = 3.2088719308376312e-06

Training epoch-62 batch-96
Running loss of epoch-62 batch-96 = 5.009351298213005e-06

Training epoch-62 batch-97
Running loss of epoch-62 batch-97 = 6.336485967040062e-06

Training epoch-62 batch-98
Running loss of epoch-62 batch-98 = 6.741844117641449e-06

Training epoch-62 batch-99
Running loss of epoch-62 batch-99 = 6.171874701976776e-06

Training epoch-62 batch-100
Running loss of epoch-62 batch-100 = 1.998385414481163e-06

Training epoch-62 batch-101
Running loss of epoch-62 batch-101 = 1.9223662093281746e-05

Training epoch-62 batch-102
Running loss of epoch-62 batch-102 = 8.76840204000473e-06

Training epoch-62 batch-103
Running loss of epoch-62 batch-103 = 1.9848812371492386e-06

Training epoch-62 batch-104
Running loss of epoch-62 batch-104 = 3.929249942302704e-06

Training epoch-62 batch-105
Running loss of epoch-62 batch-105 = 1.3441313058137894e-05

Training epoch-62 batch-106
Running loss of epoch-62 batch-106 = 7.582828402519226e-06

Training epoch-62 batch-107
Running loss of epoch-62 batch-107 = 8.084578439593315e-06

Training epoch-62 batch-108
Running loss of epoch-62 batch-108 = 8.346978574991226e-06

Training epoch-62 batch-109
Running loss of epoch-62 batch-109 = 5.564652383327484e-06

Training epoch-62 batch-110
Running loss of epoch-62 batch-110 = 3.605848178267479e-06

Training epoch-62 batch-111
Running loss of epoch-62 batch-111 = 4.324363544583321e-06

Training epoch-62 batch-112
Running loss of epoch-62 batch-112 = 3.2440293580293655e-06

Training epoch-62 batch-113
Running loss of epoch-62 batch-113 = 3.559282049536705e-06

Training epoch-62 batch-114
Running loss of epoch-62 batch-114 = 8.38027335703373e-06

Training epoch-62 batch-115
Running loss of epoch-62 batch-115 = 2.288026735186577e-06

Training epoch-62 batch-116
Running loss of epoch-62 batch-116 = 1.3567740097641945e-05

Training epoch-62 batch-117
Running loss of epoch-62 batch-117 = 5.461042746901512e-06

Training epoch-62 batch-118
Running loss of epoch-62 batch-118 = 5.134148523211479e-06

Training epoch-62 batch-119
Running loss of epoch-62 batch-119 = 3.7318095564842224e-06

Training epoch-62 batch-120
Running loss of epoch-62 batch-120 = 8.925562724471092e-06

Training epoch-62 batch-121
Running loss of epoch-62 batch-121 = 9.406590834259987e-06

Training epoch-62 batch-122
Running loss of epoch-62 batch-122 = 4.198402166366577e-06

Training epoch-62 batch-123
Running loss of epoch-62 batch-123 = 4.162779077887535e-06

Training epoch-62 batch-124
Running loss of epoch-62 batch-124 = 3.550201654434204e-06

Training epoch-62 batch-125
Running loss of epoch-62 batch-125 = 4.6710483729839325e-06

Training epoch-62 batch-126
Running loss of epoch-62 batch-126 = 1.0618241503834724e-05

Training epoch-62 batch-127
Running loss of epoch-62 batch-127 = 2.057058736681938e-06

Training epoch-62 batch-128
Running loss of epoch-62 batch-128 = 2.6785768568515778e-05

Training epoch-62 batch-129
Running loss of epoch-62 batch-129 = 2.4919863790273666e-06

Training epoch-62 batch-130
Running loss of epoch-62 batch-130 = 2.111378125846386e-05

Training epoch-62 batch-131
Running loss of epoch-62 batch-131 = 3.99700365960598e-06

Training epoch-62 batch-132
Running loss of epoch-62 batch-132 = 8.651986718177795e-06

Training epoch-62 batch-133
Running loss of epoch-62 batch-133 = 2.3688189685344696e-06

Training epoch-62 batch-134
Running loss of epoch-62 batch-134 = 4.991656169295311e-06

Training epoch-62 batch-135
Running loss of epoch-62 batch-135 = 6.098998710513115e-06

Training epoch-62 batch-136
Running loss of epoch-62 batch-136 = 6.586778908967972e-06

Training epoch-62 batch-137
Running loss of epoch-62 batch-137 = 8.848030120134354e-06

Training epoch-62 batch-138
Running loss of epoch-62 batch-138 = 2.8531067073345184e-06

Training epoch-62 batch-139
Running loss of epoch-62 batch-139 = 9.346520528197289e-06

Training epoch-62 batch-140
Running loss of epoch-62 batch-140 = 5.194451659917831e-06

Training epoch-62 batch-141
Running loss of epoch-62 batch-141 = 6.988411769270897e-06

Training epoch-62 batch-142
Running loss of epoch-62 batch-142 = 1.1381227523088455e-05

Training epoch-62 batch-143
Running loss of epoch-62 batch-143 = 3.5087577998638153e-06

Training epoch-62 batch-144
Running loss of epoch-62 batch-144 = 4.808185622096062e-06

Training epoch-62 batch-145
Running loss of epoch-62 batch-145 = 4.215864464640617e-06

Training epoch-62 batch-146
Running loss of epoch-62 batch-146 = 2.3655593395233154e-06

Training epoch-62 batch-147
Running loss of epoch-62 batch-147 = 2.6355613954365253e-05

Training epoch-62 batch-148
Running loss of epoch-62 batch-148 = 2.566492184996605e-06

Training epoch-62 batch-149
Running loss of epoch-62 batch-149 = 1.4339340850710869e-05

Training epoch-62 batch-150
Running loss of epoch-62 batch-150 = 2.2526364773511887e-06

Training epoch-62 batch-151
Running loss of epoch-62 batch-151 = 9.054318070411682e-06

Training epoch-62 batch-152
Running loss of epoch-62 batch-152 = 2.3909378796815872e-06

Training epoch-62 batch-153
Running loss of epoch-62 batch-153 = 4.299217835068703e-06

Training epoch-62 batch-154
Running loss of epoch-62 batch-154 = 7.185153663158417e-06

Training epoch-62 batch-155
Running loss of epoch-62 batch-155 = 1.4683464542031288e-05

Training epoch-62 batch-156
Running loss of epoch-62 batch-156 = 1.4119548723101616e-05

Training epoch-62 batch-157
Running loss of epoch-62 batch-157 = 3.762543201446533e-06

Finished training epoch-62.



Average train loss at epoch-62 = 7.435201853513717e-06

Started Evaluation

Average val loss at epoch-62 = 1.7148354880410301

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-63


Training epoch-63 batch-1
Running loss of epoch-63 batch-1 = 5.434965714812279e-06

Training epoch-63 batch-2
Running loss of epoch-63 batch-2 = 7.993308827280998e-06

Training epoch-63 batch-3
Running loss of epoch-63 batch-3 = 6.841728463768959e-06

Training epoch-63 batch-4
Running loss of epoch-63 batch-4 = 3.02167609333992e-06

Training epoch-63 batch-5
Running loss of epoch-63 batch-5 = 4.648929461836815e-06

Training epoch-63 batch-6
Running loss of epoch-63 batch-6 = 8.406583219766617e-06

Training epoch-63 batch-7
Running loss of epoch-63 batch-7 = 4.270812496542931e-06

Training epoch-63 batch-8
Running loss of epoch-63 batch-8 = 8.379807695746422e-06

Training epoch-63 batch-9
Running loss of epoch-63 batch-9 = 1.0130694136023521e-05

Training epoch-63 batch-10
Running loss of epoch-63 batch-10 = 5.4871197789907455e-06

Training epoch-63 batch-11
Running loss of epoch-63 batch-11 = 6.168847903609276e-06

Training epoch-63 batch-12
Running loss of epoch-63 batch-12 = 2.7224188670516014e-05

Training epoch-63 batch-13
Running loss of epoch-63 batch-13 = 1.542968675494194e-06

Training epoch-63 batch-14
Running loss of epoch-63 batch-14 = 9.129522368311882e-06

Training epoch-63 batch-15
Running loss of epoch-63 batch-15 = 6.279675289988518e-06

Training epoch-63 batch-16
Running loss of epoch-63 batch-16 = 4.559522494673729e-06

Training epoch-63 batch-17
Running loss of epoch-63 batch-17 = 6.36884942650795e-06

Training epoch-63 batch-18
Running loss of epoch-63 batch-18 = 1.0062707588076591e-05

Training epoch-63 batch-19
Running loss of epoch-63 batch-19 = 3.484543412923813e-06

Training epoch-63 batch-20
Running loss of epoch-63 batch-20 = 7.873866707086563e-06

Training epoch-63 batch-21
Running loss of epoch-63 batch-21 = 3.2722018659114838e-06

Training epoch-63 batch-22
Running loss of epoch-63 batch-22 = 4.341825842857361e-06

Training epoch-63 batch-23
Running loss of epoch-63 batch-23 = 7.81170092523098e-06

Training epoch-63 batch-24
Running loss of epoch-63 batch-24 = 1.7069512978196144e-05

Training epoch-63 batch-25
Running loss of epoch-63 batch-25 = 4.105269908905029e-06

Training epoch-63 batch-26
Running loss of epoch-63 batch-26 = 1.089414581656456e-05

Training epoch-63 batch-27
Running loss of epoch-63 batch-27 = 5.706446245312691e-06

Training epoch-63 batch-28
Running loss of epoch-63 batch-28 = 2.6523368433117867e-05

Training epoch-63 batch-29
Running loss of epoch-63 batch-29 = 1.095375046133995e-05

Training epoch-63 batch-30
Running loss of epoch-63 batch-30 = 3.9851292967796326e-06

Training epoch-63 batch-31
Running loss of epoch-63 batch-31 = 8.684350177645683e-06

Training epoch-63 batch-32
Running loss of epoch-63 batch-32 = 4.443107172846794e-06

Training epoch-63 batch-33
Running loss of epoch-63 batch-33 = 7.920432835817337e-06

Training epoch-63 batch-34
Running loss of epoch-63 batch-34 = 6.990740075707436e-06

Training epoch-63 batch-35
Running loss of epoch-63 batch-35 = 5.499925464391708e-06

Training epoch-63 batch-36
Running loss of epoch-63 batch-36 = 7.641734555363655e-06

Training epoch-63 batch-37
Running loss of epoch-63 batch-37 = 1.8994323909282684e-06

Training epoch-63 batch-38
Running loss of epoch-63 batch-38 = 1.4028279110789299e-05

Training epoch-63 batch-39
Running loss of epoch-63 batch-39 = 4.792818799614906e-06

Training epoch-63 batch-40
Running loss of epoch-63 batch-40 = 5.428679287433624e-06

Training epoch-63 batch-41
Running loss of epoch-63 batch-41 = 8.301343768835068e-06

Training epoch-63 batch-42
Running loss of epoch-63 batch-42 = 1.246877945959568e-05

Training epoch-63 batch-43
Running loss of epoch-63 batch-43 = 1.1674826964735985e-05

Training epoch-63 batch-44
Running loss of epoch-63 batch-44 = 6.631482392549515e-06

Training epoch-63 batch-45
Running loss of epoch-63 batch-45 = 4.513654857873917e-06

Training epoch-63 batch-46
Running loss of epoch-63 batch-46 = 8.127884939312935e-06

Training epoch-63 batch-47
Running loss of epoch-63 batch-47 = 2.2766180336475372e-06

Training epoch-63 batch-48
Running loss of epoch-63 batch-48 = 8.635688573122025e-06

Training epoch-63 batch-49
Running loss of epoch-63 batch-49 = 8.345115929841995e-06

Training epoch-63 batch-50
Running loss of epoch-63 batch-50 = 7.29365274310112e-06

Training epoch-63 batch-51
Running loss of epoch-63 batch-51 = 5.452893674373627e-06

Training epoch-63 batch-52
Running loss of epoch-63 batch-52 = 5.790498107671738e-06

Training epoch-63 batch-53
Running loss of epoch-63 batch-53 = 8.000759407877922e-06

Training epoch-63 batch-54
Running loss of epoch-63 batch-54 = 6.9821253418922424e-06

Training epoch-63 batch-55
Running loss of epoch-63 batch-55 = 1.950422301888466e-06

Training epoch-63 batch-56
Running loss of epoch-63 batch-56 = 6.567919626832008e-06

Training epoch-63 batch-57
Running loss of epoch-63 batch-57 = 5.161389708518982e-06

Training epoch-63 batch-58
Running loss of epoch-63 batch-58 = 6.117159500718117e-06

Training epoch-63 batch-59
Running loss of epoch-63 batch-59 = 2.3369211703538895e-06

Training epoch-63 batch-60
Running loss of epoch-63 batch-60 = 1.2530945241451263e-06

Training epoch-63 batch-61
Running loss of epoch-63 batch-61 = 1.0917428880929947e-05

Training epoch-63 batch-62
Running loss of epoch-63 batch-62 = 3.059394657611847e-06

Training epoch-63 batch-63
Running loss of epoch-63 batch-63 = 1.1281343176960945e-05

Training epoch-63 batch-64
Running loss of epoch-63 batch-64 = 6.063841283321381e-06

Training epoch-63 batch-65
Running loss of epoch-63 batch-65 = 2.9001384973526e-06

Training epoch-63 batch-66
Running loss of epoch-63 batch-66 = 1.451326534152031e-05

Training epoch-63 batch-67
Running loss of epoch-63 batch-67 = 7.071765139698982e-06

Training epoch-63 batch-68
Running loss of epoch-63 batch-68 = 2.9532238841056824e-06

Training epoch-63 batch-69
Running loss of epoch-63 batch-69 = 4.821689799427986e-06

Training epoch-63 batch-70
Running loss of epoch-63 batch-70 = 8.087605237960815e-06

Training epoch-63 batch-71
Running loss of epoch-63 batch-71 = 7.322290912270546e-06

Training epoch-63 batch-72
Running loss of epoch-63 batch-72 = 7.5320713222026825e-06

Training epoch-63 batch-73
Running loss of epoch-63 batch-73 = 7.577007636427879e-06

Training epoch-63 batch-74
Running loss of epoch-63 batch-74 = 3.3155083656311035e-06

Training epoch-63 batch-75
Running loss of epoch-63 batch-75 = 8.644303306937218e-06

Training epoch-63 batch-76
Running loss of epoch-63 batch-76 = 6.422866135835648e-06

Training epoch-63 batch-77
Running loss of epoch-63 batch-77 = 6.905756890773773e-06

Training epoch-63 batch-78
Running loss of epoch-63 batch-78 = 9.936513379216194e-06

Training epoch-63 batch-79
Running loss of epoch-63 batch-79 = 4.153931513428688e-06

Training epoch-63 batch-80
Running loss of epoch-63 batch-80 = 8.305534720420837e-06

Training epoch-63 batch-81
Running loss of epoch-63 batch-81 = 4.77558933198452e-06

Training epoch-63 batch-82
Running loss of epoch-63 batch-82 = 8.124392479658127e-06

Training epoch-63 batch-83
Running loss of epoch-63 batch-83 = 1.8815044313669205e-05

Training epoch-63 batch-84
Running loss of epoch-63 batch-84 = 3.121793270111084e-06

Training epoch-63 batch-85
Running loss of epoch-63 batch-85 = 3.3767428249120712e-06

Training epoch-63 batch-86
Running loss of epoch-63 batch-86 = 7.441500201821327e-06

Training epoch-63 batch-87
Running loss of epoch-63 batch-87 = 1.0754913091659546e-05

Training epoch-63 batch-88
Running loss of epoch-63 batch-88 = 3.4796539694070816e-06

Training epoch-63 batch-89
Running loss of epoch-63 batch-89 = 7.855473086237907e-06

Training epoch-63 batch-90
Running loss of epoch-63 batch-90 = 4.158820956945419e-06

Training epoch-63 batch-91
Running loss of epoch-63 batch-91 = 1.795426942408085e-05

Training epoch-63 batch-92
Running loss of epoch-63 batch-92 = 8.960720151662827e-06

Training epoch-63 batch-93
Running loss of epoch-63 batch-93 = 4.22564335167408e-06

Training epoch-63 batch-94
Running loss of epoch-63 batch-94 = 5.548587068915367e-06

Training epoch-63 batch-95
Running loss of epoch-63 batch-95 = 1.350371167063713e-05

Training epoch-63 batch-96
Running loss of epoch-63 batch-96 = 2.2617168724536896e-06

Training epoch-63 batch-97
Running loss of epoch-63 batch-97 = 7.6214782893657684e-06

Training epoch-63 batch-98
Running loss of epoch-63 batch-98 = 2.456805668771267e-05

Training epoch-63 batch-99
Running loss of epoch-63 batch-99 = 6.384681910276413e-06

Training epoch-63 batch-100
Running loss of epoch-63 batch-100 = 5.319481715559959e-06

Training epoch-63 batch-101
Running loss of epoch-63 batch-101 = 1.126783899962902e-05

Training epoch-63 batch-102
Running loss of epoch-63 batch-102 = 2.9099872335791588e-05

Training epoch-63 batch-103
Running loss of epoch-63 batch-103 = 1.1829659342765808e-05

Training epoch-63 batch-104
Running loss of epoch-63 batch-104 = 1.698499545454979e-06

Training epoch-63 batch-105
Running loss of epoch-63 batch-105 = 6.292248144745827e-06

Training epoch-63 batch-106
Running loss of epoch-63 batch-106 = 7.394002750515938e-06

Training epoch-63 batch-107
Running loss of epoch-63 batch-107 = 1.7853453755378723e-06

Training epoch-63 batch-108
Running loss of epoch-63 batch-108 = 4.879431799054146e-06

Training epoch-63 batch-109
Running loss of epoch-63 batch-109 = 8.313683792948723e-06

Training epoch-63 batch-110
Running loss of epoch-63 batch-110 = 1.7220154404640198e-06

Training epoch-63 batch-111
Running loss of epoch-63 batch-111 = 5.496665835380554e-06

Training epoch-63 batch-112
Running loss of epoch-63 batch-112 = 1.2164236977696419e-05

Training epoch-63 batch-113
Running loss of epoch-63 batch-113 = 3.9800070226192474e-06

Training epoch-63 batch-114
Running loss of epoch-63 batch-114 = 1.1081341654062271e-05

Training epoch-63 batch-115
Running loss of epoch-63 batch-115 = 5.5015552788972855e-06

Training epoch-63 batch-116
Running loss of epoch-63 batch-116 = 6.107613444328308e-06

Training epoch-63 batch-117
Running loss of epoch-63 batch-117 = 1.1563533917069435e-05

Training epoch-63 batch-118
Running loss of epoch-63 batch-118 = 7.773051038384438e-06

Training epoch-63 batch-119
Running loss of epoch-63 batch-119 = 3.877794370055199e-06

Training epoch-63 batch-120
Running loss of epoch-63 batch-120 = 1.8134014680981636e-05

Training epoch-63 batch-121
Running loss of epoch-63 batch-121 = 1.3541430234909058e-06

Training epoch-63 batch-122
Running loss of epoch-63 batch-122 = 4.6263448894023895e-06

Training epoch-63 batch-123
Running loss of epoch-63 batch-123 = 7.5302086770534515e-06

Training epoch-63 batch-124
Running loss of epoch-63 batch-124 = 7.72206112742424e-06

Training epoch-63 batch-125
Running loss of epoch-63 batch-125 = 3.010733053088188e-06

Training epoch-63 batch-126
Running loss of epoch-63 batch-126 = 6.099464371800423e-06

Training epoch-63 batch-127
Running loss of epoch-63 batch-127 = 1.0005896911025047e-05

Training epoch-63 batch-128
Running loss of epoch-63 batch-128 = 3.531109541654587e-06

Training epoch-63 batch-129
Running loss of epoch-63 batch-129 = 3.908295184373856e-06

Training epoch-63 batch-130
Running loss of epoch-63 batch-130 = 6.18956983089447e-06

Training epoch-63 batch-131
Running loss of epoch-63 batch-131 = 3.7206336855888367e-06

Training epoch-63 batch-132
Running loss of epoch-63 batch-132 = 7.992144674062729e-06

Training epoch-63 batch-133
Running loss of epoch-63 batch-133 = 3.5990960896015167e-06

Training epoch-63 batch-134
Running loss of epoch-63 batch-134 = 6.779097020626068e-06

Training epoch-63 batch-135
Running loss of epoch-63 batch-135 = 5.518551915884018e-06

Training epoch-63 batch-136
Running loss of epoch-63 batch-136 = 4.936475306749344e-06

Training epoch-63 batch-137
Running loss of epoch-63 batch-137 = 3.1457748264074326e-06

Training epoch-63 batch-138
Running loss of epoch-63 batch-138 = 7.727998308837414e-06

Training epoch-63 batch-139
Running loss of epoch-63 batch-139 = 4.545086994767189e-06

Training epoch-63 batch-140
Running loss of epoch-63 batch-140 = 1.0757707059383392e-05

Training epoch-63 batch-141
Running loss of epoch-63 batch-141 = 9.290175512433052e-06

Training epoch-63 batch-142
Running loss of epoch-63 batch-142 = 5.5371783673763275e-06

Training epoch-63 batch-143
Running loss of epoch-63 batch-143 = 7.186317816376686e-06

Training epoch-63 batch-144
Running loss of epoch-63 batch-144 = 9.228475391864777e-06

Training epoch-63 batch-145
Running loss of epoch-63 batch-145 = 2.9620714485645294e-06

Training epoch-63 batch-146
Running loss of epoch-63 batch-146 = 1.8780119717121124e-06

Training epoch-63 batch-147
Running loss of epoch-63 batch-147 = 5.43985515832901e-06

Training epoch-63 batch-148
Running loss of epoch-63 batch-148 = 4.823319613933563e-06

Training epoch-63 batch-149
Running loss of epoch-63 batch-149 = 3.3117830753326416e-06

Training epoch-63 batch-150
Running loss of epoch-63 batch-150 = 2.266606315970421e-06

Training epoch-63 batch-151
Running loss of epoch-63 batch-151 = 8.817296475172043e-06

Training epoch-63 batch-152
Running loss of epoch-63 batch-152 = 4.672212526202202e-06

Training epoch-63 batch-153
Running loss of epoch-63 batch-153 = 4.438683390617371e-06

Training epoch-63 batch-154
Running loss of epoch-63 batch-154 = 4.501547664403915e-06

Training epoch-63 batch-155
Running loss of epoch-63 batch-155 = 7.386785000562668e-06

Training epoch-63 batch-156
Running loss of epoch-63 batch-156 = 8.485512807965279e-06

Training epoch-63 batch-157
Running loss of epoch-63 batch-157 = 1.3519078493118286e-05

Finished training epoch-63.



Average train loss at epoch-63 = 7.190345972776413e-06

Started Evaluation

Average val loss at epoch-63 = 1.720661408806764

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.12 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.08 %

Finished Evaluation



Started training epoch-64


Training epoch-64 batch-1
Running loss of epoch-64 batch-1 = 6.523216143250465e-06

Training epoch-64 batch-2
Running loss of epoch-64 batch-2 = 1.1136755347251892e-05

Training epoch-64 batch-3
Running loss of epoch-64 batch-3 = 1.420942135155201e-05

Training epoch-64 batch-4
Running loss of epoch-64 batch-4 = 1.4847610145807266e-06

Training epoch-64 batch-5
Running loss of epoch-64 batch-5 = 4.001660272479057e-06

Training epoch-64 batch-6
Running loss of epoch-64 batch-6 = 5.564885213971138e-06

Training epoch-64 batch-7
Running loss of epoch-64 batch-7 = 4.341360181570053e-06

Training epoch-64 batch-8
Running loss of epoch-64 batch-8 = 3.6458950489759445e-06

Training epoch-64 batch-9
Running loss of epoch-64 batch-9 = 3.6461278796195984e-06

Training epoch-64 batch-10
Running loss of epoch-64 batch-10 = 6.466638296842575e-06

Training epoch-64 batch-11
Running loss of epoch-64 batch-11 = 6.43683597445488e-06

Training epoch-64 batch-12
Running loss of epoch-64 batch-12 = 4.257773980498314e-06

Training epoch-64 batch-13
Running loss of epoch-64 batch-13 = 6.2873587012290955e-06

Training epoch-64 batch-14
Running loss of epoch-64 batch-14 = 9.384239092469215e-06

Training epoch-64 batch-15
Running loss of epoch-64 batch-15 = 7.325084879994392e-06

Training epoch-64 batch-16
Running loss of epoch-64 batch-16 = 5.331123247742653e-06

Training epoch-64 batch-17
Running loss of epoch-64 batch-17 = 4.802830517292023e-06

Training epoch-64 batch-18
Running loss of epoch-64 batch-18 = 7.912050932645798e-06

Training epoch-64 batch-19
Running loss of epoch-64 batch-19 = 4.91878017783165e-06

Training epoch-64 batch-20
Running loss of epoch-64 batch-20 = 2.2419262677431107e-06

Training epoch-64 batch-21
Running loss of epoch-64 batch-21 = 3.2945536077022552e-06

Training epoch-64 batch-22
Running loss of epoch-64 batch-22 = 1.73195730894804e-05

Training epoch-64 batch-23
Running loss of epoch-64 batch-23 = 3.789784386754036e-06

Training epoch-64 batch-24
Running loss of epoch-64 batch-24 = 7.151160389184952e-06

Training epoch-64 batch-25
Running loss of epoch-64 batch-25 = 3.971857950091362e-06

Training epoch-64 batch-26
Running loss of epoch-64 batch-26 = 3.775348886847496e-06

Training epoch-64 batch-27
Running loss of epoch-64 batch-27 = 3.2780226320028305e-06

Training epoch-64 batch-28
Running loss of epoch-64 batch-28 = 7.72625207901001e-06

Training epoch-64 batch-29
Running loss of epoch-64 batch-29 = 2.315966412425041e-06

Training epoch-64 batch-30
Running loss of epoch-64 batch-30 = 5.261274054646492e-06

Training epoch-64 batch-31
Running loss of epoch-64 batch-31 = 6.0908496379852295e-06

Training epoch-64 batch-32
Running loss of epoch-64 batch-32 = 3.2775569707155228e-06

Training epoch-64 batch-33
Running loss of epoch-64 batch-33 = 3.0330847948789597e-06

Training epoch-64 batch-34
Running loss of epoch-64 batch-34 = 6.8140216171741486e-06

Training epoch-64 batch-35
Running loss of epoch-64 batch-35 = 7.635680958628654e-06

Training epoch-64 batch-36
Running loss of epoch-64 batch-36 = 8.958857506513596e-06

Training epoch-64 batch-37
Running loss of epoch-64 batch-37 = 6.290385499596596e-06

Training epoch-64 batch-38
Running loss of epoch-64 batch-38 = 5.680834874510765e-06

Training epoch-64 batch-39
Running loss of epoch-64 batch-39 = 3.913650289177895e-06

Training epoch-64 batch-40
Running loss of epoch-64 batch-40 = 1.477263867855072e-05

Training epoch-64 batch-41
Running loss of epoch-64 batch-41 = 6.417045369744301e-06

Training epoch-64 batch-42
Running loss of epoch-64 batch-42 = 6.220303475856781e-06

Training epoch-64 batch-43
Running loss of epoch-64 batch-43 = 6.113201379776001e-06

Training epoch-64 batch-44
Running loss of epoch-64 batch-44 = 8.628470823168755e-06

Training epoch-64 batch-45
Running loss of epoch-64 batch-45 = 5.149515345692635e-06

Training epoch-64 batch-46
Running loss of epoch-64 batch-46 = 9.027775377035141e-06

Training epoch-64 batch-47
Running loss of epoch-64 batch-47 = 7.220543920993805e-06

Training epoch-64 batch-48
Running loss of epoch-64 batch-48 = 1.4028279110789299e-05

Training epoch-64 batch-49
Running loss of epoch-64 batch-49 = 7.542083039879799e-06

Training epoch-64 batch-50
Running loss of epoch-64 batch-50 = 3.5564880818128586e-06

Training epoch-64 batch-51
Running loss of epoch-64 batch-51 = 1.3606157153844833e-05

Training epoch-64 batch-52
Running loss of epoch-64 batch-52 = 5.576293915510178e-06

Training epoch-64 batch-53
Running loss of epoch-64 batch-53 = 8.11438076198101e-06

Training epoch-64 batch-54
Running loss of epoch-64 batch-54 = 5.832407623529434e-06

Training epoch-64 batch-55
Running loss of epoch-64 batch-55 = 2.248678356409073e-06

Training epoch-64 batch-56
Running loss of epoch-64 batch-56 = 1.914752647280693e-05

Training epoch-64 batch-57
Running loss of epoch-64 batch-57 = 2.898508682847023e-06

Training epoch-64 batch-58
Running loss of epoch-64 batch-58 = 1.1678086593747139e-05

Training epoch-64 batch-59
Running loss of epoch-64 batch-59 = 2.521323040127754e-06

Training epoch-64 batch-60
Running loss of epoch-64 batch-60 = 6.6283391788601875e-06

Training epoch-64 batch-61
Running loss of epoch-64 batch-61 = 3.995141014456749e-06

Training epoch-64 batch-62
Running loss of epoch-64 batch-62 = 3.7031713873147964e-06

Training epoch-64 batch-63
Running loss of epoch-64 batch-63 = 8.310424163937569e-06

Training epoch-64 batch-64
Running loss of epoch-64 batch-64 = 3.943685442209244e-06

Training epoch-64 batch-65
Running loss of epoch-64 batch-65 = 5.707377567887306e-06

Training epoch-64 batch-66
Running loss of epoch-64 batch-66 = 3.1210947781801224e-06

Training epoch-64 batch-67
Running loss of epoch-64 batch-67 = 4.086876288056374e-06

Training epoch-64 batch-68
Running loss of epoch-64 batch-68 = 1.4542602002620697e-06

Training epoch-64 batch-69
Running loss of epoch-64 batch-69 = 1.3446202501654625e-05

Training epoch-64 batch-70
Running loss of epoch-64 batch-70 = 7.837312296032906e-06

Training epoch-64 batch-71
Running loss of epoch-64 batch-71 = 4.388624802231789e-06

Training epoch-64 batch-72
Running loss of epoch-64 batch-72 = 9.007751941680908e-06

Training epoch-64 batch-73
Running loss of epoch-64 batch-73 = 1.0529765859246254e-05

Training epoch-64 batch-74
Running loss of epoch-64 batch-74 = 4.022382199764252e-06

Training epoch-64 batch-75
Running loss of epoch-64 batch-75 = 9.22870822250843e-06

Training epoch-64 batch-76
Running loss of epoch-64 batch-76 = 6.876187399029732e-06

Training epoch-64 batch-77
Running loss of epoch-64 batch-77 = 8.238246664404869e-06

Training epoch-64 batch-78
Running loss of epoch-64 batch-78 = 6.43986277282238e-06

Training epoch-64 batch-79
Running loss of epoch-64 batch-79 = 1.4010583981871605e-05

Training epoch-64 batch-80
Running loss of epoch-64 batch-80 = 6.751855835318565e-06

Training epoch-64 batch-81
Running loss of epoch-64 batch-81 = 5.386071279644966e-06

Training epoch-64 batch-82
Running loss of epoch-64 batch-82 = 6.546033546328545e-06

Training epoch-64 batch-83
Running loss of epoch-64 batch-83 = 5.664769560098648e-06

Training epoch-64 batch-84
Running loss of epoch-64 batch-84 = 2.9511284083127975e-06

Training epoch-64 batch-85
Running loss of epoch-64 batch-85 = 2.3778993636369705e-06

Training epoch-64 batch-86
Running loss of epoch-64 batch-86 = 3.0209776014089584e-06

Training epoch-64 batch-87
Running loss of epoch-64 batch-87 = 3.6866404116153717e-06

Training epoch-64 batch-88
Running loss of epoch-64 batch-88 = 7.795402780175209e-06

Training epoch-64 batch-89
Running loss of epoch-64 batch-89 = 5.441019311547279e-06

Training epoch-64 batch-90
Running loss of epoch-64 batch-90 = 1.8779654055833817e-05

Training epoch-64 batch-91
Running loss of epoch-64 batch-91 = 5.63240610063076e-06

Training epoch-64 batch-92
Running loss of epoch-64 batch-92 = 4.449393600225449e-06

Training epoch-64 batch-93
Running loss of epoch-64 batch-93 = 1.1411495506763458e-05

Training epoch-64 batch-94
Running loss of epoch-64 batch-94 = 3.5606790333986282e-06

Training epoch-64 batch-95
Running loss of epoch-64 batch-95 = 4.437984898686409e-06

Training epoch-64 batch-96
Running loss of epoch-64 batch-96 = 2.944725565612316e-05

Training epoch-64 batch-97
Running loss of epoch-64 batch-97 = 3.344425931572914e-05

Training epoch-64 batch-98
Running loss of epoch-64 batch-98 = 7.298309355974197e-06

Training epoch-64 batch-99
Running loss of epoch-64 batch-99 = 6.193527951836586e-06

Training epoch-64 batch-100
Running loss of epoch-64 batch-100 = 5.16488216817379e-06

Training epoch-64 batch-101
Running loss of epoch-64 batch-101 = 6.698304787278175e-06

Training epoch-64 batch-102
Running loss of epoch-64 batch-102 = 3.6263372749090195e-06

Training epoch-64 batch-103
Running loss of epoch-64 batch-103 = 4.18233685195446e-06

Training epoch-64 batch-104
Running loss of epoch-64 batch-104 = 4.590954631567001e-06

Training epoch-64 batch-105
Running loss of epoch-64 batch-105 = 8.809147402644157e-06

Training epoch-64 batch-106
Running loss of epoch-64 batch-106 = 3.112480044364929e-06

Training epoch-64 batch-107
Running loss of epoch-64 batch-107 = 3.7956051528453827e-06

Training epoch-64 batch-108
Running loss of epoch-64 batch-108 = 4.9907248467206955e-06

Training epoch-64 batch-109
Running loss of epoch-64 batch-109 = 6.833579391241074e-06

Training epoch-64 batch-110
Running loss of epoch-64 batch-110 = 3.1564850360155106e-06

Training epoch-64 batch-111
Running loss of epoch-64 batch-111 = 1.5994999557733536e-05

Training epoch-64 batch-112
Running loss of epoch-64 batch-112 = 6.3963234424591064e-06

Training epoch-64 batch-113
Running loss of epoch-64 batch-113 = 6.357207894325256e-06

Training epoch-64 batch-114
Running loss of epoch-64 batch-114 = 1.0718125849962234e-05

Training epoch-64 batch-115
Running loss of epoch-64 batch-115 = 3.1213276088237762e-06

Training epoch-64 batch-116
Running loss of epoch-64 batch-116 = 9.893206879496574e-06

Training epoch-64 batch-117
Running loss of epoch-64 batch-117 = 2.4798791855573654e-06

Training epoch-64 batch-118
Running loss of epoch-64 batch-118 = 6.458256393671036e-06

Training epoch-64 batch-119
Running loss of epoch-64 batch-119 = 5.346257239580154e-06

Training epoch-64 batch-120
Running loss of epoch-64 batch-120 = 1.089111901819706e-05

Training epoch-64 batch-121
Running loss of epoch-64 batch-121 = 6.225891411304474e-06

Training epoch-64 batch-122
Running loss of epoch-64 batch-122 = 4.281522706151009e-06

Training epoch-64 batch-123
Running loss of epoch-64 batch-123 = 8.889706805348396e-06

Training epoch-64 batch-124
Running loss of epoch-64 batch-124 = 1.2351665645837784e-05

Training epoch-64 batch-125
Running loss of epoch-64 batch-125 = 4.507135599851608e-06

Training epoch-64 batch-126
Running loss of epoch-64 batch-126 = 5.610054358839989e-06

Training epoch-64 batch-127
Running loss of epoch-64 batch-127 = 2.0519597455859184e-05

Training epoch-64 batch-128
Running loss of epoch-64 batch-128 = 4.493631422519684e-06

Training epoch-64 batch-129
Running loss of epoch-64 batch-129 = 4.530884325504303e-06

Training epoch-64 batch-130
Running loss of epoch-64 batch-130 = 6.420537829399109e-06

Training epoch-64 batch-131
Running loss of epoch-64 batch-131 = 7.165130227804184e-06

Training epoch-64 batch-132
Running loss of epoch-64 batch-132 = 7.582362741231918e-06

Training epoch-64 batch-133
Running loss of epoch-64 batch-133 = 7.355818524956703e-06

Training epoch-64 batch-134
Running loss of epoch-64 batch-134 = 7.355120033025742e-06

Training epoch-64 batch-135
Running loss of epoch-64 batch-135 = 2.514338120818138e-06

Training epoch-64 batch-136
Running loss of epoch-64 batch-136 = 1.5186378732323647e-05

Training epoch-64 batch-137
Running loss of epoch-64 batch-137 = 5.71901910007e-06

Training epoch-64 batch-138
Running loss of epoch-64 batch-138 = 4.393281415104866e-06

Training epoch-64 batch-139
Running loss of epoch-64 batch-139 = 4.941597580909729e-06

Training epoch-64 batch-140
Running loss of epoch-64 batch-140 = 2.814456820487976e-06

Training epoch-64 batch-141
Running loss of epoch-64 batch-141 = 6.909482181072235e-06

Training epoch-64 batch-142
Running loss of epoch-64 batch-142 = 6.0603488236665726e-06

Training epoch-64 batch-143
Running loss of epoch-64 batch-143 = 5.613081157207489e-06

Training epoch-64 batch-144
Running loss of epoch-64 batch-144 = 3.6945566534996033e-06

Training epoch-64 batch-145
Running loss of epoch-64 batch-145 = 7.675262168049812e-06

Training epoch-64 batch-146
Running loss of epoch-64 batch-146 = 7.346738129854202e-06

Training epoch-64 batch-147
Running loss of epoch-64 batch-147 = 6.1336904764175415e-06

Training epoch-64 batch-148
Running loss of epoch-64 batch-148 = 9.977957233786583e-06

Training epoch-64 batch-149
Running loss of epoch-64 batch-149 = 4.615169018507004e-06

Training epoch-64 batch-150
Running loss of epoch-64 batch-150 = 5.572102963924408e-06

Training epoch-64 batch-151
Running loss of epoch-64 batch-151 = 9.0824905782938e-06

Training epoch-64 batch-152
Running loss of epoch-64 batch-152 = 8.887844160199165e-06

Training epoch-64 batch-153
Running loss of epoch-64 batch-153 = 5.48316165804863e-06

Training epoch-64 batch-154
Running loss of epoch-64 batch-154 = 7.111579179763794e-06

Training epoch-64 batch-155
Running loss of epoch-64 batch-155 = 3.0709896236658096e-05

Training epoch-64 batch-156
Running loss of epoch-64 batch-156 = 1.0796589776873589e-05

Training epoch-64 batch-157
Running loss of epoch-64 batch-157 = 3.262236714363098e-05

Finished training epoch-64.



Average train loss at epoch-64 = 7.157447189092636e-06

Started Evaluation

Average val loss at epoch-64 = 1.7250834162049447

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.36 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.25 %

Finished Evaluation



Started training epoch-65


Training epoch-65 batch-1
Running loss of epoch-65 batch-1 = 2.109212800860405e-06

Training epoch-65 batch-2
Running loss of epoch-65 batch-2 = 5.980487912893295e-06

Training epoch-65 batch-3
Running loss of epoch-65 batch-3 = 2.287561073899269e-06

Training epoch-65 batch-4
Running loss of epoch-65 batch-4 = 1.5325145795941353e-05

Training epoch-65 batch-5
Running loss of epoch-65 batch-5 = 6.717396900057793e-06

Training epoch-65 batch-6
Running loss of epoch-65 batch-6 = 5.006557330489159e-06

Training epoch-65 batch-7
Running loss of epoch-65 batch-7 = 4.443805664777756e-06

Training epoch-65 batch-8
Running loss of epoch-65 batch-8 = 1.0878313332796097e-05

Training epoch-65 batch-9
Running loss of epoch-65 batch-9 = 2.2512394934892654e-06

Training epoch-65 batch-10
Running loss of epoch-65 batch-10 = 8.169328793883324e-06

Training epoch-65 batch-11
Running loss of epoch-65 batch-11 = 7.210066542029381e-06

Training epoch-65 batch-12
Running loss of epoch-65 batch-12 = 3.715045750141144e-06

Training epoch-65 batch-13
Running loss of epoch-65 batch-13 = 5.320180207490921e-06

Training epoch-65 batch-14
Running loss of epoch-65 batch-14 = 2.3634638637304306e-06

Training epoch-65 batch-15
Running loss of epoch-65 batch-15 = 6.500398740172386e-06

Training epoch-65 batch-16
Running loss of epoch-65 batch-16 = 7.449416443705559e-06

Training epoch-65 batch-17
Running loss of epoch-65 batch-17 = 7.251277565956116e-06

Training epoch-65 batch-18
Running loss of epoch-65 batch-18 = 1.3101380318403244e-06

Training epoch-65 batch-19
Running loss of epoch-65 batch-19 = 4.30690124630928e-06

Training epoch-65 batch-20
Running loss of epoch-65 batch-20 = 3.603752702474594e-06

Training epoch-65 batch-21
Running loss of epoch-65 batch-21 = 3.0349474400281906e-06

Training epoch-65 batch-22
Running loss of epoch-65 batch-22 = 1.0530231520533562e-05

Training epoch-65 batch-23
Running loss of epoch-65 batch-23 = 3.466615453362465e-06

Training epoch-65 batch-24
Running loss of epoch-65 batch-24 = 2.112938091158867e-06

Training epoch-65 batch-25
Running loss of epoch-65 batch-25 = 2.601882442831993e-06

Training epoch-65 batch-26
Running loss of epoch-65 batch-26 = 6.398418918251991e-06

Training epoch-65 batch-27
Running loss of epoch-65 batch-27 = 1.8542632460594177e-06

Training epoch-65 batch-28
Running loss of epoch-65 batch-28 = 7.447320967912674e-06

Training epoch-65 batch-29
Running loss of epoch-65 batch-29 = 1.3962853699922562e-05

Training epoch-65 batch-30
Running loss of epoch-65 batch-30 = 1.2881821021437645e-05

Training epoch-65 batch-31
Running loss of epoch-65 batch-31 = 6.845453754067421e-06

Training epoch-65 batch-32
Running loss of epoch-65 batch-32 = 7.902970537543297e-06

Training epoch-65 batch-33
Running loss of epoch-65 batch-33 = 6.764661520719528e-06

Training epoch-65 batch-34
Running loss of epoch-65 batch-34 = 4.426809027791023e-06

Training epoch-65 batch-35
Running loss of epoch-65 batch-35 = 5.732756108045578e-06

Training epoch-65 batch-36
Running loss of epoch-65 batch-36 = 7.625669240951538e-06

Training epoch-65 batch-37
Running loss of epoch-65 batch-37 = 1.075398176908493e-05

Training epoch-65 batch-38
Running loss of epoch-65 batch-38 = 4.268251359462738e-06

Training epoch-65 batch-39
Running loss of epoch-65 batch-39 = 4.447996616363525e-06

Training epoch-65 batch-40
Running loss of epoch-65 batch-40 = 6.198650225996971e-06

Training epoch-65 batch-41
Running loss of epoch-65 batch-41 = 1.3995449990034103e-06

Training epoch-65 batch-42
Running loss of epoch-65 batch-42 = 3.02167609333992e-06

Training epoch-65 batch-43
Running loss of epoch-65 batch-43 = 1.8568243831396103e-06

Training epoch-65 batch-44
Running loss of epoch-65 batch-44 = 2.903630957007408e-06

Training epoch-65 batch-45
Running loss of epoch-65 batch-45 = 4.392117261886597e-06

Training epoch-65 batch-46
Running loss of epoch-65 batch-46 = 6.30086287856102e-06

Training epoch-65 batch-47
Running loss of epoch-65 batch-47 = 7.0212408900260925e-06

Training epoch-65 batch-48
Running loss of epoch-65 batch-48 = 2.2757332772016525e-05

Training epoch-65 batch-49
Running loss of epoch-65 batch-49 = 4.691071808338165e-06

Training epoch-65 batch-50
Running loss of epoch-65 batch-50 = 3.1443778425455093e-06

Training epoch-65 batch-51
Running loss of epoch-65 batch-51 = 2.359086647629738e-05

Training epoch-65 batch-52
Running loss of epoch-65 batch-52 = 9.549083188176155e-06

Training epoch-65 batch-53
Running loss of epoch-65 batch-53 = 9.146519005298615e-06

Training epoch-65 batch-54
Running loss of epoch-65 batch-54 = 4.982808604836464e-06

Training epoch-65 batch-55
Running loss of epoch-65 batch-55 = 2.9567163437604904e-06

Training epoch-65 batch-56
Running loss of epoch-65 batch-56 = 3.4035183489322662e-06

Training epoch-65 batch-57
Running loss of epoch-65 batch-57 = 1.1287163943052292e-05

Training epoch-65 batch-58
Running loss of epoch-65 batch-58 = 9.159557521343231e-06

Training epoch-65 batch-59
Running loss of epoch-65 batch-59 = 3.7336722016334534e-06

Training epoch-65 batch-60
Running loss of epoch-65 batch-60 = 7.0156529545784e-06

Training epoch-65 batch-61
Running loss of epoch-65 batch-61 = 1.1462951079010963e-05

Training epoch-65 batch-62
Running loss of epoch-65 batch-62 = 3.6335550248622894e-06

Training epoch-65 batch-63
Running loss of epoch-65 batch-63 = 1.7934944480657578e-06

Training epoch-65 batch-64
Running loss of epoch-65 batch-64 = 5.498062819242477e-06

Training epoch-65 batch-65
Running loss of epoch-65 batch-65 = 2.883467823266983e-05

Training epoch-65 batch-66
Running loss of epoch-65 batch-66 = 5.056383088231087e-06

Training epoch-65 batch-67
Running loss of epoch-65 batch-67 = 6.913905963301659e-06

Training epoch-65 batch-68
Running loss of epoch-65 batch-68 = 3.18232923746109e-06

Training epoch-65 batch-69
Running loss of epoch-65 batch-69 = 6.441026926040649e-06

Training epoch-65 batch-70
Running loss of epoch-65 batch-70 = 4.260800778865814e-06

Training epoch-65 batch-71
Running loss of epoch-65 batch-71 = 2.7222558856010437e-06

Training epoch-65 batch-72
Running loss of epoch-65 batch-72 = 7.233582437038422e-06

Training epoch-65 batch-73
Running loss of epoch-65 batch-73 = 8.030561730265617e-06

Training epoch-65 batch-74
Running loss of epoch-65 batch-74 = 6.907153874635696e-06

Training epoch-65 batch-75
Running loss of epoch-65 batch-75 = 4.2782630771398544e-06

Training epoch-65 batch-76
Running loss of epoch-65 batch-76 = 9.505776688456535e-06

Training epoch-65 batch-77
Running loss of epoch-65 batch-77 = 4.31109219789505e-06

Training epoch-65 batch-78
Running loss of epoch-65 batch-78 = 4.665926098823547e-06

Training epoch-65 batch-79
Running loss of epoch-65 batch-79 = 7.014954462647438e-06

Training epoch-65 batch-80
Running loss of epoch-65 batch-80 = 8.903909474611282e-06

Training epoch-65 batch-81
Running loss of epoch-65 batch-81 = 5.67571260035038e-06

Training epoch-65 batch-82
Running loss of epoch-65 batch-82 = 8.181435987353325e-06

Training epoch-65 batch-83
Running loss of epoch-65 batch-83 = 3.081047907471657e-06

Training epoch-65 batch-84
Running loss of epoch-65 batch-84 = 1.1132564395666122e-05

Training epoch-65 batch-85
Running loss of epoch-65 batch-85 = 3.2514799386262894e-06

Training epoch-65 batch-86
Running loss of epoch-65 batch-86 = 3.719935193657875e-06

Training epoch-65 batch-87
Running loss of epoch-65 batch-87 = 4.631001502275467e-06

Training epoch-65 batch-88
Running loss of epoch-65 batch-88 = 2.582091838121414e-06

Training epoch-65 batch-89
Running loss of epoch-65 batch-89 = 4.391418769955635e-06

Training epoch-65 batch-90
Running loss of epoch-65 batch-90 = 4.493165761232376e-06

Training epoch-65 batch-91
Running loss of epoch-65 batch-91 = 2.61492095887661e-06

Training epoch-65 batch-92
Running loss of epoch-65 batch-92 = 1.0703690350055695e-05

Training epoch-65 batch-93
Running loss of epoch-65 batch-93 = 6.527407094836235e-06

Training epoch-65 batch-94
Running loss of epoch-65 batch-94 = 8.492032065987587e-06

Training epoch-65 batch-95
Running loss of epoch-65 batch-95 = 2.560904249548912e-06

Training epoch-65 batch-96
Running loss of epoch-65 batch-96 = 6.499234586954117e-06

Training epoch-65 batch-97
Running loss of epoch-65 batch-97 = 1.3154232874512672e-05

Training epoch-65 batch-98
Running loss of epoch-65 batch-98 = 2.6794150471687317e-06

Training epoch-65 batch-99
Running loss of epoch-65 batch-99 = 9.408220648765564e-06

Training epoch-65 batch-100
Running loss of epoch-65 batch-100 = 5.092006176710129e-06

Training epoch-65 batch-101
Running loss of epoch-65 batch-101 = 4.479195922613144e-06

Training epoch-65 batch-102
Running loss of epoch-65 batch-102 = 6.356509402394295e-06

Training epoch-65 batch-103
Running loss of epoch-65 batch-103 = 7.86641612648964e-06

Training epoch-65 batch-104
Running loss of epoch-65 batch-104 = 4.53973188996315e-06

Training epoch-65 batch-105
Running loss of epoch-65 batch-105 = 1.2300442904233932e-05

Training epoch-65 batch-106
Running loss of epoch-65 batch-106 = 2.5975052267313004e-05

Training epoch-65 batch-107
Running loss of epoch-65 batch-107 = 3.644963726401329e-06

Training epoch-65 batch-108
Running loss of epoch-65 batch-108 = 4.6032946556806564e-06

Training epoch-65 batch-109
Running loss of epoch-65 batch-109 = 3.3210963010787964e-06

Training epoch-65 batch-110
Running loss of epoch-65 batch-110 = 3.428896889090538e-06

Training epoch-65 batch-111
Running loss of epoch-65 batch-111 = 8.290167897939682e-06

Training epoch-65 batch-112
Running loss of epoch-65 batch-112 = 1.1289725080132484e-05

Training epoch-65 batch-113
Running loss of epoch-65 batch-113 = 4.779547452926636e-06

Training epoch-65 batch-114
Running loss of epoch-65 batch-114 = 6.675953045487404e-06

Training epoch-65 batch-115
Running loss of epoch-65 batch-115 = 4.763482138514519e-06

Training epoch-65 batch-116
Running loss of epoch-65 batch-116 = 3.318069502711296e-06

Training epoch-65 batch-117
Running loss of epoch-65 batch-117 = 3.1210947781801224e-06

Training epoch-65 batch-118
Running loss of epoch-65 batch-118 = 8.925795555114746e-06

Training epoch-65 batch-119
Running loss of epoch-65 batch-119 = 1.9486993551254272e-05

Training epoch-65 batch-120
Running loss of epoch-65 batch-120 = 3.2817479223012924e-06

Training epoch-65 batch-121
Running loss of epoch-65 batch-121 = 7.206108421087265e-06

Training epoch-65 batch-122
Running loss of epoch-65 batch-122 = 5.705747753381729e-06

Training epoch-65 batch-123
Running loss of epoch-65 batch-123 = 5.201669409871101e-06

Training epoch-65 batch-124
Running loss of epoch-65 batch-124 = 9.429175406694412e-06

Training epoch-65 batch-125
Running loss of epoch-65 batch-125 = 2.279249019920826e-05

Training epoch-65 batch-126
Running loss of epoch-65 batch-126 = 9.049894288182259e-06

Training epoch-65 batch-127
Running loss of epoch-65 batch-127 = 6.654299795627594e-06

Training epoch-65 batch-128
Running loss of epoch-65 batch-128 = 2.282671630382538e-06

Training epoch-65 batch-129
Running loss of epoch-65 batch-129 = 8.612405508756638e-06

Training epoch-65 batch-130
Running loss of epoch-65 batch-130 = 7.0445239543914795e-06

Training epoch-65 batch-131
Running loss of epoch-65 batch-131 = 2.8423964977264404e-06

Training epoch-65 batch-132
Running loss of epoch-65 batch-132 = 5.144160240888596e-06

Training epoch-65 batch-133
Running loss of epoch-65 batch-133 = 3.3383257687091827e-06

Training epoch-65 batch-134
Running loss of epoch-65 batch-134 = 6.247544661164284e-06

Training epoch-65 batch-135
Running loss of epoch-65 batch-135 = 4.232162609696388e-06

Training epoch-65 batch-136
Running loss of epoch-65 batch-136 = 5.239155143499374e-06

Training epoch-65 batch-137
Running loss of epoch-65 batch-137 = 5.8002769947052e-06

Training epoch-65 batch-138
Running loss of epoch-65 batch-138 = 8.980277925729752e-06

Training epoch-65 batch-139
Running loss of epoch-65 batch-139 = 1.7079059034585953e-05

Training epoch-65 batch-140
Running loss of epoch-65 batch-140 = 3.2712705433368683e-06

Training epoch-65 batch-141
Running loss of epoch-65 batch-141 = 2.667773514986038e-06

Training epoch-65 batch-142
Running loss of epoch-65 batch-142 = 6.828922778367996e-06

Training epoch-65 batch-143
Running loss of epoch-65 batch-143 = 5.4729171097278595e-06

Training epoch-65 batch-144
Running loss of epoch-65 batch-144 = 1.6609206795692444e-05

Training epoch-65 batch-145
Running loss of epoch-65 batch-145 = 4.363479092717171e-06

Training epoch-65 batch-146
Running loss of epoch-65 batch-146 = 1.84657983481884e-06

Training epoch-65 batch-147
Running loss of epoch-65 batch-147 = 1.3246433809399605e-05

Training epoch-65 batch-148
Running loss of epoch-65 batch-148 = 2.3033935576677322e-06

Training epoch-65 batch-149
Running loss of epoch-65 batch-149 = 3.0102673918008804e-06

Training epoch-65 batch-150
Running loss of epoch-65 batch-150 = 6.603775545954704e-06

Training epoch-65 batch-151
Running loss of epoch-65 batch-151 = 1.446274109184742e-05

Training epoch-65 batch-152
Running loss of epoch-65 batch-152 = 5.022622644901276e-06

Training epoch-65 batch-153
Running loss of epoch-65 batch-153 = 1.292000524699688e-05

Training epoch-65 batch-154
Running loss of epoch-65 batch-154 = 8.858740329742432e-06

Training epoch-65 batch-155
Running loss of epoch-65 batch-155 = 7.060123607516289e-06

Training epoch-65 batch-156
Running loss of epoch-65 batch-156 = 3.0293595045804977e-06

Training epoch-65 batch-157
Running loss of epoch-65 batch-157 = 4.108250141143799e-05

Finished training epoch-65.



Average train loss at epoch-65 = 6.812632083892822e-06

Started Evaluation

Average val loss at epoch-65 = 1.736609549040642

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 88.91 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.16 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-66


Training epoch-66 batch-1
Running loss of epoch-66 batch-1 = 5.039386451244354e-06

Training epoch-66 batch-2
Running loss of epoch-66 batch-2 = 1.3724667951464653e-05

Training epoch-66 batch-3
Running loss of epoch-66 batch-3 = 1.320638693869114e-05

Training epoch-66 batch-4
Running loss of epoch-66 batch-4 = 3.0943192541599274e-06

Training epoch-66 batch-5
Running loss of epoch-66 batch-5 = 5.1085371524095535e-06

Training epoch-66 batch-6
Running loss of epoch-66 batch-6 = 2.121319994330406e-06

Training epoch-66 batch-7
Running loss of epoch-66 batch-7 = 2.9442831873893738e-05

Training epoch-66 batch-8
Running loss of epoch-66 batch-8 = 1.537729986011982e-05

Training epoch-66 batch-9
Running loss of epoch-66 batch-9 = 9.379815310239792e-06

Training epoch-66 batch-10
Running loss of epoch-66 batch-10 = 6.5837521106004715e-06

Training epoch-66 batch-11
Running loss of epoch-66 batch-11 = 4.827976226806641e-06

Training epoch-66 batch-12
Running loss of epoch-66 batch-12 = 7.575610652565956e-06

Training epoch-66 batch-13
Running loss of epoch-66 batch-13 = 6.834976375102997e-06

Training epoch-66 batch-14
Running loss of epoch-66 batch-14 = 8.5469800978899e-06

Training epoch-66 batch-15
Running loss of epoch-66 batch-15 = 3.598630428314209e-06

Training epoch-66 batch-16
Running loss of epoch-66 batch-16 = 5.546724423766136e-06

Training epoch-66 batch-17
Running loss of epoch-66 batch-17 = 4.398869350552559e-06

Training epoch-66 batch-18
Running loss of epoch-66 batch-18 = 1.144525595009327e-05

Training epoch-66 batch-19
Running loss of epoch-66 batch-19 = 9.50787216424942e-06

Training epoch-66 batch-20
Running loss of epoch-66 batch-20 = 5.059875547885895e-06

Training epoch-66 batch-21
Running loss of epoch-66 batch-21 = 5.163019523024559e-06

Training epoch-66 batch-22
Running loss of epoch-66 batch-22 = 5.2584800869226456e-06

Training epoch-66 batch-23
Running loss of epoch-66 batch-23 = 6.4838677644729614e-06

Training epoch-66 batch-24
Running loss of epoch-66 batch-24 = 4.202360287308693e-06

Training epoch-66 batch-25
Running loss of epoch-66 batch-25 = 4.298053681850433e-06

Training epoch-66 batch-26
Running loss of epoch-66 batch-26 = 2.9650982469320297e-06

Training epoch-66 batch-27
Running loss of epoch-66 batch-27 = 4.345551133155823e-06

Training epoch-66 batch-28
Running loss of epoch-66 batch-28 = 9.324401617050171e-06

Training epoch-66 batch-29
Running loss of epoch-66 batch-29 = 6.562797352671623e-06

Training epoch-66 batch-30
Running loss of epoch-66 batch-30 = 2.589309588074684e-06

Training epoch-66 batch-31
Running loss of epoch-66 batch-31 = 4.35439869761467e-06

Training epoch-66 batch-32
Running loss of epoch-66 batch-32 = 4.184199497103691e-06

Training epoch-66 batch-33
Running loss of epoch-66 batch-33 = 6.167450919747353e-06

Training epoch-66 batch-34
Running loss of epoch-66 batch-34 = 1.0670861229300499e-05

Training epoch-66 batch-35
Running loss of epoch-66 batch-35 = 6.261281669139862e-06

Training epoch-66 batch-36
Running loss of epoch-66 batch-36 = 1.0237563401460648e-06

Training epoch-66 batch-37
Running loss of epoch-66 batch-37 = 3.471504896879196e-06

Training epoch-66 batch-38
Running loss of epoch-66 batch-38 = 3.677094355225563e-06

Training epoch-66 batch-39
Running loss of epoch-66 batch-39 = 2.794410102069378e-05

Training epoch-66 batch-40
Running loss of epoch-66 batch-40 = 6.45429827272892e-06

Training epoch-66 batch-41
Running loss of epoch-66 batch-41 = 5.277805030345917e-06

Training epoch-66 batch-42
Running loss of epoch-66 batch-42 = 2.68593430519104e-06

Training epoch-66 batch-43
Running loss of epoch-66 batch-43 = 5.682231858372688e-06

Training epoch-66 batch-44
Running loss of epoch-66 batch-44 = 1.451629213988781e-05

Training epoch-66 batch-45
Running loss of epoch-66 batch-45 = 8.232193067669868e-06

Training epoch-66 batch-46
Running loss of epoch-66 batch-46 = 4.3315812945365906e-06

Training epoch-66 batch-47
Running loss of epoch-66 batch-47 = 3.7157442420721054e-06

Training epoch-66 batch-48
Running loss of epoch-66 batch-48 = 7.862923666834831e-06

Training epoch-66 batch-49
Running loss of epoch-66 batch-49 = 6.36884942650795e-06

Training epoch-66 batch-50
Running loss of epoch-66 batch-50 = 1.1126743629574776e-05

Training epoch-66 batch-51
Running loss of epoch-66 batch-51 = 3.702472895383835e-06

Training epoch-66 batch-52
Running loss of epoch-66 batch-52 = 6.821705028414726e-06

Training epoch-66 batch-53
Running loss of epoch-66 batch-53 = 6.213318556547165e-06

Training epoch-66 batch-54
Running loss of epoch-66 batch-54 = 5.370005965232849e-06

Training epoch-66 batch-55
Running loss of epoch-66 batch-55 = 4.876637831330299e-06

Training epoch-66 batch-56
Running loss of epoch-66 batch-56 = 4.911329597234726e-06

Training epoch-66 batch-57
Running loss of epoch-66 batch-57 = 7.091555744409561e-06

Training epoch-66 batch-58
Running loss of epoch-66 batch-58 = 8.956994861364365e-06

Training epoch-66 batch-59
Running loss of epoch-66 batch-59 = 5.105510354042053e-06

Training epoch-66 batch-60
Running loss of epoch-66 batch-60 = 2.409098669886589e-06

Training epoch-66 batch-61
Running loss of epoch-66 batch-61 = 4.8282090574502945e-06

Training epoch-66 batch-62
Running loss of epoch-66 batch-62 = 4.393281415104866e-06

Training epoch-66 batch-63
Running loss of epoch-66 batch-63 = 3.1027011573314667e-06

Training epoch-66 batch-64
Running loss of epoch-66 batch-64 = 6.435206159949303e-06

Training epoch-66 batch-65
Running loss of epoch-66 batch-65 = 7.879920303821564e-06

Training epoch-66 batch-66
Running loss of epoch-66 batch-66 = 7.061520591378212e-06

Training epoch-66 batch-67
Running loss of epoch-66 batch-67 = 3.5653356462717056e-06

Training epoch-66 batch-68
Running loss of epoch-66 batch-68 = 7.429858669638634e-06

Training epoch-66 batch-69
Running loss of epoch-66 batch-69 = 4.747183993458748e-06

Training epoch-66 batch-70
Running loss of epoch-66 batch-70 = 4.948116838932037e-06

Training epoch-66 batch-71
Running loss of epoch-66 batch-71 = 8.708331733942032e-06

Training epoch-66 batch-72
Running loss of epoch-66 batch-72 = 2.561137080192566e-06

Training epoch-66 batch-73
Running loss of epoch-66 batch-73 = 7.1285758167505264e-06

Training epoch-66 batch-74
Running loss of epoch-66 batch-74 = 2.541113644838333e-06

Training epoch-66 batch-75
Running loss of epoch-66 batch-75 = 2.7462374418973923e-06

Training epoch-66 batch-76
Running loss of epoch-66 batch-76 = 6.783055141568184e-06

Training epoch-66 batch-77
Running loss of epoch-66 batch-77 = 2.3026950657367706e-06

Training epoch-66 batch-78
Running loss of epoch-66 batch-78 = 1.8648337572813034e-05

Training epoch-66 batch-79
Running loss of epoch-66 batch-79 = 9.784242138266563e-06

Training epoch-66 batch-80
Running loss of epoch-66 batch-80 = 3.7562567740678787e-06

Training epoch-66 batch-81
Running loss of epoch-66 batch-81 = 3.113178536295891e-06

Training epoch-66 batch-82
Running loss of epoch-66 batch-82 = 6.038462743163109e-06

Training epoch-66 batch-83
Running loss of epoch-66 batch-83 = 8.112052455544472e-06

Training epoch-66 batch-84
Running loss of epoch-66 batch-84 = 4.425877705216408e-06

Training epoch-66 batch-85
Running loss of epoch-66 batch-85 = 5.433103069663048e-06

Training epoch-66 batch-86
Running loss of epoch-66 batch-86 = 6.292480975389481e-06

Training epoch-66 batch-87
Running loss of epoch-66 batch-87 = 1.2034783139824867e-05

Training epoch-66 batch-88
Running loss of epoch-66 batch-88 = 4.689674824476242e-06

Training epoch-66 batch-89
Running loss of epoch-66 batch-89 = 4.348577931523323e-06

Training epoch-66 batch-90
Running loss of epoch-66 batch-90 = 5.543697625398636e-06

Training epoch-66 batch-91
Running loss of epoch-66 batch-91 = 4.1050370782613754e-06

Training epoch-66 batch-92
Running loss of epoch-66 batch-92 = 9.783310815691948e-06

Training epoch-66 batch-93
Running loss of epoch-66 batch-93 = 6.38328492641449e-06

Training epoch-66 batch-94
Running loss of epoch-66 batch-94 = 1.5844125300645828e-06

Training epoch-66 batch-95
Running loss of epoch-66 batch-95 = 1.4261575415730476e-05

Training epoch-66 batch-96
Running loss of epoch-66 batch-96 = 6.0372985899448395e-06

Training epoch-66 batch-97
Running loss of epoch-66 batch-97 = 4.054279997944832e-06

Training epoch-66 batch-98
Running loss of epoch-66 batch-98 = 4.684785380959511e-06

Training epoch-66 batch-99
Running loss of epoch-66 batch-99 = 4.752073436975479e-06

Training epoch-66 batch-100
Running loss of epoch-66 batch-100 = 5.899230018258095e-06

Training epoch-66 batch-101
Running loss of epoch-66 batch-101 = 9.81474295258522e-06

Training epoch-66 batch-102
Running loss of epoch-66 batch-102 = 4.2710453271865845e-06

Training epoch-66 batch-103
Running loss of epoch-66 batch-103 = 4.0458980947732925e-06

Training epoch-66 batch-104
Running loss of epoch-66 batch-104 = 1.6218982636928558e-06

Training epoch-66 batch-105
Running loss of epoch-66 batch-105 = 1.0067597031593323e-05

Training epoch-66 batch-106
Running loss of epoch-66 batch-106 = 6.453599780797958e-06

Training epoch-66 batch-107
Running loss of epoch-66 batch-107 = 6.2640756368637085e-06

Training epoch-66 batch-108
Running loss of epoch-66 batch-108 = 3.3562537282705307e-06

Training epoch-66 batch-109
Running loss of epoch-66 batch-109 = 1.0197516530752182e-05

Training epoch-66 batch-110
Running loss of epoch-66 batch-110 = 3.6598648875951767e-06

Training epoch-66 batch-111
Running loss of epoch-66 batch-111 = 6.628921255469322e-06

Training epoch-66 batch-112
Running loss of epoch-66 batch-112 = 3.2456591725349426e-06

Training epoch-66 batch-113
Running loss of epoch-66 batch-113 = 6.0927122831344604e-06

Training epoch-66 batch-114
Running loss of epoch-66 batch-114 = 6.613787263631821e-06

Training epoch-66 batch-115
Running loss of epoch-66 batch-115 = 1.977803185582161e-05

Training epoch-66 batch-116
Running loss of epoch-66 batch-116 = 1.4644581824541092e-05

Training epoch-66 batch-117
Running loss of epoch-66 batch-117 = 3.600260242819786e-06

Training epoch-66 batch-118
Running loss of epoch-66 batch-118 = 2.4990178644657135e-05

Training epoch-66 batch-119
Running loss of epoch-66 batch-119 = 7.783528417348862e-07

Training epoch-66 batch-120
Running loss of epoch-66 batch-120 = 6.7481305450201035e-06

Training epoch-66 batch-121
Running loss of epoch-66 batch-121 = 3.7979334592819214e-06

Training epoch-66 batch-122
Running loss of epoch-66 batch-122 = 3.250082954764366e-06

Training epoch-66 batch-123
Running loss of epoch-66 batch-123 = 4.1800085455179214e-06

Training epoch-66 batch-124
Running loss of epoch-66 batch-124 = 6.606103852391243e-06

Training epoch-66 batch-125
Running loss of epoch-66 batch-125 = 4.159985110163689e-06

Training epoch-66 batch-126
Running loss of epoch-66 batch-126 = 1.0610325261950493e-05

Training epoch-66 batch-127
Running loss of epoch-66 batch-127 = 1.7851125448942184e-06

Training epoch-66 batch-128
Running loss of epoch-66 batch-128 = 4.893401637673378e-06

Training epoch-66 batch-129
Running loss of epoch-66 batch-129 = 1.5934929251670837e-06

Training epoch-66 batch-130
Running loss of epoch-66 batch-130 = 4.3245963752269745e-06

Training epoch-66 batch-131
Running loss of epoch-66 batch-131 = 8.83452594280243e-06

Training epoch-66 batch-132
Running loss of epoch-66 batch-132 = 3.020279109477997e-06

Training epoch-66 batch-133
Running loss of epoch-66 batch-133 = 4.150206223130226e-06

Training epoch-66 batch-134
Running loss of epoch-66 batch-134 = 4.795147106051445e-06

Training epoch-66 batch-135
Running loss of epoch-66 batch-135 = 1.0872958227992058e-05

Training epoch-66 batch-136
Running loss of epoch-66 batch-136 = 2.171378582715988e-06

Training epoch-66 batch-137
Running loss of epoch-66 batch-137 = 1.0417308658361435e-05

Training epoch-66 batch-138
Running loss of epoch-66 batch-138 = 9.967945516109467e-06

Training epoch-66 batch-139
Running loss of epoch-66 batch-139 = 1.6617123037576675e-06

Training epoch-66 batch-140
Running loss of epoch-66 batch-140 = 2.880813553929329e-06

Training epoch-66 batch-141
Running loss of epoch-66 batch-141 = 6.4338091760873795e-06

Training epoch-66 batch-142
Running loss of epoch-66 batch-142 = 7.184222340583801e-06

Training epoch-66 batch-143
Running loss of epoch-66 batch-143 = 3.037741407752037e-06

Training epoch-66 batch-144
Running loss of epoch-66 batch-144 = 7.977243512868881e-06

Training epoch-66 batch-145
Running loss of epoch-66 batch-145 = 5.000736564397812e-06

Training epoch-66 batch-146
Running loss of epoch-66 batch-146 = 3.6421697586774826e-06

Training epoch-66 batch-147
Running loss of epoch-66 batch-147 = 6.522750481963158e-06

Training epoch-66 batch-148
Running loss of epoch-66 batch-148 = 3.0370429158210754e-06

Training epoch-66 batch-149
Running loss of epoch-66 batch-149 = 8.750706911087036e-06

Training epoch-66 batch-150
Running loss of epoch-66 batch-150 = 2.932269126176834e-06

Training epoch-66 batch-151
Running loss of epoch-66 batch-151 = 2.986285835504532e-06

Training epoch-66 batch-152
Running loss of epoch-66 batch-152 = 6.4338091760873795e-06

Training epoch-66 batch-153
Running loss of epoch-66 batch-153 = 1.1127674952149391e-05

Training epoch-66 batch-154
Running loss of epoch-66 batch-154 = 7.6105352491140366e-06

Training epoch-66 batch-155
Running loss of epoch-66 batch-155 = 9.131152182817459e-06

Training epoch-66 batch-156
Running loss of epoch-66 batch-156 = 6.078043952584267e-06

Training epoch-66 batch-157
Running loss of epoch-66 batch-157 = 1.8715858459472656e-05

Finished training epoch-66.



Average train loss at epoch-66 = 6.537100672721863e-06

Started Evaluation

Average val loss at epoch-66 = 1.7457601273870251

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-67


Training epoch-67 batch-1
Running loss of epoch-67 batch-1 = 5.455454811453819e-06

Training epoch-67 batch-2
Running loss of epoch-67 batch-2 = 5.276640877127647e-06

Training epoch-67 batch-3
Running loss of epoch-67 batch-3 = 2.9248185455799103e-06

Training epoch-67 batch-4
Running loss of epoch-67 batch-4 = 5.817273631691933e-06

Training epoch-67 batch-5
Running loss of epoch-67 batch-5 = 5.5676791816949844e-06

Training epoch-67 batch-6
Running loss of epoch-67 batch-6 = 6.467336788773537e-06

Training epoch-67 batch-7
Running loss of epoch-67 batch-7 = 9.168405085802078e-06

Training epoch-67 batch-8
Running loss of epoch-67 batch-8 = 4.9693044275045395e-06

Training epoch-67 batch-9
Running loss of epoch-67 batch-9 = 4.704343155026436e-06

Training epoch-67 batch-10
Running loss of epoch-67 batch-10 = 6.9641973823308945e-06

Training epoch-67 batch-11
Running loss of epoch-67 batch-11 = 4.063360393047333e-06

Training epoch-67 batch-12
Running loss of epoch-67 batch-12 = 8.023111149668694e-06

Training epoch-67 batch-13
Running loss of epoch-67 batch-13 = 1.742970198392868e-06

Training epoch-67 batch-14
Running loss of epoch-67 batch-14 = 6.2854960560798645e-06

Training epoch-67 batch-15
Running loss of epoch-67 batch-15 = 4.61493618786335e-06

Training epoch-67 batch-16
Running loss of epoch-67 batch-16 = 7.431721314787865e-06

Training epoch-67 batch-17
Running loss of epoch-67 batch-17 = 1.018622424453497e-05

Training epoch-67 batch-18
Running loss of epoch-67 batch-18 = 6.974674761295319e-06

Training epoch-67 batch-19
Running loss of epoch-67 batch-19 = 8.118804544210434e-06

Training epoch-67 batch-20
Running loss of epoch-67 batch-20 = 7.50483013689518e-06

Training epoch-67 batch-21
Running loss of epoch-67 batch-21 = 8.220085874199867e-06

Training epoch-67 batch-22
Running loss of epoch-67 batch-22 = 6.817281246185303e-06

Training epoch-67 batch-23
Running loss of epoch-67 batch-23 = 3.523426130414009e-06

Training epoch-67 batch-24
Running loss of epoch-67 batch-24 = 4.2049214243888855e-06

Training epoch-67 batch-25
Running loss of epoch-67 batch-25 = 7.886439561843872e-06

Training epoch-67 batch-26
Running loss of epoch-67 batch-26 = 1.0870397090911865e-05

Training epoch-67 batch-27
Running loss of epoch-67 batch-27 = 1.475214958190918e-06

Training epoch-67 batch-28
Running loss of epoch-67 batch-28 = 1.942669041454792e-05

Training epoch-67 batch-29
Running loss of epoch-67 batch-29 = 4.575354978442192e-06

Training epoch-67 batch-30
Running loss of epoch-67 batch-30 = 2.885935828089714e-06

Training epoch-67 batch-31
Running loss of epoch-67 batch-31 = 5.460577085614204e-06

Training epoch-67 batch-32
Running loss of epoch-67 batch-32 = 5.445210263133049e-06

Training epoch-67 batch-33
Running loss of epoch-67 batch-33 = 5.1336828619241714e-06

Training epoch-67 batch-34
Running loss of epoch-67 batch-34 = 4.359753802418709e-06

Training epoch-67 batch-35
Running loss of epoch-67 batch-35 = 4.4014304876327515e-06

Training epoch-67 batch-36
Running loss of epoch-67 batch-36 = 3.4994445741176605e-06

Training epoch-67 batch-37
Running loss of epoch-67 batch-37 = 8.65827314555645e-06

Training epoch-67 batch-38
Running loss of epoch-67 batch-38 = 5.569541826844215e-06

Training epoch-67 batch-39
Running loss of epoch-67 batch-39 = 3.783963620662689e-06

Training epoch-67 batch-40
Running loss of epoch-67 batch-40 = 6.9374218583106995e-06

Training epoch-67 batch-41
Running loss of epoch-67 batch-41 = 4.4370535761117935e-06

Training epoch-67 batch-42
Running loss of epoch-67 batch-42 = 3.5688281059265137e-06

Training epoch-67 batch-43
Running loss of epoch-67 batch-43 = 4.627043381333351e-06

Training epoch-67 batch-44
Running loss of epoch-67 batch-44 = 5.3138937801122665e-06

Training epoch-67 batch-45
Running loss of epoch-67 batch-45 = 1.5336554497480392e-06

Training epoch-67 batch-46
Running loss of epoch-67 batch-46 = 2.5932677090168e-06

Training epoch-67 batch-47
Running loss of epoch-67 batch-47 = 3.897584974765778e-06

Training epoch-67 batch-48
Running loss of epoch-67 batch-48 = 3.0060764402151108e-06

Training epoch-67 batch-49
Running loss of epoch-67 batch-49 = 2.4444889277219772e-06

Training epoch-67 batch-50
Running loss of epoch-67 batch-50 = 1.592491753399372e-05

Training epoch-67 batch-51
Running loss of epoch-67 batch-51 = 6.170012056827545e-06

Training epoch-67 batch-52
Running loss of epoch-67 batch-52 = 4.421686753630638e-06

Training epoch-67 batch-53
Running loss of epoch-67 batch-53 = 5.423557013273239e-06

Training epoch-67 batch-54
Running loss of epoch-67 batch-54 = 2.5134067982435226e-06

Training epoch-67 batch-55
Running loss of epoch-67 batch-55 = 5.400972440838814e-06

Training epoch-67 batch-56
Running loss of epoch-67 batch-56 = 2.444721758365631e-06

Training epoch-67 batch-57
Running loss of epoch-67 batch-57 = 1.020822674036026e-05

Training epoch-67 batch-58
Running loss of epoch-67 batch-58 = 3.721565008163452e-06

Training epoch-67 batch-59
Running loss of epoch-67 batch-59 = 8.244998753070831e-06

Training epoch-67 batch-60
Running loss of epoch-67 batch-60 = 6.789341568946838e-06

Training epoch-67 batch-61
Running loss of epoch-67 batch-61 = 6.000511348247528e-06

Training epoch-67 batch-62
Running loss of epoch-67 batch-62 = 6.552552804350853e-06

Training epoch-67 batch-63
Running loss of epoch-67 batch-63 = 4.607951268553734e-06

Training epoch-67 batch-64
Running loss of epoch-67 batch-64 = 9.119277819991112e-06

Training epoch-67 batch-65
Running loss of epoch-67 batch-65 = 8.278992027044296e-06

Training epoch-67 batch-66
Running loss of epoch-67 batch-66 = 3.8603320717811584e-06

Training epoch-67 batch-67
Running loss of epoch-67 batch-67 = 3.7832651287317276e-06

Training epoch-67 batch-68
Running loss of epoch-67 batch-68 = 3.2156240195035934e-06

Training epoch-67 batch-69
Running loss of epoch-67 batch-69 = 5.464768037199974e-06

Training epoch-67 batch-70
Running loss of epoch-67 batch-70 = 2.8882641345262527e-06

Training epoch-67 batch-71
Running loss of epoch-67 batch-71 = 8.513219654560089e-06

Training epoch-67 batch-72
Running loss of epoch-67 batch-72 = 4.7173816710710526e-06

Training epoch-67 batch-73
Running loss of epoch-67 batch-73 = 3.1886156648397446e-06

Training epoch-67 batch-74
Running loss of epoch-67 batch-74 = 4.440080374479294e-06

Training epoch-67 batch-75
Running loss of epoch-67 batch-75 = 8.53976234793663e-06

Training epoch-67 batch-76
Running loss of epoch-67 batch-76 = 1.667998731136322e-05

Training epoch-67 batch-77
Running loss of epoch-67 batch-77 = 2.577435225248337e-06

Training epoch-67 batch-78
Running loss of epoch-67 batch-78 = 4.005618393421173e-06

Training epoch-67 batch-79
Running loss of epoch-67 batch-79 = 3.0535738915205e-06

Training epoch-67 batch-80
Running loss of epoch-67 batch-80 = 1.4049001038074493e-05

Training epoch-67 batch-81
Running loss of epoch-67 batch-81 = 5.774898454546928e-06

Training epoch-67 batch-82
Running loss of epoch-67 batch-82 = 6.384681910276413e-06

Training epoch-67 batch-83
Running loss of epoch-67 batch-83 = 5.056615918874741e-06

Training epoch-67 batch-84
Running loss of epoch-67 batch-84 = 5.573267117142677e-06

Training epoch-67 batch-85
Running loss of epoch-67 batch-85 = 5.33577986061573e-06

Training epoch-67 batch-86
Running loss of epoch-67 batch-86 = 5.502719432115555e-06

Training epoch-67 batch-87
Running loss of epoch-67 batch-87 = 1.1211028322577477e-05

Training epoch-67 batch-88
Running loss of epoch-67 batch-88 = 5.281995981931686e-06

Training epoch-67 batch-89
Running loss of epoch-67 batch-89 = 5.059409886598587e-06

Training epoch-67 batch-90
Running loss of epoch-67 batch-90 = 3.7904828786849976e-06

Training epoch-67 batch-91
Running loss of epoch-67 batch-91 = 6.759772077202797e-06

Training epoch-67 batch-92
Running loss of epoch-67 batch-92 = 3.287568688392639e-06

Training epoch-67 batch-93
Running loss of epoch-67 batch-93 = 1.8763355910778046e-05

Training epoch-67 batch-94
Running loss of epoch-67 batch-94 = 5.527399480342865e-06

Training epoch-67 batch-95
Running loss of epoch-67 batch-95 = 1.1077150702476501e-05

Training epoch-67 batch-96
Running loss of epoch-67 batch-96 = 5.7371798902750015e-06

Training epoch-67 batch-97
Running loss of epoch-67 batch-97 = 1.628766767680645e-05

Training epoch-67 batch-98
Running loss of epoch-67 batch-98 = 3.875931724905968e-06

Training epoch-67 batch-99
Running loss of epoch-67 batch-99 = 4.062661901116371e-06

Training epoch-67 batch-100
Running loss of epoch-67 batch-100 = 4.661502316594124e-06

Training epoch-67 batch-101
Running loss of epoch-67 batch-101 = 9.916722774505615e-06

Training epoch-67 batch-102
Running loss of epoch-67 batch-102 = 3.7222635000944138e-06

Training epoch-67 batch-103
Running loss of epoch-67 batch-103 = 5.9537123888731e-06

Training epoch-67 batch-104
Running loss of epoch-67 batch-104 = 7.371185347437859e-06

Training epoch-67 batch-105
Running loss of epoch-67 batch-105 = 4.217727109789848e-06

Training epoch-67 batch-106
Running loss of epoch-67 batch-106 = 4.72203828394413e-06

Training epoch-67 batch-107
Running loss of epoch-67 batch-107 = 1.1168420314788818e-05

Training epoch-67 batch-108
Running loss of epoch-67 batch-108 = 4.961621016263962e-06

Training epoch-67 batch-109
Running loss of epoch-67 batch-109 = 4.013534635305405e-06

Training epoch-67 batch-110
Running loss of epoch-67 batch-110 = 8.460134267807007e-06

Training epoch-67 batch-111
Running loss of epoch-67 batch-111 = 1.3945391401648521e-05

Training epoch-67 batch-112
Running loss of epoch-67 batch-112 = 6.614485755562782e-06

Training epoch-67 batch-113
Running loss of epoch-67 batch-113 = 4.470115527510643e-06

Training epoch-67 batch-114
Running loss of epoch-67 batch-114 = 3.6428682506084442e-06

Training epoch-67 batch-115
Running loss of epoch-67 batch-115 = 4.340428858995438e-06

Training epoch-67 batch-116
Running loss of epoch-67 batch-116 = 2.601882442831993e-06

Training epoch-67 batch-117
Running loss of epoch-67 batch-117 = 1.1855736374855042e-05

Training epoch-67 batch-118
Running loss of epoch-67 batch-118 = 5.189329385757446e-06

Training epoch-67 batch-119
Running loss of epoch-67 batch-119 = 1.8875114619731903e-05

Training epoch-67 batch-120
Running loss of epoch-67 batch-120 = 3.1436793506145477e-06

Training epoch-67 batch-121
Running loss of epoch-67 batch-121 = 7.485039532184601e-06

Training epoch-67 batch-122
Running loss of epoch-67 batch-122 = 3.375578671693802e-06

Training epoch-67 batch-123
Running loss of epoch-67 batch-123 = 7.870839908719063e-06

Training epoch-67 batch-124
Running loss of epoch-67 batch-124 = 1.1143507435917854e-05

Training epoch-67 batch-125
Running loss of epoch-67 batch-125 = 1.0139541700482368e-05

Training epoch-67 batch-126
Running loss of epoch-67 batch-126 = 4.763714969158173e-06

Training epoch-67 batch-127
Running loss of epoch-67 batch-127 = 7.218681275844574e-06

Training epoch-67 batch-128
Running loss of epoch-67 batch-128 = 4.174886271357536e-06

Training epoch-67 batch-129
Running loss of epoch-67 batch-129 = 3.351829946041107e-06

Training epoch-67 batch-130
Running loss of epoch-67 batch-130 = 3.41515988111496e-05

Training epoch-67 batch-131
Running loss of epoch-67 batch-131 = 7.781898602843285e-06

Training epoch-67 batch-132
Running loss of epoch-67 batch-132 = 7.835915312170982e-06

Training epoch-67 batch-133
Running loss of epoch-67 batch-133 = 8.567236363887787e-06

Training epoch-67 batch-134
Running loss of epoch-67 batch-134 = 3.5387929528951645e-06

Training epoch-67 batch-135
Running loss of epoch-67 batch-135 = 5.857786163687706e-06

Training epoch-67 batch-136
Running loss of epoch-67 batch-136 = 9.381910786032677e-06

Training epoch-67 batch-137
Running loss of epoch-67 batch-137 = 4.371395334601402e-06

Training epoch-67 batch-138
Running loss of epoch-67 batch-138 = 7.972121238708496e-06

Training epoch-67 batch-139
Running loss of epoch-67 batch-139 = 6.512738764286041e-06

Training epoch-67 batch-140
Running loss of epoch-67 batch-140 = 6.259186193346977e-06

Training epoch-67 batch-141
Running loss of epoch-67 batch-141 = 3.5802368074655533e-06

Training epoch-67 batch-142
Running loss of epoch-67 batch-142 = 3.979308530688286e-06

Training epoch-67 batch-143
Running loss of epoch-67 batch-143 = 3.875931724905968e-06

Training epoch-67 batch-144
Running loss of epoch-67 batch-144 = 3.1874515116214752e-06

Training epoch-67 batch-145
Running loss of epoch-67 batch-145 = 1.3643177226185799e-05

Training epoch-67 batch-146
Running loss of epoch-67 batch-146 = 2.629123628139496e-06

Training epoch-67 batch-147
Running loss of epoch-67 batch-147 = 7.419846951961517e-06

Training epoch-67 batch-148
Running loss of epoch-67 batch-148 = 5.431473255157471e-06

Training epoch-67 batch-149
Running loss of epoch-67 batch-149 = 5.479669198393822e-06

Training epoch-67 batch-150
Running loss of epoch-67 batch-150 = 1.8260907381772995e-06

Training epoch-67 batch-151
Running loss of epoch-67 batch-151 = 1.1934665963053703e-05

Training epoch-67 batch-152
Running loss of epoch-67 batch-152 = 4.024943336844444e-06

Training epoch-67 batch-153
Running loss of epoch-67 batch-153 = 8.36281105875969e-06

Training epoch-67 batch-154
Running loss of epoch-67 batch-154 = 7.06920400261879e-06

Training epoch-67 batch-155
Running loss of epoch-67 batch-155 = 2.477318048477173e-06

Training epoch-67 batch-156
Running loss of epoch-67 batch-156 = 3.938097506761551e-06

Training epoch-67 batch-157
Running loss of epoch-67 batch-157 = 3.242865204811096e-05

Finished training epoch-67.



Average train loss at epoch-67 = 6.454632431268692e-06

Started Evaluation

Average val loss at epoch-67 = 1.7530766826512278

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.23 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 52.61 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.33 %

Finished Evaluation



Started training epoch-68


Training epoch-68 batch-1
Running loss of epoch-68 batch-1 = 2.817949280142784e-06

Training epoch-68 batch-2
Running loss of epoch-68 batch-2 = 2.7581118047237396e-06

Training epoch-68 batch-3
Running loss of epoch-68 batch-3 = 1.6082078218460083e-05

Training epoch-68 batch-4
Running loss of epoch-68 batch-4 = 3.6244746297597885e-06

Training epoch-68 batch-5
Running loss of epoch-68 batch-5 = 8.783536031842232e-06

Training epoch-68 batch-6
Running loss of epoch-68 batch-6 = 4.829606041312218e-06

Training epoch-68 batch-7
Running loss of epoch-68 batch-7 = 6.110640242695808e-06

Training epoch-68 batch-8
Running loss of epoch-68 batch-8 = 7.0070382207632065e-06

Training epoch-68 batch-9
Running loss of epoch-68 batch-9 = 4.68594953417778e-06

Training epoch-68 batch-10
Running loss of epoch-68 batch-10 = 3.72203066945076e-06

Training epoch-68 batch-11
Running loss of epoch-68 batch-11 = 3.0635856091976166e-06

Training epoch-68 batch-12
Running loss of epoch-68 batch-12 = 4.345318302512169e-06

Training epoch-68 batch-13
Running loss of epoch-68 batch-13 = 3.044959157705307e-06

Training epoch-68 batch-14
Running loss of epoch-68 batch-14 = 2.5478657335042953e-06

Training epoch-68 batch-15
Running loss of epoch-68 batch-15 = 9.409850463271141e-06

Training epoch-68 batch-16
Running loss of epoch-68 batch-16 = 2.4903565645217896e-06

Training epoch-68 batch-17
Running loss of epoch-68 batch-17 = 1.030205748975277e-05

Training epoch-68 batch-18
Running loss of epoch-68 batch-18 = 6.716232746839523e-06

Training epoch-68 batch-19
Running loss of epoch-68 batch-19 = 6.4997002482414246e-06

Training epoch-68 batch-20
Running loss of epoch-68 batch-20 = 4.45009209215641e-06

Training epoch-68 batch-21
Running loss of epoch-68 batch-21 = 6.493180990219116e-06

Training epoch-68 batch-22
Running loss of epoch-68 batch-22 = 3.6170240491628647e-06

Training epoch-68 batch-23
Running loss of epoch-68 batch-23 = 1.8619466572999954e-06

Training epoch-68 batch-24
Running loss of epoch-68 batch-24 = 3.8512516766786575e-06

Training epoch-68 batch-25
Running loss of epoch-68 batch-25 = 4.204688593745232e-06

Training epoch-68 batch-26
Running loss of epoch-68 batch-26 = 1.5410128980875015e-05

Training epoch-68 batch-27
Running loss of epoch-68 batch-27 = 5.570705980062485e-06

Training epoch-68 batch-28
Running loss of epoch-68 batch-28 = 3.655906766653061e-06

Training epoch-68 batch-29
Running loss of epoch-68 batch-29 = 3.909924998879433e-06

Training epoch-68 batch-30
Running loss of epoch-68 batch-30 = 1.0429415851831436e-05

Training epoch-68 batch-31
Running loss of epoch-68 batch-31 = 1.239660196006298e-05

Training epoch-68 batch-32
Running loss of epoch-68 batch-32 = 1.3659941032528877e-05

Training epoch-68 batch-33
Running loss of epoch-68 batch-33 = 2.703862264752388e-06

Training epoch-68 batch-34
Running loss of epoch-68 batch-34 = 5.0228554755449295e-06

Training epoch-68 batch-35
Running loss of epoch-68 batch-35 = 1.5732133761048317e-05

Training epoch-68 batch-36
Running loss of epoch-68 batch-36 = 9.462470188736916e-06

Training epoch-68 batch-37
Running loss of epoch-68 batch-37 = 1.8925871700048447e-05

Training epoch-68 batch-38
Running loss of epoch-68 batch-38 = 4.9069058150053024e-06

Training epoch-68 batch-39
Running loss of epoch-68 batch-39 = 6.798421964049339e-06

Training epoch-68 batch-40
Running loss of epoch-68 batch-40 = 3.680586814880371e-06

Training epoch-68 batch-41
Running loss of epoch-68 batch-41 = 7.753027603030205e-06

Training epoch-68 batch-42
Running loss of epoch-68 batch-42 = 5.925074219703674e-06

Training epoch-68 batch-43
Running loss of epoch-68 batch-43 = 3.25031578540802e-06

Training epoch-68 batch-44
Running loss of epoch-68 batch-44 = 4.644738510251045e-06

Training epoch-68 batch-45
Running loss of epoch-68 batch-45 = 5.020527169108391e-06

Training epoch-68 batch-46
Running loss of epoch-68 batch-46 = 1.5994301065802574e-05

Training epoch-68 batch-47
Running loss of epoch-68 batch-47 = 4.945788532495499e-06

Training epoch-68 batch-48
Running loss of epoch-68 batch-48 = 5.492009222507477e-06

Training epoch-68 batch-49
Running loss of epoch-68 batch-49 = 6.447779014706612e-06

Training epoch-68 batch-50
Running loss of epoch-68 batch-50 = 4.065921530127525e-06

Training epoch-68 batch-51
Running loss of epoch-68 batch-51 = 3.102002665400505e-06

Training epoch-68 batch-52
Running loss of epoch-68 batch-52 = 7.326016202569008e-06

Training epoch-68 batch-53
Running loss of epoch-68 batch-53 = 4.260102286934853e-06

Training epoch-68 batch-54
Running loss of epoch-68 batch-54 = 5.591893568634987e-06

Training epoch-68 batch-55
Running loss of epoch-68 batch-55 = 6.52228482067585e-06

Training epoch-68 batch-56
Running loss of epoch-68 batch-56 = 1.1463649570941925e-05

Training epoch-68 batch-57
Running loss of epoch-68 batch-57 = 3.7478748708963394e-06

Training epoch-68 batch-58
Running loss of epoch-68 batch-58 = 7.337657734751701e-06

Training epoch-68 batch-59
Running loss of epoch-68 batch-59 = 4.498520866036415e-06

Training epoch-68 batch-60
Running loss of epoch-68 batch-60 = 7.801223546266556e-06

Training epoch-68 batch-61
Running loss of epoch-68 batch-61 = 8.582137525081635e-07

Training epoch-68 batch-62
Running loss of epoch-68 batch-62 = 3.862427547574043e-06

Training epoch-68 batch-63
Running loss of epoch-68 batch-63 = 3.373483195900917e-06

Training epoch-68 batch-64
Running loss of epoch-68 batch-64 = 5.055451765656471e-06

Training epoch-68 batch-65
Running loss of epoch-68 batch-65 = 7.099239155650139e-06

Training epoch-68 batch-66
Running loss of epoch-68 batch-66 = 2.944841980934143e-06

Training epoch-68 batch-67
Running loss of epoch-68 batch-67 = 4.704343155026436e-06

Training epoch-68 batch-68
Running loss of epoch-68 batch-68 = 2.682674676179886e-06

Training epoch-68 batch-69
Running loss of epoch-68 batch-69 = 6.688525900244713e-06

Training epoch-68 batch-70
Running loss of epoch-68 batch-70 = 6.9176312536001205e-06

Training epoch-68 batch-71
Running loss of epoch-68 batch-71 = 6.203539669513702e-06

Training epoch-68 batch-72
Running loss of epoch-68 batch-72 = 1.0407529771327972e-05

Training epoch-68 batch-73
Running loss of epoch-68 batch-73 = 5.89410774409771e-06

Training epoch-68 batch-74
Running loss of epoch-68 batch-74 = 2.4102628231048584e-06

Training epoch-68 batch-75
Running loss of epoch-68 batch-75 = 3.881519660353661e-06

Training epoch-68 batch-76
Running loss of epoch-68 batch-76 = 3.967201337218285e-06

Training epoch-68 batch-77
Running loss of epoch-68 batch-77 = 1.990934833884239e-06

Training epoch-68 batch-78
Running loss of epoch-68 batch-78 = 4.532979801297188e-06

Training epoch-68 batch-79
Running loss of epoch-68 batch-79 = 1.979921944439411e-05

Training epoch-68 batch-80
Running loss of epoch-68 batch-80 = 9.23406332731247e-07

Training epoch-68 batch-81
Running loss of epoch-68 batch-81 = 6.2587205320596695e-06

Training epoch-68 batch-82
Running loss of epoch-68 batch-82 = 3.316672518849373e-06

Training epoch-68 batch-83
Running loss of epoch-68 batch-83 = 7.689464837312698e-06

Training epoch-68 batch-84
Running loss of epoch-68 batch-84 = 3.848224878311157e-06

Training epoch-68 batch-85
Running loss of epoch-68 batch-85 = 7.361406460404396e-06

Training epoch-68 batch-86
Running loss of epoch-68 batch-86 = 4.651723429560661e-06

Training epoch-68 batch-87
Running loss of epoch-68 batch-87 = 2.3944303393363953e-06

Training epoch-68 batch-88
Running loss of epoch-68 batch-88 = 4.509231075644493e-06

Training epoch-68 batch-89
Running loss of epoch-68 batch-89 = 2.994900569319725e-06

Training epoch-68 batch-90
Running loss of epoch-68 batch-90 = 4.491535946726799e-06

Training epoch-68 batch-91
Running loss of epoch-68 batch-91 = 4.9497466534376144e-06

Training epoch-68 batch-92
Running loss of epoch-68 batch-92 = 1.0551651939749718e-05

Training epoch-68 batch-93
Running loss of epoch-68 batch-93 = 6.443820893764496e-06

Training epoch-68 batch-94
Running loss of epoch-68 batch-94 = 1.0537449270486832e-05

Training epoch-68 batch-95
Running loss of epoch-68 batch-95 = 2.9853777959942818e-05

Training epoch-68 batch-96
Running loss of epoch-68 batch-96 = 6.335321813821793e-06

Training epoch-68 batch-97
Running loss of epoch-68 batch-97 = 5.190260708332062e-06

Training epoch-68 batch-98
Running loss of epoch-68 batch-98 = 9.000999853014946e-06

Training epoch-68 batch-99
Running loss of epoch-68 batch-99 = 7.319962605834007e-06

Training epoch-68 batch-100
Running loss of epoch-68 batch-100 = 2.630520612001419e-06

Training epoch-68 batch-101
Running loss of epoch-68 batch-101 = 8.650589734315872e-06

Training epoch-68 batch-102
Running loss of epoch-68 batch-102 = 4.520174115896225e-06

Training epoch-68 batch-103
Running loss of epoch-68 batch-103 = 2.5909394025802612e-06

Training epoch-68 batch-104
Running loss of epoch-68 batch-104 = 4.763249307870865e-06

Training epoch-68 batch-105
Running loss of epoch-68 batch-105 = 9.145587682723999e-06

Training epoch-68 batch-106
Running loss of epoch-68 batch-106 = 6.3818879425525665e-06

Training epoch-68 batch-107
Running loss of epoch-68 batch-107 = 6.291782483458519e-06

Training epoch-68 batch-108
Running loss of epoch-68 batch-108 = 7.5891148298978806e-06

Training epoch-68 batch-109
Running loss of epoch-68 batch-109 = 3.0884984880685806e-06

Training epoch-68 batch-110
Running loss of epoch-68 batch-110 = 2.2614840418100357e-06

Training epoch-68 batch-111
Running loss of epoch-68 batch-111 = 2.0441832020878792e-05

Training epoch-68 batch-112
Running loss of epoch-68 batch-112 = 1.1958880349993706e-05

Training epoch-68 batch-113
Running loss of epoch-68 batch-113 = 8.685048669576645e-06

Training epoch-68 batch-114
Running loss of epoch-68 batch-114 = 4.63961623609066e-06

Training epoch-68 batch-115
Running loss of epoch-68 batch-115 = 9.120209142565727e-06

Training epoch-68 batch-116
Running loss of epoch-68 batch-116 = 1.1314405128359795e-05

Training epoch-68 batch-117
Running loss of epoch-68 batch-117 = 2.359505742788315e-06

Training epoch-68 batch-118
Running loss of epoch-68 batch-118 = 4.00724820792675e-06

Training epoch-68 batch-119
Running loss of epoch-68 batch-119 = 5.89410774409771e-06

Training epoch-68 batch-120
Running loss of epoch-68 batch-120 = 2.210726961493492e-06

Training epoch-68 batch-121
Running loss of epoch-68 batch-121 = 4.634493961930275e-06

Training epoch-68 batch-122
Running loss of epoch-68 batch-122 = 4.598405212163925e-06

Training epoch-68 batch-123
Running loss of epoch-68 batch-123 = 4.865461960434914e-06

Training epoch-68 batch-124
Running loss of epoch-68 batch-124 = 1.125875860452652e-05

Training epoch-68 batch-125
Running loss of epoch-68 batch-125 = 2.42539681494236e-06

Training epoch-68 batch-126
Running loss of epoch-68 batch-126 = 1.0051997378468513e-05

Training epoch-68 batch-127
Running loss of epoch-68 batch-127 = 8.963048458099365e-06

Training epoch-68 batch-128
Running loss of epoch-68 batch-128 = 7.593538612127304e-06

Training epoch-68 batch-129
Running loss of epoch-68 batch-129 = 6.864545866847038e-06

Training epoch-68 batch-130
Running loss of epoch-68 batch-130 = 5.340203642845154e-06

Training epoch-68 batch-131
Running loss of epoch-68 batch-131 = 4.9907248467206955e-06

Training epoch-68 batch-132
Running loss of epoch-68 batch-132 = 4.638219252228737e-06

Training epoch-68 batch-133
Running loss of epoch-68 batch-133 = 1.6831327229738235e-06

Training epoch-68 batch-134
Running loss of epoch-68 batch-134 = 1.4018500223755836e-05

Training epoch-68 batch-135
Running loss of epoch-68 batch-135 = 3.5741832107305527e-06

Training epoch-68 batch-136
Running loss of epoch-68 batch-136 = 2.843094989657402e-06

Training epoch-68 batch-137
Running loss of epoch-68 batch-137 = 7.733702659606934e-06

Training epoch-68 batch-138
Running loss of epoch-68 batch-138 = 7.439404726028442e-06

Training epoch-68 batch-139
Running loss of epoch-68 batch-139 = 4.6244822442531586e-06

Training epoch-68 batch-140
Running loss of epoch-68 batch-140 = 5.244975909590721e-06

Training epoch-68 batch-141
Running loss of epoch-68 batch-141 = 2.788146957755089e-06

Training epoch-68 batch-142
Running loss of epoch-68 batch-142 = 7.563037797808647e-06

Training epoch-68 batch-143
Running loss of epoch-68 batch-143 = 5.0317030400037766e-06

Training epoch-68 batch-144
Running loss of epoch-68 batch-144 = 7.3856208473443985e-06

Training epoch-68 batch-145
Running loss of epoch-68 batch-145 = 1.598382368683815e-06

Training epoch-68 batch-146
Running loss of epoch-68 batch-146 = 3.1944364309310913e-06

Training epoch-68 batch-147
Running loss of epoch-68 batch-147 = 1.68592669069767e-06

Training epoch-68 batch-148
Running loss of epoch-68 batch-148 = 4.097120836377144e-06

Training epoch-68 batch-149
Running loss of epoch-68 batch-149 = 1.664506271481514e-06

Training epoch-68 batch-150
Running loss of epoch-68 batch-150 = 6.37606717646122e-06

Training epoch-68 batch-151
Running loss of epoch-68 batch-151 = 3.5136472433805466e-06

Training epoch-68 batch-152
Running loss of epoch-68 batch-152 = 3.3811666071414948e-06

Training epoch-68 batch-153
Running loss of epoch-68 batch-153 = 2.5420449674129486e-06

Training epoch-68 batch-154
Running loss of epoch-68 batch-154 = 3.486406058073044e-06

Training epoch-68 batch-155
Running loss of epoch-68 batch-155 = 7.367925718426704e-06

Training epoch-68 batch-156
Running loss of epoch-68 batch-156 = 7.656170055270195e-06

Training epoch-68 batch-157
Running loss of epoch-68 batch-157 = 6.981194019317627e-06

Finished training epoch-68.



Average train loss at epoch-68 = 6.18036538362503e-06

Started Evaluation

Average val loss at epoch-68 = 1.7470101421499764

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-69


Training epoch-69 batch-1
Running loss of epoch-69 batch-1 = 4.485016688704491e-06

Training epoch-69 batch-2
Running loss of epoch-69 batch-2 = 4.936009645462036e-06

Training epoch-69 batch-3
Running loss of epoch-69 batch-3 = 2.0496081560850143e-06

Training epoch-69 batch-4
Running loss of epoch-69 batch-4 = 5.91157004237175e-06

Training epoch-69 batch-5
Running loss of epoch-69 batch-5 = 3.3599790185689926e-06

Training epoch-69 batch-6
Running loss of epoch-69 batch-6 = 4.756031557917595e-06

Training epoch-69 batch-7
Running loss of epoch-69 batch-7 = 4.169531166553497e-06

Training epoch-69 batch-8
Running loss of epoch-69 batch-8 = 6.911111995577812e-06

Training epoch-69 batch-9
Running loss of epoch-69 batch-9 = 2.2814609110355377e-05

Training epoch-69 batch-10
Running loss of epoch-69 batch-10 = 4.441943019628525e-06

Training epoch-69 batch-11
Running loss of epoch-69 batch-11 = 5.5015552788972855e-06

Training epoch-69 batch-12
Running loss of epoch-69 batch-12 = 2.8742942959070206e-06

Training epoch-69 batch-13
Running loss of epoch-69 batch-13 = 3.0328519642353058e-06

Training epoch-69 batch-14
Running loss of epoch-69 batch-14 = 7.223570719361305e-06

Training epoch-69 batch-15
Running loss of epoch-69 batch-15 = 5.329260602593422e-06

Training epoch-69 batch-16
Running loss of epoch-69 batch-16 = 3.970228135585785e-06

Training epoch-69 batch-17
Running loss of epoch-69 batch-17 = 6.7444052547216415e-06

Training epoch-69 batch-18
Running loss of epoch-69 batch-18 = 4.0316954255104065e-06

Training epoch-69 batch-19
Running loss of epoch-69 batch-19 = 4.258705303072929e-06

Training epoch-69 batch-20
Running loss of epoch-69 batch-20 = 6.765010766685009e-06

Training epoch-69 batch-21
Running loss of epoch-69 batch-21 = 3.3976975828409195e-06

Training epoch-69 batch-22
Running loss of epoch-69 batch-22 = 1.9441358745098114e-06

Training epoch-69 batch-23
Running loss of epoch-69 batch-23 = 4.938337951898575e-06

Training epoch-69 batch-24
Running loss of epoch-69 batch-24 = 6.609130650758743e-06

Training epoch-69 batch-25
Running loss of epoch-69 batch-25 = 6.903661414980888e-06

Training epoch-69 batch-26
Running loss of epoch-69 batch-26 = 5.274079740047455e-06

Training epoch-69 batch-27
Running loss of epoch-69 batch-27 = 5.444744601845741e-06

Training epoch-69 batch-28
Running loss of epoch-69 batch-28 = 1.0942341759800911e-05

Training epoch-69 batch-29
Running loss of epoch-69 batch-29 = 5.051027983427048e-06

Training epoch-69 batch-30
Running loss of epoch-69 batch-30 = 3.97232361137867e-06

Training epoch-69 batch-31
Running loss of epoch-69 batch-31 = 5.285022780299187e-06

Training epoch-69 batch-32
Running loss of epoch-69 batch-32 = 9.682029485702515e-06

Training epoch-69 batch-33
Running loss of epoch-69 batch-33 = 1.3125129044055939e-05

Training epoch-69 batch-34
Running loss of epoch-69 batch-34 = 6.8838708102703094e-06

Training epoch-69 batch-35
Running loss of epoch-69 batch-35 = 6.322050467133522e-06

Training epoch-69 batch-36
Running loss of epoch-69 batch-36 = 6.3427723944187164e-06

Training epoch-69 batch-37
Running loss of epoch-69 batch-37 = 5.75813464820385e-06

Training epoch-69 batch-38
Running loss of epoch-69 batch-38 = 2.515967935323715e-06

Training epoch-69 batch-39
Running loss of epoch-69 batch-39 = 2.775341272354126e-06

Training epoch-69 batch-40
Running loss of epoch-69 batch-40 = 6.048940122127533e-06

Training epoch-69 batch-41
Running loss of epoch-69 batch-41 = 4.4298358261585236e-06

Training epoch-69 batch-42
Running loss of epoch-69 batch-42 = 2.5150366127490997e-06

Training epoch-69 batch-43
Running loss of epoch-69 batch-43 = 5.058245733380318e-06

Training epoch-69 batch-44
Running loss of epoch-69 batch-44 = 2.4786917492747307e-05

Training epoch-69 batch-45
Running loss of epoch-69 batch-45 = 5.60050830245018e-06

Training epoch-69 batch-46
Running loss of epoch-69 batch-46 = 1.04452483355999e-05

Training epoch-69 batch-47
Running loss of epoch-69 batch-47 = 1.172325573861599e-05

Training epoch-69 batch-48
Running loss of epoch-69 batch-48 = 5.401903763413429e-06

Training epoch-69 batch-49
Running loss of epoch-69 batch-49 = 2.307351678609848e-06

Training epoch-69 batch-50
Running loss of epoch-69 batch-50 = 4.3406616896390915e-06

Training epoch-69 batch-51
Running loss of epoch-69 batch-51 = 8.15163366496563e-06

Training epoch-69 batch-52
Running loss of epoch-69 batch-52 = 4.142755642533302e-06

Training epoch-69 batch-53
Running loss of epoch-69 batch-53 = 9.818235412240028e-06

Training epoch-69 batch-54
Running loss of epoch-69 batch-54 = 6.300630047917366e-06

Training epoch-69 batch-55
Running loss of epoch-69 batch-55 = 6.653135642409325e-06

Training epoch-69 batch-56
Running loss of epoch-69 batch-56 = 4.033790901303291e-06

Training epoch-69 batch-57
Running loss of epoch-69 batch-57 = 4.294561222195625e-06

Training epoch-69 batch-58
Running loss of epoch-69 batch-58 = 1.0841293260455132e-05

Training epoch-69 batch-59
Running loss of epoch-69 batch-59 = 8.18958505988121e-06

Training epoch-69 batch-60
Running loss of epoch-69 batch-60 = 1.0787276551127434e-05

Training epoch-69 batch-61
Running loss of epoch-69 batch-61 = 4.621921107172966e-06

Training epoch-69 batch-62
Running loss of epoch-69 batch-62 = 5.221692845225334e-06

Training epoch-69 batch-63
Running loss of epoch-69 batch-63 = 1.1553056538105011e-05

Training epoch-69 batch-64
Running loss of epoch-69 batch-64 = 2.142740413546562e-06

Training epoch-69 batch-65
Running loss of epoch-69 batch-65 = 1.12124253064394e-05

Training epoch-69 batch-66
Running loss of epoch-69 batch-66 = 4.729023203253746e-06

Training epoch-69 batch-67
Running loss of epoch-69 batch-67 = 1.0453164577484131e-05

Training epoch-69 batch-68
Running loss of epoch-69 batch-68 = 5.262903869152069e-06

Training epoch-69 batch-69
Running loss of epoch-69 batch-69 = 4.090368747711182e-06

Training epoch-69 batch-70
Running loss of epoch-69 batch-70 = 3.214459866285324e-06

Training epoch-69 batch-71
Running loss of epoch-69 batch-71 = 1.2700911611318588e-06

Training epoch-69 batch-72
Running loss of epoch-69 batch-72 = 6.7106448113918304e-06

Training epoch-69 batch-73
Running loss of epoch-69 batch-73 = 3.5285484045743942e-06

Training epoch-69 batch-74
Running loss of epoch-69 batch-74 = 3.83215956389904e-06

Training epoch-69 batch-75
Running loss of epoch-69 batch-75 = 3.1099189072847366e-06

Training epoch-69 batch-76
Running loss of epoch-69 batch-76 = 8.503440767526627e-06

Training epoch-69 batch-77
Running loss of epoch-69 batch-77 = 3.966735675930977e-06

Training epoch-69 batch-78
Running loss of epoch-69 batch-78 = 4.704343155026436e-06

Training epoch-69 batch-79
Running loss of epoch-69 batch-79 = 5.029374733567238e-06

Training epoch-69 batch-80
Running loss of epoch-69 batch-80 = 1.7131678760051727e-06

Training epoch-69 batch-81
Running loss of epoch-69 batch-81 = 5.285022780299187e-06

Training epoch-69 batch-82
Running loss of epoch-69 batch-82 = 2.962537109851837e-06

Training epoch-69 batch-83
Running loss of epoch-69 batch-83 = 2.9248185455799103e-06

Training epoch-69 batch-84
Running loss of epoch-69 batch-84 = 5.686888471245766e-06

Training epoch-69 batch-85
Running loss of epoch-69 batch-85 = 5.619833245873451e-06

Training epoch-69 batch-86
Running loss of epoch-69 batch-86 = 7.386552169919014e-06

Training epoch-69 batch-87
Running loss of epoch-69 batch-87 = 7.991446182131767e-06

Training epoch-69 batch-88
Running loss of epoch-69 batch-88 = 1.5879515558481216e-05

Training epoch-69 batch-89
Running loss of epoch-69 batch-89 = 7.29365274310112e-06

Training epoch-69 batch-90
Running loss of epoch-69 batch-90 = 7.239403203129768e-06

Training epoch-69 batch-91
Running loss of epoch-69 batch-91 = 2.7141068130731583e-06

Training epoch-69 batch-92
Running loss of epoch-69 batch-92 = 4.1353050619363785e-06

Training epoch-69 batch-93
Running loss of epoch-69 batch-93 = 4.215165972709656e-06

Training epoch-69 batch-94
Running loss of epoch-69 batch-94 = 7.109483703970909e-06

Training epoch-69 batch-95
Running loss of epoch-69 batch-95 = 5.418900400400162e-06

Training epoch-69 batch-96
Running loss of epoch-69 batch-96 = 1.1909054592251778e-05

Training epoch-69 batch-97
Running loss of epoch-69 batch-97 = 6.702030077576637e-06

Training epoch-69 batch-98
Running loss of epoch-69 batch-98 = 3.938563168048859e-06

Training epoch-69 batch-99
Running loss of epoch-69 batch-99 = 1.5082070603966713e-05

Training epoch-69 batch-100
Running loss of epoch-69 batch-100 = 7.621711120009422e-06

Training epoch-69 batch-101
Running loss of epoch-69 batch-101 = 3.0535738915205e-06

Training epoch-69 batch-102
Running loss of epoch-69 batch-102 = 9.502284228801727e-06

Training epoch-69 batch-103
Running loss of epoch-69 batch-103 = 7.93510116636753e-06

Training epoch-69 batch-104
Running loss of epoch-69 batch-104 = 1.047714613378048e-05

Training epoch-69 batch-105
Running loss of epoch-69 batch-105 = 5.768844857811928e-06

Training epoch-69 batch-106
Running loss of epoch-69 batch-106 = 5.929730832576752e-06

Training epoch-69 batch-107
Running loss of epoch-69 batch-107 = 4.759989678859711e-06

Training epoch-69 batch-108
Running loss of epoch-69 batch-108 = 6.300164386630058e-06

Training epoch-69 batch-109
Running loss of epoch-69 batch-109 = 2.27196142077446e-06

Training epoch-69 batch-110
Running loss of epoch-69 batch-110 = 1.3676472008228302e-05

Training epoch-69 batch-111
Running loss of epoch-69 batch-111 = 1.9060447812080383e-05

Training epoch-69 batch-112
Running loss of epoch-69 batch-112 = 7.58003443479538e-06

Training epoch-69 batch-113
Running loss of epoch-69 batch-113 = 1.9047874957323074e-06

Training epoch-69 batch-114
Running loss of epoch-69 batch-114 = 2.471962943673134e-06

Training epoch-69 batch-115
Running loss of epoch-69 batch-115 = 3.728782758116722e-06

Training epoch-69 batch-116
Running loss of epoch-69 batch-116 = 4.253815859556198e-06

Training epoch-69 batch-117
Running loss of epoch-69 batch-117 = 2.0454172044992447e-06

Training epoch-69 batch-118
Running loss of epoch-69 batch-118 = 2.4989712983369827e-06

Training epoch-69 batch-119
Running loss of epoch-69 batch-119 = 8.179806172847748e-06

Training epoch-69 batch-120
Running loss of epoch-69 batch-120 = 4.132743924856186e-06

Training epoch-69 batch-121
Running loss of epoch-69 batch-121 = 4.989327862858772e-06

Training epoch-69 batch-122
Running loss of epoch-69 batch-122 = 2.127373591065407e-06

Training epoch-69 batch-123
Running loss of epoch-69 batch-123 = 5.272449925541878e-06

Training epoch-69 batch-124
Running loss of epoch-69 batch-124 = 1.2703007087111473e-05

Training epoch-69 batch-125
Running loss of epoch-69 batch-125 = 4.200031980872154e-06

Training epoch-69 batch-126
Running loss of epoch-69 batch-126 = 4.390254616737366e-06

Training epoch-69 batch-127
Running loss of epoch-69 batch-127 = 3.6729034036397934e-06

Training epoch-69 batch-128
Running loss of epoch-69 batch-128 = 1.737847924232483e-06

Training epoch-69 batch-129
Running loss of epoch-69 batch-129 = 6.085960194468498e-06

Training epoch-69 batch-130
Running loss of epoch-69 batch-130 = 4.3942127376794815e-06

Training epoch-69 batch-131
Running loss of epoch-69 batch-131 = 1.800013706088066e-06

Training epoch-69 batch-132
Running loss of epoch-69 batch-132 = 4.105968400835991e-06

Training epoch-69 batch-133
Running loss of epoch-69 batch-133 = 2.310611307621002e-06

Training epoch-69 batch-134
Running loss of epoch-69 batch-134 = 8.146511390805244e-06

Training epoch-69 batch-135
Running loss of epoch-69 batch-135 = 2.846820279955864e-06

Training epoch-69 batch-136
Running loss of epoch-69 batch-136 = 6.78584910929203e-06

Training epoch-69 batch-137
Running loss of epoch-69 batch-137 = 4.2514875531196594e-06

Training epoch-69 batch-138
Running loss of epoch-69 batch-138 = 6.623566150665283e-06

Training epoch-69 batch-139
Running loss of epoch-69 batch-139 = 3.896187990903854e-06

Training epoch-69 batch-140
Running loss of epoch-69 batch-140 = 7.124152034521103e-06

Training epoch-69 batch-141
Running loss of epoch-69 batch-141 = 5.118316039443016e-06

Training epoch-69 batch-142
Running loss of epoch-69 batch-142 = 6.026355549693108e-06

Training epoch-69 batch-143
Running loss of epoch-69 batch-143 = 2.0277220755815506e-06

Training epoch-69 batch-144
Running loss of epoch-69 batch-144 = 9.169569239020348e-06

Training epoch-69 batch-145
Running loss of epoch-69 batch-145 = 3.8105063140392303e-06

Training epoch-69 batch-146
Running loss of epoch-69 batch-146 = 3.291293978691101e-06

Training epoch-69 batch-147
Running loss of epoch-69 batch-147 = 5.73648139834404e-06

Training epoch-69 batch-148
Running loss of epoch-69 batch-148 = 3.4973490983247757e-06

Training epoch-69 batch-149
Running loss of epoch-69 batch-149 = 2.453802153468132e-06

Training epoch-69 batch-150
Running loss of epoch-69 batch-150 = 7.731374353170395e-06

Training epoch-69 batch-151
Running loss of epoch-69 batch-151 = 4.877801984548569e-06

Training epoch-69 batch-152
Running loss of epoch-69 batch-152 = 8.452218025922775e-06

Training epoch-69 batch-153
Running loss of epoch-69 batch-153 = 6.610061973333359e-06

Training epoch-69 batch-154
Running loss of epoch-69 batch-154 = 1.06769148260355e-05

Training epoch-69 batch-155
Running loss of epoch-69 batch-155 = 3.3548567444086075e-06

Training epoch-69 batch-156
Running loss of epoch-69 batch-156 = 4.541361704468727e-06

Training epoch-69 batch-157
Running loss of epoch-69 batch-157 = 7.145106792449951e-06

Finished training epoch-69.



Average train loss at epoch-69 = 5.995204299688339e-06

Started Evaluation

Average val loss at epoch-69 = 1.7528684220126804

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-70


Training epoch-70 batch-1
Running loss of epoch-70 batch-1 = 2.7990899980068207e-06

Training epoch-70 batch-2
Running loss of epoch-70 batch-2 = 6.1104074120521545e-06

Training epoch-70 batch-3
Running loss of epoch-70 batch-3 = 4.712259396910667e-06

Training epoch-70 batch-4
Running loss of epoch-70 batch-4 = 8.436152711510658e-06

Training epoch-70 batch-5
Running loss of epoch-70 batch-5 = 5.499226972460747e-06

Training epoch-70 batch-6
Running loss of epoch-70 batch-6 = 6.331130862236023e-06

Training epoch-70 batch-7
Running loss of epoch-70 batch-7 = 4.142755642533302e-06

Training epoch-70 batch-8
Running loss of epoch-70 batch-8 = 3.5336706787347794e-06

Training epoch-70 batch-9
Running loss of epoch-70 batch-9 = 5.025649443268776e-06

Training epoch-70 batch-10
Running loss of epoch-70 batch-10 = 5.330657586455345e-06

Training epoch-70 batch-11
Running loss of epoch-70 batch-11 = 4.375819116830826e-06

Training epoch-70 batch-12
Running loss of epoch-70 batch-12 = 6.521819159388542e-06

Training epoch-70 batch-13
Running loss of epoch-70 batch-13 = 4.257075488567352e-06

Training epoch-70 batch-14
Running loss of epoch-70 batch-14 = 3.3061951398849487e-06

Training epoch-70 batch-15
Running loss of epoch-70 batch-15 = 3.9907172322273254e-06

Training epoch-70 batch-16
Running loss of epoch-70 batch-16 = 6.326008588075638e-06

Training epoch-70 batch-17
Running loss of epoch-70 batch-17 = 2.666609361767769e-06

Training epoch-70 batch-18
Running loss of epoch-70 batch-18 = 3.7462450563907623e-06

Training epoch-70 batch-19
Running loss of epoch-70 batch-19 = 2.8600916266441345e-06

Training epoch-70 batch-20
Running loss of epoch-70 batch-20 = 7.509021088480949e-06

Training epoch-70 batch-21
Running loss of epoch-70 batch-21 = 6.262911483645439e-06

Training epoch-70 batch-22
Running loss of epoch-70 batch-22 = 4.512257874011993e-06

Training epoch-70 batch-23
Running loss of epoch-70 batch-23 = 6.947899237275124e-06

Training epoch-70 batch-24
Running loss of epoch-70 batch-24 = 3.934139385819435e-06

Training epoch-70 batch-25
Running loss of epoch-70 batch-25 = 1.7455313354730606e-06

Training epoch-70 batch-26
Running loss of epoch-70 batch-26 = 4.002824425697327e-06

Training epoch-70 batch-27
Running loss of epoch-70 batch-27 = 3.405380994081497e-06

Training epoch-70 batch-28
Running loss of epoch-70 batch-28 = 2.791406586766243e-06

Training epoch-70 batch-29
Running loss of epoch-70 batch-29 = 5.078036338090897e-06

Training epoch-70 batch-30
Running loss of epoch-70 batch-30 = 6.331363692879677e-06

Training epoch-70 batch-31
Running loss of epoch-70 batch-31 = 6.966758519411087e-06

Training epoch-70 batch-32
Running loss of epoch-70 batch-32 = 4.2244791984558105e-06

Training epoch-70 batch-33
Running loss of epoch-70 batch-33 = 6.282469257712364e-06

Training epoch-70 batch-34
Running loss of epoch-70 batch-34 = 5.352310836315155e-06

Training epoch-70 batch-35
Running loss of epoch-70 batch-35 = 3.387685865163803e-06

Training epoch-70 batch-36
Running loss of epoch-70 batch-36 = 6.089918315410614e-06

Training epoch-70 batch-37
Running loss of epoch-70 batch-37 = 1.6302801668643951e-06

Training epoch-70 batch-38
Running loss of epoch-70 batch-38 = 4.8908405005931854e-06

Training epoch-70 batch-39
Running loss of epoch-70 batch-39 = 4.699453711509705e-06

Training epoch-70 batch-40
Running loss of epoch-70 batch-40 = 5.26895746588707e-06

Training epoch-70 batch-41
Running loss of epoch-70 batch-41 = 3.168359398841858e-06

Training epoch-70 batch-42
Running loss of epoch-70 batch-42 = 8.634990081191063e-06

Training epoch-70 batch-43
Running loss of epoch-70 batch-43 = 4.2584724724292755e-06

Training epoch-70 batch-44
Running loss of epoch-70 batch-44 = 6.195157766342163e-06

Training epoch-70 batch-45
Running loss of epoch-70 batch-45 = 4.715286195278168e-06

Training epoch-70 batch-46
Running loss of epoch-70 batch-46 = 7.93440267443657e-06

Training epoch-70 batch-47
Running loss of epoch-70 batch-47 = 6.932998076081276e-06

Training epoch-70 batch-48
Running loss of epoch-70 batch-48 = 2.8314534574747086e-06

Training epoch-70 batch-49
Running loss of epoch-70 batch-49 = 3.0265655368566513e-06

Training epoch-70 batch-50
Running loss of epoch-70 batch-50 = 3.1942036002874374e-06

Training epoch-70 batch-51
Running loss of epoch-70 batch-51 = 1.9396888092160225e-05

Training epoch-70 batch-52
Running loss of epoch-70 batch-52 = 8.140690624713898e-06

Training epoch-70 batch-53
Running loss of epoch-70 batch-53 = 5.381647497415543e-06

Training epoch-70 batch-54
Running loss of epoch-70 batch-54 = 9.866897016763687e-06

Training epoch-70 batch-55
Running loss of epoch-70 batch-55 = 6.049405783414841e-06

Training epoch-70 batch-56
Running loss of epoch-70 batch-56 = 5.583278834819794e-06

Training epoch-70 batch-57
Running loss of epoch-70 batch-57 = 5.142763257026672e-06

Training epoch-70 batch-58
Running loss of epoch-70 batch-58 = 2.2687017917633057e-06

Training epoch-70 batch-59
Running loss of epoch-70 batch-59 = 9.76654700934887e-06

Training epoch-70 batch-60
Running loss of epoch-70 batch-60 = 2.6936177164316177e-06

Training epoch-70 batch-61
Running loss of epoch-70 batch-61 = 5.47175295650959e-06

Training epoch-70 batch-62
Running loss of epoch-70 batch-62 = 5.3942203521728516e-06

Training epoch-70 batch-63
Running loss of epoch-70 batch-63 = 1.3641547411680222e-06

Training epoch-70 batch-64
Running loss of epoch-70 batch-64 = 5.754176527261734e-06

Training epoch-70 batch-65
Running loss of epoch-70 batch-65 = 6.776070222258568e-06

Training epoch-70 batch-66
Running loss of epoch-70 batch-66 = 4.20399010181427e-06

Training epoch-70 batch-67
Running loss of epoch-70 batch-67 = 2.7497299015522003e-06

Training epoch-70 batch-68
Running loss of epoch-70 batch-68 = 5.89410774409771e-06

Training epoch-70 batch-69
Running loss of epoch-70 batch-69 = 6.392132490873337e-06

Training epoch-70 batch-70
Running loss of epoch-70 batch-70 = 5.4105184972286224e-06

Training epoch-70 batch-71
Running loss of epoch-70 batch-71 = 4.788162186741829e-06

Training epoch-70 batch-72
Running loss of epoch-70 batch-72 = 4.723202437162399e-06

Training epoch-70 batch-73
Running loss of epoch-70 batch-73 = 6.01448118686676e-06

Training epoch-70 batch-74
Running loss of epoch-70 batch-74 = 1.4832476153969765e-05

Training epoch-70 batch-75
Running loss of epoch-70 batch-75 = 9.10228118300438e-06

Training epoch-70 batch-76
Running loss of epoch-70 batch-76 = 6.895279511809349e-06

Training epoch-70 batch-77
Running loss of epoch-70 batch-77 = 8.374452590942383e-06

Training epoch-70 batch-78
Running loss of epoch-70 batch-78 = 6.083864718675613e-06

Training epoch-70 batch-79
Running loss of epoch-70 batch-79 = 2.818182110786438e-06

Training epoch-70 batch-80
Running loss of epoch-70 batch-80 = 5.680369213223457e-06

Training epoch-70 batch-81
Running loss of epoch-70 batch-81 = 4.364177584648132e-06

Training epoch-70 batch-82
Running loss of epoch-70 batch-82 = 6.057322025299072e-06

Training epoch-70 batch-83
Running loss of epoch-70 batch-83 = 1.0268064215779305e-05

Training epoch-70 batch-84
Running loss of epoch-70 batch-84 = 9.108567610383034e-06

Training epoch-70 batch-85
Running loss of epoch-70 batch-85 = 7.044291123747826e-06

Training epoch-70 batch-86
Running loss of epoch-70 batch-86 = 2.223299816250801e-06

Training epoch-70 batch-87
Running loss of epoch-70 batch-87 = 4.693400114774704e-06

Training epoch-70 batch-88
Running loss of epoch-70 batch-88 = 2.032297197729349e-05

Training epoch-70 batch-89
Running loss of epoch-70 batch-89 = 3.4782569855451584e-06

Training epoch-70 batch-90
Running loss of epoch-70 batch-90 = 3.251945599913597e-06

Training epoch-70 batch-91
Running loss of epoch-70 batch-91 = 6.62938691675663e-06

Training epoch-70 batch-92
Running loss of epoch-70 batch-92 = 5.073612555861473e-06

Training epoch-70 batch-93
Running loss of epoch-70 batch-93 = 4.372093826532364e-06

Training epoch-70 batch-94
Running loss of epoch-70 batch-94 = 6.004702299833298e-06

Training epoch-70 batch-95
Running loss of epoch-70 batch-95 = 7.790978997945786e-06

Training epoch-70 batch-96
Running loss of epoch-70 batch-96 = 4.66429628431797e-06

Training epoch-70 batch-97
Running loss of epoch-70 batch-97 = 8.791685104370117e-06

Training epoch-70 batch-98
Running loss of epoch-70 batch-98 = 2.8260983526706696e-06

Training epoch-70 batch-99
Running loss of epoch-70 batch-99 = 2.871965989470482e-06

Training epoch-70 batch-100
Running loss of epoch-70 batch-100 = 7.733004167675972e-06

Training epoch-70 batch-101
Running loss of epoch-70 batch-101 = 6.267102435231209e-06

Training epoch-70 batch-102
Running loss of epoch-70 batch-102 = 7.741386070847511e-06

Training epoch-70 batch-103
Running loss of epoch-70 batch-103 = 1.6451813280582428e-06

Training epoch-70 batch-104
Running loss of epoch-70 batch-104 = 1.639360561966896e-05

Training epoch-70 batch-105
Running loss of epoch-70 batch-105 = 1.0736286640167236e-05

Training epoch-70 batch-106
Running loss of epoch-70 batch-106 = 4.5245978981256485e-06

Training epoch-70 batch-107
Running loss of epoch-70 batch-107 = 3.504101186990738e-06

Training epoch-70 batch-108
Running loss of epoch-70 batch-108 = 1.0524177923798561e-05

Training epoch-70 batch-109
Running loss of epoch-70 batch-109 = 4.496658220887184e-06

Training epoch-70 batch-110
Running loss of epoch-70 batch-110 = 2.8347130864858627e-06

Training epoch-70 batch-111
Running loss of epoch-70 batch-111 = 3.705499693751335e-06

Training epoch-70 batch-112
Running loss of epoch-70 batch-112 = 7.7015720307827e-06

Training epoch-70 batch-113
Running loss of epoch-70 batch-113 = 6.279675289988518e-06

Training epoch-70 batch-114
Running loss of epoch-70 batch-114 = 6.11436553299427e-06

Training epoch-70 batch-115
Running loss of epoch-70 batch-115 = 5.59748150408268e-06

Training epoch-70 batch-116
Running loss of epoch-70 batch-116 = 1.1221040040254593e-05

Training epoch-70 batch-117
Running loss of epoch-70 batch-117 = 4.3066684156656265e-06

Training epoch-70 batch-118
Running loss of epoch-70 batch-118 = 9.739305824041367e-06

Training epoch-70 batch-119
Running loss of epoch-70 batch-119 = 3.0242372304201126e-06

Training epoch-70 batch-120
Running loss of epoch-70 batch-120 = 6.5676867961883545e-06

Training epoch-70 batch-121
Running loss of epoch-70 batch-121 = 1.7797574400901794e-06

Training epoch-70 batch-122
Running loss of epoch-70 batch-122 = 2.5492627173662186e-06

Training epoch-70 batch-123
Running loss of epoch-70 batch-123 = 4.15463000535965e-06

Training epoch-70 batch-124
Running loss of epoch-70 batch-124 = 1.2214761227369308e-05

Training epoch-70 batch-125
Running loss of epoch-70 batch-125 = 2.2919848561286926e-06

Training epoch-70 batch-126
Running loss of epoch-70 batch-126 = 1.792237162590027e-05

Training epoch-70 batch-127
Running loss of epoch-70 batch-127 = 3.3709220588207245e-06

Training epoch-70 batch-128
Running loss of epoch-70 batch-128 = 8.825212717056274e-06

Training epoch-70 batch-129
Running loss of epoch-70 batch-129 = 1.6762875020503998e-05

Training epoch-70 batch-130
Running loss of epoch-70 batch-130 = 7.1784015744924545e-06

Training epoch-70 batch-131
Running loss of epoch-70 batch-131 = 1.1373776942491531e-06

Training epoch-70 batch-132
Running loss of epoch-70 batch-132 = 5.015404894948006e-06

Training epoch-70 batch-133
Running loss of epoch-70 batch-133 = 3.905501216650009e-06

Training epoch-70 batch-134
Running loss of epoch-70 batch-134 = 1.7744023352861404e-06

Training epoch-70 batch-135
Running loss of epoch-70 batch-135 = 2.4989712983369827e-06

Training epoch-70 batch-136
Running loss of epoch-70 batch-136 = 1.4839926734566689e-05

Training epoch-70 batch-137
Running loss of epoch-70 batch-137 = 1.8102582544088364e-06

Training epoch-70 batch-138
Running loss of epoch-70 batch-138 = 3.909692168235779e-06

Training epoch-70 batch-139
Running loss of epoch-70 batch-139 = 3.4675467759370804e-06

Training epoch-70 batch-140
Running loss of epoch-70 batch-140 = 1.348857767879963e-05

Training epoch-70 batch-141
Running loss of epoch-70 batch-141 = 3.881985321640968e-06

Training epoch-70 batch-142
Running loss of epoch-70 batch-142 = 4.607019945979118e-06

Training epoch-70 batch-143
Running loss of epoch-70 batch-143 = 5.83333894610405e-06

Training epoch-70 batch-144
Running loss of epoch-70 batch-144 = 5.08246012032032e-06

Training epoch-70 batch-145
Running loss of epoch-70 batch-145 = 6.8820081651210785e-06

Training epoch-70 batch-146
Running loss of epoch-70 batch-146 = 5.416572093963623e-06

Training epoch-70 batch-147
Running loss of epoch-70 batch-147 = 1.1575408279895782e-05

Training epoch-70 batch-148
Running loss of epoch-70 batch-148 = 2.9366929084062576e-06

Training epoch-70 batch-149
Running loss of epoch-70 batch-149 = 4.443805664777756e-06

Training epoch-70 batch-150
Running loss of epoch-70 batch-150 = 1.2677628546953201e-06

Training epoch-70 batch-151
Running loss of epoch-70 batch-151 = 6.247544661164284e-06

Training epoch-70 batch-152
Running loss of epoch-70 batch-152 = 4.7460198402404785e-06

Training epoch-70 batch-153
Running loss of epoch-70 batch-153 = 4.804227501153946e-06

Training epoch-70 batch-154
Running loss of epoch-70 batch-154 = 7.113208994269371e-06

Training epoch-70 batch-155
Running loss of epoch-70 batch-155 = 8.270377293229103e-06

Training epoch-70 batch-156
Running loss of epoch-70 batch-156 = 1.171068288385868e-05

Training epoch-70 batch-157
Running loss of epoch-70 batch-157 = 1.0773539543151855e-05

Finished training epoch-70.



Average train loss at epoch-70 = 5.90142235159874e-06

Started Evaluation

Average val loss at epoch-70 = 1.7569604874585931

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 89.02 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-71


Training epoch-71 batch-1
Running loss of epoch-71 batch-1 = 4.454748705029488e-06

Training epoch-71 batch-2
Running loss of epoch-71 batch-2 = 3.627501428127289e-06

Training epoch-71 batch-3
Running loss of epoch-71 batch-3 = 6.5818894654512405e-06

Training epoch-71 batch-4
Running loss of epoch-71 batch-4 = 7.472001016139984e-06

Training epoch-71 batch-5
Running loss of epoch-71 batch-5 = 2.5692861527204514e-06

Training epoch-71 batch-6
Running loss of epoch-71 batch-6 = 1.2618256732821465e-05

Training epoch-71 batch-7
Running loss of epoch-71 batch-7 = 4.5388005673885345e-06

Training epoch-71 batch-8
Running loss of epoch-71 batch-8 = 6.712041795253754e-06

Training epoch-71 batch-9
Running loss of epoch-71 batch-9 = 7.584225386381149e-06

Training epoch-71 batch-10
Running loss of epoch-71 batch-10 = 4.823319613933563e-06

Training epoch-71 batch-11
Running loss of epoch-71 batch-11 = 4.7567300498485565e-06

Training epoch-71 batch-12
Running loss of epoch-71 batch-12 = 4.066852852702141e-06

Training epoch-71 batch-13
Running loss of epoch-71 batch-13 = 2.4461187422275543e-06

Training epoch-71 batch-14
Running loss of epoch-71 batch-14 = 3.530411049723625e-06

Training epoch-71 batch-15
Running loss of epoch-71 batch-15 = 1.8780119717121124e-06

Training epoch-71 batch-16
Running loss of epoch-71 batch-16 = 1.1044321581721306e-05

Training epoch-71 batch-17
Running loss of epoch-71 batch-17 = 1.0349089279770851e-05

Training epoch-71 batch-18
Running loss of epoch-71 batch-18 = 5.4568517953157425e-06

Training epoch-71 batch-19
Running loss of epoch-71 batch-19 = 2.1650921553373337e-06

Training epoch-71 batch-20
Running loss of epoch-71 batch-20 = 3.9583537727594376e-06

Training epoch-71 batch-21
Running loss of epoch-71 batch-21 = 5.489448085427284e-06

Training epoch-71 batch-22
Running loss of epoch-71 batch-22 = 3.988388925790787e-06

Training epoch-71 batch-23
Running loss of epoch-71 batch-23 = 9.018462151288986e-06

Training epoch-71 batch-24
Running loss of epoch-71 batch-24 = 5.4729171097278595e-06

Training epoch-71 batch-25
Running loss of epoch-71 batch-25 = 3.61492857336998e-06

Training epoch-71 batch-26
Running loss of epoch-71 batch-26 = 6.9553498178720474e-06

Training epoch-71 batch-27
Running loss of epoch-71 batch-27 = 1.0237563401460648e-05

Training epoch-71 batch-28
Running loss of epoch-71 batch-28 = 4.5995693653821945e-06

Training epoch-71 batch-29
Running loss of epoch-71 batch-29 = 5.904817953705788e-06

Training epoch-71 batch-30
Running loss of epoch-71 batch-30 = 9.196344763040543e-06

Training epoch-71 batch-31
Running loss of epoch-71 batch-31 = 3.923196345567703e-06

Training epoch-71 batch-32
Running loss of epoch-71 batch-32 = 4.0424056351184845e-06

Training epoch-71 batch-33
Running loss of epoch-71 batch-33 = 4.425179213285446e-06

Training epoch-71 batch-34
Running loss of epoch-71 batch-34 = 2.8721988201141357e-06

Training epoch-71 batch-35
Running loss of epoch-71 batch-35 = 2.8454232960939407e-06

Training epoch-71 batch-36
Running loss of epoch-71 batch-36 = 3.867549821734428e-06

Training epoch-71 batch-37
Running loss of epoch-71 batch-37 = 1.1350028216838837e-05

Training epoch-71 batch-38
Running loss of epoch-71 batch-38 = 1.2796372175216675e-05

Training epoch-71 batch-39
Running loss of epoch-71 batch-39 = 5.877343937754631e-06

Training epoch-71 batch-40
Running loss of epoch-71 batch-40 = 1.370883546769619e-05

Training epoch-71 batch-41
Running loss of epoch-71 batch-41 = 2.521323040127754e-06

Training epoch-71 batch-42
Running loss of epoch-71 batch-42 = 3.971857950091362e-06

Training epoch-71 batch-43
Running loss of epoch-71 batch-43 = 5.3283292800188065e-06

Training epoch-71 batch-44
Running loss of epoch-71 batch-44 = 9.348848834633827e-06

Training epoch-71 batch-45
Running loss of epoch-71 batch-45 = 4.1997991502285e-06

Training epoch-71 batch-46
Running loss of epoch-71 batch-46 = 3.6901328712701797e-06

Training epoch-71 batch-47
Running loss of epoch-71 batch-47 = 8.906470611691475e-06

Training epoch-71 batch-48
Running loss of epoch-71 batch-48 = 4.728790372610092e-06

Training epoch-71 batch-49
Running loss of epoch-71 batch-49 = 2.889661118388176e-06

Training epoch-71 batch-50
Running loss of epoch-71 batch-50 = 5.705980584025383e-06

Training epoch-71 batch-51
Running loss of epoch-71 batch-51 = 2.015591599047184e-05

Training epoch-71 batch-52
Running loss of epoch-71 batch-52 = 1.008366234600544e-05

Training epoch-71 batch-53
Running loss of epoch-71 batch-53 = 3.550201654434204e-06

Training epoch-71 batch-54
Running loss of epoch-71 batch-54 = 7.882481440901756e-06

Training epoch-71 batch-55
Running loss of epoch-71 batch-55 = 3.310851752758026e-06

Training epoch-71 batch-56
Running loss of epoch-71 batch-56 = 1.1499971151351929e-05

Training epoch-71 batch-57
Running loss of epoch-71 batch-57 = 6.741611286997795e-06

Training epoch-71 batch-58
Running loss of epoch-71 batch-58 = 2.591637894511223e-06

Training epoch-71 batch-59
Running loss of epoch-71 batch-59 = 6.686197593808174e-06

Training epoch-71 batch-60
Running loss of epoch-71 batch-60 = 3.2356474548578262e-06

Training epoch-71 batch-61
Running loss of epoch-71 batch-61 = 4.388624802231789e-06

Training epoch-71 batch-62
Running loss of epoch-71 batch-62 = 5.137408152222633e-06

Training epoch-71 batch-63
Running loss of epoch-71 batch-63 = 5.38933090865612e-06

Training epoch-71 batch-64
Running loss of epoch-71 batch-64 = 5.6729186326265335e-06

Training epoch-71 batch-65
Running loss of epoch-71 batch-65 = 5.045905709266663e-06

Training epoch-71 batch-66
Running loss of epoch-71 batch-66 = 7.643364369869232e-06

Training epoch-71 batch-67
Running loss of epoch-71 batch-67 = 4.555797204375267e-06

Training epoch-71 batch-68
Running loss of epoch-71 batch-68 = 5.886191502213478e-06

Training epoch-71 batch-69
Running loss of epoch-71 batch-69 = 4.3280888348817825e-06

Training epoch-71 batch-70
Running loss of epoch-71 batch-70 = 5.869660526514053e-06

Training epoch-71 batch-71
Running loss of epoch-71 batch-71 = 4.195608198642731e-06

Training epoch-71 batch-72
Running loss of epoch-71 batch-72 = 1.5344936400651932e-05

Training epoch-71 batch-73
Running loss of epoch-71 batch-73 = 1.9241124391555786e-06

Training epoch-71 batch-74
Running loss of epoch-71 batch-74 = 3.5972334444522858e-06

Training epoch-71 batch-75
Running loss of epoch-71 batch-75 = 3.3152755349874496e-06

Training epoch-71 batch-76
Running loss of epoch-71 batch-76 = 1.0181916877627373e-05

Training epoch-71 batch-77
Running loss of epoch-71 batch-77 = 4.163244739174843e-06

Training epoch-71 batch-78
Running loss of epoch-71 batch-78 = 1.7494894564151764e-06

Training epoch-71 batch-79
Running loss of epoch-71 batch-79 = 1.0754913091659546e-05

Training epoch-71 batch-80
Running loss of epoch-71 batch-80 = 1.7220154404640198e-06

Training epoch-71 batch-81
Running loss of epoch-71 batch-81 = 3.2016541808843613e-06

Training epoch-71 batch-82
Running loss of epoch-71 batch-82 = 2.880813553929329e-06

Training epoch-71 batch-83
Running loss of epoch-71 batch-83 = 3.841239959001541e-06

Training epoch-71 batch-84
Running loss of epoch-71 batch-84 = 5.631009116768837e-06

Training epoch-71 batch-85
Running loss of epoch-71 batch-85 = 1.2693926692008972e-06

Training epoch-71 batch-86
Running loss of epoch-71 batch-86 = 2.1294690668582916e-06

Training epoch-71 batch-87
Running loss of epoch-71 batch-87 = 6.148125976324081e-06

Training epoch-71 batch-88
Running loss of epoch-71 batch-88 = 1.1416152119636536e-05

Training epoch-71 batch-89
Running loss of epoch-71 batch-89 = 7.0123933255672455e-06

Training epoch-71 batch-90
Running loss of epoch-71 batch-90 = 3.795372322201729e-06

Training epoch-71 batch-91
Running loss of epoch-71 batch-91 = 3.596534952521324e-06

Training epoch-71 batch-92
Running loss of epoch-71 batch-92 = 3.6314595490694046e-06

Training epoch-71 batch-93
Running loss of epoch-71 batch-93 = 4.530418664216995e-06

Training epoch-71 batch-94
Running loss of epoch-71 batch-94 = 1.096632331609726e-06

Training epoch-71 batch-95
Running loss of epoch-71 batch-95 = 2.535991370677948e-06

Training epoch-71 batch-96
Running loss of epoch-71 batch-96 = 7.809838280081749e-06

Training epoch-71 batch-97
Running loss of epoch-71 batch-97 = 5.490612238645554e-06

Training epoch-71 batch-98
Running loss of epoch-71 batch-98 = 3.0151568353176117e-06

Training epoch-71 batch-99
Running loss of epoch-71 batch-99 = 7.015420123934746e-06

Training epoch-71 batch-100
Running loss of epoch-71 batch-100 = 2.20397487282753e-06

Training epoch-71 batch-101
Running loss of epoch-71 batch-101 = 4.575354978442192e-06

Training epoch-71 batch-102
Running loss of epoch-71 batch-102 = 8.44523310661316e-06

Training epoch-71 batch-103
Running loss of epoch-71 batch-103 = 3.043562173843384e-06

Training epoch-71 batch-104
Running loss of epoch-71 batch-104 = 8.97841528058052e-06

Training epoch-71 batch-105
Running loss of epoch-71 batch-105 = 2.8640497475862503e-06

Training epoch-71 batch-106
Running loss of epoch-71 batch-106 = 7.329043000936508e-06

Training epoch-71 batch-107
Running loss of epoch-71 batch-107 = 4.808185622096062e-06

Training epoch-71 batch-108
Running loss of epoch-71 batch-108 = 9.413575753569603e-06

Training epoch-71 batch-109
Running loss of epoch-71 batch-109 = 5.4265838116407394e-06

Training epoch-71 batch-110
Running loss of epoch-71 batch-110 = 8.02590511739254e-06

Training epoch-71 batch-111
Running loss of epoch-71 batch-111 = 4.5441556721925735e-06

Training epoch-71 batch-112
Running loss of epoch-71 batch-112 = 6.299233064055443e-06

Training epoch-71 batch-113
Running loss of epoch-71 batch-113 = 4.511093720793724e-06

Training epoch-71 batch-114
Running loss of epoch-71 batch-114 = 3.0193477869033813e-06

Training epoch-71 batch-115
Running loss of epoch-71 batch-115 = 1.376960426568985e-06

Training epoch-71 batch-116
Running loss of epoch-71 batch-116 = 4.398170858621597e-06

Training epoch-71 batch-117
Running loss of epoch-71 batch-117 = 7.953960448503494e-06

Training epoch-71 batch-118
Running loss of epoch-71 batch-118 = 3.496883437037468e-06

Training epoch-71 batch-119
Running loss of epoch-71 batch-119 = 2.8798822313547134e-06

Training epoch-71 batch-120
Running loss of epoch-71 batch-120 = 2.0258594304323196e-06

Training epoch-71 batch-121
Running loss of epoch-71 batch-121 = 4.215165972709656e-06

Training epoch-71 batch-122
Running loss of epoch-71 batch-122 = 1.0522548109292984e-05

Training epoch-71 batch-123
Running loss of epoch-71 batch-123 = 2.9203947633504868e-06

Training epoch-71 batch-124
Running loss of epoch-71 batch-124 = 6.77257776260376e-06

Training epoch-71 batch-125
Running loss of epoch-71 batch-125 = 1.2783799320459366e-05

Training epoch-71 batch-126
Running loss of epoch-71 batch-126 = 1.855846494436264e-05

Training epoch-71 batch-127
Running loss of epoch-71 batch-127 = 3.159511834383011e-06

Training epoch-71 batch-128
Running loss of epoch-71 batch-128 = 9.821262210607529e-06

Training epoch-71 batch-129
Running loss of epoch-71 batch-129 = 2.1513551473617554e-06

Training epoch-71 batch-130
Running loss of epoch-71 batch-130 = 2.232147380709648e-06

Training epoch-71 batch-131
Running loss of epoch-71 batch-131 = 3.6638230085372925e-06

Training epoch-71 batch-132
Running loss of epoch-71 batch-132 = 3.1478703022003174e-06

Training epoch-71 batch-133
Running loss of epoch-71 batch-133 = 4.565343260765076e-06

Training epoch-71 batch-134
Running loss of epoch-71 batch-134 = 9.811948984861374e-06

Training epoch-71 batch-135
Running loss of epoch-71 batch-135 = 3.952067345380783e-06

Training epoch-71 batch-136
Running loss of epoch-71 batch-136 = 1.1870870366692543e-05

Training epoch-71 batch-137
Running loss of epoch-71 batch-137 = 2.3976899683475494e-06

Training epoch-71 batch-138
Running loss of epoch-71 batch-138 = 4.958827048540115e-06

Training epoch-71 batch-139
Running loss of epoch-71 batch-139 = 3.1997915357351303e-06

Training epoch-71 batch-140
Running loss of epoch-71 batch-140 = 3.354158252477646e-06

Training epoch-71 batch-141
Running loss of epoch-71 batch-141 = 3.075692802667618e-06

Training epoch-71 batch-142
Running loss of epoch-71 batch-142 = 4.493631422519684e-07

Training epoch-71 batch-143
Running loss of epoch-71 batch-143 = 6.474554538726807e-06

Training epoch-71 batch-144
Running loss of epoch-71 batch-144 = 2.898741513490677e-06

Training epoch-71 batch-145
Running loss of epoch-71 batch-145 = 9.122188203036785e-06

Training epoch-71 batch-146
Running loss of epoch-71 batch-146 = 2.6440247893333435e-06

Training epoch-71 batch-147
Running loss of epoch-71 batch-147 = 4.394678398966789e-06

Training epoch-71 batch-148
Running loss of epoch-71 batch-148 = 3.5115517675876617e-06

Training epoch-71 batch-149
Running loss of epoch-71 batch-149 = 2.798624336719513e-06

Training epoch-71 batch-150
Running loss of epoch-71 batch-150 = 2.4054665118455887e-05

Training epoch-71 batch-151
Running loss of epoch-71 batch-151 = 4.027737304568291e-06

Training epoch-71 batch-152
Running loss of epoch-71 batch-152 = 1.3164477422833443e-05

Training epoch-71 batch-153
Running loss of epoch-71 batch-153 = 5.855923518538475e-06

Training epoch-71 batch-154
Running loss of epoch-71 batch-154 = 4.275236278772354e-06

Training epoch-71 batch-155
Running loss of epoch-71 batch-155 = 5.316920578479767e-06

Training epoch-71 batch-156
Running loss of epoch-71 batch-156 = 3.40375117957592e-06

Training epoch-71 batch-157
Running loss of epoch-71 batch-157 = 1.2900680303573608e-05

Finished training epoch-71.



Average train loss at epoch-71 = 5.716068297624588e-06

Started Evaluation

Average val loss at epoch-71 = 1.7648615022187417

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.14 %

Finished Evaluation



Started training epoch-72


Training epoch-72 batch-1
Running loss of epoch-72 batch-1 = 2.5532208383083344e-06

Training epoch-72 batch-2
Running loss of epoch-72 batch-2 = 3.4905970096588135e-06

Training epoch-72 batch-3
Running loss of epoch-72 batch-3 = 6.434042006731033e-06

Training epoch-72 batch-4
Running loss of epoch-72 batch-4 = 5.009351298213005e-06

Training epoch-72 batch-5
Running loss of epoch-72 batch-5 = 4.681525751948357e-06

Training epoch-72 batch-6
Running loss of epoch-72 batch-6 = 6.936956197023392e-06

Training epoch-72 batch-7
Running loss of epoch-72 batch-7 = 6.427522748708725e-06

Training epoch-72 batch-8
Running loss of epoch-72 batch-8 = 1.0661548003554344e-05

Training epoch-72 batch-9
Running loss of epoch-72 batch-9 = 7.141614332795143e-06

Training epoch-72 batch-10
Running loss of epoch-72 batch-10 = 2.1061860024929047e-06

Training epoch-72 batch-11
Running loss of epoch-72 batch-11 = 2.007698640227318e-06

Training epoch-72 batch-12
Running loss of epoch-72 batch-12 = 4.074769094586372e-06

Training epoch-72 batch-13
Running loss of epoch-72 batch-13 = 5.292939022183418e-06

Training epoch-72 batch-14
Running loss of epoch-72 batch-14 = 3.731111064553261e-06

Training epoch-72 batch-15
Running loss of epoch-72 batch-15 = 9.872950613498688e-06

Training epoch-72 batch-16
Running loss of epoch-72 batch-16 = 1.165410503745079e-05

Training epoch-72 batch-17
Running loss of epoch-72 batch-17 = 1.1331867426633835e-05

Training epoch-72 batch-18
Running loss of epoch-72 batch-18 = 4.175817593932152e-06

Training epoch-72 batch-19
Running loss of epoch-72 batch-19 = 6.682472303509712e-06

Training epoch-72 batch-20
Running loss of epoch-72 batch-20 = 3.3923424780368805e-06

Training epoch-72 batch-21
Running loss of epoch-72 batch-21 = 3.419816493988037e-06

Training epoch-72 batch-22
Running loss of epoch-72 batch-22 = 5.08246012032032e-06

Training epoch-72 batch-23
Running loss of epoch-72 batch-23 = 7.139984518289566e-06

Training epoch-72 batch-24
Running loss of epoch-72 batch-24 = 2.9616057872772217e-06

Training epoch-72 batch-25
Running loss of epoch-72 batch-25 = 2.6337802410125732e-06

Training epoch-72 batch-26
Running loss of epoch-72 batch-26 = 1.3227108865976334e-06

Training epoch-72 batch-27
Running loss of epoch-72 batch-27 = 8.394476026296616e-06

Training epoch-72 batch-28
Running loss of epoch-72 batch-28 = 5.447305738925934e-06

Training epoch-72 batch-29
Running loss of epoch-72 batch-29 = 5.373731255531311e-06

Training epoch-72 batch-30
Running loss of epoch-72 batch-30 = 4.121800884604454e-06

Training epoch-72 batch-31
Running loss of epoch-72 batch-31 = 5.474314093589783e-06

Training epoch-72 batch-32
Running loss of epoch-72 batch-32 = 2.971617504954338e-06

Training epoch-72 batch-33
Running loss of epoch-72 batch-33 = 3.848923370242119e-06

Training epoch-72 batch-34
Running loss of epoch-72 batch-34 = 5.728099495172501e-06

Training epoch-72 batch-35
Running loss of epoch-72 batch-35 = 5.7194847613573074e-06

Training epoch-72 batch-36
Running loss of epoch-72 batch-36 = 4.956033080816269e-06

Training epoch-72 batch-37
Running loss of epoch-72 batch-37 = 1.1317897588014603e-05

Training epoch-72 batch-38
Running loss of epoch-72 batch-38 = 7.421011105179787e-06

Training epoch-72 batch-39
Running loss of epoch-72 batch-39 = 6.694113835692406e-06

Training epoch-72 batch-40
Running loss of epoch-72 batch-40 = 3.3203978091478348e-06

Training epoch-72 batch-41
Running loss of epoch-72 batch-41 = 7.486902177333832e-06

Training epoch-72 batch-42
Running loss of epoch-72 batch-42 = 1.7478596419095993e-06

Training epoch-72 batch-43
Running loss of epoch-72 batch-43 = 6.24009408056736e-06

Training epoch-72 batch-44
Running loss of epoch-72 batch-44 = 3.5581178963184357e-06

Training epoch-72 batch-45
Running loss of epoch-72 batch-45 = 6.200745701789856e-06

Training epoch-72 batch-46
Running loss of epoch-72 batch-46 = 4.071509465575218e-06

Training epoch-72 batch-47
Running loss of epoch-72 batch-47 = 6.975606083869934e-06

Training epoch-72 batch-48
Running loss of epoch-72 batch-48 = 8.475035429000854e-06

Training epoch-72 batch-49
Running loss of epoch-72 batch-49 = 1.3776589184999466e-06

Training epoch-72 batch-50
Running loss of epoch-72 batch-50 = 4.096189513802528e-06

Training epoch-72 batch-51
Running loss of epoch-72 batch-51 = 8.66432674229145e-06

Training epoch-72 batch-52
Running loss of epoch-72 batch-52 = 5.2102841436862946e-06

Training epoch-72 batch-53
Running loss of epoch-72 batch-53 = 6.931833922863007e-06

Training epoch-72 batch-54
Running loss of epoch-72 batch-54 = 1.8307473510503769e-06

Training epoch-72 batch-55
Running loss of epoch-72 batch-55 = 9.051524102687836e-06

Training epoch-72 batch-56
Running loss of epoch-72 batch-56 = 9.259209036827087e-06

Training epoch-72 batch-57
Running loss of epoch-72 batch-57 = 2.9276125133037567e-06

Training epoch-72 batch-58
Running loss of epoch-72 batch-58 = 2.473359927535057e-06

Training epoch-72 batch-59
Running loss of epoch-72 batch-59 = 2.8498470783233643e-06

Training epoch-72 batch-60
Running loss of epoch-72 batch-60 = 4.126923158764839e-06

Training epoch-72 batch-61
Running loss of epoch-72 batch-61 = 1.7415732145309448e-06

Training epoch-72 batch-62
Running loss of epoch-72 batch-62 = 6.533460691571236e-06

Training epoch-72 batch-63
Running loss of epoch-72 batch-63 = 1.5941215679049492e-05

Training epoch-72 batch-64
Running loss of epoch-72 batch-64 = 4.037981852889061e-06

Training epoch-72 batch-65
Running loss of epoch-72 batch-65 = 7.099006325006485e-06

Training epoch-72 batch-66
Running loss of epoch-72 batch-66 = 9.227311238646507e-06

Training epoch-72 batch-67
Running loss of epoch-72 batch-67 = 8.668750524520874e-06

Training epoch-72 batch-68
Running loss of epoch-72 batch-68 = 5.624955520033836e-06

Training epoch-72 batch-69
Running loss of epoch-72 batch-69 = 4.791887477040291e-06

Training epoch-72 batch-70
Running loss of epoch-72 batch-70 = 3.709690645337105e-06

Training epoch-72 batch-71
Running loss of epoch-72 batch-71 = 3.6247074604034424e-06

Training epoch-72 batch-72
Running loss of epoch-72 batch-72 = 1.3706739991903305e-06

Training epoch-72 batch-73
Running loss of epoch-72 batch-73 = 3.60771082341671e-06

Training epoch-72 batch-74
Running loss of epoch-72 batch-74 = 9.399140253663063e-06

Training epoch-72 batch-75
Running loss of epoch-72 batch-75 = 4.811445251107216e-06

Training epoch-72 batch-76
Running loss of epoch-72 batch-76 = 4.827976226806641e-06

Training epoch-72 batch-77
Running loss of epoch-72 batch-77 = 5.1173847168684006e-06

Training epoch-72 batch-78
Running loss of epoch-72 batch-78 = 2.642860636115074e-06

Training epoch-72 batch-79
Running loss of epoch-72 batch-79 = 2.9900111258029938e-06

Training epoch-72 batch-80
Running loss of epoch-72 batch-80 = 2.414220944046974e-06

Training epoch-72 batch-81
Running loss of epoch-72 batch-81 = 3.983033820986748e-06

Training epoch-72 batch-82
Running loss of epoch-72 batch-82 = 8.422648534178734e-06

Training epoch-72 batch-83
Running loss of epoch-72 batch-83 = 3.0330847948789597e-06

Training epoch-72 batch-84
Running loss of epoch-72 batch-84 = 6.235670298337936e-06

Training epoch-72 batch-85
Running loss of epoch-72 batch-85 = 4.923669621348381e-06

Training epoch-72 batch-86
Running loss of epoch-72 batch-86 = 7.434980943799019e-06

Training epoch-72 batch-87
Running loss of epoch-72 batch-87 = 2.164160832762718e-06

Training epoch-72 batch-88
Running loss of epoch-72 batch-88 = 5.193520337343216e-06

Training epoch-72 batch-89
Running loss of epoch-72 batch-89 = 5.200039595365524e-06

Training epoch-72 batch-90
Running loss of epoch-72 batch-90 = 1.6133999451994896e-05

Training epoch-72 batch-91
Running loss of epoch-72 batch-91 = 5.745561793446541e-06

Training epoch-72 batch-92
Running loss of epoch-72 batch-92 = 3.448920324444771e-06

Training epoch-72 batch-93
Running loss of epoch-72 batch-93 = 3.054272383451462e-06

Training epoch-72 batch-94
Running loss of epoch-72 batch-94 = 5.068257451057434e-06

Training epoch-72 batch-95
Running loss of epoch-72 batch-95 = 6.923452019691467e-06

Training epoch-72 batch-96
Running loss of epoch-72 batch-96 = 3.905734047293663e-06

Training epoch-72 batch-97
Running loss of epoch-72 batch-97 = 4.101544618606567e-06

Training epoch-72 batch-98
Running loss of epoch-72 batch-98 = 4.493864253163338e-06

Training epoch-72 batch-99
Running loss of epoch-72 batch-99 = 7.830560207366943e-06

Training epoch-72 batch-100
Running loss of epoch-72 batch-100 = 2.814223989844322e-06

Training epoch-72 batch-101
Running loss of epoch-72 batch-101 = 6.00423663854599e-06

Training epoch-72 batch-102
Running loss of epoch-72 batch-102 = 5.655223503708839e-06

Training epoch-72 batch-103
Running loss of epoch-72 batch-103 = 8.898787200450897e-06

Training epoch-72 batch-104
Running loss of epoch-72 batch-104 = 7.842201739549637e-06

Training epoch-72 batch-105
Running loss of epoch-72 batch-105 = 2.7974601835012436e-06

Training epoch-72 batch-106
Running loss of epoch-72 batch-106 = 4.575587809085846e-06

Training epoch-72 batch-107
Running loss of epoch-72 batch-107 = 9.702052921056747e-07

Training epoch-72 batch-108
Running loss of epoch-72 batch-108 = 9.73651185631752e-06

Training epoch-72 batch-109
Running loss of epoch-72 batch-109 = 1.2124888598918915e-05

Training epoch-72 batch-110
Running loss of epoch-72 batch-110 = 5.914829671382904e-06

Training epoch-72 batch-111
Running loss of epoch-72 batch-111 = 5.489680916070938e-06

Training epoch-72 batch-112
Running loss of epoch-72 batch-112 = 1.2938398867845535e-06

Training epoch-72 batch-113
Running loss of epoch-72 batch-113 = 4.071509465575218e-06

Training epoch-72 batch-114
Running loss of epoch-72 batch-114 = 2.2652093321084976e-06

Training epoch-72 batch-115
Running loss of epoch-72 batch-115 = 4.133209586143494e-06

Training epoch-72 batch-116
Running loss of epoch-72 batch-116 = 1.0796822607517242e-05

Training epoch-72 batch-117
Running loss of epoch-72 batch-117 = 1.857522875070572e-06

Training epoch-72 batch-118
Running loss of epoch-72 batch-118 = 4.782341420650482e-06

Training epoch-72 batch-119
Running loss of epoch-72 batch-119 = 7.962808012962341e-06

Training epoch-72 batch-120
Running loss of epoch-72 batch-120 = 3.991415724158287e-06

Training epoch-72 batch-121
Running loss of epoch-72 batch-121 = 1.1538155376911163e-05

Training epoch-72 batch-122
Running loss of epoch-72 batch-122 = 3.2295938581228256e-06

Training epoch-72 batch-123
Running loss of epoch-72 batch-123 = 4.494795575737953e-06

Training epoch-72 batch-124
Running loss of epoch-72 batch-124 = 7.934635505080223e-06

Training epoch-72 batch-125
Running loss of epoch-72 batch-125 = 3.064749762415886e-06

Training epoch-72 batch-126
Running loss of epoch-72 batch-126 = 2.6710331439971924e-06

Training epoch-72 batch-127
Running loss of epoch-72 batch-127 = 5.780952051281929e-06

Training epoch-72 batch-128
Running loss of epoch-72 batch-128 = 3.5136472433805466e-06

Training epoch-72 batch-129
Running loss of epoch-72 batch-129 = 3.126915544271469e-06

Training epoch-72 batch-130
Running loss of epoch-72 batch-130 = 3.1564850360155106e-06

Training epoch-72 batch-131
Running loss of epoch-72 batch-131 = 2.7923379093408585e-06

Training epoch-72 batch-132
Running loss of epoch-72 batch-132 = 3.471970558166504e-06

Training epoch-72 batch-133
Running loss of epoch-72 batch-133 = 7.261289283633232e-06

Training epoch-72 batch-134
Running loss of epoch-72 batch-134 = 1.6422709450125694e-05

Training epoch-72 batch-135
Running loss of epoch-72 batch-135 = 6.166286766529083e-06

Training epoch-72 batch-136
Running loss of epoch-72 batch-136 = 6.806571036577225e-06

Training epoch-72 batch-137
Running loss of epoch-72 batch-137 = 2.366025000810623e-06

Training epoch-72 batch-138
Running loss of epoch-72 batch-138 = 2.887798473238945e-06

Training epoch-72 batch-139
Running loss of epoch-72 batch-139 = 3.412598744034767e-06

Training epoch-72 batch-140
Running loss of epoch-72 batch-140 = 1.3033859431743622e-05

Training epoch-72 batch-141
Running loss of epoch-72 batch-141 = 5.765119567513466e-06

Training epoch-72 batch-142
Running loss of epoch-72 batch-142 = 9.642913937568665e-06

Training epoch-72 batch-143
Running loss of epoch-72 batch-143 = 8.088536560535431e-06

Training epoch-72 batch-144
Running loss of epoch-72 batch-144 = 5.977693945169449e-06

Training epoch-72 batch-145
Running loss of epoch-72 batch-145 = 2.9134098440408707e-06

Training epoch-72 batch-146
Running loss of epoch-72 batch-146 = 7.757917046546936e-07

Training epoch-72 batch-147
Running loss of epoch-72 batch-147 = 1.6115373000502586e-05

Training epoch-72 batch-148
Running loss of epoch-72 batch-148 = 3.849854692816734e-06

Training epoch-72 batch-149
Running loss of epoch-72 batch-149 = 5.1711685955524445e-06

Training epoch-72 batch-150
Running loss of epoch-72 batch-150 = 5.985144525766373e-06

Training epoch-72 batch-151
Running loss of epoch-72 batch-151 = 1.3395678251981735e-05

Training epoch-72 batch-152
Running loss of epoch-72 batch-152 = 8.21542926132679e-06

Training epoch-72 batch-153
Running loss of epoch-72 batch-153 = 5.319947376847267e-06

Training epoch-72 batch-154
Running loss of epoch-72 batch-154 = 5.2587129175662994e-06

Training epoch-72 batch-155
Running loss of epoch-72 batch-155 = 1.3604294508695602e-06

Training epoch-72 batch-156
Running loss of epoch-72 batch-156 = 3.904104232788086e-06

Training epoch-72 batch-157
Running loss of epoch-72 batch-157 = 3.54573130607605e-05

Finished training epoch-72.



Average train loss at epoch-72 = 5.617645382881165e-06

Started Evaluation

Average val loss at epoch-72 = 1.7751741997013226

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.23 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 50.90 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.18 %

Finished Evaluation



Started training epoch-73


Training epoch-73 batch-1
Running loss of epoch-73 batch-1 = 1.743203029036522e-06

Training epoch-73 batch-2
Running loss of epoch-73 batch-2 = 3.457535058259964e-06

Training epoch-73 batch-3
Running loss of epoch-73 batch-3 = 1.3821292668581009e-05

Training epoch-73 batch-4
Running loss of epoch-73 batch-4 = 1.7439015209674835e-06

Training epoch-73 batch-5
Running loss of epoch-73 batch-5 = 3.9907172322273254e-06

Training epoch-73 batch-6
Running loss of epoch-73 batch-6 = 3.966270014643669e-06

Training epoch-73 batch-7
Running loss of epoch-73 batch-7 = 4.755565896630287e-06

Training epoch-73 batch-8
Running loss of epoch-73 batch-8 = 4.8959627747535706e-06

Training epoch-73 batch-9
Running loss of epoch-73 batch-9 = 5.539506673812866e-06

Training epoch-73 batch-10
Running loss of epoch-73 batch-10 = 3.4454278647899628e-06

Training epoch-73 batch-11
Running loss of epoch-73 batch-11 = 3.377208486199379e-06

Training epoch-73 batch-12
Running loss of epoch-73 batch-12 = 2.9283110052347183e-06

Training epoch-73 batch-13
Running loss of epoch-73 batch-13 = 1.994892954826355e-06

Training epoch-73 batch-14
Running loss of epoch-73 batch-14 = 6.3015613704919815e-06

Training epoch-73 batch-15
Running loss of epoch-73 batch-15 = 5.36162406206131e-06

Training epoch-73 batch-16
Running loss of epoch-73 batch-16 = 6.627058610320091e-06

Training epoch-73 batch-17
Running loss of epoch-73 batch-17 = 7.7858567237854e-07

Training epoch-73 batch-18
Running loss of epoch-73 batch-18 = 3.10712493956089e-06

Training epoch-73 batch-19
Running loss of epoch-73 batch-19 = 3.4712720662355423e-06

Training epoch-73 batch-20
Running loss of epoch-73 batch-20 = 3.1334348022937775e-06

Training epoch-73 batch-21
Running loss of epoch-73 batch-21 = 2.9709190130233765e-06

Training epoch-73 batch-22
Running loss of epoch-73 batch-22 = 2.982094883918762e-06

Training epoch-73 batch-23
Running loss of epoch-73 batch-23 = 9.479233995079994e-06

Training epoch-73 batch-24
Running loss of epoch-73 batch-24 = 7.179100066423416e-06

Training epoch-73 batch-25
Running loss of epoch-73 batch-25 = 3.7606805562973022e-06

Training epoch-73 batch-26
Running loss of epoch-73 batch-26 = 4.2549800127744675e-06

Training epoch-73 batch-27
Running loss of epoch-73 batch-27 = 5.200272426009178e-06

Training epoch-73 batch-28
Running loss of epoch-73 batch-28 = 3.1029339879751205e-06

Training epoch-73 batch-29
Running loss of epoch-73 batch-29 = 1.12846028059721e-05

Training epoch-73 batch-30
Running loss of epoch-73 batch-30 = 9.431969374418259e-06

Training epoch-73 batch-31
Running loss of epoch-73 batch-31 = 4.059867933392525e-06

Training epoch-73 batch-32
Running loss of epoch-73 batch-32 = 1.09146349132061e-05

Training epoch-73 batch-33
Running loss of epoch-73 batch-33 = 4.386063665151596e-06

Training epoch-73 batch-34
Running loss of epoch-73 batch-34 = 1.187669113278389e-06

Training epoch-73 batch-35
Running loss of epoch-73 batch-35 = 5.494104698300362e-06

Training epoch-73 batch-36
Running loss of epoch-73 batch-36 = 6.202142685651779e-06

Training epoch-73 batch-37
Running loss of epoch-73 batch-37 = 1.238752156496048e-05

Training epoch-73 batch-38
Running loss of epoch-73 batch-38 = 2.8563663363456726e-06

Training epoch-73 batch-39
Running loss of epoch-73 batch-39 = 4.975590854883194e-06

Training epoch-73 batch-40
Running loss of epoch-73 batch-40 = 7.416354492306709e-06

Training epoch-73 batch-41
Running loss of epoch-73 batch-41 = 4.146946594119072e-06

Training epoch-73 batch-42
Running loss of epoch-73 batch-42 = 3.426801413297653e-06

Training epoch-73 batch-43
Running loss of epoch-73 batch-43 = 2.9334332793951035e-06

Training epoch-73 batch-44
Running loss of epoch-73 batch-44 = 5.972804501652718e-06

Training epoch-73 batch-45
Running loss of epoch-73 batch-45 = 3.97232361137867e-06

Training epoch-73 batch-46
Running loss of epoch-73 batch-46 = 2.4510081857442856e-06

Training epoch-73 batch-47
Running loss of epoch-73 batch-47 = 9.550713002681732e-07

Training epoch-73 batch-48
Running loss of epoch-73 batch-48 = 3.3280812203884125e-06

Training epoch-73 batch-49
Running loss of epoch-73 batch-49 = 1.065107062458992e-05

Training epoch-73 batch-50
Running loss of epoch-73 batch-50 = 6.400514394044876e-06

Training epoch-73 batch-51
Running loss of epoch-73 batch-51 = 4.5655760914087296e-06

Training epoch-73 batch-52
Running loss of epoch-73 batch-52 = 3.6177225410938263e-06

Training epoch-73 batch-53
Running loss of epoch-73 batch-53 = 6.8855006247758865e-06

Training epoch-73 batch-54
Running loss of epoch-73 batch-54 = 4.545319825410843e-06

Training epoch-73 batch-55
Running loss of epoch-73 batch-55 = 3.913650289177895e-06

Training epoch-73 batch-56
Running loss of epoch-73 batch-56 = 1.6584061086177826e-05

Training epoch-73 batch-57
Running loss of epoch-73 batch-57 = 3.202352672815323e-06

Training epoch-73 batch-58
Running loss of epoch-73 batch-58 = 3.943918272852898e-06

Training epoch-73 batch-59
Running loss of epoch-73 batch-59 = 4.561152309179306e-06

Training epoch-73 batch-60
Running loss of epoch-73 batch-60 = 5.021458491683006e-06

Training epoch-73 batch-61
Running loss of epoch-73 batch-61 = 2.4209730327129364e-06

Training epoch-73 batch-62
Running loss of epoch-73 batch-62 = 3.889203071594238e-06

Training epoch-73 batch-63
Running loss of epoch-73 batch-63 = 2.3436732590198517e-06

Training epoch-73 batch-64
Running loss of epoch-73 batch-64 = 1.9001075997948647e-05

Training epoch-73 batch-65
Running loss of epoch-73 batch-65 = 4.902016371488571e-06

Training epoch-73 batch-66
Running loss of epoch-73 batch-66 = 3.8051512092351913e-06

Training epoch-73 batch-67
Running loss of epoch-73 batch-67 = 6.896909326314926e-06

Training epoch-73 batch-68
Running loss of epoch-73 batch-68 = 8.766306564211845e-06

Training epoch-73 batch-69
Running loss of epoch-73 batch-69 = 3.3634714782238007e-06

Training epoch-73 batch-70
Running loss of epoch-73 batch-70 = 3.125518560409546e-06

Training epoch-73 batch-71
Running loss of epoch-73 batch-71 = 3.734370693564415e-06

Training epoch-73 batch-72
Running loss of epoch-73 batch-72 = 7.63626303523779e-06

Training epoch-73 batch-73
Running loss of epoch-73 batch-73 = 1.6803387552499771e-06

Training epoch-73 batch-74
Running loss of epoch-73 batch-74 = 7.322290912270546e-06

Training epoch-73 batch-75
Running loss of epoch-73 batch-75 = 5.036825314164162e-06

Training epoch-73 batch-76
Running loss of epoch-73 batch-76 = 6.50365836918354e-06

Training epoch-73 batch-77
Running loss of epoch-73 batch-77 = 8.780742064118385e-06

Training epoch-73 batch-78
Running loss of epoch-73 batch-78 = 4.630535840988159e-06

Training epoch-73 batch-79
Running loss of epoch-73 batch-79 = 3.7760473787784576e-06

Training epoch-73 batch-80
Running loss of epoch-73 batch-80 = 3.0212104320526123e-06

Training epoch-73 batch-81
Running loss of epoch-73 batch-81 = 1.1194497346878052e-06

Training epoch-73 batch-82
Running loss of epoch-73 batch-82 = 7.733935490250587e-06

Training epoch-73 batch-83
Running loss of epoch-73 batch-83 = 2.6957131922245026e-06

Training epoch-73 batch-84
Running loss of epoch-73 batch-84 = 4.605855792760849e-06

Training epoch-73 batch-85
Running loss of epoch-73 batch-85 = 1.1136289685964584e-05

Training epoch-73 batch-86
Running loss of epoch-73 batch-86 = 5.916459485888481e-06

Training epoch-73 batch-87
Running loss of epoch-73 batch-87 = 6.687594577670097e-06

Training epoch-73 batch-88
Running loss of epoch-73 batch-88 = 1.2065982446074486e-05

Training epoch-73 batch-89
Running loss of epoch-73 batch-89 = 6.173504516482353e-06

Training epoch-73 batch-90
Running loss of epoch-73 batch-90 = 6.895512342453003e-06

Training epoch-73 batch-91
Running loss of epoch-73 batch-91 = 7.73346982896328e-06

Training epoch-73 batch-92
Running loss of epoch-73 batch-92 = 2.1366868168115616e-06

Training epoch-73 batch-93
Running loss of epoch-73 batch-93 = 2.866145223379135e-06

Training epoch-73 batch-94
Running loss of epoch-73 batch-94 = 3.297114744782448e-06

Training epoch-73 batch-95
Running loss of epoch-73 batch-95 = 4.853354766964912e-06

Training epoch-73 batch-96
Running loss of epoch-73 batch-96 = 6.316229701042175e-06

Training epoch-73 batch-97
Running loss of epoch-73 batch-97 = 6.656628102064133e-06

Training epoch-73 batch-98
Running loss of epoch-73 batch-98 = 6.841728463768959e-06

Training epoch-73 batch-99
Running loss of epoch-73 batch-99 = 2.4870969355106354e-06

Training epoch-73 batch-100
Running loss of epoch-73 batch-100 = 3.3637043088674545e-06

Training epoch-73 batch-101
Running loss of epoch-73 batch-101 = 3.0314549803733826e-06

Training epoch-73 batch-102
Running loss of epoch-73 batch-102 = 4.959991201758385e-06

Training epoch-73 batch-103
Running loss of epoch-73 batch-103 = 8.930917829275131e-06

Training epoch-73 batch-104
Running loss of epoch-73 batch-104 = 2.7015339583158493e-06

Training epoch-73 batch-105
Running loss of epoch-73 batch-105 = 1.5538185834884644e-05

Training epoch-73 batch-106
Running loss of epoch-73 batch-106 = 2.6507768779993057e-06

Training epoch-73 batch-107
Running loss of epoch-73 batch-107 = 2.52528116106987e-06

Training epoch-73 batch-108
Running loss of epoch-73 batch-108 = 3.0228402465581894e-06

Training epoch-73 batch-109
Running loss of epoch-73 batch-109 = 5.521578714251518e-06

Training epoch-73 batch-110
Running loss of epoch-73 batch-110 = 1.9029248505830765e-06

Training epoch-73 batch-111
Running loss of epoch-73 batch-111 = 5.848938599228859e-06

Training epoch-73 batch-112
Running loss of epoch-73 batch-112 = 6.344867870211601e-06

Training epoch-73 batch-113
Running loss of epoch-73 batch-113 = 2.339715138077736e-06

Training epoch-73 batch-114
Running loss of epoch-73 batch-114 = 6.986083462834358e-06

Training epoch-73 batch-115
Running loss of epoch-73 batch-115 = 7.4007548391819e-06

Training epoch-73 batch-116
Running loss of epoch-73 batch-116 = 4.795845597982407e-06

Training epoch-73 batch-117
Running loss of epoch-73 batch-117 = 3.282446414232254e-06

Training epoch-73 batch-118
Running loss of epoch-73 batch-118 = 5.429377779364586e-06

Training epoch-73 batch-119
Running loss of epoch-73 batch-119 = 1.0535353794693947e-05

Training epoch-73 batch-120
Running loss of epoch-73 batch-120 = 3.868946805596352e-06

Training epoch-73 batch-121
Running loss of epoch-73 batch-121 = 1.364084891974926e-05

Training epoch-73 batch-122
Running loss of epoch-73 batch-122 = 1.02317426353693e-05

Training epoch-73 batch-123
Running loss of epoch-73 batch-123 = 4.43682074546814e-06

Training epoch-73 batch-124
Running loss of epoch-73 batch-124 = 5.480367690324783e-06

Training epoch-73 batch-125
Running loss of epoch-73 batch-125 = 2.8812792152166367e-06

Training epoch-73 batch-126
Running loss of epoch-73 batch-126 = 1.8517021089792252e-06

Training epoch-73 batch-127
Running loss of epoch-73 batch-127 = 5.048466846346855e-06

Training epoch-73 batch-128
Running loss of epoch-73 batch-128 = 3.630993887782097e-06

Training epoch-73 batch-129
Running loss of epoch-73 batch-129 = 9.410548955202103e-06

Training epoch-73 batch-130
Running loss of epoch-73 batch-130 = 4.227040335536003e-06

Training epoch-73 batch-131
Running loss of epoch-73 batch-131 = 3.232387825846672e-06

Training epoch-73 batch-132
Running loss of epoch-73 batch-132 = 3.648921847343445e-06

Training epoch-73 batch-133
Running loss of epoch-73 batch-133 = 3.0132941901683807e-06

Training epoch-73 batch-134
Running loss of epoch-73 batch-134 = 2.382555976510048e-06

Training epoch-73 batch-135
Running loss of epoch-73 batch-135 = 7.1160029619932175e-06

Training epoch-73 batch-136
Running loss of epoch-73 batch-136 = 7.704831659793854e-06

Training epoch-73 batch-137
Running loss of epoch-73 batch-137 = 4.019588232040405e-06

Training epoch-73 batch-138
Running loss of epoch-73 batch-138 = 5.568377673625946e-06

Training epoch-73 batch-139
Running loss of epoch-73 batch-139 = 8.568866178393364e-06

Training epoch-73 batch-140
Running loss of epoch-73 batch-140 = 5.413079634308815e-06

Training epoch-73 batch-141
Running loss of epoch-73 batch-141 = 6.587943062186241e-06

Training epoch-73 batch-142
Running loss of epoch-73 batch-142 = 4.2729079723358154e-06

Training epoch-73 batch-143
Running loss of epoch-73 batch-143 = 6.085028871893883e-06

Training epoch-73 batch-144
Running loss of epoch-73 batch-144 = 7.05476850271225e-06

Training epoch-73 batch-145
Running loss of epoch-73 batch-145 = 3.486406058073044e-06

Training epoch-73 batch-146
Running loss of epoch-73 batch-146 = 1.9529368728399277e-05

Training epoch-73 batch-147
Running loss of epoch-73 batch-147 = 3.639841452240944e-06

Training epoch-73 batch-148
Running loss of epoch-73 batch-148 = 7.372582331299782e-06

Training epoch-73 batch-149
Running loss of epoch-73 batch-149 = 1.4225952327251434e-06

Training epoch-73 batch-150
Running loss of epoch-73 batch-150 = 5.7248398661613464e-06

Training epoch-73 batch-151
Running loss of epoch-73 batch-151 = 1.4709075912833214e-05

Training epoch-73 batch-152
Running loss of epoch-73 batch-152 = 3.7620775401592255e-06

Training epoch-73 batch-153
Running loss of epoch-73 batch-153 = 1.807231456041336e-06

Training epoch-73 batch-154
Running loss of epoch-73 batch-154 = 3.088032826781273e-06

Training epoch-73 batch-155
Running loss of epoch-73 batch-155 = 1.0303454473614693e-05

Training epoch-73 batch-156
Running loss of epoch-73 batch-156 = 5.007022991776466e-06

Training epoch-73 batch-157
Running loss of epoch-73 batch-157 = 9.387731552124023e-06

Finished training epoch-73.



Average train loss at epoch-73 = 5.4565824568271635e-06

Started Evaluation

Average val loss at epoch-73 = 1.770821367365679

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 62.56 %

Overall Accuracy = 81.04 %

Finished Evaluation



Started training epoch-74


Training epoch-74 batch-1
Running loss of epoch-74 batch-1 = 3.5811681300401688e-06

Training epoch-74 batch-2
Running loss of epoch-74 batch-2 = 4.0263403207063675e-06

Training epoch-74 batch-3
Running loss of epoch-74 batch-3 = 5.834735929965973e-06

Training epoch-74 batch-4
Running loss of epoch-74 batch-4 = 1.6775447875261307e-06

Training epoch-74 batch-5
Running loss of epoch-74 batch-5 = 5.727401003241539e-06

Training epoch-74 batch-6
Running loss of epoch-74 batch-6 = 2.6314519345760345e-06

Training epoch-74 batch-7
Running loss of epoch-74 batch-7 = 7.4533745646476746e-06

Training epoch-74 batch-8
Running loss of epoch-74 batch-8 = 4.157889634370804e-06

Training epoch-74 batch-9
Running loss of epoch-74 batch-9 = 7.110182195901871e-06

Training epoch-74 batch-10
Running loss of epoch-74 batch-10 = 4.661967977881432e-06

Training epoch-74 batch-11
Running loss of epoch-74 batch-11 = 7.245922461152077e-06

Training epoch-74 batch-12
Running loss of epoch-74 batch-12 = 4.316680133342743e-06

Training epoch-74 batch-13
Running loss of epoch-74 batch-13 = 4.07942570745945e-06

Training epoch-74 batch-14
Running loss of epoch-74 batch-14 = 6.3409097492694855e-06

Training epoch-74 batch-15
Running loss of epoch-74 batch-15 = 3.573251888155937e-06

Training epoch-74 batch-16
Running loss of epoch-74 batch-16 = 1.3605691492557526e-05

Training epoch-74 batch-17
Running loss of epoch-74 batch-17 = 2.6938505470752716e-06

Training epoch-74 batch-18
Running loss of epoch-74 batch-18 = 5.359994247555733e-06

Training epoch-74 batch-19
Running loss of epoch-74 batch-19 = 1.4521181583404541e-05

Training epoch-74 batch-20
Running loss of epoch-74 batch-20 = 4.073139280080795e-06

Training epoch-74 batch-21
Running loss of epoch-74 batch-21 = 3.1420495361089706e-06

Training epoch-74 batch-22
Running loss of epoch-74 batch-22 = 1.9783619791269302e-06

Training epoch-74 batch-23
Running loss of epoch-74 batch-23 = 4.6836212277412415e-06

Training epoch-74 batch-24
Running loss of epoch-74 batch-24 = 4.022615030407906e-06

Training epoch-74 batch-25
Running loss of epoch-74 batch-25 = 3.4924596548080444e-06

Training epoch-74 batch-26
Running loss of epoch-74 batch-26 = 2.027256414294243e-06

Training epoch-74 batch-27
Running loss of epoch-74 batch-27 = 2.566259354352951e-06

Training epoch-74 batch-28
Running loss of epoch-74 batch-28 = 4.249159246683121e-06

Training epoch-74 batch-29
Running loss of epoch-74 batch-29 = 1.628650352358818e-06

Training epoch-74 batch-30
Running loss of epoch-74 batch-30 = 1.5702098608016968e-06

Training epoch-74 batch-31
Running loss of epoch-74 batch-31 = 6.169779226183891e-06

Training epoch-74 batch-32
Running loss of epoch-74 batch-32 = 6.723217666149139e-06

Training epoch-74 batch-33
Running loss of epoch-74 batch-33 = 8.141156286001205e-06

Training epoch-74 batch-34
Running loss of epoch-74 batch-34 = 1.4621764421463013e-06

Training epoch-74 batch-35
Running loss of epoch-74 batch-35 = 6.665242835879326e-06

Training epoch-74 batch-36
Running loss of epoch-74 batch-36 = 3.837514668703079e-06

Training epoch-74 batch-37
Running loss of epoch-74 batch-37 = 5.689682438969612e-06

Training epoch-74 batch-38
Running loss of epoch-74 batch-38 = 3.543216735124588e-06

Training epoch-74 batch-39
Running loss of epoch-74 batch-39 = 5.567912012338638e-06

Training epoch-74 batch-40
Running loss of epoch-74 batch-40 = 8.694827556610107e-06

Training epoch-74 batch-41
Running loss of epoch-74 batch-41 = 7.306225597858429e-07

Training epoch-74 batch-42
Running loss of epoch-74 batch-42 = 1.6165431588888168e-06

Training epoch-74 batch-43
Running loss of epoch-74 batch-43 = 5.474546924233437e-06

Training epoch-74 batch-44
Running loss of epoch-74 batch-44 = 2.70036980509758e-06

Training epoch-74 batch-45
Running loss of epoch-74 batch-45 = 4.206318408250809e-06

Training epoch-74 batch-46
Running loss of epoch-74 batch-46 = 8.766306564211845e-06

Training epoch-74 batch-47
Running loss of epoch-74 batch-47 = 5.109468474984169e-06

Training epoch-74 batch-48
Running loss of epoch-74 batch-48 = 4.788627848029137e-06

Training epoch-74 batch-49
Running loss of epoch-74 batch-49 = 1.969514414668083e-06

Training epoch-74 batch-50
Running loss of epoch-74 batch-50 = 2.2188760340213776e-06

Training epoch-74 batch-51
Running loss of epoch-74 batch-51 = 4.845904186367989e-06

Training epoch-74 batch-52
Running loss of epoch-74 batch-52 = 4.895264282822609e-06

Training epoch-74 batch-53
Running loss of epoch-74 batch-53 = 4.412606358528137e-06

Training epoch-74 batch-54
Running loss of epoch-74 batch-54 = 3.7641730159521103e-06

Training epoch-74 batch-55
Running loss of epoch-74 batch-55 = 7.601222023367882e-06

Training epoch-74 batch-56
Running loss of epoch-74 batch-56 = 7.526017725467682e-06

Training epoch-74 batch-57
Running loss of epoch-74 batch-57 = 3.938330337405205e-06

Training epoch-74 batch-58
Running loss of epoch-74 batch-58 = 8.669449016451836e-06

Training epoch-74 batch-59
Running loss of epoch-74 batch-59 = 8.188653737306595e-06

Training epoch-74 batch-60
Running loss of epoch-74 batch-60 = 2.942979335784912e-06

Training epoch-74 batch-61
Running loss of epoch-74 batch-61 = 2.7443747967481613e-06

Training epoch-74 batch-62
Running loss of epoch-74 batch-62 = 6.370246410369873e-06

Training epoch-74 batch-63
Running loss of epoch-74 batch-63 = 2.898741513490677e-06

Training epoch-74 batch-64
Running loss of epoch-74 batch-64 = 6.391433998942375e-06

Training epoch-74 batch-65
Running loss of epoch-74 batch-65 = 4.6710483729839325e-06

Training epoch-74 batch-66
Running loss of epoch-74 batch-66 = 6.39050267636776e-06

Training epoch-74 batch-67
Running loss of epoch-74 batch-67 = 5.252193659543991e-06

Training epoch-74 batch-68
Running loss of epoch-74 batch-68 = 5.356501787900925e-06

Training epoch-74 batch-69
Running loss of epoch-74 batch-69 = 2.1583400666713715e-06

Training epoch-74 batch-70
Running loss of epoch-74 batch-70 = 2.8652139008045197e-06

Training epoch-74 batch-71
Running loss of epoch-74 batch-71 = 1.4297198504209518e-05

Training epoch-74 batch-72
Running loss of epoch-74 batch-72 = 5.081063136458397e-06

Training epoch-74 batch-73
Running loss of epoch-74 batch-73 = 7.237773388624191e-06

Training epoch-74 batch-74
Running loss of epoch-74 batch-74 = 5.777692422270775e-06

Training epoch-74 batch-75
Running loss of epoch-74 batch-75 = 5.237758159637451e-06

Training epoch-74 batch-76
Running loss of epoch-74 batch-76 = 2.0200852304697037e-05

Training epoch-74 batch-77
Running loss of epoch-74 batch-77 = 4.935078322887421e-06

Training epoch-74 batch-78
Running loss of epoch-74 batch-78 = 2.88989394903183e-06

Training epoch-74 batch-79
Running loss of epoch-74 batch-79 = 3.7481077015399933e-06

Training epoch-74 batch-80
Running loss of epoch-74 batch-80 = 5.932990461587906e-06

Training epoch-74 batch-81
Running loss of epoch-74 batch-81 = 9.669922292232513e-06

Training epoch-74 batch-82
Running loss of epoch-74 batch-82 = 3.7406571209430695e-06

Training epoch-74 batch-83
Running loss of epoch-74 batch-83 = 1.7262063920497894e-06

Training epoch-74 batch-84
Running loss of epoch-74 batch-84 = 3.962777554988861e-06

Training epoch-74 batch-85
Running loss of epoch-74 batch-85 = 7.838709279894829e-06

Training epoch-74 batch-86
Running loss of epoch-74 batch-86 = 4.666391760110855e-06

Training epoch-74 batch-87
Running loss of epoch-74 batch-87 = 7.337657734751701e-06

Training epoch-74 batch-88
Running loss of epoch-74 batch-88 = 1.0260846465826035e-05

Training epoch-74 batch-89
Running loss of epoch-74 batch-89 = 8.194008842110634e-06

Training epoch-74 batch-90
Running loss of epoch-74 batch-90 = 6.252666935324669e-06

Training epoch-74 batch-91
Running loss of epoch-74 batch-91 = 1.5725381672382355e-06

Training epoch-74 batch-92
Running loss of epoch-74 batch-92 = 1.8782448023557663e-06

Training epoch-74 batch-93
Running loss of epoch-74 batch-93 = 3.373948857188225e-06

Training epoch-74 batch-94
Running loss of epoch-74 batch-94 = 9.445473551750183e-06

Training epoch-74 batch-95
Running loss of epoch-74 batch-95 = 5.638226866722107e-06

Training epoch-74 batch-96
Running loss of epoch-74 batch-96 = 1.6116537153720856e-06

Training epoch-74 batch-97
Running loss of epoch-74 batch-97 = 1.194886863231659e-06

Training epoch-74 batch-98
Running loss of epoch-74 batch-98 = 6.871065124869347e-06

Training epoch-74 batch-99
Running loss of epoch-74 batch-99 = 2.346700057387352e-06

Training epoch-74 batch-100
Running loss of epoch-74 batch-100 = 4.228437319397926e-06

Training epoch-74 batch-101
Running loss of epoch-74 batch-101 = 3.9639417082071304e-06

Training epoch-74 batch-102
Running loss of epoch-74 batch-102 = 1.1994736269116402e-05

Training epoch-74 batch-103
Running loss of epoch-74 batch-103 = 3.212597221136093e-06

Training epoch-74 batch-104
Running loss of epoch-74 batch-104 = 7.15767964720726e-06

Training epoch-74 batch-105
Running loss of epoch-74 batch-105 = 4.656147211790085e-06

Training epoch-74 batch-106
Running loss of epoch-74 batch-106 = 9.87551175057888e-06

Training epoch-74 batch-107
Running loss of epoch-74 batch-107 = 4.123896360397339e-06

Training epoch-74 batch-108
Running loss of epoch-74 batch-108 = 4.415865987539291e-06

Training epoch-74 batch-109
Running loss of epoch-74 batch-109 = 6.460119038820267e-06

Training epoch-74 batch-110
Running loss of epoch-74 batch-110 = 7.98562541604042e-06

Training epoch-74 batch-111
Running loss of epoch-74 batch-111 = 3.7222635000944138e-06

Training epoch-74 batch-112
Running loss of epoch-74 batch-112 = 6.9926027208566666e-06

Training epoch-74 batch-113
Running loss of epoch-74 batch-113 = 4.623085260391235e-06

Training epoch-74 batch-114
Running loss of epoch-74 batch-114 = 4.246365278959274e-06

Training epoch-74 batch-115
Running loss of epoch-74 batch-115 = 2.307118847966194e-06

Training epoch-74 batch-116
Running loss of epoch-74 batch-116 = 8.548609912395477e-06

Training epoch-74 batch-117
Running loss of epoch-74 batch-117 = 3.370223566889763e-06

Training epoch-74 batch-118
Running loss of epoch-74 batch-118 = 2.278038300573826e-05

Training epoch-74 batch-119
Running loss of epoch-74 batch-119 = 3.0891969799995422e-06

Training epoch-74 batch-120
Running loss of epoch-74 batch-120 = 6.291549652814865e-06

Training epoch-74 batch-121
Running loss of epoch-74 batch-121 = 4.183733835816383e-06

Training epoch-74 batch-122
Running loss of epoch-74 batch-122 = 3.305729478597641e-06

Training epoch-74 batch-123
Running loss of epoch-74 batch-123 = 2.579065039753914e-06

Training epoch-74 batch-124
Running loss of epoch-74 batch-124 = 2.730404958128929e-06

Training epoch-74 batch-125
Running loss of epoch-74 batch-125 = 2.891290932893753e-06

Training epoch-74 batch-126
Running loss of epoch-74 batch-126 = 2.76835635304451e-06

Training epoch-74 batch-127
Running loss of epoch-74 batch-127 = 3.6100391298532486e-06

Training epoch-74 batch-128
Running loss of epoch-74 batch-128 = 3.5401899367570877e-06

Training epoch-74 batch-129
Running loss of epoch-74 batch-129 = 4.251021891832352e-06

Training epoch-74 batch-130
Running loss of epoch-74 batch-130 = 2.3506581783294678e-06

Training epoch-74 batch-131
Running loss of epoch-74 batch-131 = 4.359288141131401e-06

Training epoch-74 batch-132
Running loss of epoch-74 batch-132 = 4.375120624899864e-06

Training epoch-74 batch-133
Running loss of epoch-74 batch-133 = 2.1944288164377213e-06

Training epoch-74 batch-134
Running loss of epoch-74 batch-134 = 4.051020368933678e-06

Training epoch-74 batch-135
Running loss of epoch-74 batch-135 = 2.948334440588951e-06

Training epoch-74 batch-136
Running loss of epoch-74 batch-136 = 6.16791658103466e-06

Training epoch-74 batch-137
Running loss of epoch-74 batch-137 = 6.242888048291206e-06

Training epoch-74 batch-138
Running loss of epoch-74 batch-138 = 1.2394506484270096e-05

Training epoch-74 batch-139
Running loss of epoch-74 batch-139 = 7.958151400089264e-06

Training epoch-74 batch-140
Running loss of epoch-74 batch-140 = 5.645444616675377e-06

Training epoch-74 batch-141
Running loss of epoch-74 batch-141 = 5.055917426943779e-06

Training epoch-74 batch-142
Running loss of epoch-74 batch-142 = 6.058951839804649e-06

Training epoch-74 batch-143
Running loss of epoch-74 batch-143 = 6.295973435044289e-06

Training epoch-74 batch-144
Running loss of epoch-74 batch-144 = 6.383983418345451e-06

Training epoch-74 batch-145
Running loss of epoch-74 batch-145 = 1.2774253264069557e-05

Training epoch-74 batch-146
Running loss of epoch-74 batch-146 = 2.1313317120075226e-06

Training epoch-74 batch-147
Running loss of epoch-74 batch-147 = 4.454748705029488e-06

Training epoch-74 batch-148
Running loss of epoch-74 batch-148 = 1.3301614671945572e-05

Training epoch-74 batch-149
Running loss of epoch-74 batch-149 = 6.735790520906448e-06

Training epoch-74 batch-150
Running loss of epoch-74 batch-150 = 7.225200533866882e-06

Training epoch-74 batch-151
Running loss of epoch-74 batch-151 = 1.469627022743225e-06

Training epoch-74 batch-152
Running loss of epoch-74 batch-152 = 8.559087291359901e-06

Training epoch-74 batch-153
Running loss of epoch-74 batch-153 = 1.151813194155693e-06

Training epoch-74 batch-154
Running loss of epoch-74 batch-154 = 1.4149583876132965e-05

Training epoch-74 batch-155
Running loss of epoch-74 batch-155 = 4.025641828775406e-06

Training epoch-74 batch-156
Running loss of epoch-74 batch-156 = 4.024012014269829e-06

Training epoch-74 batch-157
Running loss of epoch-74 batch-157 = 3.5315752029418945e-06

Finished training epoch-74.



Average train loss at epoch-74 = 5.364395678043365e-06

Started Evaluation

Average val loss at epoch-74 = 1.7714888108891178

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.16 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.14 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.23 %

Finished Evaluation



Started training epoch-75


Training epoch-75 batch-1
Running loss of epoch-75 batch-1 = 5.124835297465324e-06

Training epoch-75 batch-2
Running loss of epoch-75 batch-2 = 4.0906015783548355e-06

Training epoch-75 batch-3
Running loss of epoch-75 batch-3 = 2.3527536541223526e-06

Training epoch-75 batch-4
Running loss of epoch-75 batch-4 = 3.6598648875951767e-06

Training epoch-75 batch-5
Running loss of epoch-75 batch-5 = 3.6014243960380554e-06

Training epoch-75 batch-6
Running loss of epoch-75 batch-6 = 7.239403203129768e-06

Training epoch-75 batch-7
Running loss of epoch-75 batch-7 = 7.002381607890129e-06

Training epoch-75 batch-8
Running loss of epoch-75 batch-8 = 4.340428858995438e-06

Training epoch-75 batch-9
Running loss of epoch-75 batch-9 = 5.002599209547043e-06

Training epoch-75 batch-10
Running loss of epoch-75 batch-10 = 4.210509359836578e-06

Training epoch-75 batch-11
Running loss of epoch-75 batch-11 = 2.682441845536232e-06

Training epoch-75 batch-12
Running loss of epoch-75 batch-12 = 5.716457962989807e-06

Training epoch-75 batch-13
Running loss of epoch-75 batch-13 = 1.4735851436853409e-06

Training epoch-75 batch-14
Running loss of epoch-75 batch-14 = 3.4212134778499603e-06

Training epoch-75 batch-15
Running loss of epoch-75 batch-15 = 8.753500878810883e-06

Training epoch-75 batch-16
Running loss of epoch-75 batch-16 = 3.341352567076683e-06

Training epoch-75 batch-17
Running loss of epoch-75 batch-17 = 9.405659511685371e-06

Training epoch-75 batch-18
Running loss of epoch-75 batch-18 = 1.153070479631424e-05

Training epoch-75 batch-19
Running loss of epoch-75 batch-19 = 6.33695162832737e-06

Training epoch-75 batch-20
Running loss of epoch-75 batch-20 = 1.3979151844978333e-06

Training epoch-75 batch-21
Running loss of epoch-75 batch-21 = 1.6256235539913177e-06

Training epoch-75 batch-22
Running loss of epoch-75 batch-22 = 3.9031729102134705e-06

Training epoch-75 batch-23
Running loss of epoch-75 batch-23 = 9.322306141257286e-06

Training epoch-75 batch-24
Running loss of epoch-75 batch-24 = 2.9639340937137604e-06

Training epoch-75 batch-25
Running loss of epoch-75 batch-25 = 3.5760458558797836e-06

Training epoch-75 batch-26
Running loss of epoch-75 batch-26 = 4.027970135211945e-06

Training epoch-75 batch-27
Running loss of epoch-75 batch-27 = 2.830754965543747e-06

Training epoch-75 batch-28
Running loss of epoch-75 batch-28 = 3.7818681448698044e-06

Training epoch-75 batch-29
Running loss of epoch-75 batch-29 = 3.5248231142759323e-06

Training epoch-75 batch-30
Running loss of epoch-75 batch-30 = 3.0209776014089584e-06

Training epoch-75 batch-31
Running loss of epoch-75 batch-31 = 5.663838237524033e-06

Training epoch-75 batch-32
Running loss of epoch-75 batch-32 = 7.478287443518639e-06

Training epoch-75 batch-33
Running loss of epoch-75 batch-33 = 1.1358410120010376e-05

Training epoch-75 batch-34
Running loss of epoch-75 batch-34 = 1.707347109913826e-06

Training epoch-75 batch-35
Running loss of epoch-75 batch-35 = 4.952773451805115e-06

Training epoch-75 batch-36
Running loss of epoch-75 batch-36 = 3.830995410680771e-06

Training epoch-75 batch-37
Running loss of epoch-75 batch-37 = 3.504101186990738e-06

Training epoch-75 batch-38
Running loss of epoch-75 batch-38 = 2.3155007511377335e-06

Training epoch-75 batch-39
Running loss of epoch-75 batch-39 = 3.5299453884363174e-06

Training epoch-75 batch-40
Running loss of epoch-75 batch-40 = 2.2634165361523628e-05

Training epoch-75 batch-41
Running loss of epoch-75 batch-41 = 7.931375876069069e-06

Training epoch-75 batch-42
Running loss of epoch-75 batch-42 = 4.3227337300777435e-06

Training epoch-75 batch-43
Running loss of epoch-75 batch-43 = 1.3308599591255188e-06

Training epoch-75 batch-44
Running loss of epoch-75 batch-44 = 6.716931238770485e-06

Training epoch-75 batch-45
Running loss of epoch-75 batch-45 = 3.409106284379959e-06

Training epoch-75 batch-46
Running loss of epoch-75 batch-46 = 3.025401383638382e-06

Training epoch-75 batch-47
Running loss of epoch-75 batch-47 = 3.1674280762672424e-06

Training epoch-75 batch-48
Running loss of epoch-75 batch-48 = 5.09992241859436e-06

Training epoch-75 batch-49
Running loss of epoch-75 batch-49 = 8.161645382642746e-06

Training epoch-75 batch-50
Running loss of epoch-75 batch-50 = 2.453802153468132e-06

Training epoch-75 batch-51
Running loss of epoch-75 batch-51 = 3.546476364135742e-06

Training epoch-75 batch-52
Running loss of epoch-75 batch-52 = 1.0097399353981018e-05

Training epoch-75 batch-53
Running loss of epoch-75 batch-53 = 2.0490027964115143e-05

Training epoch-75 batch-54
Running loss of epoch-75 batch-54 = 1.991633325815201e-06

Training epoch-75 batch-55
Running loss of epoch-75 batch-55 = 9.894836694002151e-06

Training epoch-75 batch-56
Running loss of epoch-75 batch-56 = 8.312053978443146e-07

Training epoch-75 batch-57
Running loss of epoch-75 batch-57 = 1.501990482211113e-06

Training epoch-75 batch-58
Running loss of epoch-75 batch-58 = 7.367227226495743e-06

Training epoch-75 batch-59
Running loss of epoch-75 batch-59 = 6.2247272580862045e-06

Training epoch-75 batch-60
Running loss of epoch-75 batch-60 = 3.458932042121887e-06

Training epoch-75 batch-61
Running loss of epoch-75 batch-61 = 3.3690594136714935e-06

Training epoch-75 batch-62
Running loss of epoch-75 batch-62 = 5.327397957444191e-06

Training epoch-75 batch-63
Running loss of epoch-75 batch-63 = 5.38630411028862e-06

Training epoch-75 batch-64
Running loss of epoch-75 batch-64 = 2.068234607577324e-06

Training epoch-75 batch-65
Running loss of epoch-75 batch-65 = 1.0247109457850456e-05

Training epoch-75 batch-66
Running loss of epoch-75 batch-66 = 4.705740138888359e-06

Training epoch-75 batch-67
Running loss of epoch-75 batch-67 = 3.2258685678243637e-06

Training epoch-75 batch-68
Running loss of epoch-75 batch-68 = 5.223322659730911e-06

Training epoch-75 batch-69
Running loss of epoch-75 batch-69 = 1.8551945686340332e-06

Training epoch-75 batch-70
Running loss of epoch-75 batch-70 = 4.202593117952347e-06

Training epoch-75 batch-71
Running loss of epoch-75 batch-71 = 2.3192260414361954e-06

Training epoch-75 batch-72
Running loss of epoch-75 batch-72 = 7.247319445014e-06

Training epoch-75 batch-73
Running loss of epoch-75 batch-73 = 3.603985533118248e-06

Training epoch-75 batch-74
Running loss of epoch-75 batch-74 = 4.056841135025024e-06

Training epoch-75 batch-75
Running loss of epoch-75 batch-75 = 2.4531036615371704e-06

Training epoch-75 batch-76
Running loss of epoch-75 batch-76 = 2.7350615710020065e-06

Training epoch-75 batch-77
Running loss of epoch-75 batch-77 = 7.92602077126503e-06

Training epoch-75 batch-78
Running loss of epoch-75 batch-78 = 5.425186827778816e-06

Training epoch-75 batch-79
Running loss of epoch-75 batch-79 = 4.135072231292725e-06

Training epoch-75 batch-80
Running loss of epoch-75 batch-80 = 2.823304384946823e-06

Training epoch-75 batch-81
Running loss of epoch-75 batch-81 = 6.208429113030434e-06

Training epoch-75 batch-82
Running loss of epoch-75 batch-82 = 4.698056727647781e-06

Training epoch-75 batch-83
Running loss of epoch-75 batch-83 = 5.637528374791145e-06

Training epoch-75 batch-84
Running loss of epoch-75 batch-84 = 5.384208634495735e-06

Training epoch-75 batch-85
Running loss of epoch-75 batch-85 = 2.444954589009285e-06

Training epoch-75 batch-86
Running loss of epoch-75 batch-86 = 4.231464117765427e-06

Training epoch-75 batch-87
Running loss of epoch-75 batch-87 = 3.520399332046509e-06

Training epoch-75 batch-88
Running loss of epoch-75 batch-88 = 4.981411620974541e-06

Training epoch-75 batch-89
Running loss of epoch-75 batch-89 = 5.379552021622658e-06

Training epoch-75 batch-90
Running loss of epoch-75 batch-90 = 5.711568519473076e-06

Training epoch-75 batch-91
Running loss of epoch-75 batch-91 = 4.493515007197857e-06

Training epoch-75 batch-92
Running loss of epoch-75 batch-92 = 7.4945855885744095e-06

Training epoch-75 batch-93
Running loss of epoch-75 batch-93 = 2.9015354812145233e-06

Training epoch-75 batch-94
Running loss of epoch-75 batch-94 = 3.976514562964439e-06

Training epoch-75 batch-95
Running loss of epoch-75 batch-95 = 2.1117739379405975e-06

Training epoch-75 batch-96
Running loss of epoch-75 batch-96 = 9.448034688830376e-06

Training epoch-75 batch-97
Running loss of epoch-75 batch-97 = 3.821682184934616e-06

Training epoch-75 batch-98
Running loss of epoch-75 batch-98 = 7.135095074772835e-06

Training epoch-75 batch-99
Running loss of epoch-75 batch-99 = 7.84965232014656e-06

Training epoch-75 batch-100
Running loss of epoch-75 batch-100 = 4.468020051717758e-06

Training epoch-75 batch-101
Running loss of epoch-75 batch-101 = 2.4118926376104355e-06

Training epoch-75 batch-102
Running loss of epoch-75 batch-102 = 2.7704518288373947e-06

Training epoch-75 batch-103
Running loss of epoch-75 batch-103 = 1.1634547263383865e-06

Training epoch-75 batch-104
Running loss of epoch-75 batch-104 = 1.8973369151353836e-06

Training epoch-75 batch-105
Running loss of epoch-75 batch-105 = 7.963273674249649e-06

Training epoch-75 batch-106
Running loss of epoch-75 batch-106 = 3.814697265625e-06

Training epoch-75 batch-107
Running loss of epoch-75 batch-107 = 8.631031960248947e-07

Training epoch-75 batch-108
Running loss of epoch-75 batch-108 = 1.3608718290925026e-05

Training epoch-75 batch-109
Running loss of epoch-75 batch-109 = 1.3362383469939232e-05

Training epoch-75 batch-110
Running loss of epoch-75 batch-110 = 5.956739187240601e-06

Training epoch-75 batch-111
Running loss of epoch-75 batch-111 = 5.567213520407677e-06

Training epoch-75 batch-112
Running loss of epoch-75 batch-112 = 4.889443516731262e-06

Training epoch-75 batch-113
Running loss of epoch-75 batch-113 = 2.0847655832767487e-06

Training epoch-75 batch-114
Running loss of epoch-75 batch-114 = 1.478521153330803e-05

Training epoch-75 batch-115
Running loss of epoch-75 batch-115 = 1.2479955330491066e-05

Training epoch-75 batch-116
Running loss of epoch-75 batch-116 = 1.5717465430498123e-05

Training epoch-75 batch-117
Running loss of epoch-75 batch-117 = 1.2458302080631256e-05

Training epoch-75 batch-118
Running loss of epoch-75 batch-118 = 1.6984296962618828e-05

Training epoch-75 batch-119
Running loss of epoch-75 batch-119 = 5.866168066859245e-06

Training epoch-75 batch-120
Running loss of epoch-75 batch-120 = 2.7068890631198883e-06

Training epoch-75 batch-121
Running loss of epoch-75 batch-121 = 4.130648449063301e-06

Training epoch-75 batch-122
Running loss of epoch-75 batch-122 = 3.18232923746109e-06

Training epoch-75 batch-123
Running loss of epoch-75 batch-123 = 3.937399014830589e-06

Training epoch-75 batch-124
Running loss of epoch-75 batch-124 = 7.302500307559967e-06

Training epoch-75 batch-125
Running loss of epoch-75 batch-125 = 4.806555807590485e-06

Training epoch-75 batch-126
Running loss of epoch-75 batch-126 = 3.6212150007486343e-06

Training epoch-75 batch-127
Running loss of epoch-75 batch-127 = 2.039596438407898e-06

Training epoch-75 batch-128
Running loss of epoch-75 batch-128 = 4.215165972709656e-06

Training epoch-75 batch-129
Running loss of epoch-75 batch-129 = 5.5085401982069016e-06

Training epoch-75 batch-130
Running loss of epoch-75 batch-130 = 1.119682565331459e-06

Training epoch-75 batch-131
Running loss of epoch-75 batch-131 = 4.664994776248932e-06

Training epoch-75 batch-132
Running loss of epoch-75 batch-132 = 1.184176653623581e-06

Training epoch-75 batch-133
Running loss of epoch-75 batch-133 = 3.712717443704605e-06

Training epoch-75 batch-134
Running loss of epoch-75 batch-134 = 4.4549815356731415e-06

Training epoch-75 batch-135
Running loss of epoch-75 batch-135 = 5.198176950216293e-06

Training epoch-75 batch-136
Running loss of epoch-75 batch-136 = 5.350448191165924e-06

Training epoch-75 batch-137
Running loss of epoch-75 batch-137 = 3.637745976448059e-06

Training epoch-75 batch-138
Running loss of epoch-75 batch-138 = 6.793299689888954e-06

Training epoch-75 batch-139
Running loss of epoch-75 batch-139 = 6.058253347873688e-06

Training epoch-75 batch-140
Running loss of epoch-75 batch-140 = 5.797017365694046e-06

Training epoch-75 batch-141
Running loss of epoch-75 batch-141 = 6.349990144371986e-06

Training epoch-75 batch-142
Running loss of epoch-75 batch-142 = 3.7867575883865356e-06

Training epoch-75 batch-143
Running loss of epoch-75 batch-143 = 2.3345928639173508e-06

Training epoch-75 batch-144
Running loss of epoch-75 batch-144 = 4.505971446633339e-06

Training epoch-75 batch-145
Running loss of epoch-75 batch-145 = 2.577202394604683e-06

Training epoch-75 batch-146
Running loss of epoch-75 batch-146 = 5.373731255531311e-06

Training epoch-75 batch-147
Running loss of epoch-75 batch-147 = 2.8170179575681686e-06

Training epoch-75 batch-148
Running loss of epoch-75 batch-148 = 8.915318176150322e-06

Training epoch-75 batch-149
Running loss of epoch-75 batch-149 = 3.8156285881996155e-06

Training epoch-75 batch-150
Running loss of epoch-75 batch-150 = 3.8549769669771194e-06

Training epoch-75 batch-151
Running loss of epoch-75 batch-151 = 4.972098395228386e-06

Training epoch-75 batch-152
Running loss of epoch-75 batch-152 = 2.6319175958633423e-06

Training epoch-75 batch-153
Running loss of epoch-75 batch-153 = 5.559064447879791e-06

Training epoch-75 batch-154
Running loss of epoch-75 batch-154 = 4.16184775531292e-06

Training epoch-75 batch-155
Running loss of epoch-75 batch-155 = 4.427274689078331e-06

Training epoch-75 batch-156
Running loss of epoch-75 batch-156 = 7.571186870336533e-06

Training epoch-75 batch-157
Running loss of epoch-75 batch-157 = 5.46872615814209e-06

Finished training epoch-75.



Average train loss at epoch-75 = 5.226137489080429e-06

Started Evaluation

Average val loss at epoch-75 = 1.7794745334568527

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 89.18 %
Accuracy for class onCreate is: 89.23 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.33 %

Finished Evaluation



Started training epoch-76


Training epoch-76 batch-1
Running loss of epoch-76 batch-1 = 2.4649780243635178e-06

Training epoch-76 batch-2
Running loss of epoch-76 batch-2 = 1.2371689081192017e-05

Training epoch-76 batch-3
Running loss of epoch-76 batch-3 = 1.6099773347377777e-05

Training epoch-76 batch-4
Running loss of epoch-76 batch-4 = 5.1781535148620605e-06

Training epoch-76 batch-5
Running loss of epoch-76 batch-5 = 3.871973603963852e-06

Training epoch-76 batch-6
Running loss of epoch-76 batch-6 = 5.42844645678997e-06

Training epoch-76 batch-7
Running loss of epoch-76 batch-7 = 3.5811681300401688e-06

Training epoch-76 batch-8
Running loss of epoch-76 batch-8 = 7.380964234471321e-06

Training epoch-76 batch-9
Running loss of epoch-76 batch-9 = 3.898749127984047e-06

Training epoch-76 batch-10
Running loss of epoch-76 batch-10 = 4.016794264316559e-06

Training epoch-76 batch-11
Running loss of epoch-76 batch-11 = 5.931127816438675e-06

Training epoch-76 batch-12
Running loss of epoch-76 batch-12 = 2.9543880373239517e-06

Training epoch-76 batch-13
Running loss of epoch-76 batch-13 = 2.2116582840681076e-06

Training epoch-76 batch-14
Running loss of epoch-76 batch-14 = 6.2121544033288956e-06

Training epoch-76 batch-15
Running loss of epoch-76 batch-15 = 2.841232344508171e-06

Training epoch-76 batch-16
Running loss of epoch-76 batch-16 = 1.1187512427568436e-05

Training epoch-76 batch-17
Running loss of epoch-76 batch-17 = 4.2137689888477325e-06

Training epoch-76 batch-18
Running loss of epoch-76 batch-18 = 3.5925768315792084e-06

Training epoch-76 batch-19
Running loss of epoch-76 batch-19 = 1.7033889889717102e-06

Training epoch-76 batch-20
Running loss of epoch-76 batch-20 = 1.3567041605710983e-06

Training epoch-76 batch-21
Running loss of epoch-76 batch-21 = 7.626134902238846e-06

Training epoch-76 batch-22
Running loss of epoch-76 batch-22 = 5.082227289676666e-06

Training epoch-76 batch-23
Running loss of epoch-76 batch-23 = 4.3425243347883224e-06

Training epoch-76 batch-24
Running loss of epoch-76 batch-24 = 2.841698005795479e-06

Training epoch-76 batch-25
Running loss of epoch-76 batch-25 = 3.1578820198774338e-06

Training epoch-76 batch-26
Running loss of epoch-76 batch-26 = 5.937414243817329e-06

Training epoch-76 batch-27
Running loss of epoch-76 batch-27 = 2.522021532058716e-06

Training epoch-76 batch-28
Running loss of epoch-76 batch-28 = 2.639833837747574e-06

Training epoch-76 batch-29
Running loss of epoch-76 batch-29 = 5.015172064304352e-06

Training epoch-76 batch-30
Running loss of epoch-76 batch-30 = 4.550907760858536e-06

Training epoch-76 batch-31
Running loss of epoch-76 batch-31 = 1.1006835848093033e-05

Training epoch-76 batch-32
Running loss of epoch-76 batch-32 = 3.1099189072847366e-06

Training epoch-76 batch-33
Running loss of epoch-76 batch-33 = 4.1692983359098434e-06

Training epoch-76 batch-34
Running loss of epoch-76 batch-34 = 6.126705557107925e-06

Training epoch-76 batch-35
Running loss of epoch-76 batch-35 = 3.9960723370313644e-06

Training epoch-76 batch-36
Running loss of epoch-76 batch-36 = 2.2961758077144623e-06

Training epoch-76 batch-37
Running loss of epoch-76 batch-37 = 6.552319973707199e-06

Training epoch-76 batch-38
Running loss of epoch-76 batch-38 = 1.5283003449440002e-06

Training epoch-76 batch-39
Running loss of epoch-76 batch-39 = 5.372334271669388e-06

Training epoch-76 batch-40
Running loss of epoch-76 batch-40 = 3.835884854197502e-06

Training epoch-76 batch-41
Running loss of epoch-76 batch-41 = 1.828884705901146e-06

Training epoch-76 batch-42
Running loss of epoch-76 batch-42 = 2.6442576199769974e-06

Training epoch-76 batch-43
Running loss of epoch-76 batch-43 = 6.428919732570648e-06

Training epoch-76 batch-44
Running loss of epoch-76 batch-44 = 3.4472905099391937e-06

Training epoch-76 batch-45
Running loss of epoch-76 batch-45 = 5.509704351425171e-06

Training epoch-76 batch-46
Running loss of epoch-76 batch-46 = 1.6998965293169022e-06

Training epoch-76 batch-47
Running loss of epoch-76 batch-47 = 1.2954697012901306e-06

Training epoch-76 batch-48
Running loss of epoch-76 batch-48 = 5.676643922924995e-06

Training epoch-76 batch-49
Running loss of epoch-76 batch-49 = 4.196306690573692e-06

Training epoch-76 batch-50
Running loss of epoch-76 batch-50 = 3.3294782042503357e-06

Training epoch-76 batch-51
Running loss of epoch-76 batch-51 = 3.3837277442216873e-06

Training epoch-76 batch-52
Running loss of epoch-76 batch-52 = 5.273148417472839e-06

Training epoch-76 batch-53
Running loss of epoch-76 batch-53 = 3.119697794318199e-06

Training epoch-76 batch-54
Running loss of epoch-76 batch-54 = 7.376540452241898e-06

Training epoch-76 batch-55
Running loss of epoch-76 batch-55 = 9.207986295223236e-06

Training epoch-76 batch-56
Running loss of epoch-76 batch-56 = 3.066379576921463e-06

Training epoch-76 batch-57
Running loss of epoch-76 batch-57 = 4.693865776062012e-06

Training epoch-76 batch-58
Running loss of epoch-76 batch-58 = 1.2407777830958366e-05

Training epoch-76 batch-59
Running loss of epoch-76 batch-59 = 4.89410012960434e-06

Training epoch-76 batch-60
Running loss of epoch-76 batch-60 = 2.7436763048171997e-06

Training epoch-76 batch-61
Running loss of epoch-76 batch-61 = 5.064299330115318e-06

Training epoch-76 batch-62
Running loss of epoch-76 batch-62 = 6.4999330788850784e-06

Training epoch-76 batch-63
Running loss of epoch-76 batch-63 = 1.1981464922428131e-06

Training epoch-76 batch-64
Running loss of epoch-76 batch-64 = 1.2012897059321404e-05

Training epoch-76 batch-65
Running loss of epoch-76 batch-65 = 5.54276630282402e-06

Training epoch-76 batch-66
Running loss of epoch-76 batch-66 = 5.077337846159935e-06

Training epoch-76 batch-67
Running loss of epoch-76 batch-67 = 2.11433507502079e-06

Training epoch-76 batch-68
Running loss of epoch-76 batch-68 = 6.566056981682777e-06

Training epoch-76 batch-69
Running loss of epoch-76 batch-69 = 7.68597237765789e-06

Training epoch-76 batch-70
Running loss of epoch-76 batch-70 = 4.334375262260437e-06

Training epoch-76 batch-71
Running loss of epoch-76 batch-71 = 5.855457857251167e-06

Training epoch-76 batch-72
Running loss of epoch-76 batch-72 = 6.282934918999672e-06

Training epoch-76 batch-73
Running loss of epoch-76 batch-73 = 4.600267857313156e-06

Training epoch-76 batch-74
Running loss of epoch-76 batch-74 = 4.562782123684883e-06

Training epoch-76 batch-75
Running loss of epoch-76 batch-75 = 3.6961864680051804e-06

Training epoch-76 batch-76
Running loss of epoch-76 batch-76 = 3.999099135398865e-06

Training epoch-76 batch-77
Running loss of epoch-76 batch-77 = 1.223105937242508e-05

Training epoch-76 batch-78
Running loss of epoch-76 batch-78 = 3.5928096622228622e-06

Training epoch-76 batch-79
Running loss of epoch-76 batch-79 = 1.684296876192093e-06

Training epoch-76 batch-80
Running loss of epoch-76 batch-80 = 6.210291758179665e-06

Training epoch-76 batch-81
Running loss of epoch-76 batch-81 = 3.041233867406845e-06

Training epoch-76 batch-82
Running loss of epoch-76 batch-82 = 3.232154995203018e-06

Training epoch-76 batch-83
Running loss of epoch-76 batch-83 = 3.718305379152298e-06

Training epoch-76 batch-84
Running loss of epoch-76 batch-84 = 2.228887751698494e-06

Training epoch-76 batch-85
Running loss of epoch-76 batch-85 = 1.1912081390619278e-05

Training epoch-76 batch-86
Running loss of epoch-76 batch-86 = 6.854301318526268e-06

Training epoch-76 batch-87
Running loss of epoch-76 batch-87 = 1.0114628821611404e-05

Training epoch-76 batch-88
Running loss of epoch-76 batch-88 = 4.140427336096764e-06

Training epoch-76 batch-89
Running loss of epoch-76 batch-89 = 4.717614501714706e-06

Training epoch-76 batch-90
Running loss of epoch-76 batch-90 = 3.925524652004242e-06

Training epoch-76 batch-91
Running loss of epoch-76 batch-91 = 2.251705154776573e-06

Training epoch-76 batch-92
Running loss of epoch-76 batch-92 = 3.902241587638855e-06

Training epoch-76 batch-93
Running loss of epoch-76 batch-93 = 3.950903192162514e-06

Training epoch-76 batch-94
Running loss of epoch-76 batch-94 = 5.302485078573227e-06

Training epoch-76 batch-95
Running loss of epoch-76 batch-95 = 1.210952177643776e-06

Training epoch-76 batch-96
Running loss of epoch-76 batch-96 = 1.9960571080446243e-06

Training epoch-76 batch-97
Running loss of epoch-76 batch-97 = 3.556022420525551e-06

Training epoch-76 batch-98
Running loss of epoch-76 batch-98 = 3.932276740670204e-06

Training epoch-76 batch-99
Running loss of epoch-76 batch-99 = 2.771615982055664e-06

Training epoch-76 batch-100
Running loss of epoch-76 batch-100 = 4.9888622015714645e-06

Training epoch-76 batch-101
Running loss of epoch-76 batch-101 = 4.2370520532131195e-06

Training epoch-76 batch-102
Running loss of epoch-76 batch-102 = 3.734370693564415e-06

Training epoch-76 batch-103
Running loss of epoch-76 batch-103 = 4.245201125741005e-06

Training epoch-76 batch-104
Running loss of epoch-76 batch-104 = 7.661525160074234e-06

Training epoch-76 batch-105
Running loss of epoch-76 batch-105 = 1.5776371583342552e-05

Training epoch-76 batch-106
Running loss of epoch-76 batch-106 = 8.602859452366829e-06

Training epoch-76 batch-107
Running loss of epoch-76 batch-107 = 4.75812703371048e-06

Training epoch-76 batch-108
Running loss of epoch-76 batch-108 = 4.94043342769146e-06

Training epoch-76 batch-109
Running loss of epoch-76 batch-109 = 7.140683010220528e-06

Training epoch-76 batch-110
Running loss of epoch-76 batch-110 = 4.286644980311394e-06

Training epoch-76 batch-111
Running loss of epoch-76 batch-111 = 5.077570676803589e-06

Training epoch-76 batch-112
Running loss of epoch-76 batch-112 = 3.8116704672574997e-06

Training epoch-76 batch-113
Running loss of epoch-76 batch-113 = 4.834262654185295e-06

Training epoch-76 batch-114
Running loss of epoch-76 batch-114 = 5.342997610569e-06

Training epoch-76 batch-115
Running loss of epoch-76 batch-115 = 2.8989743441343307e-06

Training epoch-76 batch-116
Running loss of epoch-76 batch-116 = 2.135755494236946e-06

Training epoch-76 batch-117
Running loss of epoch-76 batch-117 = 9.413808584213257e-06

Training epoch-76 batch-118
Running loss of epoch-76 batch-118 = 1.0576099157333374e-05

Training epoch-76 batch-119
Running loss of epoch-76 batch-119 = 5.138572305440903e-06

Training epoch-76 batch-120
Running loss of epoch-76 batch-120 = 1.5906989574432373e-06

Training epoch-76 batch-121
Running loss of epoch-76 batch-121 = 6.4086634665727615e-06

Training epoch-76 batch-122
Running loss of epoch-76 batch-122 = 7.055466994643211e-06

Training epoch-76 batch-123
Running loss of epoch-76 batch-123 = 2.5103799998760223e-06

Training epoch-76 batch-124
Running loss of epoch-76 batch-124 = 2.0919833332300186e-06

Training epoch-76 batch-125
Running loss of epoch-76 batch-125 = 4.633795469999313e-06

Training epoch-76 batch-126
Running loss of epoch-76 batch-126 = 5.792360752820969e-06

Training epoch-76 batch-127
Running loss of epoch-76 batch-127 = 5.585839971899986e-06

Training epoch-76 batch-128
Running loss of epoch-76 batch-128 = 4.761619493365288e-06

Training epoch-76 batch-129
Running loss of epoch-76 batch-129 = 3.8032885640859604e-06

Training epoch-76 batch-130
Running loss of epoch-76 batch-130 = 8.86525958776474e-06

Training epoch-76 batch-131
Running loss of epoch-76 batch-131 = 1.6654841601848602e-05

Training epoch-76 batch-132
Running loss of epoch-76 batch-132 = 4.9727968871593475e-06

Training epoch-76 batch-133
Running loss of epoch-76 batch-133 = 5.213776603341103e-06

Training epoch-76 batch-134
Running loss of epoch-76 batch-134 = 1.2523960322141647e-06

Training epoch-76 batch-135
Running loss of epoch-76 batch-135 = 7.680384442210197e-06

Training epoch-76 batch-136
Running loss of epoch-76 batch-136 = 1.9383151084184647e-06

Training epoch-76 batch-137
Running loss of epoch-76 batch-137 = 2.0086299628019333e-06

Training epoch-76 batch-138
Running loss of epoch-76 batch-138 = 8.418690413236618e-06

Training epoch-76 batch-139
Running loss of epoch-76 batch-139 = 2.705724909901619e-06

Training epoch-76 batch-140
Running loss of epoch-76 batch-140 = 3.3585820347070694e-06

Training epoch-76 batch-141
Running loss of epoch-76 batch-141 = 3.970460966229439e-06

Training epoch-76 batch-142
Running loss of epoch-76 batch-142 = 1.5987548977136612e-05

Training epoch-76 batch-143
Running loss of epoch-76 batch-143 = 8.635921403765678e-06

Training epoch-76 batch-144
Running loss of epoch-76 batch-144 = 2.9513612389564514e-06

Training epoch-76 batch-145
Running loss of epoch-76 batch-145 = 5.301320925354958e-06

Training epoch-76 batch-146
Running loss of epoch-76 batch-146 = 1.6307458281517029e-06

Training epoch-76 batch-147
Running loss of epoch-76 batch-147 = 2.4114269763231277e-06

Training epoch-76 batch-148
Running loss of epoch-76 batch-148 = 1.6915146261453629e-06

Training epoch-76 batch-149
Running loss of epoch-76 batch-149 = 4.818663001060486e-06

Training epoch-76 batch-150
Running loss of epoch-76 batch-150 = 5.990499630570412e-06

Training epoch-76 batch-151
Running loss of epoch-76 batch-151 = 2.4281907826662064e-06

Training epoch-76 batch-152
Running loss of epoch-76 batch-152 = 4.404457286000252e-06

Training epoch-76 batch-153
Running loss of epoch-76 batch-153 = 3.2887328416109085e-06

Training epoch-76 batch-154
Running loss of epoch-76 batch-154 = 9.98377799987793e-06

Training epoch-76 batch-155
Running loss of epoch-76 batch-155 = 1.2693926692008972e-06

Training epoch-76 batch-156
Running loss of epoch-76 batch-156 = 6.744870916008949e-06

Training epoch-76 batch-157
Running loss of epoch-76 batch-157 = 1.298263669013977e-05

Finished training epoch-76.



Average train loss at epoch-76 = 5.079431831836701e-06

Started Evaluation

Average val loss at epoch-76 = 1.791467488865665

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.59 %
Accuracy for class run is: 63.70 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.06 %

Finished Evaluation



Started training epoch-77


Training epoch-77 batch-1
Running loss of epoch-77 batch-1 = 6.366986781358719e-06

Training epoch-77 batch-2
Running loss of epoch-77 batch-2 = 3.423308953642845e-06

Training epoch-77 batch-3
Running loss of epoch-77 batch-3 = 1.0193325579166412e-06

Training epoch-77 batch-4
Running loss of epoch-77 batch-4 = 2.2763852030038834e-06

Training epoch-77 batch-5
Running loss of epoch-77 batch-5 = 2.405373379588127e-06

Training epoch-77 batch-6
Running loss of epoch-77 batch-6 = 4.054279997944832e-06

Training epoch-77 batch-7
Running loss of epoch-77 batch-7 = 7.214490324258804e-06

Training epoch-77 batch-8
Running loss of epoch-77 batch-8 = 2.2242311388254166e-06

Training epoch-77 batch-9
Running loss of epoch-77 batch-9 = 6.214366294443607e-06

Training epoch-77 batch-10
Running loss of epoch-77 batch-10 = 3.91458161175251e-06

Training epoch-77 batch-11
Running loss of epoch-77 batch-11 = 3.334367647767067e-06

Training epoch-77 batch-12
Running loss of epoch-77 batch-12 = 1.51775311678648e-05

Training epoch-77 batch-13
Running loss of epoch-77 batch-13 = 4.151836037635803e-06

Training epoch-77 batch-14
Running loss of epoch-77 batch-14 = 4.044966772198677e-06

Training epoch-77 batch-15
Running loss of epoch-77 batch-15 = 4.536937922239304e-06

Training epoch-77 batch-16
Running loss of epoch-77 batch-16 = 2.2351741790771484e-06

Training epoch-77 batch-17
Running loss of epoch-77 batch-17 = 4.161149263381958e-06

Training epoch-77 batch-18
Running loss of epoch-77 batch-18 = 4.500150680541992e-06

Training epoch-77 batch-19
Running loss of epoch-77 batch-19 = 1.213839277625084e-05

Training epoch-77 batch-20
Running loss of epoch-77 batch-20 = 4.809116944670677e-06

Training epoch-77 batch-21
Running loss of epoch-77 batch-21 = 5.966052412986755e-06

Training epoch-77 batch-22
Running loss of epoch-77 batch-22 = 2.825632691383362e-06

Training epoch-77 batch-23
Running loss of epoch-77 batch-23 = 3.5141129046678543e-06

Training epoch-77 batch-24
Running loss of epoch-77 batch-24 = 2.8989743441343307e-06

Training epoch-77 batch-25
Running loss of epoch-77 batch-25 = 4.321802407503128e-06

Training epoch-77 batch-26
Running loss of epoch-77 batch-26 = 6.520422175526619e-06

Training epoch-77 batch-27
Running loss of epoch-77 batch-27 = 2.389540895819664e-06

Training epoch-77 batch-28
Running loss of epoch-77 batch-28 = 6.973743438720703e-06

Training epoch-77 batch-29
Running loss of epoch-77 batch-29 = 4.213536158204079e-06

Training epoch-77 batch-30
Running loss of epoch-77 batch-30 = 1.7345882952213287e-06

Training epoch-77 batch-31
Running loss of epoch-77 batch-31 = 4.0691811591386795e-06

Training epoch-77 batch-32
Running loss of epoch-77 batch-32 = 5.20097091794014e-06

Training epoch-77 batch-33
Running loss of epoch-77 batch-33 = 7.174443453550339e-06

Training epoch-77 batch-34
Running loss of epoch-77 batch-34 = 3.5779085010290146e-06

Training epoch-77 batch-35
Running loss of epoch-77 batch-35 = 5.488516762852669e-06

Training epoch-77 batch-36
Running loss of epoch-77 batch-36 = 1.7655547708272934e-06

Training epoch-77 batch-37
Running loss of epoch-77 batch-37 = 4.70457598567009e-06

Training epoch-77 batch-38
Running loss of epoch-77 batch-38 = 5.217967554926872e-06

Training epoch-77 batch-39
Running loss of epoch-77 batch-39 = 5.825888365507126e-06

Training epoch-77 batch-40
Running loss of epoch-77 batch-40 = 4.14624810218811e-06

Training epoch-77 batch-41
Running loss of epoch-77 batch-41 = 2.7408823370933533e-06

Training epoch-77 batch-42
Running loss of epoch-77 batch-42 = 2.41817906498909e-06

Training epoch-77 batch-43
Running loss of epoch-77 batch-43 = 3.1620729714632034e-06

Training epoch-77 batch-44
Running loss of epoch-77 batch-44 = 5.471287295222282e-06

Training epoch-77 batch-45
Running loss of epoch-77 batch-45 = 3.812834620475769e-06

Training epoch-77 batch-46
Running loss of epoch-77 batch-46 = 7.541151717305183e-06

Training epoch-77 batch-47
Running loss of epoch-77 batch-47 = 3.612600266933441e-06

Training epoch-77 batch-48
Running loss of epoch-77 batch-48 = 2.7562491595745087e-06

Training epoch-77 batch-49
Running loss of epoch-77 batch-49 = 6.791204214096069e-06

Training epoch-77 batch-50
Running loss of epoch-77 batch-50 = 1.3967743143439293e-05

Training epoch-77 batch-51
Running loss of epoch-77 batch-51 = 1.7739133909344673e-05

Training epoch-77 batch-52
Running loss of epoch-77 batch-52 = 7.832422852516174e-06

Training epoch-77 batch-53
Running loss of epoch-77 batch-53 = 2.3783650249242783e-06

Training epoch-77 batch-54
Running loss of epoch-77 batch-54 = 4.034955054521561e-06

Training epoch-77 batch-55
Running loss of epoch-77 batch-55 = 1.2933742254972458e-05

Training epoch-77 batch-56
Running loss of epoch-77 batch-56 = 6.094807758927345e-06

Training epoch-77 batch-57
Running loss of epoch-77 batch-57 = 2.8207432478666306e-06

Training epoch-77 batch-58
Running loss of epoch-77 batch-58 = 3.878260031342506e-06

Training epoch-77 batch-59
Running loss of epoch-77 batch-59 = 4.377914592623711e-06

Training epoch-77 batch-60
Running loss of epoch-77 batch-60 = 4.4710468500852585e-06

Training epoch-77 batch-61
Running loss of epoch-77 batch-61 = 2.8971116989851e-06

Training epoch-77 batch-62
Running loss of epoch-77 batch-62 = 6.35022297501564e-06

Training epoch-77 batch-63
Running loss of epoch-77 batch-63 = 5.190260708332062e-06

Training epoch-77 batch-64
Running loss of epoch-77 batch-64 = 1.385807991027832e-06

Training epoch-77 batch-65
Running loss of epoch-77 batch-65 = 1.6489066183567047e-06

Training epoch-77 batch-66
Running loss of epoch-77 batch-66 = 4.465458914637566e-06

Training epoch-77 batch-67
Running loss of epoch-77 batch-67 = 4.462897777557373e-06

Training epoch-77 batch-68
Running loss of epoch-77 batch-68 = 6.922287866473198e-06

Training epoch-77 batch-69
Running loss of epoch-77 batch-69 = 4.176981747150421e-06

Training epoch-77 batch-70
Running loss of epoch-77 batch-70 = 6.487825885415077e-06

Training epoch-77 batch-71
Running loss of epoch-77 batch-71 = 3.4463591873645782e-06

Training epoch-77 batch-72
Running loss of epoch-77 batch-72 = 3.35020013153553e-06

Training epoch-77 batch-73
Running loss of epoch-77 batch-73 = 8.741626515984535e-06

Training epoch-77 batch-74
Running loss of epoch-77 batch-74 = 1.4103483408689499e-05

Training epoch-77 batch-75
Running loss of epoch-77 batch-75 = 2.150190994143486e-06

Training epoch-77 batch-76
Running loss of epoch-77 batch-76 = 4.5085325837135315e-06

Training epoch-77 batch-77
Running loss of epoch-77 batch-77 = 3.942754119634628e-06

Training epoch-77 batch-78
Running loss of epoch-77 batch-78 = 6.075715646147728e-06

Training epoch-77 batch-79
Running loss of epoch-77 batch-79 = 5.659414455294609e-06

Training epoch-77 batch-80
Running loss of epoch-77 batch-80 = 6.521586328744888e-06

Training epoch-77 batch-81
Running loss of epoch-77 batch-81 = 2.359272912144661e-06

Training epoch-77 batch-82
Running loss of epoch-77 batch-82 = 4.379311576485634e-06

Training epoch-77 batch-83
Running loss of epoch-77 batch-83 = 2.149958163499832e-06

Training epoch-77 batch-84
Running loss of epoch-77 batch-84 = 2.8461217880249023e-06

Training epoch-77 batch-85
Running loss of epoch-77 batch-85 = 4.659406840801239e-06

Training epoch-77 batch-86
Running loss of epoch-77 batch-86 = 4.023313522338867e-06

Training epoch-77 batch-87
Running loss of epoch-77 batch-87 = 2.6065390557050705e-06

Training epoch-77 batch-88
Running loss of epoch-77 batch-88 = 7.080845534801483e-06

Training epoch-77 batch-89
Running loss of epoch-77 batch-89 = 2.062646672129631e-06

Training epoch-77 batch-90
Running loss of epoch-77 batch-90 = 6.452435627579689e-06

Training epoch-77 batch-91
Running loss of epoch-77 batch-91 = 7.303664460778236e-06

Training epoch-77 batch-92
Running loss of epoch-77 batch-92 = 2.873362973332405e-06

Training epoch-77 batch-93
Running loss of epoch-77 batch-93 = 5.175825208425522e-06

Training epoch-77 batch-94
Running loss of epoch-77 batch-94 = 3.6670826375484467e-06

Training epoch-77 batch-95
Running loss of epoch-77 batch-95 = 6.213318556547165e-06

Training epoch-77 batch-96
Running loss of epoch-77 batch-96 = 4.872679710388184e-06

Training epoch-77 batch-97
Running loss of epoch-77 batch-97 = 1.4133285731077194e-05

Training epoch-77 batch-98
Running loss of epoch-77 batch-98 = 9.295763447880745e-06

Training epoch-77 batch-99
Running loss of epoch-77 batch-99 = 1.5734229236841202e-05

Training epoch-77 batch-100
Running loss of epoch-77 batch-100 = 2.8370413929224014e-06

Training epoch-77 batch-101
Running loss of epoch-77 batch-101 = 4.838453605771065e-06

Training epoch-77 batch-102
Running loss of epoch-77 batch-102 = 4.796544089913368e-06

Training epoch-77 batch-103
Running loss of epoch-77 batch-103 = 5.271518602967262e-06

Training epoch-77 batch-104
Running loss of epoch-77 batch-104 = 2.8244685381650925e-06

Training epoch-77 batch-105
Running loss of epoch-77 batch-105 = 3.827037289738655e-06

Training epoch-77 batch-106
Running loss of epoch-77 batch-106 = 5.8282166719436646e-06

Training epoch-77 batch-107
Running loss of epoch-77 batch-107 = 5.240552127361298e-06

Training epoch-77 batch-108
Running loss of epoch-77 batch-108 = 8.011935278773308e-06

Training epoch-77 batch-109
Running loss of epoch-77 batch-109 = 2.074986696243286e-06

Training epoch-77 batch-110
Running loss of epoch-77 batch-110 = 2.704095095396042e-06

Training epoch-77 batch-111
Running loss of epoch-77 batch-111 = 6.004702299833298e-06

Training epoch-77 batch-112
Running loss of epoch-77 batch-112 = 3.3348333090543747e-06

Training epoch-77 batch-113
Running loss of epoch-77 batch-113 = 7.4249692261219025e-06

Training epoch-77 batch-114
Running loss of epoch-77 batch-114 = 1.325993798673153e-05

Training epoch-77 batch-115
Running loss of epoch-77 batch-115 = 7.127644494175911e-06

Training epoch-77 batch-116
Running loss of epoch-77 batch-116 = 3.22563573718071e-06

Training epoch-77 batch-117
Running loss of epoch-77 batch-117 = 3.4205149859189987e-06

Training epoch-77 batch-118
Running loss of epoch-77 batch-118 = 3.2777898013591766e-06

Training epoch-77 batch-119
Running loss of epoch-77 batch-119 = 4.388624802231789e-06

Training epoch-77 batch-120
Running loss of epoch-77 batch-120 = 2.2598542273044586e-06

Training epoch-77 batch-121
Running loss of epoch-77 batch-121 = 5.097128450870514e-06

Training epoch-77 batch-122
Running loss of epoch-77 batch-122 = 6.300164386630058e-06

Training epoch-77 batch-123
Running loss of epoch-77 batch-123 = 1.5187542885541916e-06

Training epoch-77 batch-124
Running loss of epoch-77 batch-124 = 7.269205525517464e-06

Training epoch-77 batch-125
Running loss of epoch-77 batch-125 = 3.3902470022439957e-06

Training epoch-77 batch-126
Running loss of epoch-77 batch-126 = 4.038447514176369e-06

Training epoch-77 batch-127
Running loss of epoch-77 batch-127 = 3.2349489629268646e-06

Training epoch-77 batch-128
Running loss of epoch-77 batch-128 = 5.462439730763435e-06

Training epoch-77 batch-129
Running loss of epoch-77 batch-129 = 7.093651220202446e-06

Training epoch-77 batch-130
Running loss of epoch-77 batch-130 = 2.2726599127054214e-06

Training epoch-77 batch-131
Running loss of epoch-77 batch-131 = 5.967915058135986e-06

Training epoch-77 batch-132
Running loss of epoch-77 batch-132 = 1.9476283341646194e-06

Training epoch-77 batch-133
Running loss of epoch-77 batch-133 = 1.0539544746279716e-05

Training epoch-77 batch-134
Running loss of epoch-77 batch-134 = 3.6961864680051804e-06

Training epoch-77 batch-135
Running loss of epoch-77 batch-135 = 5.2961986511945724e-06

Training epoch-77 batch-136
Running loss of epoch-77 batch-136 = 5.926936864852905e-06

Training epoch-77 batch-137
Running loss of epoch-77 batch-137 = 3.00724059343338e-06

Training epoch-77 batch-138
Running loss of epoch-77 batch-138 = 4.232395440340042e-06

Training epoch-77 batch-139
Running loss of epoch-77 batch-139 = 8.134637027978897e-06

Training epoch-77 batch-140
Running loss of epoch-77 batch-140 = 3.3776741474866867e-06

Training epoch-77 batch-141
Running loss of epoch-77 batch-141 = 4.156492650508881e-06

Training epoch-77 batch-142
Running loss of epoch-77 batch-142 = 3.164634108543396e-06

Training epoch-77 batch-143
Running loss of epoch-77 batch-143 = 8.896458894014359e-06

Training epoch-77 batch-144
Running loss of epoch-77 batch-144 = 2.7962960302829742e-06

Training epoch-77 batch-145
Running loss of epoch-77 batch-145 = 2.5921035557985306e-06

Training epoch-77 batch-146
Running loss of epoch-77 batch-146 = 3.833789378404617e-06

Training epoch-77 batch-147
Running loss of epoch-77 batch-147 = 1.92900188267231e-06

Training epoch-77 batch-148
Running loss of epoch-77 batch-148 = 9.335111826658249e-06

Training epoch-77 batch-149
Running loss of epoch-77 batch-149 = 5.1050446927547455e-06

Training epoch-77 batch-150
Running loss of epoch-77 batch-150 = 3.0838418751955032e-06

Training epoch-77 batch-151
Running loss of epoch-77 batch-151 = 2.369750291109085e-06

Training epoch-77 batch-152
Running loss of epoch-77 batch-152 = 5.426816642284393e-06

Training epoch-77 batch-153
Running loss of epoch-77 batch-153 = 8.995411917567253e-06

Training epoch-77 batch-154
Running loss of epoch-77 batch-154 = 1.6454141587018967e-06

Training epoch-77 batch-155
Running loss of epoch-77 batch-155 = 6.653601303696632e-06

Training epoch-77 batch-156
Running loss of epoch-77 batch-156 = 4.745088517665863e-06

Training epoch-77 batch-157
Running loss of epoch-77 batch-157 = 2.1029263734817505e-05

Finished training epoch-77.



Average train loss at epoch-77 = 5.092572420835495e-06

Started Evaluation

Average val loss at epoch-77 = 1.7853653807386416

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.91 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-78


Training epoch-78 batch-1
Running loss of epoch-78 batch-1 = 4.248926416039467e-06

Training epoch-78 batch-2
Running loss of epoch-78 batch-2 = 1.7669517546892166e-06

Training epoch-78 batch-3
Running loss of epoch-78 batch-3 = 5.10411337018013e-06

Training epoch-78 batch-4
Running loss of epoch-78 batch-4 = 4.824018105864525e-06

Training epoch-78 batch-5
Running loss of epoch-78 batch-5 = 6.600050255656242e-06

Training epoch-78 batch-6
Running loss of epoch-78 batch-6 = 4.517612978816032e-06

Training epoch-78 batch-7
Running loss of epoch-78 batch-7 = 1.2567965313792229e-05

Training epoch-78 batch-8
Running loss of epoch-78 batch-8 = 3.0705705285072327e-06

Training epoch-78 batch-9
Running loss of epoch-78 batch-9 = 4.037981852889061e-06

Training epoch-78 batch-10
Running loss of epoch-78 batch-10 = 1.432374119758606e-06

Training epoch-78 batch-11
Running loss of epoch-78 batch-11 = 5.800509825348854e-06

Training epoch-78 batch-12
Running loss of epoch-78 batch-12 = 3.0582305043935776e-06

Training epoch-78 batch-13
Running loss of epoch-78 batch-13 = 2.3527536541223526e-06

Training epoch-78 batch-14
Running loss of epoch-78 batch-14 = 3.861496224999428e-06

Training epoch-78 batch-15
Running loss of epoch-78 batch-15 = 4.868721589446068e-06

Training epoch-78 batch-16
Running loss of epoch-78 batch-16 = 4.110625013709068e-06

Training epoch-78 batch-17
Running loss of epoch-78 batch-17 = 2.9136426746845245e-06

Training epoch-78 batch-18
Running loss of epoch-78 batch-18 = 1.061498187482357e-05

Training epoch-78 batch-19
Running loss of epoch-78 batch-19 = 6.165355443954468e-06

Training epoch-78 batch-20
Running loss of epoch-78 batch-20 = 4.185130819678307e-06

Training epoch-78 batch-21
Running loss of epoch-78 batch-21 = 5.0086528062820435e-06

Training epoch-78 batch-22
Running loss of epoch-78 batch-22 = 4.7227367758750916e-06

Training epoch-78 batch-23
Running loss of epoch-78 batch-23 = 1.0093441233038902e-05

Training epoch-78 batch-24
Running loss of epoch-78 batch-24 = 6.334157660603523e-06

Training epoch-78 batch-25
Running loss of epoch-78 batch-25 = 4.27919439971447e-06

Training epoch-78 batch-26
Running loss of epoch-78 batch-26 = 4.92599792778492e-06

Training epoch-78 batch-27
Running loss of epoch-78 batch-27 = 1.1068303138017654e-05

Training epoch-78 batch-28
Running loss of epoch-78 batch-28 = 4.2764004319906235e-06

Training epoch-78 batch-29
Running loss of epoch-78 batch-29 = 1.5848781913518906e-06

Training epoch-78 batch-30
Running loss of epoch-78 batch-30 = 5.017733201384544e-06

Training epoch-78 batch-31
Running loss of epoch-78 batch-31 = 9.546754881739616e-06

Training epoch-78 batch-32
Running loss of epoch-78 batch-32 = 1.915963366627693e-06

Training epoch-78 batch-33
Running loss of epoch-78 batch-33 = 2.7962960302829742e-06

Training epoch-78 batch-34
Running loss of epoch-78 batch-34 = 1.491047441959381e-06

Training epoch-78 batch-35
Running loss of epoch-78 batch-35 = 4.17511910200119e-06

Training epoch-78 batch-36
Running loss of epoch-78 batch-36 = 1.1753290891647339e-06

Training epoch-78 batch-37
Running loss of epoch-78 batch-37 = 8.277129381895065e-06

Training epoch-78 batch-38
Running loss of epoch-78 batch-38 = 1.734308898448944e-05

Training epoch-78 batch-39
Running loss of epoch-78 batch-39 = 1.5606638044118881e-06

Training epoch-78 batch-40
Running loss of epoch-78 batch-40 = 7.079914212226868e-06

Training epoch-78 batch-41
Running loss of epoch-78 batch-41 = 3.054272383451462e-06

Training epoch-78 batch-42
Running loss of epoch-78 batch-42 = 4.988396540284157e-06

Training epoch-78 batch-43
Running loss of epoch-78 batch-43 = 3.280816599726677e-06

Training epoch-78 batch-44
Running loss of epoch-78 batch-44 = 1.5851110219955444e-06

Training epoch-78 batch-45
Running loss of epoch-78 batch-45 = 2.943212166428566e-06

Training epoch-78 batch-46
Running loss of epoch-78 batch-46 = 3.0011869966983795e-06

Training epoch-78 batch-47
Running loss of epoch-78 batch-47 = 1.5084166079759598e-05

Training epoch-78 batch-48
Running loss of epoch-78 batch-48 = 3.529014065861702e-06

Training epoch-78 batch-49
Running loss of epoch-78 batch-49 = 3.603985533118248e-06

Training epoch-78 batch-50
Running loss of epoch-78 batch-50 = 2.7623027563095093e-06

Training epoch-78 batch-51
Running loss of epoch-78 batch-51 = 3.0954834073781967e-06

Training epoch-78 batch-52
Running loss of epoch-78 batch-52 = 2.4475157260894775e-06

Training epoch-78 batch-53
Running loss of epoch-78 batch-53 = 5.893642082810402e-06

Training epoch-78 batch-54
Running loss of epoch-78 batch-54 = 5.228910595178604e-06

Training epoch-78 batch-55
Running loss of epoch-78 batch-55 = 4.946254193782806e-06

Training epoch-78 batch-56
Running loss of epoch-78 batch-56 = 8.40681605041027e-06

Training epoch-78 batch-57
Running loss of epoch-78 batch-57 = 4.478497430682182e-06

Training epoch-78 batch-58
Running loss of epoch-78 batch-58 = 3.5392586141824722e-06

Training epoch-78 batch-59
Running loss of epoch-78 batch-59 = 5.647772923111916e-06

Training epoch-78 batch-60
Running loss of epoch-78 batch-60 = 6.83218240737915e-06

Training epoch-78 batch-61
Running loss of epoch-78 batch-61 = 4.342058673501015e-06

Training epoch-78 batch-62
Running loss of epoch-78 batch-62 = 3.1471718102693558e-06

Training epoch-78 batch-63
Running loss of epoch-78 batch-63 = 4.783272743225098e-06

Training epoch-78 batch-64
Running loss of epoch-78 batch-64 = 5.252426490187645e-06

Training epoch-78 batch-65
Running loss of epoch-78 batch-65 = 3.41096892952919e-07

Training epoch-78 batch-66
Running loss of epoch-78 batch-66 = 3.4410040825605392e-06

Training epoch-78 batch-67
Running loss of epoch-78 batch-67 = 1.3157026842236519e-05

Training epoch-78 batch-68
Running loss of epoch-78 batch-68 = 2.573477104306221e-06

Training epoch-78 batch-69
Running loss of epoch-78 batch-69 = 3.5117845982313156e-06

Training epoch-78 batch-70
Running loss of epoch-78 batch-70 = 2.678949385881424e-06

Training epoch-78 batch-71
Running loss of epoch-78 batch-71 = 3.6375131458044052e-06

Training epoch-78 batch-72
Running loss of epoch-78 batch-72 = 6.003538146615028e-06

Training epoch-78 batch-73
Running loss of epoch-78 batch-73 = 1.276843249797821e-06

Training epoch-78 batch-74
Running loss of epoch-78 batch-74 = 3.1793024390935898e-06

Training epoch-78 batch-75
Running loss of epoch-78 batch-75 = 2.046464942395687e-05

Training epoch-78 batch-76
Running loss of epoch-78 batch-76 = 1.1731637641787529e-05

Training epoch-78 batch-77
Running loss of epoch-78 batch-77 = 2.5585759431123734e-06

Training epoch-78 batch-78
Running loss of epoch-78 batch-78 = 6.413785740733147e-06

Training epoch-78 batch-79
Running loss of epoch-78 batch-79 = 5.888286978006363e-06

Training epoch-78 batch-80
Running loss of epoch-78 batch-80 = 6.207497790455818e-06

Training epoch-78 batch-81
Running loss of epoch-78 batch-81 = 5.010981112718582e-06

Training epoch-78 batch-82
Running loss of epoch-78 batch-82 = 3.461260348558426e-06

Training epoch-78 batch-83
Running loss of epoch-78 batch-83 = 1.080567017197609e-06

Training epoch-78 batch-84
Running loss of epoch-78 batch-84 = 3.652181476354599e-06

Training epoch-78 batch-85
Running loss of epoch-78 batch-85 = 2.046581357717514e-06

Training epoch-78 batch-86
Running loss of epoch-78 batch-86 = 3.478722646832466e-06

Training epoch-78 batch-87
Running loss of epoch-78 batch-87 = 3.998866304755211e-06

Training epoch-78 batch-88
Running loss of epoch-78 batch-88 = 3.766734153032303e-06

Training epoch-78 batch-89
Running loss of epoch-78 batch-89 = 4.827976226806641e-06

Training epoch-78 batch-90
Running loss of epoch-78 batch-90 = 3.826804459095001e-06

Training epoch-78 batch-91
Running loss of epoch-78 batch-91 = 2.05356627702713e-06

Training epoch-78 batch-92
Running loss of epoch-78 batch-92 = 2.534128725528717e-06

Training epoch-78 batch-93
Running loss of epoch-78 batch-93 = 4.184897989034653e-06

Training epoch-78 batch-94
Running loss of epoch-78 batch-94 = 1.3497192412614822e-05

Training epoch-78 batch-95
Running loss of epoch-78 batch-95 = 7.107621058821678e-06

Training epoch-78 batch-96
Running loss of epoch-78 batch-96 = 6.383052095770836e-06

Training epoch-78 batch-97
Running loss of epoch-78 batch-97 = 4.9443915486335754e-06

Training epoch-78 batch-98
Running loss of epoch-78 batch-98 = 1.6549602150917053e-06

Training epoch-78 batch-99
Running loss of epoch-78 batch-99 = 4.287343472242355e-06

Training epoch-78 batch-100
Running loss of epoch-78 batch-100 = 3.210734575986862e-06

Training epoch-78 batch-101
Running loss of epoch-78 batch-101 = 4.77978028357029e-06

Training epoch-78 batch-102
Running loss of epoch-78 batch-102 = 1.1655502021312714e-05

Training epoch-78 batch-103
Running loss of epoch-78 batch-103 = 5.5998098105192184e-06

Training epoch-78 batch-104
Running loss of epoch-78 batch-104 = 7.117167115211487e-06

Training epoch-78 batch-105
Running loss of epoch-78 batch-105 = 3.390014171600342e-06

Training epoch-78 batch-106
Running loss of epoch-78 batch-106 = 1.0710908100008965e-05

Training epoch-78 batch-107
Running loss of epoch-78 batch-107 = 1.8868595361709595e-06

Training epoch-78 batch-108
Running loss of epoch-78 batch-108 = 4.268018528819084e-06

Training epoch-78 batch-109
Running loss of epoch-78 batch-109 = 2.7562491595745087e-06

Training epoch-78 batch-110
Running loss of epoch-78 batch-110 = 1.3408251106739044e-05

Training epoch-78 batch-111
Running loss of epoch-78 batch-111 = 2.427259460091591e-06

Training epoch-78 batch-112
Running loss of epoch-78 batch-112 = 2.5739427655935287e-06

Training epoch-78 batch-113
Running loss of epoch-78 batch-113 = 3.1760428100824356e-06

Training epoch-78 batch-114
Running loss of epoch-78 batch-114 = 8.217757567763329e-06

Training epoch-78 batch-115
Running loss of epoch-78 batch-115 = 5.145557224750519e-06

Training epoch-78 batch-116
Running loss of epoch-78 batch-116 = 1.7171259969472885e-06

Training epoch-78 batch-117
Running loss of epoch-78 batch-117 = 3.673601895570755e-06

Training epoch-78 batch-118
Running loss of epoch-78 batch-118 = 4.520872607827187e-06

Training epoch-78 batch-119
Running loss of epoch-78 batch-119 = 5.318783223628998e-06

Training epoch-78 batch-120
Running loss of epoch-78 batch-120 = 7.818685844540596e-06

Training epoch-78 batch-121
Running loss of epoch-78 batch-121 = 8.153263479471207e-06

Training epoch-78 batch-122
Running loss of epoch-78 batch-122 = 6.971880793571472e-06

Training epoch-78 batch-123
Running loss of epoch-78 batch-123 = 3.984430804848671e-06

Training epoch-78 batch-124
Running loss of epoch-78 batch-124 = 1.18231400847435e-06

Training epoch-78 batch-125
Running loss of epoch-78 batch-125 = 5.198409780859947e-06

Training epoch-78 batch-126
Running loss of epoch-78 batch-126 = 4.852423444390297e-06

Training epoch-78 batch-127
Running loss of epoch-78 batch-127 = 4.2994506657123566e-06

Training epoch-78 batch-128
Running loss of epoch-78 batch-128 = 3.2598618417978287e-06

Training epoch-78 batch-129
Running loss of epoch-78 batch-129 = 1.3909535482525826e-05

Training epoch-78 batch-130
Running loss of epoch-78 batch-130 = 2.780696377158165e-06

Training epoch-78 batch-131
Running loss of epoch-78 batch-131 = 1.6349367797374725e-06

Training epoch-78 batch-132
Running loss of epoch-78 batch-132 = 1.0502524673938751e-05

Training epoch-78 batch-133
Running loss of epoch-78 batch-133 = 3.7364661693573e-06

Training epoch-78 batch-134
Running loss of epoch-78 batch-134 = 6.0962047427892685e-06

Training epoch-78 batch-135
Running loss of epoch-78 batch-135 = 6.005400791764259e-06

Training epoch-78 batch-136
Running loss of epoch-78 batch-136 = 2.5457702577114105e-06

Training epoch-78 batch-137
Running loss of epoch-78 batch-137 = 4.8745423555374146e-06

Training epoch-78 batch-138
Running loss of epoch-78 batch-138 = 2.4603214114904404e-06

Training epoch-78 batch-139
Running loss of epoch-78 batch-139 = 6.117392331361771e-06

Training epoch-78 batch-140
Running loss of epoch-78 batch-140 = 5.959998816251755e-06

Training epoch-78 batch-141
Running loss of epoch-78 batch-141 = 3.962311893701553e-06

Training epoch-78 batch-142
Running loss of epoch-78 batch-142 = 2.4512410163879395e-06

Training epoch-78 batch-143
Running loss of epoch-78 batch-143 = 1.0307412594556808e-06

Training epoch-78 batch-144
Running loss of epoch-78 batch-144 = 6.875256076455116e-06

Training epoch-78 batch-145
Running loss of epoch-78 batch-145 = 2.044718712568283e-06

Training epoch-78 batch-146
Running loss of epoch-78 batch-146 = 2.51084566116333e-06

Training epoch-78 batch-147
Running loss of epoch-78 batch-147 = 2.016546204686165e-06

Training epoch-78 batch-148
Running loss of epoch-78 batch-148 = 3.3436808735132217e-06

Training epoch-78 batch-149
Running loss of epoch-78 batch-149 = 6.901798769831657e-06

Training epoch-78 batch-150
Running loss of epoch-78 batch-150 = 1.1855736374855042e-06

Training epoch-78 batch-151
Running loss of epoch-78 batch-151 = 2.9639340937137604e-06

Training epoch-78 batch-152
Running loss of epoch-78 batch-152 = 5.674315616488457e-06

Training epoch-78 batch-153
Running loss of epoch-78 batch-153 = 1.834006980061531e-06

Training epoch-78 batch-154
Running loss of epoch-78 batch-154 = 4.800967872142792e-06

Training epoch-78 batch-155
Running loss of epoch-78 batch-155 = 1.4540273696184158e-06

Training epoch-78 batch-156
Running loss of epoch-78 batch-156 = 1.7723068594932556e-06

Training epoch-78 batch-157
Running loss of epoch-78 batch-157 = 3.427267074584961e-07

Finished training epoch-78.



Average train loss at epoch-78 = 4.8916354775428775e-06

Started Evaluation

Average val loss at epoch-78 = 1.7963764578788088

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.25 %

Finished Evaluation



Started training epoch-79


Training epoch-79 batch-1
Running loss of epoch-79 batch-1 = 8.150003850460052e-06

Training epoch-79 batch-2
Running loss of epoch-79 batch-2 = 5.049398168921471e-06

Training epoch-79 batch-3
Running loss of epoch-79 batch-3 = 3.427267074584961e-06

Training epoch-79 batch-4
Running loss of epoch-79 batch-4 = 3.0153896659612656e-06

Training epoch-79 batch-5
Running loss of epoch-79 batch-5 = 5.656387656927109e-06

Training epoch-79 batch-6
Running loss of epoch-79 batch-6 = 1.2135598808526993e-05

Training epoch-79 batch-7
Running loss of epoch-79 batch-7 = 1.3348180800676346e-06

Training epoch-79 batch-8
Running loss of epoch-79 batch-8 = 3.8191210478544235e-06

Training epoch-79 batch-9
Running loss of epoch-79 batch-9 = 4.0424056351184845e-06

Training epoch-79 batch-10
Running loss of epoch-79 batch-10 = 3.5944394767284393e-06

Training epoch-79 batch-11
Running loss of epoch-79 batch-11 = 4.450557753443718e-06

Training epoch-79 batch-12
Running loss of epoch-79 batch-12 = 3.621913492679596e-06

Training epoch-79 batch-13
Running loss of epoch-79 batch-13 = 8.222414180636406e-06

Training epoch-79 batch-14
Running loss of epoch-79 batch-14 = 2.198154106736183e-06

Training epoch-79 batch-15
Running loss of epoch-79 batch-15 = 8.210306987166405e-06

Training epoch-79 batch-16
Running loss of epoch-79 batch-16 = 3.2365787774324417e-06

Training epoch-79 batch-17
Running loss of epoch-79 batch-17 = 6.489455699920654e-06

Training epoch-79 batch-18
Running loss of epoch-79 batch-18 = 6.397021934390068e-06

Training epoch-79 batch-19
Running loss of epoch-79 batch-19 = 1.99698843061924e-06

Training epoch-79 batch-20
Running loss of epoch-79 batch-20 = 5.9176236391067505e-06

Training epoch-79 batch-21
Running loss of epoch-79 batch-21 = 3.2365787774324417e-06

Training epoch-79 batch-22
Running loss of epoch-79 batch-22 = 1.0576331987977028e-05

Training epoch-79 batch-23
Running loss of epoch-79 batch-23 = 5.571404471993446e-06

Training epoch-79 batch-24
Running loss of epoch-79 batch-24 = 4.02471050620079e-06

Training epoch-79 batch-25
Running loss of epoch-79 batch-25 = 3.444962203502655e-06

Training epoch-79 batch-26
Running loss of epoch-79 batch-26 = 6.993301212787628e-06

Training epoch-79 batch-27
Running loss of epoch-79 batch-27 = 4.882458597421646e-07

Training epoch-79 batch-28
Running loss of epoch-79 batch-28 = 3.455905243754387e-06

Training epoch-79 batch-29
Running loss of epoch-79 batch-29 = 1.032138243317604e-05

Training epoch-79 batch-30
Running loss of epoch-79 batch-30 = 7.866881787776947e-06

Training epoch-79 batch-31
Running loss of epoch-79 batch-31 = 3.0195806175470352e-06

Training epoch-79 batch-32
Running loss of epoch-79 batch-32 = 3.5157427191734314e-06

Training epoch-79 batch-33
Running loss of epoch-79 batch-33 = 4.875939339399338e-06

Training epoch-79 batch-34
Running loss of epoch-79 batch-34 = 2.6442576199769974e-06

Training epoch-79 batch-35
Running loss of epoch-79 batch-35 = 8.203322067856789e-06

Training epoch-79 batch-36
Running loss of epoch-79 batch-36 = 4.774192348122597e-06

Training epoch-79 batch-37
Running loss of epoch-79 batch-37 = 3.076856955885887e-06

Training epoch-79 batch-38
Running loss of epoch-79 batch-38 = 5.0086528062820435e-06

Training epoch-79 batch-39
Running loss of epoch-79 batch-39 = 3.7241261452436447e-06

Training epoch-79 batch-40
Running loss of epoch-79 batch-40 = 4.898523911833763e-06

Training epoch-79 batch-41
Running loss of epoch-79 batch-41 = 4.393281415104866e-06

Training epoch-79 batch-42
Running loss of epoch-79 batch-42 = 1.90013088285923e-06

Training epoch-79 batch-43
Running loss of epoch-79 batch-43 = 7.867347449064255e-06

Training epoch-79 batch-44
Running loss of epoch-79 batch-44 = 6.479443982243538e-06

Training epoch-79 batch-45
Running loss of epoch-79 batch-45 = 4.530185833573341e-06

Training epoch-79 batch-46
Running loss of epoch-79 batch-46 = 3.6980491131544113e-06

Training epoch-79 batch-47
Running loss of epoch-79 batch-47 = 5.485257133841515e-06

Training epoch-79 batch-48
Running loss of epoch-79 batch-48 = 4.20399010181427e-06

Training epoch-79 batch-49
Running loss of epoch-79 batch-49 = 3.8014259189367294e-06

Training epoch-79 batch-50
Running loss of epoch-79 batch-50 = 4.597241058945656e-06

Training epoch-79 batch-51
Running loss of epoch-79 batch-51 = 5.214940756559372e-06

Training epoch-79 batch-52
Running loss of epoch-79 batch-52 = 1.269858330488205e-06

Training epoch-79 batch-53
Running loss of epoch-79 batch-53 = 2.766260877251625e-06

Training epoch-79 batch-54
Running loss of epoch-79 batch-54 = 1.7243437469005585e-06

Training epoch-79 batch-55
Running loss of epoch-79 batch-55 = 9.021488949656487e-06

Training epoch-79 batch-56
Running loss of epoch-79 batch-56 = 1.866137608885765e-06

Training epoch-79 batch-57
Running loss of epoch-79 batch-57 = 2.8901267796754837e-06

Training epoch-79 batch-58
Running loss of epoch-79 batch-58 = 3.109220415353775e-06

Training epoch-79 batch-59
Running loss of epoch-79 batch-59 = 1.5003606677055359e-06

Training epoch-79 batch-60
Running loss of epoch-79 batch-60 = 4.289206117391586e-06

Training epoch-79 batch-61
Running loss of epoch-79 batch-61 = 4.9390364438295364e-06

Training epoch-79 batch-62
Running loss of epoch-79 batch-62 = 2.789776772260666e-06

Training epoch-79 batch-63
Running loss of epoch-79 batch-63 = 3.6372803151607513e-06

Training epoch-79 batch-64
Running loss of epoch-79 batch-64 = 1.8170103430747986e-06

Training epoch-79 batch-65
Running loss of epoch-79 batch-65 = 2.2975727915763855e-06

Training epoch-79 batch-66
Running loss of epoch-79 batch-66 = 6.041256710886955e-06

Training epoch-79 batch-67
Running loss of epoch-79 batch-67 = 5.665235221385956e-06

Training epoch-79 batch-68
Running loss of epoch-79 batch-68 = 2.246815711259842e-06

Training epoch-79 batch-69
Running loss of epoch-79 batch-69 = 1.4560762792825699e-05

Training epoch-79 batch-70
Running loss of epoch-79 batch-70 = 3.5171397030353546e-06

Training epoch-79 batch-71
Running loss of epoch-79 batch-71 = 5.669659003615379e-06

Training epoch-79 batch-72
Running loss of epoch-79 batch-72 = 1.7075799405574799e-06

Training epoch-79 batch-73
Running loss of epoch-79 batch-73 = 2.7641654014587402e-06

Training epoch-79 batch-74
Running loss of epoch-79 batch-74 = 4.366040229797363e-06

Training epoch-79 batch-75
Running loss of epoch-79 batch-75 = 4.167202860116959e-06

Training epoch-79 batch-76
Running loss of epoch-79 batch-76 = 4.511559382081032e-06

Training epoch-79 batch-77
Running loss of epoch-79 batch-77 = 1.7636921256780624e-06

Training epoch-79 batch-78
Running loss of epoch-79 batch-78 = 1.8419232219457626e-06

Training epoch-79 batch-79
Running loss of epoch-79 batch-79 = 6.258487701416016e-06

Training epoch-79 batch-80
Running loss of epoch-79 batch-80 = 3.4782569855451584e-06

Training epoch-79 batch-81
Running loss of epoch-79 batch-81 = 6.121350452303886e-06

Training epoch-79 batch-82
Running loss of epoch-79 batch-82 = 9.414507076144218e-06

Training epoch-79 batch-83
Running loss of epoch-79 batch-83 = 3.4833792597055435e-06

Training epoch-79 batch-84
Running loss of epoch-79 batch-84 = 2.2833701223134995e-06

Training epoch-79 batch-85
Running loss of epoch-79 batch-85 = 2.4244654923677444e-06

Training epoch-79 batch-86
Running loss of epoch-79 batch-86 = 1.0783318430185318e-05

Training epoch-79 batch-87
Running loss of epoch-79 batch-87 = 2.384185791015625e-06

Training epoch-79 batch-88
Running loss of epoch-79 batch-88 = 9.757699444890022e-06

Training epoch-79 batch-89
Running loss of epoch-79 batch-89 = 9.030569344758987e-06

Training epoch-79 batch-90
Running loss of epoch-79 batch-90 = 9.716488420963287e-06

Training epoch-79 batch-91
Running loss of epoch-79 batch-91 = 2.9157381504774094e-06

Training epoch-79 batch-92
Running loss of epoch-79 batch-92 = 3.0354131013154984e-06

Training epoch-79 batch-93
Running loss of epoch-79 batch-93 = 3.1848903745412827e-06

Training epoch-79 batch-94
Running loss of epoch-79 batch-94 = 1.2586824595928192e-06

Training epoch-79 batch-95
Running loss of epoch-79 batch-95 = 1.228228211402893e-05

Training epoch-79 batch-96
Running loss of epoch-79 batch-96 = 2.4940818548202515e-06

Training epoch-79 batch-97
Running loss of epoch-79 batch-97 = 5.6442804634571075e-06

Training epoch-79 batch-98
Running loss of epoch-79 batch-98 = 2.287793904542923e-06

Training epoch-79 batch-99
Running loss of epoch-79 batch-99 = 2.7066562324762344e-06

Training epoch-79 batch-100
Running loss of epoch-79 batch-100 = 2.5066547095775604e-06

Training epoch-79 batch-101
Running loss of epoch-79 batch-101 = 2.6759225875139236e-06

Training epoch-79 batch-102
Running loss of epoch-79 batch-102 = 4.097586497664452e-06

Training epoch-79 batch-103
Running loss of epoch-79 batch-103 = 3.8009602576494217e-06

Training epoch-79 batch-104
Running loss of epoch-79 batch-104 = 4.980713129043579e-06

Training epoch-79 batch-105
Running loss of epoch-79 batch-105 = 3.5818666219711304e-06

Training epoch-79 batch-106
Running loss of epoch-79 batch-106 = 1.987908035516739e-06

Training epoch-79 batch-107
Running loss of epoch-79 batch-107 = 5.3585972636938095e-06

Training epoch-79 batch-108
Running loss of epoch-79 batch-108 = 2.9900111258029938e-06

Training epoch-79 batch-109
Running loss of epoch-79 batch-109 = 6.353948265314102e-06

Training epoch-79 batch-110
Running loss of epoch-79 batch-110 = 4.768604412674904e-06

Training epoch-79 batch-111
Running loss of epoch-79 batch-111 = 1.312396489083767e-05

Training epoch-79 batch-112
Running loss of epoch-79 batch-112 = 3.6030542105436325e-06

Training epoch-79 batch-113
Running loss of epoch-79 batch-113 = 4.906207323074341e-06

Training epoch-79 batch-114
Running loss of epoch-79 batch-114 = 4.800735041499138e-06

Training epoch-79 batch-115
Running loss of epoch-79 batch-115 = 6.759539246559143e-06

Training epoch-79 batch-116
Running loss of epoch-79 batch-116 = 2.7944333851337433e-06

Training epoch-79 batch-117
Running loss of epoch-79 batch-117 = 5.582813173532486e-06

Training epoch-79 batch-118
Running loss of epoch-79 batch-118 = 8.252449333667755e-06

Training epoch-79 batch-119
Running loss of epoch-79 batch-119 = 5.048699676990509e-06

Training epoch-79 batch-120
Running loss of epoch-79 batch-120 = 4.073837772011757e-06

Training epoch-79 batch-121
Running loss of epoch-79 batch-121 = 7.328810170292854e-06

Training epoch-79 batch-122
Running loss of epoch-79 batch-122 = 1.756683923304081e-05

Training epoch-79 batch-123
Running loss of epoch-79 batch-123 = 1.875218003988266e-06

Training epoch-79 batch-124
Running loss of epoch-79 batch-124 = 1.623528078198433e-06

Training epoch-79 batch-125
Running loss of epoch-79 batch-125 = 2.019805833697319e-06

Training epoch-79 batch-126
Running loss of epoch-79 batch-126 = 3.8763973861932755e-06

Training epoch-79 batch-127
Running loss of epoch-79 batch-127 = 3.886409103870392e-06

Training epoch-79 batch-128
Running loss of epoch-79 batch-128 = 3.019813448190689e-06

Training epoch-79 batch-129
Running loss of epoch-79 batch-129 = 1.993030309677124e-06

Training epoch-79 batch-130
Running loss of epoch-79 batch-130 = 2.675224095582962e-06

Training epoch-79 batch-131
Running loss of epoch-79 batch-131 = 3.369757905602455e-06

Training epoch-79 batch-132
Running loss of epoch-79 batch-132 = 4.431465640664101e-06

Training epoch-79 batch-133
Running loss of epoch-79 batch-133 = 4.8729125410318375e-06

Training epoch-79 batch-134
Running loss of epoch-79 batch-134 = 7.765833288431168e-06

Training epoch-79 batch-135
Running loss of epoch-79 batch-135 = 6.868503987789154e-06

Training epoch-79 batch-136
Running loss of epoch-79 batch-136 = 1.986278221011162e-06

Training epoch-79 batch-137
Running loss of epoch-79 batch-137 = 5.017267540097237e-06

Training epoch-79 batch-138
Running loss of epoch-79 batch-138 = 4.098284989595413e-06

Training epoch-79 batch-139
Running loss of epoch-79 batch-139 = 2.612592652440071e-06

Training epoch-79 batch-140
Running loss of epoch-79 batch-140 = 2.8049107640981674e-06

Training epoch-79 batch-141
Running loss of epoch-79 batch-141 = 6.088754162192345e-06

Training epoch-79 batch-142
Running loss of epoch-79 batch-142 = 3.468478098511696e-06

Training epoch-79 batch-143
Running loss of epoch-79 batch-143 = 1.288391649723053e-05

Training epoch-79 batch-144
Running loss of epoch-79 batch-144 = 7.006805390119553e-06

Training epoch-79 batch-145
Running loss of epoch-79 batch-145 = 5.817972123622894e-06

Training epoch-79 batch-146
Running loss of epoch-79 batch-146 = 3.980472683906555e-06

Training epoch-79 batch-147
Running loss of epoch-79 batch-147 = 3.773253411054611e-06

Training epoch-79 batch-148
Running loss of epoch-79 batch-148 = 2.491055056452751e-06

Training epoch-79 batch-149
Running loss of epoch-79 batch-149 = 1.2640375643968582e-06

Training epoch-79 batch-150
Running loss of epoch-79 batch-150 = 5.809590220451355e-06

Training epoch-79 batch-151
Running loss of epoch-79 batch-151 = 7.045688107609749e-06

Training epoch-79 batch-152
Running loss of epoch-79 batch-152 = 2.3781321942806244e-06

Training epoch-79 batch-153
Running loss of epoch-79 batch-153 = 8.047092705965042e-06

Training epoch-79 batch-154
Running loss of epoch-79 batch-154 = 2.9080547392368317e-06

Training epoch-79 batch-155
Running loss of epoch-79 batch-155 = 1.8063001334667206e-06

Training epoch-79 batch-156
Running loss of epoch-79 batch-156 = 5.620764568448067e-06

Training epoch-79 batch-157
Running loss of epoch-79 batch-157 = 1.5500932931900024e-05

Finished training epoch-79.



Average train loss at epoch-79 = 4.752330482006073e-06

Started Evaluation

Average val loss at epoch-79 = 1.803156402942199

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-80


Training epoch-80 batch-1
Running loss of epoch-80 batch-1 = 5.187699571251869e-06

Training epoch-80 batch-2
Running loss of epoch-80 batch-2 = 4.381407052278519e-06

Training epoch-80 batch-3
Running loss of epoch-80 batch-3 = 3.3744145184755325e-06

Training epoch-80 batch-4
Running loss of epoch-80 batch-4 = 1.5480909496545792e-06

Training epoch-80 batch-5
Running loss of epoch-80 batch-5 = 3.446824848651886e-06

Training epoch-80 batch-6
Running loss of epoch-80 batch-6 = 5.982816219329834e-06

Training epoch-80 batch-7
Running loss of epoch-80 batch-7 = 5.00003807246685e-06

Training epoch-80 batch-8
Running loss of epoch-80 batch-8 = 9.197741746902466e-06

Training epoch-80 batch-9
Running loss of epoch-80 batch-9 = 4.513422027230263e-06

Training epoch-80 batch-10
Running loss of epoch-80 batch-10 = 4.0351878851652145e-06

Training epoch-80 batch-11
Running loss of epoch-80 batch-11 = 4.805158823728561e-06

Training epoch-80 batch-12
Running loss of epoch-80 batch-12 = 2.9141083359718323e-06

Training epoch-80 batch-13
Running loss of epoch-80 batch-13 = 2.4551991373300552e-06

Training epoch-80 batch-14
Running loss of epoch-80 batch-14 = 2.6097986847162247e-06

Training epoch-80 batch-15
Running loss of epoch-80 batch-15 = 2.2368039935827255e-06

Training epoch-80 batch-16
Running loss of epoch-80 batch-16 = 3.641704097390175e-06

Training epoch-80 batch-17
Running loss of epoch-80 batch-17 = 3.489665687084198e-06

Training epoch-80 batch-18
Running loss of epoch-80 batch-18 = 1.4307443052530289e-06

Training epoch-80 batch-19
Running loss of epoch-80 batch-19 = 4.896428436040878e-06

Training epoch-80 batch-20
Running loss of epoch-80 batch-20 = 1.618172973394394e-06

Training epoch-80 batch-21
Running loss of epoch-80 batch-21 = 2.7960631996393204e-06

Training epoch-80 batch-22
Running loss of epoch-80 batch-22 = 5.553127266466618e-06

Training epoch-80 batch-23
Running loss of epoch-80 batch-23 = 3.4079421311616898e-06

Training epoch-80 batch-24
Running loss of epoch-80 batch-24 = 5.369773134589195e-06

Training epoch-80 batch-25
Running loss of epoch-80 batch-25 = 2.660788595676422e-06

Training epoch-80 batch-26
Running loss of epoch-80 batch-26 = 6.841961294412613e-06

Training epoch-80 batch-27
Running loss of epoch-80 batch-27 = 5.402602255344391e-06

Training epoch-80 batch-28
Running loss of epoch-80 batch-28 = 1.6351696103811264e-06

Training epoch-80 batch-29
Running loss of epoch-80 batch-29 = 4.676170647144318e-06

Training epoch-80 batch-30
Running loss of epoch-80 batch-30 = 3.3383257687091827e-06

Training epoch-80 batch-31
Running loss of epoch-80 batch-31 = 2.4887267500162125e-06

Training epoch-80 batch-32
Running loss of epoch-80 batch-32 = 3.9013102650642395e-06

Training epoch-80 batch-33
Running loss of epoch-80 batch-33 = 2.1473970264196396e-06

Training epoch-80 batch-34
Running loss of epoch-80 batch-34 = 2.616085112094879e-06

Training epoch-80 batch-35
Running loss of epoch-80 batch-35 = 5.036825314164162e-06

Training epoch-80 batch-36
Running loss of epoch-80 batch-36 = 2.6389025151729584e-06

Training epoch-80 batch-37
Running loss of epoch-80 batch-37 = 2.644956111907959e-06

Training epoch-80 batch-38
Running loss of epoch-80 batch-38 = 1.3639219105243683e-06

Training epoch-80 batch-39
Running loss of epoch-80 batch-39 = 3.0763912945985794e-06

Training epoch-80 batch-40
Running loss of epoch-80 batch-40 = 2.2149179130792618e-06

Training epoch-80 batch-41
Running loss of epoch-80 batch-41 = 7.367227226495743e-06

Training epoch-80 batch-42
Running loss of epoch-80 batch-42 = 6.420537829399109e-06

Training epoch-80 batch-43
Running loss of epoch-80 batch-43 = 2.2586900740861893e-06

Training epoch-80 batch-44
Running loss of epoch-80 batch-44 = 3.177206963300705e-06

Training epoch-80 batch-45
Running loss of epoch-80 batch-45 = 1.2989155948162079e-05

Training epoch-80 batch-46
Running loss of epoch-80 batch-46 = 2.223532646894455e-06

Training epoch-80 batch-47
Running loss of epoch-80 batch-47 = 3.5527627915143967e-06

Training epoch-80 batch-48
Running loss of epoch-80 batch-48 = 5.615409463644028e-06

Training epoch-80 batch-49
Running loss of epoch-80 batch-49 = 6.440328434109688e-06

Training epoch-80 batch-50
Running loss of epoch-80 batch-50 = 5.629146471619606e-06

Training epoch-80 batch-51
Running loss of epoch-80 batch-51 = 2.955552190542221e-06

Training epoch-80 batch-52
Running loss of epoch-80 batch-52 = 4.5227352529764175e-06

Training epoch-80 batch-53
Running loss of epoch-80 batch-53 = 3.4831464290618896e-06

Training epoch-80 batch-54
Running loss of epoch-80 batch-54 = 1.6088597476482391e-06

Training epoch-80 batch-55
Running loss of epoch-80 batch-55 = 4.019355401396751e-06

Training epoch-80 batch-56
Running loss of epoch-80 batch-56 = 5.2694231271743774e-06

Training epoch-80 batch-57
Running loss of epoch-80 batch-57 = 5.1138922572135925e-06

Training epoch-80 batch-58
Running loss of epoch-80 batch-58 = 5.910638719797134e-06

Training epoch-80 batch-59
Running loss of epoch-80 batch-59 = 1.2538628652691841e-05

Training epoch-80 batch-60
Running loss of epoch-80 batch-60 = 6.241025403141975e-06

Training epoch-80 batch-61
Running loss of epoch-80 batch-61 = 4.877569153904915e-06

Training epoch-80 batch-62
Running loss of epoch-80 batch-62 = 6.865710020065308e-06

Training epoch-80 batch-63
Running loss of epoch-80 batch-63 = 2.5331974029541016e-06

Training epoch-80 batch-64
Running loss of epoch-80 batch-64 = 6.03613443672657e-06

Training epoch-80 batch-65
Running loss of epoch-80 batch-65 = 3.8619618862867355e-06

Training epoch-80 batch-66
Running loss of epoch-80 batch-66 = 2.4030450731515884e-06

Training epoch-80 batch-67
Running loss of epoch-80 batch-67 = 1.5266705304384232e-06

Training epoch-80 batch-68
Running loss of epoch-80 batch-68 = 2.269633114337921e-06

Training epoch-80 batch-69
Running loss of epoch-80 batch-69 = 4.0496233850717545e-06

Training epoch-80 batch-70
Running loss of epoch-80 batch-70 = 1.9143568351864815e-05

Training epoch-80 batch-71
Running loss of epoch-80 batch-71 = 2.887798473238945e-06

Training epoch-80 batch-72
Running loss of epoch-80 batch-72 = 1.8472783267498016e-06

Training epoch-80 batch-73
Running loss of epoch-80 batch-73 = 2.523651346564293e-06

Training epoch-80 batch-74
Running loss of epoch-80 batch-74 = 5.800975486636162e-06

Training epoch-80 batch-75
Running loss of epoch-80 batch-75 = 5.129724740982056e-06

Training epoch-80 batch-76
Running loss of epoch-80 batch-76 = 4.194444045424461e-06

Training epoch-80 batch-77
Running loss of epoch-80 batch-77 = 1.6337726265192032e-06

Training epoch-80 batch-78
Running loss of epoch-80 batch-78 = 4.529254510998726e-06

Training epoch-80 batch-79
Running loss of epoch-80 batch-79 = 4.185596480965614e-06

Training epoch-80 batch-80
Running loss of epoch-80 batch-80 = 3.7569552659988403e-06

Training epoch-80 batch-81
Running loss of epoch-80 batch-81 = 1.1790543794631958e-05

Training epoch-80 batch-82
Running loss of epoch-80 batch-82 = 7.357914000749588e-06

Training epoch-80 batch-83
Running loss of epoch-80 batch-83 = 4.911329597234726e-06

Training epoch-80 batch-84
Running loss of epoch-80 batch-84 = 3.3264514058828354e-06

Training epoch-80 batch-85
Running loss of epoch-80 batch-85 = 4.127621650695801e-06

Training epoch-80 batch-86
Running loss of epoch-80 batch-86 = 2.6945490390062332e-06

Training epoch-80 batch-87
Running loss of epoch-80 batch-87 = 3.635883331298828e-06

Training epoch-80 batch-88
Running loss of epoch-80 batch-88 = 4.001893103122711e-06

Training epoch-80 batch-89
Running loss of epoch-80 batch-89 = 4.123197868466377e-06

Training epoch-80 batch-90
Running loss of epoch-80 batch-90 = 1.5251338481903076e-05

Training epoch-80 batch-91
Running loss of epoch-80 batch-91 = 1.3805227354168892e-05

Training epoch-80 batch-92
Running loss of epoch-80 batch-92 = 4.091300070285797e-06

Training epoch-80 batch-93
Running loss of epoch-80 batch-93 = 5.681533366441727e-06

Training epoch-80 batch-94
Running loss of epoch-80 batch-94 = 5.024950951337814e-06

Training epoch-80 batch-95
Running loss of epoch-80 batch-95 = 3.664987161755562e-06

Training epoch-80 batch-96
Running loss of epoch-80 batch-96 = 3.0905939638614655e-06

Training epoch-80 batch-97
Running loss of epoch-80 batch-97 = 2.850312739610672e-06

Training epoch-80 batch-98
Running loss of epoch-80 batch-98 = 3.1727831810712814e-06

Training epoch-80 batch-99
Running loss of epoch-80 batch-99 = 3.139721229672432e-06

Training epoch-80 batch-100
Running loss of epoch-80 batch-100 = 1.1994503438472748e-05

Training epoch-80 batch-101
Running loss of epoch-80 batch-101 = 9.829644113779068e-06

Training epoch-80 batch-102
Running loss of epoch-80 batch-102 = 2.023531123995781e-06

Training epoch-80 batch-103
Running loss of epoch-80 batch-103 = 3.0943192541599274e-06

Training epoch-80 batch-104
Running loss of epoch-80 batch-104 = 1.1755852028727531e-05

Training epoch-80 batch-105
Running loss of epoch-80 batch-105 = 3.076856955885887e-06

Training epoch-80 batch-106
Running loss of epoch-80 batch-106 = 5.898764356970787e-06

Training epoch-80 batch-107
Running loss of epoch-80 batch-107 = 2.1765008568763733e-06

Training epoch-80 batch-108
Running loss of epoch-80 batch-108 = 2.9865186661481857e-06

Training epoch-80 batch-109
Running loss of epoch-80 batch-109 = 2.9972288757562637e-06

Training epoch-80 batch-110
Running loss of epoch-80 batch-110 = 6.477115675806999e-06

Training epoch-80 batch-111
Running loss of epoch-80 batch-111 = 5.094567313790321e-06

Training epoch-80 batch-112
Running loss of epoch-80 batch-112 = 2.155313268303871e-06

Training epoch-80 batch-113
Running loss of epoch-80 batch-113 = 2.4060718715190887e-06

Training epoch-80 batch-114
Running loss of epoch-80 batch-114 = 4.6407803893089294e-06

Training epoch-80 batch-115
Running loss of epoch-80 batch-115 = 9.17818397283554e-06

Training epoch-80 batch-116
Running loss of epoch-80 batch-116 = 6.8801455199718475e-06

Training epoch-80 batch-117
Running loss of epoch-80 batch-117 = 5.678972229361534e-06

Training epoch-80 batch-118
Running loss of epoch-80 batch-118 = 5.440553650259972e-06

Training epoch-80 batch-119
Running loss of epoch-80 batch-119 = 3.915047273039818e-06

Training epoch-80 batch-120
Running loss of epoch-80 batch-120 = 3.502471372485161e-06

Training epoch-80 batch-121
Running loss of epoch-80 batch-121 = 2.7776695787906647e-06

Training epoch-80 batch-122
Running loss of epoch-80 batch-122 = 4.610046744346619e-06

Training epoch-80 batch-123
Running loss of epoch-80 batch-123 = 5.022156983613968e-06

Training epoch-80 batch-124
Running loss of epoch-80 batch-124 = 6.145564839243889e-06

Training epoch-80 batch-125
Running loss of epoch-80 batch-125 = 9.508104994893074e-06

Training epoch-80 batch-126
Running loss of epoch-80 batch-126 = 1.3813376426696777e-05

Training epoch-80 batch-127
Running loss of epoch-80 batch-127 = 3.566266968846321e-06

Training epoch-80 batch-128
Running loss of epoch-80 batch-128 = 3.7818681448698044e-06

Training epoch-80 batch-129
Running loss of epoch-80 batch-129 = 8.664093911647797e-06

Training epoch-80 batch-130
Running loss of epoch-80 batch-130 = 2.0829029381275177e-06

Training epoch-80 batch-131
Running loss of epoch-80 batch-131 = 2.561137080192566e-06

Training epoch-80 batch-132
Running loss of epoch-80 batch-132 = 6.837770342826843e-06

Training epoch-80 batch-133
Running loss of epoch-80 batch-133 = 4.582572728395462e-06

Training epoch-80 batch-134
Running loss of epoch-80 batch-134 = 6.227521225810051e-06

Training epoch-80 batch-135
Running loss of epoch-80 batch-135 = 3.1248200684785843e-06

Training epoch-80 batch-136
Running loss of epoch-80 batch-136 = 4.15463000535965e-06

Training epoch-80 batch-137
Running loss of epoch-80 batch-137 = 2.9047951102256775e-06

Training epoch-80 batch-138
Running loss of epoch-80 batch-138 = 1.8184073269367218e-06

Training epoch-80 batch-139
Running loss of epoch-80 batch-139 = 5.587935447692871e-06

Training epoch-80 batch-140
Running loss of epoch-80 batch-140 = 6.682239472866058e-07

Training epoch-80 batch-141
Running loss of epoch-80 batch-141 = 3.160908818244934e-06

Training epoch-80 batch-142
Running loss of epoch-80 batch-142 = 1.630512997508049e-06

Training epoch-80 batch-143
Running loss of epoch-80 batch-143 = 4.230532795190811e-06

Training epoch-80 batch-144
Running loss of epoch-80 batch-144 = 3.1096860766410828e-06

Training epoch-80 batch-145
Running loss of epoch-80 batch-145 = 1.6491394490003586e-06

Training epoch-80 batch-146
Running loss of epoch-80 batch-146 = 4.81470488011837e-06

Training epoch-80 batch-147
Running loss of epoch-80 batch-147 = 1.8612481653690338e-06

Training epoch-80 batch-148
Running loss of epoch-80 batch-148 = 6.11855648458004e-06

Training epoch-80 batch-149
Running loss of epoch-80 batch-149 = 3.230525180697441e-06

Training epoch-80 batch-150
Running loss of epoch-80 batch-150 = 4.323897883296013e-06

Training epoch-80 batch-151
Running loss of epoch-80 batch-151 = 6.992369890213013e-06

Training epoch-80 batch-152
Running loss of epoch-80 batch-152 = 6.7462678998708725e-06

Training epoch-80 batch-153
Running loss of epoch-80 batch-153 = 3.1508971005678177e-06

Training epoch-80 batch-154
Running loss of epoch-80 batch-154 = 1.142965629696846e-05

Training epoch-80 batch-155
Running loss of epoch-80 batch-155 = 6.3730403780937195e-06

Training epoch-80 batch-156
Running loss of epoch-80 batch-156 = 2.8810463845729828e-06

Training epoch-80 batch-157
Running loss of epoch-80 batch-157 = 5.211681127548218e-06

Finished training epoch-80.



Average train loss at epoch-80 = 4.68711331486702e-06

Started Evaluation

Average val loss at epoch-80 = 1.8014770620745733

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 89.02 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.27 %

Finished Evaluation



Started training epoch-81


Training epoch-81 batch-1
Running loss of epoch-81 batch-1 = 4.4512562453746796e-06

Training epoch-81 batch-2
Running loss of epoch-81 batch-2 = 6.393296644091606e-06

Training epoch-81 batch-3
Running loss of epoch-81 batch-3 = 1.8100254237651825e-06

Training epoch-81 batch-4
Running loss of epoch-81 batch-4 = 4.794681444764137e-06

Training epoch-81 batch-5
Running loss of epoch-81 batch-5 = 5.673849955201149e-06

Training epoch-81 batch-6
Running loss of epoch-81 batch-6 = 5.991198122501373e-06

Training epoch-81 batch-7
Running loss of epoch-81 batch-7 = 2.6649795472621918e-06

Training epoch-81 batch-8
Running loss of epoch-81 batch-8 = 6.219604983925819e-06

Training epoch-81 batch-9
Running loss of epoch-81 batch-9 = 4.121335223317146e-06

Training epoch-81 batch-10
Running loss of epoch-81 batch-10 = 4.817266017198563e-06

Training epoch-81 batch-11
Running loss of epoch-81 batch-11 = 1.2611271813511848e-05

Training epoch-81 batch-12
Running loss of epoch-81 batch-12 = 6.299931555986404e-06

Training epoch-81 batch-13
Running loss of epoch-81 batch-13 = 1.0407762601971626e-05

Training epoch-81 batch-14
Running loss of epoch-81 batch-14 = 4.027737304568291e-06

Training epoch-81 batch-15
Running loss of epoch-81 batch-15 = 6.36163167655468e-06

Training epoch-81 batch-16
Running loss of epoch-81 batch-16 = 3.894791007041931e-06

Training epoch-81 batch-17
Running loss of epoch-81 batch-17 = 1.1480879038572311e-06

Training epoch-81 batch-18
Running loss of epoch-81 batch-18 = 1.0797521099448204e-05

Training epoch-81 batch-19
Running loss of epoch-81 batch-19 = 2.616550773382187e-06

Training epoch-81 batch-20
Running loss of epoch-81 batch-20 = 3.3420510590076447e-06

Training epoch-81 batch-21
Running loss of epoch-81 batch-21 = 7.592607289552689e-06

Training epoch-81 batch-22
Running loss of epoch-81 batch-22 = 3.8549769669771194e-06

Training epoch-81 batch-23
Running loss of epoch-81 batch-23 = 1.2150267139077187e-05

Training epoch-81 batch-24
Running loss of epoch-81 batch-24 = 2.028420567512512e-06

Training epoch-81 batch-25
Running loss of epoch-81 batch-25 = 9.740935638546944e-06

Training epoch-81 batch-26
Running loss of epoch-81 batch-26 = 3.807246685028076e-06

Training epoch-81 batch-27
Running loss of epoch-81 batch-27 = 4.049157723784447e-06

Training epoch-81 batch-28
Running loss of epoch-81 batch-28 = 4.251021891832352e-06

Training epoch-81 batch-29
Running loss of epoch-81 batch-29 = 3.998633474111557e-06

Training epoch-81 batch-30
Running loss of epoch-81 batch-30 = 2.512708306312561e-06

Training epoch-81 batch-31
Running loss of epoch-81 batch-31 = 5.6067947298288345e-06

Training epoch-81 batch-32
Running loss of epoch-81 batch-32 = 7.5874850153923035e-06

Training epoch-81 batch-33
Running loss of epoch-81 batch-33 = 2.1455343812704086e-06

Training epoch-81 batch-34
Running loss of epoch-81 batch-34 = 4.988396540284157e-06

Training epoch-81 batch-35
Running loss of epoch-81 batch-35 = 1.155538484454155e-05

Training epoch-81 batch-36
Running loss of epoch-81 batch-36 = 3.880355507135391e-06

Training epoch-81 batch-37
Running loss of epoch-81 batch-37 = 6.721122190356255e-06

Training epoch-81 batch-38
Running loss of epoch-81 batch-38 = 3.341352567076683e-06

Training epoch-81 batch-39
Running loss of epoch-81 batch-39 = 4.039611667394638e-06

Training epoch-81 batch-40
Running loss of epoch-81 batch-40 = 4.084780812263489e-06

Training epoch-81 batch-41
Running loss of epoch-81 batch-41 = 5.628447979688644e-06

Training epoch-81 batch-42
Running loss of epoch-81 batch-42 = 2.4603214114904404e-06

Training epoch-81 batch-43
Running loss of epoch-81 batch-43 = 3.647059202194214e-06

Training epoch-81 batch-44
Running loss of epoch-81 batch-44 = 4.127854481339455e-06

Training epoch-81 batch-45
Running loss of epoch-81 batch-45 = 1.5061814337968826e-06

Training epoch-81 batch-46
Running loss of epoch-81 batch-46 = 2.5150366127490997e-06

Training epoch-81 batch-47
Running loss of epoch-81 batch-47 = 2.648448571562767e-06

Training epoch-81 batch-48
Running loss of epoch-81 batch-48 = 2.386048436164856e-06

Training epoch-81 batch-49
Running loss of epoch-81 batch-49 = 4.055676981806755e-06

Training epoch-81 batch-50
Running loss of epoch-81 batch-50 = 2.014683559536934e-06

Training epoch-81 batch-51
Running loss of epoch-81 batch-51 = 1.1457595974206924e-06

Training epoch-81 batch-52
Running loss of epoch-81 batch-52 = 2.8847716748714447e-06

Training epoch-81 batch-53
Running loss of epoch-81 batch-53 = 4.626112058758736e-06

Training epoch-81 batch-54
Running loss of epoch-81 batch-54 = 6.473390385508537e-06

Training epoch-81 batch-55
Running loss of epoch-81 batch-55 = 1.4100223779678345e-06

Training epoch-81 batch-56
Running loss of epoch-81 batch-56 = 3.293389454483986e-06

Training epoch-81 batch-57
Running loss of epoch-81 batch-57 = 3.1837262213230133e-06

Training epoch-81 batch-58
Running loss of epoch-81 batch-58 = 1.1524418368935585e-05

Training epoch-81 batch-59
Running loss of epoch-81 batch-59 = 3.743218258023262e-06

Training epoch-81 batch-60
Running loss of epoch-81 batch-60 = 1.4253892004489899e-06

Training epoch-81 batch-61
Running loss of epoch-81 batch-61 = 4.024943336844444e-06

Training epoch-81 batch-62
Running loss of epoch-81 batch-62 = 8.482951670885086e-06

Training epoch-81 batch-63
Running loss of epoch-81 batch-63 = 2.261251211166382e-06

Training epoch-81 batch-64
Running loss of epoch-81 batch-64 = 4.754168912768364e-06

Training epoch-81 batch-65
Running loss of epoch-81 batch-65 = 4.614703357219696e-06

Training epoch-81 batch-66
Running loss of epoch-81 batch-66 = 4.335073754191399e-06

Training epoch-81 batch-67
Running loss of epoch-81 batch-67 = 1.9832514226436615e-06

Training epoch-81 batch-68
Running loss of epoch-81 batch-68 = 3.284076228737831e-06

Training epoch-81 batch-69
Running loss of epoch-81 batch-69 = 2.4668406695127487e-06

Training epoch-81 batch-70
Running loss of epoch-81 batch-70 = 1.8603168427944183e-06

Training epoch-81 batch-71
Running loss of epoch-81 batch-71 = 4.462432116270065e-06

Training epoch-81 batch-72
Running loss of epoch-81 batch-72 = 2.430984750390053e-06

Training epoch-81 batch-73
Running loss of epoch-81 batch-73 = 3.0882656574249268e-06

Training epoch-81 batch-74
Running loss of epoch-81 batch-74 = 4.567205905914307e-06

Training epoch-81 batch-75
Running loss of epoch-81 batch-75 = 5.3336843848228455e-06

Training epoch-81 batch-76
Running loss of epoch-81 batch-76 = 3.9585866034030914e-06

Training epoch-81 batch-77
Running loss of epoch-81 batch-77 = 3.777211531996727e-06

Training epoch-81 batch-78
Running loss of epoch-81 batch-78 = 5.541834980249405e-06

Training epoch-81 batch-79
Running loss of epoch-81 batch-79 = 2.684537321329117e-06

Training epoch-81 batch-80
Running loss of epoch-81 batch-80 = 1.7560087144374847e-06

Training epoch-81 batch-81
Running loss of epoch-81 batch-81 = 4.0959566831588745e-06

Training epoch-81 batch-82
Running loss of epoch-81 batch-82 = 3.2347161322832108e-06

Training epoch-81 batch-83
Running loss of epoch-81 batch-83 = 2.7117785066366196e-06

Training epoch-81 batch-84
Running loss of epoch-81 batch-84 = 3.5802368074655533e-06

Training epoch-81 batch-85
Running loss of epoch-81 batch-85 = 8.038245141506195e-06

Training epoch-81 batch-86
Running loss of epoch-81 batch-86 = 3.4347176551818848e-06

Training epoch-81 batch-87
Running loss of epoch-81 batch-87 = 3.2440293580293655e-06

Training epoch-81 batch-88
Running loss of epoch-81 batch-88 = 4.441710188984871e-06

Training epoch-81 batch-89
Running loss of epoch-81 batch-89 = 2.691056579351425e-06

Training epoch-81 batch-90
Running loss of epoch-81 batch-90 = 6.268266588449478e-06

Training epoch-81 batch-91
Running loss of epoch-81 batch-91 = 4.455912858247757e-06

Training epoch-81 batch-92
Running loss of epoch-81 batch-92 = 1.7265789210796356e-05

Training epoch-81 batch-93
Running loss of epoch-81 batch-93 = 6.932299584150314e-06

Training epoch-81 batch-94
Running loss of epoch-81 batch-94 = 4.754168912768364e-06

Training epoch-81 batch-95
Running loss of epoch-81 batch-95 = 3.246590495109558e-06

Training epoch-81 batch-96
Running loss of epoch-81 batch-96 = 2.3997854441404343e-06

Training epoch-81 batch-97
Running loss of epoch-81 batch-97 = 4.689674824476242e-06

Training epoch-81 batch-98
Running loss of epoch-81 batch-98 = 5.721580237150192e-06

Training epoch-81 batch-99
Running loss of epoch-81 batch-99 = 3.5297125577926636e-06

Training epoch-81 batch-100
Running loss of epoch-81 batch-100 = 2.6747584342956543e-06

Training epoch-81 batch-101
Running loss of epoch-81 batch-101 = 4.41819429397583e-06

Training epoch-81 batch-102
Running loss of epoch-81 batch-102 = 5.241250619292259e-06

Training epoch-81 batch-103
Running loss of epoch-81 batch-103 = 3.188150003552437e-06

Training epoch-81 batch-104
Running loss of epoch-81 batch-104 = 3.2617244869470596e-06

Training epoch-81 batch-105
Running loss of epoch-81 batch-105 = 1.4628749340772629e-05

Training epoch-81 batch-106
Running loss of epoch-81 batch-106 = 2.398388460278511e-06

Training epoch-81 batch-107
Running loss of epoch-81 batch-107 = 4.66848723590374e-06

Training epoch-81 batch-108
Running loss of epoch-81 batch-108 = 2.2831372916698456e-06

Training epoch-81 batch-109
Running loss of epoch-81 batch-109 = 1.0854564607143402e-06

Training epoch-81 batch-110
Running loss of epoch-81 batch-110 = 4.579080268740654e-06

Training epoch-81 batch-111
Running loss of epoch-81 batch-111 = 4.349742084741592e-06

Training epoch-81 batch-112
Running loss of epoch-81 batch-112 = 3.3427495509386063e-06

Training epoch-81 batch-113
Running loss of epoch-81 batch-113 = 6.576068699359894e-06

Training epoch-81 batch-114
Running loss of epoch-81 batch-114 = 2.8221402317285538e-06

Training epoch-81 batch-115
Running loss of epoch-81 batch-115 = 3.3762771636247635e-06

Training epoch-81 batch-116
Running loss of epoch-81 batch-116 = 4.873611032962799e-06

Training epoch-81 batch-117
Running loss of epoch-81 batch-117 = 3.1620729714632034e-06

Training epoch-81 batch-118
Running loss of epoch-81 batch-118 = 2.8137583285570145e-06

Training epoch-81 batch-119
Running loss of epoch-81 batch-119 = 2.5562476366758347e-06

Training epoch-81 batch-120
Running loss of epoch-81 batch-120 = 4.3942127376794815e-06

Training epoch-81 batch-121
Running loss of epoch-81 batch-121 = 3.7865247577428818e-06

Training epoch-81 batch-122
Running loss of epoch-81 batch-122 = 3.847992047667503e-06

Training epoch-81 batch-123
Running loss of epoch-81 batch-123 = 3.10409814119339e-06

Training epoch-81 batch-124
Running loss of epoch-81 batch-124 = 1.5972182154655457e-06

Training epoch-81 batch-125
Running loss of epoch-81 batch-125 = 1.9024591892957687e-06

Training epoch-81 batch-126
Running loss of epoch-81 batch-126 = 6.027286872267723e-06

Training epoch-81 batch-127
Running loss of epoch-81 batch-127 = 4.656612873077393e-06

Training epoch-81 batch-128
Running loss of epoch-81 batch-128 = 6.462913006544113e-06

Training epoch-81 batch-129
Running loss of epoch-81 batch-129 = 1.1304393410682678e-05

Training epoch-81 batch-130
Running loss of epoch-81 batch-130 = 4.66848723590374e-06

Training epoch-81 batch-131
Running loss of epoch-81 batch-131 = 2.876855432987213e-06

Training epoch-81 batch-132
Running loss of epoch-81 batch-132 = 3.6901328712701797e-06

Training epoch-81 batch-133
Running loss of epoch-81 batch-133 = 4.990026354789734e-06

Training epoch-81 batch-134
Running loss of epoch-81 batch-134 = 2.173474058508873e-06

Training epoch-81 batch-135
Running loss of epoch-81 batch-135 = 5.068723112344742e-06

Training epoch-81 batch-136
Running loss of epoch-81 batch-136 = 6.045214831829071e-06

Training epoch-81 batch-137
Running loss of epoch-81 batch-137 = 6.327871233224869e-06

Training epoch-81 batch-138
Running loss of epoch-81 batch-138 = 1.7837155610322952e-06

Training epoch-81 batch-139
Running loss of epoch-81 batch-139 = 4.226109012961388e-06

Training epoch-81 batch-140
Running loss of epoch-81 batch-140 = 4.4691842049360275e-06

Training epoch-81 batch-141
Running loss of epoch-81 batch-141 = 6.261048838496208e-06

Training epoch-81 batch-142
Running loss of epoch-81 batch-142 = 2.55717895925045e-06

Training epoch-81 batch-143
Running loss of epoch-81 batch-143 = 2.4917535483837128e-06

Training epoch-81 batch-144
Running loss of epoch-81 batch-144 = 2.496642991900444e-06

Training epoch-81 batch-145
Running loss of epoch-81 batch-145 = 5.576061084866524e-06

Training epoch-81 batch-146
Running loss of epoch-81 batch-146 = 1.3031531125307083e-06

Training epoch-81 batch-147
Running loss of epoch-81 batch-147 = 3.509223461151123e-06

Training epoch-81 batch-148
Running loss of epoch-81 batch-148 = 5.125068128108978e-06

Training epoch-81 batch-149
Running loss of epoch-81 batch-149 = 2.7173664420843124e-06

Training epoch-81 batch-150
Running loss of epoch-81 batch-150 = 5.267327651381493e-06

Training epoch-81 batch-151
Running loss of epoch-81 batch-151 = 3.3820979297161102e-06

Training epoch-81 batch-152
Running loss of epoch-81 batch-152 = 1.8881401047110558e-05

Training epoch-81 batch-153
Running loss of epoch-81 batch-153 = 3.6400742828845978e-06

Training epoch-81 batch-154
Running loss of epoch-81 batch-154 = 4.737870767712593e-06

Training epoch-81 batch-155
Running loss of epoch-81 batch-155 = 5.398411303758621e-06

Training epoch-81 batch-156
Running loss of epoch-81 batch-156 = 2.3189932107925415e-06

Training epoch-81 batch-157
Running loss of epoch-81 batch-157 = 6.645545363426208e-05

Finished training epoch-81.



Average train loss at epoch-81 = 4.642990231513977e-06

Started Evaluation

Average val loss at epoch-81 = 1.7985196005992126

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-82


Training epoch-82 batch-1
Running loss of epoch-82 batch-1 = 1.3916287571191788e-06

Training epoch-82 batch-2
Running loss of epoch-82 batch-2 = 2.6065390557050705e-06

Training epoch-82 batch-3
Running loss of epoch-82 batch-3 = 4.924368113279343e-06

Training epoch-82 batch-4
Running loss of epoch-82 batch-4 = 7.354654371738434e-06

Training epoch-82 batch-5
Running loss of epoch-82 batch-5 = 1.0504154488444328e-05

Training epoch-82 batch-6
Running loss of epoch-82 batch-6 = 2.3294705897569656e-06

Training epoch-82 batch-7
Running loss of epoch-82 batch-7 = 2.6442576199769974e-06

Training epoch-82 batch-8
Running loss of epoch-82 batch-8 = 3.983033820986748e-06

Training epoch-82 batch-9
Running loss of epoch-82 batch-9 = 3.964640200138092e-06

Training epoch-82 batch-10
Running loss of epoch-82 batch-10 = 4.16790135204792e-06

Training epoch-82 batch-11
Running loss of epoch-82 batch-11 = 3.7511344999074936e-06

Training epoch-82 batch-12
Running loss of epoch-82 batch-12 = 1.2451782822608948e-06

Training epoch-82 batch-13
Running loss of epoch-82 batch-13 = 3.840774297714233e-06

Training epoch-82 batch-14
Running loss of epoch-82 batch-14 = 6.906455382704735e-06

Training epoch-82 batch-15
Running loss of epoch-82 batch-15 = 2.2104941308498383e-06

Training epoch-82 batch-16
Running loss of epoch-82 batch-16 = 8.370960131287575e-06

Training epoch-82 batch-17
Running loss of epoch-82 batch-17 = 2.455431967973709e-06

Training epoch-82 batch-18
Running loss of epoch-82 batch-18 = 1.6137491911649704e-06

Training epoch-82 batch-19
Running loss of epoch-82 batch-19 = 7.00424425303936e-06

Training epoch-82 batch-20
Running loss of epoch-82 batch-20 = 1.7262063920497894e-06

Training epoch-82 batch-21
Running loss of epoch-82 batch-21 = 3.6035198718309402e-06

Training epoch-82 batch-22
Running loss of epoch-82 batch-22 = 4.98676672577858e-06

Training epoch-82 batch-23
Running loss of epoch-82 batch-23 = 1.714564859867096e-06

Training epoch-82 batch-24
Running loss of epoch-82 batch-24 = 3.840308636426926e-06

Training epoch-82 batch-25
Running loss of epoch-82 batch-25 = 2.9753427952528e-06

Training epoch-82 batch-26
Running loss of epoch-82 batch-26 = 4.412606358528137e-06

Training epoch-82 batch-27
Running loss of epoch-82 batch-27 = 3.496883437037468e-06

Training epoch-82 batch-28
Running loss of epoch-82 batch-28 = 9.035691618919373e-06

Training epoch-82 batch-29
Running loss of epoch-82 batch-29 = 7.029622793197632e-06

Training epoch-82 batch-30
Running loss of epoch-82 batch-30 = 3.2798852771520615e-06

Training epoch-82 batch-31
Running loss of epoch-82 batch-31 = 5.231006070971489e-06

Training epoch-82 batch-32
Running loss of epoch-82 batch-32 = 1.5757977962493896e-06

Training epoch-82 batch-33
Running loss of epoch-82 batch-33 = 1.7790822312235832e-05

Training epoch-82 batch-34
Running loss of epoch-82 batch-34 = 8.747098036110401e-06

Training epoch-82 batch-35
Running loss of epoch-82 batch-35 = 5.338340997695923e-06

Training epoch-82 batch-36
Running loss of epoch-82 batch-36 = 3.763241693377495e-06

Training epoch-82 batch-37
Running loss of epoch-82 batch-37 = 6.4354389905929565e-06

Training epoch-82 batch-38
Running loss of epoch-82 batch-38 = 4.038680344820023e-06

Training epoch-82 batch-39
Running loss of epoch-82 batch-39 = 9.332550689578056e-06

Training epoch-82 batch-40
Running loss of epoch-82 batch-40 = 4.1185412555933e-06

Training epoch-82 batch-41
Running loss of epoch-82 batch-41 = 5.60772605240345e-06

Training epoch-82 batch-42
Running loss of epoch-82 batch-42 = 3.871740773320198e-06

Training epoch-82 batch-43
Running loss of epoch-82 batch-43 = 4.375120624899864e-06

Training epoch-82 batch-44
Running loss of epoch-82 batch-44 = 3.0228402465581894e-06

Training epoch-82 batch-45
Running loss of epoch-82 batch-45 = 2.559507265686989e-06

Training epoch-82 batch-46
Running loss of epoch-82 batch-46 = 2.605840563774109e-06

Training epoch-82 batch-47
Running loss of epoch-82 batch-47 = 2.9194634407758713e-06

Training epoch-82 batch-48
Running loss of epoch-82 batch-48 = 3.6803539842367172e-06

Training epoch-82 batch-49
Running loss of epoch-82 batch-49 = 1.7499551177024841e-06

Training epoch-82 batch-50
Running loss of epoch-82 batch-50 = 3.600027412176132e-06

Training epoch-82 batch-51
Running loss of epoch-82 batch-51 = 6.545800715684891e-06

Training epoch-82 batch-52
Running loss of epoch-82 batch-52 = 8.188188076019287e-06

Training epoch-82 batch-53
Running loss of epoch-82 batch-53 = 3.6177225410938263e-06

Training epoch-82 batch-54
Running loss of epoch-82 batch-54 = 6.19376078248024e-06

Training epoch-82 batch-55
Running loss of epoch-82 batch-55 = 5.1192473620176315e-06

Training epoch-82 batch-56
Running loss of epoch-82 batch-56 = 3.666151314973831e-06

Training epoch-82 batch-57
Running loss of epoch-82 batch-57 = 4.420755431056023e-06

Training epoch-82 batch-58
Running loss of epoch-82 batch-58 = 2.662884071469307e-06

Training epoch-82 batch-59
Running loss of epoch-82 batch-59 = 2.093147486448288e-06

Training epoch-82 batch-60
Running loss of epoch-82 batch-60 = 2.346932888031006e-06

Training epoch-82 batch-61
Running loss of epoch-82 batch-61 = 2.816086634993553e-06

Training epoch-82 batch-62
Running loss of epoch-82 batch-62 = 1.085922122001648e-06

Training epoch-82 batch-63
Running loss of epoch-82 batch-63 = 2.132263034582138e-06

Training epoch-82 batch-64
Running loss of epoch-82 batch-64 = 5.772104486823082e-06

Training epoch-82 batch-65
Running loss of epoch-82 batch-65 = 8.604023605585098e-06

Training epoch-82 batch-66
Running loss of epoch-82 batch-66 = 5.222158506512642e-06

Training epoch-82 batch-67
Running loss of epoch-82 batch-67 = 2.3283064365386963e-06

Training epoch-82 batch-68
Running loss of epoch-82 batch-68 = 8.569564670324326e-06

Training epoch-82 batch-69
Running loss of epoch-82 batch-69 = 1.0691583156585693e-06

Training epoch-82 batch-70
Running loss of epoch-82 batch-70 = 4.746252670884132e-06

Training epoch-82 batch-71
Running loss of epoch-82 batch-71 = 8.183065801858902e-06

Training epoch-82 batch-72
Running loss of epoch-82 batch-72 = 2.407468855381012e-06

Training epoch-82 batch-73
Running loss of epoch-82 batch-73 = 1.648208126425743e-06

Training epoch-82 batch-74
Running loss of epoch-82 batch-74 = 3.2603275030851364e-06

Training epoch-82 batch-75
Running loss of epoch-82 batch-75 = 1.3849930837750435e-05

Training epoch-82 batch-76
Running loss of epoch-82 batch-76 = 5.602603778243065e-06

Training epoch-82 batch-77
Running loss of epoch-82 batch-77 = 7.830094546079636e-07

Training epoch-82 batch-78
Running loss of epoch-82 batch-78 = 2.5907065719366074e-06

Training epoch-82 batch-79
Running loss of epoch-82 batch-79 = 4.2994506657123566e-06

Training epoch-82 batch-80
Running loss of epoch-82 batch-80 = 2.7546193450689316e-06

Training epoch-82 batch-81
Running loss of epoch-82 batch-81 = 4.028435796499252e-06

Training epoch-82 batch-82
Running loss of epoch-82 batch-82 = 1.6281846910715103e-06

Training epoch-82 batch-83
Running loss of epoch-82 batch-83 = 3.2545067369937897e-06

Training epoch-82 batch-84
Running loss of epoch-82 batch-84 = 2.5152694433927536e-06

Training epoch-82 batch-85
Running loss of epoch-82 batch-85 = 7.563736289739609e-06

Training epoch-82 batch-86
Running loss of epoch-82 batch-86 = 4.3674372136592865e-06

Training epoch-82 batch-87
Running loss of epoch-82 batch-87 = 1.6372650861740112e-06

Training epoch-82 batch-88
Running loss of epoch-82 batch-88 = 1.4918157830834389e-05

Training epoch-82 batch-89
Running loss of epoch-82 batch-89 = 1.8177088350057602e-06

Training epoch-82 batch-90
Running loss of epoch-82 batch-90 = 3.7741847336292267e-06

Training epoch-82 batch-91
Running loss of epoch-82 batch-91 = 3.882916644215584e-06

Training epoch-82 batch-92
Running loss of epoch-82 batch-92 = 5.9390440583229065e-06

Training epoch-82 batch-93
Running loss of epoch-82 batch-93 = 5.085021257400513e-06

Training epoch-82 batch-94
Running loss of epoch-82 batch-94 = 5.729729309678078e-06

Training epoch-82 batch-95
Running loss of epoch-82 batch-95 = 5.642883479595184e-06

Training epoch-82 batch-96
Running loss of epoch-82 batch-96 = 2.3476313799619675e-06

Training epoch-82 batch-97
Running loss of epoch-82 batch-97 = 4.75393608212471e-06

Training epoch-82 batch-98
Running loss of epoch-82 batch-98 = 5.386071279644966e-06

Training epoch-82 batch-99
Running loss of epoch-82 batch-99 = 2.4186447262763977e-06

Training epoch-82 batch-100
Running loss of epoch-82 batch-100 = 2.1134037524461746e-06

Training epoch-82 batch-101
Running loss of epoch-82 batch-101 = 5.109701305627823e-06

Training epoch-82 batch-102
Running loss of epoch-82 batch-102 = 4.546251147985458e-06

Training epoch-82 batch-103
Running loss of epoch-82 batch-103 = 4.177214577794075e-06

Training epoch-82 batch-104
Running loss of epoch-82 batch-104 = 1.207226887345314e-05

Training epoch-82 batch-105
Running loss of epoch-82 batch-105 = 6.571412086486816e-06

Training epoch-82 batch-106
Running loss of epoch-82 batch-106 = 2.957182005047798e-06

Training epoch-82 batch-107
Running loss of epoch-82 batch-107 = 3.707828000187874e-06

Training epoch-82 batch-108
Running loss of epoch-82 batch-108 = 3.889668732881546e-06

Training epoch-82 batch-109
Running loss of epoch-82 batch-109 = 6.495509296655655e-06

Training epoch-82 batch-110
Running loss of epoch-82 batch-110 = 1.908978447318077e-06

Training epoch-82 batch-111
Running loss of epoch-82 batch-111 = 1.4451798051595688e-06

Training epoch-82 batch-112
Running loss of epoch-82 batch-112 = 1.7762649804353714e-06

Training epoch-82 batch-113
Running loss of epoch-82 batch-113 = 3.6454293876886368e-06

Training epoch-82 batch-114
Running loss of epoch-82 batch-114 = 3.609573468565941e-06

Training epoch-82 batch-115
Running loss of epoch-82 batch-115 = 6.612623110413551e-06

Training epoch-82 batch-116
Running loss of epoch-82 batch-116 = 1.2045260518789291e-05

Training epoch-82 batch-117
Running loss of epoch-82 batch-117 = 1.1922093108296394e-05

Training epoch-82 batch-118
Running loss of epoch-82 batch-118 = 1.2940727174282074e-06

Training epoch-82 batch-119
Running loss of epoch-82 batch-119 = 4.572328180074692e-06

Training epoch-82 batch-120
Running loss of epoch-82 batch-120 = 1.4817342162132263e-06

Training epoch-82 batch-121
Running loss of epoch-82 batch-121 = 4.506204277276993e-06

Training epoch-82 batch-122
Running loss of epoch-82 batch-122 = 6.263144314289093e-07

Training epoch-82 batch-123
Running loss of epoch-82 batch-123 = 4.5816414058208466e-06

Training epoch-82 batch-124
Running loss of epoch-82 batch-124 = 8.932780474424362e-06

Training epoch-82 batch-125
Running loss of epoch-82 batch-125 = 2.8726644814014435e-06

Training epoch-82 batch-126
Running loss of epoch-82 batch-126 = 4.362780600786209e-06

Training epoch-82 batch-127
Running loss of epoch-82 batch-127 = 5.10108657181263e-06

Training epoch-82 batch-128
Running loss of epoch-82 batch-128 = 2.882210537791252e-06

Training epoch-82 batch-129
Running loss of epoch-82 batch-129 = 5.342997610569e-06

Training epoch-82 batch-130
Running loss of epoch-82 batch-130 = 4.040077328681946e-06

Training epoch-82 batch-131
Running loss of epoch-82 batch-131 = 1.239590346813202e-06

Training epoch-82 batch-132
Running loss of epoch-82 batch-132 = 3.750436007976532e-06

Training epoch-82 batch-133
Running loss of epoch-82 batch-133 = 2.8463546186685562e-06

Training epoch-82 batch-134
Running loss of epoch-82 batch-134 = 5.88572584092617e-06

Training epoch-82 batch-135
Running loss of epoch-82 batch-135 = 1.5534460544586182e-06

Training epoch-82 batch-136
Running loss of epoch-82 batch-136 = 4.054512828588486e-06

Training epoch-82 batch-137
Running loss of epoch-82 batch-137 = 3.0351802706718445e-06

Training epoch-82 batch-138
Running loss of epoch-82 batch-138 = 1.7725396901369095e-06

Training epoch-82 batch-139
Running loss of epoch-82 batch-139 = 5.530426278710365e-06

Training epoch-82 batch-140
Running loss of epoch-82 batch-140 = 5.341134965419769e-06

Training epoch-82 batch-141
Running loss of epoch-82 batch-141 = 3.6281999200582504e-06

Training epoch-82 batch-142
Running loss of epoch-82 batch-142 = 1.6019446775317192e-05

Training epoch-82 batch-143
Running loss of epoch-82 batch-143 = 4.952074959874153e-06

Training epoch-82 batch-144
Running loss of epoch-82 batch-144 = 3.4926924854516983e-06

Training epoch-82 batch-145
Running loss of epoch-82 batch-145 = 6.695743650197983e-06

Training epoch-82 batch-146
Running loss of epoch-82 batch-146 = 3.085937350988388e-06

Training epoch-82 batch-147
Running loss of epoch-82 batch-147 = 3.834720700979233e-06

Training epoch-82 batch-148
Running loss of epoch-82 batch-148 = 3.4223776310682297e-06

Training epoch-82 batch-149
Running loss of epoch-82 batch-149 = 5.37489540874958e-06

Training epoch-82 batch-150
Running loss of epoch-82 batch-150 = 4.180939868092537e-06

Training epoch-82 batch-151
Running loss of epoch-82 batch-151 = 4.431465640664101e-06

Training epoch-82 batch-152
Running loss of epoch-82 batch-152 = 4.597241058945656e-06

Training epoch-82 batch-153
Running loss of epoch-82 batch-153 = 2.9508955776691437e-06

Training epoch-82 batch-154
Running loss of epoch-82 batch-154 = 5.890615284442902e-07

Training epoch-82 batch-155
Running loss of epoch-82 batch-155 = 3.0507799237966537e-06

Training epoch-82 batch-156
Running loss of epoch-82 batch-156 = 2.7390196919441223e-06

Training epoch-82 batch-157
Running loss of epoch-82 batch-157 = 1.753866672515869e-05

Finished training epoch-82.



Average train loss at epoch-82 = 4.509743303060532e-06

Started Evaluation

Average val loss at epoch-82 = 1.8083871049363005

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.16 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-83


Training epoch-83 batch-1
Running loss of epoch-83 batch-1 = 2.353917807340622e-06

Training epoch-83 batch-2
Running loss of epoch-83 batch-2 = 1.6565900295972824e-06

Training epoch-83 batch-3
Running loss of epoch-83 batch-3 = 5.178619176149368e-06

Training epoch-83 batch-4
Running loss of epoch-83 batch-4 = 4.890142008662224e-06

Training epoch-83 batch-5
Running loss of epoch-83 batch-5 = 5.481066182255745e-06

Training epoch-83 batch-6
Running loss of epoch-83 batch-6 = 1.296401023864746e-06

Training epoch-83 batch-7
Running loss of epoch-83 batch-7 = 6.307847797870636e-06

Training epoch-83 batch-8
Running loss of epoch-83 batch-8 = 4.507368430495262e-06

Training epoch-83 batch-9
Running loss of epoch-83 batch-9 = 2.830754965543747e-06

Training epoch-83 batch-10
Running loss of epoch-83 batch-10 = 1.4477409422397614e-06

Training epoch-83 batch-11
Running loss of epoch-83 batch-11 = 4.359520971775055e-06

Training epoch-83 batch-12
Running loss of epoch-83 batch-12 = 5.321810021996498e-06

Training epoch-83 batch-13
Running loss of epoch-83 batch-13 = 3.859167918562889e-06

Training epoch-83 batch-14
Running loss of epoch-83 batch-14 = 7.603317499160767e-06

Training epoch-83 batch-15
Running loss of epoch-83 batch-15 = 3.9315782487392426e-06

Training epoch-83 batch-16
Running loss of epoch-83 batch-16 = 9.783543646335602e-07

Training epoch-83 batch-17
Running loss of epoch-83 batch-17 = 2.873595803976059e-06

Training epoch-83 batch-18
Running loss of epoch-83 batch-18 = 7.394235581159592e-06

Training epoch-83 batch-19
Running loss of epoch-83 batch-19 = 5.529727786779404e-06

Training epoch-83 batch-20
Running loss of epoch-83 batch-20 = 3.7758145481348038e-06

Training epoch-83 batch-21
Running loss of epoch-83 batch-21 = 2.7723144739866257e-06

Training epoch-83 batch-22
Running loss of epoch-83 batch-22 = 6.198417395353317e-06

Training epoch-83 batch-23
Running loss of epoch-83 batch-23 = 5.522510036826134e-06

Training epoch-83 batch-24
Running loss of epoch-83 batch-24 = 5.224253982305527e-06

Training epoch-83 batch-25
Running loss of epoch-83 batch-25 = 2.35741026699543e-06

Training epoch-83 batch-26
Running loss of epoch-83 batch-26 = 3.4996774047613144e-06

Training epoch-83 batch-27
Running loss of epoch-83 batch-27 = 2.7427449822425842e-06

Training epoch-83 batch-28
Running loss of epoch-83 batch-28 = 2.71783210337162e-06

Training epoch-83 batch-29
Running loss of epoch-83 batch-29 = 3.1136441975831985e-06

Training epoch-83 batch-30
Running loss of epoch-83 batch-30 = 6.476417183876038e-06

Training epoch-83 batch-31
Running loss of epoch-83 batch-31 = 3.5224948078393936e-06

Training epoch-83 batch-32
Running loss of epoch-83 batch-32 = 4.307366907596588e-06

Training epoch-83 batch-33
Running loss of epoch-83 batch-33 = 3.075692802667618e-06

Training epoch-83 batch-34
Running loss of epoch-83 batch-34 = 2.3816246539354324e-06

Training epoch-83 batch-35
Running loss of epoch-83 batch-35 = 2.185581251978874e-06

Training epoch-83 batch-36
Running loss of epoch-83 batch-36 = 3.579305484890938e-06

Training epoch-83 batch-37
Running loss of epoch-83 batch-37 = 4.286179319024086e-06

Training epoch-83 batch-38
Running loss of epoch-83 batch-38 = 3.109220415353775e-06

Training epoch-83 batch-39
Running loss of epoch-83 batch-39 = 3.502238541841507e-06

Training epoch-83 batch-40
Running loss of epoch-83 batch-40 = 3.995141014456749e-06

Training epoch-83 batch-41
Running loss of epoch-83 batch-41 = 3.7543941289186478e-06

Training epoch-83 batch-42
Running loss of epoch-83 batch-42 = 2.3997854441404343e-06

Training epoch-83 batch-43
Running loss of epoch-83 batch-43 = 2.8673093765974045e-06

Training epoch-83 batch-44
Running loss of epoch-83 batch-44 = 3.575114533305168e-06

Training epoch-83 batch-45
Running loss of epoch-83 batch-45 = 6.014714017510414e-06

Training epoch-83 batch-46
Running loss of epoch-83 batch-46 = 4.152301698923111e-06

Training epoch-83 batch-47
Running loss of epoch-83 batch-47 = 3.107823431491852e-06

Training epoch-83 batch-48
Running loss of epoch-83 batch-48 = 2.630986273288727e-06

Training epoch-83 batch-49
Running loss of epoch-83 batch-49 = 4.7085341066122055e-06

Training epoch-83 batch-50
Running loss of epoch-83 batch-50 = 3.063119947910309e-06

Training epoch-83 batch-51
Running loss of epoch-83 batch-51 = 3.5096891224384308e-06

Training epoch-83 batch-52
Running loss of epoch-83 batch-52 = 1.4937249943614006e-05

Training epoch-83 batch-53
Running loss of epoch-83 batch-53 = 2.294778823852539e-06

Training epoch-83 batch-54
Running loss of epoch-83 batch-54 = 1.6808044165372849e-06

Training epoch-83 batch-55
Running loss of epoch-83 batch-55 = 2.455897629261017e-06

Training epoch-83 batch-56
Running loss of epoch-83 batch-56 = 8.67573544383049e-06

Training epoch-83 batch-57
Running loss of epoch-83 batch-57 = 1.5688128769397736e-06

Training epoch-83 batch-58
Running loss of epoch-83 batch-58 = 3.2584648579359055e-06

Training epoch-83 batch-59
Running loss of epoch-83 batch-59 = 1.0986113920807838e-05

Training epoch-83 batch-60
Running loss of epoch-83 batch-60 = 7.4855051934719086e-06

Training epoch-83 batch-61
Running loss of epoch-83 batch-61 = 1.0653631761670113e-05

Training epoch-83 batch-62
Running loss of epoch-83 batch-62 = 2.09989957511425e-06

Training epoch-83 batch-63
Running loss of epoch-83 batch-63 = 7.821479812264442e-06

Training epoch-83 batch-64
Running loss of epoch-83 batch-64 = 1.2922100722789764e-06

Training epoch-83 batch-65
Running loss of epoch-83 batch-65 = 3.051944077014923e-06

Training epoch-83 batch-66
Running loss of epoch-83 batch-66 = 3.0838418751955032e-06

Training epoch-83 batch-67
Running loss of epoch-83 batch-67 = 1.1739321053028107e-05

Training epoch-83 batch-68
Running loss of epoch-83 batch-68 = 1.93621963262558e-06

Training epoch-83 batch-69
Running loss of epoch-83 batch-69 = 5.397479981184006e-06

Training epoch-83 batch-70
Running loss of epoch-83 batch-70 = 5.991198122501373e-06

Training epoch-83 batch-71
Running loss of epoch-83 batch-71 = 3.3741816878318787e-06

Training epoch-83 batch-72
Running loss of epoch-83 batch-72 = 3.7366990000009537e-06

Training epoch-83 batch-73
Running loss of epoch-83 batch-73 = 3.998400643467903e-06

Training epoch-83 batch-74
Running loss of epoch-83 batch-74 = 3.532739356160164e-06

Training epoch-83 batch-75
Running loss of epoch-83 batch-75 = 3.0137598514556885e-06

Training epoch-83 batch-76
Running loss of epoch-83 batch-76 = 2.533895894885063e-06

Training epoch-83 batch-77
Running loss of epoch-83 batch-77 = 2.6542693376541138e-06

Training epoch-83 batch-78
Running loss of epoch-83 batch-78 = 2.4065375328063965e-06

Training epoch-83 batch-79
Running loss of epoch-83 batch-79 = 8.698320016264915e-06

Training epoch-83 batch-80
Running loss of epoch-83 batch-80 = 2.6600901037454605e-06

Training epoch-83 batch-81
Running loss of epoch-83 batch-81 = 4.511093720793724e-06

Training epoch-83 batch-82
Running loss of epoch-83 batch-82 = 1.2300442904233932e-06

Training epoch-83 batch-83
Running loss of epoch-83 batch-83 = 2.827029675245285e-06

Training epoch-83 batch-84
Running loss of epoch-83 batch-84 = 6.228918209671974e-06

Training epoch-83 batch-85
Running loss of epoch-83 batch-85 = 1.12967099994421e-05

Training epoch-83 batch-86
Running loss of epoch-83 batch-86 = 3.911321982741356e-06

Training epoch-83 batch-87
Running loss of epoch-83 batch-87 = 4.526227712631226e-06

Training epoch-83 batch-88
Running loss of epoch-83 batch-88 = 1.5173573046922684e-06

Training epoch-83 batch-89
Running loss of epoch-83 batch-89 = 1.8326099961996078e-06

Training epoch-83 batch-90
Running loss of epoch-83 batch-90 = 6.896443665027618e-06

Training epoch-83 batch-91
Running loss of epoch-83 batch-91 = 1.978594809770584e-06

Training epoch-83 batch-92
Running loss of epoch-83 batch-92 = 1.3979151844978333e-05

Training epoch-83 batch-93
Running loss of epoch-83 batch-93 = 7.524155080318451e-06

Training epoch-83 batch-94
Running loss of epoch-83 batch-94 = 5.552545189857483e-06

Training epoch-83 batch-95
Running loss of epoch-83 batch-95 = 3.4314580261707306e-06

Training epoch-83 batch-96
Running loss of epoch-83 batch-96 = 7.15046189725399e-06

Training epoch-83 batch-97
Running loss of epoch-83 batch-97 = 6.264308467507362e-06

Training epoch-83 batch-98
Running loss of epoch-83 batch-98 = 2.118293195962906e-06

Training epoch-83 batch-99
Running loss of epoch-83 batch-99 = 1.2540258467197418e-06

Training epoch-83 batch-100
Running loss of epoch-83 batch-100 = 4.15043905377388e-06

Training epoch-83 batch-101
Running loss of epoch-83 batch-101 = 3.0212104320526123e-06

Training epoch-83 batch-102
Running loss of epoch-83 batch-102 = 6.109708920121193e-06

Training epoch-83 batch-103
Running loss of epoch-83 batch-103 = 3.7883874028921127e-06

Training epoch-83 batch-104
Running loss of epoch-83 batch-104 = 2.632150426506996e-06

Training epoch-83 batch-105
Running loss of epoch-83 batch-105 = 3.5534612834453583e-06

Training epoch-83 batch-106
Running loss of epoch-83 batch-106 = 7.492490112781525e-07

Training epoch-83 batch-107
Running loss of epoch-83 batch-107 = 3.0351802706718445e-06

Training epoch-83 batch-108
Running loss of epoch-83 batch-108 = 2.273591235280037e-06

Training epoch-83 batch-109
Running loss of epoch-83 batch-109 = 2.357177436351776e-06

Training epoch-83 batch-110
Running loss of epoch-83 batch-110 = 1.3736076653003693e-05

Training epoch-83 batch-111
Running loss of epoch-83 batch-111 = 4.638684913516045e-06

Training epoch-83 batch-112
Running loss of epoch-83 batch-112 = 3.476860001683235e-06

Training epoch-83 batch-113
Running loss of epoch-83 batch-113 = 3.898516297340393e-06

Training epoch-83 batch-114
Running loss of epoch-83 batch-114 = 3.225402906537056e-06

Training epoch-83 batch-115
Running loss of epoch-83 batch-115 = 5.686422809958458e-06

Training epoch-83 batch-116
Running loss of epoch-83 batch-116 = 2.6456546038389206e-06

Training epoch-83 batch-117
Running loss of epoch-83 batch-117 = 1.8104910850524902e-06

Training epoch-83 batch-118
Running loss of epoch-83 batch-118 = 2.643093466758728e-06

Training epoch-83 batch-119
Running loss of epoch-83 batch-119 = 6.855465471744537e-06

Training epoch-83 batch-120
Running loss of epoch-83 batch-120 = 7.175840437412262e-07

Training epoch-83 batch-121
Running loss of epoch-83 batch-121 = 4.858244210481644e-06

Training epoch-83 batch-122
Running loss of epoch-83 batch-122 = 3.4032855182886124e-06

Training epoch-83 batch-123
Running loss of epoch-83 batch-123 = 8.322997018694878e-06

Training epoch-83 batch-124
Running loss of epoch-83 batch-124 = 4.0798913687467575e-06

Training epoch-83 batch-125
Running loss of epoch-83 batch-125 = 3.771623596549034e-06

Training epoch-83 batch-126
Running loss of epoch-83 batch-126 = 7.892260327935219e-06

Training epoch-83 batch-127
Running loss of epoch-83 batch-127 = 1.9439030438661575e-06

Training epoch-83 batch-128
Running loss of epoch-83 batch-128 = 5.644978955388069e-06

Training epoch-83 batch-129
Running loss of epoch-83 batch-129 = 2.766260877251625e-06

Training epoch-83 batch-130
Running loss of epoch-83 batch-130 = 5.338108167052269e-06

Training epoch-83 batch-131
Running loss of epoch-83 batch-131 = 2.1301675587892532e-06

Training epoch-83 batch-132
Running loss of epoch-83 batch-132 = 1.193443313241005e-05

Training epoch-83 batch-133
Running loss of epoch-83 batch-133 = 3.868015483021736e-06

Training epoch-83 batch-134
Running loss of epoch-83 batch-134 = 5.40376640856266e-06

Training epoch-83 batch-135
Running loss of epoch-83 batch-135 = 3.705732524394989e-06

Training epoch-83 batch-136
Running loss of epoch-83 batch-136 = 6.70459121465683e-06

Training epoch-83 batch-137
Running loss of epoch-83 batch-137 = 6.501795724034309e-06

Training epoch-83 batch-138
Running loss of epoch-83 batch-138 = 1.5934929251670837e-06

Training epoch-83 batch-139
Running loss of epoch-83 batch-139 = 3.1283125281333923e-06

Training epoch-83 batch-140
Running loss of epoch-83 batch-140 = 2.1760351955890656e-06

Training epoch-83 batch-141
Running loss of epoch-83 batch-141 = 4.8496294766664505e-06

Training epoch-83 batch-142
Running loss of epoch-83 batch-142 = 5.481531843543053e-06

Training epoch-83 batch-143
Running loss of epoch-83 batch-143 = 3.196531906723976e-06

Training epoch-83 batch-144
Running loss of epoch-83 batch-144 = 4.452187567949295e-06

Training epoch-83 batch-145
Running loss of epoch-83 batch-145 = 2.1313317120075226e-06

Training epoch-83 batch-146
Running loss of epoch-83 batch-146 = 8.81054438650608e-06

Training epoch-83 batch-147
Running loss of epoch-83 batch-147 = 3.002118319272995e-06

Training epoch-83 batch-148
Running loss of epoch-83 batch-148 = 3.6654528230428696e-06

Training epoch-83 batch-149
Running loss of epoch-83 batch-149 = 5.0996895879507065e-06

Training epoch-83 batch-150
Running loss of epoch-83 batch-150 = 2.767890691757202e-06

Training epoch-83 batch-151
Running loss of epoch-83 batch-151 = 4.287343472242355e-06

Training epoch-83 batch-152
Running loss of epoch-83 batch-152 = 1.3962853699922562e-06

Training epoch-83 batch-153
Running loss of epoch-83 batch-153 = 4.362780600786209e-06

Training epoch-83 batch-154
Running loss of epoch-83 batch-154 = 3.7315767258405685e-06

Training epoch-83 batch-155
Running loss of epoch-83 batch-155 = 1.0572373867034912e-05

Training epoch-83 batch-156
Running loss of epoch-83 batch-156 = 3.6729034036397934e-06

Training epoch-83 batch-157
Running loss of epoch-83 batch-157 = 3.0856579542160034e-05

Finished training epoch-83.



Average train loss at epoch-83 = 4.4054314494133e-06

Started Evaluation

Average val loss at epoch-83 = 1.8160064014555426

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.25 %

Finished Evaluation



Started training epoch-84


Training epoch-84 batch-1
Running loss of epoch-84 batch-1 = 8.46991315484047e-06

Training epoch-84 batch-2
Running loss of epoch-84 batch-2 = 3.809574991464615e-06

Training epoch-84 batch-3
Running loss of epoch-84 batch-3 = 4.536937922239304e-06

Training epoch-84 batch-4
Running loss of epoch-84 batch-4 = 1.6971025615930557e-06

Training epoch-84 batch-5
Running loss of epoch-84 batch-5 = 3.5015400499105453e-06

Training epoch-84 batch-6
Running loss of epoch-84 batch-6 = 2.1869782358407974e-06

Training epoch-84 batch-7
Running loss of epoch-84 batch-7 = 2.668006345629692e-06

Training epoch-84 batch-8
Running loss of epoch-84 batch-8 = 3.025634214282036e-06

Training epoch-84 batch-9
Running loss of epoch-84 batch-9 = 5.010981112718582e-06

Training epoch-84 batch-10
Running loss of epoch-84 batch-10 = 3.871740773320198e-06

Training epoch-84 batch-11
Running loss of epoch-84 batch-11 = 9.638722985982895e-06

Training epoch-84 batch-12
Running loss of epoch-84 batch-12 = 4.364876076579094e-06

Training epoch-84 batch-13
Running loss of epoch-84 batch-13 = 5.448935553431511e-06

Training epoch-84 batch-14
Running loss of epoch-84 batch-14 = 1.5022233128547668e-06

Training epoch-84 batch-15
Running loss of epoch-84 batch-15 = 3.10409814119339e-06

Training epoch-84 batch-16
Running loss of epoch-84 batch-16 = 1.974869519472122e-06

Training epoch-84 batch-17
Running loss of epoch-84 batch-17 = 4.615867510437965e-06

Training epoch-84 batch-18
Running loss of epoch-84 batch-18 = 2.5515910238027573e-06

Training epoch-84 batch-19
Running loss of epoch-84 batch-19 = 3.8370490074157715e-06

Training epoch-84 batch-20
Running loss of epoch-84 batch-20 = 2.2312160581350327e-06

Training epoch-84 batch-21
Running loss of epoch-84 batch-21 = 4.466157406568527e-06

Training epoch-84 batch-22
Running loss of epoch-84 batch-22 = 8.85804183781147e-06

Training epoch-84 batch-23
Running loss of epoch-84 batch-23 = 1.9315630197525024e-06

Training epoch-84 batch-24
Running loss of epoch-84 batch-24 = 3.862660378217697e-06

Training epoch-84 batch-25
Running loss of epoch-84 batch-25 = 4.297355189919472e-06

Training epoch-84 batch-26
Running loss of epoch-84 batch-26 = 2.969987690448761e-06

Training epoch-84 batch-27
Running loss of epoch-84 batch-27 = 1.2551899999380112e-06

Training epoch-84 batch-28
Running loss of epoch-84 batch-28 = 1.4168443158268929e-05

Training epoch-84 batch-29
Running loss of epoch-84 batch-29 = 6.184680387377739e-06

Training epoch-84 batch-30
Running loss of epoch-84 batch-30 = 4.846369847655296e-06

Training epoch-84 batch-31
Running loss of epoch-84 batch-31 = 6.314599886536598e-06

Training epoch-84 batch-32
Running loss of epoch-84 batch-32 = 5.545793101191521e-06

Training epoch-84 batch-33
Running loss of epoch-84 batch-33 = 7.408205419778824e-06

Training epoch-84 batch-34
Running loss of epoch-84 batch-34 = 1.3303710147738457e-05

Training epoch-84 batch-35
Running loss of epoch-84 batch-35 = 2.908986061811447e-06

Training epoch-84 batch-36
Running loss of epoch-84 batch-36 = 2.6708003133535385e-06

Training epoch-84 batch-37
Running loss of epoch-84 batch-37 = 4.7602225095033646e-06

Training epoch-84 batch-38
Running loss of epoch-84 batch-38 = 4.275701940059662e-06

Training epoch-84 batch-39
Running loss of epoch-84 batch-39 = 1.5632249414920807e-06

Training epoch-84 batch-40
Running loss of epoch-84 batch-40 = 3.6910641938447952e-06

Training epoch-84 batch-41
Running loss of epoch-84 batch-41 = 4.446366801857948e-06

Training epoch-84 batch-42
Running loss of epoch-84 batch-42 = 1.8831342458724976e-06

Training epoch-84 batch-43
Running loss of epoch-84 batch-43 = 5.375361070036888e-06

Training epoch-84 batch-44
Running loss of epoch-84 batch-44 = 3.519933670759201e-06

Training epoch-84 batch-45
Running loss of epoch-84 batch-45 = 2.1585728973150253e-06

Training epoch-84 batch-46
Running loss of epoch-84 batch-46 = 1.662643626332283e-06

Training epoch-84 batch-47
Running loss of epoch-84 batch-47 = 4.9925874918699265e-06

Training epoch-84 batch-48
Running loss of epoch-84 batch-48 = 3.0705705285072327e-06

Training epoch-84 batch-49
Running loss of epoch-84 batch-49 = 3.4437980502843857e-06

Training epoch-84 batch-50
Running loss of epoch-84 batch-50 = 3.880588337779045e-06

Training epoch-84 batch-51
Running loss of epoch-84 batch-51 = 3.6740675568580627e-06

Training epoch-84 batch-52
Running loss of epoch-84 batch-52 = 3.373250365257263e-06

Training epoch-84 batch-53
Running loss of epoch-84 batch-53 = 5.7194847613573074e-06

Training epoch-84 batch-54
Running loss of epoch-84 batch-54 = 4.369532689452171e-06

Training epoch-84 batch-55
Running loss of epoch-84 batch-55 = 1.3686716556549072e-05

Training epoch-84 batch-56
Running loss of epoch-84 batch-56 = 2.2095628082752228e-06

Training epoch-84 batch-57
Running loss of epoch-84 batch-57 = 6.675254553556442e-07

Training epoch-84 batch-58
Running loss of epoch-84 batch-58 = 5.318783223628998e-06

Training epoch-84 batch-59
Running loss of epoch-84 batch-59 = 1.0453630238771439e-05

Training epoch-84 batch-60
Running loss of epoch-84 batch-60 = 4.024012014269829e-06

Training epoch-84 batch-61
Running loss of epoch-84 batch-61 = 3.391178324818611e-06

Training epoch-84 batch-62
Running loss of epoch-84 batch-62 = 3.6049168556928635e-06

Training epoch-84 batch-63
Running loss of epoch-84 batch-63 = 3.4137628972530365e-06

Training epoch-84 batch-64
Running loss of epoch-84 batch-64 = 6.553716957569122e-06

Training epoch-84 batch-65
Running loss of epoch-84 batch-65 = 1.5867408365011215e-06

Training epoch-84 batch-66
Running loss of epoch-84 batch-66 = 3.1150411814451218e-06

Training epoch-84 batch-67
Running loss of epoch-84 batch-67 = 3.6691781133413315e-06

Training epoch-84 batch-68
Running loss of epoch-84 batch-68 = 4.306435585021973e-06

Training epoch-84 batch-69
Running loss of epoch-84 batch-69 = 3.7958379834890366e-06

Training epoch-84 batch-70
Running loss of epoch-84 batch-70 = 5.089212208986282e-06

Training epoch-84 batch-71
Running loss of epoch-84 batch-71 = 7.66734592616558e-06

Training epoch-84 batch-72
Running loss of epoch-84 batch-72 = 3.56859527528286e-06

Training epoch-84 batch-73
Running loss of epoch-84 batch-73 = 1.9720755517482758e-06

Training epoch-84 batch-74
Running loss of epoch-84 batch-74 = 6.129033863544464e-06

Training epoch-84 batch-75
Running loss of epoch-84 batch-75 = 3.937864676117897e-06

Training epoch-84 batch-76
Running loss of epoch-84 batch-76 = 5.018897354602814e-06

Training epoch-84 batch-77
Running loss of epoch-84 batch-77 = 3.820750862360001e-06

Training epoch-84 batch-78
Running loss of epoch-84 batch-78 = 4.505738615989685e-06

Training epoch-84 batch-79
Running loss of epoch-84 batch-79 = 7.295282557606697e-06

Training epoch-84 batch-80
Running loss of epoch-84 batch-80 = 3.1194649636745453e-06

Training epoch-84 batch-81
Running loss of epoch-84 batch-81 = 4.632864147424698e-06

Training epoch-84 batch-82
Running loss of epoch-84 batch-82 = 2.2186432033777237e-06

Training epoch-84 batch-83
Running loss of epoch-84 batch-83 = 7.32974149286747e-06

Training epoch-84 batch-84
Running loss of epoch-84 batch-84 = 3.793276846408844e-06

Training epoch-84 batch-85
Running loss of epoch-84 batch-85 = 2.189772203564644e-06

Training epoch-84 batch-86
Running loss of epoch-84 batch-86 = 4.582572728395462e-06

Training epoch-84 batch-87
Running loss of epoch-84 batch-87 = 6.715301424264908e-06

Training epoch-84 batch-88
Running loss of epoch-84 batch-88 = 1.1698808521032333e-05

Training epoch-84 batch-89
Running loss of epoch-84 batch-89 = 5.512731149792671e-06

Training epoch-84 batch-90
Running loss of epoch-84 batch-90 = 4.201661795377731e-06

Training epoch-84 batch-91
Running loss of epoch-84 batch-91 = 5.753478035330772e-06

Training epoch-84 batch-92
Running loss of epoch-84 batch-92 = 2.2177118808031082e-06

Training epoch-84 batch-93
Running loss of epoch-84 batch-93 = 4.258006811141968e-06

Training epoch-84 batch-94
Running loss of epoch-84 batch-94 = 4.513654857873917e-06

Training epoch-84 batch-95
Running loss of epoch-84 batch-95 = 1.0245945304632187e-05

Training epoch-84 batch-96
Running loss of epoch-84 batch-96 = 3.453809767961502e-06

Training epoch-84 batch-97
Running loss of epoch-84 batch-97 = 5.021225661039352e-06

Training epoch-84 batch-98
Running loss of epoch-84 batch-98 = 2.323184162378311e-06

Training epoch-84 batch-99
Running loss of epoch-84 batch-99 = 1.003500074148178e-06

Training epoch-84 batch-100
Running loss of epoch-84 batch-100 = 2.3336615413427353e-06

Training epoch-84 batch-101
Running loss of epoch-84 batch-101 = 2.541579306125641e-06

Training epoch-84 batch-102
Running loss of epoch-84 batch-102 = 1.5264376997947693e-06

Training epoch-84 batch-103
Running loss of epoch-84 batch-103 = 2.316199243068695e-06

Training epoch-84 batch-104
Running loss of epoch-84 batch-104 = 3.3883843570947647e-06

Training epoch-84 batch-105
Running loss of epoch-84 batch-105 = 2.953922376036644e-06

Training epoch-84 batch-106
Running loss of epoch-84 batch-106 = 1.7406418919563293e-06

Training epoch-84 batch-107
Running loss of epoch-84 batch-107 = 4.651956260204315e-06

Training epoch-84 batch-108
Running loss of epoch-84 batch-108 = 5.659181624650955e-06

Training epoch-84 batch-109
Running loss of epoch-84 batch-109 = 3.4566037356853485e-06

Training epoch-84 batch-110
Running loss of epoch-84 batch-110 = 3.4833792597055435e-06

Training epoch-84 batch-111
Running loss of epoch-84 batch-111 = 2.607237547636032e-06

Training epoch-84 batch-112
Running loss of epoch-84 batch-112 = 8.327886462211609e-06

Training epoch-84 batch-113
Running loss of epoch-84 batch-113 = 1.916196197271347e-06

Training epoch-84 batch-114
Running loss of epoch-84 batch-114 = 3.6889687180519104e-06

Training epoch-84 batch-115
Running loss of epoch-84 batch-115 = 2.7741771191358566e-06

Training epoch-84 batch-116
Running loss of epoch-84 batch-116 = 3.6659184843301773e-06

Training epoch-84 batch-117
Running loss of epoch-84 batch-117 = 3.394903615117073e-06

Training epoch-84 batch-118
Running loss of epoch-84 batch-118 = 4.3264590203762054e-06

Training epoch-84 batch-119
Running loss of epoch-84 batch-119 = 6.78584910929203e-06

Training epoch-84 batch-120
Running loss of epoch-84 batch-120 = 2.3264437913894653e-06

Training epoch-84 batch-121
Running loss of epoch-84 batch-121 = 5.870824679732323e-06

Training epoch-84 batch-122
Running loss of epoch-84 batch-122 = 8.416594937443733e-06

Training epoch-84 batch-123
Running loss of epoch-84 batch-123 = 3.1241215765476227e-06

Training epoch-84 batch-124
Running loss of epoch-84 batch-124 = 1.4500692486763e-06

Training epoch-84 batch-125
Running loss of epoch-84 batch-125 = 1.3718381524085999e-06

Training epoch-84 batch-126
Running loss of epoch-84 batch-126 = 1.0414514690637589e-05

Training epoch-84 batch-127
Running loss of epoch-84 batch-127 = 6.511807441711426e-06

Training epoch-84 batch-128
Running loss of epoch-84 batch-128 = 4.273373633623123e-06

Training epoch-84 batch-129
Running loss of epoch-84 batch-129 = 2.766028046607971e-06

Training epoch-84 batch-130
Running loss of epoch-84 batch-130 = 2.5399494916200638e-06

Training epoch-84 batch-131
Running loss of epoch-84 batch-131 = 1.3695796951651573e-05

Training epoch-84 batch-132
Running loss of epoch-84 batch-132 = 4.629138857126236e-06

Training epoch-84 batch-133
Running loss of epoch-84 batch-133 = 1.077074557542801e-06

Training epoch-84 batch-134
Running loss of epoch-84 batch-134 = 3.2281968742609024e-06

Training epoch-84 batch-135
Running loss of epoch-84 batch-135 = 1.5771947801113129e-06

Training epoch-84 batch-136
Running loss of epoch-84 batch-136 = 5.0978269428014755e-06

Training epoch-84 batch-137
Running loss of epoch-84 batch-137 = 4.587695002555847e-06

Training epoch-84 batch-138
Running loss of epoch-84 batch-138 = 1.764390617609024e-06

Training epoch-84 batch-139
Running loss of epoch-84 batch-139 = 3.476976417005062e-06

Training epoch-84 batch-140
Running loss of epoch-84 batch-140 = 8.252914994955063e-06

Training epoch-84 batch-141
Running loss of epoch-84 batch-141 = 2.9043294489383698e-06

Training epoch-84 batch-142
Running loss of epoch-84 batch-142 = 2.034008502960205e-06

Training epoch-84 batch-143
Running loss of epoch-84 batch-143 = 8.533243089914322e-07

Training epoch-84 batch-144
Running loss of epoch-84 batch-144 = 4.514120519161224e-06

Training epoch-84 batch-145
Running loss of epoch-84 batch-145 = 3.864988684654236e-06

Training epoch-84 batch-146
Running loss of epoch-84 batch-146 = 5.194451659917831e-06

Training epoch-84 batch-147
Running loss of epoch-84 batch-147 = 3.7259887903928757e-06

Training epoch-84 batch-148
Running loss of epoch-84 batch-148 = 3.892229869961739e-06

Training epoch-84 batch-149
Running loss of epoch-84 batch-149 = 4.603760316967964e-06

Training epoch-84 batch-150
Running loss of epoch-84 batch-150 = 6.12880103290081e-06

Training epoch-84 batch-151
Running loss of epoch-84 batch-151 = 3.6351848393678665e-06

Training epoch-84 batch-152
Running loss of epoch-84 batch-152 = 3.0319206416606903e-06

Training epoch-84 batch-153
Running loss of epoch-84 batch-153 = 2.0426232367753983e-06

Training epoch-84 batch-154
Running loss of epoch-84 batch-154 = 3.7141144275665283e-06

Training epoch-84 batch-155
Running loss of epoch-84 batch-155 = 2.932269126176834e-06

Training epoch-84 batch-156
Running loss of epoch-84 batch-156 = 2.6780180633068085e-06

Training epoch-84 batch-157
Running loss of epoch-84 batch-157 = 2.4586915969848633e-06

Finished training epoch-84.



Average train loss at epoch-84 = 4.306206852197647e-06

Started Evaluation

Average val loss at epoch-84 = 1.8269706960569876

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.69 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.12 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-85


Training epoch-85 batch-1
Running loss of epoch-85 batch-1 = 6.109708920121193e-06

Training epoch-85 batch-2
Running loss of epoch-85 batch-2 = 5.444511771202087e-06

Training epoch-85 batch-3
Running loss of epoch-85 batch-3 = 4.824250936508179e-06

Training epoch-85 batch-4
Running loss of epoch-85 batch-4 = 5.262438207864761e-06

Training epoch-85 batch-5
Running loss of epoch-85 batch-5 = 6.879912689328194e-06

Training epoch-85 batch-6
Running loss of epoch-85 batch-6 = 6.013782694935799e-06

Training epoch-85 batch-7
Running loss of epoch-85 batch-7 = 3.2400712370872498e-06

Training epoch-85 batch-8
Running loss of epoch-85 batch-8 = 3.3478718250989914e-06

Training epoch-85 batch-9
Running loss of epoch-85 batch-9 = 2.602115273475647e-06

Training epoch-85 batch-10
Running loss of epoch-85 batch-10 = 2.2721942514181137e-06

Training epoch-85 batch-11
Running loss of epoch-85 batch-11 = 3.0461233109235764e-06

Training epoch-85 batch-12
Running loss of epoch-85 batch-12 = 6.774440407752991e-06

Training epoch-85 batch-13
Running loss of epoch-85 batch-13 = 3.162538632750511e-06

Training epoch-85 batch-14
Running loss of epoch-85 batch-14 = 4.5758206397295e-06

Training epoch-85 batch-15
Running loss of epoch-85 batch-15 = 3.737630322575569e-06

Training epoch-85 batch-16
Running loss of epoch-85 batch-16 = 4.6675559133291245e-06

Training epoch-85 batch-17
Running loss of epoch-85 batch-17 = 7.690396159887314e-07

Training epoch-85 batch-18
Running loss of epoch-85 batch-18 = 3.0100345611572266e-06

Training epoch-85 batch-19
Running loss of epoch-85 batch-19 = 2.582557499408722e-06

Training epoch-85 batch-20
Running loss of epoch-85 batch-20 = 1.8593855202198029e-06

Training epoch-85 batch-21
Running loss of epoch-85 batch-21 = 3.3224932849407196e-06

Training epoch-85 batch-22
Running loss of epoch-85 batch-22 = 3.632856532931328e-06

Training epoch-85 batch-23
Running loss of epoch-85 batch-23 = 5.444977432489395e-06

Training epoch-85 batch-24
Running loss of epoch-85 batch-24 = 2.7026981115341187e-06

Training epoch-85 batch-25
Running loss of epoch-85 batch-25 = 5.988171324133873e-06

Training epoch-85 batch-26
Running loss of epoch-85 batch-26 = 1.3506505638360977e-06

Training epoch-85 batch-27
Running loss of epoch-85 batch-27 = 1.1913012713193893e-05

Training epoch-85 batch-28
Running loss of epoch-85 batch-28 = 2.3364555090665817e-06

Training epoch-85 batch-29
Running loss of epoch-85 batch-29 = 1.7862766981124878e-06

Training epoch-85 batch-30
Running loss of epoch-85 batch-30 = 7.799826562404633e-06

Training epoch-85 batch-31
Running loss of epoch-85 batch-31 = 2.2440217435359955e-06

Training epoch-85 batch-32
Running loss of epoch-85 batch-32 = 1.1162366718053818e-05

Training epoch-85 batch-33
Running loss of epoch-85 batch-33 = 7.279450073838234e-06

Training epoch-85 batch-34
Running loss of epoch-85 batch-34 = 1.4051329344511032e-06

Training epoch-85 batch-35
Running loss of epoch-85 batch-35 = 2.4032779037952423e-06

Training epoch-85 batch-36
Running loss of epoch-85 batch-36 = 4.722271114587784e-06

Training epoch-85 batch-37
Running loss of epoch-85 batch-37 = 2.0673032850027084e-06

Training epoch-85 batch-38
Running loss of epoch-85 batch-38 = 2.603977918624878e-06

Training epoch-85 batch-39
Running loss of epoch-85 batch-39 = 5.362322553992271e-06

Training epoch-85 batch-40
Running loss of epoch-85 batch-40 = 5.3104013204574585e-06

Training epoch-85 batch-41
Running loss of epoch-85 batch-41 = 2.917833626270294e-06

Training epoch-85 batch-42
Running loss of epoch-85 batch-42 = 3.1762756407260895e-06

Training epoch-85 batch-43
Running loss of epoch-85 batch-43 = 3.938330337405205e-06

Training epoch-85 batch-44
Running loss of epoch-85 batch-44 = 1.6777776181697845e-06

Training epoch-85 batch-45
Running loss of epoch-85 batch-45 = 3.1373929232358932e-06

Training epoch-85 batch-46
Running loss of epoch-85 batch-46 = 2.146465703845024e-06

Training epoch-85 batch-47
Running loss of epoch-85 batch-47 = 6.0175079852342606e-06

Training epoch-85 batch-48
Running loss of epoch-85 batch-48 = 3.982102498412132e-06

Training epoch-85 batch-49
Running loss of epoch-85 batch-49 = 5.747424438595772e-06

Training epoch-85 batch-50
Running loss of epoch-85 batch-50 = 9.62754711508751e-06

Training epoch-85 batch-51
Running loss of epoch-85 batch-51 = 6.4838677644729614e-06

Training epoch-85 batch-52
Running loss of epoch-85 batch-52 = 4.280591383576393e-06

Training epoch-85 batch-53
Running loss of epoch-85 batch-53 = 5.102250725030899e-06

Training epoch-85 batch-54
Running loss of epoch-85 batch-54 = 3.4938566386699677e-06

Training epoch-85 batch-55
Running loss of epoch-85 batch-55 = 5.334848538041115e-06

Training epoch-85 batch-56
Running loss of epoch-85 batch-56 = 4.346715286374092e-06

Training epoch-85 batch-57
Running loss of epoch-85 batch-57 = 6.264541298151016e-06

Training epoch-85 batch-58
Running loss of epoch-85 batch-58 = 1.6042031347751617e-06

Training epoch-85 batch-59
Running loss of epoch-85 batch-59 = 5.777226760983467e-06

Training epoch-85 batch-60
Running loss of epoch-85 batch-60 = 8.876435458660126e-06

Training epoch-85 batch-61
Running loss of epoch-85 batch-61 = 1.3111857697367668e-05

Training epoch-85 batch-62
Running loss of epoch-85 batch-62 = 1.5920959413051605e-06

Training epoch-85 batch-63
Running loss of epoch-85 batch-63 = 5.414243787527084e-06

Training epoch-85 batch-64
Running loss of epoch-85 batch-64 = 1.1506490409374237e-05

Training epoch-85 batch-65
Running loss of epoch-85 batch-65 = 3.105960786342621e-06

Training epoch-85 batch-66
Running loss of epoch-85 batch-66 = 4.824250936508179e-06

Training epoch-85 batch-67
Running loss of epoch-85 batch-67 = 4.43984754383564e-06

Training epoch-85 batch-68
Running loss of epoch-85 batch-68 = 4.373490810394287e-06

Training epoch-85 batch-69
Running loss of epoch-85 batch-69 = 3.362540155649185e-06

Training epoch-85 batch-70
Running loss of epoch-85 batch-70 = 2.196989953517914e-06

Training epoch-85 batch-71
Running loss of epoch-85 batch-71 = 2.1280720829963684e-06

Training epoch-85 batch-72
Running loss of epoch-85 batch-72 = 3.1529925763607025e-06

Training epoch-85 batch-73
Running loss of epoch-85 batch-73 = 3.132270649075508e-06

Training epoch-85 batch-74
Running loss of epoch-85 batch-74 = 2.484070137143135e-06

Training epoch-85 batch-75
Running loss of epoch-85 batch-75 = 2.6689376682043076e-06

Training epoch-85 batch-76
Running loss of epoch-85 batch-76 = 3.7427525967359543e-06

Training epoch-85 batch-77
Running loss of epoch-85 batch-77 = 2.7818605303764343e-06

Training epoch-85 batch-78
Running loss of epoch-85 batch-78 = 4.1066668927669525e-06

Training epoch-85 batch-79
Running loss of epoch-85 batch-79 = 4.152301698923111e-06

Training epoch-85 batch-80
Running loss of epoch-85 batch-80 = 3.3990945667028427e-06

Training epoch-85 batch-81
Running loss of epoch-85 batch-81 = 6.707850843667984e-06

Training epoch-85 batch-82
Running loss of epoch-85 batch-82 = 2.3227185010910034e-06

Training epoch-85 batch-83
Running loss of epoch-85 batch-83 = 2.0568259060382843e-06

Training epoch-85 batch-84
Running loss of epoch-85 batch-84 = 1.2095551937818527e-06

Training epoch-85 batch-85
Running loss of epoch-85 batch-85 = 2.93925404548645e-06

Training epoch-85 batch-86
Running loss of epoch-85 batch-86 = 3.0174851417541504e-06

Training epoch-85 batch-87
Running loss of epoch-85 batch-87 = 5.2496325224637985e-06

Training epoch-85 batch-88
Running loss of epoch-85 batch-88 = 3.04332934319973e-06

Training epoch-85 batch-89
Running loss of epoch-85 batch-89 = 2.901768311858177e-06

Training epoch-85 batch-90
Running loss of epoch-85 batch-90 = 1.251930370926857e-06

Training epoch-85 batch-91
Running loss of epoch-85 batch-91 = 6.377929821610451e-06

Training epoch-85 batch-92
Running loss of epoch-85 batch-92 = 3.65404412150383e-06

Training epoch-85 batch-93
Running loss of epoch-85 batch-93 = 2.4668406695127487e-06

Training epoch-85 batch-94
Running loss of epoch-85 batch-94 = 2.1297018975019455e-06

Training epoch-85 batch-95
Running loss of epoch-85 batch-95 = 1.298799179494381e-05

Training epoch-85 batch-96
Running loss of epoch-85 batch-96 = 9.65828076004982e-06

Training epoch-85 batch-97
Running loss of epoch-85 batch-97 = 5.753710865974426e-06

Training epoch-85 batch-98
Running loss of epoch-85 batch-98 = 2.240994945168495e-06

Training epoch-85 batch-99
Running loss of epoch-85 batch-99 = 2.6230700314044952e-06

Training epoch-85 batch-100
Running loss of epoch-85 batch-100 = 4.425877705216408e-06

Training epoch-85 batch-101
Running loss of epoch-85 batch-101 = 2.6421621441841125e-06

Training epoch-85 batch-102
Running loss of epoch-85 batch-102 = 1.9471626728773117e-06

Training epoch-85 batch-103
Running loss of epoch-85 batch-103 = 4.741363227367401e-06

Training epoch-85 batch-104
Running loss of epoch-85 batch-104 = 2.720160409808159e-06

Training epoch-85 batch-105
Running loss of epoch-85 batch-105 = 2.5064218789339066e-06

Training epoch-85 batch-106
Running loss of epoch-85 batch-106 = 1.4763791114091873e-06

Training epoch-85 batch-107
Running loss of epoch-85 batch-107 = 2.5478657335042953e-06

Training epoch-85 batch-108
Running loss of epoch-85 batch-108 = 2.2596213966608047e-06

Training epoch-85 batch-109
Running loss of epoch-85 batch-109 = 3.8084108382463455e-06

Training epoch-85 batch-110
Running loss of epoch-85 batch-110 = 3.881519660353661e-06

Training epoch-85 batch-111
Running loss of epoch-85 batch-111 = 2.9674265533685684e-06

Training epoch-85 batch-112
Running loss of epoch-85 batch-112 = 5.67571260035038e-06

Training epoch-85 batch-113
Running loss of epoch-85 batch-113 = 1.6812700778245926e-06

Training epoch-85 batch-114
Running loss of epoch-85 batch-114 = 3.0242372304201126e-06

Training epoch-85 batch-115
Running loss of epoch-85 batch-115 = 5.059177055954933e-06

Training epoch-85 batch-116
Running loss of epoch-85 batch-116 = 2.430286258459091e-06

Training epoch-85 batch-117
Running loss of epoch-85 batch-117 = 5.356036126613617e-06

Training epoch-85 batch-118
Running loss of epoch-85 batch-118 = 2.264278009533882e-06

Training epoch-85 batch-119
Running loss of epoch-85 batch-119 = 2.005835995078087e-06

Training epoch-85 batch-120
Running loss of epoch-85 batch-120 = 4.171393811702728e-06

Training epoch-85 batch-121
Running loss of epoch-85 batch-121 = 7.5623393058776855e-06

Training epoch-85 batch-122
Running loss of epoch-85 batch-122 = 4.652887582778931e-06

Training epoch-85 batch-123
Running loss of epoch-85 batch-123 = 2.7478672564029694e-06

Training epoch-85 batch-124
Running loss of epoch-85 batch-124 = 3.2954849302768707e-06

Training epoch-85 batch-125
Running loss of epoch-85 batch-125 = 3.6391429603099823e-06

Training epoch-85 batch-126
Running loss of epoch-85 batch-126 = 5.160924047231674e-06

Training epoch-85 batch-127
Running loss of epoch-85 batch-127 = 2.5851186364889145e-06

Training epoch-85 batch-128
Running loss of epoch-85 batch-128 = 6.784452125430107e-06

Training epoch-85 batch-129
Running loss of epoch-85 batch-129 = 2.945074811577797e-06

Training epoch-85 batch-130
Running loss of epoch-85 batch-130 = 2.3138709366321564e-06

Training epoch-85 batch-131
Running loss of epoch-85 batch-131 = 5.585141479969025e-06

Training epoch-85 batch-132
Running loss of epoch-85 batch-132 = 5.624955520033836e-06

Training epoch-85 batch-133
Running loss of epoch-85 batch-133 = 2.850545570254326e-06

Training epoch-85 batch-134
Running loss of epoch-85 batch-134 = 1.3516983017325401e-05

Training epoch-85 batch-135
Running loss of epoch-85 batch-135 = 1.5084631741046906e-05

Training epoch-85 batch-136
Running loss of epoch-85 batch-136 = 3.237510100007057e-06

Training epoch-85 batch-137
Running loss of epoch-85 batch-137 = 1.2263190001249313e-06

Training epoch-85 batch-138
Running loss of epoch-85 batch-138 = 1.5690457075834274e-06

Training epoch-85 batch-139
Running loss of epoch-85 batch-139 = 3.4726690500974655e-06

Training epoch-85 batch-140
Running loss of epoch-85 batch-140 = 2.7210917323827744e-06

Training epoch-85 batch-141
Running loss of epoch-85 batch-141 = 3.981869667768478e-06

Training epoch-85 batch-142
Running loss of epoch-85 batch-142 = 8.112750947475433e-06

Training epoch-85 batch-143
Running loss of epoch-85 batch-143 = 1.8035061657428741e-06

Training epoch-85 batch-144
Running loss of epoch-85 batch-144 = 1.0065268725156784e-06

Training epoch-85 batch-145
Running loss of epoch-85 batch-145 = 4.551606252789497e-06

Training epoch-85 batch-146
Running loss of epoch-85 batch-146 = 4.187459126114845e-06

Training epoch-85 batch-147
Running loss of epoch-85 batch-147 = 2.9373914003372192e-06

Training epoch-85 batch-148
Running loss of epoch-85 batch-148 = 2.8391368687152863e-06

Training epoch-85 batch-149
Running loss of epoch-85 batch-149 = 5.299458280205727e-06

Training epoch-85 batch-150
Running loss of epoch-85 batch-150 = 2.2782478481531143e-06

Training epoch-85 batch-151
Running loss of epoch-85 batch-151 = 2.5103799998760223e-06

Training epoch-85 batch-152
Running loss of epoch-85 batch-152 = 4.923436790704727e-06

Training epoch-85 batch-153
Running loss of epoch-85 batch-153 = 1.4195684343576431e-06

Training epoch-85 batch-154
Running loss of epoch-85 batch-154 = 2.8419308364391327e-06

Training epoch-85 batch-155
Running loss of epoch-85 batch-155 = 6.49038702249527e-06

Training epoch-85 batch-156
Running loss of epoch-85 batch-156 = 3.5017728805541992e-06

Training epoch-85 batch-157
Running loss of epoch-85 batch-157 = 3.784894943237305e-05

Finished training epoch-85.



Average train loss at epoch-85 = 4.233850538730621e-06

Started Evaluation

Average val loss at epoch-85 = 1.8232896350028474

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.36 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.18 %

Finished Evaluation



Started training epoch-86


Training epoch-86 batch-1
Running loss of epoch-86 batch-1 = 3.2386742532253265e-06

Training epoch-86 batch-2
Running loss of epoch-86 batch-2 = 3.6999117583036423e-06

Training epoch-86 batch-3
Running loss of epoch-86 batch-3 = 4.710862413048744e-06

Training epoch-86 batch-4
Running loss of epoch-86 batch-4 = 1.493142917752266e-06

Training epoch-86 batch-5
Running loss of epoch-86 batch-5 = 7.115770131349564e-06

Training epoch-86 batch-6
Running loss of epoch-86 batch-6 = 8.918344974517822e-06

Training epoch-86 batch-7
Running loss of epoch-86 batch-7 = 2.7581118047237396e-06

Training epoch-86 batch-8
Running loss of epoch-86 batch-8 = 2.5187619030475616e-06

Training epoch-86 batch-9
Running loss of epoch-86 batch-9 = 3.527151420712471e-06

Training epoch-86 batch-10
Running loss of epoch-86 batch-10 = 2.2633466869592667e-06

Training epoch-86 batch-11
Running loss of epoch-86 batch-11 = 1.768115907907486e-06

Training epoch-86 batch-12
Running loss of epoch-86 batch-12 = 2.4123582988977432e-06

Training epoch-86 batch-13
Running loss of epoch-86 batch-13 = 3.403984010219574e-06

Training epoch-86 batch-14
Running loss of epoch-86 batch-14 = 4.783971235156059e-06

Training epoch-86 batch-15
Running loss of epoch-86 batch-15 = 2.003507688641548e-06

Training epoch-86 batch-16
Running loss of epoch-86 batch-16 = 3.899913281202316e-06

Training epoch-86 batch-17
Running loss of epoch-86 batch-17 = 3.5457778722047806e-06

Training epoch-86 batch-18
Running loss of epoch-86 batch-18 = 7.088063284754753e-06

Training epoch-86 batch-19
Running loss of epoch-86 batch-19 = 1.0319985449314117e-05

Training epoch-86 batch-20
Running loss of epoch-86 batch-20 = 3.773253411054611e-06

Training epoch-86 batch-21
Running loss of epoch-86 batch-21 = 3.802357241511345e-06

Training epoch-86 batch-22
Running loss of epoch-86 batch-22 = 2.72924080491066e-06

Training epoch-86 batch-23
Running loss of epoch-86 batch-23 = 5.807261914014816e-06

Training epoch-86 batch-24
Running loss of epoch-86 batch-24 = 7.168855518102646e-06

Training epoch-86 batch-25
Running loss of epoch-86 batch-25 = 1.3390090316534042e-06

Training epoch-86 batch-26
Running loss of epoch-86 batch-26 = 3.6491546779870987e-06

Training epoch-86 batch-27
Running loss of epoch-86 batch-27 = 5.29363751411438e-06

Training epoch-86 batch-28
Running loss of epoch-86 batch-28 = 2.6903580874204636e-06

Training epoch-86 batch-29
Running loss of epoch-86 batch-29 = 2.0766165107488632e-06

Training epoch-86 batch-30
Running loss of epoch-86 batch-30 = 4.694797098636627e-06

Training epoch-86 batch-31
Running loss of epoch-86 batch-31 = 3.911089152097702e-06

Training epoch-86 batch-32
Running loss of epoch-86 batch-32 = 5.310866981744766e-06

Training epoch-86 batch-33
Running loss of epoch-86 batch-33 = 1.4944933354854584e-05

Training epoch-86 batch-34
Running loss of epoch-86 batch-34 = 1.4016404747962952e-06

Training epoch-86 batch-35
Running loss of epoch-86 batch-35 = 3.664987161755562e-06

Training epoch-86 batch-36
Running loss of epoch-86 batch-36 = 5.982816219329834e-06

Training epoch-86 batch-37
Running loss of epoch-86 batch-37 = 3.963243216276169e-06

Training epoch-86 batch-38
Running loss of epoch-86 batch-38 = 4.187459126114845e-06

Training epoch-86 batch-39
Running loss of epoch-86 batch-39 = 5.077105015516281e-06

Training epoch-86 batch-40
Running loss of epoch-86 batch-40 = 2.0512379705905914e-06

Training epoch-86 batch-41
Running loss of epoch-86 batch-41 = 1.4672987163066864e-06

Training epoch-86 batch-42
Running loss of epoch-86 batch-42 = 4.122033715248108e-06

Training epoch-86 batch-43
Running loss of epoch-86 batch-43 = 4.198169335722923e-06

Training epoch-86 batch-44
Running loss of epoch-86 batch-44 = 4.020053893327713e-06

Training epoch-86 batch-45
Running loss of epoch-86 batch-45 = 2.357177436351776e-06

Training epoch-86 batch-46
Running loss of epoch-86 batch-46 = 4.463363438844681e-06

Training epoch-86 batch-47
Running loss of epoch-86 batch-47 = 3.1855888664722443e-06

Training epoch-86 batch-48
Running loss of epoch-86 batch-48 = 5.0817616283893585e-06

Training epoch-86 batch-49
Running loss of epoch-86 batch-49 = 4.100147634744644e-06

Training epoch-86 batch-50
Running loss of epoch-86 batch-50 = 1.583946868777275e-06

Training epoch-86 batch-51
Running loss of epoch-86 batch-51 = 1.4826655387878418e-06

Training epoch-86 batch-52
Running loss of epoch-86 batch-52 = 5.426118150353432e-06

Training epoch-86 batch-53
Running loss of epoch-86 batch-53 = 2.7222558856010437e-06

Training epoch-86 batch-54
Running loss of epoch-86 batch-54 = 4.477333277463913e-06

Training epoch-86 batch-55
Running loss of epoch-86 batch-55 = 1.1700205504894257e-05

Training epoch-86 batch-56
Running loss of epoch-86 batch-56 = 1.5986151993274689e-06

Training epoch-86 batch-57
Running loss of epoch-86 batch-57 = 3.3474061638116837e-06

Training epoch-86 batch-58
Running loss of epoch-86 batch-58 = 3.725523129105568e-06

Training epoch-86 batch-59
Running loss of epoch-86 batch-59 = 3.407243639230728e-06

Training epoch-86 batch-60
Running loss of epoch-86 batch-60 = 2.376735210418701e-06

Training epoch-86 batch-61
Running loss of epoch-86 batch-61 = 2.2689346224069595e-06

Training epoch-86 batch-62
Running loss of epoch-86 batch-62 = 3.0044466257095337e-06

Training epoch-86 batch-63
Running loss of epoch-86 batch-63 = 1.6149133443832397e-06

Training epoch-86 batch-64
Running loss of epoch-86 batch-64 = 1.1818483471870422e-06

Training epoch-86 batch-65
Running loss of epoch-86 batch-65 = 2.9101502150297165e-06

Training epoch-86 batch-66
Running loss of epoch-86 batch-66 = 4.051718860864639e-06

Training epoch-86 batch-67
Running loss of epoch-86 batch-67 = 8.057570084929466e-06

Training epoch-86 batch-68
Running loss of epoch-86 batch-68 = 2.5723129510879517e-06

Training epoch-86 batch-69
Running loss of epoch-86 batch-69 = 2.6614870876073837e-06

Training epoch-86 batch-70
Running loss of epoch-86 batch-70 = 2.2139865905046463e-06

Training epoch-86 batch-71
Running loss of epoch-86 batch-71 = 2.757180482149124e-06

Training epoch-86 batch-72
Running loss of epoch-86 batch-72 = 3.7031713873147964e-06

Training epoch-86 batch-73
Running loss of epoch-86 batch-73 = 4.242872819304466e-06

Training epoch-86 batch-74
Running loss of epoch-86 batch-74 = 4.2940955609083176e-06

Training epoch-86 batch-75
Running loss of epoch-86 batch-75 = 2.576736733317375e-06

Training epoch-86 batch-76
Running loss of epoch-86 batch-76 = 1.9744038581848145e-06

Training epoch-86 batch-77
Running loss of epoch-86 batch-77 = 2.723652869462967e-06

Training epoch-86 batch-78
Running loss of epoch-86 batch-78 = 5.139270797371864e-06

Training epoch-86 batch-79
Running loss of epoch-86 batch-79 = 2.400483936071396e-06

Training epoch-86 batch-80
Running loss of epoch-86 batch-80 = 9.482959285378456e-06

Training epoch-86 batch-81
Running loss of epoch-86 batch-81 = 2.6116613298654556e-06

Training epoch-86 batch-82
Running loss of epoch-86 batch-82 = 5.776062607765198e-06

Training epoch-86 batch-83
Running loss of epoch-86 batch-83 = 1.3924902305006981e-05

Training epoch-86 batch-84
Running loss of epoch-86 batch-84 = 4.417728632688522e-06

Training epoch-86 batch-85
Running loss of epoch-86 batch-85 = 1.9276048988103867e-06

Training epoch-86 batch-86
Running loss of epoch-86 batch-86 = 4.506204277276993e-06

Training epoch-86 batch-87
Running loss of epoch-86 batch-87 = 3.607477992773056e-06

Training epoch-86 batch-88
Running loss of epoch-86 batch-88 = 3.023305907845497e-06

Training epoch-86 batch-89
Running loss of epoch-86 batch-89 = 3.3616088330745697e-06

Training epoch-86 batch-90
Running loss of epoch-86 batch-90 = 2.4328473955392838e-06

Training epoch-86 batch-91
Running loss of epoch-86 batch-91 = 5.591660737991333e-06

Training epoch-86 batch-92
Running loss of epoch-86 batch-92 = 2.9136426746845245e-06

Training epoch-86 batch-93
Running loss of epoch-86 batch-93 = 2.1029263734817505e-06

Training epoch-86 batch-94
Running loss of epoch-86 batch-94 = 7.027992978692055e-06

Training epoch-86 batch-95
Running loss of epoch-86 batch-95 = 2.3620668798685074e-06

Training epoch-86 batch-96
Running loss of epoch-86 batch-96 = 4.514586180448532e-06

Training epoch-86 batch-97
Running loss of epoch-86 batch-97 = 5.097361281514168e-06

Training epoch-86 batch-98
Running loss of epoch-86 batch-98 = 5.001667886972427e-06

Training epoch-86 batch-99
Running loss of epoch-86 batch-99 = 2.3294705897569656e-06

Training epoch-86 batch-100
Running loss of epoch-86 batch-100 = 2.916203811764717e-06

Training epoch-86 batch-101
Running loss of epoch-86 batch-101 = 1.5955884009599686e-05

Training epoch-86 batch-102
Running loss of epoch-86 batch-102 = 4.6209897845983505e-06

Training epoch-86 batch-103
Running loss of epoch-86 batch-103 = 1.4971010386943817e-06

Training epoch-86 batch-104
Running loss of epoch-86 batch-104 = 3.4994445741176605e-06

Training epoch-86 batch-105
Running loss of epoch-86 batch-105 = 3.159279003739357e-06

Training epoch-86 batch-106
Running loss of epoch-86 batch-106 = 3.6277342587709427e-06

Training epoch-86 batch-107
Running loss of epoch-86 batch-107 = 3.2794196158647537e-06

Training epoch-86 batch-108
Running loss of epoch-86 batch-108 = 3.316439688205719e-06

Training epoch-86 batch-109
Running loss of epoch-86 batch-109 = 2.6065390557050705e-06

Training epoch-86 batch-110
Running loss of epoch-86 batch-110 = 1.6479752957820892e-06

Training epoch-86 batch-111
Running loss of epoch-86 batch-111 = 2.2382009774446487e-06

Training epoch-86 batch-112
Running loss of epoch-86 batch-112 = 7.811002433300018e-06

Training epoch-86 batch-113
Running loss of epoch-86 batch-113 = 5.424488335847855e-06

Training epoch-86 batch-114
Running loss of epoch-86 batch-114 = 1.0831281542778015e-06

Training epoch-86 batch-115
Running loss of epoch-86 batch-115 = 2.1953601390123367e-06

Training epoch-86 batch-116
Running loss of epoch-86 batch-116 = 4.185829311609268e-06

Training epoch-86 batch-117
Running loss of epoch-86 batch-117 = 5.071284249424934e-06

Training epoch-86 batch-118
Running loss of epoch-86 batch-118 = 2.162531018257141e-06

Training epoch-86 batch-119
Running loss of epoch-86 batch-119 = 2.007465809583664e-06

Training epoch-86 batch-120
Running loss of epoch-86 batch-120 = 5.499226972460747e-06

Training epoch-86 batch-121
Running loss of epoch-86 batch-121 = 4.826812073588371e-06

Training epoch-86 batch-122
Running loss of epoch-86 batch-122 = 2.4279579520225525e-06

Training epoch-86 batch-123
Running loss of epoch-86 batch-123 = 3.054272383451462e-06

Training epoch-86 batch-124
Running loss of epoch-86 batch-124 = 1.013721339404583e-05

Training epoch-86 batch-125
Running loss of epoch-86 batch-125 = 3.844965249300003e-06

Training epoch-86 batch-126
Running loss of epoch-86 batch-126 = 1.1205673217773438e-05

Training epoch-86 batch-127
Running loss of epoch-86 batch-127 = 1.9138678908348083e-06

Training epoch-86 batch-128
Running loss of epoch-86 batch-128 = 2.0796433091163635e-06

Training epoch-86 batch-129
Running loss of epoch-86 batch-129 = 9.186333045363426e-06

Training epoch-86 batch-130
Running loss of epoch-86 batch-130 = 1.1630123481154442e-05

Training epoch-86 batch-131
Running loss of epoch-86 batch-131 = 6.957678124308586e-06

Training epoch-86 batch-132
Running loss of epoch-86 batch-132 = 1.6978010535240173e-06

Training epoch-86 batch-133
Running loss of epoch-86 batch-133 = 2.8258655220270157e-06

Training epoch-86 batch-134
Running loss of epoch-86 batch-134 = 4.142988473176956e-06

Training epoch-86 batch-135
Running loss of epoch-86 batch-135 = 4.720874130725861e-06

Training epoch-86 batch-136
Running loss of epoch-86 batch-136 = 1.373467966914177e-06

Training epoch-86 batch-137
Running loss of epoch-86 batch-137 = 3.8028229027986526e-06

Training epoch-86 batch-138
Running loss of epoch-86 batch-138 = 5.0747767090797424e-06

Training epoch-86 batch-139
Running loss of epoch-86 batch-139 = 3.92599031329155e-06

Training epoch-86 batch-140
Running loss of epoch-86 batch-140 = 5.024252459406853e-06

Training epoch-86 batch-141
Running loss of epoch-86 batch-141 = 2.9318034648895264e-06

Training epoch-86 batch-142
Running loss of epoch-86 batch-142 = 5.119713023304939e-06

Training epoch-86 batch-143
Running loss of epoch-86 batch-143 = 1.7578713595867157e-06

Training epoch-86 batch-144
Running loss of epoch-86 batch-144 = 3.6847777664661407e-06

Training epoch-86 batch-145
Running loss of epoch-86 batch-145 = 2.2745225578546524e-06

Training epoch-86 batch-146
Running loss of epoch-86 batch-146 = 5.6801363825798035e-06

Training epoch-86 batch-147
Running loss of epoch-86 batch-147 = 1.6577541828155518e-06

Training epoch-86 batch-148
Running loss of epoch-86 batch-148 = 1.7005950212478638e-06

Training epoch-86 batch-149
Running loss of epoch-86 batch-149 = 3.4195836633443832e-06

Training epoch-86 batch-150
Running loss of epoch-86 batch-150 = 7.780501618981361e-06

Training epoch-86 batch-151
Running loss of epoch-86 batch-151 = 2.579065039753914e-06

Training epoch-86 batch-152
Running loss of epoch-86 batch-152 = 1.8612481653690338e-06

Training epoch-86 batch-153
Running loss of epoch-86 batch-153 = 6.331363692879677e-06

Training epoch-86 batch-154
Running loss of epoch-86 batch-154 = 1.919921487569809e-06

Training epoch-86 batch-155
Running loss of epoch-86 batch-155 = 2.377200871706009e-06

Training epoch-86 batch-156
Running loss of epoch-86 batch-156 = 4.109693691134453e-06

Training epoch-86 batch-157
Running loss of epoch-86 batch-157 = 1.3597309589385986e-06

Finished training epoch-86.



Average train loss at epoch-86 = 4.092037677764893e-06

Started Evaluation

Average val loss at epoch-86 = 1.8264574850907662

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-87


Training epoch-87 batch-1
Running loss of epoch-87 batch-1 = 8.356291800737381e-07

Training epoch-87 batch-2
Running loss of epoch-87 batch-2 = 1.7371494323015213e-06

Training epoch-87 batch-3
Running loss of epoch-87 batch-3 = 3.400258719921112e-06

Training epoch-87 batch-4
Running loss of epoch-87 batch-4 = 4.924135282635689e-06

Training epoch-87 batch-5
Running loss of epoch-87 batch-5 = 5.205860361456871e-06

Training epoch-87 batch-6
Running loss of epoch-87 batch-6 = 6.223563104867935e-06

Training epoch-87 batch-7
Running loss of epoch-87 batch-7 = 2.8542708605527878e-06

Training epoch-87 batch-8
Running loss of epoch-87 batch-8 = 8.994946256279945e-06

Training epoch-87 batch-9
Running loss of epoch-87 batch-9 = 3.59327532351017e-06

Training epoch-87 batch-10
Running loss of epoch-87 batch-10 = 2.459157258272171e-06

Training epoch-87 batch-11
Running loss of epoch-87 batch-11 = 2.6545021682977676e-06

Training epoch-87 batch-12
Running loss of epoch-87 batch-12 = 4.325760528445244e-06

Training epoch-87 batch-13
Running loss of epoch-87 batch-13 = 8.781440556049347e-06

Training epoch-87 batch-14
Running loss of epoch-87 batch-14 = 1.3508833944797516e-06

Training epoch-87 batch-15
Running loss of epoch-87 batch-15 = 8.978880941867828e-06

Training epoch-87 batch-16
Running loss of epoch-87 batch-16 = 2.630753442645073e-06

Training epoch-87 batch-17
Running loss of epoch-87 batch-17 = 2.8724316507577896e-06

Training epoch-87 batch-18
Running loss of epoch-87 batch-18 = 7.844064384698868e-07

Training epoch-87 batch-19
Running loss of epoch-87 batch-19 = 6.8694353103637695e-06

Training epoch-87 batch-20
Running loss of epoch-87 batch-20 = 5.2102841436862946e-06

Training epoch-87 batch-21
Running loss of epoch-87 batch-21 = 3.13599593937397e-06

Training epoch-87 batch-22
Running loss of epoch-87 batch-22 = 5.85382804274559e-06

Training epoch-87 batch-23
Running loss of epoch-87 batch-23 = 3.219814971089363e-06

Training epoch-87 batch-24
Running loss of epoch-87 batch-24 = 3.750203177332878e-06

Training epoch-87 batch-25
Running loss of epoch-87 batch-25 = 2.9194634407758713e-06

Training epoch-87 batch-26
Running loss of epoch-87 batch-26 = 3.6670826375484467e-07

Training epoch-87 batch-27
Running loss of epoch-87 batch-27 = 1.7764978110790253e-06

Training epoch-87 batch-28
Running loss of epoch-87 batch-28 = 3.831228241324425e-06

Training epoch-87 batch-29
Running loss of epoch-87 batch-29 = 5.162553861737251e-06

Training epoch-87 batch-30
Running loss of epoch-87 batch-30 = 5.524139851331711e-06

Training epoch-87 batch-31
Running loss of epoch-87 batch-31 = 6.452202796936035e-06

Training epoch-87 batch-32
Running loss of epoch-87 batch-32 = 4.700617864727974e-06

Training epoch-87 batch-33
Running loss of epoch-87 batch-33 = 3.373716026544571e-06

Training epoch-87 batch-34
Running loss of epoch-87 batch-34 = 1.5888363122940063e-06

Training epoch-87 batch-35
Running loss of epoch-87 batch-35 = 2.72504985332489e-06

Training epoch-87 batch-36
Running loss of epoch-87 batch-36 = 2.0244624465703964e-06

Training epoch-87 batch-37
Running loss of epoch-87 batch-37 = 2.123182639479637e-06

Training epoch-87 batch-38
Running loss of epoch-87 batch-38 = 1.908978447318077e-06

Training epoch-87 batch-39
Running loss of epoch-87 batch-39 = 3.0032824724912643e-06

Training epoch-87 batch-40
Running loss of epoch-87 batch-40 = 3.0186492949724197e-06

Training epoch-87 batch-41
Running loss of epoch-87 batch-41 = 3.370223566889763e-06

Training epoch-87 batch-42
Running loss of epoch-87 batch-42 = 2.983957529067993e-06

Training epoch-87 batch-43
Running loss of epoch-87 batch-43 = 2.8801150619983673e-06

Training epoch-87 batch-44
Running loss of epoch-87 batch-44 = 2.596992999315262e-06

Training epoch-87 batch-45
Running loss of epoch-87 batch-45 = 3.5604462027549744e-06

Training epoch-87 batch-46
Running loss of epoch-87 batch-46 = 2.6156194508075714e-06

Training epoch-87 batch-47
Running loss of epoch-87 batch-47 = 2.1550804376602173e-06

Training epoch-87 batch-48
Running loss of epoch-87 batch-48 = 4.2638275772333145e-06

Training epoch-87 batch-49
Running loss of epoch-87 batch-49 = 1.1206371709704399e-05

Training epoch-87 batch-50
Running loss of epoch-87 batch-50 = 2.9064249247312546e-06

Training epoch-87 batch-51
Running loss of epoch-87 batch-51 = 2.800021320581436e-06

Training epoch-87 batch-52
Running loss of epoch-87 batch-52 = 2.973712980747223e-06

Training epoch-87 batch-53
Running loss of epoch-87 batch-53 = 5.100853741168976e-06

Training epoch-87 batch-54
Running loss of epoch-87 batch-54 = 2.9851216822862625e-06

Training epoch-87 batch-55
Running loss of epoch-87 batch-55 = 2.7171336114406586e-06

Training epoch-87 batch-56
Running loss of epoch-87 batch-56 = 5.2193645387887955e-06

Training epoch-87 batch-57
Running loss of epoch-87 batch-57 = 6.547430530190468e-06

Training epoch-87 batch-58
Running loss of epoch-87 batch-58 = 7.424270734190941e-06

Training epoch-87 batch-59
Running loss of epoch-87 batch-59 = 4.423316568136215e-06

Training epoch-87 batch-60
Running loss of epoch-87 batch-60 = 4.278961569070816e-06

Training epoch-87 batch-61
Running loss of epoch-87 batch-61 = 3.6186538636684418e-06

Training epoch-87 batch-62
Running loss of epoch-87 batch-62 = 6.428221240639687e-06

Training epoch-87 batch-63
Running loss of epoch-87 batch-63 = 7.799826562404633e-06

Training epoch-87 batch-64
Running loss of epoch-87 batch-64 = 3.800494596362114e-06

Training epoch-87 batch-65
Running loss of epoch-87 batch-65 = 1.898268237709999e-06

Training epoch-87 batch-66
Running loss of epoch-87 batch-66 = 2.748565748333931e-06

Training epoch-87 batch-67
Running loss of epoch-87 batch-67 = 7.2820112109184265e-06

Training epoch-87 batch-68
Running loss of epoch-87 batch-68 = 2.964399755001068e-06

Training epoch-87 batch-69
Running loss of epoch-87 batch-69 = 5.33577986061573e-06

Training epoch-87 batch-70
Running loss of epoch-87 batch-70 = 1.4763791114091873e-06

Training epoch-87 batch-71
Running loss of epoch-87 batch-71 = 4.931353032588959e-06

Training epoch-87 batch-72
Running loss of epoch-87 batch-72 = 3.654276952147484e-06

Training epoch-87 batch-73
Running loss of epoch-87 batch-73 = 2.3192260414361954e-06

Training epoch-87 batch-74
Running loss of epoch-87 batch-74 = 2.5990884751081467e-06

Training epoch-87 batch-75
Running loss of epoch-87 batch-75 = 4.874309524893761e-06

Training epoch-87 batch-76
Running loss of epoch-87 batch-76 = 2.4279579520225525e-06

Training epoch-87 batch-77
Running loss of epoch-87 batch-77 = 1.473352313041687e-06

Training epoch-87 batch-78
Running loss of epoch-87 batch-78 = 4.296191036701202e-06

Training epoch-87 batch-79
Running loss of epoch-87 batch-79 = 3.209104761481285e-06

Training epoch-87 batch-80
Running loss of epoch-87 batch-80 = 3.284309059381485e-06

Training epoch-87 batch-81
Running loss of epoch-87 batch-81 = 2.2426247596740723e-06

Training epoch-87 batch-82
Running loss of epoch-87 batch-82 = 2.6514753699302673e-06

Training epoch-87 batch-83
Running loss of epoch-87 batch-83 = 2.351822331547737e-06

Training epoch-87 batch-84
Running loss of epoch-87 batch-84 = 1.296401023864746e-06

Training epoch-87 batch-85
Running loss of epoch-87 batch-85 = 5.026813596487045e-06

Training epoch-87 batch-86
Running loss of epoch-87 batch-86 = 5.054287612438202e-06

Training epoch-87 batch-87
Running loss of epoch-87 batch-87 = 4.716916009783745e-06

Training epoch-87 batch-88
Running loss of epoch-87 batch-88 = 4.33996319770813e-06

Training epoch-87 batch-89
Running loss of epoch-87 batch-89 = 7.683411240577698e-07

Training epoch-87 batch-90
Running loss of epoch-87 batch-90 = 7.99586996436119e-06

Training epoch-87 batch-91
Running loss of epoch-87 batch-91 = 1.4223624020814896e-06

Training epoch-87 batch-92
Running loss of epoch-87 batch-92 = 4.141591489315033e-06

Training epoch-87 batch-93
Running loss of epoch-87 batch-93 = 2.5883782655000687e-06

Training epoch-87 batch-94
Running loss of epoch-87 batch-94 = 6.041023880243301e-06

Training epoch-87 batch-95
Running loss of epoch-87 batch-95 = 1.3115350157022476e-06

Training epoch-87 batch-96
Running loss of epoch-87 batch-96 = 3.1224917620420456e-06

Training epoch-87 batch-97
Running loss of epoch-87 batch-97 = 9.13604162633419e-06

Training epoch-87 batch-98
Running loss of epoch-87 batch-98 = 3.2531097531318665e-06

Training epoch-87 batch-99
Running loss of epoch-87 batch-99 = 3.3064279705286026e-06

Training epoch-87 batch-100
Running loss of epoch-87 batch-100 = 7.612630724906921e-06

Training epoch-87 batch-101
Running loss of epoch-87 batch-101 = 1.3506971299648285e-05

Training epoch-87 batch-102
Running loss of epoch-87 batch-102 = 7.137656211853027e-06

Training epoch-87 batch-103
Running loss of epoch-87 batch-103 = 3.6081764847040176e-06

Training epoch-87 batch-104
Running loss of epoch-87 batch-104 = 2.7185305953025818e-06

Training epoch-87 batch-105
Running loss of epoch-87 batch-105 = 2.945074811577797e-06

Training epoch-87 batch-106
Running loss of epoch-87 batch-106 = 3.777211531996727e-06

Training epoch-87 batch-107
Running loss of epoch-87 batch-107 = 6.464775651693344e-06

Training epoch-87 batch-108
Running loss of epoch-87 batch-108 = 3.527151420712471e-06

Training epoch-87 batch-109
Running loss of epoch-87 batch-109 = 4.775123670697212e-06

Training epoch-87 batch-110
Running loss of epoch-87 batch-110 = 1.194654032588005e-06

Training epoch-87 batch-111
Running loss of epoch-87 batch-111 = 5.498295649886131e-06

Training epoch-87 batch-112
Running loss of epoch-87 batch-112 = 1.4225952327251434e-06

Training epoch-87 batch-113
Running loss of epoch-87 batch-113 = 3.491295501589775e-06

Training epoch-87 batch-114
Running loss of epoch-87 batch-114 = 1.982087269425392e-06

Training epoch-87 batch-115
Running loss of epoch-87 batch-115 = 7.00005330145359e-06

Training epoch-87 batch-116
Running loss of epoch-87 batch-116 = 2.7636997401714325e-06

Training epoch-87 batch-117
Running loss of epoch-87 batch-117 = 5.141133442521095e-06

Training epoch-87 batch-118
Running loss of epoch-87 batch-118 = 4.098750650882721e-06

Training epoch-87 batch-119
Running loss of epoch-87 batch-119 = 4.967208951711655e-06

Training epoch-87 batch-120
Running loss of epoch-87 batch-120 = 4.926696419715881e-07

Training epoch-87 batch-121
Running loss of epoch-87 batch-121 = 4.460103809833527e-06

Training epoch-87 batch-122
Running loss of epoch-87 batch-122 = 3.7918798625469208e-06

Training epoch-87 batch-123
Running loss of epoch-87 batch-123 = 2.660788595676422e-06

Training epoch-87 batch-124
Running loss of epoch-87 batch-124 = 3.1171366572380066e-06

Training epoch-87 batch-125
Running loss of epoch-87 batch-125 = 2.5921035557985306e-06

Training epoch-87 batch-126
Running loss of epoch-87 batch-126 = 5.137873813509941e-06

Training epoch-87 batch-127
Running loss of epoch-87 batch-127 = 5.2175018936395645e-06

Training epoch-87 batch-128
Running loss of epoch-87 batch-128 = 8.311355486512184e-06

Training epoch-87 batch-129
Running loss of epoch-87 batch-129 = 6.150221452116966e-06

Training epoch-87 batch-130
Running loss of epoch-87 batch-130 = 3.720168024301529e-06

Training epoch-87 batch-131
Running loss of epoch-87 batch-131 = 2.32132151722908e-06

Training epoch-87 batch-132
Running loss of epoch-87 batch-132 = 1.8749851733446121e-06

Training epoch-87 batch-133
Running loss of epoch-87 batch-133 = 1.3909302651882172e-06

Training epoch-87 batch-134
Running loss of epoch-87 batch-134 = 3.753695636987686e-06

Training epoch-87 batch-135
Running loss of epoch-87 batch-135 = 2.5366898626089096e-06

Training epoch-87 batch-136
Running loss of epoch-87 batch-136 = 8.551869541406631e-07

Training epoch-87 batch-137
Running loss of epoch-87 batch-137 = 2.7597416192293167e-06

Training epoch-87 batch-138
Running loss of epoch-87 batch-138 = 3.200024366378784e-06

Training epoch-87 batch-139
Running loss of epoch-87 batch-139 = 3.5136472433805466e-06

Training epoch-87 batch-140
Running loss of epoch-87 batch-140 = 4.353933036327362e-06

Training epoch-87 batch-141
Running loss of epoch-87 batch-141 = 5.5800192058086395e-06

Training epoch-87 batch-142
Running loss of epoch-87 batch-142 = 4.169531166553497e-06

Training epoch-87 batch-143
Running loss of epoch-87 batch-143 = 4.765111953020096e-06

Training epoch-87 batch-144
Running loss of epoch-87 batch-144 = 4.697591066360474e-06

Training epoch-87 batch-145
Running loss of epoch-87 batch-145 = 3.5283155739307404e-06

Training epoch-87 batch-146
Running loss of epoch-87 batch-146 = 1.2325355783104897e-05

Training epoch-87 batch-147
Running loss of epoch-87 batch-147 = 4.696892574429512e-06

Training epoch-87 batch-148
Running loss of epoch-87 batch-148 = 2.3674219846725464e-06

Training epoch-87 batch-149
Running loss of epoch-87 batch-149 = 3.3222604542970657e-06

Training epoch-87 batch-150
Running loss of epoch-87 batch-150 = 1.4732126146554947e-05

Training epoch-87 batch-151
Running loss of epoch-87 batch-151 = 4.078727215528488e-06

Training epoch-87 batch-152
Running loss of epoch-87 batch-152 = 2.612592652440071e-06

Training epoch-87 batch-153
Running loss of epoch-87 batch-153 = 4.274770617485046e-06

Training epoch-87 batch-154
Running loss of epoch-87 batch-154 = 2.5315675884485245e-06

Training epoch-87 batch-155
Running loss of epoch-87 batch-155 = 2.6330817490816116e-06

Training epoch-87 batch-156
Running loss of epoch-87 batch-156 = 3.816327080130577e-06

Training epoch-87 batch-157
Running loss of epoch-87 batch-157 = 2.898275852203369e-06

Finished training epoch-87.



Average train loss at epoch-87 = 4.019495844841004e-06

Started Evaluation

Average val loss at epoch-87 = 1.8293637973254944

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.53 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.25 %

Finished Evaluation



Started training epoch-88


Training epoch-88 batch-1
Running loss of epoch-88 batch-1 = 2.884073182940483e-06

Training epoch-88 batch-2
Running loss of epoch-88 batch-2 = 9.499490261077881e-07

Training epoch-88 batch-3
Running loss of epoch-88 batch-3 = 3.5890843719244003e-06

Training epoch-88 batch-4
Running loss of epoch-88 batch-4 = 2.308981493115425e-06

Training epoch-88 batch-5
Running loss of epoch-88 batch-5 = 2.2228341549634933e-06

Training epoch-88 batch-6
Running loss of epoch-88 batch-6 = 6.375135853886604e-06

Training epoch-88 batch-7
Running loss of epoch-88 batch-7 = 1.516309566795826e-05

Training epoch-88 batch-8
Running loss of epoch-88 batch-8 = 1.937383785843849e-06

Training epoch-88 batch-9
Running loss of epoch-88 batch-9 = 1.5087425708770752e-06

Training epoch-88 batch-10
Running loss of epoch-88 batch-10 = 3.3616088330745697e-06

Training epoch-88 batch-11
Running loss of epoch-88 batch-11 = 1.2093223631381989e-06

Training epoch-88 batch-12
Running loss of epoch-88 batch-12 = 2.064742147922516e-06

Training epoch-88 batch-13
Running loss of epoch-88 batch-13 = 4.468485713005066e-06

Training epoch-88 batch-14
Running loss of epoch-88 batch-14 = 5.934853106737137e-06

Training epoch-88 batch-15
Running loss of epoch-88 batch-15 = 2.664513885974884e-06

Training epoch-88 batch-16
Running loss of epoch-88 batch-16 = 1.2066913768649101e-05

Training epoch-88 batch-17
Running loss of epoch-88 batch-17 = 5.332985892891884e-06

Training epoch-88 batch-18
Running loss of epoch-88 batch-18 = 8.246861398220062e-06

Training epoch-88 batch-19
Running loss of epoch-88 batch-19 = 3.5585835576057434e-06

Training epoch-88 batch-20
Running loss of epoch-88 batch-20 = 3.595370799303055e-06

Training epoch-88 batch-21
Running loss of epoch-88 batch-21 = 3.1830277293920517e-06

Training epoch-88 batch-22
Running loss of epoch-88 batch-22 = 2.796994522213936e-06

Training epoch-88 batch-23
Running loss of epoch-88 batch-23 = 1.809094101190567e-06

Training epoch-88 batch-24
Running loss of epoch-88 batch-24 = 7.554423063993454e-06

Training epoch-88 batch-25
Running loss of epoch-88 batch-25 = 8.570263162255287e-06

Training epoch-88 batch-26
Running loss of epoch-88 batch-26 = 5.3085386753082275e-06

Training epoch-88 batch-27
Running loss of epoch-88 batch-27 = 3.1301751732826233e-06

Training epoch-88 batch-28
Running loss of epoch-88 batch-28 = 2.103857696056366e-06

Training epoch-88 batch-29
Running loss of epoch-88 batch-29 = 1.6938429325819016e-06

Training epoch-88 batch-30
Running loss of epoch-88 batch-30 = 3.883149474859238e-06

Training epoch-88 batch-31
Running loss of epoch-88 batch-31 = 3.362772986292839e-06

Training epoch-88 batch-32
Running loss of epoch-88 batch-32 = 2.269633114337921e-06

Training epoch-88 batch-33
Running loss of epoch-88 batch-33 = 4.334375262260437e-06

Training epoch-88 batch-34
Running loss of epoch-88 batch-34 = 3.6240089684724808e-06

Training epoch-88 batch-35
Running loss of epoch-88 batch-35 = 5.500856786966324e-06

Training epoch-88 batch-36
Running loss of epoch-88 batch-36 = 1.7525162547826767e-06

Training epoch-88 batch-37
Running loss of epoch-88 batch-37 = 4.320405423641205e-06

Training epoch-88 batch-38
Running loss of epoch-88 batch-38 = 2.007000148296356e-06

Training epoch-88 batch-39
Running loss of epoch-88 batch-39 = 3.795372322201729e-06

Training epoch-88 batch-40
Running loss of epoch-88 batch-40 = 1.5106052160263062e-06

Training epoch-88 batch-41
Running loss of epoch-88 batch-41 = 2.9364600777626038e-06

Training epoch-88 batch-42
Running loss of epoch-88 batch-42 = 4.6032946556806564e-06

Training epoch-88 batch-43
Running loss of epoch-88 batch-43 = 1.1976808309555054e-06

Training epoch-88 batch-44
Running loss of epoch-88 batch-44 = 7.487600669264793e-06

Training epoch-88 batch-45
Running loss of epoch-88 batch-45 = 1.877080649137497e-06

Training epoch-88 batch-46
Running loss of epoch-88 batch-46 = 1.2489035725593567e-06

Training epoch-88 batch-47
Running loss of epoch-88 batch-47 = 2.421671524643898e-06

Training epoch-88 batch-48
Running loss of epoch-88 batch-48 = 2.8391368687152863e-06

Training epoch-88 batch-49
Running loss of epoch-88 batch-49 = 1.1301599442958832e-06

Training epoch-88 batch-50
Running loss of epoch-88 batch-50 = 8.26898030936718e-06

Training epoch-88 batch-51
Running loss of epoch-88 batch-51 = 4.906440153717995e-06

Training epoch-88 batch-52
Running loss of epoch-88 batch-52 = 2.3457687348127365e-06

Training epoch-88 batch-53
Running loss of epoch-88 batch-53 = 5.328794941306114e-06

Training epoch-88 batch-54
Running loss of epoch-88 batch-54 = 2.6533380150794983e-06

Training epoch-88 batch-55
Running loss of epoch-88 batch-55 = 1.9238796085119247e-06

Training epoch-88 batch-56
Running loss of epoch-88 batch-56 = 5.274312570691109e-06

Training epoch-88 batch-57
Running loss of epoch-88 batch-57 = 9.415438398718834e-06

Training epoch-88 batch-58
Running loss of epoch-88 batch-58 = 6.172806024551392e-06

Training epoch-88 batch-59
Running loss of epoch-88 batch-59 = 4.020053893327713e-06

Training epoch-88 batch-60
Running loss of epoch-88 batch-60 = 3.219814971089363e-06

Training epoch-88 batch-61
Running loss of epoch-88 batch-61 = 9.307870641350746e-06

Training epoch-88 batch-62
Running loss of epoch-88 batch-62 = 3.2598618417978287e-06

Training epoch-88 batch-63
Running loss of epoch-88 batch-63 = 4.64683398604393e-06

Training epoch-88 batch-64
Running loss of epoch-88 batch-64 = 7.3767732828855515e-06

Training epoch-88 batch-65
Running loss of epoch-88 batch-65 = 5.1227398216724396e-06

Training epoch-88 batch-66
Running loss of epoch-88 batch-66 = 2.0817387849092484e-06

Training epoch-88 batch-67
Running loss of epoch-88 batch-67 = 1.1215219274163246e-05

Training epoch-88 batch-68
Running loss of epoch-88 batch-68 = 2.073124051094055e-06

Training epoch-88 batch-69
Running loss of epoch-88 batch-69 = 4.349742084741592e-06

Training epoch-88 batch-70
Running loss of epoch-88 batch-70 = 8.890405297279358e-06

Training epoch-88 batch-71
Running loss of epoch-88 batch-71 = 4.352070391178131e-06

Training epoch-88 batch-72
Running loss of epoch-88 batch-72 = 2.9581133276224136e-06

Training epoch-88 batch-73
Running loss of epoch-88 batch-73 = 4.308996722102165e-06

Training epoch-88 batch-74
Running loss of epoch-88 batch-74 = 3.032153472304344e-06

Training epoch-88 batch-75
Running loss of epoch-88 batch-75 = 6.471527740359306e-06

Training epoch-88 batch-76
Running loss of epoch-88 batch-76 = 1.4030374586582184e-06

Training epoch-88 batch-77
Running loss of epoch-88 batch-77 = 3.1373929232358932e-06

Training epoch-88 batch-78
Running loss of epoch-88 batch-78 = 9.271083399653435e-06

Training epoch-88 batch-79
Running loss of epoch-88 batch-79 = 1.8212012946605682e-06

Training epoch-88 batch-80
Running loss of epoch-88 batch-80 = 1.4215009286999702e-05

Training epoch-88 batch-81
Running loss of epoch-88 batch-81 = 1.5811529010534286e-06

Training epoch-88 batch-82
Running loss of epoch-88 batch-82 = 4.553003236651421e-06

Training epoch-88 batch-83
Running loss of epoch-88 batch-83 = 1.7925631254911423e-06

Training epoch-88 batch-84
Running loss of epoch-88 batch-84 = 3.130640834569931e-06

Training epoch-88 batch-85
Running loss of epoch-88 batch-85 = 1.7194543033838272e-06

Training epoch-88 batch-86
Running loss of epoch-88 batch-86 = 4.000728949904442e-06

Training epoch-88 batch-87
Running loss of epoch-88 batch-87 = 4.7585926949977875e-06

Training epoch-88 batch-88
Running loss of epoch-88 batch-88 = 9.445939213037491e-07

Training epoch-88 batch-89
Running loss of epoch-88 batch-89 = 1.6444828361272812e-06

Training epoch-88 batch-90
Running loss of epoch-88 batch-90 = 5.3674448281526566e-06

Training epoch-88 batch-91
Running loss of epoch-88 batch-91 = 4.502246156334877e-06

Training epoch-88 batch-92
Running loss of epoch-88 batch-92 = 3.059394657611847e-06

Training epoch-88 batch-93
Running loss of epoch-88 batch-93 = 3.723893314599991e-06

Training epoch-88 batch-94
Running loss of epoch-88 batch-94 = 7.33998604118824e-06

Training epoch-88 batch-95
Running loss of epoch-88 batch-95 = 3.372086212038994e-06

Training epoch-88 batch-96
Running loss of epoch-88 batch-96 = 1.991167664527893e-06

Training epoch-88 batch-97
Running loss of epoch-88 batch-97 = 8.502043783664703e-06

Training epoch-88 batch-98
Running loss of epoch-88 batch-98 = 3.2186508178710938e-06

Training epoch-88 batch-99
Running loss of epoch-88 batch-99 = 4.124362021684647e-06

Training epoch-88 batch-100
Running loss of epoch-88 batch-100 = 3.6153942346572876e-06

Training epoch-88 batch-101
Running loss of epoch-88 batch-101 = 5.16185536980629e-06

Training epoch-88 batch-102
Running loss of epoch-88 batch-102 = 3.4240074455738068e-06

Training epoch-88 batch-103
Running loss of epoch-88 batch-103 = 2.398621290922165e-06

Training epoch-88 batch-104
Running loss of epoch-88 batch-104 = 7.511116564273834e-06

Training epoch-88 batch-105
Running loss of epoch-88 batch-105 = 1.503853127360344e-06

Training epoch-88 batch-106
Running loss of epoch-88 batch-106 = 2.0659063011407852e-06

Training epoch-88 batch-107
Running loss of epoch-88 batch-107 = 2.6954803615808487e-06

Training epoch-88 batch-108
Running loss of epoch-88 batch-108 = 3.4014228731393814e-06

Training epoch-88 batch-109
Running loss of epoch-88 batch-109 = 2.062646672129631e-06

Training epoch-88 batch-110
Running loss of epoch-88 batch-110 = 1.0668765753507614e-05

Training epoch-88 batch-111
Running loss of epoch-88 batch-111 = 6.268732249736786e-06

Training epoch-88 batch-112
Running loss of epoch-88 batch-112 = 2.57161445915699e-06

Training epoch-88 batch-113
Running loss of epoch-88 batch-113 = 4.26173210144043e-06

Training epoch-88 batch-114
Running loss of epoch-88 batch-114 = 4.807021468877792e-06

Training epoch-88 batch-115
Running loss of epoch-88 batch-115 = 2.0707957446575165e-06

Training epoch-88 batch-116
Running loss of epoch-88 batch-116 = 1.9720755517482758e-06

Training epoch-88 batch-117
Running loss of epoch-88 batch-117 = 2.6866327971220016e-06

Training epoch-88 batch-118
Running loss of epoch-88 batch-118 = 1.1133961379528046e-06

Training epoch-88 batch-119
Running loss of epoch-88 batch-119 = 3.127381205558777e-06

Training epoch-88 batch-120
Running loss of epoch-88 batch-120 = 2.726912498474121e-06

Training epoch-88 batch-121
Running loss of epoch-88 batch-121 = 3.4205149859189987e-06

Training epoch-88 batch-122
Running loss of epoch-88 batch-122 = 5.7872384786605835e-06

Training epoch-88 batch-123
Running loss of epoch-88 batch-123 = 1.6971025615930557e-06

Training epoch-88 batch-124
Running loss of epoch-88 batch-124 = 2.085696905851364e-06

Training epoch-88 batch-125
Running loss of epoch-88 batch-125 = 3.746943548321724e-06

Training epoch-88 batch-126
Running loss of epoch-88 batch-126 = 2.20397487282753e-06

Training epoch-88 batch-127
Running loss of epoch-88 batch-127 = 2.8263311833143234e-06

Training epoch-88 batch-128
Running loss of epoch-88 batch-128 = 3.534602001309395e-06

Training epoch-88 batch-129
Running loss of epoch-88 batch-129 = 3.899913281202316e-06

Training epoch-88 batch-130
Running loss of epoch-88 batch-130 = 3.341585397720337e-06

Training epoch-88 batch-131
Running loss of epoch-88 batch-131 = 4.983274266123772e-06

Training epoch-88 batch-132
Running loss of epoch-88 batch-132 = 4.295259714126587e-06

Training epoch-88 batch-133
Running loss of epoch-88 batch-133 = 2.1061860024929047e-06

Training epoch-88 batch-134
Running loss of epoch-88 batch-134 = 3.6512501537799835e-06

Training epoch-88 batch-135
Running loss of epoch-88 batch-135 = 3.222143277525902e-06

Training epoch-88 batch-136
Running loss of epoch-88 batch-136 = 3.1085219234228134e-06

Training epoch-88 batch-137
Running loss of epoch-88 batch-137 = 1.6058329492807388e-06

Training epoch-88 batch-138
Running loss of epoch-88 batch-138 = 2.1257437765598297e-06

Training epoch-88 batch-139
Running loss of epoch-88 batch-139 = 2.4817418307065964e-06

Training epoch-88 batch-140
Running loss of epoch-88 batch-140 = 2.3802276700735092e-06

Training epoch-88 batch-141
Running loss of epoch-88 batch-141 = 4.005851224064827e-06

Training epoch-88 batch-142
Running loss of epoch-88 batch-142 = 6.035203114151955e-06

Training epoch-88 batch-143
Running loss of epoch-88 batch-143 = 5.92903234064579e-06

Training epoch-88 batch-144
Running loss of epoch-88 batch-144 = 2.9746443033218384e-06

Training epoch-88 batch-145
Running loss of epoch-88 batch-145 = 1.5837140381336212e-06

Training epoch-88 batch-146
Running loss of epoch-88 batch-146 = 3.5886187106370926e-06

Training epoch-88 batch-147
Running loss of epoch-88 batch-147 = 2.357643097639084e-06

Training epoch-88 batch-148
Running loss of epoch-88 batch-148 = 3.216788172721863e-06

Training epoch-88 batch-149
Running loss of epoch-88 batch-149 = 2.310844138264656e-06

Training epoch-88 batch-150
Running loss of epoch-88 batch-150 = 2.141343429684639e-06

Training epoch-88 batch-151
Running loss of epoch-88 batch-151 = 4.20701690018177e-06

Training epoch-88 batch-152
Running loss of epoch-88 batch-152 = 5.27198426425457e-06

Training epoch-88 batch-153
Running loss of epoch-88 batch-153 = 3.654276952147484e-06

Training epoch-88 batch-154
Running loss of epoch-88 batch-154 = 2.0563602447509766e-06

Training epoch-88 batch-155
Running loss of epoch-88 batch-155 = 3.0205119401216507e-06

Training epoch-88 batch-156
Running loss of epoch-88 batch-156 = 5.07524237036705e-06

Training epoch-88 batch-157
Running loss of epoch-88 batch-157 = 4.700198769569397e-05

Finished training epoch-88.



Average train loss at epoch-88 = 4.0169373154640194e-06

Started Evaluation

Average val loss at epoch-88 = 1.8338466254551289

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.14 %

Finished Evaluation



Started training epoch-89


Training epoch-89 batch-1
Running loss of epoch-89 batch-1 = 7.120193913578987e-06

Training epoch-89 batch-2
Running loss of epoch-89 batch-2 = 6.22263178229332e-06

Training epoch-89 batch-3
Running loss of epoch-89 batch-3 = 3.734137862920761e-06

Training epoch-89 batch-4
Running loss of epoch-89 batch-4 = 2.7171336114406586e-06

Training epoch-89 batch-5
Running loss of epoch-89 batch-5 = 4.923436790704727e-06

Training epoch-89 batch-6
Running loss of epoch-89 batch-6 = 1.0488089174032211e-05

Training epoch-89 batch-7
Running loss of epoch-89 batch-7 = 2.055894583463669e-06

Training epoch-89 batch-8
Running loss of epoch-89 batch-8 = 1.7236452549695969e-06

Training epoch-89 batch-9
Running loss of epoch-89 batch-9 = 3.942986950278282e-06

Training epoch-89 batch-10
Running loss of epoch-89 batch-10 = 3.3960677683353424e-06

Training epoch-89 batch-11
Running loss of epoch-89 batch-11 = 9.610317647457123e-06

Training epoch-89 batch-12
Running loss of epoch-89 batch-12 = 3.6980491131544113e-06

Training epoch-89 batch-13
Running loss of epoch-89 batch-13 = 5.973502993583679e-06

Training epoch-89 batch-14
Running loss of epoch-89 batch-14 = 3.80747951567173e-06

Training epoch-89 batch-15
Running loss of epoch-89 batch-15 = 8.428702130913734e-06

Training epoch-89 batch-16
Running loss of epoch-89 batch-16 = 2.919696271419525e-06

Training epoch-89 batch-17
Running loss of epoch-89 batch-17 = 2.1175947040319443e-06

Training epoch-89 batch-18
Running loss of epoch-89 batch-18 = 1.3362150639295578e-06

Training epoch-89 batch-19
Running loss of epoch-89 batch-19 = 4.007015377283096e-06

Training epoch-89 batch-20
Running loss of epoch-89 batch-20 = 1.5653204172849655e-06

Training epoch-89 batch-21
Running loss of epoch-89 batch-21 = 2.0011793822050095e-06

Training epoch-89 batch-22
Running loss of epoch-89 batch-22 = 3.909924998879433e-06

Training epoch-89 batch-23
Running loss of epoch-89 batch-23 = 9.851064532995224e-07

Training epoch-89 batch-24
Running loss of epoch-89 batch-24 = 2.739252522587776e-06

Training epoch-89 batch-25
Running loss of epoch-89 batch-25 = 1.0634539648890495e-05

Training epoch-89 batch-26
Running loss of epoch-89 batch-26 = 2.8621871024370193e-06

Training epoch-89 batch-27
Running loss of epoch-89 batch-27 = 3.7527643144130707e-06

Training epoch-89 batch-28
Running loss of epoch-89 batch-28 = 4.4477637857198715e-06

Training epoch-89 batch-29
Running loss of epoch-89 batch-29 = 3.6689452826976776e-06

Training epoch-89 batch-30
Running loss of epoch-89 batch-30 = 2.408865839242935e-06

Training epoch-89 batch-31
Running loss of epoch-89 batch-31 = 1.57160684466362e-06

Training epoch-89 batch-32
Running loss of epoch-89 batch-32 = 1.1958181858062744e-06

Training epoch-89 batch-33
Running loss of epoch-89 batch-33 = 2.9492657631635666e-06

Training epoch-89 batch-34
Running loss of epoch-89 batch-34 = 9.30088572204113e-06

Training epoch-89 batch-35
Running loss of epoch-89 batch-35 = 3.55718657374382e-06

Training epoch-89 batch-36
Running loss of epoch-89 batch-36 = 4.815869033336639e-06

Training epoch-89 batch-37
Running loss of epoch-89 batch-37 = 3.866851329803467e-06

Training epoch-89 batch-38
Running loss of epoch-89 batch-38 = 2.7101486921310425e-06

Training epoch-89 batch-39
Running loss of epoch-89 batch-39 = 1.7888378351926804e-06

Training epoch-89 batch-40
Running loss of epoch-89 batch-40 = 3.3578835427761078e-06

Training epoch-89 batch-41
Running loss of epoch-89 batch-41 = 4.207249730825424e-07

Training epoch-89 batch-42
Running loss of epoch-89 batch-42 = 7.505528628826141e-06

Training epoch-89 batch-43
Running loss of epoch-89 batch-43 = 5.445443093776703e-06

Training epoch-89 batch-44
Running loss of epoch-89 batch-44 = 4.607019945979118e-06

Training epoch-89 batch-45
Running loss of epoch-89 batch-45 = 3.74671071767807e-06

Training epoch-89 batch-46
Running loss of epoch-89 batch-46 = 2.920161932706833e-06

Training epoch-89 batch-47
Running loss of epoch-89 batch-47 = 1.7334241420030594e-06

Training epoch-89 batch-48
Running loss of epoch-89 batch-48 = 4.860572516918182e-06

Training epoch-89 batch-49
Running loss of epoch-89 batch-49 = 2.4263281375169754e-06

Training epoch-89 batch-50
Running loss of epoch-89 batch-50 = 4.834961146116257e-06

Training epoch-89 batch-51
Running loss of epoch-89 batch-51 = 2.520158886909485e-06

Training epoch-89 batch-52
Running loss of epoch-89 batch-52 = 7.003545761108398e-07

Training epoch-89 batch-53
Running loss of epoch-89 batch-53 = 6.023794412612915e-06

Training epoch-89 batch-54
Running loss of epoch-89 batch-54 = 6.285728886723518e-06

Training epoch-89 batch-55
Running loss of epoch-89 batch-55 = 3.3942051231861115e-06

Training epoch-89 batch-56
Running loss of epoch-89 batch-56 = 3.1287781894207e-06

Training epoch-89 batch-57
Running loss of epoch-89 batch-57 = 2.4265609681606293e-06

Training epoch-89 batch-58
Running loss of epoch-89 batch-58 = 2.3872125893831253e-06

Training epoch-89 batch-59
Running loss of epoch-89 batch-59 = 4.858709871768951e-06

Training epoch-89 batch-60
Running loss of epoch-89 batch-60 = 2.4102628231048584e-06

Training epoch-89 batch-61
Running loss of epoch-89 batch-61 = 2.571847289800644e-06

Training epoch-89 batch-62
Running loss of epoch-89 batch-62 = 2.41096131503582e-06

Training epoch-89 batch-63
Running loss of epoch-89 batch-63 = 3.3339019864797592e-06

Training epoch-89 batch-64
Running loss of epoch-89 batch-64 = 1.9937288016080856e-06

Training epoch-89 batch-65
Running loss of epoch-89 batch-65 = 3.102235496044159e-06

Training epoch-89 batch-66
Running loss of epoch-89 batch-66 = 2.339249476790428e-06

Training epoch-89 batch-67
Running loss of epoch-89 batch-67 = 1.5774276107549667e-06

Training epoch-89 batch-68
Running loss of epoch-89 batch-68 = 2.6461202651262283e-06

Training epoch-89 batch-69
Running loss of epoch-89 batch-69 = 4.153931513428688e-06

Training epoch-89 batch-70
Running loss of epoch-89 batch-70 = 4.693865776062012e-06

Training epoch-89 batch-71
Running loss of epoch-89 batch-71 = 3.489665687084198e-06

Training epoch-89 batch-72
Running loss of epoch-89 batch-72 = 4.435889422893524e-06

Training epoch-89 batch-73
Running loss of epoch-89 batch-73 = 3.1546223908662796e-06

Training epoch-89 batch-74
Running loss of epoch-89 batch-74 = 4.677101969718933e-06

Training epoch-89 batch-75
Running loss of epoch-89 batch-75 = 1.2184027582406998e-06

Training epoch-89 batch-76
Running loss of epoch-89 batch-76 = 1.4044344425201416e-06

Training epoch-89 batch-77
Running loss of epoch-89 batch-77 = 2.6079360395669937e-06

Training epoch-89 batch-78
Running loss of epoch-89 batch-78 = 1.1513475328683853e-06

Training epoch-89 batch-79
Running loss of epoch-89 batch-79 = 1.230509951710701e-06

Training epoch-89 batch-80
Running loss of epoch-89 batch-80 = 1.4968682080507278e-06

Training epoch-89 batch-81
Running loss of epoch-89 batch-81 = 1.4954712241888046e-06

Training epoch-89 batch-82
Running loss of epoch-89 batch-82 = 2.3175962269306183e-06

Training epoch-89 batch-83
Running loss of epoch-89 batch-83 = 1.5222467482089996e-06

Training epoch-89 batch-84
Running loss of epoch-89 batch-84 = 4.627276211977005e-06

Training epoch-89 batch-85
Running loss of epoch-89 batch-85 = 2.2507738322019577e-06

Training epoch-89 batch-86
Running loss of epoch-89 batch-86 = 4.03611920773983e-06

Training epoch-89 batch-87
Running loss of epoch-89 batch-87 = 1.2745149433612823e-06

Training epoch-89 batch-88
Running loss of epoch-89 batch-88 = 6.06197863817215e-06

Training epoch-89 batch-89
Running loss of epoch-89 batch-89 = 6.189337000250816e-06

Training epoch-89 batch-90
Running loss of epoch-89 batch-90 = 4.7567300498485565e-06

Training epoch-89 batch-91
Running loss of epoch-89 batch-91 = 4.1923485696315765e-06

Training epoch-89 batch-92
Running loss of epoch-89 batch-92 = 4.244502633810043e-06

Training epoch-89 batch-93
Running loss of epoch-89 batch-93 = 2.1564774215221405e-06

Training epoch-89 batch-94
Running loss of epoch-89 batch-94 = 2.923421561717987e-06

Training epoch-89 batch-95
Running loss of epoch-89 batch-95 = 2.2416934370994568e-06

Training epoch-89 batch-96
Running loss of epoch-89 batch-96 = 8.570263162255287e-06

Training epoch-89 batch-97
Running loss of epoch-89 batch-97 = 4.707835614681244e-06

Training epoch-89 batch-98
Running loss of epoch-89 batch-98 = 4.840083420276642e-06

Training epoch-89 batch-99
Running loss of epoch-89 batch-99 = 2.7159694582223892e-06

Training epoch-89 batch-100
Running loss of epoch-89 batch-100 = 3.913650289177895e-06

Training epoch-89 batch-101
Running loss of epoch-89 batch-101 = 2.874992787837982e-06

Training epoch-89 batch-102
Running loss of epoch-89 batch-102 = 1.661013811826706e-06

Training epoch-89 batch-103
Running loss of epoch-89 batch-103 = 2.8745271265506744e-06

Training epoch-89 batch-104
Running loss of epoch-89 batch-104 = 2.9262155294418335e-06

Training epoch-89 batch-105
Running loss of epoch-89 batch-105 = 1.955544576048851e-06

Training epoch-89 batch-106
Running loss of epoch-89 batch-106 = 3.189314156770706e-06

Training epoch-89 batch-107
Running loss of epoch-89 batch-107 = 3.6729034036397934e-06

Training epoch-89 batch-108
Running loss of epoch-89 batch-108 = 4.235655069351196e-06

Training epoch-89 batch-109
Running loss of epoch-89 batch-109 = 1.2943055480718613e-06

Training epoch-89 batch-110
Running loss of epoch-89 batch-110 = 3.157416358590126e-06

Training epoch-89 batch-111
Running loss of epoch-89 batch-111 = 2.033775672316551e-06

Training epoch-89 batch-112
Running loss of epoch-89 batch-112 = 4.159286618232727e-06

Training epoch-89 batch-113
Running loss of epoch-89 batch-113 = 5.502952262759209e-06

Training epoch-89 batch-114
Running loss of epoch-89 batch-114 = 3.877328708767891e-06

Training epoch-89 batch-115
Running loss of epoch-89 batch-115 = 5.840091034770012e-06

Training epoch-89 batch-116
Running loss of epoch-89 batch-116 = 3.453576937317848e-06

Training epoch-89 batch-117
Running loss of epoch-89 batch-117 = 7.090624421834946e-06

Training epoch-89 batch-118
Running loss of epoch-89 batch-118 = 6.864778697490692e-06

Training epoch-89 batch-119
Running loss of epoch-89 batch-119 = 2.429122105240822e-06

Training epoch-89 batch-120
Running loss of epoch-89 batch-120 = 1.8290942534804344e-05

Training epoch-89 batch-121
Running loss of epoch-89 batch-121 = 1.596519723534584e-06

Training epoch-89 batch-122
Running loss of epoch-89 batch-122 = 3.2668467611074448e-06

Training epoch-89 batch-123
Running loss of epoch-89 batch-123 = 4.653353244066238e-06

Training epoch-89 batch-124
Running loss of epoch-89 batch-124 = 1.0679941624403e-06

Training epoch-89 batch-125
Running loss of epoch-89 batch-125 = 4.113651812076569e-06

Training epoch-89 batch-126
Running loss of epoch-89 batch-126 = 3.725523129105568e-06

Training epoch-89 batch-127
Running loss of epoch-89 batch-127 = 1.3204291462898254e-05

Training epoch-89 batch-128
Running loss of epoch-89 batch-128 = 1.5976838767528534e-06

Training epoch-89 batch-129
Running loss of epoch-89 batch-129 = 3.930879756808281e-06

Training epoch-89 batch-130
Running loss of epoch-89 batch-130 = 4.600267857313156e-06

Training epoch-89 batch-131
Running loss of epoch-89 batch-131 = 8.502975106239319e-06

Training epoch-89 batch-132
Running loss of epoch-89 batch-132 = 1.02166086435318e-05

Training epoch-89 batch-133
Running loss of epoch-89 batch-133 = 4.258006811141968e-06

Training epoch-89 batch-134
Running loss of epoch-89 batch-134 = 2.3746397346258163e-06

Training epoch-89 batch-135
Running loss of epoch-89 batch-135 = 2.3494940251111984e-06

Training epoch-89 batch-136
Running loss of epoch-89 batch-136 = 3.391411155462265e-06

Training epoch-89 batch-137
Running loss of epoch-89 batch-137 = 7.622409611940384e-06

Training epoch-89 batch-138
Running loss of epoch-89 batch-138 = 2.4959444999694824e-06

Training epoch-89 batch-139
Running loss of epoch-89 batch-139 = 9.819865226745605e-06

Training epoch-89 batch-140
Running loss of epoch-89 batch-140 = 4.662433639168739e-06

Training epoch-89 batch-141
Running loss of epoch-89 batch-141 = 2.671731635928154e-06

Training epoch-89 batch-142
Running loss of epoch-89 batch-142 = 3.009336069226265e-06

Training epoch-89 batch-143
Running loss of epoch-89 batch-143 = 3.6722049117088318e-06

Training epoch-89 batch-144
Running loss of epoch-89 batch-144 = 1.4763791114091873e-06

Training epoch-89 batch-145
Running loss of epoch-89 batch-145 = 2.651708200573921e-06

Training epoch-89 batch-146
Running loss of epoch-89 batch-146 = 3.864988684654236e-06

Training epoch-89 batch-147
Running loss of epoch-89 batch-147 = 4.907604306936264e-06

Training epoch-89 batch-148
Running loss of epoch-89 batch-148 = 1.4060642570257187e-06

Training epoch-89 batch-149
Running loss of epoch-89 batch-149 = 1.4775432646274567e-06

Training epoch-89 batch-150
Running loss of epoch-89 batch-150 = 3.949040547013283e-06

Training epoch-89 batch-151
Running loss of epoch-89 batch-151 = 2.169748768210411e-06

Training epoch-89 batch-152
Running loss of epoch-89 batch-152 = 2.509215846657753e-06

Training epoch-89 batch-153
Running loss of epoch-89 batch-153 = 4.322966560721397e-06

Training epoch-89 batch-154
Running loss of epoch-89 batch-154 = 8.552102372050285e-06

Training epoch-89 batch-155
Running loss of epoch-89 batch-155 = 3.756023943424225e-06

Training epoch-89 batch-156
Running loss of epoch-89 batch-156 = 1.5352852642536163e-06

Training epoch-89 batch-157
Running loss of epoch-89 batch-157 = 6.15827739238739e-05

Finished training epoch-89.



Average train loss at epoch-89 = 3.941380977630615e-06

Started Evaluation

Average val loss at epoch-89 = 1.8357710409172923

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.36 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 63.85 %

Overall Accuracy = 81.18 %

Finished Evaluation



Started training epoch-90


Training epoch-90 batch-1
Running loss of epoch-90 batch-1 = 4.214700311422348e-06

Training epoch-90 batch-2
Running loss of epoch-90 batch-2 = 4.9334485083818436e-06

Training epoch-90 batch-3
Running loss of epoch-90 batch-3 = 2.54996120929718e-06

Training epoch-90 batch-4
Running loss of epoch-90 batch-4 = 6.879214197397232e-06

Training epoch-90 batch-5
Running loss of epoch-90 batch-5 = 7.493654265999794e-06

Training epoch-90 batch-6
Running loss of epoch-90 batch-6 = 3.156019374728203e-06

Training epoch-90 batch-7
Running loss of epoch-90 batch-7 = 5.991663783788681e-06

Training epoch-90 batch-8
Running loss of epoch-90 batch-8 = 2.7050264179706573e-06

Training epoch-90 batch-9
Running loss of epoch-90 batch-9 = 4.47593629360199e-06

Training epoch-90 batch-10
Running loss of epoch-90 batch-10 = 3.543216735124588e-06

Training epoch-90 batch-11
Running loss of epoch-90 batch-11 = 5.445210263133049e-06

Training epoch-90 batch-12
Running loss of epoch-90 batch-12 = 3.0314549803733826e-06

Training epoch-90 batch-13
Running loss of epoch-90 batch-13 = 1.0349322110414505e-06

Training epoch-90 batch-14
Running loss of epoch-90 batch-14 = 1.7452985048294067e-06

Training epoch-90 batch-15
Running loss of epoch-90 batch-15 = 2.711080014705658e-06

Training epoch-90 batch-16
Running loss of epoch-90 batch-16 = 1.9995495676994324e-06

Training epoch-90 batch-17
Running loss of epoch-90 batch-17 = 2.6316847652196884e-06

Training epoch-90 batch-18
Running loss of epoch-90 batch-18 = 1.0628718882799149e-06

Training epoch-90 batch-19
Running loss of epoch-90 batch-19 = 4.394445568323135e-06

Training epoch-90 batch-20
Running loss of epoch-90 batch-20 = 4.825415089726448e-06

Training epoch-90 batch-21
Running loss of epoch-90 batch-21 = 3.6905985325574875e-06

Training epoch-90 batch-22
Running loss of epoch-90 batch-22 = 4.340428858995438e-06

Training epoch-90 batch-23
Running loss of epoch-90 batch-23 = 1.2938398867845535e-06

Training epoch-90 batch-24
Running loss of epoch-90 batch-24 = 9.083421900868416e-06

Training epoch-90 batch-25
Running loss of epoch-90 batch-25 = 6.170012056827545e-06

Training epoch-90 batch-26
Running loss of epoch-90 batch-26 = 1.5487894415855408e-06

Training epoch-90 batch-27
Running loss of epoch-90 batch-27 = 2.4274922907352448e-06

Training epoch-90 batch-28
Running loss of epoch-90 batch-28 = 5.638226866722107e-06

Training epoch-90 batch-29
Running loss of epoch-90 batch-29 = 1.980457454919815e-06

Training epoch-90 batch-30
Running loss of epoch-90 batch-30 = 2.3993197828531265e-06

Training epoch-90 batch-31
Running loss of epoch-90 batch-31 = 5.880370736122131e-06

Training epoch-90 batch-32
Running loss of epoch-90 batch-32 = 2.620276063680649e-06

Training epoch-90 batch-33
Running loss of epoch-90 batch-33 = 4.09386120736599e-06

Training epoch-90 batch-34
Running loss of epoch-90 batch-34 = 3.874069079756737e-06

Training epoch-90 batch-35
Running loss of epoch-90 batch-35 = 7.166527211666107e-07

Training epoch-90 batch-36
Running loss of epoch-90 batch-36 = 9.972136467695236e-07

Training epoch-90 batch-37
Running loss of epoch-90 batch-37 = 3.93204391002655e-06

Training epoch-90 batch-38
Running loss of epoch-90 batch-38 = 4.454050213098526e-06

Training epoch-90 batch-39
Running loss of epoch-90 batch-39 = 1.8030405044555664e-06

Training epoch-90 batch-40
Running loss of epoch-90 batch-40 = 3.498746082186699e-06

Training epoch-90 batch-41
Running loss of epoch-90 batch-41 = 5.044508725404739e-06

Training epoch-90 batch-42
Running loss of epoch-90 batch-42 = 1.2309756129980087e-06

Training epoch-90 batch-43
Running loss of epoch-90 batch-43 = 3.698747605085373e-06

Training epoch-90 batch-44
Running loss of epoch-90 batch-44 = 3.428896889090538e-06

Training epoch-90 batch-45
Running loss of epoch-90 batch-45 = 1.8384307622909546e-06

Training epoch-90 batch-46
Running loss of epoch-90 batch-46 = 3.4654513001441956e-06

Training epoch-90 batch-47
Running loss of epoch-90 batch-47 = 5.658017471432686e-06

Training epoch-90 batch-48
Running loss of epoch-90 batch-48 = 3.382330760359764e-06

Training epoch-90 batch-49
Running loss of epoch-90 batch-49 = 1.3739336282014847e-06

Training epoch-90 batch-50
Running loss of epoch-90 batch-50 = 2.1348241716623306e-06

Training epoch-90 batch-51
Running loss of epoch-90 batch-51 = 2.4230685085058212e-06

Training epoch-90 batch-52
Running loss of epoch-90 batch-52 = 3.412831574678421e-06

Training epoch-90 batch-53
Running loss of epoch-90 batch-53 = 4.649395123124123e-06

Training epoch-90 batch-54
Running loss of epoch-90 batch-54 = 4.4745393097400665e-06

Training epoch-90 batch-55
Running loss of epoch-90 batch-55 = 5.593523383140564e-06

Training epoch-90 batch-56
Running loss of epoch-90 batch-56 = 5.697598680853844e-06

Training epoch-90 batch-57
Running loss of epoch-90 batch-57 = 6.269663572311401e-06

Training epoch-90 batch-58
Running loss of epoch-90 batch-58 = 2.1525193005800247e-06

Training epoch-90 batch-59
Running loss of epoch-90 batch-59 = 2.4873297661542892e-06

Training epoch-90 batch-60
Running loss of epoch-90 batch-60 = 5.546025931835175e-07

Training epoch-90 batch-61
Running loss of epoch-90 batch-61 = 4.517845809459686e-06

Training epoch-90 batch-62
Running loss of epoch-90 batch-62 = 2.988148480653763e-06

Training epoch-90 batch-63
Running loss of epoch-90 batch-63 = 1.7033889889717102e-06

Training epoch-90 batch-64
Running loss of epoch-90 batch-64 = 2.1012965589761734e-06

Training epoch-90 batch-65
Running loss of epoch-90 batch-65 = 5.445908755064011e-06

Training epoch-90 batch-66
Running loss of epoch-90 batch-66 = 5.915062502026558e-06

Training epoch-90 batch-67
Running loss of epoch-90 batch-67 = 4.463363438844681e-06

Training epoch-90 batch-68
Running loss of epoch-90 batch-68 = 2.657296136021614e-06

Training epoch-90 batch-69
Running loss of epoch-90 batch-69 = 1.7401762306690216e-06

Training epoch-90 batch-70
Running loss of epoch-90 batch-70 = 2.596527338027954e-06

Training epoch-90 batch-71
Running loss of epoch-90 batch-71 = 2.8882641345262527e-06

Training epoch-90 batch-72
Running loss of epoch-90 batch-72 = 4.805158823728561e-06

Training epoch-90 batch-73
Running loss of epoch-90 batch-73 = 9.632902219891548e-06

Training epoch-90 batch-74
Running loss of epoch-90 batch-74 = 2.8677750378847122e-06

Training epoch-90 batch-75
Running loss of epoch-90 batch-75 = 2.983957529067993e-06

Training epoch-90 batch-76
Running loss of epoch-90 batch-76 = 2.810964360833168e-06

Training epoch-90 batch-77
Running loss of epoch-90 batch-77 = 3.400258719921112e-06

Training epoch-90 batch-78
Running loss of epoch-90 batch-78 = 5.918089300394058e-06

Training epoch-90 batch-79
Running loss of epoch-90 batch-79 = 1.941574737429619e-06

Training epoch-90 batch-80
Running loss of epoch-90 batch-80 = 1.937616616487503e-06

Training epoch-90 batch-81
Running loss of epoch-90 batch-81 = 7.795868441462517e-06

Training epoch-90 batch-82
Running loss of epoch-90 batch-82 = 3.0689407140016556e-06

Training epoch-90 batch-83
Running loss of epoch-90 batch-83 = 1.9827857613563538e-06

Training epoch-90 batch-84
Running loss of epoch-90 batch-84 = 3.7373974919319153e-06

Training epoch-90 batch-85
Running loss of epoch-90 batch-85 = 1.852167770266533e-06

Training epoch-90 batch-86
Running loss of epoch-90 batch-86 = 2.046814188361168e-06

Training epoch-90 batch-87
Running loss of epoch-90 batch-87 = 1.6209669411182404e-06

Training epoch-90 batch-88
Running loss of epoch-90 batch-88 = 2.7581118047237396e-06

Training epoch-90 batch-89
Running loss of epoch-90 batch-89 = 5.02890907227993e-06

Training epoch-90 batch-90
Running loss of epoch-90 batch-90 = 4.809582605957985e-06

Training epoch-90 batch-91
Running loss of epoch-90 batch-91 = 1.8856953829526901e-06

Training epoch-90 batch-92
Running loss of epoch-90 batch-92 = 5.7639554142951965e-06

Training epoch-90 batch-93
Running loss of epoch-90 batch-93 = 2.957414835691452e-06

Training epoch-90 batch-94
Running loss of epoch-90 batch-94 = 1.4007091522216797e-06

Training epoch-90 batch-95
Running loss of epoch-90 batch-95 = 3.203749656677246e-06

Training epoch-90 batch-96
Running loss of epoch-90 batch-96 = 3.502238541841507e-06

Training epoch-90 batch-97
Running loss of epoch-90 batch-97 = 3.957655280828476e-06

Training epoch-90 batch-98
Running loss of epoch-90 batch-98 = 1.160893589258194e-06

Training epoch-90 batch-99
Running loss of epoch-90 batch-99 = 1.0687625035643578e-05

Training epoch-90 batch-100
Running loss of epoch-90 batch-100 = 8.33394005894661e-06

Training epoch-90 batch-101
Running loss of epoch-90 batch-101 = 2.091284841299057e-06

Training epoch-90 batch-102
Running loss of epoch-90 batch-102 = 5.20399771630764e-06

Training epoch-90 batch-103
Running loss of epoch-90 batch-103 = 4.124362021684647e-06

Training epoch-90 batch-104
Running loss of epoch-90 batch-104 = 2.439366653561592e-06

Training epoch-90 batch-105
Running loss of epoch-90 batch-105 = 2.9229559004306793e-06

Training epoch-90 batch-106
Running loss of epoch-90 batch-106 = 5.1013194024562836e-06

Training epoch-90 batch-107
Running loss of epoch-90 batch-107 = 2.432148903608322e-06

Training epoch-90 batch-108
Running loss of epoch-90 batch-108 = 2.1185260266065598e-06

Training epoch-90 batch-109
Running loss of epoch-90 batch-109 = 4.219589754939079e-06

Training epoch-90 batch-110
Running loss of epoch-90 batch-110 = 3.9923470467329025e-06

Training epoch-90 batch-111
Running loss of epoch-90 batch-111 = 4.103407263755798e-06

Training epoch-90 batch-112
Running loss of epoch-90 batch-112 = 4.824018105864525e-06

Training epoch-90 batch-113
Running loss of epoch-90 batch-113 = 1.3289973139762878e-06

Training epoch-90 batch-114
Running loss of epoch-90 batch-114 = 5.620298907160759e-06

Training epoch-90 batch-115
Running loss of epoch-90 batch-115 = 1.7480924725532532e-06

Training epoch-90 batch-116
Running loss of epoch-90 batch-116 = 4.35742549598217e-06

Training epoch-90 batch-117
Running loss of epoch-90 batch-117 = 3.5085249692201614e-06

Training epoch-90 batch-118
Running loss of epoch-90 batch-118 = 7.724389433860779e-06

Training epoch-90 batch-119
Running loss of epoch-90 batch-119 = 4.090135917067528e-06

Training epoch-90 batch-120
Running loss of epoch-90 batch-120 = 1.0418007150292397e-05

Training epoch-90 batch-121
Running loss of epoch-90 batch-121 = 4.365807399153709e-06

Training epoch-90 batch-122
Running loss of epoch-90 batch-122 = 3.5082921385765076e-06

Training epoch-90 batch-123
Running loss of epoch-90 batch-123 = 1.2191012501716614e-06

Training epoch-90 batch-124
Running loss of epoch-90 batch-124 = 2.857530489563942e-06

Training epoch-90 batch-125
Running loss of epoch-90 batch-125 = 1.1735362932085991e-05

Training epoch-90 batch-126
Running loss of epoch-90 batch-126 = 3.1248200684785843e-06

Training epoch-90 batch-127
Running loss of epoch-90 batch-127 = 3.037508577108383e-06

Training epoch-90 batch-128
Running loss of epoch-90 batch-128 = 1.9348226487636566e-06

Training epoch-90 batch-129
Running loss of epoch-90 batch-129 = 2.4011824280023575e-06

Training epoch-90 batch-130
Running loss of epoch-90 batch-130 = 1.3122102245688438e-05

Training epoch-90 batch-131
Running loss of epoch-90 batch-131 = 1.707347109913826e-06

Training epoch-90 batch-132
Running loss of epoch-90 batch-132 = 1.9830185920000076e-06

Training epoch-90 batch-133
Running loss of epoch-90 batch-133 = 9.727664291858673e-07

Training epoch-90 batch-134
Running loss of epoch-90 batch-134 = 4.761619493365288e-06

Training epoch-90 batch-135
Running loss of epoch-90 batch-135 = 2.651708200573921e-06

Training epoch-90 batch-136
Running loss of epoch-90 batch-136 = 4.155561327934265e-06

Training epoch-90 batch-137
Running loss of epoch-90 batch-137 = 4.76418063044548e-06

Training epoch-90 batch-138
Running loss of epoch-90 batch-138 = 4.452420398592949e-06

Training epoch-90 batch-139
Running loss of epoch-90 batch-139 = 3.1779054552316666e-06

Training epoch-90 batch-140
Running loss of epoch-90 batch-140 = 3.3043324947357178e-06

Training epoch-90 batch-141
Running loss of epoch-90 batch-141 = 2.7087517082691193e-06

Training epoch-90 batch-142
Running loss of epoch-90 batch-142 = 6.6480133682489395e-06

Training epoch-90 batch-143
Running loss of epoch-90 batch-143 = 2.2817403078079224e-06

Training epoch-90 batch-144
Running loss of epoch-90 batch-144 = 3.014924004673958e-06

Training epoch-90 batch-145
Running loss of epoch-90 batch-145 = 3.708992153406143e-06

Training epoch-90 batch-146
Running loss of epoch-90 batch-146 = 1.2246193364262581e-05

Training epoch-90 batch-147
Running loss of epoch-90 batch-147 = 2.466607838869095e-06

Training epoch-90 batch-148
Running loss of epoch-90 batch-148 = 3.865454345941544e-06

Training epoch-90 batch-149
Running loss of epoch-90 batch-149 = 3.5045668482780457e-06

Training epoch-90 batch-150
Running loss of epoch-90 batch-150 = 5.3942203521728516e-06

Training epoch-90 batch-151
Running loss of epoch-90 batch-151 = 6.133923307061195e-06

Training epoch-90 batch-152
Running loss of epoch-90 batch-152 = 2.868473529815674e-06

Training epoch-90 batch-153
Running loss of epoch-90 batch-153 = 2.192799001932144e-06

Training epoch-90 batch-154
Running loss of epoch-90 batch-154 = 4.978850483894348e-06

Training epoch-90 batch-155
Running loss of epoch-90 batch-155 = 3.439607098698616e-06

Training epoch-90 batch-156
Running loss of epoch-90 batch-156 = 2.8777867555618286e-06

Training epoch-90 batch-157
Running loss of epoch-90 batch-157 = 2.2005289793014526e-05

Finished training epoch-90.



Average train loss at epoch-90 = 3.843511641025543e-06

Started Evaluation

Average val loss at epoch-90 = 1.8380502677570316

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.25 %

Finished Evaluation



Started training epoch-91


Training epoch-91 batch-1
Running loss of epoch-91 batch-1 = 2.044485881924629e-06

Training epoch-91 batch-2
Running loss of epoch-91 batch-2 = 2.198154106736183e-06

Training epoch-91 batch-3
Running loss of epoch-91 batch-3 = 2.8014183044433594e-06

Training epoch-91 batch-4
Running loss of epoch-91 batch-4 = 2.898741513490677e-06

Training epoch-91 batch-5
Running loss of epoch-91 batch-5 = 1.8957071006298065e-06

Training epoch-91 batch-6
Running loss of epoch-91 batch-6 = 1.7273705452680588e-06

Training epoch-91 batch-7
Running loss of epoch-91 batch-7 = 2.505257725715637e-06

Training epoch-91 batch-8
Running loss of epoch-91 batch-8 = 4.8317015171051025e-06

Training epoch-91 batch-9
Running loss of epoch-91 batch-9 = 4.293397068977356e-06

Training epoch-91 batch-10
Running loss of epoch-91 batch-10 = 1.4475081115961075e-06

Training epoch-91 batch-11
Running loss of epoch-91 batch-11 = 2.378830686211586e-06

Training epoch-91 batch-12
Running loss of epoch-91 batch-12 = 5.458947271108627e-06

Training epoch-91 batch-13
Running loss of epoch-91 batch-13 = 2.9206275939941406e-06

Training epoch-91 batch-14
Running loss of epoch-91 batch-14 = 4.200031980872154e-06

Training epoch-91 batch-15
Running loss of epoch-91 batch-15 = 5.249865353107452e-06

Training epoch-91 batch-16
Running loss of epoch-91 batch-16 = 2.5781337171792984e-06

Training epoch-91 batch-17
Running loss of epoch-91 batch-17 = 2.250075340270996e-06

Training epoch-91 batch-18
Running loss of epoch-91 batch-18 = 1.8975697457790375e-06

Training epoch-91 batch-19
Running loss of epoch-91 batch-19 = 1.6505364328622818e-06

Training epoch-91 batch-20
Running loss of epoch-91 batch-20 = 3.5706907510757446e-06

Training epoch-91 batch-21
Running loss of epoch-91 batch-21 = 1.244526356458664e-05

Training epoch-91 batch-22
Running loss of epoch-91 batch-22 = 2.5846529752016068e-06

Training epoch-91 batch-23
Running loss of epoch-91 batch-23 = 6.390269845724106e-06

Training epoch-91 batch-24
Running loss of epoch-91 batch-24 = 1.5068799257278442e-06

Training epoch-91 batch-25
Running loss of epoch-91 batch-25 = 2.578599378466606e-06

Training epoch-91 batch-26
Running loss of epoch-91 batch-26 = 6.912974640727043e-06

Training epoch-91 batch-27
Running loss of epoch-91 batch-27 = 4.617031663656235e-06

Training epoch-91 batch-28
Running loss of epoch-91 batch-28 = 7.309252396225929e-06

Training epoch-91 batch-29
Running loss of epoch-91 batch-29 = 2.4649780243635178e-06

Training epoch-91 batch-30
Running loss of epoch-91 batch-30 = 5.3406693041324615e-06

Training epoch-91 batch-31
Running loss of epoch-91 batch-31 = 1.2188684195280075e-06

Training epoch-91 batch-32
Running loss of epoch-91 batch-32 = 5.275942385196686e-06

Training epoch-91 batch-33
Running loss of epoch-91 batch-33 = 4.249159246683121e-06

Training epoch-91 batch-34
Running loss of epoch-91 batch-34 = 3.7597492337226868e-06

Training epoch-91 batch-35
Running loss of epoch-91 batch-35 = 2.560671418905258e-06

Training epoch-91 batch-36
Running loss of epoch-91 batch-36 = 1.6426201909780502e-06

Training epoch-91 batch-37
Running loss of epoch-91 batch-37 = 2.987915650010109e-06

Training epoch-91 batch-38
Running loss of epoch-91 batch-38 = 2.7329660952091217e-06

Training epoch-91 batch-39
Running loss of epoch-91 batch-39 = 1.5075784176588058e-06

Training epoch-91 batch-40
Running loss of epoch-91 batch-40 = 3.348803147673607e-06

Training epoch-91 batch-41
Running loss of epoch-91 batch-41 = 7.05476850271225e-07

Training epoch-91 batch-42
Running loss of epoch-91 batch-42 = 2.8063077479600906e-06

Training epoch-91 batch-43
Running loss of epoch-91 batch-43 = 3.7597492337226868e-06

Training epoch-91 batch-44
Running loss of epoch-91 batch-44 = 3.287568688392639e-06

Training epoch-91 batch-45
Running loss of epoch-91 batch-45 = 3.2747630029916763e-06

Training epoch-91 batch-46
Running loss of epoch-91 batch-46 = 2.04332172870636e-06

Training epoch-91 batch-47
Running loss of epoch-91 batch-47 = 4.796776920557022e-06

Training epoch-91 batch-48
Running loss of epoch-91 batch-48 = 4.7157518565654755e-06

Training epoch-91 batch-49
Running loss of epoch-91 batch-49 = 3.7711579352617264e-06

Training epoch-91 batch-50
Running loss of epoch-91 batch-50 = 3.0775554478168488e-06

Training epoch-91 batch-51
Running loss of epoch-91 batch-51 = 1.4936085790395737e-06

Training epoch-91 batch-52
Running loss of epoch-91 batch-52 = 1.6563571989536285e-06

Training epoch-91 batch-53
Running loss of epoch-91 batch-53 = 2.1602027118206024e-06

Training epoch-91 batch-54
Running loss of epoch-91 batch-54 = 2.2246968001127243e-06

Training epoch-91 batch-55
Running loss of epoch-91 batch-55 = 4.295492544770241e-06

Training epoch-91 batch-56
Running loss of epoch-91 batch-56 = 2.5567132979631424e-06

Training epoch-91 batch-57
Running loss of epoch-91 batch-57 = 3.8566067814826965e-06

Training epoch-91 batch-58
Running loss of epoch-91 batch-58 = 2.62516550719738e-06

Training epoch-91 batch-59
Running loss of epoch-91 batch-59 = 1.553678885102272e-06

Training epoch-91 batch-60
Running loss of epoch-91 batch-60 = 3.4456606954336166e-06

Training epoch-91 batch-61
Running loss of epoch-91 batch-61 = 2.1085143089294434e-06

Training epoch-91 batch-62
Running loss of epoch-91 batch-62 = 2.253800630569458e-06

Training epoch-91 batch-63
Running loss of epoch-91 batch-63 = 2.500368282198906e-06

Training epoch-91 batch-64
Running loss of epoch-91 batch-64 = 3.0794180929660797e-06

Training epoch-91 batch-65
Running loss of epoch-91 batch-65 = 2.7834903448820114e-06

Training epoch-91 batch-66
Running loss of epoch-91 batch-66 = 4.07826155424118e-06

Training epoch-91 batch-67
Running loss of epoch-91 batch-67 = 3.5034026950597763e-06

Training epoch-91 batch-68
Running loss of epoch-91 batch-68 = 1.6153790056705475e-06

Training epoch-91 batch-69
Running loss of epoch-91 batch-69 = 2.34297476708889e-06

Training epoch-91 batch-70
Running loss of epoch-91 batch-70 = 4.058238118886948e-06

Training epoch-91 batch-71
Running loss of epoch-91 batch-71 = 1.6903504729270935e-06

Training epoch-91 batch-72
Running loss of epoch-91 batch-72 = 2.5513581931591034e-06

Training epoch-91 batch-73
Running loss of epoch-91 batch-73 = 3.1159725040197372e-06

Training epoch-91 batch-74
Running loss of epoch-91 batch-74 = 7.14324414730072e-07

Training epoch-91 batch-75
Running loss of epoch-91 batch-75 = 3.4195836633443832e-06

Training epoch-91 batch-76
Running loss of epoch-91 batch-76 = 1.3316981494426727e-05

Training epoch-91 batch-77
Running loss of epoch-91 batch-77 = 2.692919224500656e-06

Training epoch-91 batch-78
Running loss of epoch-91 batch-78 = 2.2707972675561905e-06

Training epoch-91 batch-79
Running loss of epoch-91 batch-79 = 2.7045607566833496e-06

Training epoch-91 batch-80
Running loss of epoch-91 batch-80 = 2.7136411517858505e-06

Training epoch-91 batch-81
Running loss of epoch-91 batch-81 = 3.877794370055199e-06

Training epoch-91 batch-82
Running loss of epoch-91 batch-82 = 4.696426913142204e-06

Training epoch-91 batch-83
Running loss of epoch-91 batch-83 = 2.68593430519104e-06

Training epoch-91 batch-84
Running loss of epoch-91 batch-84 = 2.7562491595745087e-06

Training epoch-91 batch-85
Running loss of epoch-91 batch-85 = 5.8247242122888565e-06

Training epoch-91 batch-86
Running loss of epoch-91 batch-86 = 3.730645403265953e-06

Training epoch-91 batch-87
Running loss of epoch-91 batch-87 = 3.8244761526584625e-06

Training epoch-91 batch-88
Running loss of epoch-91 batch-88 = 2.7942005544900894e-06

Training epoch-91 batch-89
Running loss of epoch-91 batch-89 = 4.747183993458748e-06

Training epoch-91 batch-90
Running loss of epoch-91 batch-90 = 1.209694892168045e-05

Training epoch-91 batch-91
Running loss of epoch-91 batch-91 = 3.5888515412807465e-06

Training epoch-91 batch-92
Running loss of epoch-91 batch-92 = 2.90735624730587e-06

Training epoch-91 batch-93
Running loss of epoch-91 batch-93 = 2.5636982172727585e-06

Training epoch-91 batch-94
Running loss of epoch-91 batch-94 = 5.22984191775322e-06

Training epoch-91 batch-95
Running loss of epoch-91 batch-95 = 3.6335550248622894e-06

Training epoch-91 batch-96
Running loss of epoch-91 batch-96 = 4.948815330862999e-06

Training epoch-91 batch-97
Running loss of epoch-91 batch-97 = 3.7366990000009537e-06

Training epoch-91 batch-98
Running loss of epoch-91 batch-98 = 1.635868102312088e-06

Training epoch-91 batch-99
Running loss of epoch-91 batch-99 = 3.295252099633217e-06

Training epoch-91 batch-100
Running loss of epoch-91 batch-100 = 1.4882534742355347e-06

Training epoch-91 batch-101
Running loss of epoch-91 batch-101 = 1.129903830587864e-05

Training epoch-91 batch-102
Running loss of epoch-91 batch-102 = 5.27198426425457e-06

Training epoch-91 batch-103
Running loss of epoch-91 batch-103 = 1.882202923297882e-06

Training epoch-91 batch-104
Running loss of epoch-91 batch-104 = 2.485932782292366e-06

Training epoch-91 batch-105
Running loss of epoch-91 batch-105 = 3.4978147596120834e-06

Training epoch-91 batch-106
Running loss of epoch-91 batch-106 = 2.8905924409627914e-06

Training epoch-91 batch-107
Running loss of epoch-91 batch-107 = 1.973705366253853e-06

Training epoch-91 batch-108
Running loss of epoch-91 batch-108 = 2.6831403374671936e-06

Training epoch-91 batch-109
Running loss of epoch-91 batch-109 = 2.062646672129631e-06

Training epoch-91 batch-110
Running loss of epoch-91 batch-110 = 5.204463377594948e-06

Training epoch-91 batch-111
Running loss of epoch-91 batch-111 = 5.109468474984169e-06

Training epoch-91 batch-112
Running loss of epoch-91 batch-112 = 7.225200533866882e-06

Training epoch-91 batch-113
Running loss of epoch-91 batch-113 = 4.741363227367401e-06

Training epoch-91 batch-114
Running loss of epoch-91 batch-114 = 4.3262261897325516e-06

Training epoch-91 batch-115
Running loss of epoch-91 batch-115 = 6.8461522459983826e-06

Training epoch-91 batch-116
Running loss of epoch-91 batch-116 = 3.8316939026117325e-06

Training epoch-91 batch-117
Running loss of epoch-91 batch-117 = 1.3345852494239807e-06

Training epoch-91 batch-118
Running loss of epoch-91 batch-118 = 2.736225724220276e-06

Training epoch-91 batch-119
Running loss of epoch-91 batch-119 = 1.9760336726903915e-06

Training epoch-91 batch-120
Running loss of epoch-91 batch-120 = 7.797963917255402e-06

Training epoch-91 batch-121
Running loss of epoch-91 batch-121 = 4.392582923173904e-06

Training epoch-91 batch-122
Running loss of epoch-91 batch-122 = 1.8319115042686462e-06

Training epoch-91 batch-123
Running loss of epoch-91 batch-123 = 9.325798600912094e-06

Training epoch-91 batch-124
Running loss of epoch-91 batch-124 = 4.3371692299842834e-06

Training epoch-91 batch-125
Running loss of epoch-91 batch-125 = 3.5457778722047806e-06

Training epoch-91 batch-126
Running loss of epoch-91 batch-126 = 8.784467354416847e-06

Training epoch-91 batch-127
Running loss of epoch-91 batch-127 = 1.7958227545022964e-06

Training epoch-91 batch-128
Running loss of epoch-91 batch-128 = 2.7371570467948914e-06

Training epoch-91 batch-129
Running loss of epoch-91 batch-129 = 4.3173786252737045e-06

Training epoch-91 batch-130
Running loss of epoch-91 batch-130 = 8.879462257027626e-06

Training epoch-91 batch-131
Running loss of epoch-91 batch-131 = 3.747176378965378e-06

Training epoch-91 batch-132
Running loss of epoch-91 batch-132 = 2.4116598069667816e-06

Training epoch-91 batch-133
Running loss of epoch-91 batch-133 = 3.309454768896103e-06

Training epoch-91 batch-134
Running loss of epoch-91 batch-134 = 9.568408131599426e-06

Training epoch-91 batch-135
Running loss of epoch-91 batch-135 = 6.703194230794907e-06

Training epoch-91 batch-136
Running loss of epoch-91 batch-136 = 7.369322702288628e-06

Training epoch-91 batch-137
Running loss of epoch-91 batch-137 = 1.3648532330989838e-06

Training epoch-91 batch-138
Running loss of epoch-91 batch-138 = 3.0221417546272278e-06

Training epoch-91 batch-139
Running loss of epoch-91 batch-139 = 4.576751962304115e-06

Training epoch-91 batch-140
Running loss of epoch-91 batch-140 = 2.69315205514431e-06

Training epoch-91 batch-141
Running loss of epoch-91 batch-141 = 1.6523990780115128e-06

Training epoch-91 batch-142
Running loss of epoch-91 batch-142 = 4.180707037448883e-06

Training epoch-91 batch-143
Running loss of epoch-91 batch-143 = 2.6386696845293045e-06

Training epoch-91 batch-144
Running loss of epoch-91 batch-144 = 8.33394005894661e-06

Training epoch-91 batch-145
Running loss of epoch-91 batch-145 = 2.7171336114406586e-06

Training epoch-91 batch-146
Running loss of epoch-91 batch-146 = 3.204215317964554e-06

Training epoch-91 batch-147
Running loss of epoch-91 batch-147 = 3.741122782230377e-06

Training epoch-91 batch-148
Running loss of epoch-91 batch-148 = 8.482951670885086e-06

Training epoch-91 batch-149
Running loss of epoch-91 batch-149 = 4.708999767899513e-06

Training epoch-91 batch-150
Running loss of epoch-91 batch-150 = 2.496642991900444e-06

Training epoch-91 batch-151
Running loss of epoch-91 batch-151 = 2.414686605334282e-06

Training epoch-91 batch-152
Running loss of epoch-91 batch-152 = 2.473127096891403e-06

Training epoch-91 batch-153
Running loss of epoch-91 batch-153 = 3.0782539397478104e-06

Training epoch-91 batch-154
Running loss of epoch-91 batch-154 = 2.3620668798685074e-06

Training epoch-91 batch-155
Running loss of epoch-91 batch-155 = 3.0638184398412704e-06

Training epoch-91 batch-156
Running loss of epoch-91 batch-156 = 6.51087611913681e-06

Training epoch-91 batch-157
Running loss of epoch-91 batch-157 = 4.1387975215911865e-06

Finished training epoch-91.



Average train loss at epoch-91 = 3.7433728575706483e-06

Started Evaluation

Average val loss at epoch-91 = 1.8511766662672822

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.14 %

Finished Evaluation



Started training epoch-92


Training epoch-92 batch-1
Running loss of epoch-92 batch-1 = 2.559507265686989e-06

Training epoch-92 batch-2
Running loss of epoch-92 batch-2 = 3.78582626581192e-06

Training epoch-92 batch-3
Running loss of epoch-92 batch-3 = 2.4226028472185135e-06

Training epoch-92 batch-4
Running loss of epoch-92 batch-4 = 1.6205012798309326e-06

Training epoch-92 batch-5
Running loss of epoch-92 batch-5 = 2.678949385881424e-06

Training epoch-92 batch-6
Running loss of epoch-92 batch-6 = 5.661742761731148e-06

Training epoch-92 batch-7
Running loss of epoch-92 batch-7 = 6.251968443393707e-06

Training epoch-92 batch-8
Running loss of epoch-92 batch-8 = 2.164626494050026e-06

Training epoch-92 batch-9
Running loss of epoch-92 batch-9 = 4.156259819865227e-06

Training epoch-92 batch-10
Running loss of epoch-92 batch-10 = 3.348803147673607e-06

Training epoch-92 batch-11
Running loss of epoch-92 batch-11 = 2.3613683879375458e-06

Training epoch-92 batch-12
Running loss of epoch-92 batch-12 = 2.1345913410186768e-06

Training epoch-92 batch-13
Running loss of epoch-92 batch-13 = 4.279427230358124e-06

Training epoch-92 batch-14
Running loss of epoch-92 batch-14 = 3.9441511034965515e-06

Training epoch-92 batch-15
Running loss of epoch-92 batch-15 = 2.1515879780054092e-06

Training epoch-92 batch-16
Running loss of epoch-92 batch-16 = 1.8440186977386475e-06

Training epoch-92 batch-17
Running loss of epoch-92 batch-17 = 5.726004019379616e-06

Training epoch-92 batch-18
Running loss of epoch-92 batch-18 = 4.165340214967728e-06

Training epoch-92 batch-19
Running loss of epoch-92 batch-19 = 3.4761615097522736e-06

Training epoch-92 batch-20
Running loss of epoch-92 batch-20 = 6.649643182754517e-06

Training epoch-92 batch-21
Running loss of epoch-92 batch-21 = 5.017034709453583e-06

Training epoch-92 batch-22
Running loss of epoch-92 batch-22 = 1.850305125117302e-06

Training epoch-92 batch-23
Running loss of epoch-92 batch-23 = 8.765608072280884e-06

Training epoch-92 batch-24
Running loss of epoch-92 batch-24 = 3.4919939935207367e-06

Training epoch-92 batch-25
Running loss of epoch-92 batch-25 = 2.3837201297283173e-06

Training epoch-92 batch-26
Running loss of epoch-92 batch-26 = 1.3629905879497528e-06

Training epoch-92 batch-27
Running loss of epoch-92 batch-27 = 5.5425334721803665e-06

Training epoch-92 batch-28
Running loss of epoch-92 batch-28 = 1.018960028886795e-05

Training epoch-92 batch-29
Running loss of epoch-92 batch-29 = 5.5355485528707504e-06

Training epoch-92 batch-30
Running loss of epoch-92 batch-30 = 1.0628718882799149e-06

Training epoch-92 batch-31
Running loss of epoch-92 batch-31 = 1.1913944035768509e-06

Training epoch-92 batch-32
Running loss of epoch-92 batch-32 = 1.3883691281080246e-06

Training epoch-92 batch-33
Running loss of epoch-92 batch-33 = 3.0314549803733826e-06

Training epoch-92 batch-34
Running loss of epoch-92 batch-34 = 9.026844054460526e-06

Training epoch-92 batch-35
Running loss of epoch-92 batch-35 = 2.7136411517858505e-06

Training epoch-92 batch-36
Running loss of epoch-92 batch-36 = 3.273598849773407e-06

Training epoch-92 batch-37
Running loss of epoch-92 batch-37 = 2.823770046234131e-06

Training epoch-92 batch-38
Running loss of epoch-92 batch-38 = 4.362314939498901e-06

Training epoch-92 batch-39
Running loss of epoch-92 batch-39 = 3.0780211091041565e-06

Training epoch-92 batch-40
Running loss of epoch-92 batch-40 = 3.149965777993202e-06

Training epoch-92 batch-41
Running loss of epoch-92 batch-41 = 4.601897671818733e-06

Training epoch-92 batch-42
Running loss of epoch-92 batch-42 = 2.6810448616743088e-06

Training epoch-92 batch-43
Running loss of epoch-92 batch-43 = 3.3064279705286026e-06

Training epoch-92 batch-44
Running loss of epoch-92 batch-44 = 3.0740629881620407e-06

Training epoch-92 batch-45
Running loss of epoch-92 batch-45 = 6.966292858123779e-07

Training epoch-92 batch-46
Running loss of epoch-92 batch-46 = 4.437984898686409e-06

Training epoch-92 batch-47
Running loss of epoch-92 batch-47 = 7.390044629573822e-06

Training epoch-92 batch-48
Running loss of epoch-92 batch-48 = 4.481524229049683e-06

Training epoch-92 batch-49
Running loss of epoch-92 batch-49 = 3.9907172322273254e-06

Training epoch-92 batch-50
Running loss of epoch-92 batch-50 = 5.413079634308815e-06

Training epoch-92 batch-51
Running loss of epoch-92 batch-51 = 2.9366929084062576e-06

Training epoch-92 batch-52
Running loss of epoch-92 batch-52 = 2.9159709811210632e-06

Training epoch-92 batch-53
Running loss of epoch-92 batch-53 = 1.8400605767965317e-06

Training epoch-92 batch-54
Running loss of epoch-92 batch-54 = 9.431969374418259e-06

Training epoch-92 batch-55
Running loss of epoch-92 batch-55 = 2.4104956537485123e-06

Training epoch-92 batch-56
Running loss of epoch-92 batch-56 = 4.599336534738541e-06

Training epoch-92 batch-57
Running loss of epoch-92 batch-57 = 1.6780104488134384e-06

Training epoch-92 batch-58
Running loss of epoch-92 batch-58 = 3.8619618862867355e-06

Training epoch-92 batch-59
Running loss of epoch-92 batch-59 = 3.569759428501129e-06

Training epoch-92 batch-60
Running loss of epoch-92 batch-60 = 8.616596460342407e-06

Training epoch-92 batch-61
Running loss of epoch-92 batch-61 = 1.062639057636261e-06

Training epoch-92 batch-62
Running loss of epoch-92 batch-62 = 3.2784882932901382e-06

Training epoch-92 batch-63
Running loss of epoch-92 batch-63 = 1.0283198207616806e-05

Training epoch-92 batch-64
Running loss of epoch-92 batch-64 = 3.1548552215099335e-06

Training epoch-92 batch-65
Running loss of epoch-92 batch-65 = 3.957422450184822e-06

Training epoch-92 batch-66
Running loss of epoch-92 batch-66 = 2.009095624089241e-06

Training epoch-92 batch-67
Running loss of epoch-92 batch-67 = 1.3150274753570557e-06

Training epoch-92 batch-68
Running loss of epoch-92 batch-68 = 3.552529960870743e-06

Training epoch-92 batch-69
Running loss of epoch-92 batch-69 = 1.219799742102623e-06

Training epoch-92 batch-70
Running loss of epoch-92 batch-70 = 2.63238325715065e-06

Training epoch-92 batch-71
Running loss of epoch-92 batch-71 = 1.3115350157022476e-06

Training epoch-92 batch-72
Running loss of epoch-92 batch-72 = 2.4042092263698578e-06

Training epoch-92 batch-73
Running loss of epoch-92 batch-73 = 3.3618416637182236e-06

Training epoch-92 batch-74
Running loss of epoch-92 batch-74 = 2.632616087794304e-06

Training epoch-92 batch-75
Running loss of epoch-92 batch-75 = 3.6403071135282516e-06

Training epoch-92 batch-76
Running loss of epoch-92 batch-76 = 5.139503628015518e-06

Training epoch-92 batch-77
Running loss of epoch-92 batch-77 = 2.27196142077446e-06

Training epoch-92 batch-78
Running loss of epoch-92 batch-78 = 2.0659063011407852e-06

Training epoch-92 batch-79
Running loss of epoch-92 batch-79 = 1.2509990483522415e-06

Training epoch-92 batch-80
Running loss of epoch-92 batch-80 = 3.266381099820137e-06

Training epoch-92 batch-81
Running loss of epoch-92 batch-81 = 3.1427480280399323e-06

Training epoch-92 batch-82
Running loss of epoch-92 batch-82 = 4.657311365008354e-06

Training epoch-92 batch-83
Running loss of epoch-92 batch-83 = 3.346707671880722e-06

Training epoch-92 batch-84
Running loss of epoch-92 batch-84 = 4.178844392299652e-06

Training epoch-92 batch-85
Running loss of epoch-92 batch-85 = 2.912478521466255e-06

Training epoch-92 batch-86
Running loss of epoch-92 batch-86 = 5.061505362391472e-06

Training epoch-92 batch-87
Running loss of epoch-92 batch-87 = 1.0825693607330322e-05

Training epoch-92 batch-88
Running loss of epoch-92 batch-88 = 3.1543895602226257e-06

Training epoch-92 batch-89
Running loss of epoch-92 batch-89 = 4.09386120736599e-06

Training epoch-92 batch-90
Running loss of epoch-92 batch-90 = 2.1622981876134872e-06

Training epoch-92 batch-91
Running loss of epoch-92 batch-91 = 4.112487658858299e-06

Training epoch-92 batch-92
Running loss of epoch-92 batch-92 = 3.7439167499542236e-06

Training epoch-92 batch-93
Running loss of epoch-92 batch-93 = 3.31830233335495e-06

Training epoch-92 batch-94
Running loss of epoch-92 batch-94 = 7.1802642196416855e-06

Training epoch-92 batch-95
Running loss of epoch-92 batch-95 = 2.5078188627958298e-06

Training epoch-92 batch-96
Running loss of epoch-92 batch-96 = 2.04634852707386e-06

Training epoch-92 batch-97
Running loss of epoch-92 batch-97 = 6.343936547636986e-06

Training epoch-92 batch-98
Running loss of epoch-92 batch-98 = 2.7026981115341187e-06

Training epoch-92 batch-99
Running loss of epoch-92 batch-99 = 1.712702214717865e-06

Training epoch-92 batch-100
Running loss of epoch-92 batch-100 = 1.7762649804353714e-06

Training epoch-92 batch-101
Running loss of epoch-92 batch-101 = 2.1988525986671448e-06

Training epoch-92 batch-102
Running loss of epoch-92 batch-102 = 1.6854610294103622e-06

Training epoch-92 batch-103
Running loss of epoch-92 batch-103 = 3.764405846595764e-06

Training epoch-92 batch-104
Running loss of epoch-92 batch-104 = 6.193295121192932e-06

Training epoch-92 batch-105
Running loss of epoch-92 batch-105 = 2.9085204005241394e-06

Training epoch-92 batch-106
Running loss of epoch-92 batch-106 = 2.2263266146183014e-06

Training epoch-92 batch-107
Running loss of epoch-92 batch-107 = 3.865454345941544e-06

Training epoch-92 batch-108
Running loss of epoch-92 batch-108 = 9.562354534864426e-07

Training epoch-92 batch-109
Running loss of epoch-92 batch-109 = 3.22563573718071e-06

Training epoch-92 batch-110
Running loss of epoch-92 batch-110 = 2.514803782105446e-06

Training epoch-92 batch-111
Running loss of epoch-92 batch-111 = 3.0801165848970413e-06

Training epoch-92 batch-112
Running loss of epoch-92 batch-112 = 1.2402888387441635e-06

Training epoch-92 batch-113
Running loss of epoch-92 batch-113 = 3.834720700979233e-06

Training epoch-92 batch-114
Running loss of epoch-92 batch-114 = 4.221918061375618e-06

Training epoch-92 batch-115
Running loss of epoch-92 batch-115 = 3.6424025893211365e-06

Training epoch-92 batch-116
Running loss of epoch-92 batch-116 = 3.3499673008918762e-06

Training epoch-92 batch-117
Running loss of epoch-92 batch-117 = 2.3650936782360077e-06

Training epoch-92 batch-118
Running loss of epoch-92 batch-118 = 3.245193511247635e-06

Training epoch-92 batch-119
Running loss of epoch-92 batch-119 = 6.5264757722616196e-06

Training epoch-92 batch-120
Running loss of epoch-92 batch-120 = 7.441500201821327e-06

Training epoch-92 batch-121
Running loss of epoch-92 batch-121 = 3.961147740483284e-06

Training epoch-92 batch-122
Running loss of epoch-92 batch-122 = 2.644723281264305e-06

Training epoch-92 batch-123
Running loss of epoch-92 batch-123 = 3.8906000554561615e-06

Training epoch-92 batch-124
Running loss of epoch-92 batch-124 = 4.632631316781044e-06

Training epoch-92 batch-125
Running loss of epoch-92 batch-125 = 2.786284312605858e-06

Training epoch-92 batch-126
Running loss of epoch-92 batch-126 = 8.449656888842583e-06

Training epoch-92 batch-127
Running loss of epoch-92 batch-127 = 7.276423275470734e-06

Training epoch-92 batch-128
Running loss of epoch-92 batch-128 = 4.470348358154297e-06

Training epoch-92 batch-129
Running loss of epoch-92 batch-129 = 2.870103344321251e-06

Training epoch-92 batch-130
Running loss of epoch-92 batch-130 = 3.40072438120842e-06

Training epoch-92 batch-131
Running loss of epoch-92 batch-131 = 3.2652169466018677e-06

Training epoch-92 batch-132
Running loss of epoch-92 batch-132 = 5.366979166865349e-06

Training epoch-92 batch-133
Running loss of epoch-92 batch-133 = 3.3550895750522614e-06

Training epoch-92 batch-134
Running loss of epoch-92 batch-134 = 8.069910109043121e-07

Training epoch-92 batch-135
Running loss of epoch-92 batch-135 = 2.349959686398506e-06

Training epoch-92 batch-136
Running loss of epoch-92 batch-136 = 1.8514692783355713e-06

Training epoch-92 batch-137
Running loss of epoch-92 batch-137 = 5.023321136832237e-06

Training epoch-92 batch-138
Running loss of epoch-92 batch-138 = 3.0084047466516495e-06

Training epoch-92 batch-139
Running loss of epoch-92 batch-139 = 6.422167643904686e-06

Training epoch-92 batch-140
Running loss of epoch-92 batch-140 = 3.380933776497841e-06

Training epoch-92 batch-141
Running loss of epoch-92 batch-141 = 1.4223624020814896e-06

Training epoch-92 batch-142
Running loss of epoch-92 batch-142 = 5.532987415790558e-06

Training epoch-92 batch-143
Running loss of epoch-92 batch-143 = 3.3425167202949524e-06

Training epoch-92 batch-144
Running loss of epoch-92 batch-144 = 2.460787072777748e-06

Training epoch-92 batch-145
Running loss of epoch-92 batch-145 = 4.668021574616432e-06

Training epoch-92 batch-146
Running loss of epoch-92 batch-146 = 1.7578713595867157e-06

Training epoch-92 batch-147
Running loss of epoch-92 batch-147 = 2.580927684903145e-06

Training epoch-92 batch-148
Running loss of epoch-92 batch-148 = 2.4051405489444733e-06

Training epoch-92 batch-149
Running loss of epoch-92 batch-149 = 5.008885636925697e-06

Training epoch-92 batch-150
Running loss of epoch-92 batch-150 = 3.144610673189163e-06

Training epoch-92 batch-151
Running loss of epoch-92 batch-151 = 5.194917321205139e-06

Training epoch-92 batch-152
Running loss of epoch-92 batch-152 = 1.1243391782045364e-06

Training epoch-92 batch-153
Running loss of epoch-92 batch-153 = 2.809334546327591e-06

Training epoch-92 batch-154
Running loss of epoch-92 batch-154 = 3.5993289202451706e-06

Training epoch-92 batch-155
Running loss of epoch-92 batch-155 = 5.625421181321144e-06

Training epoch-92 batch-156
Running loss of epoch-92 batch-156 = 5.005393177270889e-06

Training epoch-92 batch-157
Running loss of epoch-92 batch-157 = 3.2708048820495605e-06

Finished training epoch-92.



Average train loss at epoch-92 = 3.6835238337516786e-06

Started Evaluation

Average val loss at epoch-92 = 1.8415168812006926

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.12 %
Accuracy for class execute is: 51.00 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.16 %

Finished Evaluation



Started training epoch-93


Training epoch-93 batch-1
Running loss of epoch-93 batch-1 = 5.027977749705315e-06

Training epoch-93 batch-2
Running loss of epoch-93 batch-2 = 1.5457626432180405e-06

Training epoch-93 batch-3
Running loss of epoch-93 batch-3 = 2.7567148208618164e-06

Training epoch-93 batch-4
Running loss of epoch-93 batch-4 = 6.227754056453705e-06

Training epoch-93 batch-5
Running loss of epoch-93 batch-5 = 4.076631739735603e-06

Training epoch-93 batch-6
Running loss of epoch-93 batch-6 = 2.2309832274913788e-06

Training epoch-93 batch-7
Running loss of epoch-93 batch-7 = 2.6759225875139236e-06

Training epoch-93 batch-8
Running loss of epoch-93 batch-8 = 2.0435545593500137e-06

Training epoch-93 batch-9
Running loss of epoch-93 batch-9 = 1.2624077498912811e-06

Training epoch-93 batch-10
Running loss of epoch-93 batch-10 = 2.9690563678741455e-06

Training epoch-93 batch-11
Running loss of epoch-93 batch-11 = 4.228437319397926e-06

Training epoch-93 batch-12
Running loss of epoch-93 batch-12 = 2.0496081560850143e-06

Training epoch-93 batch-13
Running loss of epoch-93 batch-13 = 1.952052116394043e-06

Training epoch-93 batch-14
Running loss of epoch-93 batch-14 = 4.781177267432213e-06

Training epoch-93 batch-15
Running loss of epoch-93 batch-15 = 1.0282034054398537e-05

Training epoch-93 batch-16
Running loss of epoch-93 batch-16 = 1.1688098311424255e-06

Training epoch-93 batch-17
Running loss of epoch-93 batch-17 = 5.0587113946676254e-06

Training epoch-93 batch-18
Running loss of epoch-93 batch-18 = 5.902489647269249e-06

Training epoch-93 batch-19
Running loss of epoch-93 batch-19 = 1.7944257706403732e-06

Training epoch-93 batch-20
Running loss of epoch-93 batch-20 = 3.880355507135391e-06

Training epoch-93 batch-21
Running loss of epoch-93 batch-21 = 2.2924505174160004e-06

Training epoch-93 batch-22
Running loss of epoch-93 batch-22 = 4.1937455534935e-06

Training epoch-93 batch-23
Running loss of epoch-93 batch-23 = 2.9855873435735703e-06

Training epoch-93 batch-24
Running loss of epoch-93 batch-24 = 3.676861524581909e-06

Training epoch-93 batch-25
Running loss of epoch-93 batch-25 = 4.3050386011600494e-07

Training epoch-93 batch-26
Running loss of epoch-93 batch-26 = 2.7213245630264282e-06

Training epoch-93 batch-27
Running loss of epoch-93 batch-27 = 4.700850695371628e-06

Training epoch-93 batch-28
Running loss of epoch-93 batch-28 = 1.1467374861240387e-05

Training epoch-93 batch-29
Running loss of epoch-93 batch-29 = 5.072448402643204e-06

Training epoch-93 batch-30
Running loss of epoch-93 batch-30 = 2.0083971321582794e-06

Training epoch-93 batch-31
Running loss of epoch-93 batch-31 = 8.891802281141281e-07

Training epoch-93 batch-32
Running loss of epoch-93 batch-32 = 7.245689630508423e-06

Training epoch-93 batch-33
Running loss of epoch-93 batch-33 = 6.3497573137283325e-06

Training epoch-93 batch-34
Running loss of epoch-93 batch-34 = 4.777451977133751e-06

Training epoch-93 batch-35
Running loss of epoch-93 batch-35 = 1.4456454664468765e-06

Training epoch-93 batch-36
Running loss of epoch-93 batch-36 = 2.7369242161512375e-06

Training epoch-93 batch-37
Running loss of epoch-93 batch-37 = 2.6586931198835373e-06

Training epoch-93 batch-38
Running loss of epoch-93 batch-38 = 5.000270903110504e-06

Training epoch-93 batch-39
Running loss of epoch-93 batch-39 = 3.47895547747612e-06

Training epoch-93 batch-40
Running loss of epoch-93 batch-40 = 2.7802307158708572e-06

Training epoch-93 batch-41
Running loss of epoch-93 batch-41 = 3.245193511247635e-06

Training epoch-93 batch-42
Running loss of epoch-93 batch-42 = 4.576053470373154e-06

Training epoch-93 batch-43
Running loss of epoch-93 batch-43 = 4.7548674046993256e-06

Training epoch-93 batch-44
Running loss of epoch-93 batch-44 = 2.11433507502079e-06

Training epoch-93 batch-45
Running loss of epoch-93 batch-45 = 5.061505362391472e-06

Training epoch-93 batch-46
Running loss of epoch-93 batch-46 = 3.47895547747612e-06

Training epoch-93 batch-47
Running loss of epoch-93 batch-47 = 2.630986273288727e-06

Training epoch-93 batch-48
Running loss of epoch-93 batch-48 = 2.7569476515054703e-06

Training epoch-93 batch-49
Running loss of epoch-93 batch-49 = 2.1934974938631058e-06

Training epoch-93 batch-50
Running loss of epoch-93 batch-50 = 2.8386712074279785e-06

Training epoch-93 batch-51
Running loss of epoch-93 batch-51 = 3.914814442396164e-06

Training epoch-93 batch-52
Running loss of epoch-93 batch-52 = 4.237983375787735e-06

Training epoch-93 batch-53
Running loss of epoch-93 batch-53 = 1.2812670320272446e-06

Training epoch-93 batch-54
Running loss of epoch-93 batch-54 = 2.435874193906784e-06

Training epoch-93 batch-55
Running loss of epoch-93 batch-55 = 2.0801089704036713e-06

Training epoch-93 batch-56
Running loss of epoch-93 batch-56 = 4.102010279893875e-06

Training epoch-93 batch-57
Running loss of epoch-93 batch-57 = 2.1490268409252167e-06

Training epoch-93 batch-58
Running loss of epoch-93 batch-58 = 1.5043187886476517e-06

Training epoch-93 batch-59
Running loss of epoch-93 batch-59 = 3.0177179723978043e-06

Training epoch-93 batch-60
Running loss of epoch-93 batch-60 = 3.1739473342895508e-06

Training epoch-93 batch-61
Running loss of epoch-93 batch-61 = 8.449656888842583e-06

Training epoch-93 batch-62
Running loss of epoch-93 batch-62 = 1.932494342327118e-06

Training epoch-93 batch-63
Running loss of epoch-93 batch-63 = 1.3706739991903305e-06

Training epoch-93 batch-64
Running loss of epoch-93 batch-64 = 5.32134436070919e-06

Training epoch-93 batch-65
Running loss of epoch-93 batch-65 = 2.1941959857940674e-06

Training epoch-93 batch-66
Running loss of epoch-93 batch-66 = 1.3848766684532166e-06

Training epoch-93 batch-67
Running loss of epoch-93 batch-67 = 8.783303201198578e-06

Training epoch-93 batch-68
Running loss of epoch-93 batch-68 = 4.5835040509700775e-06

Training epoch-93 batch-69
Running loss of epoch-93 batch-69 = 3.448687493801117e-06

Training epoch-93 batch-70
Running loss of epoch-93 batch-70 = 6.498768925666809e-06

Training epoch-93 batch-71
Running loss of epoch-93 batch-71 = 7.99819827079773e-06

Training epoch-93 batch-72
Running loss of epoch-93 batch-72 = 2.040993422269821e-06

Training epoch-93 batch-73
Running loss of epoch-93 batch-73 = 5.050096660852432e-06

Training epoch-93 batch-74
Running loss of epoch-93 batch-74 = 2.6205088943243027e-06

Training epoch-93 batch-75
Running loss of epoch-93 batch-75 = 5.0389207899570465e-06

Training epoch-93 batch-76
Running loss of epoch-93 batch-76 = 2.32551246881485e-06

Training epoch-93 batch-77
Running loss of epoch-93 batch-77 = 7.456168532371521e-06

Training epoch-93 batch-78
Running loss of epoch-93 batch-78 = 1.0087387636303902e-05

Training epoch-93 batch-79
Running loss of epoch-93 batch-79 = 4.258938133716583e-06

Training epoch-93 batch-80
Running loss of epoch-93 batch-80 = 3.0668452382087708e-06

Training epoch-93 batch-81
Running loss of epoch-93 batch-81 = 2.086162567138672e-06

Training epoch-93 batch-82
Running loss of epoch-93 batch-82 = 2.9422808438539505e-06

Training epoch-93 batch-83
Running loss of epoch-93 batch-83 = 7.043592631816864e-06

Training epoch-93 batch-84
Running loss of epoch-93 batch-84 = 1.1990778148174286e-06

Training epoch-93 batch-85
Running loss of epoch-93 batch-85 = 7.302034646272659e-06

Training epoch-93 batch-86
Running loss of epoch-93 batch-86 = 1.8065329641103745e-06

Training epoch-93 batch-87
Running loss of epoch-93 batch-87 = 5.567912012338638e-06

Training epoch-93 batch-88
Running loss of epoch-93 batch-88 = 2.500135451555252e-06

Training epoch-93 batch-89
Running loss of epoch-93 batch-89 = 1.6281846910715103e-06

Training epoch-93 batch-90
Running loss of epoch-93 batch-90 = 4.407716915011406e-06

Training epoch-93 batch-91
Running loss of epoch-93 batch-91 = 2.47173011302948e-06

Training epoch-93 batch-92
Running loss of epoch-93 batch-92 = 4.995381459593773e-06

Training epoch-93 batch-93
Running loss of epoch-93 batch-93 = 3.4803524613380432e-06

Training epoch-93 batch-94
Running loss of epoch-93 batch-94 = 1.780688762664795e-06

Training epoch-93 batch-95
Running loss of epoch-93 batch-95 = 3.1834933906793594e-06

Training epoch-93 batch-96
Running loss of epoch-93 batch-96 = 2.11130827665329e-06

Training epoch-93 batch-97
Running loss of epoch-93 batch-97 = 4.795147106051445e-06

Training epoch-93 batch-98
Running loss of epoch-93 batch-98 = 7.824506610631943e-06

Training epoch-93 batch-99
Running loss of epoch-93 batch-99 = 3.3869873732328415e-06

Training epoch-93 batch-100
Running loss of epoch-93 batch-100 = 5.465000867843628e-06

Training epoch-93 batch-101
Running loss of epoch-93 batch-101 = 3.16486693918705e-06

Training epoch-93 batch-102
Running loss of epoch-93 batch-102 = 1.0436400771141052e-05

Training epoch-93 batch-103
Running loss of epoch-93 batch-103 = 4.80026938021183e-06

Training epoch-93 batch-104
Running loss of epoch-93 batch-104 = 2.0423904061317444e-06

Training epoch-93 batch-105
Running loss of epoch-93 batch-105 = 2.430519089102745e-06

Training epoch-93 batch-106
Running loss of epoch-93 batch-106 = 4.898523911833763e-06

Training epoch-93 batch-107
Running loss of epoch-93 batch-107 = 4.490138962864876e-06

Training epoch-93 batch-108
Running loss of epoch-93 batch-108 = 3.64682637155056e-06

Training epoch-93 batch-109
Running loss of epoch-93 batch-109 = 1.8526334315538406e-06

Training epoch-93 batch-110
Running loss of epoch-93 batch-110 = 1.5865080058574677e-06

Training epoch-93 batch-111
Running loss of epoch-93 batch-111 = 1.8684659153223038e-06

Training epoch-93 batch-112
Running loss of epoch-93 batch-112 = 2.0086299628019333e-06

Training epoch-93 batch-113
Running loss of epoch-93 batch-113 = 2.369983121752739e-06

Training epoch-93 batch-114
Running loss of epoch-93 batch-114 = 2.314802259206772e-06

Training epoch-93 batch-115
Running loss of epoch-93 batch-115 = 2.339482307434082e-06

Training epoch-93 batch-116
Running loss of epoch-93 batch-116 = 4.408182576298714e-06

Training epoch-93 batch-117
Running loss of epoch-93 batch-117 = 4.062196239829063e-06

Training epoch-93 batch-118
Running loss of epoch-93 batch-118 = 3.523658961057663e-06

Training epoch-93 batch-119
Running loss of epoch-93 batch-119 = 3.4766271710395813e-06

Training epoch-93 batch-120
Running loss of epoch-93 batch-120 = 6.427522748708725e-06

Training epoch-93 batch-121
Running loss of epoch-93 batch-121 = 4.181405529379845e-06

Training epoch-93 batch-122
Running loss of epoch-93 batch-122 = 2.3115426301956177e-06

Training epoch-93 batch-123
Running loss of epoch-93 batch-123 = 2.567889168858528e-06

Training epoch-93 batch-124
Running loss of epoch-93 batch-124 = 1.2668315321207047e-06

Training epoch-93 batch-125
Running loss of epoch-93 batch-125 = 4.380242899060249e-06

Training epoch-93 batch-126
Running loss of epoch-93 batch-126 = 2.4151522666215897e-06

Training epoch-93 batch-127
Running loss of epoch-93 batch-127 = 2.225162461400032e-06

Training epoch-93 batch-128
Running loss of epoch-93 batch-128 = 1.4096731320023537e-05

Training epoch-93 batch-129
Running loss of epoch-93 batch-129 = 1.2463424354791641e-06

Training epoch-93 batch-130
Running loss of epoch-93 batch-130 = 2.572080120444298e-06

Training epoch-93 batch-131
Running loss of epoch-93 batch-131 = 2.571847289800644e-06

Training epoch-93 batch-132
Running loss of epoch-93 batch-132 = 2.800021320581436e-06

Training epoch-93 batch-133
Running loss of epoch-93 batch-133 = 1.6659032553434372e-06

Training epoch-93 batch-134
Running loss of epoch-93 batch-134 = 1.0786345228552818e-05

Training epoch-93 batch-135
Running loss of epoch-93 batch-135 = 1.387670636177063e-06

Training epoch-93 batch-136
Running loss of epoch-93 batch-136 = 3.7909485399723053e-06

Training epoch-93 batch-137
Running loss of epoch-93 batch-137 = 2.7350615710020065e-06

Training epoch-93 batch-138
Running loss of epoch-93 batch-138 = 2.3781321942806244e-06

Training epoch-93 batch-139
Running loss of epoch-93 batch-139 = 2.507120370864868e-06

Training epoch-93 batch-140
Running loss of epoch-93 batch-140 = 3.562774509191513e-06

Training epoch-93 batch-141
Running loss of epoch-93 batch-141 = 5.3299590945243835e-06

Training epoch-93 batch-142
Running loss of epoch-93 batch-142 = 2.2284220904111862e-06

Training epoch-93 batch-143
Running loss of epoch-93 batch-143 = 1.2293457984924316e-06

Training epoch-93 batch-144
Running loss of epoch-93 batch-144 = 2.2388994693756104e-06

Training epoch-93 batch-145
Running loss of epoch-93 batch-145 = 3.0829105526208878e-06

Training epoch-93 batch-146
Running loss of epoch-93 batch-146 = 1.5853438526391983e-06

Training epoch-93 batch-147
Running loss of epoch-93 batch-147 = 3.619818016886711e-06

Training epoch-93 batch-148
Running loss of epoch-93 batch-148 = 3.0132941901683807e-06

Training epoch-93 batch-149
Running loss of epoch-93 batch-149 = 1.5497207641601562e-06

Training epoch-93 batch-150
Running loss of epoch-93 batch-150 = 1.487787812948227e-06

Training epoch-93 batch-151
Running loss of epoch-93 batch-151 = 2.6293564587831497e-06

Training epoch-93 batch-152
Running loss of epoch-93 batch-152 = 3.7481077015399933e-06

Training epoch-93 batch-153
Running loss of epoch-93 batch-153 = 1.916429027915001e-06

Training epoch-93 batch-154
Running loss of epoch-93 batch-154 = 1.5208497643470764e-06

Training epoch-93 batch-155
Running loss of epoch-93 batch-155 = 4.10107895731926e-06

Training epoch-93 batch-156
Running loss of epoch-93 batch-156 = 2.7492642402648926e-06

Training epoch-93 batch-157
Running loss of epoch-93 batch-157 = 2.7120113372802734e-06

Finished training epoch-93.



Average train loss at epoch-93 = 3.626054525375366e-06

Started Evaluation

Average val loss at epoch-93 = 1.8521062069277254

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.69 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-94


Training epoch-94 batch-1
Running loss of epoch-94 batch-1 = 5.0049275159835815e-06

Training epoch-94 batch-2
Running loss of epoch-94 batch-2 = 1.2901145964860916e-06

Training epoch-94 batch-3
Running loss of epoch-94 batch-3 = 2.2780150175094604e-06

Training epoch-94 batch-4
Running loss of epoch-94 batch-4 = 2.868007868528366e-06

Training epoch-94 batch-5
Running loss of epoch-94 batch-5 = 2.949964255094528e-06

Training epoch-94 batch-6
Running loss of epoch-94 batch-6 = 1.469859853386879e-06

Training epoch-94 batch-7
Running loss of epoch-94 batch-7 = 4.288041964173317e-06

Training epoch-94 batch-8
Running loss of epoch-94 batch-8 = 1.9455328583717346e-06

Training epoch-94 batch-9
Running loss of epoch-94 batch-9 = 2.7674250304698944e-06

Training epoch-94 batch-10
Running loss of epoch-94 batch-10 = 6.861984729766846e-06

Training epoch-94 batch-11
Running loss of epoch-94 batch-11 = 1.0932562872767448e-05

Training epoch-94 batch-12
Running loss of epoch-94 batch-12 = 1.123407855629921e-06

Training epoch-94 batch-13
Running loss of epoch-94 batch-13 = 7.111113518476486e-06

Training epoch-94 batch-14
Running loss of epoch-94 batch-14 = 3.0102673918008804e-06

Training epoch-94 batch-15
Running loss of epoch-94 batch-15 = 3.0137598514556885e-06

Training epoch-94 batch-16
Running loss of epoch-94 batch-16 = 2.591172233223915e-06

Training epoch-94 batch-17
Running loss of epoch-94 batch-17 = 9.890645742416382e-07

Training epoch-94 batch-18
Running loss of epoch-94 batch-18 = 2.2207386791706085e-06

Training epoch-94 batch-19
Running loss of epoch-94 batch-19 = 2.620276063680649e-06

Training epoch-94 batch-20
Running loss of epoch-94 batch-20 = 8.228234946727753e-07

Training epoch-94 batch-21
Running loss of epoch-94 batch-21 = 5.065929144620895e-06

Training epoch-94 batch-22
Running loss of epoch-94 batch-22 = 2.575106918811798e-06

Training epoch-94 batch-23
Running loss of epoch-94 batch-23 = 3.3641699701547623e-06

Training epoch-94 batch-24
Running loss of epoch-94 batch-24 = 2.0815059542655945e-06

Training epoch-94 batch-25
Running loss of epoch-94 batch-25 = 1.5781261026859283e-06

Training epoch-94 batch-26
Running loss of epoch-94 batch-26 = 3.473600372672081e-06

Training epoch-94 batch-27
Running loss of epoch-94 batch-27 = 3.284541890025139e-06

Training epoch-94 batch-28
Running loss of epoch-94 batch-28 = 3.2139942049980164e-06

Training epoch-94 batch-29
Running loss of epoch-94 batch-29 = 9.091338142752647e-06

Training epoch-94 batch-30
Running loss of epoch-94 batch-30 = 3.377441316843033e-06

Training epoch-94 batch-31
Running loss of epoch-94 batch-31 = 6.011920049786568e-06

Training epoch-94 batch-32
Running loss of epoch-94 batch-32 = 3.7141144275665283e-06

Training epoch-94 batch-33
Running loss of epoch-94 batch-33 = 3.8496218621730804e-06

Training epoch-94 batch-34
Running loss of epoch-94 batch-34 = 6.742076948285103e-06

Training epoch-94 batch-35
Running loss of epoch-94 batch-35 = 2.339715138077736e-06

Training epoch-94 batch-36
Running loss of epoch-94 batch-36 = 3.5222619771957397e-06

Training epoch-94 batch-37
Running loss of epoch-94 batch-37 = 2.8351787477731705e-06

Training epoch-94 batch-38
Running loss of epoch-94 batch-38 = 8.140923455357552e-06

Training epoch-94 batch-39
Running loss of epoch-94 batch-39 = 2.7276109904050827e-06

Training epoch-94 batch-40
Running loss of epoch-94 batch-40 = 1.223292201757431e-06

Training epoch-94 batch-41
Running loss of epoch-94 batch-41 = 3.4533441066741943e-06

Training epoch-94 batch-42
Running loss of epoch-94 batch-42 = 6.5644271671772e-06

Training epoch-94 batch-43
Running loss of epoch-94 batch-43 = 3.827735781669617e-06

Training epoch-94 batch-44
Running loss of epoch-94 batch-44 = 4.5120250433683395e-06

Training epoch-94 batch-45
Running loss of epoch-94 batch-45 = 2.525513991713524e-06

Training epoch-94 batch-46
Running loss of epoch-94 batch-46 = 5.4640695452690125e-06

Training epoch-94 batch-47
Running loss of epoch-94 batch-47 = 3.071501851081848e-06

Training epoch-94 batch-48
Running loss of epoch-94 batch-48 = 4.506902769207954e-06

Training epoch-94 batch-49
Running loss of epoch-94 batch-49 = 1.5941914170980453e-06

Training epoch-94 batch-50
Running loss of epoch-94 batch-50 = 2.991175279021263e-06

Training epoch-94 batch-51
Running loss of epoch-94 batch-51 = 1.1497177183628082e-06

Training epoch-94 batch-52
Running loss of epoch-94 batch-52 = 4.221918061375618e-06

Training epoch-94 batch-53
Running loss of epoch-94 batch-53 = 2.503860741853714e-06

Training epoch-94 batch-54
Running loss of epoch-94 batch-54 = 6.677582859992981e-06

Training epoch-94 batch-55
Running loss of epoch-94 batch-55 = 1.6132835298776627e-06

Training epoch-94 batch-56
Running loss of epoch-94 batch-56 = 3.130408003926277e-06

Training epoch-94 batch-57
Running loss of epoch-94 batch-57 = 2.923421561717987e-06

Training epoch-94 batch-58
Running loss of epoch-94 batch-58 = 3.3329706639051437e-06

Training epoch-94 batch-59
Running loss of epoch-94 batch-59 = 2.5401823222637177e-06

Training epoch-94 batch-60
Running loss of epoch-94 batch-60 = 3.4903641790151596e-06

Training epoch-94 batch-61
Running loss of epoch-94 batch-61 = 1.528766006231308e-06

Training epoch-94 batch-62
Running loss of epoch-94 batch-62 = 2.014217898249626e-06

Training epoch-94 batch-63
Running loss of epoch-94 batch-63 = 7.188646122813225e-06

Training epoch-94 batch-64
Running loss of epoch-94 batch-64 = 2.5955960154533386e-06

Training epoch-94 batch-65
Running loss of epoch-94 batch-65 = 1.2058299034833908e-06

Training epoch-94 batch-66
Running loss of epoch-94 batch-66 = 3.6463607102632523e-06

Training epoch-94 batch-67
Running loss of epoch-94 batch-67 = 3.8079451769590378e-06

Training epoch-94 batch-68
Running loss of epoch-94 batch-68 = 7.820315659046173e-06

Training epoch-94 batch-69
Running loss of epoch-94 batch-69 = 3.28640453517437e-06

Training epoch-94 batch-70
Running loss of epoch-94 batch-70 = 2.6891939342021942e-06

Training epoch-94 batch-71
Running loss of epoch-94 batch-71 = 3.011198714375496e-06

Training epoch-94 batch-72
Running loss of epoch-94 batch-72 = 7.247086614370346e-06

Training epoch-94 batch-73
Running loss of epoch-94 batch-73 = 2.8489157557487488e-06

Training epoch-94 batch-74
Running loss of epoch-94 batch-74 = 5.905982106924057e-06

Training epoch-94 batch-75
Running loss of epoch-94 batch-75 = 2.3602042347192764e-06

Training epoch-94 batch-76
Running loss of epoch-94 batch-76 = 4.671746864914894e-06

Training epoch-94 batch-77
Running loss of epoch-94 batch-77 = 9.180745109915733e-06

Training epoch-94 batch-78
Running loss of epoch-94 batch-78 = 4.974426701664925e-06

Training epoch-94 batch-79
Running loss of epoch-94 batch-79 = 1.1692754924297333e-06

Training epoch-94 batch-80
Running loss of epoch-94 batch-80 = 4.787696525454521e-06

Training epoch-94 batch-81
Running loss of epoch-94 batch-81 = 3.2479874789714813e-06

Training epoch-94 batch-82
Running loss of epoch-94 batch-82 = 3.000488504767418e-06

Training epoch-94 batch-83
Running loss of epoch-94 batch-83 = 3.9888545870780945e-06

Training epoch-94 batch-84
Running loss of epoch-94 batch-84 = 1.7208512872457504e-06

Training epoch-94 batch-85
Running loss of epoch-94 batch-85 = 2.09989957511425e-06

Training epoch-94 batch-86
Running loss of epoch-94 batch-86 = 2.5902409106492996e-06

Training epoch-94 batch-87
Running loss of epoch-94 batch-87 = 3.3546239137649536e-06

Training epoch-94 batch-88
Running loss of epoch-94 batch-88 = 3.4186523407697678e-06

Training epoch-94 batch-89
Running loss of epoch-94 batch-89 = 5.6873541325330734e-06

Training epoch-94 batch-90
Running loss of epoch-94 batch-90 = 2.680812031030655e-06

Training epoch-94 batch-91
Running loss of epoch-94 batch-91 = 5.746493116021156e-06

Training epoch-94 batch-92
Running loss of epoch-94 batch-92 = 2.9383227229118347e-06

Training epoch-94 batch-93
Running loss of epoch-94 batch-93 = 2.6070047169923782e-06

Training epoch-94 batch-94
Running loss of epoch-94 batch-94 = 3.840075805783272e-06

Training epoch-94 batch-95
Running loss of epoch-94 batch-95 = 1.2200325727462769e-06

Training epoch-94 batch-96
Running loss of epoch-94 batch-96 = 3.960682079195976e-06

Training epoch-94 batch-97
Running loss of epoch-94 batch-97 = 3.1653326004743576e-06

Training epoch-94 batch-98
Running loss of epoch-94 batch-98 = 3.908993676304817e-06

Training epoch-94 batch-99
Running loss of epoch-94 batch-99 = 1.4242250472307205e-06

Training epoch-94 batch-100
Running loss of epoch-94 batch-100 = 2.166256308555603e-06

Training epoch-94 batch-101
Running loss of epoch-94 batch-101 = 4.018889740109444e-06

Training epoch-94 batch-102
Running loss of epoch-94 batch-102 = 4.156026989221573e-06

Training epoch-94 batch-103
Running loss of epoch-94 batch-103 = 1.253490336239338e-05

Training epoch-94 batch-104
Running loss of epoch-94 batch-104 = 5.076173692941666e-06

Training epoch-94 batch-105
Running loss of epoch-94 batch-105 = 3.5187695175409317e-06

Training epoch-94 batch-106
Running loss of epoch-94 batch-106 = 2.7997884899377823e-06

Training epoch-94 batch-107
Running loss of epoch-94 batch-107 = 2.3383181542158127e-06

Training epoch-94 batch-108
Running loss of epoch-94 batch-108 = 2.4349428713321686e-06

Training epoch-94 batch-109
Running loss of epoch-94 batch-109 = 8.642673492431641e-07

Training epoch-94 batch-110
Running loss of epoch-94 batch-110 = 2.9418151825666428e-06

Training epoch-94 batch-111
Running loss of epoch-94 batch-111 = 3.2440293580293655e-06

Training epoch-94 batch-112
Running loss of epoch-94 batch-112 = 4.539266228675842e-06

Training epoch-94 batch-113
Running loss of epoch-94 batch-113 = 1.346692442893982e-06

Training epoch-94 batch-114
Running loss of epoch-94 batch-114 = 3.7259887903928757e-06

Training epoch-94 batch-115
Running loss of epoch-94 batch-115 = 4.6030618250370026e-06

Training epoch-94 batch-116
Running loss of epoch-94 batch-116 = 3.200722858309746e-06

Training epoch-94 batch-117
Running loss of epoch-94 batch-117 = 1.8933787941932678e-06

Training epoch-94 batch-118
Running loss of epoch-94 batch-118 = 9.653158485889435e-07

Training epoch-94 batch-119
Running loss of epoch-94 batch-119 = 2.546701580286026e-06

Training epoch-94 batch-120
Running loss of epoch-94 batch-120 = 9.687384590506554e-06

Training epoch-94 batch-121
Running loss of epoch-94 batch-121 = 3.628665581345558e-06

Training epoch-94 batch-122
Running loss of epoch-94 batch-122 = 3.134133294224739e-06

Training epoch-94 batch-123
Running loss of epoch-94 batch-123 = 1.551816239953041e-06

Training epoch-94 batch-124
Running loss of epoch-94 batch-124 = 3.562774509191513e-06

Training epoch-94 batch-125
Running loss of epoch-94 batch-125 = 4.927860572934151e-06

Training epoch-94 batch-126
Running loss of epoch-94 batch-126 = 3.4263357520103455e-06

Training epoch-94 batch-127
Running loss of epoch-94 batch-127 = 1.0652001947164536e-06

Training epoch-94 batch-128
Running loss of epoch-94 batch-128 = 5.0389207899570465e-06

Training epoch-94 batch-129
Running loss of epoch-94 batch-129 = 4.027271643280983e-06

Training epoch-94 batch-130
Running loss of epoch-94 batch-130 = 5.615875124931335e-07

Training epoch-94 batch-131
Running loss of epoch-94 batch-131 = 1.8300488591194153e-06

Training epoch-94 batch-132
Running loss of epoch-94 batch-132 = 2.33273021876812e-06

Training epoch-94 batch-133
Running loss of epoch-94 batch-133 = 3.1134113669395447e-06

Training epoch-94 batch-134
Running loss of epoch-94 batch-134 = 1.971377059817314e-06

Training epoch-94 batch-135
Running loss of epoch-94 batch-135 = 1.2360978871583939e-06

Training epoch-94 batch-136
Running loss of epoch-94 batch-136 = 1.258915290236473e-06

Training epoch-94 batch-137
Running loss of epoch-94 batch-137 = 1.8980354070663452e-06

Training epoch-94 batch-138
Running loss of epoch-94 batch-138 = 1.3874378055334091e-06

Training epoch-94 batch-139
Running loss of epoch-94 batch-139 = 3.1765084713697433e-06

Training epoch-94 batch-140
Running loss of epoch-94 batch-140 = 1.097354106605053e-05

Training epoch-94 batch-141
Running loss of epoch-94 batch-141 = 7.130671292543411e-06

Training epoch-94 batch-142
Running loss of epoch-94 batch-142 = 1.9071158021688461e-06

Training epoch-94 batch-143
Running loss of epoch-94 batch-143 = 1.4470424503087997e-06

Training epoch-94 batch-144
Running loss of epoch-94 batch-144 = 4.436122253537178e-06

Training epoch-94 batch-145
Running loss of epoch-94 batch-145 = 3.6766286939382553e-06

Training epoch-94 batch-146
Running loss of epoch-94 batch-146 = 6.277346983551979e-06

Training epoch-94 batch-147
Running loss of epoch-94 batch-147 = 1.8300488591194153e-06

Training epoch-94 batch-148
Running loss of epoch-94 batch-148 = 3.5641714930534363e-06

Training epoch-94 batch-149
Running loss of epoch-94 batch-149 = 4.302011802792549e-06

Training epoch-94 batch-150
Running loss of epoch-94 batch-150 = 2.2493768483400345e-06

Training epoch-94 batch-151
Running loss of epoch-94 batch-151 = 4.419591277837753e-06

Training epoch-94 batch-152
Running loss of epoch-94 batch-152 = 3.0209776014089584e-06

Training epoch-94 batch-153
Running loss of epoch-94 batch-153 = 6.9462694227695465e-06

Training epoch-94 batch-154
Running loss of epoch-94 batch-154 = 8.246861398220062e-07

Training epoch-94 batch-155
Running loss of epoch-94 batch-155 = 2.7213245630264282e-06

Training epoch-94 batch-156
Running loss of epoch-94 batch-156 = 4.957662895321846e-06

Training epoch-94 batch-157
Running loss of epoch-94 batch-157 = 6.891787052154541e-06

Finished training epoch-94.



Average train loss at epoch-94 = 3.5657405853271484e-06

Started Evaluation

Average val loss at epoch-94 = 1.8558914668513706

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.36 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-95


Training epoch-95 batch-1
Running loss of epoch-95 batch-1 = 3.838213160634041e-06

Training epoch-95 batch-2
Running loss of epoch-95 batch-2 = 4.912726581096649e-06

Training epoch-95 batch-3
Running loss of epoch-95 batch-3 = 1.5057157725095749e-06

Training epoch-95 batch-4
Running loss of epoch-95 batch-4 = 2.6659108698368073e-06

Training epoch-95 batch-5
Running loss of epoch-95 batch-5 = 9.167706593871117e-06

Training epoch-95 batch-6
Running loss of epoch-95 batch-6 = 2.4724286049604416e-06

Training epoch-95 batch-7
Running loss of epoch-95 batch-7 = 2.2763852030038834e-06

Training epoch-95 batch-8
Running loss of epoch-95 batch-8 = 2.0363368093967438e-06

Training epoch-95 batch-9
Running loss of epoch-95 batch-9 = 4.880595952272415e-06

Training epoch-95 batch-10
Running loss of epoch-95 batch-10 = 3.443565219640732e-06

Training epoch-95 batch-11
Running loss of epoch-95 batch-11 = 2.4070031940937042e-06

Training epoch-95 batch-12
Running loss of epoch-95 batch-12 = 3.596534952521324e-06

Training epoch-95 batch-13
Running loss of epoch-95 batch-13 = 4.971399903297424e-06

Training epoch-95 batch-14
Running loss of epoch-95 batch-14 = 1.9688159227371216e-06

Training epoch-95 batch-15
Running loss of epoch-95 batch-15 = 4.606321454048157e-06

Training epoch-95 batch-16
Running loss of epoch-95 batch-16 = 4.180474206805229e-06

Training epoch-95 batch-17
Running loss of epoch-95 batch-17 = 1.983949914574623e-06

Training epoch-95 batch-18
Running loss of epoch-95 batch-18 = 8.269911631941795e-06

Training epoch-95 batch-19
Running loss of epoch-95 batch-19 = 1.7655547708272934e-06

Training epoch-95 batch-20
Running loss of epoch-95 batch-20 = 7.669441401958466e-06

Training epoch-95 batch-21
Running loss of epoch-95 batch-21 = 8.349306881427765e-07

Training epoch-95 batch-22
Running loss of epoch-95 batch-22 = 3.971392288804054e-06

Training epoch-95 batch-23
Running loss of epoch-95 batch-23 = 1.8400605767965317e-06

Training epoch-95 batch-24
Running loss of epoch-95 batch-24 = 2.477318048477173e-06

Training epoch-95 batch-25
Running loss of epoch-95 batch-25 = 3.2298266887664795e-06

Training epoch-95 batch-26
Running loss of epoch-95 batch-26 = 1.1899974197149277e-06

Training epoch-95 batch-27
Running loss of epoch-95 batch-27 = 1.38930045068264e-06

Training epoch-95 batch-28
Running loss of epoch-95 batch-28 = 2.8016511350870132e-06

Training epoch-95 batch-29
Running loss of epoch-95 batch-29 = 2.2924505174160004e-06

Training epoch-95 batch-30
Running loss of epoch-95 batch-30 = 5.795853212475777e-06

Training epoch-95 batch-31
Running loss of epoch-95 batch-31 = 1.473352313041687e-06

Training epoch-95 batch-32
Running loss of epoch-95 batch-32 = 3.2389070838689804e-06

Training epoch-95 batch-33
Running loss of epoch-95 batch-33 = 3.2624229788780212e-06

Training epoch-95 batch-34
Running loss of epoch-95 batch-34 = 1.5639234334230423e-06

Training epoch-95 batch-35
Running loss of epoch-95 batch-35 = 2.216314896941185e-06

Training epoch-95 batch-36
Running loss of epoch-95 batch-36 = 2.8279609978199005e-06

Training epoch-95 batch-37
Running loss of epoch-95 batch-37 = 3.984197974205017e-06

Training epoch-95 batch-38
Running loss of epoch-95 batch-38 = 3.148568794131279e-06

Training epoch-95 batch-39
Running loss of epoch-95 batch-39 = 2.2798776626586914e-06

Training epoch-95 batch-40
Running loss of epoch-95 batch-40 = 2.7467031031847e-06

Training epoch-95 batch-41
Running loss of epoch-95 batch-41 = 5.887588486075401e-06

Training epoch-95 batch-42
Running loss of epoch-95 batch-42 = 1.935986801981926e-06

Training epoch-95 batch-43
Running loss of epoch-95 batch-43 = 2.4174805730581284e-06

Training epoch-95 batch-44
Running loss of epoch-95 batch-44 = 2.2905878722667694e-06

Training epoch-95 batch-45
Running loss of epoch-95 batch-45 = 3.3332034945487976e-06

Training epoch-95 batch-46
Running loss of epoch-95 batch-46 = 6.254762411117554e-06

Training epoch-95 batch-47
Running loss of epoch-95 batch-47 = 2.5427434593439102e-06

Training epoch-95 batch-48
Running loss of epoch-95 batch-48 = 5.1853712648153305e-06

Training epoch-95 batch-49
Running loss of epoch-95 batch-49 = 2.441229298710823e-06

Training epoch-95 batch-50
Running loss of epoch-95 batch-50 = 2.5033950805664062e-06

Training epoch-95 batch-51
Running loss of epoch-95 batch-51 = 5.925539880990982e-06

Training epoch-95 batch-52
Running loss of epoch-95 batch-52 = 2.291519194841385e-06

Training epoch-95 batch-53
Running loss of epoch-95 batch-53 = 1.9515864551067352e-06

Training epoch-95 batch-54
Running loss of epoch-95 batch-54 = 5.5728014558553696e-06

Training epoch-95 batch-55
Running loss of epoch-95 batch-55 = 4.825182259082794e-06

Training epoch-95 batch-56
Running loss of epoch-95 batch-56 = 7.263617590069771e-06

Training epoch-95 batch-57
Running loss of epoch-95 batch-57 = 3.2973475754261017e-06

Training epoch-95 batch-58
Running loss of epoch-95 batch-58 = 2.411194145679474e-06

Training epoch-95 batch-59
Running loss of epoch-95 batch-59 = 2.2919848561286926e-06

Training epoch-95 batch-60
Running loss of epoch-95 batch-60 = 4.610046744346619e-06

Training epoch-95 batch-61
Running loss of epoch-95 batch-61 = 1.6600824892520905e-06

Training epoch-95 batch-62
Running loss of epoch-95 batch-62 = 2.871965989470482e-06

Training epoch-95 batch-63
Running loss of epoch-95 batch-63 = 1.991633325815201e-06

Training epoch-95 batch-64
Running loss of epoch-95 batch-64 = 3.4365803003311157e-06

Training epoch-95 batch-65
Running loss of epoch-95 batch-65 = 5.012843757867813e-06

Training epoch-95 batch-66
Running loss of epoch-95 batch-66 = 5.382578819990158e-06

Training epoch-95 batch-67
Running loss of epoch-95 batch-67 = 3.1581148505210876e-06

Training epoch-95 batch-68
Running loss of epoch-95 batch-68 = 2.784421667456627e-06

Training epoch-95 batch-69
Running loss of epoch-95 batch-69 = 5.325302481651306e-06

Training epoch-95 batch-70
Running loss of epoch-95 batch-70 = 1.1599622666835785e-06

Training epoch-95 batch-71
Running loss of epoch-95 batch-71 = 1.2586824595928192e-06

Training epoch-95 batch-72
Running loss of epoch-95 batch-72 = 2.2526364773511887e-06

Training epoch-95 batch-73
Running loss of epoch-95 batch-73 = 3.248453140258789e-06

Training epoch-95 batch-74
Running loss of epoch-95 batch-74 = 4.957197234034538e-06

Training epoch-95 batch-75
Running loss of epoch-95 batch-75 = 3.5406555980443954e-06

Training epoch-95 batch-76
Running loss of epoch-95 batch-76 = 2.5241170078516006e-06

Training epoch-95 batch-77
Running loss of epoch-95 batch-77 = 3.3120159059762955e-06

Training epoch-95 batch-78
Running loss of epoch-95 batch-78 = 5.742767825722694e-06

Training epoch-95 batch-79
Running loss of epoch-95 batch-79 = 1.453561708331108e-06

Training epoch-95 batch-80
Running loss of epoch-95 batch-80 = 2.9243528842926025e-06

Training epoch-95 batch-81
Running loss of epoch-95 batch-81 = 3.312947228550911e-06

Training epoch-95 batch-82
Running loss of epoch-95 batch-82 = 3.1944364309310913e-06

Training epoch-95 batch-83
Running loss of epoch-95 batch-83 = 2.3923348635435104e-06

Training epoch-95 batch-84
Running loss of epoch-95 batch-84 = 2.1725427359342575e-06

Training epoch-95 batch-85
Running loss of epoch-95 batch-85 = 5.740206688642502e-06

Training epoch-95 batch-86
Running loss of epoch-95 batch-86 = 5.480367690324783e-06

Training epoch-95 batch-87
Running loss of epoch-95 batch-87 = 2.605840563774109e-06

Training epoch-95 batch-88
Running loss of epoch-95 batch-88 = 1.0157469660043716e-05

Training epoch-95 batch-89
Running loss of epoch-95 batch-89 = 5.099456757307053e-06

Training epoch-95 batch-90
Running loss of epoch-95 batch-90 = 2.889428287744522e-06

Training epoch-95 batch-91
Running loss of epoch-95 batch-91 = 1.8479768186807632e-06

Training epoch-95 batch-92
Running loss of epoch-95 batch-92 = 6.198883056640625e-06

Training epoch-95 batch-93
Running loss of epoch-95 batch-93 = 2.357643097639084e-06

Training epoch-95 batch-94
Running loss of epoch-95 batch-94 = 2.2884923964738846e-06

Training epoch-95 batch-95
Running loss of epoch-95 batch-95 = 3.384659066796303e-06

Training epoch-95 batch-96
Running loss of epoch-95 batch-96 = 9.51811671257019e-07

Training epoch-95 batch-97
Running loss of epoch-95 batch-97 = 2.1974556148052216e-06

Training epoch-95 batch-98
Running loss of epoch-95 batch-98 = 9.154900908470154e-07

Training epoch-95 batch-99
Running loss of epoch-95 batch-99 = 1.5974510461091995e-06

Training epoch-95 batch-100
Running loss of epoch-95 batch-100 = 2.175569534301758e-06

Training epoch-95 batch-101
Running loss of epoch-95 batch-101 = 3.1294766813516617e-06

Training epoch-95 batch-102
Running loss of epoch-95 batch-102 = 5.874317139387131e-06

Training epoch-95 batch-103
Running loss of epoch-95 batch-103 = 2.7245841920375824e-06

Training epoch-95 batch-104
Running loss of epoch-95 batch-104 = 1.380685716867447e-06

Training epoch-95 batch-105
Running loss of epoch-95 batch-105 = 2.7534551918506622e-06

Training epoch-95 batch-106
Running loss of epoch-95 batch-106 = 3.3332034945487976e-06

Training epoch-95 batch-107
Running loss of epoch-95 batch-107 = 1.8542632460594177e-06

Training epoch-95 batch-108
Running loss of epoch-95 batch-108 = 3.166496753692627e-06

Training epoch-95 batch-109
Running loss of epoch-95 batch-109 = 3.6370474845170975e-06

Training epoch-95 batch-110
Running loss of epoch-95 batch-110 = 5.9390440583229065e-06

Training epoch-95 batch-111
Running loss of epoch-95 batch-111 = 4.0102750062942505e-06

Training epoch-95 batch-112
Running loss of epoch-95 batch-112 = 1.1981464922428131e-05

Training epoch-95 batch-113
Running loss of epoch-95 batch-113 = 4.861271008849144e-06

Training epoch-95 batch-114
Running loss of epoch-95 batch-114 = 4.4943299144506454e-06

Training epoch-95 batch-115
Running loss of epoch-95 batch-115 = 2.8777867555618286e-06

Training epoch-95 batch-116
Running loss of epoch-95 batch-116 = 4.241010174155235e-06

Training epoch-95 batch-117
Running loss of epoch-95 batch-117 = 2.9427465051412582e-06

Training epoch-95 batch-118
Running loss of epoch-95 batch-118 = 4.5727938413619995e-06

Training epoch-95 batch-119
Running loss of epoch-95 batch-119 = 6.029382348060608e-06

Training epoch-95 batch-120
Running loss of epoch-95 batch-120 = 2.916203811764717e-06

Training epoch-95 batch-121
Running loss of epoch-95 batch-121 = 1.0502059012651443e-05

Training epoch-95 batch-122
Running loss of epoch-95 batch-122 = 2.1292362362146378e-06

Training epoch-95 batch-123
Running loss of epoch-95 batch-123 = 6.88992440700531e-06

Training epoch-95 batch-124
Running loss of epoch-95 batch-124 = 3.973022103309631e-06

Training epoch-95 batch-125
Running loss of epoch-95 batch-125 = 3.3371616154909134e-06

Training epoch-95 batch-126
Running loss of epoch-95 batch-126 = 1.00000761449337e-06

Training epoch-95 batch-127
Running loss of epoch-95 batch-127 = 3.285706043243408e-06

Training epoch-95 batch-128
Running loss of epoch-95 batch-128 = 3.125052899122238e-06

Training epoch-95 batch-129
Running loss of epoch-95 batch-129 = 1.9059516489505768e-06

Training epoch-95 batch-130
Running loss of epoch-95 batch-130 = 4.392582923173904e-06

Training epoch-95 batch-131
Running loss of epoch-95 batch-131 = 1.6828998923301697e-06

Training epoch-95 batch-132
Running loss of epoch-95 batch-132 = 1.3776589184999466e-06

Training epoch-95 batch-133
Running loss of epoch-95 batch-133 = 3.4964177757501602e-06

Training epoch-95 batch-134
Running loss of epoch-95 batch-134 = 4.9835070967674255e-06

Training epoch-95 batch-135
Running loss of epoch-95 batch-135 = 6.752787157893181e-06

Training epoch-95 batch-136
Running loss of epoch-95 batch-136 = 2.594897523522377e-06

Training epoch-95 batch-137
Running loss of epoch-95 batch-137 = 3.898283466696739e-06

Training epoch-95 batch-138
Running loss of epoch-95 batch-138 = 4.002591595053673e-06

Training epoch-95 batch-139
Running loss of epoch-95 batch-139 = 2.2957101464271545e-06

Training epoch-95 batch-140
Running loss of epoch-95 batch-140 = 4.1690655052661896e-06

Training epoch-95 batch-141
Running loss of epoch-95 batch-141 = 2.6498455554246902e-06

Training epoch-95 batch-142
Running loss of epoch-95 batch-142 = 1.4738179743289948e-06

Training epoch-95 batch-143
Running loss of epoch-95 batch-143 = 3.419816493988037e-06

Training epoch-95 batch-144
Running loss of epoch-95 batch-144 = 2.1418090909719467e-06

Training epoch-95 batch-145
Running loss of epoch-95 batch-145 = 2.8249341994524e-06

Training epoch-95 batch-146
Running loss of epoch-95 batch-146 = 2.759043127298355e-06

Training epoch-95 batch-147
Running loss of epoch-95 batch-147 = 2.014217898249626e-06

Training epoch-95 batch-148
Running loss of epoch-95 batch-148 = 3.4635886549949646e-06

Training epoch-95 batch-149
Running loss of epoch-95 batch-149 = 5.490146577358246e-07

Training epoch-95 batch-150
Running loss of epoch-95 batch-150 = 1.2866221368312836e-06

Training epoch-95 batch-151
Running loss of epoch-95 batch-151 = 2.7224887162446976e-06

Training epoch-95 batch-152
Running loss of epoch-95 batch-152 = 3.5781413316726685e-06

Training epoch-95 batch-153
Running loss of epoch-95 batch-153 = 5.898997187614441e-06

Training epoch-95 batch-154
Running loss of epoch-95 batch-154 = 2.755783498287201e-06

Training epoch-95 batch-155
Running loss of epoch-95 batch-155 = 7.593072950839996e-06

Training epoch-95 batch-156
Running loss of epoch-95 batch-156 = 4.805391654372215e-06

Training epoch-95 batch-157
Running loss of epoch-95 batch-157 = 4.328787326812744e-06

Finished training epoch-95.



Average train loss at epoch-95 = 3.500215709209442e-06

Started Evaluation

Average val loss at epoch-95 = 1.8544506577430626

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.18 %

Finished Evaluation



Started training epoch-96


Training epoch-96 batch-1
Running loss of epoch-96 batch-1 = 8.302740752696991e-07

Training epoch-96 batch-2
Running loss of epoch-96 batch-2 = 2.8847716748714447e-06

Training epoch-96 batch-3
Running loss of epoch-96 batch-3 = 3.1669624149799347e-06

Training epoch-96 batch-4
Running loss of epoch-96 batch-4 = 8.043600246310234e-06

Training epoch-96 batch-5
Running loss of epoch-96 batch-5 = 4.358822479844093e-06

Training epoch-96 batch-6
Running loss of epoch-96 batch-6 = 9.960494935512543e-07

Training epoch-96 batch-7
Running loss of epoch-96 batch-7 = 3.6705750972032547e-06

Training epoch-96 batch-8
Running loss of epoch-96 batch-8 = 1.444714143872261e-06

Training epoch-96 batch-9
Running loss of epoch-96 batch-9 = 2.764631062746048e-06

Training epoch-96 batch-10
Running loss of epoch-96 batch-10 = 9.533716365695e-06

Training epoch-96 batch-11
Running loss of epoch-96 batch-11 = 1.862645149230957e-06

Training epoch-96 batch-12
Running loss of epoch-96 batch-12 = 5.845213308930397e-06

Training epoch-96 batch-13
Running loss of epoch-96 batch-13 = 9.126029908657074e-06

Training epoch-96 batch-14
Running loss of epoch-96 batch-14 = 2.68593430519104e-06

Training epoch-96 batch-15
Running loss of epoch-96 batch-15 = 2.498505637049675e-06

Training epoch-96 batch-16
Running loss of epoch-96 batch-16 = 1.783948391675949e-06

Training epoch-96 batch-17
Running loss of epoch-96 batch-17 = 4.256144165992737e-06

Training epoch-96 batch-18
Running loss of epoch-96 batch-18 = 2.814223989844322e-06

Training epoch-96 batch-19
Running loss of epoch-96 batch-19 = 5.63659705221653e-06

Training epoch-96 batch-20
Running loss of epoch-96 batch-20 = 1.2344680726528168e-06

Training epoch-96 batch-21
Running loss of epoch-96 batch-21 = 1.7667189240455627e-06

Training epoch-96 batch-22
Running loss of epoch-96 batch-22 = 6.8035442382097244e-06

Training epoch-96 batch-23
Running loss of epoch-96 batch-23 = 1.887558028101921e-06

Training epoch-96 batch-24
Running loss of epoch-96 batch-24 = 4.838686436414719e-06

Training epoch-96 batch-25
Running loss of epoch-96 batch-25 = 3.1604431569576263e-06

Training epoch-96 batch-26
Running loss of epoch-96 batch-26 = 3.1192321330308914e-06

Training epoch-96 batch-27
Running loss of epoch-96 batch-27 = 1.1548399925231934e-06

Training epoch-96 batch-28
Running loss of epoch-96 batch-28 = 3.058928996324539e-06

Training epoch-96 batch-29
Running loss of epoch-96 batch-29 = 4.2282044887542725e-06

Training epoch-96 batch-30
Running loss of epoch-96 batch-30 = 3.2724346965551376e-06

Training epoch-96 batch-31
Running loss of epoch-96 batch-31 = 1.1399388313293457e-06

Training epoch-96 batch-32
Running loss of epoch-96 batch-32 = 3.1418167054653168e-06

Training epoch-96 batch-33
Running loss of epoch-96 batch-33 = 2.3567117750644684e-06

Training epoch-96 batch-34
Running loss of epoch-96 batch-34 = 8.901813998818398e-06

Training epoch-96 batch-35
Running loss of epoch-96 batch-35 = 4.60352748632431e-06

Training epoch-96 batch-36
Running loss of epoch-96 batch-36 = 3.2954849302768707e-06

Training epoch-96 batch-37
Running loss of epoch-96 batch-37 = 2.5979243218898773e-06

Training epoch-96 batch-38
Running loss of epoch-96 batch-38 = 4.1371677070856094e-06

Training epoch-96 batch-39
Running loss of epoch-96 batch-39 = 2.112472429871559e-06

Training epoch-96 batch-40
Running loss of epoch-96 batch-40 = 5.333218723535538e-06

Training epoch-96 batch-41
Running loss of epoch-96 batch-41 = 2.7508940547704697e-06

Training epoch-96 batch-42
Running loss of epoch-96 batch-42 = 1.2919772416353226e-06

Training epoch-96 batch-43
Running loss of epoch-96 batch-43 = 4.207482561469078e-06

Training epoch-96 batch-44
Running loss of epoch-96 batch-44 = 2.1329615265130997e-06

Training epoch-96 batch-45
Running loss of epoch-96 batch-45 = 9.6810981631279e-07

Training epoch-96 batch-46
Running loss of epoch-96 batch-46 = 5.773967131972313e-06

Training epoch-96 batch-47
Running loss of epoch-96 batch-47 = 2.580462023615837e-06

Training epoch-96 batch-48
Running loss of epoch-96 batch-48 = 1.041218638420105e-06

Training epoch-96 batch-49
Running loss of epoch-96 batch-49 = 2.827262505888939e-06

Training epoch-96 batch-50
Running loss of epoch-96 batch-50 = 1.8207356333732605e-06

Training epoch-96 batch-51
Running loss of epoch-96 batch-51 = 3.1939707696437836e-06

Training epoch-96 batch-52
Running loss of epoch-96 batch-52 = 1.6763806343078613e-06

Training epoch-96 batch-53
Running loss of epoch-96 batch-53 = 3.1348317861557007e-06

Training epoch-96 batch-54
Running loss of epoch-96 batch-54 = 3.081047907471657e-06

Training epoch-96 batch-55
Running loss of epoch-96 batch-55 = 7.548369467258453e-07

Training epoch-96 batch-56
Running loss of epoch-96 batch-56 = 2.4158507585525513e-06

Training epoch-96 batch-57
Running loss of epoch-96 batch-57 = 2.007000148296356e-06

Training epoch-96 batch-58
Running loss of epoch-96 batch-58 = 2.509215846657753e-06

Training epoch-96 batch-59
Running loss of epoch-96 batch-59 = 4.709232598543167e-06

Training epoch-96 batch-60
Running loss of epoch-96 batch-60 = 2.5567132979631424e-06

Training epoch-96 batch-61
Running loss of epoch-96 batch-61 = 2.92179174721241e-06

Training epoch-96 batch-62
Running loss of epoch-96 batch-62 = 1.2961681932210922e-06

Training epoch-96 batch-63
Running loss of epoch-96 batch-63 = 3.419816493988037e-06

Training epoch-96 batch-64
Running loss of epoch-96 batch-64 = 2.102227881550789e-06

Training epoch-96 batch-65
Running loss of epoch-96 batch-65 = 3.0957162380218506e-06

Training epoch-96 batch-66
Running loss of epoch-96 batch-66 = 4.514353349804878e-06

Training epoch-96 batch-67
Running loss of epoch-96 batch-67 = 3.6908313632011414e-06

Training epoch-96 batch-68
Running loss of epoch-96 batch-68 = 2.2167805582284927e-06

Training epoch-96 batch-69
Running loss of epoch-96 batch-69 = 3.985827788710594e-06

Training epoch-96 batch-70
Running loss of epoch-96 batch-70 = 4.33693639934063e-06

Training epoch-96 batch-71
Running loss of epoch-96 batch-71 = 3.7336722016334534e-06

Training epoch-96 batch-72
Running loss of epoch-96 batch-72 = 1.4898832887411118e-06

Training epoch-96 batch-73
Running loss of epoch-96 batch-73 = 1.6808044165372849e-06

Training epoch-96 batch-74
Running loss of epoch-96 batch-74 = 5.322042852640152e-06

Training epoch-96 batch-75
Running loss of epoch-96 batch-75 = 2.3243483155965805e-06

Training epoch-96 batch-76
Running loss of epoch-96 batch-76 = 1.6638077795505524e-06

Training epoch-96 batch-77
Running loss of epoch-96 batch-77 = 4.067784175276756e-06

Training epoch-96 batch-78
Running loss of epoch-96 batch-78 = 1.8747523427009583e-06

Training epoch-96 batch-79
Running loss of epoch-96 batch-79 = 4.842178896069527e-06

Training epoch-96 batch-80
Running loss of epoch-96 batch-80 = 2.908986061811447e-06

Training epoch-96 batch-81
Running loss of epoch-96 batch-81 = 2.6274938136339188e-06

Training epoch-96 batch-82
Running loss of epoch-96 batch-82 = 2.364395186305046e-06

Training epoch-96 batch-83
Running loss of epoch-96 batch-83 = 1.4004763215780258e-06

Training epoch-96 batch-84
Running loss of epoch-96 batch-84 = 1.293141394853592e-06

Training epoch-96 batch-85
Running loss of epoch-96 batch-85 = 2.1669548004865646e-06

Training epoch-96 batch-86
Running loss of epoch-96 batch-86 = 2.9816292226314545e-06

Training epoch-96 batch-87
Running loss of epoch-96 batch-87 = 2.7583446353673935e-06

Training epoch-96 batch-88
Running loss of epoch-96 batch-88 = 1.3317912817001343e-06

Training epoch-96 batch-89
Running loss of epoch-96 batch-89 = 7.117632776498795e-07

Training epoch-96 batch-90
Running loss of epoch-96 batch-90 = 4.023779183626175e-06

Training epoch-96 batch-91
Running loss of epoch-96 batch-91 = 3.455905243754387e-06

Training epoch-96 batch-92
Running loss of epoch-96 batch-92 = 2.7078203856945038e-06

Training epoch-96 batch-93
Running loss of epoch-96 batch-93 = 4.803761839866638e-06

Training epoch-96 batch-94
Running loss of epoch-96 batch-94 = 3.305729478597641e-06

Training epoch-96 batch-95
Running loss of epoch-96 batch-95 = 9.480863809585571e-07

Training epoch-96 batch-96
Running loss of epoch-96 batch-96 = 2.2293534129858017e-06

Training epoch-96 batch-97
Running loss of epoch-96 batch-97 = 2.208864316344261e-06

Training epoch-96 batch-98
Running loss of epoch-96 batch-98 = 9.909970685839653e-06

Training epoch-96 batch-99
Running loss of epoch-96 batch-99 = 1.7408747225999832e-06

Training epoch-96 batch-100
Running loss of epoch-96 batch-100 = 3.0922237783670425e-06

Training epoch-96 batch-101
Running loss of epoch-96 batch-101 = 3.537628799676895e-06

Training epoch-96 batch-102
Running loss of epoch-96 batch-102 = 7.595401257276535e-06

Training epoch-96 batch-103
Running loss of epoch-96 batch-103 = 3.559049218893051e-06

Training epoch-96 batch-104
Running loss of epoch-96 batch-104 = 5.778158083558083e-06

Training epoch-96 batch-105
Running loss of epoch-96 batch-105 = 3.8016587495803833e-06

Training epoch-96 batch-106
Running loss of epoch-96 batch-106 = 3.945082426071167e-06

Training epoch-96 batch-107
Running loss of epoch-96 batch-107 = 4.673609510064125e-06

Training epoch-96 batch-108
Running loss of epoch-96 batch-108 = 3.1639356166124344e-06

Training epoch-96 batch-109
Running loss of epoch-96 batch-109 = 7.3497649282217026e-06

Training epoch-96 batch-110
Running loss of epoch-96 batch-110 = 2.139713615179062e-06

Training epoch-96 batch-111
Running loss of epoch-96 batch-111 = 6.077811121940613e-06

Training epoch-96 batch-112
Running loss of epoch-96 batch-112 = 2.5050248950719833e-06

Training epoch-96 batch-113
Running loss of epoch-96 batch-113 = 5.326233804225922e-06

Training epoch-96 batch-114
Running loss of epoch-96 batch-114 = 4.916219040751457e-06

Training epoch-96 batch-115
Running loss of epoch-96 batch-115 = 1.0677613317966461e-05

Training epoch-96 batch-116
Running loss of epoch-96 batch-116 = 4.590023308992386e-06

Training epoch-96 batch-117
Running loss of epoch-96 batch-117 = 6.613088771700859e-06

Training epoch-96 batch-118
Running loss of epoch-96 batch-118 = 3.768131136894226e-06

Training epoch-96 batch-119
Running loss of epoch-96 batch-119 = 1.2281816452741623e-06

Training epoch-96 batch-120
Running loss of epoch-96 batch-120 = 4.285015165805817e-06

Training epoch-96 batch-121
Running loss of epoch-96 batch-121 = 2.368353307247162e-06

Training epoch-96 batch-122
Running loss of epoch-96 batch-122 = 3.416324034333229e-06

Training epoch-96 batch-123
Running loss of epoch-96 batch-123 = 2.0617153495550156e-06

Training epoch-96 batch-124
Running loss of epoch-96 batch-124 = 2.9567163437604904e-06

Training epoch-96 batch-125
Running loss of epoch-96 batch-125 = 2.634013071656227e-06

Training epoch-96 batch-126
Running loss of epoch-96 batch-126 = 3.0167866498231888e-06

Training epoch-96 batch-127
Running loss of epoch-96 batch-127 = 1.3019191101193428e-05

Training epoch-96 batch-128
Running loss of epoch-96 batch-128 = 3.637978807091713e-06

Training epoch-96 batch-129
Running loss of epoch-96 batch-129 = 2.552056685090065e-06

Training epoch-96 batch-130
Running loss of epoch-96 batch-130 = 2.922024577856064e-06

Training epoch-96 batch-131
Running loss of epoch-96 batch-131 = 1.0433373972773552e-05

Training epoch-96 batch-132
Running loss of epoch-96 batch-132 = 2.0638108253479004e-06

Training epoch-96 batch-133
Running loss of epoch-96 batch-133 = 2.7422793209552765e-06

Training epoch-96 batch-134
Running loss of epoch-96 batch-134 = 3.2568350434303284e-06

Training epoch-96 batch-135
Running loss of epoch-96 batch-135 = 1.7061829566955566e-06

Training epoch-96 batch-136
Running loss of epoch-96 batch-136 = 2.9997900128364563e-06

Training epoch-96 batch-137
Running loss of epoch-96 batch-137 = 1.9315630197525024e-06

Training epoch-96 batch-138
Running loss of epoch-96 batch-138 = 5.029374733567238e-06

Training epoch-96 batch-139
Running loss of epoch-96 batch-139 = 1.4889519661664963e-06

Training epoch-96 batch-140
Running loss of epoch-96 batch-140 = 2.616317942738533e-06

Training epoch-96 batch-141
Running loss of epoch-96 batch-141 = 1.6640406101942062e-06

Training epoch-96 batch-142
Running loss of epoch-96 batch-142 = 7.718568667769432e-06

Training epoch-96 batch-143
Running loss of epoch-96 batch-143 = 1.7255079001188278e-06

Training epoch-96 batch-144
Running loss of epoch-96 batch-144 = 1.3415701687335968e-06

Training epoch-96 batch-145
Running loss of epoch-96 batch-145 = 4.73950058221817e-06

Training epoch-96 batch-146
Running loss of epoch-96 batch-146 = 2.2819731384515762e-06

Training epoch-96 batch-147
Running loss of epoch-96 batch-147 = 3.0638184398412704e-06

Training epoch-96 batch-148
Running loss of epoch-96 batch-148 = 2.8333161026239395e-06

Training epoch-96 batch-149
Running loss of epoch-96 batch-149 = 1.725275069475174e-06

Training epoch-96 batch-150
Running loss of epoch-96 batch-150 = 3.880355507135391e-06

Training epoch-96 batch-151
Running loss of epoch-96 batch-151 = 2.673827111721039e-06

Training epoch-96 batch-152
Running loss of epoch-96 batch-152 = 8.101342245936394e-06

Training epoch-96 batch-153
Running loss of epoch-96 batch-153 = 3.5178381949663162e-06

Training epoch-96 batch-154
Running loss of epoch-96 batch-154 = 8.486676961183548e-07

Training epoch-96 batch-155
Running loss of epoch-96 batch-155 = 2.661021426320076e-06

Training epoch-96 batch-156
Running loss of epoch-96 batch-156 = 5.312962457537651e-06

Training epoch-96 batch-157
Running loss of epoch-96 batch-157 = 1.1555850505828857e-05

Finished training epoch-96.



Average train loss at epoch-96 = 3.4604623913764953e-06

Started Evaluation

Average val loss at epoch-96 = 1.8638404191372435

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.20 %

Finished Evaluation



Started training epoch-97


Training epoch-97 batch-1
Running loss of epoch-97 batch-1 = 1.1778902262449265e-06

Training epoch-97 batch-2
Running loss of epoch-97 batch-2 = 2.287793904542923e-06

Training epoch-97 batch-3
Running loss of epoch-97 batch-3 = 5.7765282690525055e-06

Training epoch-97 batch-4
Running loss of epoch-97 batch-4 = 2.6489142328500748e-06

Training epoch-97 batch-5
Running loss of epoch-97 batch-5 = 8.90856608748436e-06

Training epoch-97 batch-6
Running loss of epoch-97 batch-6 = 7.312046363949776e-06

Training epoch-97 batch-7
Running loss of epoch-97 batch-7 = 3.3173710107803345e-06

Training epoch-97 batch-8
Running loss of epoch-97 batch-8 = 4.141591489315033e-06

Training epoch-97 batch-9
Running loss of epoch-97 batch-9 = 1.0373536497354507e-05

Training epoch-97 batch-10
Running loss of epoch-97 batch-10 = 2.316199243068695e-06

Training epoch-97 batch-11
Running loss of epoch-97 batch-11 = 1.0828953236341476e-06

Training epoch-97 batch-12
Running loss of epoch-97 batch-12 = 3.762776032090187e-06

Training epoch-97 batch-13
Running loss of epoch-97 batch-13 = 1.1031515896320343e-06

Training epoch-97 batch-14
Running loss of epoch-97 batch-14 = 6.006332114338875e-06

Training epoch-97 batch-15
Running loss of epoch-97 batch-15 = 4.055444151163101e-06

Training epoch-97 batch-16
Running loss of epoch-97 batch-16 = 2.214685082435608e-06

Training epoch-97 batch-17
Running loss of epoch-97 batch-17 = 2.7245841920375824e-06

Training epoch-97 batch-18
Running loss of epoch-97 batch-18 = 1.9567087292671204e-06

Training epoch-97 batch-19
Running loss of epoch-97 batch-19 = 2.4742912501096725e-06

Training epoch-97 batch-20
Running loss of epoch-97 batch-20 = 3.2009556889533997e-06

Training epoch-97 batch-21
Running loss of epoch-97 batch-21 = 2.8603244572877884e-06

Training epoch-97 batch-22
Running loss of epoch-97 batch-22 = 1.2365635484457016e-06

Training epoch-97 batch-23
Running loss of epoch-97 batch-23 = 3.1222589313983917e-06

Training epoch-97 batch-24
Running loss of epoch-97 batch-24 = 5.691312253475189e-06

Training epoch-97 batch-25
Running loss of epoch-97 batch-25 = 4.073604941368103e-06

Training epoch-97 batch-26
Running loss of epoch-97 batch-26 = 2.0514708012342453e-06

Training epoch-97 batch-27
Running loss of epoch-97 batch-27 = 5.201669409871101e-06

Training epoch-97 batch-28
Running loss of epoch-97 batch-28 = 2.1317973732948303e-06

Training epoch-97 batch-29
Running loss of epoch-97 batch-29 = 1.6677659004926682e-06

Training epoch-97 batch-30
Running loss of epoch-97 batch-30 = 3.1588133424520493e-06

Training epoch-97 batch-31
Running loss of epoch-97 batch-31 = 1.6081612557172775e-06

Training epoch-97 batch-32
Running loss of epoch-97 batch-32 = 2.6605557650327682e-06

Training epoch-97 batch-33
Running loss of epoch-97 batch-33 = 1.0605435818433762e-05

Training epoch-97 batch-34
Running loss of epoch-97 batch-34 = 7.409835234284401e-06

Training epoch-97 batch-35
Running loss of epoch-97 batch-35 = 3.0549708753824234e-06

Training epoch-97 batch-36
Running loss of epoch-97 batch-36 = 2.0782463252544403e-06

Training epoch-97 batch-37
Running loss of epoch-97 batch-37 = 1.2584496289491653e-06

Training epoch-97 batch-38
Running loss of epoch-97 batch-38 = 6.71902671456337e-06

Training epoch-97 batch-39
Running loss of epoch-97 batch-39 = 1.3026874512434006e-06

Training epoch-97 batch-40
Running loss of epoch-97 batch-40 = 1.682201400399208e-06

Training epoch-97 batch-41
Running loss of epoch-97 batch-41 = 1.010228879749775e-05

Training epoch-97 batch-42
Running loss of epoch-97 batch-42 = 4.676170647144318e-06

Training epoch-97 batch-43
Running loss of epoch-97 batch-43 = 5.716923624277115e-06

Training epoch-97 batch-44
Running loss of epoch-97 batch-44 = 3.830296918749809e-06

Training epoch-97 batch-45
Running loss of epoch-97 batch-45 = 2.1832529455423355e-06

Training epoch-97 batch-46
Running loss of epoch-97 batch-46 = 2.292916178703308e-06

Training epoch-97 batch-47
Running loss of epoch-97 batch-47 = 2.3210886865854263e-06

Training epoch-97 batch-48
Running loss of epoch-97 batch-48 = 1.5434343367815018e-06

Training epoch-97 batch-49
Running loss of epoch-97 batch-49 = 2.112938091158867e-06

Training epoch-97 batch-50
Running loss of epoch-97 batch-50 = 3.812834620475769e-06

Training epoch-97 batch-51
Running loss of epoch-97 batch-51 = 1.8337741494178772e-06

Training epoch-97 batch-52
Running loss of epoch-97 batch-52 = 4.05777245759964e-06

Training epoch-97 batch-53
Running loss of epoch-97 batch-53 = 4.081288352608681e-06

Training epoch-97 batch-54
Running loss of epoch-97 batch-54 = 3.3194664865732193e-06

Training epoch-97 batch-55
Running loss of epoch-97 batch-55 = 5.577225238084793e-06

Training epoch-97 batch-56
Running loss of epoch-97 batch-56 = 2.5208573788404465e-06

Training epoch-97 batch-57
Running loss of epoch-97 batch-57 = 2.4863984435796738e-06

Training epoch-97 batch-58
Running loss of epoch-97 batch-58 = 1.4398247003555298e-06

Training epoch-97 batch-59
Running loss of epoch-97 batch-59 = 3.130640834569931e-06

Training epoch-97 batch-60
Running loss of epoch-97 batch-60 = 3.7546269595623016e-06

Training epoch-97 batch-61
Running loss of epoch-97 batch-61 = 9.933719411492348e-06

Training epoch-97 batch-62
Running loss of epoch-97 batch-62 = 2.057058736681938e-06

Training epoch-97 batch-63
Running loss of epoch-97 batch-63 = 1.362524926662445e-06

Training epoch-97 batch-64
Running loss of epoch-97 batch-64 = 1.5792902559041977e-06

Training epoch-97 batch-65
Running loss of epoch-97 batch-65 = 3.001885488629341e-06

Training epoch-97 batch-66
Running loss of epoch-97 batch-66 = 1.5827827155590057e-06

Training epoch-97 batch-67
Running loss of epoch-97 batch-67 = 2.158805727958679e-06

Training epoch-97 batch-68
Running loss of epoch-97 batch-68 = 2.9960647225379944e-06

Training epoch-97 batch-69
Running loss of epoch-97 batch-69 = 5.927635356783867e-06

Training epoch-97 batch-70
Running loss of epoch-97 batch-70 = 2.1792948246002197e-06

Training epoch-97 batch-71
Running loss of epoch-97 batch-71 = 1.7527490854263306e-06

Training epoch-97 batch-72
Running loss of epoch-97 batch-72 = 4.156026989221573e-06

Training epoch-97 batch-73
Running loss of epoch-97 batch-73 = 7.300404831767082e-06

Training epoch-97 batch-74
Running loss of epoch-97 batch-74 = 4.550907760858536e-06

Training epoch-97 batch-75
Running loss of epoch-97 batch-75 = 1.5131663531064987e-06

Training epoch-97 batch-76
Running loss of epoch-97 batch-76 = 4.275236278772354e-06

Training epoch-97 batch-77
Running loss of epoch-97 batch-77 = 4.099216312170029e-06

Training epoch-97 batch-78
Running loss of epoch-97 batch-78 = 2.77860090136528e-06

Training epoch-97 batch-79
Running loss of epoch-97 batch-79 = 2.9406510293483734e-06

Training epoch-97 batch-80
Running loss of epoch-97 batch-80 = 3.773719072341919e-06

Training epoch-97 batch-81
Running loss of epoch-97 batch-81 = 2.9995571821928024e-06

Training epoch-97 batch-82
Running loss of epoch-97 batch-82 = 1.1455267667770386e-06

Training epoch-97 batch-83
Running loss of epoch-97 batch-83 = 2.499902620911598e-06

Training epoch-97 batch-84
Running loss of epoch-97 batch-84 = 3.5134144127368927e-06

Training epoch-97 batch-85
Running loss of epoch-97 batch-85 = 4.904577508568764e-06

Training epoch-97 batch-86
Running loss of epoch-97 batch-86 = 1.4598481357097626e-06

Training epoch-97 batch-87
Running loss of epoch-97 batch-87 = 9.636860340833664e-06

Training epoch-97 batch-88
Running loss of epoch-97 batch-88 = 6.1194878071546555e-06

Training epoch-97 batch-89
Running loss of epoch-97 batch-89 = 3.5047996789216995e-06

Training epoch-97 batch-90
Running loss of epoch-97 batch-90 = 3.5567209124565125e-06

Training epoch-97 batch-91
Running loss of epoch-97 batch-91 = 2.8514768928289413e-06

Training epoch-97 batch-92
Running loss of epoch-97 batch-92 = 2.3096799850463867e-06

Training epoch-97 batch-93
Running loss of epoch-97 batch-93 = 2.932501956820488e-06

Training epoch-97 batch-94
Running loss of epoch-97 batch-94 = 2.6421621441841125e-06

Training epoch-97 batch-95
Running loss of epoch-97 batch-95 = 2.591172233223915e-06

Training epoch-97 batch-96
Running loss of epoch-97 batch-96 = 1.9138678908348083e-06

Training epoch-97 batch-97
Running loss of epoch-97 batch-97 = 3.3052638173103333e-06

Training epoch-97 batch-98
Running loss of epoch-97 batch-98 = 8.557457476854324e-06

Training epoch-97 batch-99
Running loss of epoch-97 batch-99 = 4.081986844539642e-06

Training epoch-97 batch-100
Running loss of epoch-97 batch-100 = 2.951826900243759e-06

Training epoch-97 batch-101
Running loss of epoch-97 batch-101 = 2.402346581220627e-06

Training epoch-97 batch-102
Running loss of epoch-97 batch-102 = 2.3616012185811996e-06

Training epoch-97 batch-103
Running loss of epoch-97 batch-103 = 4.6335626393556595e-06

Training epoch-97 batch-104
Running loss of epoch-97 batch-104 = 2.6081688702106476e-06

Training epoch-97 batch-105
Running loss of epoch-97 batch-105 = 2.793269231915474e-06

Training epoch-97 batch-106
Running loss of epoch-97 batch-106 = 4.07523475587368e-06

Training epoch-97 batch-107
Running loss of epoch-97 batch-107 = 1.5068799257278442e-06

Training epoch-97 batch-108
Running loss of epoch-97 batch-108 = 1.539476215839386e-06

Training epoch-97 batch-109
Running loss of epoch-97 batch-109 = 1.2514647096395493e-06

Training epoch-97 batch-110
Running loss of epoch-97 batch-110 = 2.535758540034294e-06

Training epoch-97 batch-111
Running loss of epoch-97 batch-111 = 3.4475233405828476e-06

Training epoch-97 batch-112
Running loss of epoch-97 batch-112 = 1.9257422536611557e-06

Training epoch-97 batch-113
Running loss of epoch-97 batch-113 = 2.384418621659279e-06

Training epoch-97 batch-114
Running loss of epoch-97 batch-114 = 1.260777935385704e-06

Training epoch-97 batch-115
Running loss of epoch-97 batch-115 = 1.8423888832330704e-06

Training epoch-97 batch-116
Running loss of epoch-97 batch-116 = 2.834247425198555e-06

Training epoch-97 batch-117
Running loss of epoch-97 batch-117 = 2.6782508939504623e-06

Training epoch-97 batch-118
Running loss of epoch-97 batch-118 = 3.034016117453575e-06

Training epoch-97 batch-119
Running loss of epoch-97 batch-119 = 2.376968041062355e-06

Training epoch-97 batch-120
Running loss of epoch-97 batch-120 = 2.4011824280023575e-06

Training epoch-97 batch-121
Running loss of epoch-97 batch-121 = 5.519017577171326e-06

Training epoch-97 batch-122
Running loss of epoch-97 batch-122 = 4.097120836377144e-06

Training epoch-97 batch-123
Running loss of epoch-97 batch-123 = 2.823304384946823e-06

Training epoch-97 batch-124
Running loss of epoch-97 batch-124 = 2.4507753551006317e-06

Training epoch-97 batch-125
Running loss of epoch-97 batch-125 = 4.6836212277412415e-06

Training epoch-97 batch-126
Running loss of epoch-97 batch-126 = 3.476860001683235e-06

Training epoch-97 batch-127
Running loss of epoch-97 batch-127 = 1.950189471244812e-06

Training epoch-97 batch-128
Running loss of epoch-97 batch-128 = 2.35741026699543e-06

Training epoch-97 batch-129
Running loss of epoch-97 batch-129 = 2.4079345166683197e-06

Training epoch-97 batch-130
Running loss of epoch-97 batch-130 = 1.9830185920000076e-06

Training epoch-97 batch-131
Running loss of epoch-97 batch-131 = 1.1792872101068497e-06

Training epoch-97 batch-132
Running loss of epoch-97 batch-132 = 6.7336950451135635e-06

Training epoch-97 batch-133
Running loss of epoch-97 batch-133 = 2.064509317278862e-06

Training epoch-97 batch-134
Running loss of epoch-97 batch-134 = 2.330867573618889e-06

Training epoch-97 batch-135
Running loss of epoch-97 batch-135 = 4.794914275407791e-06

Training epoch-97 batch-136
Running loss of epoch-97 batch-136 = 9.434297680854797e-07

Training epoch-97 batch-137
Running loss of epoch-97 batch-137 = 2.22865492105484e-06

Training epoch-97 batch-138
Running loss of epoch-97 batch-138 = 3.6794226616621017e-06

Training epoch-97 batch-139
Running loss of epoch-97 batch-139 = 2.6426278054714203e-06

Training epoch-97 batch-140
Running loss of epoch-97 batch-140 = 3.5064294934272766e-06

Training epoch-97 batch-141
Running loss of epoch-97 batch-141 = 1.8866267055273056e-06

Training epoch-97 batch-142
Running loss of epoch-97 batch-142 = 1.5755649656057358e-06

Training epoch-97 batch-143
Running loss of epoch-97 batch-143 = 2.3085158318281174e-06

Training epoch-97 batch-144
Running loss of epoch-97 batch-144 = 1.1618249118328094e-06

Training epoch-97 batch-145
Running loss of epoch-97 batch-145 = 5.579320713877678e-06

Training epoch-97 batch-146
Running loss of epoch-97 batch-146 = 4.186062142252922e-06

Training epoch-97 batch-147
Running loss of epoch-97 batch-147 = 1.4635734260082245e-06

Training epoch-97 batch-148
Running loss of epoch-97 batch-148 = 8.547445759177208e-06

Training epoch-97 batch-149
Running loss of epoch-97 batch-149 = 5.316687747836113e-06

Training epoch-97 batch-150
Running loss of epoch-97 batch-150 = 3.4177210181951523e-06

Training epoch-97 batch-151
Running loss of epoch-97 batch-151 = 4.7388020902872086e-06

Training epoch-97 batch-152
Running loss of epoch-97 batch-152 = 2.870103344321251e-06

Training epoch-97 batch-153
Running loss of epoch-97 batch-153 = 4.275469109416008e-06

Training epoch-97 batch-154
Running loss of epoch-97 batch-154 = 1.2100208550691605e-06

Training epoch-97 batch-155
Running loss of epoch-97 batch-155 = 2.105720341205597e-06

Training epoch-97 batch-156
Running loss of epoch-97 batch-156 = 2.410728484392166e-06

Training epoch-97 batch-157
Running loss of epoch-97 batch-157 = 5.960464477539063e-08

Finished training epoch-97.



Average train loss at epoch-97 = 3.362718224525452e-06

Started Evaluation

Average val loss at epoch-97 = 1.860449141479514

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 81.14 %

Finished Evaluation



Started training epoch-98


Training epoch-98 batch-1
Running loss of epoch-98 batch-1 = 8.384231477975845e-07

Training epoch-98 batch-2
Running loss of epoch-98 batch-2 = 2.4498440325260162e-06

Training epoch-98 batch-3
Running loss of epoch-98 batch-3 = 1.796521246433258e-06

Training epoch-98 batch-4
Running loss of epoch-98 batch-4 = 2.2444874048233032e-06

Training epoch-98 batch-5
Running loss of epoch-98 batch-5 = 4.320172592997551e-06

Training epoch-98 batch-6
Running loss of epoch-98 batch-6 = 2.1359883248806e-06

Training epoch-98 batch-7
Running loss of epoch-98 batch-7 = 3.2822135835886e-06

Training epoch-98 batch-8
Running loss of epoch-98 batch-8 = 3.420282155275345e-06

Training epoch-98 batch-9
Running loss of epoch-98 batch-9 = 2.596527338027954e-06

Training epoch-98 batch-10
Running loss of epoch-98 batch-10 = 1.593027263879776e-06

Training epoch-98 batch-11
Running loss of epoch-98 batch-11 = 2.6421621441841125e-06

Training epoch-98 batch-12
Running loss of epoch-98 batch-12 = 4.072906449437141e-06

Training epoch-98 batch-13
Running loss of epoch-98 batch-13 = 7.497146725654602e-06

Training epoch-98 batch-14
Running loss of epoch-98 batch-14 = 2.144835889339447e-06

Training epoch-98 batch-15
Running loss of epoch-98 batch-15 = 1.7129350453615189e-06

Training epoch-98 batch-16
Running loss of epoch-98 batch-16 = 1.1522788554430008e-06

Training epoch-98 batch-17
Running loss of epoch-98 batch-17 = 2.3597385734319687e-06

Training epoch-98 batch-18
Running loss of epoch-98 batch-18 = 6.036367267370224e-06

Training epoch-98 batch-19
Running loss of epoch-98 batch-19 = 2.9704533517360687e-06

Training epoch-98 batch-20
Running loss of epoch-98 batch-20 = 1.4454824849963188e-05

Training epoch-98 batch-21
Running loss of epoch-98 batch-21 = 3.041233867406845e-06

Training epoch-98 batch-22
Running loss of epoch-98 batch-22 = 3.4908298403024673e-06

Training epoch-98 batch-23
Running loss of epoch-98 batch-23 = 4.663364961743355e-06

Training epoch-98 batch-24
Running loss of epoch-98 batch-24 = 5.831010639667511e-06

Training epoch-98 batch-25
Running loss of epoch-98 batch-25 = 3.7190038710832596e-06

Training epoch-98 batch-26
Running loss of epoch-98 batch-26 = 1.8461141735315323e-06

Training epoch-98 batch-27
Running loss of epoch-98 batch-27 = 2.0209699869155884e-06

Training epoch-98 batch-28
Running loss of epoch-98 batch-28 = 1.9650906324386597e-06

Training epoch-98 batch-29
Running loss of epoch-98 batch-29 = 4.710163921117783e-06

Training epoch-98 batch-30
Running loss of epoch-98 batch-30 = 2.539250999689102e-06

Training epoch-98 batch-31
Running loss of epoch-98 batch-31 = 1.5415716916322708e-06

Training epoch-98 batch-32
Running loss of epoch-98 batch-32 = 2.343207597732544e-06

Training epoch-98 batch-33
Running loss of epoch-98 batch-33 = 1.7117708921432495e-06

Training epoch-98 batch-34
Running loss of epoch-98 batch-34 = 1.218169927597046e-06

Training epoch-98 batch-35
Running loss of epoch-98 batch-35 = 6.3800252974033356e-06

Training epoch-98 batch-36
Running loss of epoch-98 batch-36 = 4.0745362639427185e-06

Training epoch-98 batch-37
Running loss of epoch-98 batch-37 = 3.2638199627399445e-06

Training epoch-98 batch-38
Running loss of epoch-98 batch-38 = 1.2032687664031982e-06

Training epoch-98 batch-39
Running loss of epoch-98 batch-39 = 4.597241058945656e-06

Training epoch-98 batch-40
Running loss of epoch-98 batch-40 = 5.434267222881317e-06

Training epoch-98 batch-41
Running loss of epoch-98 batch-41 = 1.1508818715810776e-06

Training epoch-98 batch-42
Running loss of epoch-98 batch-42 = 3.7690624594688416e-06

Training epoch-98 batch-43
Running loss of epoch-98 batch-43 = 1.7017591744661331e-06

Training epoch-98 batch-44
Running loss of epoch-98 batch-44 = 3.236345946788788e-06

Training epoch-98 batch-45
Running loss of epoch-98 batch-45 = 3.7329737097024918e-06

Training epoch-98 batch-46
Running loss of epoch-98 batch-46 = 1.516193151473999e-06

Training epoch-98 batch-47
Running loss of epoch-98 batch-47 = 2.0670704543590546e-06

Training epoch-98 batch-48
Running loss of epoch-98 batch-48 = 2.064742147922516e-06

Training epoch-98 batch-49
Running loss of epoch-98 batch-49 = 3.4908298403024673e-06

Training epoch-98 batch-50
Running loss of epoch-98 batch-50 = 4.375586286187172e-06

Training epoch-98 batch-51
Running loss of epoch-98 batch-51 = 4.091532900929451e-06

Training epoch-98 batch-52
Running loss of epoch-98 batch-52 = 2.010958269238472e-06

Training epoch-98 batch-53
Running loss of epoch-98 batch-53 = 1.2540258467197418e-06

Training epoch-98 batch-54
Running loss of epoch-98 batch-54 = 4.077563062310219e-06

Training epoch-98 batch-55
Running loss of epoch-98 batch-55 = 2.0926818251609802e-06

Training epoch-98 batch-56
Running loss of epoch-98 batch-56 = 4.232162609696388e-06

Training epoch-98 batch-57
Running loss of epoch-98 batch-57 = 5.9551093727350235e-06

Training epoch-98 batch-58
Running loss of epoch-98 batch-58 = 2.509215846657753e-06

Training epoch-98 batch-59
Running loss of epoch-98 batch-59 = 1.7015263438224792e-06

Training epoch-98 batch-60
Running loss of epoch-98 batch-60 = 2.0728912204504013e-06

Training epoch-98 batch-61
Running loss of epoch-98 batch-61 = 1.0014045983552933e-06

Training epoch-98 batch-62
Running loss of epoch-98 batch-62 = 6.177695468068123e-06

Training epoch-98 batch-63
Running loss of epoch-98 batch-63 = 2.6940833777189255e-06

Training epoch-98 batch-64
Running loss of epoch-98 batch-64 = 2.9883813112974167e-06

Training epoch-98 batch-65
Running loss of epoch-98 batch-65 = 3.4293625503778458e-06

Training epoch-98 batch-66
Running loss of epoch-98 batch-66 = 6.304820999503136e-06

Training epoch-98 batch-67
Running loss of epoch-98 batch-67 = 7.889466360211372e-06

Training epoch-98 batch-68
Running loss of epoch-98 batch-68 = 2.2365711629390717e-06

Training epoch-98 batch-69
Running loss of epoch-98 batch-69 = 3.712950274348259e-06

Training epoch-98 batch-70
Running loss of epoch-98 batch-70 = 3.702007234096527e-06

Training epoch-98 batch-71
Running loss of epoch-98 batch-71 = 1.5771947801113129e-06

Training epoch-98 batch-72
Running loss of epoch-98 batch-72 = 1.887558028101921e-06

Training epoch-98 batch-73
Running loss of epoch-98 batch-73 = 3.071967512369156e-06

Training epoch-98 batch-74
Running loss of epoch-98 batch-74 = 3.941124305129051e-06

Training epoch-98 batch-75
Running loss of epoch-98 batch-75 = 3.4654513001441956e-06

Training epoch-98 batch-76
Running loss of epoch-98 batch-76 = 6.5658241510391235e-06

Training epoch-98 batch-77
Running loss of epoch-98 batch-77 = 2.714572474360466e-06

Training epoch-98 batch-78
Running loss of epoch-98 batch-78 = 3.7436839193105698e-06

Training epoch-98 batch-79
Running loss of epoch-98 batch-79 = 4.38210554420948e-06

Training epoch-98 batch-80
Running loss of epoch-98 batch-80 = 2.6552006602287292e-06

Training epoch-98 batch-81
Running loss of epoch-98 batch-81 = 9.78400930762291e-06

Training epoch-98 batch-82
Running loss of epoch-98 batch-82 = 2.741347998380661e-06

Training epoch-98 batch-83
Running loss of epoch-98 batch-83 = 1.839594915509224e-06

Training epoch-98 batch-84
Running loss of epoch-98 batch-84 = 4.732050001621246e-06

Training epoch-98 batch-85
Running loss of epoch-98 batch-85 = 5.328096449375153e-06

Training epoch-98 batch-86
Running loss of epoch-98 batch-86 = 4.1318126022815704e-06

Training epoch-98 batch-87
Running loss of epoch-98 batch-87 = 4.764413461089134e-06

Training epoch-98 batch-88
Running loss of epoch-98 batch-88 = 3.0691735446453094e-06

Training epoch-98 batch-89
Running loss of epoch-98 batch-89 = 2.912944182753563e-06

Training epoch-98 batch-90
Running loss of epoch-98 batch-90 = 3.1653326004743576e-06

Training epoch-98 batch-91
Running loss of epoch-98 batch-91 = 9.06502828001976e-06

Training epoch-98 batch-92
Running loss of epoch-98 batch-92 = 1.7066486179828644e-06

Training epoch-98 batch-93
Running loss of epoch-98 batch-93 = 1.4489050954580307e-06

Training epoch-98 batch-94
Running loss of epoch-98 batch-94 = 3.612833097577095e-06

Training epoch-98 batch-95
Running loss of epoch-98 batch-95 = 2.837507054209709e-06

Training epoch-98 batch-96
Running loss of epoch-98 batch-96 = 1.1528609320521355e-05

Training epoch-98 batch-97
Running loss of epoch-98 batch-97 = 2.9401853680610657e-06

Training epoch-98 batch-98
Running loss of epoch-98 batch-98 = 2.8223730623722076e-06

Training epoch-98 batch-99
Running loss of epoch-98 batch-99 = 2.314802259206772e-06

Training epoch-98 batch-100
Running loss of epoch-98 batch-100 = 2.5532208383083344e-06

Training epoch-98 batch-101
Running loss of epoch-98 batch-101 = 1.866370439529419e-06

Training epoch-98 batch-102
Running loss of epoch-98 batch-102 = 2.0936131477355957e-06

Training epoch-98 batch-103
Running loss of epoch-98 batch-103 = 3.7353020161390305e-06

Training epoch-98 batch-104
Running loss of epoch-98 batch-104 = 8.741859346628189e-06

Training epoch-98 batch-105
Running loss of epoch-98 batch-105 = 3.559049218893051e-06

Training epoch-98 batch-106
Running loss of epoch-98 batch-106 = 2.2584572434425354e-06

Training epoch-98 batch-107
Running loss of epoch-98 batch-107 = 1.986278221011162e-06

Training epoch-98 batch-108
Running loss of epoch-98 batch-108 = 2.468470484018326e-06

Training epoch-98 batch-109
Running loss of epoch-98 batch-109 = 4.883389919996262e-06

Training epoch-98 batch-110
Running loss of epoch-98 batch-110 = 3.1820964068174362e-06

Training epoch-98 batch-111
Running loss of epoch-98 batch-111 = 8.035451173782349e-06

Training epoch-98 batch-112
Running loss of epoch-98 batch-112 = 3.6046840250492096e-06

Training epoch-98 batch-113
Running loss of epoch-98 batch-113 = 2.1331943571567535e-06

Training epoch-98 batch-114
Running loss of epoch-98 batch-114 = 2.0158477127552032e-06

Training epoch-98 batch-115
Running loss of epoch-98 batch-115 = 1.494772732257843e-06

Training epoch-98 batch-116
Running loss of epoch-98 batch-116 = 3.355788066983223e-06

Training epoch-98 batch-117
Running loss of epoch-98 batch-117 = 7.674098014831543e-07

Training epoch-98 batch-118
Running loss of epoch-98 batch-118 = 5.601206794381142e-06

Training epoch-98 batch-119
Running loss of epoch-98 batch-119 = 1.4819670468568802e-06

Training epoch-98 batch-120
Running loss of epoch-98 batch-120 = 3.6098062992095947e-06

Training epoch-98 batch-121
Running loss of epoch-98 batch-121 = 1.932727172970772e-06

Training epoch-98 batch-122
Running loss of epoch-98 batch-122 = 1.4828983694314957e-06

Training epoch-98 batch-123
Running loss of epoch-98 batch-123 = 4.040775820612907e-06

Training epoch-98 batch-124
Running loss of epoch-98 batch-124 = 1.4903489500284195e-06

Training epoch-98 batch-125
Running loss of epoch-98 batch-125 = 1.6207341104745865e-06

Training epoch-98 batch-126
Running loss of epoch-98 batch-126 = 3.6172568798065186e-06

Training epoch-98 batch-127
Running loss of epoch-98 batch-127 = 4.052184522151947e-06

Training epoch-98 batch-128
Running loss of epoch-98 batch-128 = 4.335073754191399e-06

Training epoch-98 batch-129
Running loss of epoch-98 batch-129 = 2.0922161638736725e-06

Training epoch-98 batch-130
Running loss of epoch-98 batch-130 = 4.264991730451584e-06

Training epoch-98 batch-131
Running loss of epoch-98 batch-131 = 3.1134113669395447e-06

Training epoch-98 batch-132
Running loss of epoch-98 batch-132 = 5.3765252232551575e-06

Training epoch-98 batch-133
Running loss of epoch-98 batch-133 = 2.516200765967369e-06

Training epoch-98 batch-134
Running loss of epoch-98 batch-134 = 5.263369530439377e-06

Training epoch-98 batch-135
Running loss of epoch-98 batch-135 = 3.0300579965114594e-06

Training epoch-98 batch-136
Running loss of epoch-98 batch-136 = 3.7122517824172974e-06

Training epoch-98 batch-137
Running loss of epoch-98 batch-137 = 2.4335458874702454e-06

Training epoch-98 batch-138
Running loss of epoch-98 batch-138 = 5.690380930900574e-07

Training epoch-98 batch-139
Running loss of epoch-98 batch-139 = 1.4237593859434128e-06

Training epoch-98 batch-140
Running loss of epoch-98 batch-140 = 3.254041075706482e-06

Training epoch-98 batch-141
Running loss of epoch-98 batch-141 = 3.134133294224739e-06

Training epoch-98 batch-142
Running loss of epoch-98 batch-142 = 3.980239853262901e-06

Training epoch-98 batch-143
Running loss of epoch-98 batch-143 = 4.532281309366226e-06

Training epoch-98 batch-144
Running loss of epoch-98 batch-144 = 1.5187542885541916e-06

Training epoch-98 batch-145
Running loss of epoch-98 batch-145 = 1.1867377907037735e-06

Training epoch-98 batch-146
Running loss of epoch-98 batch-146 = 2.426793798804283e-06

Training epoch-98 batch-147
Running loss of epoch-98 batch-147 = 2.1369196474552155e-06

Training epoch-98 batch-148
Running loss of epoch-98 batch-148 = 2.176733687520027e-06

Training epoch-98 batch-149
Running loss of epoch-98 batch-149 = 3.125518560409546e-06

Training epoch-98 batch-150
Running loss of epoch-98 batch-150 = 5.848705768585205e-07

Training epoch-98 batch-151
Running loss of epoch-98 batch-151 = 1.5317928045988083e-06

Training epoch-98 batch-152
Running loss of epoch-98 batch-152 = 3.5064294934272766e-06

Training epoch-98 batch-153
Running loss of epoch-98 batch-153 = 9.094364941120148e-07

Training epoch-98 batch-154
Running loss of epoch-98 batch-154 = 2.6007182896137238e-06

Training epoch-98 batch-155
Running loss of epoch-98 batch-155 = 2.08243727684021e-06

Training epoch-98 batch-156
Running loss of epoch-98 batch-156 = 8.917413651943207e-06

Training epoch-98 batch-157
Running loss of epoch-98 batch-157 = 8.244067430496216e-06

Finished training epoch-98.



Average train loss at epoch-98 = 3.3457159996032714e-06

Started Evaluation

Average val loss at epoch-98 = 1.8714432334036022

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.18 %

Finished Evaluation



Started training epoch-99


Training epoch-99 batch-1
Running loss of epoch-99 batch-1 = 2.923654392361641e-06

Training epoch-99 batch-2
Running loss of epoch-99 batch-2 = 4.797009751200676e-06

Training epoch-99 batch-3
Running loss of epoch-99 batch-3 = 1.4200340956449509e-06

Training epoch-99 batch-4
Running loss of epoch-99 batch-4 = 7.603084668517113e-06

Training epoch-99 batch-5
Running loss of epoch-99 batch-5 = 3.1921081244945526e-06

Training epoch-99 batch-6
Running loss of epoch-99 batch-6 = 2.6060733944177628e-06

Training epoch-99 batch-7
Running loss of epoch-99 batch-7 = 3.1830277293920517e-06

Training epoch-99 batch-8
Running loss of epoch-99 batch-8 = 3.779074177145958e-06

Training epoch-99 batch-9
Running loss of epoch-99 batch-9 = 1.578591763973236e-06

Training epoch-99 batch-10
Running loss of epoch-99 batch-10 = 1.8952414393424988e-06

Training epoch-99 batch-11
Running loss of epoch-99 batch-11 = 3.0954834073781967e-06

Training epoch-99 batch-12
Running loss of epoch-99 batch-12 = 4.254048690199852e-06

Training epoch-99 batch-13
Running loss of epoch-99 batch-13 = 3.475230187177658e-06

Training epoch-99 batch-14
Running loss of epoch-99 batch-14 = 1.3927929103374481e-06

Training epoch-99 batch-15
Running loss of epoch-99 batch-15 = 3.5597477108240128e-06

Training epoch-99 batch-16
Running loss of epoch-99 batch-16 = 2.8819777071475983e-06

Training epoch-99 batch-17
Running loss of epoch-99 batch-17 = 1.9171275198459625e-06

Training epoch-99 batch-18
Running loss of epoch-99 batch-18 = 2.877088263630867e-06

Training epoch-99 batch-19
Running loss of epoch-99 batch-19 = 2.162996679544449e-06

Training epoch-99 batch-20
Running loss of epoch-99 batch-20 = 1.2754462659358978e-06

Training epoch-99 batch-21
Running loss of epoch-99 batch-21 = 1.2777745723724365e-06

Training epoch-99 batch-22
Running loss of epoch-99 batch-22 = 3.5758130252361298e-06

Training epoch-99 batch-23
Running loss of epoch-99 batch-23 = 1.5313271433115005e-06

Training epoch-99 batch-24
Running loss of epoch-99 batch-24 = 4.083150997757912e-06

Training epoch-99 batch-25
Running loss of epoch-99 batch-25 = 4.868721589446068e-06

Training epoch-99 batch-26
Running loss of epoch-99 batch-26 = 1.1159572750329971e-06

Training epoch-99 batch-27
Running loss of epoch-99 batch-27 = 1.8118880689144135e-06

Training epoch-99 batch-28
Running loss of epoch-99 batch-28 = 3.6796554923057556e-06

Training epoch-99 batch-29
Running loss of epoch-99 batch-29 = 3.645196557044983e-06

Training epoch-99 batch-30
Running loss of epoch-99 batch-30 = 2.6174820959568024e-06

Training epoch-99 batch-31
Running loss of epoch-99 batch-31 = 1.1336524039506912e-06

Training epoch-99 batch-32
Running loss of epoch-99 batch-32 = 2.86102294921875e-06

Training epoch-99 batch-33
Running loss of epoch-99 batch-33 = 4.63658943772316e-06

Training epoch-99 batch-34
Running loss of epoch-99 batch-34 = 2.130400389432907e-06

Training epoch-99 batch-35
Running loss of epoch-99 batch-35 = 6.103888154029846e-06

Training epoch-99 batch-36
Running loss of epoch-99 batch-36 = 1.141335815191269e-06

Training epoch-99 batch-37
Running loss of epoch-99 batch-37 = 1.5422701835632324e-06

Training epoch-99 batch-38
Running loss of epoch-99 batch-38 = 2.409098669886589e-06

Training epoch-99 batch-39
Running loss of epoch-99 batch-39 = 2.7674250304698944e-06

Training epoch-99 batch-40
Running loss of epoch-99 batch-40 = 3.908062353730202e-06

Training epoch-99 batch-41
Running loss of epoch-99 batch-41 = 4.184665158390999e-06

Training epoch-99 batch-42
Running loss of epoch-99 batch-42 = 1.0887160897254944e-06

Training epoch-99 batch-43
Running loss of epoch-99 batch-43 = 2.739252522587776e-06

Training epoch-99 batch-44
Running loss of epoch-99 batch-44 = 1.915963366627693e-06

Training epoch-99 batch-45
Running loss of epoch-99 batch-45 = 2.7657952159643173e-06

Training epoch-99 batch-46
Running loss of epoch-99 batch-46 = 3.6372803151607513e-06

Training epoch-99 batch-47
Running loss of epoch-99 batch-47 = 3.0603259801864624e-06

Training epoch-99 batch-48
Running loss of epoch-99 batch-48 = 8.204253390431404e-06

Training epoch-99 batch-49
Running loss of epoch-99 batch-49 = 5.862675607204437e-07

Training epoch-99 batch-50
Running loss of epoch-99 batch-50 = 2.3667234927415848e-06

Training epoch-99 batch-51
Running loss of epoch-99 batch-51 = 3.3373944461345673e-06

Training epoch-99 batch-52
Running loss of epoch-99 batch-52 = 5.878042429685593e-06

Training epoch-99 batch-53
Running loss of epoch-99 batch-53 = 2.741580829024315e-06

Training epoch-99 batch-54
Running loss of epoch-99 batch-54 = 1.5427358448505402e-06

Training epoch-99 batch-55
Running loss of epoch-99 batch-55 = 9.722542017698288e-06

Training epoch-99 batch-56
Running loss of epoch-99 batch-56 = 2.2654421627521515e-06

Training epoch-99 batch-57
Running loss of epoch-99 batch-57 = 2.9634684324264526e-06

Training epoch-99 batch-58
Running loss of epoch-99 batch-58 = 2.888031303882599e-06

Training epoch-99 batch-59
Running loss of epoch-99 batch-59 = 1.1348165571689606e-06

Training epoch-99 batch-60
Running loss of epoch-99 batch-60 = 2.2868625819683075e-06

Training epoch-99 batch-61
Running loss of epoch-99 batch-61 = 1.61794014275074e-06

Training epoch-99 batch-62
Running loss of epoch-99 batch-62 = 1.6938429325819016e-06

Training epoch-99 batch-63
Running loss of epoch-99 batch-63 = 2.667773514986038e-06

Training epoch-99 batch-64
Running loss of epoch-99 batch-64 = 8.037546649575233e-06

Training epoch-99 batch-65
Running loss of epoch-99 batch-65 = 2.0957086235284805e-06

Training epoch-99 batch-66
Running loss of epoch-99 batch-66 = 4.588393494486809e-06

Training epoch-99 batch-67
Running loss of epoch-99 batch-67 = 2.055894583463669e-06

Training epoch-99 batch-68
Running loss of epoch-99 batch-68 = 2.5208573788404465e-06

Training epoch-99 batch-69
Running loss of epoch-99 batch-69 = 4.679197445511818e-06

Training epoch-99 batch-70
Running loss of epoch-99 batch-70 = 5.375826731324196e-06

Training epoch-99 batch-71
Running loss of epoch-99 batch-71 = 3.4207478165626526e-06

Training epoch-99 batch-72
Running loss of epoch-99 batch-72 = 3.439374268054962e-06

Training epoch-99 batch-73
Running loss of epoch-99 batch-73 = 3.1623058021068573e-06

Training epoch-99 batch-74
Running loss of epoch-99 batch-74 = 2.2568274289369583e-06

Training epoch-99 batch-75
Running loss of epoch-99 batch-75 = 5.207490175962448e-06

Training epoch-99 batch-76
Running loss of epoch-99 batch-76 = 9.958166629076004e-06

Training epoch-99 batch-77
Running loss of epoch-99 batch-77 = 5.1620882004499435e-06

Training epoch-99 batch-78
Running loss of epoch-99 batch-78 = 3.3623073250055313e-06

Training epoch-99 batch-79
Running loss of epoch-99 batch-79 = 2.5990884751081467e-06

Training epoch-99 batch-80
Running loss of epoch-99 batch-80 = 2.332031726837158e-06

Training epoch-99 batch-81
Running loss of epoch-99 batch-81 = 2.885935828089714e-06

Training epoch-99 batch-82
Running loss of epoch-99 batch-82 = 4.027504473924637e-06

Training epoch-99 batch-83
Running loss of epoch-99 batch-83 = 7.513910531997681e-06

Training epoch-99 batch-84
Running loss of epoch-99 batch-84 = 1.2691598385572433e-06

Training epoch-99 batch-85
Running loss of epoch-99 batch-85 = 7.764669135212898e-06

Training epoch-99 batch-86
Running loss of epoch-99 batch-86 = 1.0137446224689484e-06

Training epoch-99 batch-87
Running loss of epoch-99 batch-87 = 2.644723281264305e-06

Training epoch-99 batch-88
Running loss of epoch-99 batch-88 = 8.780276402831078e-06

Training epoch-99 batch-89
Running loss of epoch-99 batch-89 = 4.614703357219696e-06

Training epoch-99 batch-90
Running loss of epoch-99 batch-90 = 2.326676622033119e-06

Training epoch-99 batch-91
Running loss of epoch-99 batch-91 = 1.6356352716684341e-06

Training epoch-99 batch-92
Running loss of epoch-99 batch-92 = 3.343448042869568e-06

Training epoch-99 batch-93
Running loss of epoch-99 batch-93 = 1.762760803103447e-06

Training epoch-99 batch-94
Running loss of epoch-99 batch-94 = 1.4384277164936066e-06

Training epoch-99 batch-95
Running loss of epoch-99 batch-95 = 2.3227185010910034e-06

Training epoch-99 batch-96
Running loss of epoch-99 batch-96 = 2.4209730327129364e-06

Training epoch-99 batch-97
Running loss of epoch-99 batch-97 = 1.525040715932846e-06

Training epoch-99 batch-98
Running loss of epoch-99 batch-98 = 3.3043324947357178e-06

Training epoch-99 batch-99
Running loss of epoch-99 batch-99 = 3.473600372672081e-06

Training epoch-99 batch-100
Running loss of epoch-99 batch-100 = 2.6547349989414215e-06

Training epoch-99 batch-101
Running loss of epoch-99 batch-101 = 2.5529880076646805e-06

Training epoch-99 batch-102
Running loss of epoch-99 batch-102 = 2.8936192393302917e-06

Training epoch-99 batch-103
Running loss of epoch-99 batch-103 = 1.178588718175888e-06

Training epoch-99 batch-104
Running loss of epoch-99 batch-104 = 1.4561228454113007e-06

Training epoch-99 batch-105
Running loss of epoch-99 batch-105 = 3.703869879245758e-06

Training epoch-99 batch-106
Running loss of epoch-99 batch-106 = 1.9653234630823135e-06

Training epoch-99 batch-107
Running loss of epoch-99 batch-107 = 1.9334256649017334e-06

Training epoch-99 batch-108
Running loss of epoch-99 batch-108 = 2.6372727006673813e-06

Training epoch-99 batch-109
Running loss of epoch-99 batch-109 = 5.033798515796661e-06

Training epoch-99 batch-110
Running loss of epoch-99 batch-110 = 6.320420652627945e-06

Training epoch-99 batch-111
Running loss of epoch-99 batch-111 = 2.8249341994524e-06

Training epoch-99 batch-112
Running loss of epoch-99 batch-112 = 1.1434312909841537e-06

Training epoch-99 batch-113
Running loss of epoch-99 batch-113 = 1.3206154108047485e-06

Training epoch-99 batch-114
Running loss of epoch-99 batch-114 = 3.103865310549736e-06

Training epoch-99 batch-115
Running loss of epoch-99 batch-115 = 2.5811605155467987e-06

Training epoch-99 batch-116
Running loss of epoch-99 batch-116 = 4.796544089913368e-06

Training epoch-99 batch-117
Running loss of epoch-99 batch-117 = 3.973022103309631e-06

Training epoch-99 batch-118
Running loss of epoch-99 batch-118 = 4.030298441648483e-06

Training epoch-99 batch-119
Running loss of epoch-99 batch-119 = 4.019355401396751e-06

Training epoch-99 batch-120
Running loss of epoch-99 batch-120 = 4.162313416600227e-06

Training epoch-99 batch-121
Running loss of epoch-99 batch-121 = 3.396766260266304e-06

Training epoch-99 batch-122
Running loss of epoch-99 batch-122 = 4.689907655119896e-06

Training epoch-99 batch-123
Running loss of epoch-99 batch-123 = 2.5727786123752594e-06

Training epoch-99 batch-124
Running loss of epoch-99 batch-124 = 1.6577541828155518e-06

Training epoch-99 batch-125
Running loss of epoch-99 batch-125 = 3.1099189072847366e-06

Training epoch-99 batch-126
Running loss of epoch-99 batch-126 = 2.7443747967481613e-06

Training epoch-99 batch-127
Running loss of epoch-99 batch-127 = 2.3960601538419724e-06

Training epoch-99 batch-128
Running loss of epoch-99 batch-128 = 4.008878022432327e-06

Training epoch-99 batch-129
Running loss of epoch-99 batch-129 = 2.9937364161014557e-06

Training epoch-99 batch-130
Running loss of epoch-99 batch-130 = 3.2940879464149475e-06

Training epoch-99 batch-131
Running loss of epoch-99 batch-131 = 1.1811498552560806e-06

Training epoch-99 batch-132
Running loss of epoch-99 batch-132 = 7.91158527135849e-07

Training epoch-99 batch-133
Running loss of epoch-99 batch-133 = 4.2603351175785065e-06

Training epoch-99 batch-134
Running loss of epoch-99 batch-134 = 3.236345946788788e-06

Training epoch-99 batch-135
Running loss of epoch-99 batch-135 = 2.8624199330806732e-06

Training epoch-99 batch-136
Running loss of epoch-99 batch-136 = 3.1602103263139725e-06

Training epoch-99 batch-137
Running loss of epoch-99 batch-137 = 2.2479798644781113e-06

Training epoch-99 batch-138
Running loss of epoch-99 batch-138 = 2.9739458113908768e-06

Training epoch-99 batch-139
Running loss of epoch-99 batch-139 = 3.2025855034589767e-06

Training epoch-99 batch-140
Running loss of epoch-99 batch-140 = 2.7492642402648926e-06

Training epoch-99 batch-141
Running loss of epoch-99 batch-141 = 1.8598511815071106e-06

Training epoch-99 batch-142
Running loss of epoch-99 batch-142 = 1.1055264621973038e-05

Training epoch-99 batch-143
Running loss of epoch-99 batch-143 = 1.9953586161136627e-06

Training epoch-99 batch-144
Running loss of epoch-99 batch-144 = 4.274770617485046e-06

Training epoch-99 batch-145
Running loss of epoch-99 batch-145 = 2.0225998014211655e-06

Training epoch-99 batch-146
Running loss of epoch-99 batch-146 = 3.521796315908432e-06

Training epoch-99 batch-147
Running loss of epoch-99 batch-147 = 4.344852641224861e-06

Training epoch-99 batch-148
Running loss of epoch-99 batch-148 = 1.1787982657551765e-05

Training epoch-99 batch-149
Running loss of epoch-99 batch-149 = 4.102475941181183e-06

Training epoch-99 batch-150
Running loss of epoch-99 batch-150 = 2.5497283786535263e-06

Training epoch-99 batch-151
Running loss of epoch-99 batch-151 = 2.32551246881485e-06

Training epoch-99 batch-152
Running loss of epoch-99 batch-152 = 3.769993782043457e-06

Training epoch-99 batch-153
Running loss of epoch-99 batch-153 = 3.403984010219574e-06

Training epoch-99 batch-154
Running loss of epoch-99 batch-154 = 2.2239983081817627e-06

Training epoch-99 batch-155
Running loss of epoch-99 batch-155 = 4.185596480965614e-06

Training epoch-99 batch-156
Running loss of epoch-99 batch-156 = 2.6274938136339188e-06

Training epoch-99 batch-157
Running loss of epoch-99 batch-157 = 6.3478946685791016e-06

Finished training epoch-99.



Average train loss at epoch-99 = 3.275366127490997e-06

Started Evaluation

Average val loss at epoch-99 = 1.868707194262507

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.79 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.10 %

Finished Evaluation



Started training epoch-100


Training epoch-100 batch-1
Running loss of epoch-100 batch-1 = 4.172557964920998e-06

Training epoch-100 batch-2
Running loss of epoch-100 batch-2 = 3.005610778927803e-06

Training epoch-100 batch-3
Running loss of epoch-100 batch-3 = 2.9669608920812607e-06

Training epoch-100 batch-4
Running loss of epoch-100 batch-4 = 2.312706783413887e-06

Training epoch-100 batch-5
Running loss of epoch-100 batch-5 = 2.864282578229904e-06

Training epoch-100 batch-6
Running loss of epoch-100 batch-6 = 3.5816337913274765e-06

Training epoch-100 batch-7
Running loss of epoch-100 batch-7 = 1.2337695807218552e-06

Training epoch-100 batch-8
Running loss of epoch-100 batch-8 = 1.6961712390184402e-06

Training epoch-100 batch-9
Running loss of epoch-100 batch-9 = 1.932727172970772e-06

Training epoch-100 batch-10
Running loss of epoch-100 batch-10 = 2.957414835691452e-06

Training epoch-100 batch-11
Running loss of epoch-100 batch-11 = 7.550232112407684e-06

Training epoch-100 batch-12
Running loss of epoch-100 batch-12 = 1.3187527656555176e-06

Training epoch-100 batch-13
Running loss of epoch-100 batch-13 = 2.453569322824478e-06

Training epoch-100 batch-14
Running loss of epoch-100 batch-14 = 2.073124051094055e-06

Training epoch-100 batch-15
Running loss of epoch-100 batch-15 = 2.4707987904548645e-06

Training epoch-100 batch-16
Running loss of epoch-100 batch-16 = 3.987923264503479e-06

Training epoch-100 batch-17
Running loss of epoch-100 batch-17 = 2.193031832575798e-06

Training epoch-100 batch-18
Running loss of epoch-100 batch-18 = 2.1816231310367584e-06

Training epoch-100 batch-19
Running loss of epoch-100 batch-19 = 1.662643626332283e-06

Training epoch-100 batch-20
Running loss of epoch-100 batch-20 = 1.1811498552560806e-06

Training epoch-100 batch-21
Running loss of epoch-100 batch-21 = 9.349081665277481e-06

Training epoch-100 batch-22
Running loss of epoch-100 batch-22 = 2.7243513613939285e-06

Training epoch-100 batch-23
Running loss of epoch-100 batch-23 = 2.184417098760605e-06

Training epoch-100 batch-24
Running loss of epoch-100 batch-24 = 2.984190359711647e-06

Training epoch-100 batch-25
Running loss of epoch-100 batch-25 = 2.9902439564466476e-06

Training epoch-100 batch-26
Running loss of epoch-100 batch-26 = 1.93621963262558e-06

Training epoch-100 batch-27
Running loss of epoch-100 batch-27 = 4.955334588885307e-06

Training epoch-100 batch-28
Running loss of epoch-100 batch-28 = 1.8388964235782623e-06

Training epoch-100 batch-29
Running loss of epoch-100 batch-29 = 1.99279747903347e-06

Training epoch-100 batch-30
Running loss of epoch-100 batch-30 = 1.771608367562294e-06

Training epoch-100 batch-31
Running loss of epoch-100 batch-31 = 2.342741936445236e-06

Training epoch-100 batch-32
Running loss of epoch-100 batch-32 = 8.392846211791039e-06

Training epoch-100 batch-33
Running loss of epoch-100 batch-33 = 2.5427434593439102e-06

Training epoch-100 batch-34
Running loss of epoch-100 batch-34 = 3.1781382858753204e-06

Training epoch-100 batch-35
Running loss of epoch-100 batch-35 = 1.8491409718990326e-06

Training epoch-100 batch-36
Running loss of epoch-100 batch-36 = 1.0158400982618332e-06

Training epoch-100 batch-37
Running loss of epoch-100 batch-37 = 2.189306542277336e-06

Training epoch-100 batch-38
Running loss of epoch-100 batch-38 = 5.715992301702499e-06

Training epoch-100 batch-39
Running loss of epoch-100 batch-39 = 1.2526288628578186e-06

Training epoch-100 batch-40
Running loss of epoch-100 batch-40 = 3.216788172721863e-06

Training epoch-100 batch-41
Running loss of epoch-100 batch-41 = 1.9441358745098114e-06

Training epoch-100 batch-42
Running loss of epoch-100 batch-42 = 2.4016480892896652e-06

Training epoch-100 batch-43
Running loss of epoch-100 batch-43 = 2.0049046725034714e-06

Training epoch-100 batch-44
Running loss of epoch-100 batch-44 = 4.568835720419884e-06

Training epoch-100 batch-45
Running loss of epoch-100 batch-45 = 3.7422869354486465e-06

Training epoch-100 batch-46
Running loss of epoch-100 batch-46 = 3.100372850894928e-06

Training epoch-100 batch-47
Running loss of epoch-100 batch-47 = 3.2724346965551376e-06

Training epoch-100 batch-48
Running loss of epoch-100 batch-48 = 1.012696884572506e-05

Training epoch-100 batch-49
Running loss of epoch-100 batch-49 = 2.52528116106987e-06

Training epoch-100 batch-50
Running loss of epoch-100 batch-50 = 1.5757977962493896e-06

Training epoch-100 batch-51
Running loss of epoch-100 batch-51 = 3.000255674123764e-06

Training epoch-100 batch-52
Running loss of epoch-100 batch-52 = 8.724629878997803e-06

Training epoch-100 batch-53
Running loss of epoch-100 batch-53 = 1.912936568260193e-06

Training epoch-100 batch-54
Running loss of epoch-100 batch-54 = 2.473825588822365e-06

Training epoch-100 batch-55
Running loss of epoch-100 batch-55 = 4.214700311422348e-06

Training epoch-100 batch-56
Running loss of epoch-100 batch-56 = 1.6852281987667084e-06

Training epoch-100 batch-57
Running loss of epoch-100 batch-57 = 2.0381994545459747e-06

Training epoch-100 batch-58
Running loss of epoch-100 batch-58 = 2.3597385734319687e-06

Training epoch-100 batch-59
Running loss of epoch-100 batch-59 = 2.2067688405513763e-06

Training epoch-100 batch-60
Running loss of epoch-100 batch-60 = 6.011221557855606e-06

Training epoch-100 batch-61
Running loss of epoch-100 batch-61 = 2.0579900592565536e-06

Training epoch-100 batch-62
Running loss of epoch-100 batch-62 = 3.931811079382896e-06

Training epoch-100 batch-63
Running loss of epoch-100 batch-63 = 5.811918526887894e-06

Training epoch-100 batch-64
Running loss of epoch-100 batch-64 = 1.6011763364076614e-06

Training epoch-100 batch-65
Running loss of epoch-100 batch-65 = 1.5315599739551544e-06

Training epoch-100 batch-66
Running loss of epoch-100 batch-66 = 2.548331394791603e-06

Training epoch-100 batch-67
Running loss of epoch-100 batch-67 = 9.731389582157135e-06

Training epoch-100 batch-68
Running loss of epoch-100 batch-68 = 3.505963832139969e-06

Training epoch-100 batch-69
Running loss of epoch-100 batch-69 = 3.005610778927803e-06

Training epoch-100 batch-70
Running loss of epoch-100 batch-70 = 1.162756234407425e-06

Training epoch-100 batch-71
Running loss of epoch-100 batch-71 = 3.6333221942186356e-06

Training epoch-100 batch-72
Running loss of epoch-100 batch-72 = 1.5939585864543915e-06

Training epoch-100 batch-73
Running loss of epoch-100 batch-73 = 1.628883183002472e-06

Training epoch-100 batch-74
Running loss of epoch-100 batch-74 = 2.1422747522592545e-06

Training epoch-100 batch-75
Running loss of epoch-100 batch-75 = 3.5765115171670914e-06

Training epoch-100 batch-76
Running loss of epoch-100 batch-76 = 5.553010851144791e-06

Training epoch-100 batch-77
Running loss of epoch-100 batch-77 = 2.70036980509758e-06

Training epoch-100 batch-78
Running loss of epoch-100 batch-78 = 4.358123987913132e-06

Training epoch-100 batch-79
Running loss of epoch-100 batch-79 = 3.159279003739357e-06

Training epoch-100 batch-80
Running loss of epoch-100 batch-80 = 1.1434312909841537e-06

Training epoch-100 batch-81
Running loss of epoch-100 batch-81 = 2.0046718418598175e-06

Training epoch-100 batch-82
Running loss of epoch-100 batch-82 = 2.1799933165311813e-06

Training epoch-100 batch-83
Running loss of epoch-100 batch-83 = 9.497161954641342e-07

Training epoch-100 batch-84
Running loss of epoch-100 batch-84 = 1.5618279576301575e-06

Training epoch-100 batch-85
Running loss of epoch-100 batch-85 = 3.2493844628334045e-06

Training epoch-100 batch-86
Running loss of epoch-100 batch-86 = 4.918081685900688e-06

Training epoch-100 batch-87
Running loss of epoch-100 batch-87 = 1.0121148079633713e-06

Training epoch-100 batch-88
Running loss of epoch-100 batch-88 = 2.608867362141609e-06

Training epoch-100 batch-89
Running loss of epoch-100 batch-89 = 7.92602077126503e-06

Training epoch-100 batch-90
Running loss of epoch-100 batch-90 = 3.0479859560728073e-06

Training epoch-100 batch-91
Running loss of epoch-100 batch-91 = 2.2633466869592667e-06

Training epoch-100 batch-92
Running loss of epoch-100 batch-92 = 1.521781086921692e-06

Training epoch-100 batch-93
Running loss of epoch-100 batch-93 = 1.4249235391616821e-06

Training epoch-100 batch-94
Running loss of epoch-100 batch-94 = 1.7550773918628693e-06

Training epoch-100 batch-95
Running loss of epoch-100 batch-95 = 1.9459985196590424e-06

Training epoch-100 batch-96
Running loss of epoch-100 batch-96 = 6.169779226183891e-06

Training epoch-100 batch-97
Running loss of epoch-100 batch-97 = 3.2726675271987915e-06

Training epoch-100 batch-98
Running loss of epoch-100 batch-98 = 4.056142643094063e-06

Training epoch-100 batch-99
Running loss of epoch-100 batch-99 = 2.4437904357910156e-06

Training epoch-100 batch-100
Running loss of epoch-100 batch-100 = 2.728775143623352e-06

Training epoch-100 batch-101
Running loss of epoch-100 batch-101 = 4.78280708193779e-06

Training epoch-100 batch-102
Running loss of epoch-100 batch-102 = 1.9280705600976944e-06

Training epoch-100 batch-103
Running loss of epoch-100 batch-103 = 3.57162207365036e-06

Training epoch-100 batch-104
Running loss of epoch-100 batch-104 = 1.589301973581314e-06

Training epoch-100 batch-105
Running loss of epoch-100 batch-105 = 2.5799963623285294e-06

Training epoch-100 batch-106
Running loss of epoch-100 batch-106 = 3.657536581158638e-06

Training epoch-100 batch-107
Running loss of epoch-100 batch-107 = 2.2454187273979187e-06

Training epoch-100 batch-108
Running loss of epoch-100 batch-108 = 2.5760382413864136e-06

Training epoch-100 batch-109
Running loss of epoch-100 batch-109 = 2.5457702577114105e-06

Training epoch-100 batch-110
Running loss of epoch-100 batch-110 = 2.1078158169984818e-06

Training epoch-100 batch-111
Running loss of epoch-100 batch-111 = 3.2102689146995544e-06

Training epoch-100 batch-112
Running loss of epoch-100 batch-112 = 2.725282683968544e-06

Training epoch-100 batch-113
Running loss of epoch-100 batch-113 = 1.7124693840742111e-06

Training epoch-100 batch-114
Running loss of epoch-100 batch-114 = 2.9907096177339554e-06

Training epoch-100 batch-115
Running loss of epoch-100 batch-115 = 7.934868335723877e-07

Training epoch-100 batch-116
Running loss of epoch-100 batch-116 = 5.673384293913841e-06

Training epoch-100 batch-117
Running loss of epoch-100 batch-117 = 2.252170816063881e-06

Training epoch-100 batch-118
Running loss of epoch-100 batch-118 = 4.38210554420948e-06

Training epoch-100 batch-119
Running loss of epoch-100 batch-119 = 7.946975529193878e-06

Training epoch-100 batch-120
Running loss of epoch-100 batch-120 = 4.057539626955986e-06

Training epoch-100 batch-121
Running loss of epoch-100 batch-121 = 1.991167664527893e-06

Training epoch-100 batch-122
Running loss of epoch-100 batch-122 = 4.773493856191635e-06

Training epoch-100 batch-123
Running loss of epoch-100 batch-123 = 3.2531097531318665e-06

Training epoch-100 batch-124
Running loss of epoch-100 batch-124 = 3.764638677239418e-06

Training epoch-100 batch-125
Running loss of epoch-100 batch-125 = 5.0140079110860825e-06

Training epoch-100 batch-126
Running loss of epoch-100 batch-126 = 8.180039003491402e-06

Training epoch-100 batch-127
Running loss of epoch-100 batch-127 = 6.278743967413902e-06

Training epoch-100 batch-128
Running loss of epoch-100 batch-128 = 2.5494955480098724e-06

Training epoch-100 batch-129
Running loss of epoch-100 batch-129 = 1.8307473510503769e-06

Training epoch-100 batch-130
Running loss of epoch-100 batch-130 = 1.7618294805288315e-06

Training epoch-100 batch-131
Running loss of epoch-100 batch-131 = 4.896661266684532e-06

Training epoch-100 batch-132
Running loss of epoch-100 batch-132 = 9.171199053525925e-07

Training epoch-100 batch-133
Running loss of epoch-100 batch-133 = 4.176050424575806e-06

Training epoch-100 batch-134
Running loss of epoch-100 batch-134 = 1.2009404599666595e-06

Training epoch-100 batch-135
Running loss of epoch-100 batch-135 = 3.0223745852708817e-06

Training epoch-100 batch-136
Running loss of epoch-100 batch-136 = 6.831251084804535e-06

Training epoch-100 batch-137
Running loss of epoch-100 batch-137 = 2.0388979464769363e-06

Training epoch-100 batch-138
Running loss of epoch-100 batch-138 = 1.9795261323451996e-06

Training epoch-100 batch-139
Running loss of epoch-100 batch-139 = 2.507120370864868e-06

Training epoch-100 batch-140
Running loss of epoch-100 batch-140 = 3.651948645710945e-06

Training epoch-100 batch-141
Running loss of epoch-100 batch-141 = 9.462237358093262e-07

Training epoch-100 batch-142
Running loss of epoch-100 batch-142 = 3.1189993023872375e-06

Training epoch-100 batch-143
Running loss of epoch-100 batch-143 = 4.205852746963501e-06

Training epoch-100 batch-144
Running loss of epoch-100 batch-144 = 6.7050568759441376e-06

Training epoch-100 batch-145
Running loss of epoch-100 batch-145 = 3.0116643756628036e-06

Training epoch-100 batch-146
Running loss of epoch-100 batch-146 = 1.6917474567890167e-06

Training epoch-100 batch-147
Running loss of epoch-100 batch-147 = 3.445195034146309e-06

Training epoch-100 batch-148
Running loss of epoch-100 batch-148 = 8.472008630633354e-06

Training epoch-100 batch-149
Running loss of epoch-100 batch-149 = 2.395128831267357e-06

Training epoch-100 batch-150
Running loss of epoch-100 batch-150 = 7.852213457226753e-06

Training epoch-100 batch-151
Running loss of epoch-100 batch-151 = 3.068475052714348e-06

Training epoch-100 batch-152
Running loss of epoch-100 batch-152 = 3.630528226494789e-06

Training epoch-100 batch-153
Running loss of epoch-100 batch-153 = 3.266148269176483e-06

Training epoch-100 batch-154
Running loss of epoch-100 batch-154 = 1.18953175842762e-06

Training epoch-100 batch-155
Running loss of epoch-100 batch-155 = 4.7548674046993256e-06

Training epoch-100 batch-156
Running loss of epoch-100 batch-156 = 2.1408777683973312e-06

Training epoch-100 batch-157
Running loss of epoch-100 batch-157 = 1.146271824836731e-05

Finished training epoch-100.



Average train loss at epoch-100 = 3.2399162650108337e-06

Started Evaluation

Average val loss at epoch-100 = 1.8738030096220775

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 88.52 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 51.35 %
Accuracy for class execute is: 51.41 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.14 %

Finished Evaluation

