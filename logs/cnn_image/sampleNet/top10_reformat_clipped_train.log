
DataLoader:

Started trainloader.
trainset = #10000
Finished trainloader.
Started valloader.
valset = #4847
Finished valloader.

Configuration:

DB_NAME = java-top10, IMG_TYPE = reformat_clipped, TRANS_SIZE = 1024
MAX_EPOCH = 100, BATCH_SIZE = 64

device = cuda:0



Started training epoch-1


Training epoch-1 batch-1
Running loss of epoch-1 batch-1 = 0.03590783476829529

Training epoch-1 batch-2
Running loss of epoch-1 batch-2 = 0.03605738282203674

Training epoch-1 batch-3
Running loss of epoch-1 batch-3 = 0.035793062299489975

Training epoch-1 batch-4
Running loss of epoch-1 batch-4 = 0.036020759493112564

Training epoch-1 batch-5
Running loss of epoch-1 batch-5 = 0.03579052910208702

Training epoch-1 batch-6
Running loss of epoch-1 batch-6 = 0.03580522537231445

Training epoch-1 batch-7
Running loss of epoch-1 batch-7 = 0.03573665767908096

Training epoch-1 batch-8
Running loss of epoch-1 batch-8 = 0.03599492087960243

Training epoch-1 batch-9
Running loss of epoch-1 batch-9 = 0.03576568514108658

Training epoch-1 batch-10
Running loss of epoch-1 batch-10 = 0.03594307228922844

Training epoch-1 batch-11
Running loss of epoch-1 batch-11 = 0.03579932078719139

Training epoch-1 batch-12
Running loss of epoch-1 batch-12 = 0.03583671525120735

Training epoch-1 batch-13
Running loss of epoch-1 batch-13 = 0.03601241484284401

Training epoch-1 batch-14
Running loss of epoch-1 batch-14 = 0.03611002117395401

Training epoch-1 batch-15
Running loss of epoch-1 batch-15 = 0.035945404320955276

Training epoch-1 batch-16
Running loss of epoch-1 batch-16 = 0.03612908720970154

Training epoch-1 batch-17
Running loss of epoch-1 batch-17 = 0.03657035529613495

Training epoch-1 batch-18
Running loss of epoch-1 batch-18 = 0.03598804399371147

Training epoch-1 batch-19
Running loss of epoch-1 batch-19 = 0.03617526590824127

Training epoch-1 batch-20
Running loss of epoch-1 batch-20 = 0.03587226942181587

Training epoch-1 batch-21
Running loss of epoch-1 batch-21 = 0.035889603197574615

Training epoch-1 batch-22
Running loss of epoch-1 batch-22 = 0.03582071140408516

Training epoch-1 batch-23
Running loss of epoch-1 batch-23 = 0.036111921072006226

Training epoch-1 batch-24
Running loss of epoch-1 batch-24 = 0.036089856177568436

Training epoch-1 batch-25
Running loss of epoch-1 batch-25 = 0.036131601780653

Training epoch-1 batch-26
Running loss of epoch-1 batch-26 = 0.03584086522459984

Training epoch-1 batch-27
Running loss of epoch-1 batch-27 = 0.03603122755885124

Training epoch-1 batch-28
Running loss of epoch-1 batch-28 = 0.0360867865383625

Training epoch-1 batch-29
Running loss of epoch-1 batch-29 = 0.036094147711992264

Training epoch-1 batch-30
Running loss of epoch-1 batch-30 = 0.03594760596752167

Training epoch-1 batch-31
Running loss of epoch-1 batch-31 = 0.03621307387948036

Training epoch-1 batch-32
Running loss of epoch-1 batch-32 = 0.03591722995042801

Training epoch-1 batch-33
Running loss of epoch-1 batch-33 = 0.03605499863624573

Training epoch-1 batch-34
Running loss of epoch-1 batch-34 = 0.03592091426253319

Training epoch-1 batch-35
Running loss of epoch-1 batch-35 = 0.03604862093925476

Training epoch-1 batch-36
Running loss of epoch-1 batch-36 = 0.03584764152765274

Training epoch-1 batch-37
Running loss of epoch-1 batch-37 = 0.035882193595170975

Training epoch-1 batch-38
Running loss of epoch-1 batch-38 = 0.035946689546108246

Training epoch-1 batch-39
Running loss of epoch-1 batch-39 = 0.03616824373602867

Training epoch-1 batch-40
Running loss of epoch-1 batch-40 = 0.035741571336984634

Training epoch-1 batch-41
Running loss of epoch-1 batch-41 = 0.03593006730079651

Training epoch-1 batch-42
Running loss of epoch-1 batch-42 = 0.03581513091921806

Training epoch-1 batch-43
Running loss of epoch-1 batch-43 = 0.03571215271949768

Training epoch-1 batch-44
Running loss of epoch-1 batch-44 = 0.03580591082572937

Training epoch-1 batch-45
Running loss of epoch-1 batch-45 = 0.03579537570476532

Training epoch-1 batch-46
Running loss of epoch-1 batch-46 = 0.03575698286294937

Training epoch-1 batch-47
Running loss of epoch-1 batch-47 = 0.03576686605811119

Training epoch-1 batch-48
Running loss of epoch-1 batch-48 = 0.035978950560092926

Training epoch-1 batch-49
Running loss of epoch-1 batch-49 = 0.035778988152742386

Training epoch-1 batch-50
Running loss of epoch-1 batch-50 = 0.03605007007718086

Training epoch-1 batch-51
Running loss of epoch-1 batch-51 = 0.036007851362228394

Training epoch-1 batch-52
Running loss of epoch-1 batch-52 = 0.0356915257871151

Training epoch-1 batch-53
Running loss of epoch-1 batch-53 = 0.03653948754072189

Training epoch-1 batch-54
Running loss of epoch-1 batch-54 = 0.03649602830410004

Training epoch-1 batch-55
Running loss of epoch-1 batch-55 = 0.03618650510907173

Training epoch-1 batch-56
Running loss of epoch-1 batch-56 = 0.03567831590771675

Training epoch-1 batch-57
Running loss of epoch-1 batch-57 = 0.03591901808977127

Training epoch-1 batch-58
Running loss of epoch-1 batch-58 = 0.03586927428841591

Training epoch-1 batch-59
Running loss of epoch-1 batch-59 = 0.036243680864572525

Training epoch-1 batch-60
Running loss of epoch-1 batch-60 = 0.03595861792564392

Training epoch-1 batch-61
Running loss of epoch-1 batch-61 = 0.036147817969322205

Training epoch-1 batch-62
Running loss of epoch-1 batch-62 = 0.03577160835266113

Training epoch-1 batch-63
Running loss of epoch-1 batch-63 = 0.03596693277359009

Training epoch-1 batch-64
Running loss of epoch-1 batch-64 = 0.035897426307201385

Training epoch-1 batch-65
Running loss of epoch-1 batch-65 = 0.035937752574682236

Training epoch-1 batch-66
Running loss of epoch-1 batch-66 = 0.03591414913535118

Training epoch-1 batch-67
Running loss of epoch-1 batch-67 = 0.03584267199039459

Training epoch-1 batch-68
Running loss of epoch-1 batch-68 = 0.035811830312013626

Training epoch-1 batch-69
Running loss of epoch-1 batch-69 = 0.035912178456783295

Training epoch-1 batch-70
Running loss of epoch-1 batch-70 = 0.035948414355516434

Training epoch-1 batch-71
Running loss of epoch-1 batch-71 = 0.03594399243593216

Training epoch-1 batch-72
Running loss of epoch-1 batch-72 = 0.03590407967567444

Training epoch-1 batch-73
Running loss of epoch-1 batch-73 = 0.035883184522390366

Training epoch-1 batch-74
Running loss of epoch-1 batch-74 = 0.03588518500328064

Training epoch-1 batch-75
Running loss of epoch-1 batch-75 = 0.035708680748939514

Training epoch-1 batch-76
Running loss of epoch-1 batch-76 = 0.03592187166213989

Training epoch-1 batch-77
Running loss of epoch-1 batch-77 = 0.03577316924929619

Training epoch-1 batch-78
Running loss of epoch-1 batch-78 = 0.035748567432165146

Training epoch-1 batch-79
Running loss of epoch-1 batch-79 = 0.035969413816928864

Training epoch-1 batch-80
Running loss of epoch-1 batch-80 = 0.03580532968044281

Training epoch-1 batch-81
Running loss of epoch-1 batch-81 = 0.03581125661730766

Training epoch-1 batch-82
Running loss of epoch-1 batch-82 = 0.03593448922038078

Training epoch-1 batch-83
Running loss of epoch-1 batch-83 = 0.03580746799707413

Training epoch-1 batch-84
Running loss of epoch-1 batch-84 = 0.03586715832352638

Training epoch-1 batch-85
Running loss of epoch-1 batch-85 = 0.036078400909900665

Training epoch-1 batch-86
Running loss of epoch-1 batch-86 = 0.03584258630871773

Training epoch-1 batch-87
Running loss of epoch-1 batch-87 = 0.03567082807421684

Training epoch-1 batch-88
Running loss of epoch-1 batch-88 = 0.035977672785520554

Training epoch-1 batch-89
Running loss of epoch-1 batch-89 = 0.03561446815729141

Training epoch-1 batch-90
Running loss of epoch-1 batch-90 = 0.035968977957963943

Training epoch-1 batch-91
Running loss of epoch-1 batch-91 = 0.03597593307495117

Training epoch-1 batch-92
Running loss of epoch-1 batch-92 = 0.03616228699684143

Training epoch-1 batch-93
Running loss of epoch-1 batch-93 = 0.035713568329811096

Training epoch-1 batch-94
Running loss of epoch-1 batch-94 = 0.035860516130924225

Training epoch-1 batch-95
Running loss of epoch-1 batch-95 = 0.035552751272916794

Training epoch-1 batch-96
Running loss of epoch-1 batch-96 = 0.03574974462389946

Training epoch-1 batch-97
Running loss of epoch-1 batch-97 = 0.036055415868759155

Training epoch-1 batch-98
Running loss of epoch-1 batch-98 = 0.03588832542300224

Training epoch-1 batch-99
Running loss of epoch-1 batch-99 = 0.035646431148052216

Training epoch-1 batch-100
Running loss of epoch-1 batch-100 = 0.035804588347673416

Training epoch-1 batch-101
Running loss of epoch-1 batch-101 = 0.03585141897201538

Training epoch-1 batch-102
Running loss of epoch-1 batch-102 = 0.035605538636446

Training epoch-1 batch-103
Running loss of epoch-1 batch-103 = 0.03594803065061569

Training epoch-1 batch-104
Running loss of epoch-1 batch-104 = 0.03596608713269234

Training epoch-1 batch-105
Running loss of epoch-1 batch-105 = 0.035698533058166504

Training epoch-1 batch-106
Running loss of epoch-1 batch-106 = 0.03571430221199989

Training epoch-1 batch-107
Running loss of epoch-1 batch-107 = 0.03579447790980339

Training epoch-1 batch-108
Running loss of epoch-1 batch-108 = 0.03567853569984436

Training epoch-1 batch-109
Running loss of epoch-1 batch-109 = 0.0358240082859993

Training epoch-1 batch-110
Running loss of epoch-1 batch-110 = 0.03569634258747101

Training epoch-1 batch-111
Running loss of epoch-1 batch-111 = 0.035674501210451126

Training epoch-1 batch-112
Running loss of epoch-1 batch-112 = 0.0358729250729084

Training epoch-1 batch-113
Running loss of epoch-1 batch-113 = 0.035718709230422974

Training epoch-1 batch-114
Running loss of epoch-1 batch-114 = 0.03574730083346367

Training epoch-1 batch-115
Running loss of epoch-1 batch-115 = 0.035804104059934616

Training epoch-1 batch-116
Running loss of epoch-1 batch-116 = 0.035786934196949005

Training epoch-1 batch-117
Running loss of epoch-1 batch-117 = 0.03562591224908829

Training epoch-1 batch-118
Running loss of epoch-1 batch-118 = 0.0361129529774189

Training epoch-1 batch-119
Running loss of epoch-1 batch-119 = 0.03570500761270523

Training epoch-1 batch-120
Running loss of epoch-1 batch-120 = 0.03594781830906868

Training epoch-1 batch-121
Running loss of epoch-1 batch-121 = 0.0356576181948185

Training epoch-1 batch-122
Running loss of epoch-1 batch-122 = 0.0358622707426548

Training epoch-1 batch-123
Running loss of epoch-1 batch-123 = 0.03583121299743652

Training epoch-1 batch-124
Running loss of epoch-1 batch-124 = 0.03565092384815216

Training epoch-1 batch-125
Running loss of epoch-1 batch-125 = 0.035903200507164

Training epoch-1 batch-126
Running loss of epoch-1 batch-126 = 0.03582429140806198

Training epoch-1 batch-127
Running loss of epoch-1 batch-127 = 0.03559021279215813

Training epoch-1 batch-128
Running loss of epoch-1 batch-128 = 0.035546351224184036

Training epoch-1 batch-129
Running loss of epoch-1 batch-129 = 0.03565531224012375

Training epoch-1 batch-130
Running loss of epoch-1 batch-130 = 0.03561794012784958

Training epoch-1 batch-131
Running loss of epoch-1 batch-131 = 0.03576761111617088

Training epoch-1 batch-132
Running loss of epoch-1 batch-132 = 0.035704612731933594

Training epoch-1 batch-133
Running loss of epoch-1 batch-133 = 0.03560888394713402

Training epoch-1 batch-134
Running loss of epoch-1 batch-134 = 0.03570161759853363

Training epoch-1 batch-135
Running loss of epoch-1 batch-135 = 0.03586672618985176

Training epoch-1 batch-136
Running loss of epoch-1 batch-136 = 0.03554331511259079

Training epoch-1 batch-137
Running loss of epoch-1 batch-137 = 0.03556586802005768

Training epoch-1 batch-138
Running loss of epoch-1 batch-138 = 0.03571435064077377

Training epoch-1 batch-139
Running loss of epoch-1 batch-139 = 0.03551080450415611

Training epoch-1 batch-140
Running loss of epoch-1 batch-140 = 0.03569938614964485

Training epoch-1 batch-141
Running loss of epoch-1 batch-141 = 0.0357537642121315

Training epoch-1 batch-142
Running loss of epoch-1 batch-142 = 0.03569243103265762

Training epoch-1 batch-143
Running loss of epoch-1 batch-143 = 0.0356803722679615

Training epoch-1 batch-144
Running loss of epoch-1 batch-144 = 0.03580763563513756

Training epoch-1 batch-145
Running loss of epoch-1 batch-145 = 0.03550504893064499

Training epoch-1 batch-146
Running loss of epoch-1 batch-146 = 0.03545606508851051

Training epoch-1 batch-147
Running loss of epoch-1 batch-147 = 0.03561602905392647

Training epoch-1 batch-148
Running loss of epoch-1 batch-148 = 0.03571980074048042

Training epoch-1 batch-149
Running loss of epoch-1 batch-149 = 0.035590484738349915

Training epoch-1 batch-150
Running loss of epoch-1 batch-150 = 0.035741135478019714

Training epoch-1 batch-151
Running loss of epoch-1 batch-151 = 0.03547824174165726

Training epoch-1 batch-152
Running loss of epoch-1 batch-152 = 0.03563036397099495

Training epoch-1 batch-153
Running loss of epoch-1 batch-153 = 0.035605356097221375

Training epoch-1 batch-154
Running loss of epoch-1 batch-154 = 0.03566373512148857

Training epoch-1 batch-155
Running loss of epoch-1 batch-155 = 0.03554420918226242

Training epoch-1 batch-156
Running loss of epoch-1 batch-156 = 0.035713307559490204

Training epoch-1 batch-157
Running loss of epoch-1 batch-157 = 0.14507055282592773

Finished training epoch-1.



Average train loss at epoch-1 = 0.03602405776977539

Started Evaluation

Average val loss at epoch-1 = 2.260866657683724

Accuracy for classes:
Accuracy for class equals is: 57.76 %
Accuracy for class main is: 67.54 %
Accuracy for class setUp is: 0.33 %
Accuracy for class onCreate is: 38.49 %
Accuracy for class toString is: 25.94 %
Accuracy for class run is: 0.00 %
Accuracy for class hashCode is: 30.71 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 0.00 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 26.47 %


Best Accuracy = 26.47 % at Epoch-1
Saving model after best epoch-1

Finished Evaluation



Started training epoch-2


Training epoch-2 batch-1
Running loss of epoch-2 batch-1 = 0.035483963787555695

Training epoch-2 batch-2
Running loss of epoch-2 batch-2 = 0.035459838807582855

Training epoch-2 batch-3
Running loss of epoch-2 batch-3 = 0.035468995571136475

Training epoch-2 batch-4
Running loss of epoch-2 batch-4 = 0.03530444577336311

Training epoch-2 batch-5
Running loss of epoch-2 batch-5 = 0.03556755185127258

Training epoch-2 batch-6
Running loss of epoch-2 batch-6 = 0.035575877875089645

Training epoch-2 batch-7
Running loss of epoch-2 batch-7 = 0.03540746495127678

Training epoch-2 batch-8
Running loss of epoch-2 batch-8 = 0.03551723435521126

Training epoch-2 batch-9
Running loss of epoch-2 batch-9 = 0.03532751649618149

Training epoch-2 batch-10
Running loss of epoch-2 batch-10 = 0.035639677196741104

Training epoch-2 batch-11
Running loss of epoch-2 batch-11 = 0.03579900786280632

Training epoch-2 batch-12
Running loss of epoch-2 batch-12 = 0.035234592854976654

Training epoch-2 batch-13
Running loss of epoch-2 batch-13 = 0.035532884299755096

Training epoch-2 batch-14
Running loss of epoch-2 batch-14 = 0.03582412376999855

Training epoch-2 batch-15
Running loss of epoch-2 batch-15 = 0.03521350026130676

Training epoch-2 batch-16
Running loss of epoch-2 batch-16 = 0.035653647035360336

Training epoch-2 batch-17
Running loss of epoch-2 batch-17 = 0.035711850970983505

Training epoch-2 batch-18
Running loss of epoch-2 batch-18 = 0.03538905084133148

Training epoch-2 batch-19
Running loss of epoch-2 batch-19 = 0.03571375086903572

Training epoch-2 batch-20
Running loss of epoch-2 batch-20 = 0.035440411418676376

Training epoch-2 batch-21
Running loss of epoch-2 batch-21 = 0.03542954847216606

Training epoch-2 batch-22
Running loss of epoch-2 batch-22 = 0.035135455429553986

Training epoch-2 batch-23
Running loss of epoch-2 batch-23 = 0.03546632081270218

Training epoch-2 batch-24
Running loss of epoch-2 batch-24 = 0.03518497943878174

Training epoch-2 batch-25
Running loss of epoch-2 batch-25 = 0.03543806076049805

Training epoch-2 batch-26
Running loss of epoch-2 batch-26 = 0.03540787100791931

Training epoch-2 batch-27
Running loss of epoch-2 batch-27 = 0.03552696481347084

Training epoch-2 batch-28
Running loss of epoch-2 batch-28 = 0.0353904627263546

Training epoch-2 batch-29
Running loss of epoch-2 batch-29 = 0.03534228727221489

Training epoch-2 batch-30
Running loss of epoch-2 batch-30 = 0.03544539585709572

Training epoch-2 batch-31
Running loss of epoch-2 batch-31 = 0.03539348766207695

Training epoch-2 batch-32
Running loss of epoch-2 batch-32 = 0.035078492015600204

Training epoch-2 batch-33
Running loss of epoch-2 batch-33 = 0.03556928038597107

Training epoch-2 batch-34
Running loss of epoch-2 batch-34 = 0.03538399934768677

Training epoch-2 batch-35
Running loss of epoch-2 batch-35 = 0.03489973768591881

Training epoch-2 batch-36
Running loss of epoch-2 batch-36 = 0.035493671894073486

Training epoch-2 batch-37
Running loss of epoch-2 batch-37 = 0.03517235815525055

Training epoch-2 batch-38
Running loss of epoch-2 batch-38 = 0.03552385792136192

Training epoch-2 batch-39
Running loss of epoch-2 batch-39 = 0.03508974239230156

Training epoch-2 batch-40
Running loss of epoch-2 batch-40 = 0.03518557921051979

Training epoch-2 batch-41
Running loss of epoch-2 batch-41 = 0.035312503576278687

Training epoch-2 batch-42
Running loss of epoch-2 batch-42 = 0.03529449924826622

Training epoch-2 batch-43
Running loss of epoch-2 batch-43 = 0.03537142276763916

Training epoch-2 batch-44
Running loss of epoch-2 batch-44 = 0.03505004569888115

Training epoch-2 batch-45
Running loss of epoch-2 batch-45 = 0.03475380316376686

Training epoch-2 batch-46
Running loss of epoch-2 batch-46 = 0.03531273826956749

Training epoch-2 batch-47
Running loss of epoch-2 batch-47 = 0.034821342676877975

Training epoch-2 batch-48
Running loss of epoch-2 batch-48 = 0.03521687909960747

Training epoch-2 batch-49
Running loss of epoch-2 batch-49 = 0.034889038652181625

Training epoch-2 batch-50
Running loss of epoch-2 batch-50 = 0.03529585152864456

Training epoch-2 batch-51
Running loss of epoch-2 batch-51 = 0.034684278070926666

Training epoch-2 batch-52
Running loss of epoch-2 batch-52 = 0.034572917968034744

Training epoch-2 batch-53
Running loss of epoch-2 batch-53 = 0.035563189536333084

Training epoch-2 batch-54
Running loss of epoch-2 batch-54 = 0.03471417725086212

Training epoch-2 batch-55
Running loss of epoch-2 batch-55 = 0.035240672528743744

Training epoch-2 batch-56
Running loss of epoch-2 batch-56 = 0.035037342458963394

Training epoch-2 batch-57
Running loss of epoch-2 batch-57 = 0.034892890602350235

Training epoch-2 batch-58
Running loss of epoch-2 batch-58 = 0.03504486009478569

Training epoch-2 batch-59
Running loss of epoch-2 batch-59 = 0.035166095942258835

Training epoch-2 batch-60
Running loss of epoch-2 batch-60 = 0.035627394914627075

Training epoch-2 batch-61
Running loss of epoch-2 batch-61 = 0.03452269360423088

Training epoch-2 batch-62
Running loss of epoch-2 batch-62 = 0.03491056337952614

Training epoch-2 batch-63
Running loss of epoch-2 batch-63 = 0.035122886300086975

Training epoch-2 batch-64
Running loss of epoch-2 batch-64 = 0.03501085564494133

Training epoch-2 batch-65
Running loss of epoch-2 batch-65 = 0.03577595576643944

Training epoch-2 batch-66
Running loss of epoch-2 batch-66 = 0.034918393939733505

Training epoch-2 batch-67
Running loss of epoch-2 batch-67 = 0.03473685309290886

Training epoch-2 batch-68
Running loss of epoch-2 batch-68 = 0.03472209721803665

Training epoch-2 batch-69
Running loss of epoch-2 batch-69 = 0.03496959060430527

Training epoch-2 batch-70
Running loss of epoch-2 batch-70 = 0.034723225980997086

Training epoch-2 batch-71
Running loss of epoch-2 batch-71 = 0.0347115695476532

Training epoch-2 batch-72
Running loss of epoch-2 batch-72 = 0.03466792777180672

Training epoch-2 batch-73
Running loss of epoch-2 batch-73 = 0.03411785885691643

Training epoch-2 batch-74
Running loss of epoch-2 batch-74 = 0.03508570045232773

Training epoch-2 batch-75
Running loss of epoch-2 batch-75 = 0.035204872488975525

Training epoch-2 batch-76
Running loss of epoch-2 batch-76 = 0.03440236672759056

Training epoch-2 batch-77
Running loss of epoch-2 batch-77 = 0.034642625600099564

Training epoch-2 batch-78
Running loss of epoch-2 batch-78 = 0.03464876860380173

Training epoch-2 batch-79
Running loss of epoch-2 batch-79 = 0.03479602560400963

Training epoch-2 batch-80
Running loss of epoch-2 batch-80 = 0.034514229744672775

Training epoch-2 batch-81
Running loss of epoch-2 batch-81 = 0.03493032231926918

Training epoch-2 batch-82
Running loss of epoch-2 batch-82 = 0.034396473318338394

Training epoch-2 batch-83
Running loss of epoch-2 batch-83 = 0.03459195792675018

Training epoch-2 batch-84
Running loss of epoch-2 batch-84 = 0.03519875183701515

Training epoch-2 batch-85
Running loss of epoch-2 batch-85 = 0.03435928374528885

Training epoch-2 batch-86
Running loss of epoch-2 batch-86 = 0.03463264927268028

Training epoch-2 batch-87
Running loss of epoch-2 batch-87 = 0.034119561314582825

Training epoch-2 batch-88
Running loss of epoch-2 batch-88 = 0.03496967628598213

Training epoch-2 batch-89
Running loss of epoch-2 batch-89 = 0.03411649912595749

Training epoch-2 batch-90
Running loss of epoch-2 batch-90 = 0.03487824648618698

Training epoch-2 batch-91
Running loss of epoch-2 batch-91 = 0.034608885645866394

Training epoch-2 batch-92
Running loss of epoch-2 batch-92 = 0.03431228548288345

Training epoch-2 batch-93
Running loss of epoch-2 batch-93 = 0.03452809154987335

Training epoch-2 batch-94
Running loss of epoch-2 batch-94 = 0.03506774455308914

Training epoch-2 batch-95
Running loss of epoch-2 batch-95 = 0.034643158316612244

Training epoch-2 batch-96
Running loss of epoch-2 batch-96 = 0.03375338762998581

Training epoch-2 batch-97
Running loss of epoch-2 batch-97 = 0.03475257754325867

Training epoch-2 batch-98
Running loss of epoch-2 batch-98 = 0.03434291481971741

Training epoch-2 batch-99
Running loss of epoch-2 batch-99 = 0.034193459898233414

Training epoch-2 batch-100
Running loss of epoch-2 batch-100 = 0.03381383419036865

Training epoch-2 batch-101
Running loss of epoch-2 batch-101 = 0.03435856103897095

Training epoch-2 batch-102
Running loss of epoch-2 batch-102 = 0.0346580408513546

Training epoch-2 batch-103
Running loss of epoch-2 batch-103 = 0.03406684100627899

Training epoch-2 batch-104
Running loss of epoch-2 batch-104 = 0.03323270007967949

Training epoch-2 batch-105
Running loss of epoch-2 batch-105 = 0.034591760486364365

Training epoch-2 batch-106
Running loss of epoch-2 batch-106 = 0.03476651757955551

Training epoch-2 batch-107
Running loss of epoch-2 batch-107 = 0.034029364585876465

Training epoch-2 batch-108
Running loss of epoch-2 batch-108 = 0.03406253084540367

Training epoch-2 batch-109
Running loss of epoch-2 batch-109 = 0.034060098230838776

Training epoch-2 batch-110
Running loss of epoch-2 batch-110 = 0.035173915326595306

Training epoch-2 batch-111
Running loss of epoch-2 batch-111 = 0.03389248251914978

Training epoch-2 batch-112
Running loss of epoch-2 batch-112 = 0.033078692853450775

Training epoch-2 batch-113
Running loss of epoch-2 batch-113 = 0.03313256427645683

Training epoch-2 batch-114
Running loss of epoch-2 batch-114 = 0.033821191638708115

Training epoch-2 batch-115
Running loss of epoch-2 batch-115 = 0.03361087664961815

Training epoch-2 batch-116
Running loss of epoch-2 batch-116 = 0.03353876620531082

Training epoch-2 batch-117
Running loss of epoch-2 batch-117 = 0.03439495339989662

Training epoch-2 batch-118
Running loss of epoch-2 batch-118 = 0.03342626616358757

Training epoch-2 batch-119
Running loss of epoch-2 batch-119 = 0.03367220610380173

Training epoch-2 batch-120
Running loss of epoch-2 batch-120 = 0.034219805151224136

Training epoch-2 batch-121
Running loss of epoch-2 batch-121 = 0.034935396164655685

Training epoch-2 batch-122
Running loss of epoch-2 batch-122 = 0.03349340707063675

Training epoch-2 batch-123
Running loss of epoch-2 batch-123 = 0.03438248857855797

Training epoch-2 batch-124
Running loss of epoch-2 batch-124 = 0.03324539214372635

Training epoch-2 batch-125
Running loss of epoch-2 batch-125 = 0.034046921879053116

Training epoch-2 batch-126
Running loss of epoch-2 batch-126 = 0.03331110626459122

Training epoch-2 batch-127
Running loss of epoch-2 batch-127 = 0.03391643613576889

Training epoch-2 batch-128
Running loss of epoch-2 batch-128 = 0.033247627317905426

Training epoch-2 batch-129
Running loss of epoch-2 batch-129 = 0.03295770287513733

Training epoch-2 batch-130
Running loss of epoch-2 batch-130 = 0.033489890396595

Training epoch-2 batch-131
Running loss of epoch-2 batch-131 = 0.0335615836083889

Training epoch-2 batch-132
Running loss of epoch-2 batch-132 = 0.033896856009960175

Training epoch-2 batch-133
Running loss of epoch-2 batch-133 = 0.03277377411723137

Training epoch-2 batch-134
Running loss of epoch-2 batch-134 = 0.034006666392087936

Training epoch-2 batch-135
Running loss of epoch-2 batch-135 = 0.03335275501012802

Training epoch-2 batch-136
Running loss of epoch-2 batch-136 = 0.03267228603363037

Training epoch-2 batch-137
Running loss of epoch-2 batch-137 = 0.03269078955054283

Training epoch-2 batch-138
Running loss of epoch-2 batch-138 = 0.03281844034790993

Training epoch-2 batch-139
Running loss of epoch-2 batch-139 = 0.033192627131938934

Training epoch-2 batch-140
Running loss of epoch-2 batch-140 = 0.03389652445912361

Training epoch-2 batch-141
Running loss of epoch-2 batch-141 = 0.033402904868125916

Training epoch-2 batch-142
Running loss of epoch-2 batch-142 = 0.03473949059844017

Training epoch-2 batch-143
Running loss of epoch-2 batch-143 = 0.03213781490921974

Training epoch-2 batch-144
Running loss of epoch-2 batch-144 = 0.033447884023189545

Training epoch-2 batch-145
Running loss of epoch-2 batch-145 = 0.0339665561914444

Training epoch-2 batch-146
Running loss of epoch-2 batch-146 = 0.03234037384390831

Training epoch-2 batch-147
Running loss of epoch-2 batch-147 = 0.03327564522624016

Training epoch-2 batch-148
Running loss of epoch-2 batch-148 = 0.032003507018089294

Training epoch-2 batch-149
Running loss of epoch-2 batch-149 = 0.03333286568522453

Training epoch-2 batch-150
Running loss of epoch-2 batch-150 = 0.03365353122353554

Training epoch-2 batch-151
Running loss of epoch-2 batch-151 = 0.033148832619190216

Training epoch-2 batch-152
Running loss of epoch-2 batch-152 = 0.03188811242580414

Training epoch-2 batch-153
Running loss of epoch-2 batch-153 = 0.03256397321820259

Training epoch-2 batch-154
Running loss of epoch-2 batch-154 = 0.031209547072649002

Training epoch-2 batch-155
Running loss of epoch-2 batch-155 = 0.031902458518743515

Training epoch-2 batch-156
Running loss of epoch-2 batch-156 = 0.0323377400636673

Training epoch-2 batch-157
Running loss of epoch-2 batch-157 = 0.13083644211292267

Finished training epoch-2.



Average train loss at epoch-2 = 0.03464215590953827

Started Evaluation

Average val loss at epoch-2 = 2.0617530534141943

Accuracy for classes:
Accuracy for class equals is: 68.32 %
Accuracy for class main is: 28.52 %
Accuracy for class setUp is: 37.21 %
Accuracy for class onCreate is: 32.62 %
Accuracy for class toString is: 8.53 %
Accuracy for class run is: 0.00 %
Accuracy for class hashCode is: 77.15 %
Accuracy for class init is: 7.40 %
Accuracy for class execute is: 14.46 %
Accuracy for class get is: 3.59 %

Overall Accuracy = 29.61 %


Best Accuracy = 29.61 % at Epoch-2
Saving model after best epoch-2

Finished Evaluation



Started training epoch-3


Training epoch-3 batch-1
Running loss of epoch-3 batch-1 = 0.03234672546386719

Training epoch-3 batch-2
Running loss of epoch-3 batch-2 = 0.031721409410238266

Training epoch-3 batch-3
Running loss of epoch-3 batch-3 = 0.03177082911133766

Training epoch-3 batch-4
Running loss of epoch-3 batch-4 = 0.03306550905108452

Training epoch-3 batch-5
Running loss of epoch-3 batch-5 = 0.033925388008356094

Training epoch-3 batch-6
Running loss of epoch-3 batch-6 = 0.03177574276924133

Training epoch-3 batch-7
Running loss of epoch-3 batch-7 = 0.032580096274614334

Training epoch-3 batch-8
Running loss of epoch-3 batch-8 = 0.03134831041097641

Training epoch-3 batch-9
Running loss of epoch-3 batch-9 = 0.03255292773246765

Training epoch-3 batch-10
Running loss of epoch-3 batch-10 = 0.03221181035041809

Training epoch-3 batch-11
Running loss of epoch-3 batch-11 = 0.03136839717626572

Training epoch-3 batch-12
Running loss of epoch-3 batch-12 = 0.03205840289592743

Training epoch-3 batch-13
Running loss of epoch-3 batch-13 = 0.03228775039315224

Training epoch-3 batch-14
Running loss of epoch-3 batch-14 = 0.0317651703953743

Training epoch-3 batch-15
Running loss of epoch-3 batch-15 = 0.03228391334414482

Training epoch-3 batch-16
Running loss of epoch-3 batch-16 = 0.031129255890846252

Training epoch-3 batch-17
Running loss of epoch-3 batch-17 = 0.03032010607421398

Training epoch-3 batch-18
Running loss of epoch-3 batch-18 = 0.03171343728899956

Training epoch-3 batch-19
Running loss of epoch-3 batch-19 = 0.03126712515950203

Training epoch-3 batch-20
Running loss of epoch-3 batch-20 = 0.031172052025794983

Training epoch-3 batch-21
Running loss of epoch-3 batch-21 = 0.03293808922171593

Training epoch-3 batch-22
Running loss of epoch-3 batch-22 = 0.031492650508880615

Training epoch-3 batch-23
Running loss of epoch-3 batch-23 = 0.029958155006170273

Training epoch-3 batch-24
Running loss of epoch-3 batch-24 = 0.03373878449201584

Training epoch-3 batch-25
Running loss of epoch-3 batch-25 = 0.030951809138059616

Training epoch-3 batch-26
Running loss of epoch-3 batch-26 = 0.03145059943199158

Training epoch-3 batch-27
Running loss of epoch-3 batch-27 = 0.03154699131846428

Training epoch-3 batch-28
Running loss of epoch-3 batch-28 = 0.0329894982278347

Training epoch-3 batch-29
Running loss of epoch-3 batch-29 = 0.03377704322338104

Training epoch-3 batch-30
Running loss of epoch-3 batch-30 = 0.03151972219347954

Training epoch-3 batch-31
Running loss of epoch-3 batch-31 = 0.03102141059935093

Training epoch-3 batch-32
Running loss of epoch-3 batch-32 = 0.032233938574790955

Training epoch-3 batch-33
Running loss of epoch-3 batch-33 = 0.029318900778889656

Training epoch-3 batch-34
Running loss of epoch-3 batch-34 = 0.029925929382443428

Training epoch-3 batch-35
Running loss of epoch-3 batch-35 = 0.03128574416041374

Training epoch-3 batch-36
Running loss of epoch-3 batch-36 = 0.0327456071972847

Training epoch-3 batch-37
Running loss of epoch-3 batch-37 = 0.0314355343580246

Training epoch-3 batch-38
Running loss of epoch-3 batch-38 = 0.030538298189640045

Training epoch-3 batch-39
Running loss of epoch-3 batch-39 = 0.03276142105460167

Training epoch-3 batch-40
Running loss of epoch-3 batch-40 = 0.030128315091133118

Training epoch-3 batch-41
Running loss of epoch-3 batch-41 = 0.03089405782520771

Training epoch-3 batch-42
Running loss of epoch-3 batch-42 = 0.032369378954172134

Training epoch-3 batch-43
Running loss of epoch-3 batch-43 = 0.029871448874473572

Training epoch-3 batch-44
Running loss of epoch-3 batch-44 = 0.0294497050344944

Training epoch-3 batch-45
Running loss of epoch-3 batch-45 = 0.029426898807287216

Training epoch-3 batch-46
Running loss of epoch-3 batch-46 = 0.030699141323566437

Training epoch-3 batch-47
Running loss of epoch-3 batch-47 = 0.031017186120152473

Training epoch-3 batch-48
Running loss of epoch-3 batch-48 = 0.02882988750934601

Training epoch-3 batch-49
Running loss of epoch-3 batch-49 = 0.029616303741931915

Training epoch-3 batch-50
Running loss of epoch-3 batch-50 = 0.031999245285987854

Training epoch-3 batch-51
Running loss of epoch-3 batch-51 = 0.029761815443634987

Training epoch-3 batch-52
Running loss of epoch-3 batch-52 = 0.03201662376523018

Training epoch-3 batch-53
Running loss of epoch-3 batch-53 = 0.03232704848051071

Training epoch-3 batch-54
Running loss of epoch-3 batch-54 = 0.029778113588690758

Training epoch-3 batch-55
Running loss of epoch-3 batch-55 = 0.029281090945005417

Training epoch-3 batch-56
Running loss of epoch-3 batch-56 = 0.029640015214681625

Training epoch-3 batch-57
Running loss of epoch-3 batch-57 = 0.02928367629647255

Training epoch-3 batch-58
Running loss of epoch-3 batch-58 = 0.030080020427703857

Training epoch-3 batch-59
Running loss of epoch-3 batch-59 = 0.029319608584046364

Training epoch-3 batch-60
Running loss of epoch-3 batch-60 = 0.029464738443493843

Training epoch-3 batch-61
Running loss of epoch-3 batch-61 = 0.03132900968194008

Training epoch-3 batch-62
Running loss of epoch-3 batch-62 = 0.028765197843313217

Training epoch-3 batch-63
Running loss of epoch-3 batch-63 = 0.03260309621691704

Training epoch-3 batch-64
Running loss of epoch-3 batch-64 = 0.030656853690743446

Training epoch-3 batch-65
Running loss of epoch-3 batch-65 = 0.03180205449461937

Training epoch-3 batch-66
Running loss of epoch-3 batch-66 = 0.030056064948439598

Training epoch-3 batch-67
Running loss of epoch-3 batch-67 = 0.031095081940293312

Training epoch-3 batch-68
Running loss of epoch-3 batch-68 = 0.032243143767118454

Training epoch-3 batch-69
Running loss of epoch-3 batch-69 = 0.03157414123415947

Training epoch-3 batch-70
Running loss of epoch-3 batch-70 = 0.030477803200483322

Training epoch-3 batch-71
Running loss of epoch-3 batch-71 = 0.032427843660116196

Training epoch-3 batch-72
Running loss of epoch-3 batch-72 = 0.03161408752202988

Training epoch-3 batch-73
Running loss of epoch-3 batch-73 = 0.030130181461572647

Training epoch-3 batch-74
Running loss of epoch-3 batch-74 = 0.028105564415454865

Training epoch-3 batch-75
Running loss of epoch-3 batch-75 = 0.030149295926094055

Training epoch-3 batch-76
Running loss of epoch-3 batch-76 = 0.03053884394466877

Training epoch-3 batch-77
Running loss of epoch-3 batch-77 = 0.029885049909353256

Training epoch-3 batch-78
Running loss of epoch-3 batch-78 = 0.03157326579093933

Training epoch-3 batch-79
Running loss of epoch-3 batch-79 = 0.028638698160648346

Training epoch-3 batch-80
Running loss of epoch-3 batch-80 = 0.031173130497336388

Training epoch-3 batch-81
Running loss of epoch-3 batch-81 = 0.029153233394026756

Training epoch-3 batch-82
Running loss of epoch-3 batch-82 = 0.02973984368145466

Training epoch-3 batch-83
Running loss of epoch-3 batch-83 = 0.031228993088006973

Training epoch-3 batch-84
Running loss of epoch-3 batch-84 = 0.03110617771744728

Training epoch-3 batch-85
Running loss of epoch-3 batch-85 = 0.029984809458255768

Training epoch-3 batch-86
Running loss of epoch-3 batch-86 = 0.030169494450092316

Training epoch-3 batch-87
Running loss of epoch-3 batch-87 = 0.030576763674616814

Training epoch-3 batch-88
Running loss of epoch-3 batch-88 = 0.030860869213938713

Training epoch-3 batch-89
Running loss of epoch-3 batch-89 = 0.03124876320362091

Training epoch-3 batch-90
Running loss of epoch-3 batch-90 = 0.027711471542716026

Training epoch-3 batch-91
Running loss of epoch-3 batch-91 = 0.029628058895468712

Training epoch-3 batch-92
Running loss of epoch-3 batch-92 = 0.03232676535844803

Training epoch-3 batch-93
Running loss of epoch-3 batch-93 = 0.03069560043513775

Training epoch-3 batch-94
Running loss of epoch-3 batch-94 = 0.028271501883864403

Training epoch-3 batch-95
Running loss of epoch-3 batch-95 = 0.03093894198536873

Training epoch-3 batch-96
Running loss of epoch-3 batch-96 = 0.028159935027360916

Training epoch-3 batch-97
Running loss of epoch-3 batch-97 = 0.032688140869140625

Training epoch-3 batch-98
Running loss of epoch-3 batch-98 = 0.030959807336330414

Training epoch-3 batch-99
Running loss of epoch-3 batch-99 = 0.02957114763557911

Training epoch-3 batch-100
Running loss of epoch-3 batch-100 = 0.027197083458304405

Training epoch-3 batch-101
Running loss of epoch-3 batch-101 = 0.02641366980969906

Training epoch-3 batch-102
Running loss of epoch-3 batch-102 = 0.030321326106786728

Training epoch-3 batch-103
Running loss of epoch-3 batch-103 = 0.02753998339176178

Training epoch-3 batch-104
Running loss of epoch-3 batch-104 = 0.02903071790933609

Training epoch-3 batch-105
Running loss of epoch-3 batch-105 = 0.02981451153755188

Training epoch-3 batch-106
Running loss of epoch-3 batch-106 = 0.029088284820318222

Training epoch-3 batch-107
Running loss of epoch-3 batch-107 = 0.02964484691619873

Training epoch-3 batch-108
Running loss of epoch-3 batch-108 = 0.028968418017029762

Training epoch-3 batch-109
Running loss of epoch-3 batch-109 = 0.028190217912197113

Training epoch-3 batch-110
Running loss of epoch-3 batch-110 = 0.03046964667737484

Training epoch-3 batch-111
Running loss of epoch-3 batch-111 = 0.030930498614907265

Training epoch-3 batch-112
Running loss of epoch-3 batch-112 = 0.025716619566082954

Training epoch-3 batch-113
Running loss of epoch-3 batch-113 = 0.026258638128638268

Training epoch-3 batch-114
Running loss of epoch-3 batch-114 = 0.026207970455288887

Training epoch-3 batch-115
Running loss of epoch-3 batch-115 = 0.02616368979215622

Training epoch-3 batch-116
Running loss of epoch-3 batch-116 = 0.02766108512878418

Training epoch-3 batch-117
Running loss of epoch-3 batch-117 = 0.032653894275426865

Training epoch-3 batch-118
Running loss of epoch-3 batch-118 = 0.027899906039237976

Training epoch-3 batch-119
Running loss of epoch-3 batch-119 = 0.03153267130255699

Training epoch-3 batch-120
Running loss of epoch-3 batch-120 = 0.02815326675772667

Training epoch-3 batch-121
Running loss of epoch-3 batch-121 = 0.028163712471723557

Training epoch-3 batch-122
Running loss of epoch-3 batch-122 = 0.028898335993289948

Training epoch-3 batch-123
Running loss of epoch-3 batch-123 = 0.027403535321354866

Training epoch-3 batch-124
Running loss of epoch-3 batch-124 = 0.024550074711441994

Training epoch-3 batch-125
Running loss of epoch-3 batch-125 = 0.030208392068743706

Training epoch-3 batch-126
Running loss of epoch-3 batch-126 = 0.027953429147601128

Training epoch-3 batch-127
Running loss of epoch-3 batch-127 = 0.024866916239261627

Training epoch-3 batch-128
Running loss of epoch-3 batch-128 = 0.030416999012231827

Training epoch-3 batch-129
Running loss of epoch-3 batch-129 = 0.027687357738614082

Training epoch-3 batch-130
Running loss of epoch-3 batch-130 = 0.028617355972528458

Training epoch-3 batch-131
Running loss of epoch-3 batch-131 = 0.028572652488946915

Training epoch-3 batch-132
Running loss of epoch-3 batch-132 = 0.02809632569551468

Training epoch-3 batch-133
Running loss of epoch-3 batch-133 = 0.02716149017214775

Training epoch-3 batch-134
Running loss of epoch-3 batch-134 = 0.02916605770587921

Training epoch-3 batch-135
Running loss of epoch-3 batch-135 = 0.0302827637642622

Training epoch-3 batch-136
Running loss of epoch-3 batch-136 = 0.028914574533700943

Training epoch-3 batch-137
Running loss of epoch-3 batch-137 = 0.030590461567044258

Training epoch-3 batch-138
Running loss of epoch-3 batch-138 = 0.027907760813832283

Training epoch-3 batch-139
Running loss of epoch-3 batch-139 = 0.02931966260075569

Training epoch-3 batch-140
Running loss of epoch-3 batch-140 = 0.024951787665486336

Training epoch-3 batch-141
Running loss of epoch-3 batch-141 = 0.027238754555583

Training epoch-3 batch-142
Running loss of epoch-3 batch-142 = 0.027559833601117134

Training epoch-3 batch-143
Running loss of epoch-3 batch-143 = 0.027293400838971138

Training epoch-3 batch-144
Running loss of epoch-3 batch-144 = 0.02948283962905407

Training epoch-3 batch-145
Running loss of epoch-3 batch-145 = 0.025986123830080032

Training epoch-3 batch-146
Running loss of epoch-3 batch-146 = 0.028682172298431396

Training epoch-3 batch-147
Running loss of epoch-3 batch-147 = 0.026034601032733917

Training epoch-3 batch-148
Running loss of epoch-3 batch-148 = 0.02774869091808796

Training epoch-3 batch-149
Running loss of epoch-3 batch-149 = 0.02826083078980446

Training epoch-3 batch-150
Running loss of epoch-3 batch-150 = 0.027323288843035698

Training epoch-3 batch-151
Running loss of epoch-3 batch-151 = 0.02815937250852585

Training epoch-3 batch-152
Running loss of epoch-3 batch-152 = 0.027055270969867706

Training epoch-3 batch-153
Running loss of epoch-3 batch-153 = 0.027318287640810013

Training epoch-3 batch-154
Running loss of epoch-3 batch-154 = 0.026566840708255768

Training epoch-3 batch-155
Running loss of epoch-3 batch-155 = 0.025617321953177452

Training epoch-3 batch-156
Running loss of epoch-3 batch-156 = 0.02342088520526886

Training epoch-3 batch-157
Running loss of epoch-3 batch-157 = 0.09737920761108398

Finished training epoch-3.



Average train loss at epoch-3 = 0.029997359633445738

Started Evaluation

Average val loss at epoch-3 = 1.7041129924749072

Accuracy for classes:
Accuracy for class equals is: 82.34 %
Accuracy for class main is: 48.36 %
Accuracy for class setUp is: 58.03 %
Accuracy for class onCreate is: 29.53 %
Accuracy for class toString is: 13.31 %
Accuracy for class run is: 21.00 %
Accuracy for class hashCode is: 78.65 %
Accuracy for class init is: 6.50 %
Accuracy for class execute is: 2.41 %
Accuracy for class get is: 17.18 %

Overall Accuracy = 38.54 %


Best Accuracy = 38.54 % at Epoch-3
Saving model after best epoch-3

Finished Evaluation



Started training epoch-4


Training epoch-4 batch-1
Running loss of epoch-4 batch-1 = 0.028472360223531723

Training epoch-4 batch-2
Running loss of epoch-4 batch-2 = 0.02579539269208908

Training epoch-4 batch-3
Running loss of epoch-4 batch-3 = 0.027701428160071373

Training epoch-4 batch-4
Running loss of epoch-4 batch-4 = 0.024148261174559593

Training epoch-4 batch-5
Running loss of epoch-4 batch-5 = 0.026107436046004295

Training epoch-4 batch-6
Running loss of epoch-4 batch-6 = 0.025893939658999443

Training epoch-4 batch-7
Running loss of epoch-4 batch-7 = 0.02750932238996029

Training epoch-4 batch-8
Running loss of epoch-4 batch-8 = 0.025339482352137566

Training epoch-4 batch-9
Running loss of epoch-4 batch-9 = 0.026544366031885147

Training epoch-4 batch-10
Running loss of epoch-4 batch-10 = 0.026280682533979416

Training epoch-4 batch-11
Running loss of epoch-4 batch-11 = 0.02415456250309944

Training epoch-4 batch-12
Running loss of epoch-4 batch-12 = 0.029387306421995163

Training epoch-4 batch-13
Running loss of epoch-4 batch-13 = 0.026069995015859604

Training epoch-4 batch-14
Running loss of epoch-4 batch-14 = 0.026951681822538376

Training epoch-4 batch-15
Running loss of epoch-4 batch-15 = 0.02588176168501377

Training epoch-4 batch-16
Running loss of epoch-4 batch-16 = 0.02510884404182434

Training epoch-4 batch-17
Running loss of epoch-4 batch-17 = 0.027515005320310593

Training epoch-4 batch-18
Running loss of epoch-4 batch-18 = 0.027928752824664116

Training epoch-4 batch-19
Running loss of epoch-4 batch-19 = 0.026553776115179062

Training epoch-4 batch-20
Running loss of epoch-4 batch-20 = 0.027568941935896873

Training epoch-4 batch-21
Running loss of epoch-4 batch-21 = 0.024546392261981964

Training epoch-4 batch-22
Running loss of epoch-4 batch-22 = 0.023711975663900375

Training epoch-4 batch-23
Running loss of epoch-4 batch-23 = 0.02779180184006691

Training epoch-4 batch-24
Running loss of epoch-4 batch-24 = 0.026371615007519722

Training epoch-4 batch-25
Running loss of epoch-4 batch-25 = 0.027935916557908058

Training epoch-4 batch-26
Running loss of epoch-4 batch-26 = 0.023875420913100243

Training epoch-4 batch-27
Running loss of epoch-4 batch-27 = 0.025081122294068336

Training epoch-4 batch-28
Running loss of epoch-4 batch-28 = 0.025394994765520096

Training epoch-4 batch-29
Running loss of epoch-4 batch-29 = 0.025138305500149727

Training epoch-4 batch-30
Running loss of epoch-4 batch-30 = 0.026304306462407112

Training epoch-4 batch-31
Running loss of epoch-4 batch-31 = 0.024810517206788063

Training epoch-4 batch-32
Running loss of epoch-4 batch-32 = 0.02563503012061119

Training epoch-4 batch-33
Running loss of epoch-4 batch-33 = 0.02473471872508526

Training epoch-4 batch-34
Running loss of epoch-4 batch-34 = 0.02710934542119503

Training epoch-4 batch-35
Running loss of epoch-4 batch-35 = 0.02747700735926628

Training epoch-4 batch-36
Running loss of epoch-4 batch-36 = 0.02655663900077343

Training epoch-4 batch-37
Running loss of epoch-4 batch-37 = 0.0238101314753294

Training epoch-4 batch-38
Running loss of epoch-4 batch-38 = 0.02410435490310192

Training epoch-4 batch-39
Running loss of epoch-4 batch-39 = 0.02623417228460312

Training epoch-4 batch-40
Running loss of epoch-4 batch-40 = 0.02439858391880989

Training epoch-4 batch-41
Running loss of epoch-4 batch-41 = 0.025301702320575714

Training epoch-4 batch-42
Running loss of epoch-4 batch-42 = 0.023032991215586662

Training epoch-4 batch-43
Running loss of epoch-4 batch-43 = 0.020927120000123978

Training epoch-4 batch-44
Running loss of epoch-4 batch-44 = 0.02710961364209652

Training epoch-4 batch-45
Running loss of epoch-4 batch-45 = 0.026022151112556458

Training epoch-4 batch-46
Running loss of epoch-4 batch-46 = 0.02570769563317299

Training epoch-4 batch-47
Running loss of epoch-4 batch-47 = 0.02739940769970417

Training epoch-4 batch-48
Running loss of epoch-4 batch-48 = 0.026586130261421204

Training epoch-4 batch-49
Running loss of epoch-4 batch-49 = 0.024032216519117355

Training epoch-4 batch-50
Running loss of epoch-4 batch-50 = 0.021088732406497

Training epoch-4 batch-51
Running loss of epoch-4 batch-51 = 0.026788121089339256

Training epoch-4 batch-52
Running loss of epoch-4 batch-52 = 0.024166982620954514

Training epoch-4 batch-53
Running loss of epoch-4 batch-53 = 0.02532772161066532

Training epoch-4 batch-54
Running loss of epoch-4 batch-54 = 0.02449597790837288

Training epoch-4 batch-55
Running loss of epoch-4 batch-55 = 0.027315255254507065

Training epoch-4 batch-56
Running loss of epoch-4 batch-56 = 0.023970527574419975

Training epoch-4 batch-57
Running loss of epoch-4 batch-57 = 0.028363006189465523

Training epoch-4 batch-58
Running loss of epoch-4 batch-58 = 0.023330040276050568

Training epoch-4 batch-59
Running loss of epoch-4 batch-59 = 0.02988460659980774

Training epoch-4 batch-60
Running loss of epoch-4 batch-60 = 0.02719666250050068

Training epoch-4 batch-61
Running loss of epoch-4 batch-61 = 0.024720562621951103

Training epoch-4 batch-62
Running loss of epoch-4 batch-62 = 0.025064842775464058

Training epoch-4 batch-63
Running loss of epoch-4 batch-63 = 0.020085571333765984

Training epoch-4 batch-64
Running loss of epoch-4 batch-64 = 0.025732193142175674

Training epoch-4 batch-65
Running loss of epoch-4 batch-65 = 0.02288646623492241

Training epoch-4 batch-66
Running loss of epoch-4 batch-66 = 0.024233724921941757

Training epoch-4 batch-67
Running loss of epoch-4 batch-67 = 0.02543976716697216

Training epoch-4 batch-68
Running loss of epoch-4 batch-68 = 0.02430671826004982

Training epoch-4 batch-69
Running loss of epoch-4 batch-69 = 0.027954379096627235

Training epoch-4 batch-70
Running loss of epoch-4 batch-70 = 0.023735743016004562

Training epoch-4 batch-71
Running loss of epoch-4 batch-71 = 0.025215143337845802

Training epoch-4 batch-72
Running loss of epoch-4 batch-72 = 0.027441320940852165

Training epoch-4 batch-73
Running loss of epoch-4 batch-73 = 0.02219737321138382

Training epoch-4 batch-74
Running loss of epoch-4 batch-74 = 0.02770157903432846

Training epoch-4 batch-75
Running loss of epoch-4 batch-75 = 0.026277918368577957

Training epoch-4 batch-76
Running loss of epoch-4 batch-76 = 0.025014609098434448

Training epoch-4 batch-77
Running loss of epoch-4 batch-77 = 0.023519955575466156

Training epoch-4 batch-78
Running loss of epoch-4 batch-78 = 0.02352028898894787

Training epoch-4 batch-79
Running loss of epoch-4 batch-79 = 0.024135081097483635

Training epoch-4 batch-80
Running loss of epoch-4 batch-80 = 0.027014069259166718

Training epoch-4 batch-81
Running loss of epoch-4 batch-81 = 0.025020800530910492

Training epoch-4 batch-82
Running loss of epoch-4 batch-82 = 0.021583158522844315

Training epoch-4 batch-83
Running loss of epoch-4 batch-83 = 0.024849386885762215

Training epoch-4 batch-84
Running loss of epoch-4 batch-84 = 0.02273792214691639

Training epoch-4 batch-85
Running loss of epoch-4 batch-85 = 0.024988140910863876

Training epoch-4 batch-86
Running loss of epoch-4 batch-86 = 0.024991707876324654

Training epoch-4 batch-87
Running loss of epoch-4 batch-87 = 0.025760294869542122

Training epoch-4 batch-88
Running loss of epoch-4 batch-88 = 0.02352026291191578

Training epoch-4 batch-89
Running loss of epoch-4 batch-89 = 0.02443920262157917

Training epoch-4 batch-90
Running loss of epoch-4 batch-90 = 0.026082394644618034

Training epoch-4 batch-91
Running loss of epoch-4 batch-91 = 0.023914705961942673

Training epoch-4 batch-92
Running loss of epoch-4 batch-92 = 0.023225292563438416

Training epoch-4 batch-93
Running loss of epoch-4 batch-93 = 0.02391192317008972

Training epoch-4 batch-94
Running loss of epoch-4 batch-94 = 0.025175124406814575

Training epoch-4 batch-95
Running loss of epoch-4 batch-95 = 0.02569231390953064

Training epoch-4 batch-96
Running loss of epoch-4 batch-96 = 0.02626781351864338

Training epoch-4 batch-97
Running loss of epoch-4 batch-97 = 0.02482897974550724

Training epoch-4 batch-98
Running loss of epoch-4 batch-98 = 0.024631183594465256

Training epoch-4 batch-99
Running loss of epoch-4 batch-99 = 0.025074221193790436

Training epoch-4 batch-100
Running loss of epoch-4 batch-100 = 0.020629409700632095

Training epoch-4 batch-101
Running loss of epoch-4 batch-101 = 0.02609570510685444

Training epoch-4 batch-102
Running loss of epoch-4 batch-102 = 0.024847310036420822

Training epoch-4 batch-103
Running loss of epoch-4 batch-103 = 0.02598470076918602

Training epoch-4 batch-104
Running loss of epoch-4 batch-104 = 0.02558663673698902

Training epoch-4 batch-105
Running loss of epoch-4 batch-105 = 0.02504534274339676

Training epoch-4 batch-106
Running loss of epoch-4 batch-106 = 0.02663666009902954

Training epoch-4 batch-107
Running loss of epoch-4 batch-107 = 0.026782860979437828

Training epoch-4 batch-108
Running loss of epoch-4 batch-108 = 0.025947993621230125

Training epoch-4 batch-109
Running loss of epoch-4 batch-109 = 0.022477956488728523

Training epoch-4 batch-110
Running loss of epoch-4 batch-110 = 0.022576021030545235

Training epoch-4 batch-111
Running loss of epoch-4 batch-111 = 0.024569639936089516

Training epoch-4 batch-112
Running loss of epoch-4 batch-112 = 0.022769711911678314

Training epoch-4 batch-113
Running loss of epoch-4 batch-113 = 0.023276610299944878

Training epoch-4 batch-114
Running loss of epoch-4 batch-114 = 0.02668895572423935

Training epoch-4 batch-115
Running loss of epoch-4 batch-115 = 0.02227046899497509

Training epoch-4 batch-116
Running loss of epoch-4 batch-116 = 0.025279304012656212

Training epoch-4 batch-117
Running loss of epoch-4 batch-117 = 0.026495106518268585

Training epoch-4 batch-118
Running loss of epoch-4 batch-118 = 0.025647280737757683

Training epoch-4 batch-119
Running loss of epoch-4 batch-119 = 0.020604360848665237

Training epoch-4 batch-120
Running loss of epoch-4 batch-120 = 0.02423248626291752

Training epoch-4 batch-121
Running loss of epoch-4 batch-121 = 0.024229906499385834

Training epoch-4 batch-122
Running loss of epoch-4 batch-122 = 0.02621227130293846

Training epoch-4 batch-123
Running loss of epoch-4 batch-123 = 0.022607803344726562

Training epoch-4 batch-124
Running loss of epoch-4 batch-124 = 0.027361605316400528

Training epoch-4 batch-125
Running loss of epoch-4 batch-125 = 0.020698685199022293

Training epoch-4 batch-126
Running loss of epoch-4 batch-126 = 0.024186622351408005

Training epoch-4 batch-127
Running loss of epoch-4 batch-127 = 0.023078838363289833

Training epoch-4 batch-128
Running loss of epoch-4 batch-128 = 0.024205682799220085

Training epoch-4 batch-129
Running loss of epoch-4 batch-129 = 0.025207972154021263

Training epoch-4 batch-130
Running loss of epoch-4 batch-130 = 0.024847373366355896

Training epoch-4 batch-131
Running loss of epoch-4 batch-131 = 0.022735606878995895

Training epoch-4 batch-132
Running loss of epoch-4 batch-132 = 0.021457500755786896

Training epoch-4 batch-133
Running loss of epoch-4 batch-133 = 0.02340874820947647

Training epoch-4 batch-134
Running loss of epoch-4 batch-134 = 0.023701895028352737

Training epoch-4 batch-135
Running loss of epoch-4 batch-135 = 0.023511121049523354

Training epoch-4 batch-136
Running loss of epoch-4 batch-136 = 0.025258731096982956

Training epoch-4 batch-137
Running loss of epoch-4 batch-137 = 0.025755662471055984

Training epoch-4 batch-138
Running loss of epoch-4 batch-138 = 0.024796221405267715

Training epoch-4 batch-139
Running loss of epoch-4 batch-139 = 0.025520309805870056

Training epoch-4 batch-140
Running loss of epoch-4 batch-140 = 0.02730085514485836

Training epoch-4 batch-141
Running loss of epoch-4 batch-141 = 0.024958178400993347

Training epoch-4 batch-142
Running loss of epoch-4 batch-142 = 0.02682189829647541

Training epoch-4 batch-143
Running loss of epoch-4 batch-143 = 0.02550269104540348

Training epoch-4 batch-144
Running loss of epoch-4 batch-144 = 0.024921920150518417

Training epoch-4 batch-145
Running loss of epoch-4 batch-145 = 0.023697949945926666

Training epoch-4 batch-146
Running loss of epoch-4 batch-146 = 0.02165987156331539

Training epoch-4 batch-147
Running loss of epoch-4 batch-147 = 0.02035890705883503

Training epoch-4 batch-148
Running loss of epoch-4 batch-148 = 0.024694273248314857

Training epoch-4 batch-149
Running loss of epoch-4 batch-149 = 0.024246821179986

Training epoch-4 batch-150
Running loss of epoch-4 batch-150 = 0.027004491537809372

Training epoch-4 batch-151
Running loss of epoch-4 batch-151 = 0.02198728919029236

Training epoch-4 batch-152
Running loss of epoch-4 batch-152 = 0.021719537675380707

Training epoch-4 batch-153
Running loss of epoch-4 batch-153 = 0.023476168513298035

Training epoch-4 batch-154
Running loss of epoch-4 batch-154 = 0.02779773250222206

Training epoch-4 batch-155
Running loss of epoch-4 batch-155 = 0.024363970384001732

Training epoch-4 batch-156
Running loss of epoch-4 batch-156 = 0.021587541326880455

Training epoch-4 batch-157
Running loss of epoch-4 batch-157 = 0.09982160478830338

Finished training epoch-4.



Average train loss at epoch-4 = 0.025108204984664918

Started Evaluation

Average val loss at epoch-4 = 1.4830587380810787

Accuracy for classes:
Accuracy for class equals is: 69.64 %
Accuracy for class main is: 75.74 %
Accuracy for class setUp is: 58.69 %
Accuracy for class onCreate is: 58.64 %
Accuracy for class toString is: 39.59 %
Accuracy for class run is: 20.78 %
Accuracy for class hashCode is: 72.28 %
Accuracy for class init is: 14.35 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 26.67 %

Overall Accuracy = 50.11 %


Best Accuracy = 50.11 % at Epoch-4
Saving model after best epoch-4

Finished Evaluation



Started training epoch-5


Training epoch-5 batch-1
Running loss of epoch-5 batch-1 = 0.018360234797000885

Training epoch-5 batch-2
Running loss of epoch-5 batch-2 = 0.020453112199902534

Training epoch-5 batch-3
Running loss of epoch-5 batch-3 = 0.02274361066520214

Training epoch-5 batch-4
Running loss of epoch-5 batch-4 = 0.02210504561662674

Training epoch-5 batch-5
Running loss of epoch-5 batch-5 = 0.023328380659222603

Training epoch-5 batch-6
Running loss of epoch-5 batch-6 = 0.02235964499413967

Training epoch-5 batch-7
Running loss of epoch-5 batch-7 = 0.01920693926513195

Training epoch-5 batch-8
Running loss of epoch-5 batch-8 = 0.02093411609530449

Training epoch-5 batch-9
Running loss of epoch-5 batch-9 = 0.017021991312503815

Training epoch-5 batch-10
Running loss of epoch-5 batch-10 = 0.020406316965818405

Training epoch-5 batch-11
Running loss of epoch-5 batch-11 = 0.023906126618385315

Training epoch-5 batch-12
Running loss of epoch-5 batch-12 = 0.021106140688061714

Training epoch-5 batch-13
Running loss of epoch-5 batch-13 = 0.019944777712225914

Training epoch-5 batch-14
Running loss of epoch-5 batch-14 = 0.018714990466833115

Training epoch-5 batch-15
Running loss of epoch-5 batch-15 = 0.021510150283575058

Training epoch-5 batch-16
Running loss of epoch-5 batch-16 = 0.0212138332426548

Training epoch-5 batch-17
Running loss of epoch-5 batch-17 = 0.01879129931330681

Training epoch-5 batch-18
Running loss of epoch-5 batch-18 = 0.019255591556429863

Training epoch-5 batch-19
Running loss of epoch-5 batch-19 = 0.018300427123904228

Training epoch-5 batch-20
Running loss of epoch-5 batch-20 = 0.018934551626443863

Training epoch-5 batch-21
Running loss of epoch-5 batch-21 = 0.01875890977680683

Training epoch-5 batch-22
Running loss of epoch-5 batch-22 = 0.027174146845936775

Training epoch-5 batch-23
Running loss of epoch-5 batch-23 = 0.018767014145851135

Training epoch-5 batch-24
Running loss of epoch-5 batch-24 = 0.020823132246732712

Training epoch-5 batch-25
Running loss of epoch-5 batch-25 = 0.021858958527445793

Training epoch-5 batch-26
Running loss of epoch-5 batch-26 = 0.01814533770084381

Training epoch-5 batch-27
Running loss of epoch-5 batch-27 = 0.024870499968528748

Training epoch-5 batch-28
Running loss of epoch-5 batch-28 = 0.022318711504340172

Training epoch-5 batch-29
Running loss of epoch-5 batch-29 = 0.023686256259679794

Training epoch-5 batch-30
Running loss of epoch-5 batch-30 = 0.02192589081823826

Training epoch-5 batch-31
Running loss of epoch-5 batch-31 = 0.020106498152017593

Training epoch-5 batch-32
Running loss of epoch-5 batch-32 = 0.020786968991160393

Training epoch-5 batch-33
Running loss of epoch-5 batch-33 = 0.01948198489844799

Training epoch-5 batch-34
Running loss of epoch-5 batch-34 = 0.019506022334098816

Training epoch-5 batch-35
Running loss of epoch-5 batch-35 = 0.020340366289019585

Training epoch-5 batch-36
Running loss of epoch-5 batch-36 = 0.017472781240940094

Training epoch-5 batch-37
Running loss of epoch-5 batch-37 = 0.020269647240638733

Training epoch-5 batch-38
Running loss of epoch-5 batch-38 = 0.019936716184020042

Training epoch-5 batch-39
Running loss of epoch-5 batch-39 = 0.017845893278717995

Training epoch-5 batch-40
Running loss of epoch-5 batch-40 = 0.02149001881480217

Training epoch-5 batch-41
Running loss of epoch-5 batch-41 = 0.02113429270684719

Training epoch-5 batch-42
Running loss of epoch-5 batch-42 = 0.02192562445998192

Training epoch-5 batch-43
Running loss of epoch-5 batch-43 = 0.01964629627764225

Training epoch-5 batch-44
Running loss of epoch-5 batch-44 = 0.02126413583755493

Training epoch-5 batch-45
Running loss of epoch-5 batch-45 = 0.0209641270339489

Training epoch-5 batch-46
Running loss of epoch-5 batch-46 = 0.023541387170553207

Training epoch-5 batch-47
Running loss of epoch-5 batch-47 = 0.020530499517917633

Training epoch-5 batch-48
Running loss of epoch-5 batch-48 = 0.019231142476201057

Training epoch-5 batch-49
Running loss of epoch-5 batch-49 = 0.017775027081370354

Training epoch-5 batch-50
Running loss of epoch-5 batch-50 = 0.02151438407599926

Training epoch-5 batch-51
Running loss of epoch-5 batch-51 = 0.02194095589220524

Training epoch-5 batch-52
Running loss of epoch-5 batch-52 = 0.023671476170420647

Training epoch-5 batch-53
Running loss of epoch-5 batch-53 = 0.020744528621435165

Training epoch-5 batch-54
Running loss of epoch-5 batch-54 = 0.021957896649837494

Training epoch-5 batch-55
Running loss of epoch-5 batch-55 = 0.0245877243578434

Training epoch-5 batch-56
Running loss of epoch-5 batch-56 = 0.021126722916960716

Training epoch-5 batch-57
Running loss of epoch-5 batch-57 = 0.019609035924077034

Training epoch-5 batch-58
Running loss of epoch-5 batch-58 = 0.021694611757993698

Training epoch-5 batch-59
Running loss of epoch-5 batch-59 = 0.019562382251024246

Training epoch-5 batch-60
Running loss of epoch-5 batch-60 = 0.02159746177494526

Training epoch-5 batch-61
Running loss of epoch-5 batch-61 = 0.018971623852849007

Training epoch-5 batch-62
Running loss of epoch-5 batch-62 = 0.02273104153573513

Training epoch-5 batch-63
Running loss of epoch-5 batch-63 = 0.022465556859970093

Training epoch-5 batch-64
Running loss of epoch-5 batch-64 = 0.023400766775012016

Training epoch-5 batch-65
Running loss of epoch-5 batch-65 = 0.023376762866973877

Training epoch-5 batch-66
Running loss of epoch-5 batch-66 = 0.02218458615243435

Training epoch-5 batch-67
Running loss of epoch-5 batch-67 = 0.019586902111768723

Training epoch-5 batch-68
Running loss of epoch-5 batch-68 = 0.02064700238406658

Training epoch-5 batch-69
Running loss of epoch-5 batch-69 = 0.022625621408224106

Training epoch-5 batch-70
Running loss of epoch-5 batch-70 = 0.020290011540055275

Training epoch-5 batch-71
Running loss of epoch-5 batch-71 = 0.02014048397541046

Training epoch-5 batch-72
Running loss of epoch-5 batch-72 = 0.01880916766822338

Training epoch-5 batch-73
Running loss of epoch-5 batch-73 = 0.02112601324915886

Training epoch-5 batch-74
Running loss of epoch-5 batch-74 = 0.018889686092734337

Training epoch-5 batch-75
Running loss of epoch-5 batch-75 = 0.022088801488280296

Training epoch-5 batch-76
Running loss of epoch-5 batch-76 = 0.020774751901626587

Training epoch-5 batch-77
Running loss of epoch-5 batch-77 = 0.021165892481803894

Training epoch-5 batch-78
Running loss of epoch-5 batch-78 = 0.024543525651097298

Training epoch-5 batch-79
Running loss of epoch-5 batch-79 = 0.021082600578665733

Training epoch-5 batch-80
Running loss of epoch-5 batch-80 = 0.018937300890684128

Training epoch-5 batch-81
Running loss of epoch-5 batch-81 = 0.021224791184067726

Training epoch-5 batch-82
Running loss of epoch-5 batch-82 = 0.021153582260012627

Training epoch-5 batch-83
Running loss of epoch-5 batch-83 = 0.019881928339600563

Training epoch-5 batch-84
Running loss of epoch-5 batch-84 = 0.01889532245695591

Training epoch-5 batch-85
Running loss of epoch-5 batch-85 = 0.023507706820964813

Training epoch-5 batch-86
Running loss of epoch-5 batch-86 = 0.02351672574877739

Training epoch-5 batch-87
Running loss of epoch-5 batch-87 = 0.02206077240407467

Training epoch-5 batch-88
Running loss of epoch-5 batch-88 = 0.0189735796302557

Training epoch-5 batch-89
Running loss of epoch-5 batch-89 = 0.019169097766280174

Training epoch-5 batch-90
Running loss of epoch-5 batch-90 = 0.018940888345241547

Training epoch-5 batch-91
Running loss of epoch-5 batch-91 = 0.019648954272270203

Training epoch-5 batch-92
Running loss of epoch-5 batch-92 = 0.01686871238052845

Training epoch-5 batch-93
Running loss of epoch-5 batch-93 = 0.019984595477581024

Training epoch-5 batch-94
Running loss of epoch-5 batch-94 = 0.018415607511997223

Training epoch-5 batch-95
Running loss of epoch-5 batch-95 = 0.02105914242565632

Training epoch-5 batch-96
Running loss of epoch-5 batch-96 = 0.021129092201590538

Training epoch-5 batch-97
Running loss of epoch-5 batch-97 = 0.01627344638109207

Training epoch-5 batch-98
Running loss of epoch-5 batch-98 = 0.023103734478354454

Training epoch-5 batch-99
Running loss of epoch-5 batch-99 = 0.01813051477074623

Training epoch-5 batch-100
Running loss of epoch-5 batch-100 = 0.022545963525772095

Training epoch-5 batch-101
Running loss of epoch-5 batch-101 = 0.02274211123585701

Training epoch-5 batch-102
Running loss of epoch-5 batch-102 = 0.01792704313993454

Training epoch-5 batch-103
Running loss of epoch-5 batch-103 = 0.021922685205936432

Training epoch-5 batch-104
Running loss of epoch-5 batch-104 = 0.01944143883883953

Training epoch-5 batch-105
Running loss of epoch-5 batch-105 = 0.022709853947162628

Training epoch-5 batch-106
Running loss of epoch-5 batch-106 = 0.019818464294075966

Training epoch-5 batch-107
Running loss of epoch-5 batch-107 = 0.020886214450001717

Training epoch-5 batch-108
Running loss of epoch-5 batch-108 = 0.022046877071261406

Training epoch-5 batch-109
Running loss of epoch-5 batch-109 = 0.02068847045302391

Training epoch-5 batch-110
Running loss of epoch-5 batch-110 = 0.02040737308561802

Training epoch-5 batch-111
Running loss of epoch-5 batch-111 = 0.018956495448946953

Training epoch-5 batch-112
Running loss of epoch-5 batch-112 = 0.015726979821920395

Training epoch-5 batch-113
Running loss of epoch-5 batch-113 = 0.024609096348285675

Training epoch-5 batch-114
Running loss of epoch-5 batch-114 = 0.020001569762825966

Training epoch-5 batch-115
Running loss of epoch-5 batch-115 = 0.017877407371997833

Training epoch-5 batch-116
Running loss of epoch-5 batch-116 = 0.022347185760736465

Training epoch-5 batch-117
Running loss of epoch-5 batch-117 = 0.021758688613772392

Training epoch-5 batch-118
Running loss of epoch-5 batch-118 = 0.023971518501639366

Training epoch-5 batch-119
Running loss of epoch-5 batch-119 = 0.01893322914838791

Training epoch-5 batch-120
Running loss of epoch-5 batch-120 = 0.02312573231756687

Training epoch-5 batch-121
Running loss of epoch-5 batch-121 = 0.019351281225681305

Training epoch-5 batch-122
Running loss of epoch-5 batch-122 = 0.020936857908964157

Training epoch-5 batch-123
Running loss of epoch-5 batch-123 = 0.020669886842370033

Training epoch-5 batch-124
Running loss of epoch-5 batch-124 = 0.018682831898331642

Training epoch-5 batch-125
Running loss of epoch-5 batch-125 = 0.02322799526154995

Training epoch-5 batch-126
Running loss of epoch-5 batch-126 = 0.020300034433603287

Training epoch-5 batch-127
Running loss of epoch-5 batch-127 = 0.02040914073586464

Training epoch-5 batch-128
Running loss of epoch-5 batch-128 = 0.026092130690813065

Training epoch-5 batch-129
Running loss of epoch-5 batch-129 = 0.02404644712805748

Training epoch-5 batch-130
Running loss of epoch-5 batch-130 = 0.019876226782798767

Training epoch-5 batch-131
Running loss of epoch-5 batch-131 = 0.022444408386945724

Training epoch-5 batch-132
Running loss of epoch-5 batch-132 = 0.025126146152615547

Training epoch-5 batch-133
Running loss of epoch-5 batch-133 = 0.0193520188331604

Training epoch-5 batch-134
Running loss of epoch-5 batch-134 = 0.020265325903892517

Training epoch-5 batch-135
Running loss of epoch-5 batch-135 = 0.01977691985666752

Training epoch-5 batch-136
Running loss of epoch-5 batch-136 = 0.022533725947141647

Training epoch-5 batch-137
Running loss of epoch-5 batch-137 = 0.019554316997528076

Training epoch-5 batch-138
Running loss of epoch-5 batch-138 = 0.016091419383883476

Training epoch-5 batch-139
Running loss of epoch-5 batch-139 = 0.01965702697634697

Training epoch-5 batch-140
Running loss of epoch-5 batch-140 = 0.018059493973851204

Training epoch-5 batch-141
Running loss of epoch-5 batch-141 = 0.02092132158577442

Training epoch-5 batch-142
Running loss of epoch-5 batch-142 = 0.019560206681489944

Training epoch-5 batch-143
Running loss of epoch-5 batch-143 = 0.021957000717520714

Training epoch-5 batch-144
Running loss of epoch-5 batch-144 = 0.024072740226984024

Training epoch-5 batch-145
Running loss of epoch-5 batch-145 = 0.019250934943556786

Training epoch-5 batch-146
Running loss of epoch-5 batch-146 = 0.02119472064077854

Training epoch-5 batch-147
Running loss of epoch-5 batch-147 = 0.022887879982590675

Training epoch-5 batch-148
Running loss of epoch-5 batch-148 = 0.022228235378861427

Training epoch-5 batch-149
Running loss of epoch-5 batch-149 = 0.018575526773929596

Training epoch-5 batch-150
Running loss of epoch-5 batch-150 = 0.017300957813858986

Training epoch-5 batch-151
Running loss of epoch-5 batch-151 = 0.022833388298749924

Training epoch-5 batch-152
Running loss of epoch-5 batch-152 = 0.020750656723976135

Training epoch-5 batch-153
Running loss of epoch-5 batch-153 = 0.01866370253264904

Training epoch-5 batch-154
Running loss of epoch-5 batch-154 = 0.020321285352110863

Training epoch-5 batch-155
Running loss of epoch-5 batch-155 = 0.020574340596795082

Training epoch-5 batch-156
Running loss of epoch-5 batch-156 = 0.022408433258533478

Training epoch-5 batch-157
Running loss of epoch-5 batch-157 = 0.10762447118759155

Finished training epoch-5.



Average train loss at epoch-5 = 0.02092329297065735

Started Evaluation

Average val loss at epoch-5 = 1.4243964403867722

Accuracy for classes:
Accuracy for class equals is: 83.50 %
Accuracy for class main is: 59.84 %
Accuracy for class setUp is: 43.44 %
Accuracy for class onCreate is: 68.34 %
Accuracy for class toString is: 41.30 %
Accuracy for class run is: 24.43 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 10.99 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 32.82 %

Overall Accuracy = 50.51 %


Best Accuracy = 50.51 % at Epoch-5
Saving model after best epoch-5

Finished Evaluation



Started training epoch-6


Training epoch-6 batch-1
Running loss of epoch-6 batch-1 = 0.01566927321255207

Training epoch-6 batch-2
Running loss of epoch-6 batch-2 = 0.01591288112103939

Training epoch-6 batch-3
Running loss of epoch-6 batch-3 = 0.019035249948501587

Training epoch-6 batch-4
Running loss of epoch-6 batch-4 = 0.018757715821266174

Training epoch-6 batch-5
Running loss of epoch-6 batch-5 = 0.015861326828598976

Training epoch-6 batch-6
Running loss of epoch-6 batch-6 = 0.01872352510690689

Training epoch-6 batch-7
Running loss of epoch-6 batch-7 = 0.01733109913766384

Training epoch-6 batch-8
Running loss of epoch-6 batch-8 = 0.017685353755950928

Training epoch-6 batch-9
Running loss of epoch-6 batch-9 = 0.016277994960546494

Training epoch-6 batch-10
Running loss of epoch-6 batch-10 = 0.017745601013302803

Training epoch-6 batch-11
Running loss of epoch-6 batch-11 = 0.012784136459231377

Training epoch-6 batch-12
Running loss of epoch-6 batch-12 = 0.014877893030643463

Training epoch-6 batch-13
Running loss of epoch-6 batch-13 = 0.01658499613404274

Training epoch-6 batch-14
Running loss of epoch-6 batch-14 = 0.016621548682451248

Training epoch-6 batch-15
Running loss of epoch-6 batch-15 = 0.016173580661416054

Training epoch-6 batch-16
Running loss of epoch-6 batch-16 = 0.014750190079212189

Training epoch-6 batch-17
Running loss of epoch-6 batch-17 = 0.014230430126190186

Training epoch-6 batch-18
Running loss of epoch-6 batch-18 = 0.015639111399650574

Training epoch-6 batch-19
Running loss of epoch-6 batch-19 = 0.015433592721819878

Training epoch-6 batch-20
Running loss of epoch-6 batch-20 = 0.016221487894654274

Training epoch-6 batch-21
Running loss of epoch-6 batch-21 = 0.01595349982380867

Training epoch-6 batch-22
Running loss of epoch-6 batch-22 = 0.017137892544269562

Training epoch-6 batch-23
Running loss of epoch-6 batch-23 = 0.013406290672719479

Training epoch-6 batch-24
Running loss of epoch-6 batch-24 = 0.014600839465856552

Training epoch-6 batch-25
Running loss of epoch-6 batch-25 = 0.015102041885256767

Training epoch-6 batch-26
Running loss of epoch-6 batch-26 = 0.01772976480424404

Training epoch-6 batch-27
Running loss of epoch-6 batch-27 = 0.01823228783905506

Training epoch-6 batch-28
Running loss of epoch-6 batch-28 = 0.017002034932374954

Training epoch-6 batch-29
Running loss of epoch-6 batch-29 = 0.014874519780278206

Training epoch-6 batch-30
Running loss of epoch-6 batch-30 = 0.01596645452082157

Training epoch-6 batch-31
Running loss of epoch-6 batch-31 = 0.015927573665976524

Training epoch-6 batch-32
Running loss of epoch-6 batch-32 = 0.014053525403141975

Training epoch-6 batch-33
Running loss of epoch-6 batch-33 = 0.014878526329994202

Training epoch-6 batch-34
Running loss of epoch-6 batch-34 = 0.013440976850688457

Training epoch-6 batch-35
Running loss of epoch-6 batch-35 = 0.016202472150325775

Training epoch-6 batch-36
Running loss of epoch-6 batch-36 = 0.014355876483023167

Training epoch-6 batch-37
Running loss of epoch-6 batch-37 = 0.013332769274711609

Training epoch-6 batch-38
Running loss of epoch-6 batch-38 = 0.01980157569050789

Training epoch-6 batch-39
Running loss of epoch-6 batch-39 = 0.014974139630794525

Training epoch-6 batch-40
Running loss of epoch-6 batch-40 = 0.016952382400631905

Training epoch-6 batch-41
Running loss of epoch-6 batch-41 = 0.014052048325538635

Training epoch-6 batch-42
Running loss of epoch-6 batch-42 = 0.015026253648102283

Training epoch-6 batch-43
Running loss of epoch-6 batch-43 = 0.018998567014932632

Training epoch-6 batch-44
Running loss of epoch-6 batch-44 = 0.014513542875647545

Training epoch-6 batch-45
Running loss of epoch-6 batch-45 = 0.016259586438536644

Training epoch-6 batch-46
Running loss of epoch-6 batch-46 = 0.019883913919329643

Training epoch-6 batch-47
Running loss of epoch-6 batch-47 = 0.01648443378508091

Training epoch-6 batch-48
Running loss of epoch-6 batch-48 = 0.014804065227508545

Training epoch-6 batch-49
Running loss of epoch-6 batch-49 = 0.015823503956198692

Training epoch-6 batch-50
Running loss of epoch-6 batch-50 = 0.01504909060895443

Training epoch-6 batch-51
Running loss of epoch-6 batch-51 = 0.016428830102086067

Training epoch-6 batch-52
Running loss of epoch-6 batch-52 = 0.015877019613981247

Training epoch-6 batch-53
Running loss of epoch-6 batch-53 = 0.017297616228461266

Training epoch-6 batch-54
Running loss of epoch-6 batch-54 = 0.018047332763671875

Training epoch-6 batch-55
Running loss of epoch-6 batch-55 = 0.01548173651099205

Training epoch-6 batch-56
Running loss of epoch-6 batch-56 = 0.01614789292216301

Training epoch-6 batch-57
Running loss of epoch-6 batch-57 = 0.015787577256560326

Training epoch-6 batch-58
Running loss of epoch-6 batch-58 = 0.01421098317950964

Training epoch-6 batch-59
Running loss of epoch-6 batch-59 = 0.020766183733940125

Training epoch-6 batch-60
Running loss of epoch-6 batch-60 = 0.01876441389322281

Training epoch-6 batch-61
Running loss of epoch-6 batch-61 = 0.02172471210360527

Training epoch-6 batch-62
Running loss of epoch-6 batch-62 = 0.016487695276737213

Training epoch-6 batch-63
Running loss of epoch-6 batch-63 = 0.01291918195784092

Training epoch-6 batch-64
Running loss of epoch-6 batch-64 = 0.016481168568134308

Training epoch-6 batch-65
Running loss of epoch-6 batch-65 = 0.01554400660097599

Training epoch-6 batch-66
Running loss of epoch-6 batch-66 = 0.015243170782923698

Training epoch-6 batch-67
Running loss of epoch-6 batch-67 = 0.014264760538935661

Training epoch-6 batch-68
Running loss of epoch-6 batch-68 = 0.01330014131963253

Training epoch-6 batch-69
Running loss of epoch-6 batch-69 = 0.015108207240700722

Training epoch-6 batch-70
Running loss of epoch-6 batch-70 = 0.013310671783983707

Training epoch-6 batch-71
Running loss of epoch-6 batch-71 = 0.02089126966893673

Training epoch-6 batch-72
Running loss of epoch-6 batch-72 = 0.014237768016755581

Training epoch-6 batch-73
Running loss of epoch-6 batch-73 = 0.013823973946273327

Training epoch-6 batch-74
Running loss of epoch-6 batch-74 = 0.013427099213004112

Training epoch-6 batch-75
Running loss of epoch-6 batch-75 = 0.01972074806690216

Training epoch-6 batch-76
Running loss of epoch-6 batch-76 = 0.014910271391272545

Training epoch-6 batch-77
Running loss of epoch-6 batch-77 = 0.0158215444535017

Training epoch-6 batch-78
Running loss of epoch-6 batch-78 = 0.01280334871262312

Training epoch-6 batch-79
Running loss of epoch-6 batch-79 = 0.01478923112154007

Training epoch-6 batch-80
Running loss of epoch-6 batch-80 = 0.017274420708417892

Training epoch-6 batch-81
Running loss of epoch-6 batch-81 = 0.013114545494318008

Training epoch-6 batch-82
Running loss of epoch-6 batch-82 = 0.01407705433666706

Training epoch-6 batch-83
Running loss of epoch-6 batch-83 = 0.014573285356163979

Training epoch-6 batch-84
Running loss of epoch-6 batch-84 = 0.017531411722302437

Training epoch-6 batch-85
Running loss of epoch-6 batch-85 = 0.015576605685055256

Training epoch-6 batch-86
Running loss of epoch-6 batch-86 = 0.018801569938659668

Training epoch-6 batch-87
Running loss of epoch-6 batch-87 = 0.015782317146658897

Training epoch-6 batch-88
Running loss of epoch-6 batch-88 = 0.01374183502048254

Training epoch-6 batch-89
Running loss of epoch-6 batch-89 = 0.015365722589194775

Training epoch-6 batch-90
Running loss of epoch-6 batch-90 = 0.018353179097175598

Training epoch-6 batch-91
Running loss of epoch-6 batch-91 = 0.014291942119598389

Training epoch-6 batch-92
Running loss of epoch-6 batch-92 = 0.01588044874370098

Training epoch-6 batch-93
Running loss of epoch-6 batch-93 = 0.020338324829936028

Training epoch-6 batch-94
Running loss of epoch-6 batch-94 = 0.016829457134008408

Training epoch-6 batch-95
Running loss of epoch-6 batch-95 = 0.0201234370470047

Training epoch-6 batch-96
Running loss of epoch-6 batch-96 = 0.016957933083176613

Training epoch-6 batch-97
Running loss of epoch-6 batch-97 = 0.016111185774207115

Training epoch-6 batch-98
Running loss of epoch-6 batch-98 = 0.01678423210978508

Training epoch-6 batch-99
Running loss of epoch-6 batch-99 = 0.01511595118790865

Training epoch-6 batch-100
Running loss of epoch-6 batch-100 = 0.014432736672461033

Training epoch-6 batch-101
Running loss of epoch-6 batch-101 = 0.01416003331542015

Training epoch-6 batch-102
Running loss of epoch-6 batch-102 = 0.011591244488954544

Training epoch-6 batch-103
Running loss of epoch-6 batch-103 = 0.019746407866477966

Training epoch-6 batch-104
Running loss of epoch-6 batch-104 = 0.014777214266359806

Training epoch-6 batch-105
Running loss of epoch-6 batch-105 = 0.01376112550497055

Training epoch-6 batch-106
Running loss of epoch-6 batch-106 = 0.013179143890738487

Training epoch-6 batch-107
Running loss of epoch-6 batch-107 = 0.016500424593687057

Training epoch-6 batch-108
Running loss of epoch-6 batch-108 = 0.013331519439816475

Training epoch-6 batch-109
Running loss of epoch-6 batch-109 = 0.016157597303390503

Training epoch-6 batch-110
Running loss of epoch-6 batch-110 = 0.016072073951363564

Training epoch-6 batch-111
Running loss of epoch-6 batch-111 = 0.01474609225988388

Training epoch-6 batch-112
Running loss of epoch-6 batch-112 = 0.013127105310559273

Training epoch-6 batch-113
Running loss of epoch-6 batch-113 = 0.01416003704071045

Training epoch-6 batch-114
Running loss of epoch-6 batch-114 = 0.016909603029489517

Training epoch-6 batch-115
Running loss of epoch-6 batch-115 = 0.016673758625984192

Training epoch-6 batch-116
Running loss of epoch-6 batch-116 = 0.015474335290491581

Training epoch-6 batch-117
Running loss of epoch-6 batch-117 = 0.017858734354376793

Training epoch-6 batch-118
Running loss of epoch-6 batch-118 = 0.014632795937359333

Training epoch-6 batch-119
Running loss of epoch-6 batch-119 = 0.017114434391260147

Training epoch-6 batch-120
Running loss of epoch-6 batch-120 = 0.01854761689901352

Training epoch-6 batch-121
Running loss of epoch-6 batch-121 = 0.0155421057716012

Training epoch-6 batch-122
Running loss of epoch-6 batch-122 = 0.013143837451934814

Training epoch-6 batch-123
Running loss of epoch-6 batch-123 = 0.014490718953311443

Training epoch-6 batch-124
Running loss of epoch-6 batch-124 = 0.016248732805252075

Training epoch-6 batch-125
Running loss of epoch-6 batch-125 = 0.016974812373518944

Training epoch-6 batch-126
Running loss of epoch-6 batch-126 = 0.015380848199129105

Training epoch-6 batch-127
Running loss of epoch-6 batch-127 = 0.013299712911248207

Training epoch-6 batch-128
Running loss of epoch-6 batch-128 = 0.0171392560005188

Training epoch-6 batch-129
Running loss of epoch-6 batch-129 = 0.015248444862663746

Training epoch-6 batch-130
Running loss of epoch-6 batch-130 = 0.012875208631157875

Training epoch-6 batch-131
Running loss of epoch-6 batch-131 = 0.014511961489915848

Training epoch-6 batch-132
Running loss of epoch-6 batch-132 = 0.017051292583346367

Training epoch-6 batch-133
Running loss of epoch-6 batch-133 = 0.015099967829883099

Training epoch-6 batch-134
Running loss of epoch-6 batch-134 = 0.013901962898671627

Training epoch-6 batch-135
Running loss of epoch-6 batch-135 = 0.01453797984868288

Training epoch-6 batch-136
Running loss of epoch-6 batch-136 = 0.014984648674726486

Training epoch-6 batch-137
Running loss of epoch-6 batch-137 = 0.016174305230379105

Training epoch-6 batch-138
Running loss of epoch-6 batch-138 = 0.01519999373704195

Training epoch-6 batch-139
Running loss of epoch-6 batch-139 = 0.021089965477585793

Training epoch-6 batch-140
Running loss of epoch-6 batch-140 = 0.015513600781559944

Training epoch-6 batch-141
Running loss of epoch-6 batch-141 = 0.0145586421713233

Training epoch-6 batch-142
Running loss of epoch-6 batch-142 = 0.016791898757219315

Training epoch-6 batch-143
Running loss of epoch-6 batch-143 = 0.01492904033511877

Training epoch-6 batch-144
Running loss of epoch-6 batch-144 = 0.011262677609920502

Training epoch-6 batch-145
Running loss of epoch-6 batch-145 = 0.017601069062948227

Training epoch-6 batch-146
Running loss of epoch-6 batch-146 = 0.018190933391451836

Training epoch-6 batch-147
Running loss of epoch-6 batch-147 = 0.01404458936303854

Training epoch-6 batch-148
Running loss of epoch-6 batch-148 = 0.014916732907295227

Training epoch-6 batch-149
Running loss of epoch-6 batch-149 = 0.013710894621908665

Training epoch-6 batch-150
Running loss of epoch-6 batch-150 = 0.016031378880143166

Training epoch-6 batch-151
Running loss of epoch-6 batch-151 = 0.013955283910036087

Training epoch-6 batch-152
Running loss of epoch-6 batch-152 = 0.018064875155687332

Training epoch-6 batch-153
Running loss of epoch-6 batch-153 = 0.024081911891698837

Training epoch-6 batch-154
Running loss of epoch-6 batch-154 = 0.0197626780718565

Training epoch-6 batch-155
Running loss of epoch-6 batch-155 = 0.013476183637976646

Training epoch-6 batch-156
Running loss of epoch-6 batch-156 = 0.013150837272405624

Training epoch-6 batch-157
Running loss of epoch-6 batch-157 = 0.04491204768419266

Finished training epoch-6.



Average train loss at epoch-6 = 0.015926412057876586

Started Evaluation

Average val loss at epoch-6 = 1.4271880466687052

Accuracy for classes:
Accuracy for class equals is: 69.64 %
Accuracy for class main is: 72.95 %
Accuracy for class setUp is: 52.30 %
Accuracy for class onCreate is: 69.30 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 69.29 %
Accuracy for class init is: 10.76 %
Accuracy for class execute is: 23.69 %
Accuracy for class get is: 56.92 %

Overall Accuracy = 54.40 %


Best Accuracy = 54.40 % at Epoch-6
Saving model after best epoch-6

Finished Evaluation



Started training epoch-7


Training epoch-7 batch-1
Running loss of epoch-7 batch-1 = 0.014252829365432262

Training epoch-7 batch-2
Running loss of epoch-7 batch-2 = 0.01260023657232523

Training epoch-7 batch-3
Running loss of epoch-7 batch-3 = 0.009149420075118542

Training epoch-7 batch-4
Running loss of epoch-7 batch-4 = 0.012696691788733006

Training epoch-7 batch-5
Running loss of epoch-7 batch-5 = 0.01022519450634718

Training epoch-7 batch-6
Running loss of epoch-7 batch-6 = 0.012059224769473076

Training epoch-7 batch-7
Running loss of epoch-7 batch-7 = 0.011272053234279156

Training epoch-7 batch-8
Running loss of epoch-7 batch-8 = 0.010157878510653973

Training epoch-7 batch-9
Running loss of epoch-7 batch-9 = 0.011692923493683338

Training epoch-7 batch-10
Running loss of epoch-7 batch-10 = 0.013677527196705341

Training epoch-7 batch-11
Running loss of epoch-7 batch-11 = 0.012809238396584988

Training epoch-7 batch-12
Running loss of epoch-7 batch-12 = 0.010835135355591774

Training epoch-7 batch-13
Running loss of epoch-7 batch-13 = 0.01116156205534935

Training epoch-7 batch-14
Running loss of epoch-7 batch-14 = 0.010234707035124302

Training epoch-7 batch-15
Running loss of epoch-7 batch-15 = 0.010241222567856312

Training epoch-7 batch-16
Running loss of epoch-7 batch-16 = 0.011644719168543816

Training epoch-7 batch-17
Running loss of epoch-7 batch-17 = 0.01121810358017683

Training epoch-7 batch-18
Running loss of epoch-7 batch-18 = 0.01148119755089283

Training epoch-7 batch-19
Running loss of epoch-7 batch-19 = 0.012029657140374184

Training epoch-7 batch-20
Running loss of epoch-7 batch-20 = 0.010639413259923458

Training epoch-7 batch-21
Running loss of epoch-7 batch-21 = 0.009429692290723324

Training epoch-7 batch-22
Running loss of epoch-7 batch-22 = 0.009708505123853683

Training epoch-7 batch-23
Running loss of epoch-7 batch-23 = 0.00875815562903881

Training epoch-7 batch-24
Running loss of epoch-7 batch-24 = 0.009768305346369743

Training epoch-7 batch-25
Running loss of epoch-7 batch-25 = 0.010425894521176815

Training epoch-7 batch-26
Running loss of epoch-7 batch-26 = 0.00831807404756546

Training epoch-7 batch-27
Running loss of epoch-7 batch-27 = 0.01240795012563467

Training epoch-7 batch-28
Running loss of epoch-7 batch-28 = 0.010522737167775631

Training epoch-7 batch-29
Running loss of epoch-7 batch-29 = 0.010092836804687977

Training epoch-7 batch-30
Running loss of epoch-7 batch-30 = 0.010424258187413216

Training epoch-7 batch-31
Running loss of epoch-7 batch-31 = 0.008747328072786331

Training epoch-7 batch-32
Running loss of epoch-7 batch-32 = 0.011070950888097286

Training epoch-7 batch-33
Running loss of epoch-7 batch-33 = 0.010079334490001202

Training epoch-7 batch-34
Running loss of epoch-7 batch-34 = 0.012280317023396492

Training epoch-7 batch-35
Running loss of epoch-7 batch-35 = 0.01123386062681675

Training epoch-7 batch-36
Running loss of epoch-7 batch-36 = 0.010962952859699726

Training epoch-7 batch-37
Running loss of epoch-7 batch-37 = 0.010798344388604164

Training epoch-7 batch-38
Running loss of epoch-7 batch-38 = 0.012774947099387646

Training epoch-7 batch-39
Running loss of epoch-7 batch-39 = 0.012250577099621296

Training epoch-7 batch-40
Running loss of epoch-7 batch-40 = 0.009028597734868526

Training epoch-7 batch-41
Running loss of epoch-7 batch-41 = 0.012427457608282566

Training epoch-7 batch-42
Running loss of epoch-7 batch-42 = 0.012599420733749866

Training epoch-7 batch-43
Running loss of epoch-7 batch-43 = 0.014163004234433174

Training epoch-7 batch-44
Running loss of epoch-7 batch-44 = 0.014044025912880898

Training epoch-7 batch-45
Running loss of epoch-7 batch-45 = 0.006802685558795929

Training epoch-7 batch-46
Running loss of epoch-7 batch-46 = 0.013818474486470222

Training epoch-7 batch-47
Running loss of epoch-7 batch-47 = 0.012815359979867935

Training epoch-7 batch-48
Running loss of epoch-7 batch-48 = 0.009983563795685768

Training epoch-7 batch-49
Running loss of epoch-7 batch-49 = 0.012048369273543358

Training epoch-7 batch-50
Running loss of epoch-7 batch-50 = 0.010518919676542282

Training epoch-7 batch-51
Running loss of epoch-7 batch-51 = 0.010243761353194714

Training epoch-7 batch-52
Running loss of epoch-7 batch-52 = 0.012136994861066341

Training epoch-7 batch-53
Running loss of epoch-7 batch-53 = 0.010612722486257553

Training epoch-7 batch-54
Running loss of epoch-7 batch-54 = 0.010017463006079197

Training epoch-7 batch-55
Running loss of epoch-7 batch-55 = 0.00861545279622078

Training epoch-7 batch-56
Running loss of epoch-7 batch-56 = 0.012845021672546864

Training epoch-7 batch-57
Running loss of epoch-7 batch-57 = 0.011466867290437222

Training epoch-7 batch-58
Running loss of epoch-7 batch-58 = 0.014202460646629333

Training epoch-7 batch-59
Running loss of epoch-7 batch-59 = 0.01081637479364872

Training epoch-7 batch-60
Running loss of epoch-7 batch-60 = 0.01242458913475275

Training epoch-7 batch-61
Running loss of epoch-7 batch-61 = 0.010883273556828499

Training epoch-7 batch-62
Running loss of epoch-7 batch-62 = 0.01235132198780775

Training epoch-7 batch-63
Running loss of epoch-7 batch-63 = 0.011042997241020203

Training epoch-7 batch-64
Running loss of epoch-7 batch-64 = 0.01437102910131216

Training epoch-7 batch-65
Running loss of epoch-7 batch-65 = 0.010139429941773415

Training epoch-7 batch-66
Running loss of epoch-7 batch-66 = 0.010975404642522335

Training epoch-7 batch-67
Running loss of epoch-7 batch-67 = 0.011833746917545795

Training epoch-7 batch-68
Running loss of epoch-7 batch-68 = 0.008966521359980106

Training epoch-7 batch-69
Running loss of epoch-7 batch-69 = 0.009903252124786377

Training epoch-7 batch-70
Running loss of epoch-7 batch-70 = 0.013839197345077991

Training epoch-7 batch-71
Running loss of epoch-7 batch-71 = 0.008157914504408836

Training epoch-7 batch-72
Running loss of epoch-7 batch-72 = 0.010100454092025757

Training epoch-7 batch-73
Running loss of epoch-7 batch-73 = 0.01119236834347248

Training epoch-7 batch-74
Running loss of epoch-7 batch-74 = 0.01322011835873127

Training epoch-7 batch-75
Running loss of epoch-7 batch-75 = 0.009147858247160912

Training epoch-7 batch-76
Running loss of epoch-7 batch-76 = 0.013403705321252346

Training epoch-7 batch-77
Running loss of epoch-7 batch-77 = 0.012709248811006546

Training epoch-7 batch-78
Running loss of epoch-7 batch-78 = 0.009743799455463886

Training epoch-7 batch-79
Running loss of epoch-7 batch-79 = 0.01153225265443325

Training epoch-7 batch-80
Running loss of epoch-7 batch-80 = 0.011443531140685081

Training epoch-7 batch-81
Running loss of epoch-7 batch-81 = 0.010830901563167572

Training epoch-7 batch-82
Running loss of epoch-7 batch-82 = 0.010935313999652863

Training epoch-7 batch-83
Running loss of epoch-7 batch-83 = 0.011846342124044895

Training epoch-7 batch-84
Running loss of epoch-7 batch-84 = 0.011167257092893124

Training epoch-7 batch-85
Running loss of epoch-7 batch-85 = 0.008325998671352863

Training epoch-7 batch-86
Running loss of epoch-7 batch-86 = 0.008734292350709438

Training epoch-7 batch-87
Running loss of epoch-7 batch-87 = 0.010870756581425667

Training epoch-7 batch-88
Running loss of epoch-7 batch-88 = 0.011051535606384277

Training epoch-7 batch-89
Running loss of epoch-7 batch-89 = 0.01306793000549078

Training epoch-7 batch-90
Running loss of epoch-7 batch-90 = 0.009837951511144638

Training epoch-7 batch-91
Running loss of epoch-7 batch-91 = 0.00884309969842434

Training epoch-7 batch-92
Running loss of epoch-7 batch-92 = 0.011761527508497238

Training epoch-7 batch-93
Running loss of epoch-7 batch-93 = 0.011413494125008583

Training epoch-7 batch-94
Running loss of epoch-7 batch-94 = 0.014096861705183983

Training epoch-7 batch-95
Running loss of epoch-7 batch-95 = 0.011328157968819141

Training epoch-7 batch-96
Running loss of epoch-7 batch-96 = 0.007000446319580078

Training epoch-7 batch-97
Running loss of epoch-7 batch-97 = 0.010911711491644382

Training epoch-7 batch-98
Running loss of epoch-7 batch-98 = 0.012777813710272312

Training epoch-7 batch-99
Running loss of epoch-7 batch-99 = 0.008912250399589539

Training epoch-7 batch-100
Running loss of epoch-7 batch-100 = 0.009854930453002453

Training epoch-7 batch-101
Running loss of epoch-7 batch-101 = 0.009577023796737194

Training epoch-7 batch-102
Running loss of epoch-7 batch-102 = 0.010241888463497162

Training epoch-7 batch-103
Running loss of epoch-7 batch-103 = 0.00819304957985878

Training epoch-7 batch-104
Running loss of epoch-7 batch-104 = 0.010477719828486443

Training epoch-7 batch-105
Running loss of epoch-7 batch-105 = 0.012082288041710854

Training epoch-7 batch-106
Running loss of epoch-7 batch-106 = 0.012057228945195675

Training epoch-7 batch-107
Running loss of epoch-7 batch-107 = 0.010771424509584904

Training epoch-7 batch-108
Running loss of epoch-7 batch-108 = 0.012815202586352825

Training epoch-7 batch-109
Running loss of epoch-7 batch-109 = 0.01233772374689579

Training epoch-7 batch-110
Running loss of epoch-7 batch-110 = 0.008888384327292442

Training epoch-7 batch-111
Running loss of epoch-7 batch-111 = 0.012669704854488373

Training epoch-7 batch-112
Running loss of epoch-7 batch-112 = 0.011088497005403042

Training epoch-7 batch-113
Running loss of epoch-7 batch-113 = 0.008777325041592121

Training epoch-7 batch-114
Running loss of epoch-7 batch-114 = 0.01096497755497694

Training epoch-7 batch-115
Running loss of epoch-7 batch-115 = 0.012479128316044807

Training epoch-7 batch-116
Running loss of epoch-7 batch-116 = 0.010359007865190506

Training epoch-7 batch-117
Running loss of epoch-7 batch-117 = 0.010252920910716057

Training epoch-7 batch-118
Running loss of epoch-7 batch-118 = 0.009024636819958687

Training epoch-7 batch-119
Running loss of epoch-7 batch-119 = 0.010997270233929157

Training epoch-7 batch-120
Running loss of epoch-7 batch-120 = 0.009401252493262291

Training epoch-7 batch-121
Running loss of epoch-7 batch-121 = 0.01014728844165802

Training epoch-7 batch-122
Running loss of epoch-7 batch-122 = 0.008589670062065125

Training epoch-7 batch-123
Running loss of epoch-7 batch-123 = 0.010427000932395458

Training epoch-7 batch-124
Running loss of epoch-7 batch-124 = 0.009323366917669773

Training epoch-7 batch-125
Running loss of epoch-7 batch-125 = 0.012260745279490948

Training epoch-7 batch-126
Running loss of epoch-7 batch-126 = 0.011646891012787819

Training epoch-7 batch-127
Running loss of epoch-7 batch-127 = 0.008125081658363342

Training epoch-7 batch-128
Running loss of epoch-7 batch-128 = 0.006551428698003292

Training epoch-7 batch-129
Running loss of epoch-7 batch-129 = 0.011983063071966171

Training epoch-7 batch-130
Running loss of epoch-7 batch-130 = 0.013736237771809101

Training epoch-7 batch-131
Running loss of epoch-7 batch-131 = 0.011226635426282883

Training epoch-7 batch-132
Running loss of epoch-7 batch-132 = 0.010466194711625576

Training epoch-7 batch-133
Running loss of epoch-7 batch-133 = 0.009229784831404686

Training epoch-7 batch-134
Running loss of epoch-7 batch-134 = 0.011472475714981556

Training epoch-7 batch-135
Running loss of epoch-7 batch-135 = 0.010981501080095768

Training epoch-7 batch-136
Running loss of epoch-7 batch-136 = 0.010880216024816036

Training epoch-7 batch-137
Running loss of epoch-7 batch-137 = 0.015556973405182362

Training epoch-7 batch-138
Running loss of epoch-7 batch-138 = 0.011898486874997616

Training epoch-7 batch-139
Running loss of epoch-7 batch-139 = 0.013652618043124676

Training epoch-7 batch-140
Running loss of epoch-7 batch-140 = 0.011285234242677689

Training epoch-7 batch-141
Running loss of epoch-7 batch-141 = 0.012061974965035915

Training epoch-7 batch-142
Running loss of epoch-7 batch-142 = 0.01032199151813984

Training epoch-7 batch-143
Running loss of epoch-7 batch-143 = 0.012964731082320213

Training epoch-7 batch-144
Running loss of epoch-7 batch-144 = 0.012280098162591457

Training epoch-7 batch-145
Running loss of epoch-7 batch-145 = 0.00994352251291275

Training epoch-7 batch-146
Running loss of epoch-7 batch-146 = 0.013064208440482616

Training epoch-7 batch-147
Running loss of epoch-7 batch-147 = 0.012264028191566467

Training epoch-7 batch-148
Running loss of epoch-7 batch-148 = 0.009738289751112461

Training epoch-7 batch-149
Running loss of epoch-7 batch-149 = 0.011392196640372276

Training epoch-7 batch-150
Running loss of epoch-7 batch-150 = 0.008516782894730568

Training epoch-7 batch-151
Running loss of epoch-7 batch-151 = 0.012565817683935165

Training epoch-7 batch-152
Running loss of epoch-7 batch-152 = 0.00957623589783907

Training epoch-7 batch-153
Running loss of epoch-7 batch-153 = 0.01390113215893507

Training epoch-7 batch-154
Running loss of epoch-7 batch-154 = 0.012620161287486553

Training epoch-7 batch-155
Running loss of epoch-7 batch-155 = 0.011680149473249912

Training epoch-7 batch-156
Running loss of epoch-7 batch-156 = 0.007420096546411514

Training epoch-7 batch-157
Running loss of epoch-7 batch-157 = 0.028809309005737305

Finished training epoch-7.



Average train loss at epoch-7 = 0.0110764981508255

Started Evaluation

Average val loss at epoch-7 = 1.792374113672658

Accuracy for classes:
Accuracy for class equals is: 80.69 %
Accuracy for class main is: 57.38 %
Accuracy for class setUp is: 33.93 %
Accuracy for class onCreate is: 40.83 %
Accuracy for class toString is: 46.08 %
Accuracy for class run is: 29.22 %
Accuracy for class hashCode is: 78.28 %
Accuracy for class init is: 22.42 %
Accuracy for class execute is: 37.75 %
Accuracy for class get is: 45.64 %

Overall Accuracy = 46.89 %

Finished Evaluation



Started training epoch-8


Training epoch-8 batch-1
Running loss of epoch-8 batch-1 = 0.007020185235887766

Training epoch-8 batch-2
Running loss of epoch-8 batch-2 = 0.005999535322189331

Training epoch-8 batch-3
Running loss of epoch-8 batch-3 = 0.009827647358179092

Training epoch-8 batch-4
Running loss of epoch-8 batch-4 = 0.004736951552331448

Training epoch-8 batch-5
Running loss of epoch-8 batch-5 = 0.008709276095032692

Training epoch-8 batch-6
Running loss of epoch-8 batch-6 = 0.008243819698691368

Training epoch-8 batch-7
Running loss of epoch-8 batch-7 = 0.006907117553055286

Training epoch-8 batch-8
Running loss of epoch-8 batch-8 = 0.008184707723557949

Training epoch-8 batch-9
Running loss of epoch-8 batch-9 = 0.005909870378673077

Training epoch-8 batch-10
Running loss of epoch-8 batch-10 = 0.006757915951311588

Training epoch-8 batch-11
Running loss of epoch-8 batch-11 = 0.009428944438695908

Training epoch-8 batch-12
Running loss of epoch-8 batch-12 = 0.004578165709972382

Training epoch-8 batch-13
Running loss of epoch-8 batch-13 = 0.007565104402601719

Training epoch-8 batch-14
Running loss of epoch-8 batch-14 = 0.004065562039613724

Training epoch-8 batch-15
Running loss of epoch-8 batch-15 = 0.005968373268842697

Training epoch-8 batch-16
Running loss of epoch-8 batch-16 = 0.005917037837207317

Training epoch-8 batch-17
Running loss of epoch-8 batch-17 = 0.0067290253937244415

Training epoch-8 batch-18
Running loss of epoch-8 batch-18 = 0.006296681240200996

Training epoch-8 batch-19
Running loss of epoch-8 batch-19 = 0.005108397454023361

Training epoch-8 batch-20
Running loss of epoch-8 batch-20 = 0.004915366880595684

Training epoch-8 batch-21
Running loss of epoch-8 batch-21 = 0.005556588061153889

Training epoch-8 batch-22
Running loss of epoch-8 batch-22 = 0.0070514255203306675

Training epoch-8 batch-23
Running loss of epoch-8 batch-23 = 0.004276771564036608

Training epoch-8 batch-24
Running loss of epoch-8 batch-24 = 0.008256278000772

Training epoch-8 batch-25
Running loss of epoch-8 batch-25 = 0.008300009183585644

Training epoch-8 batch-26
Running loss of epoch-8 batch-26 = 0.006127668544650078

Training epoch-8 batch-27
Running loss of epoch-8 batch-27 = 0.007381945848464966

Training epoch-8 batch-28
Running loss of epoch-8 batch-28 = 0.006227834150195122

Training epoch-8 batch-29
Running loss of epoch-8 batch-29 = 0.005613948684185743

Training epoch-8 batch-30
Running loss of epoch-8 batch-30 = 0.007818879559636116

Training epoch-8 batch-31
Running loss of epoch-8 batch-31 = 0.004896732047200203

Training epoch-8 batch-32
Running loss of epoch-8 batch-32 = 0.004054407589137554

Training epoch-8 batch-33
Running loss of epoch-8 batch-33 = 0.006284425035119057

Training epoch-8 batch-34
Running loss of epoch-8 batch-34 = 0.007016892544925213

Training epoch-8 batch-35
Running loss of epoch-8 batch-35 = 0.005228934809565544

Training epoch-8 batch-36
Running loss of epoch-8 batch-36 = 0.004501665942370892

Training epoch-8 batch-37
Running loss of epoch-8 batch-37 = 0.0049810148775577545

Training epoch-8 batch-38
Running loss of epoch-8 batch-38 = 0.004569308832287788

Training epoch-8 batch-39
Running loss of epoch-8 batch-39 = 0.00558589119464159

Training epoch-8 batch-40
Running loss of epoch-8 batch-40 = 0.004057543817907572

Training epoch-8 batch-41
Running loss of epoch-8 batch-41 = 0.006725849583745003

Training epoch-8 batch-42
Running loss of epoch-8 batch-42 = 0.005323516670614481

Training epoch-8 batch-43
Running loss of epoch-8 batch-43 = 0.005322043783962727

Training epoch-8 batch-44
Running loss of epoch-8 batch-44 = 0.004587933421134949

Training epoch-8 batch-45
Running loss of epoch-8 batch-45 = 0.005113883875310421

Training epoch-8 batch-46
Running loss of epoch-8 batch-46 = 0.0030265357345342636

Training epoch-8 batch-47
Running loss of epoch-8 batch-47 = 0.00506033468991518

Training epoch-8 batch-48
Running loss of epoch-8 batch-48 = 0.006348609924316406

Training epoch-8 batch-49
Running loss of epoch-8 batch-49 = 0.004263223148882389

Training epoch-8 batch-50
Running loss of epoch-8 batch-50 = 0.004412454552948475

Training epoch-8 batch-51
Running loss of epoch-8 batch-51 = 0.003509078174829483

Training epoch-8 batch-52
Running loss of epoch-8 batch-52 = 0.005445035640150309

Training epoch-8 batch-53
Running loss of epoch-8 batch-53 = 0.005109408404678106

Training epoch-8 batch-54
Running loss of epoch-8 batch-54 = 0.004099342972040176

Training epoch-8 batch-55
Running loss of epoch-8 batch-55 = 0.007208239287137985

Training epoch-8 batch-56
Running loss of epoch-8 batch-56 = 0.005581133998930454

Training epoch-8 batch-57
Running loss of epoch-8 batch-57 = 0.005428302101790905

Training epoch-8 batch-58
Running loss of epoch-8 batch-58 = 0.0060328333638608456

Training epoch-8 batch-59
Running loss of epoch-8 batch-59 = 0.0065266164019703865

Training epoch-8 batch-60
Running loss of epoch-8 batch-60 = 0.006799040827900171

Training epoch-8 batch-61
Running loss of epoch-8 batch-61 = 0.00441726902499795

Training epoch-8 batch-62
Running loss of epoch-8 batch-62 = 0.006608742754906416

Training epoch-8 batch-63
Running loss of epoch-8 batch-63 = 0.00734696676954627

Training epoch-8 batch-64
Running loss of epoch-8 batch-64 = 0.006336512975394726

Training epoch-8 batch-65
Running loss of epoch-8 batch-65 = 0.009036821313202381

Training epoch-8 batch-66
Running loss of epoch-8 batch-66 = 0.004490042105317116

Training epoch-8 batch-67
Running loss of epoch-8 batch-67 = 0.004517720080912113

Training epoch-8 batch-68
Running loss of epoch-8 batch-68 = 0.004033290781080723

Training epoch-8 batch-69
Running loss of epoch-8 batch-69 = 0.008063757792115211

Training epoch-8 batch-70
Running loss of epoch-8 batch-70 = 0.005168642848730087

Training epoch-8 batch-71
Running loss of epoch-8 batch-71 = 0.005061281379312277

Training epoch-8 batch-72
Running loss of epoch-8 batch-72 = 0.004017621278762817

Training epoch-8 batch-73
Running loss of epoch-8 batch-73 = 0.006065212190151215

Training epoch-8 batch-74
Running loss of epoch-8 batch-74 = 0.0038407447282224894

Training epoch-8 batch-75
Running loss of epoch-8 batch-75 = 0.0038829694967716932

Training epoch-8 batch-76
Running loss of epoch-8 batch-76 = 0.003974105231463909

Training epoch-8 batch-77
Running loss of epoch-8 batch-77 = 0.00785084255039692

Training epoch-8 batch-78
Running loss of epoch-8 batch-78 = 0.006203100085258484

Training epoch-8 batch-79
Running loss of epoch-8 batch-79 = 0.0069766356609761715

Training epoch-8 batch-80
Running loss of epoch-8 batch-80 = 0.006210777442902327

Training epoch-8 batch-81
Running loss of epoch-8 batch-81 = 0.0076350439339876175

Training epoch-8 batch-82
Running loss of epoch-8 batch-82 = 0.007402309216558933

Training epoch-8 batch-83
Running loss of epoch-8 batch-83 = 0.005159205757081509

Training epoch-8 batch-84
Running loss of epoch-8 batch-84 = 0.00638204300776124

Training epoch-8 batch-85
Running loss of epoch-8 batch-85 = 0.00557546503841877

Training epoch-8 batch-86
Running loss of epoch-8 batch-86 = 0.007592931389808655

Training epoch-8 batch-87
Running loss of epoch-8 batch-87 = 0.004295526072382927

Training epoch-8 batch-88
Running loss of epoch-8 batch-88 = 0.004673756659030914

Training epoch-8 batch-89
Running loss of epoch-8 batch-89 = 0.005697133485227823

Training epoch-8 batch-90
Running loss of epoch-8 batch-90 = 0.005336114205420017

Training epoch-8 batch-91
Running loss of epoch-8 batch-91 = 0.0062876613810658455

Training epoch-8 batch-92
Running loss of epoch-8 batch-92 = 0.006119191646575928

Training epoch-8 batch-93
Running loss of epoch-8 batch-93 = 0.004537694156169891

Training epoch-8 batch-94
Running loss of epoch-8 batch-94 = 0.006134224124252796

Training epoch-8 batch-95
Running loss of epoch-8 batch-95 = 0.0036771022714674473

Training epoch-8 batch-96
Running loss of epoch-8 batch-96 = 0.009138680063188076

Training epoch-8 batch-97
Running loss of epoch-8 batch-97 = 0.0069098505191504955

Training epoch-8 batch-98
Running loss of epoch-8 batch-98 = 0.00713345455005765

Training epoch-8 batch-99
Running loss of epoch-8 batch-99 = 0.004213567357510328

Training epoch-8 batch-100
Running loss of epoch-8 batch-100 = 0.007260540034621954

Training epoch-8 batch-101
Running loss of epoch-8 batch-101 = 0.004533268511295319

Training epoch-8 batch-102
Running loss of epoch-8 batch-102 = 0.0037631173618137836

Training epoch-8 batch-103
Running loss of epoch-8 batch-103 = 0.004253965336829424

Training epoch-8 batch-104
Running loss of epoch-8 batch-104 = 0.0046832747757434845

Training epoch-8 batch-105
Running loss of epoch-8 batch-105 = 0.004729707725346088

Training epoch-8 batch-106
Running loss of epoch-8 batch-106 = 0.004881302826106548

Training epoch-8 batch-107
Running loss of epoch-8 batch-107 = 0.007217267528176308

Training epoch-8 batch-108
Running loss of epoch-8 batch-108 = 0.004625852219760418

Training epoch-8 batch-109
Running loss of epoch-8 batch-109 = 0.00616864487528801

Training epoch-8 batch-110
Running loss of epoch-8 batch-110 = 0.005296317860484123

Training epoch-8 batch-111
Running loss of epoch-8 batch-111 = 0.005721003748476505

Training epoch-8 batch-112
Running loss of epoch-8 batch-112 = 0.0050956145860254765

Training epoch-8 batch-113
Running loss of epoch-8 batch-113 = 0.009693069383502007

Training epoch-8 batch-114
Running loss of epoch-8 batch-114 = 0.005947858560830355

Training epoch-8 batch-115
Running loss of epoch-8 batch-115 = 0.006510584149509668

Training epoch-8 batch-116
Running loss of epoch-8 batch-116 = 0.006934275850653648

Training epoch-8 batch-117
Running loss of epoch-8 batch-117 = 0.005813896656036377

Training epoch-8 batch-118
Running loss of epoch-8 batch-118 = 0.005493737757205963

Training epoch-8 batch-119
Running loss of epoch-8 batch-119 = 0.004866528324782848

Training epoch-8 batch-120
Running loss of epoch-8 batch-120 = 0.006255917716771364

Training epoch-8 batch-121
Running loss of epoch-8 batch-121 = 0.003925563767552376

Training epoch-8 batch-122
Running loss of epoch-8 batch-122 = 0.005250460468232632

Training epoch-8 batch-123
Running loss of epoch-8 batch-123 = 0.005045743193477392

Training epoch-8 batch-124
Running loss of epoch-8 batch-124 = 0.006492283195257187

Training epoch-8 batch-125
Running loss of epoch-8 batch-125 = 0.006637951359152794

Training epoch-8 batch-126
Running loss of epoch-8 batch-126 = 0.005621326621621847

Training epoch-8 batch-127
Running loss of epoch-8 batch-127 = 0.006254661828279495

Training epoch-8 batch-128
Running loss of epoch-8 batch-128 = 0.005102477967739105

Training epoch-8 batch-129
Running loss of epoch-8 batch-129 = 0.0066472068428993225

Training epoch-8 batch-130
Running loss of epoch-8 batch-130 = 0.004812335595488548

Training epoch-8 batch-131
Running loss of epoch-8 batch-131 = 0.008875462226569653

Training epoch-8 batch-132
Running loss of epoch-8 batch-132 = 0.0063900877721607685

Training epoch-8 batch-133
Running loss of epoch-8 batch-133 = 0.007225056178867817

Training epoch-8 batch-134
Running loss of epoch-8 batch-134 = 0.004361703991889954

Training epoch-8 batch-135
Running loss of epoch-8 batch-135 = 0.004758887924253941

Training epoch-8 batch-136
Running loss of epoch-8 batch-136 = 0.006189190782606602

Training epoch-8 batch-137
Running loss of epoch-8 batch-137 = 0.00801662914454937

Training epoch-8 batch-138
Running loss of epoch-8 batch-138 = 0.005539708770811558

Training epoch-8 batch-139
Running loss of epoch-8 batch-139 = 0.005787012167274952

Training epoch-8 batch-140
Running loss of epoch-8 batch-140 = 0.006912133656442165

Training epoch-8 batch-141
Running loss of epoch-8 batch-141 = 0.006316537968814373

Training epoch-8 batch-142
Running loss of epoch-8 batch-142 = 0.005481894128024578

Training epoch-8 batch-143
Running loss of epoch-8 batch-143 = 0.005826651118695736

Training epoch-8 batch-144
Running loss of epoch-8 batch-144 = 0.006674767937511206

Training epoch-8 batch-145
Running loss of epoch-8 batch-145 = 0.006734758149832487

Training epoch-8 batch-146
Running loss of epoch-8 batch-146 = 0.0058511365205049515

Training epoch-8 batch-147
Running loss of epoch-8 batch-147 = 0.00567516079172492

Training epoch-8 batch-148
Running loss of epoch-8 batch-148 = 0.004453490022569895

Training epoch-8 batch-149
Running loss of epoch-8 batch-149 = 0.005505592096596956

Training epoch-8 batch-150
Running loss of epoch-8 batch-150 = 0.005741313099861145

Training epoch-8 batch-151
Running loss of epoch-8 batch-151 = 0.002976661082357168

Training epoch-8 batch-152
Running loss of epoch-8 batch-152 = 0.006520180031657219

Training epoch-8 batch-153
Running loss of epoch-8 batch-153 = 0.002816789550706744

Training epoch-8 batch-154
Running loss of epoch-8 batch-154 = 0.006601711735129356

Training epoch-8 batch-155
Running loss of epoch-8 batch-155 = 0.005751320160925388

Training epoch-8 batch-156
Running loss of epoch-8 batch-156 = 0.007445241324603558

Training epoch-8 batch-157
Running loss of epoch-8 batch-157 = 0.021518390625715256

Finished training epoch-8.



Average train loss at epoch-8 = 0.005887550155818462

Started Evaluation

Average val loss at epoch-8 = 2.147978781869537

Accuracy for classes:
Accuracy for class equals is: 81.85 %
Accuracy for class main is: 55.41 %
Accuracy for class setUp is: 45.90 %
Accuracy for class onCreate is: 44.35 %
Accuracy for class toString is: 43.69 %
Accuracy for class run is: 47.03 %
Accuracy for class hashCode is: 82.02 %
Accuracy for class init is: 19.96 %
Accuracy for class execute is: 4.82 %
Accuracy for class get is: 38.46 %

Overall Accuracy = 48.15 %

Finished Evaluation



Started training epoch-9


Training epoch-9 batch-1
Running loss of epoch-9 batch-1 = 0.005784913431853056

Training epoch-9 batch-2
Running loss of epoch-9 batch-2 = 0.004033896140754223

Training epoch-9 batch-3
Running loss of epoch-9 batch-3 = 0.003165258327499032

Training epoch-9 batch-4
Running loss of epoch-9 batch-4 = 0.0038462961092591286

Training epoch-9 batch-5
Running loss of epoch-9 batch-5 = 0.0032540499232709408

Training epoch-9 batch-6
Running loss of epoch-9 batch-6 = 0.0015000603161752224

Training epoch-9 batch-7
Running loss of epoch-9 batch-7 = 0.003246705047786236

Training epoch-9 batch-8
Running loss of epoch-9 batch-8 = 0.00668338080868125

Training epoch-9 batch-9
Running loss of epoch-9 batch-9 = 0.0029150100890547037

Training epoch-9 batch-10
Running loss of epoch-9 batch-10 = 0.0036235724110156298

Training epoch-9 batch-11
Running loss of epoch-9 batch-11 = 0.006301712244749069

Training epoch-9 batch-12
Running loss of epoch-9 batch-12 = 0.002663847990334034

Training epoch-9 batch-13
Running loss of epoch-9 batch-13 = 0.002683703787624836

Training epoch-9 batch-14
Running loss of epoch-9 batch-14 = 0.0026823156513273716

Training epoch-9 batch-15
Running loss of epoch-9 batch-15 = 0.0025809304788708687

Training epoch-9 batch-16
Running loss of epoch-9 batch-16 = 0.004221079405397177

Training epoch-9 batch-17
Running loss of epoch-9 batch-17 = 0.003317924914881587

Training epoch-9 batch-18
Running loss of epoch-9 batch-18 = 0.0025442575570195913

Training epoch-9 batch-19
Running loss of epoch-9 batch-19 = 0.0022606768179684877

Training epoch-9 batch-20
Running loss of epoch-9 batch-20 = 0.0029348188545554876

Training epoch-9 batch-21
Running loss of epoch-9 batch-21 = 0.0036878352984786034

Training epoch-9 batch-22
Running loss of epoch-9 batch-22 = 0.0016180576058104634

Training epoch-9 batch-23
Running loss of epoch-9 batch-23 = 0.0049826656468212605

Training epoch-9 batch-24
Running loss of epoch-9 batch-24 = 0.00301883602514863

Training epoch-9 batch-25
Running loss of epoch-9 batch-25 = 0.0017499078530818224

Training epoch-9 batch-26
Running loss of epoch-9 batch-26 = 0.0043331473134458065

Training epoch-9 batch-27
Running loss of epoch-9 batch-27 = 0.004140323959290981

Training epoch-9 batch-28
Running loss of epoch-9 batch-28 = 0.003234941978007555

Training epoch-9 batch-29
Running loss of epoch-9 batch-29 = 0.0026661637239158154

Training epoch-9 batch-30
Running loss of epoch-9 batch-30 = 0.002817930653691292

Training epoch-9 batch-31
Running loss of epoch-9 batch-31 = 0.0027576014399528503

Training epoch-9 batch-32
Running loss of epoch-9 batch-32 = 0.0022134752944111824

Training epoch-9 batch-33
Running loss of epoch-9 batch-33 = 0.0024248980917036533

Training epoch-9 batch-34
Running loss of epoch-9 batch-34 = 0.003986596129834652

Training epoch-9 batch-35
Running loss of epoch-9 batch-35 = 0.0027424246072769165

Training epoch-9 batch-36
Running loss of epoch-9 batch-36 = 0.003698276821523905

Training epoch-9 batch-37
Running loss of epoch-9 batch-37 = 0.0028916168957948685

Training epoch-9 batch-38
Running loss of epoch-9 batch-38 = 0.0032721529714763165

Training epoch-9 batch-39
Running loss of epoch-9 batch-39 = 0.0014063072158023715

Training epoch-9 batch-40
Running loss of epoch-9 batch-40 = 0.0036930451169610023

Training epoch-9 batch-41
Running loss of epoch-9 batch-41 = 0.004412779118865728

Training epoch-9 batch-42
Running loss of epoch-9 batch-42 = 0.002625004155561328

Training epoch-9 batch-43
Running loss of epoch-9 batch-43 = 0.0026708173099905252

Training epoch-9 batch-44
Running loss of epoch-9 batch-44 = 0.002823099959641695

Training epoch-9 batch-45
Running loss of epoch-9 batch-45 = 0.0035995924845337868

Training epoch-9 batch-46
Running loss of epoch-9 batch-46 = 0.00215714774094522

Training epoch-9 batch-47
Running loss of epoch-9 batch-47 = 0.0029257626738399267

Training epoch-9 batch-48
Running loss of epoch-9 batch-48 = 0.0028919619508087635

Training epoch-9 batch-49
Running loss of epoch-9 batch-49 = 0.0030132888350635767

Training epoch-9 batch-50
Running loss of epoch-9 batch-50 = 0.003140946850180626

Training epoch-9 batch-51
Running loss of epoch-9 batch-51 = 0.0037315792869776487

Training epoch-9 batch-52
Running loss of epoch-9 batch-52 = 0.0017971504712477326

Training epoch-9 batch-53
Running loss of epoch-9 batch-53 = 0.002819797722622752

Training epoch-9 batch-54
Running loss of epoch-9 batch-54 = 0.0034057069569826126

Training epoch-9 batch-55
Running loss of epoch-9 batch-55 = 0.004180217161774635

Training epoch-9 batch-56
Running loss of epoch-9 batch-56 = 0.002148360013961792

Training epoch-9 batch-57
Running loss of epoch-9 batch-57 = 0.002075224881991744

Training epoch-9 batch-58
Running loss of epoch-9 batch-58 = 0.002628931775689125

Training epoch-9 batch-59
Running loss of epoch-9 batch-59 = 0.002310869749635458

Training epoch-9 batch-60
Running loss of epoch-9 batch-60 = 0.002854269230738282

Training epoch-9 batch-61
Running loss of epoch-9 batch-61 = 0.004462819080799818

Training epoch-9 batch-62
Running loss of epoch-9 batch-62 = 0.0029663508757948875

Training epoch-9 batch-63
Running loss of epoch-9 batch-63 = 0.003722148248925805

Training epoch-9 batch-64
Running loss of epoch-9 batch-64 = 0.002269182587042451

Training epoch-9 batch-65
Running loss of epoch-9 batch-65 = 0.0032648169435560703

Training epoch-9 batch-66
Running loss of epoch-9 batch-66 = 0.0018184654181823134

Training epoch-9 batch-67
Running loss of epoch-9 batch-67 = 0.0027642908971756697

Training epoch-9 batch-68
Running loss of epoch-9 batch-68 = 0.0026356836315244436

Training epoch-9 batch-69
Running loss of epoch-9 batch-69 = 0.002762861317023635

Training epoch-9 batch-70
Running loss of epoch-9 batch-70 = 0.002910871058702469

Training epoch-9 batch-71
Running loss of epoch-9 batch-71 = 0.002006939612329006

Training epoch-9 batch-72
Running loss of epoch-9 batch-72 = 0.0035859893541783094

Training epoch-9 batch-73
Running loss of epoch-9 batch-73 = 0.0024443159345537424

Training epoch-9 batch-74
Running loss of epoch-9 batch-74 = 0.0029635271057486534

Training epoch-9 batch-75
Running loss of epoch-9 batch-75 = 0.0040351953357458115

Training epoch-9 batch-76
Running loss of epoch-9 batch-76 = 0.0034614636097103357

Training epoch-9 batch-77
Running loss of epoch-9 batch-77 = 0.0029087932780385017

Training epoch-9 batch-78
Running loss of epoch-9 batch-78 = 0.0022303799632936716

Training epoch-9 batch-79
Running loss of epoch-9 batch-79 = 0.003839937038719654

Training epoch-9 batch-80
Running loss of epoch-9 batch-80 = 0.002730350010097027

Training epoch-9 batch-81
Running loss of epoch-9 batch-81 = 0.0024673165753483772

Training epoch-9 batch-82
Running loss of epoch-9 batch-82 = 0.0019032496493309736

Training epoch-9 batch-83
Running loss of epoch-9 batch-83 = 0.001881468459032476

Training epoch-9 batch-84
Running loss of epoch-9 batch-84 = 0.0019923278596252203

Training epoch-9 batch-85
Running loss of epoch-9 batch-85 = 0.002570485230535269

Training epoch-9 batch-86
Running loss of epoch-9 batch-86 = 0.0016776404809206724

Training epoch-9 batch-87
Running loss of epoch-9 batch-87 = 0.0017962324200198054

Training epoch-9 batch-88
Running loss of epoch-9 batch-88 = 0.0034819552674889565

Training epoch-9 batch-89
Running loss of epoch-9 batch-89 = 0.003598846262320876

Training epoch-9 batch-90
Running loss of epoch-9 batch-90 = 0.0017506281146779656

Training epoch-9 batch-91
Running loss of epoch-9 batch-91 = 0.003859446384012699

Training epoch-9 batch-92
Running loss of epoch-9 batch-92 = 0.0028749341145157814

Training epoch-9 batch-93
Running loss of epoch-9 batch-93 = 0.004058883525431156

Training epoch-9 batch-94
Running loss of epoch-9 batch-94 = 0.0015131281688809395

Training epoch-9 batch-95
Running loss of epoch-9 batch-95 = 0.0030939187854528427

Training epoch-9 batch-96
Running loss of epoch-9 batch-96 = 0.0024775746278464794

Training epoch-9 batch-97
Running loss of epoch-9 batch-97 = 0.0032743369229137897

Training epoch-9 batch-98
Running loss of epoch-9 batch-98 = 0.002180117880925536

Training epoch-9 batch-99
Running loss of epoch-9 batch-99 = 0.002116293180733919

Training epoch-9 batch-100
Running loss of epoch-9 batch-100 = 0.0025801658630371094

Training epoch-9 batch-101
Running loss of epoch-9 batch-101 = 0.0022395134437829256

Training epoch-9 batch-102
Running loss of epoch-9 batch-102 = 0.0033056624233722687

Training epoch-9 batch-103
Running loss of epoch-9 batch-103 = 0.0036309834104031324

Training epoch-9 batch-104
Running loss of epoch-9 batch-104 = 0.00300065940245986

Training epoch-9 batch-105
Running loss of epoch-9 batch-105 = 0.002009959891438484

Training epoch-9 batch-106
Running loss of epoch-9 batch-106 = 0.0016360763693228364

Training epoch-9 batch-107
Running loss of epoch-9 batch-107 = 0.004296415485441685

Training epoch-9 batch-108
Running loss of epoch-9 batch-108 = 0.003561311401426792

Training epoch-9 batch-109
Running loss of epoch-9 batch-109 = 0.002278827363625169

Training epoch-9 batch-110
Running loss of epoch-9 batch-110 = 0.00328328600153327

Training epoch-9 batch-111
Running loss of epoch-9 batch-111 = 0.0025232997722923756

Training epoch-9 batch-112
Running loss of epoch-9 batch-112 = 0.003775007789954543

Training epoch-9 batch-113
Running loss of epoch-9 batch-113 = 0.002111142734065652

Training epoch-9 batch-114
Running loss of epoch-9 batch-114 = 0.0034855525009334087

Training epoch-9 batch-115
Running loss of epoch-9 batch-115 = 0.0027194858994334936

Training epoch-9 batch-116
Running loss of epoch-9 batch-116 = 0.002746199257671833

Training epoch-9 batch-117
Running loss of epoch-9 batch-117 = 0.0025642376858741045

Training epoch-9 batch-118
Running loss of epoch-9 batch-118 = 0.0033550525549799204

Training epoch-9 batch-119
Running loss of epoch-9 batch-119 = 0.0036226974334567785

Training epoch-9 batch-120
Running loss of epoch-9 batch-120 = 0.002541240770369768

Training epoch-9 batch-121
Running loss of epoch-9 batch-121 = 0.0017405918333679438

Training epoch-9 batch-122
Running loss of epoch-9 batch-122 = 0.0035903356038033962

Training epoch-9 batch-123
Running loss of epoch-9 batch-123 = 0.004817718639969826

Training epoch-9 batch-124
Running loss of epoch-9 batch-124 = 0.002904034685343504

Training epoch-9 batch-125
Running loss of epoch-9 batch-125 = 0.0022645588032901287

Training epoch-9 batch-126
Running loss of epoch-9 batch-126 = 0.0024608438834547997

Training epoch-9 batch-127
Running loss of epoch-9 batch-127 = 0.0020171436481177807

Training epoch-9 batch-128
Running loss of epoch-9 batch-128 = 0.0035802717320621014

Training epoch-9 batch-129
Running loss of epoch-9 batch-129 = 0.007678309455513954

Training epoch-9 batch-130
Running loss of epoch-9 batch-130 = 0.003761394415050745

Training epoch-9 batch-131
Running loss of epoch-9 batch-131 = 0.00221819756552577

Training epoch-9 batch-132
Running loss of epoch-9 batch-132 = 0.0020291530527174473

Training epoch-9 batch-133
Running loss of epoch-9 batch-133 = 0.0028842880856245756

Training epoch-9 batch-134
Running loss of epoch-9 batch-134 = 0.0038929772563278675

Training epoch-9 batch-135
Running loss of epoch-9 batch-135 = 0.0026841796934604645

Training epoch-9 batch-136
Running loss of epoch-9 batch-136 = 0.003552665701135993

Training epoch-9 batch-137
Running loss of epoch-9 batch-137 = 0.003988662734627724

Training epoch-9 batch-138
Running loss of epoch-9 batch-138 = 0.003146878443658352

Training epoch-9 batch-139
Running loss of epoch-9 batch-139 = 0.003405890427529812

Training epoch-9 batch-140
Running loss of epoch-9 batch-140 = 0.002846929244697094

Training epoch-9 batch-141
Running loss of epoch-9 batch-141 = 0.003091193735599518

Training epoch-9 batch-142
Running loss of epoch-9 batch-142 = 0.0018494720570743084

Training epoch-9 batch-143
Running loss of epoch-9 batch-143 = 0.001865382306277752

Training epoch-9 batch-144
Running loss of epoch-9 batch-144 = 0.0044670430943369865

Training epoch-9 batch-145
Running loss of epoch-9 batch-145 = 0.003254437819123268

Training epoch-9 batch-146
Running loss of epoch-9 batch-146 = 0.0015783803537487984

Training epoch-9 batch-147
Running loss of epoch-9 batch-147 = 0.0024870354682207108

Training epoch-9 batch-148
Running loss of epoch-9 batch-148 = 0.005044782534241676

Training epoch-9 batch-149
Running loss of epoch-9 batch-149 = 0.0016598005313426256

Training epoch-9 batch-150
Running loss of epoch-9 batch-150 = 0.0028993026353418827

Training epoch-9 batch-151
Running loss of epoch-9 batch-151 = 0.0027049861382693052

Training epoch-9 batch-152
Running loss of epoch-9 batch-152 = 0.003016657428815961

Training epoch-9 batch-153
Running loss of epoch-9 batch-153 = 0.004924410954117775

Training epoch-9 batch-154
Running loss of epoch-9 batch-154 = 0.004360808525234461

Training epoch-9 batch-155
Running loss of epoch-9 batch-155 = 0.006594548467546701

Training epoch-9 batch-156
Running loss of epoch-9 batch-156 = 0.0034038883168250322

Training epoch-9 batch-157
Running loss of epoch-9 batch-157 = 0.026023484766483307

Finished training epoch-9.



Average train loss at epoch-9 = 0.0031013509914278982

Started Evaluation

Average val loss at epoch-9 = 2.352651272165148

Accuracy for classes:
Accuracy for class equals is: 77.39 %
Accuracy for class main is: 37.05 %
Accuracy for class setUp is: 71.48 %
Accuracy for class onCreate is: 53.09 %
Accuracy for class toString is: 41.30 %
Accuracy for class run is: 40.87 %
Accuracy for class hashCode is: 74.53 %
Accuracy for class init is: 19.51 %
Accuracy for class execute is: 16.47 %
Accuracy for class get is: 45.64 %

Overall Accuracy = 50.22 %

Finished Evaluation



Started training epoch-10


Training epoch-10 batch-1
Running loss of epoch-10 batch-1 = 0.0017494488274678588

Training epoch-10 batch-2
Running loss of epoch-10 batch-2 = 0.005124208517372608

Training epoch-10 batch-3
Running loss of epoch-10 batch-3 = 0.007581755053251982

Training epoch-10 batch-4
Running loss of epoch-10 batch-4 = 0.0021599074825644493

Training epoch-10 batch-5
Running loss of epoch-10 batch-5 = 0.0017734374850988388

Training epoch-10 batch-6
Running loss of epoch-10 batch-6 = 0.004911302588880062

Training epoch-10 batch-7
Running loss of epoch-10 batch-7 = 0.004288180731236935

Training epoch-10 batch-8
Running loss of epoch-10 batch-8 = 0.005360293667763472

Training epoch-10 batch-9
Running loss of epoch-10 batch-9 = 0.0013397062430158257

Training epoch-10 batch-10
Running loss of epoch-10 batch-10 = 0.0015981356846168637

Training epoch-10 batch-11
Running loss of epoch-10 batch-11 = 0.0020236619748175144

Training epoch-10 batch-12
Running loss of epoch-10 batch-12 = 0.003523056861013174

Training epoch-10 batch-13
Running loss of epoch-10 batch-13 = 0.005252848379313946

Training epoch-10 batch-14
Running loss of epoch-10 batch-14 = 0.003861584234982729

Training epoch-10 batch-15
Running loss of epoch-10 batch-15 = 0.004953737370669842

Training epoch-10 batch-16
Running loss of epoch-10 batch-16 = 0.004862035624682903

Training epoch-10 batch-17
Running loss of epoch-10 batch-17 = 0.0053559886291623116

Training epoch-10 batch-18
Running loss of epoch-10 batch-18 = 0.0030329497531056404

Training epoch-10 batch-19
Running loss of epoch-10 batch-19 = 0.003050071420148015

Training epoch-10 batch-20
Running loss of epoch-10 batch-20 = 0.0036155306734144688

Training epoch-10 batch-21
Running loss of epoch-10 batch-21 = 0.003892519511282444

Training epoch-10 batch-22
Running loss of epoch-10 batch-22 = 0.001783232088200748

Training epoch-10 batch-23
Running loss of epoch-10 batch-23 = 0.0031507154926657677

Training epoch-10 batch-24
Running loss of epoch-10 batch-24 = 0.0035941614769399166

Training epoch-10 batch-25
Running loss of epoch-10 batch-25 = 0.002713654190301895

Training epoch-10 batch-26
Running loss of epoch-10 batch-26 = 0.0015682498924434185

Training epoch-10 batch-27
Running loss of epoch-10 batch-27 = 0.0024519828148186207

Training epoch-10 batch-28
Running loss of epoch-10 batch-28 = 0.002551410114392638

Training epoch-10 batch-29
Running loss of epoch-10 batch-29 = 0.0012603063369169831

Training epoch-10 batch-30
Running loss of epoch-10 batch-30 = 0.0023282405454665422

Training epoch-10 batch-31
Running loss of epoch-10 batch-31 = 0.0018698681378737092

Training epoch-10 batch-32
Running loss of epoch-10 batch-32 = 0.0020225336775183678

Training epoch-10 batch-33
Running loss of epoch-10 batch-33 = 0.00452011451125145

Training epoch-10 batch-34
Running loss of epoch-10 batch-34 = 0.0013774273684248328

Training epoch-10 batch-35
Running loss of epoch-10 batch-35 = 0.0015832793433219194

Training epoch-10 batch-36
Running loss of epoch-10 batch-36 = 0.0037884805351495743

Training epoch-10 batch-37
Running loss of epoch-10 batch-37 = 0.001866459846496582

Training epoch-10 batch-38
Running loss of epoch-10 batch-38 = 0.0024451862554997206

Training epoch-10 batch-39
Running loss of epoch-10 batch-39 = 0.0019278919789940119

Training epoch-10 batch-40
Running loss of epoch-10 batch-40 = 0.002150012645870447

Training epoch-10 batch-41
Running loss of epoch-10 batch-41 = 0.002466881647706032

Training epoch-10 batch-42
Running loss of epoch-10 batch-42 = 0.0009530975949019194

Training epoch-10 batch-43
Running loss of epoch-10 batch-43 = 0.0034140925854444504

Training epoch-10 batch-44
Running loss of epoch-10 batch-44 = 0.0027048392221331596

Training epoch-10 batch-45
Running loss of epoch-10 batch-45 = 0.0013484873343259096

Training epoch-10 batch-46
Running loss of epoch-10 batch-46 = 0.0016800279263406992

Training epoch-10 batch-47
Running loss of epoch-10 batch-47 = 0.0018213223665952682

Training epoch-10 batch-48
Running loss of epoch-10 batch-48 = 0.0012501922901719809

Training epoch-10 batch-49
Running loss of epoch-10 batch-49 = 0.0019704403821378946

Training epoch-10 batch-50
Running loss of epoch-10 batch-50 = 0.0014004992553964257

Training epoch-10 batch-51
Running loss of epoch-10 batch-51 = 0.0015173364663496614

Training epoch-10 batch-52
Running loss of epoch-10 batch-52 = 0.0020662955939769745

Training epoch-10 batch-53
Running loss of epoch-10 batch-53 = 0.00170931126922369

Training epoch-10 batch-54
Running loss of epoch-10 batch-54 = 0.0009566383669152856

Training epoch-10 batch-55
Running loss of epoch-10 batch-55 = 0.0019392818212509155

Training epoch-10 batch-56
Running loss of epoch-10 batch-56 = 0.0011419515358284116

Training epoch-10 batch-57
Running loss of epoch-10 batch-57 = 0.0024371049366891384

Training epoch-10 batch-58
Running loss of epoch-10 batch-58 = 0.0008296280866488814

Training epoch-10 batch-59
Running loss of epoch-10 batch-59 = 0.0020680606830865145

Training epoch-10 batch-60
Running loss of epoch-10 batch-60 = 0.0019599671941250563

Training epoch-10 batch-61
Running loss of epoch-10 batch-61 = 0.001285113045014441

Training epoch-10 batch-62
Running loss of epoch-10 batch-62 = 0.0007345440099015832

Training epoch-10 batch-63
Running loss of epoch-10 batch-63 = 0.0016122861998155713

Training epoch-10 batch-64
Running loss of epoch-10 batch-64 = 0.004229620099067688

Training epoch-10 batch-65
Running loss of epoch-10 batch-65 = 0.0009135028813034296

Training epoch-10 batch-66
Running loss of epoch-10 batch-66 = 0.001401554443873465

Training epoch-10 batch-67
Running loss of epoch-10 batch-67 = 0.0011818758212029934

Training epoch-10 batch-68
Running loss of epoch-10 batch-68 = 0.0023696061689406633

Training epoch-10 batch-69
Running loss of epoch-10 batch-69 = 0.0027951295487582684

Training epoch-10 batch-70
Running loss of epoch-10 batch-70 = 0.0016139531508088112

Training epoch-10 batch-71
Running loss of epoch-10 batch-71 = 0.0016650879988446832

Training epoch-10 batch-72
Running loss of epoch-10 batch-72 = 0.0010820756433531642

Training epoch-10 batch-73
Running loss of epoch-10 batch-73 = 0.002856356557458639

Training epoch-10 batch-74
Running loss of epoch-10 batch-74 = 0.001686848932877183

Training epoch-10 batch-75
Running loss of epoch-10 batch-75 = 0.0017467456636950374

Training epoch-10 batch-76
Running loss of epoch-10 batch-76 = 0.0022422000765800476

Training epoch-10 batch-77
Running loss of epoch-10 batch-77 = 0.0014826235128566623

Training epoch-10 batch-78
Running loss of epoch-10 batch-78 = 0.00295228511095047

Training epoch-10 batch-79
Running loss of epoch-10 batch-79 = 0.001402519061230123

Training epoch-10 batch-80
Running loss of epoch-10 batch-80 = 0.003051850711926818

Training epoch-10 batch-81
Running loss of epoch-10 batch-81 = 0.001094676903448999

Training epoch-10 batch-82
Running loss of epoch-10 batch-82 = 0.0007090965518727899

Training epoch-10 batch-83
Running loss of epoch-10 batch-83 = 0.0021899498533457518

Training epoch-10 batch-84
Running loss of epoch-10 batch-84 = 0.0007489841664209962

Training epoch-10 batch-85
Running loss of epoch-10 batch-85 = 0.001751052332110703

Training epoch-10 batch-86
Running loss of epoch-10 batch-86 = 0.0011084036668762565

Training epoch-10 batch-87
Running loss of epoch-10 batch-87 = 0.002691596280783415

Training epoch-10 batch-88
Running loss of epoch-10 batch-88 = 0.0023126699961721897

Training epoch-10 batch-89
Running loss of epoch-10 batch-89 = 0.002151935826987028

Training epoch-10 batch-90
Running loss of epoch-10 batch-90 = 0.0025713033974170685

Training epoch-10 batch-91
Running loss of epoch-10 batch-91 = 0.0015837971586734056

Training epoch-10 batch-92
Running loss of epoch-10 batch-92 = 0.0012549395905807614

Training epoch-10 batch-93
Running loss of epoch-10 batch-93 = 0.0010163565166294575

Training epoch-10 batch-94
Running loss of epoch-10 batch-94 = 0.003452109405770898

Training epoch-10 batch-95
Running loss of epoch-10 batch-95 = 0.0018592342967167497

Training epoch-10 batch-96
Running loss of epoch-10 batch-96 = 0.0011754705337807536

Training epoch-10 batch-97
Running loss of epoch-10 batch-97 = 0.0010949515271931887

Training epoch-10 batch-98
Running loss of epoch-10 batch-98 = 0.0008781349752098322

Training epoch-10 batch-99
Running loss of epoch-10 batch-99 = 0.001385884708724916

Training epoch-10 batch-100
Running loss of epoch-10 batch-100 = 0.0030245985835790634

Training epoch-10 batch-101
Running loss of epoch-10 batch-101 = 0.0008290495024994016

Training epoch-10 batch-102
Running loss of epoch-10 batch-102 = 0.0028684157878160477

Training epoch-10 batch-103
Running loss of epoch-10 batch-103 = 0.0016053899889811873

Training epoch-10 batch-104
Running loss of epoch-10 batch-104 = 0.0012467559427022934

Training epoch-10 batch-105
Running loss of epoch-10 batch-105 = 0.0022462750785052776

Training epoch-10 batch-106
Running loss of epoch-10 batch-106 = 0.0012441729195415974

Training epoch-10 batch-107
Running loss of epoch-10 batch-107 = 0.0016371223609894514

Training epoch-10 batch-108
Running loss of epoch-10 batch-108 = 0.002524950774386525

Training epoch-10 batch-109
Running loss of epoch-10 batch-109 = 0.0022404761984944344

Training epoch-10 batch-110
Running loss of epoch-10 batch-110 = 0.0012749716406688094

Training epoch-10 batch-111
Running loss of epoch-10 batch-111 = 0.002786986529827118

Training epoch-10 batch-112
Running loss of epoch-10 batch-112 = 0.0022067015524953604

Training epoch-10 batch-113
Running loss of epoch-10 batch-113 = 0.002507082186639309

Training epoch-10 batch-114
Running loss of epoch-10 batch-114 = 0.0025892737321555614

Training epoch-10 batch-115
Running loss of epoch-10 batch-115 = 0.003868450177833438

Training epoch-10 batch-116
Running loss of epoch-10 batch-116 = 0.0029821349307894707

Training epoch-10 batch-117
Running loss of epoch-10 batch-117 = 0.0022623110562562943

Training epoch-10 batch-118
Running loss of epoch-10 batch-118 = 0.0017687026411294937

Training epoch-10 batch-119
Running loss of epoch-10 batch-119 = 0.0016284043667837977

Training epoch-10 batch-120
Running loss of epoch-10 batch-120 = 0.0028488244861364365

Training epoch-10 batch-121
Running loss of epoch-10 batch-121 = 0.0017958597745746374

Training epoch-10 batch-122
Running loss of epoch-10 batch-122 = 0.003315775189548731

Training epoch-10 batch-123
Running loss of epoch-10 batch-123 = 0.001140385982580483

Training epoch-10 batch-124
Running loss of epoch-10 batch-124 = 0.0019070538692176342

Training epoch-10 batch-125
Running loss of epoch-10 batch-125 = 0.0017955830553546548

Training epoch-10 batch-126
Running loss of epoch-10 batch-126 = 0.0027227243408560753

Training epoch-10 batch-127
Running loss of epoch-10 batch-127 = 0.002305550267919898

Training epoch-10 batch-128
Running loss of epoch-10 batch-128 = 0.002070679096505046

Training epoch-10 batch-129
Running loss of epoch-10 batch-129 = 0.002958984114229679

Training epoch-10 batch-130
Running loss of epoch-10 batch-130 = 0.002031010342761874

Training epoch-10 batch-131
Running loss of epoch-10 batch-131 = 0.002111973939463496

Training epoch-10 batch-132
Running loss of epoch-10 batch-132 = 0.0037339935079216957

Training epoch-10 batch-133
Running loss of epoch-10 batch-133 = 0.0028783525340259075

Training epoch-10 batch-134
Running loss of epoch-10 batch-134 = 0.0008166249608621001

Training epoch-10 batch-135
Running loss of epoch-10 batch-135 = 0.0013189972378313541

Training epoch-10 batch-136
Running loss of epoch-10 batch-136 = 0.0024115764535963535

Training epoch-10 batch-137
Running loss of epoch-10 batch-137 = 0.0016228083986788988

Training epoch-10 batch-138
Running loss of epoch-10 batch-138 = 0.002036422025412321

Training epoch-10 batch-139
Running loss of epoch-10 batch-139 = 0.0038018901832401752

Training epoch-10 batch-140
Running loss of epoch-10 batch-140 = 0.0008579837158322334

Training epoch-10 batch-141
Running loss of epoch-10 batch-141 = 0.0012269231956452131

Training epoch-10 batch-142
Running loss of epoch-10 batch-142 = 0.0029745446518063545

Training epoch-10 batch-143
Running loss of epoch-10 batch-143 = 0.0027590144891291857

Training epoch-10 batch-144
Running loss of epoch-10 batch-144 = 0.0015694948378950357

Training epoch-10 batch-145
Running loss of epoch-10 batch-145 = 0.0014888399746268988

Training epoch-10 batch-146
Running loss of epoch-10 batch-146 = 0.0032030073925852776

Training epoch-10 batch-147
Running loss of epoch-10 batch-147 = 0.0014124986482784152

Training epoch-10 batch-148
Running loss of epoch-10 batch-148 = 0.004019679501652718

Training epoch-10 batch-149
Running loss of epoch-10 batch-149 = 0.0026911734603345394

Training epoch-10 batch-150
Running loss of epoch-10 batch-150 = 0.0038082068786025047

Training epoch-10 batch-151
Running loss of epoch-10 batch-151 = 0.0015857595717534423

Training epoch-10 batch-152
Running loss of epoch-10 batch-152 = 0.0008828068384900689

Training epoch-10 batch-153
Running loss of epoch-10 batch-153 = 0.0018707968993112445

Training epoch-10 batch-154
Running loss of epoch-10 batch-154 = 0.0018935409607365727

Training epoch-10 batch-155
Running loss of epoch-10 batch-155 = 0.0039412472397089005

Training epoch-10 batch-156
Running loss of epoch-10 batch-156 = 0.0026303089689463377

Training epoch-10 batch-157
Running loss of epoch-10 batch-157 = 0.008936140686273575

Finished training epoch-10.



Average train loss at epoch-10 = 0.0022992160573601724

Started Evaluation

Average val loss at epoch-10 = 2.491809285000751

Accuracy for classes:
Accuracy for class equals is: 81.68 %
Accuracy for class main is: 50.82 %
Accuracy for class setUp is: 45.90 %
Accuracy for class onCreate is: 47.87 %
Accuracy for class toString is: 49.49 %
Accuracy for class run is: 42.69 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 25.34 %
Accuracy for class execute is: 30.52 %
Accuracy for class get is: 28.72 %

Overall Accuracy = 49.16 %

Finished Evaluation



Started training epoch-11


Training epoch-11 batch-1
Running loss of epoch-11 batch-1 = 0.0006162520730867982

Training epoch-11 batch-2
Running loss of epoch-11 batch-2 = 0.0024437427055090666

Training epoch-11 batch-3
Running loss of epoch-11 batch-3 = 0.002098974771797657

Training epoch-11 batch-4
Running loss of epoch-11 batch-4 = 0.001425037276931107

Training epoch-11 batch-5
Running loss of epoch-11 batch-5 = 0.0022438447922468185

Training epoch-11 batch-6
Running loss of epoch-11 batch-6 = 0.0007630853215232491

Training epoch-11 batch-7
Running loss of epoch-11 batch-7 = 0.0007828354137018323

Training epoch-11 batch-8
Running loss of epoch-11 batch-8 = 0.0009655854664742947

Training epoch-11 batch-9
Running loss of epoch-11 batch-9 = 0.0015061681624501944

Training epoch-11 batch-10
Running loss of epoch-11 batch-10 = 0.0024587223306298256

Training epoch-11 batch-11
Running loss of epoch-11 batch-11 = 0.001852397108450532

Training epoch-11 batch-12
Running loss of epoch-11 batch-12 = 0.0020611637737601995

Training epoch-11 batch-13
Running loss of epoch-11 batch-13 = 0.001247862004674971

Training epoch-11 batch-14
Running loss of epoch-11 batch-14 = 0.0010794728295877576

Training epoch-11 batch-15
Running loss of epoch-11 batch-15 = 0.001717400853522122

Training epoch-11 batch-16
Running loss of epoch-11 batch-16 = 0.0008961103158071637

Training epoch-11 batch-17
Running loss of epoch-11 batch-17 = 0.001177965197712183

Training epoch-11 batch-18
Running loss of epoch-11 batch-18 = 0.0012763795675709844

Training epoch-11 batch-19
Running loss of epoch-11 batch-19 = 0.0008551015052944422

Training epoch-11 batch-20
Running loss of epoch-11 batch-20 = 0.0013079516356810927

Training epoch-11 batch-21
Running loss of epoch-11 batch-21 = 0.0012043180176988244

Training epoch-11 batch-22
Running loss of epoch-11 batch-22 = 0.0005707078380510211

Training epoch-11 batch-23
Running loss of epoch-11 batch-23 = 0.0018058465793728828

Training epoch-11 batch-24
Running loss of epoch-11 batch-24 = 0.0012852861545979977

Training epoch-11 batch-25
Running loss of epoch-11 batch-25 = 0.0006278440123423934

Training epoch-11 batch-26
Running loss of epoch-11 batch-26 = 0.0016711244825273752

Training epoch-11 batch-27
Running loss of epoch-11 batch-27 = 0.0017075478099286556

Training epoch-11 batch-28
Running loss of epoch-11 batch-28 = 0.0009552610572427511

Training epoch-11 batch-29
Running loss of epoch-11 batch-29 = 0.001250816392712295

Training epoch-11 batch-30
Running loss of epoch-11 batch-30 = 0.002051100367680192

Training epoch-11 batch-31
Running loss of epoch-11 batch-31 = 0.0010696050012484193

Training epoch-11 batch-32
Running loss of epoch-11 batch-32 = 0.0008006037678569555

Training epoch-11 batch-33
Running loss of epoch-11 batch-33 = 0.0020957021042704582

Training epoch-11 batch-34
Running loss of epoch-11 batch-34 = 0.001796984113752842

Training epoch-11 batch-35
Running loss of epoch-11 batch-35 = 0.0021608832757920027

Training epoch-11 batch-36
Running loss of epoch-11 batch-36 = 0.0004406186053529382

Training epoch-11 batch-37
Running loss of epoch-11 batch-37 = 0.00042555108666419983

Training epoch-11 batch-38
Running loss of epoch-11 batch-38 = 0.0012606421951204538

Training epoch-11 batch-39
Running loss of epoch-11 batch-39 = 0.0003024735487997532

Training epoch-11 batch-40
Running loss of epoch-11 batch-40 = 0.0010697062825784087

Training epoch-11 batch-41
Running loss of epoch-11 batch-41 = 0.002363333711400628

Training epoch-11 batch-42
Running loss of epoch-11 batch-42 = 0.0007053184090182185

Training epoch-11 batch-43
Running loss of epoch-11 batch-43 = 0.0013537562917917967

Training epoch-11 batch-44
Running loss of epoch-11 batch-44 = 0.0011499833781272173

Training epoch-11 batch-45
Running loss of epoch-11 batch-45 = 0.001437717699445784

Training epoch-11 batch-46
Running loss of epoch-11 batch-46 = 0.0005566414911299944

Training epoch-11 batch-47
Running loss of epoch-11 batch-47 = 0.0005474669160321355

Training epoch-11 batch-48
Running loss of epoch-11 batch-48 = 0.0008791779400780797

Training epoch-11 batch-49
Running loss of epoch-11 batch-49 = 0.0006530629470944405

Training epoch-11 batch-50
Running loss of epoch-11 batch-50 = 0.001495135365985334

Training epoch-11 batch-51
Running loss of epoch-11 batch-51 = 0.0010669527109712362

Training epoch-11 batch-52
Running loss of epoch-11 batch-52 = 0.0015038968995213509

Training epoch-11 batch-53
Running loss of epoch-11 batch-53 = 0.0007713226368650794

Training epoch-11 batch-54
Running loss of epoch-11 batch-54 = 0.0005826374981552362

Training epoch-11 batch-55
Running loss of epoch-11 batch-55 = 0.0005579943535849452

Training epoch-11 batch-56
Running loss of epoch-11 batch-56 = 0.0012508395593613386

Training epoch-11 batch-57
Running loss of epoch-11 batch-57 = 0.001289541251026094

Training epoch-11 batch-58
Running loss of epoch-11 batch-58 = 0.0012159458128735423

Training epoch-11 batch-59
Running loss of epoch-11 batch-59 = 0.0010116229532286525

Training epoch-11 batch-60
Running loss of epoch-11 batch-60 = 0.0007156888023018837

Training epoch-11 batch-61
Running loss of epoch-11 batch-61 = 0.0003937085857614875

Training epoch-11 batch-62
Running loss of epoch-11 batch-62 = 0.0005865888088010252

Training epoch-11 batch-63
Running loss of epoch-11 batch-63 = 0.0005916613154113293

Training epoch-11 batch-64
Running loss of epoch-11 batch-64 = 0.00034186779521405697

Training epoch-11 batch-65
Running loss of epoch-11 batch-65 = 0.000850820797495544

Training epoch-11 batch-66
Running loss of epoch-11 batch-66 = 0.000744581688195467

Training epoch-11 batch-67
Running loss of epoch-11 batch-67 = 0.0009230212308466434

Training epoch-11 batch-68
Running loss of epoch-11 batch-68 = 0.0005984812742099166

Training epoch-11 batch-69
Running loss of epoch-11 batch-69 = 0.0003192252479493618

Training epoch-11 batch-70
Running loss of epoch-11 batch-70 = 0.0007998303044587374

Training epoch-11 batch-71
Running loss of epoch-11 batch-71 = 0.0002313415752723813

Training epoch-11 batch-72
Running loss of epoch-11 batch-72 = 0.0005806153640151024

Training epoch-11 batch-73
Running loss of epoch-11 batch-73 = 0.0002188320504501462

Training epoch-11 batch-74
Running loss of epoch-11 batch-74 = 0.0005414473125711083

Training epoch-11 batch-75
Running loss of epoch-11 batch-75 = 0.0008706027874723077

Training epoch-11 batch-76
Running loss of epoch-11 batch-76 = 0.0010836407309398055

Training epoch-11 batch-77
Running loss of epoch-11 batch-77 = 0.0005968704354017973

Training epoch-11 batch-78
Running loss of epoch-11 batch-78 = 0.0004673652583733201

Training epoch-11 batch-79
Running loss of epoch-11 batch-79 = 0.0004542182432487607

Training epoch-11 batch-80
Running loss of epoch-11 batch-80 = 0.0006174825830385089

Training epoch-11 batch-81
Running loss of epoch-11 batch-81 = 0.0015494098188355565

Training epoch-11 batch-82
Running loss of epoch-11 batch-82 = 0.002006974769756198

Training epoch-11 batch-83
Running loss of epoch-11 batch-83 = 0.0006335516227409244

Training epoch-11 batch-84
Running loss of epoch-11 batch-84 = 0.0007488896371796727

Training epoch-11 batch-85
Running loss of epoch-11 batch-85 = 0.0015453062951564789

Training epoch-11 batch-86
Running loss of epoch-11 batch-86 = 0.0007672276115044951

Training epoch-11 batch-87
Running loss of epoch-11 batch-87 = 0.0004863828653469682

Training epoch-11 batch-88
Running loss of epoch-11 batch-88 = 0.0011794238816946745

Training epoch-11 batch-89
Running loss of epoch-11 batch-89 = 0.0008158024866133928

Training epoch-11 batch-90
Running loss of epoch-11 batch-90 = 0.0012903597671538591

Training epoch-11 batch-91
Running loss of epoch-11 batch-91 = 0.0012682327069342136

Training epoch-11 batch-92
Running loss of epoch-11 batch-92 = 0.000335307908244431

Training epoch-11 batch-93
Running loss of epoch-11 batch-93 = 0.0006558705354109406

Training epoch-11 batch-94
Running loss of epoch-11 batch-94 = 0.0012183680664747953

Training epoch-11 batch-95
Running loss of epoch-11 batch-95 = 0.000659144832752645

Training epoch-11 batch-96
Running loss of epoch-11 batch-96 = 0.0008434701012447476

Training epoch-11 batch-97
Running loss of epoch-11 batch-97 = 0.0005378632340580225

Training epoch-11 batch-98
Running loss of epoch-11 batch-98 = 0.0005848559085279703

Training epoch-11 batch-99
Running loss of epoch-11 batch-99 = 0.00037977693136781454

Training epoch-11 batch-100
Running loss of epoch-11 batch-100 = 0.0004351973766461015

Training epoch-11 batch-101
Running loss of epoch-11 batch-101 = 0.0008894358761608601

Training epoch-11 batch-102
Running loss of epoch-11 batch-102 = 0.0008479594253003597

Training epoch-11 batch-103
Running loss of epoch-11 batch-103 = 0.0003916379064321518

Training epoch-11 batch-104
Running loss of epoch-11 batch-104 = 0.00044630991760641336

Training epoch-11 batch-105
Running loss of epoch-11 batch-105 = 0.0004700173158198595

Training epoch-11 batch-106
Running loss of epoch-11 batch-106 = 0.00032215449027717113

Training epoch-11 batch-107
Running loss of epoch-11 batch-107 = 0.0007292421068996191

Training epoch-11 batch-108
Running loss of epoch-11 batch-108 = 0.00028068898245692253

Training epoch-11 batch-109
Running loss of epoch-11 batch-109 = 0.00044932193122804165

Training epoch-11 batch-110
Running loss of epoch-11 batch-110 = 0.0005823562387377024

Training epoch-11 batch-111
Running loss of epoch-11 batch-111 = 0.0005543761653825641

Training epoch-11 batch-112
Running loss of epoch-11 batch-112 = 0.0012740768725052476

Training epoch-11 batch-113
Running loss of epoch-11 batch-113 = 0.0007409945828840137

Training epoch-11 batch-114
Running loss of epoch-11 batch-114 = 0.0003189870622009039

Training epoch-11 batch-115
Running loss of epoch-11 batch-115 = 0.0010144500993192196

Training epoch-11 batch-116
Running loss of epoch-11 batch-116 = 0.0010539655340835452

Training epoch-11 batch-117
Running loss of epoch-11 batch-117 = 0.0006449602078646421

Training epoch-11 batch-118
Running loss of epoch-11 batch-118 = 0.0003291877801530063

Training epoch-11 batch-119
Running loss of epoch-11 batch-119 = 0.0008879470406100154

Training epoch-11 batch-120
Running loss of epoch-11 batch-120 = 0.0004192462656646967

Training epoch-11 batch-121
Running loss of epoch-11 batch-121 = 0.00047797057777643204

Training epoch-11 batch-122
Running loss of epoch-11 batch-122 = 0.0004643956199288368

Training epoch-11 batch-123
Running loss of epoch-11 batch-123 = 0.0021723718382418156

Training epoch-11 batch-124
Running loss of epoch-11 batch-124 = 0.0017559885745868087

Training epoch-11 batch-125
Running loss of epoch-11 batch-125 = 0.0007336587877944112

Training epoch-11 batch-126
Running loss of epoch-11 batch-126 = 0.0005015262868255377

Training epoch-11 batch-127
Running loss of epoch-11 batch-127 = 0.0005859582452103496

Training epoch-11 batch-128
Running loss of epoch-11 batch-128 = 0.0018959192093461752

Training epoch-11 batch-129
Running loss of epoch-11 batch-129 = 0.0017175197135657072

Training epoch-11 batch-130
Running loss of epoch-11 batch-130 = 0.0010524556273594499

Training epoch-11 batch-131
Running loss of epoch-11 batch-131 = 0.0004507052944973111

Training epoch-11 batch-132
Running loss of epoch-11 batch-132 = 0.0007799331797286868

Training epoch-11 batch-133
Running loss of epoch-11 batch-133 = 0.0006491146050393581

Training epoch-11 batch-134
Running loss of epoch-11 batch-134 = 0.0007058533374220133

Training epoch-11 batch-135
Running loss of epoch-11 batch-135 = 0.0025991243310272694

Training epoch-11 batch-136
Running loss of epoch-11 batch-136 = 0.0005401019006967545

Training epoch-11 batch-137
Running loss of epoch-11 batch-137 = 0.0037908763624727726

Training epoch-11 batch-138
Running loss of epoch-11 batch-138 = 0.0005116324173286557

Training epoch-11 batch-139
Running loss of epoch-11 batch-139 = 0.0008585606701672077

Training epoch-11 batch-140
Running loss of epoch-11 batch-140 = 0.001008810242637992

Training epoch-11 batch-141
Running loss of epoch-11 batch-141 = 0.0006260110531002283

Training epoch-11 batch-142
Running loss of epoch-11 batch-142 = 0.0007480696076527238

Training epoch-11 batch-143
Running loss of epoch-11 batch-143 = 0.001408000010997057

Training epoch-11 batch-144
Running loss of epoch-11 batch-144 = 0.0009380752453580499

Training epoch-11 batch-145
Running loss of epoch-11 batch-145 = 0.00032613473013043404

Training epoch-11 batch-146
Running loss of epoch-11 batch-146 = 0.0006802696734666824

Training epoch-11 batch-147
Running loss of epoch-11 batch-147 = 0.0013175125932320952

Training epoch-11 batch-148
Running loss of epoch-11 batch-148 = 0.000578272040002048

Training epoch-11 batch-149
Running loss of epoch-11 batch-149 = 0.0008826223202049732

Training epoch-11 batch-150
Running loss of epoch-11 batch-150 = 0.0006477405549958348

Training epoch-11 batch-151
Running loss of epoch-11 batch-151 = 0.0006352022755891085

Training epoch-11 batch-152
Running loss of epoch-11 batch-152 = 0.0017472609179094434

Training epoch-11 batch-153
Running loss of epoch-11 batch-153 = 0.00044499465730041265

Training epoch-11 batch-154
Running loss of epoch-11 batch-154 = 0.00036329252179712057

Training epoch-11 batch-155
Running loss of epoch-11 batch-155 = 0.000838755164295435

Training epoch-11 batch-156
Running loss of epoch-11 batch-156 = 0.0011727556120604277

Training epoch-11 batch-157
Running loss of epoch-11 batch-157 = 0.0023211631923913956

Finished training epoch-11.



Average train loss at epoch-11 = 0.0009857624612748623

Started Evaluation

Average val loss at epoch-11 = 2.5951312984290875

Accuracy for classes:
Accuracy for class equals is: 72.11 %
Accuracy for class main is: 61.31 %
Accuracy for class setUp is: 42.95 %
Accuracy for class onCreate is: 59.49 %
Accuracy for class toString is: 57.34 %
Accuracy for class run is: 26.94 %
Accuracy for class hashCode is: 72.28 %
Accuracy for class init is: 20.85 %
Accuracy for class execute is: 33.73 %
Accuracy for class get is: 38.72 %

Overall Accuracy = 50.30 %

Finished Evaluation



Started training epoch-12


Training epoch-12 batch-1
Running loss of epoch-12 batch-1 = 0.0006445153267122805

Training epoch-12 batch-2
Running loss of epoch-12 batch-2 = 0.0020693521946668625

Training epoch-12 batch-3
Running loss of epoch-12 batch-3 = 0.0013421368785202503

Training epoch-12 batch-4
Running loss of epoch-12 batch-4 = 0.0004801553441211581

Training epoch-12 batch-5
Running loss of epoch-12 batch-5 = 0.0010269776685163379

Training epoch-12 batch-6
Running loss of epoch-12 batch-6 = 0.00040365313179790974

Training epoch-12 batch-7
Running loss of epoch-12 batch-7 = 0.0005714005092158914

Training epoch-12 batch-8
Running loss of epoch-12 batch-8 = 0.001158796832896769

Training epoch-12 batch-9
Running loss of epoch-12 batch-9 = 0.0007413385901600122

Training epoch-12 batch-10
Running loss of epoch-12 batch-10 = 0.00034018722362816334

Training epoch-12 batch-11
Running loss of epoch-12 batch-11 = 0.0004435391165316105

Training epoch-12 batch-12
Running loss of epoch-12 batch-12 = 0.0006732814945280552

Training epoch-12 batch-13
Running loss of epoch-12 batch-13 = 0.0006468890933319926

Training epoch-12 batch-14
Running loss of epoch-12 batch-14 = 0.000804333365522325

Training epoch-12 batch-15
Running loss of epoch-12 batch-15 = 0.00042978720739483833

Training epoch-12 batch-16
Running loss of epoch-12 batch-16 = 0.0012682090746238828

Training epoch-12 batch-17
Running loss of epoch-12 batch-17 = 0.0008048019371926785

Training epoch-12 batch-18
Running loss of epoch-12 batch-18 = 0.00014859263319522142

Training epoch-12 batch-19
Running loss of epoch-12 batch-19 = 0.00020654092077165842

Training epoch-12 batch-20
Running loss of epoch-12 batch-20 = 0.0003242267994210124

Training epoch-12 batch-21
Running loss of epoch-12 batch-21 = 0.0005313765723258257

Training epoch-12 batch-22
Running loss of epoch-12 batch-22 = 0.0006430544890463352

Training epoch-12 batch-23
Running loss of epoch-12 batch-23 = 0.0013666949234902859

Training epoch-12 batch-24
Running loss of epoch-12 batch-24 = 0.0006771825719624758

Training epoch-12 batch-25
Running loss of epoch-12 batch-25 = 0.0002847659634426236

Training epoch-12 batch-26
Running loss of epoch-12 batch-26 = 0.00028937356546521187

Training epoch-12 batch-27
Running loss of epoch-12 batch-27 = 0.0002968752523884177

Training epoch-12 batch-28
Running loss of epoch-12 batch-28 = 0.00020599830895662308

Training epoch-12 batch-29
Running loss of epoch-12 batch-29 = 0.00018521619495004416

Training epoch-12 batch-30
Running loss of epoch-12 batch-30 = 0.00019373255781829357

Training epoch-12 batch-31
Running loss of epoch-12 batch-31 = 0.0007181863766163588

Training epoch-12 batch-32
Running loss of epoch-12 batch-32 = 0.00026729702949523926

Training epoch-12 batch-33
Running loss of epoch-12 batch-33 = 0.0003588638501241803

Training epoch-12 batch-34
Running loss of epoch-12 batch-34 = 0.00029064633417874575

Training epoch-12 batch-35
Running loss of epoch-12 batch-35 = 0.00036992039531469345

Training epoch-12 batch-36
Running loss of epoch-12 batch-36 = 0.0007734190439805388

Training epoch-12 batch-37
Running loss of epoch-12 batch-37 = 0.00043588841799646616

Training epoch-12 batch-38
Running loss of epoch-12 batch-38 = 0.00018330919556319714

Training epoch-12 batch-39
Running loss of epoch-12 batch-39 = 0.00013668113388121128

Training epoch-12 batch-40
Running loss of epoch-12 batch-40 = 0.00017050770111382008

Training epoch-12 batch-41
Running loss of epoch-12 batch-41 = 0.00019806891214102507

Training epoch-12 batch-42
Running loss of epoch-12 batch-42 = 0.00021320919040590525

Training epoch-12 batch-43
Running loss of epoch-12 batch-43 = 0.00026034237816929817

Training epoch-12 batch-44
Running loss of epoch-12 batch-44 = 0.0001731442753225565

Training epoch-12 batch-45
Running loss of epoch-12 batch-45 = 0.0005187984788790345

Training epoch-12 batch-46
Running loss of epoch-12 batch-46 = 0.0002000222448259592

Training epoch-12 batch-47
Running loss of epoch-12 batch-47 = 0.0001816463191062212

Training epoch-12 batch-48
Running loss of epoch-12 batch-48 = 0.0001091208541765809

Training epoch-12 batch-49
Running loss of epoch-12 batch-49 = 0.00021926756016910076

Training epoch-12 batch-50
Running loss of epoch-12 batch-50 = 0.0009115322027355433

Training epoch-12 batch-51
Running loss of epoch-12 batch-51 = 0.0002880179090425372

Training epoch-12 batch-52
Running loss of epoch-12 batch-52 = 0.0002099008997902274

Training epoch-12 batch-53
Running loss of epoch-12 batch-53 = 0.0004783006152138114

Training epoch-12 batch-54
Running loss of epoch-12 batch-54 = 0.00014313298743218184

Training epoch-12 batch-55
Running loss of epoch-12 batch-55 = 0.00016869371756911278

Training epoch-12 batch-56
Running loss of epoch-12 batch-56 = 0.0003119775792583823

Training epoch-12 batch-57
Running loss of epoch-12 batch-57 = 8.438271470367908e-05

Training epoch-12 batch-58
Running loss of epoch-12 batch-58 = 0.002647876041010022

Training epoch-12 batch-59
Running loss of epoch-12 batch-59 = 0.00045315828174352646

Training epoch-12 batch-60
Running loss of epoch-12 batch-60 = 0.00014308292884379625

Training epoch-12 batch-61
Running loss of epoch-12 batch-61 = 0.000150438048876822

Training epoch-12 batch-62
Running loss of epoch-12 batch-62 = 0.00032515195198357105

Training epoch-12 batch-63
Running loss of epoch-12 batch-63 = 0.0012716299388557673

Training epoch-12 batch-64
Running loss of epoch-12 batch-64 = 0.001604564138688147

Training epoch-12 batch-65
Running loss of epoch-12 batch-65 = 0.0003005982143804431

Training epoch-12 batch-66
Running loss of epoch-12 batch-66 = 0.0006196311442181468

Training epoch-12 batch-67
Running loss of epoch-12 batch-67 = 0.0003456700360402465

Training epoch-12 batch-68
Running loss of epoch-12 batch-68 = 0.0002129306085407734

Training epoch-12 batch-69
Running loss of epoch-12 batch-69 = 0.0010317080887034535

Training epoch-12 batch-70
Running loss of epoch-12 batch-70 = 0.00012246647384017706

Training epoch-12 batch-71
Running loss of epoch-12 batch-71 = 0.0006438503041863441

Training epoch-12 batch-72
Running loss of epoch-12 batch-72 = 0.00037838262505829334

Training epoch-12 batch-73
Running loss of epoch-12 batch-73 = 0.00046763091813772917

Training epoch-12 batch-74
Running loss of epoch-12 batch-74 = 0.0004908137489110231

Training epoch-12 batch-75
Running loss of epoch-12 batch-75 = 0.0007127617718651891

Training epoch-12 batch-76
Running loss of epoch-12 batch-76 = 0.00010881538037210703

Training epoch-12 batch-77
Running loss of epoch-12 batch-77 = 0.00020042061805725098

Training epoch-12 batch-78
Running loss of epoch-12 batch-78 = 0.00018052721861749887

Training epoch-12 batch-79
Running loss of epoch-12 batch-79 = 0.0002780637005344033

Training epoch-12 batch-80
Running loss of epoch-12 batch-80 = 0.0006259798537939787

Training epoch-12 batch-81
Running loss of epoch-12 batch-81 = 0.0007219824474304914

Training epoch-12 batch-82
Running loss of epoch-12 batch-82 = 0.00037027616053819656

Training epoch-12 batch-83
Running loss of epoch-12 batch-83 = 0.00028683163691312075

Training epoch-12 batch-84
Running loss of epoch-12 batch-84 = 0.000448190257884562

Training epoch-12 batch-85
Running loss of epoch-12 batch-85 = 0.000355348689481616

Training epoch-12 batch-86
Running loss of epoch-12 batch-86 = 0.0003107985248789191

Training epoch-12 batch-87
Running loss of epoch-12 batch-87 = 0.0003417922998778522

Training epoch-12 batch-88
Running loss of epoch-12 batch-88 = 0.00017781206406652927

Training epoch-12 batch-89
Running loss of epoch-12 batch-89 = 0.0006385870510712266

Training epoch-12 batch-90
Running loss of epoch-12 batch-90 = 0.00032776291482150555

Training epoch-12 batch-91
Running loss of epoch-12 batch-91 = 0.0001995818456634879

Training epoch-12 batch-92
Running loss of epoch-12 batch-92 = 0.000264375121332705

Training epoch-12 batch-93
Running loss of epoch-12 batch-93 = 0.00016569672152400017

Training epoch-12 batch-94
Running loss of epoch-12 batch-94 = 0.00020359386689960957

Training epoch-12 batch-95
Running loss of epoch-12 batch-95 = 0.0002293719444423914

Training epoch-12 batch-96
Running loss of epoch-12 batch-96 = 0.0005246533546596766

Training epoch-12 batch-97
Running loss of epoch-12 batch-97 = 0.00013471755664795637

Training epoch-12 batch-98
Running loss of epoch-12 batch-98 = 0.0005677019944414496

Training epoch-12 batch-99
Running loss of epoch-12 batch-99 = 0.0002085238229483366

Training epoch-12 batch-100
Running loss of epoch-12 batch-100 = 8.098932448774576e-05

Training epoch-12 batch-101
Running loss of epoch-12 batch-101 = 0.0002586693735793233

Training epoch-12 batch-102
Running loss of epoch-12 batch-102 = 0.0006327931769192219

Training epoch-12 batch-103
Running loss of epoch-12 batch-103 = 0.00014939880929887295

Training epoch-12 batch-104
Running loss of epoch-12 batch-104 = 0.0002769492566585541

Training epoch-12 batch-105
Running loss of epoch-12 batch-105 = 0.00013599952217191458

Training epoch-12 batch-106
Running loss of epoch-12 batch-106 = 0.0005666707875207067

Training epoch-12 batch-107
Running loss of epoch-12 batch-107 = 0.0003317317459732294

Training epoch-12 batch-108
Running loss of epoch-12 batch-108 = 0.00010896811727434397

Training epoch-12 batch-109
Running loss of epoch-12 batch-109 = 0.0002979155397042632

Training epoch-12 batch-110
Running loss of epoch-12 batch-110 = 0.00011386617552489042

Training epoch-12 batch-111
Running loss of epoch-12 batch-111 = 0.0003313786583021283

Training epoch-12 batch-112
Running loss of epoch-12 batch-112 = 0.0001140005188062787

Training epoch-12 batch-113
Running loss of epoch-12 batch-113 = 0.00017745955847203732

Training epoch-12 batch-114
Running loss of epoch-12 batch-114 = 0.00017875817138701677

Training epoch-12 batch-115
Running loss of epoch-12 batch-115 = 0.00017022248357534409

Training epoch-12 batch-116
Running loss of epoch-12 batch-116 = 0.000337558682076633

Training epoch-12 batch-117
Running loss of epoch-12 batch-117 = 0.0002641541650518775

Training epoch-12 batch-118
Running loss of epoch-12 batch-118 = 0.00036773504689335823

Training epoch-12 batch-119
Running loss of epoch-12 batch-119 = 0.00019679975230246782

Training epoch-12 batch-120
Running loss of epoch-12 batch-120 = 0.0004513661842793226

Training epoch-12 batch-121
Running loss of epoch-12 batch-121 = 0.00010758009739220142

Training epoch-12 batch-122
Running loss of epoch-12 batch-122 = 0.0001433714060112834

Training epoch-12 batch-123
Running loss of epoch-12 batch-123 = 0.00015996303409337997

Training epoch-12 batch-124
Running loss of epoch-12 batch-124 = 0.0003697893116623163

Training epoch-12 batch-125
Running loss of epoch-12 batch-125 = 0.00019829627126455307

Training epoch-12 batch-126
Running loss of epoch-12 batch-126 = 0.00014698179438710213

Training epoch-12 batch-127
Running loss of epoch-12 batch-127 = 0.00023530470207333565

Training epoch-12 batch-128
Running loss of epoch-12 batch-128 = 0.00024341384414583445

Training epoch-12 batch-129
Running loss of epoch-12 batch-129 = 0.0004134235205128789

Training epoch-12 batch-130
Running loss of epoch-12 batch-130 = 0.00021068938076496124

Training epoch-12 batch-131
Running loss of epoch-12 batch-131 = 0.00018247636035084724

Training epoch-12 batch-132
Running loss of epoch-12 batch-132 = 0.00011249538511037827

Training epoch-12 batch-133
Running loss of epoch-12 batch-133 = 0.00011584802996367216

Training epoch-12 batch-134
Running loss of epoch-12 batch-134 = 0.00012227857951074839

Training epoch-12 batch-135
Running loss of epoch-12 batch-135 = 0.00013124500401318073

Training epoch-12 batch-136
Running loss of epoch-12 batch-136 = 0.0004176566144451499

Training epoch-12 batch-137
Running loss of epoch-12 batch-137 = 0.0003098022425547242

Training epoch-12 batch-138
Running loss of epoch-12 batch-138 = 0.0001796196447685361

Training epoch-12 batch-139
Running loss of epoch-12 batch-139 = 0.00010547845158725977

Training epoch-12 batch-140
Running loss of epoch-12 batch-140 = 0.00017874303739517927

Training epoch-12 batch-141
Running loss of epoch-12 batch-141 = 0.00013103685341775417

Training epoch-12 batch-142
Running loss of epoch-12 batch-142 = 0.0002920076949521899

Training epoch-12 batch-143
Running loss of epoch-12 batch-143 = 0.00024167029187083244

Training epoch-12 batch-144
Running loss of epoch-12 batch-144 = 8.751475252211094e-05

Training epoch-12 batch-145
Running loss of epoch-12 batch-145 = 0.0001486785477027297

Training epoch-12 batch-146
Running loss of epoch-12 batch-146 = 0.00011889741290360689

Training epoch-12 batch-147
Running loss of epoch-12 batch-147 = 0.0001480509527027607

Training epoch-12 batch-148
Running loss of epoch-12 batch-148 = 6.31050206720829e-05

Training epoch-12 batch-149
Running loss of epoch-12 batch-149 = 0.00021965615451335907

Training epoch-12 batch-150
Running loss of epoch-12 batch-150 = 0.00014810124412178993

Training epoch-12 batch-151
Running loss of epoch-12 batch-151 = 0.00023816991597414017

Training epoch-12 batch-152
Running loss of epoch-12 batch-152 = 0.00015131861437112093

Training epoch-12 batch-153
Running loss of epoch-12 batch-153 = 5.2005983889102936e-05

Training epoch-12 batch-154
Running loss of epoch-12 batch-154 = 0.0001703518209978938

Training epoch-12 batch-155
Running loss of epoch-12 batch-155 = 0.00028279994148761034

Training epoch-12 batch-156
Running loss of epoch-12 batch-156 = 0.00010642956476658583

Training epoch-12 batch-157
Running loss of epoch-12 batch-157 = 9.889528155326843e-05

Finished training epoch-12.



Average train loss at epoch-12 = 0.00038158959820866587

Started Evaluation

Average val loss at epoch-12 = 2.454722283702148

Accuracy for classes:
Accuracy for class equals is: 75.74 %
Accuracy for class main is: 61.48 %
Accuracy for class setUp is: 53.11 %
Accuracy for class onCreate is: 59.28 %
Accuracy for class toString is: 51.88 %
Accuracy for class run is: 33.56 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 38.15 %
Accuracy for class get is: 37.95 %

Overall Accuracy = 53.77 %

Finished Evaluation



Started training epoch-13


Training epoch-13 batch-1
Running loss of epoch-13 batch-1 = 8.679216261953115e-05

Training epoch-13 batch-2
Running loss of epoch-13 batch-2 = 0.00012180744670331478

Training epoch-13 batch-3
Running loss of epoch-13 batch-3 = 0.0001110368175432086

Training epoch-13 batch-4
Running loss of epoch-13 batch-4 = 4.994263872504234e-05

Training epoch-13 batch-5
Running loss of epoch-13 batch-5 = 0.0001315085683017969

Training epoch-13 batch-6
Running loss of epoch-13 batch-6 = 0.0002652282128110528

Training epoch-13 batch-7
Running loss of epoch-13 batch-7 = 0.0001946783158928156

Training epoch-13 batch-8
Running loss of epoch-13 batch-8 = 0.0001910533756017685

Training epoch-13 batch-9
Running loss of epoch-13 batch-9 = 0.00013468519318848848

Training epoch-13 batch-10
Running loss of epoch-13 batch-10 = 5.384220276027918e-05

Training epoch-13 batch-11
Running loss of epoch-13 batch-11 = 7.375015411525965e-05

Training epoch-13 batch-12
Running loss of epoch-13 batch-12 = 5.1963492296636105e-05

Training epoch-13 batch-13
Running loss of epoch-13 batch-13 = 0.0001310983207076788

Training epoch-13 batch-14
Running loss of epoch-13 batch-14 = 9.913055691868067e-05

Training epoch-13 batch-15
Running loss of epoch-13 batch-15 = 6.668665446341038e-05

Training epoch-13 batch-16
Running loss of epoch-13 batch-16 = 7.961760275065899e-05

Training epoch-13 batch-17
Running loss of epoch-13 batch-17 = 6.424286402761936e-05

Training epoch-13 batch-18
Running loss of epoch-13 batch-18 = 5.007232539355755e-05

Training epoch-13 batch-19
Running loss of epoch-13 batch-19 = 9.679363574832678e-05

Training epoch-13 batch-20
Running loss of epoch-13 batch-20 = 7.545715197920799e-05

Training epoch-13 batch-21
Running loss of epoch-13 batch-21 = 4.3286243453621864e-05

Training epoch-13 batch-22
Running loss of epoch-13 batch-22 = 7.126503624022007e-05

Training epoch-13 batch-23
Running loss of epoch-13 batch-23 = 7.614248897880316e-05

Training epoch-13 batch-24
Running loss of epoch-13 batch-24 = 6.895838305354118e-05

Training epoch-13 batch-25
Running loss of epoch-13 batch-25 = 9.15484270080924e-05

Training epoch-13 batch-26
Running loss of epoch-13 batch-26 = 4.678324330598116e-05

Training epoch-13 batch-27
Running loss of epoch-13 batch-27 = 0.00010362535249441862

Training epoch-13 batch-28
Running loss of epoch-13 batch-28 = 7.617601659148932e-05

Training epoch-13 batch-29
Running loss of epoch-13 batch-29 = 4.109693691134453e-05

Training epoch-13 batch-30
Running loss of epoch-13 batch-30 = 0.0001221079146489501

Training epoch-13 batch-31
Running loss of epoch-13 batch-31 = 0.00010201812256127596

Training epoch-13 batch-32
Running loss of epoch-13 batch-32 = 5.782803054898977e-05

Training epoch-13 batch-33
Running loss of epoch-13 batch-33 = 5.738029722124338e-05

Training epoch-13 batch-34
Running loss of epoch-13 batch-34 = 7.526949048042297e-05

Training epoch-13 batch-35
Running loss of epoch-13 batch-35 = 4.722503945231438e-05

Training epoch-13 batch-36
Running loss of epoch-13 batch-36 = 0.00010395352728664875

Training epoch-13 batch-37
Running loss of epoch-13 batch-37 = 6.084004417061806e-05

Training epoch-13 batch-38
Running loss of epoch-13 batch-38 = 6.845046300441027e-05

Training epoch-13 batch-39
Running loss of epoch-13 batch-39 = 0.00010312267113476992

Training epoch-13 batch-40
Running loss of epoch-13 batch-40 = 0.0001751212403178215

Training epoch-13 batch-41
Running loss of epoch-13 batch-41 = 5.570508074015379e-05

Training epoch-13 batch-42
Running loss of epoch-13 batch-42 = 0.00015268323477357626

Training epoch-13 batch-43
Running loss of epoch-13 batch-43 = 7.556518539786339e-05

Training epoch-13 batch-44
Running loss of epoch-13 batch-44 = 4.207529127597809e-05

Training epoch-13 batch-45
Running loss of epoch-13 batch-45 = 3.8223108276724815e-05

Training epoch-13 batch-46
Running loss of epoch-13 batch-46 = 0.00010768533684313297

Training epoch-13 batch-47
Running loss of epoch-13 batch-47 = 6.434426177293062e-05

Training epoch-13 batch-48
Running loss of epoch-13 batch-48 = 5.247665103524923e-05

Training epoch-13 batch-49
Running loss of epoch-13 batch-49 = 7.165537681430578e-05

Training epoch-13 batch-50
Running loss of epoch-13 batch-50 = 4.810676909983158e-05

Training epoch-13 batch-51
Running loss of epoch-13 batch-51 = 9.84810758382082e-05

Training epoch-13 batch-52
Running loss of epoch-13 batch-52 = 7.713888771831989e-05

Training epoch-13 batch-53
Running loss of epoch-13 batch-53 = 2.6199035346508026e-05

Training epoch-13 batch-54
Running loss of epoch-13 batch-54 = 5.7229772210121155e-05

Training epoch-13 batch-55
Running loss of epoch-13 batch-55 = 7.489451672881842e-05

Training epoch-13 batch-56
Running loss of epoch-13 batch-56 = 6.643880624324083e-05

Training epoch-13 batch-57
Running loss of epoch-13 batch-57 = 6.083888001739979e-05

Training epoch-13 batch-58
Running loss of epoch-13 batch-58 = 9.436416439712048e-05

Training epoch-13 batch-59
Running loss of epoch-13 batch-59 = 4.986103158444166e-05

Training epoch-13 batch-60
Running loss of epoch-13 batch-60 = 6.335403304547071e-05

Training epoch-13 batch-61
Running loss of epoch-13 batch-61 = 0.00012568256352096796

Training epoch-13 batch-62
Running loss of epoch-13 batch-62 = 4.5291148126125336e-05

Training epoch-13 batch-63
Running loss of epoch-13 batch-63 = 5.81077765673399e-05

Training epoch-13 batch-64
Running loss of epoch-13 batch-64 = 4.167028237134218e-05

Training epoch-13 batch-65
Running loss of epoch-13 batch-65 = 7.17903021723032e-05

Training epoch-13 batch-66
Running loss of epoch-13 batch-66 = 3.875419497489929e-05

Training epoch-13 batch-67
Running loss of epoch-13 batch-67 = 0.0001341740135103464

Training epoch-13 batch-68
Running loss of epoch-13 batch-68 = 4.07298794016242e-05

Training epoch-13 batch-69
Running loss of epoch-13 batch-69 = 8.649949450045824e-05

Training epoch-13 batch-70
Running loss of epoch-13 batch-70 = 0.00011031236499547958

Training epoch-13 batch-71
Running loss of epoch-13 batch-71 = 2.9900111258029938e-05

Training epoch-13 batch-72
Running loss of epoch-13 batch-72 = 9.235506877303123e-05

Training epoch-13 batch-73
Running loss of epoch-13 batch-73 = 4.616216756403446e-05

Training epoch-13 batch-74
Running loss of epoch-13 batch-74 = 9.704462718218565e-05

Training epoch-13 batch-75
Running loss of epoch-13 batch-75 = 8.154939860105515e-05

Training epoch-13 batch-76
Running loss of epoch-13 batch-76 = 7.433746941387653e-05

Training epoch-13 batch-77
Running loss of epoch-13 batch-77 = 6.63163373246789e-05

Training epoch-13 batch-78
Running loss of epoch-13 batch-78 = 5.497701931744814e-05

Training epoch-13 batch-79
Running loss of epoch-13 batch-79 = 6.463169120252132e-05

Training epoch-13 batch-80
Running loss of epoch-13 batch-80 = 7.084326352924109e-05

Training epoch-13 batch-81
Running loss of epoch-13 batch-81 = 6.817840039730072e-05

Training epoch-13 batch-82
Running loss of epoch-13 batch-82 = 3.534206189215183e-05

Training epoch-13 batch-83
Running loss of epoch-13 batch-83 = 9.922042954713106e-05

Training epoch-13 batch-84
Running loss of epoch-13 batch-84 = 2.5341752916574478e-05

Training epoch-13 batch-85
Running loss of epoch-13 batch-85 = 6.487220525741577e-05

Training epoch-13 batch-86
Running loss of epoch-13 batch-86 = 6.420013960450888e-05

Training epoch-13 batch-87
Running loss of epoch-13 batch-87 = 6.372551433742046e-05

Training epoch-13 batch-88
Running loss of epoch-13 batch-88 = 7.570500019937754e-05

Training epoch-13 batch-89
Running loss of epoch-13 batch-89 = 7.872353307902813e-05

Training epoch-13 batch-90
Running loss of epoch-13 batch-90 = 6.19466882199049e-05

Training epoch-13 batch-91
Running loss of epoch-13 batch-91 = 7.18564260751009e-05

Training epoch-13 batch-92
Running loss of epoch-13 batch-92 = 8.465151768177748e-05

Training epoch-13 batch-93
Running loss of epoch-13 batch-93 = 7.260101847350597e-05

Training epoch-13 batch-94
Running loss of epoch-13 batch-94 = 4.4843065552413464e-05

Training epoch-13 batch-95
Running loss of epoch-13 batch-95 = 8.103006985038519e-05

Training epoch-13 batch-96
Running loss of epoch-13 batch-96 = 4.4099753722548485e-05

Training epoch-13 batch-97
Running loss of epoch-13 batch-97 = 5.5689713917672634e-05

Training epoch-13 batch-98
Running loss of epoch-13 batch-98 = 5.6267017498612404e-05

Training epoch-13 batch-99
Running loss of epoch-13 batch-99 = 3.870343789458275e-05

Training epoch-13 batch-100
Running loss of epoch-13 batch-100 = 8.162122685462236e-05

Training epoch-13 batch-101
Running loss of epoch-13 batch-101 = 9.924336336553097e-05

Training epoch-13 batch-102
Running loss of epoch-13 batch-102 = 6.324879359453917e-05

Training epoch-13 batch-103
Running loss of epoch-13 batch-103 = 0.00014768028631806374

Training epoch-13 batch-104
Running loss of epoch-13 batch-104 = 5.4669566452503204e-05

Training epoch-13 batch-105
Running loss of epoch-13 batch-105 = 5.6480057537555695e-05

Training epoch-13 batch-106
Running loss of epoch-13 batch-106 = 0.00011146720498800278

Training epoch-13 batch-107
Running loss of epoch-13 batch-107 = 5.414662882685661e-05

Training epoch-13 batch-108
Running loss of epoch-13 batch-108 = 2.4973880499601364e-05

Training epoch-13 batch-109
Running loss of epoch-13 batch-109 = 0.0001146842259913683

Training epoch-13 batch-110
Running loss of epoch-13 batch-110 = 9.432085789740086e-05

Training epoch-13 batch-111
Running loss of epoch-13 batch-111 = 5.862338002771139e-05

Training epoch-13 batch-112
Running loss of epoch-13 batch-112 = 4.671758506447077e-05

Training epoch-13 batch-113
Running loss of epoch-13 batch-113 = 2.9981136322021484e-05

Training epoch-13 batch-114
Running loss of epoch-13 batch-114 = 6.766605656594038e-05

Training epoch-13 batch-115
Running loss of epoch-13 batch-115 = 6.183946970850229e-05

Training epoch-13 batch-116
Running loss of epoch-13 batch-116 = 3.618467599153519e-05

Training epoch-13 batch-117
Running loss of epoch-13 batch-117 = 8.460215758532286e-05

Training epoch-13 batch-118
Running loss of epoch-13 batch-118 = 9.604450315237045e-05

Training epoch-13 batch-119
Running loss of epoch-13 batch-119 = 4.690245259553194e-05

Training epoch-13 batch-120
Running loss of epoch-13 batch-120 = 5.940732080489397e-05

Training epoch-13 batch-121
Running loss of epoch-13 batch-121 = 7.214420475065708e-05

Training epoch-13 batch-122
Running loss of epoch-13 batch-122 = 9.104865603148937e-05

Training epoch-13 batch-123
Running loss of epoch-13 batch-123 = 9.186787065118551e-05

Training epoch-13 batch-124
Running loss of epoch-13 batch-124 = 6.732274778187275e-05

Training epoch-13 batch-125
Running loss of epoch-13 batch-125 = 0.00010327552445232868

Training epoch-13 batch-126
Running loss of epoch-13 batch-126 = 4.354212433099747e-05

Training epoch-13 batch-127
Running loss of epoch-13 batch-127 = 0.00010189332533627748

Training epoch-13 batch-128
Running loss of epoch-13 batch-128 = 3.6588520742952824e-05

Training epoch-13 batch-129
Running loss of epoch-13 batch-129 = 5.588878411799669e-05

Training epoch-13 batch-130
Running loss of epoch-13 batch-130 = 4.99009620398283e-05

Training epoch-13 batch-131
Running loss of epoch-13 batch-131 = 4.22564335167408e-05

Training epoch-13 batch-132
Running loss of epoch-13 batch-132 = 0.0001150758471339941

Training epoch-13 batch-133
Running loss of epoch-13 batch-133 = 3.772496711462736e-05

Training epoch-13 batch-134
Running loss of epoch-13 batch-134 = 3.6858487874269485e-05

Training epoch-13 batch-135
Running loss of epoch-13 batch-135 = 6.094831041991711e-05

Training epoch-13 batch-136
Running loss of epoch-13 batch-136 = 8.253124542534351e-05

Training epoch-13 batch-137
Running loss of epoch-13 batch-137 = 3.836303949356079e-05

Training epoch-13 batch-138
Running loss of epoch-13 batch-138 = 5.704467184841633e-05

Training epoch-13 batch-139
Running loss of epoch-13 batch-139 = 3.680749796330929e-05

Training epoch-13 batch-140
Running loss of epoch-13 batch-140 = 6.0044461861252785e-05

Training epoch-13 batch-141
Running loss of epoch-13 batch-141 = 8.356780745089054e-05

Training epoch-13 batch-142
Running loss of epoch-13 batch-142 = 3.5888515412807465e-05

Training epoch-13 batch-143
Running loss of epoch-13 batch-143 = 3.658328205347061e-05

Training epoch-13 batch-144
Running loss of epoch-13 batch-144 = 4.747905768454075e-05

Training epoch-13 batch-145
Running loss of epoch-13 batch-145 = 3.699795342981815e-05

Training epoch-13 batch-146
Running loss of epoch-13 batch-146 = 9.967363439500332e-05

Training epoch-13 batch-147
Running loss of epoch-13 batch-147 = 7.815612480044365e-05

Training epoch-13 batch-148
Running loss of epoch-13 batch-148 = 6.338127423077822e-05

Training epoch-13 batch-149
Running loss of epoch-13 batch-149 = 0.00013327610213309526

Training epoch-13 batch-150
Running loss of epoch-13 batch-150 = 0.00010470987763255835

Training epoch-13 batch-151
Running loss of epoch-13 batch-151 = 6.928283255547285e-05

Training epoch-13 batch-152
Running loss of epoch-13 batch-152 = 0.00010883761569857597

Training epoch-13 batch-153
Running loss of epoch-13 batch-153 = 7.675669621676207e-05

Training epoch-13 batch-154
Running loss of epoch-13 batch-154 = 6.503332406282425e-05

Training epoch-13 batch-155
Running loss of epoch-13 batch-155 = 7.375539280474186e-05

Training epoch-13 batch-156
Running loss of epoch-13 batch-156 = 3.0070310458540916e-05

Training epoch-13 batch-157
Running loss of epoch-13 batch-157 = 5.11854887008667e-05

Finished training epoch-13.



Average train loss at epoch-13 = 7.452744171023368e-05

Started Evaluation

Average val loss at epoch-13 = 2.531368497955172

Accuracy for classes:
Accuracy for class equals is: 75.58 %
Accuracy for class main is: 62.62 %
Accuracy for class setUp is: 62.62 %
Accuracy for class onCreate is: 55.86 %
Accuracy for class toString is: 50.51 %
Accuracy for class run is: 34.25 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 27.13 %
Accuracy for class execute is: 34.94 %
Accuracy for class get is: 36.92 %

Overall Accuracy = 53.93 %

Finished Evaluation



Started training epoch-14


Training epoch-14 batch-1
Running loss of epoch-14 batch-1 = 3.4206430427730083e-05

Training epoch-14 batch-2
Running loss of epoch-14 batch-2 = 1.4745164662599564e-05

Training epoch-14 batch-3
Running loss of epoch-14 batch-3 = 5.660555325448513e-05

Training epoch-14 batch-4
Running loss of epoch-14 batch-4 = 3.725034184753895e-05

Training epoch-14 batch-5
Running loss of epoch-14 batch-5 = 6.028811912983656e-05

Training epoch-14 batch-6
Running loss of epoch-14 batch-6 = 5.5001815780997276e-05

Training epoch-14 batch-7
Running loss of epoch-14 batch-7 = 5.231343675404787e-05

Training epoch-14 batch-8
Running loss of epoch-14 batch-8 = 3.7317629903554916e-05

Training epoch-14 batch-9
Running loss of epoch-14 batch-9 = 5.8067264035344124e-05

Training epoch-14 batch-10
Running loss of epoch-14 batch-10 = 4.755682311952114e-05

Training epoch-14 batch-11
Running loss of epoch-14 batch-11 = 4.3118372559547424e-05

Training epoch-14 batch-12
Running loss of epoch-14 batch-12 = 4.663853906095028e-05

Training epoch-14 batch-13
Running loss of epoch-14 batch-13 = 4.767801146954298e-05

Training epoch-14 batch-14
Running loss of epoch-14 batch-14 = 2.670520916581154e-05

Training epoch-14 batch-15
Running loss of epoch-14 batch-15 = 4.614656791090965e-05

Training epoch-14 batch-16
Running loss of epoch-14 batch-16 = 2.825772389769554e-05

Training epoch-14 batch-17
Running loss of epoch-14 batch-17 = 2.7480535209178925e-05

Training epoch-14 batch-18
Running loss of epoch-14 batch-18 = 3.4607830457389355e-05

Training epoch-14 batch-19
Running loss of epoch-14 batch-19 = 5.204114131629467e-05

Training epoch-14 batch-20
Running loss of epoch-14 batch-20 = 5.0980597734451294e-05

Training epoch-14 batch-21
Running loss of epoch-14 batch-21 = 5.407677963376045e-05

Training epoch-14 batch-22
Running loss of epoch-14 batch-22 = 2.447725273668766e-05

Training epoch-14 batch-23
Running loss of epoch-14 batch-23 = 0.00010226445738226175

Training epoch-14 batch-24
Running loss of epoch-14 batch-24 = 2.8584618121385574e-05

Training epoch-14 batch-25
Running loss of epoch-14 batch-25 = 4.860805347561836e-05

Training epoch-14 batch-26
Running loss of epoch-14 batch-26 = 4.7856010496616364e-05

Training epoch-14 batch-27
Running loss of epoch-14 batch-27 = 6.470386870205402e-05

Training epoch-14 batch-28
Running loss of epoch-14 batch-28 = 2.505071461200714e-05

Training epoch-14 batch-29
Running loss of epoch-14 batch-29 = 4.200066905468702e-05

Training epoch-14 batch-30
Running loss of epoch-14 batch-30 = 5.3610303439199924e-05

Training epoch-14 batch-31
Running loss of epoch-14 batch-31 = 4.910188727080822e-05

Training epoch-14 batch-32
Running loss of epoch-14 batch-32 = 3.17930243909359e-05

Training epoch-14 batch-33
Running loss of epoch-14 batch-33 = 2.3343367502093315e-05

Training epoch-14 batch-34
Running loss of epoch-14 batch-34 = 7.075641769915819e-05

Training epoch-14 batch-35
Running loss of epoch-14 batch-35 = 5.341065116226673e-05

Training epoch-14 batch-36
Running loss of epoch-14 batch-36 = 3.976188600063324e-05

Training epoch-14 batch-37
Running loss of epoch-14 batch-37 = 5.083903670310974e-05

Training epoch-14 batch-38
Running loss of epoch-14 batch-38 = 2.1394342184066772e-05

Training epoch-14 batch-39
Running loss of epoch-14 batch-39 = 4.8585934564471245e-05

Training epoch-14 batch-40
Running loss of epoch-14 batch-40 = 3.005075268447399e-05

Training epoch-14 batch-41
Running loss of epoch-14 batch-41 = 5.582126323133707e-05

Training epoch-14 batch-42
Running loss of epoch-14 batch-42 = 3.361073322594166e-05

Training epoch-14 batch-43
Running loss of epoch-14 batch-43 = 3.3095479011535645e-05

Training epoch-14 batch-44
Running loss of epoch-14 batch-44 = 4.6831672079861164e-05

Training epoch-14 batch-45
Running loss of epoch-14 batch-45 = 4.1111838072538376e-05

Training epoch-14 batch-46
Running loss of epoch-14 batch-46 = 3.8733938708901405e-05

Training epoch-14 batch-47
Running loss of epoch-14 batch-47 = 4.428764805197716e-05

Training epoch-14 batch-48
Running loss of epoch-14 batch-48 = 2.8452370315790176e-05

Training epoch-14 batch-49
Running loss of epoch-14 batch-49 = 3.195879980921745e-05

Training epoch-14 batch-50
Running loss of epoch-14 batch-50 = 3.7833466194570065e-05

Training epoch-14 batch-51
Running loss of epoch-14 batch-51 = 5.067349411547184e-05

Training epoch-14 batch-52
Running loss of epoch-14 batch-52 = 3.10440082103014e-05

Training epoch-14 batch-53
Running loss of epoch-14 batch-53 = 3.5115983337163925e-05

Training epoch-14 batch-54
Running loss of epoch-14 batch-54 = 4.517845809459686e-05

Training epoch-14 batch-55
Running loss of epoch-14 batch-55 = 5.292519927024841e-05

Training epoch-14 batch-56
Running loss of epoch-14 batch-56 = 4.264491144567728e-05

Training epoch-14 batch-57
Running loss of epoch-14 batch-57 = 3.090524114668369e-05

Training epoch-14 batch-58
Running loss of epoch-14 batch-58 = 2.808612771332264e-05

Training epoch-14 batch-59
Running loss of epoch-14 batch-59 = 5.079398397356272e-05

Training epoch-14 batch-60
Running loss of epoch-14 batch-60 = 3.956619184464216e-05

Training epoch-14 batch-61
Running loss of epoch-14 batch-61 = 6.031582597643137e-05

Training epoch-14 batch-62
Running loss of epoch-14 batch-62 = 4.756217822432518e-05

Training epoch-14 batch-63
Running loss of epoch-14 batch-63 = 2.5552348233759403e-05

Training epoch-14 batch-64
Running loss of epoch-14 batch-64 = 3.758573438972235e-05

Training epoch-14 batch-65
Running loss of epoch-14 batch-65 = 4.77437861263752e-05

Training epoch-14 batch-66
Running loss of epoch-14 batch-66 = 3.573414869606495e-05

Training epoch-14 batch-67
Running loss of epoch-14 batch-67 = 3.986223600804806e-05

Training epoch-14 batch-68
Running loss of epoch-14 batch-68 = 2.8219539672136307e-05

Training epoch-14 batch-69
Running loss of epoch-14 batch-69 = 2.7287285774946213e-05

Training epoch-14 batch-70
Running loss of epoch-14 batch-70 = 6.671517621725798e-05

Training epoch-14 batch-71
Running loss of epoch-14 batch-71 = 3.9569567888975143e-05

Training epoch-14 batch-72
Running loss of epoch-14 batch-72 = 5.3406693041324615e-05

Training epoch-14 batch-73
Running loss of epoch-14 batch-73 = 4.475307650864124e-05

Training epoch-14 batch-74
Running loss of epoch-14 batch-74 = 6.174610462039709e-05

Training epoch-14 batch-75
Running loss of epoch-14 batch-75 = 6.778677925467491e-05

Training epoch-14 batch-76
Running loss of epoch-14 batch-76 = 2.3862114176154137e-05

Training epoch-14 batch-77
Running loss of epoch-14 batch-77 = 2.8151553124189377e-05

Training epoch-14 batch-78
Running loss of epoch-14 batch-78 = 4.316447302699089e-05

Training epoch-14 batch-79
Running loss of epoch-14 batch-79 = 3.708701115101576e-05

Training epoch-14 batch-80
Running loss of epoch-14 batch-80 = 4.656054079532623e-05

Training epoch-14 batch-81
Running loss of epoch-14 batch-81 = 3.749714232981205e-05

Training epoch-14 batch-82
Running loss of epoch-14 batch-82 = 2.4332664906978607e-05

Training epoch-14 batch-83
Running loss of epoch-14 batch-83 = 8.519587572664022e-05

Training epoch-14 batch-84
Running loss of epoch-14 batch-84 = 2.7003232389688492e-05

Training epoch-14 batch-85
Running loss of epoch-14 batch-85 = 3.3892691135406494e-05

Training epoch-14 batch-86
Running loss of epoch-14 batch-86 = 3.8716476410627365e-05

Training epoch-14 batch-87
Running loss of epoch-14 batch-87 = 3.651145379990339e-05

Training epoch-14 batch-88
Running loss of epoch-14 batch-88 = 2.7449335902929306e-05

Training epoch-14 batch-89
Running loss of epoch-14 batch-89 = 4.187098238617182e-05

Training epoch-14 batch-90
Running loss of epoch-14 batch-90 = 3.5174423828721046e-05

Training epoch-14 batch-91
Running loss of epoch-14 batch-91 = 4.2799743823707104e-05

Training epoch-14 batch-92
Running loss of epoch-14 batch-92 = 4.8222020268440247e-05

Training epoch-14 batch-93
Running loss of epoch-14 batch-93 = 2.167397178709507e-05

Training epoch-14 batch-94
Running loss of epoch-14 batch-94 = 5.552545189857483e-05

Training epoch-14 batch-95
Running loss of epoch-14 batch-95 = 2.375338226556778e-05

Training epoch-14 batch-96
Running loss of epoch-14 batch-96 = 4.170835018157959e-05

Training epoch-14 batch-97
Running loss of epoch-14 batch-97 = 2.5908229872584343e-05

Training epoch-14 batch-98
Running loss of epoch-14 batch-98 = 5.2712042815983295e-05

Training epoch-14 batch-99
Running loss of epoch-14 batch-99 = 3.758794628083706e-05

Training epoch-14 batch-100
Running loss of epoch-14 batch-100 = 5.1730661652982235e-05

Training epoch-14 batch-101
Running loss of epoch-14 batch-101 = 4.104513209313154e-05

Training epoch-14 batch-102
Running loss of epoch-14 batch-102 = 4.39411960542202e-05

Training epoch-14 batch-103
Running loss of epoch-14 batch-103 = 5.759194027632475e-05

Training epoch-14 batch-104
Running loss of epoch-14 batch-104 = 2.6789959520101547e-05

Training epoch-14 batch-105
Running loss of epoch-14 batch-105 = 3.254693001508713e-05

Training epoch-14 batch-106
Running loss of epoch-14 batch-106 = 3.11101321130991e-05

Training epoch-14 batch-107
Running loss of epoch-14 batch-107 = 2.8402311727404594e-05

Training epoch-14 batch-108
Running loss of epoch-14 batch-108 = 3.704847767949104e-05

Training epoch-14 batch-109
Running loss of epoch-14 batch-109 = 4.382035695016384e-05

Training epoch-14 batch-110
Running loss of epoch-14 batch-110 = 4.234339576214552e-05

Training epoch-14 batch-111
Running loss of epoch-14 batch-111 = 3.628211561590433e-05

Training epoch-14 batch-112
Running loss of epoch-14 batch-112 = 4.865182563662529e-05

Training epoch-14 batch-113
Running loss of epoch-14 batch-113 = 2.6412075385451317e-05

Training epoch-14 batch-114
Running loss of epoch-14 batch-114 = 4.391523543745279e-05

Training epoch-14 batch-115
Running loss of epoch-14 batch-115 = 3.470340743660927e-05

Training epoch-14 batch-116
Running loss of epoch-14 batch-116 = 3.522774204611778e-05

Training epoch-14 batch-117
Running loss of epoch-14 batch-117 = 3.7428922951221466e-05

Training epoch-14 batch-118
Running loss of epoch-14 batch-118 = 9.128300007432699e-05

Training epoch-14 batch-119
Running loss of epoch-14 batch-119 = 1.6926322132349014e-05

Training epoch-14 batch-120
Running loss of epoch-14 batch-120 = 3.042980097234249e-05

Training epoch-14 batch-121
Running loss of epoch-14 batch-121 = 5.086977034807205e-05

Training epoch-14 batch-122
Running loss of epoch-14 batch-122 = 3.3522723242640495e-05

Training epoch-14 batch-123
Running loss of epoch-14 batch-123 = 4.965765401721001e-05

Training epoch-14 batch-124
Running loss of epoch-14 batch-124 = 5.1512615755200386e-05

Training epoch-14 batch-125
Running loss of epoch-14 batch-125 = 2.9398826882243156e-05

Training epoch-14 batch-126
Running loss of epoch-14 batch-126 = 5.268573295325041e-05

Training epoch-14 batch-127
Running loss of epoch-14 batch-127 = 2.6506837457418442e-05

Training epoch-14 batch-128
Running loss of epoch-14 batch-128 = 2.869078889489174e-05

Training epoch-14 batch-129
Running loss of epoch-14 batch-129 = 6.118358578532934e-05

Training epoch-14 batch-130
Running loss of epoch-14 batch-130 = 3.032502718269825e-05

Training epoch-14 batch-131
Running loss of epoch-14 batch-131 = 6.0122692957520485e-05

Training epoch-14 batch-132
Running loss of epoch-14 batch-132 = 4.2172265239059925e-05

Training epoch-14 batch-133
Running loss of epoch-14 batch-133 = 1.5171244740486145e-05

Training epoch-14 batch-134
Running loss of epoch-14 batch-134 = 3.501260653138161e-05

Training epoch-14 batch-135
Running loss of epoch-14 batch-135 = 4.451838321983814e-05

Training epoch-14 batch-136
Running loss of epoch-14 batch-136 = 5.131692159920931e-05

Training epoch-14 batch-137
Running loss of epoch-14 batch-137 = 3.058323636651039e-05

Training epoch-14 batch-138
Running loss of epoch-14 batch-138 = 4.545052070170641e-05

Training epoch-14 batch-139
Running loss of epoch-14 batch-139 = 3.446184564381838e-05

Training epoch-14 batch-140
Running loss of epoch-14 batch-140 = 5.5193668231368065e-05

Training epoch-14 batch-141
Running loss of epoch-14 batch-141 = 3.0523864552378654e-05

Training epoch-14 batch-142
Running loss of epoch-14 batch-142 = 3.680563531816006e-05

Training epoch-14 batch-143
Running loss of epoch-14 batch-143 = 1.547415740787983e-05

Training epoch-14 batch-144
Running loss of epoch-14 batch-144 = 3.067753277719021e-05

Training epoch-14 batch-145
Running loss of epoch-14 batch-145 = 3.950600512325764e-05

Training epoch-14 batch-146
Running loss of epoch-14 batch-146 = 5.926366429775953e-05

Training epoch-14 batch-147
Running loss of epoch-14 batch-147 = 2.105836756527424e-05

Training epoch-14 batch-148
Running loss of epoch-14 batch-148 = 6.099790334701538e-05

Training epoch-14 batch-149
Running loss of epoch-14 batch-149 = 6.322725676000118e-05

Training epoch-14 batch-150
Running loss of epoch-14 batch-150 = 2.3671891540288925e-05

Training epoch-14 batch-151
Running loss of epoch-14 batch-151 = 3.730086609721184e-05

Training epoch-14 batch-152
Running loss of epoch-14 batch-152 = 3.461213782429695e-05

Training epoch-14 batch-153
Running loss of epoch-14 batch-153 = 4.980259109288454e-05

Training epoch-14 batch-154
Running loss of epoch-14 batch-154 = 2.2498657926917076e-05

Training epoch-14 batch-155
Running loss of epoch-14 batch-155 = 3.2077543437480927e-05

Training epoch-14 batch-156
Running loss of epoch-14 batch-156 = 5.4530100896954536e-05

Training epoch-14 batch-157
Running loss of epoch-14 batch-157 = 0.0001390799880027771

Finished training epoch-14.



Average train loss at epoch-14 = 4.1465187817811966e-05

Started Evaluation

Average val loss at epoch-14 = 2.565212012121552

Accuracy for classes:
Accuracy for class equals is: 73.76 %
Accuracy for class main is: 65.74 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.29 %
Accuracy for class toString is: 52.22 %
Accuracy for class run is: 36.07 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.49 %
Accuracy for class execute is: 26.91 %
Accuracy for class get is: 37.18 %

Overall Accuracy = 54.45 %


Best Accuracy = 54.45 % at Epoch-14
Saving model after best epoch-14

Finished Evaluation



Started training epoch-15


Training epoch-15 batch-1
Running loss of epoch-15 batch-1 = 3.7770019844174385e-05

Training epoch-15 batch-2
Running loss of epoch-15 batch-2 = 2.747995313256979e-05

Training epoch-15 batch-3
Running loss of epoch-15 batch-3 = 3.133947029709816e-05

Training epoch-15 batch-4
Running loss of epoch-15 batch-4 = 4.86691715195775e-05

Training epoch-15 batch-5
Running loss of epoch-15 batch-5 = 2.362276427447796e-05

Training epoch-15 batch-6
Running loss of epoch-15 batch-6 = 2.8239795938134193e-05

Training epoch-15 batch-7
Running loss of epoch-15 batch-7 = 1.9644619897007942e-05

Training epoch-15 batch-8
Running loss of epoch-15 batch-8 = 3.517116419970989e-05

Training epoch-15 batch-9
Running loss of epoch-15 batch-9 = 3.720435779541731e-05

Training epoch-15 batch-10
Running loss of epoch-15 batch-10 = 2.5179237127304077e-05

Training epoch-15 batch-11
Running loss of epoch-15 batch-11 = 2.110237255692482e-05

Training epoch-15 batch-12
Running loss of epoch-15 batch-12 = 2.499110996723175e-05

Training epoch-15 batch-13
Running loss of epoch-15 batch-13 = 3.699969965964556e-05

Training epoch-15 batch-14
Running loss of epoch-15 batch-14 = 2.4424400180578232e-05

Training epoch-15 batch-15
Running loss of epoch-15 batch-15 = 4.0261074900627136e-05

Training epoch-15 batch-16
Running loss of epoch-15 batch-16 = 1.2936769053339958e-05

Training epoch-15 batch-17
Running loss of epoch-15 batch-17 = 4.555610939860344e-05

Training epoch-15 batch-18
Running loss of epoch-15 batch-18 = 2.4818582460284233e-05

Training epoch-15 batch-19
Running loss of epoch-15 batch-19 = 2.5199726223945618e-05

Training epoch-15 batch-20
Running loss of epoch-15 batch-20 = 5.4646749049425125e-05

Training epoch-15 batch-21
Running loss of epoch-15 batch-21 = 2.867099829018116e-05

Training epoch-15 batch-22
Running loss of epoch-15 batch-22 = 2.590753138065338e-05

Training epoch-15 batch-23
Running loss of epoch-15 batch-23 = 2.5404850021004677e-05

Training epoch-15 batch-24
Running loss of epoch-15 batch-24 = 2.911384217441082e-05

Training epoch-15 batch-25
Running loss of epoch-15 batch-25 = 3.3724354580044746e-05

Training epoch-15 batch-26
Running loss of epoch-15 batch-26 = 3.264332190155983e-05

Training epoch-15 batch-27
Running loss of epoch-15 batch-27 = 2.7998583391308784e-05

Training epoch-15 batch-28
Running loss of epoch-15 batch-28 = 2.0581530407071114e-05

Training epoch-15 batch-29
Running loss of epoch-15 batch-29 = 3.603566437959671e-05

Training epoch-15 batch-30
Running loss of epoch-15 batch-30 = 2.8924085199832916e-05

Training epoch-15 batch-31
Running loss of epoch-15 batch-31 = 3.4482916817069054e-05

Training epoch-15 batch-32
Running loss of epoch-15 batch-32 = 3.9480626583099365e-05

Training epoch-15 batch-33
Running loss of epoch-15 batch-33 = 1.8437858670949936e-05

Training epoch-15 batch-34
Running loss of epoch-15 batch-34 = 2.6367371901869774e-05

Training epoch-15 batch-35
Running loss of epoch-15 batch-35 = 2.460367977619171e-05

Training epoch-15 batch-36
Running loss of epoch-15 batch-36 = 2.2496795281767845e-05

Training epoch-15 batch-37
Running loss of epoch-15 batch-37 = 2.5923363864421844e-05

Training epoch-15 batch-38
Running loss of epoch-15 batch-38 = 5.967461038380861e-05

Training epoch-15 batch-39
Running loss of epoch-15 batch-39 = 4.415283910930157e-05

Training epoch-15 batch-40
Running loss of epoch-15 batch-40 = 2.3898901417851448e-05

Training epoch-15 batch-41
Running loss of epoch-15 batch-41 = 2.3375265300273895e-05

Training epoch-15 batch-42
Running loss of epoch-15 batch-42 = 3.0470313504338264e-05

Training epoch-15 batch-43
Running loss of epoch-15 batch-43 = 3.48228495568037e-05

Training epoch-15 batch-44
Running loss of epoch-15 batch-44 = 3.0106864869594574e-05

Training epoch-15 batch-45
Running loss of epoch-15 batch-45 = 2.7245841920375824e-05

Training epoch-15 batch-46
Running loss of epoch-15 batch-46 = 2.5957589969038963e-05

Training epoch-15 batch-47
Running loss of epoch-15 batch-47 = 3.8047321140766144e-05

Training epoch-15 batch-48
Running loss of epoch-15 batch-48 = 1.9144965335726738e-05

Training epoch-15 batch-49
Running loss of epoch-15 batch-49 = 3.792101051658392e-05

Training epoch-15 batch-50
Running loss of epoch-15 batch-50 = 2.8309645131230354e-05

Training epoch-15 batch-51
Running loss of epoch-15 batch-51 = 4.140031524002552e-05

Training epoch-15 batch-52
Running loss of epoch-15 batch-52 = 2.3630447685718536e-05

Training epoch-15 batch-53
Running loss of epoch-15 batch-53 = 4.73483232781291e-05

Training epoch-15 batch-54
Running loss of epoch-15 batch-54 = 3.437022678554058e-05

Training epoch-15 batch-55
Running loss of epoch-15 batch-55 = 3.104889765381813e-05

Training epoch-15 batch-56
Running loss of epoch-15 batch-56 = 3.98317351937294e-05

Training epoch-15 batch-57
Running loss of epoch-15 batch-57 = 4.4677057303488255e-05

Training epoch-15 batch-58
Running loss of epoch-15 batch-58 = 3.987504169344902e-05

Training epoch-15 batch-59
Running loss of epoch-15 batch-59 = 2.2102613002061844e-05

Training epoch-15 batch-60
Running loss of epoch-15 batch-60 = 2.9856571927666664e-05

Training epoch-15 batch-61
Running loss of epoch-15 batch-61 = 3.3311545848846436e-05

Training epoch-15 batch-62
Running loss of epoch-15 batch-62 = 2.845260314643383e-05

Training epoch-15 batch-63
Running loss of epoch-15 batch-63 = 1.6930047422647476e-05

Training epoch-15 batch-64
Running loss of epoch-15 batch-64 = 4.661001730710268e-05

Training epoch-15 batch-65
Running loss of epoch-15 batch-65 = 2.6066089048981667e-05

Training epoch-15 batch-66
Running loss of epoch-15 batch-66 = 4.5301858335733414e-05

Training epoch-15 batch-67
Running loss of epoch-15 batch-67 = 3.7759775295853615e-05

Training epoch-15 batch-68
Running loss of epoch-15 batch-68 = 2.7630245313048363e-05

Training epoch-15 batch-69
Running loss of epoch-15 batch-69 = 2.6637455448508263e-05

Training epoch-15 batch-70
Running loss of epoch-15 batch-70 = 3.803009167313576e-05

Training epoch-15 batch-71
Running loss of epoch-15 batch-71 = 3.4240190871059895e-05

Training epoch-15 batch-72
Running loss of epoch-15 batch-72 = 2.7135247364640236e-05

Training epoch-15 batch-73
Running loss of epoch-15 batch-73 = 6.245227996259928e-05

Training epoch-15 batch-74
Running loss of epoch-15 batch-74 = 3.24128195643425e-05

Training epoch-15 batch-75
Running loss of epoch-15 batch-75 = 3.312877379357815e-05

Training epoch-15 batch-76
Running loss of epoch-15 batch-76 = 2.3989705368876457e-05

Training epoch-15 batch-77
Running loss of epoch-15 batch-77 = 2.525246236473322e-05

Training epoch-15 batch-78
Running loss of epoch-15 batch-78 = 3.1138304620981216e-05

Training epoch-15 batch-79
Running loss of epoch-15 batch-79 = 2.8777169063687325e-05

Training epoch-15 batch-80
Running loss of epoch-15 batch-80 = 2.529681660234928e-05

Training epoch-15 batch-81
Running loss of epoch-15 batch-81 = 3.153202123939991e-05

Training epoch-15 batch-82
Running loss of epoch-15 batch-82 = 3.7032063119113445e-05

Training epoch-15 batch-83
Running loss of epoch-15 batch-83 = 4.2182509787380695e-05

Training epoch-15 batch-84
Running loss of epoch-15 batch-84 = 1.2554926797747612e-05

Training epoch-15 batch-85
Running loss of epoch-15 batch-85 = 1.659570261836052e-05

Training epoch-15 batch-86
Running loss of epoch-15 batch-86 = 2.7895905077457428e-05

Training epoch-15 batch-87
Running loss of epoch-15 batch-87 = 2.7014175429940224e-05

Training epoch-15 batch-88
Running loss of epoch-15 batch-88 = 4.139682278037071e-05

Training epoch-15 batch-89
Running loss of epoch-15 batch-89 = 2.24430114030838e-05

Training epoch-15 batch-90
Running loss of epoch-15 batch-90 = 1.9878149032592773e-05

Training epoch-15 batch-91
Running loss of epoch-15 batch-91 = 2.4803681299090385e-05

Training epoch-15 batch-92
Running loss of epoch-15 batch-92 = 3.372831270098686e-05

Training epoch-15 batch-93
Running loss of epoch-15 batch-93 = 2.1167565137147903e-05

Training epoch-15 batch-94
Running loss of epoch-15 batch-94 = 1.8615974113345146e-05

Training epoch-15 batch-95
Running loss of epoch-15 batch-95 = 2.4835113435983658e-05

Training epoch-15 batch-96
Running loss of epoch-15 batch-96 = 2.2846274077892303e-05

Training epoch-15 batch-97
Running loss of epoch-15 batch-97 = 3.100594040006399e-05

Training epoch-15 batch-98
Running loss of epoch-15 batch-98 = 2.7422094717621803e-05

Training epoch-15 batch-99
Running loss of epoch-15 batch-99 = 6.10952265560627e-05

Training epoch-15 batch-100
Running loss of epoch-15 batch-100 = 2.9621878638863564e-05

Training epoch-15 batch-101
Running loss of epoch-15 batch-101 = 1.7849961295723915e-05

Training epoch-15 batch-102
Running loss of epoch-15 batch-102 = 5.723827052861452e-05

Training epoch-15 batch-103
Running loss of epoch-15 batch-103 = 3.6995625123381615e-05

Training epoch-15 batch-104
Running loss of epoch-15 batch-104 = 2.3089349269866943e-05

Training epoch-15 batch-105
Running loss of epoch-15 batch-105 = 3.479979932308197e-05

Training epoch-15 batch-106
Running loss of epoch-15 batch-106 = 1.4118617400527e-05

Training epoch-15 batch-107
Running loss of epoch-15 batch-107 = 1.6706064343452454e-05

Training epoch-15 batch-108
Running loss of epoch-15 batch-108 = 2.5890767574310303e-05

Training epoch-15 batch-109
Running loss of epoch-15 batch-109 = 1.923530362546444e-05

Training epoch-15 batch-110
Running loss of epoch-15 batch-110 = 3.7460471503436565e-05

Training epoch-15 batch-111
Running loss of epoch-15 batch-111 = 3.06558795273304e-05

Training epoch-15 batch-112
Running loss of epoch-15 batch-112 = 2.4638604372739792e-05

Training epoch-15 batch-113
Running loss of epoch-15 batch-113 = 2.110120840370655e-05

Training epoch-15 batch-114
Running loss of epoch-15 batch-114 = 2.8128502890467644e-05

Training epoch-15 batch-115
Running loss of epoch-15 batch-115 = 4.5261112973093987e-05

Training epoch-15 batch-116
Running loss of epoch-15 batch-116 = 1.4181947335600853e-05

Training epoch-15 batch-117
Running loss of epoch-15 batch-117 = 2.894480712711811e-05

Training epoch-15 batch-118
Running loss of epoch-15 batch-118 = 2.9881717637181282e-05

Training epoch-15 batch-119
Running loss of epoch-15 batch-119 = 2.749660052359104e-05

Training epoch-15 batch-120
Running loss of epoch-15 batch-120 = 2.2461172193288803e-05

Training epoch-15 batch-121
Running loss of epoch-15 batch-121 = 4.429672844707966e-05

Training epoch-15 batch-122
Running loss of epoch-15 batch-122 = 2.1280953660607338e-05

Training epoch-15 batch-123
Running loss of epoch-15 batch-123 = 2.34988983720541e-05

Training epoch-15 batch-124
Running loss of epoch-15 batch-124 = 2.1823681890964508e-05

Training epoch-15 batch-125
Running loss of epoch-15 batch-125 = 2.1842308342456818e-05

Training epoch-15 batch-126
Running loss of epoch-15 batch-126 = 3.0049937777221203e-05

Training epoch-15 batch-127
Running loss of epoch-15 batch-127 = 3.742717672139406e-05

Training epoch-15 batch-128
Running loss of epoch-15 batch-128 = 2.1881191059947014e-05

Training epoch-15 batch-129
Running loss of epoch-15 batch-129 = 1.862109638750553e-05

Training epoch-15 batch-130
Running loss of epoch-15 batch-130 = 4.973646719008684e-05

Training epoch-15 batch-131
Running loss of epoch-15 batch-131 = 3.833486698567867e-05

Training epoch-15 batch-132
Running loss of epoch-15 batch-132 = 3.089138772338629e-05

Training epoch-15 batch-133
Running loss of epoch-15 batch-133 = 2.9833638109266758e-05

Training epoch-15 batch-134
Running loss of epoch-15 batch-134 = 3.229500725865364e-05

Training epoch-15 batch-135
Running loss of epoch-15 batch-135 = 3.0289171263575554e-05

Training epoch-15 batch-136
Running loss of epoch-15 batch-136 = 4.3725594878196716e-05

Training epoch-15 batch-137
Running loss of epoch-15 batch-137 = 3.7452904507517815e-05

Training epoch-15 batch-138
Running loss of epoch-15 batch-138 = 3.087660297751427e-05

Training epoch-15 batch-139
Running loss of epoch-15 batch-139 = 2.3398082703351974e-05

Training epoch-15 batch-140
Running loss of epoch-15 batch-140 = 2.3392029106616974e-05

Training epoch-15 batch-141
Running loss of epoch-15 batch-141 = 3.703194670379162e-05

Training epoch-15 batch-142
Running loss of epoch-15 batch-142 = 3.012979868799448e-05

Training epoch-15 batch-143
Running loss of epoch-15 batch-143 = 2.2761058062314987e-05

Training epoch-15 batch-144
Running loss of epoch-15 batch-144 = 3.270676825195551e-05

Training epoch-15 batch-145
Running loss of epoch-15 batch-145 = 3.5115983337163925e-05

Training epoch-15 batch-146
Running loss of epoch-15 batch-146 = 3.0554248951375484e-05

Training epoch-15 batch-147
Running loss of epoch-15 batch-147 = 2.620881423354149e-05

Training epoch-15 batch-148
Running loss of epoch-15 batch-148 = 2.514489460736513e-05

Training epoch-15 batch-149
Running loss of epoch-15 batch-149 = 3.097974695265293e-05

Training epoch-15 batch-150
Running loss of epoch-15 batch-150 = 3.1681498512625694e-05

Training epoch-15 batch-151
Running loss of epoch-15 batch-151 = 2.916972152888775e-05

Training epoch-15 batch-152
Running loss of epoch-15 batch-152 = 3.782764542847872e-05

Training epoch-15 batch-153
Running loss of epoch-15 batch-153 = 2.488819882273674e-05

Training epoch-15 batch-154
Running loss of epoch-15 batch-154 = 2.7713365852832794e-05

Training epoch-15 batch-155
Running loss of epoch-15 batch-155 = 2.8438633307814598e-05

Training epoch-15 batch-156
Running loss of epoch-15 batch-156 = 1.8487917259335518e-05

Training epoch-15 batch-157
Running loss of epoch-15 batch-157 = 0.00021744146943092346

Finished training epoch-15.



Average train loss at epoch-15 = 3.070610389113426e-05

Started Evaluation

Average val loss at epoch-15 = 2.59349693122663

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 65.41 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 57.14 %
Accuracy for class toString is: 51.54 %
Accuracy for class run is: 36.99 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 31.17 %
Accuracy for class execute is: 25.70 %
Accuracy for class get is: 37.69 %

Overall Accuracy = 54.82 %


Best Accuracy = 54.82 % at Epoch-15
Saving model after best epoch-15

Finished Evaluation



Started training epoch-16


Training epoch-16 batch-1
Running loss of epoch-16 batch-1 = 3.1507574021816254e-05

Training epoch-16 batch-2
Running loss of epoch-16 batch-2 = 3.822473809123039e-05

Training epoch-16 batch-3
Running loss of epoch-16 batch-3 = 3.3166841603815556e-05

Training epoch-16 batch-4
Running loss of epoch-16 batch-4 = 1.775100827217102e-05

Training epoch-16 batch-5
Running loss of epoch-16 batch-5 = 4.5902212150394917e-05

Training epoch-16 batch-6
Running loss of epoch-16 batch-6 = 2.8172042220830917e-05

Training epoch-16 batch-7
Running loss of epoch-16 batch-7 = 2.012774348258972e-05

Training epoch-16 batch-8
Running loss of epoch-16 batch-8 = 1.8271850422024727e-05

Training epoch-16 batch-9
Running loss of epoch-16 batch-9 = 3.1439936719834805e-05

Training epoch-16 batch-10
Running loss of epoch-16 batch-10 = 2.3996224626898766e-05

Training epoch-16 batch-11
Running loss of epoch-16 batch-11 = 4.096701741218567e-05

Training epoch-16 batch-12
Running loss of epoch-16 batch-12 = 3.110500983893871e-05

Training epoch-16 batch-13
Running loss of epoch-16 batch-13 = 2.3644650354981422e-05

Training epoch-16 batch-14
Running loss of epoch-16 batch-14 = 1.795310527086258e-05

Training epoch-16 batch-15
Running loss of epoch-16 batch-15 = 1.8735183402895927e-05

Training epoch-16 batch-16
Running loss of epoch-16 batch-16 = 2.8082868084311485e-05

Training epoch-16 batch-17
Running loss of epoch-16 batch-17 = 2.480694092810154e-05

Training epoch-16 batch-18
Running loss of epoch-16 batch-18 = 2.4295877665281296e-05

Training epoch-16 batch-19
Running loss of epoch-16 batch-19 = 1.3536540791392326e-05

Training epoch-16 batch-20
Running loss of epoch-16 batch-20 = 2.7659814804792404e-05

Training epoch-16 batch-21
Running loss of epoch-16 batch-21 = 1.2235017493367195e-05

Training epoch-16 batch-22
Running loss of epoch-16 batch-22 = 3.857852425426245e-05

Training epoch-16 batch-23
Running loss of epoch-16 batch-23 = 2.9864255338907242e-05

Training epoch-16 batch-24
Running loss of epoch-16 batch-24 = 1.7567072063684464e-05

Training epoch-16 batch-25
Running loss of epoch-16 batch-25 = 3.014307003468275e-05

Training epoch-16 batch-26
Running loss of epoch-16 batch-26 = 1.6562175005674362e-05

Training epoch-16 batch-27
Running loss of epoch-16 batch-27 = 1.9497470930218697e-05

Training epoch-16 batch-28
Running loss of epoch-16 batch-28 = 3.0246097594499588e-05

Training epoch-16 batch-29
Running loss of epoch-16 batch-29 = 1.5755649656057358e-05

Training epoch-16 batch-30
Running loss of epoch-16 batch-30 = 3.752228803932667e-05

Training epoch-16 batch-31
Running loss of epoch-16 batch-31 = 3.094063140451908e-05

Training epoch-16 batch-32
Running loss of epoch-16 batch-32 = 1.5894416719675064e-05

Training epoch-16 batch-33
Running loss of epoch-16 batch-33 = 2.101203426718712e-05

Training epoch-16 batch-34
Running loss of epoch-16 batch-34 = 4.596472717821598e-05

Training epoch-16 batch-35
Running loss of epoch-16 batch-35 = 2.1206913515925407e-05

Training epoch-16 batch-36
Running loss of epoch-16 batch-36 = 2.6133377104997635e-05

Training epoch-16 batch-37
Running loss of epoch-16 batch-37 = 4.595005884766579e-05

Training epoch-16 batch-38
Running loss of epoch-16 batch-38 = 1.8165679648518562e-05

Training epoch-16 batch-39
Running loss of epoch-16 batch-39 = 2.588971983641386e-05

Training epoch-16 batch-40
Running loss of epoch-16 batch-40 = 1.9073020666837692e-05

Training epoch-16 batch-41
Running loss of epoch-16 batch-41 = 2.8296257369220257e-05

Training epoch-16 batch-42
Running loss of epoch-16 batch-42 = 2.0677456632256508e-05

Training epoch-16 batch-43
Running loss of epoch-16 batch-43 = 2.6998110115528107e-05

Training epoch-16 batch-44
Running loss of epoch-16 batch-44 = 4.259753040969372e-05

Training epoch-16 batch-45
Running loss of epoch-16 batch-45 = 2.7585658244788647e-05

Training epoch-16 batch-46
Running loss of epoch-16 batch-46 = 2.1520769223570824e-05

Training epoch-16 batch-47
Running loss of epoch-16 batch-47 = 2.617877908051014e-05

Training epoch-16 batch-48
Running loss of epoch-16 batch-48 = 2.059456892311573e-05

Training epoch-16 batch-49
Running loss of epoch-16 batch-49 = 1.9466271623969078e-05

Training epoch-16 batch-50
Running loss of epoch-16 batch-50 = 2.1157553419470787e-05

Training epoch-16 batch-51
Running loss of epoch-16 batch-51 = 2.9465649276971817e-05

Training epoch-16 batch-52
Running loss of epoch-16 batch-52 = 3.5631353966891766e-05

Training epoch-16 batch-53
Running loss of epoch-16 batch-53 = 2.861255779862404e-05

Training epoch-16 batch-54
Running loss of epoch-16 batch-54 = 2.3577595129609108e-05

Training epoch-16 batch-55
Running loss of epoch-16 batch-55 = 1.3981945812702179e-05

Training epoch-16 batch-56
Running loss of epoch-16 batch-56 = 2.1117273718118668e-05

Training epoch-16 batch-57
Running loss of epoch-16 batch-57 = 2.549774944782257e-05

Training epoch-16 batch-58
Running loss of epoch-16 batch-58 = 1.7105136066675186e-05

Training epoch-16 batch-59
Running loss of epoch-16 batch-59 = 2.99159437417984e-05

Training epoch-16 batch-60
Running loss of epoch-16 batch-60 = 3.766582813113928e-05

Training epoch-16 batch-61
Running loss of epoch-16 batch-61 = 1.7320970073342323e-05

Training epoch-16 batch-62
Running loss of epoch-16 batch-62 = 3.141665365546942e-05

Training epoch-16 batch-63
Running loss of epoch-16 batch-63 = 2.6247696951031685e-05

Training epoch-16 batch-64
Running loss of epoch-16 batch-64 = 3.406614996492863e-05

Training epoch-16 batch-65
Running loss of epoch-16 batch-65 = 2.7892179787158966e-05

Training epoch-16 batch-66
Running loss of epoch-16 batch-66 = 2.5237444788217545e-05

Training epoch-16 batch-67
Running loss of epoch-16 batch-67 = 2.0572682842612267e-05

Training epoch-16 batch-68
Running loss of epoch-16 batch-68 = 2.7829548344016075e-05

Training epoch-16 batch-69
Running loss of epoch-16 batch-69 = 2.1052546799182892e-05

Training epoch-16 batch-70
Running loss of epoch-16 batch-70 = 2.6360503397881985e-05

Training epoch-16 batch-71
Running loss of epoch-16 batch-71 = 2.0142761059105396e-05

Training epoch-16 batch-72
Running loss of epoch-16 batch-72 = 2.221018075942993e-05

Training epoch-16 batch-73
Running loss of epoch-16 batch-73 = 2.2078165784478188e-05

Training epoch-16 batch-74
Running loss of epoch-16 batch-74 = 2.50150915235281e-05

Training epoch-16 batch-75
Running loss of epoch-16 batch-75 = 2.2679800167679787e-05

Training epoch-16 batch-76
Running loss of epoch-16 batch-76 = 1.6982434317469597e-05

Training epoch-16 batch-77
Running loss of epoch-16 batch-77 = 2.798088826239109e-05

Training epoch-16 batch-78
Running loss of epoch-16 batch-78 = 3.205169923603535e-05

Training epoch-16 batch-79
Running loss of epoch-16 batch-79 = 2.1811574697494507e-05

Training epoch-16 batch-80
Running loss of epoch-16 batch-80 = 2.1524261683225632e-05

Training epoch-16 batch-81
Running loss of epoch-16 batch-81 = 2.0785490050911903e-05

Training epoch-16 batch-82
Running loss of epoch-16 batch-82 = 1.4626886695623398e-05

Training epoch-16 batch-83
Running loss of epoch-16 batch-83 = 2.0848819985985756e-05

Training epoch-16 batch-84
Running loss of epoch-16 batch-84 = 2.7095433324575424e-05

Training epoch-16 batch-85
Running loss of epoch-16 batch-85 = 2.702116034924984e-05

Training epoch-16 batch-86
Running loss of epoch-16 batch-86 = 2.9437243938446045e-05

Training epoch-16 batch-87
Running loss of epoch-16 batch-87 = 2.2419611923396587e-05

Training epoch-16 batch-88
Running loss of epoch-16 batch-88 = 2.233521081507206e-05

Training epoch-16 batch-89
Running loss of epoch-16 batch-89 = 2.6264693588018417e-05

Training epoch-16 batch-90
Running loss of epoch-16 batch-90 = 2.416467759758234e-05

Training epoch-16 batch-91
Running loss of epoch-16 batch-91 = 4.160159733146429e-05

Training epoch-16 batch-92
Running loss of epoch-16 batch-92 = 3.54651128873229e-05

Training epoch-16 batch-93
Running loss of epoch-16 batch-93 = 3.427639603614807e-05

Training epoch-16 batch-94
Running loss of epoch-16 batch-94 = 2.8936658054590225e-05

Training epoch-16 batch-95
Running loss of epoch-16 batch-95 = 2.1273037418723106e-05

Training epoch-16 batch-96
Running loss of epoch-16 batch-96 = 2.764933742582798e-05

Training epoch-16 batch-97
Running loss of epoch-16 batch-97 = 2.8722919523715973e-05

Training epoch-16 batch-98
Running loss of epoch-16 batch-98 = 2.5772256776690483e-05

Training epoch-16 batch-99
Running loss of epoch-16 batch-99 = 1.5118159353733063e-05

Training epoch-16 batch-100
Running loss of epoch-16 batch-100 = 2.2147316485643387e-05

Training epoch-16 batch-101
Running loss of epoch-16 batch-101 = 2.6522320695221424e-05

Training epoch-16 batch-102
Running loss of epoch-16 batch-102 = 2.1084677428007126e-05

Training epoch-16 batch-103
Running loss of epoch-16 batch-103 = 3.293203189969063e-05

Training epoch-16 batch-104
Running loss of epoch-16 batch-104 = 2.563605085015297e-05

Training epoch-16 batch-105
Running loss of epoch-16 batch-105 = 2.5845365598797798e-05

Training epoch-16 batch-106
Running loss of epoch-16 batch-106 = 2.461974509060383e-05

Training epoch-16 batch-107
Running loss of epoch-16 batch-107 = 1.9142054952681065e-05

Training epoch-16 batch-108
Running loss of epoch-16 batch-108 = 1.939176581799984e-05

Training epoch-16 batch-109
Running loss of epoch-16 batch-109 = 1.9792933017015457e-05

Training epoch-16 batch-110
Running loss of epoch-16 batch-110 = 3.92119400203228e-05

Training epoch-16 batch-111
Running loss of epoch-16 batch-111 = 1.4955643564462662e-05

Training epoch-16 batch-112
Running loss of epoch-16 batch-112 = 2.1398067474365234e-05

Training epoch-16 batch-113
Running loss of epoch-16 batch-113 = 1.4811521396040916e-05

Training epoch-16 batch-114
Running loss of epoch-16 batch-114 = 2.92360782623291e-05

Training epoch-16 batch-115
Running loss of epoch-16 batch-115 = 1.806439831852913e-05

Training epoch-16 batch-116
Running loss of epoch-16 batch-116 = 1.7336104065179825e-05

Training epoch-16 batch-117
Running loss of epoch-16 batch-117 = 1.7055077478289604e-05

Training epoch-16 batch-118
Running loss of epoch-16 batch-118 = 3.568548709154129e-05

Training epoch-16 batch-119
Running loss of epoch-16 batch-119 = 1.5889527276158333e-05

Training epoch-16 batch-120
Running loss of epoch-16 batch-120 = 1.715961843729019e-05

Training epoch-16 batch-121
Running loss of epoch-16 batch-121 = 1.2662960216403008e-05

Training epoch-16 batch-122
Running loss of epoch-16 batch-122 = 5.164160393178463e-05

Training epoch-16 batch-123
Running loss of epoch-16 batch-123 = 2.8016744181513786e-05

Training epoch-16 batch-124
Running loss of epoch-16 batch-124 = 1.9514351151883602e-05

Training epoch-16 batch-125
Running loss of epoch-16 batch-125 = 2.7390429750084877e-05

Training epoch-16 batch-126
Running loss of epoch-16 batch-126 = 1.3915356248617172e-05

Training epoch-16 batch-127
Running loss of epoch-16 batch-127 = 1.5564030036330223e-05

Training epoch-16 batch-128
Running loss of epoch-16 batch-128 = 1.878791954368353e-05

Training epoch-16 batch-129
Running loss of epoch-16 batch-129 = 1.5587080270051956e-05

Training epoch-16 batch-130
Running loss of epoch-16 batch-130 = 3.3384538255631924e-05

Training epoch-16 batch-131
Running loss of epoch-16 batch-131 = 3.734265919774771e-05

Training epoch-16 batch-132
Running loss of epoch-16 batch-132 = 1.7091166228055954e-05

Training epoch-16 batch-133
Running loss of epoch-16 batch-133 = 2.4649081751704216e-05

Training epoch-16 batch-134
Running loss of epoch-16 batch-134 = 1.2179138138890266e-05

Training epoch-16 batch-135
Running loss of epoch-16 batch-135 = 2.8032343834638596e-05

Training epoch-16 batch-136
Running loss of epoch-16 batch-136 = 1.230696216225624e-05

Training epoch-16 batch-137
Running loss of epoch-16 batch-137 = 2.649053931236267e-05

Training epoch-16 batch-138
Running loss of epoch-16 batch-138 = 1.4193123206496239e-05

Training epoch-16 batch-139
Running loss of epoch-16 batch-139 = 2.0898878574371338e-05

Training epoch-16 batch-140
Running loss of epoch-16 batch-140 = 2.5215791538357735e-05

Training epoch-16 batch-141
Running loss of epoch-16 batch-141 = 1.6918638721108437e-05

Training epoch-16 batch-142
Running loss of epoch-16 batch-142 = 3.1057512387633324e-05

Training epoch-16 batch-143
Running loss of epoch-16 batch-143 = 3.484170883893967e-05

Training epoch-16 batch-144
Running loss of epoch-16 batch-144 = 2.8372276574373245e-05

Training epoch-16 batch-145
Running loss of epoch-16 batch-145 = 2.3438595235347748e-05

Training epoch-16 batch-146
Running loss of epoch-16 batch-146 = 1.3356795534491539e-05

Training epoch-16 batch-147
Running loss of epoch-16 batch-147 = 2.295011654496193e-05

Training epoch-16 batch-148
Running loss of epoch-16 batch-148 = 1.438637264072895e-05

Training epoch-16 batch-149
Running loss of epoch-16 batch-149 = 3.1889649108052254e-05

Training epoch-16 batch-150
Running loss of epoch-16 batch-150 = 1.7574289813637733e-05

Training epoch-16 batch-151
Running loss of epoch-16 batch-151 = 2.3605534806847572e-05

Training epoch-16 batch-152
Running loss of epoch-16 batch-152 = 1.6471371054649353e-05

Training epoch-16 batch-153
Running loss of epoch-16 batch-153 = 1.2334436178207397e-05

Training epoch-16 batch-154
Running loss of epoch-16 batch-154 = 1.407228410243988e-05

Training epoch-16 batch-155
Running loss of epoch-16 batch-155 = 1.756916753947735e-05

Training epoch-16 batch-156
Running loss of epoch-16 batch-156 = 2.4127191863954067e-05

Training epoch-16 batch-157
Running loss of epoch-16 batch-157 = 0.00016313046216964722

Finished training epoch-16.



Average train loss at epoch-16 = 2.48064786195755e-05

Started Evaluation

Average val loss at epoch-16 = 2.6529975423687384

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 62.95 %
Accuracy for class onCreate is: 56.18 %
Accuracy for class toString is: 51.88 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 25.30 %
Accuracy for class get is: 38.46 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-17


Training epoch-17 batch-1
Running loss of epoch-17 batch-1 = 3.426056355237961e-05

Training epoch-17 batch-2
Running loss of epoch-17 batch-2 = 2.0707258954644203e-05

Training epoch-17 batch-3
Running loss of epoch-17 batch-3 = 1.7869984731078148e-05

Training epoch-17 batch-4
Running loss of epoch-17 batch-4 = 1.2295320630073547e-05

Training epoch-17 batch-5
Running loss of epoch-17 batch-5 = 1.6262521967291832e-05

Training epoch-17 batch-6
Running loss of epoch-17 batch-6 = 1.6988487914204597e-05

Training epoch-17 batch-7
Running loss of epoch-17 batch-7 = 1.9702361896634102e-05

Training epoch-17 batch-8
Running loss of epoch-17 batch-8 = 1.938454806804657e-05

Training epoch-17 batch-9
Running loss of epoch-17 batch-9 = 3.4168828278779984e-05

Training epoch-17 batch-10
Running loss of epoch-17 batch-10 = 2.3766187950968742e-05

Training epoch-17 batch-11
Running loss of epoch-17 batch-11 = 1.4592893421649933e-05

Training epoch-17 batch-12
Running loss of epoch-17 batch-12 = 1.8475577235221863e-05

Training epoch-17 batch-13
Running loss of epoch-17 batch-13 = 1.6445759683847427e-05

Training epoch-17 batch-14
Running loss of epoch-17 batch-14 = 1.3521872460842133e-05

Training epoch-17 batch-15
Running loss of epoch-17 batch-15 = 2.5657820515334606e-05

Training epoch-17 batch-16
Running loss of epoch-17 batch-16 = 1.9724946469068527e-05

Training epoch-17 batch-17
Running loss of epoch-17 batch-17 = 2.2731022909283638e-05

Training epoch-17 batch-18
Running loss of epoch-17 batch-18 = 3.6733923479914665e-05

Training epoch-17 batch-19
Running loss of epoch-17 batch-19 = 3.291037864983082e-05

Training epoch-17 batch-20
Running loss of epoch-17 batch-20 = 1.8516089767217636e-05

Training epoch-17 batch-21
Running loss of epoch-17 batch-21 = 1.2445030733942986e-05

Training epoch-17 batch-22
Running loss of epoch-17 batch-22 = 3.010325599461794e-05

Training epoch-17 batch-23
Running loss of epoch-17 batch-23 = 2.9303599148988724e-05

Training epoch-17 batch-24
Running loss of epoch-17 batch-24 = 2.5430344976484776e-05

Training epoch-17 batch-25
Running loss of epoch-17 batch-25 = 3.4328317269682884e-05

Training epoch-17 batch-26
Running loss of epoch-17 batch-26 = 2.2027641534805298e-05

Training epoch-17 batch-27
Running loss of epoch-17 batch-27 = 1.598196104168892e-05

Training epoch-17 batch-28
Running loss of epoch-17 batch-28 = 2.2064894437789917e-05

Training epoch-17 batch-29
Running loss of epoch-17 batch-29 = 2.12215818464756e-05

Training epoch-17 batch-30
Running loss of epoch-17 batch-30 = 1.6757287085056305e-05

Training epoch-17 batch-31
Running loss of epoch-17 batch-31 = 1.4408724382519722e-05

Training epoch-17 batch-32
Running loss of epoch-17 batch-32 = 2.756621688604355e-05

Training epoch-17 batch-33
Running loss of epoch-17 batch-33 = 2.2546853870153427e-05

Training epoch-17 batch-34
Running loss of epoch-17 batch-34 = 1.4279969036579132e-05

Training epoch-17 batch-35
Running loss of epoch-17 batch-35 = 1.5596160665154457e-05

Training epoch-17 batch-36
Running loss of epoch-17 batch-36 = 8.740928024053574e-06

Training epoch-17 batch-37
Running loss of epoch-17 batch-37 = 1.856568269431591e-05

Training epoch-17 batch-38
Running loss of epoch-17 batch-38 = 1.6609905287623405e-05

Training epoch-17 batch-39
Running loss of epoch-17 batch-39 = 1.8718652427196503e-05

Training epoch-17 batch-40
Running loss of epoch-17 batch-40 = 1.325272023677826e-05

Training epoch-17 batch-41
Running loss of epoch-17 batch-41 = 1.2892764061689377e-05

Training epoch-17 batch-42
Running loss of epoch-17 batch-42 = 2.0489445887506008e-05

Training epoch-17 batch-43
Running loss of epoch-17 batch-43 = 2.7977628633379936e-05

Training epoch-17 batch-44
Running loss of epoch-17 batch-44 = 1.4857621863484383e-05

Training epoch-17 batch-45
Running loss of epoch-17 batch-45 = 2.7691363357007504e-05

Training epoch-17 batch-46
Running loss of epoch-17 batch-46 = 2.8176349587738514e-05

Training epoch-17 batch-47
Running loss of epoch-17 batch-47 = 1.2615462765097618e-05

Training epoch-17 batch-48
Running loss of epoch-17 batch-48 = 1.7464393749833107e-05

Training epoch-17 batch-49
Running loss of epoch-17 batch-49 = 2.040015533566475e-05

Training epoch-17 batch-50
Running loss of epoch-17 batch-50 = 1.8310965970158577e-05

Training epoch-17 batch-51
Running loss of epoch-17 batch-51 = 2.5174813345074654e-05

Training epoch-17 batch-52
Running loss of epoch-17 batch-52 = 1.284782774746418e-05

Training epoch-17 batch-53
Running loss of epoch-17 batch-53 = 1.3625482097268105e-05

Training epoch-17 batch-54
Running loss of epoch-17 batch-54 = 1.834274735301733e-05

Training epoch-17 batch-55
Running loss of epoch-17 batch-55 = 1.6137724742293358e-05

Training epoch-17 batch-56
Running loss of epoch-17 batch-56 = 2.0458013750612736e-05

Training epoch-17 batch-57
Running loss of epoch-17 batch-57 = 1.543154940009117e-05

Training epoch-17 batch-58
Running loss of epoch-17 batch-58 = 2.192496322095394e-05

Training epoch-17 batch-59
Running loss of epoch-17 batch-59 = 2.0672567188739777e-05

Training epoch-17 batch-60
Running loss of epoch-17 batch-60 = 1.898081973195076e-05

Training epoch-17 batch-61
Running loss of epoch-17 batch-61 = 1.8020393326878548e-05

Training epoch-17 batch-62
Running loss of epoch-17 batch-62 = 2.0438339561223984e-05

Training epoch-17 batch-63
Running loss of epoch-17 batch-63 = 2.0070234313607216e-05

Training epoch-17 batch-64
Running loss of epoch-17 batch-64 = 9.820796549320221e-06

Training epoch-17 batch-65
Running loss of epoch-17 batch-65 = 2.0473962649703026e-05

Training epoch-17 batch-66
Running loss of epoch-17 batch-66 = 1.3452954590320587e-05

Training epoch-17 batch-67
Running loss of epoch-17 batch-67 = 2.5301123969256878e-05

Training epoch-17 batch-68
Running loss of epoch-17 batch-68 = 2.767518162727356e-05

Training epoch-17 batch-69
Running loss of epoch-17 batch-69 = 2.778531052172184e-05

Training epoch-17 batch-70
Running loss of epoch-17 batch-70 = 2.8096837922930717e-05

Training epoch-17 batch-71
Running loss of epoch-17 batch-71 = 2.2857217118144035e-05

Training epoch-17 batch-72
Running loss of epoch-17 batch-72 = 2.1832529455423355e-05

Training epoch-17 batch-73
Running loss of epoch-17 batch-73 = 2.804666291922331e-05

Training epoch-17 batch-74
Running loss of epoch-17 batch-74 = 1.1982396245002747e-05

Training epoch-17 batch-75
Running loss of epoch-17 batch-75 = 2.222810871899128e-05

Training epoch-17 batch-76
Running loss of epoch-17 batch-76 = 1.4849239960312843e-05

Training epoch-17 batch-77
Running loss of epoch-17 batch-77 = 2.4736160412430763e-05

Training epoch-17 batch-78
Running loss of epoch-17 batch-78 = 2.0632287487387657e-05

Training epoch-17 batch-79
Running loss of epoch-17 batch-79 = 1.789536327123642e-05

Training epoch-17 batch-80
Running loss of epoch-17 batch-80 = 2.1138228476047516e-05

Training epoch-17 batch-81
Running loss of epoch-17 batch-81 = 1.7697224393486977e-05

Training epoch-17 batch-82
Running loss of epoch-17 batch-82 = 1.681898720562458e-05

Training epoch-17 batch-83
Running loss of epoch-17 batch-83 = 2.5702640414237976e-05

Training epoch-17 batch-84
Running loss of epoch-17 batch-84 = 1.2977048754692078e-05

Training epoch-17 batch-85
Running loss of epoch-17 batch-85 = 2.9546557925641537e-05

Training epoch-17 batch-86
Running loss of epoch-17 batch-86 = 1.943623647093773e-05

Training epoch-17 batch-87
Running loss of epoch-17 batch-87 = 2.0025763660669327e-05

Training epoch-17 batch-88
Running loss of epoch-17 batch-88 = 2.4874461814761162e-05

Training epoch-17 batch-89
Running loss of epoch-17 batch-89 = 1.6721663996577263e-05

Training epoch-17 batch-90
Running loss of epoch-17 batch-90 = 1.0385410860180855e-05

Training epoch-17 batch-91
Running loss of epoch-17 batch-91 = 3.083585761487484e-05

Training epoch-17 batch-92
Running loss of epoch-17 batch-92 = 3.247195854783058e-05

Training epoch-17 batch-93
Running loss of epoch-17 batch-93 = 2.4428358301520348e-05

Training epoch-17 batch-94
Running loss of epoch-17 batch-94 = 1.7405487596988678e-05

Training epoch-17 batch-95
Running loss of epoch-17 batch-95 = 1.5050172805786133e-05

Training epoch-17 batch-96
Running loss of epoch-17 batch-96 = 2.168840728700161e-05

Training epoch-17 batch-97
Running loss of epoch-17 batch-97 = 3.351143095642328e-05

Training epoch-17 batch-98
Running loss of epoch-17 batch-98 = 3.659655340015888e-05

Training epoch-17 batch-99
Running loss of epoch-17 batch-99 = 1.8517719581723213e-05

Training epoch-17 batch-100
Running loss of epoch-17 batch-100 = 2.725434023886919e-05

Training epoch-17 batch-101
Running loss of epoch-17 batch-101 = 1.8676510080695152e-05

Training epoch-17 batch-102
Running loss of epoch-17 batch-102 = 9.94047150015831e-06

Training epoch-17 batch-103
Running loss of epoch-17 batch-103 = 2.2573163732886314e-05

Training epoch-17 batch-104
Running loss of epoch-17 batch-104 = 2.431892789900303e-05

Training epoch-17 batch-105
Running loss of epoch-17 batch-105 = 2.216384746134281e-05

Training epoch-17 batch-106
Running loss of epoch-17 batch-106 = 9.397277608513832e-06

Training epoch-17 batch-107
Running loss of epoch-17 batch-107 = 2.2434396669268608e-05

Training epoch-17 batch-108
Running loss of epoch-17 batch-108 = 2.0122388377785683e-05

Training epoch-17 batch-109
Running loss of epoch-17 batch-109 = 1.5738187357783318e-05

Training epoch-17 batch-110
Running loss of epoch-17 batch-110 = 2.0624836906790733e-05

Training epoch-17 batch-111
Running loss of epoch-17 batch-111 = 2.1120533347129822e-05

Training epoch-17 batch-112
Running loss of epoch-17 batch-112 = 1.9714469090104103e-05

Training epoch-17 batch-113
Running loss of epoch-17 batch-113 = 2.4304375983774662e-05

Training epoch-17 batch-114
Running loss of epoch-17 batch-114 = 1.5967991203069687e-05

Training epoch-17 batch-115
Running loss of epoch-17 batch-115 = 3.456277772784233e-05

Training epoch-17 batch-116
Running loss of epoch-17 batch-116 = 2.2781314328312874e-05

Training epoch-17 batch-117
Running loss of epoch-17 batch-117 = 2.45007686316967e-05

Training epoch-17 batch-118
Running loss of epoch-17 batch-118 = 3.3082207664847374e-05

Training epoch-17 batch-119
Running loss of epoch-17 batch-119 = 2.1859072148799896e-05

Training epoch-17 batch-120
Running loss of epoch-17 batch-120 = 2.3215077817440033e-05

Training epoch-17 batch-121
Running loss of epoch-17 batch-121 = 1.6084173694252968e-05

Training epoch-17 batch-122
Running loss of epoch-17 batch-122 = 3.236078191548586e-05

Training epoch-17 batch-123
Running loss of epoch-17 batch-123 = 1.376355066895485e-05

Training epoch-17 batch-124
Running loss of epoch-17 batch-124 = 1.692189835011959e-05

Training epoch-17 batch-125
Running loss of epoch-17 batch-125 = 2.664141356945038e-05

Training epoch-17 batch-126
Running loss of epoch-17 batch-126 = 1.848675310611725e-05

Training epoch-17 batch-127
Running loss of epoch-17 batch-127 = 9.923707693815231e-06

Training epoch-17 batch-128
Running loss of epoch-17 batch-128 = 2.0175473764538765e-05

Training epoch-17 batch-129
Running loss of epoch-17 batch-129 = 9.913230314850807e-06

Training epoch-17 batch-130
Running loss of epoch-17 batch-130 = 1.45728699862957e-05

Training epoch-17 batch-131
Running loss of epoch-17 batch-131 = 1.0805437341332436e-05

Training epoch-17 batch-132
Running loss of epoch-17 batch-132 = 1.4086253941059113e-05

Training epoch-17 batch-133
Running loss of epoch-17 batch-133 = 1.937197521328926e-05

Training epoch-17 batch-134
Running loss of epoch-17 batch-134 = 1.4818040654063225e-05

Training epoch-17 batch-135
Running loss of epoch-17 batch-135 = 3.810669295489788e-05

Training epoch-17 batch-136
Running loss of epoch-17 batch-136 = 2.3504020646214485e-05

Training epoch-17 batch-137
Running loss of epoch-17 batch-137 = 1.6774749383330345e-05

Training epoch-17 batch-138
Running loss of epoch-17 batch-138 = 2.4263747036457062e-05

Training epoch-17 batch-139
Running loss of epoch-17 batch-139 = 2.6306603103876114e-05

Training epoch-17 batch-140
Running loss of epoch-17 batch-140 = 1.3511162251234055e-05

Training epoch-17 batch-141
Running loss of epoch-17 batch-141 = 2.2068852558732033e-05

Training epoch-17 batch-142
Running loss of epoch-17 batch-142 = 1.7196405678987503e-05

Training epoch-17 batch-143
Running loss of epoch-17 batch-143 = 1.2158649042248726e-05

Training epoch-17 batch-144
Running loss of epoch-17 batch-144 = 2.1076295524835587e-05

Training epoch-17 batch-145
Running loss of epoch-17 batch-145 = 3.053434193134308e-05

Training epoch-17 batch-146
Running loss of epoch-17 batch-146 = 2.9659364372491837e-05

Training epoch-17 batch-147
Running loss of epoch-17 batch-147 = 1.9249971956014633e-05

Training epoch-17 batch-148
Running loss of epoch-17 batch-148 = 2.0132167264819145e-05

Training epoch-17 batch-149
Running loss of epoch-17 batch-149 = 1.4548888429999352e-05

Training epoch-17 batch-150
Running loss of epoch-17 batch-150 = 1.7196638509631157e-05

Training epoch-17 batch-151
Running loss of epoch-17 batch-151 = 2.0175357349216938e-05

Training epoch-17 batch-152
Running loss of epoch-17 batch-152 = 1.41847413033247e-05

Training epoch-17 batch-153
Running loss of epoch-17 batch-153 = 1.8134014680981636e-05

Training epoch-17 batch-154
Running loss of epoch-17 batch-154 = 1.1413823813199997e-05

Training epoch-17 batch-155
Running loss of epoch-17 batch-155 = 1.1961441487073898e-05

Training epoch-17 batch-156
Running loss of epoch-17 batch-156 = 2.6852590963244438e-05

Training epoch-17 batch-157
Running loss of epoch-17 batch-157 = 3.785640001296997e-05

Finished training epoch-17.



Average train loss at epoch-17 = 2.0556269586086274e-05

Started Evaluation

Average val loss at epoch-17 = 2.680407816642209

Accuracy for classes:
Accuracy for class equals is: 76.24 %
Accuracy for class main is: 64.75 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 51.88 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 28.70 %
Accuracy for class execute is: 26.91 %
Accuracy for class get is: 37.69 %

Overall Accuracy = 54.76 %

Finished Evaluation



Started training epoch-18


Training epoch-18 batch-1
Running loss of epoch-18 batch-1 = 2.2484688088297844e-05

Training epoch-18 batch-2
Running loss of epoch-18 batch-2 = 1.4869729056954384e-05

Training epoch-18 batch-3
Running loss of epoch-18 batch-3 = 2.738647162914276e-05

Training epoch-18 batch-4
Running loss of epoch-18 batch-4 = 1.5228521078824997e-05

Training epoch-18 batch-5
Running loss of epoch-18 batch-5 = 1.4361459761857986e-05

Training epoch-18 batch-6
Running loss of epoch-18 batch-6 = 1.4913966879248619e-05

Training epoch-18 batch-7
Running loss of epoch-18 batch-7 = 1.805904321372509e-05

Training epoch-18 batch-8
Running loss of epoch-18 batch-8 = 1.435563899576664e-05

Training epoch-18 batch-9
Running loss of epoch-18 batch-9 = 1.5681376680731773e-05

Training epoch-18 batch-10
Running loss of epoch-18 batch-10 = 1.745019108057022e-05

Training epoch-18 batch-11
Running loss of epoch-18 batch-11 = 3.4515862353146076e-05

Training epoch-18 batch-12
Running loss of epoch-18 batch-12 = 1.6886740922927856e-05

Training epoch-18 batch-13
Running loss of epoch-18 batch-13 = 1.695158425718546e-05

Training epoch-18 batch-14
Running loss of epoch-18 batch-14 = 1.1462136171758175e-05

Training epoch-18 batch-15
Running loss of epoch-18 batch-15 = 2.8296606615185738e-05

Training epoch-18 batch-16
Running loss of epoch-18 batch-16 = 1.4008255675435066e-05

Training epoch-18 batch-17
Running loss of epoch-18 batch-17 = 8.273869752883911e-06

Training epoch-18 batch-18
Running loss of epoch-18 batch-18 = 1.7105136066675186e-05

Training epoch-18 batch-19
Running loss of epoch-18 batch-19 = 1.549883745610714e-05

Training epoch-18 batch-20
Running loss of epoch-18 batch-20 = 1.6032252460718155e-05

Training epoch-18 batch-21
Running loss of epoch-18 batch-21 = 1.7889076843857765e-05

Training epoch-18 batch-22
Running loss of epoch-18 batch-22 = 1.2210337445139885e-05

Training epoch-18 batch-23
Running loss of epoch-18 batch-23 = 2.4191569536924362e-05

Training epoch-18 batch-24
Running loss of epoch-18 batch-24 = 1.3577984645962715e-05

Training epoch-18 batch-25
Running loss of epoch-18 batch-25 = 1.4600111171603203e-05

Training epoch-18 batch-26
Running loss of epoch-18 batch-26 = 1.8549617379903793e-05

Training epoch-18 batch-27
Running loss of epoch-18 batch-27 = 1.6155419871211052e-05

Training epoch-18 batch-28
Running loss of epoch-18 batch-28 = 1.2456672266125679e-05

Training epoch-18 batch-29
Running loss of epoch-18 batch-29 = 3.138743340969086e-05

Training epoch-18 batch-30
Running loss of epoch-18 batch-30 = 1.634354703128338e-05

Training epoch-18 batch-31
Running loss of epoch-18 batch-31 = 1.3042939826846123e-05

Training epoch-18 batch-32
Running loss of epoch-18 batch-32 = 1.8919352442026138e-05

Training epoch-18 batch-33
Running loss of epoch-18 batch-33 = 1.6180798411369324e-05

Training epoch-18 batch-34
Running loss of epoch-18 batch-34 = 2.201995812356472e-05

Training epoch-18 batch-35
Running loss of epoch-18 batch-35 = 1.72504223883152e-05

Training epoch-18 batch-36
Running loss of epoch-18 batch-36 = 1.7058569937944412e-05

Training epoch-18 batch-37
Running loss of epoch-18 batch-37 = 1.3674376532435417e-05

Training epoch-18 batch-38
Running loss of epoch-18 batch-38 = 1.816730946302414e-05

Training epoch-18 batch-39
Running loss of epoch-18 batch-39 = 2.0057428628206253e-05

Training epoch-18 batch-40
Running loss of epoch-18 batch-40 = 2.1694693714380264e-05

Training epoch-18 batch-41
Running loss of epoch-18 batch-41 = 2.955435775220394e-05

Training epoch-18 batch-42
Running loss of epoch-18 batch-42 = 2.1507032215595245e-05

Training epoch-18 batch-43
Running loss of epoch-18 batch-43 = 1.5387777239084244e-05

Training epoch-18 batch-44
Running loss of epoch-18 batch-44 = 2.625212073326111e-05

Training epoch-18 batch-45
Running loss of epoch-18 batch-45 = 3.752531483769417e-05

Training epoch-18 batch-46
Running loss of epoch-18 batch-46 = 1.5910016372799873e-05

Training epoch-18 batch-47
Running loss of epoch-18 batch-47 = 1.0019633919000626e-05

Training epoch-18 batch-48
Running loss of epoch-18 batch-48 = 1.6520731151103973e-05

Training epoch-18 batch-49
Running loss of epoch-18 batch-49 = 2.4436041712760925e-05

Training epoch-18 batch-50
Running loss of epoch-18 batch-50 = 1.565041020512581e-05

Training epoch-18 batch-51
Running loss of epoch-18 batch-51 = 1.4290446415543556e-05

Training epoch-18 batch-52
Running loss of epoch-18 batch-52 = 1.3043172657489777e-05

Training epoch-18 batch-53
Running loss of epoch-18 batch-53 = 1.4096731320023537e-05

Training epoch-18 batch-54
Running loss of epoch-18 batch-54 = 1.3780314475297928e-05

Training epoch-18 batch-55
Running loss of epoch-18 batch-55 = 1.834845170378685e-05

Training epoch-18 batch-56
Running loss of epoch-18 batch-56 = 1.9337749108672142e-05

Training epoch-18 batch-57
Running loss of epoch-18 batch-57 = 1.852423883974552e-05

Training epoch-18 batch-58
Running loss of epoch-18 batch-58 = 2.030283212661743e-05

Training epoch-18 batch-59
Running loss of epoch-18 batch-59 = 2.2486085072159767e-05

Training epoch-18 batch-60
Running loss of epoch-18 batch-60 = 1.2801727280020714e-05

Training epoch-18 batch-61
Running loss of epoch-18 batch-61 = 1.893751323223114e-05

Training epoch-18 batch-62
Running loss of epoch-18 batch-62 = 8.897855877876282e-06

Training epoch-18 batch-63
Running loss of epoch-18 batch-63 = 2.394244074821472e-05

Training epoch-18 batch-64
Running loss of epoch-18 batch-64 = 2.298201434314251e-05

Training epoch-18 batch-65
Running loss of epoch-18 batch-65 = 1.3162381947040558e-05

Training epoch-18 batch-66
Running loss of epoch-18 batch-66 = 2.8447364456951618e-05

Training epoch-18 batch-67
Running loss of epoch-18 batch-67 = 1.3719778507947922e-05

Training epoch-18 batch-68
Running loss of epoch-18 batch-68 = 1.771119423210621e-05

Training epoch-18 batch-69
Running loss of epoch-18 batch-69 = 1.518777571618557e-05

Training epoch-18 batch-70
Running loss of epoch-18 batch-70 = 1.033092848956585e-05

Training epoch-18 batch-71
Running loss of epoch-18 batch-71 = 2.260936889797449e-05

Training epoch-18 batch-72
Running loss of epoch-18 batch-72 = 2.108188346028328e-05

Training epoch-18 batch-73
Running loss of epoch-18 batch-73 = 2.2576656192541122e-05

Training epoch-18 batch-74
Running loss of epoch-18 batch-74 = 1.2673670426011086e-05

Training epoch-18 batch-75
Running loss of epoch-18 batch-75 = 1.4984281733632088e-05

Training epoch-18 batch-76
Running loss of epoch-18 batch-76 = 1.3710232451558113e-05

Training epoch-18 batch-77
Running loss of epoch-18 batch-77 = 3.149034455418587e-05

Training epoch-18 batch-78
Running loss of epoch-18 batch-78 = 2.631545066833496e-05

Training epoch-18 batch-79
Running loss of epoch-18 batch-79 = 1.5735626220703125e-05

Training epoch-18 batch-80
Running loss of epoch-18 batch-80 = 1.7875339835882187e-05

Training epoch-18 batch-81
Running loss of epoch-18 batch-81 = 2.6326277293264866e-05

Training epoch-18 batch-82
Running loss of epoch-18 batch-82 = 1.700245775282383e-05

Training epoch-18 batch-83
Running loss of epoch-18 batch-83 = 1.2592179700732231e-05

Training epoch-18 batch-84
Running loss of epoch-18 batch-84 = 1.8123304471373558e-05

Training epoch-18 batch-85
Running loss of epoch-18 batch-85 = 1.561385579407215e-05

Training epoch-18 batch-86
Running loss of epoch-18 batch-86 = 1.2444565072655678e-05

Training epoch-18 batch-87
Running loss of epoch-18 batch-87 = 1.1992407962679863e-05

Training epoch-18 batch-88
Running loss of epoch-18 batch-88 = 9.711366146802902e-06

Training epoch-18 batch-89
Running loss of epoch-18 batch-89 = 1.184782013297081e-05

Training epoch-18 batch-90
Running loss of epoch-18 batch-90 = 1.2944918125867844e-05

Training epoch-18 batch-91
Running loss of epoch-18 batch-91 = 8.369795978069305e-06

Training epoch-18 batch-92
Running loss of epoch-18 batch-92 = 1.7115380614995956e-05

Training epoch-18 batch-93
Running loss of epoch-18 batch-93 = 1.8449383787810802e-05

Training epoch-18 batch-94
Running loss of epoch-18 batch-94 = 1.4104880392551422e-05

Training epoch-18 batch-95
Running loss of epoch-18 batch-95 = 1.6993843019008636e-05

Training epoch-18 batch-96
Running loss of epoch-18 batch-96 = 1.5583587810397148e-05

Training epoch-18 batch-97
Running loss of epoch-18 batch-97 = 1.62515789270401e-05

Training epoch-18 batch-98
Running loss of epoch-18 batch-98 = 1.6516773030161858e-05

Training epoch-18 batch-99
Running loss of epoch-18 batch-99 = 9.534647688269615e-06

Training epoch-18 batch-100
Running loss of epoch-18 batch-100 = 1.352769322693348e-05

Training epoch-18 batch-101
Running loss of epoch-18 batch-101 = 1.0870862752199173e-05

Training epoch-18 batch-102
Running loss of epoch-18 batch-102 = 1.93743035197258e-05

Training epoch-18 batch-103
Running loss of epoch-18 batch-103 = 2.3699365556240082e-05

Training epoch-18 batch-104
Running loss of epoch-18 batch-104 = 1.9622966647148132e-05

Training epoch-18 batch-105
Running loss of epoch-18 batch-105 = 2.0029721781611443e-05

Training epoch-18 batch-106
Running loss of epoch-18 batch-106 = 1.927325502038002e-05

Training epoch-18 batch-107
Running loss of epoch-18 batch-107 = 1.8693972378969193e-05

Training epoch-18 batch-108
Running loss of epoch-18 batch-108 = 1.3530021533370018e-05

Training epoch-18 batch-109
Running loss of epoch-18 batch-109 = 1.9606435671448708e-05

Training epoch-18 batch-110
Running loss of epoch-18 batch-110 = 1.4904886484146118e-05

Training epoch-18 batch-111
Running loss of epoch-18 batch-111 = 2.245674841105938e-05

Training epoch-18 batch-112
Running loss of epoch-18 batch-112 = 1.8495135009288788e-05

Training epoch-18 batch-113
Running loss of epoch-18 batch-113 = 1.9847415387630463e-05

Training epoch-18 batch-114
Running loss of epoch-18 batch-114 = 2.0344741642475128e-05

Training epoch-18 batch-115
Running loss of epoch-18 batch-115 = 1.986417919397354e-05

Training epoch-18 batch-116
Running loss of epoch-18 batch-116 = 1.8408987671136856e-05

Training epoch-18 batch-117
Running loss of epoch-18 batch-117 = 1.8883496522903442e-05

Training epoch-18 batch-118
Running loss of epoch-18 batch-118 = 1.627136953175068e-05

Training epoch-18 batch-119
Running loss of epoch-18 batch-119 = 1.5711644664406776e-05

Training epoch-18 batch-120
Running loss of epoch-18 batch-120 = 1.9190600141882896e-05

Training epoch-18 batch-121
Running loss of epoch-18 batch-121 = 2.4474458768963814e-05

Training epoch-18 batch-122
Running loss of epoch-18 batch-122 = 2.1888641640543938e-05

Training epoch-18 batch-123
Running loss of epoch-18 batch-123 = 1.6303500160574913e-05

Training epoch-18 batch-124
Running loss of epoch-18 batch-124 = 2.4566310457885265e-05

Training epoch-18 batch-125
Running loss of epoch-18 batch-125 = 7.655005902051926e-06

Training epoch-18 batch-126
Running loss of epoch-18 batch-126 = 1.748441718518734e-05

Training epoch-18 batch-127
Running loss of epoch-18 batch-127 = 1.1530006304383278e-05

Training epoch-18 batch-128
Running loss of epoch-18 batch-128 = 9.00845043361187e-06

Training epoch-18 batch-129
Running loss of epoch-18 batch-129 = 1.600733958184719e-05

Training epoch-18 batch-130
Running loss of epoch-18 batch-130 = 1.7855782061815262e-05

Training epoch-18 batch-131
Running loss of epoch-18 batch-131 = 2.515898086130619e-05

Training epoch-18 batch-132
Running loss of epoch-18 batch-132 = 1.8023070879280567e-05

Training epoch-18 batch-133
Running loss of epoch-18 batch-133 = 1.5148194506764412e-05

Training epoch-18 batch-134
Running loss of epoch-18 batch-134 = 2.7821864932775497e-05

Training epoch-18 batch-135
Running loss of epoch-18 batch-135 = 1.4096498489379883e-05

Training epoch-18 batch-136
Running loss of epoch-18 batch-136 = 2.4942681193351746e-05

Training epoch-18 batch-137
Running loss of epoch-18 batch-137 = 1.6194768249988556e-05

Training epoch-18 batch-138
Running loss of epoch-18 batch-138 = 2.028048038482666e-05

Training epoch-18 batch-139
Running loss of epoch-18 batch-139 = 1.0969117283821106e-05

Training epoch-18 batch-140
Running loss of epoch-18 batch-140 = 1.4417339116334915e-05

Training epoch-18 batch-141
Running loss of epoch-18 batch-141 = 1.3530254364013672e-05

Training epoch-18 batch-142
Running loss of epoch-18 batch-142 = 1.3760291039943695e-05

Training epoch-18 batch-143
Running loss of epoch-18 batch-143 = 1.2660864740610123e-05

Training epoch-18 batch-144
Running loss of epoch-18 batch-144 = 1.2388918548822403e-05

Training epoch-18 batch-145
Running loss of epoch-18 batch-145 = 2.8852373361587524e-05

Training epoch-18 batch-146
Running loss of epoch-18 batch-146 = 1.8633902072906494e-05

Training epoch-18 batch-147
Running loss of epoch-18 batch-147 = 1.6108853742480278e-05

Training epoch-18 batch-148
Running loss of epoch-18 batch-148 = 2.267665695399046e-05

Training epoch-18 batch-149
Running loss of epoch-18 batch-149 = 1.3167737051844597e-05

Training epoch-18 batch-150
Running loss of epoch-18 batch-150 = 9.446637704968452e-06

Training epoch-18 batch-151
Running loss of epoch-18 batch-151 = 2.2630207240581512e-05

Training epoch-18 batch-152
Running loss of epoch-18 batch-152 = 1.5315832570195198e-05

Training epoch-18 batch-153
Running loss of epoch-18 batch-153 = 1.9772560335695744e-05

Training epoch-18 batch-154
Running loss of epoch-18 batch-154 = 2.063554711639881e-05

Training epoch-18 batch-155
Running loss of epoch-18 batch-155 = 2.655514981597662e-05

Training epoch-18 batch-156
Running loss of epoch-18 batch-156 = 2.6372727006673813e-05

Training epoch-18 batch-157
Running loss of epoch-18 batch-157 = 5.4329633712768555e-05

Finished training epoch-18.



Average train loss at epoch-18 = 1.7826828360557555e-05

Started Evaluation

Average val loss at epoch-18 = 2.711921852670218

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 65.08 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.93 %
Accuracy for class toString is: 52.22 %
Accuracy for class run is: 36.30 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 26.91 %
Accuracy for class get is: 37.18 %

Overall Accuracy = 54.59 %

Finished Evaluation



Started training epoch-19


Training epoch-19 batch-1
Running loss of epoch-19 batch-1 = 1.3736775144934654e-05

Training epoch-19 batch-2
Running loss of epoch-19 batch-2 = 1.1869939044117928e-05

Training epoch-19 batch-3
Running loss of epoch-19 batch-3 = 1.630396582186222e-05

Training epoch-19 batch-4
Running loss of epoch-19 batch-4 = 1.2341421097517014e-05

Training epoch-19 batch-5
Running loss of epoch-19 batch-5 = 1.6664969734847546e-05

Training epoch-19 batch-6
Running loss of epoch-19 batch-6 = 1.4787772670388222e-05

Training epoch-19 batch-7
Running loss of epoch-19 batch-7 = 1.4404300600290298e-05

Training epoch-19 batch-8
Running loss of epoch-19 batch-8 = 1.7284415662288666e-05

Training epoch-19 batch-9
Running loss of epoch-19 batch-9 = 1.6711070202291012e-05

Training epoch-19 batch-10
Running loss of epoch-19 batch-10 = 1.1570518836379051e-05

Training epoch-19 batch-11
Running loss of epoch-19 batch-11 = 1.4756107702851295e-05

Training epoch-19 batch-12
Running loss of epoch-19 batch-12 = 9.791459888219833e-06

Training epoch-19 batch-13
Running loss of epoch-19 batch-13 = 1.9686296582221985e-05

Training epoch-19 batch-14
Running loss of epoch-19 batch-14 = 2.308189868927002e-05

Training epoch-19 batch-15
Running loss of epoch-19 batch-15 = 1.7036916688084602e-05

Training epoch-19 batch-16
Running loss of epoch-19 batch-16 = 1.4048069715499878e-05

Training epoch-19 batch-17
Running loss of epoch-19 batch-17 = 2.0381761714816093e-05

Training epoch-19 batch-18
Running loss of epoch-19 batch-18 = 1.2391479685902596e-05

Training epoch-19 batch-19
Running loss of epoch-19 batch-19 = 1.7011770978569984e-05

Training epoch-19 batch-20
Running loss of epoch-19 batch-20 = 1.7271144315600395e-05

Training epoch-19 batch-21
Running loss of epoch-19 batch-21 = 1.745251938700676e-05

Training epoch-19 batch-22
Running loss of epoch-19 batch-22 = 1.893145963549614e-05

Training epoch-19 batch-23
Running loss of epoch-19 batch-23 = 1.2685544788837433e-05

Training epoch-19 batch-24
Running loss of epoch-19 batch-24 = 1.6387784853577614e-05

Training epoch-19 batch-25
Running loss of epoch-19 batch-25 = 1.6924459487199783e-05

Training epoch-19 batch-26
Running loss of epoch-19 batch-26 = 1.475098542869091e-05

Training epoch-19 batch-27
Running loss of epoch-19 batch-27 = 1.7933780327439308e-05

Training epoch-19 batch-28
Running loss of epoch-19 batch-28 = 7.416587322950363e-06

Training epoch-19 batch-29
Running loss of epoch-19 batch-29 = 8.582603186368942e-06

Training epoch-19 batch-30
Running loss of epoch-19 batch-30 = 1.5187542885541916e-05

Training epoch-19 batch-31
Running loss of epoch-19 batch-31 = 1.7530983313918114e-05

Training epoch-19 batch-32
Running loss of epoch-19 batch-32 = 1.3776938430964947e-05

Training epoch-19 batch-33
Running loss of epoch-19 batch-33 = 9.209848940372467e-06

Training epoch-19 batch-34
Running loss of epoch-19 batch-34 = 1.4137476682662964e-05

Training epoch-19 batch-35
Running loss of epoch-19 batch-35 = 1.6209669411182404e-05

Training epoch-19 batch-36
Running loss of epoch-19 batch-36 = 1.3378215953707695e-05

Training epoch-19 batch-37
Running loss of epoch-19 batch-37 = 1.1833850294351578e-05

Training epoch-19 batch-38
Running loss of epoch-19 batch-38 = 1.707114279270172e-05

Training epoch-19 batch-39
Running loss of epoch-19 batch-39 = 1.389719545841217e-05

Training epoch-19 batch-40
Running loss of epoch-19 batch-40 = 1.3703480362892151e-05

Training epoch-19 batch-41
Running loss of epoch-19 batch-41 = 1.902296207845211e-05

Training epoch-19 batch-42
Running loss of epoch-19 batch-42 = 7.134629413485527e-06

Training epoch-19 batch-43
Running loss of epoch-19 batch-43 = 2.131843939423561e-05

Training epoch-19 batch-44
Running loss of epoch-19 batch-44 = 1.174071803689003e-05

Training epoch-19 batch-45
Running loss of epoch-19 batch-45 = 1.0730931535363197e-05

Training epoch-19 batch-46
Running loss of epoch-19 batch-46 = 1.6271136701107025e-05

Training epoch-19 batch-47
Running loss of epoch-19 batch-47 = 1.3234559446573257e-05

Training epoch-19 batch-48
Running loss of epoch-19 batch-48 = 2.0892126485705376e-05

Training epoch-19 batch-49
Running loss of epoch-19 batch-49 = 1.4159129932522774e-05

Training epoch-19 batch-50
Running loss of epoch-19 batch-50 = 1.3514654710888863e-05

Training epoch-19 batch-51
Running loss of epoch-19 batch-51 = 2.4170847609639168e-05

Training epoch-19 batch-52
Running loss of epoch-19 batch-52 = 1.0672025382518768e-05

Training epoch-19 batch-53
Running loss of epoch-19 batch-53 = 1.587020233273506e-05

Training epoch-19 batch-54
Running loss of epoch-19 batch-54 = 2.5543849915266037e-05

Training epoch-19 batch-55
Running loss of epoch-19 batch-55 = 2.1455343812704086e-05

Training epoch-19 batch-56
Running loss of epoch-19 batch-56 = 2.312706783413887e-05

Training epoch-19 batch-57
Running loss of epoch-19 batch-57 = 2.3777363821864128e-05

Training epoch-19 batch-58
Running loss of epoch-19 batch-58 = 1.6558682546019554e-05

Training epoch-19 batch-59
Running loss of epoch-19 batch-59 = 2.1597719751298428e-05

Training epoch-19 batch-60
Running loss of epoch-19 batch-60 = 1.2100441381335258e-05

Training epoch-19 batch-61
Running loss of epoch-19 batch-61 = 1.3027805835008621e-05

Training epoch-19 batch-62
Running loss of epoch-19 batch-62 = 1.803925260901451e-05

Training epoch-19 batch-63
Running loss of epoch-19 batch-63 = 1.8834369257092476e-05

Training epoch-19 batch-64
Running loss of epoch-19 batch-64 = 1.620827242732048e-05

Training epoch-19 batch-65
Running loss of epoch-19 batch-65 = 1.8276972696185112e-05

Training epoch-19 batch-66
Running loss of epoch-19 batch-66 = 1.57882459461689e-05

Training epoch-19 batch-67
Running loss of epoch-19 batch-67 = 1.6247620806097984e-05

Training epoch-19 batch-68
Running loss of epoch-19 batch-68 = 1.887371763586998e-05

Training epoch-19 batch-69
Running loss of epoch-19 batch-69 = 1.3090670108795166e-05

Training epoch-19 batch-70
Running loss of epoch-19 batch-70 = 7.064547389745712e-06

Training epoch-19 batch-71
Running loss of epoch-19 batch-71 = 1.9345199689269066e-05

Training epoch-19 batch-72
Running loss of epoch-19 batch-72 = 2.2138003259897232e-05

Training epoch-19 batch-73
Running loss of epoch-19 batch-73 = 1.0021263733506203e-05

Training epoch-19 batch-74
Running loss of epoch-19 batch-74 = 1.319078728556633e-05

Training epoch-19 batch-75
Running loss of epoch-19 batch-75 = 1.3162847608327866e-05

Training epoch-19 batch-76
Running loss of epoch-19 batch-76 = 1.3626180589199066e-05

Training epoch-19 batch-77
Running loss of epoch-19 batch-77 = 1.2188218533992767e-05

Training epoch-19 batch-78
Running loss of epoch-19 batch-78 = 1.847604289650917e-05

Training epoch-19 batch-79
Running loss of epoch-19 batch-79 = 2.9753195121884346e-05

Training epoch-19 batch-80
Running loss of epoch-19 batch-80 = 1.8823426216840744e-05

Training epoch-19 batch-81
Running loss of epoch-19 batch-81 = 1.53491273522377e-05

Training epoch-19 batch-82
Running loss of epoch-19 batch-82 = 1.5420373529195786e-05

Training epoch-19 batch-83
Running loss of epoch-19 batch-83 = 2.6458059437572956e-05

Training epoch-19 batch-84
Running loss of epoch-19 batch-84 = 2.07370612770319e-05

Training epoch-19 batch-85
Running loss of epoch-19 batch-85 = 1.8761027604341507e-05

Training epoch-19 batch-86
Running loss of epoch-19 batch-86 = 1.3646436855196953e-05

Training epoch-19 batch-87
Running loss of epoch-19 batch-87 = 2.6026740670204163e-05

Training epoch-19 batch-88
Running loss of epoch-19 batch-88 = 1.3705575838685036e-05

Training epoch-19 batch-89
Running loss of epoch-19 batch-89 = 1.9265804439783096e-05

Training epoch-19 batch-90
Running loss of epoch-19 batch-90 = 2.2437656298279762e-05

Training epoch-19 batch-91
Running loss of epoch-19 batch-91 = 1.5111640095710754e-05

Training epoch-19 batch-92
Running loss of epoch-19 batch-92 = 1.1310214176774025e-05

Training epoch-19 batch-93
Running loss of epoch-19 batch-93 = 1.2738863006234169e-05

Training epoch-19 batch-94
Running loss of epoch-19 batch-94 = 1.493026502430439e-05

Training epoch-19 batch-95
Running loss of epoch-19 batch-95 = 2.06048134714365e-05

Training epoch-19 batch-96
Running loss of epoch-19 batch-96 = 1.1424301192164421e-05

Training epoch-19 batch-97
Running loss of epoch-19 batch-97 = 1.3584503903985023e-05

Training epoch-19 batch-98
Running loss of epoch-19 batch-98 = 1.4102552086114883e-05

Training epoch-19 batch-99
Running loss of epoch-19 batch-99 = 1.398497261106968e-05

Training epoch-19 batch-100
Running loss of epoch-19 batch-100 = 2.0189210772514343e-05

Training epoch-19 batch-101
Running loss of epoch-19 batch-101 = 1.5631550922989845e-05

Training epoch-19 batch-102
Running loss of epoch-19 batch-102 = 1.871422864496708e-05

Training epoch-19 batch-103
Running loss of epoch-19 batch-103 = 1.632445491850376e-05

Training epoch-19 batch-104
Running loss of epoch-19 batch-104 = 1.1903932318091393e-05

Training epoch-19 batch-105
Running loss of epoch-19 batch-105 = 1.3946788385510445e-05

Training epoch-19 batch-106
Running loss of epoch-19 batch-106 = 2.684909850358963e-05

Training epoch-19 batch-107
Running loss of epoch-19 batch-107 = 8.706701919436455e-06

Training epoch-19 batch-108
Running loss of epoch-19 batch-108 = 8.712289854884148e-06

Training epoch-19 batch-109
Running loss of epoch-19 batch-109 = 1.4279969036579132e-05

Training epoch-19 batch-110
Running loss of epoch-19 batch-110 = 1.1114869266748428e-05

Training epoch-19 batch-111
Running loss of epoch-19 batch-111 = 6.972113624215126e-06

Training epoch-19 batch-112
Running loss of epoch-19 batch-112 = 1.4689983800053596e-05

Training epoch-19 batch-113
Running loss of epoch-19 batch-113 = 1.4463905245065689e-05

Training epoch-19 batch-114
Running loss of epoch-19 batch-114 = 2.1852203644812107e-05

Training epoch-19 batch-115
Running loss of epoch-19 batch-115 = 1.3532815501093864e-05

Training epoch-19 batch-116
Running loss of epoch-19 batch-116 = 2.1392013877630234e-05

Training epoch-19 batch-117
Running loss of epoch-19 batch-117 = 1.1535361409187317e-05

Training epoch-19 batch-118
Running loss of epoch-19 batch-118 = 1.6538775525987148e-05

Training epoch-19 batch-119
Running loss of epoch-19 batch-119 = 9.125564247369766e-06

Training epoch-19 batch-120
Running loss of epoch-19 batch-120 = 1.6034115105867386e-05

Training epoch-19 batch-121
Running loss of epoch-19 batch-121 = 2.024741843342781e-05

Training epoch-19 batch-122
Running loss of epoch-19 batch-122 = 1.5364261344075203e-05

Training epoch-19 batch-123
Running loss of epoch-19 batch-123 = 1.2288335710763931e-05

Training epoch-19 batch-124
Running loss of epoch-19 batch-124 = 1.7084414139389992e-05

Training epoch-19 batch-125
Running loss of epoch-19 batch-125 = 2.0990148186683655e-05

Training epoch-19 batch-126
Running loss of epoch-19 batch-126 = 1.1073891073465347e-05

Training epoch-19 batch-127
Running loss of epoch-19 batch-127 = 1.3011274859309196e-05

Training epoch-19 batch-128
Running loss of epoch-19 batch-128 = 1.4976831153035164e-05

Training epoch-19 batch-129
Running loss of epoch-19 batch-129 = 1.2870877981185913e-05

Training epoch-19 batch-130
Running loss of epoch-19 batch-130 = 7.927883416414261e-06

Training epoch-19 batch-131
Running loss of epoch-19 batch-131 = 1.4340505003929138e-05

Training epoch-19 batch-132
Running loss of epoch-19 batch-132 = 1.0573072358965874e-05

Training epoch-19 batch-133
Running loss of epoch-19 batch-133 = 2.3255357518792152e-05

Training epoch-19 batch-134
Running loss of epoch-19 batch-134 = 1.601269468665123e-05

Training epoch-19 batch-135
Running loss of epoch-19 batch-135 = 1.828535459935665e-05

Training epoch-19 batch-136
Running loss of epoch-19 batch-136 = 1.1662021279335022e-05

Training epoch-19 batch-137
Running loss of epoch-19 batch-137 = 1.680338755249977e-05

Training epoch-19 batch-138
Running loss of epoch-19 batch-138 = 1.3879965990781784e-05

Training epoch-19 batch-139
Running loss of epoch-19 batch-139 = 1.484365202486515e-05

Training epoch-19 batch-140
Running loss of epoch-19 batch-140 = 8.899020031094551e-06

Training epoch-19 batch-141
Running loss of epoch-19 batch-141 = 9.835697710514069e-06

Training epoch-19 batch-142
Running loss of epoch-19 batch-142 = 1.426483504474163e-05

Training epoch-19 batch-143
Running loss of epoch-19 batch-143 = 1.593935303390026e-05

Training epoch-19 batch-144
Running loss of epoch-19 batch-144 = 1.8080114386975765e-05

Training epoch-19 batch-145
Running loss of epoch-19 batch-145 = 1.0617310181260109e-05

Training epoch-19 batch-146
Running loss of epoch-19 batch-146 = 1.849699765443802e-05

Training epoch-19 batch-147
Running loss of epoch-19 batch-147 = 7.461290806531906e-06

Training epoch-19 batch-148
Running loss of epoch-19 batch-148 = 1.772330142557621e-05

Training epoch-19 batch-149
Running loss of epoch-19 batch-149 = 1.2744683772325516e-05

Training epoch-19 batch-150
Running loss of epoch-19 batch-150 = 1.867616083472967e-05

Training epoch-19 batch-151
Running loss of epoch-19 batch-151 = 1.7377897165715694e-05

Training epoch-19 batch-152
Running loss of epoch-19 batch-152 = 1.5298603102564812e-05

Training epoch-19 batch-153
Running loss of epoch-19 batch-153 = 2.0451843738555908e-05

Training epoch-19 batch-154
Running loss of epoch-19 batch-154 = 1.2000324204564095e-05

Training epoch-19 batch-155
Running loss of epoch-19 batch-155 = 1.1614756658673286e-05

Training epoch-19 batch-156
Running loss of epoch-19 batch-156 = 1.0796356946229935e-05

Training epoch-19 batch-157
Running loss of epoch-19 batch-157 = 5.568191409111023e-05

Finished training epoch-19.



Average train loss at epoch-19 = 1.560962498188019e-05

Started Evaluation

Average val loss at epoch-19 = 2.744103128188535

Accuracy for classes:
Accuracy for class equals is: 73.93 %
Accuracy for class main is: 64.75 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 55.97 %
Accuracy for class toString is: 52.22 %
Accuracy for class run is: 37.21 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 33.41 %
Accuracy for class execute is: 26.10 %
Accuracy for class get is: 36.67 %

Overall Accuracy = 54.57 %

Finished Evaluation



Started training epoch-20


Training epoch-20 batch-1
Running loss of epoch-20 batch-1 = 1.120520755648613e-05

Training epoch-20 batch-2
Running loss of epoch-20 batch-2 = 1.0926742106676102e-05

Training epoch-20 batch-3
Running loss of epoch-20 batch-3 = 1.7909100279211998e-05

Training epoch-20 batch-4
Running loss of epoch-20 batch-4 = 1.4465069398283958e-05

Training epoch-20 batch-5
Running loss of epoch-20 batch-5 = 1.5617115423083305e-05

Training epoch-20 batch-6
Running loss of epoch-20 batch-6 = 1.7232494428753853e-05

Training epoch-20 batch-7
Running loss of epoch-20 batch-7 = 1.9931932911276817e-05

Training epoch-20 batch-8
Running loss of epoch-20 batch-8 = 1.52678694576025e-05

Training epoch-20 batch-9
Running loss of epoch-20 batch-9 = 1.7843209207057953e-05

Training epoch-20 batch-10
Running loss of epoch-20 batch-10 = 1.3823853805661201e-05

Training epoch-20 batch-11
Running loss of epoch-20 batch-11 = 1.6427598893642426e-05

Training epoch-20 batch-12
Running loss of epoch-20 batch-12 = 1.2475298717617989e-05

Training epoch-20 batch-13
Running loss of epoch-20 batch-13 = 1.1155614629387856e-05

Training epoch-20 batch-14
Running loss of epoch-20 batch-14 = 9.899260476231575e-06

Training epoch-20 batch-15
Running loss of epoch-20 batch-15 = 6.532296538352966e-06

Training epoch-20 batch-16
Running loss of epoch-20 batch-16 = 8.945353329181671e-06

Training epoch-20 batch-17
Running loss of epoch-20 batch-17 = 2.1886546164751053e-05

Training epoch-20 batch-18
Running loss of epoch-20 batch-18 = 1.0785646736621857e-05

Training epoch-20 batch-19
Running loss of epoch-20 batch-19 = 1.4311634004116058e-05

Training epoch-20 batch-20
Running loss of epoch-20 batch-20 = 1.6590580344200134e-05

Training epoch-20 batch-21
Running loss of epoch-20 batch-21 = 1.522956881672144e-05

Training epoch-20 batch-22
Running loss of epoch-20 batch-22 = 1.0015210136771202e-05

Training epoch-20 batch-23
Running loss of epoch-20 batch-23 = 1.7279060557484627e-05

Training epoch-20 batch-24
Running loss of epoch-20 batch-24 = 1.0372139513492584e-05

Training epoch-20 batch-25
Running loss of epoch-20 batch-25 = 1.2325821444392204e-05

Training epoch-20 batch-26
Running loss of epoch-20 batch-26 = 2.2172462195158005e-05

Training epoch-20 batch-27
Running loss of epoch-20 batch-27 = 7.683876901865005e-06

Training epoch-20 batch-28
Running loss of epoch-20 batch-28 = 1.771911047399044e-05

Training epoch-20 batch-29
Running loss of epoch-20 batch-29 = 1.107645221054554e-05

Training epoch-20 batch-30
Running loss of epoch-20 batch-30 = 7.834983989596367e-06

Training epoch-20 batch-31
Running loss of epoch-20 batch-31 = 1.694587990641594e-05

Training epoch-20 batch-32
Running loss of epoch-20 batch-32 = 1.2957490980625153e-05

Training epoch-20 batch-33
Running loss of epoch-20 batch-33 = 1.7777783796191216e-05

Training epoch-20 batch-34
Running loss of epoch-20 batch-34 = 1.5674158930778503e-05

Training epoch-20 batch-35
Running loss of epoch-20 batch-35 = 1.3929791748523712e-05

Training epoch-20 batch-36
Running loss of epoch-20 batch-36 = 7.83754512667656e-06

Training epoch-20 batch-37
Running loss of epoch-20 batch-37 = 1.8486054614186287e-05

Training epoch-20 batch-38
Running loss of epoch-20 batch-38 = 1.8001068383455276e-05

Training epoch-20 batch-39
Running loss of epoch-20 batch-39 = 1.3797776773571968e-05

Training epoch-20 batch-40
Running loss of epoch-20 batch-40 = 2.9293587431311607e-05

Training epoch-20 batch-41
Running loss of epoch-20 batch-41 = 1.6330741345882416e-05

Training epoch-20 batch-42
Running loss of epoch-20 batch-42 = 1.5325844287872314e-05

Training epoch-20 batch-43
Running loss of epoch-20 batch-43 = 8.637085556983948e-06

Training epoch-20 batch-44
Running loss of epoch-20 batch-44 = 1.1446652933955193e-05

Training epoch-20 batch-45
Running loss of epoch-20 batch-45 = 1.2419885024428368e-05

Training epoch-20 batch-46
Running loss of epoch-20 batch-46 = 1.622037962079048e-05

Training epoch-20 batch-47
Running loss of epoch-20 batch-47 = 1.6149599105119705e-05

Training epoch-20 batch-48
Running loss of epoch-20 batch-48 = 1.2283213436603546e-05

Training epoch-20 batch-49
Running loss of epoch-20 batch-49 = 1.4943769201636314e-05

Training epoch-20 batch-50
Running loss of epoch-20 batch-50 = 9.993091225624084e-06

Training epoch-20 batch-51
Running loss of epoch-20 batch-51 = 1.950119622051716e-05

Training epoch-20 batch-52
Running loss of epoch-20 batch-52 = 1.4382181689143181e-05

Training epoch-20 batch-53
Running loss of epoch-20 batch-53 = 1.3716984540224075e-05

Training epoch-20 batch-54
Running loss of epoch-20 batch-54 = 1.8600840121507645e-05

Training epoch-20 batch-55
Running loss of epoch-20 batch-55 = 2.3987609893083572e-05

Training epoch-20 batch-56
Running loss of epoch-20 batch-56 = 1.1341646313667297e-05

Training epoch-20 batch-57
Running loss of epoch-20 batch-57 = 1.5988829545676708e-05

Training epoch-20 batch-58
Running loss of epoch-20 batch-58 = 1.4341669157147408e-05

Training epoch-20 batch-59
Running loss of epoch-20 batch-59 = 1.3164244592189789e-05

Training epoch-20 batch-60
Running loss of epoch-20 batch-60 = 1.4339573681354523e-05

Training epoch-20 batch-61
Running loss of epoch-20 batch-61 = 1.0025454685091972e-05

Training epoch-20 batch-62
Running loss of epoch-20 batch-62 = 2.3189233615994453e-05

Training epoch-20 batch-63
Running loss of epoch-20 batch-63 = 7.7828299254179e-06

Training epoch-20 batch-64
Running loss of epoch-20 batch-64 = 1.1094845831394196e-05

Training epoch-20 batch-65
Running loss of epoch-20 batch-65 = 1.2281350791454315e-05

Training epoch-20 batch-66
Running loss of epoch-20 batch-66 = 2.4889828637242317e-05

Training epoch-20 batch-67
Running loss of epoch-20 batch-67 = 1.9763829186558723e-05

Training epoch-20 batch-68
Running loss of epoch-20 batch-68 = 1.7450423911213875e-05

Training epoch-20 batch-69
Running loss of epoch-20 batch-69 = 1.6431789845228195e-05

Training epoch-20 batch-70
Running loss of epoch-20 batch-70 = 1.1446652933955193e-05

Training epoch-20 batch-71
Running loss of epoch-20 batch-71 = 1.7296290025115013e-05

Training epoch-20 batch-72
Running loss of epoch-20 batch-72 = 1.1673429980874062e-05

Training epoch-20 batch-73
Running loss of epoch-20 batch-73 = 1.543154940009117e-05

Training epoch-20 batch-74
Running loss of epoch-20 batch-74 = 1.851562410593033e-05

Training epoch-20 batch-75
Running loss of epoch-20 batch-75 = 1.3334443792700768e-05

Training epoch-20 batch-76
Running loss of epoch-20 batch-76 = 1.2639909982681274e-05

Training epoch-20 batch-77
Running loss of epoch-20 batch-77 = 1.4414079487323761e-05

Training epoch-20 batch-78
Running loss of epoch-20 batch-78 = 8.567003533244133e-06

Training epoch-20 batch-79
Running loss of epoch-20 batch-79 = 1.671421341598034e-05

Training epoch-20 batch-80
Running loss of epoch-20 batch-80 = 9.969808161258698e-06

Training epoch-20 batch-81
Running loss of epoch-20 batch-81 = 1.9352184608578682e-05

Training epoch-20 batch-82
Running loss of epoch-20 batch-82 = 9.984010830521584e-06

Training epoch-20 batch-83
Running loss of epoch-20 batch-83 = 7.173977792263031e-06

Training epoch-20 batch-84
Running loss of epoch-20 batch-84 = 1.7361249774694443e-05

Training epoch-20 batch-85
Running loss of epoch-20 batch-85 = 1.2220814824104309e-05

Training epoch-20 batch-86
Running loss of epoch-20 batch-86 = 9.9909957498312e-06

Training epoch-20 batch-87
Running loss of epoch-20 batch-87 = 1.9686412997543812e-05

Training epoch-20 batch-88
Running loss of epoch-20 batch-88 = 1.1819647625088692e-05

Training epoch-20 batch-89
Running loss of epoch-20 batch-89 = 1.9424594938755035e-05

Training epoch-20 batch-90
Running loss of epoch-20 batch-90 = 2.2752443328499794e-05

Training epoch-20 batch-91
Running loss of epoch-20 batch-91 = 1.2854347005486488e-05

Training epoch-20 batch-92
Running loss of epoch-20 batch-92 = 9.070383384823799e-06

Training epoch-20 batch-93
Running loss of epoch-20 batch-93 = 1.6926554962992668e-05

Training epoch-20 batch-94
Running loss of epoch-20 batch-94 = 9.306473657488823e-06

Training epoch-20 batch-95
Running loss of epoch-20 batch-95 = 1.8901890143752098e-05

Training epoch-20 batch-96
Running loss of epoch-20 batch-96 = 1.3613374903798103e-05

Training epoch-20 batch-97
Running loss of epoch-20 batch-97 = 1.1399388313293457e-05

Training epoch-20 batch-98
Running loss of epoch-20 batch-98 = 1.3699522241950035e-05

Training epoch-20 batch-99
Running loss of epoch-20 batch-99 = 1.4051096513867378e-05

Training epoch-20 batch-100
Running loss of epoch-20 batch-100 = 1.0819872841238976e-05

Training epoch-20 batch-101
Running loss of epoch-20 batch-101 = 7.161404937505722e-06

Training epoch-20 batch-102
Running loss of epoch-20 batch-102 = 1.5452038496732712e-05

Training epoch-20 batch-103
Running loss of epoch-20 batch-103 = 1.066969707608223e-05

Training epoch-20 batch-104
Running loss of epoch-20 batch-104 = 1.4876946806907654e-05

Training epoch-20 batch-105
Running loss of epoch-20 batch-105 = 1.2438278645277023e-05

Training epoch-20 batch-106
Running loss of epoch-20 batch-106 = 1.2221862562000751e-05

Training epoch-20 batch-107
Running loss of epoch-20 batch-107 = 1.7562415450811386e-05

Training epoch-20 batch-108
Running loss of epoch-20 batch-108 = 1.6802456229925156e-05

Training epoch-20 batch-109
Running loss of epoch-20 batch-109 = 1.311558298766613e-05

Training epoch-20 batch-110
Running loss of epoch-20 batch-110 = 8.242437615990639e-06

Training epoch-20 batch-111
Running loss of epoch-20 batch-111 = 8.519040420651436e-06

Training epoch-20 batch-112
Running loss of epoch-20 batch-112 = 1.4064367860555649e-05

Training epoch-20 batch-113
Running loss of epoch-20 batch-113 = 1.2494390830397606e-05

Training epoch-20 batch-114
Running loss of epoch-20 batch-114 = 1.1076917871832848e-05

Training epoch-20 batch-115
Running loss of epoch-20 batch-115 = 1.4144927263259888e-05

Training epoch-20 batch-116
Running loss of epoch-20 batch-116 = 1.2970762327313423e-05

Training epoch-20 batch-117
Running loss of epoch-20 batch-117 = 1.2568081729114056e-05

Training epoch-20 batch-118
Running loss of epoch-20 batch-118 = 1.1151423677802086e-05

Training epoch-20 batch-119
Running loss of epoch-20 batch-119 = 1.566018909215927e-05

Training epoch-20 batch-120
Running loss of epoch-20 batch-120 = 1.2983568012714386e-05

Training epoch-20 batch-121
Running loss of epoch-20 batch-121 = 1.0635005310177803e-05

Training epoch-20 batch-122
Running loss of epoch-20 batch-122 = 1.870887354016304e-05

Training epoch-20 batch-123
Running loss of epoch-20 batch-123 = 1.025153324007988e-05

Training epoch-20 batch-124
Running loss of epoch-20 batch-124 = 9.986106306314468e-06

Training epoch-20 batch-125
Running loss of epoch-20 batch-125 = 8.356058970093727e-06

Training epoch-20 batch-126
Running loss of epoch-20 batch-126 = 1.1633848771452904e-05

Training epoch-20 batch-127
Running loss of epoch-20 batch-127 = 1.3224082067608833e-05

Training epoch-20 batch-128
Running loss of epoch-20 batch-128 = 8.926726877689362e-06

Training epoch-20 batch-129
Running loss of epoch-20 batch-129 = 1.2337462976574898e-05

Training epoch-20 batch-130
Running loss of epoch-20 batch-130 = 1.100962981581688e-05

Training epoch-20 batch-131
Running loss of epoch-20 batch-131 = 8.448492735624313e-06

Training epoch-20 batch-132
Running loss of epoch-20 batch-132 = 9.974930435419083e-06

Training epoch-20 batch-133
Running loss of epoch-20 batch-133 = 9.689712896943092e-06

Training epoch-20 batch-134
Running loss of epoch-20 batch-134 = 8.529750630259514e-06

Training epoch-20 batch-135
Running loss of epoch-20 batch-135 = 8.097384124994278e-06

Training epoch-20 batch-136
Running loss of epoch-20 batch-136 = 1.4230608940124512e-05

Training epoch-20 batch-137
Running loss of epoch-20 batch-137 = 6.416346877813339e-06

Training epoch-20 batch-138
Running loss of epoch-20 batch-138 = 1.2311851605772972e-05

Training epoch-20 batch-139
Running loss of epoch-20 batch-139 = 1.0462244972586632e-05

Training epoch-20 batch-140
Running loss of epoch-20 batch-140 = 1.3595446944236755e-05

Training epoch-20 batch-141
Running loss of epoch-20 batch-141 = 1.087249256670475e-05

Training epoch-20 batch-142
Running loss of epoch-20 batch-142 = 1.4811521396040916e-05

Training epoch-20 batch-143
Running loss of epoch-20 batch-143 = 1.3342127203941345e-05

Training epoch-20 batch-144
Running loss of epoch-20 batch-144 = 9.600305929780006e-06

Training epoch-20 batch-145
Running loss of epoch-20 batch-145 = 1.0290183126926422e-05

Training epoch-20 batch-146
Running loss of epoch-20 batch-146 = 1.3237353414297104e-05

Training epoch-20 batch-147
Running loss of epoch-20 batch-147 = 1.618289388716221e-05

Training epoch-20 batch-148
Running loss of epoch-20 batch-148 = 7.502501830458641e-06

Training epoch-20 batch-149
Running loss of epoch-20 batch-149 = 1.6133533790707588e-05

Training epoch-20 batch-150
Running loss of epoch-20 batch-150 = 2.2840453311800957e-05

Training epoch-20 batch-151
Running loss of epoch-20 batch-151 = 1.681526191532612e-05

Training epoch-20 batch-152
Running loss of epoch-20 batch-152 = 1.5262281522154808e-05

Training epoch-20 batch-153
Running loss of epoch-20 batch-153 = 2.195499837398529e-05

Training epoch-20 batch-154
Running loss of epoch-20 batch-154 = 1.810421235859394e-05

Training epoch-20 batch-155
Running loss of epoch-20 batch-155 = 1.3189390301704407e-05

Training epoch-20 batch-156
Running loss of epoch-20 batch-156 = 9.614741429686546e-06

Training epoch-20 batch-157
Running loss of epoch-20 batch-157 = 9.203329682350159e-05

Finished training epoch-20.



Average train loss at epoch-20 = 1.3887005299329758e-05

Started Evaluation

Average val loss at epoch-20 = 2.7666095495224

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.92 %
Accuracy for class setUp is: 63.11 %
Accuracy for class onCreate is: 56.29 %
Accuracy for class toString is: 51.54 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.72 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.55 %

Finished Evaluation



Started training epoch-21


Training epoch-21 batch-1
Running loss of epoch-21 batch-1 = 1.5249941498041153e-05

Training epoch-21 batch-2
Running loss of epoch-21 batch-2 = 1.4514429494738579e-05

Training epoch-21 batch-3
Running loss of epoch-21 batch-3 = 1.0153977200388908e-05

Training epoch-21 batch-4
Running loss of epoch-21 batch-4 = 1.3968907296657562e-05

Training epoch-21 batch-5
Running loss of epoch-21 batch-5 = 1.708860509097576e-05

Training epoch-21 batch-6
Running loss of epoch-21 batch-6 = 1.1784723028540611e-05

Training epoch-21 batch-7
Running loss of epoch-21 batch-7 = 1.1036638170480728e-05

Training epoch-21 batch-8
Running loss of epoch-21 batch-8 = 1.908978447318077e-05

Training epoch-21 batch-9
Running loss of epoch-21 batch-9 = 1.5924684703350067e-05

Training epoch-21 batch-10
Running loss of epoch-21 batch-10 = 7.2503462433815e-06

Training epoch-21 batch-11
Running loss of epoch-21 batch-11 = 1.3181241229176521e-05

Training epoch-21 batch-12
Running loss of epoch-21 batch-12 = 1.2120464816689491e-05

Training epoch-21 batch-13
Running loss of epoch-21 batch-13 = 1.2505566701292992e-05

Training epoch-21 batch-14
Running loss of epoch-21 batch-14 = 1.6367994248867035e-05

Training epoch-21 batch-15
Running loss of epoch-21 batch-15 = 9.988434612751007e-06

Training epoch-21 batch-16
Running loss of epoch-21 batch-16 = 9.9970493465662e-06

Training epoch-21 batch-17
Running loss of epoch-21 batch-17 = 1.5248777344822884e-05

Training epoch-21 batch-18
Running loss of epoch-21 batch-18 = 7.95372761785984e-06

Training epoch-21 batch-19
Running loss of epoch-21 batch-19 = 7.57281668484211e-06

Training epoch-21 batch-20
Running loss of epoch-21 batch-20 = 1.2272503226995468e-05

Training epoch-21 batch-21
Running loss of epoch-21 batch-21 = 9.440584108233452e-06

Training epoch-21 batch-22
Running loss of epoch-21 batch-22 = 1.6972888261079788e-05

Training epoch-21 batch-23
Running loss of epoch-21 batch-23 = 9.26479697227478e-06

Training epoch-21 batch-24
Running loss of epoch-21 batch-24 = 1.3344688341021538e-05

Training epoch-21 batch-25
Running loss of epoch-21 batch-25 = 9.802868589758873e-06

Training epoch-21 batch-26
Running loss of epoch-21 batch-26 = 1.4836201444268227e-05

Training epoch-21 batch-27
Running loss of epoch-21 batch-27 = 1.3778451830148697e-05

Training epoch-21 batch-28
Running loss of epoch-21 batch-28 = 1.2201489880681038e-05

Training epoch-21 batch-29
Running loss of epoch-21 batch-29 = 1.3540731742978096e-05

Training epoch-21 batch-30
Running loss of epoch-21 batch-30 = 1.4662975445389748e-05

Training epoch-21 batch-31
Running loss of epoch-21 batch-31 = 1.09837856143713e-05

Training epoch-21 batch-32
Running loss of epoch-21 batch-32 = 1.5992438420653343e-05

Training epoch-21 batch-33
Running loss of epoch-21 batch-33 = 1.9252998754382133e-05

Training epoch-21 batch-34
Running loss of epoch-21 batch-34 = 1.7246464267373085e-05

Training epoch-21 batch-35
Running loss of epoch-21 batch-35 = 6.2836334109306335e-06

Training epoch-21 batch-36
Running loss of epoch-21 batch-36 = 1.0034535080194473e-05

Training epoch-21 batch-37
Running loss of epoch-21 batch-37 = 1.6919337213039398e-05

Training epoch-21 batch-38
Running loss of epoch-21 batch-38 = 2.32551246881485e-05

Training epoch-21 batch-39
Running loss of epoch-21 batch-39 = 1.2593111023306847e-05

Training epoch-21 batch-40
Running loss of epoch-21 batch-40 = 1.504342071712017e-05

Training epoch-21 batch-41
Running loss of epoch-21 batch-41 = 1.877499744296074e-05

Training epoch-21 batch-42
Running loss of epoch-21 batch-42 = 1.0306714102625847e-05

Training epoch-21 batch-43
Running loss of epoch-21 batch-43 = 1.38839241117239e-05

Training epoch-21 batch-44
Running loss of epoch-21 batch-44 = 1.6008038073778152e-05

Training epoch-21 batch-45
Running loss of epoch-21 batch-45 = 9.432435035705566e-06

Training epoch-21 batch-46
Running loss of epoch-21 batch-46 = 1.8358929082751274e-05

Training epoch-21 batch-47
Running loss of epoch-21 batch-47 = 9.283190593123436e-06

Training epoch-21 batch-48
Running loss of epoch-21 batch-48 = 1.0745367035269737e-05

Training epoch-21 batch-49
Running loss of epoch-21 batch-49 = 1.1010328307747841e-05

Training epoch-21 batch-50
Running loss of epoch-21 batch-50 = 1.5021185390651226e-05

Training epoch-21 batch-51
Running loss of epoch-21 batch-51 = 1.175934448838234e-05

Training epoch-21 batch-52
Running loss of epoch-21 batch-52 = 1.0244781151413918e-05

Training epoch-21 batch-53
Running loss of epoch-21 batch-53 = 1.0255258530378342e-05

Training epoch-21 batch-54
Running loss of epoch-21 batch-54 = 1.129182055592537e-05

Training epoch-21 batch-55
Running loss of epoch-21 batch-55 = 7.727881893515587e-06

Training epoch-21 batch-56
Running loss of epoch-21 batch-56 = 1.4360761269927025e-05

Training epoch-21 batch-57
Running loss of epoch-21 batch-57 = 1.7351936548948288e-05

Training epoch-21 batch-58
Running loss of epoch-21 batch-58 = 1.0276446118950844e-05

Training epoch-21 batch-59
Running loss of epoch-21 batch-59 = 1.2014992535114288e-05

Training epoch-21 batch-60
Running loss of epoch-21 batch-60 = 1.38117466121912e-05

Training epoch-21 batch-61
Running loss of epoch-21 batch-61 = 1.8145423382520676e-05

Training epoch-21 batch-62
Running loss of epoch-21 batch-62 = 1.2013595551252365e-05

Training epoch-21 batch-63
Running loss of epoch-21 batch-63 = 9.770505130290985e-06

Training epoch-21 batch-64
Running loss of epoch-21 batch-64 = 9.469222277402878e-06

Training epoch-21 batch-65
Running loss of epoch-21 batch-65 = 9.984709322452545e-06

Training epoch-21 batch-66
Running loss of epoch-21 batch-66 = 1.5740282833576202e-05

Training epoch-21 batch-67
Running loss of epoch-21 batch-67 = 9.202631190419197e-06

Training epoch-21 batch-68
Running loss of epoch-21 batch-68 = 1.0043848305940628e-05

Training epoch-21 batch-69
Running loss of epoch-21 batch-69 = 1.3747485354542732e-05

Training epoch-21 batch-70
Running loss of epoch-21 batch-70 = 6.783520802855492e-06

Training epoch-21 batch-71
Running loss of epoch-21 batch-71 = 1.5830853953957558e-05

Training epoch-21 batch-72
Running loss of epoch-21 batch-72 = 1.00000761449337e-05

Training epoch-21 batch-73
Running loss of epoch-21 batch-73 = 1.4137942343950272e-05

Training epoch-21 batch-74
Running loss of epoch-21 batch-74 = 1.2945383787155151e-05

Training epoch-21 batch-75
Running loss of epoch-21 batch-75 = 1.0713934898376465e-05

Training epoch-21 batch-76
Running loss of epoch-21 batch-76 = 1.4721183106303215e-05

Training epoch-21 batch-77
Running loss of epoch-21 batch-77 = 8.589820936322212e-06

Training epoch-21 batch-78
Running loss of epoch-21 batch-78 = 9.88389365375042e-06

Training epoch-21 batch-79
Running loss of epoch-21 batch-79 = 1.4891847968101501e-05

Training epoch-21 batch-80
Running loss of epoch-21 batch-80 = 7.160939276218414e-06

Training epoch-21 batch-81
Running loss of epoch-21 batch-81 = 1.1144671589136124e-05

Training epoch-21 batch-82
Running loss of epoch-21 batch-82 = 1.1573312804102898e-05

Training epoch-21 batch-83
Running loss of epoch-21 batch-83 = 9.892741218209267e-06

Training epoch-21 batch-84
Running loss of epoch-21 batch-84 = 7.263384759426117e-06

Training epoch-21 batch-85
Running loss of epoch-21 batch-85 = 1.598149538040161e-05

Training epoch-21 batch-86
Running loss of epoch-21 batch-86 = 1.3879500329494476e-05

Training epoch-21 batch-87
Running loss of epoch-21 batch-87 = 9.692972525954247e-06

Training epoch-21 batch-88
Running loss of epoch-21 batch-88 = 1.5751225873827934e-05

Training epoch-21 batch-89
Running loss of epoch-21 batch-89 = 8.311355486512184e-06

Training epoch-21 batch-90
Running loss of epoch-21 batch-90 = 7.284805178642273e-06

Training epoch-21 batch-91
Running loss of epoch-21 batch-91 = 1.2648757547140121e-05

Training epoch-21 batch-92
Running loss of epoch-21 batch-92 = 1.1687632650136948e-05

Training epoch-21 batch-93
Running loss of epoch-21 batch-93 = 1.1835712939500809e-05

Training epoch-21 batch-94
Running loss of epoch-21 batch-94 = 1.2442702427506447e-05

Training epoch-21 batch-95
Running loss of epoch-21 batch-95 = 1.282314769923687e-05

Training epoch-21 batch-96
Running loss of epoch-21 batch-96 = 9.95607115328312e-06

Training epoch-21 batch-97
Running loss of epoch-21 batch-97 = 1.1690659448504448e-05

Training epoch-21 batch-98
Running loss of epoch-21 batch-98 = 1.3505574315786362e-05

Training epoch-21 batch-99
Running loss of epoch-21 batch-99 = 9.06386412680149e-06

Training epoch-21 batch-100
Running loss of epoch-21 batch-100 = 8.041970431804657e-06

Training epoch-21 batch-101
Running loss of epoch-21 batch-101 = 1.084175892174244e-05

Training epoch-21 batch-102
Running loss of epoch-21 batch-102 = 1.2913718819618225e-05

Training epoch-21 batch-103
Running loss of epoch-21 batch-103 = 1.6840407624840736e-05

Training epoch-21 batch-104
Running loss of epoch-21 batch-104 = 1.6736797988414764e-05

Training epoch-21 batch-105
Running loss of epoch-21 batch-105 = 7.0678070187568665e-06

Training epoch-21 batch-106
Running loss of epoch-21 batch-106 = 3.612833097577095e-06

Training epoch-21 batch-107
Running loss of epoch-21 batch-107 = 1.2398231774568558e-05

Training epoch-21 batch-108
Running loss of epoch-21 batch-108 = 9.78703610599041e-06

Training epoch-21 batch-109
Running loss of epoch-21 batch-109 = 8.465023711323738e-06

Training epoch-21 batch-110
Running loss of epoch-21 batch-110 = 6.510177627205849e-06

Training epoch-21 batch-111
Running loss of epoch-21 batch-111 = 1.2748409062623978e-05

Training epoch-21 batch-112
Running loss of epoch-21 batch-112 = 2.0105158910155296e-05

Training epoch-21 batch-113
Running loss of epoch-21 batch-113 = 1.850118860602379e-05

Training epoch-21 batch-114
Running loss of epoch-21 batch-114 = 1.164688728749752e-05

Training epoch-21 batch-115
Running loss of epoch-21 batch-115 = 1.0384945198893547e-05

Training epoch-21 batch-116
Running loss of epoch-21 batch-116 = 1.245969906449318e-05

Training epoch-21 batch-117
Running loss of epoch-21 batch-117 = 1.439475454390049e-05

Training epoch-21 batch-118
Running loss of epoch-21 batch-118 = 1.1091586202383041e-05

Training epoch-21 batch-119
Running loss of epoch-21 batch-119 = 1.0136747732758522e-05

Training epoch-21 batch-120
Running loss of epoch-21 batch-120 = 1.0652467608451843e-05

Training epoch-21 batch-121
Running loss of epoch-21 batch-121 = 1.1371448636054993e-05

Training epoch-21 batch-122
Running loss of epoch-21 batch-122 = 1.5475554391741753e-05

Training epoch-21 batch-123
Running loss of epoch-21 batch-123 = 1.5483936294913292e-05

Training epoch-21 batch-124
Running loss of epoch-21 batch-124 = 1.3032928109169006e-05

Training epoch-21 batch-125
Running loss of epoch-21 batch-125 = 1.0940944775938988e-05

Training epoch-21 batch-126
Running loss of epoch-21 batch-126 = 8.921604603528976e-06

Training epoch-21 batch-127
Running loss of epoch-21 batch-127 = 2.081296406686306e-05

Training epoch-21 batch-128
Running loss of epoch-21 batch-128 = 1.3030366972088814e-05

Training epoch-21 batch-129
Running loss of epoch-21 batch-129 = 1.7573125660419464e-05

Training epoch-21 batch-130
Running loss of epoch-21 batch-130 = 1.8502119928598404e-05

Training epoch-21 batch-131
Running loss of epoch-21 batch-131 = 1.2731645256280899e-05

Training epoch-21 batch-132
Running loss of epoch-21 batch-132 = 1.399521715939045e-05

Training epoch-21 batch-133
Running loss of epoch-21 batch-133 = 1.2444565072655678e-05

Training epoch-21 batch-134
Running loss of epoch-21 batch-134 = 2.0473962649703026e-05

Training epoch-21 batch-135
Running loss of epoch-21 batch-135 = 9.51625406742096e-06

Training epoch-21 batch-136
Running loss of epoch-21 batch-136 = 1.1028489097952843e-05

Training epoch-21 batch-137
Running loss of epoch-21 batch-137 = 6.998423486948013e-06

Training epoch-21 batch-138
Running loss of epoch-21 batch-138 = 1.2835720553994179e-05

Training epoch-21 batch-139
Running loss of epoch-21 batch-139 = 1.1540716513991356e-05

Training epoch-21 batch-140
Running loss of epoch-21 batch-140 = 1.1601485311985016e-05

Training epoch-21 batch-141
Running loss of epoch-21 batch-141 = 9.470852091908455e-06

Training epoch-21 batch-142
Running loss of epoch-21 batch-142 = 7.556751370429993e-06

Training epoch-21 batch-143
Running loss of epoch-21 batch-143 = 7.241498678922653e-06

Training epoch-21 batch-144
Running loss of epoch-21 batch-144 = 1.3487180694937706e-05

Training epoch-21 batch-145
Running loss of epoch-21 batch-145 = 7.628230378031731e-06

Training epoch-21 batch-146
Running loss of epoch-21 batch-146 = 1.3812677934765816e-05

Training epoch-21 batch-147
Running loss of epoch-21 batch-147 = 1.168716698884964e-05

Training epoch-21 batch-148
Running loss of epoch-21 batch-148 = 9.443843737244606e-06

Training epoch-21 batch-149
Running loss of epoch-21 batch-149 = 2.1136016584932804e-05

Training epoch-21 batch-150
Running loss of epoch-21 batch-150 = 9.571900591254234e-06

Training epoch-21 batch-151
Running loss of epoch-21 batch-151 = 9.548384696245193e-06

Training epoch-21 batch-152
Running loss of epoch-21 batch-152 = 8.858973160386086e-06

Training epoch-21 batch-153
Running loss of epoch-21 batch-153 = 1.4137476682662964e-05

Training epoch-21 batch-154
Running loss of epoch-21 batch-154 = 9.873881936073303e-06

Training epoch-21 batch-155
Running loss of epoch-21 batch-155 = 1.1922908015549183e-05

Training epoch-21 batch-156
Running loss of epoch-21 batch-156 = 1.5384750440716743e-05

Training epoch-21 batch-157
Running loss of epoch-21 batch-157 = 8.269771933555603e-05

Finished training epoch-21.



Average train loss at epoch-21 = 1.2453977018594742e-05

Started Evaluation

Average val loss at epoch-21 = 2.7870755744607827

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 65.25 %
Accuracy for class setUp is: 64.10 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.88 %
Accuracy for class run is: 37.21 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 25.70 %
Accuracy for class get is: 36.92 %

Overall Accuracy = 54.59 %

Finished Evaluation



Started training epoch-22


Training epoch-22 batch-1
Running loss of epoch-22 batch-1 = 1.5809200704097748e-05

Training epoch-22 batch-2
Running loss of epoch-22 batch-2 = 6.3141342252492905e-06

Training epoch-22 batch-3
Running loss of epoch-22 batch-3 = 7.337890565395355e-06

Training epoch-22 batch-4
Running loss of epoch-22 batch-4 = 8.486676961183548e-06

Training epoch-22 batch-5
Running loss of epoch-22 batch-5 = 6.927875801920891e-06

Training epoch-22 batch-6
Running loss of epoch-22 batch-6 = 7.421476766467094e-06

Training epoch-22 batch-7
Running loss of epoch-22 batch-7 = 1.0550953447818756e-05

Training epoch-22 batch-8
Running loss of epoch-22 batch-8 = 4.042871296405792e-06

Training epoch-22 batch-9
Running loss of epoch-22 batch-9 = 6.016111001372337e-06

Training epoch-22 batch-10
Running loss of epoch-22 batch-10 = 1.3788696378469467e-05

Training epoch-22 batch-11
Running loss of epoch-22 batch-11 = 8.26292671263218e-06

Training epoch-22 batch-12
Running loss of epoch-22 batch-12 = 1.1516502127051353e-05

Training epoch-22 batch-13
Running loss of epoch-22 batch-13 = 8.71904194355011e-06

Training epoch-22 batch-14
Running loss of epoch-22 batch-14 = 1.5637371689081192e-05

Training epoch-22 batch-15
Running loss of epoch-22 batch-15 = 1.3626180589199066e-05

Training epoch-22 batch-16
Running loss of epoch-22 batch-16 = 1.4540506526827812e-05

Training epoch-22 batch-17
Running loss of epoch-22 batch-17 = 1.2193107977509499e-05

Training epoch-22 batch-18
Running loss of epoch-22 batch-18 = 1.1074822396039963e-05

Training epoch-22 batch-19
Running loss of epoch-22 batch-19 = 4.812842234969139e-06

Training epoch-22 batch-20
Running loss of epoch-22 batch-20 = 8.369097486138344e-06

Training epoch-22 batch-21
Running loss of epoch-22 batch-21 = 1.248205080628395e-05

Training epoch-22 batch-22
Running loss of epoch-22 batch-22 = 9.165611118078232e-06

Training epoch-22 batch-23
Running loss of epoch-22 batch-23 = 1.2687873095273972e-05

Training epoch-22 batch-24
Running loss of epoch-22 batch-24 = 1.239660196006298e-05

Training epoch-22 batch-25
Running loss of epoch-22 batch-25 = 9.72021371126175e-06

Training epoch-22 batch-26
Running loss of epoch-22 batch-26 = 1.3354234397411346e-05

Training epoch-22 batch-27
Running loss of epoch-22 batch-27 = 9.667128324508667e-06

Training epoch-22 batch-28
Running loss of epoch-22 batch-28 = 9.710900485515594e-06

Training epoch-22 batch-29
Running loss of epoch-22 batch-29 = 8.865492418408394e-06

Training epoch-22 batch-30
Running loss of epoch-22 batch-30 = 1.5066470950841904e-05

Training epoch-22 batch-31
Running loss of epoch-22 batch-31 = 1.1064810678362846e-05

Training epoch-22 batch-32
Running loss of epoch-22 batch-32 = 8.177012205123901e-06

Training epoch-22 batch-33
Running loss of epoch-22 batch-33 = 1.7293496057391167e-05

Training epoch-22 batch-34
Running loss of epoch-22 batch-34 = 1.1899974197149277e-05

Training epoch-22 batch-35
Running loss of epoch-22 batch-35 = 1.1906027793884277e-05

Training epoch-22 batch-36
Running loss of epoch-22 batch-36 = 9.65828076004982e-06

Training epoch-22 batch-37
Running loss of epoch-22 batch-37 = 1.1617550626397133e-05

Training epoch-22 batch-38
Running loss of epoch-22 batch-38 = 1.501571387052536e-05

Training epoch-22 batch-39
Running loss of epoch-22 batch-39 = 8.702045306563377e-06

Training epoch-22 batch-40
Running loss of epoch-22 batch-40 = 1.2555858120322227e-05

Training epoch-22 batch-41
Running loss of epoch-22 batch-41 = 9.035691618919373e-06

Training epoch-22 batch-42
Running loss of epoch-22 batch-42 = 1.2973323464393616e-05

Training epoch-22 batch-43
Running loss of epoch-22 batch-43 = 1.6108970157802105e-05

Training epoch-22 batch-44
Running loss of epoch-22 batch-44 = 1.0307645425200462e-05

Training epoch-22 batch-45
Running loss of epoch-22 batch-45 = 9.220791980624199e-06

Training epoch-22 batch-46
Running loss of epoch-22 batch-46 = 1.0023824870586395e-05

Training epoch-22 batch-47
Running loss of epoch-22 batch-47 = 1.0377494618296623e-05

Training epoch-22 batch-48
Running loss of epoch-22 batch-48 = 7.916009053587914e-06

Training epoch-22 batch-49
Running loss of epoch-22 batch-49 = 1.0999152436852455e-05

Training epoch-22 batch-50
Running loss of epoch-22 batch-50 = 1.0841991752386093e-05

Training epoch-22 batch-51
Running loss of epoch-22 batch-51 = 7.276888936758041e-06

Training epoch-22 batch-52
Running loss of epoch-22 batch-52 = 1.7691636458039284e-05

Training epoch-22 batch-53
Running loss of epoch-22 batch-53 = 1.3990211300551891e-05

Training epoch-22 batch-54
Running loss of epoch-22 batch-54 = 8.63778404891491e-06

Training epoch-22 batch-55
Running loss of epoch-22 batch-55 = 6.509479135274887e-06

Training epoch-22 batch-56
Running loss of epoch-22 batch-56 = 9.329058229923248e-06

Training epoch-22 batch-57
Running loss of epoch-22 batch-57 = 9.312992915511131e-06

Training epoch-22 batch-58
Running loss of epoch-22 batch-58 = 5.965353921055794e-06

Training epoch-22 batch-59
Running loss of epoch-22 batch-59 = 1.337728463113308e-05

Training epoch-22 batch-60
Running loss of epoch-22 batch-60 = 1.0313466191291809e-05

Training epoch-22 batch-61
Running loss of epoch-22 batch-61 = 1.4521647244691849e-05

Training epoch-22 batch-62
Running loss of epoch-22 batch-62 = 9.025447070598602e-06

Training epoch-22 batch-63
Running loss of epoch-22 batch-63 = 9.774230420589447e-06

Training epoch-22 batch-64
Running loss of epoch-22 batch-64 = 7.25151039659977e-06

Training epoch-22 batch-65
Running loss of epoch-22 batch-65 = 1.575401984155178e-05

Training epoch-22 batch-66
Running loss of epoch-22 batch-66 = 1.0453863069415092e-05

Training epoch-22 batch-67
Running loss of epoch-22 batch-67 = 8.738134056329727e-06

Training epoch-22 batch-68
Running loss of epoch-22 batch-68 = 1.010950654745102e-05

Training epoch-22 batch-69
Running loss of epoch-22 batch-69 = 1.2973323464393616e-05

Training epoch-22 batch-70
Running loss of epoch-22 batch-70 = 1.2019183486700058e-05

Training epoch-22 batch-71
Running loss of epoch-22 batch-71 = 9.00845043361187e-06

Training epoch-22 batch-72
Running loss of epoch-22 batch-72 = 1.3863667845726013e-05

Training epoch-22 batch-73
Running loss of epoch-22 batch-73 = 1.2500444427132607e-05

Training epoch-22 batch-74
Running loss of epoch-22 batch-74 = 1.078099012374878e-05

Training epoch-22 batch-75
Running loss of epoch-22 batch-75 = 1.6582198441028595e-05

Training epoch-22 batch-76
Running loss of epoch-22 batch-76 = 1.2511620298027992e-05

Training epoch-22 batch-77
Running loss of epoch-22 batch-77 = 1.1941185221076012e-05

Training epoch-22 batch-78
Running loss of epoch-22 batch-78 = 1.3592187315225601e-05

Training epoch-22 batch-79
Running loss of epoch-22 batch-79 = 5.641719326376915e-06

Training epoch-22 batch-80
Running loss of epoch-22 batch-80 = 1.623225398361683e-05

Training epoch-22 batch-81
Running loss of epoch-22 batch-81 = 1.2673204764723778e-05

Training epoch-22 batch-82
Running loss of epoch-22 batch-82 = 1.210952177643776e-05

Training epoch-22 batch-83
Running loss of epoch-22 batch-83 = 1.2051546946167946e-05

Training epoch-22 batch-84
Running loss of epoch-22 batch-84 = 9.32253897190094e-06

Training epoch-22 batch-85
Running loss of epoch-22 batch-85 = 1.5075784176588058e-05

Training epoch-22 batch-86
Running loss of epoch-22 batch-86 = 9.900657460093498e-06

Training epoch-22 batch-87
Running loss of epoch-22 batch-87 = 1.2456905096769333e-05

Training epoch-22 batch-88
Running loss of epoch-22 batch-88 = 1.713540405035019e-05

Training epoch-22 batch-89
Running loss of epoch-22 batch-89 = 1.4005345292389393e-05

Training epoch-22 batch-90
Running loss of epoch-22 batch-90 = 1.2360047549009323e-05

Training epoch-22 batch-91
Running loss of epoch-22 batch-91 = 1.3998476788401604e-05

Training epoch-22 batch-92
Running loss of epoch-22 batch-92 = 1.0537682101130486e-05

Training epoch-22 batch-93
Running loss of epoch-22 batch-93 = 1.070881262421608e-05

Training epoch-22 batch-94
Running loss of epoch-22 batch-94 = 1.5541445463895798e-05

Training epoch-22 batch-95
Running loss of epoch-22 batch-95 = 1.0521500371396542e-05

Training epoch-22 batch-96
Running loss of epoch-22 batch-96 = 7.127644494175911e-06

Training epoch-22 batch-97
Running loss of epoch-22 batch-97 = 9.181676432490349e-06

Training epoch-22 batch-98
Running loss of epoch-22 batch-98 = 8.415430784225464e-06

Training epoch-22 batch-99
Running loss of epoch-22 batch-99 = 1.2884614989161491e-05

Training epoch-22 batch-100
Running loss of epoch-22 batch-100 = 1.8655555322766304e-05

Training epoch-22 batch-101
Running loss of epoch-22 batch-101 = 1.138029620051384e-05

Training epoch-22 batch-102
Running loss of epoch-22 batch-102 = 1.4023622497916222e-05

Training epoch-22 batch-103
Running loss of epoch-22 batch-103 = 9.980285540223122e-06

Training epoch-22 batch-104
Running loss of epoch-22 batch-104 = 6.985384970903397e-06

Training epoch-22 batch-105
Running loss of epoch-22 batch-105 = 1.3968674466013908e-05

Training epoch-22 batch-106
Running loss of epoch-22 batch-106 = 1.0405434295535088e-05

Training epoch-22 batch-107
Running loss of epoch-22 batch-107 = 1.398078165948391e-05

Training epoch-22 batch-108
Running loss of epoch-22 batch-108 = 1.3601966202259064e-05

Training epoch-22 batch-109
Running loss of epoch-22 batch-109 = 1.2657605111598969e-05

Training epoch-22 batch-110
Running loss of epoch-22 batch-110 = 1.1061783879995346e-05

Training epoch-22 batch-111
Running loss of epoch-22 batch-111 = 7.967464625835419e-06

Training epoch-22 batch-112
Running loss of epoch-22 batch-112 = 1.7095706425607204e-05

Training epoch-22 batch-113
Running loss of epoch-22 batch-113 = 8.301576599478722e-06

Training epoch-22 batch-114
Running loss of epoch-22 batch-114 = 7.551861926913261e-06

Training epoch-22 batch-115
Running loss of epoch-22 batch-115 = 1.587974838912487e-05

Training epoch-22 batch-116
Running loss of epoch-22 batch-116 = 1.1914409697055817e-05

Training epoch-22 batch-117
Running loss of epoch-22 batch-117 = 1.1473894119262695e-05

Training epoch-22 batch-118
Running loss of epoch-22 batch-118 = 7.155584171414375e-06

Training epoch-22 batch-119
Running loss of epoch-22 batch-119 = 1.4676246792078018e-05

Training epoch-22 batch-120
Running loss of epoch-22 batch-120 = 9.97958704829216e-06

Training epoch-22 batch-121
Running loss of epoch-22 batch-121 = 4.4014304876327515e-06

Training epoch-22 batch-122
Running loss of epoch-22 batch-122 = 1.2157484889030457e-05

Training epoch-22 batch-123
Running loss of epoch-22 batch-123 = 1.1537456884980202e-05

Training epoch-22 batch-124
Running loss of epoch-22 batch-124 = 3.3262185752391815e-06

Training epoch-22 batch-125
Running loss of epoch-22 batch-125 = 9.902287274599075e-06

Training epoch-22 batch-126
Running loss of epoch-22 batch-126 = 8.775154128670692e-06

Training epoch-22 batch-127
Running loss of epoch-22 batch-127 = 1.0602874681353569e-05

Training epoch-22 batch-128
Running loss of epoch-22 batch-128 = 1.5229452401399612e-05

Training epoch-22 batch-129
Running loss of epoch-22 batch-129 = 1.5384051948785782e-05

Training epoch-22 batch-130
Running loss of epoch-22 batch-130 = 8.743023499846458e-06

Training epoch-22 batch-131
Running loss of epoch-22 batch-131 = 1.3377051800489426e-05

Training epoch-22 batch-132
Running loss of epoch-22 batch-132 = 1.0264338925480843e-05

Training epoch-22 batch-133
Running loss of epoch-22 batch-133 = 1.051928848028183e-05

Training epoch-22 batch-134
Running loss of epoch-22 batch-134 = 1.008785329759121e-05

Training epoch-22 batch-135
Running loss of epoch-22 batch-135 = 8.696690201759338e-06

Training epoch-22 batch-136
Running loss of epoch-22 batch-136 = 7.58725218474865e-06

Training epoch-22 batch-137
Running loss of epoch-22 batch-137 = 1.504877582192421e-05

Training epoch-22 batch-138
Running loss of epoch-22 batch-138 = 1.5047378838062286e-05

Training epoch-22 batch-139
Running loss of epoch-22 batch-139 = 1.5369849279522896e-05

Training epoch-22 batch-140
Running loss of epoch-22 batch-140 = 7.98259861767292e-06

Training epoch-22 batch-141
Running loss of epoch-22 batch-141 = 1.0840361937880516e-05

Training epoch-22 batch-142
Running loss of epoch-22 batch-142 = 1.7140060663223267e-05

Training epoch-22 batch-143
Running loss of epoch-22 batch-143 = 9.506940841674805e-06

Training epoch-22 batch-144
Running loss of epoch-22 batch-144 = 1.1052237823605537e-05

Training epoch-22 batch-145
Running loss of epoch-22 batch-145 = 8.797505870461464e-06

Training epoch-22 batch-146
Running loss of epoch-22 batch-146 = 1.5639234334230423e-05

Training epoch-22 batch-147
Running loss of epoch-22 batch-147 = 1.4717923477292061e-05

Training epoch-22 batch-148
Running loss of epoch-22 batch-148 = 1.1151889339089394e-05

Training epoch-22 batch-149
Running loss of epoch-22 batch-149 = 1.0752817615866661e-05

Training epoch-22 batch-150
Running loss of epoch-22 batch-150 = 8.797738701105118e-06

Training epoch-22 batch-151
Running loss of epoch-22 batch-151 = 1.2614764273166656e-05

Training epoch-22 batch-152
Running loss of epoch-22 batch-152 = 2.5534769520163536e-05

Training epoch-22 batch-153
Running loss of epoch-22 batch-153 = 1.0002870112657547e-05

Training epoch-22 batch-154
Running loss of epoch-22 batch-154 = 6.599584594368935e-06

Training epoch-22 batch-155
Running loss of epoch-22 batch-155 = 6.0175079852342606e-06

Training epoch-22 batch-156
Running loss of epoch-22 batch-156 = 9.598443284630775e-06

Training epoch-22 batch-157
Running loss of epoch-22 batch-157 = 3.1031668186187744e-05

Finished training epoch-22.



Average train loss at epoch-22 = 1.1233337968587875e-05

Started Evaluation

Average val loss at epoch-22 = 2.8075022266099325

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.92 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.29 %
Accuracy for class toString is: 51.54 %
Accuracy for class run is: 38.36 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 32.06 %
Accuracy for class execute is: 26.10 %
Accuracy for class get is: 36.41 %

Overall Accuracy = 54.73 %

Finished Evaluation



Started training epoch-23


Training epoch-23 batch-1
Running loss of epoch-23 batch-1 = 1.4742370694875717e-05

Training epoch-23 batch-2
Running loss of epoch-23 batch-2 = 6.926245987415314e-06

Training epoch-23 batch-3
Running loss of epoch-23 batch-3 = 1.0534771718084812e-05

Training epoch-23 batch-4
Running loss of epoch-23 batch-4 = 8.616829290986061e-06

Training epoch-23 batch-5
Running loss of epoch-23 batch-5 = 7.366761565208435e-06

Training epoch-23 batch-6
Running loss of epoch-23 batch-6 = 1.2215692549943924e-05

Training epoch-23 batch-7
Running loss of epoch-23 batch-7 = 6.737536750733852e-06

Training epoch-23 batch-8
Running loss of epoch-23 batch-8 = 4.704343155026436e-06

Training epoch-23 batch-9
Running loss of epoch-23 batch-9 = 9.131385013461113e-06

Training epoch-23 batch-10
Running loss of epoch-23 batch-10 = 1.2240838259458542e-05

Training epoch-23 batch-11
Running loss of epoch-23 batch-11 = 8.010771125555038e-06

Training epoch-23 batch-12
Running loss of epoch-23 batch-12 = 1.0771676898002625e-05

Training epoch-23 batch-13
Running loss of epoch-23 batch-13 = 8.30087810754776e-06

Training epoch-23 batch-14
Running loss of epoch-23 batch-14 = 5.604932084679604e-06

Training epoch-23 batch-15
Running loss of epoch-23 batch-15 = 1.5201978385448456e-05

Training epoch-23 batch-16
Running loss of epoch-23 batch-16 = 1.2111850082874298e-05

Training epoch-23 batch-17
Running loss of epoch-23 batch-17 = 1.276540569961071e-05

Training epoch-23 batch-18
Running loss of epoch-23 batch-18 = 1.170765608549118e-05

Training epoch-23 batch-19
Running loss of epoch-23 batch-19 = 8.89296643435955e-06

Training epoch-23 batch-20
Running loss of epoch-23 batch-20 = 1.061498187482357e-05

Training epoch-23 batch-21
Running loss of epoch-23 batch-21 = 1.0000541806221008e-05

Training epoch-23 batch-22
Running loss of epoch-23 batch-22 = 5.8228615671396255e-06

Training epoch-23 batch-23
Running loss of epoch-23 batch-23 = 6.7213550209999084e-06

Training epoch-23 batch-24
Running loss of epoch-23 batch-24 = 1.3333745300769806e-05

Training epoch-23 batch-25
Running loss of epoch-23 batch-25 = 1.4494638890028e-05

Training epoch-23 batch-26
Running loss of epoch-23 batch-26 = 1.7763348296284676e-05

Training epoch-23 batch-27
Running loss of epoch-23 batch-27 = 9.921612218022346e-06

Training epoch-23 batch-28
Running loss of epoch-23 batch-28 = 1.430627889931202e-05

Training epoch-23 batch-29
Running loss of epoch-23 batch-29 = 7.893657311797142e-06

Training epoch-23 batch-30
Running loss of epoch-23 batch-30 = 7.880385965108871e-06

Training epoch-23 batch-31
Running loss of epoch-23 batch-31 = 1.3630371540784836e-05

Training epoch-23 batch-32
Running loss of epoch-23 batch-32 = 9.120907634496689e-06

Training epoch-23 batch-33
Running loss of epoch-23 batch-33 = 8.914386853575706e-06

Training epoch-23 batch-34
Running loss of epoch-23 batch-34 = 1.0481569916009903e-05

Training epoch-23 batch-35
Running loss of epoch-23 batch-35 = 1.1004973202943802e-05

Training epoch-23 batch-36
Running loss of epoch-23 batch-36 = 1.1181226000189781e-05

Training epoch-23 batch-37
Running loss of epoch-23 batch-37 = 9.10833477973938e-06

Training epoch-23 batch-38
Running loss of epoch-23 batch-38 = 1.363665796816349e-05

Training epoch-23 batch-39
Running loss of epoch-23 batch-39 = 1.1767027899622917e-05

Training epoch-23 batch-40
Running loss of epoch-23 batch-40 = 4.911329597234726e-06

Training epoch-23 batch-41
Running loss of epoch-23 batch-41 = 1.4884397387504578e-05

Training epoch-23 batch-42
Running loss of epoch-23 batch-42 = 1.3844575732946396e-05

Training epoch-23 batch-43
Running loss of epoch-23 batch-43 = 1.1963536962866783e-05

Training epoch-23 batch-44
Running loss of epoch-23 batch-44 = 1.1653872206807137e-05

Training epoch-23 batch-45
Running loss of epoch-23 batch-45 = 9.852927178144455e-06

Training epoch-23 batch-46
Running loss of epoch-23 batch-46 = 1.3391487300395966e-05

Training epoch-23 batch-47
Running loss of epoch-23 batch-47 = 1.3512559235095978e-05

Training epoch-23 batch-48
Running loss of epoch-23 batch-48 = 1.308252103626728e-05

Training epoch-23 batch-49
Running loss of epoch-23 batch-49 = 1.2922100722789764e-05

Training epoch-23 batch-50
Running loss of epoch-23 batch-50 = 1.4344928786158562e-05

Training epoch-23 batch-51
Running loss of epoch-23 batch-51 = 6.973510608077049e-06

Training epoch-23 batch-52
Running loss of epoch-23 batch-52 = 1.0433141142129898e-05

Training epoch-23 batch-53
Running loss of epoch-23 batch-53 = 8.87131318449974e-06

Training epoch-23 batch-54
Running loss of epoch-23 batch-54 = 1.3083219528198242e-05

Training epoch-23 batch-55
Running loss of epoch-23 batch-55 = 9.644078090786934e-06

Training epoch-23 batch-56
Running loss of epoch-23 batch-56 = 1.142360270023346e-05

Training epoch-23 batch-57
Running loss of epoch-23 batch-57 = 6.158370524644852e-06

Training epoch-23 batch-58
Running loss of epoch-23 batch-58 = 8.012400940060616e-06

Training epoch-23 batch-59
Running loss of epoch-23 batch-59 = 1.121428795158863e-05

Training epoch-23 batch-60
Running loss of epoch-23 batch-60 = 1.2492528185248375e-05

Training epoch-23 batch-61
Running loss of epoch-23 batch-61 = 9.801005944609642e-06

Training epoch-23 batch-62
Running loss of epoch-23 batch-62 = 8.1618782132864e-06

Training epoch-23 batch-63
Running loss of epoch-23 batch-63 = 6.409361958503723e-06

Training epoch-23 batch-64
Running loss of epoch-23 batch-64 = 9.704846888780594e-06

Training epoch-23 batch-65
Running loss of epoch-23 batch-65 = 8.041970431804657e-06

Training epoch-23 batch-66
Running loss of epoch-23 batch-66 = 7.703201845288277e-06

Training epoch-23 batch-67
Running loss of epoch-23 batch-67 = 6.903661414980888e-06

Training epoch-23 batch-68
Running loss of epoch-23 batch-68 = 1.1311378329992294e-05

Training epoch-23 batch-69
Running loss of epoch-23 batch-69 = 6.6570937633514404e-06

Training epoch-23 batch-70
Running loss of epoch-23 batch-70 = 1.2355390936136246e-05

Training epoch-23 batch-71
Running loss of epoch-23 batch-71 = 1.0498333722352982e-05

Training epoch-23 batch-72
Running loss of epoch-23 batch-72 = 1.4670658856630325e-05

Training epoch-23 batch-73
Running loss of epoch-23 batch-73 = 1.2803822755813599e-05

Training epoch-23 batch-74
Running loss of epoch-23 batch-74 = 9.417533874511719e-06

Training epoch-23 batch-75
Running loss of epoch-23 batch-75 = 1.0950956493616104e-05

Training epoch-23 batch-76
Running loss of epoch-23 batch-76 = 9.548850357532501e-06

Training epoch-23 batch-77
Running loss of epoch-23 batch-77 = 1.2401724234223366e-05

Training epoch-23 batch-78
Running loss of epoch-23 batch-78 = 9.028124623000622e-06

Training epoch-23 batch-79
Running loss of epoch-23 batch-79 = 6.56861811876297e-06

Training epoch-23 batch-80
Running loss of epoch-23 batch-80 = 9.519513696432114e-06

Training epoch-23 batch-81
Running loss of epoch-23 batch-81 = 1.0904856026172638e-05

Training epoch-23 batch-82
Running loss of epoch-23 batch-82 = 1.2214761227369308e-05

Training epoch-23 batch-83
Running loss of epoch-23 batch-83 = 8.497387170791626e-06

Training epoch-23 batch-84
Running loss of epoch-23 batch-84 = 5.894573405385017e-06

Training epoch-23 batch-85
Running loss of epoch-23 batch-85 = 1.1462951079010963e-05

Training epoch-23 batch-86
Running loss of epoch-23 batch-86 = 1.0365620255470276e-05

Training epoch-23 batch-87
Running loss of epoch-23 batch-87 = 1.2282747775316238e-05

Training epoch-23 batch-88
Running loss of epoch-23 batch-88 = 1.620408147573471e-05

Training epoch-23 batch-89
Running loss of epoch-23 batch-89 = 8.980510756373405e-06

Training epoch-23 batch-90
Running loss of epoch-23 batch-90 = 1.073651947081089e-05

Training epoch-23 batch-91
Running loss of epoch-23 batch-91 = 1.2053176760673523e-05

Training epoch-23 batch-92
Running loss of epoch-23 batch-92 = 1.1570053175091743e-05

Training epoch-23 batch-93
Running loss of epoch-23 batch-93 = 4.680827260017395e-06

Training epoch-23 batch-94
Running loss of epoch-23 batch-94 = 7.136259227991104e-06

Training epoch-23 batch-95
Running loss of epoch-23 batch-95 = 5.152774974703789e-06

Training epoch-23 batch-96
Running loss of epoch-23 batch-96 = 6.861519068479538e-06

Training epoch-23 batch-97
Running loss of epoch-23 batch-97 = 1.758616417646408e-05

Training epoch-23 batch-98
Running loss of epoch-23 batch-98 = 9.245472028851509e-06

Training epoch-23 batch-99
Running loss of epoch-23 batch-99 = 1.4306046068668365e-05

Training epoch-23 batch-100
Running loss of epoch-23 batch-100 = 1.1290423572063446e-05

Training epoch-23 batch-101
Running loss of epoch-23 batch-101 = 1.0771211236715317e-05

Training epoch-23 batch-102
Running loss of epoch-23 batch-102 = 7.815193384885788e-06

Training epoch-23 batch-103
Running loss of epoch-23 batch-103 = 1.809059176594019e-05

Training epoch-23 batch-104
Running loss of epoch-23 batch-104 = 1.4178687706589699e-05

Training epoch-23 batch-105
Running loss of epoch-23 batch-105 = 9.78854950517416e-06

Training epoch-23 batch-106
Running loss of epoch-23 batch-106 = 9.337440133094788e-06

Training epoch-23 batch-107
Running loss of epoch-23 batch-107 = 8.779345080256462e-06

Training epoch-23 batch-108
Running loss of epoch-23 batch-108 = 1.118052750825882e-05

Training epoch-23 batch-109
Running loss of epoch-23 batch-109 = 1.4339340850710869e-05

Training epoch-23 batch-110
Running loss of epoch-23 batch-110 = 1.0659219697117805e-05

Training epoch-23 batch-111
Running loss of epoch-23 batch-111 = 1.4330726116895676e-05

Training epoch-23 batch-112
Running loss of epoch-23 batch-112 = 7.723458111286163e-06

Training epoch-23 batch-113
Running loss of epoch-23 batch-113 = 6.1604660004377365e-06

Training epoch-23 batch-114
Running loss of epoch-23 batch-114 = 9.817536920309067e-06

Training epoch-23 batch-115
Running loss of epoch-23 batch-115 = 8.988194167613983e-06

Training epoch-23 batch-116
Running loss of epoch-23 batch-116 = 9.564915671944618e-06

Training epoch-23 batch-117
Running loss of epoch-23 batch-117 = 1.2953998520970345e-05

Training epoch-23 batch-118
Running loss of epoch-23 batch-118 = 1.0183081030845642e-05

Training epoch-23 batch-119
Running loss of epoch-23 batch-119 = 9.466428309679031e-06

Training epoch-23 batch-120
Running loss of epoch-23 batch-120 = 7.907627150416374e-06

Training epoch-23 batch-121
Running loss of epoch-23 batch-121 = 1.0945135727524757e-05

Training epoch-23 batch-122
Running loss of epoch-23 batch-122 = 1.2880191206932068e-05

Training epoch-23 batch-123
Running loss of epoch-23 batch-123 = 7.624737918376923e-06

Training epoch-23 batch-124
Running loss of epoch-23 batch-124 = 7.73346982896328e-06

Training epoch-23 batch-125
Running loss of epoch-23 batch-125 = 1.0244781151413918e-05

Training epoch-23 batch-126
Running loss of epoch-23 batch-126 = 1.2032687664031982e-05

Training epoch-23 batch-127
Running loss of epoch-23 batch-127 = 1.2047355994582176e-05

Training epoch-23 batch-128
Running loss of epoch-23 batch-128 = 8.051982149481773e-06

Training epoch-23 batch-129
Running loss of epoch-23 batch-129 = 1.0230345651507378e-05

Training epoch-23 batch-130
Running loss of epoch-23 batch-130 = 8.319038897752762e-06

Training epoch-23 batch-131
Running loss of epoch-23 batch-131 = 5.528563633561134e-06

Training epoch-23 batch-132
Running loss of epoch-23 batch-132 = 9.081093594431877e-06

Training epoch-23 batch-133
Running loss of epoch-23 batch-133 = 1.1422671377658844e-05

Training epoch-23 batch-134
Running loss of epoch-23 batch-134 = 1.0553747415542603e-05

Training epoch-23 batch-135
Running loss of epoch-23 batch-135 = 7.195165380835533e-06

Training epoch-23 batch-136
Running loss of epoch-23 batch-136 = 1.237005926668644e-05

Training epoch-23 batch-137
Running loss of epoch-23 batch-137 = 1.578172668814659e-05

Training epoch-23 batch-138
Running loss of epoch-23 batch-138 = 9.112060070037842e-06

Training epoch-23 batch-139
Running loss of epoch-23 batch-139 = 1.3705343008041382e-05

Training epoch-23 batch-140
Running loss of epoch-23 batch-140 = 9.27201472222805e-06

Training epoch-23 batch-141
Running loss of epoch-23 batch-141 = 1.2968434020876884e-05

Training epoch-23 batch-142
Running loss of epoch-23 batch-142 = 8.465489372611046e-06

Training epoch-23 batch-143
Running loss of epoch-23 batch-143 = 8.074101060628891e-06

Training epoch-23 batch-144
Running loss of epoch-23 batch-144 = 1.1837459169328213e-05

Training epoch-23 batch-145
Running loss of epoch-23 batch-145 = 6.703194230794907e-06

Training epoch-23 batch-146
Running loss of epoch-23 batch-146 = 1.0267598554491997e-05

Training epoch-23 batch-147
Running loss of epoch-23 batch-147 = 6.252434104681015e-06

Training epoch-23 batch-148
Running loss of epoch-23 batch-148 = 6.67991116642952e-06

Training epoch-23 batch-149
Running loss of epoch-23 batch-149 = 1.2774602510035038e-05

Training epoch-23 batch-150
Running loss of epoch-23 batch-150 = 1.2346310541033745e-05

Training epoch-23 batch-151
Running loss of epoch-23 batch-151 = 7.6158903539180756e-06

Training epoch-23 batch-152
Running loss of epoch-23 batch-152 = 1.040380448102951e-05

Training epoch-23 batch-153
Running loss of epoch-23 batch-153 = 6.506685167551041e-06

Training epoch-23 batch-154
Running loss of epoch-23 batch-154 = 7.83940777182579e-06

Training epoch-23 batch-155
Running loss of epoch-23 batch-155 = 9.73651185631752e-06

Training epoch-23 batch-156
Running loss of epoch-23 batch-156 = 6.849179044365883e-06

Training epoch-23 batch-157
Running loss of epoch-23 batch-157 = 1.4912337064743042e-05

Finished training epoch-23.



Average train loss at epoch-23 = 1.0252524167299271e-05

Started Evaluation

Average val loss at epoch-23 = 2.8332496387393853

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 51.88 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 29.15 %
Accuracy for class execute is: 26.10 %
Accuracy for class get is: 36.41 %

Overall Accuracy = 54.40 %

Finished Evaluation



Started training epoch-24


Training epoch-24 batch-1
Running loss of epoch-24 batch-1 = 7.458264008164406e-06

Training epoch-24 batch-2
Running loss of epoch-24 batch-2 = 5.936948582530022e-06

Training epoch-24 batch-3
Running loss of epoch-24 batch-3 = 9.487150236964226e-06

Training epoch-24 batch-4
Running loss of epoch-24 batch-4 = 1.1844327673316002e-05

Training epoch-24 batch-5
Running loss of epoch-24 batch-5 = 8.361414074897766e-06

Training epoch-24 batch-6
Running loss of epoch-24 batch-6 = 8.49878415465355e-06

Training epoch-24 batch-7
Running loss of epoch-24 batch-7 = 1.2469012290239334e-05

Training epoch-24 batch-8
Running loss of epoch-24 batch-8 = 8.119270205497742e-06

Training epoch-24 batch-9
Running loss of epoch-24 batch-9 = 1.4887424185872078e-05

Training epoch-24 batch-10
Running loss of epoch-24 batch-10 = 5.286885425448418e-06

Training epoch-24 batch-11
Running loss of epoch-24 batch-11 = 8.484115824103355e-06

Training epoch-24 batch-12
Running loss of epoch-24 batch-12 = 9.401235729455948e-06

Training epoch-24 batch-13
Running loss of epoch-24 batch-13 = 5.816575139760971e-06

Training epoch-24 batch-14
Running loss of epoch-24 batch-14 = 8.767005056142807e-06

Training epoch-24 batch-15
Running loss of epoch-24 batch-15 = 9.637558832764626e-06

Training epoch-24 batch-16
Running loss of epoch-24 batch-16 = 9.997515007853508e-06

Training epoch-24 batch-17
Running loss of epoch-24 batch-17 = 1.380604226142168e-05

Training epoch-24 batch-18
Running loss of epoch-24 batch-18 = 1.5797559171915054e-05

Training epoch-24 batch-19
Running loss of epoch-24 batch-19 = 6.01448118686676e-06

Training epoch-24 batch-20
Running loss of epoch-24 batch-20 = 1.3326061889529228e-05

Training epoch-24 batch-21
Running loss of epoch-24 batch-21 = 5.7513825595378876e-06

Training epoch-24 batch-22
Running loss of epoch-24 batch-22 = 1.2334669008851051e-05

Training epoch-24 batch-23
Running loss of epoch-24 batch-23 = 8.12159851193428e-06

Training epoch-24 batch-24
Running loss of epoch-24 batch-24 = 5.481299012899399e-06

Training epoch-24 batch-25
Running loss of epoch-24 batch-25 = 1.0505318641662598e-05

Training epoch-24 batch-26
Running loss of epoch-24 batch-26 = 1.06890220195055e-05

Training epoch-24 batch-27
Running loss of epoch-24 batch-27 = 1.2896023690700531e-05

Training epoch-24 batch-28
Running loss of epoch-24 batch-28 = 6.752321496605873e-06

Training epoch-24 batch-29
Running loss of epoch-24 batch-29 = 8.90018418431282e-06

Training epoch-24 batch-30
Running loss of epoch-24 batch-30 = 7.104361429810524e-06

Training epoch-24 batch-31
Running loss of epoch-24 batch-31 = 5.873385816812515e-06

Training epoch-24 batch-32
Running loss of epoch-24 batch-32 = 5.0361268222332e-06

Training epoch-24 batch-33
Running loss of epoch-24 batch-33 = 9.35140997171402e-06

Training epoch-24 batch-34
Running loss of epoch-24 batch-34 = 8.21962021291256e-06

Training epoch-24 batch-35
Running loss of epoch-24 batch-35 = 1.1798692867159843e-05

Training epoch-24 batch-36
Running loss of epoch-24 batch-36 = 1.2229429557919502e-05

Training epoch-24 batch-37
Running loss of epoch-24 batch-37 = 4.445668309926987e-06

Training epoch-24 batch-38
Running loss of epoch-24 batch-38 = 6.905058398842812e-06

Training epoch-24 batch-39
Running loss of epoch-24 batch-39 = 1.3145385310053825e-05

Training epoch-24 batch-40
Running loss of epoch-24 batch-40 = 8.799368515610695e-06

Training epoch-24 batch-41
Running loss of epoch-24 batch-41 = 1.0525574907660484e-05

Training epoch-24 batch-42
Running loss of epoch-24 batch-42 = 1.0749325156211853e-05

Training epoch-24 batch-43
Running loss of epoch-24 batch-43 = 1.0805903002619743e-05

Training epoch-24 batch-44
Running loss of epoch-24 batch-44 = 8.990522474050522e-06

Training epoch-24 batch-45
Running loss of epoch-24 batch-45 = 8.269445970654488e-06

Training epoch-24 batch-46
Running loss of epoch-24 batch-46 = 8.933711796998978e-06

Training epoch-24 batch-47
Running loss of epoch-24 batch-47 = 8.968636393547058e-06

Training epoch-24 batch-48
Running loss of epoch-24 batch-48 = 5.297129973769188e-06

Training epoch-24 batch-49
Running loss of epoch-24 batch-49 = 9.886687621474266e-06

Training epoch-24 batch-50
Running loss of epoch-24 batch-50 = 7.716240361332893e-06

Training epoch-24 batch-51
Running loss of epoch-24 batch-51 = 6.892485544085503e-06

Training epoch-24 batch-52
Running loss of epoch-24 batch-52 = 9.786803275346756e-06

Training epoch-24 batch-53
Running loss of epoch-24 batch-53 = 7.904833182692528e-06

Training epoch-24 batch-54
Running loss of epoch-24 batch-54 = 9.154900908470154e-06

Training epoch-24 batch-55
Running loss of epoch-24 batch-55 = 1.295912079513073e-05

Training epoch-24 batch-56
Running loss of epoch-24 batch-56 = 1.0655028745532036e-05

Training epoch-24 batch-57
Running loss of epoch-24 batch-57 = 1.3575423508882523e-05

Training epoch-24 batch-58
Running loss of epoch-24 batch-58 = 1.1634081602096558e-05

Training epoch-24 batch-59
Running loss of epoch-24 batch-59 = 9.600073099136353e-06

Training epoch-24 batch-60
Running loss of epoch-24 batch-60 = 6.659654900431633e-06

Training epoch-24 batch-61
Running loss of epoch-24 batch-61 = 8.981674909591675e-06

Training epoch-24 batch-62
Running loss of epoch-24 batch-62 = 1.4274148270487785e-05

Training epoch-24 batch-63
Running loss of epoch-24 batch-63 = 1.3757729902863503e-05

Training epoch-24 batch-64
Running loss of epoch-24 batch-64 = 1.2348638847470284e-05

Training epoch-24 batch-65
Running loss of epoch-24 batch-65 = 5.7942233979702e-06

Training epoch-24 batch-66
Running loss of epoch-24 batch-66 = 9.042676538228989e-06

Training epoch-24 batch-67
Running loss of epoch-24 batch-67 = 8.122064173221588e-06

Training epoch-24 batch-68
Running loss of epoch-24 batch-68 = 1.02317426353693e-05

Training epoch-24 batch-69
Running loss of epoch-24 batch-69 = 9.218929335474968e-06

Training epoch-24 batch-70
Running loss of epoch-24 batch-70 = 1.2995908036828041e-05

Training epoch-24 batch-71
Running loss of epoch-24 batch-71 = 6.091548129916191e-06

Training epoch-24 batch-72
Running loss of epoch-24 batch-72 = 9.299488738179207e-06

Training epoch-24 batch-73
Running loss of epoch-24 batch-73 = 9.6901785582304e-06

Training epoch-24 batch-74
Running loss of epoch-24 batch-74 = 9.022420272231102e-06

Training epoch-24 batch-75
Running loss of epoch-24 batch-75 = 9.053153917193413e-06

Training epoch-24 batch-76
Running loss of epoch-24 batch-76 = 7.7076256275177e-06

Training epoch-24 batch-77
Running loss of epoch-24 batch-77 = 7.560942322015762e-06

Training epoch-24 batch-78
Running loss of epoch-24 batch-78 = 8.790288120508194e-06

Training epoch-24 batch-79
Running loss of epoch-24 batch-79 = 1.1386116966605186e-05

Training epoch-24 batch-80
Running loss of epoch-24 batch-80 = 8.362578228116035e-06

Training epoch-24 batch-81
Running loss of epoch-24 batch-81 = 1.6215723007917404e-05

Training epoch-24 batch-82
Running loss of epoch-24 batch-82 = 6.393296644091606e-06

Training epoch-24 batch-83
Running loss of epoch-24 batch-83 = 8.87131318449974e-06

Training epoch-24 batch-84
Running loss of epoch-24 batch-84 = 8.396338671445847e-06

Training epoch-24 batch-85
Running loss of epoch-24 batch-85 = 8.997973054647446e-06

Training epoch-24 batch-86
Running loss of epoch-24 batch-86 = 1.7108977772295475e-05

Training epoch-24 batch-87
Running loss of epoch-24 batch-87 = 9.16793942451477e-06

Training epoch-24 batch-88
Running loss of epoch-24 batch-88 = 6.9802626967430115e-06

Training epoch-24 batch-89
Running loss of epoch-24 batch-89 = 7.690628990530968e-06

Training epoch-24 batch-90
Running loss of epoch-24 batch-90 = 6.263609975576401e-06

Training epoch-24 batch-91
Running loss of epoch-24 batch-91 = 1.3792887330055237e-05

Training epoch-24 batch-92
Running loss of epoch-24 batch-92 = 6.608897820115089e-06

Training epoch-24 batch-93
Running loss of epoch-24 batch-93 = 1.2445496395230293e-05

Training epoch-24 batch-94
Running loss of epoch-24 batch-94 = 9.752810001373291e-06

Training epoch-24 batch-95
Running loss of epoch-24 batch-95 = 9.857118129730225e-06

Training epoch-24 batch-96
Running loss of epoch-24 batch-96 = 9.257113561034203e-06

Training epoch-24 batch-97
Running loss of epoch-24 batch-97 = 1.0022660717368126e-05

Training epoch-24 batch-98
Running loss of epoch-24 batch-98 = 8.399132639169693e-06

Training epoch-24 batch-99
Running loss of epoch-24 batch-99 = 1.0361429303884506e-05

Training epoch-24 batch-100
Running loss of epoch-24 batch-100 = 6.6731590777635574e-06

Training epoch-24 batch-101
Running loss of epoch-24 batch-101 = 5.172332748770714e-06

Training epoch-24 batch-102
Running loss of epoch-24 batch-102 = 9.064329788088799e-06

Training epoch-24 batch-103
Running loss of epoch-24 batch-103 = 8.029630407691002e-06

Training epoch-24 batch-104
Running loss of epoch-24 batch-104 = 1.51023268699646e-05

Training epoch-24 batch-105
Running loss of epoch-24 batch-105 = 1.1265277862548828e-05

Training epoch-24 batch-106
Running loss of epoch-24 batch-106 = 7.509719580411911e-06

Training epoch-24 batch-107
Running loss of epoch-24 batch-107 = 6.8712979555130005e-06

Training epoch-24 batch-108
Running loss of epoch-24 batch-108 = 1.2684380635619164e-05

Training epoch-24 batch-109
Running loss of epoch-24 batch-109 = 1.3028271496295929e-05

Training epoch-24 batch-110
Running loss of epoch-24 batch-110 = 6.309244781732559e-06

Training epoch-24 batch-111
Running loss of epoch-24 batch-111 = 6.707850843667984e-06

Training epoch-24 batch-112
Running loss of epoch-24 batch-112 = 1.6630161553621292e-05

Training epoch-24 batch-113
Running loss of epoch-24 batch-113 = 7.5511634349823e-06

Training epoch-24 batch-114
Running loss of epoch-24 batch-114 = 7.971655577421188e-06

Training epoch-24 batch-115
Running loss of epoch-24 batch-115 = 9.867828339338303e-06

Training epoch-24 batch-116
Running loss of epoch-24 batch-116 = 1.0319054126739502e-05

Training epoch-24 batch-117
Running loss of epoch-24 batch-117 = 1.1093216016888618e-05

Training epoch-24 batch-118
Running loss of epoch-24 batch-118 = 6.920192390680313e-06

Training epoch-24 batch-119
Running loss of epoch-24 batch-119 = 8.49645584821701e-06

Training epoch-24 batch-120
Running loss of epoch-24 batch-120 = 1.2574950233101845e-05

Training epoch-24 batch-121
Running loss of epoch-24 batch-121 = 9.161420166492462e-06

Training epoch-24 batch-122
Running loss of epoch-24 batch-122 = 8.194241672754288e-06

Training epoch-24 batch-123
Running loss of epoch-24 batch-123 = 1.4761579222977161e-05

Training epoch-24 batch-124
Running loss of epoch-24 batch-124 = 1.0511139407753944e-05

Training epoch-24 batch-125
Running loss of epoch-24 batch-125 = 1.2205448001623154e-05

Training epoch-24 batch-126
Running loss of epoch-24 batch-126 = 5.796318873763084e-06

Training epoch-24 batch-127
Running loss of epoch-24 batch-127 = 8.765142410993576e-06

Training epoch-24 batch-128
Running loss of epoch-24 batch-128 = 8.424744009971619e-06

Training epoch-24 batch-129
Running loss of epoch-24 batch-129 = 1.200847327709198e-05

Training epoch-24 batch-130
Running loss of epoch-24 batch-130 = 9.98377799987793e-06

Training epoch-24 batch-131
Running loss of epoch-24 batch-131 = 1.1881580576300621e-05

Training epoch-24 batch-132
Running loss of epoch-24 batch-132 = 8.839182555675507e-06

Training epoch-24 batch-133
Running loss of epoch-24 batch-133 = 7.295282557606697e-06

Training epoch-24 batch-134
Running loss of epoch-24 batch-134 = 1.0938383638858795e-05

Training epoch-24 batch-135
Running loss of epoch-24 batch-135 = 3.8107391446828842e-06

Training epoch-24 batch-136
Running loss of epoch-24 batch-136 = 1.0094605386257172e-05

Training epoch-24 batch-137
Running loss of epoch-24 batch-137 = 1.3526296243071556e-05

Training epoch-24 batch-138
Running loss of epoch-24 batch-138 = 1.1709285899996758e-05

Training epoch-24 batch-139
Running loss of epoch-24 batch-139 = 7.861293852329254e-06

Training epoch-24 batch-140
Running loss of epoch-24 batch-140 = 8.3183404058218e-06

Training epoch-24 batch-141
Running loss of epoch-24 batch-141 = 7.221708074212074e-06

Training epoch-24 batch-142
Running loss of epoch-24 batch-142 = 8.181203156709671e-06

Training epoch-24 batch-143
Running loss of epoch-24 batch-143 = 8.492730557918549e-06

Training epoch-24 batch-144
Running loss of epoch-24 batch-144 = 1.2459466233849525e-05

Training epoch-24 batch-145
Running loss of epoch-24 batch-145 = 6.5229833126068115e-06

Training epoch-24 batch-146
Running loss of epoch-24 batch-146 = 8.559087291359901e-06

Training epoch-24 batch-147
Running loss of epoch-24 batch-147 = 7.210997864603996e-06

Training epoch-24 batch-148
Running loss of epoch-24 batch-148 = 4.0531158447265625e-06

Training epoch-24 batch-149
Running loss of epoch-24 batch-149 = 9.213108569383621e-06

Training epoch-24 batch-150
Running loss of epoch-24 batch-150 = 1.0102754458785057e-05

Training epoch-24 batch-151
Running loss of epoch-24 batch-151 = 8.188653737306595e-06

Training epoch-24 batch-152
Running loss of epoch-24 batch-152 = 8.617527782917023e-06

Training epoch-24 batch-153
Running loss of epoch-24 batch-153 = 7.077353075146675e-06

Training epoch-24 batch-154
Running loss of epoch-24 batch-154 = 9.573530405759811e-06

Training epoch-24 batch-155
Running loss of epoch-24 batch-155 = 1.3268552720546722e-05

Training epoch-24 batch-156
Running loss of epoch-24 batch-156 = 1.2508127838373184e-05

Training epoch-24 batch-157
Running loss of epoch-24 batch-157 = 7.720664143562317e-05

Finished training epoch-24.



Average train loss at epoch-24 = 9.532810002565384e-06

Started Evaluation

Average val loss at epoch-24 = 2.850121163223919

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 30.49 %
Accuracy for class execute is: 26.10 %
Accuracy for class get is: 36.92 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-25


Training epoch-25 batch-1
Running loss of epoch-25 batch-1 = 5.939975380897522e-06

Training epoch-25 batch-2
Running loss of epoch-25 batch-2 = 8.902978152036667e-06

Training epoch-25 batch-3
Running loss of epoch-25 batch-3 = 9.502284228801727e-06

Training epoch-25 batch-4
Running loss of epoch-25 batch-4 = 9.388430044054985e-06

Training epoch-25 batch-5
Running loss of epoch-25 batch-5 = 7.39353708922863e-06

Training epoch-25 batch-6
Running loss of epoch-25 batch-6 = 5.81471249461174e-06

Training epoch-25 batch-7
Running loss of epoch-25 batch-7 = 1.1673662811517715e-05

Training epoch-25 batch-8
Running loss of epoch-25 batch-8 = 8.685747161507607e-06

Training epoch-25 batch-9
Running loss of epoch-25 batch-9 = 9.568408131599426e-06

Training epoch-25 batch-10
Running loss of epoch-25 batch-10 = 5.089445039629936e-06

Training epoch-25 batch-11
Running loss of epoch-25 batch-11 = 1.2789387255907059e-05

Training epoch-25 batch-12
Running loss of epoch-25 batch-12 = 1.086411066353321e-05

Training epoch-25 batch-13
Running loss of epoch-25 batch-13 = 1.2376345694065094e-05

Training epoch-25 batch-14
Running loss of epoch-25 batch-14 = 7.996335625648499e-06

Training epoch-25 batch-15
Running loss of epoch-25 batch-15 = 1.0625692084431648e-05

Training epoch-25 batch-16
Running loss of epoch-25 batch-16 = 1.2777280062437057e-05

Training epoch-25 batch-17
Running loss of epoch-25 batch-17 = 8.267117664217949e-06

Training epoch-25 batch-18
Running loss of epoch-25 batch-18 = 5.941139534115791e-06

Training epoch-25 batch-19
Running loss of epoch-25 batch-19 = 7.898779585957527e-06

Training epoch-25 batch-20
Running loss of epoch-25 batch-20 = 5.464302375912666e-06

Training epoch-25 batch-21
Running loss of epoch-25 batch-21 = 6.985152140259743e-06

Training epoch-25 batch-22
Running loss of epoch-25 batch-22 = 5.00120222568512e-06

Training epoch-25 batch-23
Running loss of epoch-25 batch-23 = 1.2974487617611885e-05

Training epoch-25 batch-24
Running loss of epoch-25 batch-24 = 7.75395892560482e-06

Training epoch-25 batch-25
Running loss of epoch-25 batch-25 = 1.2647360563278198e-05

Training epoch-25 batch-26
Running loss of epoch-25 batch-26 = 9.639188647270203e-06

Training epoch-25 batch-27
Running loss of epoch-25 batch-27 = 9.405892342329025e-06

Training epoch-25 batch-28
Running loss of epoch-25 batch-28 = 1.5532365068793297e-05

Training epoch-25 batch-29
Running loss of epoch-25 batch-29 = 5.2764080464839935e-06

Training epoch-25 batch-30
Running loss of epoch-25 batch-30 = 8.187955245375633e-06

Training epoch-25 batch-31
Running loss of epoch-25 batch-31 = 6.166286766529083e-06

Training epoch-25 batch-32
Running loss of epoch-25 batch-32 = 8.012400940060616e-06

Training epoch-25 batch-33
Running loss of epoch-25 batch-33 = 1.0113930329680443e-05

Training epoch-25 batch-34
Running loss of epoch-25 batch-34 = 6.5355561673641205e-06

Training epoch-25 batch-35
Running loss of epoch-25 batch-35 = 8.731847628951073e-06

Training epoch-25 batch-36
Running loss of epoch-25 batch-36 = 8.662231266498566e-06

Training epoch-25 batch-37
Running loss of epoch-25 batch-37 = 8.336734026670456e-06

Training epoch-25 batch-38
Running loss of epoch-25 batch-38 = 6.1194878071546555e-06

Training epoch-25 batch-39
Running loss of epoch-25 batch-39 = 8.441973477602005e-06

Training epoch-25 batch-40
Running loss of epoch-25 batch-40 = 7.7106524258852e-06

Training epoch-25 batch-41
Running loss of epoch-25 batch-41 = 8.032424375414848e-06

Training epoch-25 batch-42
Running loss of epoch-25 batch-42 = 7.807277143001556e-06

Training epoch-25 batch-43
Running loss of epoch-25 batch-43 = 1.1870404705405235e-05

Training epoch-25 batch-44
Running loss of epoch-25 batch-44 = 1.4051096513867378e-05

Training epoch-25 batch-45
Running loss of epoch-25 batch-45 = 8.505303412675858e-06

Training epoch-25 batch-46
Running loss of epoch-25 batch-46 = 8.28527845442295e-06

Training epoch-25 batch-47
Running loss of epoch-25 batch-47 = 5.511101335287094e-06

Training epoch-25 batch-48
Running loss of epoch-25 batch-48 = 1.194165088236332e-05

Training epoch-25 batch-49
Running loss of epoch-25 batch-49 = 5.8980658650398254e-06

Training epoch-25 batch-50
Running loss of epoch-25 batch-50 = 5.484791472554207e-06

Training epoch-25 batch-51
Running loss of epoch-25 batch-51 = 6.782589480280876e-06

Training epoch-25 batch-52
Running loss of epoch-25 batch-52 = 1.1335127055644989e-05

Training epoch-25 batch-53
Running loss of epoch-25 batch-53 = 6.799586117267609e-06

Training epoch-25 batch-54
Running loss of epoch-25 batch-54 = 6.660353392362595e-06

Training epoch-25 batch-55
Running loss of epoch-25 batch-55 = 5.2978284657001495e-06

Training epoch-25 batch-56
Running loss of epoch-25 batch-56 = 4.8228539526462555e-06

Training epoch-25 batch-57
Running loss of epoch-25 batch-57 = 5.185604095458984e-06

Training epoch-25 batch-58
Running loss of epoch-25 batch-58 = 8.396804332733154e-06

Training epoch-25 batch-59
Running loss of epoch-25 batch-59 = 7.851049304008484e-06

Training epoch-25 batch-60
Running loss of epoch-25 batch-60 = 6.358139216899872e-06

Training epoch-25 batch-61
Running loss of epoch-25 batch-61 = 5.078036338090897e-06

Training epoch-25 batch-62
Running loss of epoch-25 batch-62 = 1.0078540071845055e-05

Training epoch-25 batch-63
Running loss of epoch-25 batch-63 = 8.603325113654137e-06

Training epoch-25 batch-64
Running loss of epoch-25 batch-64 = 8.032657206058502e-06

Training epoch-25 batch-65
Running loss of epoch-25 batch-65 = 7.495051249861717e-06

Training epoch-25 batch-66
Running loss of epoch-25 batch-66 = 8.060131222009659e-06

Training epoch-25 batch-67
Running loss of epoch-25 batch-67 = 1.4255056157708168e-05

Training epoch-25 batch-68
Running loss of epoch-25 batch-68 = 5.731126293540001e-06

Training epoch-25 batch-69
Running loss of epoch-25 batch-69 = 5.751848220825195e-06

Training epoch-25 batch-70
Running loss of epoch-25 batch-70 = 9.11671668291092e-06

Training epoch-25 batch-71
Running loss of epoch-25 batch-71 = 1.1917203664779663e-05

Training epoch-25 batch-72
Running loss of epoch-25 batch-72 = 7.622642442584038e-06

Training epoch-25 batch-73
Running loss of epoch-25 batch-73 = 9.840121492743492e-06

Training epoch-25 batch-74
Running loss of epoch-25 batch-74 = 1.2844568118453026e-05

Training epoch-25 batch-75
Running loss of epoch-25 batch-75 = 1.2005213648080826e-05

Training epoch-25 batch-76
Running loss of epoch-25 batch-76 = 6.919261068105698e-06

Training epoch-25 batch-77
Running loss of epoch-25 batch-77 = 9.336275979876518e-06

Training epoch-25 batch-78
Running loss of epoch-25 batch-78 = 1.0204501450061798e-05

Training epoch-25 batch-79
Running loss of epoch-25 batch-79 = 1.1912314221262932e-05

Training epoch-25 batch-80
Running loss of epoch-25 batch-80 = 1.2308475561439991e-05

Training epoch-25 batch-81
Running loss of epoch-25 batch-81 = 6.966758519411087e-06

Training epoch-25 batch-82
Running loss of epoch-25 batch-82 = 3.627501428127289e-06

Training epoch-25 batch-83
Running loss of epoch-25 batch-83 = 1.0304851457476616e-05

Training epoch-25 batch-84
Running loss of epoch-25 batch-84 = 6.133457645773888e-06

Training epoch-25 batch-85
Running loss of epoch-25 batch-85 = 4.2531173676252365e-06

Training epoch-25 batch-86
Running loss of epoch-25 batch-86 = 1.1424534022808075e-05

Training epoch-25 batch-87
Running loss of epoch-25 batch-87 = 8.241971954703331e-06

Training epoch-25 batch-88
Running loss of epoch-25 batch-88 = 9.599374607205391e-06

Training epoch-25 batch-89
Running loss of epoch-25 batch-89 = 1.0926509276032448e-05

Training epoch-25 batch-90
Running loss of epoch-25 batch-90 = 7.807277143001556e-06

Training epoch-25 batch-91
Running loss of epoch-25 batch-91 = 4.736939445137978e-06

Training epoch-25 batch-92
Running loss of epoch-25 batch-92 = 6.851274520158768e-06

Training epoch-25 batch-93
Running loss of epoch-25 batch-93 = 1.0881805792450905e-05

Training epoch-25 batch-94
Running loss of epoch-25 batch-94 = 9.937910363078117e-06

Training epoch-25 batch-95
Running loss of epoch-25 batch-95 = 1.031649298965931e-05

Training epoch-25 batch-96
Running loss of epoch-25 batch-96 = 5.763722583651543e-06

Training epoch-25 batch-97
Running loss of epoch-25 batch-97 = 5.861977115273476e-06

Training epoch-25 batch-98
Running loss of epoch-25 batch-98 = 1.184176653623581e-05

Training epoch-25 batch-99
Running loss of epoch-25 batch-99 = 6.549293175339699e-06

Training epoch-25 batch-100
Running loss of epoch-25 batch-100 = 1.095072366297245e-05

Training epoch-25 batch-101
Running loss of epoch-25 batch-101 = 1.0338379070162773e-05

Training epoch-25 batch-102
Running loss of epoch-25 batch-102 = 8.87364149093628e-06

Training epoch-25 batch-103
Running loss of epoch-25 batch-103 = 5.139969289302826e-06

Training epoch-25 batch-104
Running loss of epoch-25 batch-104 = 9.716721251606941e-06

Training epoch-25 batch-105
Running loss of epoch-25 batch-105 = 7.698778063058853e-06

Training epoch-25 batch-106
Running loss of epoch-25 batch-106 = 8.267350494861603e-06

Training epoch-25 batch-107
Running loss of epoch-25 batch-107 = 1.030811108648777e-05

Training epoch-25 batch-108
Running loss of epoch-25 batch-108 = 9.393086656928062e-06

Training epoch-25 batch-109
Running loss of epoch-25 batch-109 = 5.207723006606102e-06

Training epoch-25 batch-110
Running loss of epoch-25 batch-110 = 9.624520316720009e-06

Training epoch-25 batch-111
Running loss of epoch-25 batch-111 = 1.0494026355445385e-05

Training epoch-25 batch-112
Running loss of epoch-25 batch-112 = 7.427530363202095e-06

Training epoch-25 batch-113
Running loss of epoch-25 batch-113 = 1.5476602129638195e-05

Training epoch-25 batch-114
Running loss of epoch-25 batch-114 = 7.77142122387886e-06

Training epoch-25 batch-115
Running loss of epoch-25 batch-115 = 8.049188181757927e-06

Training epoch-25 batch-116
Running loss of epoch-25 batch-116 = 7.858267053961754e-06

Training epoch-25 batch-117
Running loss of epoch-25 batch-117 = 6.284331902861595e-06

Training epoch-25 batch-118
Running loss of epoch-25 batch-118 = 8.075730875134468e-06

Training epoch-25 batch-119
Running loss of epoch-25 batch-119 = 1.1227093636989594e-05

Training epoch-25 batch-120
Running loss of epoch-25 batch-120 = 9.896932169795036e-06

Training epoch-25 batch-121
Running loss of epoch-25 batch-121 = 1.038028858602047e-05

Training epoch-25 batch-122
Running loss of epoch-25 batch-122 = 6.859423592686653e-06

Training epoch-25 batch-123
Running loss of epoch-25 batch-123 = 5.842186510562897e-06

Training epoch-25 batch-124
Running loss of epoch-25 batch-124 = 4.498520866036415e-06

Training epoch-25 batch-125
Running loss of epoch-25 batch-125 = 9.33278352022171e-06

Training epoch-25 batch-126
Running loss of epoch-25 batch-126 = 7.415656000375748e-06

Training epoch-25 batch-127
Running loss of epoch-25 batch-127 = 9.79122705757618e-06

Training epoch-25 batch-128
Running loss of epoch-25 batch-128 = 1.136166974902153e-05

Training epoch-25 batch-129
Running loss of epoch-25 batch-129 = 9.704846888780594e-06

Training epoch-25 batch-130
Running loss of epoch-25 batch-130 = 1.3930723071098328e-05

Training epoch-25 batch-131
Running loss of epoch-25 batch-131 = 1.1890428140759468e-05

Training epoch-25 batch-132
Running loss of epoch-25 batch-132 = 7.077818736433983e-06

Training epoch-25 batch-133
Running loss of epoch-25 batch-133 = 1.0551884770393372e-05

Training epoch-25 batch-134
Running loss of epoch-25 batch-134 = 9.707175195217133e-06

Training epoch-25 batch-135
Running loss of epoch-25 batch-135 = 9.02707688510418e-06

Training epoch-25 batch-136
Running loss of epoch-25 batch-136 = 1.2870179489254951e-05

Training epoch-25 batch-137
Running loss of epoch-25 batch-137 = 9.359093382954597e-06

Training epoch-25 batch-138
Running loss of epoch-25 batch-138 = 1.3124663382768631e-05

Training epoch-25 batch-139
Running loss of epoch-25 batch-139 = 1.0827556252479553e-05

Training epoch-25 batch-140
Running loss of epoch-25 batch-140 = 1.0770512744784355e-05

Training epoch-25 batch-141
Running loss of epoch-25 batch-141 = 5.070352926850319e-06

Training epoch-25 batch-142
Running loss of epoch-25 batch-142 = 1.1156313121318817e-05

Training epoch-25 batch-143
Running loss of epoch-25 batch-143 = 8.993549272418022e-06

Training epoch-25 batch-144
Running loss of epoch-25 batch-144 = 6.34416937828064e-06

Training epoch-25 batch-145
Running loss of epoch-25 batch-145 = 1.0757939890027046e-05

Training epoch-25 batch-146
Running loss of epoch-25 batch-146 = 5.198875442147255e-06

Training epoch-25 batch-147
Running loss of epoch-25 batch-147 = 6.716232746839523e-06

Training epoch-25 batch-148
Running loss of epoch-25 batch-148 = 7.223570719361305e-06

Training epoch-25 batch-149
Running loss of epoch-25 batch-149 = 6.002141162753105e-06

Training epoch-25 batch-150
Running loss of epoch-25 batch-150 = 8.186325430870056e-06

Training epoch-25 batch-151
Running loss of epoch-25 batch-151 = 9.386567398905754e-06

Training epoch-25 batch-152
Running loss of epoch-25 batch-152 = 8.939066901803017e-06

Training epoch-25 batch-153
Running loss of epoch-25 batch-153 = 9.569106623530388e-06

Training epoch-25 batch-154
Running loss of epoch-25 batch-154 = 9.439187124371529e-06

Training epoch-25 batch-155
Running loss of epoch-25 batch-155 = 9.42847691476345e-06

Training epoch-25 batch-156
Running loss of epoch-25 batch-156 = 1.0411487892270088e-05

Training epoch-25 batch-157
Running loss of epoch-25 batch-157 = 2.5425106287002563e-05

Finished training epoch-25.



Average train loss at epoch-25 = 8.747489005327224e-06

Started Evaluation

Average val loss at epoch-25 = 2.865078413172772

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.59 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.37 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.55 %

Finished Evaluation



Started training epoch-26


Training epoch-26 batch-1
Running loss of epoch-26 batch-1 = 1.2181000784039497e-05

Training epoch-26 batch-2
Running loss of epoch-26 batch-2 = 6.987713277339935e-06

Training epoch-26 batch-3
Running loss of epoch-26 batch-3 = 8.63242894411087e-06

Training epoch-26 batch-4
Running loss of epoch-26 batch-4 = 1.0125571861863136e-05

Training epoch-26 batch-5
Running loss of epoch-26 batch-5 = 7.82194547355175e-06

Training epoch-26 batch-6
Running loss of epoch-26 batch-6 = 5.516922101378441e-06

Training epoch-26 batch-7
Running loss of epoch-26 batch-7 = 6.788875907659531e-06

Training epoch-26 batch-8
Running loss of epoch-26 batch-8 = 6.015179678797722e-06

Training epoch-26 batch-9
Running loss of epoch-26 batch-9 = 5.766982212662697e-06

Training epoch-26 batch-10
Running loss of epoch-26 batch-10 = 6.852438673377037e-06

Training epoch-26 batch-11
Running loss of epoch-26 batch-11 = 6.7069195210933685e-06

Training epoch-26 batch-12
Running loss of epoch-26 batch-12 = 7.854541763663292e-06

Training epoch-26 batch-13
Running loss of epoch-26 batch-13 = 1.0101590305566788e-05

Training epoch-26 batch-14
Running loss of epoch-26 batch-14 = 7.277820259332657e-06

Training epoch-26 batch-15
Running loss of epoch-26 batch-15 = 7.778871804475784e-06

Training epoch-26 batch-16
Running loss of epoch-26 batch-16 = 8.213566616177559e-06

Training epoch-26 batch-17
Running loss of epoch-26 batch-17 = 9.928597137331963e-06

Training epoch-26 batch-18
Running loss of epoch-26 batch-18 = 9.812647476792336e-06

Training epoch-26 batch-19
Running loss of epoch-26 batch-19 = 6.595859304070473e-06

Training epoch-26 batch-20
Running loss of epoch-26 batch-20 = 9.912298992276192e-06

Training epoch-26 batch-21
Running loss of epoch-26 batch-21 = 8.616363629698753e-06

Training epoch-26 batch-22
Running loss of epoch-26 batch-22 = 1.0024290531873703e-05

Training epoch-26 batch-23
Running loss of epoch-26 batch-23 = 1.1113239452242851e-05

Training epoch-26 batch-24
Running loss of epoch-26 batch-24 = 6.112502887845039e-06

Training epoch-26 batch-25
Running loss of epoch-26 batch-25 = 1.1154217645525932e-05

Training epoch-26 batch-26
Running loss of epoch-26 batch-26 = 8.44104215502739e-06

Training epoch-26 batch-27
Running loss of epoch-26 batch-27 = 6.754882633686066e-06

Training epoch-26 batch-28
Running loss of epoch-26 batch-28 = 6.462680175900459e-06

Training epoch-26 batch-29
Running loss of epoch-26 batch-29 = 7.377937436103821e-06

Training epoch-26 batch-30
Running loss of epoch-26 batch-30 = 8.825212717056274e-06

Training epoch-26 batch-31
Running loss of epoch-26 batch-31 = 4.573259502649307e-06

Training epoch-26 batch-32
Running loss of epoch-26 batch-32 = 4.449859261512756e-06

Training epoch-26 batch-33
Running loss of epoch-26 batch-33 = 7.829396054148674e-06

Training epoch-26 batch-34
Running loss of epoch-26 batch-34 = 5.873851478099823e-06

Training epoch-26 batch-35
Running loss of epoch-26 batch-35 = 6.43264502286911e-06

Training epoch-26 batch-36
Running loss of epoch-26 batch-36 = 5.473382771015167e-06

Training epoch-26 batch-37
Running loss of epoch-26 batch-37 = 7.247086614370346e-06

Training epoch-26 batch-38
Running loss of epoch-26 batch-38 = 1.0101590305566788e-05

Training epoch-26 batch-39
Running loss of epoch-26 batch-39 = 6.135785952210426e-06

Training epoch-26 batch-40
Running loss of epoch-26 batch-40 = 1.092907041311264e-05

Training epoch-26 batch-41
Running loss of epoch-26 batch-41 = 8.477596566081047e-06

Training epoch-26 batch-42
Running loss of epoch-26 batch-42 = 9.425915777683258e-06

Training epoch-26 batch-43
Running loss of epoch-26 batch-43 = 6.3909683376550674e-06

Training epoch-26 batch-44
Running loss of epoch-26 batch-44 = 1.0495772585272789e-05

Training epoch-26 batch-45
Running loss of epoch-26 batch-45 = 7.347436621785164e-06

Training epoch-26 batch-46
Running loss of epoch-26 batch-46 = 8.395873010158539e-06

Training epoch-26 batch-47
Running loss of epoch-26 batch-47 = 1.1251308023929596e-05

Training epoch-26 batch-48
Running loss of epoch-26 batch-48 = 8.514849469065666e-06

Training epoch-26 batch-49
Running loss of epoch-26 batch-49 = 8.231028914451599e-06

Training epoch-26 batch-50
Running loss of epoch-26 batch-50 = 5.387701094150543e-06

Training epoch-26 batch-51
Running loss of epoch-26 batch-51 = 9.138835594058037e-06

Training epoch-26 batch-52
Running loss of epoch-26 batch-52 = 9.944196790456772e-06

Training epoch-26 batch-53
Running loss of epoch-26 batch-53 = 9.854324162006378e-06

Training epoch-26 batch-54
Running loss of epoch-26 batch-54 = 7.71298073232174e-06

Training epoch-26 batch-55
Running loss of epoch-26 batch-55 = 9.417301043868065e-06

Training epoch-26 batch-56
Running loss of epoch-26 batch-56 = 1.0930467396974564e-05

Training epoch-26 batch-57
Running loss of epoch-26 batch-57 = 9.949086233973503e-06

Training epoch-26 batch-58
Running loss of epoch-26 batch-58 = 7.417518645524979e-06

Training epoch-26 batch-59
Running loss of epoch-26 batch-59 = 1.0233605280518532e-05

Training epoch-26 batch-60
Running loss of epoch-26 batch-60 = 5.564652383327484e-06

Training epoch-26 batch-61
Running loss of epoch-26 batch-61 = 7.967231795191765e-06

Training epoch-26 batch-62
Running loss of epoch-26 batch-62 = 5.315057933330536e-06

Training epoch-26 batch-63
Running loss of epoch-26 batch-63 = 8.026603609323502e-06

Training epoch-26 batch-64
Running loss of epoch-26 batch-64 = 6.169779226183891e-06

Training epoch-26 batch-65
Running loss of epoch-26 batch-65 = 7.144175469875336e-06

Training epoch-26 batch-66
Running loss of epoch-26 batch-66 = 8.147209882736206e-06

Training epoch-26 batch-67
Running loss of epoch-26 batch-67 = 8.04755836725235e-06

Training epoch-26 batch-68
Running loss of epoch-26 batch-68 = 6.8552326411008835e-06

Training epoch-26 batch-69
Running loss of epoch-26 batch-69 = 1.0683434084057808e-05

Training epoch-26 batch-70
Running loss of epoch-26 batch-70 = 1.0271091014146805e-05

Training epoch-26 batch-71
Running loss of epoch-26 batch-71 = 8.708098903298378e-06

Training epoch-26 batch-72
Running loss of epoch-26 batch-72 = 9.033596143126488e-06

Training epoch-26 batch-73
Running loss of epoch-26 batch-73 = 1.2298696674406528e-05

Training epoch-26 batch-74
Running loss of epoch-26 batch-74 = 9.262701496481895e-06

Training epoch-26 batch-75
Running loss of epoch-26 batch-75 = 1.027318648993969e-05

Training epoch-26 batch-76
Running loss of epoch-26 batch-76 = 5.78584149479866e-06

Training epoch-26 batch-77
Running loss of epoch-26 batch-77 = 8.74372199177742e-06

Training epoch-26 batch-78
Running loss of epoch-26 batch-78 = 1.1175638064742088e-05

Training epoch-26 batch-79
Running loss of epoch-26 batch-79 = 7.231021299958229e-06

Training epoch-26 batch-80
Running loss of epoch-26 batch-80 = 9.333249181509018e-06

Training epoch-26 batch-81
Running loss of epoch-26 batch-81 = 9.11252573132515e-06

Training epoch-26 batch-82
Running loss of epoch-26 batch-82 = 1.2123724445700645e-05

Training epoch-26 batch-83
Running loss of epoch-26 batch-83 = 6.061745807528496e-06

Training epoch-26 batch-84
Running loss of epoch-26 batch-84 = 7.81472772359848e-06

Training epoch-26 batch-85
Running loss of epoch-26 batch-85 = 9.592622518539429e-06

Training epoch-26 batch-86
Running loss of epoch-26 batch-86 = 6.642425432801247e-06

Training epoch-26 batch-87
Running loss of epoch-26 batch-87 = 7.755355909466743e-06

Training epoch-26 batch-88
Running loss of epoch-26 batch-88 = 5.627050995826721e-06

Training epoch-26 batch-89
Running loss of epoch-26 batch-89 = 6.788875907659531e-06

Training epoch-26 batch-90
Running loss of epoch-26 batch-90 = 7.44406133890152e-06

Training epoch-26 batch-91
Running loss of epoch-26 batch-91 = 5.526235327124596e-06

Training epoch-26 batch-92
Running loss of epoch-26 batch-92 = 6.279675289988518e-06

Training epoch-26 batch-93
Running loss of epoch-26 batch-93 = 9.203795343637466e-06

Training epoch-26 batch-94
Running loss of epoch-26 batch-94 = 6.170710548758507e-06

Training epoch-26 batch-95
Running loss of epoch-26 batch-95 = 9.297393262386322e-06

Training epoch-26 batch-96
Running loss of epoch-26 batch-96 = 5.837064236402512e-06

Training epoch-26 batch-97
Running loss of epoch-26 batch-97 = 4.7495122998952866e-06

Training epoch-26 batch-98
Running loss of epoch-26 batch-98 = 1.376960426568985e-05

Training epoch-26 batch-99
Running loss of epoch-26 batch-99 = 5.534617230296135e-06

Training epoch-26 batch-100
Running loss of epoch-26 batch-100 = 9.767943993210793e-06

Training epoch-26 batch-101
Running loss of epoch-26 batch-101 = 8.06502066552639e-06

Training epoch-26 batch-102
Running loss of epoch-26 batch-102 = 8.395873010158539e-06

Training epoch-26 batch-103
Running loss of epoch-26 batch-103 = 6.568850949406624e-06

Training epoch-26 batch-104
Running loss of epoch-26 batch-104 = 5.0854869186878204e-06

Training epoch-26 batch-105
Running loss of epoch-26 batch-105 = 6.017042323946953e-06

Training epoch-26 batch-106
Running loss of epoch-26 batch-106 = 1.2230593711137772e-05

Training epoch-26 batch-107
Running loss of epoch-26 batch-107 = 7.806345820426941e-06

Training epoch-26 batch-108
Running loss of epoch-26 batch-108 = 6.723217666149139e-06

Training epoch-26 batch-109
Running loss of epoch-26 batch-109 = 6.566988304257393e-06

Training epoch-26 batch-110
Running loss of epoch-26 batch-110 = 9.196111932396889e-06

Training epoch-26 batch-111
Running loss of epoch-26 batch-111 = 6.041023880243301e-06

Training epoch-26 batch-112
Running loss of epoch-26 batch-112 = 9.507406502962112e-06

Training epoch-26 batch-113
Running loss of epoch-26 batch-113 = 7.892027497291565e-06

Training epoch-26 batch-114
Running loss of epoch-26 batch-114 = 8.651753887534142e-06

Training epoch-26 batch-115
Running loss of epoch-26 batch-115 = 7.1302056312561035e-06

Training epoch-26 batch-116
Running loss of epoch-26 batch-116 = 1.0637566447257996e-05

Training epoch-26 batch-117
Running loss of epoch-26 batch-117 = 7.1909744292497635e-06

Training epoch-26 batch-118
Running loss of epoch-26 batch-118 = 9.780516847968102e-06

Training epoch-26 batch-119
Running loss of epoch-26 batch-119 = 8.172355592250824e-06

Training epoch-26 batch-120
Running loss of epoch-26 batch-120 = 7.985159754753113e-06

Training epoch-26 batch-121
Running loss of epoch-26 batch-121 = 5.1960814744234085e-06

Training epoch-26 batch-122
Running loss of epoch-26 batch-122 = 5.421927198767662e-06

Training epoch-26 batch-123
Running loss of epoch-26 batch-123 = 9.441282600164413e-06

Training epoch-26 batch-124
Running loss of epoch-26 batch-124 = 4.296889528632164e-06

Training epoch-26 batch-125
Running loss of epoch-26 batch-125 = 8.819391950964928e-06

Training epoch-26 batch-126
Running loss of epoch-26 batch-126 = 7.395865395665169e-06

Training epoch-26 batch-127
Running loss of epoch-26 batch-127 = 1.0397285223007202e-05

Training epoch-26 batch-128
Running loss of epoch-26 batch-128 = 6.824033334851265e-06

Training epoch-26 batch-129
Running loss of epoch-26 batch-129 = 7.96094536781311e-06

Training epoch-26 batch-130
Running loss of epoch-26 batch-130 = 6.0193706303834915e-06

Training epoch-26 batch-131
Running loss of epoch-26 batch-131 = 7.676426321268082e-06

Training epoch-26 batch-132
Running loss of epoch-26 batch-132 = 1.287856139242649e-05

Training epoch-26 batch-133
Running loss of epoch-26 batch-133 = 5.299225449562073e-06

Training epoch-26 batch-134
Running loss of epoch-26 batch-134 = 6.05056993663311e-06

Training epoch-26 batch-135
Running loss of epoch-26 batch-135 = 1.0087387636303902e-05

Training epoch-26 batch-136
Running loss of epoch-26 batch-136 = 9.104609489440918e-06

Training epoch-26 batch-137
Running loss of epoch-26 batch-137 = 6.127636879682541e-06

Training epoch-26 batch-138
Running loss of epoch-26 batch-138 = 1.170882023870945e-05

Training epoch-26 batch-139
Running loss of epoch-26 batch-139 = 1.0489951819181442e-05

Training epoch-26 batch-140
Running loss of epoch-26 batch-140 = 9.62824560701847e-06

Training epoch-26 batch-141
Running loss of epoch-26 batch-141 = 8.840812370181084e-06

Training epoch-26 batch-142
Running loss of epoch-26 batch-142 = 4.955800250172615e-06

Training epoch-26 batch-143
Running loss of epoch-26 batch-143 = 6.574438884854317e-06

Training epoch-26 batch-144
Running loss of epoch-26 batch-144 = 7.64802098274231e-06

Training epoch-26 batch-145
Running loss of epoch-26 batch-145 = 1.0407064110040665e-05

Training epoch-26 batch-146
Running loss of epoch-26 batch-146 = 7.130671292543411e-06

Training epoch-26 batch-147
Running loss of epoch-26 batch-147 = 9.004492312669754e-06

Training epoch-26 batch-148
Running loss of epoch-26 batch-148 = 1.056678593158722e-05

Training epoch-26 batch-149
Running loss of epoch-26 batch-149 = 7.317168638110161e-06

Training epoch-26 batch-150
Running loss of epoch-26 batch-150 = 5.5872369557619095e-06

Training epoch-26 batch-151
Running loss of epoch-26 batch-151 = 5.389563739299774e-06

Training epoch-26 batch-152
Running loss of epoch-26 batch-152 = 1.0969815775752068e-05

Training epoch-26 batch-153
Running loss of epoch-26 batch-153 = 9.457115083932877e-06

Training epoch-26 batch-154
Running loss of epoch-26 batch-154 = 9.916489943861961e-06

Training epoch-26 batch-155
Running loss of epoch-26 batch-155 = 6.232410669326782e-06

Training epoch-26 batch-156
Running loss of epoch-26 batch-156 = 9.371666237711906e-06

Training epoch-26 batch-157
Running loss of epoch-26 batch-157 = 1.3478100299835205e-05

Finished training epoch-26.



Average train loss at epoch-26 = 8.113039284944535e-06

Started Evaluation

Average val loss at epoch-26 = 2.8791584403891313

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 63.61 %
Accuracy for class setUp is: 63.11 %
Accuracy for class onCreate is: 57.04 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 31.17 %
Accuracy for class execute is: 25.70 %
Accuracy for class get is: 36.41 %

Overall Accuracy = 54.55 %

Finished Evaluation



Started training epoch-27


Training epoch-27 batch-1
Running loss of epoch-27 batch-1 = 8.239643648266792e-06

Training epoch-27 batch-2
Running loss of epoch-27 batch-2 = 4.6496279537677765e-06

Training epoch-27 batch-3
Running loss of epoch-27 batch-3 = 9.797979146242142e-06

Training epoch-27 batch-4
Running loss of epoch-27 batch-4 = 7.362570613622665e-06

Training epoch-27 batch-5
Running loss of epoch-27 batch-5 = 7.423572242259979e-06

Training epoch-27 batch-6
Running loss of epoch-27 batch-6 = 6.301794201135635e-06

Training epoch-27 batch-7
Running loss of epoch-27 batch-7 = 7.100868970155716e-06

Training epoch-27 batch-8
Running loss of epoch-27 batch-8 = 6.430316716432571e-06

Training epoch-27 batch-9
Running loss of epoch-27 batch-9 = 7.766066119074821e-06

Training epoch-27 batch-10
Running loss of epoch-27 batch-10 = 8.899718523025513e-06

Training epoch-27 batch-11
Running loss of epoch-27 batch-11 = 7.20820389688015e-06

Training epoch-27 batch-12
Running loss of epoch-27 batch-12 = 6.0480087995529175e-06

Training epoch-27 batch-13
Running loss of epoch-27 batch-13 = 8.58306884765625e-06

Training epoch-27 batch-14
Running loss of epoch-27 batch-14 = 5.573732778429985e-06

Training epoch-27 batch-15
Running loss of epoch-27 batch-15 = 6.484799087047577e-06

Training epoch-27 batch-16
Running loss of epoch-27 batch-16 = 4.9390364438295364e-06

Training epoch-27 batch-17
Running loss of epoch-27 batch-17 = 6.9122761487960815e-06

Training epoch-27 batch-18
Running loss of epoch-27 batch-18 = 9.899260476231575e-06

Training epoch-27 batch-19
Running loss of epoch-27 batch-19 = 5.659647285938263e-06

Training epoch-27 batch-20
Running loss of epoch-27 batch-20 = 9.256182238459587e-06

Training epoch-27 batch-21
Running loss of epoch-27 batch-21 = 8.789589628577232e-06

Training epoch-27 batch-22
Running loss of epoch-27 batch-22 = 6.182119250297546e-06

Training epoch-27 batch-23
Running loss of epoch-27 batch-23 = 7.5015705078840256e-06

Training epoch-27 batch-24
Running loss of epoch-27 batch-24 = 6.658490747213364e-06

Training epoch-27 batch-25
Running loss of epoch-27 batch-25 = 1.2159813195466995e-05

Training epoch-27 batch-26
Running loss of epoch-27 batch-26 = 5.403999239206314e-06

Training epoch-27 batch-27
Running loss of epoch-27 batch-27 = 6.6373031586408615e-06

Training epoch-27 batch-28
Running loss of epoch-27 batch-28 = 7.625902071595192e-06

Training epoch-27 batch-29
Running loss of epoch-27 batch-29 = 8.102040737867355e-06

Training epoch-27 batch-30
Running loss of epoch-27 batch-30 = 6.41215592622757e-06

Training epoch-27 batch-31
Running loss of epoch-27 batch-31 = 8.120434358716011e-06

Training epoch-27 batch-32
Running loss of epoch-27 batch-32 = 1.1934665963053703e-05

Training epoch-27 batch-33
Running loss of epoch-27 batch-33 = 9.710201993584633e-06

Training epoch-27 batch-34
Running loss of epoch-27 batch-34 = 5.718320608139038e-06

Training epoch-27 batch-35
Running loss of epoch-27 batch-35 = 5.65405935049057e-06

Training epoch-27 batch-36
Running loss of epoch-27 batch-36 = 6.956281140446663e-06

Training epoch-27 batch-37
Running loss of epoch-27 batch-37 = 9.566545486450195e-06

Training epoch-27 batch-38
Running loss of epoch-27 batch-38 = 5.7551078498363495e-06

Training epoch-27 batch-39
Running loss of epoch-27 batch-39 = 1.074071042239666e-05

Training epoch-27 batch-40
Running loss of epoch-27 batch-40 = 1.3242708519101143e-05

Training epoch-27 batch-41
Running loss of epoch-27 batch-41 = 7.092254236340523e-06

Training epoch-27 batch-42
Running loss of epoch-27 batch-42 = 1.0336050763726234e-05

Training epoch-27 batch-43
Running loss of epoch-27 batch-43 = 5.277106538414955e-06

Training epoch-27 batch-44
Running loss of epoch-27 batch-44 = 3.4854747354984283e-06

Training epoch-27 batch-45
Running loss of epoch-27 batch-45 = 6.466405466198921e-06

Training epoch-27 batch-46
Running loss of epoch-27 batch-46 = 5.830312147736549e-06

Training epoch-27 batch-47
Running loss of epoch-27 batch-47 = 6.057554855942726e-06

Training epoch-27 batch-48
Running loss of epoch-27 batch-48 = 4.401197656989098e-06

Training epoch-27 batch-49
Running loss of epoch-27 batch-49 = 5.544628947973251e-06

Training epoch-27 batch-50
Running loss of epoch-27 batch-50 = 6.861286237835884e-06

Training epoch-27 batch-51
Running loss of epoch-27 batch-51 = 1.200882252305746e-05

Training epoch-27 batch-52
Running loss of epoch-27 batch-52 = 7.1302056312561035e-06

Training epoch-27 batch-53
Running loss of epoch-27 batch-53 = 7.487600669264793e-06

Training epoch-27 batch-54
Running loss of epoch-27 batch-54 = 1.041102223098278e-05

Training epoch-27 batch-55
Running loss of epoch-27 batch-55 = 6.716465577483177e-06

Training epoch-27 batch-56
Running loss of epoch-27 batch-56 = 7.3977280408144e-06

Training epoch-27 batch-57
Running loss of epoch-27 batch-57 = 7.161172106862068e-06

Training epoch-27 batch-58
Running loss of epoch-27 batch-58 = 9.108101949095726e-06

Training epoch-27 batch-59
Running loss of epoch-27 batch-59 = 8.055474609136581e-06

Training epoch-27 batch-60
Running loss of epoch-27 batch-60 = 7.811235263943672e-06

Training epoch-27 batch-61
Running loss of epoch-27 batch-61 = 7.794005796313286e-06

Training epoch-27 batch-62
Running loss of epoch-27 batch-62 = 7.940689101815224e-06

Training epoch-27 batch-63
Running loss of epoch-27 batch-63 = 8.334405720233917e-06

Training epoch-27 batch-64
Running loss of epoch-27 batch-64 = 5.88269904255867e-06

Training epoch-27 batch-65
Running loss of epoch-27 batch-65 = 1.051626168191433e-05

Training epoch-27 batch-66
Running loss of epoch-27 batch-66 = 6.616348400712013e-06

Training epoch-27 batch-67
Running loss of epoch-27 batch-67 = 6.227055564522743e-06

Training epoch-27 batch-68
Running loss of epoch-27 batch-68 = 1.0288786143064499e-05

Training epoch-27 batch-69
Running loss of epoch-27 batch-69 = 9.677372872829437e-06

Training epoch-27 batch-70
Running loss of epoch-27 batch-70 = 1.0628718882799149e-05

Training epoch-27 batch-71
Running loss of epoch-27 batch-71 = 1.1470634490251541e-05

Training epoch-27 batch-72
Running loss of epoch-27 batch-72 = 1.0248273611068726e-05

Training epoch-27 batch-73
Running loss of epoch-27 batch-73 = 5.538575351238251e-06

Training epoch-27 batch-74
Running loss of epoch-27 batch-74 = 1.225760206580162e-05

Training epoch-27 batch-75
Running loss of epoch-27 batch-75 = 6.305752322077751e-06

Training epoch-27 batch-76
Running loss of epoch-27 batch-76 = 7.77444802224636e-06

Training epoch-27 batch-77
Running loss of epoch-27 batch-77 = 6.661983206868172e-06

Training epoch-27 batch-78
Running loss of epoch-27 batch-78 = 6.75395131111145e-06

Training epoch-27 batch-79
Running loss of epoch-27 batch-79 = 9.386101737618446e-06

Training epoch-27 batch-80
Running loss of epoch-27 batch-80 = 7.721362635493279e-06

Training epoch-27 batch-81
Running loss of epoch-27 batch-81 = 1.1163996532559395e-05

Training epoch-27 batch-82
Running loss of epoch-27 batch-82 = 9.401468560099602e-06

Training epoch-27 batch-83
Running loss of epoch-27 batch-83 = 5.46150840818882e-06

Training epoch-27 batch-84
Running loss of epoch-27 batch-84 = 6.148358806967735e-06

Training epoch-27 batch-85
Running loss of epoch-27 batch-85 = 5.716457962989807e-06

Training epoch-27 batch-86
Running loss of epoch-27 batch-86 = 5.683861672878265e-06

Training epoch-27 batch-87
Running loss of epoch-27 batch-87 = 6.7944638431072235e-06

Training epoch-27 batch-88
Running loss of epoch-27 batch-88 = 1.177145168185234e-05

Training epoch-27 batch-89
Running loss of epoch-27 batch-89 = 6.634276360273361e-06

Training epoch-27 batch-90
Running loss of epoch-27 batch-90 = 1.435563899576664e-05

Training epoch-27 batch-91
Running loss of epoch-27 batch-91 = 4.911096766591072e-06

Training epoch-27 batch-92
Running loss of epoch-27 batch-92 = 7.8610610216856e-06

Training epoch-27 batch-93
Running loss of epoch-27 batch-93 = 9.88575629889965e-06

Training epoch-27 batch-94
Running loss of epoch-27 batch-94 = 8.309958502650261e-06

Training epoch-27 batch-95
Running loss of epoch-27 batch-95 = 6.208894774317741e-06

Training epoch-27 batch-96
Running loss of epoch-27 batch-96 = 8.787494152784348e-06

Training epoch-27 batch-97
Running loss of epoch-27 batch-97 = 7.607974112033844e-06

Training epoch-27 batch-98
Running loss of epoch-27 batch-98 = 6.666406989097595e-06

Training epoch-27 batch-99
Running loss of epoch-27 batch-99 = 7.793772965669632e-06

Training epoch-27 batch-100
Running loss of epoch-27 batch-100 = 8.75932164490223e-06

Training epoch-27 batch-101
Running loss of epoch-27 batch-101 = 7.377704605460167e-06

Training epoch-27 batch-102
Running loss of epoch-27 batch-102 = 7.50063918530941e-06

Training epoch-27 batch-103
Running loss of epoch-27 batch-103 = 7.077585905790329e-06

Training epoch-27 batch-104
Running loss of epoch-27 batch-104 = 9.56258736550808e-06

Training epoch-27 batch-105
Running loss of epoch-27 batch-105 = 6.789574399590492e-06

Training epoch-27 batch-106
Running loss of epoch-27 batch-106 = 7.988419383764267e-06

Training epoch-27 batch-107
Running loss of epoch-27 batch-107 = 1.0113464668393135e-05

Training epoch-27 batch-108
Running loss of epoch-27 batch-108 = 5.5746641010046005e-06

Training epoch-27 batch-109
Running loss of epoch-27 batch-109 = 6.0622114688158035e-06

Training epoch-27 batch-110
Running loss of epoch-27 batch-110 = 5.843816325068474e-06

Training epoch-27 batch-111
Running loss of epoch-27 batch-111 = 9.452691301703453e-06

Training epoch-27 batch-112
Running loss of epoch-27 batch-112 = 6.720423698425293e-06

Training epoch-27 batch-113
Running loss of epoch-27 batch-113 = 8.933478966355324e-06

Training epoch-27 batch-114
Running loss of epoch-27 batch-114 = 6.171874701976776e-06

Training epoch-27 batch-115
Running loss of epoch-27 batch-115 = 6.086425855755806e-06

Training epoch-27 batch-116
Running loss of epoch-27 batch-116 = 5.689216777682304e-06

Training epoch-27 batch-117
Running loss of epoch-27 batch-117 = 8.77981074154377e-06

Training epoch-27 batch-118
Running loss of epoch-27 batch-118 = 9.235693141818047e-06

Training epoch-27 batch-119
Running loss of epoch-27 batch-119 = 5.1835086196660995e-06

Training epoch-27 batch-120
Running loss of epoch-27 batch-120 = 6.814952939748764e-06

Training epoch-27 batch-121
Running loss of epoch-27 batch-121 = 7.922528311610222e-06

Training epoch-27 batch-122
Running loss of epoch-27 batch-122 = 9.577255696058273e-06

Training epoch-27 batch-123
Running loss of epoch-27 batch-123 = 5.2943360060453415e-06

Training epoch-27 batch-124
Running loss of epoch-27 batch-124 = 7.912283763289452e-06

Training epoch-27 batch-125
Running loss of epoch-27 batch-125 = 7.869908586144447e-06

Training epoch-27 batch-126
Running loss of epoch-27 batch-126 = 7.4945855885744095e-06

Training epoch-27 batch-127
Running loss of epoch-27 batch-127 = 5.5585987865924835e-06

Training epoch-27 batch-128
Running loss of epoch-27 batch-128 = 7.238239049911499e-06

Training epoch-27 batch-129
Running loss of epoch-27 batch-129 = 4.516681656241417e-06

Training epoch-27 batch-130
Running loss of epoch-27 batch-130 = 5.225418135523796e-06

Training epoch-27 batch-131
Running loss of epoch-27 batch-131 = 5.895504727959633e-06

Training epoch-27 batch-132
Running loss of epoch-27 batch-132 = 8.535105735063553e-06

Training epoch-27 batch-133
Running loss of epoch-27 batch-133 = 8.32858495414257e-06

Training epoch-27 batch-134
Running loss of epoch-27 batch-134 = 6.242422387003899e-06

Training epoch-27 batch-135
Running loss of epoch-27 batch-135 = 2.99699604511261e-06

Training epoch-27 batch-136
Running loss of epoch-27 batch-136 = 9.58994496613741e-06

Training epoch-27 batch-137
Running loss of epoch-27 batch-137 = 7.340451702475548e-06

Training epoch-27 batch-138
Running loss of epoch-27 batch-138 = 6.509944796562195e-06

Training epoch-27 batch-139
Running loss of epoch-27 batch-139 = 6.089918315410614e-06

Training epoch-27 batch-140
Running loss of epoch-27 batch-140 = 4.321569576859474e-06

Training epoch-27 batch-141
Running loss of epoch-27 batch-141 = 7.69621692597866e-06

Training epoch-27 batch-142
Running loss of epoch-27 batch-142 = 6.454763934016228e-06

Training epoch-27 batch-143
Running loss of epoch-27 batch-143 = 1.0778196156024933e-05

Training epoch-27 batch-144
Running loss of epoch-27 batch-144 = 9.57888551056385e-06

Training epoch-27 batch-145
Running loss of epoch-27 batch-145 = 5.353940650820732e-06

Training epoch-27 batch-146
Running loss of epoch-27 batch-146 = 7.987720891833305e-06

Training epoch-27 batch-147
Running loss of epoch-27 batch-147 = 6.372574716806412e-06

Training epoch-27 batch-148
Running loss of epoch-27 batch-148 = 8.239177986979485e-06

Training epoch-27 batch-149
Running loss of epoch-27 batch-149 = 9.447569027543068e-06

Training epoch-27 batch-150
Running loss of epoch-27 batch-150 = 5.14020211994648e-06

Training epoch-27 batch-151
Running loss of epoch-27 batch-151 = 1.0987045243382454e-05

Training epoch-27 batch-152
Running loss of epoch-27 batch-152 = 7.3069240897893906e-06

Training epoch-27 batch-153
Running loss of epoch-27 batch-153 = 6.886199116706848e-06

Training epoch-27 batch-154
Running loss of epoch-27 batch-154 = 5.388166755437851e-06

Training epoch-27 batch-155
Running loss of epoch-27 batch-155 = 5.912734195590019e-06

Training epoch-27 batch-156
Running loss of epoch-27 batch-156 = 7.792143151164055e-06

Training epoch-27 batch-157
Running loss of epoch-27 batch-157 = 1.2218952178955078e-05

Finished training epoch-27.



Average train loss at epoch-27 = 7.589074969291687e-06

Started Evaluation

Average val loss at epoch-27 = 2.8950675537711694

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.92 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.38 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-28


Training epoch-28 batch-1
Running loss of epoch-28 batch-1 = 7.28224404156208e-06

Training epoch-28 batch-2
Running loss of epoch-28 batch-2 = 4.309229552745819e-06

Training epoch-28 batch-3
Running loss of epoch-28 batch-3 = 1.0584015399217606e-05

Training epoch-28 batch-4
Running loss of epoch-28 batch-4 = 8.020782843232155e-06

Training epoch-28 batch-5
Running loss of epoch-28 batch-5 = 8.213799446821213e-06

Training epoch-28 batch-6
Running loss of epoch-28 batch-6 = 9.12998802959919e-06

Training epoch-28 batch-7
Running loss of epoch-28 batch-7 = 8.011003956198692e-06

Training epoch-28 batch-8
Running loss of epoch-28 batch-8 = 6.380490958690643e-06

Training epoch-28 batch-9
Running loss of epoch-28 batch-9 = 6.440561264753342e-06

Training epoch-28 batch-10
Running loss of epoch-28 batch-10 = 1.11372210085392e-05

Training epoch-28 batch-11
Running loss of epoch-28 batch-11 = 7.524155080318451e-06

Training epoch-28 batch-12
Running loss of epoch-28 batch-12 = 4.757894203066826e-06

Training epoch-28 batch-13
Running loss of epoch-28 batch-13 = 7.348833605647087e-06

Training epoch-28 batch-14
Running loss of epoch-28 batch-14 = 6.721122190356255e-06

Training epoch-28 batch-15
Running loss of epoch-28 batch-15 = 5.832407623529434e-06

Training epoch-28 batch-16
Running loss of epoch-28 batch-16 = 7.3784030973911285e-06

Training epoch-28 batch-17
Running loss of epoch-28 batch-17 = 8.534174412488937e-06

Training epoch-28 batch-18
Running loss of epoch-28 batch-18 = 7.041264325380325e-06

Training epoch-28 batch-19
Running loss of epoch-28 batch-19 = 8.713454008102417e-06

Training epoch-28 batch-20
Running loss of epoch-28 batch-20 = 8.814968168735504e-06

Training epoch-28 batch-21
Running loss of epoch-28 batch-21 = 1.0674586519598961e-05

Training epoch-28 batch-22
Running loss of epoch-28 batch-22 = 6.158370524644852e-06

Training epoch-28 batch-23
Running loss of epoch-28 batch-23 = 4.0065497159957886e-06

Training epoch-28 batch-24
Running loss of epoch-28 batch-24 = 1.0169344022870064e-05

Training epoch-28 batch-25
Running loss of epoch-28 batch-25 = 4.421919584274292e-06

Training epoch-28 batch-26
Running loss of epoch-28 batch-26 = 4.254747182130814e-06

Training epoch-28 batch-27
Running loss of epoch-28 batch-27 = 9.404495358467102e-06

Training epoch-28 batch-28
Running loss of epoch-28 batch-28 = 3.3373944461345673e-06

Training epoch-28 batch-29
Running loss of epoch-28 batch-29 = 7.283641025424004e-06

Training epoch-28 batch-30
Running loss of epoch-28 batch-30 = 8.57282429933548e-06

Training epoch-28 batch-31
Running loss of epoch-28 batch-31 = 5.30807301402092e-06

Training epoch-28 batch-32
Running loss of epoch-28 batch-32 = 1.0350020602345467e-05

Training epoch-28 batch-33
Running loss of epoch-28 batch-33 = 5.16488216817379e-06

Training epoch-28 batch-34
Running loss of epoch-28 batch-34 = 5.9409067034721375e-06

Training epoch-28 batch-35
Running loss of epoch-28 batch-35 = 7.1320682764053345e-06

Training epoch-28 batch-36
Running loss of epoch-28 batch-36 = 1.0158400982618332e-05

Training epoch-28 batch-37
Running loss of epoch-28 batch-37 = 6.288522854447365e-06

Training epoch-28 batch-38
Running loss of epoch-28 batch-38 = 1.1724187061190605e-05

Training epoch-28 batch-39
Running loss of epoch-28 batch-39 = 4.684785380959511e-06

Training epoch-28 batch-40
Running loss of epoch-28 batch-40 = 7.1392860263586044e-06

Training epoch-28 batch-41
Running loss of epoch-28 batch-41 = 8.619623258709908e-06

Training epoch-28 batch-42
Running loss of epoch-28 batch-42 = 7.740221917629242e-06

Training epoch-28 batch-43
Running loss of epoch-28 batch-43 = 6.4838677644729614e-06

Training epoch-28 batch-44
Running loss of epoch-28 batch-44 = 5.987472832202911e-06

Training epoch-28 batch-45
Running loss of epoch-28 batch-45 = 8.57282429933548e-06

Training epoch-28 batch-46
Running loss of epoch-28 batch-46 = 7.914146408438683e-06

Training epoch-28 batch-47
Running loss of epoch-28 batch-47 = 5.914364010095596e-06

Training epoch-28 batch-48
Running loss of epoch-28 batch-48 = 6.523681804537773e-06

Training epoch-28 batch-49
Running loss of epoch-28 batch-49 = 2.9793009161949158e-06

Training epoch-28 batch-50
Running loss of epoch-28 batch-50 = 7.664784789085388e-06

Training epoch-28 batch-51
Running loss of epoch-28 batch-51 = 7.274560630321503e-06

Training epoch-28 batch-52
Running loss of epoch-28 batch-52 = 6.362795829772949e-06

Training epoch-28 batch-53
Running loss of epoch-28 batch-53 = 6.7052897065877914e-06

Training epoch-28 batch-54
Running loss of epoch-28 batch-54 = 9.186798706650734e-06

Training epoch-28 batch-55
Running loss of epoch-28 batch-55 = 6.125075742602348e-06

Training epoch-28 batch-56
Running loss of epoch-28 batch-56 = 5.00725582242012e-06

Training epoch-28 batch-57
Running loss of epoch-28 batch-57 = 4.9746595323085785e-06

Training epoch-28 batch-58
Running loss of epoch-28 batch-58 = 6.794696673750877e-06

Training epoch-28 batch-59
Running loss of epoch-28 batch-59 = 6.268732249736786e-06

Training epoch-28 batch-60
Running loss of epoch-28 batch-60 = 5.7301949709653854e-06

Training epoch-28 batch-61
Running loss of epoch-28 batch-61 = 6.727175787091255e-06

Training epoch-28 batch-62
Running loss of epoch-28 batch-62 = 5.831709131598473e-06

Training epoch-28 batch-63
Running loss of epoch-28 batch-63 = 5.693873390555382e-06

Training epoch-28 batch-64
Running loss of epoch-28 batch-64 = 8.644070476293564e-06

Training epoch-28 batch-65
Running loss of epoch-28 batch-65 = 6.545335054397583e-06

Training epoch-28 batch-66
Running loss of epoch-28 batch-66 = 1.2323260307312012e-05

Training epoch-28 batch-67
Running loss of epoch-28 batch-67 = 1.0940013453364372e-05

Training epoch-28 batch-68
Running loss of epoch-28 batch-68 = 1.257588155567646e-05

Training epoch-28 batch-69
Running loss of epoch-28 batch-69 = 5.890615284442902e-06

Training epoch-28 batch-70
Running loss of epoch-28 batch-70 = 8.525094017386436e-06

Training epoch-28 batch-71
Running loss of epoch-28 batch-71 = 5.4782722145318985e-06

Training epoch-28 batch-72
Running loss of epoch-28 batch-72 = 6.258022040128708e-06

Training epoch-28 batch-73
Running loss of epoch-28 batch-73 = 7.958849892020226e-06

Training epoch-28 batch-74
Running loss of epoch-28 batch-74 = 8.703675121068954e-06

Training epoch-28 batch-75
Running loss of epoch-28 batch-75 = 3.275694325566292e-06

Training epoch-28 batch-76
Running loss of epoch-28 batch-76 = 6.546499207615852e-06

Training epoch-28 batch-77
Running loss of epoch-28 batch-77 = 7.137889042496681e-06

Training epoch-28 batch-78
Running loss of epoch-28 batch-78 = 7.788185030221939e-06

Training epoch-28 batch-79
Running loss of epoch-28 batch-79 = 5.073146894574165e-06

Training epoch-28 batch-80
Running loss of epoch-28 batch-80 = 7.633352652192116e-06

Training epoch-28 batch-81
Running loss of epoch-28 batch-81 = 8.729984983801842e-06

Training epoch-28 batch-82
Running loss of epoch-28 batch-82 = 8.181203156709671e-06

Training epoch-28 batch-83
Running loss of epoch-28 batch-83 = 8.11857171356678e-06

Training epoch-28 batch-84
Running loss of epoch-28 batch-84 = 1.3640383258461952e-05

Training epoch-28 batch-85
Running loss of epoch-28 batch-85 = 5.344860255718231e-06

Training epoch-28 batch-86
Running loss of epoch-28 batch-86 = 6.292248144745827e-06

Training epoch-28 batch-87
Running loss of epoch-28 batch-87 = 4.4798944145441055e-06

Training epoch-28 batch-88
Running loss of epoch-28 batch-88 = 7.1998219937086105e-06

Training epoch-28 batch-89
Running loss of epoch-28 batch-89 = 1.0363757610321045e-05

Training epoch-28 batch-90
Running loss of epoch-28 batch-90 = 5.657784640789032e-06

Training epoch-28 batch-91
Running loss of epoch-28 batch-91 = 4.701782017946243e-06

Training epoch-28 batch-92
Running loss of epoch-28 batch-92 = 6.8927183747291565e-06

Training epoch-28 batch-93
Running loss of epoch-28 batch-93 = 9.694835171103477e-06

Training epoch-28 batch-94
Running loss of epoch-28 batch-94 = 6.3695479184389114e-06

Training epoch-28 batch-95
Running loss of epoch-28 batch-95 = 5.988869816064835e-06

Training epoch-28 batch-96
Running loss of epoch-28 batch-96 = 9.27923247218132e-06

Training epoch-28 batch-97
Running loss of epoch-28 batch-97 = 6.073853000998497e-06

Training epoch-28 batch-98
Running loss of epoch-28 batch-98 = 6.886431947350502e-06

Training epoch-28 batch-99
Running loss of epoch-28 batch-99 = 3.521796315908432e-06

Training epoch-28 batch-100
Running loss of epoch-28 batch-100 = 4.262896254658699e-06

Training epoch-28 batch-101
Running loss of epoch-28 batch-101 = 5.09992241859436e-06

Training epoch-28 batch-102
Running loss of epoch-28 batch-102 = 9.113689884543419e-06

Training epoch-28 batch-103
Running loss of epoch-28 batch-103 = 4.7388020902872086e-06

Training epoch-28 batch-104
Running loss of epoch-28 batch-104 = 7.243361324071884e-06

Training epoch-28 batch-105
Running loss of epoch-28 batch-105 = 6.217043846845627e-06

Training epoch-28 batch-106
Running loss of epoch-28 batch-106 = 1.0348157957196236e-05

Training epoch-28 batch-107
Running loss of epoch-28 batch-107 = 7.903436198830605e-06

Training epoch-28 batch-108
Running loss of epoch-28 batch-108 = 8.267117664217949e-06

Training epoch-28 batch-109
Running loss of epoch-28 batch-109 = 6.679445505142212e-06

Training epoch-28 batch-110
Running loss of epoch-28 batch-110 = 6.660120561718941e-06

Training epoch-28 batch-111
Running loss of epoch-28 batch-111 = 7.198657840490341e-06

Training epoch-28 batch-112
Running loss of epoch-28 batch-112 = 8.031027391552925e-06

Training epoch-28 batch-113
Running loss of epoch-28 batch-113 = 7.87363387644291e-06

Training epoch-28 batch-114
Running loss of epoch-28 batch-114 = 7.865950465202332e-06

Training epoch-28 batch-115
Running loss of epoch-28 batch-115 = 5.612382665276527e-06

Training epoch-28 batch-116
Running loss of epoch-28 batch-116 = 6.361166015267372e-06

Training epoch-28 batch-117
Running loss of epoch-28 batch-117 = 1.0215910151600838e-05

Training epoch-28 batch-118
Running loss of epoch-28 batch-118 = 5.972804501652718e-06

Training epoch-28 batch-119
Running loss of epoch-28 batch-119 = 7.955590263009071e-06

Training epoch-28 batch-120
Running loss of epoch-28 batch-120 = 7.0873647928237915e-06

Training epoch-28 batch-121
Running loss of epoch-28 batch-121 = 5.49759715795517e-06

Training epoch-28 batch-122
Running loss of epoch-28 batch-122 = 5.984678864479065e-06

Training epoch-28 batch-123
Running loss of epoch-28 batch-123 = 5.724141374230385e-06

Training epoch-28 batch-124
Running loss of epoch-28 batch-124 = 6.236368790268898e-06

Training epoch-28 batch-125
Running loss of epoch-28 batch-125 = 8.256640285253525e-06

Training epoch-28 batch-126
Running loss of epoch-28 batch-126 = 9.366776794195175e-06

Training epoch-28 batch-127
Running loss of epoch-28 batch-127 = 9.01985913515091e-06

Training epoch-28 batch-128
Running loss of epoch-28 batch-128 = 6.556045264005661e-06

Training epoch-28 batch-129
Running loss of epoch-28 batch-129 = 9.593553841114044e-06

Training epoch-28 batch-130
Running loss of epoch-28 batch-130 = 7.347669452428818e-06

Training epoch-28 batch-131
Running loss of epoch-28 batch-131 = 6.2766484916210175e-06

Training epoch-28 batch-132
Running loss of epoch-28 batch-132 = 4.886416718363762e-06

Training epoch-28 batch-133
Running loss of epoch-28 batch-133 = 5.178852006793022e-06

Training epoch-28 batch-134
Running loss of epoch-28 batch-134 = 1.026294194161892e-05

Training epoch-28 batch-135
Running loss of epoch-28 batch-135 = 7.492490112781525e-06

Training epoch-28 batch-136
Running loss of epoch-28 batch-136 = 4.541827365756035e-06

Training epoch-28 batch-137
Running loss of epoch-28 batch-137 = 7.2212424129247665e-06

Training epoch-28 batch-138
Running loss of epoch-28 batch-138 = 5.873152986168861e-06

Training epoch-28 batch-139
Running loss of epoch-28 batch-139 = 7.177703082561493e-06

Training epoch-28 batch-140
Running loss of epoch-28 batch-140 = 7.1963295340538025e-06

Training epoch-28 batch-141
Running loss of epoch-28 batch-141 = 7.314374670386314e-06

Training epoch-28 batch-142
Running loss of epoch-28 batch-142 = 5.580252036452293e-06

Training epoch-28 batch-143
Running loss of epoch-28 batch-143 = 6.9462694227695465e-06

Training epoch-28 batch-144
Running loss of epoch-28 batch-144 = 7.950933650135994e-06

Training epoch-28 batch-145
Running loss of epoch-28 batch-145 = 4.492932930588722e-06

Training epoch-28 batch-146
Running loss of epoch-28 batch-146 = 7.750466465950012e-06

Training epoch-28 batch-147
Running loss of epoch-28 batch-147 = 6.16069883108139e-06

Training epoch-28 batch-148
Running loss of epoch-28 batch-148 = 7.218215614557266e-06

Training epoch-28 batch-149
Running loss of epoch-28 batch-149 = 5.5783893913030624e-06

Training epoch-28 batch-150
Running loss of epoch-28 batch-150 = 5.656154826283455e-06

Training epoch-28 batch-151
Running loss of epoch-28 batch-151 = 7.652444764971733e-06

Training epoch-28 batch-152
Running loss of epoch-28 batch-152 = 4.7192443162202835e-06

Training epoch-28 batch-153
Running loss of epoch-28 batch-153 = 9.011244401335716e-06

Training epoch-28 batch-154
Running loss of epoch-28 batch-154 = 6.973743438720703e-06

Training epoch-28 batch-155
Running loss of epoch-28 batch-155 = 4.760688170790672e-06

Training epoch-28 batch-156
Running loss of epoch-28 batch-156 = 6.210058927536011e-06

Training epoch-28 batch-157
Running loss of epoch-28 batch-157 = 2.5097280740737915e-05

Finished training epoch-28.



Average train loss at epoch-28 = 7.1347162127494814e-06

Started Evaluation

Average val loss at epoch-28 = 2.9076385231394517

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.36 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.49 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 36.92 %

Overall Accuracy = 54.59 %

Finished Evaluation



Started training epoch-29


Training epoch-29 batch-1
Running loss of epoch-29 batch-1 = 7.138121873140335e-06

Training epoch-29 batch-2
Running loss of epoch-29 batch-2 = 8.02590511739254e-06

Training epoch-29 batch-3
Running loss of epoch-29 batch-3 = 6.535788998007774e-06

Training epoch-29 batch-4
Running loss of epoch-29 batch-4 = 7.678987458348274e-06

Training epoch-29 batch-5
Running loss of epoch-29 batch-5 = 7.720431312918663e-06

Training epoch-29 batch-6
Running loss of epoch-29 batch-6 = 7.605878636240959e-06

Training epoch-29 batch-7
Running loss of epoch-29 batch-7 = 5.837762728333473e-06

Training epoch-29 batch-8
Running loss of epoch-29 batch-8 = 4.707369953393936e-06

Training epoch-29 batch-9
Running loss of epoch-29 batch-9 = 6.056390702724457e-06

Training epoch-29 batch-10
Running loss of epoch-29 batch-10 = 5.589332431554794e-06

Training epoch-29 batch-11
Running loss of epoch-29 batch-11 = 6.686896085739136e-06

Training epoch-29 batch-12
Running loss of epoch-29 batch-12 = 6.981659680604935e-06

Training epoch-29 batch-13
Running loss of epoch-29 batch-13 = 5.727866664528847e-06

Training epoch-29 batch-14
Running loss of epoch-29 batch-14 = 6.586313247680664e-06

Training epoch-29 batch-15
Running loss of epoch-29 batch-15 = 5.300622433423996e-06

Training epoch-29 batch-16
Running loss of epoch-29 batch-16 = 7.484573870897293e-06

Training epoch-29 batch-17
Running loss of epoch-29 batch-17 = 5.176058039069176e-06

Training epoch-29 batch-18
Running loss of epoch-29 batch-18 = 6.319722160696983e-06

Training epoch-29 batch-19
Running loss of epoch-29 batch-19 = 5.051726475358009e-06

Training epoch-29 batch-20
Running loss of epoch-29 batch-20 = 7.146503776311874e-06

Training epoch-29 batch-21
Running loss of epoch-29 batch-21 = 6.815185770392418e-06

Training epoch-29 batch-22
Running loss of epoch-29 batch-22 = 1.0448507964611053e-05

Training epoch-29 batch-23
Running loss of epoch-29 batch-23 = 6.806105375289917e-06

Training epoch-29 batch-24
Running loss of epoch-29 batch-24 = 6.392365321516991e-06

Training epoch-29 batch-25
Running loss of epoch-29 batch-25 = 6.570946425199509e-06

Training epoch-29 batch-26
Running loss of epoch-29 batch-26 = 1.0058283805847168e-05

Training epoch-29 batch-27
Running loss of epoch-29 batch-27 = 6.4927153289318085e-06

Training epoch-29 batch-28
Running loss of epoch-29 batch-28 = 7.781432941555977e-06

Training epoch-29 batch-29
Running loss of epoch-29 batch-29 = 5.002832040190697e-06

Training epoch-29 batch-30
Running loss of epoch-29 batch-30 = 6.8463850766420364e-06

Training epoch-29 batch-31
Running loss of epoch-29 batch-31 = 7.717404514551163e-06

Training epoch-29 batch-32
Running loss of epoch-29 batch-32 = 6.4461492002010345e-06

Training epoch-29 batch-33
Running loss of epoch-29 batch-33 = 9.004841558635235e-06

Training epoch-29 batch-34
Running loss of epoch-29 batch-34 = 7.935799658298492e-06

Training epoch-29 batch-35
Running loss of epoch-29 batch-35 = 7.43987038731575e-06

Training epoch-29 batch-36
Running loss of epoch-29 batch-36 = 7.515307515859604e-06

Training epoch-29 batch-37
Running loss of epoch-29 batch-37 = 7.006572559475899e-06

Training epoch-29 batch-38
Running loss of epoch-29 batch-38 = 7.386785000562668e-06

Training epoch-29 batch-39
Running loss of epoch-29 batch-39 = 5.010515451431274e-06

Training epoch-29 batch-40
Running loss of epoch-29 batch-40 = 9.490875527262688e-06

Training epoch-29 batch-41
Running loss of epoch-29 batch-41 = 8.520204573869705e-06

Training epoch-29 batch-42
Running loss of epoch-29 batch-42 = 7.997965440154076e-06

Training epoch-29 batch-43
Running loss of epoch-29 batch-43 = 6.360700353980064e-06

Training epoch-29 batch-44
Running loss of epoch-29 batch-44 = 6.466405466198921e-06

Training epoch-29 batch-45
Running loss of epoch-29 batch-45 = 4.237284883856773e-06

Training epoch-29 batch-46
Running loss of epoch-29 batch-46 = 4.728790372610092e-06

Training epoch-29 batch-47
Running loss of epoch-29 batch-47 = 6.809365004301071e-06

Training epoch-29 batch-48
Running loss of epoch-29 batch-48 = 5.245907232165337e-06

Training epoch-29 batch-49
Running loss of epoch-29 batch-49 = 1.1031748726963997e-05

Training epoch-29 batch-50
Running loss of epoch-29 batch-50 = 6.286660209298134e-06

Training epoch-29 batch-51
Running loss of epoch-29 batch-51 = 6.921589374542236e-06

Training epoch-29 batch-52
Running loss of epoch-29 batch-52 = 7.049646228551865e-06

Training epoch-29 batch-53
Running loss of epoch-29 batch-53 = 9.384704753756523e-06

Training epoch-29 batch-54
Running loss of epoch-29 batch-54 = 5.5620912462472916e-06

Training epoch-29 batch-55
Running loss of epoch-29 batch-55 = 1.1302297934889793e-05

Training epoch-29 batch-56
Running loss of epoch-29 batch-56 = 6.514601409435272e-06

Training epoch-29 batch-57
Running loss of epoch-29 batch-57 = 9.065959602594376e-06

Training epoch-29 batch-58
Running loss of epoch-29 batch-58 = 5.428912118077278e-06

Training epoch-29 batch-59
Running loss of epoch-29 batch-59 = 6.702262908220291e-06

Training epoch-29 batch-60
Running loss of epoch-29 batch-60 = 6.926478818058968e-06

Training epoch-29 batch-61
Running loss of epoch-29 batch-61 = 6.003770977258682e-06

Training epoch-29 batch-62
Running loss of epoch-29 batch-62 = 5.808891728520393e-06

Training epoch-29 batch-63
Running loss of epoch-29 batch-63 = 6.7765358835458755e-06

Training epoch-29 batch-64
Running loss of epoch-29 batch-64 = 8.75932164490223e-06

Training epoch-29 batch-65
Running loss of epoch-29 batch-65 = 8.897390216588974e-06

Training epoch-29 batch-66
Running loss of epoch-29 batch-66 = 6.338581442832947e-06

Training epoch-29 batch-67
Running loss of epoch-29 batch-67 = 5.0156377255916595e-06

Training epoch-29 batch-68
Running loss of epoch-29 batch-68 = 7.218914106488228e-06

Training epoch-29 batch-69
Running loss of epoch-29 batch-69 = 8.77678394317627e-06

Training epoch-29 batch-70
Running loss of epoch-29 batch-70 = 6.444286555051804e-06

Training epoch-29 batch-71
Running loss of epoch-29 batch-71 = 7.2624534368515015e-06

Training epoch-29 batch-72
Running loss of epoch-29 batch-72 = 8.270610123872757e-06

Training epoch-29 batch-73
Running loss of epoch-29 batch-73 = 5.788402631878853e-06

Training epoch-29 batch-74
Running loss of epoch-29 batch-74 = 4.536937922239304e-06

Training epoch-29 batch-75
Running loss of epoch-29 batch-75 = 8.887378498911858e-06

Training epoch-29 batch-76
Running loss of epoch-29 batch-76 = 4.806322976946831e-06

Training epoch-29 batch-77
Running loss of epoch-29 batch-77 = 5.451031029224396e-06

Training epoch-29 batch-78
Running loss of epoch-29 batch-78 = 7.2964467108249664e-06

Training epoch-29 batch-79
Running loss of epoch-29 batch-79 = 6.910413503646851e-06

Training epoch-29 batch-80
Running loss of epoch-29 batch-80 = 6.412854418158531e-06

Training epoch-29 batch-81
Running loss of epoch-29 batch-81 = 8.895760402083397e-06

Training epoch-29 batch-82
Running loss of epoch-29 batch-82 = 7.041962817311287e-06

Training epoch-29 batch-83
Running loss of epoch-29 batch-83 = 9.25757922232151e-06

Training epoch-29 batch-84
Running loss of epoch-29 batch-84 = 6.634509190917015e-06

Training epoch-29 batch-85
Running loss of epoch-29 batch-85 = 5.300389602780342e-06

Training epoch-29 batch-86
Running loss of epoch-29 batch-86 = 5.255918949842453e-06

Training epoch-29 batch-87
Running loss of epoch-29 batch-87 = 4.1620805859565735e-06

Training epoch-29 batch-88
Running loss of epoch-29 batch-88 = 4.2351894080638885e-06

Training epoch-29 batch-89
Running loss of epoch-29 batch-89 = 7.3802657425403595e-06

Training epoch-29 batch-90
Running loss of epoch-29 batch-90 = 4.680827260017395e-06

Training epoch-29 batch-91
Running loss of epoch-29 batch-91 = 5.757901817560196e-06

Training epoch-29 batch-92
Running loss of epoch-29 batch-92 = 6.546033546328545e-06

Training epoch-29 batch-93
Running loss of epoch-29 batch-93 = 5.292706191539764e-06

Training epoch-29 batch-94
Running loss of epoch-29 batch-94 = 8.344883099198341e-06

Training epoch-29 batch-95
Running loss of epoch-29 batch-95 = 4.356028512120247e-06

Training epoch-29 batch-96
Running loss of epoch-29 batch-96 = 2.8347130864858627e-06

Training epoch-29 batch-97
Running loss of epoch-29 batch-97 = 6.075948476791382e-06

Training epoch-29 batch-98
Running loss of epoch-29 batch-98 = 6.741378456354141e-06

Training epoch-29 batch-99
Running loss of epoch-29 batch-99 = 4.907138645648956e-06

Training epoch-29 batch-100
Running loss of epoch-29 batch-100 = 4.686880856752396e-06

Training epoch-29 batch-101
Running loss of epoch-29 batch-101 = 5.9211160987615585e-06

Training epoch-29 batch-102
Running loss of epoch-29 batch-102 = 7.3125120252370834e-06

Training epoch-29 batch-103
Running loss of epoch-29 batch-103 = 7.420079782605171e-06

Training epoch-29 batch-104
Running loss of epoch-29 batch-104 = 8.506467565894127e-06

Training epoch-29 batch-105
Running loss of epoch-29 batch-105 = 6.080372259020805e-06

Training epoch-29 batch-106
Running loss of epoch-29 batch-106 = 5.345325917005539e-06

Training epoch-29 batch-107
Running loss of epoch-29 batch-107 = 5.286186933517456e-06

Training epoch-29 batch-108
Running loss of epoch-29 batch-108 = 6.213318556547165e-06

Training epoch-29 batch-109
Running loss of epoch-29 batch-109 = 5.40376640856266e-06

Training epoch-29 batch-110
Running loss of epoch-29 batch-110 = 6.752088665962219e-06

Training epoch-29 batch-111
Running loss of epoch-29 batch-111 = 8.622417226433754e-06

Training epoch-29 batch-112
Running loss of epoch-29 batch-112 = 6.486428901553154e-06

Training epoch-29 batch-113
Running loss of epoch-29 batch-113 = 5.545560270547867e-06

Training epoch-29 batch-114
Running loss of epoch-29 batch-114 = 5.914829671382904e-06

Training epoch-29 batch-115
Running loss of epoch-29 batch-115 = 6.674323230981827e-06

Training epoch-29 batch-116
Running loss of epoch-29 batch-116 = 6.0265883803367615e-06

Training epoch-29 batch-117
Running loss of epoch-29 batch-117 = 6.343470886349678e-06

Training epoch-29 batch-118
Running loss of epoch-29 batch-118 = 4.646368324756622e-06

Training epoch-29 batch-119
Running loss of epoch-29 batch-119 = 6.12880103290081e-06

Training epoch-29 batch-120
Running loss of epoch-29 batch-120 = 5.5888667702674866e-06

Training epoch-29 batch-121
Running loss of epoch-29 batch-121 = 3.889435902237892e-06

Training epoch-29 batch-122
Running loss of epoch-29 batch-122 = 6.8105291575193405e-06

Training epoch-29 batch-123
Running loss of epoch-29 batch-123 = 5.831709131598473e-06

Training epoch-29 batch-124
Running loss of epoch-29 batch-124 = 6.511341780424118e-06

Training epoch-29 batch-125
Running loss of epoch-29 batch-125 = 9.371200576424599e-06

Training epoch-29 batch-126
Running loss of epoch-29 batch-126 = 8.356058970093727e-06

Training epoch-29 batch-127
Running loss of epoch-29 batch-127 = 5.830544978380203e-06

Training epoch-29 batch-128
Running loss of epoch-29 batch-128 = 7.763272151350975e-06

Training epoch-29 batch-129
Running loss of epoch-29 batch-129 = 3.892229869961739e-06

Training epoch-29 batch-130
Running loss of epoch-29 batch-130 = 4.912726581096649e-06

Training epoch-29 batch-131
Running loss of epoch-29 batch-131 = 2.4170149117708206e-06

Training epoch-29 batch-132
Running loss of epoch-29 batch-132 = 5.520880222320557e-06

Training epoch-29 batch-133
Running loss of epoch-29 batch-133 = 7.751863449811935e-06

Training epoch-29 batch-134
Running loss of epoch-29 batch-134 = 5.434034392237663e-06

Training epoch-29 batch-135
Running loss of epoch-29 batch-135 = 8.97119753062725e-06

Training epoch-29 batch-136
Running loss of epoch-29 batch-136 = 5.15463761985302e-06

Training epoch-29 batch-137
Running loss of epoch-29 batch-137 = 7.722526788711548e-06

Training epoch-29 batch-138
Running loss of epoch-29 batch-138 = 1.0370276868343353e-05

Training epoch-29 batch-139
Running loss of epoch-29 batch-139 = 9.378185495734215e-06

Training epoch-29 batch-140
Running loss of epoch-29 batch-140 = 7.041264325380325e-06

Training epoch-29 batch-141
Running loss of epoch-29 batch-141 = 7.068738341331482e-06

Training epoch-29 batch-142
Running loss of epoch-29 batch-142 = 8.173519745469093e-06

Training epoch-29 batch-143
Running loss of epoch-29 batch-143 = 5.309469997882843e-06

Training epoch-29 batch-144
Running loss of epoch-29 batch-144 = 8.323229849338531e-06

Training epoch-29 batch-145
Running loss of epoch-29 batch-145 = 5.159527063369751e-06

Training epoch-29 batch-146
Running loss of epoch-29 batch-146 = 5.256384611129761e-06

Training epoch-29 batch-147
Running loss of epoch-29 batch-147 = 1.0204501450061798e-05

Training epoch-29 batch-148
Running loss of epoch-29 batch-148 = 5.6889839470386505e-06

Training epoch-29 batch-149
Running loss of epoch-29 batch-149 = 6.067799404263496e-06

Training epoch-29 batch-150
Running loss of epoch-29 batch-150 = 4.727626219391823e-06

Training epoch-29 batch-151
Running loss of epoch-29 batch-151 = 7.692957296967506e-06

Training epoch-29 batch-152
Running loss of epoch-29 batch-152 = 6.811926141381264e-06

Training epoch-29 batch-153
Running loss of epoch-29 batch-153 = 6.5409112721681595e-06

Training epoch-29 batch-154
Running loss of epoch-29 batch-154 = 1.0486459359526634e-05

Training epoch-29 batch-155
Running loss of epoch-29 batch-155 = 5.941139534115791e-06

Training epoch-29 batch-156
Running loss of epoch-29 batch-156 = 4.745787009596825e-06

Training epoch-29 batch-157
Running loss of epoch-29 batch-157 = 3.4831464290618896e-05

Finished training epoch-29.



Average train loss at epoch-29 = 6.721139699220657e-06

Started Evaluation

Average val loss at epoch-29 = 2.9232778564879767

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.75 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.93 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.15 %
Accuracy for class execute is: 26.91 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-30


Training epoch-30 batch-1
Running loss of epoch-30 batch-1 = 4.983041435480118e-06

Training epoch-30 batch-2
Running loss of epoch-30 batch-2 = 4.97233122587204e-06

Training epoch-30 batch-3
Running loss of epoch-30 batch-3 = 6.132293492555618e-06

Training epoch-30 batch-4
Running loss of epoch-30 batch-4 = 5.097128450870514e-06

Training epoch-30 batch-5
Running loss of epoch-30 batch-5 = 4.358822479844093e-06

Training epoch-30 batch-6
Running loss of epoch-30 batch-6 = 4.338566213846207e-06

Training epoch-30 batch-7
Running loss of epoch-30 batch-7 = 5.65103255212307e-06

Training epoch-30 batch-8
Running loss of epoch-30 batch-8 = 9.035691618919373e-06

Training epoch-30 batch-9
Running loss of epoch-30 batch-9 = 7.415656000375748e-06

Training epoch-30 batch-10
Running loss of epoch-30 batch-10 = 9.436625987291336e-06

Training epoch-30 batch-11
Running loss of epoch-30 batch-11 = 6.099464371800423e-06

Training epoch-30 batch-12
Running loss of epoch-30 batch-12 = 7.166294381022453e-06

Training epoch-30 batch-13
Running loss of epoch-30 batch-13 = 6.837071850895882e-06

Training epoch-30 batch-14
Running loss of epoch-30 batch-14 = 8.149538189172745e-06

Training epoch-30 batch-15
Running loss of epoch-30 batch-15 = 7.65942968428135e-06

Training epoch-30 batch-16
Running loss of epoch-30 batch-16 = 1.0355142876505852e-05

Training epoch-30 batch-17
Running loss of epoch-30 batch-17 = 5.1031820476055145e-06

Training epoch-30 batch-18
Running loss of epoch-30 batch-18 = 5.6943390518426895e-06

Training epoch-30 batch-19
Running loss of epoch-30 batch-19 = 7.380731403827667e-06

Training epoch-30 batch-20
Running loss of epoch-30 batch-20 = 5.534850060939789e-06

Training epoch-30 batch-21
Running loss of epoch-30 batch-21 = 5.975598469376564e-06

Training epoch-30 batch-22
Running loss of epoch-30 batch-22 = 7.701106369495392e-06

Training epoch-30 batch-23
Running loss of epoch-30 batch-23 = 8.040806278586388e-06

Training epoch-30 batch-24
Running loss of epoch-30 batch-24 = 5.464302375912666e-06

Training epoch-30 batch-25
Running loss of epoch-30 batch-25 = 7.906230166554451e-06

Training epoch-30 batch-26
Running loss of epoch-30 batch-26 = 7.47176818549633e-06

Training epoch-30 batch-27
Running loss of epoch-30 batch-27 = 4.4943299144506454e-06

Training epoch-30 batch-28
Running loss of epoch-30 batch-28 = 6.716931238770485e-06

Training epoch-30 batch-29
Running loss of epoch-30 batch-29 = 6.7374203354120255e-06

Training epoch-30 batch-30
Running loss of epoch-30 batch-30 = 9.227311238646507e-06

Training epoch-30 batch-31
Running loss of epoch-30 batch-31 = 5.28222881257534e-06

Training epoch-30 batch-32
Running loss of epoch-30 batch-32 = 4.86825592815876e-06

Training epoch-30 batch-33
Running loss of epoch-30 batch-33 = 4.478497430682182e-06

Training epoch-30 batch-34
Running loss of epoch-30 batch-34 = 4.681292921304703e-06

Training epoch-30 batch-35
Running loss of epoch-30 batch-35 = 6.408896297216415e-06

Training epoch-30 batch-36
Running loss of epoch-30 batch-36 = 7.604481652379036e-06

Training epoch-30 batch-37
Running loss of epoch-30 batch-37 = 4.976289346814156e-06

Training epoch-30 batch-38
Running loss of epoch-30 batch-38 = 6.484333425760269e-06

Training epoch-30 batch-39
Running loss of epoch-30 batch-39 = 7.545342668890953e-06

Training epoch-30 batch-40
Running loss of epoch-30 batch-40 = 9.933486580848694e-06

Training epoch-30 batch-41
Running loss of epoch-30 batch-41 = 4.963018000125885e-06

Training epoch-30 batch-42
Running loss of epoch-30 batch-42 = 4.6316999942064285e-06

Training epoch-30 batch-43
Running loss of epoch-30 batch-43 = 7.763272151350975e-06

Training epoch-30 batch-44
Running loss of epoch-30 batch-44 = 5.736015737056732e-06

Training epoch-30 batch-45
Running loss of epoch-30 batch-45 = 8.096219971776009e-06

Training epoch-30 batch-46
Running loss of epoch-30 batch-46 = 8.140457794070244e-06

Training epoch-30 batch-47
Running loss of epoch-30 batch-47 = 5.107838660478592e-06

Training epoch-30 batch-48
Running loss of epoch-30 batch-48 = 5.9264712035655975e-06

Training epoch-30 batch-49
Running loss of epoch-30 batch-49 = 1.0427786037325859e-05

Training epoch-30 batch-50
Running loss of epoch-30 batch-50 = 4.109926521778107e-06

Training epoch-30 batch-51
Running loss of epoch-30 batch-51 = 5.4943375289440155e-06

Training epoch-30 batch-52
Running loss of epoch-30 batch-52 = 6.955582648515701e-06

Training epoch-30 batch-53
Running loss of epoch-30 batch-53 = 7.203081622719765e-06

Training epoch-30 batch-54
Running loss of epoch-30 batch-54 = 6.538582965731621e-06

Training epoch-30 batch-55
Running loss of epoch-30 batch-55 = 5.717389285564423e-06

Training epoch-30 batch-56
Running loss of epoch-30 batch-56 = 7.33695924282074e-06

Training epoch-30 batch-57
Running loss of epoch-30 batch-57 = 5.163718014955521e-06

Training epoch-30 batch-58
Running loss of epoch-30 batch-58 = 7.622409611940384e-06

Training epoch-30 batch-59
Running loss of epoch-30 batch-59 = 6.60773366689682e-06

Training epoch-30 batch-60
Running loss of epoch-30 batch-60 = 7.685506716370583e-06

Training epoch-30 batch-61
Running loss of epoch-30 batch-61 = 5.666632205247879e-06

Training epoch-30 batch-62
Running loss of epoch-30 batch-62 = 4.12575900554657e-06

Training epoch-30 batch-63
Running loss of epoch-30 batch-63 = 5.797483026981354e-06

Training epoch-30 batch-64
Running loss of epoch-30 batch-64 = 5.993293598294258e-06

Training epoch-30 batch-65
Running loss of epoch-30 batch-65 = 6.498536095023155e-06

Training epoch-30 batch-66
Running loss of epoch-30 batch-66 = 5.727633833885193e-06

Training epoch-30 batch-67
Running loss of epoch-30 batch-67 = 5.895970389246941e-06

Training epoch-30 batch-68
Running loss of epoch-30 batch-68 = 6.3141342252492905e-06

Training epoch-30 batch-69
Running loss of epoch-30 batch-69 = 7.665716111660004e-06

Training epoch-30 batch-70
Running loss of epoch-30 batch-70 = 5.902955308556557e-06

Training epoch-30 batch-71
Running loss of epoch-30 batch-71 = 8.53370875120163e-06

Training epoch-30 batch-72
Running loss of epoch-30 batch-72 = 4.176516085863113e-06

Training epoch-30 batch-73
Running loss of epoch-30 batch-73 = 6.878050044178963e-06

Training epoch-30 batch-74
Running loss of epoch-30 batch-74 = 4.8477668315172195e-06

Training epoch-30 batch-75
Running loss of epoch-30 batch-75 = 5.522742867469788e-06

Training epoch-30 batch-76
Running loss of epoch-30 batch-76 = 7.4980780482292175e-06

Training epoch-30 batch-77
Running loss of epoch-30 batch-77 = 6.1979517340660095e-06

Training epoch-30 batch-78
Running loss of epoch-30 batch-78 = 9.719748049974442e-06

Training epoch-30 batch-79
Running loss of epoch-30 batch-79 = 8.259667083621025e-06

Training epoch-30 batch-80
Running loss of epoch-30 batch-80 = 6.998423486948013e-06

Training epoch-30 batch-81
Running loss of epoch-30 batch-81 = 6.123911589384079e-06

Training epoch-30 batch-82
Running loss of epoch-30 batch-82 = 3.978610038757324e-06

Training epoch-30 batch-83
Running loss of epoch-30 batch-83 = 6.555579602718353e-06

Training epoch-30 batch-84
Running loss of epoch-30 batch-84 = 7.202848792076111e-06

Training epoch-30 batch-85
Running loss of epoch-30 batch-85 = 7.527181878685951e-06

Training epoch-30 batch-86
Running loss of epoch-30 batch-86 = 7.708091288805008e-06

Training epoch-30 batch-87
Running loss of epoch-30 batch-87 = 7.182825356721878e-06

Training epoch-30 batch-88
Running loss of epoch-30 batch-88 = 4.083849489688873e-06

Training epoch-30 batch-89
Running loss of epoch-30 batch-89 = 6.640562787652016e-06

Training epoch-30 batch-90
Running loss of epoch-30 batch-90 = 3.4847762435674667e-06

Training epoch-30 batch-91
Running loss of epoch-30 batch-91 = 5.795853212475777e-06

Training epoch-30 batch-92
Running loss of epoch-30 batch-92 = 7.2196125984191895e-06

Training epoch-30 batch-93
Running loss of epoch-30 batch-93 = 5.698530003428459e-06

Training epoch-30 batch-94
Running loss of epoch-30 batch-94 = 2.7869828045368195e-06

Training epoch-30 batch-95
Running loss of epoch-30 batch-95 = 4.534376785159111e-06

Training epoch-30 batch-96
Running loss of epoch-30 batch-96 = 5.135545507073402e-06

Training epoch-30 batch-97
Running loss of epoch-30 batch-97 = 5.12157566845417e-06

Training epoch-30 batch-98
Running loss of epoch-30 batch-98 = 3.9655715227127075e-06

Training epoch-30 batch-99
Running loss of epoch-30 batch-99 = 5.898997187614441e-06

Training epoch-30 batch-100
Running loss of epoch-30 batch-100 = 7.335329428315163e-06

Training epoch-30 batch-101
Running loss of epoch-30 batch-101 = 5.766050890088081e-06

Training epoch-30 batch-102
Running loss of epoch-30 batch-102 = 9.37306322157383e-06

Training epoch-30 batch-103
Running loss of epoch-30 batch-103 = 6.0070306062698364e-06

Training epoch-30 batch-104
Running loss of epoch-30 batch-104 = 4.843808710575104e-06

Training epoch-30 batch-105
Running loss of epoch-30 batch-105 = 4.832400009036064e-06

Training epoch-30 batch-106
Running loss of epoch-30 batch-106 = 4.648230969905853e-06

Training epoch-30 batch-107
Running loss of epoch-30 batch-107 = 6.346963346004486e-06

Training epoch-30 batch-108
Running loss of epoch-30 batch-108 = 6.840331479907036e-06

Training epoch-30 batch-109
Running loss of epoch-30 batch-109 = 5.163485184311867e-06

Training epoch-30 batch-110
Running loss of epoch-30 batch-110 = 6.43683597445488e-06

Training epoch-30 batch-111
Running loss of epoch-30 batch-111 = 6.73229806125164e-06

Training epoch-30 batch-112
Running loss of epoch-30 batch-112 = 6.47897832095623e-06

Training epoch-30 batch-113
Running loss of epoch-30 batch-113 = 8.579576388001442e-06

Training epoch-30 batch-114
Running loss of epoch-30 batch-114 = 6.2070321291685104e-06

Training epoch-30 batch-115
Running loss of epoch-30 batch-115 = 5.310634151101112e-06

Training epoch-30 batch-116
Running loss of epoch-30 batch-116 = 6.3213519752025604e-06

Training epoch-30 batch-117
Running loss of epoch-30 batch-117 = 9.644543752074242e-06

Training epoch-30 batch-118
Running loss of epoch-30 batch-118 = 4.462664946913719e-06

Training epoch-30 batch-119
Running loss of epoch-30 batch-119 = 7.74976797401905e-06

Training epoch-30 batch-120
Running loss of epoch-30 batch-120 = 6.884569302201271e-06

Training epoch-30 batch-121
Running loss of epoch-30 batch-121 = 7.959548383951187e-06

Training epoch-30 batch-122
Running loss of epoch-30 batch-122 = 7.513212040066719e-06

Training epoch-30 batch-123
Running loss of epoch-30 batch-123 = 3.734370693564415e-06

Training epoch-30 batch-124
Running loss of epoch-30 batch-124 = 5.972804501652718e-06

Training epoch-30 batch-125
Running loss of epoch-30 batch-125 = 5.9085432440042496e-06

Training epoch-30 batch-126
Running loss of epoch-30 batch-126 = 7.156515493988991e-06

Training epoch-30 batch-127
Running loss of epoch-30 batch-127 = 5.889451131224632e-06

Training epoch-30 batch-128
Running loss of epoch-30 batch-128 = 6.987946107983589e-06

Training epoch-30 batch-129
Running loss of epoch-30 batch-129 = 5.270121619105339e-06

Training epoch-30 batch-130
Running loss of epoch-30 batch-130 = 7.3749106377363205e-06

Training epoch-30 batch-131
Running loss of epoch-30 batch-131 = 5.7586003094911575e-06

Training epoch-30 batch-132
Running loss of epoch-30 batch-132 = 6.953952834010124e-06

Training epoch-30 batch-133
Running loss of epoch-30 batch-133 = 6.731366738677025e-06

Training epoch-30 batch-134
Running loss of epoch-30 batch-134 = 6.4338091760873795e-06

Training epoch-30 batch-135
Running loss of epoch-30 batch-135 = 6.259419023990631e-06

Training epoch-30 batch-136
Running loss of epoch-30 batch-136 = 1.034303568303585e-05

Training epoch-30 batch-137
Running loss of epoch-30 batch-137 = 6.5998174250125885e-06

Training epoch-30 batch-138
Running loss of epoch-30 batch-138 = 5.818437784910202e-06

Training epoch-30 batch-139
Running loss of epoch-30 batch-139 = 5.762558430433273e-06

Training epoch-30 batch-140
Running loss of epoch-30 batch-140 = 5.620997399091721e-06

Training epoch-30 batch-141
Running loss of epoch-30 batch-141 = 5.391426384449005e-06

Training epoch-30 batch-142
Running loss of epoch-30 batch-142 = 6.7926011979579926e-06

Training epoch-30 batch-143
Running loss of epoch-30 batch-143 = 7.190508767962456e-06

Training epoch-30 batch-144
Running loss of epoch-30 batch-144 = 4.256144165992737e-06

Training epoch-30 batch-145
Running loss of epoch-30 batch-145 = 6.029382348060608e-06

Training epoch-30 batch-146
Running loss of epoch-30 batch-146 = 8.449656888842583e-06

Training epoch-30 batch-147
Running loss of epoch-30 batch-147 = 5.361856892704964e-06

Training epoch-30 batch-148
Running loss of epoch-30 batch-148 = 5.138339474797249e-06

Training epoch-30 batch-149
Running loss of epoch-30 batch-149 = 4.654284566640854e-06

Training epoch-30 batch-150
Running loss of epoch-30 batch-150 = 3.5623088479042053e-06

Training epoch-30 batch-151
Running loss of epoch-30 batch-151 = 6.765127182006836e-06

Training epoch-30 batch-152
Running loss of epoch-30 batch-152 = 3.956491127610207e-06

Training epoch-30 batch-153
Running loss of epoch-30 batch-153 = 5.795620381832123e-06

Training epoch-30 batch-154
Running loss of epoch-30 batch-154 = 5.773268640041351e-06

Training epoch-30 batch-155
Running loss of epoch-30 batch-155 = 5.7336874306201935e-06

Training epoch-30 batch-156
Running loss of epoch-30 batch-156 = 3.545312210917473e-06

Training epoch-30 batch-157
Running loss of epoch-30 batch-157 = 1.784414052963257e-05

Finished training epoch-30.



Average train loss at epoch-30 = 6.325539946556091e-06

Started Evaluation

Average val loss at epoch-30 = 2.938124303755007

Accuracy for classes:
Accuracy for class equals is: 74.75 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 26.91 %
Accuracy for class get is: 36.67 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-31


Training epoch-31 batch-1
Running loss of epoch-31 batch-1 = 4.513189196586609e-06

Training epoch-31 batch-2
Running loss of epoch-31 batch-2 = 5.531590431928635e-06

Training epoch-31 batch-3
Running loss of epoch-31 batch-3 = 6.759772077202797e-06

Training epoch-31 batch-4
Running loss of epoch-31 batch-4 = 7.355120033025742e-06

Training epoch-31 batch-5
Running loss of epoch-31 batch-5 = 6.3516199588775635e-06

Training epoch-31 batch-6
Running loss of epoch-31 batch-6 = 1.1488562449812889e-05

Training epoch-31 batch-7
Running loss of epoch-31 batch-7 = 4.932284355163574e-06

Training epoch-31 batch-8
Running loss of epoch-31 batch-8 = 6.629852578043938e-06

Training epoch-31 batch-9
Running loss of epoch-31 batch-9 = 6.574671715497971e-06

Training epoch-31 batch-10
Running loss of epoch-31 batch-10 = 3.962777554988861e-06

Training epoch-31 batch-11
Running loss of epoch-31 batch-11 = 4.303641617298126e-06

Training epoch-31 batch-12
Running loss of epoch-31 batch-12 = 7.848022505640984e-06

Training epoch-31 batch-13
Running loss of epoch-31 batch-13 = 7.531838491559029e-06

Training epoch-31 batch-14
Running loss of epoch-31 batch-14 = 8.160946890711784e-06

Training epoch-31 batch-15
Running loss of epoch-31 batch-15 = 6.620539352297783e-06

Training epoch-31 batch-16
Running loss of epoch-31 batch-16 = 3.3436808735132217e-06

Training epoch-31 batch-17
Running loss of epoch-31 batch-17 = 7.497146725654602e-06

Training epoch-31 batch-18
Running loss of epoch-31 batch-18 = 5.582813173532486e-06

Training epoch-31 batch-19
Running loss of epoch-31 batch-19 = 6.474554538726807e-06

Training epoch-31 batch-20
Running loss of epoch-31 batch-20 = 6.803777068853378e-06

Training epoch-31 batch-21
Running loss of epoch-31 batch-21 = 8.424278348684311e-06

Training epoch-31 batch-22
Running loss of epoch-31 batch-22 = 4.606321454048157e-06

Training epoch-31 batch-23
Running loss of epoch-31 batch-23 = 3.7704594433307648e-06

Training epoch-31 batch-24
Running loss of epoch-31 batch-24 = 4.340894520282745e-06

Training epoch-31 batch-25
Running loss of epoch-31 batch-25 = 4.226574674248695e-06

Training epoch-31 batch-26
Running loss of epoch-31 batch-26 = 3.4370459616184235e-06

Training epoch-31 batch-27
Running loss of epoch-31 batch-27 = 7.803551852703094e-06

Training epoch-31 batch-28
Running loss of epoch-31 batch-28 = 3.325054422020912e-06

Training epoch-31 batch-29
Running loss of epoch-31 batch-29 = 6.49341382086277e-06

Training epoch-31 batch-30
Running loss of epoch-31 batch-30 = 9.630108252167702e-06

Training epoch-31 batch-31
Running loss of epoch-31 batch-31 = 5.4102856665849686e-06

Training epoch-31 batch-32
Running loss of epoch-31 batch-32 = 6.101792678236961e-06

Training epoch-31 batch-33
Running loss of epoch-31 batch-33 = 5.780719220638275e-06

Training epoch-31 batch-34
Running loss of epoch-31 batch-34 = 6.278976798057556e-06

Training epoch-31 batch-35
Running loss of epoch-31 batch-35 = 6.741145625710487e-06

Training epoch-31 batch-36
Running loss of epoch-31 batch-36 = 5.889218300580978e-06

Training epoch-31 batch-37
Running loss of epoch-31 batch-37 = 4.1441526263952255e-06

Training epoch-31 batch-38
Running loss of epoch-31 batch-38 = 4.25335019826889e-06

Training epoch-31 batch-39
Running loss of epoch-31 batch-39 = 4.953937605023384e-06

Training epoch-31 batch-40
Running loss of epoch-31 batch-40 = 8.301110938191414e-06

Training epoch-31 batch-41
Running loss of epoch-31 batch-41 = 7.256865501403809e-06

Training epoch-31 batch-42
Running loss of epoch-31 batch-42 = 5.84358349442482e-06

Training epoch-31 batch-43
Running loss of epoch-31 batch-43 = 7.289694622159004e-06

Training epoch-31 batch-44
Running loss of epoch-31 batch-44 = 6.023561581969261e-06

Training epoch-31 batch-45
Running loss of epoch-31 batch-45 = 6.204703822731972e-06

Training epoch-31 batch-46
Running loss of epoch-31 batch-46 = 5.330424755811691e-06

Training epoch-31 batch-47
Running loss of epoch-31 batch-47 = 3.5990960896015167e-06

Training epoch-31 batch-48
Running loss of epoch-31 batch-48 = 6.075482815504074e-06

Training epoch-31 batch-49
Running loss of epoch-31 batch-49 = 8.010072633624077e-06

Training epoch-31 batch-50
Running loss of epoch-31 batch-50 = 6.468966603279114e-06

Training epoch-31 batch-51
Running loss of epoch-31 batch-51 = 2.973480150103569e-06

Training epoch-31 batch-52
Running loss of epoch-31 batch-52 = 4.967674612998962e-06

Training epoch-31 batch-53
Running loss of epoch-31 batch-53 = 4.258006811141968e-06

Training epoch-31 batch-54
Running loss of epoch-31 batch-54 = 3.86405736207962e-06

Training epoch-31 batch-55
Running loss of epoch-31 batch-55 = 5.705282092094421e-06

Training epoch-31 batch-56
Running loss of epoch-31 batch-56 = 5.098292604088783e-06

Training epoch-31 batch-57
Running loss of epoch-31 batch-57 = 5.711102858185768e-06

Training epoch-31 batch-58
Running loss of epoch-31 batch-58 = 6.039626896381378e-06

Training epoch-31 batch-59
Running loss of epoch-31 batch-59 = 3.7071295082569122e-06

Training epoch-31 batch-60
Running loss of epoch-31 batch-60 = 6.198883056640625e-06

Training epoch-31 batch-61
Running loss of epoch-31 batch-61 = 7.387716323137283e-06

Training epoch-31 batch-62
Running loss of epoch-31 batch-62 = 4.7709327191114426e-06

Training epoch-31 batch-63
Running loss of epoch-31 batch-63 = 2.9262155294418335e-06

Training epoch-31 batch-64
Running loss of epoch-31 batch-64 = 6.775138899683952e-06

Training epoch-31 batch-65
Running loss of epoch-31 batch-65 = 5.461275577545166e-06

Training epoch-31 batch-66
Running loss of epoch-31 batch-66 = 7.737893611192703e-06

Training epoch-31 batch-67
Running loss of epoch-31 batch-67 = 6.88270665705204e-06

Training epoch-31 batch-68
Running loss of epoch-31 batch-68 = 4.49037179350853e-06

Training epoch-31 batch-69
Running loss of epoch-31 batch-69 = 8.101575076580048e-06

Training epoch-31 batch-70
Running loss of epoch-31 batch-70 = 5.886424332857132e-06

Training epoch-31 batch-71
Running loss of epoch-31 batch-71 = 4.627043381333351e-06

Training epoch-31 batch-72
Running loss of epoch-31 batch-72 = 5.699926987290382e-06

Training epoch-31 batch-73
Running loss of epoch-31 batch-73 = 7.63777643442154e-06

Training epoch-31 batch-74
Running loss of epoch-31 batch-74 = 6.355810910463333e-06

Training epoch-31 batch-75
Running loss of epoch-31 batch-75 = 5.44288195669651e-06

Training epoch-31 batch-76
Running loss of epoch-31 batch-76 = 7.408671081066132e-06

Training epoch-31 batch-77
Running loss of epoch-31 batch-77 = 6.43567182123661e-06

Training epoch-31 batch-78
Running loss of epoch-31 batch-78 = 5.817040801048279e-06

Training epoch-31 batch-79
Running loss of epoch-31 batch-79 = 7.233349606394768e-06

Training epoch-31 batch-80
Running loss of epoch-31 batch-80 = 7.03311525285244e-06

Training epoch-31 batch-81
Running loss of epoch-31 batch-81 = 8.494826033711433e-06

Training epoch-31 batch-82
Running loss of epoch-31 batch-82 = 5.2514951676130295e-06

Training epoch-31 batch-83
Running loss of epoch-31 batch-83 = 4.54694963991642e-06

Training epoch-31 batch-84
Running loss of epoch-31 batch-84 = 4.5567285269498825e-06

Training epoch-31 batch-85
Running loss of epoch-31 batch-85 = 5.100155249238014e-06

Training epoch-31 batch-86
Running loss of epoch-31 batch-86 = 5.309935659170151e-06

Training epoch-31 batch-87
Running loss of epoch-31 batch-87 = 7.001915946602821e-06

Training epoch-31 batch-88
Running loss of epoch-31 batch-88 = 6.0016755014657974e-06

Training epoch-31 batch-89
Running loss of epoch-31 batch-89 = 3.297114744782448e-06

Training epoch-31 batch-90
Running loss of epoch-31 batch-90 = 6.3998159021139145e-06

Training epoch-31 batch-91
Running loss of epoch-31 batch-91 = 9.078066796064377e-06

Training epoch-31 batch-92
Running loss of epoch-31 batch-92 = 5.495268851518631e-06

Training epoch-31 batch-93
Running loss of epoch-31 batch-93 = 4.842178896069527e-06

Training epoch-31 batch-94
Running loss of epoch-31 batch-94 = 6.5425410866737366e-06

Training epoch-31 batch-95
Running loss of epoch-31 batch-95 = 1.0139308869838715e-05

Training epoch-31 batch-96
Running loss of epoch-31 batch-96 = 9.55536961555481e-06

Training epoch-31 batch-97
Running loss of epoch-31 batch-97 = 6.29364512860775e-06

Training epoch-31 batch-98
Running loss of epoch-31 batch-98 = 6.636371836066246e-06

Training epoch-31 batch-99
Running loss of epoch-31 batch-99 = 8.690869435667992e-06

Training epoch-31 batch-100
Running loss of epoch-31 batch-100 = 6.4354389905929565e-06

Training epoch-31 batch-101
Running loss of epoch-31 batch-101 = 6.607966497540474e-06

Training epoch-31 batch-102
Running loss of epoch-31 batch-102 = 4.327856004238129e-06

Training epoch-31 batch-103
Running loss of epoch-31 batch-103 = 6.06081448495388e-06

Training epoch-31 batch-104
Running loss of epoch-31 batch-104 = 9.291572496294975e-06

Training epoch-31 batch-105
Running loss of epoch-31 batch-105 = 4.551606252789497e-06

Training epoch-31 batch-106
Running loss of epoch-31 batch-106 = 8.09389166533947e-06

Training epoch-31 batch-107
Running loss of epoch-31 batch-107 = 5.983281880617142e-06

Training epoch-31 batch-108
Running loss of epoch-31 batch-108 = 4.825415089726448e-06

Training epoch-31 batch-109
Running loss of epoch-31 batch-109 = 4.44403849542141e-06

Training epoch-31 batch-110
Running loss of epoch-31 batch-110 = 4.426576197147369e-06

Training epoch-31 batch-111
Running loss of epoch-31 batch-111 = 6.632180884480476e-06

Training epoch-31 batch-112
Running loss of epoch-31 batch-112 = 6.048241630196571e-06

Training epoch-31 batch-113
Running loss of epoch-31 batch-113 = 4.0619634091854095e-06

Training epoch-31 batch-114
Running loss of epoch-31 batch-114 = 3.746943548321724e-06

Training epoch-31 batch-115
Running loss of epoch-31 batch-115 = 4.650326445698738e-06

Training epoch-31 batch-116
Running loss of epoch-31 batch-116 = 6.935093551874161e-06

Training epoch-31 batch-117
Running loss of epoch-31 batch-117 = 7.316470146179199e-06

Training epoch-31 batch-118
Running loss of epoch-31 batch-118 = 6.317161023616791e-06

Training epoch-31 batch-119
Running loss of epoch-31 batch-119 = 4.341825842857361e-06

Training epoch-31 batch-120
Running loss of epoch-31 batch-120 = 5.2747782319784164e-06

Training epoch-31 batch-121
Running loss of epoch-31 batch-121 = 8.369563147425652e-06

Training epoch-31 batch-122
Running loss of epoch-31 batch-122 = 6.96512870490551e-06

Training epoch-31 batch-123
Running loss of epoch-31 batch-123 = 6.801914423704147e-06

Training epoch-31 batch-124
Running loss of epoch-31 batch-124 = 4.348577931523323e-06

Training epoch-31 batch-125
Running loss of epoch-31 batch-125 = 5.468260496854782e-06

Training epoch-31 batch-126
Running loss of epoch-31 batch-126 = 7.192371413111687e-06

Training epoch-31 batch-127
Running loss of epoch-31 batch-127 = 5.849171429872513e-06

Training epoch-31 batch-128
Running loss of epoch-31 batch-128 = 7.743947207927704e-06

Training epoch-31 batch-129
Running loss of epoch-31 batch-129 = 7.077818736433983e-06

Training epoch-31 batch-130
Running loss of epoch-31 batch-130 = 5.570473149418831e-06

Training epoch-31 batch-131
Running loss of epoch-31 batch-131 = 8.502276614308357e-06

Training epoch-31 batch-132
Running loss of epoch-31 batch-132 = 1.0727904736995697e-05

Training epoch-31 batch-133
Running loss of epoch-31 batch-133 = 4.180474206805229e-06

Training epoch-31 batch-134
Running loss of epoch-31 batch-134 = 6.2014441937208176e-06

Training epoch-31 batch-135
Running loss of epoch-31 batch-135 = 6.891787052154541e-06

Training epoch-31 batch-136
Running loss of epoch-31 batch-136 = 2.8728973120450974e-06

Training epoch-31 batch-137
Running loss of epoch-31 batch-137 = 6.9434754550457e-06

Training epoch-31 batch-138
Running loss of epoch-31 batch-138 = 5.661742761731148e-06

Training epoch-31 batch-139
Running loss of epoch-31 batch-139 = 4.770234227180481e-06

Training epoch-31 batch-140
Running loss of epoch-31 batch-140 = 4.307599738240242e-06

Training epoch-31 batch-141
Running loss of epoch-31 batch-141 = 7.261987775564194e-06

Training epoch-31 batch-142
Running loss of epoch-31 batch-142 = 5.4710544645786285e-06

Training epoch-31 batch-143
Running loss of epoch-31 batch-143 = 4.085013642907143e-06

Training epoch-31 batch-144
Running loss of epoch-31 batch-144 = 6.4854975789785385e-06

Training epoch-31 batch-145
Running loss of epoch-31 batch-145 = 4.991190508008003e-06

Training epoch-31 batch-146
Running loss of epoch-31 batch-146 = 3.7599820643663406e-06

Training epoch-31 batch-147
Running loss of epoch-31 batch-147 = 8.431030437350273e-06

Training epoch-31 batch-148
Running loss of epoch-31 batch-148 = 2.739951014518738e-06

Training epoch-31 batch-149
Running loss of epoch-31 batch-149 = 4.08245250582695e-06

Training epoch-31 batch-150
Running loss of epoch-31 batch-150 = 3.4086406230926514e-06

Training epoch-31 batch-151
Running loss of epoch-31 batch-151 = 4.900386556982994e-06

Training epoch-31 batch-152
Running loss of epoch-31 batch-152 = 6.212852895259857e-06

Training epoch-31 batch-153
Running loss of epoch-31 batch-153 = 3.4046825021505356e-06

Training epoch-31 batch-154
Running loss of epoch-31 batch-154 = 6.109941750764847e-06

Training epoch-31 batch-155
Running loss of epoch-31 batch-155 = 7.287017069756985e-06

Training epoch-31 batch-156
Running loss of epoch-31 batch-156 = 4.83216717839241e-06

Training epoch-31 batch-157
Running loss of epoch-31 batch-157 = 2.8535723686218262e-05

Finished training epoch-31.



Average train loss at epoch-31 = 6.000440567731857e-06

Started Evaluation

Average val loss at epoch-31 = 2.948250916443373

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.11 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 39.73 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 26.91 %
Accuracy for class get is: 36.67 %

Overall Accuracy = 54.59 %

Finished Evaluation



Started training epoch-32


Training epoch-32 batch-1
Running loss of epoch-32 batch-1 = 6.193993613123894e-06

Training epoch-32 batch-2
Running loss of epoch-32 batch-2 = 2.900604158639908e-06

Training epoch-32 batch-3
Running loss of epoch-32 batch-3 = 7.616588845849037e-06

Training epoch-32 batch-4
Running loss of epoch-32 batch-4 = 5.676876753568649e-06

Training epoch-32 batch-5
Running loss of epoch-32 batch-5 = 4.659406840801239e-06

Training epoch-32 batch-6
Running loss of epoch-32 batch-6 = 5.951151251792908e-06

Training epoch-32 batch-7
Running loss of epoch-32 batch-7 = 7.0193782448768616e-06

Training epoch-32 batch-8
Running loss of epoch-32 batch-8 = 4.577916115522385e-06

Training epoch-32 batch-9
Running loss of epoch-32 batch-9 = 8.502975106239319e-06

Training epoch-32 batch-10
Running loss of epoch-32 batch-10 = 6.95488415658474e-06

Training epoch-32 batch-11
Running loss of epoch-32 batch-11 = 5.7569704949855804e-06

Training epoch-32 batch-12
Running loss of epoch-32 batch-12 = 6.729969754815102e-06

Training epoch-32 batch-13
Running loss of epoch-32 batch-13 = 4.532979801297188e-06

Training epoch-32 batch-14
Running loss of epoch-32 batch-14 = 4.5138876885175705e-06

Training epoch-32 batch-15
Running loss of epoch-32 batch-15 = 7.054070010781288e-06

Training epoch-32 batch-16
Running loss of epoch-32 batch-16 = 5.550449714064598e-06

Training epoch-32 batch-17
Running loss of epoch-32 batch-17 = 5.24311326444149e-06

Training epoch-32 batch-18
Running loss of epoch-32 batch-18 = 7.184920832514763e-06

Training epoch-32 batch-19
Running loss of epoch-32 batch-19 = 8.366070687770844e-06

Training epoch-32 batch-20
Running loss of epoch-32 batch-20 = 8.840812370181084e-06

Training epoch-32 batch-21
Running loss of epoch-32 batch-21 = 7.291324436664581e-06

Training epoch-32 batch-22
Running loss of epoch-32 batch-22 = 6.566056981682777e-06

Training epoch-32 batch-23
Running loss of epoch-32 batch-23 = 4.7336798161268234e-06

Training epoch-32 batch-24
Running loss of epoch-32 batch-24 = 6.0319434851408005e-06

Training epoch-32 batch-25
Running loss of epoch-32 batch-25 = 4.512956365942955e-06

Training epoch-32 batch-26
Running loss of epoch-32 batch-26 = 6.695510819554329e-06

Training epoch-32 batch-27
Running loss of epoch-32 batch-27 = 5.87967224419117e-06

Training epoch-32 batch-28
Running loss of epoch-32 batch-28 = 2.369750291109085e-06

Training epoch-32 batch-29
Running loss of epoch-32 batch-29 = 7.385155186057091e-06

Training epoch-32 batch-30
Running loss of epoch-32 batch-30 = 5.5551063269376755e-06

Training epoch-32 batch-31
Running loss of epoch-32 batch-31 = 6.632646545767784e-06

Training epoch-32 batch-32
Running loss of epoch-32 batch-32 = 4.6354252845048904e-06

Training epoch-32 batch-33
Running loss of epoch-32 batch-33 = 4.9120280891656876e-06

Training epoch-32 batch-34
Running loss of epoch-32 batch-34 = 5.577923730015755e-06

Training epoch-32 batch-35
Running loss of epoch-32 batch-35 = 6.965361535549164e-06

Training epoch-32 batch-36
Running loss of epoch-32 batch-36 = 4.112720489501953e-06

Training epoch-32 batch-37
Running loss of epoch-32 batch-37 = 6.580492481589317e-06

Training epoch-32 batch-38
Running loss of epoch-32 batch-38 = 3.420049324631691e-06

Training epoch-32 batch-39
Running loss of epoch-32 batch-39 = 3.8044527173042297e-06

Training epoch-32 batch-40
Running loss of epoch-32 batch-40 = 2.7467031031847e-06

Training epoch-32 batch-41
Running loss of epoch-32 batch-41 = 5.337642505764961e-06

Training epoch-32 batch-42
Running loss of epoch-32 batch-42 = 6.438465788960457e-06

Training epoch-32 batch-43
Running loss of epoch-32 batch-43 = 5.221692845225334e-06

Training epoch-32 batch-44
Running loss of epoch-32 batch-44 = 6.018904969096184e-06

Training epoch-32 batch-45
Running loss of epoch-32 batch-45 = 7.689581252634525e-06

Training epoch-32 batch-46
Running loss of epoch-32 batch-46 = 6.070826202630997e-06

Training epoch-32 batch-47
Running loss of epoch-32 batch-47 = 4.269648343324661e-06

Training epoch-32 batch-48
Running loss of epoch-32 batch-48 = 8.462229743599892e-06

Training epoch-32 batch-49
Running loss of epoch-32 batch-49 = 6.033340469002724e-06

Training epoch-32 batch-50
Running loss of epoch-32 batch-50 = 5.149282515048981e-06

Training epoch-32 batch-51
Running loss of epoch-32 batch-51 = 7.471069693565369e-06

Training epoch-32 batch-52
Running loss of epoch-32 batch-52 = 4.870584234595299e-06

Training epoch-32 batch-53
Running loss of epoch-32 batch-53 = 6.898771971464157e-06

Training epoch-32 batch-54
Running loss of epoch-32 batch-54 = 7.450580596923828e-06

Training epoch-32 batch-55
Running loss of epoch-32 batch-55 = 3.937864676117897e-06

Training epoch-32 batch-56
Running loss of epoch-32 batch-56 = 6.218906491994858e-06

Training epoch-32 batch-57
Running loss of epoch-32 batch-57 = 5.378853529691696e-06

Training epoch-32 batch-58
Running loss of epoch-32 batch-58 = 7.214723154902458e-06

Training epoch-32 batch-59
Running loss of epoch-32 batch-59 = 6.123911589384079e-06

Training epoch-32 batch-60
Running loss of epoch-32 batch-60 = 5.319016054272652e-06

Training epoch-32 batch-61
Running loss of epoch-32 batch-61 = 6.190035492181778e-06

Training epoch-32 batch-62
Running loss of epoch-32 batch-62 = 7.73346982896328e-06

Training epoch-32 batch-63
Running loss of epoch-32 batch-63 = 4.116445779800415e-06

Training epoch-32 batch-64
Running loss of epoch-32 batch-64 = 7.334165275096893e-06

Training epoch-32 batch-65
Running loss of epoch-32 batch-65 = 5.602836608886719e-06

Training epoch-32 batch-66
Running loss of epoch-32 batch-66 = 4.517147317528725e-06

Training epoch-32 batch-67
Running loss of epoch-32 batch-67 = 4.919013008475304e-06

Training epoch-32 batch-68
Running loss of epoch-32 batch-68 = 5.400041118264198e-06

Training epoch-32 batch-69
Running loss of epoch-32 batch-69 = 5.990965291857719e-06

Training epoch-32 batch-70
Running loss of epoch-32 batch-70 = 3.6284327507019043e-06

Training epoch-32 batch-71
Running loss of epoch-32 batch-71 = 7.89412297308445e-06

Training epoch-32 batch-72
Running loss of epoch-32 batch-72 = 6.851041689515114e-06

Training epoch-32 batch-73
Running loss of epoch-32 batch-73 = 5.370937287807465e-06

Training epoch-32 batch-74
Running loss of epoch-32 batch-74 = 3.223773092031479e-06

Training epoch-32 batch-75
Running loss of epoch-32 batch-75 = 4.757661372423172e-06

Training epoch-32 batch-76
Running loss of epoch-32 batch-76 = 6.230780854821205e-06

Training epoch-32 batch-77
Running loss of epoch-32 batch-77 = 6.139744073152542e-06

Training epoch-32 batch-78
Running loss of epoch-32 batch-78 = 4.498288035392761e-06

Training epoch-32 batch-79
Running loss of epoch-32 batch-79 = 7.553026080131531e-06

Training epoch-32 batch-80
Running loss of epoch-32 batch-80 = 1.0116957128047943e-05

Training epoch-32 batch-81
Running loss of epoch-32 batch-81 = 6.831483915448189e-06

Training epoch-32 batch-82
Running loss of epoch-32 batch-82 = 6.523914635181427e-06

Training epoch-32 batch-83
Running loss of epoch-32 batch-83 = 5.105510354042053e-06

Training epoch-32 batch-84
Running loss of epoch-32 batch-84 = 6.895745173096657e-06

Training epoch-32 batch-85
Running loss of epoch-32 batch-85 = 4.862667992711067e-06

Training epoch-32 batch-86
Running loss of epoch-32 batch-86 = 5.798414349555969e-06

Training epoch-32 batch-87
Running loss of epoch-32 batch-87 = 5.389796569943428e-06

Training epoch-32 batch-88
Running loss of epoch-32 batch-88 = 4.405854269862175e-06

Training epoch-32 batch-89
Running loss of epoch-32 batch-89 = 5.558133125305176e-06

Training epoch-32 batch-90
Running loss of epoch-32 batch-90 = 3.1457748264074326e-06

Training epoch-32 batch-91
Running loss of epoch-32 batch-91 = 6.144167855381966e-06

Training epoch-32 batch-92
Running loss of epoch-32 batch-92 = 7.809139788150787e-06

Training epoch-32 batch-93
Running loss of epoch-32 batch-93 = 5.181413143873215e-06

Training epoch-32 batch-94
Running loss of epoch-32 batch-94 = 4.095258191227913e-06

Training epoch-32 batch-95
Running loss of epoch-32 batch-95 = 4.038214683532715e-06

Training epoch-32 batch-96
Running loss of epoch-32 batch-96 = 6.356276571750641e-06

Training epoch-32 batch-97
Running loss of epoch-32 batch-97 = 6.138114258646965e-06

Training epoch-32 batch-98
Running loss of epoch-32 batch-98 = 5.0943344831466675e-06

Training epoch-32 batch-99
Running loss of epoch-32 batch-99 = 6.529735401272774e-06

Training epoch-32 batch-100
Running loss of epoch-32 batch-100 = 5.183275789022446e-06

Training epoch-32 batch-101
Running loss of epoch-32 batch-101 = 6.047077476978302e-06

Training epoch-32 batch-102
Running loss of epoch-32 batch-102 = 5.705514922738075e-06

Training epoch-32 batch-103
Running loss of epoch-32 batch-103 = 7.5821299105882645e-06

Training epoch-32 batch-104
Running loss of epoch-32 batch-104 = 5.0175003707408905e-06

Training epoch-32 batch-105
Running loss of epoch-32 batch-105 = 5.015404894948006e-06

Training epoch-32 batch-106
Running loss of epoch-32 batch-106 = 3.402121365070343e-06

Training epoch-32 batch-107
Running loss of epoch-32 batch-107 = 5.888286978006363e-06

Training epoch-32 batch-108
Running loss of epoch-32 batch-108 = 5.406327545642853e-06

Training epoch-32 batch-109
Running loss of epoch-32 batch-109 = 4.992121830582619e-06

Training epoch-32 batch-110
Running loss of epoch-32 batch-110 = 4.065921530127525e-06

Training epoch-32 batch-111
Running loss of epoch-32 batch-111 = 6.5141357481479645e-06

Training epoch-32 batch-112
Running loss of epoch-32 batch-112 = 5.088280886411667e-06

Training epoch-32 batch-113
Running loss of epoch-32 batch-113 = 5.5944547057151794e-06

Training epoch-32 batch-114
Running loss of epoch-32 batch-114 = 4.529254510998726e-06

Training epoch-32 batch-115
Running loss of epoch-32 batch-115 = 3.810971975326538e-06

Training epoch-32 batch-116
Running loss of epoch-32 batch-116 = 4.438217729330063e-06

Training epoch-32 batch-117
Running loss of epoch-32 batch-117 = 5.666865035891533e-06

Training epoch-32 batch-118
Running loss of epoch-32 batch-118 = 7.028225809335709e-06

Training epoch-32 batch-119
Running loss of epoch-32 batch-119 = 5.152774974703789e-06

Training epoch-32 batch-120
Running loss of epoch-32 batch-120 = 7.216120138764381e-06

Training epoch-32 batch-121
Running loss of epoch-32 batch-121 = 4.988629370927811e-06

Training epoch-32 batch-122
Running loss of epoch-32 batch-122 = 6.106449291110039e-06

Training epoch-32 batch-123
Running loss of epoch-32 batch-123 = 5.593756213784218e-06

Training epoch-32 batch-124
Running loss of epoch-32 batch-124 = 5.986308678984642e-06

Training epoch-32 batch-125
Running loss of epoch-32 batch-125 = 4.804693162441254e-06

Training epoch-32 batch-126
Running loss of epoch-32 batch-126 = 5.3727999329566956e-06

Training epoch-32 batch-127
Running loss of epoch-32 batch-127 = 5.426118150353432e-06

Training epoch-32 batch-128
Running loss of epoch-32 batch-128 = 5.155568942427635e-06

Training epoch-32 batch-129
Running loss of epoch-32 batch-129 = 9.601004421710968e-06

Training epoch-32 batch-130
Running loss of epoch-32 batch-130 = 3.8028229027986526e-06

Training epoch-32 batch-131
Running loss of epoch-32 batch-131 = 3.6659184843301773e-06

Training epoch-32 batch-132
Running loss of epoch-32 batch-132 = 5.220761522650719e-06

Training epoch-32 batch-133
Running loss of epoch-32 batch-133 = 4.048459231853485e-06

Training epoch-32 batch-134
Running loss of epoch-32 batch-134 = 6.77257776260376e-06

Training epoch-32 batch-135
Running loss of epoch-32 batch-135 = 5.530426278710365e-06

Training epoch-32 batch-136
Running loss of epoch-32 batch-136 = 6.7730434238910675e-06

Training epoch-32 batch-137
Running loss of epoch-32 batch-137 = 6.563728675246239e-06

Training epoch-32 batch-138
Running loss of epoch-32 batch-138 = 4.670815542340279e-06

Training epoch-32 batch-139
Running loss of epoch-32 batch-139 = 4.429137334227562e-06

Training epoch-32 batch-140
Running loss of epoch-32 batch-140 = 3.1937379390001297e-06

Training epoch-32 batch-141
Running loss of epoch-32 batch-141 = 4.179775714874268e-06

Training epoch-32 batch-142
Running loss of epoch-32 batch-142 = 3.4815166145563126e-06

Training epoch-32 batch-143
Running loss of epoch-32 batch-143 = 7.17933289706707e-06

Training epoch-32 batch-144
Running loss of epoch-32 batch-144 = 5.225883796811104e-06

Training epoch-32 batch-145
Running loss of epoch-32 batch-145 = 4.59025613963604e-06

Training epoch-32 batch-146
Running loss of epoch-32 batch-146 = 4.320638254284859e-06

Training epoch-32 batch-147
Running loss of epoch-32 batch-147 = 5.75091689825058e-06

Training epoch-32 batch-148
Running loss of epoch-32 batch-148 = 3.5224948078393936e-06

Training epoch-32 batch-149
Running loss of epoch-32 batch-149 = 4.602363333106041e-06

Training epoch-32 batch-150
Running loss of epoch-32 batch-150 = 7.177004590630531e-06

Training epoch-32 batch-151
Running loss of epoch-32 batch-151 = 1.0245712473988533e-05

Training epoch-32 batch-152
Running loss of epoch-32 batch-152 = 6.146496161818504e-06

Training epoch-32 batch-153
Running loss of epoch-32 batch-153 = 3.375578671693802e-06

Training epoch-32 batch-154
Running loss of epoch-32 batch-154 = 4.400266334414482e-06

Training epoch-32 batch-155
Running loss of epoch-32 batch-155 = 3.4528784453868866e-06

Training epoch-32 batch-156
Running loss of epoch-32 batch-156 = 6.815651431679726e-06

Training epoch-32 batch-157
Running loss of epoch-32 batch-157 = 3.664568066596985e-05

Finished training epoch-32.



Average train loss at epoch-32 = 5.6989736855030056e-06

Started Evaluation

Average val loss at epoch-32 = 2.9580828233769068

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 62.79 %
Accuracy for class onCreate is: 57.14 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.21 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.41 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-33


Training epoch-33 batch-1
Running loss of epoch-33 batch-1 = 3.5599805414676666e-06

Training epoch-33 batch-2
Running loss of epoch-33 batch-2 = 4.958361387252808e-06

Training epoch-33 batch-3
Running loss of epoch-33 batch-3 = 5.404464900493622e-06

Training epoch-33 batch-4
Running loss of epoch-33 batch-4 = 5.623791366815567e-06

Training epoch-33 batch-5
Running loss of epoch-33 batch-5 = 5.982117727398872e-06

Training epoch-33 batch-6
Running loss of epoch-33 batch-6 = 4.424015060067177e-06

Training epoch-33 batch-7
Running loss of epoch-33 batch-7 = 6.5069179981946945e-06

Training epoch-33 batch-8
Running loss of epoch-33 batch-8 = 6.5267086029052734e-06

Training epoch-33 batch-9
Running loss of epoch-33 batch-9 = 4.652421921491623e-06

Training epoch-33 batch-10
Running loss of epoch-33 batch-10 = 6.232643499970436e-06

Training epoch-33 batch-11
Running loss of epoch-33 batch-11 = 4.4424086809158325e-06

Training epoch-33 batch-12
Running loss of epoch-33 batch-12 = 5.402369424700737e-06

Training epoch-33 batch-13
Running loss of epoch-33 batch-13 = 6.790738552808762e-06

Training epoch-33 batch-14
Running loss of epoch-33 batch-14 = 9.10717062652111e-06

Training epoch-33 batch-15
Running loss of epoch-33 batch-15 = 4.741596058011055e-06

Training epoch-33 batch-16
Running loss of epoch-33 batch-16 = 4.757661372423172e-06

Training epoch-33 batch-17
Running loss of epoch-33 batch-17 = 3.998167812824249e-06

Training epoch-33 batch-18
Running loss of epoch-33 batch-18 = 7.288996130228043e-06

Training epoch-33 batch-19
Running loss of epoch-33 batch-19 = 5.5497512221336365e-06

Training epoch-33 batch-20
Running loss of epoch-33 batch-20 = 5.346490070223808e-06

Training epoch-33 batch-21
Running loss of epoch-33 batch-21 = 5.966285243630409e-06

Training epoch-33 batch-22
Running loss of epoch-33 batch-22 = 7.513212040066719e-06

Training epoch-33 batch-23
Running loss of epoch-33 batch-23 = 8.896226063370705e-06

Training epoch-33 batch-24
Running loss of epoch-33 batch-24 = 5.541602149605751e-06

Training epoch-33 batch-25
Running loss of epoch-33 batch-25 = 4.5138876885175705e-06

Training epoch-33 batch-26
Running loss of epoch-33 batch-26 = 4.291534423828125e-06

Training epoch-33 batch-27
Running loss of epoch-33 batch-27 = 5.741138011217117e-06

Training epoch-33 batch-28
Running loss of epoch-33 batch-28 = 4.270114004611969e-06

Training epoch-33 batch-29
Running loss of epoch-33 batch-29 = 5.1835086196660995e-06

Training epoch-33 batch-30
Running loss of epoch-33 batch-30 = 3.073364496231079e-06

Training epoch-33 batch-31
Running loss of epoch-33 batch-31 = 7.081544026732445e-06

Training epoch-33 batch-32
Running loss of epoch-33 batch-32 = 5.520880222320557e-06

Training epoch-33 batch-33
Running loss of epoch-33 batch-33 = 4.43262979388237e-06

Training epoch-33 batch-34
Running loss of epoch-33 batch-34 = 4.488276317715645e-06

Training epoch-33 batch-35
Running loss of epoch-33 batch-35 = 9.081093594431877e-06

Training epoch-33 batch-36
Running loss of epoch-33 batch-36 = 4.495028406381607e-06

Training epoch-33 batch-37
Running loss of epoch-33 batch-37 = 7.510185241699219e-06

Training epoch-33 batch-38
Running loss of epoch-33 batch-38 = 5.566282197833061e-06

Training epoch-33 batch-39
Running loss of epoch-33 batch-39 = 3.6261044442653656e-06

Training epoch-33 batch-40
Running loss of epoch-33 batch-40 = 4.475237801671028e-06

Training epoch-33 batch-41
Running loss of epoch-33 batch-41 = 5.853595212101936e-06

Training epoch-33 batch-42
Running loss of epoch-33 batch-42 = 3.895489498972893e-06

Training epoch-33 batch-43
Running loss of epoch-33 batch-43 = 5.964189767837524e-06

Training epoch-33 batch-44
Running loss of epoch-33 batch-44 = 5.502486601471901e-06

Training epoch-33 batch-45
Running loss of epoch-33 batch-45 = 2.8030481189489365e-06

Training epoch-33 batch-46
Running loss of epoch-33 batch-46 = 6.391666829586029e-06

Training epoch-33 batch-47
Running loss of epoch-33 batch-47 = 4.328787326812744e-06

Training epoch-33 batch-48
Running loss of epoch-33 batch-48 = 4.0282029658555984e-06

Training epoch-33 batch-49
Running loss of epoch-33 batch-49 = 6.878748536109924e-06

Training epoch-33 batch-50
Running loss of epoch-33 batch-50 = 6.005633622407913e-06

Training epoch-33 batch-51
Running loss of epoch-33 batch-51 = 4.556262865662575e-06

Training epoch-33 batch-52
Running loss of epoch-33 batch-52 = 4.647299647331238e-06

Training epoch-33 batch-53
Running loss of epoch-33 batch-53 = 7.491791620850563e-06

Training epoch-33 batch-54
Running loss of epoch-33 batch-54 = 7.3802657425403595e-06

Training epoch-33 batch-55
Running loss of epoch-33 batch-55 = 6.143003702163696e-06

Training epoch-33 batch-56
Running loss of epoch-33 batch-56 = 7.285969331860542e-06

Training epoch-33 batch-57
Running loss of epoch-33 batch-57 = 5.105743184685707e-06

Training epoch-33 batch-58
Running loss of epoch-33 batch-58 = 5.236361175775528e-06

Training epoch-33 batch-59
Running loss of epoch-33 batch-59 = 5.197711288928986e-06

Training epoch-33 batch-60
Running loss of epoch-33 batch-60 = 4.082685336470604e-06

Training epoch-33 batch-61
Running loss of epoch-33 batch-61 = 3.995141014456749e-06

Training epoch-33 batch-62
Running loss of epoch-33 batch-62 = 3.5883858799934387e-06

Training epoch-33 batch-63
Running loss of epoch-33 batch-63 = 3.4910626709461212e-06

Training epoch-33 batch-64
Running loss of epoch-33 batch-64 = 4.033790901303291e-06

Training epoch-33 batch-65
Running loss of epoch-33 batch-65 = 5.3711701184511185e-06

Training epoch-33 batch-66
Running loss of epoch-33 batch-66 = 6.214482709765434e-06

Training epoch-33 batch-67
Running loss of epoch-33 batch-67 = 6.948830559849739e-06

Training epoch-33 batch-68
Running loss of epoch-33 batch-68 = 7.863156497478485e-06

Training epoch-33 batch-69
Running loss of epoch-33 batch-69 = 4.1478779166936874e-06

Training epoch-33 batch-70
Running loss of epoch-33 batch-70 = 4.586530849337578e-06

Training epoch-33 batch-71
Running loss of epoch-33 batch-71 = 5.71180135011673e-06

Training epoch-33 batch-72
Running loss of epoch-33 batch-72 = 4.269415512681007e-06

Training epoch-33 batch-73
Running loss of epoch-33 batch-73 = 4.824250936508179e-06

Training epoch-33 batch-74
Running loss of epoch-33 batch-74 = 7.204944267868996e-06

Training epoch-33 batch-75
Running loss of epoch-33 batch-75 = 4.677567631006241e-06

Training epoch-33 batch-76
Running loss of epoch-33 batch-76 = 4.25032339990139e-06

Training epoch-33 batch-77
Running loss of epoch-33 batch-77 = 3.686174750328064e-06

Training epoch-33 batch-78
Running loss of epoch-33 batch-78 = 4.824250936508179e-06

Training epoch-33 batch-79
Running loss of epoch-33 batch-79 = 6.305752322077751e-06

Training epoch-33 batch-80
Running loss of epoch-33 batch-80 = 3.8133002817630768e-06

Training epoch-33 batch-81
Running loss of epoch-33 batch-81 = 5.841720849275589e-06

Training epoch-33 batch-82
Running loss of epoch-33 batch-82 = 3.809574991464615e-06

Training epoch-33 batch-83
Running loss of epoch-33 batch-83 = 5.2105169743299484e-06

Training epoch-33 batch-84
Running loss of epoch-33 batch-84 = 9.241979569196701e-06

Training epoch-33 batch-85
Running loss of epoch-33 batch-85 = 5.145557224750519e-06

Training epoch-33 batch-86
Running loss of epoch-33 batch-86 = 4.447298124432564e-06

Training epoch-33 batch-87
Running loss of epoch-33 batch-87 = 3.766501322388649e-06

Training epoch-33 batch-88
Running loss of epoch-33 batch-88 = 4.951609298586845e-06

Training epoch-33 batch-89
Running loss of epoch-33 batch-89 = 7.652444764971733e-06

Training epoch-33 batch-90
Running loss of epoch-33 batch-90 = 4.578381776809692e-06

Training epoch-33 batch-91
Running loss of epoch-33 batch-91 = 7.213791832327843e-06

Training epoch-33 batch-92
Running loss of epoch-33 batch-92 = 5.177222192287445e-06

Training epoch-33 batch-93
Running loss of epoch-33 batch-93 = 5.263602361083031e-06

Training epoch-33 batch-94
Running loss of epoch-33 batch-94 = 4.909234121441841e-06

Training epoch-33 batch-95
Running loss of epoch-33 batch-95 = 5.376292392611504e-06

Training epoch-33 batch-96
Running loss of epoch-33 batch-96 = 4.3709296733140945e-06

Training epoch-33 batch-97
Running loss of epoch-33 batch-97 = 3.947177901864052e-06

Training epoch-33 batch-98
Running loss of epoch-33 batch-98 = 4.1548628360033035e-06

Training epoch-33 batch-99
Running loss of epoch-33 batch-99 = 6.109708920121193e-06

Training epoch-33 batch-100
Running loss of epoch-33 batch-100 = 6.3818879425525665e-06

Training epoch-33 batch-101
Running loss of epoch-33 batch-101 = 6.369780749082565e-06

Training epoch-33 batch-102
Running loss of epoch-33 batch-102 = 5.847541615366936e-06

Training epoch-33 batch-103
Running loss of epoch-33 batch-103 = 5.750218406319618e-06

Training epoch-33 batch-104
Running loss of epoch-33 batch-104 = 4.678033292293549e-06

Training epoch-33 batch-105
Running loss of epoch-33 batch-105 = 4.920875653624535e-06

Training epoch-33 batch-106
Running loss of epoch-33 batch-106 = 6.1141327023506165e-06

Training epoch-33 batch-107
Running loss of epoch-33 batch-107 = 5.526002496480942e-06

Training epoch-33 batch-108
Running loss of epoch-33 batch-108 = 4.719709977507591e-06

Training epoch-33 batch-109
Running loss of epoch-33 batch-109 = 5.643116310238838e-06

Training epoch-33 batch-110
Running loss of epoch-33 batch-110 = 4.709232598543167e-06

Training epoch-33 batch-111
Running loss of epoch-33 batch-111 = 4.015164449810982e-06

Training epoch-33 batch-112
Running loss of epoch-33 batch-112 = 5.858484655618668e-06

Training epoch-33 batch-113
Running loss of epoch-33 batch-113 = 3.973720595240593e-06

Training epoch-33 batch-114
Running loss of epoch-33 batch-114 = 5.9355515986680984e-06

Training epoch-33 batch-115
Running loss of epoch-33 batch-115 = 7.188413292169571e-06

Training epoch-33 batch-116
Running loss of epoch-33 batch-116 = 5.136243999004364e-06

Training epoch-33 batch-117
Running loss of epoch-33 batch-117 = 4.552770406007767e-06

Training epoch-33 batch-118
Running loss of epoch-33 batch-118 = 3.339489921927452e-06

Training epoch-33 batch-119
Running loss of epoch-33 batch-119 = 5.07524237036705e-06

Training epoch-33 batch-120
Running loss of epoch-33 batch-120 = 6.66850246489048e-06

Training epoch-33 batch-121
Running loss of epoch-33 batch-121 = 5.602836608886719e-06

Training epoch-33 batch-122
Running loss of epoch-33 batch-122 = 4.87128272652626e-06

Training epoch-33 batch-123
Running loss of epoch-33 batch-123 = 3.833789378404617e-06

Training epoch-33 batch-124
Running loss of epoch-33 batch-124 = 8.212635293602943e-06

Training epoch-33 batch-125
Running loss of epoch-33 batch-125 = 6.240559741854668e-06

Training epoch-33 batch-126
Running loss of epoch-33 batch-126 = 6.875256076455116e-06

Training epoch-33 batch-127
Running loss of epoch-33 batch-127 = 5.468493327498436e-06

Training epoch-33 batch-128
Running loss of epoch-33 batch-128 = 4.261033609509468e-06

Training epoch-33 batch-129
Running loss of epoch-33 batch-129 = 3.6712735891342163e-06

Training epoch-33 batch-130
Running loss of epoch-33 batch-130 = 6.453599780797958e-06

Training epoch-33 batch-131
Running loss of epoch-33 batch-131 = 3.536464646458626e-06

Training epoch-33 batch-132
Running loss of epoch-33 batch-132 = 5.701091140508652e-06

Training epoch-33 batch-133
Running loss of epoch-33 batch-133 = 3.6461278796195984e-06

Training epoch-33 batch-134
Running loss of epoch-33 batch-134 = 5.5015552788972855e-06

Training epoch-33 batch-135
Running loss of epoch-33 batch-135 = 4.756264388561249e-06

Training epoch-33 batch-136
Running loss of epoch-33 batch-136 = 4.568137228488922e-06

Training epoch-33 batch-137
Running loss of epoch-33 batch-137 = 4.331115633249283e-06

Training epoch-33 batch-138
Running loss of epoch-33 batch-138 = 7.4678100645542145e-06

Training epoch-33 batch-139
Running loss of epoch-33 batch-139 = 5.887588486075401e-06

Training epoch-33 batch-140
Running loss of epoch-33 batch-140 = 3.949971869587898e-06

Training epoch-33 batch-141
Running loss of epoch-33 batch-141 = 5.173962563276291e-06

Training epoch-33 batch-142
Running loss of epoch-33 batch-142 = 5.2675604820251465e-06

Training epoch-33 batch-143
Running loss of epoch-33 batch-143 = 4.277331754565239e-06

Training epoch-33 batch-144
Running loss of epoch-33 batch-144 = 4.723435267806053e-06

Training epoch-33 batch-145
Running loss of epoch-33 batch-145 = 6.527174264192581e-06

Training epoch-33 batch-146
Running loss of epoch-33 batch-146 = 4.246830940246582e-06

Training epoch-33 batch-147
Running loss of epoch-33 batch-147 = 5.807960405945778e-06

Training epoch-33 batch-148
Running loss of epoch-33 batch-148 = 4.330184310674667e-06

Training epoch-33 batch-149
Running loss of epoch-33 batch-149 = 9.188894182443619e-06

Training epoch-33 batch-150
Running loss of epoch-33 batch-150 = 6.437767297029495e-06

Training epoch-33 batch-151
Running loss of epoch-33 batch-151 = 3.6889687180519104e-06

Training epoch-33 batch-152
Running loss of epoch-33 batch-152 = 7.716938853263855e-06

Training epoch-33 batch-153
Running loss of epoch-33 batch-153 = 4.5388005673885345e-06

Training epoch-33 batch-154
Running loss of epoch-33 batch-154 = 4.681991413235664e-06

Training epoch-33 batch-155
Running loss of epoch-33 batch-155 = 6.248708814382553e-06

Training epoch-33 batch-156
Running loss of epoch-33 batch-156 = 5.220295861363411e-06

Training epoch-33 batch-157
Running loss of epoch-33 batch-157 = 2.166256308555603e-05

Finished training epoch-33.



Average train loss at epoch-33 = 5.395545065402985e-06

Started Evaluation

Average val loss at epoch-33 = 2.9695131166985163

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.59 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 39.27 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.63 %

Finished Evaluation



Started training epoch-34


Training epoch-34 batch-1
Running loss of epoch-34 batch-1 = 7.164431735873222e-06

Training epoch-34 batch-2
Running loss of epoch-34 batch-2 = 7.813563570380211e-06

Training epoch-34 batch-3
Running loss of epoch-34 batch-3 = 9.167240932583809e-06

Training epoch-34 batch-4
Running loss of epoch-34 batch-4 = 3.5588163882493973e-06

Training epoch-34 batch-5
Running loss of epoch-34 batch-5 = 3.411434590816498e-06

Training epoch-34 batch-6
Running loss of epoch-34 batch-6 = 5.066627636551857e-06

Training epoch-34 batch-7
Running loss of epoch-34 batch-7 = 6.058951839804649e-06

Training epoch-34 batch-8
Running loss of epoch-34 batch-8 = 5.5693089962005615e-06

Training epoch-34 batch-9
Running loss of epoch-34 batch-9 = 4.960922524333e-06

Training epoch-34 batch-10
Running loss of epoch-34 batch-10 = 5.83636574447155e-06

Training epoch-34 batch-11
Running loss of epoch-34 batch-11 = 3.29664908349514e-06

Training epoch-34 batch-12
Running loss of epoch-34 batch-12 = 4.701782017946243e-06

Training epoch-34 batch-13
Running loss of epoch-34 batch-13 = 3.583962097764015e-06

Training epoch-34 batch-14
Running loss of epoch-34 batch-14 = 8.760252967476845e-06

Training epoch-34 batch-15
Running loss of epoch-34 batch-15 = 7.798196747899055e-06

Training epoch-34 batch-16
Running loss of epoch-34 batch-16 = 5.73648139834404e-06

Training epoch-34 batch-17
Running loss of epoch-34 batch-17 = 7.134862244129181e-06

Training epoch-34 batch-18
Running loss of epoch-34 batch-18 = 4.841946065425873e-06

Training epoch-34 batch-19
Running loss of epoch-34 batch-19 = 6.080372259020805e-06

Training epoch-34 batch-20
Running loss of epoch-34 batch-20 = 6.038462743163109e-06

Training epoch-34 batch-21
Running loss of epoch-34 batch-21 = 3.80747951567173e-06

Training epoch-34 batch-22
Running loss of epoch-34 batch-22 = 4.970701411366463e-06

Training epoch-34 batch-23
Running loss of epoch-34 batch-23 = 4.406087100505829e-06

Training epoch-34 batch-24
Running loss of epoch-34 batch-24 = 5.633337423205376e-06

Training epoch-34 batch-25
Running loss of epoch-34 batch-25 = 7.19074159860611e-06

Training epoch-34 batch-26
Running loss of epoch-34 batch-26 = 6.613787263631821e-06

Training epoch-34 batch-27
Running loss of epoch-34 batch-27 = 3.4745316952466965e-06

Training epoch-34 batch-28
Running loss of epoch-34 batch-28 = 3.919471055269241e-06

Training epoch-34 batch-29
Running loss of epoch-34 batch-29 = 4.35439869761467e-06

Training epoch-34 batch-30
Running loss of epoch-34 batch-30 = 4.833796992897987e-06

Training epoch-34 batch-31
Running loss of epoch-34 batch-31 = 6.239628419280052e-06

Training epoch-34 batch-32
Running loss of epoch-34 batch-32 = 6.857328116893768e-06

Training epoch-34 batch-33
Running loss of epoch-34 batch-33 = 5.28222881257534e-06

Training epoch-34 batch-34
Running loss of epoch-34 batch-34 = 4.030065611004829e-06

Training epoch-34 batch-35
Running loss of epoch-34 batch-35 = 5.199108272790909e-06

Training epoch-34 batch-36
Running loss of epoch-34 batch-36 = 5.009816959500313e-06

Training epoch-34 batch-37
Running loss of epoch-34 batch-37 = 5.869893357157707e-06

Training epoch-34 batch-38
Running loss of epoch-34 batch-38 = 3.874767571687698e-06

Training epoch-34 batch-39
Running loss of epoch-34 batch-39 = 7.73044303059578e-06

Training epoch-34 batch-40
Running loss of epoch-34 batch-40 = 5.400041118264198e-06

Training epoch-34 batch-41
Running loss of epoch-34 batch-41 = 8.468050509691238e-06

Training epoch-34 batch-42
Running loss of epoch-34 batch-42 = 6.2070321291685104e-06

Training epoch-34 batch-43
Running loss of epoch-34 batch-43 = 3.4356489777565002e-06

Training epoch-34 batch-44
Running loss of epoch-34 batch-44 = 7.6089054346084595e-06

Training epoch-34 batch-45
Running loss of epoch-34 batch-45 = 3.1443778425455093e-06

Training epoch-34 batch-46
Running loss of epoch-34 batch-46 = 4.209345206618309e-06

Training epoch-34 batch-47
Running loss of epoch-34 batch-47 = 8.268514648079872e-06

Training epoch-34 batch-48
Running loss of epoch-34 batch-48 = 3.0868686735630035e-06

Training epoch-34 batch-49
Running loss of epoch-34 batch-49 = 3.748806193470955e-06

Training epoch-34 batch-50
Running loss of epoch-34 batch-50 = 3.908062353730202e-06

Training epoch-34 batch-51
Running loss of epoch-34 batch-51 = 4.59630973637104e-06

Training epoch-34 batch-52
Running loss of epoch-34 batch-52 = 6.306683644652367e-06

Training epoch-34 batch-53
Running loss of epoch-34 batch-53 = 5.723442882299423e-06

Training epoch-34 batch-54
Running loss of epoch-34 batch-54 = 3.6784913390874863e-06

Training epoch-34 batch-55
Running loss of epoch-34 batch-55 = 8.033355697989464e-06

Training epoch-34 batch-56
Running loss of epoch-34 batch-56 = 3.0454248189926147e-06

Training epoch-34 batch-57
Running loss of epoch-34 batch-57 = 3.4831464290618896e-06

Training epoch-34 batch-58
Running loss of epoch-34 batch-58 = 4.384899511933327e-06

Training epoch-34 batch-59
Running loss of epoch-34 batch-59 = 6.60051591694355e-06

Training epoch-34 batch-60
Running loss of epoch-34 batch-60 = 3.425171598792076e-06

Training epoch-34 batch-61
Running loss of epoch-34 batch-61 = 6.588175892829895e-06

Training epoch-34 batch-62
Running loss of epoch-34 batch-62 = 4.353467375040054e-06

Training epoch-34 batch-63
Running loss of epoch-34 batch-63 = 3.966502845287323e-06

Training epoch-34 batch-64
Running loss of epoch-34 batch-64 = 3.7087593227624893e-06

Training epoch-34 batch-65
Running loss of epoch-34 batch-65 = 5.816109478473663e-06

Training epoch-34 batch-66
Running loss of epoch-34 batch-66 = 4.078727215528488e-06

Training epoch-34 batch-67
Running loss of epoch-34 batch-67 = 3.073830157518387e-06

Training epoch-34 batch-68
Running loss of epoch-34 batch-68 = 4.277564585208893e-06

Training epoch-34 batch-69
Running loss of epoch-34 batch-69 = 3.894791007041931e-06

Training epoch-34 batch-70
Running loss of epoch-34 batch-70 = 8.43801535665989e-06

Training epoch-34 batch-71
Running loss of epoch-34 batch-71 = 5.092006176710129e-06

Training epoch-34 batch-72
Running loss of epoch-34 batch-72 = 4.12575900554657e-06

Training epoch-34 batch-73
Running loss of epoch-34 batch-73 = 3.815861418843269e-06

Training epoch-34 batch-74
Running loss of epoch-34 batch-74 = 4.166737198829651e-06

Training epoch-34 batch-75
Running loss of epoch-34 batch-75 = 4.968373104929924e-06

Training epoch-34 batch-76
Running loss of epoch-34 batch-76 = 5.7551078498363495e-06

Training epoch-34 batch-77
Running loss of epoch-34 batch-77 = 4.220288246870041e-06

Training epoch-34 batch-78
Running loss of epoch-34 batch-78 = 5.702720955014229e-06

Training epoch-34 batch-79
Running loss of epoch-34 batch-79 = 4.559522494673729e-06

Training epoch-34 batch-80
Running loss of epoch-34 batch-80 = 4.62518073618412e-06

Training epoch-34 batch-81
Running loss of epoch-34 batch-81 = 4.112720489501953e-06

Training epoch-34 batch-82
Running loss of epoch-34 batch-82 = 3.521796315908432e-06

Training epoch-34 batch-83
Running loss of epoch-34 batch-83 = 2.564629539847374e-06

Training epoch-34 batch-84
Running loss of epoch-34 batch-84 = 3.6740675568580627e-06

Training epoch-34 batch-85
Running loss of epoch-34 batch-85 = 4.680128768086433e-06

Training epoch-34 batch-86
Running loss of epoch-34 batch-86 = 7.134862244129181e-06

Training epoch-34 batch-87
Running loss of epoch-34 batch-87 = 4.5872293412685394e-06

Training epoch-34 batch-88
Running loss of epoch-34 batch-88 = 4.704110324382782e-06

Training epoch-34 batch-89
Running loss of epoch-34 batch-89 = 4.186062142252922e-06

Training epoch-34 batch-90
Running loss of epoch-34 batch-90 = 8.22381116449833e-06

Training epoch-34 batch-91
Running loss of epoch-34 batch-91 = 4.549045115709305e-06

Training epoch-34 batch-92
Running loss of epoch-34 batch-92 = 4.840316250920296e-06

Training epoch-34 batch-93
Running loss of epoch-34 batch-93 = 5.821464583277702e-06

Training epoch-34 batch-94
Running loss of epoch-34 batch-94 = 5.205627530813217e-06

Training epoch-34 batch-95
Running loss of epoch-34 batch-95 = 4.186760634183884e-06

Training epoch-34 batch-96
Running loss of epoch-34 batch-96 = 4.6263448894023895e-06

Training epoch-34 batch-97
Running loss of epoch-34 batch-97 = 6.009824573993683e-06

Training epoch-34 batch-98
Running loss of epoch-34 batch-98 = 4.342757165431976e-06

Training epoch-34 batch-99
Running loss of epoch-34 batch-99 = 5.50062395632267e-06

Training epoch-34 batch-100
Running loss of epoch-34 batch-100 = 2.87545844912529e-06

Training epoch-34 batch-101
Running loss of epoch-34 batch-101 = 3.814464434981346e-06

Training epoch-34 batch-102
Running loss of epoch-34 batch-102 = 7.15465284883976e-06

Training epoch-34 batch-103
Running loss of epoch-34 batch-103 = 4.939502105116844e-06

Training epoch-34 batch-104
Running loss of epoch-34 batch-104 = 4.456145688891411e-06

Training epoch-34 batch-105
Running loss of epoch-34 batch-105 = 3.93320806324482e-06

Training epoch-34 batch-106
Running loss of epoch-34 batch-106 = 6.523448973894119e-06

Training epoch-34 batch-107
Running loss of epoch-34 batch-107 = 4.037516191601753e-06

Training epoch-34 batch-108
Running loss of epoch-34 batch-108 = 3.909459337592125e-06

Training epoch-34 batch-109
Running loss of epoch-34 batch-109 = 5.487585440278053e-06

Training epoch-34 batch-110
Running loss of epoch-34 batch-110 = 6.2640756368637085e-06

Training epoch-34 batch-111
Running loss of epoch-34 batch-111 = 5.996320396661758e-06

Training epoch-34 batch-112
Running loss of epoch-34 batch-112 = 4.580244421958923e-06

Training epoch-34 batch-113
Running loss of epoch-34 batch-113 = 4.427973181009293e-06

Training epoch-34 batch-114
Running loss of epoch-34 batch-114 = 6.944173946976662e-06

Training epoch-34 batch-115
Running loss of epoch-34 batch-115 = 4.404457286000252e-06

Training epoch-34 batch-116
Running loss of epoch-34 batch-116 = 4.2228493839502335e-06

Training epoch-34 batch-117
Running loss of epoch-34 batch-117 = 5.782581865787506e-06

Training epoch-34 batch-118
Running loss of epoch-34 batch-118 = 5.570240318775177e-06

Training epoch-34 batch-119
Running loss of epoch-34 batch-119 = 4.17209230363369e-06

Training epoch-34 batch-120
Running loss of epoch-34 batch-120 = 4.495261237025261e-06

Training epoch-34 batch-121
Running loss of epoch-34 batch-121 = 3.0100345611572266e-06

Training epoch-34 batch-122
Running loss of epoch-34 batch-122 = 3.514811396598816e-06

Training epoch-34 batch-123
Running loss of epoch-34 batch-123 = 7.851747795939445e-06

Training epoch-34 batch-124
Running loss of epoch-34 batch-124 = 2.605840563774109e-06

Training epoch-34 batch-125
Running loss of epoch-34 batch-125 = 6.293877959251404e-06

Training epoch-34 batch-126
Running loss of epoch-34 batch-126 = 3.8906000554561615e-06

Training epoch-34 batch-127
Running loss of epoch-34 batch-127 = 4.353700205683708e-06

Training epoch-34 batch-128
Running loss of epoch-34 batch-128 = 4.60469163954258e-06

Training epoch-34 batch-129
Running loss of epoch-34 batch-129 = 4.894798621535301e-06

Training epoch-34 batch-130
Running loss of epoch-34 batch-130 = 5.523907020688057e-06

Training epoch-34 batch-131
Running loss of epoch-34 batch-131 = 3.938563168048859e-06

Training epoch-34 batch-132
Running loss of epoch-34 batch-132 = 5.83636574447155e-06

Training epoch-34 batch-133
Running loss of epoch-34 batch-133 = 7.394002750515938e-06

Training epoch-34 batch-134
Running loss of epoch-34 batch-134 = 6.33997842669487e-06

Training epoch-34 batch-135
Running loss of epoch-34 batch-135 = 5.855923518538475e-06

Training epoch-34 batch-136
Running loss of epoch-34 batch-136 = 4.956033080816269e-06

Training epoch-34 batch-137
Running loss of epoch-34 batch-137 = 2.670101821422577e-06

Training epoch-34 batch-138
Running loss of epoch-34 batch-138 = 1.0185642167925835e-05

Training epoch-34 batch-139
Running loss of epoch-34 batch-139 = 4.730885848402977e-06

Training epoch-34 batch-140
Running loss of epoch-34 batch-140 = 5.194684490561485e-06

Training epoch-34 batch-141
Running loss of epoch-34 batch-141 = 6.197718903422356e-06

Training epoch-34 batch-142
Running loss of epoch-34 batch-142 = 4.0463637560606e-06

Training epoch-34 batch-143
Running loss of epoch-34 batch-143 = 4.860106855630875e-06

Training epoch-34 batch-144
Running loss of epoch-34 batch-144 = 5.971640348434448e-06

Training epoch-34 batch-145
Running loss of epoch-34 batch-145 = 2.464279532432556e-06

Training epoch-34 batch-146
Running loss of epoch-34 batch-146 = 4.1406601667404175e-06

Training epoch-34 batch-147
Running loss of epoch-34 batch-147 = 7.281545549631119e-06

Training epoch-34 batch-148
Running loss of epoch-34 batch-148 = 7.348600775003433e-06

Training epoch-34 batch-149
Running loss of epoch-34 batch-149 = 5.3890980780124664e-06

Training epoch-34 batch-150
Running loss of epoch-34 batch-150 = 4.723435267806053e-06

Training epoch-34 batch-151
Running loss of epoch-34 batch-151 = 7.0517417043447495e-06

Training epoch-34 batch-152
Running loss of epoch-34 batch-152 = 4.958128556609154e-06

Training epoch-34 batch-153
Running loss of epoch-34 batch-153 = 4.270346835255623e-06

Training epoch-34 batch-154
Running loss of epoch-34 batch-154 = 3.7157442420721054e-06

Training epoch-34 batch-155
Running loss of epoch-34 batch-155 = 3.9816368371248245e-06

Training epoch-34 batch-156
Running loss of epoch-34 batch-156 = 4.900386556982994e-06

Training epoch-34 batch-157
Running loss of epoch-34 batch-157 = 1.3425946235656738e-05

Finished training epoch-34.



Average train loss at epoch-34 = 5.136875808238983e-06

Started Evaluation

Average val loss at epoch-34 = 2.983117666683699

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.75 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.37 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-35


Training epoch-35 batch-1
Running loss of epoch-35 batch-1 = 5.4747797548770905e-06

Training epoch-35 batch-2
Running loss of epoch-35 batch-2 = 6.160000339150429e-06

Training epoch-35 batch-3
Running loss of epoch-35 batch-3 = 5.4067932069301605e-06

Training epoch-35 batch-4
Running loss of epoch-35 batch-4 = 4.965346306562424e-06

Training epoch-35 batch-5
Running loss of epoch-35 batch-5 = 6.203772500157356e-06

Training epoch-35 batch-6
Running loss of epoch-35 batch-6 = 4.766741767525673e-06

Training epoch-35 batch-7
Running loss of epoch-35 batch-7 = 3.4312251955270767e-06

Training epoch-35 batch-8
Running loss of epoch-35 batch-8 = 3.1492672860622406e-06

Training epoch-35 batch-9
Running loss of epoch-35 batch-9 = 3.14321368932724e-06

Training epoch-35 batch-10
Running loss of epoch-35 batch-10 = 6.579328328371048e-06

Training epoch-35 batch-11
Running loss of epoch-35 batch-11 = 4.593748599290848e-06

Training epoch-35 batch-12
Running loss of epoch-35 batch-12 = 6.176764145493507e-06

Training epoch-35 batch-13
Running loss of epoch-35 batch-13 = 3.1013041734695435e-06

Training epoch-35 batch-14
Running loss of epoch-35 batch-14 = 5.399109795689583e-06

Training epoch-35 batch-15
Running loss of epoch-35 batch-15 = 3.907829523086548e-06

Training epoch-35 batch-16
Running loss of epoch-35 batch-16 = 5.082925781607628e-06

Training epoch-35 batch-17
Running loss of epoch-35 batch-17 = 4.037516191601753e-06

Training epoch-35 batch-18
Running loss of epoch-35 batch-18 = 6.291316822171211e-06

Training epoch-35 batch-19
Running loss of epoch-35 batch-19 = 4.892004653811455e-06

Training epoch-35 batch-20
Running loss of epoch-35 batch-20 = 3.889203071594238e-06

Training epoch-35 batch-21
Running loss of epoch-35 batch-21 = 5.670124664902687e-06

Training epoch-35 batch-22
Running loss of epoch-35 batch-22 = 4.982110112905502e-06

Training epoch-35 batch-23
Running loss of epoch-35 batch-23 = 4.817266017198563e-06

Training epoch-35 batch-24
Running loss of epoch-35 batch-24 = 3.5513658076524734e-06

Training epoch-35 batch-25
Running loss of epoch-35 batch-25 = 5.681533366441727e-06

Training epoch-35 batch-26
Running loss of epoch-35 batch-26 = 7.913680747151375e-06

Training epoch-35 batch-27
Running loss of epoch-35 batch-27 = 5.3925905376672745e-06

Training epoch-35 batch-28
Running loss of epoch-35 batch-28 = 5.374196916818619e-06

Training epoch-35 batch-29
Running loss of epoch-35 batch-29 = 8.980510756373405e-06

Training epoch-35 batch-30
Running loss of epoch-35 batch-30 = 4.23286110162735e-06

Training epoch-35 batch-31
Running loss of epoch-35 batch-31 = 5.259877070784569e-06

Training epoch-35 batch-32
Running loss of epoch-35 batch-32 = 3.721565008163452e-06

Training epoch-35 batch-33
Running loss of epoch-35 batch-33 = 4.210509359836578e-06

Training epoch-35 batch-34
Running loss of epoch-35 batch-34 = 4.114117473363876e-06

Training epoch-35 batch-35
Running loss of epoch-35 batch-35 = 4.658941179513931e-06

Training epoch-35 batch-36
Running loss of epoch-35 batch-36 = 3.2908283174037933e-06

Training epoch-35 batch-37
Running loss of epoch-35 batch-37 = 5.180714651942253e-06

Training epoch-35 batch-38
Running loss of epoch-35 batch-38 = 5.8319419622421265e-06

Training epoch-35 batch-39
Running loss of epoch-35 batch-39 = 4.586530849337578e-06

Training epoch-35 batch-40
Running loss of epoch-35 batch-40 = 3.7944409996271133e-06

Training epoch-35 batch-41
Running loss of epoch-35 batch-41 = 6.738351657986641e-06

Training epoch-35 batch-42
Running loss of epoch-35 batch-42 = 5.836831405758858e-06

Training epoch-35 batch-43
Running loss of epoch-35 batch-43 = 4.3371692299842834e-06

Training epoch-35 batch-44
Running loss of epoch-35 batch-44 = 2.7336645871400833e-06

Training epoch-35 batch-45
Running loss of epoch-35 batch-45 = 2.926914021372795e-06

Training epoch-35 batch-46
Running loss of epoch-35 batch-46 = 3.7599820643663406e-06

Training epoch-35 batch-47
Running loss of epoch-35 batch-47 = 4.426110535860062e-06

Training epoch-35 batch-48
Running loss of epoch-35 batch-48 = 2.920161932706833e-06

Training epoch-35 batch-49
Running loss of epoch-35 batch-49 = 4.74369153380394e-06

Training epoch-35 batch-50
Running loss of epoch-35 batch-50 = 6.3658226281404495e-06

Training epoch-35 batch-51
Running loss of epoch-35 batch-51 = 3.846827894449234e-06

Training epoch-35 batch-52
Running loss of epoch-35 batch-52 = 2.2156164050102234e-06

Training epoch-35 batch-53
Running loss of epoch-35 batch-53 = 6.88270665705204e-06

Training epoch-35 batch-54
Running loss of epoch-35 batch-54 = 4.680128768086433e-06

Training epoch-35 batch-55
Running loss of epoch-35 batch-55 = 3.6265701055526733e-06

Training epoch-35 batch-56
Running loss of epoch-35 batch-56 = 7.153023034334183e-06

Training epoch-35 batch-57
Running loss of epoch-35 batch-57 = 5.3031835705041885e-06

Training epoch-35 batch-58
Running loss of epoch-35 batch-58 = 4.889676347374916e-06

Training epoch-35 batch-59
Running loss of epoch-35 batch-59 = 8.044298738241196e-06

Training epoch-35 batch-60
Running loss of epoch-35 batch-60 = 4.7318171709775925e-06

Training epoch-35 batch-61
Running loss of epoch-35 batch-61 = 5.890382453799248e-06

Training epoch-35 batch-62
Running loss of epoch-35 batch-62 = 4.636123776435852e-06

Training epoch-35 batch-63
Running loss of epoch-35 batch-63 = 6.888527423143387e-06

Training epoch-35 batch-64
Running loss of epoch-35 batch-64 = 4.240777343511581e-06

Training epoch-35 batch-65
Running loss of epoch-35 batch-65 = 3.9085280150175095e-06

Training epoch-35 batch-66
Running loss of epoch-35 batch-66 = 4.661502316594124e-06

Training epoch-35 batch-67
Running loss of epoch-35 batch-67 = 7.226364687085152e-06

Training epoch-35 batch-68
Running loss of epoch-35 batch-68 = 4.566041752696037e-06

Training epoch-35 batch-69
Running loss of epoch-35 batch-69 = 5.594687536358833e-06

Training epoch-35 batch-70
Running loss of epoch-35 batch-70 = 3.7082936614751816e-06

Training epoch-35 batch-71
Running loss of epoch-35 batch-71 = 5.409354344010353e-06

Training epoch-35 batch-72
Running loss of epoch-35 batch-72 = 6.073620170354843e-06

Training epoch-35 batch-73
Running loss of epoch-35 batch-73 = 3.0887313187122345e-06

Training epoch-35 batch-74
Running loss of epoch-35 batch-74 = 4.632165655493736e-06

Training epoch-35 batch-75
Running loss of epoch-35 batch-75 = 5.043111741542816e-06

Training epoch-35 batch-76
Running loss of epoch-35 batch-76 = 7.226131856441498e-06

Training epoch-35 batch-77
Running loss of epoch-35 batch-77 = 7.588183507323265e-06

Training epoch-35 batch-78
Running loss of epoch-35 batch-78 = 3.908760845661163e-06

Training epoch-35 batch-79
Running loss of epoch-35 batch-79 = 2.8992071747779846e-06

Training epoch-35 batch-80
Running loss of epoch-35 batch-80 = 5.523674190044403e-06

Training epoch-35 batch-81
Running loss of epoch-35 batch-81 = 6.6158827394247055e-06

Training epoch-35 batch-82
Running loss of epoch-35 batch-82 = 3.61073762178421e-06

Training epoch-35 batch-83
Running loss of epoch-35 batch-83 = 4.864530637860298e-06

Training epoch-35 batch-84
Running loss of epoch-35 batch-84 = 2.4351757019758224e-06

Training epoch-35 batch-85
Running loss of epoch-35 batch-85 = 4.6424102038145065e-06

Training epoch-35 batch-86
Running loss of epoch-35 batch-86 = 5.998881533741951e-06

Training epoch-35 batch-87
Running loss of epoch-35 batch-87 = 3.976514562964439e-06

Training epoch-35 batch-88
Running loss of epoch-35 batch-88 = 3.5017728805541992e-06

Training epoch-35 batch-89
Running loss of epoch-35 batch-89 = 3.696652129292488e-06

Training epoch-35 batch-90
Running loss of epoch-35 batch-90 = 4.311557859182358e-06

Training epoch-35 batch-91
Running loss of epoch-35 batch-91 = 5.231006070971489e-06

Training epoch-35 batch-92
Running loss of epoch-35 batch-92 = 5.243346095085144e-06

Training epoch-35 batch-93
Running loss of epoch-35 batch-93 = 5.9658195823431015e-06

Training epoch-35 batch-94
Running loss of epoch-35 batch-94 = 4.6570785343647e-06

Training epoch-35 batch-95
Running loss of epoch-35 batch-95 = 5.956040695309639e-06

Training epoch-35 batch-96
Running loss of epoch-35 batch-96 = 3.020046278834343e-06

Training epoch-35 batch-97
Running loss of epoch-35 batch-97 = 4.047062247991562e-06

Training epoch-35 batch-98
Running loss of epoch-35 batch-98 = 4.810979589819908e-06

Training epoch-35 batch-99
Running loss of epoch-35 batch-99 = 4.971865564584732e-06

Training epoch-35 batch-100
Running loss of epoch-35 batch-100 = 3.535998985171318e-06

Training epoch-35 batch-101
Running loss of epoch-35 batch-101 = 3.186054527759552e-06

Training epoch-35 batch-102
Running loss of epoch-35 batch-102 = 5.823094397783279e-06

Training epoch-35 batch-103
Running loss of epoch-35 batch-103 = 3.883847966790199e-06

Training epoch-35 batch-104
Running loss of epoch-35 batch-104 = 3.982800990343094e-06

Training epoch-35 batch-105
Running loss of epoch-35 batch-105 = 6.861751899123192e-06

Training epoch-35 batch-106
Running loss of epoch-35 batch-106 = 4.426110535860062e-06

Training epoch-35 batch-107
Running loss of epoch-35 batch-107 = 3.623310476541519e-06

Training epoch-35 batch-108
Running loss of epoch-35 batch-108 = 5.68293035030365e-06

Training epoch-35 batch-109
Running loss of epoch-35 batch-109 = 5.044974386692047e-06

Training epoch-35 batch-110
Running loss of epoch-35 batch-110 = 3.6531127989292145e-06

Training epoch-35 batch-111
Running loss of epoch-35 batch-111 = 3.902707248926163e-06

Training epoch-35 batch-112
Running loss of epoch-35 batch-112 = 6.149057298898697e-06

Training epoch-35 batch-113
Running loss of epoch-35 batch-113 = 4.210043698549271e-06

Training epoch-35 batch-114
Running loss of epoch-35 batch-114 = 5.088746547698975e-06

Training epoch-35 batch-115
Running loss of epoch-35 batch-115 = 5.2461400628089905e-06

Training epoch-35 batch-116
Running loss of epoch-35 batch-116 = 1.996755599975586e-06

Training epoch-35 batch-117
Running loss of epoch-35 batch-117 = 7.422640919685364e-06

Training epoch-35 batch-118
Running loss of epoch-35 batch-118 = 4.41819429397583e-06

Training epoch-35 batch-119
Running loss of epoch-35 batch-119 = 4.330649971961975e-06

Training epoch-35 batch-120
Running loss of epoch-35 batch-120 = 5.177222192287445e-06

Training epoch-35 batch-121
Running loss of epoch-35 batch-121 = 5.9872400015592575e-06

Training epoch-35 batch-122
Running loss of epoch-35 batch-122 = 4.3495092540979385e-06

Training epoch-35 batch-123
Running loss of epoch-35 batch-123 = 4.31109219789505e-06

Training epoch-35 batch-124
Running loss of epoch-35 batch-124 = 5.471287295222282e-06

Training epoch-35 batch-125
Running loss of epoch-35 batch-125 = 4.836125299334526e-06

Training epoch-35 batch-126
Running loss of epoch-35 batch-126 = 5.392823368310928e-06

Training epoch-35 batch-127
Running loss of epoch-35 batch-127 = 3.670109435915947e-06

Training epoch-35 batch-128
Running loss of epoch-35 batch-128 = 4.717148840427399e-06

Training epoch-35 batch-129
Running loss of epoch-35 batch-129 = 4.880828782916069e-06

Training epoch-35 batch-130
Running loss of epoch-35 batch-130 = 5.0801318138837814e-06

Training epoch-35 batch-131
Running loss of epoch-35 batch-131 = 6.712740287184715e-06

Training epoch-35 batch-132
Running loss of epoch-35 batch-132 = 5.29061071574688e-06

Training epoch-35 batch-133
Running loss of epoch-35 batch-133 = 4.489673301577568e-06

Training epoch-35 batch-134
Running loss of epoch-35 batch-134 = 4.438683390617371e-06

Training epoch-35 batch-135
Running loss of epoch-35 batch-135 = 5.225883796811104e-06

Training epoch-35 batch-136
Running loss of epoch-35 batch-136 = 4.9122609198093414e-06

Training epoch-35 batch-137
Running loss of epoch-35 batch-137 = 4.404224455356598e-06

Training epoch-35 batch-138
Running loss of epoch-35 batch-138 = 6.036600098013878e-06

Training epoch-35 batch-139
Running loss of epoch-35 batch-139 = 4.700617864727974e-06

Training epoch-35 batch-140
Running loss of epoch-35 batch-140 = 3.0838418751955032e-06

Training epoch-35 batch-141
Running loss of epoch-35 batch-141 = 3.9800070226192474e-06

Training epoch-35 batch-142
Running loss of epoch-35 batch-142 = 6.035668775439262e-06

Training epoch-35 batch-143
Running loss of epoch-35 batch-143 = 4.022149369120598e-06

Training epoch-35 batch-144
Running loss of epoch-35 batch-144 = 4.985136911273003e-06

Training epoch-35 batch-145
Running loss of epoch-35 batch-145 = 5.713431164622307e-06

Training epoch-35 batch-146
Running loss of epoch-35 batch-146 = 4.699220880866051e-06

Training epoch-35 batch-147
Running loss of epoch-35 batch-147 = 9.912066161632538e-06

Training epoch-35 batch-148
Running loss of epoch-35 batch-148 = 5.530426278710365e-06

Training epoch-35 batch-149
Running loss of epoch-35 batch-149 = 8.625444024801254e-06

Training epoch-35 batch-150
Running loss of epoch-35 batch-150 = 3.539957106113434e-06

Training epoch-35 batch-151
Running loss of epoch-35 batch-151 = 5.645211786031723e-06

Training epoch-35 batch-152
Running loss of epoch-35 batch-152 = 4.759989678859711e-06

Training epoch-35 batch-153
Running loss of epoch-35 batch-153 = 1.1066440492868423e-06

Training epoch-35 batch-154
Running loss of epoch-35 batch-154 = 5.568377673625946e-06

Training epoch-35 batch-155
Running loss of epoch-35 batch-155 = 5.051726475358009e-06

Training epoch-35 batch-156
Running loss of epoch-35 batch-156 = 3.2507814466953278e-06

Training epoch-35 batch-157
Running loss of epoch-35 batch-157 = 2.7488917112350464e-05

Finished training epoch-35.



Average train loss at epoch-35 = 4.923315346240997e-06

Started Evaluation

Average val loss at epoch-35 = 2.9893254355380408

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.36 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-36


Training epoch-36 batch-1
Running loss of epoch-36 batch-1 = 5.291542038321495e-06

Training epoch-36 batch-2
Running loss of epoch-36 batch-2 = 5.75394369661808e-06

Training epoch-36 batch-3
Running loss of epoch-36 batch-3 = 5.042878910899162e-06

Training epoch-36 batch-4
Running loss of epoch-36 batch-4 = 5.028676241636276e-06

Training epoch-36 batch-5
Running loss of epoch-36 batch-5 = 4.75696288049221e-06

Training epoch-36 batch-6
Running loss of epoch-36 batch-6 = 4.536006599664688e-06

Training epoch-36 batch-7
Running loss of epoch-36 batch-7 = 3.107823431491852e-06

Training epoch-36 batch-8
Running loss of epoch-36 batch-8 = 3.7832651287317276e-06

Training epoch-36 batch-9
Running loss of epoch-36 batch-9 = 7.045455276966095e-06

Training epoch-36 batch-10
Running loss of epoch-36 batch-10 = 2.9443763196468353e-06

Training epoch-36 batch-11
Running loss of epoch-36 batch-11 = 3.843801096081734e-06

Training epoch-36 batch-12
Running loss of epoch-36 batch-12 = 3.0496157705783844e-06

Training epoch-36 batch-13
Running loss of epoch-36 batch-13 = 3.8025900721549988e-06

Training epoch-36 batch-14
Running loss of epoch-36 batch-14 = 6.457092240452766e-06

Training epoch-36 batch-15
Running loss of epoch-36 batch-15 = 2.5976914912462234e-06

Training epoch-36 batch-16
Running loss of epoch-36 batch-16 = 5.1069073379039764e-06

Training epoch-36 batch-17
Running loss of epoch-36 batch-17 = 5.534151569008827e-06

Training epoch-36 batch-18
Running loss of epoch-36 batch-18 = 4.8514921218156815e-06

Training epoch-36 batch-19
Running loss of epoch-36 batch-19 = 3.0517112463712692e-06

Training epoch-36 batch-20
Running loss of epoch-36 batch-20 = 3.6156270653009415e-06

Training epoch-36 batch-21
Running loss of epoch-36 batch-21 = 4.545785486698151e-06

Training epoch-36 batch-22
Running loss of epoch-36 batch-22 = 3.11853364109993e-06

Training epoch-36 batch-23
Running loss of epoch-36 batch-23 = 4.379311576485634e-06

Training epoch-36 batch-24
Running loss of epoch-36 batch-24 = 4.700617864727974e-06

Training epoch-36 batch-25
Running loss of epoch-36 batch-25 = 4.309229552745819e-06

Training epoch-36 batch-26
Running loss of epoch-36 batch-26 = 4.055444151163101e-06

Training epoch-36 batch-27
Running loss of epoch-36 batch-27 = 4.344386979937553e-06

Training epoch-36 batch-28
Running loss of epoch-36 batch-28 = 5.129491910338402e-06

Training epoch-36 batch-29
Running loss of epoch-36 batch-29 = 6.227521225810051e-06

Training epoch-36 batch-30
Running loss of epoch-36 batch-30 = 5.158828571438789e-06

Training epoch-36 batch-31
Running loss of epoch-36 batch-31 = 4.259170964360237e-06

Training epoch-36 batch-32
Running loss of epoch-36 batch-32 = 4.03192825615406e-06

Training epoch-36 batch-33
Running loss of epoch-36 batch-33 = 5.815178155899048e-06

Training epoch-36 batch-34
Running loss of epoch-36 batch-34 = 2.594664692878723e-06

Training epoch-36 batch-35
Running loss of epoch-36 batch-35 = 7.057329639792442e-06

Training epoch-36 batch-36
Running loss of epoch-36 batch-36 = 5.3944531828165054e-06

Training epoch-36 batch-37
Running loss of epoch-36 batch-37 = 6.320187821984291e-06

Training epoch-36 batch-38
Running loss of epoch-36 batch-38 = 4.8074871301651e-06

Training epoch-36 batch-39
Running loss of epoch-36 batch-39 = 5.938578397035599e-06

Training epoch-36 batch-40
Running loss of epoch-36 batch-40 = 4.770001396536827e-06

Training epoch-36 batch-41
Running loss of epoch-36 batch-41 = 3.2354146242141724e-06

Training epoch-36 batch-42
Running loss of epoch-36 batch-42 = 4.637520760297775e-06

Training epoch-36 batch-43
Running loss of epoch-36 batch-43 = 3.851484507322311e-06

Training epoch-36 batch-44
Running loss of epoch-36 batch-44 = 6.920425221323967e-06

Training epoch-36 batch-45
Running loss of epoch-36 batch-45 = 3.4640543162822723e-06

Training epoch-36 batch-46
Running loss of epoch-36 batch-46 = 6.1856117099523544e-06

Training epoch-36 batch-47
Running loss of epoch-36 batch-47 = 2.6177149266004562e-06

Training epoch-36 batch-48
Running loss of epoch-36 batch-48 = 3.3192336559295654e-06

Training epoch-36 batch-49
Running loss of epoch-36 batch-49 = 5.300389602780342e-06

Training epoch-36 batch-50
Running loss of epoch-36 batch-50 = 3.4833792597055435e-06

Training epoch-36 batch-51
Running loss of epoch-36 batch-51 = 5.007954314351082e-06

Training epoch-36 batch-52
Running loss of epoch-36 batch-52 = 4.4906046241521835e-06

Training epoch-36 batch-53
Running loss of epoch-36 batch-53 = 4.873611032962799e-06

Training epoch-36 batch-54
Running loss of epoch-36 batch-54 = 3.7141144275665283e-06

Training epoch-36 batch-55
Running loss of epoch-36 batch-55 = 5.577923730015755e-06

Training epoch-36 batch-56
Running loss of epoch-36 batch-56 = 5.052890628576279e-06

Training epoch-36 batch-57
Running loss of epoch-36 batch-57 = 5.189329385757446e-06

Training epoch-36 batch-58
Running loss of epoch-36 batch-58 = 4.8889778554439545e-06

Training epoch-36 batch-59
Running loss of epoch-36 batch-59 = 4.213070496916771e-06

Training epoch-36 batch-60
Running loss of epoch-36 batch-60 = 3.5495031625032425e-06

Training epoch-36 batch-61
Running loss of epoch-36 batch-61 = 6.4729247242212296e-06

Training epoch-36 batch-62
Running loss of epoch-36 batch-62 = 3.6121346056461334e-06

Training epoch-36 batch-63
Running loss of epoch-36 batch-63 = 5.552312359213829e-06

Training epoch-36 batch-64
Running loss of epoch-36 batch-64 = 3.3515971153974533e-06

Training epoch-36 batch-65
Running loss of epoch-36 batch-65 = 3.992579877376556e-06

Training epoch-36 batch-66
Running loss of epoch-36 batch-66 = 4.6978238970041275e-06

Training epoch-36 batch-67
Running loss of epoch-36 batch-67 = 4.84357587993145e-06

Training epoch-36 batch-68
Running loss of epoch-36 batch-68 = 3.884546458721161e-06

Training epoch-36 batch-69
Running loss of epoch-36 batch-69 = 2.9976945370435715e-06

Training epoch-36 batch-70
Running loss of epoch-36 batch-70 = 3.505731001496315e-06

Training epoch-36 batch-71
Running loss of epoch-36 batch-71 = 3.990950062870979e-06

Training epoch-36 batch-72
Running loss of epoch-36 batch-72 = 4.800502210855484e-06

Training epoch-36 batch-73
Running loss of epoch-36 batch-73 = 3.63960862159729e-06

Training epoch-36 batch-74
Running loss of epoch-36 batch-74 = 4.35439869761467e-06

Training epoch-36 batch-75
Running loss of epoch-36 batch-75 = 2.500135451555252e-06

Training epoch-36 batch-76
Running loss of epoch-36 batch-76 = 3.0316878110170364e-06

Training epoch-36 batch-77
Running loss of epoch-36 batch-77 = 5.852663889527321e-06

Training epoch-36 batch-78
Running loss of epoch-36 batch-78 = 6.879214197397232e-06

Training epoch-36 batch-79
Running loss of epoch-36 batch-79 = 4.0566083043813705e-06

Training epoch-36 batch-80
Running loss of epoch-36 batch-80 = 4.386529326438904e-06

Training epoch-36 batch-81
Running loss of epoch-36 batch-81 = 9.301351383328438e-06

Training epoch-36 batch-82
Running loss of epoch-36 batch-82 = 7.693655788898468e-06

Training epoch-36 batch-83
Running loss of epoch-36 batch-83 = 2.7264468371868134e-06

Training epoch-36 batch-84
Running loss of epoch-36 batch-84 = 4.648696631193161e-06

Training epoch-36 batch-85
Running loss of epoch-36 batch-85 = 4.331115633249283e-06

Training epoch-36 batch-86
Running loss of epoch-36 batch-86 = 4.1120219975709915e-06

Training epoch-36 batch-87
Running loss of epoch-36 batch-87 = 5.248235538601875e-06

Training epoch-36 batch-88
Running loss of epoch-36 batch-88 = 8.303672075271606e-06

Training epoch-36 batch-89
Running loss of epoch-36 batch-89 = 5.20399771630764e-06

Training epoch-36 batch-90
Running loss of epoch-36 batch-90 = 5.706679075956345e-06

Training epoch-36 batch-91
Running loss of epoch-36 batch-91 = 4.506902769207954e-06

Training epoch-36 batch-92
Running loss of epoch-36 batch-92 = 2.560904249548912e-06

Training epoch-36 batch-93
Running loss of epoch-36 batch-93 = 5.1800161600112915e-06

Training epoch-36 batch-94
Running loss of epoch-36 batch-94 = 5.04031777381897e-06

Training epoch-36 batch-95
Running loss of epoch-36 batch-95 = 5.400506779551506e-06

Training epoch-36 batch-96
Running loss of epoch-36 batch-96 = 3.843102604150772e-06

Training epoch-36 batch-97
Running loss of epoch-36 batch-97 = 4.166504368185997e-06

Training epoch-36 batch-98
Running loss of epoch-36 batch-98 = 6.38212077319622e-06

Training epoch-36 batch-99
Running loss of epoch-36 batch-99 = 5.412846803665161e-06

Training epoch-36 batch-100
Running loss of epoch-36 batch-100 = 5.052657797932625e-06

Training epoch-36 batch-101
Running loss of epoch-36 batch-101 = 5.255220457911491e-06

Training epoch-36 batch-102
Running loss of epoch-36 batch-102 = 5.166511982679367e-06

Training epoch-36 batch-103
Running loss of epoch-36 batch-103 = 4.138331860303879e-06

Training epoch-36 batch-104
Running loss of epoch-36 batch-104 = 4.6032946556806564e-06

Training epoch-36 batch-105
Running loss of epoch-36 batch-105 = 5.4263509809970856e-06

Training epoch-36 batch-106
Running loss of epoch-36 batch-106 = 6.081769242882729e-06

Training epoch-36 batch-107
Running loss of epoch-36 batch-107 = 5.2086543291807175e-06

Training epoch-36 batch-108
Running loss of epoch-36 batch-108 = 5.605863407254219e-06

Training epoch-36 batch-109
Running loss of epoch-36 batch-109 = 6.268499419093132e-06

Training epoch-36 batch-110
Running loss of epoch-36 batch-110 = 4.9173831939697266e-06

Training epoch-36 batch-111
Running loss of epoch-36 batch-111 = 3.903405740857124e-06

Training epoch-36 batch-112
Running loss of epoch-36 batch-112 = 4.797475412487984e-06

Training epoch-36 batch-113
Running loss of epoch-36 batch-113 = 3.2156240195035934e-06

Training epoch-36 batch-114
Running loss of epoch-36 batch-114 = 5.4747797548770905e-06

Training epoch-36 batch-115
Running loss of epoch-36 batch-115 = 5.479902029037476e-06

Training epoch-36 batch-116
Running loss of epoch-36 batch-116 = 4.508299753069878e-06

Training epoch-36 batch-117
Running loss of epoch-36 batch-117 = 7.685739547014236e-06

Training epoch-36 batch-118
Running loss of epoch-36 batch-118 = 3.4389086067676544e-06

Training epoch-36 batch-119
Running loss of epoch-36 batch-119 = 3.780936822295189e-06

Training epoch-36 batch-120
Running loss of epoch-36 batch-120 = 6.713205948472023e-06

Training epoch-36 batch-121
Running loss of epoch-36 batch-121 = 5.170470103621483e-06

Training epoch-36 batch-122
Running loss of epoch-36 batch-122 = 3.275228664278984e-06

Training epoch-36 batch-123
Running loss of epoch-36 batch-123 = 5.220063030719757e-06

Training epoch-36 batch-124
Running loss of epoch-36 batch-124 = 5.112960934638977e-06

Training epoch-36 batch-125
Running loss of epoch-36 batch-125 = 4.819128662347794e-06

Training epoch-36 batch-126
Running loss of epoch-36 batch-126 = 5.255686119198799e-06

Training epoch-36 batch-127
Running loss of epoch-36 batch-127 = 6.4659398049116135e-06

Training epoch-36 batch-128
Running loss of epoch-36 batch-128 = 3.993278369307518e-06

Training epoch-36 batch-129
Running loss of epoch-36 batch-129 = 4.1960738599300385e-06

Training epoch-36 batch-130
Running loss of epoch-36 batch-130 = 4.093162715435028e-06

Training epoch-36 batch-131
Running loss of epoch-36 batch-131 = 3.866618499159813e-06

Training epoch-36 batch-132
Running loss of epoch-36 batch-132 = 4.380708560347557e-06

Training epoch-36 batch-133
Running loss of epoch-36 batch-133 = 3.3704563975334167e-06

Training epoch-36 batch-134
Running loss of epoch-36 batch-134 = 4.177913069725037e-06

Training epoch-36 batch-135
Running loss of epoch-36 batch-135 = 3.404216840863228e-06

Training epoch-36 batch-136
Running loss of epoch-36 batch-136 = 5.5551063269376755e-06

Training epoch-36 batch-137
Running loss of epoch-36 batch-137 = 2.980697900056839e-06

Training epoch-36 batch-138
Running loss of epoch-36 batch-138 = 3.6491546779870987e-06

Training epoch-36 batch-139
Running loss of epoch-36 batch-139 = 4.553934559226036e-06

Training epoch-36 batch-140
Running loss of epoch-36 batch-140 = 4.503875970840454e-06

Training epoch-36 batch-141
Running loss of epoch-36 batch-141 = 4.957662895321846e-06

Training epoch-36 batch-142
Running loss of epoch-36 batch-142 = 4.573259502649307e-06

Training epoch-36 batch-143
Running loss of epoch-36 batch-143 = 3.119930624961853e-06

Training epoch-36 batch-144
Running loss of epoch-36 batch-144 = 4.975590854883194e-06

Training epoch-36 batch-145
Running loss of epoch-36 batch-145 = 4.8852525651454926e-06

Training epoch-36 batch-146
Running loss of epoch-36 batch-146 = 3.552529960870743e-06

Training epoch-36 batch-147
Running loss of epoch-36 batch-147 = 6.230315193533897e-06

Training epoch-36 batch-148
Running loss of epoch-36 batch-148 = 3.457767888903618e-06

Training epoch-36 batch-149
Running loss of epoch-36 batch-149 = 5.016103386878967e-06

Training epoch-36 batch-150
Running loss of epoch-36 batch-150 = 4.356959834694862e-06

Training epoch-36 batch-151
Running loss of epoch-36 batch-151 = 5.5176205933094025e-06

Training epoch-36 batch-152
Running loss of epoch-36 batch-152 = 3.7080608308315277e-06

Training epoch-36 batch-153
Running loss of epoch-36 batch-153 = 3.807712346315384e-06

Training epoch-36 batch-154
Running loss of epoch-36 batch-154 = 4.9711670726537704e-06

Training epoch-36 batch-155
Running loss of epoch-36 batch-155 = 4.137633368372917e-06

Training epoch-36 batch-156
Running loss of epoch-36 batch-156 = 4.419824108481407e-06

Training epoch-36 batch-157
Running loss of epoch-36 batch-157 = 3.01264226436615e-05

Finished training epoch-36.



Average train loss at epoch-36 = 4.7097072005271915e-06

Started Evaluation

Average val loss at epoch-36 = 2.998754727213006

Accuracy for classes:
Accuracy for class equals is: 74.75 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.11 %
Accuracy for class onCreate is: 57.04 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 36.99 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.41 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-37


Training epoch-37 batch-1
Running loss of epoch-37 batch-1 = 6.393063813447952e-06

Training epoch-37 batch-2
Running loss of epoch-37 batch-2 = 2.949964255094528e-06

Training epoch-37 batch-3
Running loss of epoch-37 batch-3 = 4.802132025361061e-06

Training epoch-37 batch-4
Running loss of epoch-37 batch-4 = 5.886657163500786e-06

Training epoch-37 batch-5
Running loss of epoch-37 batch-5 = 4.86103817820549e-06

Training epoch-37 batch-6
Running loss of epoch-37 batch-6 = 6.1390455812215805e-06

Training epoch-37 batch-7
Running loss of epoch-37 batch-7 = 4.938337951898575e-06

Training epoch-37 batch-8
Running loss of epoch-37 batch-8 = 4.448927938938141e-06

Training epoch-37 batch-9
Running loss of epoch-37 batch-9 = 5.530891939997673e-06

Training epoch-37 batch-10
Running loss of epoch-37 batch-10 = 3.1874515116214752e-06

Training epoch-37 batch-11
Running loss of epoch-37 batch-11 = 3.2798852771520615e-06

Training epoch-37 batch-12
Running loss of epoch-37 batch-12 = 2.929242327809334e-06

Training epoch-37 batch-13
Running loss of epoch-37 batch-13 = 4.1066668927669525e-06

Training epoch-37 batch-14
Running loss of epoch-37 batch-14 = 3.5336706787347794e-06

Training epoch-37 batch-15
Running loss of epoch-37 batch-15 = 3.814464434981346e-06

Training epoch-37 batch-16
Running loss of epoch-37 batch-16 = 6.949994713068008e-06

Training epoch-37 batch-17
Running loss of epoch-37 batch-17 = 2.9189977794885635e-06

Training epoch-37 batch-18
Running loss of epoch-37 batch-18 = 6.324611604213715e-06

Training epoch-37 batch-19
Running loss of epoch-37 batch-19 = 6.034737452864647e-06

Training epoch-37 batch-20
Running loss of epoch-37 batch-20 = 3.866618499159813e-06

Training epoch-37 batch-21
Running loss of epoch-37 batch-21 = 2.5476329028606415e-06

Training epoch-37 batch-22
Running loss of epoch-37 batch-22 = 4.787929356098175e-06

Training epoch-37 batch-23
Running loss of epoch-37 batch-23 = 3.915047273039818e-06

Training epoch-37 batch-24
Running loss of epoch-37 batch-24 = 4.9246009439229965e-06

Training epoch-37 batch-25
Running loss of epoch-37 batch-25 = 4.2298343032598495e-06

Training epoch-37 batch-26
Running loss of epoch-37 batch-26 = 3.4456606954336166e-06

Training epoch-37 batch-27
Running loss of epoch-37 batch-27 = 2.539483830332756e-06

Training epoch-37 batch-28
Running loss of epoch-37 batch-28 = 6.01145438849926e-06

Training epoch-37 batch-29
Running loss of epoch-37 batch-29 = 5.369773134589195e-06

Training epoch-37 batch-30
Running loss of epoch-37 batch-30 = 4.680128768086433e-06

Training epoch-37 batch-31
Running loss of epoch-37 batch-31 = 3.042863681912422e-06

Training epoch-37 batch-32
Running loss of epoch-37 batch-32 = 7.045920938253403e-06

Training epoch-37 batch-33
Running loss of epoch-37 batch-33 = 4.068482667207718e-06

Training epoch-37 batch-34
Running loss of epoch-37 batch-34 = 5.302252247929573e-06

Training epoch-37 batch-35
Running loss of epoch-37 batch-35 = 7.329974323511124e-06

Training epoch-37 batch-36
Running loss of epoch-37 batch-36 = 6.331130862236023e-06

Training epoch-37 batch-37
Running loss of epoch-37 batch-37 = 5.02169132232666e-06

Training epoch-37 batch-38
Running loss of epoch-37 batch-38 = 3.7527643144130707e-06

Training epoch-37 batch-39
Running loss of epoch-37 batch-39 = 5.244743078947067e-06

Training epoch-37 batch-40
Running loss of epoch-37 batch-40 = 5.674082785844803e-06

Training epoch-37 batch-41
Running loss of epoch-37 batch-41 = 4.434958100318909e-06

Training epoch-37 batch-42
Running loss of epoch-37 batch-42 = 2.8507784008979797e-06

Training epoch-37 batch-43
Running loss of epoch-37 batch-43 = 5.359528586268425e-06

Training epoch-37 batch-44
Running loss of epoch-37 batch-44 = 5.765119567513466e-06

Training epoch-37 batch-45
Running loss of epoch-37 batch-45 = 5.9979502111673355e-06

Training epoch-37 batch-46
Running loss of epoch-37 batch-46 = 2.5010667741298676e-06

Training epoch-37 batch-47
Running loss of epoch-37 batch-47 = 6.397254765033722e-06

Training epoch-37 batch-48
Running loss of epoch-37 batch-48 = 5.387002602219582e-06

Training epoch-37 batch-49
Running loss of epoch-37 batch-49 = 4.655215889215469e-06

Training epoch-37 batch-50
Running loss of epoch-37 batch-50 = 4.8852525651454926e-06

Training epoch-37 batch-51
Running loss of epoch-37 batch-51 = 5.040084943175316e-06

Training epoch-37 batch-52
Running loss of epoch-37 batch-52 = 3.3495016396045685e-06

Training epoch-37 batch-53
Running loss of epoch-37 batch-53 = 3.116205334663391e-06

Training epoch-37 batch-54
Running loss of epoch-37 batch-54 = 2.305954694747925e-06

Training epoch-37 batch-55
Running loss of epoch-37 batch-55 = 3.736000508069992e-06

Training epoch-37 batch-56
Running loss of epoch-37 batch-56 = 4.834495484828949e-06

Training epoch-37 batch-57
Running loss of epoch-37 batch-57 = 3.602122887969017e-06

Training epoch-37 batch-58
Running loss of epoch-37 batch-58 = 3.898050636053085e-06

Training epoch-37 batch-59
Running loss of epoch-37 batch-59 = 5.102716386318207e-06

Training epoch-37 batch-60
Running loss of epoch-37 batch-60 = 5.325768142938614e-06

Training epoch-37 batch-61
Running loss of epoch-37 batch-61 = 3.2025855034589767e-06

Training epoch-37 batch-62
Running loss of epoch-37 batch-62 = 6.091780960559845e-06

Training epoch-37 batch-63
Running loss of epoch-37 batch-63 = 4.198402166366577e-06

Training epoch-37 batch-64
Running loss of epoch-37 batch-64 = 4.965346306562424e-06

Training epoch-37 batch-65
Running loss of epoch-37 batch-65 = 4.304340109229088e-06

Training epoch-37 batch-66
Running loss of epoch-37 batch-66 = 4.376750439405441e-06

Training epoch-37 batch-67
Running loss of epoch-37 batch-67 = 5.087582394480705e-06

Training epoch-37 batch-68
Running loss of epoch-37 batch-68 = 3.5229604691267014e-06

Training epoch-37 batch-69
Running loss of epoch-37 batch-69 = 3.7169083952903748e-06

Training epoch-37 batch-70
Running loss of epoch-37 batch-70 = 4.904111847281456e-06

Training epoch-37 batch-71
Running loss of epoch-37 batch-71 = 4.291068762540817e-06

Training epoch-37 batch-72
Running loss of epoch-37 batch-72 = 1.535750925540924e-06

Training epoch-37 batch-73
Running loss of epoch-37 batch-73 = 5.288049578666687e-06

Training epoch-37 batch-74
Running loss of epoch-37 batch-74 = 4.3211039155721664e-06

Training epoch-37 batch-75
Running loss of epoch-37 batch-75 = 5.286885425448418e-06

Training epoch-37 batch-76
Running loss of epoch-37 batch-76 = 3.9851292967796326e-06

Training epoch-37 batch-77
Running loss of epoch-37 batch-77 = 5.702022463083267e-06

Training epoch-37 batch-78
Running loss of epoch-37 batch-78 = 5.214940756559372e-06

Training epoch-37 batch-79
Running loss of epoch-37 batch-79 = 5.816575139760971e-06

Training epoch-37 batch-80
Running loss of epoch-37 batch-80 = 4.2782630771398544e-06

Training epoch-37 batch-81
Running loss of epoch-37 batch-81 = 6.616115570068359e-06

Training epoch-37 batch-82
Running loss of epoch-37 batch-82 = 4.685483872890472e-06

Training epoch-37 batch-83
Running loss of epoch-37 batch-83 = 4.925532266497612e-06

Training epoch-37 batch-84
Running loss of epoch-37 batch-84 = 4.980713129043579e-06

Training epoch-37 batch-85
Running loss of epoch-37 batch-85 = 3.4708064049482346e-06

Training epoch-37 batch-86
Running loss of epoch-37 batch-86 = 4.17931005358696e-06

Training epoch-37 batch-87
Running loss of epoch-37 batch-87 = 3.064516931772232e-06

Training epoch-37 batch-88
Running loss of epoch-37 batch-88 = 5.01866452395916e-06

Training epoch-37 batch-89
Running loss of epoch-37 batch-89 = 5.166744813323021e-06

Training epoch-37 batch-90
Running loss of epoch-37 batch-90 = 3.562774509191513e-06

Training epoch-37 batch-91
Running loss of epoch-37 batch-91 = 4.027504473924637e-06

Training epoch-37 batch-92
Running loss of epoch-37 batch-92 = 3.5907141864299774e-06

Training epoch-37 batch-93
Running loss of epoch-37 batch-93 = 3.943219780921936e-06

Training epoch-37 batch-94
Running loss of epoch-37 batch-94 = 4.045665264129639e-06

Training epoch-37 batch-95
Running loss of epoch-37 batch-95 = 4.536006599664688e-06

Training epoch-37 batch-96
Running loss of epoch-37 batch-96 = 3.5907141864299774e-06

Training epoch-37 batch-97
Running loss of epoch-37 batch-97 = 4.659872502088547e-06

Training epoch-37 batch-98
Running loss of epoch-37 batch-98 = 4.6389177441596985e-06

Training epoch-37 batch-99
Running loss of epoch-37 batch-99 = 4.4244807213544846e-06

Training epoch-37 batch-100
Running loss of epoch-37 batch-100 = 3.22563573718071e-06

Training epoch-37 batch-101
Running loss of epoch-37 batch-101 = 3.741588443517685e-06

Training epoch-37 batch-102
Running loss of epoch-37 batch-102 = 2.662651240825653e-06

Training epoch-37 batch-103
Running loss of epoch-37 batch-103 = 4.610978066921234e-06

Training epoch-37 batch-104
Running loss of epoch-37 batch-104 = 3.3348333090543747e-06

Training epoch-37 batch-105
Running loss of epoch-37 batch-105 = 4.180939868092537e-06

Training epoch-37 batch-106
Running loss of epoch-37 batch-106 = 3.2351817935705185e-06

Training epoch-37 batch-107
Running loss of epoch-37 batch-107 = 5.2442774176597595e-06

Training epoch-37 batch-108
Running loss of epoch-37 batch-108 = 5.128094926476479e-06

Training epoch-37 batch-109
Running loss of epoch-37 batch-109 = 3.55718657374382e-06

Training epoch-37 batch-110
Running loss of epoch-37 batch-110 = 3.378605470061302e-06

Training epoch-37 batch-111
Running loss of epoch-37 batch-111 = 5.2442774176597595e-06

Training epoch-37 batch-112
Running loss of epoch-37 batch-112 = 4.0228478610515594e-06

Training epoch-37 batch-113
Running loss of epoch-37 batch-113 = 5.145324394106865e-06

Training epoch-37 batch-114
Running loss of epoch-37 batch-114 = 4.745554178953171e-06

Training epoch-37 batch-115
Running loss of epoch-37 batch-115 = 4.541827365756035e-06

Training epoch-37 batch-116
Running loss of epoch-37 batch-116 = 5.568377673625946e-06

Training epoch-37 batch-117
Running loss of epoch-37 batch-117 = 6.074551492929459e-06

Training epoch-37 batch-118
Running loss of epoch-37 batch-118 = 4.278495907783508e-06

Training epoch-37 batch-119
Running loss of epoch-37 batch-119 = 1.2437812983989716e-06

Training epoch-37 batch-120
Running loss of epoch-37 batch-120 = 4.32855449616909e-06

Training epoch-37 batch-121
Running loss of epoch-37 batch-121 = 4.051951691508293e-06

Training epoch-37 batch-122
Running loss of epoch-37 batch-122 = 7.0801470428705215e-06

Training epoch-37 batch-123
Running loss of epoch-37 batch-123 = 5.894573405385017e-06

Training epoch-37 batch-124
Running loss of epoch-37 batch-124 = 4.582339897751808e-06

Training epoch-37 batch-125
Running loss of epoch-37 batch-125 = 6.591435521841049e-06

Training epoch-37 batch-126
Running loss of epoch-37 batch-126 = 4.2996834963560104e-06

Training epoch-37 batch-127
Running loss of epoch-37 batch-127 = 4.6407803893089294e-06

Training epoch-37 batch-128
Running loss of epoch-37 batch-128 = 3.2586976885795593e-06

Training epoch-37 batch-129
Running loss of epoch-37 batch-129 = 5.9658195823431015e-06

Training epoch-37 batch-130
Running loss of epoch-37 batch-130 = 4.904810339212418e-06

Training epoch-37 batch-131
Running loss of epoch-37 batch-131 = 2.171844244003296e-06

Training epoch-37 batch-132
Running loss of epoch-37 batch-132 = 3.862427547574043e-06

Training epoch-37 batch-133
Running loss of epoch-37 batch-133 = 4.751840606331825e-06

Training epoch-37 batch-134
Running loss of epoch-37 batch-134 = 3.859866410493851e-06

Training epoch-37 batch-135
Running loss of epoch-37 batch-135 = 3.4561380743980408e-06

Training epoch-37 batch-136
Running loss of epoch-37 batch-136 = 3.3660326153039932e-06

Training epoch-37 batch-137
Running loss of epoch-37 batch-137 = 4.756497219204903e-06

Training epoch-37 batch-138
Running loss of epoch-37 batch-138 = 4.075467586517334e-06

Training epoch-37 batch-139
Running loss of epoch-37 batch-139 = 4.365108907222748e-06

Training epoch-37 batch-140
Running loss of epoch-37 batch-140 = 5.511566996574402e-06

Training epoch-37 batch-141
Running loss of epoch-37 batch-141 = 5.025416612625122e-06

Training epoch-37 batch-142
Running loss of epoch-37 batch-142 = 3.277324140071869e-06

Training epoch-37 batch-143
Running loss of epoch-37 batch-143 = 4.065223038196564e-06

Training epoch-37 batch-144
Running loss of epoch-37 batch-144 = 4.835193976759911e-06

Training epoch-37 batch-145
Running loss of epoch-37 batch-145 = 3.4908298403024673e-06

Training epoch-37 batch-146
Running loss of epoch-37 batch-146 = 3.492925316095352e-06

Training epoch-37 batch-147
Running loss of epoch-37 batch-147 = 3.5245902836322784e-06

Training epoch-37 batch-148
Running loss of epoch-37 batch-148 = 3.2212119549512863e-06

Training epoch-37 batch-149
Running loss of epoch-37 batch-149 = 3.822380676865578e-06

Training epoch-37 batch-150
Running loss of epoch-37 batch-150 = 5.243578925728798e-06

Training epoch-37 batch-151
Running loss of epoch-37 batch-151 = 3.7602148950099945e-06

Training epoch-37 batch-152
Running loss of epoch-37 batch-152 = 3.8000289350748062e-06

Training epoch-37 batch-153
Running loss of epoch-37 batch-153 = 5.391659215092659e-06

Training epoch-37 batch-154
Running loss of epoch-37 batch-154 = 8.54814425110817e-06

Training epoch-37 batch-155
Running loss of epoch-37 batch-155 = 3.5741832107305527e-06

Training epoch-37 batch-156
Running loss of epoch-37 batch-156 = 4.425644874572754e-06

Training epoch-37 batch-157
Running loss of epoch-37 batch-157 = 1.4349818229675293e-05

Finished training epoch-37.



Average train loss at epoch-37 = 4.492491483688354e-06

Started Evaluation

Average val loss at epoch-37 = 3.012802059713163

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.75 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.57 %

Finished Evaluation



Started training epoch-38


Training epoch-38 batch-1
Running loss of epoch-38 batch-1 = 3.8906000554561615e-06

Training epoch-38 batch-2
Running loss of epoch-38 batch-2 = 3.52528877556324e-06

Training epoch-38 batch-3
Running loss of epoch-38 batch-3 = 4.641478881239891e-06

Training epoch-38 batch-4
Running loss of epoch-38 batch-4 = 4.891306161880493e-06

Training epoch-38 batch-5
Running loss of epoch-38 batch-5 = 4.863599315285683e-06

Training epoch-38 batch-6
Running loss of epoch-38 batch-6 = 3.8335565477609634e-06

Training epoch-38 batch-7
Running loss of epoch-38 batch-7 = 4.249392077326775e-06

Training epoch-38 batch-8
Running loss of epoch-38 batch-8 = 5.174893885850906e-06

Training epoch-38 batch-9
Running loss of epoch-38 batch-9 = 4.152068868279457e-06

Training epoch-38 batch-10
Running loss of epoch-38 batch-10 = 3.7455465644598007e-06

Training epoch-38 batch-11
Running loss of epoch-38 batch-11 = 2.2747553884983063e-06

Training epoch-38 batch-12
Running loss of epoch-38 batch-12 = 4.314817488193512e-06

Training epoch-38 batch-13
Running loss of epoch-38 batch-13 = 3.495020791888237e-06

Training epoch-38 batch-14
Running loss of epoch-38 batch-14 = 2.489890903234482e-06

Training epoch-38 batch-15
Running loss of epoch-38 batch-15 = 3.7886202335357666e-06

Training epoch-38 batch-16
Running loss of epoch-38 batch-16 = 4.762317985296249e-06

Training epoch-38 batch-17
Running loss of epoch-38 batch-17 = 3.718305379152298e-06

Training epoch-38 batch-18
Running loss of epoch-38 batch-18 = 3.949739038944244e-06

Training epoch-38 batch-19
Running loss of epoch-38 batch-19 = 5.208887159824371e-06

Training epoch-38 batch-20
Running loss of epoch-38 batch-20 = 3.1653326004743576e-06

Training epoch-38 batch-21
Running loss of epoch-38 batch-21 = 6.242189556360245e-06

Training epoch-38 batch-22
Running loss of epoch-38 batch-22 = 5.038222298026085e-06

Training epoch-38 batch-23
Running loss of epoch-38 batch-23 = 3.417953848838806e-06

Training epoch-38 batch-24
Running loss of epoch-38 batch-24 = 4.050089046359062e-06

Training epoch-38 batch-25
Running loss of epoch-38 batch-25 = 4.961388185620308e-06

Training epoch-38 batch-26
Running loss of epoch-38 batch-26 = 4.891306161880493e-06

Training epoch-38 batch-27
Running loss of epoch-38 batch-27 = 3.0172523111104965e-06

Training epoch-38 batch-28
Running loss of epoch-38 batch-28 = 4.365108907222748e-06

Training epoch-38 batch-29
Running loss of epoch-38 batch-29 = 4.124827682971954e-06

Training epoch-38 batch-30
Running loss of epoch-38 batch-30 = 2.72924080491066e-06

Training epoch-38 batch-31
Running loss of epoch-38 batch-31 = 3.4207478165626526e-06

Training epoch-38 batch-32
Running loss of epoch-38 batch-32 = 3.97232361137867e-06

Training epoch-38 batch-33
Running loss of epoch-38 batch-33 = 5.414942279458046e-06

Training epoch-38 batch-34
Running loss of epoch-38 batch-34 = 3.878260031342506e-06

Training epoch-38 batch-35
Running loss of epoch-38 batch-35 = 2.503860741853714e-06

Training epoch-38 batch-36
Running loss of epoch-38 batch-36 = 3.889435902237892e-06

Training epoch-38 batch-37
Running loss of epoch-38 batch-37 = 5.553476512432098e-06

Training epoch-38 batch-38
Running loss of epoch-38 batch-38 = 4.7888606786727905e-06

Training epoch-38 batch-39
Running loss of epoch-38 batch-39 = 5.500158295035362e-06

Training epoch-38 batch-40
Running loss of epoch-38 batch-40 = 3.5224948078393936e-06

Training epoch-38 batch-41
Running loss of epoch-38 batch-41 = 2.927146852016449e-06

Training epoch-38 batch-42
Running loss of epoch-38 batch-42 = 4.546716809272766e-06

Training epoch-38 batch-43
Running loss of epoch-38 batch-43 = 4.058238118886948e-06

Training epoch-38 batch-44
Running loss of epoch-38 batch-44 = 4.9711670726537704e-06

Training epoch-38 batch-45
Running loss of epoch-38 batch-45 = 3.82610596716404e-06

Training epoch-38 batch-46
Running loss of epoch-38 batch-46 = 2.6954803615808487e-06

Training epoch-38 batch-47
Running loss of epoch-38 batch-47 = 3.614695742726326e-06

Training epoch-38 batch-48
Running loss of epoch-38 batch-48 = 4.59328293800354e-06

Training epoch-38 batch-49
Running loss of epoch-38 batch-49 = 5.84661029279232e-06

Training epoch-38 batch-50
Running loss of epoch-38 batch-50 = 3.252411261200905e-06

Training epoch-38 batch-51
Running loss of epoch-38 batch-51 = 5.33577986061573e-06

Training epoch-38 batch-52
Running loss of epoch-38 batch-52 = 3.827037289738655e-06

Training epoch-38 batch-53
Running loss of epoch-38 batch-53 = 3.377208486199379e-06

Training epoch-38 batch-54
Running loss of epoch-38 batch-54 = 4.270579665899277e-06

Training epoch-38 batch-55
Running loss of epoch-38 batch-55 = 2.4104956537485123e-06

Training epoch-38 batch-56
Running loss of epoch-38 batch-56 = 4.553468897938728e-06

Training epoch-38 batch-57
Running loss of epoch-38 batch-57 = 4.203291609883308e-06

Training epoch-38 batch-58
Running loss of epoch-38 batch-58 = 3.7690624594688416e-06

Training epoch-38 batch-59
Running loss of epoch-38 batch-59 = 3.3278483897447586e-06

Training epoch-38 batch-60
Running loss of epoch-38 batch-60 = 3.958819434046745e-06

Training epoch-38 batch-61
Running loss of epoch-38 batch-61 = 4.720874130725861e-06

Training epoch-38 batch-62
Running loss of epoch-38 batch-62 = 2.357643097639084e-06

Training epoch-38 batch-63
Running loss of epoch-38 batch-63 = 4.802597686648369e-06

Training epoch-38 batch-64
Running loss of epoch-38 batch-64 = 3.3634714782238007e-06

Training epoch-38 batch-65
Running loss of epoch-38 batch-65 = 3.2244715839624405e-06

Training epoch-38 batch-66
Running loss of epoch-38 batch-66 = 4.044733941555023e-06

Training epoch-38 batch-67
Running loss of epoch-38 batch-67 = 3.3765099942684174e-06

Training epoch-38 batch-68
Running loss of epoch-38 batch-68 = 3.737863153219223e-06

Training epoch-38 batch-69
Running loss of epoch-38 batch-69 = 4.616798833012581e-06

Training epoch-38 batch-70
Running loss of epoch-38 batch-70 = 5.198875442147255e-06

Training epoch-38 batch-71
Running loss of epoch-38 batch-71 = 2.8153881430625916e-06

Training epoch-38 batch-72
Running loss of epoch-38 batch-72 = 4.534376785159111e-06

Training epoch-38 batch-73
Running loss of epoch-38 batch-73 = 4.011671990156174e-06

Training epoch-38 batch-74
Running loss of epoch-38 batch-74 = 5.2102841436862946e-06

Training epoch-38 batch-75
Running loss of epoch-38 batch-75 = 4.927394911646843e-06

Training epoch-38 batch-76
Running loss of epoch-38 batch-76 = 6.972113624215126e-06

Training epoch-38 batch-77
Running loss of epoch-38 batch-77 = 5.597248673439026e-06

Training epoch-38 batch-78
Running loss of epoch-38 batch-78 = 3.632623702287674e-06

Training epoch-38 batch-79
Running loss of epoch-38 batch-79 = 3.116670995950699e-06

Training epoch-38 batch-80
Running loss of epoch-38 batch-80 = 4.025641828775406e-06

Training epoch-38 batch-81
Running loss of epoch-38 batch-81 = 3.027031198143959e-06

Training epoch-38 batch-82
Running loss of epoch-38 batch-82 = 5.795154720544815e-06

Training epoch-38 batch-83
Running loss of epoch-38 batch-83 = 3.036344423890114e-06

Training epoch-38 batch-84
Running loss of epoch-38 batch-84 = 4.3995678424835205e-06

Training epoch-38 batch-85
Running loss of epoch-38 batch-85 = 4.653818905353546e-06

Training epoch-38 batch-86
Running loss of epoch-38 batch-86 = 2.734130248427391e-06

Training epoch-38 batch-87
Running loss of epoch-38 batch-87 = 5.3015537559986115e-06

Training epoch-38 batch-88
Running loss of epoch-38 batch-88 = 5.781417712569237e-06

Training epoch-38 batch-89
Running loss of epoch-38 batch-89 = 6.97234645485878e-06

Training epoch-38 batch-90
Running loss of epoch-38 batch-90 = 3.852648660540581e-06

Training epoch-38 batch-91
Running loss of epoch-38 batch-91 = 3.741821274161339e-06

Training epoch-38 batch-92
Running loss of epoch-38 batch-92 = 4.736706614494324e-06

Training epoch-38 batch-93
Running loss of epoch-38 batch-93 = 3.3583492040634155e-06

Training epoch-38 batch-94
Running loss of epoch-38 batch-94 = 3.651948645710945e-06

Training epoch-38 batch-95
Running loss of epoch-38 batch-95 = 3.473833203315735e-06

Training epoch-38 batch-96
Running loss of epoch-38 batch-96 = 4.763249307870865e-06

Training epoch-38 batch-97
Running loss of epoch-38 batch-97 = 4.480127245187759e-06

Training epoch-38 batch-98
Running loss of epoch-38 batch-98 = 2.734363079071045e-06

Training epoch-38 batch-99
Running loss of epoch-38 batch-99 = 4.2906031012535095e-06

Training epoch-38 batch-100
Running loss of epoch-38 batch-100 = 5.011213943362236e-06

Training epoch-38 batch-101
Running loss of epoch-38 batch-101 = 7.692491635680199e-06

Training epoch-38 batch-102
Running loss of epoch-38 batch-102 = 6.333459168672562e-06

Training epoch-38 batch-103
Running loss of epoch-38 batch-103 = 3.902008756995201e-06

Training epoch-38 batch-104
Running loss of epoch-38 batch-104 = 6.263609975576401e-06

Training epoch-38 batch-105
Running loss of epoch-38 batch-105 = 2.7550850063562393e-06

Training epoch-38 batch-106
Running loss of epoch-38 batch-106 = 2.864515408873558e-06

Training epoch-38 batch-107
Running loss of epoch-38 batch-107 = 4.457077011466026e-06

Training epoch-38 batch-108
Running loss of epoch-38 batch-108 = 5.330424755811691e-06

Training epoch-38 batch-109
Running loss of epoch-38 batch-109 = 6.6373031586408615e-06

Training epoch-38 batch-110
Running loss of epoch-38 batch-110 = 6.505521014332771e-06

Training epoch-38 batch-111
Running loss of epoch-38 batch-111 = 6.349291652441025e-06

Training epoch-38 batch-112
Running loss of epoch-38 batch-112 = 7.95675441622734e-06

Training epoch-38 batch-113
Running loss of epoch-38 batch-113 = 5.294568836688995e-06

Training epoch-38 batch-114
Running loss of epoch-38 batch-114 = 3.7634745240211487e-06

Training epoch-38 batch-115
Running loss of epoch-38 batch-115 = 3.87546606361866e-06

Training epoch-38 batch-116
Running loss of epoch-38 batch-116 = 4.72203828394413e-06

Training epoch-38 batch-117
Running loss of epoch-38 batch-117 = 3.987457603216171e-06

Training epoch-38 batch-118
Running loss of epoch-38 batch-118 = 2.382555976510048e-06

Training epoch-38 batch-119
Running loss of epoch-38 batch-119 = 4.122965037822723e-06

Training epoch-38 batch-120
Running loss of epoch-38 batch-120 = 4.862900823354721e-06

Training epoch-38 batch-121
Running loss of epoch-38 batch-121 = 4.9746595323085785e-06

Training epoch-38 batch-122
Running loss of epoch-38 batch-122 = 5.514593794941902e-06

Training epoch-38 batch-123
Running loss of epoch-38 batch-123 = 3.5332050174474716e-06

Training epoch-38 batch-124
Running loss of epoch-38 batch-124 = 4.6710483729839325e-06

Training epoch-38 batch-125
Running loss of epoch-38 batch-125 = 4.411675035953522e-06

Training epoch-38 batch-126
Running loss of epoch-38 batch-126 = 5.259411409497261e-06

Training epoch-38 batch-127
Running loss of epoch-38 batch-127 = 3.8745347410440445e-06

Training epoch-38 batch-128
Running loss of epoch-38 batch-128 = 6.06919638812542e-06

Training epoch-38 batch-129
Running loss of epoch-38 batch-129 = 3.889668732881546e-06

Training epoch-38 batch-130
Running loss of epoch-38 batch-130 = 3.327382728457451e-06

Training epoch-38 batch-131
Running loss of epoch-38 batch-131 = 3.952300176024437e-06

Training epoch-38 batch-132
Running loss of epoch-38 batch-132 = 4.5602209866046906e-06

Training epoch-38 batch-133
Running loss of epoch-38 batch-133 = 4.065223038196564e-06

Training epoch-38 batch-134
Running loss of epoch-38 batch-134 = 2.9481016099452972e-06

Training epoch-38 batch-135
Running loss of epoch-38 batch-135 = 3.2938551157712936e-06

Training epoch-38 batch-136
Running loss of epoch-38 batch-136 = 3.116438165307045e-06

Training epoch-38 batch-137
Running loss of epoch-38 batch-137 = 4.013068974018097e-06

Training epoch-38 batch-138
Running loss of epoch-38 batch-138 = 4.18233685195446e-06

Training epoch-38 batch-139
Running loss of epoch-38 batch-139 = 4.08967025578022e-06

Training epoch-38 batch-140
Running loss of epoch-38 batch-140 = 3.852881491184235e-06

Training epoch-38 batch-141
Running loss of epoch-38 batch-141 = 2.8437934815883636e-06

Training epoch-38 batch-142
Running loss of epoch-38 batch-142 = 4.746951162815094e-06

Training epoch-38 batch-143
Running loss of epoch-38 batch-143 = 2.7494970709085464e-06

Training epoch-38 batch-144
Running loss of epoch-38 batch-144 = 4.703877493739128e-06

Training epoch-38 batch-145
Running loss of epoch-38 batch-145 = 3.3963005989789963e-06

Training epoch-38 batch-146
Running loss of epoch-38 batch-146 = 4.691071808338165e-06

Training epoch-38 batch-147
Running loss of epoch-38 batch-147 = 6.048474460840225e-06

Training epoch-38 batch-148
Running loss of epoch-38 batch-148 = 5.268258973956108e-06

Training epoch-38 batch-149
Running loss of epoch-38 batch-149 = 5.923444405198097e-06

Training epoch-38 batch-150
Running loss of epoch-38 batch-150 = 6.9465022534132e-06

Training epoch-38 batch-151
Running loss of epoch-38 batch-151 = 4.266854375600815e-06

Training epoch-38 batch-152
Running loss of epoch-38 batch-152 = 4.196772351861e-06

Training epoch-38 batch-153
Running loss of epoch-38 batch-153 = 4.028435796499252e-06

Training epoch-38 batch-154
Running loss of epoch-38 batch-154 = 2.8889626264572144e-06

Training epoch-38 batch-155
Running loss of epoch-38 batch-155 = 5.375826731324196e-06

Training epoch-38 batch-156
Running loss of epoch-38 batch-156 = 4.313653334975243e-06

Training epoch-38 batch-157
Running loss of epoch-38 batch-157 = 1.3988465070724487e-05

Finished training epoch-38.



Average train loss at epoch-38 = 4.316228628158569e-06

Started Evaluation

Average val loss at epoch-38 = 3.017068310787803

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.93 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-39


Training epoch-39 batch-1
Running loss of epoch-39 batch-1 = 5.725771188735962e-06

Training epoch-39 batch-2
Running loss of epoch-39 batch-2 = 4.025176167488098e-06

Training epoch-39 batch-3
Running loss of epoch-39 batch-3 = 3.3741816878318787e-06

Training epoch-39 batch-4
Running loss of epoch-39 batch-4 = 2.3972243070602417e-06

Training epoch-39 batch-5
Running loss of epoch-39 batch-5 = 3.7499703466892242e-06

Training epoch-39 batch-6
Running loss of epoch-39 batch-6 = 4.085712134838104e-06

Training epoch-39 batch-7
Running loss of epoch-39 batch-7 = 4.016095772385597e-06

Training epoch-39 batch-8
Running loss of epoch-39 batch-8 = 3.5248231142759323e-06

Training epoch-39 batch-9
Running loss of epoch-39 batch-9 = 4.202360287308693e-06

Training epoch-39 batch-10
Running loss of epoch-39 batch-10 = 3.757188096642494e-06

Training epoch-39 batch-11
Running loss of epoch-39 batch-11 = 5.2943360060453415e-06

Training epoch-39 batch-12
Running loss of epoch-39 batch-12 = 4.06499020755291e-06

Training epoch-39 batch-13
Running loss of epoch-39 batch-13 = 4.0011946111917496e-06

Training epoch-39 batch-14
Running loss of epoch-39 batch-14 = 2.896646037697792e-06

Training epoch-39 batch-15
Running loss of epoch-39 batch-15 = 2.5650952011346817e-06

Training epoch-39 batch-16
Running loss of epoch-39 batch-16 = 3.1578820198774338e-06

Training epoch-39 batch-17
Running loss of epoch-39 batch-17 = 4.738103598356247e-06

Training epoch-39 batch-18
Running loss of epoch-39 batch-18 = 3.37185338139534e-06

Training epoch-39 batch-19
Running loss of epoch-39 batch-19 = 4.353933036327362e-06

Training epoch-39 batch-20
Running loss of epoch-39 batch-20 = 2.764863893389702e-06

Training epoch-39 batch-21
Running loss of epoch-39 batch-21 = 3.857538104057312e-06

Training epoch-39 batch-22
Running loss of epoch-39 batch-22 = 3.9620790630578995e-06

Training epoch-39 batch-23
Running loss of epoch-39 batch-23 = 4.788162186741829e-06

Training epoch-39 batch-24
Running loss of epoch-39 batch-24 = 3.6980491131544113e-06

Training epoch-39 batch-25
Running loss of epoch-39 batch-25 = 2.964632585644722e-06

Training epoch-39 batch-26
Running loss of epoch-39 batch-26 = 5.318783223628998e-06

Training epoch-39 batch-27
Running loss of epoch-39 batch-27 = 3.9227306842803955e-06

Training epoch-39 batch-28
Running loss of epoch-39 batch-28 = 3.6910641938447952e-06

Training epoch-39 batch-29
Running loss of epoch-39 batch-29 = 4.484085366129875e-06

Training epoch-39 batch-30
Running loss of epoch-39 batch-30 = 2.7944333851337433e-06

Training epoch-39 batch-31
Running loss of epoch-39 batch-31 = 4.700152203440666e-06

Training epoch-39 batch-32
Running loss of epoch-39 batch-32 = 2.693384885787964e-06

Training epoch-39 batch-33
Running loss of epoch-39 batch-33 = 4.2691826820373535e-06

Training epoch-39 batch-34
Running loss of epoch-39 batch-34 = 4.467088729143143e-06

Training epoch-39 batch-35
Running loss of epoch-39 batch-35 = 4.10829670727253e-06

Training epoch-39 batch-36
Running loss of epoch-39 batch-36 = 3.511086106300354e-06

Training epoch-39 batch-37
Running loss of epoch-39 batch-37 = 3.7443824112415314e-06

Training epoch-39 batch-38
Running loss of epoch-39 batch-38 = 4.250556230545044e-06

Training epoch-39 batch-39
Running loss of epoch-39 batch-39 = 4.653586074709892e-06

Training epoch-39 batch-40
Running loss of epoch-39 batch-40 = 4.074070602655411e-06

Training epoch-39 batch-41
Running loss of epoch-39 batch-41 = 5.619833245873451e-06

Training epoch-39 batch-42
Running loss of epoch-39 batch-42 = 3.7101563066244125e-06

Training epoch-39 batch-43
Running loss of epoch-39 batch-43 = 2.9192306101322174e-06

Training epoch-39 batch-44
Running loss of epoch-39 batch-44 = 4.012603312730789e-06

Training epoch-39 batch-45
Running loss of epoch-39 batch-45 = 3.7581194192171097e-06

Training epoch-39 batch-46
Running loss of epoch-39 batch-46 = 3.881053999066353e-06

Training epoch-39 batch-47
Running loss of epoch-39 batch-47 = 3.7744175642728806e-06

Training epoch-39 batch-48
Running loss of epoch-39 batch-48 = 5.312729626893997e-06

Training epoch-39 batch-49
Running loss of epoch-39 batch-49 = 4.334608092904091e-06

Training epoch-39 batch-50
Running loss of epoch-39 batch-50 = 3.6454293876886368e-06

Training epoch-39 batch-51
Running loss of epoch-39 batch-51 = 4.859408363699913e-06

Training epoch-39 batch-52
Running loss of epoch-39 batch-52 = 2.3455359041690826e-06

Training epoch-39 batch-53
Running loss of epoch-39 batch-53 = 3.103865310549736e-06

Training epoch-39 batch-54
Running loss of epoch-39 batch-54 = 5.4943375289440155e-06

Training epoch-39 batch-55
Running loss of epoch-39 batch-55 = 2.6188790798187256e-06

Training epoch-39 batch-56
Running loss of epoch-39 batch-56 = 4.808185622096062e-06

Training epoch-39 batch-57
Running loss of epoch-39 batch-57 = 4.503410309553146e-06

Training epoch-39 batch-58
Running loss of epoch-39 batch-58 = 4.745554178953171e-06

Training epoch-39 batch-59
Running loss of epoch-39 batch-59 = 2.421904355287552e-06

Training epoch-39 batch-60
Running loss of epoch-39 batch-60 = 3.950437530875206e-06

Training epoch-39 batch-61
Running loss of epoch-39 batch-61 = 4.689209163188934e-06

Training epoch-39 batch-62
Running loss of epoch-39 batch-62 = 7.965369150042534e-06

Training epoch-39 batch-63
Running loss of epoch-39 batch-63 = 7.041264325380325e-06

Training epoch-39 batch-64
Running loss of epoch-39 batch-64 = 3.316672518849373e-06

Training epoch-39 batch-65
Running loss of epoch-39 batch-65 = 3.4668482840061188e-06

Training epoch-39 batch-66
Running loss of epoch-39 batch-66 = 3.3159740269184113e-06

Training epoch-39 batch-67
Running loss of epoch-39 batch-67 = 5.843350663781166e-06

Training epoch-39 batch-68
Running loss of epoch-39 batch-68 = 4.859175533056259e-06

Training epoch-39 batch-69
Running loss of epoch-39 batch-69 = 4.565343260765076e-06

Training epoch-39 batch-70
Running loss of epoch-39 batch-70 = 7.609138265252113e-06

Training epoch-39 batch-71
Running loss of epoch-39 batch-71 = 4.910631105303764e-06

Training epoch-39 batch-72
Running loss of epoch-39 batch-72 = 3.2228417694568634e-06

Training epoch-39 batch-73
Running loss of epoch-39 batch-73 = 4.4782646000385284e-06

Training epoch-39 batch-74
Running loss of epoch-39 batch-74 = 4.523666575551033e-06

Training epoch-39 batch-75
Running loss of epoch-39 batch-75 = 3.670342266559601e-06

Training epoch-39 batch-76
Running loss of epoch-39 batch-76 = 3.2330863177776337e-06

Training epoch-39 batch-77
Running loss of epoch-39 batch-77 = 5.94649463891983e-06

Training epoch-39 batch-78
Running loss of epoch-39 batch-78 = 5.8754812926054e-06

Training epoch-39 batch-79
Running loss of epoch-39 batch-79 = 3.958819434046745e-06

Training epoch-39 batch-80
Running loss of epoch-39 batch-80 = 2.9760412871837616e-06

Training epoch-39 batch-81
Running loss of epoch-39 batch-81 = 4.489440470933914e-06

Training epoch-39 batch-82
Running loss of epoch-39 batch-82 = 2.6794150471687317e-06

Training epoch-39 batch-83
Running loss of epoch-39 batch-83 = 4.121335223317146e-06

Training epoch-39 batch-84
Running loss of epoch-39 batch-84 = 3.5760458558797836e-06

Training epoch-39 batch-85
Running loss of epoch-39 batch-85 = 4.143454134464264e-06

Training epoch-39 batch-86
Running loss of epoch-39 batch-86 = 5.394918844103813e-06

Training epoch-39 batch-87
Running loss of epoch-39 batch-87 = 3.5511329770088196e-06

Training epoch-39 batch-88
Running loss of epoch-39 batch-88 = 2.9336661100387573e-06

Training epoch-39 batch-89
Running loss of epoch-39 batch-89 = 5.5835116654634476e-06

Training epoch-39 batch-90
Running loss of epoch-39 batch-90 = 2.8221402317285538e-06

Training epoch-39 batch-91
Running loss of epoch-39 batch-91 = 3.3208634704351425e-06

Training epoch-39 batch-92
Running loss of epoch-39 batch-92 = 4.168134182691574e-06

Training epoch-39 batch-93
Running loss of epoch-39 batch-93 = 6.863614544272423e-06

Training epoch-39 batch-94
Running loss of epoch-39 batch-94 = 4.6014320105314255e-06

Training epoch-39 batch-95
Running loss of epoch-39 batch-95 = 8.119968697428703e-06

Training epoch-39 batch-96
Running loss of epoch-39 batch-96 = 2.2957101464271545e-06

Training epoch-39 batch-97
Running loss of epoch-39 batch-97 = 4.866626113653183e-06

Training epoch-39 batch-98
Running loss of epoch-39 batch-98 = 4.217028617858887e-06

Training epoch-39 batch-99
Running loss of epoch-39 batch-99 = 4.639849066734314e-06

Training epoch-39 batch-100
Running loss of epoch-39 batch-100 = 3.45730222761631e-06

Training epoch-39 batch-101
Running loss of epoch-39 batch-101 = 4.254747182130814e-06

Training epoch-39 batch-102
Running loss of epoch-39 batch-102 = 3.7818681448698044e-06

Training epoch-39 batch-103
Running loss of epoch-39 batch-103 = 3.2247044146060944e-06

Training epoch-39 batch-104
Running loss of epoch-39 batch-104 = 3.898283466696739e-06

Training epoch-39 batch-105
Running loss of epoch-39 batch-105 = 2.8386712074279785e-06

Training epoch-39 batch-106
Running loss of epoch-39 batch-106 = 4.715053364634514e-06

Training epoch-39 batch-107
Running loss of epoch-39 batch-107 = 2.3613683879375458e-06

Training epoch-39 batch-108
Running loss of epoch-39 batch-108 = 2.859160304069519e-06

Training epoch-39 batch-109
Running loss of epoch-39 batch-109 = 3.08966264128685e-06

Training epoch-39 batch-110
Running loss of epoch-39 batch-110 = 3.866385668516159e-06

Training epoch-39 batch-111
Running loss of epoch-39 batch-111 = 6.547197699546814e-06

Training epoch-39 batch-112
Running loss of epoch-39 batch-112 = 5.830544978380203e-06

Training epoch-39 batch-113
Running loss of epoch-39 batch-113 = 6.592134013772011e-06

Training epoch-39 batch-114
Running loss of epoch-39 batch-114 = 4.7390349209308624e-06

Training epoch-39 batch-115
Running loss of epoch-39 batch-115 = 2.110609784722328e-06

Training epoch-39 batch-116
Running loss of epoch-39 batch-116 = 5.241017788648605e-06

Training epoch-39 batch-117
Running loss of epoch-39 batch-117 = 4.114117473363876e-06

Training epoch-39 batch-118
Running loss of epoch-39 batch-118 = 4.82914038002491e-06

Training epoch-39 batch-119
Running loss of epoch-39 batch-119 = 5.391892045736313e-06

Training epoch-39 batch-120
Running loss of epoch-39 batch-120 = 4.0549784898757935e-06

Training epoch-39 batch-121
Running loss of epoch-39 batch-121 = 3.5853590816259384e-06

Training epoch-39 batch-122
Running loss of epoch-39 batch-122 = 4.112720489501953e-06

Training epoch-39 batch-123
Running loss of epoch-39 batch-123 = 5.433103069663048e-06

Training epoch-39 batch-124
Running loss of epoch-39 batch-124 = 5.751149728894234e-06

Training epoch-39 batch-125
Running loss of epoch-39 batch-125 = 2.8533395379781723e-06

Training epoch-39 batch-126
Running loss of epoch-39 batch-126 = 2.857297658920288e-06

Training epoch-39 batch-127
Running loss of epoch-39 batch-127 = 3.910623490810394e-06

Training epoch-39 batch-128
Running loss of epoch-39 batch-128 = 2.7222558856010437e-06

Training epoch-39 batch-129
Running loss of epoch-39 batch-129 = 4.230299964547157e-06

Training epoch-39 batch-130
Running loss of epoch-39 batch-130 = 3.335997462272644e-06

Training epoch-39 batch-131
Running loss of epoch-39 batch-131 = 4.177913069725037e-06

Training epoch-39 batch-132
Running loss of epoch-39 batch-132 = 2.5972258299589157e-06

Training epoch-39 batch-133
Running loss of epoch-39 batch-133 = 4.518777132034302e-06

Training epoch-39 batch-134
Running loss of epoch-39 batch-134 = 4.2121391743421555e-06

Training epoch-39 batch-135
Running loss of epoch-39 batch-135 = 4.774425178766251e-06

Training epoch-39 batch-136
Running loss of epoch-39 batch-136 = 4.561152309179306e-06

Training epoch-39 batch-137
Running loss of epoch-39 batch-137 = 5.576061084866524e-06

Training epoch-39 batch-138
Running loss of epoch-39 batch-138 = 3.730878233909607e-06

Training epoch-39 batch-139
Running loss of epoch-39 batch-139 = 3.3369287848472595e-06

Training epoch-39 batch-140
Running loss of epoch-39 batch-140 = 2.9550865292549133e-06

Training epoch-39 batch-141
Running loss of epoch-39 batch-141 = 3.904104232788086e-06

Training epoch-39 batch-142
Running loss of epoch-39 batch-142 = 3.858935087919235e-06

Training epoch-39 batch-143
Running loss of epoch-39 batch-143 = 3.403984010219574e-06

Training epoch-39 batch-144
Running loss of epoch-39 batch-144 = 5.286885425448418e-06

Training epoch-39 batch-145
Running loss of epoch-39 batch-145 = 3.5332050174474716e-06

Training epoch-39 batch-146
Running loss of epoch-39 batch-146 = 5.59631735086441e-06

Training epoch-39 batch-147
Running loss of epoch-39 batch-147 = 5.1995739340782166e-06

Training epoch-39 batch-148
Running loss of epoch-39 batch-148 = 2.569984644651413e-06

Training epoch-39 batch-149
Running loss of epoch-39 batch-149 = 3.789318725466728e-06

Training epoch-39 batch-150
Running loss of epoch-39 batch-150 = 1.801876351237297e-06

Training epoch-39 batch-151
Running loss of epoch-39 batch-151 = 4.452187567949295e-06

Training epoch-39 batch-152
Running loss of epoch-39 batch-152 = 4.293164238333702e-06

Training epoch-39 batch-153
Running loss of epoch-39 batch-153 = 4.025176167488098e-06

Training epoch-39 batch-154
Running loss of epoch-39 batch-154 = 4.450324922800064e-06

Training epoch-39 batch-155
Running loss of epoch-39 batch-155 = 4.028668627142906e-06

Training epoch-39 batch-156
Running loss of epoch-39 batch-156 = 2.8086360543966293e-06

Training epoch-39 batch-157
Running loss of epoch-39 batch-157 = 1.4878809452056885e-05

Finished training epoch-39.



Average train loss at epoch-39 = 4.143676161766052e-06

Started Evaluation

Average val loss at epoch-39 = 3.035748376658088

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-40


Training epoch-40 batch-1
Running loss of epoch-40 batch-1 = 2.871965989470482e-06

Training epoch-40 batch-2
Running loss of epoch-40 batch-2 = 4.772329702973366e-06

Training epoch-40 batch-3
Running loss of epoch-40 batch-3 = 3.6857090890407562e-06

Training epoch-40 batch-4
Running loss of epoch-40 batch-4 = 4.2137689888477325e-06

Training epoch-40 batch-5
Running loss of epoch-40 batch-5 = 4.359986633062363e-06

Training epoch-40 batch-6
Running loss of epoch-40 batch-6 = 2.7033966034650803e-06

Training epoch-40 batch-7
Running loss of epoch-40 batch-7 = 3.520399332046509e-06

Training epoch-40 batch-8
Running loss of epoch-40 batch-8 = 3.6889687180519104e-06

Training epoch-40 batch-9
Running loss of epoch-40 batch-9 = 3.1918752938508987e-06

Training epoch-40 batch-10
Running loss of epoch-40 batch-10 = 4.100613296031952e-06

Training epoch-40 batch-11
Running loss of epoch-40 batch-11 = 5.696434527635574e-06

Training epoch-40 batch-12
Running loss of epoch-40 batch-12 = 5.888752639293671e-06

Training epoch-40 batch-13
Running loss of epoch-40 batch-13 = 3.730878233909607e-06

Training epoch-40 batch-14
Running loss of epoch-40 batch-14 = 4.139496013522148e-06

Training epoch-40 batch-15
Running loss of epoch-40 batch-15 = 4.044966772198677e-06

Training epoch-40 batch-16
Running loss of epoch-40 batch-16 = 3.5944394767284393e-06

Training epoch-40 batch-17
Running loss of epoch-40 batch-17 = 3.650318831205368e-06

Training epoch-40 batch-18
Running loss of epoch-40 batch-18 = 2.0617153495550156e-06

Training epoch-40 batch-19
Running loss of epoch-40 batch-19 = 3.1187664717435837e-06

Training epoch-40 batch-20
Running loss of epoch-40 batch-20 = 5.685025826096535e-06

Training epoch-40 batch-21
Running loss of epoch-40 batch-21 = 3.874767571687698e-06

Training epoch-40 batch-22
Running loss of epoch-40 batch-22 = 4.9120280891656876e-06

Training epoch-40 batch-23
Running loss of epoch-40 batch-23 = 4.648230969905853e-06

Training epoch-40 batch-24
Running loss of epoch-40 batch-24 = 6.070826202630997e-06

Training epoch-40 batch-25
Running loss of epoch-40 batch-25 = 4.360685124993324e-06

Training epoch-40 batch-26
Running loss of epoch-40 batch-26 = 5.1995739340782166e-06

Training epoch-40 batch-27
Running loss of epoch-40 batch-27 = 3.862893208861351e-06

Training epoch-40 batch-28
Running loss of epoch-40 batch-28 = 3.010965883731842e-06

Training epoch-40 batch-29
Running loss of epoch-40 batch-29 = 4.372093826532364e-06

Training epoch-40 batch-30
Running loss of epoch-40 batch-30 = 5.112495273351669e-06

Training epoch-40 batch-31
Running loss of epoch-40 batch-31 = 5.004461854696274e-06

Training epoch-40 batch-32
Running loss of epoch-40 batch-32 = 4.425877705216408e-06

Training epoch-40 batch-33
Running loss of epoch-40 batch-33 = 3.70037741959095e-06

Training epoch-40 batch-34
Running loss of epoch-40 batch-34 = 4.729023203253746e-06

Training epoch-40 batch-35
Running loss of epoch-40 batch-35 = 4.953006282448769e-06

Training epoch-40 batch-36
Running loss of epoch-40 batch-36 = 3.291526809334755e-06

Training epoch-40 batch-37
Running loss of epoch-40 batch-37 = 3.5108532756567e-06

Training epoch-40 batch-38
Running loss of epoch-40 batch-38 = 4.016328603029251e-06

Training epoch-40 batch-39
Running loss of epoch-40 batch-39 = 2.807239070534706e-06

Training epoch-40 batch-40
Running loss of epoch-40 batch-40 = 3.791647031903267e-06

Training epoch-40 batch-41
Running loss of epoch-40 batch-41 = 4.249159246683121e-06

Training epoch-40 batch-42
Running loss of epoch-40 batch-42 = 3.5658013075590134e-06

Training epoch-40 batch-43
Running loss of epoch-40 batch-43 = 4.391185939311981e-06

Training epoch-40 batch-44
Running loss of epoch-40 batch-44 = 4.381407052278519e-06

Training epoch-40 batch-45
Running loss of epoch-40 batch-45 = 4.608184099197388e-06

Training epoch-40 batch-46
Running loss of epoch-40 batch-46 = 4.795147106051445e-06

Training epoch-40 batch-47
Running loss of epoch-40 batch-47 = 3.1210947781801224e-06

Training epoch-40 batch-48
Running loss of epoch-40 batch-48 = 4.353933036327362e-06

Training epoch-40 batch-49
Running loss of epoch-40 batch-49 = 5.44404610991478e-06

Training epoch-40 batch-50
Running loss of epoch-40 batch-50 = 3.8745347410440445e-06

Training epoch-40 batch-51
Running loss of epoch-40 batch-51 = 2.491287887096405e-06

Training epoch-40 batch-52
Running loss of epoch-40 batch-52 = 3.6708079278469086e-06

Training epoch-40 batch-53
Running loss of epoch-40 batch-53 = 3.7387944757938385e-06

Training epoch-40 batch-54
Running loss of epoch-40 batch-54 = 3.343448042869568e-06

Training epoch-40 batch-55
Running loss of epoch-40 batch-55 = 3.3385585993528366e-06

Training epoch-40 batch-56
Running loss of epoch-40 batch-56 = 3.6514829844236374e-06

Training epoch-40 batch-57
Running loss of epoch-40 batch-57 = 3.725755959749222e-06

Training epoch-40 batch-58
Running loss of epoch-40 batch-58 = 4.2889732867479324e-06

Training epoch-40 batch-59
Running loss of epoch-40 batch-59 = 4.4370535761117935e-06

Training epoch-40 batch-60
Running loss of epoch-40 batch-60 = 4.0940940380096436e-06

Training epoch-40 batch-61
Running loss of epoch-40 batch-61 = 3.686407580971718e-06

Training epoch-40 batch-62
Running loss of epoch-40 batch-62 = 3.926921635866165e-06

Training epoch-40 batch-63
Running loss of epoch-40 batch-63 = 3.832625225186348e-06

Training epoch-40 batch-64
Running loss of epoch-40 batch-64 = 4.2263418436050415e-06

Training epoch-40 batch-65
Running loss of epoch-40 batch-65 = 4.569999873638153e-06

Training epoch-40 batch-66
Running loss of epoch-40 batch-66 = 3.990950062870979e-06

Training epoch-40 batch-67
Running loss of epoch-40 batch-67 = 3.6151614040136337e-06

Training epoch-40 batch-68
Running loss of epoch-40 batch-68 = 4.093162715435028e-06

Training epoch-40 batch-69
Running loss of epoch-40 batch-69 = 2.8384383767843246e-06

Training epoch-40 batch-70
Running loss of epoch-40 batch-70 = 4.163011908531189e-06

Training epoch-40 batch-71
Running loss of epoch-40 batch-71 = 4.628440365195274e-06

Training epoch-40 batch-72
Running loss of epoch-40 batch-72 = 5.32553531229496e-06

Training epoch-40 batch-73
Running loss of epoch-40 batch-73 = 2.2386666387319565e-06

Training epoch-40 batch-74
Running loss of epoch-40 batch-74 = 3.348337486386299e-06

Training epoch-40 batch-75
Running loss of epoch-40 batch-75 = 5.19677996635437e-06

Training epoch-40 batch-76
Running loss of epoch-40 batch-76 = 2.575106918811798e-06

Training epoch-40 batch-77
Running loss of epoch-40 batch-77 = 2.354150637984276e-06

Training epoch-40 batch-78
Running loss of epoch-40 batch-78 = 3.913417458534241e-06

Training epoch-40 batch-79
Running loss of epoch-40 batch-79 = 4.077795892953873e-06

Training epoch-40 batch-80
Running loss of epoch-40 batch-80 = 3.955326974391937e-06

Training epoch-40 batch-81
Running loss of epoch-40 batch-81 = 3.5867560654878616e-06

Training epoch-40 batch-82
Running loss of epoch-40 batch-82 = 3.414694219827652e-06

Training epoch-40 batch-83
Running loss of epoch-40 batch-83 = 4.3762847781181335e-06

Training epoch-40 batch-84
Running loss of epoch-40 batch-84 = 4.90015372633934e-06

Training epoch-40 batch-85
Running loss of epoch-40 batch-85 = 5.470588803291321e-06

Training epoch-40 batch-86
Running loss of epoch-40 batch-86 = 3.285706043243408e-06

Training epoch-40 batch-87
Running loss of epoch-40 batch-87 = 4.124129191040993e-06

Training epoch-40 batch-88
Running loss of epoch-40 batch-88 = 3.956491127610207e-06

Training epoch-40 batch-89
Running loss of epoch-40 batch-89 = 3.134133294224739e-06

Training epoch-40 batch-90
Running loss of epoch-40 batch-90 = 3.4722033888101578e-06

Training epoch-40 batch-91
Running loss of epoch-40 batch-91 = 4.495261237025261e-06

Training epoch-40 batch-92
Running loss of epoch-40 batch-92 = 4.7765206545591354e-06

Training epoch-40 batch-93
Running loss of epoch-40 batch-93 = 3.3958349376916885e-06

Training epoch-40 batch-94
Running loss of epoch-40 batch-94 = 3.3564865589141846e-06

Training epoch-40 batch-95
Running loss of epoch-40 batch-95 = 3.35020013153553e-06

Training epoch-40 batch-96
Running loss of epoch-40 batch-96 = 3.314577043056488e-06

Training epoch-40 batch-97
Running loss of epoch-40 batch-97 = 5.573965609073639e-06

Training epoch-40 batch-98
Running loss of epoch-40 batch-98 = 3.9245933294296265e-06

Training epoch-40 batch-99
Running loss of epoch-40 batch-99 = 5.721347406506538e-06

Training epoch-40 batch-100
Running loss of epoch-40 batch-100 = 2.8081703931093216e-06

Training epoch-40 batch-101
Running loss of epoch-40 batch-101 = 5.0086528062820435e-06

Training epoch-40 batch-102
Running loss of epoch-40 batch-102 = 3.569759428501129e-06

Training epoch-40 batch-103
Running loss of epoch-40 batch-103 = 3.839842975139618e-06

Training epoch-40 batch-104
Running loss of epoch-40 batch-104 = 2.2391323000192642e-06

Training epoch-40 batch-105
Running loss of epoch-40 batch-105 = 5.226116627454758e-06

Training epoch-40 batch-106
Running loss of epoch-40 batch-106 = 1.9492581486701965e-06

Training epoch-40 batch-107
Running loss of epoch-40 batch-107 = 3.253808245062828e-06

Training epoch-40 batch-108
Running loss of epoch-40 batch-108 = 3.407709300518036e-06

Training epoch-40 batch-109
Running loss of epoch-40 batch-109 = 3.8440339267253876e-06

Training epoch-40 batch-110
Running loss of epoch-40 batch-110 = 4.6030618250370026e-06

Training epoch-40 batch-111
Running loss of epoch-40 batch-111 = 4.455214366316795e-06

Training epoch-40 batch-112
Running loss of epoch-40 batch-112 = 3.51807102560997e-06

Training epoch-40 batch-113
Running loss of epoch-40 batch-113 = 5.583977326750755e-06

Training epoch-40 batch-114
Running loss of epoch-40 batch-114 = 2.8123613446950912e-06

Training epoch-40 batch-115
Running loss of epoch-40 batch-115 = 3.543449565768242e-06

Training epoch-40 batch-116
Running loss of epoch-40 batch-116 = 3.0978117138147354e-06

Training epoch-40 batch-117
Running loss of epoch-40 batch-117 = 2.9802322387695312e-06

Training epoch-40 batch-118
Running loss of epoch-40 batch-118 = 5.502486601471901e-06

Training epoch-40 batch-119
Running loss of epoch-40 batch-119 = 4.172325134277344e-06

Training epoch-40 batch-120
Running loss of epoch-40 batch-120 = 4.050787538290024e-06

Training epoch-40 batch-121
Running loss of epoch-40 batch-121 = 4.69316728413105e-06

Training epoch-40 batch-122
Running loss of epoch-40 batch-122 = 3.4729018807411194e-06

Training epoch-40 batch-123
Running loss of epoch-40 batch-123 = 5.800509825348854e-06

Training epoch-40 batch-124
Running loss of epoch-40 batch-124 = 2.83634290099144e-06

Training epoch-40 batch-125
Running loss of epoch-40 batch-125 = 3.330409526824951e-06

Training epoch-40 batch-126
Running loss of epoch-40 batch-126 = 2.2256281226873398e-06

Training epoch-40 batch-127
Running loss of epoch-40 batch-127 = 4.040077328681946e-06

Training epoch-40 batch-128
Running loss of epoch-40 batch-128 = 3.535766154527664e-06

Training epoch-40 batch-129
Running loss of epoch-40 batch-129 = 2.2812746465206146e-06

Training epoch-40 batch-130
Running loss of epoch-40 batch-130 = 2.937857061624527e-06

Training epoch-40 batch-131
Running loss of epoch-40 batch-131 = 5.3425319492816925e-06

Training epoch-40 batch-132
Running loss of epoch-40 batch-132 = 7.1604736149311066e-06

Training epoch-40 batch-133
Running loss of epoch-40 batch-133 = 4.1460152715444565e-06

Training epoch-40 batch-134
Running loss of epoch-40 batch-134 = 6.577465683221817e-06

Training epoch-40 batch-135
Running loss of epoch-40 batch-135 = 4.820292815566063e-06

Training epoch-40 batch-136
Running loss of epoch-40 batch-136 = 4.130648449063301e-06

Training epoch-40 batch-137
Running loss of epoch-40 batch-137 = 5.025649443268776e-06

Training epoch-40 batch-138
Running loss of epoch-40 batch-138 = 2.3497268557548523e-06

Training epoch-40 batch-139
Running loss of epoch-40 batch-139 = 4.167202860116959e-06

Training epoch-40 batch-140
Running loss of epoch-40 batch-140 = 3.1061936169862747e-06

Training epoch-40 batch-141
Running loss of epoch-40 batch-141 = 3.67872416973114e-06

Training epoch-40 batch-142
Running loss of epoch-40 batch-142 = 2.9848888516426086e-06

Training epoch-40 batch-143
Running loss of epoch-40 batch-143 = 4.207249730825424e-06

Training epoch-40 batch-144
Running loss of epoch-40 batch-144 = 4.135072231292725e-06

Training epoch-40 batch-145
Running loss of epoch-40 batch-145 = 3.4123659133911133e-06

Training epoch-40 batch-146
Running loss of epoch-40 batch-146 = 4.218658432364464e-06

Training epoch-40 batch-147
Running loss of epoch-40 batch-147 = 4.1208695620298386e-06

Training epoch-40 batch-148
Running loss of epoch-40 batch-148 = 4.298752173781395e-06

Training epoch-40 batch-149
Running loss of epoch-40 batch-149 = 2.358807250857353e-06

Training epoch-40 batch-150
Running loss of epoch-40 batch-150 = 3.5481061786413193e-06

Training epoch-40 batch-151
Running loss of epoch-40 batch-151 = 4.54275868833065e-06

Training epoch-40 batch-152
Running loss of epoch-40 batch-152 = 4.547648131847382e-06

Training epoch-40 batch-153
Running loss of epoch-40 batch-153 = 3.095017746090889e-06

Training epoch-40 batch-154
Running loss of epoch-40 batch-154 = 3.4635886549949646e-06

Training epoch-40 batch-155
Running loss of epoch-40 batch-155 = 4.330184310674667e-06

Training epoch-40 batch-156
Running loss of epoch-40 batch-156 = 3.3266842365264893e-06

Training epoch-40 batch-157
Running loss of epoch-40 batch-157 = 1.2025237083435059e-05

Finished training epoch-40.



Average train loss at epoch-40 = 3.986227512359619e-06

Started Evaluation

Average val loss at epoch-40 = 3.040045711554979

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-41


Training epoch-41 batch-1
Running loss of epoch-41 batch-1 = 4.54694963991642e-06

Training epoch-41 batch-2
Running loss of epoch-41 batch-2 = 4.689674824476242e-06

Training epoch-41 batch-3
Running loss of epoch-41 batch-3 = 4.083150997757912e-06

Training epoch-41 batch-4
Running loss of epoch-41 batch-4 = 4.834495484828949e-06

Training epoch-41 batch-5
Running loss of epoch-41 batch-5 = 2.8617214411497116e-06

Training epoch-41 batch-6
Running loss of epoch-41 batch-6 = 3.1939707696437836e-06

Training epoch-41 batch-7
Running loss of epoch-41 batch-7 = 5.22565096616745e-06

Training epoch-41 batch-8
Running loss of epoch-41 batch-8 = 4.109228029847145e-06

Training epoch-41 batch-9
Running loss of epoch-41 batch-9 = 4.223315045237541e-06

Training epoch-41 batch-10
Running loss of epoch-41 batch-10 = 3.421679139137268e-06

Training epoch-41 batch-11
Running loss of epoch-41 batch-11 = 3.4035183489322662e-06

Training epoch-41 batch-12
Running loss of epoch-41 batch-12 = 4.128552973270416e-06

Training epoch-41 batch-13
Running loss of epoch-41 batch-13 = 3.6098062992095947e-06

Training epoch-41 batch-14
Running loss of epoch-41 batch-14 = 3.6587007343769073e-06

Training epoch-41 batch-15
Running loss of epoch-41 batch-15 = 3.0258670449256897e-06

Training epoch-41 batch-16
Running loss of epoch-41 batch-16 = 6.122281774878502e-06

Training epoch-41 batch-17
Running loss of epoch-41 batch-17 = 2.960674464702606e-06

Training epoch-41 batch-18
Running loss of epoch-41 batch-18 = 5.009584128856659e-06

Training epoch-41 batch-19
Running loss of epoch-41 batch-19 = 4.77139838039875e-06

Training epoch-41 batch-20
Running loss of epoch-41 batch-20 = 2.7029309421777725e-06

Training epoch-41 batch-21
Running loss of epoch-41 batch-21 = 2.9013026505708694e-06

Training epoch-41 batch-22
Running loss of epoch-41 batch-22 = 3.1811650842428207e-06

Training epoch-41 batch-23
Running loss of epoch-41 batch-23 = 4.911329597234726e-06

Training epoch-41 batch-24
Running loss of epoch-41 batch-24 = 2.452172338962555e-06

Training epoch-41 batch-25
Running loss of epoch-41 batch-25 = 3.7956051528453827e-06

Training epoch-41 batch-26
Running loss of epoch-41 batch-26 = 3.7280842661857605e-06

Training epoch-41 batch-27
Running loss of epoch-41 batch-27 = 3.144610673189163e-06

Training epoch-41 batch-28
Running loss of epoch-41 batch-28 = 3.7173740565776825e-06

Training epoch-41 batch-29
Running loss of epoch-41 batch-29 = 4.539964720606804e-06

Training epoch-41 batch-30
Running loss of epoch-41 batch-30 = 2.650311216711998e-06

Training epoch-41 batch-31
Running loss of epoch-41 batch-31 = 3.6708079278469086e-06

Training epoch-41 batch-32
Running loss of epoch-41 batch-32 = 3.451947122812271e-06

Training epoch-41 batch-33
Running loss of epoch-41 batch-33 = 5.055218935012817e-06

Training epoch-41 batch-34
Running loss of epoch-41 batch-34 = 5.005160346627235e-06

Training epoch-41 batch-35
Running loss of epoch-41 batch-35 = 3.698980435729027e-06

Training epoch-41 batch-36
Running loss of epoch-41 batch-36 = 5.075940862298012e-06

Training epoch-41 batch-37
Running loss of epoch-41 batch-37 = 3.757420927286148e-06

Training epoch-41 batch-38
Running loss of epoch-41 batch-38 = 2.5792978703975677e-06

Training epoch-41 batch-39
Running loss of epoch-41 batch-39 = 3.3031683415174484e-06

Training epoch-41 batch-40
Running loss of epoch-41 batch-40 = 3.893161192536354e-06

Training epoch-41 batch-41
Running loss of epoch-41 batch-41 = 4.49642539024353e-06

Training epoch-41 batch-42
Running loss of epoch-41 batch-42 = 4.026107490062714e-06

Training epoch-41 batch-43
Running loss of epoch-41 batch-43 = 4.402361810207367e-06

Training epoch-41 batch-44
Running loss of epoch-41 batch-44 = 6.947200745344162e-06

Training epoch-41 batch-45
Running loss of epoch-41 batch-45 = 4.220521077513695e-06

Training epoch-41 batch-46
Running loss of epoch-41 batch-46 = 3.73227521777153e-06

Training epoch-41 batch-47
Running loss of epoch-41 batch-47 = 4.213536158204079e-06

Training epoch-41 batch-48
Running loss of epoch-41 batch-48 = 3.818189725279808e-06

Training epoch-41 batch-49
Running loss of epoch-41 batch-49 = 4.647998139262199e-06

Training epoch-41 batch-50
Running loss of epoch-41 batch-50 = 2.8533395379781723e-06

Training epoch-41 batch-51
Running loss of epoch-41 batch-51 = 4.155095666646957e-06

Training epoch-41 batch-52
Running loss of epoch-41 batch-52 = 3.898749127984047e-06

Training epoch-41 batch-53
Running loss of epoch-41 batch-53 = 2.6728957891464233e-06

Training epoch-41 batch-54
Running loss of epoch-41 batch-54 = 4.421919584274292e-06

Training epoch-41 batch-55
Running loss of epoch-41 batch-55 = 3.2547395676374435e-06

Training epoch-41 batch-56
Running loss of epoch-41 batch-56 = 2.448679879307747e-06

Training epoch-41 batch-57
Running loss of epoch-41 batch-57 = 4.8084184527397156e-06

Training epoch-41 batch-58
Running loss of epoch-41 batch-58 = 3.5474076867103577e-06

Training epoch-41 batch-59
Running loss of epoch-41 batch-59 = 3.816559910774231e-06

Training epoch-41 batch-60
Running loss of epoch-41 batch-60 = 5.807960405945778e-06

Training epoch-41 batch-61
Running loss of epoch-41 batch-61 = 3.1634699553251266e-06

Training epoch-41 batch-62
Running loss of epoch-41 batch-62 = 5.162786692380905e-06

Training epoch-41 batch-63
Running loss of epoch-41 batch-63 = 4.403293132781982e-06

Training epoch-41 batch-64
Running loss of epoch-41 batch-64 = 3.2763928174972534e-06

Training epoch-41 batch-65
Running loss of epoch-41 batch-65 = 5.04637137055397e-06

Training epoch-41 batch-66
Running loss of epoch-41 batch-66 = 6.643356755375862e-06

Training epoch-41 batch-67
Running loss of epoch-41 batch-67 = 3.337860107421875e-06

Training epoch-41 batch-68
Running loss of epoch-41 batch-68 = 3.7651043385267258e-06

Training epoch-41 batch-69
Running loss of epoch-41 batch-69 = 3.428896889090538e-06

Training epoch-41 batch-70
Running loss of epoch-41 batch-70 = 3.896187990903854e-06

Training epoch-41 batch-71
Running loss of epoch-41 batch-71 = 3.34298238158226e-06

Training epoch-41 batch-72
Running loss of epoch-41 batch-72 = 4.5066699385643005e-06

Training epoch-41 batch-73
Running loss of epoch-41 batch-73 = 4.057539626955986e-06

Training epoch-41 batch-74
Running loss of epoch-41 batch-74 = 3.892229869961739e-06

Training epoch-41 batch-75
Running loss of epoch-41 batch-75 = 5.598412826657295e-06

Training epoch-41 batch-76
Running loss of epoch-41 batch-76 = 1.653796061873436e-06

Training epoch-41 batch-77
Running loss of epoch-41 batch-77 = 3.112480044364929e-06

Training epoch-41 batch-78
Running loss of epoch-41 batch-78 = 4.025176167488098e-06

Training epoch-41 batch-79
Running loss of epoch-41 batch-79 = 2.8854701668024063e-06

Training epoch-41 batch-80
Running loss of epoch-41 batch-80 = 3.069639205932617e-06

Training epoch-41 batch-81
Running loss of epoch-41 batch-81 = 4.101544618606567e-06

Training epoch-41 batch-82
Running loss of epoch-41 batch-82 = 3.760913386940956e-06

Training epoch-41 batch-83
Running loss of epoch-41 batch-83 = 3.94042581319809e-06

Training epoch-41 batch-84
Running loss of epoch-41 batch-84 = 4.590023308992386e-06

Training epoch-41 batch-85
Running loss of epoch-41 batch-85 = 2.459157258272171e-06

Training epoch-41 batch-86
Running loss of epoch-41 batch-86 = 3.5206321626901627e-06

Training epoch-41 batch-87
Running loss of epoch-41 batch-87 = 4.35439869761467e-06

Training epoch-41 batch-88
Running loss of epoch-41 batch-88 = 2.6742927730083466e-06

Training epoch-41 batch-89
Running loss of epoch-41 batch-89 = 4.524132236838341e-06

Training epoch-41 batch-90
Running loss of epoch-41 batch-90 = 2.192799001932144e-06

Training epoch-41 batch-91
Running loss of epoch-41 batch-91 = 3.377208486199379e-06

Training epoch-41 batch-92
Running loss of epoch-41 batch-92 = 5.862908437848091e-06

Training epoch-41 batch-93
Running loss of epoch-41 batch-93 = 6.248475983738899e-06

Training epoch-41 batch-94
Running loss of epoch-41 batch-94 = 4.076864570379257e-06

Training epoch-41 batch-95
Running loss of epoch-41 batch-95 = 2.9527582228183746e-06

Training epoch-41 batch-96
Running loss of epoch-41 batch-96 = 3.3476389944553375e-06

Training epoch-41 batch-97
Running loss of epoch-41 batch-97 = 4.155794158577919e-06

Training epoch-41 batch-98
Running loss of epoch-41 batch-98 = 4.70038503408432e-06

Training epoch-41 batch-99
Running loss of epoch-41 batch-99 = 2.987915650010109e-06

Training epoch-41 batch-100
Running loss of epoch-41 batch-100 = 2.669636160135269e-06

Training epoch-41 batch-101
Running loss of epoch-41 batch-101 = 4.313420504331589e-06

Training epoch-41 batch-102
Running loss of epoch-41 batch-102 = 3.3385585993528366e-06

Training epoch-41 batch-103
Running loss of epoch-41 batch-103 = 4.0996819734573364e-06

Training epoch-41 batch-104
Running loss of epoch-41 batch-104 = 4.145316779613495e-06

Training epoch-41 batch-105
Running loss of epoch-41 batch-105 = 2.6423949748277664e-06

Training epoch-41 batch-106
Running loss of epoch-41 batch-106 = 3.696652129292488e-06

Training epoch-41 batch-107
Running loss of epoch-41 batch-107 = 3.851018846035004e-06

Training epoch-41 batch-108
Running loss of epoch-41 batch-108 = 3.6177225410938263e-06

Training epoch-41 batch-109
Running loss of epoch-41 batch-109 = 5.411449819803238e-06

Training epoch-41 batch-110
Running loss of epoch-41 batch-110 = 3.650318831205368e-06

Training epoch-41 batch-111
Running loss of epoch-41 batch-111 = 5.262671038508415e-06

Training epoch-41 batch-112
Running loss of epoch-41 batch-112 = 3.559514880180359e-06

Training epoch-41 batch-113
Running loss of epoch-41 batch-113 = 4.210975021123886e-06

Training epoch-41 batch-114
Running loss of epoch-41 batch-114 = 3.5208649933338165e-06

Training epoch-41 batch-115
Running loss of epoch-41 batch-115 = 3.759050741791725e-06

Training epoch-41 batch-116
Running loss of epoch-41 batch-116 = 4.399102181196213e-06

Training epoch-41 batch-117
Running loss of epoch-41 batch-117 = 4.245666787028313e-06

Training epoch-41 batch-118
Running loss of epoch-41 batch-118 = 4.0549784898757935e-06

Training epoch-41 batch-119
Running loss of epoch-41 batch-119 = 4.302477464079857e-06

Training epoch-41 batch-120
Running loss of epoch-41 batch-120 = 3.5588163882493973e-06

Training epoch-41 batch-121
Running loss of epoch-41 batch-121 = 4.794448614120483e-06

Training epoch-41 batch-122
Running loss of epoch-41 batch-122 = 4.272209480404854e-06

Training epoch-41 batch-123
Running loss of epoch-41 batch-123 = 3.3578835427761078e-06

Training epoch-41 batch-124
Running loss of epoch-41 batch-124 = 3.2617244869470596e-06

Training epoch-41 batch-125
Running loss of epoch-41 batch-125 = 3.2759271562099457e-06

Training epoch-41 batch-126
Running loss of epoch-41 batch-126 = 3.6347191780805588e-06

Training epoch-41 batch-127
Running loss of epoch-41 batch-127 = 3.3874530345201492e-06

Training epoch-41 batch-128
Running loss of epoch-41 batch-128 = 3.4261029213666916e-06

Training epoch-41 batch-129
Running loss of epoch-41 batch-129 = 3.169989213347435e-06

Training epoch-41 batch-130
Running loss of epoch-41 batch-130 = 2.8295908123254776e-06

Training epoch-41 batch-131
Running loss of epoch-41 batch-131 = 5.613081157207489e-06

Training epoch-41 batch-132
Running loss of epoch-41 batch-132 = 3.7869904190301895e-06

Training epoch-41 batch-133
Running loss of epoch-41 batch-133 = 3.2614916563034058e-06

Training epoch-41 batch-134
Running loss of epoch-41 batch-134 = 1.812819391489029e-06

Training epoch-41 batch-135
Running loss of epoch-41 batch-135 = 2.5264453142881393e-06

Training epoch-41 batch-136
Running loss of epoch-41 batch-136 = 3.4461263567209244e-06

Training epoch-41 batch-137
Running loss of epoch-41 batch-137 = 2.9567163437604904e-06

Training epoch-41 batch-138
Running loss of epoch-41 batch-138 = 3.492925316095352e-06

Training epoch-41 batch-139
Running loss of epoch-41 batch-139 = 2.2635795176029205e-06

Training epoch-41 batch-140
Running loss of epoch-41 batch-140 = 4.351604729890823e-06

Training epoch-41 batch-141
Running loss of epoch-41 batch-141 = 3.0158553272485733e-06

Training epoch-41 batch-142
Running loss of epoch-41 batch-142 = 3.7278514355421066e-06

Training epoch-41 batch-143
Running loss of epoch-41 batch-143 = 3.2905954867601395e-06

Training epoch-41 batch-144
Running loss of epoch-41 batch-144 = 1.8954742699861526e-06

Training epoch-41 batch-145
Running loss of epoch-41 batch-145 = 4.705972969532013e-06

Training epoch-41 batch-146
Running loss of epoch-41 batch-146 = 2.7760397642850876e-06

Training epoch-41 batch-147
Running loss of epoch-41 batch-147 = 3.050081431865692e-06

Training epoch-41 batch-148
Running loss of epoch-41 batch-148 = 3.896187990903854e-06

Training epoch-41 batch-149
Running loss of epoch-41 batch-149 = 4.7283247113227844e-06

Training epoch-41 batch-150
Running loss of epoch-41 batch-150 = 3.643101081252098e-06

Training epoch-41 batch-151
Running loss of epoch-41 batch-151 = 2.83936969935894e-06

Training epoch-41 batch-152
Running loss of epoch-41 batch-152 = 5.257315933704376e-06

Training epoch-41 batch-153
Running loss of epoch-41 batch-153 = 4.000496119260788e-06

Training epoch-41 batch-154
Running loss of epoch-41 batch-154 = 3.5902485251426697e-06

Training epoch-41 batch-155
Running loss of epoch-41 batch-155 = 2.6640482246875763e-06

Training epoch-41 batch-156
Running loss of epoch-41 batch-156 = 2.7869828045368195e-06

Training epoch-41 batch-157
Running loss of epoch-41 batch-157 = 1.4066696166992188e-05

Finished training epoch-41.



Average train loss at epoch-41 = 3.839997947216034e-06

Started Evaluation

Average val loss at epoch-41 = 3.0435859780562553

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.94 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 35.38 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-42


Training epoch-42 batch-1
Running loss of epoch-42 batch-1 = 2.4919863790273666e-06

Training epoch-42 batch-2
Running loss of epoch-42 batch-2 = 5.288748070597649e-06

Training epoch-42 batch-3
Running loss of epoch-42 batch-3 = 3.6337878555059433e-06

Training epoch-42 batch-4
Running loss of epoch-42 batch-4 = 3.6193523555994034e-06

Training epoch-42 batch-5
Running loss of epoch-42 batch-5 = 2.3134052753448486e-06

Training epoch-42 batch-6
Running loss of epoch-42 batch-6 = 3.9602164179086685e-06

Training epoch-42 batch-7
Running loss of epoch-42 batch-7 = 4.609813913702965e-06

Training epoch-42 batch-8
Running loss of epoch-42 batch-8 = 2.018408849835396e-06

Training epoch-42 batch-9
Running loss of epoch-42 batch-9 = 2.6228372007608414e-06

Training epoch-42 batch-10
Running loss of epoch-42 batch-10 = 4.216097295284271e-06

Training epoch-42 batch-11
Running loss of epoch-42 batch-11 = 3.0265655368566513e-06

Training epoch-42 batch-12
Running loss of epoch-42 batch-12 = 3.7546269595623016e-06

Training epoch-42 batch-13
Running loss of epoch-42 batch-13 = 6.517162546515465e-06

Training epoch-42 batch-14
Running loss of epoch-42 batch-14 = 5.10714016854763e-06

Training epoch-42 batch-15
Running loss of epoch-42 batch-15 = 5.22565096616745e-06

Training epoch-42 batch-16
Running loss of epoch-42 batch-16 = 4.02890145778656e-06

Training epoch-42 batch-17
Running loss of epoch-42 batch-17 = 3.0370429158210754e-06

Training epoch-42 batch-18
Running loss of epoch-42 batch-18 = 1.5576370060443878e-06

Training epoch-42 batch-19
Running loss of epoch-42 batch-19 = 3.938330337405205e-06

Training epoch-42 batch-20
Running loss of epoch-42 batch-20 = 2.9867514967918396e-06

Training epoch-42 batch-21
Running loss of epoch-42 batch-21 = 3.786291927099228e-06

Training epoch-42 batch-22
Running loss of epoch-42 batch-22 = 3.5548582673072815e-06

Training epoch-42 batch-23
Running loss of epoch-42 batch-23 = 3.407476469874382e-06

Training epoch-42 batch-24
Running loss of epoch-42 batch-24 = 3.043562173843384e-06

Training epoch-42 batch-25
Running loss of epoch-42 batch-25 = 3.9334408938884735e-06

Training epoch-42 batch-26
Running loss of epoch-42 batch-26 = 3.166729584336281e-06

Training epoch-42 batch-27
Running loss of epoch-42 batch-27 = 2.3865140974521637e-06

Training epoch-42 batch-28
Running loss of epoch-42 batch-28 = 3.822147846221924e-06

Training epoch-42 batch-29
Running loss of epoch-42 batch-29 = 6.432179361581802e-06

Training epoch-42 batch-30
Running loss of epoch-42 batch-30 = 1.3490207493305206e-06

Training epoch-42 batch-31
Running loss of epoch-42 batch-31 = 3.7159770727157593e-06

Training epoch-42 batch-32
Running loss of epoch-42 batch-32 = 3.2729003578424454e-06

Training epoch-42 batch-33
Running loss of epoch-42 batch-33 = 3.3974647521972656e-06

Training epoch-42 batch-34
Running loss of epoch-42 batch-34 = 3.5157427191734314e-06

Training epoch-42 batch-35
Running loss of epoch-42 batch-35 = 2.7581118047237396e-06

Training epoch-42 batch-36
Running loss of epoch-42 batch-36 = 3.5390257835388184e-06

Training epoch-42 batch-37
Running loss of epoch-42 batch-37 = 3.1103845685720444e-06

Training epoch-42 batch-38
Running loss of epoch-42 batch-38 = 3.504101186990738e-06

Training epoch-42 batch-39
Running loss of epoch-42 batch-39 = 5.514360964298248e-06

Training epoch-42 batch-40
Running loss of epoch-42 batch-40 = 4.197238013148308e-06

Training epoch-42 batch-41
Running loss of epoch-42 batch-41 = 2.4996697902679443e-06

Training epoch-42 batch-42
Running loss of epoch-42 batch-42 = 2.1513551473617554e-06

Training epoch-42 batch-43
Running loss of epoch-42 batch-43 = 4.863366484642029e-06

Training epoch-42 batch-44
Running loss of epoch-42 batch-44 = 3.984197974205017e-06

Training epoch-42 batch-45
Running loss of epoch-42 batch-45 = 3.184191882610321e-06

Training epoch-42 batch-46
Running loss of epoch-42 batch-46 = 5.260109901428223e-06

Training epoch-42 batch-47
Running loss of epoch-42 batch-47 = 3.3141113817691803e-06

Training epoch-42 batch-48
Running loss of epoch-42 batch-48 = 5.798880010843277e-06

Training epoch-42 batch-49
Running loss of epoch-42 batch-49 = 4.185829311609268e-06

Training epoch-42 batch-50
Running loss of epoch-42 batch-50 = 2.516200765967369e-06

Training epoch-42 batch-51
Running loss of epoch-42 batch-51 = 3.845430910587311e-06

Training epoch-42 batch-52
Running loss of epoch-42 batch-52 = 4.495028406381607e-06

Training epoch-42 batch-53
Running loss of epoch-42 batch-53 = 4.3441541492938995e-06

Training epoch-42 batch-54
Running loss of epoch-42 batch-54 = 4.173722118139267e-06

Training epoch-42 batch-55
Running loss of epoch-42 batch-55 = 3.1886156648397446e-06

Training epoch-42 batch-56
Running loss of epoch-42 batch-56 = 2.7455389499664307e-06

Training epoch-42 batch-57
Running loss of epoch-42 batch-57 = 2.3632310330867767e-06

Training epoch-42 batch-58
Running loss of epoch-42 batch-58 = 2.7853529900312424e-06

Training epoch-42 batch-59
Running loss of epoch-42 batch-59 = 3.841938450932503e-06

Training epoch-42 batch-60
Running loss of epoch-42 batch-60 = 3.061024472117424e-06

Training epoch-42 batch-61
Running loss of epoch-42 batch-61 = 2.9203947633504868e-06

Training epoch-42 batch-62
Running loss of epoch-42 batch-62 = 1.923646777868271e-06

Training epoch-42 batch-63
Running loss of epoch-42 batch-63 = 4.500150680541992e-06

Training epoch-42 batch-64
Running loss of epoch-42 batch-64 = 3.76114621758461e-06

Training epoch-42 batch-65
Running loss of epoch-42 batch-65 = 3.684312105178833e-06

Training epoch-42 batch-66
Running loss of epoch-42 batch-66 = 3.6838464438915253e-06

Training epoch-42 batch-67
Running loss of epoch-42 batch-67 = 2.46451236307621e-06

Training epoch-42 batch-68
Running loss of epoch-42 batch-68 = 2.1336600184440613e-06

Training epoch-42 batch-69
Running loss of epoch-42 batch-69 = 4.714587703347206e-06

Training epoch-42 batch-70
Running loss of epoch-42 batch-70 = 4.0943268686532974e-06

Training epoch-42 batch-71
Running loss of epoch-42 batch-71 = 3.50363552570343e-06

Training epoch-42 batch-72
Running loss of epoch-42 batch-72 = 2.3331958800554276e-06

Training epoch-42 batch-73
Running loss of epoch-42 batch-73 = 3.2796524465084076e-06

Training epoch-42 batch-74
Running loss of epoch-42 batch-74 = 4.098052158951759e-06

Training epoch-42 batch-75
Running loss of epoch-42 batch-75 = 3.606081008911133e-06

Training epoch-42 batch-76
Running loss of epoch-42 batch-76 = 3.721565008163452e-06

Training epoch-42 batch-77
Running loss of epoch-42 batch-77 = 3.616791218519211e-06

Training epoch-42 batch-78
Running loss of epoch-42 batch-78 = 1.4195684343576431e-06

Training epoch-42 batch-79
Running loss of epoch-42 batch-79 = 3.268010914325714e-06

Training epoch-42 batch-80
Running loss of epoch-42 batch-80 = 4.2263418436050415e-06

Training epoch-42 batch-81
Running loss of epoch-42 batch-81 = 2.959044650197029e-06

Training epoch-42 batch-82
Running loss of epoch-42 batch-82 = 3.507360816001892e-06

Training epoch-42 batch-83
Running loss of epoch-42 batch-83 = 3.98675911128521e-06

Training epoch-42 batch-84
Running loss of epoch-42 batch-84 = 3.5353004932403564e-06

Training epoch-42 batch-85
Running loss of epoch-42 batch-85 = 3.875000402331352e-06

Training epoch-42 batch-86
Running loss of epoch-42 batch-86 = 2.6475172489881516e-06

Training epoch-42 batch-87
Running loss of epoch-42 batch-87 = 4.726927727460861e-06

Training epoch-42 batch-88
Running loss of epoch-42 batch-88 = 4.330417141318321e-06

Training epoch-42 batch-89
Running loss of epoch-42 batch-89 = 4.737637937068939e-06

Training epoch-42 batch-90
Running loss of epoch-42 batch-90 = 4.209345206618309e-06

Training epoch-42 batch-91
Running loss of epoch-42 batch-91 = 2.9120128601789474e-06

Training epoch-42 batch-92
Running loss of epoch-42 batch-92 = 3.7320423871278763e-06

Training epoch-42 batch-93
Running loss of epoch-42 batch-93 = 3.62517312169075e-06

Training epoch-42 batch-94
Running loss of epoch-42 batch-94 = 2.922024577856064e-06

Training epoch-42 batch-95
Running loss of epoch-42 batch-95 = 5.919719114899635e-06

Training epoch-42 batch-96
Running loss of epoch-42 batch-96 = 4.614703357219696e-06

Training epoch-42 batch-97
Running loss of epoch-42 batch-97 = 3.94345261156559e-06

Training epoch-42 batch-98
Running loss of epoch-42 batch-98 = 2.961140125989914e-06

Training epoch-42 batch-99
Running loss of epoch-42 batch-99 = 3.368360921740532e-06

Training epoch-42 batch-100
Running loss of epoch-42 batch-100 = 3.0463561415672302e-06

Training epoch-42 batch-101
Running loss of epoch-42 batch-101 = 3.4919939935207367e-06

Training epoch-42 batch-102
Running loss of epoch-42 batch-102 = 3.0817463994026184e-06

Training epoch-42 batch-103
Running loss of epoch-42 batch-103 = 6.120186299085617e-06

Training epoch-42 batch-104
Running loss of epoch-42 batch-104 = 3.53972427546978e-06

Training epoch-42 batch-105
Running loss of epoch-42 batch-105 = 3.6244746297597885e-06

Training epoch-42 batch-106
Running loss of epoch-42 batch-106 = 2.7723144739866257e-06

Training epoch-42 batch-107
Running loss of epoch-42 batch-107 = 2.839602530002594e-06

Training epoch-42 batch-108
Running loss of epoch-42 batch-108 = 5.543697625398636e-06

Training epoch-42 batch-109
Running loss of epoch-42 batch-109 = 3.476860001683235e-06

Training epoch-42 batch-110
Running loss of epoch-42 batch-110 = 4.594679921865463e-06

Training epoch-42 batch-111
Running loss of epoch-42 batch-111 = 3.474997356534004e-06

Training epoch-42 batch-112
Running loss of epoch-42 batch-112 = 3.873836249113083e-06

Training epoch-42 batch-113
Running loss of epoch-42 batch-113 = 4.172325134277344e-06

Training epoch-42 batch-114
Running loss of epoch-42 batch-114 = 3.3087562769651413e-06

Training epoch-42 batch-115
Running loss of epoch-42 batch-115 = 3.488035872578621e-06

Training epoch-42 batch-116
Running loss of epoch-42 batch-116 = 5.341600626707077e-06

Training epoch-42 batch-117
Running loss of epoch-42 batch-117 = 3.9655715227127075e-06

Training epoch-42 batch-118
Running loss of epoch-42 batch-118 = 2.0863953977823257e-06

Training epoch-42 batch-119
Running loss of epoch-42 batch-119 = 4.709698259830475e-06

Training epoch-42 batch-120
Running loss of epoch-42 batch-120 = 4.343222826719284e-06

Training epoch-42 batch-121
Running loss of epoch-42 batch-121 = 2.430519089102745e-06

Training epoch-42 batch-122
Running loss of epoch-42 batch-122 = 3.5979319363832474e-06

Training epoch-42 batch-123
Running loss of epoch-42 batch-123 = 2.377200871706009e-06

Training epoch-42 batch-124
Running loss of epoch-42 batch-124 = 5.127629265189171e-06

Training epoch-42 batch-125
Running loss of epoch-42 batch-125 = 4.155328497290611e-06

Training epoch-42 batch-126
Running loss of epoch-42 batch-126 = 4.781177267432213e-06

Training epoch-42 batch-127
Running loss of epoch-42 batch-127 = 5.14020211994648e-06

Training epoch-42 batch-128
Running loss of epoch-42 batch-128 = 2.8118956834077835e-06

Training epoch-42 batch-129
Running loss of epoch-42 batch-129 = 7.682247087359428e-06

Training epoch-42 batch-130
Running loss of epoch-42 batch-130 = 5.359528586268425e-06

Training epoch-42 batch-131
Running loss of epoch-42 batch-131 = 2.9944349080324173e-06

Training epoch-42 batch-132
Running loss of epoch-42 batch-132 = 3.45427542924881e-06

Training epoch-42 batch-133
Running loss of epoch-42 batch-133 = 2.484535798430443e-06

Training epoch-42 batch-134
Running loss of epoch-42 batch-134 = 4.3471809476614e-06

Training epoch-42 batch-135
Running loss of epoch-42 batch-135 = 3.489432856440544e-06

Training epoch-42 batch-136
Running loss of epoch-42 batch-136 = 3.032153472304344e-06

Training epoch-42 batch-137
Running loss of epoch-42 batch-137 = 3.5010743886232376e-06

Training epoch-42 batch-138
Running loss of epoch-42 batch-138 = 4.016794264316559e-06

Training epoch-42 batch-139
Running loss of epoch-42 batch-139 = 2.3439060896635056e-06

Training epoch-42 batch-140
Running loss of epoch-42 batch-140 = 3.0461233109235764e-06

Training epoch-42 batch-141
Running loss of epoch-42 batch-141 = 3.352295607328415e-06

Training epoch-42 batch-142
Running loss of epoch-42 batch-142 = 2.702232450246811e-06

Training epoch-42 batch-143
Running loss of epoch-42 batch-143 = 3.1373929232358932e-06

Training epoch-42 batch-144
Running loss of epoch-42 batch-144 = 2.4980399757623672e-06

Training epoch-42 batch-145
Running loss of epoch-42 batch-145 = 2.3527536541223526e-06

Training epoch-42 batch-146
Running loss of epoch-42 batch-146 = 4.265224561095238e-06

Training epoch-42 batch-147
Running loss of epoch-42 batch-147 = 4.3746549636125565e-06

Training epoch-42 batch-148
Running loss of epoch-42 batch-148 = 6.631016731262207e-06

Training epoch-42 batch-149
Running loss of epoch-42 batch-149 = 4.647998139262199e-06

Training epoch-42 batch-150
Running loss of epoch-42 batch-150 = 2.257758751511574e-06

Training epoch-42 batch-151
Running loss of epoch-42 batch-151 = 3.5855919122695923e-06

Training epoch-42 batch-152
Running loss of epoch-42 batch-152 = 3.446824848651886e-06

Training epoch-42 batch-153
Running loss of epoch-42 batch-153 = 4.827510565519333e-06

Training epoch-42 batch-154
Running loss of epoch-42 batch-154 = 4.318077117204666e-06

Training epoch-42 batch-155
Running loss of epoch-42 batch-155 = 5.122274160385132e-06

Training epoch-42 batch-156
Running loss of epoch-42 batch-156 = 3.2512471079826355e-06

Training epoch-42 batch-157
Running loss of epoch-42 batch-157 = 1.2654811143875122e-05

Finished training epoch-42.



Average train loss at epoch-42 = 3.7019878625869753e-06

Started Evaluation

Average val loss at epoch-42 = 3.055852036727102

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.40 %

Finished Evaluation



Started training epoch-43


Training epoch-43 batch-1
Running loss of epoch-43 batch-1 = 2.9564835131168365e-06

Training epoch-43 batch-2
Running loss of epoch-43 batch-2 = 4.802364856004715e-06

Training epoch-43 batch-3
Running loss of epoch-43 batch-3 = 2.499902620911598e-06

Training epoch-43 batch-4
Running loss of epoch-43 batch-4 = 3.075692802667618e-06

Training epoch-43 batch-5
Running loss of epoch-43 batch-5 = 4.012603312730789e-06

Training epoch-43 batch-6
Running loss of epoch-43 batch-6 = 3.5830307751893997e-06

Training epoch-43 batch-7
Running loss of epoch-43 batch-7 = 2.7865171432495117e-06

Training epoch-43 batch-8
Running loss of epoch-43 batch-8 = 2.864282578229904e-06

Training epoch-43 batch-9
Running loss of epoch-43 batch-9 = 2.3008324205875397e-06

Training epoch-43 batch-10
Running loss of epoch-43 batch-10 = 3.0882656574249268e-06

Training epoch-43 batch-11
Running loss of epoch-43 batch-11 = 2.5937333703041077e-06

Training epoch-43 batch-12
Running loss of epoch-43 batch-12 = 3.126915544271469e-06

Training epoch-43 batch-13
Running loss of epoch-43 batch-13 = 3.250082954764366e-06

Training epoch-43 batch-14
Running loss of epoch-43 batch-14 = 2.0975712686777115e-06

Training epoch-43 batch-15
Running loss of epoch-43 batch-15 = 4.05474565923214e-06

Training epoch-43 batch-16
Running loss of epoch-43 batch-16 = 3.3956021070480347e-06

Training epoch-43 batch-17
Running loss of epoch-43 batch-17 = 3.15043143928051e-06

Training epoch-43 batch-18
Running loss of epoch-43 batch-18 = 3.600027412176132e-06

Training epoch-43 batch-19
Running loss of epoch-43 batch-19 = 2.2025778889656067e-06

Training epoch-43 batch-20
Running loss of epoch-43 batch-20 = 3.7371646612882614e-06

Training epoch-43 batch-21
Running loss of epoch-43 batch-21 = 5.1658134907484055e-06

Training epoch-43 batch-22
Running loss of epoch-43 batch-22 = 3.1061936169862747e-06

Training epoch-43 batch-23
Running loss of epoch-43 batch-23 = 3.933673724532127e-06

Training epoch-43 batch-24
Running loss of epoch-43 batch-24 = 2.3832544684410095e-06

Training epoch-43 batch-25
Running loss of epoch-43 batch-25 = 4.223547875881195e-06

Training epoch-43 batch-26
Running loss of epoch-43 batch-26 = 4.883855581283569e-06

Training epoch-43 batch-27
Running loss of epoch-43 batch-27 = 4.119472578167915e-06

Training epoch-43 batch-28
Running loss of epoch-43 batch-28 = 3.2908283174037933e-06

Training epoch-43 batch-29
Running loss of epoch-43 batch-29 = 4.510162398219109e-06

Training epoch-43 batch-30
Running loss of epoch-43 batch-30 = 3.75392846763134e-06

Training epoch-43 batch-31
Running loss of epoch-43 batch-31 = 4.022615030407906e-06

Training epoch-43 batch-32
Running loss of epoch-43 batch-32 = 3.1299423426389694e-06

Training epoch-43 batch-33
Running loss of epoch-43 batch-33 = 2.045184373855591e-06

Training epoch-43 batch-34
Running loss of epoch-43 batch-34 = 4.307832568883896e-06

Training epoch-43 batch-35
Running loss of epoch-43 batch-35 = 2.928776666522026e-06

Training epoch-43 batch-36
Running loss of epoch-43 batch-36 = 5.076872184872627e-06

Training epoch-43 batch-37
Running loss of epoch-43 batch-37 = 3.591645509004593e-06

Training epoch-43 batch-38
Running loss of epoch-43 batch-38 = 3.5692937672138214e-06

Training epoch-43 batch-39
Running loss of epoch-43 batch-39 = 4.963018000125885e-06

Training epoch-43 batch-40
Running loss of epoch-43 batch-40 = 4.932982847094536e-06

Training epoch-43 batch-41
Running loss of epoch-43 batch-41 = 3.4926924854516983e-06

Training epoch-43 batch-42
Running loss of epoch-43 batch-42 = 4.512490704655647e-06

Training epoch-43 batch-43
Running loss of epoch-43 batch-43 = 2.3082830011844635e-06

Training epoch-43 batch-44
Running loss of epoch-43 batch-44 = 5.53973950445652e-06

Training epoch-43 batch-45
Running loss of epoch-43 batch-45 = 3.1869858503341675e-06

Training epoch-43 batch-46
Running loss of epoch-43 batch-46 = 4.932750016450882e-06

Training epoch-43 batch-47
Running loss of epoch-43 batch-47 = 3.997236490249634e-06

Training epoch-43 batch-48
Running loss of epoch-43 batch-48 = 2.9960647225379944e-06

Training epoch-43 batch-49
Running loss of epoch-43 batch-49 = 4.286877810955048e-06

Training epoch-43 batch-50
Running loss of epoch-43 batch-50 = 3.5620760172605515e-06

Training epoch-43 batch-51
Running loss of epoch-43 batch-51 = 3.990251570940018e-06

Training epoch-43 batch-52
Running loss of epoch-43 batch-52 = 2.4428591132164e-06

Training epoch-43 batch-53
Running loss of epoch-43 batch-53 = 4.241475835442543e-06

Training epoch-43 batch-54
Running loss of epoch-43 batch-54 = 1.7078127712011337e-06

Training epoch-43 batch-55
Running loss of epoch-43 batch-55 = 3.2854732125997543e-06

Training epoch-43 batch-56
Running loss of epoch-43 batch-56 = 4.115980118513107e-06

Training epoch-43 batch-57
Running loss of epoch-43 batch-57 = 1.877080649137497e-06

Training epoch-43 batch-58
Running loss of epoch-43 batch-58 = 3.8051512092351913e-06

Training epoch-43 batch-59
Running loss of epoch-43 batch-59 = 2.164626494050026e-06

Training epoch-43 batch-60
Running loss of epoch-43 batch-60 = 4.07220795750618e-06

Training epoch-43 batch-61
Running loss of epoch-43 batch-61 = 3.003980964422226e-06

Training epoch-43 batch-62
Running loss of epoch-43 batch-62 = 2.830987796187401e-06

Training epoch-43 batch-63
Running loss of epoch-43 batch-63 = 2.75392085313797e-06

Training epoch-43 batch-64
Running loss of epoch-43 batch-64 = 4.968373104929924e-06

Training epoch-43 batch-65
Running loss of epoch-43 batch-65 = 2.6817433536052704e-06

Training epoch-43 batch-66
Running loss of epoch-43 batch-66 = 4.433095455169678e-06

Training epoch-43 batch-67
Running loss of epoch-43 batch-67 = 3.3902470022439957e-06

Training epoch-43 batch-68
Running loss of epoch-43 batch-68 = 4.644971340894699e-06

Training epoch-43 batch-69
Running loss of epoch-43 batch-69 = 4.8996880650520325e-06

Training epoch-43 batch-70
Running loss of epoch-43 batch-70 = 3.15043143928051e-06

Training epoch-43 batch-71
Running loss of epoch-43 batch-71 = 5.10411337018013e-06

Training epoch-43 batch-72
Running loss of epoch-43 batch-72 = 3.0640512704849243e-06

Training epoch-43 batch-73
Running loss of epoch-43 batch-73 = 7.497146725654602e-06

Training epoch-43 batch-74
Running loss of epoch-43 batch-74 = 2.928543835878372e-06

Training epoch-43 batch-75
Running loss of epoch-43 batch-75 = 3.020279109477997e-06

Training epoch-43 batch-76
Running loss of epoch-43 batch-76 = 3.937631845474243e-06

Training epoch-43 batch-77
Running loss of epoch-43 batch-77 = 4.469417035579681e-06

Training epoch-43 batch-78
Running loss of epoch-43 batch-78 = 3.816094249486923e-06

Training epoch-43 batch-79
Running loss of epoch-43 batch-79 = 1.691281795501709e-06

Training epoch-43 batch-80
Running loss of epoch-43 batch-80 = 3.556022420525551e-06

Training epoch-43 batch-81
Running loss of epoch-43 batch-81 = 3.473833203315735e-06

Training epoch-43 batch-82
Running loss of epoch-43 batch-82 = 2.8850045055150986e-06

Training epoch-43 batch-83
Running loss of epoch-43 batch-83 = 3.1085219234228134e-06

Training epoch-43 batch-84
Running loss of epoch-43 batch-84 = 4.270346835255623e-06

Training epoch-43 batch-85
Running loss of epoch-43 batch-85 = 4.556262865662575e-06

Training epoch-43 batch-86
Running loss of epoch-43 batch-86 = 3.7190038710832596e-06

Training epoch-43 batch-87
Running loss of epoch-43 batch-87 = 2.8463546186685562e-06

Training epoch-43 batch-88
Running loss of epoch-43 batch-88 = 3.712717443704605e-06

Training epoch-43 batch-89
Running loss of epoch-43 batch-89 = 2.8675422072410583e-06

Training epoch-43 batch-90
Running loss of epoch-43 batch-90 = 3.0014198273420334e-06

Training epoch-43 batch-91
Running loss of epoch-43 batch-91 = 4.0405429899692535e-06

Training epoch-43 batch-92
Running loss of epoch-43 batch-92 = 4.105502739548683e-06

Training epoch-43 batch-93
Running loss of epoch-43 batch-93 = 2.889428287744522e-06

Training epoch-43 batch-94
Running loss of epoch-43 batch-94 = 3.984430804848671e-06

Training epoch-43 batch-95
Running loss of epoch-43 batch-95 = 1.93621963262558e-06

Training epoch-43 batch-96
Running loss of epoch-43 batch-96 = 3.4957192838191986e-06

Training epoch-43 batch-97
Running loss of epoch-43 batch-97 = 4.044966772198677e-06

Training epoch-43 batch-98
Running loss of epoch-43 batch-98 = 3.2330863177776337e-06

Training epoch-43 batch-99
Running loss of epoch-43 batch-99 = 4.8193614929914474e-06

Training epoch-43 batch-100
Running loss of epoch-43 batch-100 = 4.00422140955925e-06

Training epoch-43 batch-101
Running loss of epoch-43 batch-101 = 3.0016526579856873e-06

Training epoch-43 batch-102
Running loss of epoch-43 batch-102 = 2.8882641345262527e-06

Training epoch-43 batch-103
Running loss of epoch-43 batch-103 = 2.88570299744606e-06

Training epoch-43 batch-104
Running loss of epoch-43 batch-104 = 6.091548129916191e-06

Training epoch-43 batch-105
Running loss of epoch-43 batch-105 = 3.56440432369709e-06

Training epoch-43 batch-106
Running loss of epoch-43 batch-106 = 2.596760168671608e-06

Training epoch-43 batch-107
Running loss of epoch-43 batch-107 = 3.159046173095703e-06

Training epoch-43 batch-108
Running loss of epoch-43 batch-108 = 4.25754114985466e-06

Training epoch-43 batch-109
Running loss of epoch-43 batch-109 = 2.9974617063999176e-06

Training epoch-43 batch-110
Running loss of epoch-43 batch-110 = 3.5583507269620895e-06

Training epoch-43 batch-111
Running loss of epoch-43 batch-111 = 4.444504156708717e-06

Training epoch-43 batch-112
Running loss of epoch-43 batch-112 = 3.2759271562099457e-06

Training epoch-43 batch-113
Running loss of epoch-43 batch-113 = 2.93925404548645e-06

Training epoch-43 batch-114
Running loss of epoch-43 batch-114 = 3.362540155649185e-06

Training epoch-43 batch-115
Running loss of epoch-43 batch-115 = 6.341142579913139e-06

Training epoch-43 batch-116
Running loss of epoch-43 batch-116 = 2.8780195862054825e-06

Training epoch-43 batch-117
Running loss of epoch-43 batch-117 = 4.214933142066002e-06

Training epoch-43 batch-118
Running loss of epoch-43 batch-118 = 4.695495590567589e-06

Training epoch-43 batch-119
Running loss of epoch-43 batch-119 = 2.575339749455452e-06

Training epoch-43 batch-120
Running loss of epoch-43 batch-120 = 2.371380105614662e-06

Training epoch-43 batch-121
Running loss of epoch-43 batch-121 = 4.030298441648483e-06

Training epoch-43 batch-122
Running loss of epoch-43 batch-122 = 3.9085280150175095e-06

Training epoch-43 batch-123
Running loss of epoch-43 batch-123 = 5.744164809584618e-06

Training epoch-43 batch-124
Running loss of epoch-43 batch-124 = 4.994682967662811e-06

Training epoch-43 batch-125
Running loss of epoch-43 batch-125 = 5.06080687046051e-06

Training epoch-43 batch-126
Running loss of epoch-43 batch-126 = 3.7814024835824966e-06

Training epoch-43 batch-127
Running loss of epoch-43 batch-127 = 3.6612618714571e-06

Training epoch-43 batch-128
Running loss of epoch-43 batch-128 = 4.102010279893875e-06

Training epoch-43 batch-129
Running loss of epoch-43 batch-129 = 1.7243437469005585e-06

Training epoch-43 batch-130
Running loss of epoch-43 batch-130 = 3.086170181632042e-06

Training epoch-43 batch-131
Running loss of epoch-43 batch-131 = 3.055669367313385e-06

Training epoch-43 batch-132
Running loss of epoch-43 batch-132 = 2.4011824280023575e-06

Training epoch-43 batch-133
Running loss of epoch-43 batch-133 = 3.0724331736564636e-06

Training epoch-43 batch-134
Running loss of epoch-43 batch-134 = 3.0051451176404953e-06

Training epoch-43 batch-135
Running loss of epoch-43 batch-135 = 2.950197085738182e-06

Training epoch-43 batch-136
Running loss of epoch-43 batch-136 = 2.501998096704483e-06

Training epoch-43 batch-137
Running loss of epoch-43 batch-137 = 4.146713763475418e-06

Training epoch-43 batch-138
Running loss of epoch-43 batch-138 = 3.687804564833641e-06

Training epoch-43 batch-139
Running loss of epoch-43 batch-139 = 4.105269908905029e-06

Training epoch-43 batch-140
Running loss of epoch-43 batch-140 = 3.3085234463214874e-06

Training epoch-43 batch-141
Running loss of epoch-43 batch-141 = 3.2293610274791718e-06

Training epoch-43 batch-142
Running loss of epoch-43 batch-142 = 2.2135209292173386e-06

Training epoch-43 batch-143
Running loss of epoch-43 batch-143 = 3.4552067518234253e-06

Training epoch-43 batch-144
Running loss of epoch-43 batch-144 = 3.934372216463089e-06

Training epoch-43 batch-145
Running loss of epoch-43 batch-145 = 3.225170075893402e-06

Training epoch-43 batch-146
Running loss of epoch-43 batch-146 = 2.3692846298217773e-06

Training epoch-43 batch-147
Running loss of epoch-43 batch-147 = 2.6605557650327682e-06

Training epoch-43 batch-148
Running loss of epoch-43 batch-148 = 3.423774614930153e-06

Training epoch-43 batch-149
Running loss of epoch-43 batch-149 = 2.6228372007608414e-06

Training epoch-43 batch-150
Running loss of epoch-43 batch-150 = 3.516441211104393e-06

Training epoch-43 batch-151
Running loss of epoch-43 batch-151 = 2.762768417596817e-06

Training epoch-43 batch-152
Running loss of epoch-43 batch-152 = 3.1318049877882004e-06

Training epoch-43 batch-153
Running loss of epoch-43 batch-153 = 3.93320806324482e-06

Training epoch-43 batch-154
Running loss of epoch-43 batch-154 = 4.81470488011837e-06

Training epoch-43 batch-155
Running loss of epoch-43 batch-155 = 2.8817448765039444e-06

Training epoch-43 batch-156
Running loss of epoch-43 batch-156 = 4.315981641411781e-06

Training epoch-43 batch-157
Running loss of epoch-43 batch-157 = 1.2177973985671997e-05

Finished training epoch-43.



Average train loss at epoch-43 = 3.573986887931824e-06

Started Evaluation

Average val loss at epoch-43 = 3.0642815891065096

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.11 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-44


Training epoch-44 batch-1
Running loss of epoch-44 batch-1 = 2.4971086531877518e-06

Training epoch-44 batch-2
Running loss of epoch-44 batch-2 = 2.457760274410248e-06

Training epoch-44 batch-3
Running loss of epoch-44 batch-3 = 3.134133294224739e-06

Training epoch-44 batch-4
Running loss of epoch-44 batch-4 = 3.458932042121887e-06

Training epoch-44 batch-5
Running loss of epoch-44 batch-5 = 2.7476344257593155e-06

Training epoch-44 batch-6
Running loss of epoch-44 batch-6 = 4.2978208512067795e-06

Training epoch-44 batch-7
Running loss of epoch-44 batch-7 = 3.9299484342336655e-06

Training epoch-44 batch-8
Running loss of epoch-44 batch-8 = 1.7709098756313324e-06

Training epoch-44 batch-9
Running loss of epoch-44 batch-9 = 3.016553819179535e-06

Training epoch-44 batch-10
Running loss of epoch-44 batch-10 = 4.243338480591774e-06

Training epoch-44 batch-11
Running loss of epoch-44 batch-11 = 2.4635810405015945e-06

Training epoch-44 batch-12
Running loss of epoch-44 batch-12 = 4.419591277837753e-06

Training epoch-44 batch-13
Running loss of epoch-44 batch-13 = 4.419591277837753e-06

Training epoch-44 batch-14
Running loss of epoch-44 batch-14 = 5.116919055581093e-06

Training epoch-44 batch-15
Running loss of epoch-44 batch-15 = 3.5353004932403564e-06

Training epoch-44 batch-16
Running loss of epoch-44 batch-16 = 4.139263182878494e-06

Training epoch-44 batch-17
Running loss of epoch-44 batch-17 = 2.441229298710823e-06

Training epoch-44 batch-18
Running loss of epoch-44 batch-18 = 2.9080547392368317e-06

Training epoch-44 batch-19
Running loss of epoch-44 batch-19 = 2.8079375624656677e-06

Training epoch-44 batch-20
Running loss of epoch-44 batch-20 = 3.363005816936493e-06

Training epoch-44 batch-21
Running loss of epoch-44 batch-21 = 3.4919939935207367e-06

Training epoch-44 batch-22
Running loss of epoch-44 batch-22 = 4.098284989595413e-06

Training epoch-44 batch-23
Running loss of epoch-44 batch-23 = 2.5245826691389084e-06

Training epoch-44 batch-24
Running loss of epoch-44 batch-24 = 3.269640728831291e-06

Training epoch-44 batch-25
Running loss of epoch-44 batch-25 = 5.607958883047104e-06

Training epoch-44 batch-26
Running loss of epoch-44 batch-26 = 3.732740879058838e-06

Training epoch-44 batch-27
Running loss of epoch-44 batch-27 = 3.1758099794387817e-06

Training epoch-44 batch-28
Running loss of epoch-44 batch-28 = 4.4477637857198715e-06

Training epoch-44 batch-29
Running loss of epoch-44 batch-29 = 3.536464646458626e-06

Training epoch-44 batch-30
Running loss of epoch-44 batch-30 = 2.882210537791252e-06

Training epoch-44 batch-31
Running loss of epoch-44 batch-31 = 3.487803041934967e-06

Training epoch-44 batch-32
Running loss of epoch-44 batch-32 = 3.0603259801864624e-06

Training epoch-44 batch-33
Running loss of epoch-44 batch-33 = 4.195608198642731e-06

Training epoch-44 batch-34
Running loss of epoch-44 batch-34 = 4.52529639005661e-06

Training epoch-44 batch-35
Running loss of epoch-44 batch-35 = 4.387227818369865e-06

Training epoch-44 batch-36
Running loss of epoch-44 batch-36 = 4.923902451992035e-06

Training epoch-44 batch-37
Running loss of epoch-44 batch-37 = 4.9567315727472305e-06

Training epoch-44 batch-38
Running loss of epoch-44 batch-38 = 2.2873282432556152e-06

Training epoch-44 batch-39
Running loss of epoch-44 batch-39 = 4.1211023926734924e-06

Training epoch-44 batch-40
Running loss of epoch-44 batch-40 = 3.93204391002655e-06

Training epoch-44 batch-41
Running loss of epoch-44 batch-41 = 3.1890813261270523e-06

Training epoch-44 batch-42
Running loss of epoch-44 batch-42 = 3.6957208067178726e-06

Training epoch-44 batch-43
Running loss of epoch-44 batch-43 = 2.7329660952091217e-06

Training epoch-44 batch-44
Running loss of epoch-44 batch-44 = 2.94344499707222e-06

Training epoch-44 batch-45
Running loss of epoch-44 batch-45 = 3.1150411814451218e-06

Training epoch-44 batch-46
Running loss of epoch-44 batch-46 = 2.4889595806598663e-06

Training epoch-44 batch-47
Running loss of epoch-44 batch-47 = 1.4526303857564926e-06

Training epoch-44 batch-48
Running loss of epoch-44 batch-48 = 3.0174851417541504e-06

Training epoch-44 batch-49
Running loss of epoch-44 batch-49 = 3.5462435334920883e-06

Training epoch-44 batch-50
Running loss of epoch-44 batch-50 = 5.9176236391067505e-06

Training epoch-44 batch-51
Running loss of epoch-44 batch-51 = 5.640555173158646e-06

Training epoch-44 batch-52
Running loss of epoch-44 batch-52 = 3.298278898000717e-06

Training epoch-44 batch-53
Running loss of epoch-44 batch-53 = 2.5462359189987183e-06

Training epoch-44 batch-54
Running loss of epoch-44 batch-54 = 2.789311110973358e-06

Training epoch-44 batch-55
Running loss of epoch-44 batch-55 = 2.7061905711889267e-06

Training epoch-44 batch-56
Running loss of epoch-44 batch-56 = 3.8107391446828842e-06

Training epoch-44 batch-57
Running loss of epoch-44 batch-57 = 3.936002030968666e-06

Training epoch-44 batch-58
Running loss of epoch-44 batch-58 = 4.038447514176369e-06

Training epoch-44 batch-59
Running loss of epoch-44 batch-59 = 3.763241693377495e-06

Training epoch-44 batch-60
Running loss of epoch-44 batch-60 = 1.9564758986234665e-06

Training epoch-44 batch-61
Running loss of epoch-44 batch-61 = 3.7266872823238373e-06

Training epoch-44 batch-62
Running loss of epoch-44 batch-62 = 3.555789589881897e-06

Training epoch-44 batch-63
Running loss of epoch-44 batch-63 = 3.2295938581228256e-06

Training epoch-44 batch-64
Running loss of epoch-44 batch-64 = 3.4798868000507355e-06

Training epoch-44 batch-65
Running loss of epoch-44 batch-65 = 3.3227261155843735e-06

Training epoch-44 batch-66
Running loss of epoch-44 batch-66 = 3.8116704672574997e-06

Training epoch-44 batch-67
Running loss of epoch-44 batch-67 = 2.4060718715190887e-06

Training epoch-44 batch-68
Running loss of epoch-44 batch-68 = 3.114575520157814e-06

Training epoch-44 batch-69
Running loss of epoch-44 batch-69 = 3.555556759238243e-06

Training epoch-44 batch-70
Running loss of epoch-44 batch-70 = 2.748565748333931e-06

Training epoch-44 batch-71
Running loss of epoch-44 batch-71 = 3.88571061193943e-06

Training epoch-44 batch-72
Running loss of epoch-44 batch-72 = 3.025168552994728e-06

Training epoch-44 batch-73
Running loss of epoch-44 batch-73 = 3.0728988349437714e-06

Training epoch-44 batch-74
Running loss of epoch-44 batch-74 = 2.927379682660103e-06

Training epoch-44 batch-75
Running loss of epoch-44 batch-75 = 1.9834842532873154e-06

Training epoch-44 batch-76
Running loss of epoch-44 batch-76 = 2.6533380150794983e-06

Training epoch-44 batch-77
Running loss of epoch-44 batch-77 = 2.787681296467781e-06

Training epoch-44 batch-78
Running loss of epoch-44 batch-78 = 2.637738361954689e-06

Training epoch-44 batch-79
Running loss of epoch-44 batch-79 = 4.264060407876968e-06

Training epoch-44 batch-80
Running loss of epoch-44 batch-80 = 2.49338336288929e-06

Training epoch-44 batch-81
Running loss of epoch-44 batch-81 = 3.061257302761078e-06

Training epoch-44 batch-82
Running loss of epoch-44 batch-82 = 4.046596586704254e-06

Training epoch-44 batch-83
Running loss of epoch-44 batch-83 = 3.427267074584961e-06

Training epoch-44 batch-84
Running loss of epoch-44 batch-84 = 3.423774614930153e-06

Training epoch-44 batch-85
Running loss of epoch-44 batch-85 = 3.471504896879196e-06

Training epoch-44 batch-86
Running loss of epoch-44 batch-86 = 2.9136426746845245e-06

Training epoch-44 batch-87
Running loss of epoch-44 batch-87 = 3.1767413020133972e-06

Training epoch-44 batch-88
Running loss of epoch-44 batch-88 = 4.2959582060575485e-06

Training epoch-44 batch-89
Running loss of epoch-44 batch-89 = 5.487818270921707e-06

Training epoch-44 batch-90
Running loss of epoch-44 batch-90 = 3.846129402518272e-06

Training epoch-44 batch-91
Running loss of epoch-44 batch-91 = 3.0351802706718445e-06

Training epoch-44 batch-92
Running loss of epoch-44 batch-92 = 2.166489139199257e-06

Training epoch-44 batch-93
Running loss of epoch-44 batch-93 = 2.569751814007759e-06

Training epoch-44 batch-94
Running loss of epoch-44 batch-94 = 2.6943162083625793e-06

Training epoch-44 batch-95
Running loss of epoch-44 batch-95 = 3.478722646832466e-06

Training epoch-44 batch-96
Running loss of epoch-44 batch-96 = 4.566041752696037e-06

Training epoch-44 batch-97
Running loss of epoch-44 batch-97 = 2.817949280142784e-06

Training epoch-44 batch-98
Running loss of epoch-44 batch-98 = 2.637971192598343e-06

Training epoch-44 batch-99
Running loss of epoch-44 batch-99 = 2.844957634806633e-06

Training epoch-44 batch-100
Running loss of epoch-44 batch-100 = 2.571847289800644e-06

Training epoch-44 batch-101
Running loss of epoch-44 batch-101 = 4.177680239081383e-06

Training epoch-44 batch-102
Running loss of epoch-44 batch-102 = 4.575354978442192e-06

Training epoch-44 batch-103
Running loss of epoch-44 batch-103 = 3.044726327061653e-06

Training epoch-44 batch-104
Running loss of epoch-44 batch-104 = 4.342291504144669e-06

Training epoch-44 batch-105
Running loss of epoch-44 batch-105 = 3.078719601035118e-06

Training epoch-44 batch-106
Running loss of epoch-44 batch-106 = 3.430526703596115e-06

Training epoch-44 batch-107
Running loss of epoch-44 batch-107 = 3.4070108085870743e-06

Training epoch-44 batch-108
Running loss of epoch-44 batch-108 = 2.71480530500412e-06

Training epoch-44 batch-109
Running loss of epoch-44 batch-109 = 2.8025824576616287e-06

Training epoch-44 batch-110
Running loss of epoch-44 batch-110 = 4.427274689078331e-06

Training epoch-44 batch-111
Running loss of epoch-44 batch-111 = 4.759524017572403e-06

Training epoch-44 batch-112
Running loss of epoch-44 batch-112 = 3.902241587638855e-06

Training epoch-44 batch-113
Running loss of epoch-44 batch-113 = 3.603752702474594e-06

Training epoch-44 batch-114
Running loss of epoch-44 batch-114 = 4.4084154069423676e-06

Training epoch-44 batch-115
Running loss of epoch-44 batch-115 = 2.7890782803297043e-06

Training epoch-44 batch-116
Running loss of epoch-44 batch-116 = 4.353700205683708e-06

Training epoch-44 batch-117
Running loss of epoch-44 batch-117 = 4.4764019548892975e-06

Training epoch-44 batch-118
Running loss of epoch-44 batch-118 = 3.070104867219925e-06

Training epoch-44 batch-119
Running loss of epoch-44 batch-119 = 4.337402060627937e-06

Training epoch-44 batch-120
Running loss of epoch-44 batch-120 = 3.1294766813516617e-06

Training epoch-44 batch-121
Running loss of epoch-44 batch-121 = 3.820285201072693e-06

Training epoch-44 batch-122
Running loss of epoch-44 batch-122 = 1.6570556908845901e-06

Training epoch-44 batch-123
Running loss of epoch-44 batch-123 = 2.505490556359291e-06

Training epoch-44 batch-124
Running loss of epoch-44 batch-124 = 2.1015293896198273e-06

Training epoch-44 batch-125
Running loss of epoch-44 batch-125 = 3.032619133591652e-06

Training epoch-44 batch-126
Running loss of epoch-44 batch-126 = 1.8936116248369217e-06

Training epoch-44 batch-127
Running loss of epoch-44 batch-127 = 2.8938520699739456e-06

Training epoch-44 batch-128
Running loss of epoch-44 batch-128 = 5.073146894574165e-06

Training epoch-44 batch-129
Running loss of epoch-44 batch-129 = 3.668246790766716e-06

Training epoch-44 batch-130
Running loss of epoch-44 batch-130 = 3.752531483769417e-06

Training epoch-44 batch-131
Running loss of epoch-44 batch-131 = 4.206085577607155e-06

Training epoch-44 batch-132
Running loss of epoch-44 batch-132 = 2.259388566017151e-06

Training epoch-44 batch-133
Running loss of epoch-44 batch-133 = 5.0996895879507065e-06

Training epoch-44 batch-134
Running loss of epoch-44 batch-134 = 2.687796950340271e-06

Training epoch-44 batch-135
Running loss of epoch-44 batch-135 = 4.030531272292137e-06

Training epoch-44 batch-136
Running loss of epoch-44 batch-136 = 3.0240043997764587e-06

Training epoch-44 batch-137
Running loss of epoch-44 batch-137 = 5.050096660852432e-06

Training epoch-44 batch-138
Running loss of epoch-44 batch-138 = 3.4670811146497726e-06

Training epoch-44 batch-139
Running loss of epoch-44 batch-139 = 3.739725798368454e-06

Training epoch-44 batch-140
Running loss of epoch-44 batch-140 = 3.7194695323705673e-06

Training epoch-44 batch-141
Running loss of epoch-44 batch-141 = 3.6978162825107574e-06

Training epoch-44 batch-142
Running loss of epoch-44 batch-142 = 3.653811290860176e-06

Training epoch-44 batch-143
Running loss of epoch-44 batch-143 = 2.832617610692978e-06

Training epoch-44 batch-144
Running loss of epoch-44 batch-144 = 2.9476359486579895e-06

Training epoch-44 batch-145
Running loss of epoch-44 batch-145 = 4.747183993458748e-06

Training epoch-44 batch-146
Running loss of epoch-44 batch-146 = 3.816327080130577e-06

Training epoch-44 batch-147
Running loss of epoch-44 batch-147 = 3.436347469687462e-06

Training epoch-44 batch-148
Running loss of epoch-44 batch-148 = 4.634959623217583e-06

Training epoch-44 batch-149
Running loss of epoch-44 batch-149 = 3.423541784286499e-06

Training epoch-44 batch-150
Running loss of epoch-44 batch-150 = 3.901775926351547e-06

Training epoch-44 batch-151
Running loss of epoch-44 batch-151 = 2.4104956537485123e-06

Training epoch-44 batch-152
Running loss of epoch-44 batch-152 = 3.5208649933338165e-06

Training epoch-44 batch-153
Running loss of epoch-44 batch-153 = 3.018183633685112e-06

Training epoch-44 batch-154
Running loss of epoch-44 batch-154 = 3.915745764970779e-06

Training epoch-44 batch-155
Running loss of epoch-44 batch-155 = 2.8784852474927902e-06

Training epoch-44 batch-156
Running loss of epoch-44 batch-156 = 3.0882656574249268e-06

Training epoch-44 batch-157
Running loss of epoch-44 batch-157 = 4.284083843231201e-06

Finished training epoch-44.



Average train loss at epoch-44 = 3.4503832459449768e-06

Started Evaluation

Average val loss at epoch-44 = 3.075254110913528

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.36 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-45


Training epoch-45 batch-1
Running loss of epoch-45 batch-1 = 2.6542693376541138e-06

Training epoch-45 batch-2
Running loss of epoch-45 batch-2 = 2.760672941803932e-06

Training epoch-45 batch-3
Running loss of epoch-45 batch-3 = 3.845663741230965e-06

Training epoch-45 batch-4
Running loss of epoch-45 batch-4 = 3.0333176255226135e-06

Training epoch-45 batch-5
Running loss of epoch-45 batch-5 = 4.086177796125412e-06

Training epoch-45 batch-6
Running loss of epoch-45 batch-6 = 2.463115379214287e-06

Training epoch-45 batch-7
Running loss of epoch-45 batch-7 = 2.9546208679676056e-06

Training epoch-45 batch-8
Running loss of epoch-45 batch-8 = 1.837732270359993e-06

Training epoch-45 batch-9
Running loss of epoch-45 batch-9 = 2.5329645723104477e-06

Training epoch-45 batch-10
Running loss of epoch-45 batch-10 = 2.9588118195533752e-06

Training epoch-45 batch-11
Running loss of epoch-45 batch-11 = 2.759275957942009e-06

Training epoch-45 batch-12
Running loss of epoch-45 batch-12 = 2.305954694747925e-06

Training epoch-45 batch-13
Running loss of epoch-45 batch-13 = 4.056142643094063e-06

Training epoch-45 batch-14
Running loss of epoch-45 batch-14 = 6.424961611628532e-06

Training epoch-45 batch-15
Running loss of epoch-45 batch-15 = 3.416324034333229e-06

Training epoch-45 batch-16
Running loss of epoch-45 batch-16 = 2.1103769540786743e-06

Training epoch-45 batch-17
Running loss of epoch-45 batch-17 = 3.905966877937317e-06

Training epoch-45 batch-18
Running loss of epoch-45 batch-18 = 4.473840817809105e-06

Training epoch-45 batch-19
Running loss of epoch-45 batch-19 = 2.8458889573812485e-06

Training epoch-45 batch-20
Running loss of epoch-45 batch-20 = 4.050787538290024e-06

Training epoch-45 batch-21
Running loss of epoch-45 batch-21 = 2.6319175958633423e-06

Training epoch-45 batch-22
Running loss of epoch-45 batch-22 = 2.5224871933460236e-06

Training epoch-45 batch-23
Running loss of epoch-45 batch-23 = 3.5408884286880493e-06

Training epoch-45 batch-24
Running loss of epoch-45 batch-24 = 3.2780226320028305e-06

Training epoch-45 batch-25
Running loss of epoch-45 batch-25 = 2.251937985420227e-06

Training epoch-45 batch-26
Running loss of epoch-45 batch-26 = 4.175584763288498e-06

Training epoch-45 batch-27
Running loss of epoch-45 batch-27 = 4.754634574055672e-06

Training epoch-45 batch-28
Running loss of epoch-45 batch-28 = 1.8547289073467255e-06

Training epoch-45 batch-29
Running loss of epoch-45 batch-29 = 3.6475248634815216e-06

Training epoch-45 batch-30
Running loss of epoch-45 batch-30 = 2.8531067073345184e-06

Training epoch-45 batch-31
Running loss of epoch-45 batch-31 = 3.476860001683235e-06

Training epoch-45 batch-32
Running loss of epoch-45 batch-32 = 3.082212060689926e-06

Training epoch-45 batch-33
Running loss of epoch-45 batch-33 = 2.505723387002945e-06

Training epoch-45 batch-34
Running loss of epoch-45 batch-34 = 2.366025000810623e-06

Training epoch-45 batch-35
Running loss of epoch-45 batch-35 = 3.007706254720688e-06

Training epoch-45 batch-36
Running loss of epoch-45 batch-36 = 3.4407712519168854e-06

Training epoch-45 batch-37
Running loss of epoch-45 batch-37 = 3.3781398087739944e-06

Training epoch-45 batch-38
Running loss of epoch-45 batch-38 = 2.9953662306070328e-06

Training epoch-45 batch-39
Running loss of epoch-45 batch-39 = 2.2763852030038834e-06

Training epoch-45 batch-40
Running loss of epoch-45 batch-40 = 4.015164449810982e-06

Training epoch-45 batch-41
Running loss of epoch-45 batch-41 = 1.8416903913021088e-06

Training epoch-45 batch-42
Running loss of epoch-45 batch-42 = 2.9900111258029938e-06

Training epoch-45 batch-43
Running loss of epoch-45 batch-43 = 2.766493707895279e-06

Training epoch-45 batch-44
Running loss of epoch-45 batch-44 = 3.225170075893402e-06

Training epoch-45 batch-45
Running loss of epoch-45 batch-45 = 3.49339097738266e-06

Training epoch-45 batch-46
Running loss of epoch-45 batch-46 = 2.9918737709522247e-06

Training epoch-45 batch-47
Running loss of epoch-45 batch-47 = 2.957414835691452e-06

Training epoch-45 batch-48
Running loss of epoch-45 batch-48 = 4.16068360209465e-06

Training epoch-45 batch-49
Running loss of epoch-45 batch-49 = 5.159992724657059e-06

Training epoch-45 batch-50
Running loss of epoch-45 batch-50 = 3.1173694878816605e-06

Training epoch-45 batch-51
Running loss of epoch-45 batch-51 = 4.0940940380096436e-06

Training epoch-45 batch-52
Running loss of epoch-45 batch-52 = 2.7902424335479736e-06

Training epoch-45 batch-53
Running loss of epoch-45 batch-53 = 3.1136441975831985e-06

Training epoch-45 batch-54
Running loss of epoch-45 batch-54 = 2.5655608624219894e-06

Training epoch-45 batch-55
Running loss of epoch-45 batch-55 = 3.881053999066353e-06

Training epoch-45 batch-56
Running loss of epoch-45 batch-56 = 2.711080014705658e-06

Training epoch-45 batch-57
Running loss of epoch-45 batch-57 = 3.489665687084198e-06

Training epoch-45 batch-58
Running loss of epoch-45 batch-58 = 4.145083948969841e-06

Training epoch-45 batch-59
Running loss of epoch-45 batch-59 = 3.5872217267751694e-06

Training epoch-45 batch-60
Running loss of epoch-45 batch-60 = 2.8440263122320175e-06

Training epoch-45 batch-61
Running loss of epoch-45 batch-61 = 4.663597792387009e-06

Training epoch-45 batch-62
Running loss of epoch-45 batch-62 = 3.0563678592443466e-06

Training epoch-45 batch-63
Running loss of epoch-45 batch-63 = 2.948334440588951e-06

Training epoch-45 batch-64
Running loss of epoch-45 batch-64 = 2.8908252716064453e-06

Training epoch-45 batch-65
Running loss of epoch-45 batch-65 = 4.578381776809692e-06

Training epoch-45 batch-66
Running loss of epoch-45 batch-66 = 2.0598527044057846e-06

Training epoch-45 batch-67
Running loss of epoch-45 batch-67 = 3.3460091799497604e-06

Training epoch-45 batch-68
Running loss of epoch-45 batch-68 = 3.668246790766716e-06

Training epoch-45 batch-69
Running loss of epoch-45 batch-69 = 3.6086421459913254e-06

Training epoch-45 batch-70
Running loss of epoch-45 batch-70 = 4.2566098272800446e-06

Training epoch-45 batch-71
Running loss of epoch-45 batch-71 = 3.3150427043437958e-06

Training epoch-45 batch-72
Running loss of epoch-45 batch-72 = 2.759043127298355e-06

Training epoch-45 batch-73
Running loss of epoch-45 batch-73 = 2.1704472601413727e-06

Training epoch-45 batch-74
Running loss of epoch-45 batch-74 = 4.281988367438316e-06

Training epoch-45 batch-75
Running loss of epoch-45 batch-75 = 1.105479896068573e-06

Training epoch-45 batch-76
Running loss of epoch-45 batch-76 = 3.3332034945487976e-06

Training epoch-45 batch-77
Running loss of epoch-45 batch-77 = 4.947418347001076e-06

Training epoch-45 batch-78
Running loss of epoch-45 batch-78 = 2.650544047355652e-06

Training epoch-45 batch-79
Running loss of epoch-45 batch-79 = 2.9294751584529877e-06

Training epoch-45 batch-80
Running loss of epoch-45 batch-80 = 3.209337592124939e-06

Training epoch-45 batch-81
Running loss of epoch-45 batch-81 = 4.654750227928162e-06

Training epoch-45 batch-82
Running loss of epoch-45 batch-82 = 4.24729660153389e-06

Training epoch-45 batch-83
Running loss of epoch-45 batch-83 = 3.2975804060697556e-06

Training epoch-45 batch-84
Running loss of epoch-45 batch-84 = 4.873843863606453e-06

Training epoch-45 batch-85
Running loss of epoch-45 batch-85 = 3.1439121812582016e-06

Training epoch-45 batch-86
Running loss of epoch-45 batch-86 = 5.514826625585556e-06

Training epoch-45 batch-87
Running loss of epoch-45 batch-87 = 2.7099158614873886e-06

Training epoch-45 batch-88
Running loss of epoch-45 batch-88 = 3.704102709889412e-06

Training epoch-45 batch-89
Running loss of epoch-45 batch-89 = 3.2335519790649414e-06

Training epoch-45 batch-90
Running loss of epoch-45 batch-90 = 3.534602001309395e-06

Training epoch-45 batch-91
Running loss of epoch-45 batch-91 = 3.6635901778936386e-06

Training epoch-45 batch-92
Running loss of epoch-45 batch-92 = 3.2084062695503235e-06

Training epoch-45 batch-93
Running loss of epoch-45 batch-93 = 2.4978071451187134e-06

Training epoch-45 batch-94
Running loss of epoch-45 batch-94 = 4.5639462769031525e-06

Training epoch-45 batch-95
Running loss of epoch-45 batch-95 = 2.6863999664783478e-06

Training epoch-45 batch-96
Running loss of epoch-45 batch-96 = 3.1490344554185867e-06

Training epoch-45 batch-97
Running loss of epoch-45 batch-97 = 3.6919955164194107e-06

Training epoch-45 batch-98
Running loss of epoch-45 batch-98 = 2.9352959245443344e-06

Training epoch-45 batch-99
Running loss of epoch-45 batch-99 = 2.3203901946544647e-06

Training epoch-45 batch-100
Running loss of epoch-45 batch-100 = 3.323657438158989e-06

Training epoch-45 batch-101
Running loss of epoch-45 batch-101 = 2.2649765014648438e-06

Training epoch-45 batch-102
Running loss of epoch-45 batch-102 = 2.782326191663742e-06

Training epoch-45 batch-103
Running loss of epoch-45 batch-103 = 3.3294782042503357e-06

Training epoch-45 batch-104
Running loss of epoch-45 batch-104 = 3.7394929677248e-06

Training epoch-45 batch-105
Running loss of epoch-45 batch-105 = 1.6456469893455505e-06

Training epoch-45 batch-106
Running loss of epoch-45 batch-106 = 4.313420504331589e-06

Training epoch-45 batch-107
Running loss of epoch-45 batch-107 = 3.562541678547859e-06

Training epoch-45 batch-108
Running loss of epoch-45 batch-108 = 4.363246262073517e-06

Training epoch-45 batch-109
Running loss of epoch-45 batch-109 = 5.112960934638977e-06

Training epoch-45 batch-110
Running loss of epoch-45 batch-110 = 2.8742942959070206e-06

Training epoch-45 batch-111
Running loss of epoch-45 batch-111 = 3.5052653402090073e-06

Training epoch-45 batch-112
Running loss of epoch-45 batch-112 = 2.905493602156639e-06

Training epoch-45 batch-113
Running loss of epoch-45 batch-113 = 2.6670750230550766e-06

Training epoch-45 batch-114
Running loss of epoch-45 batch-114 = 2.5811605155467987e-06

Training epoch-45 batch-115
Running loss of epoch-45 batch-115 = 2.589542418718338e-06

Training epoch-45 batch-116
Running loss of epoch-45 batch-116 = 3.4782569855451584e-06

Training epoch-45 batch-117
Running loss of epoch-45 batch-117 = 4.841014742851257e-06

Training epoch-45 batch-118
Running loss of epoch-45 batch-118 = 2.9704533517360687e-06

Training epoch-45 batch-119
Running loss of epoch-45 batch-119 = 2.5462359189987183e-06

Training epoch-45 batch-120
Running loss of epoch-45 batch-120 = 3.175577148795128e-06

Training epoch-45 batch-121
Running loss of epoch-45 batch-121 = 2.7976930141448975e-06

Training epoch-45 batch-122
Running loss of epoch-45 batch-122 = 2.6586931198835373e-06

Training epoch-45 batch-123
Running loss of epoch-45 batch-123 = 4.0745362639427185e-06

Training epoch-45 batch-124
Running loss of epoch-45 batch-124 = 3.330875188112259e-06

Training epoch-45 batch-125
Running loss of epoch-45 batch-125 = 3.3553224056959152e-06

Training epoch-45 batch-126
Running loss of epoch-45 batch-126 = 5.328096449375153e-06

Training epoch-45 batch-127
Running loss of epoch-45 batch-127 = 2.3639295250177383e-06

Training epoch-45 batch-128
Running loss of epoch-45 batch-128 = 2.1061860024929047e-06

Training epoch-45 batch-129
Running loss of epoch-45 batch-129 = 3.2079406082630157e-06

Training epoch-45 batch-130
Running loss of epoch-45 batch-130 = 2.5855842977762222e-06

Training epoch-45 batch-131
Running loss of epoch-45 batch-131 = 4.848465323448181e-06

Training epoch-45 batch-132
Running loss of epoch-45 batch-132 = 4.116678610444069e-06

Training epoch-45 batch-133
Running loss of epoch-45 batch-133 = 2.4652108550071716e-06

Training epoch-45 batch-134
Running loss of epoch-45 batch-134 = 2.7192290872335434e-06

Training epoch-45 batch-135
Running loss of epoch-45 batch-135 = 2.760672941803932e-06

Training epoch-45 batch-136
Running loss of epoch-45 batch-136 = 4.118774086236954e-06

Training epoch-45 batch-137
Running loss of epoch-45 batch-137 = 3.796536475419998e-06

Training epoch-45 batch-138
Running loss of epoch-45 batch-138 = 3.0295923352241516e-06

Training epoch-45 batch-139
Running loss of epoch-45 batch-139 = 2.973712980747223e-06

Training epoch-45 batch-140
Running loss of epoch-45 batch-140 = 3.466149792075157e-06

Training epoch-45 batch-141
Running loss of epoch-45 batch-141 = 3.9387959986925125e-06

Training epoch-45 batch-142
Running loss of epoch-45 batch-142 = 4.758825525641441e-06

Training epoch-45 batch-143
Running loss of epoch-45 batch-143 = 3.955792635679245e-06

Training epoch-45 batch-144
Running loss of epoch-45 batch-144 = 3.3779069781303406e-06

Training epoch-45 batch-145
Running loss of epoch-45 batch-145 = 2.8936192393302917e-06

Training epoch-45 batch-146
Running loss of epoch-45 batch-146 = 3.6444980651140213e-06

Training epoch-45 batch-147
Running loss of epoch-45 batch-147 = 3.6226119846105576e-06

Training epoch-45 batch-148
Running loss of epoch-45 batch-148 = 2.3711472749710083e-06

Training epoch-45 batch-149
Running loss of epoch-45 batch-149 = 3.3748801797628403e-06

Training epoch-45 batch-150
Running loss of epoch-45 batch-150 = 3.8852449506521225e-06

Training epoch-45 batch-151
Running loss of epoch-45 batch-151 = 4.1334424167871475e-06

Training epoch-45 batch-152
Running loss of epoch-45 batch-152 = 4.627509042620659e-06

Training epoch-45 batch-153
Running loss of epoch-45 batch-153 = 3.5101547837257385e-06

Training epoch-45 batch-154
Running loss of epoch-45 batch-154 = 2.1711457520723343e-06

Training epoch-45 batch-155
Running loss of epoch-45 batch-155 = 3.919471055269241e-06

Training epoch-45 batch-156
Running loss of epoch-45 batch-156 = 3.840774297714233e-06

Training epoch-45 batch-157
Running loss of epoch-45 batch-157 = 7.990747690200806e-06

Finished training epoch-45.



Average train loss at epoch-45 = 3.3380314707756044e-06

Started Evaluation

Average val loss at epoch-45 = 3.083109151375921

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-46


Training epoch-46 batch-1
Running loss of epoch-46 batch-1 = 2.826796844601631e-06

Training epoch-46 batch-2
Running loss of epoch-46 batch-2 = 2.8926879167556763e-06

Training epoch-46 batch-3
Running loss of epoch-46 batch-3 = 4.359288141131401e-06

Training epoch-46 batch-4
Running loss of epoch-46 batch-4 = 2.7939677238464355e-06

Training epoch-46 batch-5
Running loss of epoch-46 batch-5 = 3.705732524394989e-06

Training epoch-46 batch-6
Running loss of epoch-46 batch-6 = 3.8798898458480835e-06

Training epoch-46 batch-7
Running loss of epoch-46 batch-7 = 3.909226506948471e-06

Training epoch-46 batch-8
Running loss of epoch-46 batch-8 = 3.4568365663290024e-06

Training epoch-46 batch-9
Running loss of epoch-46 batch-9 = 2.962769940495491e-06

Training epoch-46 batch-10
Running loss of epoch-46 batch-10 = 3.1427480280399323e-06

Training epoch-46 batch-11
Running loss of epoch-46 batch-11 = 3.259396180510521e-06

Training epoch-46 batch-12
Running loss of epoch-46 batch-12 = 2.9015354812145233e-06

Training epoch-46 batch-13
Running loss of epoch-46 batch-13 = 3.1730160117149353e-06

Training epoch-46 batch-14
Running loss of epoch-46 batch-14 = 2.2475142031908035e-06

Training epoch-46 batch-15
Running loss of epoch-46 batch-15 = 4.184199497103691e-06

Training epoch-46 batch-16
Running loss of epoch-46 batch-16 = 4.368368536233902e-06

Training epoch-46 batch-17
Running loss of epoch-46 batch-17 = 2.457527443766594e-06

Training epoch-46 batch-18
Running loss of epoch-46 batch-18 = 3.541121259331703e-06

Training epoch-46 batch-19
Running loss of epoch-46 batch-19 = 3.1262170523405075e-06

Training epoch-46 batch-20
Running loss of epoch-46 batch-20 = 2.9266811907291412e-06

Training epoch-46 batch-21
Running loss of epoch-46 batch-21 = 3.7332065403461456e-06

Training epoch-46 batch-22
Running loss of epoch-46 batch-22 = 2.9192306101322174e-06

Training epoch-46 batch-23
Running loss of epoch-46 batch-23 = 3.5904813557863235e-06

Training epoch-46 batch-24
Running loss of epoch-46 batch-24 = 4.69619408249855e-06

Training epoch-46 batch-25
Running loss of epoch-46 batch-25 = 2.591405063867569e-06

Training epoch-46 batch-26
Running loss of epoch-46 batch-26 = 3.988621756434441e-06

Training epoch-46 batch-27
Running loss of epoch-46 batch-27 = 2.6728957891464233e-06

Training epoch-46 batch-28
Running loss of epoch-46 batch-28 = 2.803513780236244e-06

Training epoch-46 batch-29
Running loss of epoch-46 batch-29 = 3.318069502711296e-06

Training epoch-46 batch-30
Running loss of epoch-46 batch-30 = 3.7082936614751816e-06

Training epoch-46 batch-31
Running loss of epoch-46 batch-31 = 2.209329977631569e-06

Training epoch-46 batch-32
Running loss of epoch-46 batch-32 = 2.8205104172229767e-06

Training epoch-46 batch-33
Running loss of epoch-46 batch-33 = 5.2817631512880325e-06

Training epoch-46 batch-34
Running loss of epoch-46 batch-34 = 5.327630788087845e-06

Training epoch-46 batch-35
Running loss of epoch-46 batch-35 = 2.519926056265831e-06

Training epoch-46 batch-36
Running loss of epoch-46 batch-36 = 5.648704245686531e-06

Training epoch-46 batch-37
Running loss of epoch-46 batch-37 = 2.6368070393800735e-06

Training epoch-46 batch-38
Running loss of epoch-46 batch-38 = 3.8798898458480835e-06

Training epoch-46 batch-39
Running loss of epoch-46 batch-39 = 3.475230187177658e-06

Training epoch-46 batch-40
Running loss of epoch-46 batch-40 = 4.254747182130814e-06

Training epoch-46 batch-41
Running loss of epoch-46 batch-41 = 3.6510173231363297e-06

Training epoch-46 batch-42
Running loss of epoch-46 batch-42 = 2.836110070347786e-06

Training epoch-46 batch-43
Running loss of epoch-46 batch-43 = 2.7534551918506622e-06

Training epoch-46 batch-44
Running loss of epoch-46 batch-44 = 3.878725692629814e-06

Training epoch-46 batch-45
Running loss of epoch-46 batch-45 = 3.7548597902059555e-06

Training epoch-46 batch-46
Running loss of epoch-46 batch-46 = 2.2405292838811874e-06

Training epoch-46 batch-47
Running loss of epoch-46 batch-47 = 3.720400854945183e-06

Training epoch-46 batch-48
Running loss of epoch-46 batch-48 = 4.102010279893875e-06

Training epoch-46 batch-49
Running loss of epoch-46 batch-49 = 1.834239810705185e-06

Training epoch-46 batch-50
Running loss of epoch-46 batch-50 = 2.525048330426216e-06

Training epoch-46 batch-51
Running loss of epoch-46 batch-51 = 3.153923898935318e-06

Training epoch-46 batch-52
Running loss of epoch-46 batch-52 = 2.4244654923677444e-06

Training epoch-46 batch-53
Running loss of epoch-46 batch-53 = 3.0142255127429962e-06

Training epoch-46 batch-54
Running loss of epoch-46 batch-54 = 3.3150427043437958e-06

Training epoch-46 batch-55
Running loss of epoch-46 batch-55 = 2.8905924409627914e-06

Training epoch-46 batch-56
Running loss of epoch-46 batch-56 = 3.1045638024806976e-06

Training epoch-46 batch-57
Running loss of epoch-46 batch-57 = 3.725755959749222e-06

Training epoch-46 batch-58
Running loss of epoch-46 batch-58 = 3.5210978239774704e-06

Training epoch-46 batch-59
Running loss of epoch-46 batch-59 = 3.769993782043457e-06

Training epoch-46 batch-60
Running loss of epoch-46 batch-60 = 2.3585744202136993e-06

Training epoch-46 batch-61
Running loss of epoch-46 batch-61 = 2.664746716618538e-06

Training epoch-46 batch-62
Running loss of epoch-46 batch-62 = 1.8454156816005707e-06

Training epoch-46 batch-63
Running loss of epoch-46 batch-63 = 3.187917172908783e-06

Training epoch-46 batch-64
Running loss of epoch-46 batch-64 = 2.4810433387756348e-06

Training epoch-46 batch-65
Running loss of epoch-46 batch-65 = 4.816101863980293e-06

Training epoch-46 batch-66
Running loss of epoch-46 batch-66 = 2.475455403327942e-06

Training epoch-46 batch-67
Running loss of epoch-46 batch-67 = 4.44403849542141e-06

Training epoch-46 batch-68
Running loss of epoch-46 batch-68 = 3.2139942049980164e-06

Training epoch-46 batch-69
Running loss of epoch-46 batch-69 = 4.210975021123886e-06

Training epoch-46 batch-70
Running loss of epoch-46 batch-70 = 3.360910341143608e-06

Training epoch-46 batch-71
Running loss of epoch-46 batch-71 = 3.496650606393814e-06

Training epoch-46 batch-72
Running loss of epoch-46 batch-72 = 3.2670795917510986e-06

Training epoch-46 batch-73
Running loss of epoch-46 batch-73 = 3.835652023553848e-06

Training epoch-46 batch-74
Running loss of epoch-46 batch-74 = 3.06800939142704e-06

Training epoch-46 batch-75
Running loss of epoch-46 batch-75 = 3.007473424077034e-06

Training epoch-46 batch-76
Running loss of epoch-46 batch-76 = 2.4694018065929413e-06

Training epoch-46 batch-77
Running loss of epoch-46 batch-77 = 2.351822331547737e-06

Training epoch-46 batch-78
Running loss of epoch-46 batch-78 = 2.425629645586014e-06

Training epoch-46 batch-79
Running loss of epoch-46 batch-79 = 2.041226252913475e-06

Training epoch-46 batch-80
Running loss of epoch-46 batch-80 = 3.912718966603279e-06

Training epoch-46 batch-81
Running loss of epoch-46 batch-81 = 3.4188851714134216e-06

Training epoch-46 batch-82
Running loss of epoch-46 batch-82 = 3.3373944461345673e-06

Training epoch-46 batch-83
Running loss of epoch-46 batch-83 = 4.144851118326187e-06

Training epoch-46 batch-84
Running loss of epoch-46 batch-84 = 1.9988510757684708e-06

Training epoch-46 batch-85
Running loss of epoch-46 batch-85 = 4.489440470933914e-06

Training epoch-46 batch-86
Running loss of epoch-46 batch-86 = 4.780478775501251e-06

Training epoch-46 batch-87
Running loss of epoch-46 batch-87 = 4.243804141879082e-06

Training epoch-46 batch-88
Running loss of epoch-46 batch-88 = 1.9723083823919296e-06

Training epoch-46 batch-89
Running loss of epoch-46 batch-89 = 3.0356459319591522e-06

Training epoch-46 batch-90
Running loss of epoch-46 batch-90 = 3.2838433980941772e-06

Training epoch-46 batch-91
Running loss of epoch-46 batch-91 = 2.066371962428093e-06

Training epoch-46 batch-92
Running loss of epoch-46 batch-92 = 3.0535738915205e-06

Training epoch-46 batch-93
Running loss of epoch-46 batch-93 = 6.207264959812164e-06

Training epoch-46 batch-94
Running loss of epoch-46 batch-94 = 2.766028046607971e-06

Training epoch-46 batch-95
Running loss of epoch-46 batch-95 = 3.7390273064374924e-06

Training epoch-46 batch-96
Running loss of epoch-46 batch-96 = 4.149042069911957e-06

Training epoch-46 batch-97
Running loss of epoch-46 batch-97 = 3.426801413297653e-06

Training epoch-46 batch-98
Running loss of epoch-46 batch-98 = 3.385823220014572e-06

Training epoch-46 batch-99
Running loss of epoch-46 batch-99 = 1.9278377294540405e-06

Training epoch-46 batch-100
Running loss of epoch-46 batch-100 = 2.4621840566396713e-06

Training epoch-46 batch-101
Running loss of epoch-46 batch-101 = 3.894558176398277e-06

Training epoch-46 batch-102
Running loss of epoch-46 batch-102 = 2.623535692691803e-06

Training epoch-46 batch-103
Running loss of epoch-46 batch-103 = 3.193039447069168e-06

Training epoch-46 batch-104
Running loss of epoch-46 batch-104 = 3.7534628063440323e-06

Training epoch-46 batch-105
Running loss of epoch-46 batch-105 = 2.5811605155467987e-06

Training epoch-46 batch-106
Running loss of epoch-46 batch-106 = 3.2258685678243637e-06

Training epoch-46 batch-107
Running loss of epoch-46 batch-107 = 3.556022420525551e-06

Training epoch-46 batch-108
Running loss of epoch-46 batch-108 = 2.895016223192215e-06

Training epoch-46 batch-109
Running loss of epoch-46 batch-109 = 3.3497344702482224e-06

Training epoch-46 batch-110
Running loss of epoch-46 batch-110 = 1.6039703041315079e-06

Training epoch-46 batch-111
Running loss of epoch-46 batch-111 = 3.1830277293920517e-06

Training epoch-46 batch-112
Running loss of epoch-46 batch-112 = 2.8847716748714447e-06

Training epoch-46 batch-113
Running loss of epoch-46 batch-113 = 3.6070123314857483e-06

Training epoch-46 batch-114
Running loss of epoch-46 batch-114 = 2.5292392820119858e-06

Training epoch-46 batch-115
Running loss of epoch-46 batch-115 = 1.8426217138767242e-06

Training epoch-46 batch-116
Running loss of epoch-46 batch-116 = 2.6552006602287292e-06

Training epoch-46 batch-117
Running loss of epoch-46 batch-117 = 2.278713509440422e-06

Training epoch-46 batch-118
Running loss of epoch-46 batch-118 = 2.7061905711889267e-06

Training epoch-46 batch-119
Running loss of epoch-46 batch-119 = 3.6580022424459457e-06

Training epoch-46 batch-120
Running loss of epoch-46 batch-120 = 2.412591129541397e-06

Training epoch-46 batch-121
Running loss of epoch-46 batch-121 = 2.342741936445236e-06

Training epoch-46 batch-122
Running loss of epoch-46 batch-122 = 2.9031652957201004e-06

Training epoch-46 batch-123
Running loss of epoch-46 batch-123 = 4.578614607453346e-06

Training epoch-46 batch-124
Running loss of epoch-46 batch-124 = 4.315050318837166e-06

Training epoch-46 batch-125
Running loss of epoch-46 batch-125 = 4.755565896630287e-06

Training epoch-46 batch-126
Running loss of epoch-46 batch-126 = 3.375345841050148e-06

Training epoch-46 batch-127
Running loss of epoch-46 batch-127 = 3.577442839741707e-06

Training epoch-46 batch-128
Running loss of epoch-46 batch-128 = 3.969762474298477e-06

Training epoch-46 batch-129
Running loss of epoch-46 batch-129 = 2.1061860024929047e-06

Training epoch-46 batch-130
Running loss of epoch-46 batch-130 = 3.893626853823662e-06

Training epoch-46 batch-131
Running loss of epoch-46 batch-131 = 3.398628905415535e-06

Training epoch-46 batch-132
Running loss of epoch-46 batch-132 = 2.780696377158165e-06

Training epoch-46 batch-133
Running loss of epoch-46 batch-133 = 2.5941990315914154e-06

Training epoch-46 batch-134
Running loss of epoch-46 batch-134 = 4.679430276155472e-06

Training epoch-46 batch-135
Running loss of epoch-46 batch-135 = 2.9599759727716446e-06

Training epoch-46 batch-136
Running loss of epoch-46 batch-136 = 2.2174790501594543e-06

Training epoch-46 batch-137
Running loss of epoch-46 batch-137 = 2.877321094274521e-06

Training epoch-46 batch-138
Running loss of epoch-46 batch-138 = 2.8943177312612534e-06

Training epoch-46 batch-139
Running loss of epoch-46 batch-139 = 3.0784867703914642e-06

Training epoch-46 batch-140
Running loss of epoch-46 batch-140 = 2.621905878186226e-06

Training epoch-46 batch-141
Running loss of epoch-46 batch-141 = 2.8461217880249023e-06

Training epoch-46 batch-142
Running loss of epoch-46 batch-142 = 2.160901203751564e-06

Training epoch-46 batch-143
Running loss of epoch-46 batch-143 = 3.854045644402504e-06

Training epoch-46 batch-144
Running loss of epoch-46 batch-144 = 2.7620699256658554e-06

Training epoch-46 batch-145
Running loss of epoch-46 batch-145 = 3.373483195900917e-06

Training epoch-46 batch-146
Running loss of epoch-46 batch-146 = 3.924360498785973e-06

Training epoch-46 batch-147
Running loss of epoch-46 batch-147 = 1.3969838619232178e-06

Training epoch-46 batch-148
Running loss of epoch-46 batch-148 = 2.80025415122509e-06

Training epoch-46 batch-149
Running loss of epoch-46 batch-149 = 3.0070077627897263e-06

Training epoch-46 batch-150
Running loss of epoch-46 batch-150 = 2.5830231606960297e-06

Training epoch-46 batch-151
Running loss of epoch-46 batch-151 = 2.5024637579917908e-06

Training epoch-46 batch-152
Running loss of epoch-46 batch-152 = 3.138091415166855e-06

Training epoch-46 batch-153
Running loss of epoch-46 batch-153 = 2.8491485863924026e-06

Training epoch-46 batch-154
Running loss of epoch-46 batch-154 = 3.997469320893288e-06

Training epoch-46 batch-155
Running loss of epoch-46 batch-155 = 3.6284327507019043e-06

Training epoch-46 batch-156
Running loss of epoch-46 batch-156 = 2.821674570441246e-06

Training epoch-46 batch-157
Running loss of epoch-46 batch-157 = 1.0848045349121094e-05

Finished training epoch-46.



Average train loss at epoch-46 = 3.238038718700409e-06

Started Evaluation

Average val loss at epoch-46 = 3.0890557985556755

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-47


Training epoch-47 batch-1
Running loss of epoch-47 batch-1 = 2.0386651158332825e-06

Training epoch-47 batch-2
Running loss of epoch-47 batch-2 = 3.4437980502843857e-06

Training epoch-47 batch-3
Running loss of epoch-47 batch-3 = 3.2421667128801346e-06

Training epoch-47 batch-4
Running loss of epoch-47 batch-4 = 2.469867467880249e-06

Training epoch-47 batch-5
Running loss of epoch-47 batch-5 = 3.2456591725349426e-06

Training epoch-47 batch-6
Running loss of epoch-47 batch-6 = 4.012370482087135e-06

Training epoch-47 batch-7
Running loss of epoch-47 batch-7 = 4.768837243318558e-06

Training epoch-47 batch-8
Running loss of epoch-47 batch-8 = 2.3136381059885025e-06

Training epoch-47 batch-9
Running loss of epoch-47 batch-9 = 2.2298190742731094e-06

Training epoch-47 batch-10
Running loss of epoch-47 batch-10 = 4.061264917254448e-06

Training epoch-47 batch-11
Running loss of epoch-47 batch-11 = 4.716683179140091e-06

Training epoch-47 batch-12
Running loss of epoch-47 batch-12 = 3.5744160413742065e-06

Training epoch-47 batch-13
Running loss of epoch-47 batch-13 = 2.5278422981500626e-06

Training epoch-47 batch-14
Running loss of epoch-47 batch-14 = 2.9758084565401077e-06

Training epoch-47 batch-15
Running loss of epoch-47 batch-15 = 3.873137757182121e-06

Training epoch-47 batch-16
Running loss of epoch-47 batch-16 = 3.936467692255974e-06

Training epoch-47 batch-17
Running loss of epoch-47 batch-17 = 2.911081537604332e-06

Training epoch-47 batch-18
Running loss of epoch-47 batch-18 = 4.207249730825424e-06

Training epoch-47 batch-19
Running loss of epoch-47 batch-19 = 3.3902470022439957e-06

Training epoch-47 batch-20
Running loss of epoch-47 batch-20 = 3.234250470995903e-06

Training epoch-47 batch-21
Running loss of epoch-47 batch-21 = 2.647051587700844e-06

Training epoch-47 batch-22
Running loss of epoch-47 batch-22 = 1.8374994397163391e-06

Training epoch-47 batch-23
Running loss of epoch-47 batch-23 = 2.2330787032842636e-06

Training epoch-47 batch-24
Running loss of epoch-47 batch-24 = 3.169989213347435e-06

Training epoch-47 batch-25
Running loss of epoch-47 batch-25 = 2.820044755935669e-06

Training epoch-47 batch-26
Running loss of epoch-47 batch-26 = 3.4421682357788086e-06

Training epoch-47 batch-27
Running loss of epoch-47 batch-27 = 3.857305273413658e-06

Training epoch-47 batch-28
Running loss of epoch-47 batch-28 = 2.016313374042511e-06

Training epoch-47 batch-29
Running loss of epoch-47 batch-29 = 3.922032192349434e-06

Training epoch-47 batch-30
Running loss of epoch-47 batch-30 = 4.029367119073868e-06

Training epoch-47 batch-31
Running loss of epoch-47 batch-31 = 2.6850029826164246e-06

Training epoch-47 batch-32
Running loss of epoch-47 batch-32 = 3.200722858309746e-06

Training epoch-47 batch-33
Running loss of epoch-47 batch-33 = 3.6263372749090195e-06

Training epoch-47 batch-34
Running loss of epoch-47 batch-34 = 4.484085366129875e-06

Training epoch-47 batch-35
Running loss of epoch-47 batch-35 = 3.3925753086805344e-06

Training epoch-47 batch-36
Running loss of epoch-47 batch-36 = 2.9013026505708694e-06

Training epoch-47 batch-37
Running loss of epoch-47 batch-37 = 2.037733793258667e-06

Training epoch-47 batch-38
Running loss of epoch-47 batch-38 = 3.3476389944553375e-06

Training epoch-47 batch-39
Running loss of epoch-47 batch-39 = 3.4365803003311157e-06

Training epoch-47 batch-40
Running loss of epoch-47 batch-40 = 3.094784915447235e-06

Training epoch-47 batch-41
Running loss of epoch-47 batch-41 = 2.0973384380340576e-06

Training epoch-47 batch-42
Running loss of epoch-47 batch-42 = 2.8586946427822113e-06

Training epoch-47 batch-43
Running loss of epoch-47 batch-43 = 2.9283110052347183e-06

Training epoch-47 batch-44
Running loss of epoch-47 batch-44 = 2.491520717740059e-06

Training epoch-47 batch-45
Running loss of epoch-47 batch-45 = 4.783971235156059e-06

Training epoch-47 batch-46
Running loss of epoch-47 batch-46 = 3.0533410608768463e-06

Training epoch-47 batch-47
Running loss of epoch-47 batch-47 = 3.222143277525902e-06

Training epoch-47 batch-48
Running loss of epoch-47 batch-48 = 3.832392394542694e-06

Training epoch-47 batch-49
Running loss of epoch-47 batch-49 = 3.870343789458275e-06

Training epoch-47 batch-50
Running loss of epoch-47 batch-50 = 4.506902769207954e-06

Training epoch-47 batch-51
Running loss of epoch-47 batch-51 = 2.319924533367157e-06

Training epoch-47 batch-52
Running loss of epoch-47 batch-52 = 2.846820279955864e-06

Training epoch-47 batch-53
Running loss of epoch-47 batch-53 = 2.4011824280023575e-06

Training epoch-47 batch-54
Running loss of epoch-47 batch-54 = 3.169756382703781e-06

Training epoch-47 batch-55
Running loss of epoch-47 batch-55 = 5.203066393733025e-06

Training epoch-47 batch-56
Running loss of epoch-47 batch-56 = 2.354150637984276e-06

Training epoch-47 batch-57
Running loss of epoch-47 batch-57 = 4.358822479844093e-06

Training epoch-47 batch-58
Running loss of epoch-47 batch-58 = 2.49338336288929e-06

Training epoch-47 batch-59
Running loss of epoch-47 batch-59 = 3.4996774047613144e-06

Training epoch-47 batch-60
Running loss of epoch-47 batch-60 = 2.891290932893753e-06

Training epoch-47 batch-61
Running loss of epoch-47 batch-61 = 3.141583874821663e-06

Training epoch-47 batch-62
Running loss of epoch-47 batch-62 = 3.2463576644659042e-06

Training epoch-47 batch-63
Running loss of epoch-47 batch-63 = 2.8419308364391327e-06

Training epoch-47 batch-64
Running loss of epoch-47 batch-64 = 1.7799902707338333e-06

Training epoch-47 batch-65
Running loss of epoch-47 batch-65 = 4.260102286934853e-06

Training epoch-47 batch-66
Running loss of epoch-47 batch-66 = 3.328779712319374e-06

Training epoch-47 batch-67
Running loss of epoch-47 batch-67 = 2.6121269911527634e-06

Training epoch-47 batch-68
Running loss of epoch-47 batch-68 = 2.6940833777189255e-06

Training epoch-47 batch-69
Running loss of epoch-47 batch-69 = 2.168118953704834e-06

Training epoch-47 batch-70
Running loss of epoch-47 batch-70 = 3.4603290259838104e-06

Training epoch-47 batch-71
Running loss of epoch-47 batch-71 = 3.1511299312114716e-06

Training epoch-47 batch-72
Running loss of epoch-47 batch-72 = 2.812594175338745e-06

Training epoch-47 batch-73
Running loss of epoch-47 batch-73 = 3.969762474298477e-06

Training epoch-47 batch-74
Running loss of epoch-47 batch-74 = 3.180699422955513e-06

Training epoch-47 batch-75
Running loss of epoch-47 batch-75 = 3.443099558353424e-06

Training epoch-47 batch-76
Running loss of epoch-47 batch-76 = 3.3529940992593765e-06

Training epoch-47 batch-77
Running loss of epoch-47 batch-77 = 3.1043309718370438e-06

Training epoch-47 batch-78
Running loss of epoch-47 batch-78 = 2.895249053835869e-06

Training epoch-47 batch-79
Running loss of epoch-47 batch-79 = 3.621447831392288e-06

Training epoch-47 batch-80
Running loss of epoch-47 batch-80 = 3.354158252477646e-06

Training epoch-47 batch-81
Running loss of epoch-47 batch-81 = 3.2295938581228256e-06

Training epoch-47 batch-82
Running loss of epoch-47 batch-82 = 2.5585759431123734e-06

Training epoch-47 batch-83
Running loss of epoch-47 batch-83 = 3.052409738302231e-06

Training epoch-47 batch-84
Running loss of epoch-47 batch-84 = 3.0209776014089584e-06

Training epoch-47 batch-85
Running loss of epoch-47 batch-85 = 4.548579454421997e-06

Training epoch-47 batch-86
Running loss of epoch-47 batch-86 = 2.826796844601631e-06

Training epoch-47 batch-87
Running loss of epoch-47 batch-87 = 2.3776665329933167e-06

Training epoch-47 batch-88
Running loss of epoch-47 batch-88 = 3.6475248634815216e-06

Training epoch-47 batch-89
Running loss of epoch-47 batch-89 = 4.049390554428101e-06

Training epoch-47 batch-90
Running loss of epoch-47 batch-90 = 2.2989697754383087e-06

Training epoch-47 batch-91
Running loss of epoch-47 batch-91 = 3.0496157705783844e-06

Training epoch-47 batch-92
Running loss of epoch-47 batch-92 = 3.2854732125997543e-06

Training epoch-47 batch-93
Running loss of epoch-47 batch-93 = 2.4461187422275543e-06

Training epoch-47 batch-94
Running loss of epoch-47 batch-94 = 2.500135451555252e-06

Training epoch-47 batch-95
Running loss of epoch-47 batch-95 = 2.8219074010849e-06

Training epoch-47 batch-96
Running loss of epoch-47 batch-96 = 2.930406481027603e-06

Training epoch-47 batch-97
Running loss of epoch-47 batch-97 = 3.064749762415886e-06

Training epoch-47 batch-98
Running loss of epoch-47 batch-98 = 2.219807356595993e-06

Training epoch-47 batch-99
Running loss of epoch-47 batch-99 = 3.032619133591652e-06

Training epoch-47 batch-100
Running loss of epoch-47 batch-100 = 3.4936238080263138e-06

Training epoch-47 batch-101
Running loss of epoch-47 batch-101 = 3.4794211387634277e-06

Training epoch-47 batch-102
Running loss of epoch-47 batch-102 = 3.0514784157276154e-06

Training epoch-47 batch-103
Running loss of epoch-47 batch-103 = 2.4670735001564026e-06

Training epoch-47 batch-104
Running loss of epoch-47 batch-104 = 4.026107490062714e-06

Training epoch-47 batch-105
Running loss of epoch-47 batch-105 = 1.9830185920000076e-06

Training epoch-47 batch-106
Running loss of epoch-47 batch-106 = 3.380468115210533e-06

Training epoch-47 batch-107
Running loss of epoch-47 batch-107 = 3.2873358577489853e-06

Training epoch-47 batch-108
Running loss of epoch-47 batch-108 = 3.0819792300462723e-06

Training epoch-47 batch-109
Running loss of epoch-47 batch-109 = 1.655425876379013e-06

Training epoch-47 batch-110
Running loss of epoch-47 batch-110 = 2.016546204686165e-06

Training epoch-47 batch-111
Running loss of epoch-47 batch-111 = 2.2386666387319565e-06

Training epoch-47 batch-112
Running loss of epoch-47 batch-112 = 2.712476998567581e-06

Training epoch-47 batch-113
Running loss of epoch-47 batch-113 = 2.769753336906433e-06

Training epoch-47 batch-114
Running loss of epoch-47 batch-114 = 4.214933142066002e-06

Training epoch-47 batch-115
Running loss of epoch-47 batch-115 = 3.2533425837755203e-06

Training epoch-47 batch-116
Running loss of epoch-47 batch-116 = 3.2084062695503235e-06

Training epoch-47 batch-117
Running loss of epoch-47 batch-117 = 2.5779008865356445e-06

Training epoch-47 batch-118
Running loss of epoch-47 batch-118 = 4.190020263195038e-06

Training epoch-47 batch-119
Running loss of epoch-47 batch-119 = 3.375345841050148e-06

Training epoch-47 batch-120
Running loss of epoch-47 batch-120 = 3.702472895383835e-06

Training epoch-47 batch-121
Running loss of epoch-47 batch-121 = 2.5350600481033325e-06

Training epoch-47 batch-122
Running loss of epoch-47 batch-122 = 3.125285729765892e-06

Training epoch-47 batch-123
Running loss of epoch-47 batch-123 = 2.062879502773285e-06

Training epoch-47 batch-124
Running loss of epoch-47 batch-124 = 2.3811589926481247e-06

Training epoch-47 batch-125
Running loss of epoch-47 batch-125 = 2.162763848900795e-06

Training epoch-47 batch-126
Running loss of epoch-47 batch-126 = 2.8335489332675934e-06

Training epoch-47 batch-127
Running loss of epoch-47 batch-127 = 2.970220521092415e-06

Training epoch-47 batch-128
Running loss of epoch-47 batch-128 = 3.1155068427324295e-06

Training epoch-47 batch-129
Running loss of epoch-47 batch-129 = 4.67151403427124e-06

Training epoch-47 batch-130
Running loss of epoch-47 batch-130 = 2.1720770746469498e-06

Training epoch-47 batch-131
Running loss of epoch-47 batch-131 = 3.0905939638614655e-06

Training epoch-47 batch-132
Running loss of epoch-47 batch-132 = 4.302943125367165e-06

Training epoch-47 batch-133
Running loss of epoch-47 batch-133 = 2.2070016711950302e-06

Training epoch-47 batch-134
Running loss of epoch-47 batch-134 = 3.2533425837755203e-06

Training epoch-47 batch-135
Running loss of epoch-47 batch-135 = 3.3439137041568756e-06

Training epoch-47 batch-136
Running loss of epoch-47 batch-136 = 1.9907020032405853e-06

Training epoch-47 batch-137
Running loss of epoch-47 batch-137 = 3.823311999440193e-06

Training epoch-47 batch-138
Running loss of epoch-47 batch-138 = 2.24309042096138e-06

Training epoch-47 batch-139
Running loss of epoch-47 batch-139 = 2.4992041289806366e-06

Training epoch-47 batch-140
Running loss of epoch-47 batch-140 = 3.577210009098053e-06

Training epoch-47 batch-141
Running loss of epoch-47 batch-141 = 3.369757905602455e-06

Training epoch-47 batch-142
Running loss of epoch-47 batch-142 = 3.363005816936493e-06

Training epoch-47 batch-143
Running loss of epoch-47 batch-143 = 4.360685124993324e-06

Training epoch-47 batch-144
Running loss of epoch-47 batch-144 = 2.595130354166031e-06

Training epoch-47 batch-145
Running loss of epoch-47 batch-145 = 3.4016557037830353e-06

Training epoch-47 batch-146
Running loss of epoch-47 batch-146 = 3.017950803041458e-06

Training epoch-47 batch-147
Running loss of epoch-47 batch-147 = 2.887798473238945e-06

Training epoch-47 batch-148
Running loss of epoch-47 batch-148 = 2.9676593840122223e-06

Training epoch-47 batch-149
Running loss of epoch-47 batch-149 = 2.9497314244508743e-06

Training epoch-47 batch-150
Running loss of epoch-47 batch-150 = 3.852182999253273e-06

Training epoch-47 batch-151
Running loss of epoch-47 batch-151 = 3.890367224812508e-06

Training epoch-47 batch-152
Running loss of epoch-47 batch-152 = 3.3723190426826477e-06

Training epoch-47 batch-153
Running loss of epoch-47 batch-153 = 1.6742851585149765e-06

Training epoch-47 batch-154
Running loss of epoch-47 batch-154 = 2.7278438210487366e-06

Training epoch-47 batch-155
Running loss of epoch-47 batch-155 = 4.125060513615608e-06

Training epoch-47 batch-156
Running loss of epoch-47 batch-156 = 3.209104761481285e-06

Training epoch-47 batch-157
Running loss of epoch-47 batch-157 = 1.7583370208740234e-06

Finished training epoch-47.



Average train loss at epoch-47 = 3.1273558735847473e-06

Started Evaluation

Average val loss at epoch-47 = 3.093932630200135

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-48


Training epoch-48 batch-1
Running loss of epoch-48 batch-1 = 4.163244739174843e-06

Training epoch-48 batch-2
Running loss of epoch-48 batch-2 = 2.6230700314044952e-06

Training epoch-48 batch-3
Running loss of epoch-48 batch-3 = 4.281057044863701e-06

Training epoch-48 batch-4
Running loss of epoch-48 batch-4 = 2.9276125133037567e-06

Training epoch-48 batch-5
Running loss of epoch-48 batch-5 = 3.1830277293920517e-06

Training epoch-48 batch-6
Running loss of epoch-48 batch-6 = 2.3385509848594666e-06

Training epoch-48 batch-7
Running loss of epoch-48 batch-7 = 4.433793947100639e-06

Training epoch-48 batch-8
Running loss of epoch-48 batch-8 = 2.7674250304698944e-06

Training epoch-48 batch-9
Running loss of epoch-48 batch-9 = 2.1904706954956055e-06

Training epoch-48 batch-10
Running loss of epoch-48 batch-10 = 3.1604431569576263e-06

Training epoch-48 batch-11
Running loss of epoch-48 batch-11 = 2.3345928639173508e-06

Training epoch-48 batch-12
Running loss of epoch-48 batch-12 = 2.9061920940876007e-06

Training epoch-48 batch-13
Running loss of epoch-48 batch-13 = 2.7799978852272034e-06

Training epoch-48 batch-14
Running loss of epoch-48 batch-14 = 2.667773514986038e-06

Training epoch-48 batch-15
Running loss of epoch-48 batch-15 = 3.0368100851774216e-06

Training epoch-48 batch-16
Running loss of epoch-48 batch-16 = 4.187459126114845e-06

Training epoch-48 batch-17
Running loss of epoch-48 batch-17 = 2.61492095887661e-06

Training epoch-48 batch-18
Running loss of epoch-48 batch-18 = 2.0279549062252045e-06

Training epoch-48 batch-19
Running loss of epoch-48 batch-19 = 2.5404151529073715e-06

Training epoch-48 batch-20
Running loss of epoch-48 batch-20 = 2.4244654923677444e-06

Training epoch-48 batch-21
Running loss of epoch-48 batch-21 = 2.7103815227746964e-06

Training epoch-48 batch-22
Running loss of epoch-48 batch-22 = 2.8051435947418213e-06

Training epoch-48 batch-23
Running loss of epoch-48 batch-23 = 2.4139881134033203e-06

Training epoch-48 batch-24
Running loss of epoch-48 batch-24 = 2.296874299645424e-06

Training epoch-48 batch-25
Running loss of epoch-48 batch-25 = 2.3529864847660065e-06

Training epoch-48 batch-26
Running loss of epoch-48 batch-26 = 2.594664692878723e-06

Training epoch-48 batch-27
Running loss of epoch-48 batch-27 = 2.891523763537407e-06

Training epoch-48 batch-28
Running loss of epoch-48 batch-28 = 2.4423934519290924e-06

Training epoch-48 batch-29
Running loss of epoch-48 batch-29 = 4.021450877189636e-06

Training epoch-48 batch-30
Running loss of epoch-48 batch-30 = 3.594905138015747e-06

Training epoch-48 batch-31
Running loss of epoch-48 batch-31 = 2.4579931050539017e-06

Training epoch-48 batch-32
Running loss of epoch-48 batch-32 = 2.9993243515491486e-06

Training epoch-48 batch-33
Running loss of epoch-48 batch-33 = 3.768131136894226e-06

Training epoch-48 batch-34
Running loss of epoch-48 batch-34 = 2.3369211703538895e-06

Training epoch-48 batch-35
Running loss of epoch-48 batch-35 = 4.217261448502541e-06

Training epoch-48 batch-36
Running loss of epoch-48 batch-36 = 3.149965777993202e-06

Training epoch-48 batch-37
Running loss of epoch-48 batch-37 = 2.9567163437604904e-06

Training epoch-48 batch-38
Running loss of epoch-48 batch-38 = 3.5585835576057434e-06

Training epoch-48 batch-39
Running loss of epoch-48 batch-39 = 4.120636731386185e-06

Training epoch-48 batch-40
Running loss of epoch-48 batch-40 = 3.3436808735132217e-06

Training epoch-48 batch-41
Running loss of epoch-48 batch-41 = 3.2011885195970535e-06

Training epoch-48 batch-42
Running loss of epoch-48 batch-42 = 3.0118972063064575e-06

Training epoch-48 batch-43
Running loss of epoch-48 batch-43 = 2.81468965113163e-06

Training epoch-48 batch-44
Running loss of epoch-48 batch-44 = 4.097120836377144e-06

Training epoch-48 batch-45
Running loss of epoch-48 batch-45 = 4.276866093277931e-06

Training epoch-48 batch-46
Running loss of epoch-48 batch-46 = 3.379303961992264e-06

Training epoch-48 batch-47
Running loss of epoch-48 batch-47 = 3.441469743847847e-06

Training epoch-48 batch-48
Running loss of epoch-48 batch-48 = 1.9459985196590424e-06

Training epoch-48 batch-49
Running loss of epoch-48 batch-49 = 3.259396180510521e-06

Training epoch-48 batch-50
Running loss of epoch-48 batch-50 = 4.258239641785622e-06

Training epoch-48 batch-51
Running loss of epoch-48 batch-51 = 3.8496218621730804e-06

Training epoch-48 batch-52
Running loss of epoch-48 batch-52 = 3.10712493956089e-06

Training epoch-48 batch-53
Running loss of epoch-48 batch-53 = 4.31830994784832e-06

Training epoch-48 batch-54
Running loss of epoch-48 batch-54 = 1.726904883980751e-06

Training epoch-48 batch-55
Running loss of epoch-48 batch-55 = 3.3618416637182236e-06

Training epoch-48 batch-56
Running loss of epoch-48 batch-56 = 3.47173772752285e-06

Training epoch-48 batch-57
Running loss of epoch-48 batch-57 = 2.298271283507347e-06

Training epoch-48 batch-58
Running loss of epoch-48 batch-58 = 3.027031198143959e-06

Training epoch-48 batch-59
Running loss of epoch-48 batch-59 = 2.6400666683912277e-06

Training epoch-48 batch-60
Running loss of epoch-48 batch-60 = 2.221902832388878e-06

Training epoch-48 batch-61
Running loss of epoch-48 batch-61 = 3.3418182283639908e-06

Training epoch-48 batch-62
Running loss of epoch-48 batch-62 = 3.88268381357193e-06

Training epoch-48 batch-63
Running loss of epoch-48 batch-63 = 2.923887223005295e-06

Training epoch-48 batch-64
Running loss of epoch-48 batch-64 = 2.902001142501831e-06

Training epoch-48 batch-65
Running loss of epoch-48 batch-65 = 1.9171275198459625e-06

Training epoch-48 batch-66
Running loss of epoch-48 batch-66 = 3.275228664278984e-06

Training epoch-48 batch-67
Running loss of epoch-48 batch-67 = 2.0309817045927048e-06

Training epoch-48 batch-68
Running loss of epoch-48 batch-68 = 1.3855751603841782e-06

Training epoch-48 batch-69
Running loss of epoch-48 batch-69 = 2.9045622795820236e-06

Training epoch-48 batch-70
Running loss of epoch-48 batch-70 = 3.1855888664722443e-06

Training epoch-48 batch-71
Running loss of epoch-48 batch-71 = 3.475230187177658e-06

Training epoch-48 batch-72
Running loss of epoch-48 batch-72 = 2.748798578977585e-06

Training epoch-48 batch-73
Running loss of epoch-48 batch-73 = 4.0996819734573364e-06

Training epoch-48 batch-74
Running loss of epoch-48 batch-74 = 3.0854716897010803e-06

Training epoch-48 batch-75
Running loss of epoch-48 batch-75 = 3.7481077015399933e-06

Training epoch-48 batch-76
Running loss of epoch-48 batch-76 = 3.3439137041568756e-06

Training epoch-48 batch-77
Running loss of epoch-48 batch-77 = 2.9490329325199127e-06

Training epoch-48 batch-78
Running loss of epoch-48 batch-78 = 2.8212089091539383e-06

Training epoch-48 batch-79
Running loss of epoch-48 batch-79 = 2.8312206268310547e-06

Training epoch-48 batch-80
Running loss of epoch-48 batch-80 = 2.70758755505085e-06

Training epoch-48 batch-81
Running loss of epoch-48 batch-81 = 5.190959200263023e-06

Training epoch-48 batch-82
Running loss of epoch-48 batch-82 = 2.660788595676422e-06

Training epoch-48 batch-83
Running loss of epoch-48 batch-83 = 1.998152583837509e-06

Training epoch-48 batch-84
Running loss of epoch-48 batch-84 = 2.86102294921875e-06

Training epoch-48 batch-85
Running loss of epoch-48 batch-85 = 3.3781398087739944e-06

Training epoch-48 batch-86
Running loss of epoch-48 batch-86 = 1.7213169485330582e-06

Training epoch-48 batch-87
Running loss of epoch-48 batch-87 = 2.8135254979133606e-06

Training epoch-48 batch-88
Running loss of epoch-48 batch-88 = 4.829838871955872e-06

Training epoch-48 batch-89
Running loss of epoch-48 batch-89 = 2.946937456727028e-06

Training epoch-48 batch-90
Running loss of epoch-48 batch-90 = 3.00421379506588e-06

Training epoch-48 batch-91
Running loss of epoch-48 batch-91 = 2.8691720217466354e-06

Training epoch-48 batch-92
Running loss of epoch-48 batch-92 = 3.3283140510320663e-06

Training epoch-48 batch-93
Running loss of epoch-48 batch-93 = 3.48617322742939e-06

Training epoch-48 batch-94
Running loss of epoch-48 batch-94 = 2.6263296604156494e-06

Training epoch-48 batch-95
Running loss of epoch-48 batch-95 = 2.018408849835396e-06

Training epoch-48 batch-96
Running loss of epoch-48 batch-96 = 1.9511207938194275e-06

Training epoch-48 batch-97
Running loss of epoch-48 batch-97 = 2.5802291929721832e-06

Training epoch-48 batch-98
Running loss of epoch-48 batch-98 = 2.232613041996956e-06

Training epoch-48 batch-99
Running loss of epoch-48 batch-99 = 2.6156194508075714e-06

Training epoch-48 batch-100
Running loss of epoch-48 batch-100 = 2.100132405757904e-06

Training epoch-48 batch-101
Running loss of epoch-48 batch-101 = 3.248918801546097e-06

Training epoch-48 batch-102
Running loss of epoch-48 batch-102 = 2.5723129510879517e-06

Training epoch-48 batch-103
Running loss of epoch-48 batch-103 = 3.741355612874031e-06

Training epoch-48 batch-104
Running loss of epoch-48 batch-104 = 3.307359293103218e-06

Training epoch-48 batch-105
Running loss of epoch-48 batch-105 = 3.591412678360939e-06

Training epoch-48 batch-106
Running loss of epoch-48 batch-106 = 2.830522134900093e-06

Training epoch-48 batch-107
Running loss of epoch-48 batch-107 = 3.0873343348503113e-06

Training epoch-48 batch-108
Running loss of epoch-48 batch-108 = 2.1015293896198273e-06

Training epoch-48 batch-109
Running loss of epoch-48 batch-109 = 1.980457454919815e-06

Training epoch-48 batch-110
Running loss of epoch-48 batch-110 = 2.5350600481033325e-06

Training epoch-48 batch-111
Running loss of epoch-48 batch-111 = 3.1159725040197372e-06

Training epoch-48 batch-112
Running loss of epoch-48 batch-112 = 3.75392846763134e-06

Training epoch-48 batch-113
Running loss of epoch-48 batch-113 = 2.3457687348127365e-06

Training epoch-48 batch-114
Running loss of epoch-48 batch-114 = 3.3476389944553375e-06

Training epoch-48 batch-115
Running loss of epoch-48 batch-115 = 2.9979273676872253e-06

Training epoch-48 batch-116
Running loss of epoch-48 batch-116 = 2.5150366127490997e-06

Training epoch-48 batch-117
Running loss of epoch-48 batch-117 = 3.591645509004593e-06

Training epoch-48 batch-118
Running loss of epoch-48 batch-118 = 2.326676622033119e-06

Training epoch-48 batch-119
Running loss of epoch-48 batch-119 = 3.871740773320198e-06

Training epoch-48 batch-120
Running loss of epoch-48 batch-120 = 3.2223761081695557e-06

Training epoch-48 batch-121
Running loss of epoch-48 batch-121 = 3.253808245062828e-06

Training epoch-48 batch-122
Running loss of epoch-48 batch-122 = 1.7345882952213287e-06

Training epoch-48 batch-123
Running loss of epoch-48 batch-123 = 1.7576385289430618e-06

Training epoch-48 batch-124
Running loss of epoch-48 batch-124 = 2.7604401111602783e-06

Training epoch-48 batch-125
Running loss of epoch-48 batch-125 = 3.6547426134347916e-06

Training epoch-48 batch-126
Running loss of epoch-48 batch-126 = 3.4885015338659286e-06

Training epoch-48 batch-127
Running loss of epoch-48 batch-127 = 2.8850045055150986e-06

Training epoch-48 batch-128
Running loss of epoch-48 batch-128 = 3.5853590816259384e-06

Training epoch-48 batch-129
Running loss of epoch-48 batch-129 = 4.584901034832001e-06

Training epoch-48 batch-130
Running loss of epoch-48 batch-130 = 4.26475889980793e-06

Training epoch-48 batch-131
Running loss of epoch-48 batch-131 = 3.487803041934967e-06

Training epoch-48 batch-132
Running loss of epoch-48 batch-132 = 2.5990884751081467e-06

Training epoch-48 batch-133
Running loss of epoch-48 batch-133 = 2.262648195028305e-06

Training epoch-48 batch-134
Running loss of epoch-48 batch-134 = 2.8777867555618286e-06

Training epoch-48 batch-135
Running loss of epoch-48 batch-135 = 2.1578744053840637e-06

Training epoch-48 batch-136
Running loss of epoch-48 batch-136 = 2.589542418718338e-06

Training epoch-48 batch-137
Running loss of epoch-48 batch-137 = 1.7313286662101746e-06

Training epoch-48 batch-138
Running loss of epoch-48 batch-138 = 4.030531272292137e-06

Training epoch-48 batch-139
Running loss of epoch-48 batch-139 = 4.579545930027962e-06

Training epoch-48 batch-140
Running loss of epoch-48 batch-140 = 3.993511199951172e-06

Training epoch-48 batch-141
Running loss of epoch-48 batch-141 = 3.005377948284149e-06

Training epoch-48 batch-142
Running loss of epoch-48 batch-142 = 1.8884893506765366e-06

Training epoch-48 batch-143
Running loss of epoch-48 batch-143 = 3.529246896505356e-06

Training epoch-48 batch-144
Running loss of epoch-48 batch-144 = 3.687804564833641e-06

Training epoch-48 batch-145
Running loss of epoch-48 batch-145 = 4.167202860116959e-06

Training epoch-48 batch-146
Running loss of epoch-48 batch-146 = 3.0067749321460724e-06

Training epoch-48 batch-147
Running loss of epoch-48 batch-147 = 2.2400636225938797e-06

Training epoch-48 batch-148
Running loss of epoch-48 batch-148 = 2.385815605521202e-06

Training epoch-48 batch-149
Running loss of epoch-48 batch-149 = 2.562534064054489e-06

Training epoch-48 batch-150
Running loss of epoch-48 batch-150 = 1.318054273724556e-06

Training epoch-48 batch-151
Running loss of epoch-48 batch-151 = 3.539957106113434e-06

Training epoch-48 batch-152
Running loss of epoch-48 batch-152 = 5.066394805908203e-06

Training epoch-48 batch-153
Running loss of epoch-48 batch-153 = 3.0838418751955032e-06

Training epoch-48 batch-154
Running loss of epoch-48 batch-154 = 2.387678250670433e-06

Training epoch-48 batch-155
Running loss of epoch-48 batch-155 = 3.417953848838806e-06

Training epoch-48 batch-156
Running loss of epoch-48 batch-156 = 3.594672307372093e-06

Training epoch-48 batch-157
Running loss of epoch-48 batch-157 = 1.6860663890838623e-05

Finished training epoch-48.



Average train loss at epoch-48 = 3.050021827220917e-06

Started Evaluation

Average val loss at epoch-48 = 3.1033872946312555

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.59 %

Finished Evaluation



Started training epoch-49


Training epoch-49 batch-1
Running loss of epoch-49 batch-1 = 2.957647666335106e-06

Training epoch-49 batch-2
Running loss of epoch-49 batch-2 = 2.9902439564466476e-06

Training epoch-49 batch-3
Running loss of epoch-49 batch-3 = 3.144843503832817e-06

Training epoch-49 batch-4
Running loss of epoch-49 batch-4 = 2.7138739824295044e-06

Training epoch-49 batch-5
Running loss of epoch-49 batch-5 = 3.593042492866516e-06

Training epoch-49 batch-6
Running loss of epoch-49 batch-6 = 3.4871045500040054e-06

Training epoch-49 batch-7
Running loss of epoch-49 batch-7 = 2.9476359486579895e-06

Training epoch-49 batch-8
Running loss of epoch-49 batch-8 = 4.030065611004829e-06

Training epoch-49 batch-9
Running loss of epoch-49 batch-9 = 2.27917917072773e-06

Training epoch-49 batch-10
Running loss of epoch-49 batch-10 = 3.059394657611847e-06

Training epoch-49 batch-11
Running loss of epoch-49 batch-11 = 3.0954834073781967e-06

Training epoch-49 batch-12
Running loss of epoch-49 batch-12 = 2.187443897128105e-06

Training epoch-49 batch-13
Running loss of epoch-49 batch-13 = 2.964632585644722e-06

Training epoch-49 batch-14
Running loss of epoch-49 batch-14 = 2.3441389203071594e-06

Training epoch-49 batch-15
Running loss of epoch-49 batch-15 = 4.661967977881432e-06

Training epoch-49 batch-16
Running loss of epoch-49 batch-16 = 3.0517112463712692e-06

Training epoch-49 batch-17
Running loss of epoch-49 batch-17 = 2.9476359486579895e-06

Training epoch-49 batch-18
Running loss of epoch-49 batch-18 = 2.2365711629390717e-06

Training epoch-49 batch-19
Running loss of epoch-49 batch-19 = 3.454042598605156e-06

Training epoch-49 batch-20
Running loss of epoch-49 batch-20 = 2.6812776923179626e-06

Training epoch-49 batch-21
Running loss of epoch-49 batch-21 = 3.3976975828409195e-06

Training epoch-49 batch-22
Running loss of epoch-49 batch-22 = 2.5238841772079468e-06

Training epoch-49 batch-23
Running loss of epoch-49 batch-23 = 3.610970452427864e-06

Training epoch-49 batch-24
Running loss of epoch-49 batch-24 = 3.539957106113434e-06

Training epoch-49 batch-25
Running loss of epoch-49 batch-25 = 3.6784913390874863e-06

Training epoch-49 batch-26
Running loss of epoch-49 batch-26 = 2.2882595658302307e-06

Training epoch-49 batch-27
Running loss of epoch-49 batch-27 = 4.044035449624062e-06

Training epoch-49 batch-28
Running loss of epoch-49 batch-28 = 2.3455359041690826e-06

Training epoch-49 batch-29
Running loss of epoch-49 batch-29 = 1.760898157954216e-06

Training epoch-49 batch-30
Running loss of epoch-49 batch-30 = 3.601890057325363e-06

Training epoch-49 batch-31
Running loss of epoch-49 batch-31 = 3.2586976885795593e-06

Training epoch-49 batch-32
Running loss of epoch-49 batch-32 = 3.835652023553848e-06

Training epoch-49 batch-33
Running loss of epoch-49 batch-33 = 3.00002284348011e-06

Training epoch-49 batch-34
Running loss of epoch-49 batch-34 = 2.7795322239398956e-06

Training epoch-49 batch-35
Running loss of epoch-49 batch-35 = 3.3352989703416824e-06

Training epoch-49 batch-36
Running loss of epoch-49 batch-36 = 2.2333115339279175e-06

Training epoch-49 batch-37
Running loss of epoch-49 batch-37 = 1.9529834389686584e-06

Training epoch-49 batch-38
Running loss of epoch-49 batch-38 = 3.078952431678772e-06

Training epoch-49 batch-39
Running loss of epoch-49 batch-39 = 2.093380317091942e-06

Training epoch-49 batch-40
Running loss of epoch-49 batch-40 = 3.470340743660927e-06

Training epoch-49 batch-41
Running loss of epoch-49 batch-41 = 1.605367287993431e-06

Training epoch-49 batch-42
Running loss of epoch-49 batch-42 = 2.605840563774109e-06

Training epoch-49 batch-43
Running loss of epoch-49 batch-43 = 2.005370333790779e-06

Training epoch-49 batch-44
Running loss of epoch-49 batch-44 = 4.091765731573105e-06

Training epoch-49 batch-45
Running loss of epoch-49 batch-45 = 1.6880221664905548e-06

Training epoch-49 batch-46
Running loss of epoch-49 batch-46 = 2.1997839212417603e-06

Training epoch-49 batch-47
Running loss of epoch-49 batch-47 = 2.329004928469658e-06

Training epoch-49 batch-48
Running loss of epoch-49 batch-48 = 3.2158568501472473e-06

Training epoch-49 batch-49
Running loss of epoch-49 batch-49 = 3.368593752384186e-06

Training epoch-49 batch-50
Running loss of epoch-49 batch-50 = 2.4922192096710205e-06

Training epoch-49 batch-51
Running loss of epoch-49 batch-51 = 2.437504008412361e-06

Training epoch-49 batch-52
Running loss of epoch-49 batch-52 = 2.9508955776691437e-06

Training epoch-49 batch-53
Running loss of epoch-49 batch-53 = 2.964399755001068e-06

Training epoch-49 batch-54
Running loss of epoch-49 batch-54 = 3.7427525967359543e-06

Training epoch-49 batch-55
Running loss of epoch-49 batch-55 = 2.22865492105484e-06

Training epoch-49 batch-56
Running loss of epoch-49 batch-56 = 3.073364496231079e-06

Training epoch-49 batch-57
Running loss of epoch-49 batch-57 = 2.780929207801819e-06

Training epoch-49 batch-58
Running loss of epoch-49 batch-58 = 2.6579946279525757e-06

Training epoch-49 batch-59
Running loss of epoch-49 batch-59 = 2.1671876311302185e-06

Training epoch-49 batch-60
Running loss of epoch-49 batch-60 = 3.1739473342895508e-06

Training epoch-49 batch-61
Running loss of epoch-49 batch-61 = 5.010515451431274e-06

Training epoch-49 batch-62
Running loss of epoch-49 batch-62 = 3.0740629881620407e-06

Training epoch-49 batch-63
Running loss of epoch-49 batch-63 = 1.8565915524959564e-06

Training epoch-49 batch-64
Running loss of epoch-49 batch-64 = 3.980472683906555e-06

Training epoch-49 batch-65
Running loss of epoch-49 batch-65 = 2.7709174901247025e-06

Training epoch-49 batch-66
Running loss of epoch-49 batch-66 = 2.521555870771408e-06

Training epoch-49 batch-67
Running loss of epoch-49 batch-67 = 3.643333911895752e-06

Training epoch-49 batch-68
Running loss of epoch-49 batch-68 = 4.498288035392761e-06

Training epoch-49 batch-69
Running loss of epoch-49 batch-69 = 1.684296876192093e-06

Training epoch-49 batch-70
Running loss of epoch-49 batch-70 = 3.837980329990387e-06

Training epoch-49 batch-71
Running loss of epoch-49 batch-71 = 2.498738467693329e-06

Training epoch-49 batch-72
Running loss of epoch-49 batch-72 = 3.966037184000015e-06

Training epoch-49 batch-73
Running loss of epoch-49 batch-73 = 4.1369348764419556e-06

Training epoch-49 batch-74
Running loss of epoch-49 batch-74 = 2.3227185010910034e-06

Training epoch-49 batch-75
Running loss of epoch-49 batch-75 = 2.55415216088295e-06

Training epoch-49 batch-76
Running loss of epoch-49 batch-76 = 2.4565961211919785e-06

Training epoch-49 batch-77
Running loss of epoch-49 batch-77 = 2.0691659301519394e-06

Training epoch-49 batch-78
Running loss of epoch-49 batch-78 = 2.9224902391433716e-06

Training epoch-49 batch-79
Running loss of epoch-49 batch-79 = 2.335989847779274e-06

Training epoch-49 batch-80
Running loss of epoch-49 batch-80 = 1.4405231922864914e-06

Training epoch-49 batch-81
Running loss of epoch-49 batch-81 = 3.055436536669731e-06

Training epoch-49 batch-82
Running loss of epoch-49 batch-82 = 3.484077751636505e-06

Training epoch-49 batch-83
Running loss of epoch-49 batch-83 = 4.349276423454285e-06

Training epoch-49 batch-84
Running loss of epoch-49 batch-84 = 2.5348272174596786e-06

Training epoch-49 batch-85
Running loss of epoch-49 batch-85 = 2.100365236401558e-06

Training epoch-49 batch-86
Running loss of epoch-49 batch-86 = 3.437977284193039e-06

Training epoch-49 batch-87
Running loss of epoch-49 batch-87 = 3.7052668631076813e-06

Training epoch-49 batch-88
Running loss of epoch-49 batch-88 = 1.8237624317407608e-06

Training epoch-49 batch-89
Running loss of epoch-49 batch-89 = 3.6316923797130585e-06

Training epoch-49 batch-90
Running loss of epoch-49 batch-90 = 1.2028031051158905e-06

Training epoch-49 batch-91
Running loss of epoch-49 batch-91 = 2.9047951102256775e-06

Training epoch-49 batch-92
Running loss of epoch-49 batch-92 = 3.1909439712762833e-06

Training epoch-49 batch-93
Running loss of epoch-49 batch-93 = 3.2247044146060944e-06

Training epoch-49 batch-94
Running loss of epoch-49 batch-94 = 3.046821802854538e-06

Training epoch-49 batch-95
Running loss of epoch-49 batch-95 = 2.250541001558304e-06

Training epoch-49 batch-96
Running loss of epoch-49 batch-96 = 2.609100192785263e-06

Training epoch-49 batch-97
Running loss of epoch-49 batch-97 = 4.575122147798538e-06

Training epoch-49 batch-98
Running loss of epoch-49 batch-98 = 3.1795352697372437e-06

Training epoch-49 batch-99
Running loss of epoch-49 batch-99 = 2.4801120162010193e-06

Training epoch-49 batch-100
Running loss of epoch-49 batch-100 = 2.923654392361641e-06

Training epoch-49 batch-101
Running loss of epoch-49 batch-101 = 2.886168658733368e-06

Training epoch-49 batch-102
Running loss of epoch-49 batch-102 = 2.748798578977585e-06

Training epoch-49 batch-103
Running loss of epoch-49 batch-103 = 2.3855827748775482e-06

Training epoch-49 batch-104
Running loss of epoch-49 batch-104 = 2.546468749642372e-06

Training epoch-49 batch-105
Running loss of epoch-49 batch-105 = 1.9958242774009705e-06

Training epoch-49 batch-106
Running loss of epoch-49 batch-106 = 2.2293534129858017e-06

Training epoch-49 batch-107
Running loss of epoch-49 batch-107 = 3.425171598792076e-06

Training epoch-49 batch-108
Running loss of epoch-49 batch-108 = 2.6382040232419968e-06

Training epoch-49 batch-109
Running loss of epoch-49 batch-109 = 3.0479859560728073e-06

Training epoch-49 batch-110
Running loss of epoch-49 batch-110 = 4.262663424015045e-06

Training epoch-49 batch-111
Running loss of epoch-49 batch-111 = 2.79303640127182e-06

Training epoch-49 batch-112
Running loss of epoch-49 batch-112 = 1.933891326189041e-06

Training epoch-49 batch-113
Running loss of epoch-49 batch-113 = 2.1872110664844513e-06

Training epoch-49 batch-114
Running loss of epoch-49 batch-114 = 3.1115487217903137e-06

Training epoch-49 batch-115
Running loss of epoch-49 batch-115 = 2.5134067982435226e-06

Training epoch-49 batch-116
Running loss of epoch-49 batch-116 = 2.4565961211919785e-06

Training epoch-49 batch-117
Running loss of epoch-49 batch-117 = 3.986060619354248e-06

Training epoch-49 batch-118
Running loss of epoch-49 batch-118 = 3.0531082302331924e-06

Training epoch-49 batch-119
Running loss of epoch-49 batch-119 = 2.5336630642414093e-06

Training epoch-49 batch-120
Running loss of epoch-49 batch-120 = 2.5995541363954544e-06

Training epoch-49 batch-121
Running loss of epoch-49 batch-121 = 2.5993213057518005e-06

Training epoch-49 batch-122
Running loss of epoch-49 batch-122 = 4.1888561099767685e-06

Training epoch-49 batch-123
Running loss of epoch-49 batch-123 = 2.295244485139847e-06

Training epoch-49 batch-124
Running loss of epoch-49 batch-124 = 5.477573722600937e-06

Training epoch-49 batch-125
Running loss of epoch-49 batch-125 = 1.6952399164438248e-06

Training epoch-49 batch-126
Running loss of epoch-49 batch-126 = 2.3783650249242783e-06

Training epoch-49 batch-127
Running loss of epoch-49 batch-127 = 3.811437636613846e-06

Training epoch-49 batch-128
Running loss of epoch-49 batch-128 = 3.430992364883423e-06

Training epoch-49 batch-129
Running loss of epoch-49 batch-129 = 5.459180101752281e-06

Training epoch-49 batch-130
Running loss of epoch-49 batch-130 = 4.016328603029251e-06

Training epoch-49 batch-131
Running loss of epoch-49 batch-131 = 3.4580007195472717e-06

Training epoch-49 batch-132
Running loss of epoch-49 batch-132 = 2.494547516107559e-06

Training epoch-49 batch-133
Running loss of epoch-49 batch-133 = 3.7802383303642273e-06

Training epoch-49 batch-134
Running loss of epoch-49 batch-134 = 3.112480044364929e-06

Training epoch-49 batch-135
Running loss of epoch-49 batch-135 = 3.1364616006612778e-06

Training epoch-49 batch-136
Running loss of epoch-49 batch-136 = 3.5993289202451706e-06

Training epoch-49 batch-137
Running loss of epoch-49 batch-137 = 3.1900126487016678e-06

Training epoch-49 batch-138
Running loss of epoch-49 batch-138 = 2.116197720170021e-06

Training epoch-49 batch-139
Running loss of epoch-49 batch-139 = 3.6067795008420944e-06

Training epoch-49 batch-140
Running loss of epoch-49 batch-140 = 1.1008232831954956e-06

Training epoch-49 batch-141
Running loss of epoch-49 batch-141 = 1.3723038136959076e-06

Training epoch-49 batch-142
Running loss of epoch-49 batch-142 = 2.7942005544900894e-06

Training epoch-49 batch-143
Running loss of epoch-49 batch-143 = 2.5026965886354446e-06

Training epoch-49 batch-144
Running loss of epoch-49 batch-144 = 2.0633451640605927e-06

Training epoch-49 batch-145
Running loss of epoch-49 batch-145 = 1.9532162696123123e-06

Training epoch-49 batch-146
Running loss of epoch-49 batch-146 = 3.475463017821312e-06

Training epoch-49 batch-147
Running loss of epoch-49 batch-147 = 3.034481778740883e-06

Training epoch-49 batch-148
Running loss of epoch-49 batch-148 = 3.828899934887886e-06

Training epoch-49 batch-149
Running loss of epoch-49 batch-149 = 3.0426308512687683e-06

Training epoch-49 batch-150
Running loss of epoch-49 batch-150 = 3.9960723370313644e-06

Training epoch-49 batch-151
Running loss of epoch-49 batch-151 = 2.888031303882599e-06

Training epoch-49 batch-152
Running loss of epoch-49 batch-152 = 2.6561319828033447e-06

Training epoch-49 batch-153
Running loss of epoch-49 batch-153 = 3.338092938065529e-06

Training epoch-49 batch-154
Running loss of epoch-49 batch-154 = 2.4186447262763977e-06

Training epoch-49 batch-155
Running loss of epoch-49 batch-155 = 2.521788701415062e-06

Training epoch-49 batch-156
Running loss of epoch-49 batch-156 = 3.4153927117586136e-06

Training epoch-49 batch-157
Running loss of epoch-49 batch-157 = 8.169561624526978e-06

Finished training epoch-49.



Average train loss at epoch-49 = 2.9526710510253905e-06

Started Evaluation

Average val loss at epoch-49 = 3.106956406643516

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.81 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 26.91 %
Accuracy for class get is: 35.64 %

Overall Accuracy = 54.57 %

Finished Evaluation



Started training epoch-50


Training epoch-50 batch-1
Running loss of epoch-50 batch-1 = 2.635642886161804e-06

Training epoch-50 batch-2
Running loss of epoch-50 batch-2 = 3.043562173843384e-06

Training epoch-50 batch-3
Running loss of epoch-50 batch-3 = 2.6917550712823868e-06

Training epoch-50 batch-4
Running loss of epoch-50 batch-4 = 2.2994354367256165e-06

Training epoch-50 batch-5
Running loss of epoch-50 batch-5 = 1.7762649804353714e-06

Training epoch-50 batch-6
Running loss of epoch-50 batch-6 = 2.3616012185811996e-06

Training epoch-50 batch-7
Running loss of epoch-50 batch-7 = 2.635875716805458e-06

Training epoch-50 batch-8
Running loss of epoch-50 batch-8 = 2.0756851881742477e-06

Training epoch-50 batch-9
Running loss of epoch-50 batch-9 = 2.92179174721241e-06

Training epoch-50 batch-10
Running loss of epoch-50 batch-10 = 1.944834366440773e-06

Training epoch-50 batch-11
Running loss of epoch-50 batch-11 = 2.6703346520662308e-06

Training epoch-50 batch-12
Running loss of epoch-50 batch-12 = 3.0426308512687683e-06

Training epoch-50 batch-13
Running loss of epoch-50 batch-13 = 3.2025855034589767e-06

Training epoch-50 batch-14
Running loss of epoch-50 batch-14 = 3.334367647767067e-06

Training epoch-50 batch-15
Running loss of epoch-50 batch-15 = 3.0260998755693436e-06

Training epoch-50 batch-16
Running loss of epoch-50 batch-16 = 3.4426338970661163e-06

Training epoch-50 batch-17
Running loss of epoch-50 batch-17 = 4.001660272479057e-06

Training epoch-50 batch-18
Running loss of epoch-50 batch-18 = 2.5727786123752594e-06

Training epoch-50 batch-19
Running loss of epoch-50 batch-19 = 2.8244685381650925e-06

Training epoch-50 batch-20
Running loss of epoch-50 batch-20 = 2.8668437153100967e-06

Training epoch-50 batch-21
Running loss of epoch-50 batch-21 = 2.169515937566757e-06

Training epoch-50 batch-22
Running loss of epoch-50 batch-22 = 3.0605588108301163e-06

Training epoch-50 batch-23
Running loss of epoch-50 batch-23 = 3.8133002817630768e-06

Training epoch-50 batch-24
Running loss of epoch-50 batch-24 = 2.8319191187620163e-06

Training epoch-50 batch-25
Running loss of epoch-50 batch-25 = 2.61794775724411e-06

Training epoch-50 batch-26
Running loss of epoch-50 batch-26 = 3.473600372672081e-06

Training epoch-50 batch-27
Running loss of epoch-50 batch-27 = 3.3515971153974533e-06

Training epoch-50 batch-28
Running loss of epoch-50 batch-28 = 4.037516191601753e-06

Training epoch-50 batch-29
Running loss of epoch-50 batch-29 = 2.5837216526269913e-06

Training epoch-50 batch-30
Running loss of epoch-50 batch-30 = 2.684071660041809e-06

Training epoch-50 batch-31
Running loss of epoch-50 batch-31 = 4.297122359275818e-06

Training epoch-50 batch-32
Running loss of epoch-50 batch-32 = 3.3979304134845734e-06

Training epoch-50 batch-33
Running loss of epoch-50 batch-33 = 3.0347146093845367e-06

Training epoch-50 batch-34
Running loss of epoch-50 batch-34 = 2.5865156203508377e-06

Training epoch-50 batch-35
Running loss of epoch-50 batch-35 = 1.996755599975586e-06

Training epoch-50 batch-36
Running loss of epoch-50 batch-36 = 3.0007213354110718e-06

Training epoch-50 batch-37
Running loss of epoch-50 batch-37 = 3.5637058317661285e-06

Training epoch-50 batch-38
Running loss of epoch-50 batch-38 = 2.3138709366321564e-06

Training epoch-50 batch-39
Running loss of epoch-50 batch-39 = 2.1425075829029083e-06

Training epoch-50 batch-40
Running loss of epoch-50 batch-40 = 2.1185260266065598e-06

Training epoch-50 batch-41
Running loss of epoch-50 batch-41 = 2.6067718863487244e-06

Training epoch-50 batch-42
Running loss of epoch-50 batch-42 = 5.139969289302826e-06

Training epoch-50 batch-43
Running loss of epoch-50 batch-43 = 3.500143066048622e-06

Training epoch-50 batch-44
Running loss of epoch-50 batch-44 = 2.9066577553749084e-06

Training epoch-50 batch-45
Running loss of epoch-50 batch-45 = 4.597241058945656e-06

Training epoch-50 batch-46
Running loss of epoch-50 batch-46 = 2.6489142328500748e-06

Training epoch-50 batch-47
Running loss of epoch-50 batch-47 = 2.571381628513336e-06

Training epoch-50 batch-48
Running loss of epoch-50 batch-48 = 3.05776484310627e-06

Training epoch-50 batch-49
Running loss of epoch-50 batch-49 = 2.4614855647087097e-06

Training epoch-50 batch-50
Running loss of epoch-50 batch-50 = 2.746470272541046e-06

Training epoch-50 batch-51
Running loss of epoch-50 batch-51 = 2.035638317465782e-06

Training epoch-50 batch-52
Running loss of epoch-50 batch-52 = 1.782318577170372e-06

Training epoch-50 batch-53
Running loss of epoch-50 batch-53 = 3.4740660339593887e-06

Training epoch-50 batch-54
Running loss of epoch-50 batch-54 = 3.4491531550884247e-06

Training epoch-50 batch-55
Running loss of epoch-50 batch-55 = 3.6212150007486343e-06

Training epoch-50 batch-56
Running loss of epoch-50 batch-56 = 3.2640527933835983e-06

Training epoch-50 batch-57
Running loss of epoch-50 batch-57 = 1.841457560658455e-06

Training epoch-50 batch-58
Running loss of epoch-50 batch-58 = 3.7299469113349915e-06

Training epoch-50 batch-59
Running loss of epoch-50 batch-59 = 3.1830277293920517e-06

Training epoch-50 batch-60
Running loss of epoch-50 batch-60 = 1.6775447875261307e-06

Training epoch-50 batch-61
Running loss of epoch-50 batch-61 = 2.6456546038389206e-06

Training epoch-50 batch-62
Running loss of epoch-50 batch-62 = 4.535773769021034e-06

Training epoch-50 batch-63
Running loss of epoch-50 batch-63 = 2.6782508939504623e-06

Training epoch-50 batch-64
Running loss of epoch-50 batch-64 = 1.7401762306690216e-06

Training epoch-50 batch-65
Running loss of epoch-50 batch-65 = 2.891058102250099e-06

Training epoch-50 batch-66
Running loss of epoch-50 batch-66 = 4.126923158764839e-06

Training epoch-50 batch-67
Running loss of epoch-50 batch-67 = 5.1443930715322495e-06

Training epoch-50 batch-68
Running loss of epoch-50 batch-68 = 2.1599698811769485e-06

Training epoch-50 batch-69
Running loss of epoch-50 batch-69 = 1.9602011889219284e-06

Training epoch-50 batch-70
Running loss of epoch-50 batch-70 = 3.430526703596115e-06

Training epoch-50 batch-71
Running loss of epoch-50 batch-71 = 2.5569461286067963e-06

Training epoch-50 batch-72
Running loss of epoch-50 batch-72 = 2.7688220143318176e-06

Training epoch-50 batch-73
Running loss of epoch-50 batch-73 = 2.2277235984802246e-06

Training epoch-50 batch-74
Running loss of epoch-50 batch-74 = 2.468004822731018e-06

Training epoch-50 batch-75
Running loss of epoch-50 batch-75 = 2.2989697754383087e-06

Training epoch-50 batch-76
Running loss of epoch-50 batch-76 = 1.1050142347812653e-06

Training epoch-50 batch-77
Running loss of epoch-50 batch-77 = 9.520445019006729e-07

Training epoch-50 batch-78
Running loss of epoch-50 batch-78 = 3.893161192536354e-06

Training epoch-50 batch-79
Running loss of epoch-50 batch-79 = 2.168351784348488e-06

Training epoch-50 batch-80
Running loss of epoch-50 batch-80 = 2.6884954422712326e-06

Training epoch-50 batch-81
Running loss of epoch-50 batch-81 = 2.7532223612070084e-06

Training epoch-50 batch-82
Running loss of epoch-50 batch-82 = 3.7963036447763443e-06

Training epoch-50 batch-83
Running loss of epoch-50 batch-83 = 3.7392601370811462e-06

Training epoch-50 batch-84
Running loss of epoch-50 batch-84 = 3.3802352845668793e-06

Training epoch-50 batch-85
Running loss of epoch-50 batch-85 = 2.2442545741796494e-06

Training epoch-50 batch-86
Running loss of epoch-50 batch-86 = 2.6971101760864258e-06

Training epoch-50 batch-87
Running loss of epoch-50 batch-87 = 1.8030405044555664e-06

Training epoch-50 batch-88
Running loss of epoch-50 batch-88 = 3.423541784286499e-06

Training epoch-50 batch-89
Running loss of epoch-50 batch-89 = 2.759275957942009e-06

Training epoch-50 batch-90
Running loss of epoch-50 batch-90 = 1.8242280930280685e-06

Training epoch-50 batch-91
Running loss of epoch-50 batch-91 = 2.9525253921747208e-06

Training epoch-50 batch-92
Running loss of epoch-50 batch-92 = 4.500383511185646e-06

Training epoch-50 batch-93
Running loss of epoch-50 batch-93 = 2.8924550861120224e-06

Training epoch-50 batch-94
Running loss of epoch-50 batch-94 = 1.7709098756313324e-06

Training epoch-50 batch-95
Running loss of epoch-50 batch-95 = 1.862645149230957e-06

Training epoch-50 batch-96
Running loss of epoch-50 batch-96 = 2.91457399725914e-06

Training epoch-50 batch-97
Running loss of epoch-50 batch-97 = 3.157416358590126e-06

Training epoch-50 batch-98
Running loss of epoch-50 batch-98 = 1.6281846910715103e-06

Training epoch-50 batch-99
Running loss of epoch-50 batch-99 = 2.1739397197961807e-06

Training epoch-50 batch-100
Running loss of epoch-50 batch-100 = 2.417946234345436e-06

Training epoch-50 batch-101
Running loss of epoch-50 batch-101 = 4.220288246870041e-06

Training epoch-50 batch-102
Running loss of epoch-50 batch-102 = 1.5283003449440002e-06

Training epoch-50 batch-103
Running loss of epoch-50 batch-103 = 3.345077857375145e-06

Training epoch-50 batch-104
Running loss of epoch-50 batch-104 = 3.5422854125499725e-06

Training epoch-50 batch-105
Running loss of epoch-50 batch-105 = 2.902233973145485e-06

Training epoch-50 batch-106
Running loss of epoch-50 batch-106 = 3.452179953455925e-06

Training epoch-50 batch-107
Running loss of epoch-50 batch-107 = 3.335997462272644e-06

Training epoch-50 batch-108
Running loss of epoch-50 batch-108 = 2.5902409106492996e-06

Training epoch-50 batch-109
Running loss of epoch-50 batch-109 = 3.2829120755195618e-06

Training epoch-50 batch-110
Running loss of epoch-50 batch-110 = 2.5830231606960297e-06

Training epoch-50 batch-111
Running loss of epoch-50 batch-111 = 2.5641638785600662e-06

Training epoch-50 batch-112
Running loss of epoch-50 batch-112 = 2.371380105614662e-06

Training epoch-50 batch-113
Running loss of epoch-50 batch-113 = 3.780936822295189e-06

Training epoch-50 batch-114
Running loss of epoch-50 batch-114 = 2.5797635316848755e-06

Training epoch-50 batch-115
Running loss of epoch-50 batch-115 = 3.1534582376480103e-06

Training epoch-50 batch-116
Running loss of epoch-50 batch-116 = 2.4051405489444733e-06

Training epoch-50 batch-117
Running loss of epoch-50 batch-117 = 3.2212119549512863e-06

Training epoch-50 batch-118
Running loss of epoch-50 batch-118 = 2.2265594452619553e-06

Training epoch-50 batch-119
Running loss of epoch-50 batch-119 = 3.042863681912422e-06

Training epoch-50 batch-120
Running loss of epoch-50 batch-120 = 1.9578728824853897e-06

Training epoch-50 batch-121
Running loss of epoch-50 batch-121 = 2.8104986995458603e-06

Training epoch-50 batch-122
Running loss of epoch-50 batch-122 = 3.566732630133629e-06

Training epoch-50 batch-123
Running loss of epoch-50 batch-123 = 3.327382728457451e-06

Training epoch-50 batch-124
Running loss of epoch-50 batch-124 = 4.211440682411194e-06

Training epoch-50 batch-125
Running loss of epoch-50 batch-125 = 2.402113750576973e-06

Training epoch-50 batch-126
Running loss of epoch-50 batch-126 = 3.0167866498231888e-06

Training epoch-50 batch-127
Running loss of epoch-50 batch-127 = 2.641230821609497e-06

Training epoch-50 batch-128
Running loss of epoch-50 batch-128 = 2.9013026505708694e-06

Training epoch-50 batch-129
Running loss of epoch-50 batch-129 = 2.294778823852539e-06

Training epoch-50 batch-130
Running loss of epoch-50 batch-130 = 2.0028091967105865e-06

Training epoch-50 batch-131
Running loss of epoch-50 batch-131 = 3.1886156648397446e-06

Training epoch-50 batch-132
Running loss of epoch-50 batch-132 = 1.973705366253853e-06

Training epoch-50 batch-133
Running loss of epoch-50 batch-133 = 3.2691750675439835e-06

Training epoch-50 batch-134
Running loss of epoch-50 batch-134 = 2.9136426746845245e-06

Training epoch-50 batch-135
Running loss of epoch-50 batch-135 = 3.416789695620537e-06

Training epoch-50 batch-136
Running loss of epoch-50 batch-136 = 3.429129719734192e-06

Training epoch-50 batch-137
Running loss of epoch-50 batch-137 = 3.8079451769590378e-06

Training epoch-50 batch-138
Running loss of epoch-50 batch-138 = 3.7427525967359543e-06

Training epoch-50 batch-139
Running loss of epoch-50 batch-139 = 2.382323145866394e-06

Training epoch-50 batch-140
Running loss of epoch-50 batch-140 = 3.4028198570013046e-06

Training epoch-50 batch-141
Running loss of epoch-50 batch-141 = 3.6067795008420944e-06

Training epoch-50 batch-142
Running loss of epoch-50 batch-142 = 2.719927579164505e-06

Training epoch-50 batch-143
Running loss of epoch-50 batch-143 = 2.207700163125992e-06

Training epoch-50 batch-144
Running loss of epoch-50 batch-144 = 2.2230669856071472e-06

Training epoch-50 batch-145
Running loss of epoch-50 batch-145 = 3.3297110348939896e-06

Training epoch-50 batch-146
Running loss of epoch-50 batch-146 = 2.8852373361587524e-06

Training epoch-50 batch-147
Running loss of epoch-50 batch-147 = 2.1173618733882904e-06

Training epoch-50 batch-148
Running loss of epoch-50 batch-148 = 3.949971869587898e-06

Training epoch-50 batch-149
Running loss of epoch-50 batch-149 = 3.232154995203018e-06

Training epoch-50 batch-150
Running loss of epoch-50 batch-150 = 2.9229559004306793e-06

Training epoch-50 batch-151
Running loss of epoch-50 batch-151 = 2.2740568965673447e-06

Training epoch-50 batch-152
Running loss of epoch-50 batch-152 = 3.3364631235599518e-06

Training epoch-50 batch-153
Running loss of epoch-50 batch-153 = 1.485692337155342e-06

Training epoch-50 batch-154
Running loss of epoch-50 batch-154 = 1.996755599975586e-06

Training epoch-50 batch-155
Running loss of epoch-50 batch-155 = 2.5639310479164124e-06

Training epoch-50 batch-156
Running loss of epoch-50 batch-156 = 2.489425241947174e-06

Training epoch-50 batch-157
Running loss of epoch-50 batch-157 = 4.425644874572754e-06

Finished training epoch-50.



Average train loss at epoch-50 = 2.8615012764930727e-06

Started Evaluation

Average val loss at epoch-50 = 3.1139998655570182

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.64 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-51


Training epoch-51 batch-1
Running loss of epoch-51 batch-1 = 2.993503585457802e-06

Training epoch-51 batch-2
Running loss of epoch-51 batch-2 = 2.1080486476421356e-06

Training epoch-51 batch-3
Running loss of epoch-51 batch-3 = 2.9157381504774094e-06

Training epoch-51 batch-4
Running loss of epoch-51 batch-4 = 2.2815074771642685e-06

Training epoch-51 batch-5
Running loss of epoch-51 batch-5 = 3.61795537173748e-06

Training epoch-51 batch-6
Running loss of epoch-51 batch-6 = 3.3052638173103333e-06

Training epoch-51 batch-7
Running loss of epoch-51 batch-7 = 2.453569322824478e-06

Training epoch-51 batch-8
Running loss of epoch-51 batch-8 = 2.6316847652196884e-06

Training epoch-51 batch-9
Running loss of epoch-51 batch-9 = 4.3548643589019775e-06

Training epoch-51 batch-10
Running loss of epoch-51 batch-10 = 2.150190994143486e-06

Training epoch-51 batch-11
Running loss of epoch-51 batch-11 = 3.4621916711330414e-06

Training epoch-51 batch-12
Running loss of epoch-51 batch-12 = 3.314577043056488e-06

Training epoch-51 batch-13
Running loss of epoch-51 batch-13 = 2.9171351343393326e-06

Training epoch-51 batch-14
Running loss of epoch-51 batch-14 = 1.8994323909282684e-06

Training epoch-51 batch-15
Running loss of epoch-51 batch-15 = 2.955319359898567e-06

Training epoch-51 batch-16
Running loss of epoch-51 batch-16 = 2.0158477127552032e-06

Training epoch-51 batch-17
Running loss of epoch-51 batch-17 = 4.707137122750282e-06

Training epoch-51 batch-18
Running loss of epoch-51 batch-18 = 2.7460046112537384e-06

Training epoch-51 batch-19
Running loss of epoch-51 batch-19 = 3.489898517727852e-06

Training epoch-51 batch-20
Running loss of epoch-51 batch-20 = 2.491055056452751e-06

Training epoch-51 batch-21
Running loss of epoch-51 batch-21 = 2.357643097639084e-06

Training epoch-51 batch-22
Running loss of epoch-51 batch-22 = 2.9637012630701065e-06

Training epoch-51 batch-23
Running loss of epoch-51 batch-23 = 2.3704487830400467e-06

Training epoch-51 batch-24
Running loss of epoch-51 batch-24 = 3.0526425689458847e-06

Training epoch-51 batch-25
Running loss of epoch-51 batch-25 = 3.62517312169075e-06

Training epoch-51 batch-26
Running loss of epoch-51 batch-26 = 1.823296770453453e-06

Training epoch-51 batch-27
Running loss of epoch-51 batch-27 = 2.348097041249275e-06

Training epoch-51 batch-28
Running loss of epoch-51 batch-28 = 3.1311064958572388e-06

Training epoch-51 batch-29
Running loss of epoch-51 batch-29 = 3.7616118788719177e-06

Training epoch-51 batch-30
Running loss of epoch-51 batch-30 = 3.4356489777565002e-06

Training epoch-51 batch-31
Running loss of epoch-51 batch-31 = 3.55718657374382e-06

Training epoch-51 batch-32
Running loss of epoch-51 batch-32 = 2.2149179130792618e-06

Training epoch-51 batch-33
Running loss of epoch-51 batch-33 = 1.932494342327118e-06

Training epoch-51 batch-34
Running loss of epoch-51 batch-34 = 3.2475218176841736e-06

Training epoch-51 batch-35
Running loss of epoch-51 batch-35 = 2.539250999689102e-06

Training epoch-51 batch-36
Running loss of epoch-51 batch-36 = 3.5779085010290146e-06

Training epoch-51 batch-37
Running loss of epoch-51 batch-37 = 2.5033950805664062e-06

Training epoch-51 batch-38
Running loss of epoch-51 batch-38 = 1.46450474858284e-06

Training epoch-51 batch-39
Running loss of epoch-51 batch-39 = 1.8882565200328827e-06

Training epoch-51 batch-40
Running loss of epoch-51 batch-40 = 2.2111926227808e-06

Training epoch-51 batch-41
Running loss of epoch-51 batch-41 = 2.728542312979698e-06

Training epoch-51 batch-42
Running loss of epoch-51 batch-42 = 2.562766894698143e-06

Training epoch-51 batch-43
Running loss of epoch-51 batch-43 = 2.4638138711452484e-06

Training epoch-51 batch-44
Running loss of epoch-51 batch-44 = 1.5255063772201538e-06

Training epoch-51 batch-45
Running loss of epoch-51 batch-45 = 1.7264392226934433e-06

Training epoch-51 batch-46
Running loss of epoch-51 batch-46 = 3.645196557044983e-06

Training epoch-51 batch-47
Running loss of epoch-51 batch-47 = 3.0370429158210754e-06

Training epoch-51 batch-48
Running loss of epoch-51 batch-48 = 1.8167775124311447e-06

Training epoch-51 batch-49
Running loss of epoch-51 batch-49 = 2.8582289814949036e-06

Training epoch-51 batch-50
Running loss of epoch-51 batch-50 = 2.273591235280037e-06

Training epoch-51 batch-51
Running loss of epoch-51 batch-51 = 2.470100298523903e-06

Training epoch-51 batch-52
Running loss of epoch-51 batch-52 = 2.2831372916698456e-06

Training epoch-51 batch-53
Running loss of epoch-51 batch-53 = 2.423534169793129e-06

Training epoch-51 batch-54
Running loss of epoch-51 batch-54 = 2.423534169793129e-06

Training epoch-51 batch-55
Running loss of epoch-51 batch-55 = 2.5709159672260284e-06

Training epoch-51 batch-56
Running loss of epoch-51 batch-56 = 2.394896000623703e-06

Training epoch-51 batch-57
Running loss of epoch-51 batch-57 = 3.905268386006355e-06

Training epoch-51 batch-58
Running loss of epoch-51 batch-58 = 3.3492688089609146e-06

Training epoch-51 batch-59
Running loss of epoch-51 batch-59 = 2.4975743144750595e-06

Training epoch-51 batch-60
Running loss of epoch-51 batch-60 = 2.2416934370994568e-06

Training epoch-51 batch-61
Running loss of epoch-51 batch-61 = 2.6279594749212265e-06

Training epoch-51 batch-62
Running loss of epoch-51 batch-62 = 2.9890798032283783e-06

Training epoch-51 batch-63
Running loss of epoch-51 batch-63 = 2.148561179637909e-06

Training epoch-51 batch-64
Running loss of epoch-51 batch-64 = 3.6410056054592133e-06

Training epoch-51 batch-65
Running loss of epoch-51 batch-65 = 1.4600809663534164e-06

Training epoch-51 batch-66
Running loss of epoch-51 batch-66 = 2.241227775812149e-06

Training epoch-51 batch-67
Running loss of epoch-51 batch-67 = 3.2582320272922516e-06

Training epoch-51 batch-68
Running loss of epoch-51 batch-68 = 2.0333100110292435e-06

Training epoch-51 batch-69
Running loss of epoch-51 batch-69 = 3.117835149168968e-06

Training epoch-51 batch-70
Running loss of epoch-51 batch-70 = 2.3294705897569656e-06

Training epoch-51 batch-71
Running loss of epoch-51 batch-71 = 2.034008502960205e-06

Training epoch-51 batch-72
Running loss of epoch-51 batch-72 = 2.593500539660454e-06

Training epoch-51 batch-73
Running loss of epoch-51 batch-73 = 2.4903565645217896e-06

Training epoch-51 batch-74
Running loss of epoch-51 batch-74 = 1.8598511815071106e-06

Training epoch-51 batch-75
Running loss of epoch-51 batch-75 = 3.2815150916576385e-06

Training epoch-51 batch-76
Running loss of epoch-51 batch-76 = 2.9783695936203003e-06

Training epoch-51 batch-77
Running loss of epoch-51 batch-77 = 1.941109076142311e-06

Training epoch-51 batch-78
Running loss of epoch-51 batch-78 = 3.3606775104999542e-06

Training epoch-51 batch-79
Running loss of epoch-51 batch-79 = 1.894775778055191e-06

Training epoch-51 batch-80
Running loss of epoch-51 batch-80 = 2.202345058321953e-06

Training epoch-51 batch-81
Running loss of epoch-51 batch-81 = 1.9401777535676956e-06

Training epoch-51 batch-82
Running loss of epoch-51 batch-82 = 2.5618355721235275e-06

Training epoch-51 batch-83
Running loss of epoch-51 batch-83 = 2.3778993636369705e-06

Training epoch-51 batch-84
Running loss of epoch-51 batch-84 = 2.553919330239296e-06

Training epoch-51 batch-85
Running loss of epoch-51 batch-85 = 2.269865944981575e-06

Training epoch-51 batch-86
Running loss of epoch-51 batch-86 = 2.3331958800554276e-06

Training epoch-51 batch-87
Running loss of epoch-51 batch-87 = 1.7795246094465256e-06

Training epoch-51 batch-88
Running loss of epoch-51 batch-88 = 3.602355718612671e-06

Training epoch-51 batch-89
Running loss of epoch-51 batch-89 = 3.005843609571457e-06

Training epoch-51 batch-90
Running loss of epoch-51 batch-90 = 2.569984644651413e-06

Training epoch-51 batch-91
Running loss of epoch-51 batch-91 = 4.2156316339969635e-06

Training epoch-51 batch-92
Running loss of epoch-51 batch-92 = 3.60771082341671e-06

Training epoch-51 batch-93
Running loss of epoch-51 batch-93 = 2.2244639694690704e-06

Training epoch-51 batch-94
Running loss of epoch-51 batch-94 = 3.488035872578621e-06

Training epoch-51 batch-95
Running loss of epoch-51 batch-95 = 3.0710361897945404e-06

Training epoch-51 batch-96
Running loss of epoch-51 batch-96 = 2.4228356778621674e-06

Training epoch-51 batch-97
Running loss of epoch-51 batch-97 = 2.423534169793129e-06

Training epoch-51 batch-98
Running loss of epoch-51 batch-98 = 2.5781337171792984e-06

Training epoch-51 batch-99
Running loss of epoch-51 batch-99 = 3.437977284193039e-06

Training epoch-51 batch-100
Running loss of epoch-51 batch-100 = 2.5334302335977554e-06

Training epoch-51 batch-101
Running loss of epoch-51 batch-101 = 2.9515940696001053e-06

Training epoch-51 batch-102
Running loss of epoch-51 batch-102 = 3.0312221497297287e-06

Training epoch-51 batch-103
Running loss of epoch-51 batch-103 = 3.327382728457451e-06

Training epoch-51 batch-104
Running loss of epoch-51 batch-104 = 3.1064264476299286e-06

Training epoch-51 batch-105
Running loss of epoch-51 batch-105 = 2.1122395992279053e-06

Training epoch-51 batch-106
Running loss of epoch-51 batch-106 = 3.6549754440784454e-06

Training epoch-51 batch-107
Running loss of epoch-51 batch-107 = 2.6659108698368073e-06

Training epoch-51 batch-108
Running loss of epoch-51 batch-108 = 2.8221402317285538e-06

Training epoch-51 batch-109
Running loss of epoch-51 batch-109 = 2.7299392968416214e-06

Training epoch-51 batch-110
Running loss of epoch-51 batch-110 = 3.851018846035004e-06

Training epoch-51 batch-111
Running loss of epoch-51 batch-111 = 1.5122350305318832e-06

Training epoch-51 batch-112
Running loss of epoch-51 batch-112 = 1.6067642718553543e-06

Training epoch-51 batch-113
Running loss of epoch-51 batch-113 = 4.577217623591423e-06

Training epoch-51 batch-114
Running loss of epoch-51 batch-114 = 3.943918272852898e-06

Training epoch-51 batch-115
Running loss of epoch-51 batch-115 = 1.8600840121507645e-06

Training epoch-51 batch-116
Running loss of epoch-51 batch-116 = 2.3909378796815872e-06

Training epoch-51 batch-117
Running loss of epoch-51 batch-117 = 3.0775554478168488e-06

Training epoch-51 batch-118
Running loss of epoch-51 batch-118 = 2.912478521466255e-06

Training epoch-51 batch-119
Running loss of epoch-51 batch-119 = 3.7155114114284515e-06

Training epoch-51 batch-120
Running loss of epoch-51 batch-120 = 1.3078097254037857e-06

Training epoch-51 batch-121
Running loss of epoch-51 batch-121 = 2.4852342903614044e-06

Training epoch-51 batch-122
Running loss of epoch-51 batch-122 = 3.0514784157276154e-06

Training epoch-51 batch-123
Running loss of epoch-51 batch-123 = 2.1210871636867523e-06

Training epoch-51 batch-124
Running loss of epoch-51 batch-124 = 3.5231932997703552e-06

Training epoch-51 batch-125
Running loss of epoch-51 batch-125 = 2.539483830332756e-06

Training epoch-51 batch-126
Running loss of epoch-51 batch-126 = 1.8195714801549911e-06

Training epoch-51 batch-127
Running loss of epoch-51 batch-127 = 3.3711548894643784e-06

Training epoch-51 batch-128
Running loss of epoch-51 batch-128 = 2.273824065923691e-06

Training epoch-51 batch-129
Running loss of epoch-51 batch-129 = 2.796528860926628e-06

Training epoch-51 batch-130
Running loss of epoch-51 batch-130 = 5.323672667145729e-06

Training epoch-51 batch-131
Running loss of epoch-51 batch-131 = 1.909211277961731e-06

Training epoch-51 batch-132
Running loss of epoch-51 batch-132 = 2.1457672119140625e-06

Training epoch-51 batch-133
Running loss of epoch-51 batch-133 = 1.4873221516609192e-06

Training epoch-51 batch-134
Running loss of epoch-51 batch-134 = 3.23588028550148e-06

Training epoch-51 batch-135
Running loss of epoch-51 batch-135 = 2.7527566999197006e-06

Training epoch-51 batch-136
Running loss of epoch-51 batch-136 = 2.955785021185875e-06

Training epoch-51 batch-137
Running loss of epoch-51 batch-137 = 2.9508955776691437e-06

Training epoch-51 batch-138
Running loss of epoch-51 batch-138 = 2.932269126176834e-06

Training epoch-51 batch-139
Running loss of epoch-51 batch-139 = 3.991182893514633e-06

Training epoch-51 batch-140
Running loss of epoch-51 batch-140 = 2.462882548570633e-06

Training epoch-51 batch-141
Running loss of epoch-51 batch-141 = 2.587679773569107e-06

Training epoch-51 batch-142
Running loss of epoch-51 batch-142 = 2.462416887283325e-06

Training epoch-51 batch-143
Running loss of epoch-51 batch-143 = 2.33273021876812e-06

Training epoch-51 batch-144
Running loss of epoch-51 batch-144 = 3.118067979812622e-06

Training epoch-51 batch-145
Running loss of epoch-51 batch-145 = 2.5546178221702576e-06

Training epoch-51 batch-146
Running loss of epoch-51 batch-146 = 2.7618370950222015e-06

Training epoch-51 batch-147
Running loss of epoch-51 batch-147 = 2.5401823222637177e-06

Training epoch-51 batch-148
Running loss of epoch-51 batch-148 = 3.0978117138147354e-06

Training epoch-51 batch-149
Running loss of epoch-51 batch-149 = 4.250556230545044e-06

Training epoch-51 batch-150
Running loss of epoch-51 batch-150 = 2.146698534488678e-06

Training epoch-51 batch-151
Running loss of epoch-51 batch-151 = 3.7909485399723053e-06

Training epoch-51 batch-152
Running loss of epoch-51 batch-152 = 2.750195562839508e-06

Training epoch-51 batch-153
Running loss of epoch-51 batch-153 = 4.186062142252922e-06

Training epoch-51 batch-154
Running loss of epoch-51 batch-154 = 4.2619649320840836e-06

Training epoch-51 batch-155
Running loss of epoch-51 batch-155 = 3.7460122257471085e-06

Training epoch-51 batch-156
Running loss of epoch-51 batch-156 = 3.0442606657743454e-06

Training epoch-51 batch-157
Running loss of epoch-51 batch-157 = 2.047419548034668e-05

Finished training epoch-51.



Average train loss at epoch-51 = 2.800208330154419e-06

Started Evaluation

Average val loss at epoch-51 = 3.124381041840503

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-52


Training epoch-52 batch-1
Running loss of epoch-52 batch-1 = 2.380693331360817e-06

Training epoch-52 batch-2
Running loss of epoch-52 batch-2 = 2.025160938501358e-06

Training epoch-52 batch-3
Running loss of epoch-52 batch-3 = 3.7818681448698044e-06

Training epoch-52 batch-4
Running loss of epoch-52 batch-4 = 1.661013811826706e-06

Training epoch-52 batch-5
Running loss of epoch-52 batch-5 = 2.993037924170494e-06

Training epoch-52 batch-6
Running loss of epoch-52 batch-6 = 2.5616027414798737e-06

Training epoch-52 batch-7
Running loss of epoch-52 batch-7 = 2.609100192785263e-06

Training epoch-52 batch-8
Running loss of epoch-52 batch-8 = 2.8230715543031693e-06

Training epoch-52 batch-9
Running loss of epoch-52 batch-9 = 4.217959940433502e-06

Training epoch-52 batch-10
Running loss of epoch-52 batch-10 = 2.8850045055150986e-06

Training epoch-52 batch-11
Running loss of epoch-52 batch-11 = 2.1886080503463745e-06

Training epoch-52 batch-12
Running loss of epoch-52 batch-12 = 3.1120143830776215e-06

Training epoch-52 batch-13
Running loss of epoch-52 batch-13 = 2.5834888219833374e-06

Training epoch-52 batch-14
Running loss of epoch-52 batch-14 = 2.264510840177536e-06

Training epoch-52 batch-15
Running loss of epoch-52 batch-15 = 1.9506551325321198e-06

Training epoch-52 batch-16
Running loss of epoch-52 batch-16 = 3.0389055609703064e-06

Training epoch-52 batch-17
Running loss of epoch-52 batch-17 = 2.4917535483837128e-06

Training epoch-52 batch-18
Running loss of epoch-52 batch-18 = 2.5152694433927536e-06

Training epoch-52 batch-19
Running loss of epoch-52 batch-19 = 3.2924581319093704e-06

Training epoch-52 batch-20
Running loss of epoch-52 batch-20 = 2.0740553736686707e-06

Training epoch-52 batch-21
Running loss of epoch-52 batch-21 = 2.3401807993650436e-06

Training epoch-52 batch-22
Running loss of epoch-52 batch-22 = 3.860797733068466e-06

Training epoch-52 batch-23
Running loss of epoch-52 batch-23 = 2.2703316062688828e-06

Training epoch-52 batch-24
Running loss of epoch-52 batch-24 = 2.686167135834694e-06

Training epoch-52 batch-25
Running loss of epoch-52 batch-25 = 4.568370059132576e-06

Training epoch-52 batch-26
Running loss of epoch-52 batch-26 = 2.1012965589761734e-06

Training epoch-52 batch-27
Running loss of epoch-52 batch-27 = 2.9725488275289536e-06

Training epoch-52 batch-28
Running loss of epoch-52 batch-28 = 2.0673032850027084e-06

Training epoch-52 batch-29
Running loss of epoch-52 batch-29 = 2.866145223379135e-06

Training epoch-52 batch-30
Running loss of epoch-52 batch-30 = 2.173474058508873e-06

Training epoch-52 batch-31
Running loss of epoch-52 batch-31 = 2.151820808649063e-06

Training epoch-52 batch-32
Running loss of epoch-52 batch-32 = 3.484310582280159e-06

Training epoch-52 batch-33
Running loss of epoch-52 batch-33 = 2.1569430828094482e-06

Training epoch-52 batch-34
Running loss of epoch-52 batch-34 = 2.85380519926548e-06

Training epoch-52 batch-35
Running loss of epoch-52 batch-35 = 2.2472813725471497e-06

Training epoch-52 batch-36
Running loss of epoch-52 batch-36 = 2.8659123927354813e-06

Training epoch-52 batch-37
Running loss of epoch-52 batch-37 = 4.006316885352135e-06

Training epoch-52 batch-38
Running loss of epoch-52 batch-38 = 2.8528738766908646e-06

Training epoch-52 batch-39
Running loss of epoch-52 batch-39 = 4.150671884417534e-06

Training epoch-52 batch-40
Running loss of epoch-52 batch-40 = 2.6919879019260406e-06

Training epoch-52 batch-41
Running loss of epoch-52 batch-41 = 3.625405952334404e-06

Training epoch-52 batch-42
Running loss of epoch-52 batch-42 = 2.682441845536232e-06

Training epoch-52 batch-43
Running loss of epoch-52 batch-43 = 2.9227230697870255e-06

Training epoch-52 batch-44
Running loss of epoch-52 batch-44 = 2.960674464702606e-06

Training epoch-52 batch-45
Running loss of epoch-52 batch-45 = 2.748798578977585e-06

Training epoch-52 batch-46
Running loss of epoch-52 batch-46 = 3.644730895757675e-06

Training epoch-52 batch-47
Running loss of epoch-52 batch-47 = 3.545079380273819e-06

Training epoch-52 batch-48
Running loss of epoch-52 batch-48 = 1.3846438378095627e-06

Training epoch-52 batch-49
Running loss of epoch-52 batch-49 = 3.5800039768218994e-06

Training epoch-52 batch-50
Running loss of epoch-52 batch-50 = 2.1336600184440613e-06

Training epoch-52 batch-51
Running loss of epoch-52 batch-51 = 2.6708003133535385e-06

Training epoch-52 batch-52
Running loss of epoch-52 batch-52 = 3.3492688089609146e-06

Training epoch-52 batch-53
Running loss of epoch-52 batch-53 = 3.327149897813797e-06

Training epoch-52 batch-54
Running loss of epoch-52 batch-54 = 1.6172416508197784e-06

Training epoch-52 batch-55
Running loss of epoch-52 batch-55 = 3.1443778425455093e-06

Training epoch-52 batch-56
Running loss of epoch-52 batch-56 = 2.3527536541223526e-06

Training epoch-52 batch-57
Running loss of epoch-52 batch-57 = 2.1406449377536774e-06

Training epoch-52 batch-58
Running loss of epoch-52 batch-58 = 2.9739458113908768e-06

Training epoch-52 batch-59
Running loss of epoch-52 batch-59 = 2.928543835878372e-06

Training epoch-52 batch-60
Running loss of epoch-52 batch-60 = 2.8284266591072083e-06

Training epoch-52 batch-61
Running loss of epoch-52 batch-61 = 2.0221341401338577e-06

Training epoch-52 batch-62
Running loss of epoch-52 batch-62 = 3.000255674123764e-06

Training epoch-52 batch-63
Running loss of epoch-52 batch-63 = 2.205371856689453e-06

Training epoch-52 batch-64
Running loss of epoch-52 batch-64 = 4.016095772385597e-06

Training epoch-52 batch-65
Running loss of epoch-52 batch-65 = 2.8638169169425964e-06

Training epoch-52 batch-66
Running loss of epoch-52 batch-66 = 2.2810418158769608e-06

Training epoch-52 batch-67
Running loss of epoch-52 batch-67 = 2.316199243068695e-06

Training epoch-52 batch-68
Running loss of epoch-52 batch-68 = 3.585824742913246e-06

Training epoch-52 batch-69
Running loss of epoch-52 batch-69 = 2.4351757019758224e-06

Training epoch-52 batch-70
Running loss of epoch-52 batch-70 = 4.090368747711182e-06

Training epoch-52 batch-71
Running loss of epoch-52 batch-71 = 3.375113010406494e-06

Training epoch-52 batch-72
Running loss of epoch-52 batch-72 = 1.9131693989038467e-06

Training epoch-52 batch-73
Running loss of epoch-52 batch-73 = 3.8226135075092316e-06

Training epoch-52 batch-74
Running loss of epoch-52 batch-74 = 2.616550773382187e-06

Training epoch-52 batch-75
Running loss of epoch-52 batch-75 = 3.1993258744478226e-06

Training epoch-52 batch-76
Running loss of epoch-52 batch-76 = 2.2046733647584915e-06

Training epoch-52 batch-77
Running loss of epoch-52 batch-77 = 2.5399494916200638e-06

Training epoch-52 batch-78
Running loss of epoch-52 batch-78 = 3.107823431491852e-06

Training epoch-52 batch-79
Running loss of epoch-52 batch-79 = 1.796754077076912e-06

Training epoch-52 batch-80
Running loss of epoch-52 batch-80 = 2.0097941160202026e-06

Training epoch-52 batch-81
Running loss of epoch-52 batch-81 = 2.5602057576179504e-06

Training epoch-52 batch-82
Running loss of epoch-52 batch-82 = 1.9280705600976944e-06

Training epoch-52 batch-83
Running loss of epoch-52 batch-83 = 2.3720785975456238e-06

Training epoch-52 batch-84
Running loss of epoch-52 batch-84 = 2.882443368434906e-06

Training epoch-52 batch-85
Running loss of epoch-52 batch-85 = 3.0999071896076202e-06

Training epoch-52 batch-86
Running loss of epoch-52 batch-86 = 2.6158522814512253e-06

Training epoch-52 batch-87
Running loss of epoch-52 batch-87 = 3.129243850708008e-06

Training epoch-52 batch-88
Running loss of epoch-52 batch-88 = 4.0389131754636765e-06

Training epoch-52 batch-89
Running loss of epoch-52 batch-89 = 2.3851171135902405e-06

Training epoch-52 batch-90
Running loss of epoch-52 batch-90 = 2.3045577108860016e-06

Training epoch-52 batch-91
Running loss of epoch-52 batch-91 = 3.316439688205719e-06

Training epoch-52 batch-92
Running loss of epoch-52 batch-92 = 2.7494970709085464e-06

Training epoch-52 batch-93
Running loss of epoch-52 batch-93 = 3.0167866498231888e-06

Training epoch-52 batch-94
Running loss of epoch-52 batch-94 = 2.461019903421402e-06

Training epoch-52 batch-95
Running loss of epoch-52 batch-95 = 2.909451723098755e-06

Training epoch-52 batch-96
Running loss of epoch-52 batch-96 = 2.3245811462402344e-06

Training epoch-52 batch-97
Running loss of epoch-52 batch-97 = 2.4889595806598663e-06

Training epoch-52 batch-98
Running loss of epoch-52 batch-98 = 2.684304490685463e-06

Training epoch-52 batch-99
Running loss of epoch-52 batch-99 = 4.7138892114162445e-06

Training epoch-52 batch-100
Running loss of epoch-52 batch-100 = 2.6400666683912277e-06

Training epoch-52 batch-101
Running loss of epoch-52 batch-101 = 2.9671937227249146e-06

Training epoch-52 batch-102
Running loss of epoch-52 batch-102 = 2.439366653561592e-06

Training epoch-52 batch-103
Running loss of epoch-52 batch-103 = 3.0298251658678055e-06

Training epoch-52 batch-104
Running loss of epoch-52 batch-104 = 2.2894237190485e-06

Training epoch-52 batch-105
Running loss of epoch-52 batch-105 = 3.3264514058828354e-06

Training epoch-52 batch-106
Running loss of epoch-52 batch-106 = 3.139954060316086e-06

Training epoch-52 batch-107
Running loss of epoch-52 batch-107 = 4.1085295379161835e-06

Training epoch-52 batch-108
Running loss of epoch-52 batch-108 = 1.4828983694314957e-06

Training epoch-52 batch-109
Running loss of epoch-52 batch-109 = 2.3283064365386963e-06

Training epoch-52 batch-110
Running loss of epoch-52 batch-110 = 2.768123522400856e-06

Training epoch-52 batch-111
Running loss of epoch-52 batch-111 = 2.38628126680851e-06

Training epoch-52 batch-112
Running loss of epoch-52 batch-112 = 1.6693957149982452e-06

Training epoch-52 batch-113
Running loss of epoch-52 batch-113 = 2.4836044758558273e-06

Training epoch-52 batch-114
Running loss of epoch-52 batch-114 = 2.1837186068296432e-06

Training epoch-52 batch-115
Running loss of epoch-52 batch-115 = 2.5369226932525635e-06

Training epoch-52 batch-116
Running loss of epoch-52 batch-116 = 2.3923348635435104e-06

Training epoch-52 batch-117
Running loss of epoch-52 batch-117 = 3.577442839741707e-06

Training epoch-52 batch-118
Running loss of epoch-52 batch-118 = 1.5189871191978455e-06

Training epoch-52 batch-119
Running loss of epoch-52 batch-119 = 2.843094989657402e-06

Training epoch-52 batch-120
Running loss of epoch-52 batch-120 = 1.9797589629888535e-06

Training epoch-52 batch-121
Running loss of epoch-52 batch-121 = 2.430751919746399e-06

Training epoch-52 batch-122
Running loss of epoch-52 batch-122 = 2.19675712287426e-06

Training epoch-52 batch-123
Running loss of epoch-52 batch-123 = 2.5802291929721832e-06

Training epoch-52 batch-124
Running loss of epoch-52 batch-124 = 2.3136381059885025e-06

Training epoch-52 batch-125
Running loss of epoch-52 batch-125 = 3.191409632563591e-06

Training epoch-52 batch-126
Running loss of epoch-52 batch-126 = 2.641696482896805e-06

Training epoch-52 batch-127
Running loss of epoch-52 batch-127 = 2.332264557480812e-06

Training epoch-52 batch-128
Running loss of epoch-52 batch-128 = 4.132743924856186e-06

Training epoch-52 batch-129
Running loss of epoch-52 batch-129 = 2.613058313727379e-06

Training epoch-52 batch-130
Running loss of epoch-52 batch-130 = 2.0102597773075104e-06

Training epoch-52 batch-131
Running loss of epoch-52 batch-131 = 3.319932147860527e-06

Training epoch-52 batch-132
Running loss of epoch-52 batch-132 = 2.2316817194223404e-06

Training epoch-52 batch-133
Running loss of epoch-52 batch-133 = 1.5313271433115005e-06

Training epoch-52 batch-134
Running loss of epoch-52 batch-134 = 1.5897676348686218e-06

Training epoch-52 batch-135
Running loss of epoch-52 batch-135 = 2.039596438407898e-06

Training epoch-52 batch-136
Running loss of epoch-52 batch-136 = 1.9276048988103867e-06

Training epoch-52 batch-137
Running loss of epoch-52 batch-137 = 1.9061844795942307e-06

Training epoch-52 batch-138
Running loss of epoch-52 batch-138 = 2.6104971766471863e-06

Training epoch-52 batch-139
Running loss of epoch-52 batch-139 = 4.375120624899864e-06

Training epoch-52 batch-140
Running loss of epoch-52 batch-140 = 3.643566742539406e-06

Training epoch-52 batch-141
Running loss of epoch-52 batch-141 = 3.3811666071414948e-06

Training epoch-52 batch-142
Running loss of epoch-52 batch-142 = 1.9471626728773117e-06

Training epoch-52 batch-143
Running loss of epoch-52 batch-143 = 3.5264529287815094e-06

Training epoch-52 batch-144
Running loss of epoch-52 batch-144 = 1.9581057131290436e-06

Training epoch-52 batch-145
Running loss of epoch-52 batch-145 = 1.5704426914453506e-06

Training epoch-52 batch-146
Running loss of epoch-52 batch-146 = 2.1283049136400223e-06

Training epoch-52 batch-147
Running loss of epoch-52 batch-147 = 2.973247319459915e-06

Training epoch-52 batch-148
Running loss of epoch-52 batch-148 = 2.0980369299650192e-06

Training epoch-52 batch-149
Running loss of epoch-52 batch-149 = 2.237502485513687e-06

Training epoch-52 batch-150
Running loss of epoch-52 batch-150 = 1.7739366739988327e-06

Training epoch-52 batch-151
Running loss of epoch-52 batch-151 = 2.3641623556613922e-06

Training epoch-52 batch-152
Running loss of epoch-52 batch-152 = 2.445187419652939e-06

Training epoch-52 batch-153
Running loss of epoch-52 batch-153 = 2.369983121752739e-06

Training epoch-52 batch-154
Running loss of epoch-52 batch-154 = 2.577435225248337e-06

Training epoch-52 batch-155
Running loss of epoch-52 batch-155 = 2.6086345314979553e-06

Training epoch-52 batch-156
Running loss of epoch-52 batch-156 = 2.52528116106987e-06

Training epoch-52 batch-157
Running loss of epoch-52 batch-157 = 1.7933547496795654e-05

Finished training epoch-52.



Average train loss at epoch-52 = 2.722826600074768e-06

Started Evaluation

Average val loss at epoch-52 = 3.134044775837346

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.40 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 29.37 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-53


Training epoch-53 batch-1
Running loss of epoch-53 batch-1 = 2.3348256945610046e-06

Training epoch-53 batch-2
Running loss of epoch-53 batch-2 = 2.0684674382209778e-06

Training epoch-53 batch-3
Running loss of epoch-53 batch-3 = 2.7872156351804733e-06

Training epoch-53 batch-4
Running loss of epoch-53 batch-4 = 2.359505742788315e-06

Training epoch-53 batch-5
Running loss of epoch-53 batch-5 = 3.2533425837755203e-06

Training epoch-53 batch-6
Running loss of epoch-53 batch-6 = 3.98256815969944e-06

Training epoch-53 batch-7
Running loss of epoch-53 batch-7 = 2.7155037969350815e-06

Training epoch-53 batch-8
Running loss of epoch-53 batch-8 = 2.0174775272607803e-06

Training epoch-53 batch-9
Running loss of epoch-53 batch-9 = 2.5855842977762222e-06

Training epoch-53 batch-10
Running loss of epoch-53 batch-10 = 2.5224871933460236e-06

Training epoch-53 batch-11
Running loss of epoch-53 batch-11 = 2.7671921998262405e-06

Training epoch-53 batch-12
Running loss of epoch-53 batch-12 = 1.9727740436792374e-06

Training epoch-53 batch-13
Running loss of epoch-53 batch-13 = 2.3175962269306183e-06

Training epoch-53 batch-14
Running loss of epoch-53 batch-14 = 4.453817382454872e-06

Training epoch-53 batch-15
Running loss of epoch-53 batch-15 = 2.6419293135404587e-06

Training epoch-53 batch-16
Running loss of epoch-53 batch-16 = 2.180691808462143e-06

Training epoch-53 batch-17
Running loss of epoch-53 batch-17 = 3.913184627890587e-06

Training epoch-53 batch-18
Running loss of epoch-53 batch-18 = 1.7383135855197906e-06

Training epoch-53 batch-19
Running loss of epoch-53 batch-19 = 2.891290932893753e-06

Training epoch-53 batch-20
Running loss of epoch-53 batch-20 = 2.4347100406885147e-06

Training epoch-53 batch-21
Running loss of epoch-53 batch-21 = 2.928776666522026e-06

Training epoch-53 batch-22
Running loss of epoch-53 batch-22 = 3.4514814615249634e-06

Training epoch-53 batch-23
Running loss of epoch-53 batch-23 = 2.5529880076646805e-06

Training epoch-53 batch-24
Running loss of epoch-53 batch-24 = 2.2917520254850388e-06

Training epoch-53 batch-25
Running loss of epoch-53 batch-25 = 2.793269231915474e-06

Training epoch-53 batch-26
Running loss of epoch-53 batch-26 = 2.1120067685842514e-06

Training epoch-53 batch-27
Running loss of epoch-53 batch-27 = 2.201646566390991e-06

Training epoch-53 batch-28
Running loss of epoch-53 batch-28 = 2.71783210337162e-06

Training epoch-53 batch-29
Running loss of epoch-53 batch-29 = 3.68594191968441e-06

Training epoch-53 batch-30
Running loss of epoch-53 batch-30 = 2.8496142476797104e-06

Training epoch-53 batch-31
Running loss of epoch-53 batch-31 = 2.075452357530594e-06

Training epoch-53 batch-32
Running loss of epoch-53 batch-32 = 1.6668345779180527e-06

Training epoch-53 batch-33
Running loss of epoch-53 batch-33 = 3.0633527785539627e-06

Training epoch-53 batch-34
Running loss of epoch-53 batch-34 = 2.1080486476421356e-06

Training epoch-53 batch-35
Running loss of epoch-53 batch-35 = 2.9355287551879883e-06

Training epoch-53 batch-36
Running loss of epoch-53 batch-36 = 2.179061993956566e-06

Training epoch-53 batch-37
Running loss of epoch-53 batch-37 = 1.8863938748836517e-06

Training epoch-53 batch-38
Running loss of epoch-53 batch-38 = 1.912703737616539e-06

Training epoch-53 batch-39
Running loss of epoch-53 batch-39 = 2.6868656277656555e-06

Training epoch-53 batch-40
Running loss of epoch-53 batch-40 = 1.8673017621040344e-06

Training epoch-53 batch-41
Running loss of epoch-53 batch-41 = 4.280824214220047e-06

Training epoch-53 batch-42
Running loss of epoch-53 batch-42 = 3.351829946041107e-06

Training epoch-53 batch-43
Running loss of epoch-53 batch-43 = 3.8929283618927e-06

Training epoch-53 batch-44
Running loss of epoch-53 batch-44 = 3.22563573718071e-06

Training epoch-53 batch-45
Running loss of epoch-53 batch-45 = 3.507360816001892e-06

Training epoch-53 batch-46
Running loss of epoch-53 batch-46 = 3.0836090445518494e-06

Training epoch-53 batch-47
Running loss of epoch-53 batch-47 = 2.302229404449463e-06

Training epoch-53 batch-48
Running loss of epoch-53 batch-48 = 2.0423904061317444e-06

Training epoch-53 batch-49
Running loss of epoch-53 batch-49 = 2.955552190542221e-06

Training epoch-53 batch-50
Running loss of epoch-53 batch-50 = 2.7229543775320053e-06

Training epoch-53 batch-51
Running loss of epoch-53 batch-51 = 1.9292347133159637e-06

Training epoch-53 batch-52
Running loss of epoch-53 batch-52 = 2.087792381644249e-06

Training epoch-53 batch-53
Running loss of epoch-53 batch-53 = 1.875218003988266e-06

Training epoch-53 batch-54
Running loss of epoch-53 batch-54 = 1.7809215933084488e-06

Training epoch-53 batch-55
Running loss of epoch-53 batch-55 = 2.2766180336475372e-06

Training epoch-53 batch-56
Running loss of epoch-53 batch-56 = 2.3622997105121613e-06

Training epoch-53 batch-57
Running loss of epoch-53 batch-57 = 3.6517158150672913e-06

Training epoch-53 batch-58
Running loss of epoch-53 batch-58 = 2.658925950527191e-06

Training epoch-53 batch-59
Running loss of epoch-53 batch-59 = 2.7741771191358566e-06

Training epoch-53 batch-60
Running loss of epoch-53 batch-60 = 3.1241215765476227e-06

Training epoch-53 batch-61
Running loss of epoch-53 batch-61 = 2.0205043256282806e-06

Training epoch-53 batch-62
Running loss of epoch-53 batch-62 = 3.0260998755693436e-06

Training epoch-53 batch-63
Running loss of epoch-53 batch-63 = 1.1150259524583817e-06

Training epoch-53 batch-64
Running loss of epoch-53 batch-64 = 2.5955960154533386e-06

Training epoch-53 batch-65
Running loss of epoch-53 batch-65 = 3.0668452382087708e-06

Training epoch-53 batch-66
Running loss of epoch-53 batch-66 = 3.9620790630578995e-06

Training epoch-53 batch-67
Running loss of epoch-53 batch-67 = 1.8852297216653824e-06

Training epoch-53 batch-68
Running loss of epoch-53 batch-68 = 2.6293564587831497e-06

Training epoch-53 batch-69
Running loss of epoch-53 batch-69 = 4.1013117879629135e-06

Training epoch-53 batch-70
Running loss of epoch-53 batch-70 = 2.8677750378847122e-06

Training epoch-53 batch-71
Running loss of epoch-53 batch-71 = 3.7031713873147964e-06

Training epoch-53 batch-72
Running loss of epoch-53 batch-72 = 2.7420464903116226e-06

Training epoch-53 batch-73
Running loss of epoch-53 batch-73 = 2.3653265088796616e-06

Training epoch-53 batch-74
Running loss of epoch-53 batch-74 = 1.2258533388376236e-06

Training epoch-53 batch-75
Running loss of epoch-53 batch-75 = 2.7404166758060455e-06

Training epoch-53 batch-76
Running loss of epoch-53 batch-76 = 3.359513357281685e-06

Training epoch-53 batch-77
Running loss of epoch-53 batch-77 = 3.426801413297653e-06

Training epoch-53 batch-78
Running loss of epoch-53 batch-78 = 1.6673002392053604e-06

Training epoch-53 batch-79
Running loss of epoch-53 batch-79 = 1.7317943274974823e-06

Training epoch-53 batch-80
Running loss of epoch-53 batch-80 = 4.239147529006004e-06

Training epoch-53 batch-81
Running loss of epoch-53 batch-81 = 2.3599714040756226e-06

Training epoch-53 batch-82
Running loss of epoch-53 batch-82 = 2.1026935428380966e-06

Training epoch-53 batch-83
Running loss of epoch-53 batch-83 = 3.462890163064003e-06

Training epoch-53 batch-84
Running loss of epoch-53 batch-84 = 2.7082860469818115e-06

Training epoch-53 batch-85
Running loss of epoch-53 batch-85 = 2.6514753699302673e-06

Training epoch-53 batch-86
Running loss of epoch-53 batch-86 = 2.791406586766243e-06

Training epoch-53 batch-87
Running loss of epoch-53 batch-87 = 2.7476344257593155e-06

Training epoch-53 batch-88
Running loss of epoch-53 batch-88 = 2.398621290922165e-06

Training epoch-53 batch-89
Running loss of epoch-53 batch-89 = 3.5208649933338165e-06

Training epoch-53 batch-90
Running loss of epoch-53 batch-90 = 1.9869767129421234e-06

Training epoch-53 batch-91
Running loss of epoch-53 batch-91 = 2.991408109664917e-06

Training epoch-53 batch-92
Running loss of epoch-53 batch-92 = 1.9972212612628937e-06

Training epoch-53 batch-93
Running loss of epoch-53 batch-93 = 2.8507784008979797e-06

Training epoch-53 batch-94
Running loss of epoch-53 batch-94 = 1.7799902707338333e-06

Training epoch-53 batch-95
Running loss of epoch-53 batch-95 = 3.34298238158226e-06

Training epoch-53 batch-96
Running loss of epoch-53 batch-96 = 2.2489111870527267e-06

Training epoch-53 batch-97
Running loss of epoch-53 batch-97 = 2.8971116989851e-06

Training epoch-53 batch-98
Running loss of epoch-53 batch-98 = 2.507586032152176e-06

Training epoch-53 batch-99
Running loss of epoch-53 batch-99 = 2.855900675058365e-06

Training epoch-53 batch-100
Running loss of epoch-53 batch-100 = 2.121319994330406e-06

Training epoch-53 batch-101
Running loss of epoch-53 batch-101 = 2.817949280142784e-06

Training epoch-53 batch-102
Running loss of epoch-53 batch-102 = 2.623768523335457e-06

Training epoch-53 batch-103
Running loss of epoch-53 batch-103 = 2.37906351685524e-06

Training epoch-53 batch-104
Running loss of epoch-53 batch-104 = 1.7455313354730606e-06

Training epoch-53 batch-105
Running loss of epoch-53 batch-105 = 2.5436747819185257e-06

Training epoch-53 batch-106
Running loss of epoch-53 batch-106 = 2.4854671210050583e-06

Training epoch-53 batch-107
Running loss of epoch-53 batch-107 = 3.107590600848198e-06

Training epoch-53 batch-108
Running loss of epoch-53 batch-108 = 3.1241215765476227e-06

Training epoch-53 batch-109
Running loss of epoch-53 batch-109 = 2.273125573992729e-06

Training epoch-53 batch-110
Running loss of epoch-53 batch-110 = 2.4032779037952423e-06

Training epoch-53 batch-111
Running loss of epoch-53 batch-111 = 2.718297764658928e-06

Training epoch-53 batch-112
Running loss of epoch-53 batch-112 = 2.6063062250614166e-06

Training epoch-53 batch-113
Running loss of epoch-53 batch-113 = 2.5453045964241028e-06

Training epoch-53 batch-114
Running loss of epoch-53 batch-114 = 2.8812792152166367e-06

Training epoch-53 batch-115
Running loss of epoch-53 batch-115 = 2.541113644838333e-06

Training epoch-53 batch-116
Running loss of epoch-53 batch-116 = 1.8277205526828766e-06

Training epoch-53 batch-117
Running loss of epoch-53 batch-117 = 2.328772097826004e-06

Training epoch-53 batch-118
Running loss of epoch-53 batch-118 = 2.305256202816963e-06

Training epoch-53 batch-119
Running loss of epoch-53 batch-119 = 2.7278438210487366e-06

Training epoch-53 batch-120
Running loss of epoch-53 batch-120 = 1.8782448023557663e-06

Training epoch-53 batch-121
Running loss of epoch-53 batch-121 = 2.9013026505708694e-06

Training epoch-53 batch-122
Running loss of epoch-53 batch-122 = 2.6032794266939163e-06

Training epoch-53 batch-123
Running loss of epoch-53 batch-123 = 2.484070137143135e-06

Training epoch-53 batch-124
Running loss of epoch-53 batch-124 = 3.0456576496362686e-06

Training epoch-53 batch-125
Running loss of epoch-53 batch-125 = 2.0083971321582794e-06

Training epoch-53 batch-126
Running loss of epoch-53 batch-126 = 2.487562596797943e-06

Training epoch-53 batch-127
Running loss of epoch-53 batch-127 = 2.766028046607971e-06

Training epoch-53 batch-128
Running loss of epoch-53 batch-128 = 1.819804310798645e-06

Training epoch-53 batch-129
Running loss of epoch-53 batch-129 = 2.5120098143815994e-06

Training epoch-53 batch-130
Running loss of epoch-53 batch-130 = 1.8784776329994202e-06

Training epoch-53 batch-131
Running loss of epoch-53 batch-131 = 1.9727740436792374e-06

Training epoch-53 batch-132
Running loss of epoch-53 batch-132 = 3.6212150007486343e-06

Training epoch-53 batch-133
Running loss of epoch-53 batch-133 = 2.2996682673692703e-06

Training epoch-53 batch-134
Running loss of epoch-53 batch-134 = 1.1390075087547302e-06

Training epoch-53 batch-135
Running loss of epoch-53 batch-135 = 3.327149897813797e-06

Training epoch-53 batch-136
Running loss of epoch-53 batch-136 = 2.1546147763729095e-06

Training epoch-53 batch-137
Running loss of epoch-53 batch-137 = 2.2707972675561905e-06

Training epoch-53 batch-138
Running loss of epoch-53 batch-138 = 2.445187419652939e-06

Training epoch-53 batch-139
Running loss of epoch-53 batch-139 = 1.9907020032405853e-06

Training epoch-53 batch-140
Running loss of epoch-53 batch-140 = 2.4333130568265915e-06

Training epoch-53 batch-141
Running loss of epoch-53 batch-141 = 3.307126462459564e-06

Training epoch-53 batch-142
Running loss of epoch-53 batch-142 = 4.373257979750633e-06

Training epoch-53 batch-143
Running loss of epoch-53 batch-143 = 2.3064203560352325e-06

Training epoch-53 batch-144
Running loss of epoch-53 batch-144 = 2.1934974938631058e-06

Training epoch-53 batch-145
Running loss of epoch-53 batch-145 = 2.5623012334108353e-06

Training epoch-53 batch-146
Running loss of epoch-53 batch-146 = 2.50060111284256e-06

Training epoch-53 batch-147
Running loss of epoch-53 batch-147 = 2.753688022494316e-06

Training epoch-53 batch-148
Running loss of epoch-53 batch-148 = 2.8565991669893265e-06

Training epoch-53 batch-149
Running loss of epoch-53 batch-149 = 3.0009541660547256e-06

Training epoch-53 batch-150
Running loss of epoch-53 batch-150 = 2.2584572434425354e-06

Training epoch-53 batch-151
Running loss of epoch-53 batch-151 = 2.4386681616306305e-06

Training epoch-53 batch-152
Running loss of epoch-53 batch-152 = 3.1439121812582016e-06

Training epoch-53 batch-153
Running loss of epoch-53 batch-153 = 2.9846560209989548e-06

Training epoch-53 batch-154
Running loss of epoch-53 batch-154 = 2.830289304256439e-06

Training epoch-53 batch-155
Running loss of epoch-53 batch-155 = 2.112472429871559e-06

Training epoch-53 batch-156
Running loss of epoch-53 batch-156 = 4.15463000535965e-06

Training epoch-53 batch-157
Running loss of epoch-53 batch-157 = 8.944422006607056e-06

Finished training epoch-53.



Average train loss at epoch-53 = 2.6408016681671144e-06

Started Evaluation

Average val loss at epoch-53 = 3.133368234885366

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.36 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 30.72 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-54


Training epoch-54 batch-1
Running loss of epoch-54 batch-1 = 3.030756488442421e-06

Training epoch-54 batch-2
Running loss of epoch-54 batch-2 = 2.43261456489563e-06

Training epoch-54 batch-3
Running loss of epoch-54 batch-3 = 1.7513521015644073e-06

Training epoch-54 batch-4
Running loss of epoch-54 batch-4 = 2.823770046234131e-06

Training epoch-54 batch-5
Running loss of epoch-54 batch-5 = 2.137618139386177e-06

Training epoch-54 batch-6
Running loss of epoch-54 batch-6 = 2.771615982055664e-06

Training epoch-54 batch-7
Running loss of epoch-54 batch-7 = 1.9508879631757736e-06

Training epoch-54 batch-8
Running loss of epoch-54 batch-8 = 2.0673032850027084e-06

Training epoch-54 batch-9
Running loss of epoch-54 batch-9 = 2.6009511202573776e-06

Training epoch-54 batch-10
Running loss of epoch-54 batch-10 = 2.3634638637304306e-06

Training epoch-54 batch-11
Running loss of epoch-54 batch-11 = 2.064276486635208e-06

Training epoch-54 batch-12
Running loss of epoch-54 batch-12 = 3.3497344702482224e-06

Training epoch-54 batch-13
Running loss of epoch-54 batch-13 = 2.7904752641916275e-06

Training epoch-54 batch-14
Running loss of epoch-54 batch-14 = 2.7618370950222015e-06

Training epoch-54 batch-15
Running loss of epoch-54 batch-15 = 2.4258624762296677e-06

Training epoch-54 batch-16
Running loss of epoch-54 batch-16 = 1.7546117305755615e-06

Training epoch-54 batch-17
Running loss of epoch-54 batch-17 = 2.820044755935669e-06

Training epoch-54 batch-18
Running loss of epoch-54 batch-18 = 1.7597340047359467e-06

Training epoch-54 batch-19
Running loss of epoch-54 batch-19 = 1.578591763973236e-06

Training epoch-54 batch-20
Running loss of epoch-54 batch-20 = 2.7818605303764343e-06

Training epoch-54 batch-21
Running loss of epoch-54 batch-21 = 2.582557499408722e-06

Training epoch-54 batch-22
Running loss of epoch-54 batch-22 = 2.254033461213112e-06

Training epoch-54 batch-23
Running loss of epoch-54 batch-23 = 2.809334546327591e-06

Training epoch-54 batch-24
Running loss of epoch-54 batch-24 = 3.3939722925424576e-06

Training epoch-54 batch-25
Running loss of epoch-54 batch-25 = 2.869870513677597e-06

Training epoch-54 batch-26
Running loss of epoch-54 batch-26 = 2.3494940251111984e-06

Training epoch-54 batch-27
Running loss of epoch-54 batch-27 = 2.08243727684021e-06

Training epoch-54 batch-28
Running loss of epoch-54 batch-28 = 2.1543819457292557e-06

Training epoch-54 batch-29
Running loss of epoch-54 batch-29 = 2.868007868528366e-06

Training epoch-54 batch-30
Running loss of epoch-54 batch-30 = 2.296408638358116e-06

Training epoch-54 batch-31
Running loss of epoch-54 batch-31 = 1.8705613911151886e-06

Training epoch-54 batch-32
Running loss of epoch-54 batch-32 = 2.040993422269821e-06

Training epoch-54 batch-33
Running loss of epoch-54 batch-33 = 2.3390166461467743e-06

Training epoch-54 batch-34
Running loss of epoch-54 batch-34 = 2.4260953068733215e-06

Training epoch-54 batch-35
Running loss of epoch-54 batch-35 = 2.943212166428566e-06

Training epoch-54 batch-36
Running loss of epoch-54 batch-36 = 2.4943146854639053e-06

Training epoch-54 batch-37
Running loss of epoch-54 batch-37 = 2.134358510375023e-06

Training epoch-54 batch-38
Running loss of epoch-54 batch-38 = 2.009328454732895e-06

Training epoch-54 batch-39
Running loss of epoch-54 batch-39 = 3.3569522202014923e-06

Training epoch-54 batch-40
Running loss of epoch-54 batch-40 = 3.107590600848198e-06

Training epoch-54 batch-41
Running loss of epoch-54 batch-41 = 2.4926848709583282e-06

Training epoch-54 batch-42
Running loss of epoch-54 batch-42 = 2.202345058321953e-06

Training epoch-54 batch-43
Running loss of epoch-54 batch-43 = 2.5546178221702576e-06

Training epoch-54 batch-44
Running loss of epoch-54 batch-44 = 1.8631108105182648e-06

Training epoch-54 batch-45
Running loss of epoch-54 batch-45 = 1.3916287571191788e-06

Training epoch-54 batch-46
Running loss of epoch-54 batch-46 = 3.2549723982810974e-06

Training epoch-54 batch-47
Running loss of epoch-54 batch-47 = 2.296408638358116e-06

Training epoch-54 batch-48
Running loss of epoch-54 batch-48 = 2.623302862048149e-06

Training epoch-54 batch-49
Running loss of epoch-54 batch-49 = 2.4512410163879395e-06

Training epoch-54 batch-50
Running loss of epoch-54 batch-50 = 2.9115471988916397e-06

Training epoch-54 batch-51
Running loss of epoch-54 batch-51 = 3.678957000374794e-06

Training epoch-54 batch-52
Running loss of epoch-54 batch-52 = 2.4884939193725586e-06

Training epoch-54 batch-53
Running loss of epoch-54 batch-53 = 1.6149133443832397e-06

Training epoch-54 batch-54
Running loss of epoch-54 batch-54 = 2.0565930753946304e-06

Training epoch-54 batch-55
Running loss of epoch-54 batch-55 = 3.0708033591508865e-06

Training epoch-54 batch-56
Running loss of epoch-54 batch-56 = 2.3616012185811996e-06

Training epoch-54 batch-57
Running loss of epoch-54 batch-57 = 2.325279638171196e-06

Training epoch-54 batch-58
Running loss of epoch-54 batch-58 = 2.6924535632133484e-06

Training epoch-54 batch-59
Running loss of epoch-54 batch-59 = 2.628890797495842e-06

Training epoch-54 batch-60
Running loss of epoch-54 batch-60 = 2.263113856315613e-06

Training epoch-54 batch-61
Running loss of epoch-54 batch-61 = 2.4389009922742844e-06

Training epoch-54 batch-62
Running loss of epoch-54 batch-62 = 2.4472828954458237e-06

Training epoch-54 batch-63
Running loss of epoch-54 batch-63 = 1.7315614968538284e-06

Training epoch-54 batch-64
Running loss of epoch-54 batch-64 = 1.7837155610322952e-06

Training epoch-54 batch-65
Running loss of epoch-54 batch-65 = 1.57160684466362e-06

Training epoch-54 batch-66
Running loss of epoch-54 batch-66 = 2.586282789707184e-06

Training epoch-54 batch-67
Running loss of epoch-54 batch-67 = 1.7790589481592178e-06

Training epoch-54 batch-68
Running loss of epoch-54 batch-68 = 2.3597385734319687e-06

Training epoch-54 batch-69
Running loss of epoch-54 batch-69 = 1.7494894564151764e-06

Training epoch-54 batch-70
Running loss of epoch-54 batch-70 = 2.934597432613373e-06

Training epoch-54 batch-71
Running loss of epoch-54 batch-71 = 2.6528723537921906e-06

Training epoch-54 batch-72
Running loss of epoch-54 batch-72 = 2.0407605916261673e-06

Training epoch-54 batch-73
Running loss of epoch-54 batch-73 = 2.580927684903145e-06

Training epoch-54 batch-74
Running loss of epoch-54 batch-74 = 2.9187649488449097e-06

Training epoch-54 batch-75
Running loss of epoch-54 batch-75 = 3.055436536669731e-06

Training epoch-54 batch-76
Running loss of epoch-54 batch-76 = 2.6316847652196884e-06

Training epoch-54 batch-77
Running loss of epoch-54 batch-77 = 2.300599589943886e-06

Training epoch-54 batch-78
Running loss of epoch-54 batch-78 = 2.811197191476822e-06

Training epoch-54 batch-79
Running loss of epoch-54 batch-79 = 1.9418075680732727e-06

Training epoch-54 batch-80
Running loss of epoch-54 batch-80 = 1.5723053365945816e-06

Training epoch-54 batch-81
Running loss of epoch-54 batch-81 = 2.971617504954338e-06

Training epoch-54 batch-82
Running loss of epoch-54 batch-82 = 1.980923116207123e-06

Training epoch-54 batch-83
Running loss of epoch-54 batch-83 = 2.2042077034711838e-06

Training epoch-54 batch-84
Running loss of epoch-54 batch-84 = 3.0247028917074203e-06

Training epoch-54 batch-85
Running loss of epoch-54 batch-85 = 2.436107024550438e-06

Training epoch-54 batch-86
Running loss of epoch-54 batch-86 = 3.3222604542970657e-06

Training epoch-54 batch-87
Running loss of epoch-54 batch-87 = 2.164393663406372e-06

Training epoch-54 batch-88
Running loss of epoch-54 batch-88 = 2.385815605521202e-06

Training epoch-54 batch-89
Running loss of epoch-54 batch-89 = 2.7706846594810486e-06

Training epoch-54 batch-90
Running loss of epoch-54 batch-90 = 2.8584618121385574e-06

Training epoch-54 batch-91
Running loss of epoch-54 batch-91 = 2.9436778277158737e-06

Training epoch-54 batch-92
Running loss of epoch-54 batch-92 = 2.5529880076646805e-06

Training epoch-54 batch-93
Running loss of epoch-54 batch-93 = 2.023298293352127e-06

Training epoch-54 batch-94
Running loss of epoch-54 batch-94 = 2.2435560822486877e-06

Training epoch-54 batch-95
Running loss of epoch-54 batch-95 = 2.352055162191391e-06

Training epoch-54 batch-96
Running loss of epoch-54 batch-96 = 4.081055521965027e-06

Training epoch-54 batch-97
Running loss of epoch-54 batch-97 = 2.725282683968544e-06

Training epoch-54 batch-98
Running loss of epoch-54 batch-98 = 3.368593752384186e-06

Training epoch-54 batch-99
Running loss of epoch-54 batch-99 = 3.0105002224445343e-06

Training epoch-54 batch-100
Running loss of epoch-54 batch-100 = 1.916196197271347e-06

Training epoch-54 batch-101
Running loss of epoch-54 batch-101 = 2.544606104493141e-06

Training epoch-54 batch-102
Running loss of epoch-54 batch-102 = 3.5956036299467087e-06

Training epoch-54 batch-103
Running loss of epoch-54 batch-103 = 1.8673017621040344e-06

Training epoch-54 batch-104
Running loss of epoch-54 batch-104 = 1.4887191355228424e-06

Training epoch-54 batch-105
Running loss of epoch-54 batch-105 = 4.269415512681007e-06

Training epoch-54 batch-106
Running loss of epoch-54 batch-106 = 1.7827842384576797e-06

Training epoch-54 batch-107
Running loss of epoch-54 batch-107 = 2.4123582988977432e-06

Training epoch-54 batch-108
Running loss of epoch-54 batch-108 = 3.2603275030851364e-06

Training epoch-54 batch-109
Running loss of epoch-54 batch-109 = 3.2815150916576385e-06

Training epoch-54 batch-110
Running loss of epoch-54 batch-110 = 2.0153820514678955e-06

Training epoch-54 batch-111
Running loss of epoch-54 batch-111 = 2.0810402929782867e-06

Training epoch-54 batch-112
Running loss of epoch-54 batch-112 = 2.2943131625652313e-06

Training epoch-54 batch-113
Running loss of epoch-54 batch-113 = 2.627260982990265e-06

Training epoch-54 batch-114
Running loss of epoch-54 batch-114 = 1.964624971151352e-06

Training epoch-54 batch-115
Running loss of epoch-54 batch-115 = 2.491287887096405e-06

Training epoch-54 batch-116
Running loss of epoch-54 batch-116 = 3.3620744943618774e-06

Training epoch-54 batch-117
Running loss of epoch-54 batch-117 = 3.4195836633443832e-06

Training epoch-54 batch-118
Running loss of epoch-54 batch-118 = 2.3953616619110107e-06

Training epoch-54 batch-119
Running loss of epoch-54 batch-119 = 2.725515514612198e-06

Training epoch-54 batch-120
Running loss of epoch-54 batch-120 = 2.712476998567581e-06

Training epoch-54 batch-121
Running loss of epoch-54 batch-121 = 3.2992102205753326e-06

Training epoch-54 batch-122
Running loss of epoch-54 batch-122 = 2.1436717361211777e-06

Training epoch-54 batch-123
Running loss of epoch-54 batch-123 = 2.0943116396665573e-06

Training epoch-54 batch-124
Running loss of epoch-54 batch-124 = 2.51084566116333e-06

Training epoch-54 batch-125
Running loss of epoch-54 batch-125 = 2.3292377591133118e-06

Training epoch-54 batch-126
Running loss of epoch-54 batch-126 = 2.707354724407196e-06

Training epoch-54 batch-127
Running loss of epoch-54 batch-127 = 2.514338120818138e-06

Training epoch-54 batch-128
Running loss of epoch-54 batch-128 = 1.9567087292671204e-06

Training epoch-54 batch-129
Running loss of epoch-54 batch-129 = 3.876863047480583e-06

Training epoch-54 batch-130
Running loss of epoch-54 batch-130 = 2.4491455405950546e-06

Training epoch-54 batch-131
Running loss of epoch-54 batch-131 = 2.2782478481531143e-06

Training epoch-54 batch-132
Running loss of epoch-54 batch-132 = 2.9085204005241394e-06

Training epoch-54 batch-133
Running loss of epoch-54 batch-133 = 3.296416252851486e-06

Training epoch-54 batch-134
Running loss of epoch-54 batch-134 = 2.6819761842489243e-06

Training epoch-54 batch-135
Running loss of epoch-54 batch-135 = 3.2293610274791718e-06

Training epoch-54 batch-136
Running loss of epoch-54 batch-136 = 2.9955990612506866e-06

Training epoch-54 batch-137
Running loss of epoch-54 batch-137 = 3.138091415166855e-06

Training epoch-54 batch-138
Running loss of epoch-54 batch-138 = 2.628657966852188e-06

Training epoch-54 batch-139
Running loss of epoch-54 batch-139 = 3.1527597457170486e-06

Training epoch-54 batch-140
Running loss of epoch-54 batch-140 = 2.461019903421402e-06

Training epoch-54 batch-141
Running loss of epoch-54 batch-141 = 4.1532330214977264e-06

Training epoch-54 batch-142
Running loss of epoch-54 batch-142 = 2.220040187239647e-06

Training epoch-54 batch-143
Running loss of epoch-54 batch-143 = 2.273591235280037e-06

Training epoch-54 batch-144
Running loss of epoch-54 batch-144 = 2.209329977631569e-06

Training epoch-54 batch-145
Running loss of epoch-54 batch-145 = 2.5832559913396835e-06

Training epoch-54 batch-146
Running loss of epoch-54 batch-146 = 3.479188308119774e-06

Training epoch-54 batch-147
Running loss of epoch-54 batch-147 = 2.3292377591133118e-06

Training epoch-54 batch-148
Running loss of epoch-54 batch-148 = 2.886168658733368e-06

Training epoch-54 batch-149
Running loss of epoch-54 batch-149 = 2.4188775569200516e-06

Training epoch-54 batch-150
Running loss of epoch-54 batch-150 = 3.473367542028427e-06

Training epoch-54 batch-151
Running loss of epoch-54 batch-151 = 4.44287434220314e-06

Training epoch-54 batch-152
Running loss of epoch-54 batch-152 = 2.528773620724678e-06

Training epoch-54 batch-153
Running loss of epoch-54 batch-153 = 2.173474058508873e-06

Training epoch-54 batch-154
Running loss of epoch-54 batch-154 = 1.9799917936325073e-06

Training epoch-54 batch-155
Running loss of epoch-54 batch-155 = 3.8726720958948135e-06

Training epoch-54 batch-156
Running loss of epoch-54 batch-156 = 2.5331974029541016e-06

Training epoch-54 batch-157
Running loss of epoch-54 batch-157 = 8.56444239616394e-06

Finished training epoch-54.



Average train loss at epoch-54 = 2.5714591145515442e-06

Started Evaluation

Average val loss at epoch-54 = 3.143162272478405

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.40 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.43 %

Finished Evaluation



Started training epoch-55


Training epoch-55 batch-1
Running loss of epoch-55 batch-1 = 3.1834933906793594e-06

Training epoch-55 batch-2
Running loss of epoch-55 batch-2 = 3.1061936169862747e-06

Training epoch-55 batch-3
Running loss of epoch-55 batch-3 = 2.7837231755256653e-06

Training epoch-55 batch-4
Running loss of epoch-55 batch-4 = 2.853572368621826e-06

Training epoch-55 batch-5
Running loss of epoch-55 batch-5 = 2.3632310330867767e-06

Training epoch-55 batch-6
Running loss of epoch-55 batch-6 = 1.8952414393424988e-06

Training epoch-55 batch-7
Running loss of epoch-55 batch-7 = 1.7022248357534409e-06

Training epoch-55 batch-8
Running loss of epoch-55 batch-8 = 2.562766894698143e-06

Training epoch-55 batch-9
Running loss of epoch-55 batch-9 = 2.1900050342082977e-06

Training epoch-55 batch-10
Running loss of epoch-55 batch-10 = 2.762535586953163e-06

Training epoch-55 batch-11
Running loss of epoch-55 batch-11 = 2.0852312445640564e-06

Training epoch-55 batch-12
Running loss of epoch-55 batch-12 = 3.0247028917074203e-06

Training epoch-55 batch-13
Running loss of epoch-55 batch-13 = 2.7425121515989304e-06

Training epoch-55 batch-14
Running loss of epoch-55 batch-14 = 2.2167805582284927e-06

Training epoch-55 batch-15
Running loss of epoch-55 batch-15 = 1.7471611499786377e-06

Training epoch-55 batch-16
Running loss of epoch-55 batch-16 = 1.8763821572065353e-06

Training epoch-55 batch-17
Running loss of epoch-55 batch-17 = 2.3208558559417725e-06

Training epoch-55 batch-18
Running loss of epoch-55 batch-18 = 2.2228341549634933e-06

Training epoch-55 batch-19
Running loss of epoch-55 batch-19 = 1.3958197087049484e-06

Training epoch-55 batch-20
Running loss of epoch-55 batch-20 = 1.6544945538043976e-06

Training epoch-55 batch-21
Running loss of epoch-55 batch-21 = 2.664513885974884e-06

Training epoch-55 batch-22
Running loss of epoch-55 batch-22 = 2.4258624762296677e-06

Training epoch-55 batch-23
Running loss of epoch-55 batch-23 = 3.0410010367631912e-06

Training epoch-55 batch-24
Running loss of epoch-55 batch-24 = 2.2922176867723465e-06

Training epoch-55 batch-25
Running loss of epoch-55 batch-25 = 2.0905863493680954e-06

Training epoch-55 batch-26
Running loss of epoch-55 batch-26 = 2.0293518900871277e-06

Training epoch-55 batch-27
Running loss of epoch-55 batch-27 = 3.0174851417541504e-06

Training epoch-55 batch-28
Running loss of epoch-55 batch-28 = 3.032619133591652e-06

Training epoch-55 batch-29
Running loss of epoch-55 batch-29 = 2.8228387236595154e-06

Training epoch-55 batch-30
Running loss of epoch-55 batch-30 = 4.561617970466614e-06

Training epoch-55 batch-31
Running loss of epoch-55 batch-31 = 2.6838388293981552e-06

Training epoch-55 batch-32
Running loss of epoch-55 batch-32 = 2.19675712287426e-06

Training epoch-55 batch-33
Running loss of epoch-55 batch-33 = 2.1723099052906036e-06

Training epoch-55 batch-34
Running loss of epoch-55 batch-34 = 1.6121193766593933e-06

Training epoch-55 batch-35
Running loss of epoch-55 batch-35 = 1.9017606973648071e-06

Training epoch-55 batch-36
Running loss of epoch-55 batch-36 = 2.3990869522094727e-06

Training epoch-55 batch-37
Running loss of epoch-55 batch-37 = 7.906928658485413e-07

Training epoch-55 batch-38
Running loss of epoch-55 batch-38 = 1.9634608179330826e-06

Training epoch-55 batch-39
Running loss of epoch-55 batch-39 = 1.926673576235771e-06

Training epoch-55 batch-40
Running loss of epoch-55 batch-40 = 2.396991476416588e-06

Training epoch-55 batch-41
Running loss of epoch-55 batch-41 = 2.2100284695625305e-06

Training epoch-55 batch-42
Running loss of epoch-55 batch-42 = 3.169756382703781e-06

Training epoch-55 batch-43
Running loss of epoch-55 batch-43 = 2.3131724447011948e-06

Training epoch-55 batch-44
Running loss of epoch-55 batch-44 = 4.03192825615406e-06

Training epoch-55 batch-45
Running loss of epoch-55 batch-45 = 3.8887374103069305e-06

Training epoch-55 batch-46
Running loss of epoch-55 batch-46 = 2.0354054868221283e-06

Training epoch-55 batch-47
Running loss of epoch-55 batch-47 = 2.1567102521657944e-06

Training epoch-55 batch-48
Running loss of epoch-55 batch-48 = 3.146473318338394e-06

Training epoch-55 batch-49
Running loss of epoch-55 batch-49 = 3.323424607515335e-06

Training epoch-55 batch-50
Running loss of epoch-55 batch-50 = 2.217944711446762e-06

Training epoch-55 batch-51
Running loss of epoch-55 batch-51 = 2.7692876756191254e-06

Training epoch-55 batch-52
Running loss of epoch-55 batch-52 = 2.2549647837877274e-06

Training epoch-55 batch-53
Running loss of epoch-55 batch-53 = 2.2926833480596542e-06

Training epoch-55 batch-54
Running loss of epoch-55 batch-54 = 2.3546162992715836e-06

Training epoch-55 batch-55
Running loss of epoch-55 batch-55 = 2.2333115339279175e-06

Training epoch-55 batch-56
Running loss of epoch-55 batch-56 = 2.7588102966547012e-06

Training epoch-55 batch-57
Running loss of epoch-55 batch-57 = 2.7730129659175873e-06

Training epoch-55 batch-58
Running loss of epoch-55 batch-58 = 2.125510945916176e-06

Training epoch-55 batch-59
Running loss of epoch-55 batch-59 = 2.200482413172722e-06

Training epoch-55 batch-60
Running loss of epoch-55 batch-60 = 2.1762680262327194e-06

Training epoch-55 batch-61
Running loss of epoch-55 batch-61 = 2.5962945073843002e-06

Training epoch-55 batch-62
Running loss of epoch-55 batch-62 = 2.444721758365631e-06

Training epoch-55 batch-63
Running loss of epoch-55 batch-63 = 3.391643986105919e-06

Training epoch-55 batch-64
Running loss of epoch-55 batch-64 = 2.4405308067798615e-06

Training epoch-55 batch-65
Running loss of epoch-55 batch-65 = 1.9478611648082733e-06

Training epoch-55 batch-66
Running loss of epoch-55 batch-66 = 2.2638123482465744e-06

Training epoch-55 batch-67
Running loss of epoch-55 batch-67 = 3.15043143928051e-06

Training epoch-55 batch-68
Running loss of epoch-55 batch-68 = 2.349959686398506e-06

Training epoch-55 batch-69
Running loss of epoch-55 batch-69 = 3.020279109477997e-06

Training epoch-55 batch-70
Running loss of epoch-55 batch-70 = 1.8326099961996078e-06

Training epoch-55 batch-71
Running loss of epoch-55 batch-71 = 2.7942005544900894e-06

Training epoch-55 batch-72
Running loss of epoch-55 batch-72 = 3.739958629012108e-06

Training epoch-55 batch-73
Running loss of epoch-55 batch-73 = 1.6363337635993958e-06

Training epoch-55 batch-74
Running loss of epoch-55 batch-74 = 2.0673032850027084e-06

Training epoch-55 batch-75
Running loss of epoch-55 batch-75 = 2.7727801352739334e-06

Training epoch-55 batch-76
Running loss of epoch-55 batch-76 = 2.4971086531877518e-06

Training epoch-55 batch-77
Running loss of epoch-55 batch-77 = 2.4531036615371704e-06

Training epoch-55 batch-78
Running loss of epoch-55 batch-78 = 1.6586855053901672e-06

Training epoch-55 batch-79
Running loss of epoch-55 batch-79 = 2.5099143385887146e-06

Training epoch-55 batch-80
Running loss of epoch-55 batch-80 = 2.9618386179208755e-06

Training epoch-55 batch-81
Running loss of epoch-55 batch-81 = 2.5120098143815994e-06

Training epoch-55 batch-82
Running loss of epoch-55 batch-82 = 2.8656795620918274e-06

Training epoch-55 batch-83
Running loss of epoch-55 batch-83 = 3.3171381801366806e-06

Training epoch-55 batch-84
Running loss of epoch-55 batch-84 = 2.309447154402733e-06

Training epoch-55 batch-85
Running loss of epoch-55 batch-85 = 2.093380317091942e-06

Training epoch-55 batch-86
Running loss of epoch-55 batch-86 = 1.8686987459659576e-06

Training epoch-55 batch-87
Running loss of epoch-55 batch-87 = 2.868473529815674e-06

Training epoch-55 batch-88
Running loss of epoch-55 batch-88 = 2.78279185295105e-06

Training epoch-55 batch-89
Running loss of epoch-55 batch-89 = 2.8943177312612534e-06

Training epoch-55 batch-90
Running loss of epoch-55 batch-90 = 1.9979197531938553e-06

Training epoch-55 batch-91
Running loss of epoch-55 batch-91 = 1.8987338989973068e-06

Training epoch-55 batch-92
Running loss of epoch-55 batch-92 = 2.7283094823360443e-06

Training epoch-55 batch-93
Running loss of epoch-55 batch-93 = 2.427026629447937e-06

Training epoch-55 batch-94
Running loss of epoch-55 batch-94 = 1.614447683095932e-06

Training epoch-55 batch-95
Running loss of epoch-55 batch-95 = 2.1795276552438736e-06

Training epoch-55 batch-96
Running loss of epoch-55 batch-96 = 2.6817433536052704e-06

Training epoch-55 batch-97
Running loss of epoch-55 batch-97 = 1.51805579662323e-06

Training epoch-55 batch-98
Running loss of epoch-55 batch-98 = 4.479428753256798e-06

Training epoch-55 batch-99
Running loss of epoch-55 batch-99 = 2.351822331547737e-06

Training epoch-55 batch-100
Running loss of epoch-55 batch-100 = 2.276850864291191e-06

Training epoch-55 batch-101
Running loss of epoch-55 batch-101 = 2.4444889277219772e-06

Training epoch-55 batch-102
Running loss of epoch-55 batch-102 = 2.4016480892896652e-06

Training epoch-55 batch-103
Running loss of epoch-55 batch-103 = 1.7648562788963318e-06

Training epoch-55 batch-104
Running loss of epoch-55 batch-104 = 3.0614901334047318e-06

Training epoch-55 batch-105
Running loss of epoch-55 batch-105 = 2.6477500796318054e-06

Training epoch-55 batch-106
Running loss of epoch-55 batch-106 = 4.069879651069641e-06

Training epoch-55 batch-107
Running loss of epoch-55 batch-107 = 2.7797650545835495e-06

Training epoch-55 batch-108
Running loss of epoch-55 batch-108 = 1.9944272935390472e-06

Training epoch-55 batch-109
Running loss of epoch-55 batch-109 = 2.2687017917633057e-06

Training epoch-55 batch-110
Running loss of epoch-55 batch-110 = 2.437736839056015e-06

Training epoch-55 batch-111
Running loss of epoch-55 batch-111 = 3.3765099942684174e-06

Training epoch-55 batch-112
Running loss of epoch-55 batch-112 = 1.9099097698926926e-06

Training epoch-55 batch-113
Running loss of epoch-55 batch-113 = 3.6712735891342163e-06

Training epoch-55 batch-114
Running loss of epoch-55 batch-114 = 1.6551930457353592e-06

Training epoch-55 batch-115
Running loss of epoch-55 batch-115 = 2.17510387301445e-06

Training epoch-55 batch-116
Running loss of epoch-55 batch-116 = 2.2365711629390717e-06

Training epoch-55 batch-117
Running loss of epoch-55 batch-117 = 2.6212073862552643e-06

Training epoch-55 batch-118
Running loss of epoch-55 batch-118 = 2.2011809051036835e-06

Training epoch-55 batch-119
Running loss of epoch-55 batch-119 = 2.7010682970285416e-06

Training epoch-55 batch-120
Running loss of epoch-55 batch-120 = 2.4174805730581284e-06

Training epoch-55 batch-121
Running loss of epoch-55 batch-121 = 2.9562506824731827e-06

Training epoch-55 batch-122
Running loss of epoch-55 batch-122 = 2.434477210044861e-06

Training epoch-55 batch-123
Running loss of epoch-55 batch-123 = 2.4689361453056335e-06

Training epoch-55 batch-124
Running loss of epoch-55 batch-124 = 2.5525223463773727e-06

Training epoch-55 batch-125
Running loss of epoch-55 batch-125 = 1.9569415599107742e-06

Training epoch-55 batch-126
Running loss of epoch-55 batch-126 = 1.7285346984863281e-06

Training epoch-55 batch-127
Running loss of epoch-55 batch-127 = 1.7718411982059479e-06

Training epoch-55 batch-128
Running loss of epoch-55 batch-128 = 2.0458828657865524e-06

Training epoch-55 batch-129
Running loss of epoch-55 batch-129 = 2.9997900128364563e-06

Training epoch-55 batch-130
Running loss of epoch-55 batch-130 = 1.8165446817874908e-06

Training epoch-55 batch-131
Running loss of epoch-55 batch-131 = 2.4726614356040955e-06

Training epoch-55 batch-132
Running loss of epoch-55 batch-132 = 2.619577571749687e-06

Training epoch-55 batch-133
Running loss of epoch-55 batch-133 = 2.6444904506206512e-06

Training epoch-55 batch-134
Running loss of epoch-55 batch-134 = 3.0635856091976166e-06

Training epoch-55 batch-135
Running loss of epoch-55 batch-135 = 2.3669563233852386e-06

Training epoch-55 batch-136
Running loss of epoch-55 batch-136 = 3.0230730772018433e-06

Training epoch-55 batch-137
Running loss of epoch-55 batch-137 = 3.817025572061539e-06

Training epoch-55 batch-138
Running loss of epoch-55 batch-138 = 2.1636951714754105e-06

Training epoch-55 batch-139
Running loss of epoch-55 batch-139 = 1.8212012946605682e-06

Training epoch-55 batch-140
Running loss of epoch-55 batch-140 = 3.0205119401216507e-06

Training epoch-55 batch-141
Running loss of epoch-55 batch-141 = 3.0384398996829987e-06

Training epoch-55 batch-142
Running loss of epoch-55 batch-142 = 4.122266545891762e-06

Training epoch-55 batch-143
Running loss of epoch-55 batch-143 = 2.146698534488678e-06

Training epoch-55 batch-144
Running loss of epoch-55 batch-144 = 3.2016541808843613e-06

Training epoch-55 batch-145
Running loss of epoch-55 batch-145 = 1.8076971173286438e-06

Training epoch-55 batch-146
Running loss of epoch-55 batch-146 = 2.457527443766594e-06

Training epoch-55 batch-147
Running loss of epoch-55 batch-147 = 2.3853499442338943e-06

Training epoch-55 batch-148
Running loss of epoch-55 batch-148 = 2.7390196919441223e-06

Training epoch-55 batch-149
Running loss of epoch-55 batch-149 = 3.257300704717636e-06

Training epoch-55 batch-150
Running loss of epoch-55 batch-150 = 2.1550804376602173e-06

Training epoch-55 batch-151
Running loss of epoch-55 batch-151 = 3.2247044146060944e-06

Training epoch-55 batch-152
Running loss of epoch-55 batch-152 = 3.1813979148864746e-06

Training epoch-55 batch-153
Running loss of epoch-55 batch-153 = 1.596519723534584e-06

Training epoch-55 batch-154
Running loss of epoch-55 batch-154 = 2.4063047021627426e-06

Training epoch-55 batch-155
Running loss of epoch-55 batch-155 = 2.1869782358407974e-06

Training epoch-55 batch-156
Running loss of epoch-55 batch-156 = 1.3499520719051361e-06

Training epoch-55 batch-157
Running loss of epoch-55 batch-157 = 1.3161450624465942e-05

Finished training epoch-55.



Average train loss at epoch-55 = 2.51179039478302e-06

Started Evaluation

Average val loss at epoch-55 = 3.1488994375655524

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.36 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.43 %

Finished Evaluation



Started training epoch-56


Training epoch-56 batch-1
Running loss of epoch-56 batch-1 = 2.871965989470482e-06

Training epoch-56 batch-2
Running loss of epoch-56 batch-2 = 1.987675204873085e-06

Training epoch-56 batch-3
Running loss of epoch-56 batch-3 = 2.5259796530008316e-06

Training epoch-56 batch-4
Running loss of epoch-56 batch-4 = 2.1508894860744476e-06

Training epoch-56 batch-5
Running loss of epoch-56 batch-5 = 3.0782539397478104e-06

Training epoch-56 batch-6
Running loss of epoch-56 batch-6 = 1.7362181097269058e-06

Training epoch-56 batch-7
Running loss of epoch-56 batch-7 = 1.394655555486679e-06

Training epoch-56 batch-8
Running loss of epoch-56 batch-8 = 2.6123598217964172e-06

Training epoch-56 batch-9
Running loss of epoch-56 batch-9 = 1.990934833884239e-06

Training epoch-56 batch-10
Running loss of epoch-56 batch-10 = 2.0063016563653946e-06

Training epoch-56 batch-11
Running loss of epoch-56 batch-11 = 2.492917701601982e-06

Training epoch-56 batch-12
Running loss of epoch-56 batch-12 = 2.032611519098282e-06

Training epoch-56 batch-13
Running loss of epoch-56 batch-13 = 1.9082799553871155e-06

Training epoch-56 batch-14
Running loss of epoch-56 batch-14 = 1.975102350115776e-06

Training epoch-56 batch-15
Running loss of epoch-56 batch-15 = 2.429122105240822e-06

Training epoch-56 batch-16
Running loss of epoch-56 batch-16 = 1.6600824892520905e-06

Training epoch-56 batch-17
Running loss of epoch-56 batch-17 = 1.995125785470009e-06

Training epoch-56 batch-18
Running loss of epoch-56 batch-18 = 2.6905909180641174e-06

Training epoch-56 batch-19
Running loss of epoch-56 batch-19 = 2.2996682673692703e-06

Training epoch-56 batch-20
Running loss of epoch-56 batch-20 = 2.485699951648712e-06

Training epoch-56 batch-21
Running loss of epoch-56 batch-21 = 2.8405338525772095e-06

Training epoch-56 batch-22
Running loss of epoch-56 batch-22 = 1.9243452697992325e-06

Training epoch-56 batch-23
Running loss of epoch-56 batch-23 = 1.9778963178396225e-06

Training epoch-56 batch-24
Running loss of epoch-56 batch-24 = 2.1657906472682953e-06

Training epoch-56 batch-25
Running loss of epoch-56 batch-25 = 2.959277480840683e-06

Training epoch-56 batch-26
Running loss of epoch-56 batch-26 = 2.3276079446077347e-06

Training epoch-56 batch-27
Running loss of epoch-56 batch-27 = 2.680346369743347e-06

Training epoch-56 batch-28
Running loss of epoch-56 batch-28 = 2.8007198125123978e-06

Training epoch-56 batch-29
Running loss of epoch-56 batch-29 = 3.0205119401216507e-06

Training epoch-56 batch-30
Running loss of epoch-56 batch-30 = 2.7294736355543137e-06

Training epoch-56 batch-31
Running loss of epoch-56 batch-31 = 1.6242265701293945e-06

Training epoch-56 batch-32
Running loss of epoch-56 batch-32 = 1.9890721887350082e-06

Training epoch-56 batch-33
Running loss of epoch-56 batch-33 = 2.0724255591630936e-06

Training epoch-56 batch-34
Running loss of epoch-56 batch-34 = 2.3082830011844635e-06

Training epoch-56 batch-35
Running loss of epoch-56 batch-35 = 1.9224826246500015e-06

Training epoch-56 batch-36
Running loss of epoch-56 batch-36 = 2.466607838869095e-06

Training epoch-56 batch-37
Running loss of epoch-56 batch-37 = 1.4370307326316833e-06

Training epoch-56 batch-38
Running loss of epoch-56 batch-38 = 3.1171366572380066e-06

Training epoch-56 batch-39
Running loss of epoch-56 batch-39 = 3.246590495109558e-06

Training epoch-56 batch-40
Running loss of epoch-56 batch-40 = 3.6174897104501724e-06

Training epoch-56 batch-41
Running loss of epoch-56 batch-41 = 1.5201512724161148e-06

Training epoch-56 batch-42
Running loss of epoch-56 batch-42 = 2.869870513677597e-06

Training epoch-56 batch-43
Running loss of epoch-56 batch-43 = 2.6442576199769974e-06

Training epoch-56 batch-44
Running loss of epoch-56 batch-44 = 3.635883331298828e-06

Training epoch-56 batch-45
Running loss of epoch-56 batch-45 = 1.8973369151353836e-06

Training epoch-56 batch-46
Running loss of epoch-56 batch-46 = 2.77860090136528e-06

Training epoch-56 batch-47
Running loss of epoch-56 batch-47 = 4.120636731386185e-06

Training epoch-56 batch-48
Running loss of epoch-56 batch-48 = 2.1846499294042587e-06

Training epoch-56 batch-49
Running loss of epoch-56 batch-49 = 2.6319175958633423e-06

Training epoch-56 batch-50
Running loss of epoch-56 batch-50 = 3.236113116145134e-06

Training epoch-56 batch-51
Running loss of epoch-56 batch-51 = 1.8372666090726852e-06

Training epoch-56 batch-52
Running loss of epoch-56 batch-52 = 2.2726599127054214e-06

Training epoch-56 batch-53
Running loss of epoch-56 batch-53 = 3.5923440009355545e-06

Training epoch-56 batch-54
Running loss of epoch-56 batch-54 = 2.5208573788404465e-06

Training epoch-56 batch-55
Running loss of epoch-56 batch-55 = 2.3364555090665817e-06

Training epoch-56 batch-56
Running loss of epoch-56 batch-56 = 1.880573108792305e-06

Training epoch-56 batch-57
Running loss of epoch-56 batch-57 = 2.9169023036956787e-06

Training epoch-56 batch-58
Running loss of epoch-56 batch-58 = 2.1299347281455994e-06

Training epoch-56 batch-59
Running loss of epoch-56 batch-59 = 2.4477485567331314e-06

Training epoch-56 batch-60
Running loss of epoch-56 batch-60 = 1.7653219401836395e-06

Training epoch-56 batch-61
Running loss of epoch-56 batch-61 = 2.3723114281892776e-06

Training epoch-56 batch-62
Running loss of epoch-56 batch-62 = 2.909451723098755e-06

Training epoch-56 batch-63
Running loss of epoch-56 batch-63 = 2.9243528842926025e-06

Training epoch-56 batch-64
Running loss of epoch-56 batch-64 = 2.0978040993213654e-06

Training epoch-56 batch-65
Running loss of epoch-56 batch-65 = 2.727378159761429e-06

Training epoch-56 batch-66
Running loss of epoch-56 batch-66 = 2.1259766072034836e-06

Training epoch-56 batch-67
Running loss of epoch-56 batch-67 = 3.2347161322832108e-06

Training epoch-56 batch-68
Running loss of epoch-56 batch-68 = 2.3299362510442734e-06

Training epoch-56 batch-69
Running loss of epoch-56 batch-69 = 3.001885488629341e-06

Training epoch-56 batch-70
Running loss of epoch-56 batch-70 = 3.825174644589424e-06

Training epoch-56 batch-71
Running loss of epoch-56 batch-71 = 1.6926787793636322e-06

Training epoch-56 batch-72
Running loss of epoch-56 batch-72 = 1.80024653673172e-06

Training epoch-56 batch-73
Running loss of epoch-56 batch-73 = 2.828892320394516e-06

Training epoch-56 batch-74
Running loss of epoch-56 batch-74 = 2.232613041996956e-06

Training epoch-56 batch-75
Running loss of epoch-56 batch-75 = 3.5658013075590134e-06

Training epoch-56 batch-76
Running loss of epoch-56 batch-76 = 2.4687033146619797e-06

Training epoch-56 batch-77
Running loss of epoch-56 batch-77 = 2.0277220755815506e-06

Training epoch-56 batch-78
Running loss of epoch-56 batch-78 = 2.8370413929224014e-06

Training epoch-56 batch-79
Running loss of epoch-56 batch-79 = 1.8330756574869156e-06

Training epoch-56 batch-80
Running loss of epoch-56 batch-80 = 2.9741786420345306e-06

Training epoch-56 batch-81
Running loss of epoch-56 batch-81 = 2.476852387189865e-06

Training epoch-56 batch-82
Running loss of epoch-56 batch-82 = 2.7494970709085464e-06

Training epoch-56 batch-83
Running loss of epoch-56 batch-83 = 1.8621794879436493e-06

Training epoch-56 batch-84
Running loss of epoch-56 batch-84 = 2.0354054868221283e-06

Training epoch-56 batch-85
Running loss of epoch-56 batch-85 = 2.3122411221265793e-06

Training epoch-56 batch-86
Running loss of epoch-56 batch-86 = 1.934356987476349e-06

Training epoch-56 batch-87
Running loss of epoch-56 batch-87 = 2.337386831641197e-06

Training epoch-56 batch-88
Running loss of epoch-56 batch-88 = 1.5438999980688095e-06

Training epoch-56 batch-89
Running loss of epoch-56 batch-89 = 2.2565945982933044e-06

Training epoch-56 batch-90
Running loss of epoch-56 batch-90 = 2.0659063011407852e-06

Training epoch-56 batch-91
Running loss of epoch-56 batch-91 = 2.485932782292366e-06

Training epoch-56 batch-92
Running loss of epoch-56 batch-92 = 3.869412466883659e-06

Training epoch-56 batch-93
Running loss of epoch-56 batch-93 = 2.359040081501007e-06

Training epoch-56 batch-94
Running loss of epoch-56 batch-94 = 2.0258594304323196e-06

Training epoch-56 batch-95
Running loss of epoch-56 batch-95 = 3.941124305129051e-06

Training epoch-56 batch-96
Running loss of epoch-56 batch-96 = 1.5445984899997711e-06

Training epoch-56 batch-97
Running loss of epoch-56 batch-97 = 2.1741725504398346e-06

Training epoch-56 batch-98
Running loss of epoch-56 batch-98 = 2.775108441710472e-06

Training epoch-56 batch-99
Running loss of epoch-56 batch-99 = 2.421438694000244e-06

Training epoch-56 batch-100
Running loss of epoch-56 batch-100 = 4.860805347561836e-06

Training epoch-56 batch-101
Running loss of epoch-56 batch-101 = 2.078711986541748e-06

Training epoch-56 batch-102
Running loss of epoch-56 batch-102 = 2.8584618121385574e-06

Training epoch-56 batch-103
Running loss of epoch-56 batch-103 = 1.7008278518915176e-06

Training epoch-56 batch-104
Running loss of epoch-56 batch-104 = 2.4852342903614044e-06

Training epoch-56 batch-105
Running loss of epoch-56 batch-105 = 1.8740538507699966e-06

Training epoch-56 batch-106
Running loss of epoch-56 batch-106 = 2.589309588074684e-06

Training epoch-56 batch-107
Running loss of epoch-56 batch-107 = 2.73948535323143e-06

Training epoch-56 batch-108
Running loss of epoch-56 batch-108 = 2.334360033273697e-06

Training epoch-56 batch-109
Running loss of epoch-56 batch-109 = 2.5315675884485245e-06

Training epoch-56 batch-110
Running loss of epoch-56 batch-110 = 1.983949914574623e-06

Training epoch-56 batch-111
Running loss of epoch-56 batch-111 = 3.3713877201080322e-06

Training epoch-56 batch-112
Running loss of epoch-56 batch-112 = 1.9515864551067352e-06

Training epoch-56 batch-113
Running loss of epoch-56 batch-113 = 2.7422793209552765e-06

Training epoch-56 batch-114
Running loss of epoch-56 batch-114 = 2.505723387002945e-06

Training epoch-56 batch-115
Running loss of epoch-56 batch-115 = 1.7299316823482513e-06

Training epoch-56 batch-116
Running loss of epoch-56 batch-116 = 2.257293090224266e-06

Training epoch-56 batch-117
Running loss of epoch-56 batch-117 = 2.3385509848594666e-06

Training epoch-56 batch-118
Running loss of epoch-56 batch-118 = 2.6798807084560394e-06

Training epoch-56 batch-119
Running loss of epoch-56 batch-119 = 2.0316801965236664e-06

Training epoch-56 batch-120
Running loss of epoch-56 batch-120 = 2.572080120444298e-06

Training epoch-56 batch-121
Running loss of epoch-56 batch-121 = 2.2919848561286926e-06

Training epoch-56 batch-122
Running loss of epoch-56 batch-122 = 2.810731530189514e-06

Training epoch-56 batch-123
Running loss of epoch-56 batch-123 = 2.6868656277656555e-06

Training epoch-56 batch-124
Running loss of epoch-56 batch-124 = 3.2410025596618652e-06

Training epoch-56 batch-125
Running loss of epoch-56 batch-125 = 1.8218997865915298e-06

Training epoch-56 batch-126
Running loss of epoch-56 batch-126 = 1.8991995602846146e-06

Training epoch-56 batch-127
Running loss of epoch-56 batch-127 = 1.450302079319954e-06

Training epoch-56 batch-128
Running loss of epoch-56 batch-128 = 2.5550834834575653e-06

Training epoch-56 batch-129
Running loss of epoch-56 batch-129 = 1.703854650259018e-06

Training epoch-56 batch-130
Running loss of epoch-56 batch-130 = 1.791398972272873e-06

Training epoch-56 batch-131
Running loss of epoch-56 batch-131 = 2.1441373974084854e-06

Training epoch-56 batch-132
Running loss of epoch-56 batch-132 = 2.538086846470833e-06

Training epoch-56 batch-133
Running loss of epoch-56 batch-133 = 1.8428545445203781e-06

Training epoch-56 batch-134
Running loss of epoch-56 batch-134 = 2.6188790798187256e-06

Training epoch-56 batch-135
Running loss of epoch-56 batch-135 = 3.7872232496738434e-06

Training epoch-56 batch-136
Running loss of epoch-56 batch-136 = 2.857763320207596e-06

Training epoch-56 batch-137
Running loss of epoch-56 batch-137 = 1.969980075955391e-06

Training epoch-56 batch-138
Running loss of epoch-56 batch-138 = 1.8058344721794128e-06

Training epoch-56 batch-139
Running loss of epoch-56 batch-139 = 1.8142163753509521e-06

Training epoch-56 batch-140
Running loss of epoch-56 batch-140 = 1.4076940715312958e-06

Training epoch-56 batch-141
Running loss of epoch-56 batch-141 = 2.541113644838333e-06

Training epoch-56 batch-142
Running loss of epoch-56 batch-142 = 3.3425167202949524e-06

Training epoch-56 batch-143
Running loss of epoch-56 batch-143 = 2.5837216526269913e-06

Training epoch-56 batch-144
Running loss of epoch-56 batch-144 = 1.9222497940063477e-06

Training epoch-56 batch-145
Running loss of epoch-56 batch-145 = 2.253800630569458e-06

Training epoch-56 batch-146
Running loss of epoch-56 batch-146 = 2.3955944925546646e-06

Training epoch-56 batch-147
Running loss of epoch-56 batch-147 = 2.100598067045212e-06

Training epoch-56 batch-148
Running loss of epoch-56 batch-148 = 1.5210825949907303e-06

Training epoch-56 batch-149
Running loss of epoch-56 batch-149 = 1.7783604562282562e-06

Training epoch-56 batch-150
Running loss of epoch-56 batch-150 = 3.253808245062828e-06

Training epoch-56 batch-151
Running loss of epoch-56 batch-151 = 2.7746427804231644e-06

Training epoch-56 batch-152
Running loss of epoch-56 batch-152 = 3.2938551157712936e-06

Training epoch-56 batch-153
Running loss of epoch-56 batch-153 = 2.04634852707386e-06

Training epoch-56 batch-154
Running loss of epoch-56 batch-154 = 3.1997915357351303e-06

Training epoch-56 batch-155
Running loss of epoch-56 batch-155 = 2.2863969206809998e-06

Training epoch-56 batch-156
Running loss of epoch-56 batch-156 = 2.77138315141201e-06

Training epoch-56 batch-157
Running loss of epoch-56 batch-157 = 1.728534698486328e-05

Finished training epoch-56.



Average train loss at epoch-56 = 2.4537310004234315e-06

Started Evaluation

Average val loss at epoch-56 = 3.1543027476260534

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 29.82 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.43 %

Finished Evaluation



Started training epoch-57


Training epoch-57 batch-1
Running loss of epoch-57 batch-1 = 2.5618355721235275e-06

Training epoch-57 batch-2
Running loss of epoch-57 batch-2 = 1.7320271581411362e-06

Training epoch-57 batch-3
Running loss of epoch-57 batch-3 = 3.1688250601291656e-06

Training epoch-57 batch-4
Running loss of epoch-57 batch-4 = 2.9169023036956787e-06

Training epoch-57 batch-5
Running loss of epoch-57 batch-5 = 8.377246558666229e-07

Training epoch-57 batch-6
Running loss of epoch-57 batch-6 = 2.5588087737560272e-06

Training epoch-57 batch-7
Running loss of epoch-57 batch-7 = 2.117827534675598e-06

Training epoch-57 batch-8
Running loss of epoch-57 batch-8 = 3.992579877376556e-06

Training epoch-57 batch-9
Running loss of epoch-57 batch-9 = 3.3676624298095703e-06

Training epoch-57 batch-10
Running loss of epoch-57 batch-10 = 2.7776695787906647e-06

Training epoch-57 batch-11
Running loss of epoch-57 batch-11 = 3.3087562769651413e-06

Training epoch-57 batch-12
Running loss of epoch-57 batch-12 = 1.6253907233476639e-06

Training epoch-57 batch-13
Running loss of epoch-57 batch-13 = 2.918532118201256e-06

Training epoch-57 batch-14
Running loss of epoch-57 batch-14 = 3.0850060284137726e-06

Training epoch-57 batch-15
Running loss of epoch-57 batch-15 = 2.1974556148052216e-06

Training epoch-57 batch-16
Running loss of epoch-57 batch-16 = 1.7136335372924805e-06

Training epoch-57 batch-17
Running loss of epoch-57 batch-17 = 2.132263034582138e-06

Training epoch-57 batch-18
Running loss of epoch-57 batch-18 = 2.8496142476797104e-06

Training epoch-57 batch-19
Running loss of epoch-57 batch-19 = 3.373250365257263e-06

Training epoch-57 batch-20
Running loss of epoch-57 batch-20 = 2.9318034648895264e-06

Training epoch-57 batch-21
Running loss of epoch-57 batch-21 = 3.0212104320526123e-06

Training epoch-57 batch-22
Running loss of epoch-57 batch-22 = 1.787673681974411e-06

Training epoch-57 batch-23
Running loss of epoch-57 batch-23 = 2.1364539861679077e-06

Training epoch-57 batch-24
Running loss of epoch-57 batch-24 = 2.5012996047735214e-06

Training epoch-57 batch-25
Running loss of epoch-57 batch-25 = 2.580694854259491e-06

Training epoch-57 batch-26
Running loss of epoch-57 batch-26 = 1.3452954590320587e-06

Training epoch-57 batch-27
Running loss of epoch-57 batch-27 = 1.5799887478351593e-06

Training epoch-57 batch-28
Running loss of epoch-57 batch-28 = 3.0102673918008804e-06

Training epoch-57 batch-29
Running loss of epoch-57 batch-29 = 3.238208591938019e-06

Training epoch-57 batch-30
Running loss of epoch-57 batch-30 = 1.6365665942430496e-06

Training epoch-57 batch-31
Running loss of epoch-57 batch-31 = 2.534361556172371e-06

Training epoch-57 batch-32
Running loss of epoch-57 batch-32 = 2.218177542090416e-06

Training epoch-57 batch-33
Running loss of epoch-57 batch-33 = 1.451931893825531e-06

Training epoch-57 batch-34
Running loss of epoch-57 batch-34 = 2.9816292226314545e-06

Training epoch-57 batch-35
Running loss of epoch-57 batch-35 = 2.243323251605034e-06

Training epoch-57 batch-36
Running loss of epoch-57 batch-36 = 1.6991980373859406e-06

Training epoch-57 batch-37
Running loss of epoch-57 batch-37 = 1.4414545148611069e-06

Training epoch-57 batch-38
Running loss of epoch-57 batch-38 = 1.473352313041687e-06

Training epoch-57 batch-39
Running loss of epoch-57 batch-39 = 2.976972609758377e-06

Training epoch-57 batch-40
Running loss of epoch-57 batch-40 = 3.1052622944116592e-06

Training epoch-57 batch-41
Running loss of epoch-57 batch-41 = 1.6475096344947815e-06

Training epoch-57 batch-42
Running loss of epoch-57 batch-42 = 3.032619133591652e-06

Training epoch-57 batch-43
Running loss of epoch-57 batch-43 = 2.116197720170021e-06

Training epoch-57 batch-44
Running loss of epoch-57 batch-44 = 2.577435225248337e-06

Training epoch-57 batch-45
Running loss of epoch-57 batch-45 = 2.903863787651062e-06

Training epoch-57 batch-46
Running loss of epoch-57 batch-46 = 2.0971056073904037e-06

Training epoch-57 batch-47
Running loss of epoch-57 batch-47 = 3.4102704375982285e-06

Training epoch-57 batch-48
Running loss of epoch-57 batch-48 = 2.118293195962906e-06

Training epoch-57 batch-49
Running loss of epoch-57 batch-49 = 2.10409052670002e-06

Training epoch-57 batch-50
Running loss of epoch-57 batch-50 = 2.4687033146619797e-06

Training epoch-57 batch-51
Running loss of epoch-57 batch-51 = 2.2863969206809998e-06

Training epoch-57 batch-52
Running loss of epoch-57 batch-52 = 1.7192214727401733e-06

Training epoch-57 batch-53
Running loss of epoch-57 batch-53 = 2.3012980818748474e-06

Training epoch-57 batch-54
Running loss of epoch-57 batch-54 = 2.8510112315416336e-06

Training epoch-57 batch-55
Running loss of epoch-57 batch-55 = 2.239830791950226e-06

Training epoch-57 batch-56
Running loss of epoch-57 batch-56 = 2.5532208383083344e-06

Training epoch-57 batch-57
Running loss of epoch-57 batch-57 = 2.2705644369125366e-06

Training epoch-57 batch-58
Running loss of epoch-57 batch-58 = 1.9243452697992325e-06

Training epoch-57 batch-59
Running loss of epoch-57 batch-59 = 1.519685611128807e-06

Training epoch-57 batch-60
Running loss of epoch-57 batch-60 = 2.343207597732544e-06

Training epoch-57 batch-61
Running loss of epoch-57 batch-61 = 2.077315002679825e-06

Training epoch-57 batch-62
Running loss of epoch-57 batch-62 = 1.9758008420467377e-06

Training epoch-57 batch-63
Running loss of epoch-57 batch-63 = 1.8773134797811508e-06

Training epoch-57 batch-64
Running loss of epoch-57 batch-64 = 2.0926818251609802e-06

Training epoch-57 batch-65
Running loss of epoch-57 batch-65 = 2.873828634619713e-06

Training epoch-57 batch-66
Running loss of epoch-57 batch-66 = 2.6640482246875763e-06

Training epoch-57 batch-67
Running loss of epoch-57 batch-67 = 1.7103739082813263e-06

Training epoch-57 batch-68
Running loss of epoch-57 batch-68 = 2.260785549879074e-06

Training epoch-57 batch-69
Running loss of epoch-57 batch-69 = 2.7546193450689316e-06

Training epoch-57 batch-70
Running loss of epoch-57 batch-70 = 2.29664146900177e-06

Training epoch-57 batch-71
Running loss of epoch-57 batch-71 = 3.0954834073781967e-06

Training epoch-57 batch-72
Running loss of epoch-57 batch-72 = 2.6812776923179626e-06

Training epoch-57 batch-73
Running loss of epoch-57 batch-73 = 2.6801135390996933e-06

Training epoch-57 batch-74
Running loss of epoch-57 batch-74 = 3.7455465644598007e-06

Training epoch-57 batch-75
Running loss of epoch-57 batch-75 = 1.7343554645776749e-06

Training epoch-57 batch-76
Running loss of epoch-57 batch-76 = 1.7874408513307571e-06

Training epoch-57 batch-77
Running loss of epoch-57 batch-77 = 2.5301706045866013e-06

Training epoch-57 batch-78
Running loss of epoch-57 batch-78 = 2.1031592041254044e-06

Training epoch-57 batch-79
Running loss of epoch-57 batch-79 = 1.5848781913518906e-06

Training epoch-57 batch-80
Running loss of epoch-57 batch-80 = 1.884065568447113e-06

Training epoch-57 batch-81
Running loss of epoch-57 batch-81 = 1.7231795936822891e-06

Training epoch-57 batch-82
Running loss of epoch-57 batch-82 = 3.1315721571445465e-06

Training epoch-57 batch-83
Running loss of epoch-57 batch-83 = 2.7583446353673935e-06

Training epoch-57 batch-84
Running loss of epoch-57 batch-84 = 1.5883706510066986e-06

Training epoch-57 batch-85
Running loss of epoch-57 batch-85 = 2.930406481027603e-06

Training epoch-57 batch-86
Running loss of epoch-57 batch-86 = 2.348329871892929e-06

Training epoch-57 batch-87
Running loss of epoch-57 batch-87 = 2.298504114151001e-06

Training epoch-57 batch-88
Running loss of epoch-57 batch-88 = 2.276850864291191e-06

Training epoch-57 batch-89
Running loss of epoch-57 batch-89 = 2.467771992087364e-06

Training epoch-57 batch-90
Running loss of epoch-57 batch-90 = 2.5758054107427597e-06

Training epoch-57 batch-91
Running loss of epoch-57 batch-91 = 2.630986273288727e-06

Training epoch-57 batch-92
Running loss of epoch-57 batch-92 = 2.582557499408722e-06

Training epoch-57 batch-93
Running loss of epoch-57 batch-93 = 2.6244670152664185e-06

Training epoch-57 batch-94
Running loss of epoch-57 batch-94 = 2.894783392548561e-06

Training epoch-57 batch-95
Running loss of epoch-57 batch-95 = 2.2794120013713837e-06

Training epoch-57 batch-96
Running loss of epoch-57 batch-96 = 2.108979970216751e-06

Training epoch-57 batch-97
Running loss of epoch-57 batch-97 = 1.980224624276161e-06

Training epoch-57 batch-98
Running loss of epoch-57 batch-98 = 2.6104971766471863e-06

Training epoch-57 batch-99
Running loss of epoch-57 batch-99 = 1.9562430679798126e-06

Training epoch-57 batch-100
Running loss of epoch-57 batch-100 = 1.932727172970772e-06

Training epoch-57 batch-101
Running loss of epoch-57 batch-101 = 1.621665433049202e-06

Training epoch-57 batch-102
Running loss of epoch-57 batch-102 = 3.0258670449256897e-06

Training epoch-57 batch-103
Running loss of epoch-57 batch-103 = 2.5026965886354446e-06

Training epoch-57 batch-104
Running loss of epoch-57 batch-104 = 2.4961773306131363e-06

Training epoch-57 batch-105
Running loss of epoch-57 batch-105 = 2.2007152438163757e-06

Training epoch-57 batch-106
Running loss of epoch-57 batch-106 = 2.2083986550569534e-06

Training epoch-57 batch-107
Running loss of epoch-57 batch-107 = 1.6787089407444e-06

Training epoch-57 batch-108
Running loss of epoch-57 batch-108 = 1.718057319521904e-06

Training epoch-57 batch-109
Running loss of epoch-57 batch-109 = 3.895722329616547e-06

Training epoch-57 batch-110
Running loss of epoch-57 batch-110 = 2.8456561267375946e-06

Training epoch-57 batch-111
Running loss of epoch-57 batch-111 = 2.484302967786789e-06

Training epoch-57 batch-112
Running loss of epoch-57 batch-112 = 1.1185184121131897e-06

Training epoch-57 batch-113
Running loss of epoch-57 batch-113 = 1.1294614523649216e-06

Training epoch-57 batch-114
Running loss of epoch-57 batch-114 = 2.613058313727379e-06

Training epoch-57 batch-115
Running loss of epoch-57 batch-115 = 3.3008400350809097e-06

Training epoch-57 batch-116
Running loss of epoch-57 batch-116 = 2.5618355721235275e-06

Training epoch-57 batch-117
Running loss of epoch-57 batch-117 = 2.3704487830400467e-06

Training epoch-57 batch-118
Running loss of epoch-57 batch-118 = 2.3758038878440857e-06

Training epoch-57 batch-119
Running loss of epoch-57 batch-119 = 2.624932676553726e-06

Training epoch-57 batch-120
Running loss of epoch-57 batch-120 = 2.0258594304323196e-06

Training epoch-57 batch-121
Running loss of epoch-57 batch-121 = 2.2030435502529144e-06

Training epoch-57 batch-122
Running loss of epoch-57 batch-122 = 2.2139865905046463e-06

Training epoch-57 batch-123
Running loss of epoch-57 batch-123 = 2.1711457520723343e-06

Training epoch-57 batch-124
Running loss of epoch-57 batch-124 = 1.369742676615715e-06

Training epoch-57 batch-125
Running loss of epoch-57 batch-125 = 2.1583400666713715e-06

Training epoch-57 batch-126
Running loss of epoch-57 batch-126 = 2.1639280021190643e-06

Training epoch-57 batch-127
Running loss of epoch-57 batch-127 = 2.5264453142881393e-06

Training epoch-57 batch-128
Running loss of epoch-57 batch-128 = 1.7934944480657578e-06

Training epoch-57 batch-129
Running loss of epoch-57 batch-129 = 2.223532646894455e-06

Training epoch-57 batch-130
Running loss of epoch-57 batch-130 = 1.4388933777809143e-06

Training epoch-57 batch-131
Running loss of epoch-57 batch-131 = 2.3355241864919662e-06

Training epoch-57 batch-132
Running loss of epoch-57 batch-132 = 3.627501428127289e-06

Training epoch-57 batch-133
Running loss of epoch-57 batch-133 = 4.708301275968552e-06

Training epoch-57 batch-134
Running loss of epoch-57 batch-134 = 2.027023583650589e-06

Training epoch-57 batch-135
Running loss of epoch-57 batch-135 = 3.160908818244934e-06

Training epoch-57 batch-136
Running loss of epoch-57 batch-136 = 1.6200356185436249e-06

Training epoch-57 batch-137
Running loss of epoch-57 batch-137 = 2.5099143385887146e-06

Training epoch-57 batch-138
Running loss of epoch-57 batch-138 = 2.348329871892929e-06

Training epoch-57 batch-139
Running loss of epoch-57 batch-139 = 1.7713755369186401e-06

Training epoch-57 batch-140
Running loss of epoch-57 batch-140 = 2.0477455109357834e-06

Training epoch-57 batch-141
Running loss of epoch-57 batch-141 = 2.150190994143486e-06

Training epoch-57 batch-142
Running loss of epoch-57 batch-142 = 2.180458977818489e-06

Training epoch-57 batch-143
Running loss of epoch-57 batch-143 = 4.030065611004829e-06

Training epoch-57 batch-144
Running loss of epoch-57 batch-144 = 3.261258825659752e-06

Training epoch-57 batch-145
Running loss of epoch-57 batch-145 = 2.773711457848549e-06

Training epoch-57 batch-146
Running loss of epoch-57 batch-146 = 2.7155037969350815e-06

Training epoch-57 batch-147
Running loss of epoch-57 batch-147 = 2.0167790353298187e-06

Training epoch-57 batch-148
Running loss of epoch-57 batch-148 = 2.984190359711647e-06

Training epoch-57 batch-149
Running loss of epoch-57 batch-149 = 2.1192245185375214e-06

Training epoch-57 batch-150
Running loss of epoch-57 batch-150 = 2.959277480840683e-06

Training epoch-57 batch-151
Running loss of epoch-57 batch-151 = 2.1602027118206024e-06

Training epoch-57 batch-152
Running loss of epoch-57 batch-152 = 1.5487894415855408e-06

Training epoch-57 batch-153
Running loss of epoch-57 batch-153 = 1.394655555486679e-06

Training epoch-57 batch-154
Running loss of epoch-57 batch-154 = 2.3599714040756226e-06

Training epoch-57 batch-155
Running loss of epoch-57 batch-155 = 2.764398232102394e-06

Training epoch-57 batch-156
Running loss of epoch-57 batch-156 = 1.6097910702228546e-06

Training epoch-57 batch-157
Running loss of epoch-57 batch-157 = 1.1458992958068848e-05

Finished training epoch-57.



Average train loss at epoch-57 = 2.386155724525452e-06

Started Evaluation

Average val loss at epoch-57 = 3.1578415020516046

Accuracy for classes:
Accuracy for class equals is: 75.58 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 39.04 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.49 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.65 %

Finished Evaluation



Started training epoch-58


Training epoch-58 batch-1
Running loss of epoch-58 batch-1 = 3.0046794563531876e-06

Training epoch-58 batch-2
Running loss of epoch-58 batch-2 = 1.4458782970905304e-06

Training epoch-58 batch-3
Running loss of epoch-58 batch-3 = 2.2100284695625305e-06

Training epoch-58 batch-4
Running loss of epoch-58 batch-4 = 1.248437911272049e-06

Training epoch-58 batch-5
Running loss of epoch-58 batch-5 = 3.7713907659053802e-06

Training epoch-58 batch-6
Running loss of epoch-58 batch-6 = 2.9474031180143356e-06

Training epoch-58 batch-7
Running loss of epoch-58 batch-7 = 2.2384338080883026e-06

Training epoch-58 batch-8
Running loss of epoch-58 batch-8 = 2.3334287106990814e-06

Training epoch-58 batch-9
Running loss of epoch-58 batch-9 = 2.7543865144252777e-06

Training epoch-58 batch-10
Running loss of epoch-58 batch-10 = 1.8223654478788376e-06

Training epoch-58 batch-11
Running loss of epoch-58 batch-11 = 1.9993167370557785e-06

Training epoch-58 batch-12
Running loss of epoch-58 batch-12 = 2.805609256029129e-06

Training epoch-58 batch-13
Running loss of epoch-58 batch-13 = 3.6566052585840225e-06

Training epoch-58 batch-14
Running loss of epoch-58 batch-14 = 1.6344711184501648e-06

Training epoch-58 batch-15
Running loss of epoch-58 batch-15 = 2.7257483452558517e-06

Training epoch-58 batch-16
Running loss of epoch-58 batch-16 = 2.185581251978874e-06

Training epoch-58 batch-17
Running loss of epoch-58 batch-17 = 1.4593824744224548e-06

Training epoch-58 batch-18
Running loss of epoch-58 batch-18 = 1.6347039490938187e-06

Training epoch-58 batch-19
Running loss of epoch-58 batch-19 = 2.2405292838811874e-06

Training epoch-58 batch-20
Running loss of epoch-58 batch-20 = 2.919696271419525e-06

Training epoch-58 batch-21
Running loss of epoch-58 batch-21 = 1.4728866517543793e-06

Training epoch-58 batch-22
Running loss of epoch-58 batch-22 = 3.189779818058014e-06

Training epoch-58 batch-23
Running loss of epoch-58 batch-23 = 1.9131693989038467e-06

Training epoch-58 batch-24
Running loss of epoch-58 batch-24 = 2.012355253100395e-06

Training epoch-58 batch-25
Running loss of epoch-58 batch-25 = 2.814223989844322e-06

Training epoch-58 batch-26
Running loss of epoch-58 batch-26 = 1.362757757306099e-06

Training epoch-58 batch-27
Running loss of epoch-58 batch-27 = 2.3685861378908157e-06

Training epoch-58 batch-28
Running loss of epoch-58 batch-28 = 2.3460015654563904e-06

Training epoch-58 batch-29
Running loss of epoch-58 batch-29 = 1.8938444554805756e-06

Training epoch-58 batch-30
Running loss of epoch-58 batch-30 = 3.6086421459913254e-06

Training epoch-58 batch-31
Running loss of epoch-58 batch-31 = 2.2514723241329193e-06

Training epoch-58 batch-32
Running loss of epoch-58 batch-32 = 2.978602424263954e-06

Training epoch-58 batch-33
Running loss of epoch-58 batch-33 = 1.8649734556674957e-06

Training epoch-58 batch-34
Running loss of epoch-58 batch-34 = 2.500135451555252e-06

Training epoch-58 batch-35
Running loss of epoch-58 batch-35 = 2.271728590130806e-06

Training epoch-58 batch-36
Running loss of epoch-58 batch-36 = 1.589534804224968e-06

Training epoch-58 batch-37
Running loss of epoch-58 batch-37 = 1.950422301888466e-06

Training epoch-58 batch-38
Running loss of epoch-58 batch-38 = 2.421904355287552e-06

Training epoch-58 batch-39
Running loss of epoch-58 batch-39 = 2.80747190117836e-06

Training epoch-58 batch-40
Running loss of epoch-58 batch-40 = 3.1979288905858994e-06

Training epoch-58 batch-41
Running loss of epoch-58 batch-41 = 2.4135224521160126e-06

Training epoch-58 batch-42
Running loss of epoch-58 batch-42 = 2.373475581407547e-06

Training epoch-58 batch-43
Running loss of epoch-58 batch-43 = 1.941574737429619e-06

Training epoch-58 batch-44
Running loss of epoch-58 batch-44 = 1.7066486179828644e-06

Training epoch-58 batch-45
Running loss of epoch-58 batch-45 = 3.127148374915123e-06

Training epoch-58 batch-46
Running loss of epoch-58 batch-46 = 2.003740519285202e-06

Training epoch-58 batch-47
Running loss of epoch-58 batch-47 = 2.830522134900093e-06

Training epoch-58 batch-48
Running loss of epoch-58 batch-48 = 2.193031832575798e-06

Training epoch-58 batch-49
Running loss of epoch-58 batch-49 = 3.436114639043808e-06

Training epoch-58 batch-50
Running loss of epoch-58 batch-50 = 1.400243490934372e-06

Training epoch-58 batch-51
Running loss of epoch-58 batch-51 = 3.448920324444771e-06

Training epoch-58 batch-52
Running loss of epoch-58 batch-52 = 1.9886065274477005e-06

Training epoch-58 batch-53
Running loss of epoch-58 batch-53 = 3.2533425837755203e-06

Training epoch-58 batch-54
Running loss of epoch-58 batch-54 = 2.174871042370796e-06

Training epoch-58 batch-55
Running loss of epoch-58 batch-55 = 2.8838403522968292e-06

Training epoch-58 batch-56
Running loss of epoch-58 batch-56 = 2.4246983230113983e-06

Training epoch-58 batch-57
Running loss of epoch-58 batch-57 = 2.5581102818250656e-06

Training epoch-58 batch-58
Running loss of epoch-58 batch-58 = 2.1727755665779114e-06

Training epoch-58 batch-59
Running loss of epoch-58 batch-59 = 2.848450094461441e-06

Training epoch-58 batch-60
Running loss of epoch-58 batch-60 = 2.6668421924114227e-06

Training epoch-58 batch-61
Running loss of epoch-58 batch-61 = 1.7336569726467133e-06

Training epoch-58 batch-62
Running loss of epoch-58 batch-62 = 2.9008369892835617e-06

Training epoch-58 batch-63
Running loss of epoch-58 batch-63 = 2.162996679544449e-06

Training epoch-58 batch-64
Running loss of epoch-58 batch-64 = 2.010725438594818e-06

Training epoch-58 batch-65
Running loss of epoch-58 batch-65 = 2.0687002688646317e-06

Training epoch-58 batch-66
Running loss of epoch-58 batch-66 = 1.941109076142311e-06

Training epoch-58 batch-67
Running loss of epoch-58 batch-67 = 1.6596168279647827e-06

Training epoch-58 batch-68
Running loss of epoch-58 batch-68 = 3.287568688392639e-06

Training epoch-58 batch-69
Running loss of epoch-58 batch-69 = 2.561137080192566e-06

Training epoch-58 batch-70
Running loss of epoch-58 batch-70 = 2.6084017008543015e-06

Training epoch-58 batch-71
Running loss of epoch-58 batch-71 = 2.7692876756191254e-06

Training epoch-58 batch-72
Running loss of epoch-58 batch-72 = 1.6211997717618942e-06

Training epoch-58 batch-73
Running loss of epoch-58 batch-73 = 1.8558930605649948e-06

Training epoch-58 batch-74
Running loss of epoch-58 batch-74 = 2.4596229195594788e-06

Training epoch-58 batch-75
Running loss of epoch-58 batch-75 = 1.3110693544149399e-06

Training epoch-58 batch-76
Running loss of epoch-58 batch-76 = 2.1890737116336823e-06

Training epoch-58 batch-77
Running loss of epoch-58 batch-77 = 2.0097941160202026e-06

Training epoch-58 batch-78
Running loss of epoch-58 batch-78 = 2.161134034395218e-06

Training epoch-58 batch-79
Running loss of epoch-58 batch-79 = 2.3331958800554276e-06

Training epoch-58 batch-80
Running loss of epoch-58 batch-80 = 3.339489921927452e-06

Training epoch-58 batch-81
Running loss of epoch-58 batch-81 = 2.5776680558919907e-06

Training epoch-58 batch-82
Running loss of epoch-58 batch-82 = 2.3422762751579285e-06

Training epoch-58 batch-83
Running loss of epoch-58 batch-83 = 2.5527551770210266e-06

Training epoch-58 batch-84
Running loss of epoch-58 batch-84 = 1.3208482414484024e-06

Training epoch-58 batch-85
Running loss of epoch-58 batch-85 = 2.9709190130233765e-06

Training epoch-58 batch-86
Running loss of epoch-58 batch-86 = 2.122018486261368e-06

Training epoch-58 batch-87
Running loss of epoch-58 batch-87 = 2.126675099134445e-06

Training epoch-58 batch-88
Running loss of epoch-58 batch-88 = 2.489658072590828e-06

Training epoch-58 batch-89
Running loss of epoch-58 batch-89 = 2.1248124539852142e-06

Training epoch-58 batch-90
Running loss of epoch-58 batch-90 = 1.4980323612689972e-06

Training epoch-58 batch-91
Running loss of epoch-58 batch-91 = 3.4205149859189987e-06

Training epoch-58 batch-92
Running loss of epoch-58 batch-92 = 2.0961742848157883e-06

Training epoch-58 batch-93
Running loss of epoch-58 batch-93 = 1.707347109913826e-06

Training epoch-58 batch-94
Running loss of epoch-58 batch-94 = 2.7532223612070084e-06

Training epoch-58 batch-95
Running loss of epoch-58 batch-95 = 2.203509211540222e-06

Training epoch-58 batch-96
Running loss of epoch-58 batch-96 = 3.0333176255226135e-06

Training epoch-58 batch-97
Running loss of epoch-58 batch-97 = 2.8687063604593277e-06

Training epoch-58 batch-98
Running loss of epoch-58 batch-98 = 1.8964055925607681e-06

Training epoch-58 batch-99
Running loss of epoch-58 batch-99 = 2.3921020328998566e-06

Training epoch-58 batch-100
Running loss of epoch-58 batch-100 = 2.7262140065431595e-06

Training epoch-58 batch-101
Running loss of epoch-58 batch-101 = 1.3241078704595566e-06

Training epoch-58 batch-102
Running loss of epoch-58 batch-102 = 2.8777867555618286e-06

Training epoch-58 batch-103
Running loss of epoch-58 batch-103 = 2.0693987607955933e-06

Training epoch-58 batch-104
Running loss of epoch-58 batch-104 = 2.2957101464271545e-06

Training epoch-58 batch-105
Running loss of epoch-58 batch-105 = 3.162538632750511e-06

Training epoch-58 batch-106
Running loss of epoch-58 batch-106 = 2.2454187273979187e-06

Training epoch-58 batch-107
Running loss of epoch-58 batch-107 = 2.2065360099077225e-06

Training epoch-58 batch-108
Running loss of epoch-58 batch-108 = 2.1280720829963684e-06

Training epoch-58 batch-109
Running loss of epoch-58 batch-109 = 2.9525253921747208e-06

Training epoch-58 batch-110
Running loss of epoch-58 batch-110 = 2.4721957743167877e-06

Training epoch-58 batch-111
Running loss of epoch-58 batch-111 = 2.342741936445236e-06

Training epoch-58 batch-112
Running loss of epoch-58 batch-112 = 1.5674158930778503e-06

Training epoch-58 batch-113
Running loss of epoch-58 batch-113 = 1.655425876379013e-06

Training epoch-58 batch-114
Running loss of epoch-58 batch-114 = 2.63238325715065e-06

Training epoch-58 batch-115
Running loss of epoch-58 batch-115 = 3.2277312129735947e-06

Training epoch-58 batch-116
Running loss of epoch-58 batch-116 = 1.7380807548761368e-06

Training epoch-58 batch-117
Running loss of epoch-58 batch-117 = 3.5550910979509354e-06

Training epoch-58 batch-118
Running loss of epoch-58 batch-118 = 2.2212043404579163e-06

Training epoch-58 batch-119
Running loss of epoch-58 batch-119 = 2.253567799925804e-06

Training epoch-58 batch-120
Running loss of epoch-58 batch-120 = 1.4335382729768753e-06

Training epoch-58 batch-121
Running loss of epoch-58 batch-121 = 2.094544470310211e-06

Training epoch-58 batch-122
Running loss of epoch-58 batch-122 = 2.1331943571567535e-06

Training epoch-58 batch-123
Running loss of epoch-58 batch-123 = 3.078952431678772e-06

Training epoch-58 batch-124
Running loss of epoch-58 batch-124 = 1.4724209904670715e-06

Training epoch-58 batch-125
Running loss of epoch-58 batch-125 = 1.480570062994957e-06

Training epoch-58 batch-126
Running loss of epoch-58 batch-126 = 2.1560117602348328e-06

Training epoch-58 batch-127
Running loss of epoch-58 batch-127 = 2.3134052753448486e-06

Training epoch-58 batch-128
Running loss of epoch-58 batch-128 = 2.2391323000192642e-06

Training epoch-58 batch-129
Running loss of epoch-58 batch-129 = 3.390014171600342e-06

Training epoch-58 batch-130
Running loss of epoch-58 batch-130 = 1.541106030344963e-06

Training epoch-58 batch-131
Running loss of epoch-58 batch-131 = 3.200257197022438e-06

Training epoch-58 batch-132
Running loss of epoch-58 batch-132 = 1.8612481653690338e-06

Training epoch-58 batch-133
Running loss of epoch-58 batch-133 = 1.7334241420030594e-06

Training epoch-58 batch-134
Running loss of epoch-58 batch-134 = 2.40374356508255e-06

Training epoch-58 batch-135
Running loss of epoch-58 batch-135 = 1.9832514226436615e-06

Training epoch-58 batch-136
Running loss of epoch-58 batch-136 = 2.6188790798187256e-06

Training epoch-58 batch-137
Running loss of epoch-58 batch-137 = 3.5995617508888245e-06

Training epoch-58 batch-138
Running loss of epoch-58 batch-138 = 2.677086740732193e-06

Training epoch-58 batch-139
Running loss of epoch-58 batch-139 = 1.7087440937757492e-06

Training epoch-58 batch-140
Running loss of epoch-58 batch-140 = 2.434477210044861e-06

Training epoch-58 batch-141
Running loss of epoch-58 batch-141 = 1.8887221813201904e-06

Training epoch-58 batch-142
Running loss of epoch-58 batch-142 = 2.2081658244132996e-06

Training epoch-58 batch-143
Running loss of epoch-58 batch-143 = 3.522029146552086e-06

Training epoch-58 batch-144
Running loss of epoch-58 batch-144 = 1.4170072972774506e-06

Training epoch-58 batch-145
Running loss of epoch-58 batch-145 = 2.5115441530942917e-06

Training epoch-58 batch-146
Running loss of epoch-58 batch-146 = 1.7976853996515274e-06

Training epoch-58 batch-147
Running loss of epoch-58 batch-147 = 1.9096769392490387e-06

Training epoch-58 batch-148
Running loss of epoch-58 batch-148 = 2.6158522814512253e-06

Training epoch-58 batch-149
Running loss of epoch-58 batch-149 = 1.96089968085289e-06

Training epoch-58 batch-150
Running loss of epoch-58 batch-150 = 1.3262033462524414e-06

Training epoch-58 batch-151
Running loss of epoch-58 batch-151 = 1.878710463643074e-06

Training epoch-58 batch-152
Running loss of epoch-58 batch-152 = 1.994660124182701e-06

Training epoch-58 batch-153
Running loss of epoch-58 batch-153 = 1.9781291484832764e-06

Training epoch-58 batch-154
Running loss of epoch-58 batch-154 = 1.2421514838933945e-06

Training epoch-58 batch-155
Running loss of epoch-58 batch-155 = 2.8600916266441345e-06

Training epoch-58 batch-156
Running loss of epoch-58 batch-156 = 2.129003405570984e-06

Training epoch-58 batch-157
Running loss of epoch-58 batch-157 = 7.010996341705322e-06

Finished training epoch-58.



Average train loss at epoch-58 = 2.3256540298461914e-06

Started Evaluation

Average val loss at epoch-58 = 3.1624133100635126

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.36 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-59


Training epoch-59 batch-1
Running loss of epoch-59 batch-1 = 1.569977030158043e-06

Training epoch-59 batch-2
Running loss of epoch-59 batch-2 = 1.3427343219518661e-06

Training epoch-59 batch-3
Running loss of epoch-59 batch-3 = 2.9085204005241394e-06

Training epoch-59 batch-4
Running loss of epoch-59 batch-4 = 1.7094425857067108e-06

Training epoch-59 batch-5
Running loss of epoch-59 batch-5 = 1.7380807548761368e-06

Training epoch-59 batch-6
Running loss of epoch-59 batch-6 = 1.4149118214845657e-06

Training epoch-59 batch-7
Running loss of epoch-59 batch-7 = 3.169989213347435e-06

Training epoch-59 batch-8
Running loss of epoch-59 batch-8 = 2.0687002688646317e-06

Training epoch-59 batch-9
Running loss of epoch-59 batch-9 = 2.52528116106987e-06

Training epoch-59 batch-10
Running loss of epoch-59 batch-10 = 1.5997793525457382e-06

Training epoch-59 batch-11
Running loss of epoch-59 batch-11 = 2.376735210418701e-06

Training epoch-59 batch-12
Running loss of epoch-59 batch-12 = 1.9764993339776993e-06

Training epoch-59 batch-13
Running loss of epoch-59 batch-13 = 2.337852492928505e-06

Training epoch-59 batch-14
Running loss of epoch-59 batch-14 = 2.308981493115425e-06

Training epoch-59 batch-15
Running loss of epoch-59 batch-15 = 2.766260877251625e-06

Training epoch-59 batch-16
Running loss of epoch-59 batch-16 = 2.3583415895700455e-06

Training epoch-59 batch-17
Running loss of epoch-59 batch-17 = 2.8812792152166367e-06

Training epoch-59 batch-18
Running loss of epoch-59 batch-18 = 2.7481000870466232e-06

Training epoch-59 batch-19
Running loss of epoch-59 batch-19 = 2.6100315153598785e-06

Training epoch-59 batch-20
Running loss of epoch-59 batch-20 = 1.5606638044118881e-06

Training epoch-59 batch-21
Running loss of epoch-59 batch-21 = 3.059627488255501e-06

Training epoch-59 batch-22
Running loss of epoch-59 batch-22 = 2.780463546514511e-06

Training epoch-59 batch-23
Running loss of epoch-59 batch-23 = 1.8477439880371094e-06

Training epoch-59 batch-24
Running loss of epoch-59 batch-24 = 1.6910489648580551e-06

Training epoch-59 batch-25
Running loss of epoch-59 batch-25 = 2.1366868168115616e-06

Training epoch-59 batch-26
Running loss of epoch-59 batch-26 = 1.925509423017502e-06

Training epoch-59 batch-27
Running loss of epoch-59 batch-27 = 2.1667219698429108e-06

Training epoch-59 batch-28
Running loss of epoch-59 batch-28 = 2.4498440325260162e-06

Training epoch-59 batch-29
Running loss of epoch-59 batch-29 = 1.925043761730194e-06

Training epoch-59 batch-30
Running loss of epoch-59 batch-30 = 1.3601966202259064e-06

Training epoch-59 batch-31
Running loss of epoch-59 batch-31 = 1.6132835298776627e-06

Training epoch-59 batch-32
Running loss of epoch-59 batch-32 = 1.3369135558605194e-06

Training epoch-59 batch-33
Running loss of epoch-59 batch-33 = 1.9830185920000076e-06

Training epoch-59 batch-34
Running loss of epoch-59 batch-34 = 1.8957071006298065e-06

Training epoch-59 batch-35
Running loss of epoch-59 batch-35 = 1.4838296920061111e-06

Training epoch-59 batch-36
Running loss of epoch-59 batch-36 = 2.0477455109357834e-06

Training epoch-59 batch-37
Running loss of epoch-59 batch-37 = 2.755783498287201e-06

Training epoch-59 batch-38
Running loss of epoch-59 batch-38 = 1.9227154552936554e-06

Training epoch-59 batch-39
Running loss of epoch-59 batch-39 = 2.991640940308571e-06

Training epoch-59 batch-40
Running loss of epoch-59 batch-40 = 2.2989697754383087e-06

Training epoch-59 batch-41
Running loss of epoch-59 batch-41 = 1.7106067389249802e-06

Training epoch-59 batch-42
Running loss of epoch-59 batch-42 = 2.0801089704036713e-06

Training epoch-59 batch-43
Running loss of epoch-59 batch-43 = 1.4540273696184158e-06

Training epoch-59 batch-44
Running loss of epoch-59 batch-44 = 2.6873312890529633e-06

Training epoch-59 batch-45
Running loss of epoch-59 batch-45 = 2.7101486921310425e-06

Training epoch-59 batch-46
Running loss of epoch-59 batch-46 = 3.016553819179535e-06

Training epoch-59 batch-47
Running loss of epoch-59 batch-47 = 2.7990899980068207e-06

Training epoch-59 batch-48
Running loss of epoch-59 batch-48 = 3.330642357468605e-06

Training epoch-59 batch-49
Running loss of epoch-59 batch-49 = 2.123182639479637e-06

Training epoch-59 batch-50
Running loss of epoch-59 batch-50 = 3.089895471930504e-06

Training epoch-59 batch-51
Running loss of epoch-59 batch-51 = 2.8223730623722076e-06

Training epoch-59 batch-52
Running loss of epoch-59 batch-52 = 1.7760321497917175e-06

Training epoch-59 batch-53
Running loss of epoch-59 batch-53 = 2.4435576051473618e-06

Training epoch-59 batch-54
Running loss of epoch-59 batch-54 = 1.914333552122116e-06

Training epoch-59 batch-55
Running loss of epoch-59 batch-55 = 2.695014700293541e-06

Training epoch-59 batch-56
Running loss of epoch-59 batch-56 = 2.3585744202136993e-06

Training epoch-59 batch-57
Running loss of epoch-59 batch-57 = 2.5206245481967926e-06

Training epoch-59 batch-58
Running loss of epoch-59 batch-58 = 2.5848858058452606e-06

Training epoch-59 batch-59
Running loss of epoch-59 batch-59 = 3.007473424077034e-06

Training epoch-59 batch-60
Running loss of epoch-59 batch-60 = 2.0011793822050095e-06

Training epoch-59 batch-61
Running loss of epoch-59 batch-61 = 2.5813933461904526e-06

Training epoch-59 batch-62
Running loss of epoch-59 batch-62 = 1.7264392226934433e-06

Training epoch-59 batch-63
Running loss of epoch-59 batch-63 = 1.9939616322517395e-06

Training epoch-59 batch-64
Running loss of epoch-59 batch-64 = 2.3194588720798492e-06

Training epoch-59 batch-65
Running loss of epoch-59 batch-65 = 2.3739412426948547e-06

Training epoch-59 batch-66
Running loss of epoch-59 batch-66 = 2.3385509848594666e-06

Training epoch-59 batch-67
Running loss of epoch-59 batch-67 = 2.391636371612549e-06

Training epoch-59 batch-68
Running loss of epoch-59 batch-68 = 4.437519237399101e-06

Training epoch-59 batch-69
Running loss of epoch-59 batch-69 = 2.1904706954956055e-06

Training epoch-59 batch-70
Running loss of epoch-59 batch-70 = 3.310851752758026e-06

Training epoch-59 batch-71
Running loss of epoch-59 batch-71 = 2.4118926376104355e-06

Training epoch-59 batch-72
Running loss of epoch-59 batch-72 = 1.8917489796876907e-06

Training epoch-59 batch-73
Running loss of epoch-59 batch-73 = 1.7844140529632568e-06

Training epoch-59 batch-74
Running loss of epoch-59 batch-74 = 2.5457702577114105e-06

Training epoch-59 batch-75
Running loss of epoch-59 batch-75 = 1.8505379557609558e-06

Training epoch-59 batch-76
Running loss of epoch-59 batch-76 = 2.517830580472946e-06

Training epoch-59 batch-77
Running loss of epoch-59 batch-77 = 2.261251211166382e-06

Training epoch-59 batch-78
Running loss of epoch-59 batch-78 = 1.86101533472538e-06

Training epoch-59 batch-79
Running loss of epoch-59 batch-79 = 2.2852327674627304e-06

Training epoch-59 batch-80
Running loss of epoch-59 batch-80 = 2.8801150619983673e-06

Training epoch-59 batch-81
Running loss of epoch-59 batch-81 = 3.2186508178710938e-06

Training epoch-59 batch-82
Running loss of epoch-59 batch-82 = 2.591172233223915e-06

Training epoch-59 batch-83
Running loss of epoch-59 batch-83 = 3.0884984880685806e-06

Training epoch-59 batch-84
Running loss of epoch-59 batch-84 = 2.4635810405015945e-06

Training epoch-59 batch-85
Running loss of epoch-59 batch-85 = 2.8882641345262527e-06

Training epoch-59 batch-86
Running loss of epoch-59 batch-86 = 2.1366868168115616e-06

Training epoch-59 batch-87
Running loss of epoch-59 batch-87 = 3.332505002617836e-06

Training epoch-59 batch-88
Running loss of epoch-59 batch-88 = 2.1618325263261795e-06

Training epoch-59 batch-89
Running loss of epoch-59 batch-89 = 2.193264663219452e-06

Training epoch-59 batch-90
Running loss of epoch-59 batch-90 = 1.6666017472743988e-06

Training epoch-59 batch-91
Running loss of epoch-59 batch-91 = 1.8063001334667206e-06

Training epoch-59 batch-92
Running loss of epoch-59 batch-92 = 2.1618325263261795e-06

Training epoch-59 batch-93
Running loss of epoch-59 batch-93 = 1.880573108792305e-06

Training epoch-59 batch-94
Running loss of epoch-59 batch-94 = 2.8882641345262527e-06

Training epoch-59 batch-95
Running loss of epoch-59 batch-95 = 2.8668437153100967e-06

Training epoch-59 batch-96
Running loss of epoch-59 batch-96 = 2.1955929696559906e-06

Training epoch-59 batch-97
Running loss of epoch-59 batch-97 = 1.7762649804353714e-06

Training epoch-59 batch-98
Running loss of epoch-59 batch-98 = 1.9031576812267303e-06

Training epoch-59 batch-99
Running loss of epoch-59 batch-99 = 2.186046913266182e-06

Training epoch-59 batch-100
Running loss of epoch-59 batch-100 = 2.991175279021263e-06

Training epoch-59 batch-101
Running loss of epoch-59 batch-101 = 2.612592652440071e-06

Training epoch-59 batch-102
Running loss of epoch-59 batch-102 = 2.16485932469368e-06

Training epoch-59 batch-103
Running loss of epoch-59 batch-103 = 2.9383227229118347e-06

Training epoch-59 batch-104
Running loss of epoch-59 batch-104 = 2.3618340492248535e-06

Training epoch-59 batch-105
Running loss of epoch-59 batch-105 = 1.4496035873889923e-06

Training epoch-59 batch-106
Running loss of epoch-59 batch-106 = 1.7441343516111374e-06

Training epoch-59 batch-107
Running loss of epoch-59 batch-107 = 2.2319145500659943e-06

Training epoch-59 batch-108
Running loss of epoch-59 batch-108 = 1.9443687051534653e-06

Training epoch-59 batch-109
Running loss of epoch-59 batch-109 = 2.8328504413366318e-06

Training epoch-59 batch-110
Running loss of epoch-59 batch-110 = 8.78702849149704e-07

Training epoch-59 batch-111
Running loss of epoch-59 batch-111 = 2.837972715497017e-06

Training epoch-59 batch-112
Running loss of epoch-59 batch-112 = 2.4533364921808243e-06

Training epoch-59 batch-113
Running loss of epoch-59 batch-113 = 1.783948391675949e-06

Training epoch-59 batch-114
Running loss of epoch-59 batch-114 = 2.0565930753946304e-06

Training epoch-59 batch-115
Running loss of epoch-59 batch-115 = 1.0901130735874176e-06

Training epoch-59 batch-116
Running loss of epoch-59 batch-116 = 2.859160304069519e-06

Training epoch-59 batch-117
Running loss of epoch-59 batch-117 = 2.5175977498292923e-06

Training epoch-59 batch-118
Running loss of epoch-59 batch-118 = 2.4084001779556274e-06

Training epoch-59 batch-119
Running loss of epoch-59 batch-119 = 2.7369242161512375e-06

Training epoch-59 batch-120
Running loss of epoch-59 batch-120 = 3.542983904480934e-06

Training epoch-59 batch-121
Running loss of epoch-59 batch-121 = 2.3597385734319687e-06

Training epoch-59 batch-122
Running loss of epoch-59 batch-122 = 2.125510945916176e-06

Training epoch-59 batch-123
Running loss of epoch-59 batch-123 = 1.9082799553871155e-06

Training epoch-59 batch-124
Running loss of epoch-59 batch-124 = 1.3343524187803268e-06

Training epoch-59 batch-125
Running loss of epoch-59 batch-125 = 1.978827640414238e-06

Training epoch-59 batch-126
Running loss of epoch-59 batch-126 = 1.978594809770584e-06

Training epoch-59 batch-127
Running loss of epoch-59 batch-127 = 1.952052116394043e-06

Training epoch-59 batch-128
Running loss of epoch-59 batch-128 = 1.818174496293068e-06

Training epoch-59 batch-129
Running loss of epoch-59 batch-129 = 1.984182745218277e-06

Training epoch-59 batch-130
Running loss of epoch-59 batch-130 = 1.096399500966072e-06

Training epoch-59 batch-131
Running loss of epoch-59 batch-131 = 1.6551930457353592e-06

Training epoch-59 batch-132
Running loss of epoch-59 batch-132 = 2.5960616767406464e-06

Training epoch-59 batch-133
Running loss of epoch-59 batch-133 = 2.7171336114406586e-06

Training epoch-59 batch-134
Running loss of epoch-59 batch-134 = 1.9792933017015457e-06

Training epoch-59 batch-135
Running loss of epoch-59 batch-135 = 2.6423949748277664e-06

Training epoch-59 batch-136
Running loss of epoch-59 batch-136 = 2.329004928469658e-06

Training epoch-59 batch-137
Running loss of epoch-59 batch-137 = 1.878943294286728e-06

Training epoch-59 batch-138
Running loss of epoch-59 batch-138 = 2.751825377345085e-06

Training epoch-59 batch-139
Running loss of epoch-59 batch-139 = 3.155320882797241e-06

Training epoch-59 batch-140
Running loss of epoch-59 batch-140 = 1.5101395547389984e-06

Training epoch-59 batch-141
Running loss of epoch-59 batch-141 = 2.0978040993213654e-06

Training epoch-59 batch-142
Running loss of epoch-59 batch-142 = 2.1387822926044464e-06

Training epoch-59 batch-143
Running loss of epoch-59 batch-143 = 1.841457560658455e-06

Training epoch-59 batch-144
Running loss of epoch-59 batch-144 = 2.3401807993650436e-06

Training epoch-59 batch-145
Running loss of epoch-59 batch-145 = 2.41096131503582e-06

Training epoch-59 batch-146
Running loss of epoch-59 batch-146 = 1.937383785843849e-06

Training epoch-59 batch-147
Running loss of epoch-59 batch-147 = 1.6263220459222794e-06

Training epoch-59 batch-148
Running loss of epoch-59 batch-148 = 2.444721758365631e-06

Training epoch-59 batch-149
Running loss of epoch-59 batch-149 = 1.8617138266563416e-06

Training epoch-59 batch-150
Running loss of epoch-59 batch-150 = 2.017943188548088e-06

Training epoch-59 batch-151
Running loss of epoch-59 batch-151 = 2.1366868168115616e-06

Training epoch-59 batch-152
Running loss of epoch-59 batch-152 = 1.9634608179330826e-06

Training epoch-59 batch-153
Running loss of epoch-59 batch-153 = 1.469627022743225e-06

Training epoch-59 batch-154
Running loss of epoch-59 batch-154 = 3.1122472137212753e-06

Training epoch-59 batch-155
Running loss of epoch-59 batch-155 = 3.0116643756628036e-06

Training epoch-59 batch-156
Running loss of epoch-59 batch-156 = 2.6472844183444977e-06

Training epoch-59 batch-157
Running loss of epoch-59 batch-157 = 4.049390554428101e-06

Finished training epoch-59.



Average train loss at epoch-59 = 2.268093824386597e-06

Started Evaluation

Average val loss at epoch-59 = 3.1697903636254763

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.72 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.58 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.64 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-60


Training epoch-60 batch-1
Running loss of epoch-60 batch-1 = 2.1723099052906036e-06

Training epoch-60 batch-2
Running loss of epoch-60 batch-2 = 2.169981598854065e-06

Training epoch-60 batch-3
Running loss of epoch-60 batch-3 = 1.7900019884109497e-06

Training epoch-60 batch-4
Running loss of epoch-60 batch-4 = 2.410728484392166e-06

Training epoch-60 batch-5
Running loss of epoch-60 batch-5 = 2.6240013539791107e-06

Training epoch-60 batch-6
Running loss of epoch-60 batch-6 = 1.9208528101444244e-06

Training epoch-60 batch-7
Running loss of epoch-60 batch-7 = 1.7026904970407486e-06

Training epoch-60 batch-8
Running loss of epoch-60 batch-8 = 2.2801104933023453e-06

Training epoch-60 batch-9
Running loss of epoch-60 batch-9 = 2.0831357687711716e-06

Training epoch-60 batch-10
Running loss of epoch-60 batch-10 = 2.6936177164316177e-06

Training epoch-60 batch-11
Running loss of epoch-60 batch-11 = 3.1155068427324295e-06

Training epoch-60 batch-12
Running loss of epoch-60 batch-12 = 1.7995480448007584e-06

Training epoch-60 batch-13
Running loss of epoch-60 batch-13 = 2.1425075829029083e-06

Training epoch-60 batch-14
Running loss of epoch-60 batch-14 = 2.2193416953086853e-06

Training epoch-60 batch-15
Running loss of epoch-60 batch-15 = 1.5909317880868912e-06

Training epoch-60 batch-16
Running loss of epoch-60 batch-16 = 1.9781291484832764e-06

Training epoch-60 batch-17
Running loss of epoch-60 batch-17 = 1.1408701539039612e-06

Training epoch-60 batch-18
Running loss of epoch-60 batch-18 = 1.95368193089962e-06

Training epoch-60 batch-19
Running loss of epoch-60 batch-19 = 3.1408853828907013e-06

Training epoch-60 batch-20
Running loss of epoch-60 batch-20 = 1.3497192412614822e-06

Training epoch-60 batch-21
Running loss of epoch-60 batch-21 = 3.987457603216171e-06

Training epoch-60 batch-22
Running loss of epoch-60 batch-22 = 1.978827640414238e-06

Training epoch-60 batch-23
Running loss of epoch-60 batch-23 = 1.8374994397163391e-06

Training epoch-60 batch-24
Running loss of epoch-60 batch-24 = 2.2202730178833008e-06

Training epoch-60 batch-25
Running loss of epoch-60 batch-25 = 1.755543053150177e-06

Training epoch-60 batch-26
Running loss of epoch-60 batch-26 = 2.1262094378471375e-06

Training epoch-60 batch-27
Running loss of epoch-60 batch-27 = 2.884073182940483e-06

Training epoch-60 batch-28
Running loss of epoch-60 batch-28 = 2.141343429684639e-06

Training epoch-60 batch-29
Running loss of epoch-60 batch-29 = 2.2351741790771484e-06

Training epoch-60 batch-30
Running loss of epoch-60 batch-30 = 2.8219074010849e-06

Training epoch-60 batch-31
Running loss of epoch-60 batch-31 = 2.7904752641916275e-06

Training epoch-60 batch-32
Running loss of epoch-60 batch-32 = 1.9134022295475006e-06

Training epoch-60 batch-33
Running loss of epoch-60 batch-33 = 2.348097041249275e-06

Training epoch-60 batch-34
Running loss of epoch-60 batch-34 = 3.0549708753824234e-06

Training epoch-60 batch-35
Running loss of epoch-60 batch-35 = 2.4831388145685196e-06

Training epoch-60 batch-36
Running loss of epoch-60 batch-36 = 1.6901176422834396e-06

Training epoch-60 batch-37
Running loss of epoch-60 batch-37 = 2.0507723093032837e-06

Training epoch-60 batch-38
Running loss of epoch-60 batch-38 = 2.9746443033218384e-06

Training epoch-60 batch-39
Running loss of epoch-60 batch-39 = 1.6004778444766998e-06

Training epoch-60 batch-40
Running loss of epoch-60 batch-40 = 1.5576370060443878e-06

Training epoch-60 batch-41
Running loss of epoch-60 batch-41 = 2.0742882043123245e-06

Training epoch-60 batch-42
Running loss of epoch-60 batch-42 = 1.6242265701293945e-06

Training epoch-60 batch-43
Running loss of epoch-60 batch-43 = 1.9655562937259674e-06

Training epoch-60 batch-44
Running loss of epoch-60 batch-44 = 1.7653219401836395e-06

Training epoch-60 batch-45
Running loss of epoch-60 batch-45 = 2.010958269238472e-06

Training epoch-60 batch-46
Running loss of epoch-60 batch-46 = 2.273591235280037e-06

Training epoch-60 batch-47
Running loss of epoch-60 batch-47 = 2.78279185295105e-06

Training epoch-60 batch-48
Running loss of epoch-60 batch-48 = 1.6847625374794006e-06

Training epoch-60 batch-49
Running loss of epoch-60 batch-49 = 1.591397449374199e-06

Training epoch-60 batch-50
Running loss of epoch-60 batch-50 = 1.8712598830461502e-06

Training epoch-60 batch-51
Running loss of epoch-60 batch-51 = 3.1800009310245514e-06

Training epoch-60 batch-52
Running loss of epoch-60 batch-52 = 2.1497253328561783e-06

Training epoch-60 batch-53
Running loss of epoch-60 batch-53 = 1.614447683095932e-06

Training epoch-60 batch-54
Running loss of epoch-60 batch-54 = 1.4849938452243805e-06

Training epoch-60 batch-55
Running loss of epoch-60 batch-55 = 2.1257437765598297e-06

Training epoch-60 batch-56
Running loss of epoch-60 batch-56 = 2.3883767426013947e-06

Training epoch-60 batch-57
Running loss of epoch-60 batch-57 = 2.0153820514678955e-06

Training epoch-60 batch-58
Running loss of epoch-60 batch-58 = 1.6719568520784378e-06

Training epoch-60 batch-59
Running loss of epoch-60 batch-59 = 1.721549779176712e-06

Training epoch-60 batch-60
Running loss of epoch-60 batch-60 = 1.86823308467865e-06

Training epoch-60 batch-61
Running loss of epoch-60 batch-61 = 3.005610778927803e-06

Training epoch-60 batch-62
Running loss of epoch-60 batch-62 = 1.1860392987728119e-06

Training epoch-60 batch-63
Running loss of epoch-60 batch-63 = 2.8498470783233643e-06

Training epoch-60 batch-64
Running loss of epoch-60 batch-64 = 2.2081658244132996e-06

Training epoch-60 batch-65
Running loss of epoch-60 batch-65 = 2.2472813725471497e-06

Training epoch-60 batch-66
Running loss of epoch-60 batch-66 = 2.130400389432907e-06

Training epoch-60 batch-67
Running loss of epoch-60 batch-67 = 2.564862370491028e-06

Training epoch-60 batch-68
Running loss of epoch-60 batch-68 = 2.671731635928154e-06

Training epoch-60 batch-69
Running loss of epoch-60 batch-69 = 3.5851262509822845e-06

Training epoch-60 batch-70
Running loss of epoch-60 batch-70 = 1.9096769392490387e-06

Training epoch-60 batch-71
Running loss of epoch-60 batch-71 = 2.2111926227808e-06

Training epoch-60 batch-72
Running loss of epoch-60 batch-72 = 1.900363713502884e-06

Training epoch-60 batch-73
Running loss of epoch-60 batch-73 = 2.017943188548088e-06

Training epoch-60 batch-74
Running loss of epoch-60 batch-74 = 2.977205440402031e-06

Training epoch-60 batch-75
Running loss of epoch-60 batch-75 = 1.1031515896320343e-06

Training epoch-60 batch-76
Running loss of epoch-60 batch-76 = 1.5548430383205414e-06

Training epoch-60 batch-77
Running loss of epoch-60 batch-77 = 2.0104926079511642e-06

Training epoch-60 batch-78
Running loss of epoch-60 batch-78 = 2.6049092411994934e-06

Training epoch-60 batch-79
Running loss of epoch-60 batch-79 = 2.0079314708709717e-06

Training epoch-60 batch-80
Running loss of epoch-60 batch-80 = 2.666143700480461e-06

Training epoch-60 batch-81
Running loss of epoch-60 batch-81 = 2.957647666335106e-06

Training epoch-60 batch-82
Running loss of epoch-60 batch-82 = 1.9040890038013458e-06

Training epoch-60 batch-83
Running loss of epoch-60 batch-83 = 1.564621925354004e-06

Training epoch-60 batch-84
Running loss of epoch-60 batch-84 = 2.2314488887786865e-06

Training epoch-60 batch-85
Running loss of epoch-60 batch-85 = 2.4959444999694824e-06

Training epoch-60 batch-86
Running loss of epoch-60 batch-86 = 1.973472535610199e-06

Training epoch-60 batch-87
Running loss of epoch-60 batch-87 = 2.0852312445640564e-06

Training epoch-60 batch-88
Running loss of epoch-60 batch-88 = 2.2514723241329193e-06

Training epoch-60 batch-89
Running loss of epoch-60 batch-89 = 1.9674189388751984e-06

Training epoch-60 batch-90
Running loss of epoch-60 batch-90 = 1.9366852939128876e-06

Training epoch-60 batch-91
Running loss of epoch-60 batch-91 = 2.2051390260457993e-06

Training epoch-60 batch-92
Running loss of epoch-60 batch-92 = 3.1623058021068573e-06

Training epoch-60 batch-93
Running loss of epoch-60 batch-93 = 1.8405262380838394e-06

Training epoch-60 batch-94
Running loss of epoch-60 batch-94 = 2.334127202630043e-06

Training epoch-60 batch-95
Running loss of epoch-60 batch-95 = 3.7071295082569122e-06

Training epoch-60 batch-96
Running loss of epoch-60 batch-96 = 2.437504008412361e-06

Training epoch-60 batch-97
Running loss of epoch-60 batch-97 = 1.7795246094465256e-06

Training epoch-60 batch-98
Running loss of epoch-60 batch-98 = 2.296874299645424e-06

Training epoch-60 batch-99
Running loss of epoch-60 batch-99 = 1.953914761543274e-06

Training epoch-60 batch-100
Running loss of epoch-60 batch-100 = 9.157229214906693e-07

Training epoch-60 batch-101
Running loss of epoch-60 batch-101 = 2.1706800907850266e-06

Training epoch-60 batch-102
Running loss of epoch-60 batch-102 = 1.7937272787094116e-06

Training epoch-60 batch-103
Running loss of epoch-60 batch-103 = 1.821666955947876e-06

Training epoch-60 batch-104
Running loss of epoch-60 batch-104 = 2.6177149266004562e-06

Training epoch-60 batch-105
Running loss of epoch-60 batch-105 = 1.969514414668083e-06

Training epoch-60 batch-106
Running loss of epoch-60 batch-106 = 1.8856953829526901e-06

Training epoch-60 batch-107
Running loss of epoch-60 batch-107 = 2.5937333703041077e-06

Training epoch-60 batch-108
Running loss of epoch-60 batch-108 = 2.1150335669517517e-06

Training epoch-60 batch-109
Running loss of epoch-60 batch-109 = 1.2584496289491653e-06

Training epoch-60 batch-110
Running loss of epoch-60 batch-110 = 2.47894786298275e-06

Training epoch-60 batch-111
Running loss of epoch-60 batch-111 = 2.6714988052845e-06

Training epoch-60 batch-112
Running loss of epoch-60 batch-112 = 1.8221326172351837e-06

Training epoch-60 batch-113
Running loss of epoch-60 batch-113 = 3.7318095564842224e-06

Training epoch-60 batch-114
Running loss of epoch-60 batch-114 = 2.0691659301519394e-06

Training epoch-60 batch-115
Running loss of epoch-60 batch-115 = 3.2284297049045563e-06

Training epoch-60 batch-116
Running loss of epoch-60 batch-116 = 2.955785021185875e-06

Training epoch-60 batch-117
Running loss of epoch-60 batch-117 = 2.278946340084076e-06

Training epoch-60 batch-118
Running loss of epoch-60 batch-118 = 2.5227200239896774e-06

Training epoch-60 batch-119
Running loss of epoch-60 batch-119 = 2.2123567759990692e-06

Training epoch-60 batch-120
Running loss of epoch-60 batch-120 = 2.443091943860054e-06

Training epoch-60 batch-121
Running loss of epoch-60 batch-121 = 1.344829797744751e-06

Training epoch-60 batch-122
Running loss of epoch-60 batch-122 = 3.1027011573314667e-06

Training epoch-60 batch-123
Running loss of epoch-60 batch-123 = 2.0153820514678955e-06

Training epoch-60 batch-124
Running loss of epoch-60 batch-124 = 2.60048545897007e-06

Training epoch-60 batch-125
Running loss of epoch-60 batch-125 = 1.1131633073091507e-06

Training epoch-60 batch-126
Running loss of epoch-60 batch-126 = 2.660788595676422e-06

Training epoch-60 batch-127
Running loss of epoch-60 batch-127 = 1.9667204469442368e-06

Training epoch-60 batch-128
Running loss of epoch-60 batch-128 = 1.7122365534305573e-06

Training epoch-60 batch-129
Running loss of epoch-60 batch-129 = 1.3485550880432129e-06

Training epoch-60 batch-130
Running loss of epoch-60 batch-130 = 2.980697900056839e-06

Training epoch-60 batch-131
Running loss of epoch-60 batch-131 = 2.4866312742233276e-06

Training epoch-60 batch-132
Running loss of epoch-60 batch-132 = 2.55717895925045e-06

Training epoch-60 batch-133
Running loss of epoch-60 batch-133 = 1.9830185920000076e-06

Training epoch-60 batch-134
Running loss of epoch-60 batch-134 = 3.2605603337287903e-06

Training epoch-60 batch-135
Running loss of epoch-60 batch-135 = 3.1706877052783966e-06

Training epoch-60 batch-136
Running loss of epoch-60 batch-136 = 1.5830155462026596e-06

Training epoch-60 batch-137
Running loss of epoch-60 batch-137 = 2.7837231755256653e-06

Training epoch-60 batch-138
Running loss of epoch-60 batch-138 = 1.794658601284027e-06

Training epoch-60 batch-139
Running loss of epoch-60 batch-139 = 1.1110678315162659e-06

Training epoch-60 batch-140
Running loss of epoch-60 batch-140 = 2.512941136956215e-06

Training epoch-60 batch-141
Running loss of epoch-60 batch-141 = 1.6211997717618942e-06

Training epoch-60 batch-142
Running loss of epoch-60 batch-142 = 3.2009556889533997e-06

Training epoch-60 batch-143
Running loss of epoch-60 batch-143 = 2.2852327674627304e-06

Training epoch-60 batch-144
Running loss of epoch-60 batch-144 = 2.8796494007110596e-06

Training epoch-60 batch-145
Running loss of epoch-60 batch-145 = 1.801876351237297e-06

Training epoch-60 batch-146
Running loss of epoch-60 batch-146 = 2.037966623902321e-06

Training epoch-60 batch-147
Running loss of epoch-60 batch-147 = 2.349959686398506e-06

Training epoch-60 batch-148
Running loss of epoch-60 batch-148 = 2.0312145352363586e-06

Training epoch-60 batch-149
Running loss of epoch-60 batch-149 = 2.0775478333234787e-06

Training epoch-60 batch-150
Running loss of epoch-60 batch-150 = 1.541338860988617e-06

Training epoch-60 batch-151
Running loss of epoch-60 batch-151 = 2.03610397875309e-06

Training epoch-60 batch-152
Running loss of epoch-60 batch-152 = 2.566026523709297e-06

Training epoch-60 batch-153
Running loss of epoch-60 batch-153 = 3.3301766961812973e-06

Training epoch-60 batch-154
Running loss of epoch-60 batch-154 = 2.7061905711889267e-06

Training epoch-60 batch-155
Running loss of epoch-60 batch-155 = 1.7583370208740234e-06

Training epoch-60 batch-156
Running loss of epoch-60 batch-156 = 1.8563587218523026e-06

Training epoch-60 batch-157
Running loss of epoch-60 batch-157 = 9.804964065551758e-06

Finished training epoch-60.



Average train loss at epoch-60 = 2.2225305438041687e-06

Started Evaluation

Average val loss at epoch-60 = 3.177193062870126

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-61


Training epoch-61 batch-1
Running loss of epoch-61 batch-1 = 1.692911610007286e-06

Training epoch-61 batch-2
Running loss of epoch-61 batch-2 = 3.0819792300462723e-06

Training epoch-61 batch-3
Running loss of epoch-61 batch-3 = 2.248678356409073e-06

Training epoch-61 batch-4
Running loss of epoch-61 batch-4 = 2.0246952772140503e-06

Training epoch-61 batch-5
Running loss of epoch-61 batch-5 = 1.8167775124311447e-06

Training epoch-61 batch-6
Running loss of epoch-61 batch-6 = 2.244720235466957e-06

Training epoch-61 batch-7
Running loss of epoch-61 batch-7 = 2.289190888404846e-06

Training epoch-61 batch-8
Running loss of epoch-61 batch-8 = 3.991182893514633e-06

Training epoch-61 batch-9
Running loss of epoch-61 batch-9 = 2.7224887162446976e-06

Training epoch-61 batch-10
Running loss of epoch-61 batch-10 = 2.428889274597168e-06

Training epoch-61 batch-11
Running loss of epoch-61 batch-11 = 1.778826117515564e-06

Training epoch-61 batch-12
Running loss of epoch-61 batch-12 = 2.998160198330879e-06

Training epoch-61 batch-13
Running loss of epoch-61 batch-13 = 2.591172233223915e-06

Training epoch-61 batch-14
Running loss of epoch-61 batch-14 = 2.4926848709583282e-06

Training epoch-61 batch-15
Running loss of epoch-61 batch-15 = 1.760898157954216e-06

Training epoch-61 batch-16
Running loss of epoch-61 batch-16 = 1.4230608940124512e-06

Training epoch-61 batch-17
Running loss of epoch-61 batch-17 = 2.6726629585027695e-06

Training epoch-61 batch-18
Running loss of epoch-61 batch-18 = 2.4274922907352448e-06

Training epoch-61 batch-19
Running loss of epoch-61 batch-19 = 1.63959339261055e-06

Training epoch-61 batch-20
Running loss of epoch-61 batch-20 = 1.8004793673753738e-06

Training epoch-61 batch-21
Running loss of epoch-61 batch-21 = 1.1986121535301208e-06

Training epoch-61 batch-22
Running loss of epoch-61 batch-22 = 2.020038664340973e-06

Training epoch-61 batch-23
Running loss of epoch-61 batch-23 = 2.4328473955392838e-06

Training epoch-61 batch-24
Running loss of epoch-61 batch-24 = 2.0118895918130875e-06

Training epoch-61 batch-25
Running loss of epoch-61 batch-25 = 1.968350261449814e-06

Training epoch-61 batch-26
Running loss of epoch-61 batch-26 = 1.8130522221326828e-06

Training epoch-61 batch-27
Running loss of epoch-61 batch-27 = 1.555541530251503e-06

Training epoch-61 batch-28
Running loss of epoch-61 batch-28 = 2.196989953517914e-06

Training epoch-61 batch-29
Running loss of epoch-61 batch-29 = 2.9210932552814484e-06

Training epoch-61 batch-30
Running loss of epoch-61 batch-30 = 2.008862793445587e-06

Training epoch-61 batch-31
Running loss of epoch-61 batch-31 = 1.4211982488632202e-06

Training epoch-61 batch-32
Running loss of epoch-61 batch-32 = 3.8978178054094315e-06

Training epoch-61 batch-33
Running loss of epoch-61 batch-33 = 1.8598511815071106e-06

Training epoch-61 batch-34
Running loss of epoch-61 batch-34 = 2.6670750230550766e-06

Training epoch-61 batch-35
Running loss of epoch-61 batch-35 = 2.3830216377973557e-06

Training epoch-61 batch-36
Running loss of epoch-61 batch-36 = 2.870103344321251e-06

Training epoch-61 batch-37
Running loss of epoch-61 batch-37 = 1.9131693989038467e-06

Training epoch-61 batch-38
Running loss of epoch-61 batch-38 = 1.980690285563469e-06

Training epoch-61 batch-39
Running loss of epoch-61 batch-39 = 2.1746382117271423e-06

Training epoch-61 batch-40
Running loss of epoch-61 batch-40 = 2.7671921998262405e-06

Training epoch-61 batch-41
Running loss of epoch-61 batch-41 = 2.533895894885063e-06

Training epoch-61 batch-42
Running loss of epoch-61 batch-42 = 1.9744038581848145e-06

Training epoch-61 batch-43
Running loss of epoch-61 batch-43 = 2.1406449377536774e-06

Training epoch-61 batch-44
Running loss of epoch-61 batch-44 = 2.2044405341148376e-06

Training epoch-61 batch-45
Running loss of epoch-61 batch-45 = 1.650303602218628e-06

Training epoch-61 batch-46
Running loss of epoch-61 batch-46 = 2.343440428376198e-06

Training epoch-61 batch-47
Running loss of epoch-61 batch-47 = 1.6540288925170898e-06

Training epoch-61 batch-48
Running loss of epoch-61 batch-48 = 1.4102552086114883e-06

Training epoch-61 batch-49
Running loss of epoch-61 batch-49 = 2.591637894511223e-06

Training epoch-61 batch-50
Running loss of epoch-61 batch-50 = 2.2316817194223404e-06

Training epoch-61 batch-51
Running loss of epoch-61 batch-51 = 1.3902317732572556e-06

Training epoch-61 batch-52
Running loss of epoch-61 batch-52 = 1.4463439583778381e-06

Training epoch-61 batch-53
Running loss of epoch-61 batch-53 = 3.1283125281333923e-06

Training epoch-61 batch-54
Running loss of epoch-61 batch-54 = 2.9401853680610657e-06

Training epoch-61 batch-55
Running loss of epoch-61 batch-55 = 1.3434328138828278e-06

Training epoch-61 batch-56
Running loss of epoch-61 batch-56 = 1.8619466572999954e-06

Training epoch-61 batch-57
Running loss of epoch-61 batch-57 = 2.5569461286067963e-06

Training epoch-61 batch-58
Running loss of epoch-61 batch-58 = 2.1711457520723343e-06

Training epoch-61 batch-59
Running loss of epoch-61 batch-59 = 2.1010637283325195e-06

Training epoch-61 batch-60
Running loss of epoch-61 batch-60 = 2.30385921895504e-06

Training epoch-61 batch-61
Running loss of epoch-61 batch-61 = 2.0407605916261673e-06

Training epoch-61 batch-62
Running loss of epoch-61 batch-62 = 2.196989953517914e-06

Training epoch-61 batch-63
Running loss of epoch-61 batch-63 = 2.125278115272522e-06

Training epoch-61 batch-64
Running loss of epoch-61 batch-64 = 2.6829075068235397e-06

Training epoch-61 batch-65
Running loss of epoch-61 batch-65 = 2.47173011302948e-06

Training epoch-61 batch-66
Running loss of epoch-61 batch-66 = 2.2263266146183014e-06

Training epoch-61 batch-67
Running loss of epoch-61 batch-67 = 2.593500539660454e-06

Training epoch-61 batch-68
Running loss of epoch-61 batch-68 = 2.8423964977264404e-06

Training epoch-61 batch-69
Running loss of epoch-61 batch-69 = 2.3529864847660065e-06

Training epoch-61 batch-70
Running loss of epoch-61 batch-70 = 4.079658538103104e-06

Training epoch-61 batch-71
Running loss of epoch-61 batch-71 = 2.3408792912960052e-06

Training epoch-61 batch-72
Running loss of epoch-61 batch-72 = 2.012355253100395e-06

Training epoch-61 batch-73
Running loss of epoch-61 batch-73 = 2.2319145500659943e-06

Training epoch-61 batch-74
Running loss of epoch-61 batch-74 = 2.210726961493492e-06

Training epoch-61 batch-75
Running loss of epoch-61 batch-75 = 1.4042016118764877e-06

Training epoch-61 batch-76
Running loss of epoch-61 batch-76 = 2.446351572871208e-06

Training epoch-61 batch-77
Running loss of epoch-61 batch-77 = 1.5525147318840027e-06

Training epoch-61 batch-78
Running loss of epoch-61 batch-78 = 2.488028258085251e-06

Training epoch-61 batch-79
Running loss of epoch-61 batch-79 = 2.1422747522592545e-06

Training epoch-61 batch-80
Running loss of epoch-61 batch-80 = 2.109445631504059e-06

Training epoch-61 batch-81
Running loss of epoch-61 batch-81 = 2.1317973732948303e-06

Training epoch-61 batch-82
Running loss of epoch-61 batch-82 = 1.9604340195655823e-06

Training epoch-61 batch-83
Running loss of epoch-61 batch-83 = 2.4926848709583282e-06

Training epoch-61 batch-84
Running loss of epoch-61 batch-84 = 1.6682315617799759e-06

Training epoch-61 batch-85
Running loss of epoch-61 batch-85 = 2.371380105614662e-06

Training epoch-61 batch-86
Running loss of epoch-61 batch-86 = 2.0246952772140503e-06

Training epoch-61 batch-87
Running loss of epoch-61 batch-87 = 2.8514768928289413e-06

Training epoch-61 batch-88
Running loss of epoch-61 batch-88 = 1.5641562640666962e-06

Training epoch-61 batch-89
Running loss of epoch-61 batch-89 = 1.5867408365011215e-06

Training epoch-61 batch-90
Running loss of epoch-61 batch-90 = 2.369750291109085e-06

Training epoch-61 batch-91
Running loss of epoch-61 batch-91 = 1.519918441772461e-06

Training epoch-61 batch-92
Running loss of epoch-61 batch-92 = 1.3087410479784012e-06

Training epoch-61 batch-93
Running loss of epoch-61 batch-93 = 2.57883220911026e-06

Training epoch-61 batch-94
Running loss of epoch-61 batch-94 = 2.5008339434862137e-06

Training epoch-61 batch-95
Running loss of epoch-61 batch-95 = 2.0281877368688583e-06

Training epoch-61 batch-96
Running loss of epoch-61 batch-96 = 2.301996573805809e-06

Training epoch-61 batch-97
Running loss of epoch-61 batch-97 = 1.7778947949409485e-06

Training epoch-61 batch-98
Running loss of epoch-61 batch-98 = 1.4365650713443756e-06

Training epoch-61 batch-99
Running loss of epoch-61 batch-99 = 1.4528632164001465e-06

Training epoch-61 batch-100
Running loss of epoch-61 batch-100 = 1.5830155462026596e-06

Training epoch-61 batch-101
Running loss of epoch-61 batch-101 = 1.8959399312734604e-06

Training epoch-61 batch-102
Running loss of epoch-61 batch-102 = 2.7241185307502747e-06

Training epoch-61 batch-103
Running loss of epoch-61 batch-103 = 2.348562702536583e-06

Training epoch-61 batch-104
Running loss of epoch-61 batch-104 = 1.416075974702835e-06

Training epoch-61 batch-105
Running loss of epoch-61 batch-105 = 1.9443687051534653e-06

Training epoch-61 batch-106
Running loss of epoch-61 batch-106 = 2.18953937292099e-06

Training epoch-61 batch-107
Running loss of epoch-61 batch-107 = 2.0170118659734726e-06

Training epoch-61 batch-108
Running loss of epoch-61 batch-108 = 2.4659093469381332e-06

Training epoch-61 batch-109
Running loss of epoch-61 batch-109 = 2.200482413172722e-06

Training epoch-61 batch-110
Running loss of epoch-61 batch-110 = 2.18953937292099e-06

Training epoch-61 batch-111
Running loss of epoch-61 batch-111 = 2.1727755665779114e-06

Training epoch-61 batch-112
Running loss of epoch-61 batch-112 = 2.1955929696559906e-06

Training epoch-61 batch-113
Running loss of epoch-61 batch-113 = 4.331348463892937e-06

Training epoch-61 batch-114
Running loss of epoch-61 batch-114 = 1.737615093588829e-06

Training epoch-61 batch-115
Running loss of epoch-61 batch-115 = 1.4409888535737991e-06

Training epoch-61 batch-116
Running loss of epoch-61 batch-116 = 2.6961788535118103e-06

Training epoch-61 batch-117
Running loss of epoch-61 batch-117 = 1.5774276107549667e-06

Training epoch-61 batch-118
Running loss of epoch-61 batch-118 = 1.7401762306690216e-06

Training epoch-61 batch-119
Running loss of epoch-61 batch-119 = 2.6740599423646927e-06

Training epoch-61 batch-120
Running loss of epoch-61 batch-120 = 1.9744038581848145e-06

Training epoch-61 batch-121
Running loss of epoch-61 batch-121 = 2.894783392548561e-06

Training epoch-61 batch-122
Running loss of epoch-61 batch-122 = 1.7015263438224792e-06

Training epoch-61 batch-123
Running loss of epoch-61 batch-123 = 1.5532132238149643e-06

Training epoch-61 batch-124
Running loss of epoch-61 batch-124 = 1.7778947949409485e-06

Training epoch-61 batch-125
Running loss of epoch-61 batch-125 = 1.7974525690078735e-06

Training epoch-61 batch-126
Running loss of epoch-61 batch-126 = 2.732733264565468e-06

Training epoch-61 batch-127
Running loss of epoch-61 batch-127 = 2.461252734065056e-06

Training epoch-61 batch-128
Running loss of epoch-61 batch-128 = 1.6971025615930557e-06

Training epoch-61 batch-129
Running loss of epoch-61 batch-129 = 1.8512364476919174e-06

Training epoch-61 batch-130
Running loss of epoch-61 batch-130 = 2.162763848900795e-06

Training epoch-61 batch-131
Running loss of epoch-61 batch-131 = 2.8745271265506744e-06

Training epoch-61 batch-132
Running loss of epoch-61 batch-132 = 1.4826655387878418e-06

Training epoch-61 batch-133
Running loss of epoch-61 batch-133 = 1.8142163753509521e-06

Training epoch-61 batch-134
Running loss of epoch-61 batch-134 = 1.5327241271734238e-06

Training epoch-61 batch-135
Running loss of epoch-61 batch-135 = 1.7439015209674835e-06

Training epoch-61 batch-136
Running loss of epoch-61 batch-136 = 1.5080440789461136e-06

Training epoch-61 batch-137
Running loss of epoch-61 batch-137 = 1.4784745872020721e-06

Training epoch-61 batch-138
Running loss of epoch-61 batch-138 = 1.5362165868282318e-06

Training epoch-61 batch-139
Running loss of epoch-61 batch-139 = 1.6239937394857407e-06

Training epoch-61 batch-140
Running loss of epoch-61 batch-140 = 2.6153866201639175e-06

Training epoch-61 batch-141
Running loss of epoch-61 batch-141 = 1.8640421330928802e-06

Training epoch-61 batch-142
Running loss of epoch-61 batch-142 = 2.3709144443273544e-06

Training epoch-61 batch-143
Running loss of epoch-61 batch-143 = 2.064509317278862e-06

Training epoch-61 batch-144
Running loss of epoch-61 batch-144 = 2.680812031030655e-06

Training epoch-61 batch-145
Running loss of epoch-61 batch-145 = 1.9220169633626938e-06

Training epoch-61 batch-146
Running loss of epoch-61 batch-146 = 2.83215194940567e-06

Training epoch-61 batch-147
Running loss of epoch-61 batch-147 = 1.6691628843545914e-06

Training epoch-61 batch-148
Running loss of epoch-61 batch-148 = 1.3702083379030228e-06

Training epoch-61 batch-149
Running loss of epoch-61 batch-149 = 2.0561274141073227e-06

Training epoch-61 batch-150
Running loss of epoch-61 batch-150 = 1.9299332052469254e-06

Training epoch-61 batch-151
Running loss of epoch-61 batch-151 = 2.775108441710472e-06

Training epoch-61 batch-152
Running loss of epoch-61 batch-152 = 2.616550773382187e-06

Training epoch-61 batch-153
Running loss of epoch-61 batch-153 = 3.0705705285072327e-06

Training epoch-61 batch-154
Running loss of epoch-61 batch-154 = 2.4526380002498627e-06

Training epoch-61 batch-155
Running loss of epoch-61 batch-155 = 2.593500539660454e-06

Training epoch-61 batch-156
Running loss of epoch-61 batch-156 = 2.0014122128486633e-06

Training epoch-61 batch-157
Running loss of epoch-61 batch-157 = 4.477798938751221e-06

Finished training epoch-61.



Average train loss at epoch-61 = 2.1656930446624755e-06

Started Evaluation

Average val loss at epoch-61 = 3.182065359855953

Accuracy for classes:
Accuracy for class equals is: 75.58 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-62


Training epoch-62 batch-1
Running loss of epoch-62 batch-1 = 1.2277159839868546e-06

Training epoch-62 batch-2
Running loss of epoch-62 batch-2 = 2.2419262677431107e-06

Training epoch-62 batch-3
Running loss of epoch-62 batch-3 = 3.125285729765892e-06

Training epoch-62 batch-4
Running loss of epoch-62 batch-4 = 2.1655578166246414e-06

Training epoch-62 batch-5
Running loss of epoch-62 batch-5 = 2.4905893951654434e-06

Training epoch-62 batch-6
Running loss of epoch-62 batch-6 = 1.6344711184501648e-06

Training epoch-62 batch-7
Running loss of epoch-62 batch-7 = 2.5865156203508377e-06

Training epoch-62 batch-8
Running loss of epoch-62 batch-8 = 2.05356627702713e-06

Training epoch-62 batch-9
Running loss of epoch-62 batch-9 = 2.485699951648712e-06

Training epoch-62 batch-10
Running loss of epoch-62 batch-10 = 2.201879397034645e-06

Training epoch-62 batch-11
Running loss of epoch-62 batch-11 = 2.572080120444298e-06

Training epoch-62 batch-12
Running loss of epoch-62 batch-12 = 2.5336630642414093e-06

Training epoch-62 batch-13
Running loss of epoch-62 batch-13 = 2.286164090037346e-06

Training epoch-62 batch-14
Running loss of epoch-62 batch-14 = 2.9085204005241394e-06

Training epoch-62 batch-15
Running loss of epoch-62 batch-15 = 2.0747538655996323e-06

Training epoch-62 batch-16
Running loss of epoch-62 batch-16 = 3.5974662750959396e-06

Training epoch-62 batch-17
Running loss of epoch-62 batch-17 = 1.9073486328125e-06

Training epoch-62 batch-18
Running loss of epoch-62 batch-18 = 2.134125679731369e-06

Training epoch-62 batch-19
Running loss of epoch-62 batch-19 = 2.15345062315464e-06

Training epoch-62 batch-20
Running loss of epoch-62 batch-20 = 1.9185245037078857e-06

Training epoch-62 batch-21
Running loss of epoch-62 batch-21 = 1.8004793673753738e-06

Training epoch-62 batch-22
Running loss of epoch-62 batch-22 = 1.8221326172351837e-06

Training epoch-62 batch-23
Running loss of epoch-62 batch-23 = 2.819579094648361e-06

Training epoch-62 batch-24
Running loss of epoch-62 batch-24 = 2.2849999368190765e-06

Training epoch-62 batch-25
Running loss of epoch-62 batch-25 = 1.6093254089355469e-06

Training epoch-62 batch-26
Running loss of epoch-62 batch-26 = 8.07223841547966e-07

Training epoch-62 batch-27
Running loss of epoch-62 batch-27 = 2.55415216088295e-06

Training epoch-62 batch-28
Running loss of epoch-62 batch-28 = 1.9513536244630814e-06

Training epoch-62 batch-29
Running loss of epoch-62 batch-29 = 1.8740538507699966e-06

Training epoch-62 batch-30
Running loss of epoch-62 batch-30 = 2.357177436351776e-06

Training epoch-62 batch-31
Running loss of epoch-62 batch-31 = 1.5355180948972702e-06

Training epoch-62 batch-32
Running loss of epoch-62 batch-32 = 1.973239704966545e-06

Training epoch-62 batch-33
Running loss of epoch-62 batch-33 = 1.2372620403766632e-06

Training epoch-62 batch-34
Running loss of epoch-62 batch-34 = 2.3883767426013947e-06

Training epoch-62 batch-35
Running loss of epoch-62 batch-35 = 2.017943188548088e-06

Training epoch-62 batch-36
Running loss of epoch-62 batch-36 = 2.3962929844856262e-06

Training epoch-62 batch-37
Running loss of epoch-62 batch-37 = 1.0116491466760635e-06

Training epoch-62 batch-38
Running loss of epoch-62 batch-38 = 1.8819700926542282e-06

Training epoch-62 batch-39
Running loss of epoch-62 batch-39 = 2.2561289370059967e-06

Training epoch-62 batch-40
Running loss of epoch-62 batch-40 = 2.055894583463669e-06

Training epoch-62 batch-41
Running loss of epoch-62 batch-41 = 2.217944711446762e-06

Training epoch-62 batch-42
Running loss of epoch-62 batch-42 = 2.809567376971245e-06

Training epoch-62 batch-43
Running loss of epoch-62 batch-43 = 1.2521632015705109e-06

Training epoch-62 batch-44
Running loss of epoch-62 batch-44 = 1.6170088201761246e-06

Training epoch-62 batch-45
Running loss of epoch-62 batch-45 = 1.753680408000946e-06

Training epoch-62 batch-46
Running loss of epoch-62 batch-46 = 1.7159618437290192e-06

Training epoch-62 batch-47
Running loss of epoch-62 batch-47 = 2.123648300766945e-06

Training epoch-62 batch-48
Running loss of epoch-62 batch-48 = 1.669861376285553e-06

Training epoch-62 batch-49
Running loss of epoch-62 batch-49 = 1.2812670320272446e-06

Training epoch-62 batch-50
Running loss of epoch-62 batch-50 = 2.1778978407382965e-06

Training epoch-62 batch-51
Running loss of epoch-62 batch-51 = 2.1494925022125244e-06

Training epoch-62 batch-52
Running loss of epoch-62 batch-52 = 1.9315630197525024e-06

Training epoch-62 batch-53
Running loss of epoch-62 batch-53 = 1.952052116394043e-06

Training epoch-62 batch-54
Running loss of epoch-62 batch-54 = 2.088025212287903e-06

Training epoch-62 batch-55
Running loss of epoch-62 batch-55 = 1.9853468984365463e-06

Training epoch-62 batch-56
Running loss of epoch-62 batch-56 = 2.3511238396167755e-06

Training epoch-62 batch-57
Running loss of epoch-62 batch-57 = 2.162763848900795e-06

Training epoch-62 batch-58
Running loss of epoch-62 batch-58 = 1.3397075235843658e-06

Training epoch-62 batch-59
Running loss of epoch-62 batch-59 = 1.4759134501218796e-06

Training epoch-62 batch-60
Running loss of epoch-62 batch-60 = 1.8635764718055725e-06

Training epoch-62 batch-61
Running loss of epoch-62 batch-61 = 2.5979243218898773e-06

Training epoch-62 batch-62
Running loss of epoch-62 batch-62 = 1.8901191651821136e-06

Training epoch-62 batch-63
Running loss of epoch-62 batch-63 = 1.9585713744163513e-06

Training epoch-62 batch-64
Running loss of epoch-62 batch-64 = 1.912470906972885e-06

Training epoch-62 batch-65
Running loss of epoch-62 batch-65 = 2.1723099052906036e-06

Training epoch-62 batch-66
Running loss of epoch-62 batch-66 = 1.3832468539476395e-06

Training epoch-62 batch-67
Running loss of epoch-62 batch-67 = 2.134125679731369e-06

Training epoch-62 batch-68
Running loss of epoch-62 batch-68 = 1.3506505638360977e-06

Training epoch-62 batch-69
Running loss of epoch-62 batch-69 = 2.241227775812149e-06

Training epoch-62 batch-70
Running loss of epoch-62 batch-70 = 1.7739366739988327e-06

Training epoch-62 batch-71
Running loss of epoch-62 batch-71 = 2.0922161638736725e-06

Training epoch-62 batch-72
Running loss of epoch-62 batch-72 = 1.8856953829526901e-06

Training epoch-62 batch-73
Running loss of epoch-62 batch-73 = 1.6631092876195908e-06

Training epoch-62 batch-74
Running loss of epoch-62 batch-74 = 1.5068799257278442e-06

Training epoch-62 batch-75
Running loss of epoch-62 batch-75 = 2.6242341846227646e-06

Training epoch-62 batch-76
Running loss of epoch-62 batch-76 = 1.6747508198022842e-06

Training epoch-62 batch-77
Running loss of epoch-62 batch-77 = 2.3813918232917786e-06

Training epoch-62 batch-78
Running loss of epoch-62 batch-78 = 2.1900050342082977e-06

Training epoch-62 batch-79
Running loss of epoch-62 batch-79 = 1.708976924419403e-06

Training epoch-62 batch-80
Running loss of epoch-62 batch-80 = 1.5462283045053482e-06

Training epoch-62 batch-81
Running loss of epoch-62 batch-81 = 1.0039657354354858e-06

Training epoch-62 batch-82
Running loss of epoch-62 batch-82 = 1.8780119717121124e-06

Training epoch-62 batch-83
Running loss of epoch-62 batch-83 = 2.460787072777748e-06

Training epoch-62 batch-84
Running loss of epoch-62 batch-84 = 2.3364555090665817e-06

Training epoch-62 batch-85
Running loss of epoch-62 batch-85 = 1.714332029223442e-06

Training epoch-62 batch-86
Running loss of epoch-62 batch-86 = 2.055428922176361e-06

Training epoch-62 batch-87
Running loss of epoch-62 batch-87 = 2.1578744053840637e-06

Training epoch-62 batch-88
Running loss of epoch-62 batch-88 = 1.6528647392988205e-06

Training epoch-62 batch-89
Running loss of epoch-62 batch-89 = 3.567896783351898e-06

Training epoch-62 batch-90
Running loss of epoch-62 batch-90 = 2.754153683781624e-06

Training epoch-62 batch-91
Running loss of epoch-62 batch-91 = 3.5353004932403564e-06

Training epoch-62 batch-92
Running loss of epoch-62 batch-92 = 2.4454202502965927e-06

Training epoch-62 batch-93
Running loss of epoch-62 batch-93 = 1.8868595361709595e-06

Training epoch-62 batch-94
Running loss of epoch-62 batch-94 = 2.6437919586896896e-06

Training epoch-62 batch-95
Running loss of epoch-62 batch-95 = 1.2184027582406998e-06

Training epoch-62 batch-96
Running loss of epoch-62 batch-96 = 2.6819761842489243e-06

Training epoch-62 batch-97
Running loss of epoch-62 batch-97 = 2.5194603949785233e-06

Training epoch-62 batch-98
Running loss of epoch-62 batch-98 = 2.0295847207307816e-06

Training epoch-62 batch-99
Running loss of epoch-62 batch-99 = 3.2475218176841736e-06

Training epoch-62 batch-100
Running loss of epoch-62 batch-100 = 1.923413947224617e-06

Training epoch-62 batch-101
Running loss of epoch-62 batch-101 = 2.641463652253151e-06

Training epoch-62 batch-102
Running loss of epoch-62 batch-102 = 1.8398277461528778e-06

Training epoch-62 batch-103
Running loss of epoch-62 batch-103 = 2.4829059839248657e-06

Training epoch-62 batch-104
Running loss of epoch-62 batch-104 = 2.264278009533882e-06

Training epoch-62 batch-105
Running loss of epoch-62 batch-105 = 2.5387853384017944e-06

Training epoch-62 batch-106
Running loss of epoch-62 batch-106 = 4.0763989090919495e-06

Training epoch-62 batch-107
Running loss of epoch-62 batch-107 = 2.118060365319252e-06

Training epoch-62 batch-108
Running loss of epoch-62 batch-108 = 1.6633421182632446e-06

Training epoch-62 batch-109
Running loss of epoch-62 batch-109 = 2.057058736681938e-06

Training epoch-62 batch-110
Running loss of epoch-62 batch-110 = 2.180458977818489e-06

Training epoch-62 batch-111
Running loss of epoch-62 batch-111 = 1.362524926662445e-06

Training epoch-62 batch-112
Running loss of epoch-62 batch-112 = 3.3851247280836105e-06

Training epoch-62 batch-113
Running loss of epoch-62 batch-113 = 1.3457611203193665e-06

Training epoch-62 batch-114
Running loss of epoch-62 batch-114 = 2.337852492928505e-06

Training epoch-62 batch-115
Running loss of epoch-62 batch-115 = 1.7120037227869034e-06

Training epoch-62 batch-116
Running loss of epoch-62 batch-116 = 2.2025778889656067e-06

Training epoch-62 batch-117
Running loss of epoch-62 batch-117 = 2.264278009533882e-06

Training epoch-62 batch-118
Running loss of epoch-62 batch-118 = 2.0917505025863647e-06

Training epoch-62 batch-119
Running loss of epoch-62 batch-119 = 2.3597385734319687e-06

Training epoch-62 batch-120
Running loss of epoch-62 batch-120 = 1.401873305439949e-06

Training epoch-62 batch-121
Running loss of epoch-62 batch-121 = 2.7904752641916275e-06

Training epoch-62 batch-122
Running loss of epoch-62 batch-122 = 2.4763867259025574e-06

Training epoch-62 batch-123
Running loss of epoch-62 batch-123 = 1.990934833884239e-06

Training epoch-62 batch-124
Running loss of epoch-62 batch-124 = 9.525101631879807e-07

Training epoch-62 batch-125
Running loss of epoch-62 batch-125 = 1.7543788999319077e-06

Training epoch-62 batch-126
Running loss of epoch-62 batch-126 = 1.8670689314603806e-06

Training epoch-62 batch-127
Running loss of epoch-62 batch-127 = 2.7515925467014313e-06

Training epoch-62 batch-128
Running loss of epoch-62 batch-128 = 2.446584403514862e-06

Training epoch-62 batch-129
Running loss of epoch-62 batch-129 = 1.4950055629014969e-06

Training epoch-62 batch-130
Running loss of epoch-62 batch-130 = 2.4279579520225525e-06

Training epoch-62 batch-131
Running loss of epoch-62 batch-131 = 2.0191073417663574e-06

Training epoch-62 batch-132
Running loss of epoch-62 batch-132 = 1.964159309864044e-06

Training epoch-62 batch-133
Running loss of epoch-62 batch-133 = 2.1569430828094482e-06

Training epoch-62 batch-134
Running loss of epoch-62 batch-134 = 1.619802787899971e-06

Training epoch-62 batch-135
Running loss of epoch-62 batch-135 = 1.7031561583280563e-06

Training epoch-62 batch-136
Running loss of epoch-62 batch-136 = 3.1874515116214752e-06

Training epoch-62 batch-137
Running loss of epoch-62 batch-137 = 2.8884969651699066e-06

Training epoch-62 batch-138
Running loss of epoch-62 batch-138 = 2.8638169169425964e-06

Training epoch-62 batch-139
Running loss of epoch-62 batch-139 = 1.689419150352478e-06

Training epoch-62 batch-140
Running loss of epoch-62 batch-140 = 1.918291673064232e-06

Training epoch-62 batch-141
Running loss of epoch-62 batch-141 = 2.786051481962204e-06

Training epoch-62 batch-142
Running loss of epoch-62 batch-142 = 1.9548460841178894e-06

Training epoch-62 batch-143
Running loss of epoch-62 batch-143 = 2.2421590983867645e-06

Training epoch-62 batch-144
Running loss of epoch-62 batch-144 = 3.3585820347070694e-06

Training epoch-62 batch-145
Running loss of epoch-62 batch-145 = 1.7138663679361343e-06

Training epoch-62 batch-146
Running loss of epoch-62 batch-146 = 1.812819391489029e-06

Training epoch-62 batch-147
Running loss of epoch-62 batch-147 = 2.667773514986038e-06

Training epoch-62 batch-148
Running loss of epoch-62 batch-148 = 2.728542312979698e-06

Training epoch-62 batch-149
Running loss of epoch-62 batch-149 = 2.0510051399469376e-06

Training epoch-62 batch-150
Running loss of epoch-62 batch-150 = 2.1941959857940674e-06

Training epoch-62 batch-151
Running loss of epoch-62 batch-151 = 1.8558930605649948e-06

Training epoch-62 batch-152
Running loss of epoch-62 batch-152 = 1.5702098608016968e-06

Training epoch-62 batch-153
Running loss of epoch-62 batch-153 = 2.155313268303871e-06

Training epoch-62 batch-154
Running loss of epoch-62 batch-154 = 2.6349443942308426e-06

Training epoch-62 batch-155
Running loss of epoch-62 batch-155 = 1.5890691429376602e-06

Training epoch-62 batch-156
Running loss of epoch-62 batch-156 = 1.443084329366684e-06

Training epoch-62 batch-157
Running loss of epoch-62 batch-157 = 1.3899058103561401e-05

Finished training epoch-62.



Average train loss at epoch-62 = 2.1297723054885865e-06

Started Evaluation

Average val loss at epoch-62 = 3.1854880483526933

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.82 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-63


Training epoch-63 batch-1
Running loss of epoch-63 batch-1 = 2.2978056222200394e-06

Training epoch-63 batch-2
Running loss of epoch-63 batch-2 = 1.712702214717865e-06

Training epoch-63 batch-3
Running loss of epoch-63 batch-3 = 3.4847762435674667e-06

Training epoch-63 batch-4
Running loss of epoch-63 batch-4 = 1.4512334018945694e-06

Training epoch-63 batch-5
Running loss of epoch-63 batch-5 = 1.4477409422397614e-06

Training epoch-63 batch-6
Running loss of epoch-63 batch-6 = 1.291045919060707e-06

Training epoch-63 batch-7
Running loss of epoch-63 batch-7 = 1.7979182302951813e-06

Training epoch-63 batch-8
Running loss of epoch-63 batch-8 = 1.780688762664795e-06

Training epoch-63 batch-9
Running loss of epoch-63 batch-9 = 1.7436686903238297e-06

Training epoch-63 batch-10
Running loss of epoch-63 batch-10 = 1.805368810892105e-06

Training epoch-63 batch-11
Running loss of epoch-63 batch-11 = 1.4919787645339966e-06

Training epoch-63 batch-12
Running loss of epoch-63 batch-12 = 1.2570526450872421e-06

Training epoch-63 batch-13
Running loss of epoch-63 batch-13 = 1.850072294473648e-06

Training epoch-63 batch-14
Running loss of epoch-63 batch-14 = 1.732492819428444e-06

Training epoch-63 batch-15
Running loss of epoch-63 batch-15 = 1.989537850022316e-06

Training epoch-63 batch-16
Running loss of epoch-63 batch-16 = 2.364860847592354e-06

Training epoch-63 batch-17
Running loss of epoch-63 batch-17 = 2.4514738470315933e-06

Training epoch-63 batch-18
Running loss of epoch-63 batch-18 = 1.7343554645776749e-06

Training epoch-63 batch-19
Running loss of epoch-63 batch-19 = 3.070337697863579e-06

Training epoch-63 batch-20
Running loss of epoch-63 batch-20 = 2.2086314857006073e-06

Training epoch-63 batch-21
Running loss of epoch-63 batch-21 = 2.589542418718338e-06

Training epoch-63 batch-22
Running loss of epoch-63 batch-22 = 1.910608261823654e-06

Training epoch-63 batch-23
Running loss of epoch-63 batch-23 = 2.1455343812704086e-06

Training epoch-63 batch-24
Running loss of epoch-63 batch-24 = 1.3872049748897552e-06

Training epoch-63 batch-25
Running loss of epoch-63 batch-25 = 1.841457560658455e-06

Training epoch-63 batch-26
Running loss of epoch-63 batch-26 = 2.491520717740059e-06

Training epoch-63 batch-27
Running loss of epoch-63 batch-27 = 2.7958303689956665e-06

Training epoch-63 batch-28
Running loss of epoch-63 batch-28 = 1.1317897588014603e-06

Training epoch-63 batch-29
Running loss of epoch-63 batch-29 = 2.327142283320427e-06

Training epoch-63 batch-30
Running loss of epoch-63 batch-30 = 2.289889380335808e-06

Training epoch-63 batch-31
Running loss of epoch-63 batch-31 = 2.3746397346258163e-06

Training epoch-63 batch-32
Running loss of epoch-63 batch-32 = 2.0568259060382843e-06

Training epoch-63 batch-33
Running loss of epoch-63 batch-33 = 1.9546132534742355e-06

Training epoch-63 batch-34
Running loss of epoch-63 batch-34 = 2.8891954571008682e-06

Training epoch-63 batch-35
Running loss of epoch-63 batch-35 = 3.133900463581085e-06

Training epoch-63 batch-36
Running loss of epoch-63 batch-36 = 1.748325303196907e-06

Training epoch-63 batch-37
Running loss of epoch-63 batch-37 = 2.362532541155815e-06

Training epoch-63 batch-38
Running loss of epoch-63 batch-38 = 2.4046748876571655e-06

Training epoch-63 batch-39
Running loss of epoch-63 batch-39 = 1.1974480003118515e-06

Training epoch-63 batch-40
Running loss of epoch-63 batch-40 = 2.3478642106056213e-06

Training epoch-63 batch-41
Running loss of epoch-63 batch-41 = 1.6870908439159393e-06

Training epoch-63 batch-42
Running loss of epoch-63 batch-42 = 2.409331500530243e-06

Training epoch-63 batch-43
Running loss of epoch-63 batch-43 = 1.200009137392044e-06

Training epoch-63 batch-44
Running loss of epoch-63 batch-44 = 3.091292455792427e-06

Training epoch-63 batch-45
Running loss of epoch-63 batch-45 = 2.5008339434862137e-06

Training epoch-63 batch-46
Running loss of epoch-63 batch-46 = 2.9352959245443344e-06

Training epoch-63 batch-47
Running loss of epoch-63 batch-47 = 1.4060642570257187e-06

Training epoch-63 batch-48
Running loss of epoch-63 batch-48 = 3.187917172908783e-06

Training epoch-63 batch-49
Running loss of epoch-63 batch-49 = 2.327375113964081e-06

Training epoch-63 batch-50
Running loss of epoch-63 batch-50 = 1.7436686903238297e-06

Training epoch-63 batch-51
Running loss of epoch-63 batch-51 = 2.2887252271175385e-06

Training epoch-63 batch-52
Running loss of epoch-63 batch-52 = 1.4496035873889923e-06

Training epoch-63 batch-53
Running loss of epoch-63 batch-53 = 1.6540288925170898e-06

Training epoch-63 batch-54
Running loss of epoch-63 batch-54 = 1.6463454812765121e-06

Training epoch-63 batch-55
Running loss of epoch-63 batch-55 = 1.2414529919624329e-06

Training epoch-63 batch-56
Running loss of epoch-63 batch-56 = 2.257060259580612e-06

Training epoch-63 batch-57
Running loss of epoch-63 batch-57 = 1.6265548765659332e-06

Training epoch-63 batch-58
Running loss of epoch-63 batch-58 = 1.7671845853328705e-06

Training epoch-63 batch-59
Running loss of epoch-63 batch-59 = 2.125278115272522e-06

Training epoch-63 batch-60
Running loss of epoch-63 batch-60 = 1.4603137969970703e-06

Training epoch-63 batch-61
Running loss of epoch-63 batch-61 = 2.6167836040258408e-06

Training epoch-63 batch-62
Running loss of epoch-63 batch-62 = 2.5588087737560272e-06

Training epoch-63 batch-63
Running loss of epoch-63 batch-63 = 1.428881660103798e-06

Training epoch-63 batch-64
Running loss of epoch-63 batch-64 = 2.3939646780490875e-06

Training epoch-63 batch-65
Running loss of epoch-63 batch-65 = 3.003980964422226e-06

Training epoch-63 batch-66
Running loss of epoch-63 batch-66 = 2.1462328732013702e-06

Training epoch-63 batch-67
Running loss of epoch-63 batch-67 = 1.9010622054338455e-06

Training epoch-63 batch-68
Running loss of epoch-63 batch-68 = 1.2647360563278198e-06

Training epoch-63 batch-69
Running loss of epoch-63 batch-69 = 2.900371327996254e-06

Training epoch-63 batch-70
Running loss of epoch-63 batch-70 = 1.989537850022316e-06

Training epoch-63 batch-71
Running loss of epoch-63 batch-71 = 2.175336703658104e-06

Training epoch-63 batch-72
Running loss of epoch-63 batch-72 = 2.120155841112137e-06

Training epoch-63 batch-73
Running loss of epoch-63 batch-73 = 1.9781291484832764e-06

Training epoch-63 batch-74
Running loss of epoch-63 batch-74 = 1.8477439880371094e-06

Training epoch-63 batch-75
Running loss of epoch-63 batch-75 = 2.4545006453990936e-06

Training epoch-63 batch-76
Running loss of epoch-63 batch-76 = 2.1615996956825256e-06

Training epoch-63 batch-77
Running loss of epoch-63 batch-77 = 2.4514738470315933e-06

Training epoch-63 batch-78
Running loss of epoch-63 batch-78 = 2.0347069948911667e-06

Training epoch-63 batch-79
Running loss of epoch-63 batch-79 = 2.132030203938484e-06

Training epoch-63 batch-80
Running loss of epoch-63 batch-80 = 2.3774337023496628e-06

Training epoch-63 batch-81
Running loss of epoch-63 batch-81 = 1.7175916582345963e-06

Training epoch-63 batch-82
Running loss of epoch-63 batch-82 = 2.161134034395218e-06

Training epoch-63 batch-83
Running loss of epoch-63 batch-83 = 1.4905817806720734e-06

Training epoch-63 batch-84
Running loss of epoch-63 batch-84 = 1.9331928342580795e-06

Training epoch-63 batch-85
Running loss of epoch-63 batch-85 = 1.6246922314167023e-06

Training epoch-63 batch-86
Running loss of epoch-63 batch-86 = 1.8693972378969193e-06

Training epoch-63 batch-87
Running loss of epoch-63 batch-87 = 1.6130506992340088e-06

Training epoch-63 batch-88
Running loss of epoch-63 batch-88 = 2.3227185010910034e-06

Training epoch-63 batch-89
Running loss of epoch-63 batch-89 = 1.5278346836566925e-06

Training epoch-63 batch-90
Running loss of epoch-63 batch-90 = 2.7103815227746964e-06

Training epoch-63 batch-91
Running loss of epoch-63 batch-91 = 1.6177073121070862e-06

Training epoch-63 batch-92
Running loss of epoch-63 batch-92 = 2.007698640227318e-06

Training epoch-63 batch-93
Running loss of epoch-63 batch-93 = 2.044485881924629e-06

Training epoch-63 batch-94
Running loss of epoch-63 batch-94 = 2.416083589196205e-06

Training epoch-63 batch-95
Running loss of epoch-63 batch-95 = 2.0675361156463623e-06

Training epoch-63 batch-96
Running loss of epoch-63 batch-96 = 2.202112227678299e-06

Training epoch-63 batch-97
Running loss of epoch-63 batch-97 = 1.6365665942430496e-06

Training epoch-63 batch-98
Running loss of epoch-63 batch-98 = 2.1453015506267548e-06

Training epoch-63 batch-99
Running loss of epoch-63 batch-99 = 2.4584587663412094e-06

Training epoch-63 batch-100
Running loss of epoch-63 batch-100 = 2.994667738676071e-06

Training epoch-63 batch-101
Running loss of epoch-63 batch-101 = 2.120155841112137e-06

Training epoch-63 batch-102
Running loss of epoch-63 batch-102 = 1.7844140529632568e-06

Training epoch-63 batch-103
Running loss of epoch-63 batch-103 = 1.7723068594932556e-06

Training epoch-63 batch-104
Running loss of epoch-63 batch-104 = 1.938082277774811e-06

Training epoch-63 batch-105
Running loss of epoch-63 batch-105 = 2.039829269051552e-06

Training epoch-63 batch-106
Running loss of epoch-63 batch-106 = 1.7997808754444122e-06

Training epoch-63 batch-107
Running loss of epoch-63 batch-107 = 2.0354054868221283e-06

Training epoch-63 batch-108
Running loss of epoch-63 batch-108 = 2.062646672129631e-06

Training epoch-63 batch-109
Running loss of epoch-63 batch-109 = 2.057524397969246e-06

Training epoch-63 batch-110
Running loss of epoch-63 batch-110 = 1.8116552382707596e-06

Training epoch-63 batch-111
Running loss of epoch-63 batch-111 = 1.373235136270523e-06

Training epoch-63 batch-112
Running loss of epoch-63 batch-112 = 2.468004822731018e-06

Training epoch-63 batch-113
Running loss of epoch-63 batch-113 = 2.748798578977585e-06

Training epoch-63 batch-114
Running loss of epoch-63 batch-114 = 2.139480784535408e-06

Training epoch-63 batch-115
Running loss of epoch-63 batch-115 = 2.4137552827596664e-06

Training epoch-63 batch-116
Running loss of epoch-63 batch-116 = 2.8884969651699066e-06

Training epoch-63 batch-117
Running loss of epoch-63 batch-117 = 1.8347054719924927e-06

Training epoch-63 batch-118
Running loss of epoch-63 batch-118 = 1.5262048691511154e-06

Training epoch-63 batch-119
Running loss of epoch-63 batch-119 = 1.5168916434049606e-06

Training epoch-63 batch-120
Running loss of epoch-63 batch-120 = 1.4149118214845657e-06

Training epoch-63 batch-121
Running loss of epoch-63 batch-121 = 1.7497222870588303e-06

Training epoch-63 batch-122
Running loss of epoch-63 batch-122 = 3.1834933906793594e-06

Training epoch-63 batch-123
Running loss of epoch-63 batch-123 = 2.555781975388527e-06

Training epoch-63 batch-124
Running loss of epoch-63 batch-124 = 1.8852297216653824e-06

Training epoch-63 batch-125
Running loss of epoch-63 batch-125 = 2.273358404636383e-06

Training epoch-63 batch-126
Running loss of epoch-63 batch-126 = 1.6735866665840149e-06

Training epoch-63 batch-127
Running loss of epoch-63 batch-127 = 2.0049046725034714e-06

Training epoch-63 batch-128
Running loss of epoch-63 batch-128 = 2.186279743909836e-06

Training epoch-63 batch-129
Running loss of epoch-63 batch-129 = 1.6451813280582428e-06

Training epoch-63 batch-130
Running loss of epoch-63 batch-130 = 1.8004793673753738e-06

Training epoch-63 batch-131
Running loss of epoch-63 batch-131 = 2.4847686290740967e-06

Training epoch-63 batch-132
Running loss of epoch-63 batch-132 = 2.2989697754383087e-06

Training epoch-63 batch-133
Running loss of epoch-63 batch-133 = 1.3406388461589813e-06

Training epoch-63 batch-134
Running loss of epoch-63 batch-134 = 1.925043761730194e-06

Training epoch-63 batch-135
Running loss of epoch-63 batch-135 = 1.9008293747901917e-06

Training epoch-63 batch-136
Running loss of epoch-63 batch-136 = 2.6703346520662308e-06

Training epoch-63 batch-137
Running loss of epoch-63 batch-137 = 1.9618310034275055e-06

Training epoch-63 batch-138
Running loss of epoch-63 batch-138 = 2.825399860739708e-06

Training epoch-63 batch-139
Running loss of epoch-63 batch-139 = 9.923242032527924e-07

Training epoch-63 batch-140
Running loss of epoch-63 batch-140 = 2.0419247448444366e-06

Training epoch-63 batch-141
Running loss of epoch-63 batch-141 = 1.7713755369186401e-06

Training epoch-63 batch-142
Running loss of epoch-63 batch-142 = 1.909211277961731e-06

Training epoch-63 batch-143
Running loss of epoch-63 batch-143 = 2.2258609533309937e-06

Training epoch-63 batch-144
Running loss of epoch-63 batch-144 = 1.8349383026361465e-06

Training epoch-63 batch-145
Running loss of epoch-63 batch-145 = 1.7685815691947937e-06

Training epoch-63 batch-146
Running loss of epoch-63 batch-146 = 2.2284220904111862e-06

Training epoch-63 batch-147
Running loss of epoch-63 batch-147 = 2.476852387189865e-06

Training epoch-63 batch-148
Running loss of epoch-63 batch-148 = 1.6761478036642075e-06

Training epoch-63 batch-149
Running loss of epoch-63 batch-149 = 3.369990736246109e-06

Training epoch-63 batch-150
Running loss of epoch-63 batch-150 = 3.1767413020133972e-06

Training epoch-63 batch-151
Running loss of epoch-63 batch-151 = 2.2281892597675323e-06

Training epoch-63 batch-152
Running loss of epoch-63 batch-152 = 2.377200871706009e-06

Training epoch-63 batch-153
Running loss of epoch-63 batch-153 = 2.4598557502031326e-06

Training epoch-63 batch-154
Running loss of epoch-63 batch-154 = 1.6274861991405487e-06

Training epoch-63 batch-155
Running loss of epoch-63 batch-155 = 1.3441313058137894e-06

Training epoch-63 batch-156
Running loss of epoch-63 batch-156 = 2.3783650249242783e-06

Training epoch-63 batch-157
Running loss of epoch-63 batch-157 = 6.254762411117554e-06

Finished training epoch-63.



Average train loss at epoch-63 = 2.0762354135513305e-06

Started Evaluation

Average val loss at epoch-63 = 3.1912910655925146

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-64


Training epoch-64 batch-1
Running loss of epoch-64 batch-1 = 1.7925631254911423e-06

Training epoch-64 batch-2
Running loss of epoch-64 batch-2 = 2.3385509848594666e-06

Training epoch-64 batch-3
Running loss of epoch-64 batch-3 = 2.8975773602724075e-06

Training epoch-64 batch-4
Running loss of epoch-64 batch-4 = 1.735752448439598e-06

Training epoch-64 batch-5
Running loss of epoch-64 batch-5 = 2.344837412238121e-06

Training epoch-64 batch-6
Running loss of epoch-64 batch-6 = 1.77859328687191e-06

Training epoch-64 batch-7
Running loss of epoch-64 batch-7 = 1.9115395843982697e-06

Training epoch-64 batch-8
Running loss of epoch-64 batch-8 = 2.0561274141073227e-06

Training epoch-64 batch-9
Running loss of epoch-64 batch-9 = 1.4628749340772629e-06

Training epoch-64 batch-10
Running loss of epoch-64 batch-10 = 2.007698640227318e-06

Training epoch-64 batch-11
Running loss of epoch-64 batch-11 = 1.7203856259584427e-06

Training epoch-64 batch-12
Running loss of epoch-64 batch-12 = 2.021435648202896e-06

Training epoch-64 batch-13
Running loss of epoch-64 batch-13 = 2.066371962428093e-06

Training epoch-64 batch-14
Running loss of epoch-64 batch-14 = 2.605607733130455e-06

Training epoch-64 batch-15
Running loss of epoch-64 batch-15 = 2.9939692467451096e-06

Training epoch-64 batch-16
Running loss of epoch-64 batch-16 = 9.557697921991348e-07

Training epoch-64 batch-17
Running loss of epoch-64 batch-17 = 1.1504162102937698e-06

Training epoch-64 batch-18
Running loss of epoch-64 batch-18 = 2.5383196771144867e-06

Training epoch-64 batch-19
Running loss of epoch-64 batch-19 = 1.92900188267231e-06

Training epoch-64 batch-20
Running loss of epoch-64 batch-20 = 2.2621825337409973e-06

Training epoch-64 batch-21
Running loss of epoch-64 batch-21 = 7.57165253162384e-07

Training epoch-64 batch-22
Running loss of epoch-64 batch-22 = 2.203509211540222e-06

Training epoch-64 batch-23
Running loss of epoch-64 batch-23 = 2.1138694137334824e-06

Training epoch-64 batch-24
Running loss of epoch-64 batch-24 = 1.8267892301082611e-06

Training epoch-64 batch-25
Running loss of epoch-64 batch-25 = 1.6926787793636322e-06

Training epoch-64 batch-26
Running loss of epoch-64 batch-26 = 1.9113067537546158e-06

Training epoch-64 batch-27
Running loss of epoch-64 batch-27 = 1.8274877220392227e-06

Training epoch-64 batch-28
Running loss of epoch-64 batch-28 = 1.9795261323451996e-06

Training epoch-64 batch-29
Running loss of epoch-64 batch-29 = 1.7140991985797882e-06

Training epoch-64 batch-30
Running loss of epoch-64 batch-30 = 1.7543788999319077e-06

Training epoch-64 batch-31
Running loss of epoch-64 batch-31 = 2.0761508494615555e-06

Training epoch-64 batch-32
Running loss of epoch-64 batch-32 = 1.7953570932149887e-06

Training epoch-64 batch-33
Running loss of epoch-64 batch-33 = 2.9692891985177994e-06

Training epoch-64 batch-34
Running loss of epoch-64 batch-34 = 1.9299332052469254e-06

Training epoch-64 batch-35
Running loss of epoch-64 batch-35 = 8.01868736743927e-07

Training epoch-64 batch-36
Running loss of epoch-64 batch-36 = 1.1816155165433884e-06

Training epoch-64 batch-37
Running loss of epoch-64 batch-37 = 1.4025717973709106e-06

Training epoch-64 batch-38
Running loss of epoch-64 batch-38 = 1.6207341104745865e-06

Training epoch-64 batch-39
Running loss of epoch-64 batch-39 = 2.3278407752513885e-06

Training epoch-64 batch-40
Running loss of epoch-64 batch-40 = 1.1294614523649216e-06

Training epoch-64 batch-41
Running loss of epoch-64 batch-41 = 3.3008400350809097e-06

Training epoch-64 batch-42
Running loss of epoch-64 batch-42 = 1.5457626432180405e-06

Training epoch-64 batch-43
Running loss of epoch-64 batch-43 = 1.6361009329557419e-06

Training epoch-64 batch-44
Running loss of epoch-64 batch-44 = 1.3969838619232178e-06

Training epoch-64 batch-45
Running loss of epoch-64 batch-45 = 2.0938459783792496e-06

Training epoch-64 batch-46
Running loss of epoch-64 batch-46 = 1.9960571080446243e-06

Training epoch-64 batch-47
Running loss of epoch-64 batch-47 = 9.331852197647095e-07

Training epoch-64 batch-48
Running loss of epoch-64 batch-48 = 1.974869519472122e-06

Training epoch-64 batch-49
Running loss of epoch-64 batch-49 = 1.6638077795505524e-06

Training epoch-64 batch-50
Running loss of epoch-64 batch-50 = 2.4635810405015945e-06

Training epoch-64 batch-51
Running loss of epoch-64 batch-51 = 2.1082814782857895e-06

Training epoch-64 batch-52
Running loss of epoch-64 batch-52 = 1.768115907907486e-06

Training epoch-64 batch-53
Running loss of epoch-64 batch-53 = 2.016546204686165e-06

Training epoch-64 batch-54
Running loss of epoch-64 batch-54 = 1.5392433851957321e-06

Training epoch-64 batch-55
Running loss of epoch-64 batch-55 = 1.9443687051534653e-06

Training epoch-64 batch-56
Running loss of epoch-64 batch-56 = 1.653796061873436e-06

Training epoch-64 batch-57
Running loss of epoch-64 batch-57 = 1.4533288776874542e-06

Training epoch-64 batch-58
Running loss of epoch-64 batch-58 = 2.8382055461406708e-06

Training epoch-64 batch-59
Running loss of epoch-64 batch-59 = 2.4025794118642807e-06

Training epoch-64 batch-60
Running loss of epoch-64 batch-60 = 1.978827640414238e-06

Training epoch-64 batch-61
Running loss of epoch-64 batch-61 = 1.9066501408815384e-06

Training epoch-64 batch-62
Running loss of epoch-64 batch-62 = 1.7050188034772873e-06

Training epoch-64 batch-63
Running loss of epoch-64 batch-63 = 1.9085127860307693e-06

Training epoch-64 batch-64
Running loss of epoch-64 batch-64 = 2.2212043404579163e-06

Training epoch-64 batch-65
Running loss of epoch-64 batch-65 = 2.0209699869155884e-06

Training epoch-64 batch-66
Running loss of epoch-64 batch-66 = 1.2624077498912811e-06

Training epoch-64 batch-67
Running loss of epoch-64 batch-67 = 1.7203856259584427e-06

Training epoch-64 batch-68
Running loss of epoch-64 batch-68 = 1.6444828361272812e-06

Training epoch-64 batch-69
Running loss of epoch-64 batch-69 = 1.8831342458724976e-06

Training epoch-64 batch-70
Running loss of epoch-64 batch-70 = 1.5434343367815018e-06

Training epoch-64 batch-71
Running loss of epoch-64 batch-71 = 3.048451617360115e-06

Training epoch-64 batch-72
Running loss of epoch-64 batch-72 = 3.110850229859352e-06

Training epoch-64 batch-73
Running loss of epoch-64 batch-73 = 3.86103056371212e-06

Training epoch-64 batch-74
Running loss of epoch-64 batch-74 = 1.3317912817001343e-06

Training epoch-64 batch-75
Running loss of epoch-64 batch-75 = 2.963002771139145e-06

Training epoch-64 batch-76
Running loss of epoch-64 batch-76 = 2.647051587700844e-06

Training epoch-64 batch-77
Running loss of epoch-64 batch-77 = 1.9455328583717346e-06

Training epoch-64 batch-78
Running loss of epoch-64 batch-78 = 1.014210283756256e-06

Training epoch-64 batch-79
Running loss of epoch-64 batch-79 = 2.3881439119577408e-06

Training epoch-64 batch-80
Running loss of epoch-64 batch-80 = 1.9138678908348083e-06

Training epoch-64 batch-81
Running loss of epoch-64 batch-81 = 1.7695128917694092e-06

Training epoch-64 batch-82
Running loss of epoch-64 batch-82 = 2.4370383471250534e-06

Training epoch-64 batch-83
Running loss of epoch-64 batch-83 = 1.9462313503026962e-06

Training epoch-64 batch-84
Running loss of epoch-64 batch-84 = 2.368353307247162e-06

Training epoch-64 batch-85
Running loss of epoch-64 batch-85 = 2.2391323000192642e-06

Training epoch-64 batch-86
Running loss of epoch-64 batch-86 = 1.4924444258213043e-06

Training epoch-64 batch-87
Running loss of epoch-64 batch-87 = 2.227257937192917e-06

Training epoch-64 batch-88
Running loss of epoch-64 batch-88 = 1.893145963549614e-06

Training epoch-64 batch-89
Running loss of epoch-64 batch-89 = 2.48919241130352e-06

Training epoch-64 batch-90
Running loss of epoch-64 batch-90 = 2.1546147763729095e-06

Training epoch-64 batch-91
Running loss of epoch-64 batch-91 = 1.7064157873392105e-06

Training epoch-64 batch-92
Running loss of epoch-64 batch-92 = 2.2102613002061844e-06

Training epoch-64 batch-93
Running loss of epoch-64 batch-93 = 2.860790118575096e-06

Training epoch-64 batch-94
Running loss of epoch-64 batch-94 = 1.6745179891586304e-06

Training epoch-64 batch-95
Running loss of epoch-64 batch-95 = 1.271488144993782e-06

Training epoch-64 batch-96
Running loss of epoch-64 batch-96 = 2.0100269466638565e-06

Training epoch-64 batch-97
Running loss of epoch-64 batch-97 = 2.5834888219833374e-06

Training epoch-64 batch-98
Running loss of epoch-64 batch-98 = 1.567881554365158e-06

Training epoch-64 batch-99
Running loss of epoch-64 batch-99 = 1.4901161193847656e-06

Training epoch-64 batch-100
Running loss of epoch-64 batch-100 = 1.2957025319337845e-06

Training epoch-64 batch-101
Running loss of epoch-64 batch-101 = 1.9837170839309692e-06

Training epoch-64 batch-102
Running loss of epoch-64 batch-102 = 2.268236130475998e-06

Training epoch-64 batch-103
Running loss of epoch-64 batch-103 = 1.5248078852891922e-06

Training epoch-64 batch-104
Running loss of epoch-64 batch-104 = 1.6603153198957443e-06

Training epoch-64 batch-105
Running loss of epoch-64 batch-105 = 2.3988541215658188e-06

Training epoch-64 batch-106
Running loss of epoch-64 batch-106 = 1.8118880689144135e-06

Training epoch-64 batch-107
Running loss of epoch-64 batch-107 = 1.9101426005363464e-06

Training epoch-64 batch-108
Running loss of epoch-64 batch-108 = 2.4596229195594788e-06

Training epoch-64 batch-109
Running loss of epoch-64 batch-109 = 1.4186371117830276e-06

Training epoch-64 batch-110
Running loss of epoch-64 batch-110 = 2.1525193005800247e-06

Training epoch-64 batch-111
Running loss of epoch-64 batch-111 = 2.3408792912960052e-06

Training epoch-64 batch-112
Running loss of epoch-64 batch-112 = 2.7243513613939285e-06

Training epoch-64 batch-113
Running loss of epoch-64 batch-113 = 2.704095095396042e-06

Training epoch-64 batch-114
Running loss of epoch-64 batch-114 = 2.737622708082199e-06

Training epoch-64 batch-115
Running loss of epoch-64 batch-115 = 1.9529834389686584e-06

Training epoch-64 batch-116
Running loss of epoch-64 batch-116 = 2.07521952688694e-06

Training epoch-64 batch-117
Running loss of epoch-64 batch-117 = 3.0605588108301163e-06

Training epoch-64 batch-118
Running loss of epoch-64 batch-118 = 1.974869519472122e-06

Training epoch-64 batch-119
Running loss of epoch-64 batch-119 = 2.21841037273407e-06

Training epoch-64 batch-120
Running loss of epoch-64 batch-120 = 1.7534475773572922e-06

Training epoch-64 batch-121
Running loss of epoch-64 batch-121 = 1.4344695955514908e-06

Training epoch-64 batch-122
Running loss of epoch-64 batch-122 = 2.062646672129631e-06

Training epoch-64 batch-123
Running loss of epoch-64 batch-123 = 2.9490329325199127e-06

Training epoch-64 batch-124
Running loss of epoch-64 batch-124 = 1.8987338989973068e-06

Training epoch-64 batch-125
Running loss of epoch-64 batch-125 = 2.1944288164377213e-06

Training epoch-64 batch-126
Running loss of epoch-64 batch-126 = 1.4659017324447632e-06

Training epoch-64 batch-127
Running loss of epoch-64 batch-127 = 2.082670107483864e-06

Training epoch-64 batch-128
Running loss of epoch-64 batch-128 = 1.8130522221326828e-06

Training epoch-64 batch-129
Running loss of epoch-64 batch-129 = 1.6831327229738235e-06

Training epoch-64 batch-130
Running loss of epoch-64 batch-130 = 1.7313286662101746e-06

Training epoch-64 batch-131
Running loss of epoch-64 batch-131 = 2.6365742087364197e-06

Training epoch-64 batch-132
Running loss of epoch-64 batch-132 = 2.8512440621852875e-06

Training epoch-64 batch-133
Running loss of epoch-64 batch-133 = 2.7329660952091217e-06

Training epoch-64 batch-134
Running loss of epoch-64 batch-134 = 2.434011548757553e-06

Training epoch-64 batch-135
Running loss of epoch-64 batch-135 = 2.3711472749710083e-06

Training epoch-64 batch-136
Running loss of epoch-64 batch-136 = 1.923646777868271e-06

Training epoch-64 batch-137
Running loss of epoch-64 batch-137 = 1.505482941865921e-06

Training epoch-64 batch-138
Running loss of epoch-64 batch-138 = 1.909211277961731e-06

Training epoch-64 batch-139
Running loss of epoch-64 batch-139 = 2.2775493562221527e-06

Training epoch-64 batch-140
Running loss of epoch-64 batch-140 = 2.3189932107925415e-06

Training epoch-64 batch-141
Running loss of epoch-64 batch-141 = 1.9867438822984695e-06

Training epoch-64 batch-142
Running loss of epoch-64 batch-142 = 7.899943739175797e-07

Training epoch-64 batch-143
Running loss of epoch-64 batch-143 = 1.935986801981926e-06

Training epoch-64 batch-144
Running loss of epoch-64 batch-144 = 1.6507692635059357e-06

Training epoch-64 batch-145
Running loss of epoch-64 batch-145 = 2.0079314708709717e-06

Training epoch-64 batch-146
Running loss of epoch-64 batch-146 = 2.2617168724536896e-06

Training epoch-64 batch-147
Running loss of epoch-64 batch-147 = 3.2496172934770584e-06

Training epoch-64 batch-148
Running loss of epoch-64 batch-148 = 1.6568228602409363e-06

Training epoch-64 batch-149
Running loss of epoch-64 batch-149 = 1.660780981183052e-06

Training epoch-64 batch-150
Running loss of epoch-64 batch-150 = 1.6049016267061234e-06

Training epoch-64 batch-151
Running loss of epoch-64 batch-151 = 2.7550850063562393e-06

Training epoch-64 batch-152
Running loss of epoch-64 batch-152 = 3.019813448190689e-06

Training epoch-64 batch-153
Running loss of epoch-64 batch-153 = 3.509223461151123e-06

Training epoch-64 batch-154
Running loss of epoch-64 batch-154 = 2.675224095582962e-06

Training epoch-64 batch-155
Running loss of epoch-64 batch-155 = 1.971609890460968e-06

Training epoch-64 batch-156
Running loss of epoch-64 batch-156 = 1.373467966914177e-06

Training epoch-64 batch-157
Running loss of epoch-64 batch-157 = 8.843839168548584e-06

Finished training epoch-64.



Average train loss at epoch-64 = 2.0335957407951353e-06

Started Evaluation

Average val loss at epoch-64 = 3.1961191704398706

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.28 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.43 %

Finished Evaluation



Started training epoch-65


Training epoch-65 batch-1
Running loss of epoch-65 batch-1 = 1.921318471431732e-06

Training epoch-65 batch-2
Running loss of epoch-65 batch-2 = 1.3702083379030228e-06

Training epoch-65 batch-3
Running loss of epoch-65 batch-3 = 2.0365696400403976e-06

Training epoch-65 batch-4
Running loss of epoch-65 batch-4 = 2.25752592086792e-06

Training epoch-65 batch-5
Running loss of epoch-65 batch-5 = 1.9031576812267303e-06

Training epoch-65 batch-6
Running loss of epoch-65 batch-6 = 2.488028258085251e-06

Training epoch-65 batch-7
Running loss of epoch-65 batch-7 = 1.5825498849153519e-06

Training epoch-65 batch-8
Running loss of epoch-65 batch-8 = 1.921551302075386e-06

Training epoch-65 batch-9
Running loss of epoch-65 batch-9 = 1.7869751900434494e-06

Training epoch-65 batch-10
Running loss of epoch-65 batch-10 = 1.6004778444766998e-06

Training epoch-65 batch-11
Running loss of epoch-65 batch-11 = 1.618172973394394e-06

Training epoch-65 batch-12
Running loss of epoch-65 batch-12 = 2.0239967852830887e-06

Training epoch-65 batch-13
Running loss of epoch-65 batch-13 = 1.4898832887411118e-06

Training epoch-65 batch-14
Running loss of epoch-65 batch-14 = 1.5583354979753494e-06

Training epoch-65 batch-15
Running loss of epoch-65 batch-15 = 2.427026629447937e-06

Training epoch-65 batch-16
Running loss of epoch-65 batch-16 = 2.5480985641479492e-06

Training epoch-65 batch-17
Running loss of epoch-65 batch-17 = 1.8891878426074982e-06

Training epoch-65 batch-18
Running loss of epoch-65 batch-18 = 3.3651012927293777e-06

Training epoch-65 batch-19
Running loss of epoch-65 batch-19 = 1.900363713502884e-06

Training epoch-65 batch-20
Running loss of epoch-65 batch-20 = 2.882443368434906e-06

Training epoch-65 batch-21
Running loss of epoch-65 batch-21 = 3.4319236874580383e-06

Training epoch-65 batch-22
Running loss of epoch-65 batch-22 = 2.023763954639435e-06

Training epoch-65 batch-23
Running loss of epoch-65 batch-23 = 1.989537850022316e-06

Training epoch-65 batch-24
Running loss of epoch-65 batch-24 = 2.33575701713562e-06

Training epoch-65 batch-25
Running loss of epoch-65 batch-25 = 2.152286469936371e-06

Training epoch-65 batch-26
Running loss of epoch-65 batch-26 = 1.8242280930280685e-06

Training epoch-65 batch-27
Running loss of epoch-65 batch-27 = 2.440996468067169e-06

Training epoch-65 batch-28
Running loss of epoch-65 batch-28 = 1.0454095900058746e-06

Training epoch-65 batch-29
Running loss of epoch-65 batch-29 = 1.7746351659297943e-06

Training epoch-65 batch-30
Running loss of epoch-65 batch-30 = 2.5348272174596786e-06

Training epoch-65 batch-31
Running loss of epoch-65 batch-31 = 1.648440957069397e-06

Training epoch-65 batch-32
Running loss of epoch-65 batch-32 = 2.3739412426948547e-06

Training epoch-65 batch-33
Running loss of epoch-65 batch-33 = 3.0247028917074203e-06

Training epoch-65 batch-34
Running loss of epoch-65 batch-34 = 2.2691674530506134e-06

Training epoch-65 batch-35
Running loss of epoch-65 batch-35 = 2.7620699256658554e-06

Training epoch-65 batch-36
Running loss of epoch-65 batch-36 = 1.8666032701730728e-06

Training epoch-65 batch-37
Running loss of epoch-65 batch-37 = 2.4579931050539017e-06

Training epoch-65 batch-38
Running loss of epoch-65 batch-38 = 2.1473970264196396e-06

Training epoch-65 batch-39
Running loss of epoch-65 batch-39 = 2.5676563382148743e-06

Training epoch-65 batch-40
Running loss of epoch-65 batch-40 = 3.4335535019636154e-06

Training epoch-65 batch-41
Running loss of epoch-65 batch-41 = 1.4898832887411118e-06

Training epoch-65 batch-42
Running loss of epoch-65 batch-42 = 3.2729003578424454e-06

Training epoch-65 batch-43
Running loss of epoch-65 batch-43 = 1.4451798051595688e-06

Training epoch-65 batch-44
Running loss of epoch-65 batch-44 = 1.6398262232542038e-06

Training epoch-65 batch-45
Running loss of epoch-65 batch-45 = 1.698266714811325e-06

Training epoch-65 batch-46
Running loss of epoch-65 batch-46 = 1.7201527953147888e-06

Training epoch-65 batch-47
Running loss of epoch-65 batch-47 = 2.0652078092098236e-06

Training epoch-65 batch-48
Running loss of epoch-65 batch-48 = 2.0440202206373215e-06

Training epoch-65 batch-49
Running loss of epoch-65 batch-49 = 1.850072294473648e-06

Training epoch-65 batch-50
Running loss of epoch-65 batch-50 = 1.5473924577236176e-06

Training epoch-65 batch-51
Running loss of epoch-65 batch-51 = 1.6151461750268936e-06

Training epoch-65 batch-52
Running loss of epoch-65 batch-52 = 1.416075974702835e-06

Training epoch-65 batch-53
Running loss of epoch-65 batch-53 = 1.600012183189392e-06

Training epoch-65 batch-54
Running loss of epoch-65 batch-54 = 2.4212058633565903e-06

Training epoch-65 batch-55
Running loss of epoch-65 batch-55 = 2.2153835743665695e-06

Training epoch-65 batch-56
Running loss of epoch-65 batch-56 = 1.937849447131157e-06

Training epoch-65 batch-57
Running loss of epoch-65 batch-57 = 2.6032794266939163e-06

Training epoch-65 batch-58
Running loss of epoch-65 batch-58 = 1.9443687051534653e-06

Training epoch-65 batch-59
Running loss of epoch-65 batch-59 = 1.8617138266563416e-06

Training epoch-65 batch-60
Running loss of epoch-65 batch-60 = 2.1741725504398346e-06

Training epoch-65 batch-61
Running loss of epoch-65 batch-61 = 2.0901206880807877e-06

Training epoch-65 batch-62
Running loss of epoch-65 batch-62 = 2.023531123995781e-06

Training epoch-65 batch-63
Running loss of epoch-65 batch-63 = 2.7369242161512375e-06

Training epoch-65 batch-64
Running loss of epoch-65 batch-64 = 2.1567102521657944e-06

Training epoch-65 batch-65
Running loss of epoch-65 batch-65 = 1.841224730014801e-06

Training epoch-65 batch-66
Running loss of epoch-65 batch-66 = 2.0673032850027084e-06

Training epoch-65 batch-67
Running loss of epoch-65 batch-67 = 2.505956217646599e-06

Training epoch-65 batch-68
Running loss of epoch-65 batch-68 = 2.271728590130806e-06

Training epoch-65 batch-69
Running loss of epoch-65 batch-69 = 1.1790543794631958e-06

Training epoch-65 batch-70
Running loss of epoch-65 batch-70 = 1.0917428880929947e-06

Training epoch-65 batch-71
Running loss of epoch-65 batch-71 = 2.435874193906784e-06

Training epoch-65 batch-72
Running loss of epoch-65 batch-72 = 2.735760062932968e-06

Training epoch-65 batch-73
Running loss of epoch-65 batch-73 = 2.3818574845790863e-06

Training epoch-65 batch-74
Running loss of epoch-65 batch-74 = 2.353684976696968e-06

Training epoch-65 batch-75
Running loss of epoch-65 batch-75 = 1.6526319086551666e-06

Training epoch-65 batch-76
Running loss of epoch-65 batch-76 = 1.4700926840305328e-06

Training epoch-65 batch-77
Running loss of epoch-65 batch-77 = 1.5660189092159271e-06

Training epoch-65 batch-78
Running loss of epoch-65 batch-78 = 2.3513566702604294e-06

Training epoch-65 batch-79
Running loss of epoch-65 batch-79 = 1.5718396753072739e-06

Training epoch-65 batch-80
Running loss of epoch-65 batch-80 = 1.7604324966669083e-06

Training epoch-65 batch-81
Running loss of epoch-65 batch-81 = 1.6095582395792007e-06

Training epoch-65 batch-82
Running loss of epoch-65 batch-82 = 2.623535692691803e-06

Training epoch-65 batch-83
Running loss of epoch-65 batch-83 = 2.535758540034294e-06

Training epoch-65 batch-84
Running loss of epoch-65 batch-84 = 1.925276592373848e-06

Training epoch-65 batch-85
Running loss of epoch-65 batch-85 = 1.9073486328125e-06

Training epoch-65 batch-86
Running loss of epoch-65 batch-86 = 1.5480909496545792e-06

Training epoch-65 batch-87
Running loss of epoch-65 batch-87 = 2.4458859115839005e-06

Training epoch-65 batch-88
Running loss of epoch-65 batch-88 = 2.075452357530594e-06

Training epoch-65 batch-89
Running loss of epoch-65 batch-89 = 1.530628651380539e-06

Training epoch-65 batch-90
Running loss of epoch-65 batch-90 = 2.3925676941871643e-06

Training epoch-65 batch-91
Running loss of epoch-65 batch-91 = 1.4598481357097626e-06

Training epoch-65 batch-92
Running loss of epoch-65 batch-92 = 9.55536961555481e-07

Training epoch-65 batch-93
Running loss of epoch-65 batch-93 = 1.4693941920995712e-06

Training epoch-65 batch-94
Running loss of epoch-65 batch-94 = 1.3939570635557175e-06

Training epoch-65 batch-95
Running loss of epoch-65 batch-95 = 1.6356352716684341e-06

Training epoch-65 batch-96
Running loss of epoch-65 batch-96 = 2.7348287403583527e-06

Training epoch-65 batch-97
Running loss of epoch-65 batch-97 = 1.2551899999380112e-06

Training epoch-65 batch-98
Running loss of epoch-65 batch-98 = 1.926906406879425e-06

Training epoch-65 batch-99
Running loss of epoch-65 batch-99 = 1.4789402484893799e-06

Training epoch-65 batch-100
Running loss of epoch-65 batch-100 = 1.6510020941495895e-06

Training epoch-65 batch-101
Running loss of epoch-65 batch-101 = 2.1457672119140625e-06

Training epoch-65 batch-102
Running loss of epoch-65 batch-102 = 1.68592669069767e-06

Training epoch-65 batch-103
Running loss of epoch-65 batch-103 = 2.7867499738931656e-06

Training epoch-65 batch-104
Running loss of epoch-65 batch-104 = 1.5976838767528534e-06

Training epoch-65 batch-105
Running loss of epoch-65 batch-105 = 2.245185896754265e-06

Training epoch-65 batch-106
Running loss of epoch-65 batch-106 = 2.751825377345085e-06

Training epoch-65 batch-107
Running loss of epoch-65 batch-107 = 2.271728590130806e-06

Training epoch-65 batch-108
Running loss of epoch-65 batch-108 = 1.46450474858284e-06

Training epoch-65 batch-109
Running loss of epoch-65 batch-109 = 1.5348196029663086e-06

Training epoch-65 batch-110
Running loss of epoch-65 batch-110 = 1.917826011776924e-06

Training epoch-65 batch-111
Running loss of epoch-65 batch-111 = 1.5425030142068863e-06

Training epoch-65 batch-112
Running loss of epoch-65 batch-112 = 2.582324668765068e-06

Training epoch-65 batch-113
Running loss of epoch-65 batch-113 = 1.7886050045490265e-06

Training epoch-65 batch-114
Running loss of epoch-65 batch-114 = 2.3888424038887024e-06

Training epoch-65 batch-115
Running loss of epoch-65 batch-115 = 2.0514708012342453e-06

Training epoch-65 batch-116
Running loss of epoch-65 batch-116 = 1.6521662473678589e-06

Training epoch-65 batch-117
Running loss of epoch-65 batch-117 = 1.910608261823654e-06

Training epoch-65 batch-118
Running loss of epoch-65 batch-118 = 1.1473894119262695e-06

Training epoch-65 batch-119
Running loss of epoch-65 batch-119 = 2.0994339138269424e-06

Training epoch-65 batch-120
Running loss of epoch-65 batch-120 = 1.975568011403084e-06

Training epoch-65 batch-121
Running loss of epoch-65 batch-121 = 1.6419216990470886e-06

Training epoch-65 batch-122
Running loss of epoch-65 batch-122 = 2.419808879494667e-06

Training epoch-65 batch-123
Running loss of epoch-65 batch-123 = 1.2882519513368607e-06

Training epoch-65 batch-124
Running loss of epoch-65 batch-124 = 2.7851201593875885e-06

Training epoch-65 batch-125
Running loss of epoch-65 batch-125 = 1.877080649137497e-06

Training epoch-65 batch-126
Running loss of epoch-65 batch-126 = 1.6787089407444e-06

Training epoch-65 batch-127
Running loss of epoch-65 batch-127 = 2.1369196474552155e-06

Training epoch-65 batch-128
Running loss of epoch-65 batch-128 = 2.5068875402212143e-06

Training epoch-65 batch-129
Running loss of epoch-65 batch-129 = 1.760898157954216e-06

Training epoch-65 batch-130
Running loss of epoch-65 batch-130 = 2.3934990167617798e-06

Training epoch-65 batch-131
Running loss of epoch-65 batch-131 = 1.5639234334230423e-06

Training epoch-65 batch-132
Running loss of epoch-65 batch-132 = 1.2640375643968582e-06

Training epoch-65 batch-133
Running loss of epoch-65 batch-133 = 1.5106052160263062e-06

Training epoch-65 batch-134
Running loss of epoch-65 batch-134 = 1.566251739859581e-06

Training epoch-65 batch-135
Running loss of epoch-65 batch-135 = 1.4468096196651459e-06

Training epoch-65 batch-136
Running loss of epoch-65 batch-136 = 1.7236452549695969e-06

Training epoch-65 batch-137
Running loss of epoch-65 batch-137 = 1.794658601284027e-06

Training epoch-65 batch-138
Running loss of epoch-65 batch-138 = 2.587912604212761e-06

Training epoch-65 batch-139
Running loss of epoch-65 batch-139 = 9.220093488693237e-07

Training epoch-65 batch-140
Running loss of epoch-65 batch-140 = 2.3988541215658188e-06

Training epoch-65 batch-141
Running loss of epoch-65 batch-141 = 1.7320271581411362e-06

Training epoch-65 batch-142
Running loss of epoch-65 batch-142 = 1.7632264643907547e-06

Training epoch-65 batch-143
Running loss of epoch-65 batch-143 = 1.4042016118764877e-06

Training epoch-65 batch-144
Running loss of epoch-65 batch-144 = 1.2756790965795517e-06

Training epoch-65 batch-145
Running loss of epoch-65 batch-145 = 1.6195699572563171e-06

Training epoch-65 batch-146
Running loss of epoch-65 batch-146 = 1.9012950360774994e-06

Training epoch-65 batch-147
Running loss of epoch-65 batch-147 = 1.9869767129421234e-06

Training epoch-65 batch-148
Running loss of epoch-65 batch-148 = 2.0156148821115494e-06

Training epoch-65 batch-149
Running loss of epoch-65 batch-149 = 2.171844244003296e-06

Training epoch-65 batch-150
Running loss of epoch-65 batch-150 = 2.377200871706009e-06

Training epoch-65 batch-151
Running loss of epoch-65 batch-151 = 1.93621963262558e-06

Training epoch-65 batch-152
Running loss of epoch-65 batch-152 = 2.041459083557129e-06

Training epoch-65 batch-153
Running loss of epoch-65 batch-153 = 2.0426232367753983e-06

Training epoch-65 batch-154
Running loss of epoch-65 batch-154 = 1.3047829270362854e-06

Training epoch-65 batch-155
Running loss of epoch-65 batch-155 = 2.428656443953514e-06

Training epoch-65 batch-156
Running loss of epoch-65 batch-156 = 2.15042382478714e-06

Training epoch-65 batch-157
Running loss of epoch-65 batch-157 = 6.660819053649902e-06

Finished training epoch-65.



Average train loss at epoch-65 = 1.9882425665855407e-06

Started Evaluation

Average val loss at epoch-65 = 3.2030826697224066

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.77 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-66


Training epoch-66 batch-1
Running loss of epoch-66 batch-1 = 1.8316786736249924e-06

Training epoch-66 batch-2
Running loss of epoch-66 batch-2 = 1.8223654478788376e-06

Training epoch-66 batch-3
Running loss of epoch-66 batch-3 = 1.61072239279747e-06

Training epoch-66 batch-4
Running loss of epoch-66 batch-4 = 1.1941883713006973e-06

Training epoch-66 batch-5
Running loss of epoch-66 batch-5 = 1.394655555486679e-06

Training epoch-66 batch-6
Running loss of epoch-66 batch-6 = 1.8486753106117249e-06

Training epoch-66 batch-7
Running loss of epoch-66 batch-7 = 1.4954712241888046e-06

Training epoch-66 batch-8
Running loss of epoch-66 batch-8 = 1.3725366443395615e-06

Training epoch-66 batch-9
Running loss of epoch-66 batch-9 = 2.293149009346962e-06

Training epoch-66 batch-10
Running loss of epoch-66 batch-10 = 2.5867484509944916e-06

Training epoch-66 batch-11
Running loss of epoch-66 batch-11 = 1.4794059097766876e-06

Training epoch-66 batch-12
Running loss of epoch-66 batch-12 = 1.692911610007286e-06

Training epoch-66 batch-13
Running loss of epoch-66 batch-13 = 1.970212906599045e-06

Training epoch-66 batch-14
Running loss of epoch-66 batch-14 = 1.9655562937259674e-06

Training epoch-66 batch-15
Running loss of epoch-66 batch-15 = 2.298736944794655e-06

Training epoch-66 batch-16
Running loss of epoch-66 batch-16 = 1.7937272787094116e-06

Training epoch-66 batch-17
Running loss of epoch-66 batch-17 = 2.848450094461441e-06

Training epoch-66 batch-18
Running loss of epoch-66 batch-18 = 2.0959414541721344e-06

Training epoch-66 batch-19
Running loss of epoch-66 batch-19 = 1.9827857613563538e-06

Training epoch-66 batch-20
Running loss of epoch-66 batch-20 = 1.735752448439598e-06

Training epoch-66 batch-21
Running loss of epoch-66 batch-21 = 1.6635749489068985e-06

Training epoch-66 batch-22
Running loss of epoch-66 batch-22 = 1.7629936337471008e-06

Training epoch-66 batch-23
Running loss of epoch-66 batch-23 = 2.377200871706009e-06

Training epoch-66 batch-24
Running loss of epoch-66 batch-24 = 2.175569534301758e-06

Training epoch-66 batch-25
Running loss of epoch-66 batch-25 = 1.5047844499349594e-06

Training epoch-66 batch-26
Running loss of epoch-66 batch-26 = 2.0477455109357834e-06

Training epoch-66 batch-27
Running loss of epoch-66 batch-27 = 2.37184576690197e-06

Training epoch-66 batch-28
Running loss of epoch-66 batch-28 = 1.1778902262449265e-06

Training epoch-66 batch-29
Running loss of epoch-66 batch-29 = 1.3760291039943695e-06

Training epoch-66 batch-30
Running loss of epoch-66 batch-30 = 2.5932677090168e-06

Training epoch-66 batch-31
Running loss of epoch-66 batch-31 = 2.695247530937195e-06

Training epoch-66 batch-32
Running loss of epoch-66 batch-32 = 2.1739397197961807e-06

Training epoch-66 batch-33
Running loss of epoch-66 batch-33 = 1.8563587218523026e-06

Training epoch-66 batch-34
Running loss of epoch-66 batch-34 = 2.10711732506752e-06

Training epoch-66 batch-35
Running loss of epoch-66 batch-35 = 2.4372711777687073e-06

Training epoch-66 batch-36
Running loss of epoch-66 batch-36 = 1.3292301446199417e-06

Training epoch-66 batch-37
Running loss of epoch-66 batch-37 = 2.4351757019758224e-06

Training epoch-66 batch-38
Running loss of epoch-66 batch-38 = 1.8368009477853775e-06

Training epoch-66 batch-39
Running loss of epoch-66 batch-39 = 2.223299816250801e-06

Training epoch-66 batch-40
Running loss of epoch-66 batch-40 = 1.4123506844043732e-06

Training epoch-66 batch-41
Running loss of epoch-66 batch-41 = 1.2568198144435883e-06

Training epoch-66 batch-42
Running loss of epoch-66 batch-42 = 1.5515834093093872e-06

Training epoch-66 batch-43
Running loss of epoch-66 batch-43 = 1.3511162251234055e-06

Training epoch-66 batch-44
Running loss of epoch-66 batch-44 = 1.53249129652977e-06

Training epoch-66 batch-45
Running loss of epoch-66 batch-45 = 2.2561289370059967e-06

Training epoch-66 batch-46
Running loss of epoch-66 batch-46 = 1.2496020644903183e-06

Training epoch-66 batch-47
Running loss of epoch-66 batch-47 = 1.1937227100133896e-06

Training epoch-66 batch-48
Running loss of epoch-66 batch-48 = 1.4044344425201416e-06

Training epoch-66 batch-49
Running loss of epoch-66 batch-49 = 2.6999041438102722e-06

Training epoch-66 batch-50
Running loss of epoch-66 batch-50 = 2.1997839212417603e-06

Training epoch-66 batch-51
Running loss of epoch-66 batch-51 = 1.559033989906311e-06

Training epoch-66 batch-52
Running loss of epoch-66 batch-52 = 2.948567271232605e-06

Training epoch-66 batch-53
Running loss of epoch-66 batch-53 = 1.552049070596695e-06

Training epoch-66 batch-54
Running loss of epoch-66 batch-54 = 1.627020537853241e-06

Training epoch-66 batch-55
Running loss of epoch-66 batch-55 = 2.126675099134445e-06

Training epoch-66 batch-56
Running loss of epoch-66 batch-56 = 1.5345867723226547e-06

Training epoch-66 batch-57
Running loss of epoch-66 batch-57 = 1.9692815840244293e-06

Training epoch-66 batch-58
Running loss of epoch-66 batch-58 = 3.0081719160079956e-06

Training epoch-66 batch-59
Running loss of epoch-66 batch-59 = 2.6689376682043076e-06

Training epoch-66 batch-60
Running loss of epoch-66 batch-60 = 1.6363337635993958e-06

Training epoch-66 batch-61
Running loss of epoch-66 batch-61 = 1.8724240362644196e-06

Training epoch-66 batch-62
Running loss of epoch-66 batch-62 = 1.5457626432180405e-06

Training epoch-66 batch-63
Running loss of epoch-66 batch-63 = 2.298504114151001e-06

Training epoch-66 batch-64
Running loss of epoch-66 batch-64 = 2.0975712686777115e-06

Training epoch-66 batch-65
Running loss of epoch-66 batch-65 = 1.8891878426074982e-06

Training epoch-66 batch-66
Running loss of epoch-66 batch-66 = 1.7671845853328705e-06

Training epoch-66 batch-67
Running loss of epoch-66 batch-67 = 2.0633451640605927e-06

Training epoch-66 batch-68
Running loss of epoch-66 batch-68 = 1.9490253180265427e-06

Training epoch-66 batch-69
Running loss of epoch-66 batch-69 = 1.9692815840244293e-06

Training epoch-66 batch-70
Running loss of epoch-66 batch-70 = 2.1811574697494507e-06

Training epoch-66 batch-71
Running loss of epoch-66 batch-71 = 1.8081627786159515e-06

Training epoch-66 batch-72
Running loss of epoch-66 batch-72 = 2.4209730327129364e-06

Training epoch-66 batch-73
Running loss of epoch-66 batch-73 = 2.2046733647584915e-06

Training epoch-66 batch-74
Running loss of epoch-66 batch-74 = 1.0384246706962585e-06

Training epoch-66 batch-75
Running loss of epoch-66 batch-75 = 2.4514738470315933e-06

Training epoch-66 batch-76
Running loss of epoch-66 batch-76 = 2.150190994143486e-06

Training epoch-66 batch-77
Running loss of epoch-66 batch-77 = 1.7874408513307571e-06

Training epoch-66 batch-78
Running loss of epoch-66 batch-78 = 2.268003299832344e-06

Training epoch-66 batch-79
Running loss of epoch-66 batch-79 = 1.3147946447134018e-06

Training epoch-66 batch-80
Running loss of epoch-66 batch-80 = 2.8442591428756714e-06

Training epoch-66 batch-81
Running loss of epoch-66 batch-81 = 2.402346581220627e-06

Training epoch-66 batch-82
Running loss of epoch-66 batch-82 = 1.6186386346817017e-06

Training epoch-66 batch-83
Running loss of epoch-66 batch-83 = 1.9194558262825012e-06

Training epoch-66 batch-84
Running loss of epoch-66 batch-84 = 2.9355287551879883e-06

Training epoch-66 batch-85
Running loss of epoch-66 batch-85 = 1.3939570635557175e-06

Training epoch-66 batch-86
Running loss of epoch-66 batch-86 = 1.8256250768899918e-06

Training epoch-66 batch-87
Running loss of epoch-66 batch-87 = 1.8756836652755737e-06

Training epoch-66 batch-88
Running loss of epoch-66 batch-88 = 2.2633466869592667e-06

Training epoch-66 batch-89
Running loss of epoch-66 batch-89 = 1.6419216990470886e-06

Training epoch-66 batch-90
Running loss of epoch-66 batch-90 = 1.9653234630823135e-06

Training epoch-66 batch-91
Running loss of epoch-66 batch-91 = 1.5243422240018845e-06

Training epoch-66 batch-92
Running loss of epoch-66 batch-92 = 1.5683472156524658e-06

Training epoch-66 batch-93
Running loss of epoch-66 batch-93 = 3.1962990760803223e-06

Training epoch-66 batch-94
Running loss of epoch-66 batch-94 = 1.8705613911151886e-06

Training epoch-66 batch-95
Running loss of epoch-66 batch-95 = 1.8633436411619186e-06

Training epoch-66 batch-96
Running loss of epoch-66 batch-96 = 1.96811743080616e-06

Training epoch-66 batch-97
Running loss of epoch-66 batch-97 = 1.6402918845415115e-06

Training epoch-66 batch-98
Running loss of epoch-66 batch-98 = 1.7171259969472885e-06

Training epoch-66 batch-99
Running loss of epoch-66 batch-99 = 3.1527597457170486e-06

Training epoch-66 batch-100
Running loss of epoch-66 batch-100 = 1.9920989871025085e-06

Training epoch-66 batch-101
Running loss of epoch-66 batch-101 = 2.378830686211586e-06

Training epoch-66 batch-102
Running loss of epoch-66 batch-102 = 2.5802291929721832e-06

Training epoch-66 batch-103
Running loss of epoch-66 batch-103 = 2.3029278963804245e-06

Training epoch-66 batch-104
Running loss of epoch-66 batch-104 = 1.8856953829526901e-06

Training epoch-66 batch-105
Running loss of epoch-66 batch-105 = 2.3515895009040833e-06

Training epoch-66 batch-106
Running loss of epoch-66 batch-106 = 1.9185245037078857e-06

Training epoch-66 batch-107
Running loss of epoch-66 batch-107 = 1.6789417713880539e-06

Training epoch-66 batch-108
Running loss of epoch-66 batch-108 = 1.4898832887411118e-06

Training epoch-66 batch-109
Running loss of epoch-66 batch-109 = 2.276850864291191e-06

Training epoch-66 batch-110
Running loss of epoch-66 batch-110 = 1.3918615877628326e-06

Training epoch-66 batch-111
Running loss of epoch-66 batch-111 = 1.2784730643033981e-06

Training epoch-66 batch-112
Running loss of epoch-66 batch-112 = 1.1364463716745377e-06

Training epoch-66 batch-113
Running loss of epoch-66 batch-113 = 1.6426201909780502e-06

Training epoch-66 batch-114
Running loss of epoch-66 batch-114 = 1.757172867655754e-06

Training epoch-66 batch-115
Running loss of epoch-66 batch-115 = 1.610955223441124e-06

Training epoch-66 batch-116
Running loss of epoch-66 batch-116 = 2.0889565348625183e-06

Training epoch-66 batch-117
Running loss of epoch-66 batch-117 = 2.278713509440422e-06

Training epoch-66 batch-118
Running loss of epoch-66 batch-118 = 2.721790224313736e-06

Training epoch-66 batch-119
Running loss of epoch-66 batch-119 = 1.3771932572126389e-06

Training epoch-66 batch-120
Running loss of epoch-66 batch-120 = 2.523185685276985e-06

Training epoch-66 batch-121
Running loss of epoch-66 batch-121 = 2.073589712381363e-06

Training epoch-66 batch-122
Running loss of epoch-66 batch-122 = 1.5115365386009216e-06

Training epoch-66 batch-123
Running loss of epoch-66 batch-123 = 2.4547334760427475e-06

Training epoch-66 batch-124
Running loss of epoch-66 batch-124 = 1.8300488591194153e-06

Training epoch-66 batch-125
Running loss of epoch-66 batch-125 = 1.864507794380188e-06

Training epoch-66 batch-126
Running loss of epoch-66 batch-126 = 1.417938619852066e-06

Training epoch-66 batch-127
Running loss of epoch-66 batch-127 = 2.1960586309432983e-06

Training epoch-66 batch-128
Running loss of epoch-66 batch-128 = 1.6642734408378601e-06

Training epoch-66 batch-129
Running loss of epoch-66 batch-129 = 1.4526303857564926e-06

Training epoch-66 batch-130
Running loss of epoch-66 batch-130 = 2.039363607764244e-06

Training epoch-66 batch-131
Running loss of epoch-66 batch-131 = 1.6996636986732483e-06

Training epoch-66 batch-132
Running loss of epoch-66 batch-132 = 2.4151522666215897e-06

Training epoch-66 batch-133
Running loss of epoch-66 batch-133 = 2.639833837747574e-06

Training epoch-66 batch-134
Running loss of epoch-66 batch-134 = 1.919921487569809e-06

Training epoch-66 batch-135
Running loss of epoch-66 batch-135 = 1.698266714811325e-06

Training epoch-66 batch-136
Running loss of epoch-66 batch-136 = 1.671724021434784e-06

Training epoch-66 batch-137
Running loss of epoch-66 batch-137 = 2.050306648015976e-06

Training epoch-66 batch-138
Running loss of epoch-66 batch-138 = 1.7471611499786377e-06

Training epoch-66 batch-139
Running loss of epoch-66 batch-139 = 1.8023420125246048e-06

Training epoch-66 batch-140
Running loss of epoch-66 batch-140 = 1.5071127563714981e-06

Training epoch-66 batch-141
Running loss of epoch-66 batch-141 = 2.0009465515613556e-06

Training epoch-66 batch-142
Running loss of epoch-66 batch-142 = 2.346700057387352e-06

Training epoch-66 batch-143
Running loss of epoch-66 batch-143 = 1.6621779650449753e-06

Training epoch-66 batch-144
Running loss of epoch-66 batch-144 = 1.1795200407505035e-06

Training epoch-66 batch-145
Running loss of epoch-66 batch-145 = 1.8246937543153763e-06

Training epoch-66 batch-146
Running loss of epoch-66 batch-146 = 1.9774306565523148e-06

Training epoch-66 batch-147
Running loss of epoch-66 batch-147 = 1.7096754163503647e-06

Training epoch-66 batch-148
Running loss of epoch-66 batch-148 = 2.491055056452751e-06

Training epoch-66 batch-149
Running loss of epoch-66 batch-149 = 2.9332004487514496e-06

Training epoch-66 batch-150
Running loss of epoch-66 batch-150 = 1.4684628695249557e-06

Training epoch-66 batch-151
Running loss of epoch-66 batch-151 = 1.498498022556305e-06

Training epoch-66 batch-152
Running loss of epoch-66 batch-152 = 1.7313286662101746e-06

Training epoch-66 batch-153
Running loss of epoch-66 batch-153 = 2.3317988961935043e-06

Training epoch-66 batch-154
Running loss of epoch-66 batch-154 = 1.541106030344963e-06

Training epoch-66 batch-155
Running loss of epoch-66 batch-155 = 2.6221387088298798e-06

Training epoch-66 batch-156
Running loss of epoch-66 batch-156 = 2.3443717509508133e-06

Training epoch-66 batch-157
Running loss of epoch-66 batch-157 = 9.18656587600708e-06

Finished training epoch-66.



Average train loss at epoch-66 = 1.950344443321228e-06

Started Evaluation

Average val loss at epoch-66 = 3.2057875410506598

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-67


Training epoch-67 batch-1
Running loss of epoch-67 batch-1 = 1.930864527821541e-06

Training epoch-67 batch-2
Running loss of epoch-67 batch-2 = 1.8328428268432617e-06

Training epoch-67 batch-3
Running loss of epoch-67 batch-3 = 2.145068719983101e-06

Training epoch-67 batch-4
Running loss of epoch-67 batch-4 = 2.5453045964241028e-06

Training epoch-67 batch-5
Running loss of epoch-67 batch-5 = 3.297114744782448e-06

Training epoch-67 batch-6
Running loss of epoch-67 batch-6 = 1.8400605767965317e-06

Training epoch-67 batch-7
Running loss of epoch-67 batch-7 = 1.3790559023618698e-06

Training epoch-67 batch-8
Running loss of epoch-67 batch-8 = 2.05053947865963e-06

Training epoch-67 batch-9
Running loss of epoch-67 batch-9 = 1.1760275810956955e-06

Training epoch-67 batch-10
Running loss of epoch-67 batch-10 = 1.1171214282512665e-06

Training epoch-67 batch-11
Running loss of epoch-67 batch-11 = 1.8354039639234543e-06

Training epoch-67 batch-12
Running loss of epoch-67 batch-12 = 2.647051587700844e-06

Training epoch-67 batch-13
Running loss of epoch-67 batch-13 = 1.532258465886116e-06

Training epoch-67 batch-14
Running loss of epoch-67 batch-14 = 2.0274892449378967e-06

Training epoch-67 batch-15
Running loss of epoch-67 batch-15 = 2.142740413546562e-06

Training epoch-67 batch-16
Running loss of epoch-67 batch-16 = 2.6652123779058456e-06

Training epoch-67 batch-17
Running loss of epoch-67 batch-17 = 1.9904691725969315e-06

Training epoch-67 batch-18
Running loss of epoch-67 batch-18 = 2.109445631504059e-06

Training epoch-67 batch-19
Running loss of epoch-67 batch-19 = 2.5185290724039078e-06

Training epoch-67 batch-20
Running loss of epoch-67 batch-20 = 1.559266820549965e-06

Training epoch-67 batch-21
Running loss of epoch-67 batch-21 = 1.743203029036522e-06

Training epoch-67 batch-22
Running loss of epoch-67 batch-22 = 1.7639249563217163e-06

Training epoch-67 batch-23
Running loss of epoch-67 batch-23 = 2.0924489945173264e-06

Training epoch-67 batch-24
Running loss of epoch-67 batch-24 = 2.0295847207307816e-06

Training epoch-67 batch-25
Running loss of epoch-67 batch-25 = 2.0042061805725098e-06

Training epoch-67 batch-26
Running loss of epoch-67 batch-26 = 2.123648300766945e-06

Training epoch-67 batch-27
Running loss of epoch-67 batch-27 = 1.3404060155153275e-06

Training epoch-67 batch-28
Running loss of epoch-67 batch-28 = 1.5553086996078491e-06

Training epoch-67 batch-29
Running loss of epoch-67 batch-29 = 2.3015309125185013e-06

Training epoch-67 batch-30
Running loss of epoch-67 batch-30 = 2.763001248240471e-06

Training epoch-67 batch-31
Running loss of epoch-67 batch-31 = 1.1781230568885803e-06

Training epoch-67 batch-32
Running loss of epoch-67 batch-32 = 2.5953631848096848e-06

Training epoch-67 batch-33
Running loss of epoch-67 batch-33 = 1.791398972272873e-06

Training epoch-67 batch-34
Running loss of epoch-67 batch-34 = 2.576736733317375e-06

Training epoch-67 batch-35
Running loss of epoch-67 batch-35 = 1.3434328138828278e-06

Training epoch-67 batch-36
Running loss of epoch-67 batch-36 = 2.198619768023491e-06

Training epoch-67 batch-37
Running loss of epoch-67 batch-37 = 1.5869736671447754e-06

Training epoch-67 batch-38
Running loss of epoch-67 batch-38 = 1.4309771358966827e-06

Training epoch-67 batch-39
Running loss of epoch-67 batch-39 = 1.773238182067871e-06

Training epoch-67 batch-40
Running loss of epoch-67 batch-40 = 1.698499545454979e-06

Training epoch-67 batch-41
Running loss of epoch-67 batch-41 = 2.0007137209177017e-06

Training epoch-67 batch-42
Running loss of epoch-67 batch-42 = 1.9029248505830765e-06

Training epoch-67 batch-43
Running loss of epoch-67 batch-43 = 1.7327256500720978e-06

Training epoch-67 batch-44
Running loss of epoch-67 batch-44 = 1.9227154552936554e-06

Training epoch-67 batch-45
Running loss of epoch-67 batch-45 = 2.8887297958135605e-06

Training epoch-67 batch-46
Running loss of epoch-67 batch-46 = 1.0505318641662598e-06

Training epoch-67 batch-47
Running loss of epoch-67 batch-47 = 1.8924474716186523e-06

Training epoch-67 batch-48
Running loss of epoch-67 batch-48 = 2.126907929778099e-06

Training epoch-67 batch-49
Running loss of epoch-67 batch-49 = 1.7539132386446e-06

Training epoch-67 batch-50
Running loss of epoch-67 batch-50 = 1.3706739991903305e-06

Training epoch-67 batch-51
Running loss of epoch-67 batch-51 = 2.1944288164377213e-06

Training epoch-67 batch-52
Running loss of epoch-67 batch-52 = 1.4863908290863037e-06

Training epoch-67 batch-53
Running loss of epoch-67 batch-53 = 1.6633421182632446e-06

Training epoch-67 batch-54
Running loss of epoch-67 batch-54 = 2.591172233223915e-06

Training epoch-67 batch-55
Running loss of epoch-67 batch-55 = 1.3113021850585938e-06

Training epoch-67 batch-56
Running loss of epoch-67 batch-56 = 2.5588087737560272e-06

Training epoch-67 batch-57
Running loss of epoch-67 batch-57 = 1.8719583749771118e-06

Training epoch-67 batch-58
Running loss of epoch-67 batch-58 = 1.3120006769895554e-06

Training epoch-67 batch-59
Running loss of epoch-67 batch-59 = 1.9618310034275055e-06

Training epoch-67 batch-60
Running loss of epoch-67 batch-60 = 1.5685800462961197e-06

Training epoch-67 batch-61
Running loss of epoch-67 batch-61 = 2.9585789889097214e-06

Training epoch-67 batch-62
Running loss of epoch-67 batch-62 = 2.2777821868658066e-06

Training epoch-67 batch-63
Running loss of epoch-67 batch-63 = 1.8370337784290314e-06

Training epoch-67 batch-64
Running loss of epoch-67 batch-64 = 1.3557728379964828e-06

Training epoch-67 batch-65
Running loss of epoch-67 batch-65 = 1.8158461898565292e-06

Training epoch-67 batch-66
Running loss of epoch-67 batch-66 = 1.8815044313669205e-06

Training epoch-67 batch-67
Running loss of epoch-67 batch-67 = 2.366490662097931e-06

Training epoch-67 batch-68
Running loss of epoch-67 batch-68 = 1.4419201761484146e-06

Training epoch-67 batch-69
Running loss of epoch-67 batch-69 = 1.810723915696144e-06

Training epoch-67 batch-70
Running loss of epoch-67 batch-70 = 1.67149119079113e-06

Training epoch-67 batch-71
Running loss of epoch-67 batch-71 = 2.4747569113969803e-06

Training epoch-67 batch-72
Running loss of epoch-67 batch-72 = 1.3827811926603317e-06

Training epoch-67 batch-73
Running loss of epoch-67 batch-73 = 1.7739366739988327e-06

Training epoch-67 batch-74
Running loss of epoch-67 batch-74 = 2.0533334463834763e-06

Training epoch-67 batch-75
Running loss of epoch-67 batch-75 = 1.4116521924734116e-06

Training epoch-67 batch-76
Running loss of epoch-67 batch-76 = 1.79302878677845e-06

Training epoch-67 batch-77
Running loss of epoch-67 batch-77 = 1.6379635781049728e-06

Training epoch-67 batch-78
Running loss of epoch-67 batch-78 = 1.51805579662323e-06

Training epoch-67 batch-79
Running loss of epoch-67 batch-79 = 1.487787812948227e-06

Training epoch-67 batch-80
Running loss of epoch-67 batch-80 = 1.626787707209587e-06

Training epoch-67 batch-81
Running loss of epoch-67 batch-81 = 1.7043203115463257e-06

Training epoch-67 batch-82
Running loss of epoch-67 batch-82 = 1.6987323760986328e-06

Training epoch-67 batch-83
Running loss of epoch-67 batch-83 = 1.5990808606147766e-06

Training epoch-67 batch-84
Running loss of epoch-67 batch-84 = 2.424931153655052e-06

Training epoch-67 batch-85
Running loss of epoch-67 batch-85 = 1.6388949006795883e-06

Training epoch-67 batch-86
Running loss of epoch-67 batch-86 = 1.9599683582782745e-06

Training epoch-67 batch-87
Running loss of epoch-67 batch-87 = 1.7117708921432495e-06

Training epoch-67 batch-88
Running loss of epoch-67 batch-88 = 1.0908115655183792e-06

Training epoch-67 batch-89
Running loss of epoch-67 batch-89 = 2.0049046725034714e-06

Training epoch-67 batch-90
Running loss of epoch-67 batch-90 = 1.8582213670015335e-06

Training epoch-67 batch-91
Running loss of epoch-67 batch-91 = 2.436107024550438e-06

Training epoch-67 batch-92
Running loss of epoch-67 batch-92 = 3.973953425884247e-06

Training epoch-67 batch-93
Running loss of epoch-67 batch-93 = 1.2852251529693604e-06

Training epoch-67 batch-94
Running loss of epoch-67 batch-94 = 1.7778947949409485e-06

Training epoch-67 batch-95
Running loss of epoch-67 batch-95 = 2.0510051399469376e-06

Training epoch-67 batch-96
Running loss of epoch-67 batch-96 = 2.1527521312236786e-06

Training epoch-67 batch-97
Running loss of epoch-67 batch-97 = 2.1066516637802124e-06

Training epoch-67 batch-98
Running loss of epoch-67 batch-98 = 2.017943188548088e-06

Training epoch-67 batch-99
Running loss of epoch-67 batch-99 = 1.7704442143440247e-06

Training epoch-67 batch-100
Running loss of epoch-67 batch-100 = 1.4740508049726486e-06

Training epoch-67 batch-101
Running loss of epoch-67 batch-101 = 1.786043867468834e-06

Training epoch-67 batch-102
Running loss of epoch-67 batch-102 = 1.5438999980688095e-06

Training epoch-67 batch-103
Running loss of epoch-67 batch-103 = 1.5767291188240051e-06

Training epoch-67 batch-104
Running loss of epoch-67 batch-104 = 1.4046672731637955e-06

Training epoch-67 batch-105
Running loss of epoch-67 batch-105 = 1.4994293451309204e-06

Training epoch-67 batch-106
Running loss of epoch-67 batch-106 = 3.6170240491628647e-06

Training epoch-67 batch-107
Running loss of epoch-67 batch-107 = 1.7504207789897919e-06

Training epoch-67 batch-108
Running loss of epoch-67 batch-108 = 1.4307443052530289e-06

Training epoch-67 batch-109
Running loss of epoch-67 batch-109 = 2.193031832575798e-06

Training epoch-67 batch-110
Running loss of epoch-67 batch-110 = 1.8810387700796127e-06

Training epoch-67 batch-111
Running loss of epoch-67 batch-111 = 1.3990793377161026e-06

Training epoch-67 batch-112
Running loss of epoch-67 batch-112 = 1.2279488146305084e-06

Training epoch-67 batch-113
Running loss of epoch-67 batch-113 = 1.009088009595871e-06

Training epoch-67 batch-114
Running loss of epoch-67 batch-114 = 1.8978025764226913e-06

Training epoch-67 batch-115
Running loss of epoch-67 batch-115 = 1.9096769392490387e-06

Training epoch-67 batch-116
Running loss of epoch-67 batch-116 = 1.4209654182195663e-06

Training epoch-67 batch-117
Running loss of epoch-67 batch-117 = 1.6654375940561295e-06

Training epoch-67 batch-118
Running loss of epoch-67 batch-118 = 2.227025106549263e-06

Training epoch-67 batch-119
Running loss of epoch-67 batch-119 = 1.5301629900932312e-06

Training epoch-67 batch-120
Running loss of epoch-67 batch-120 = 2.1420419216156006e-06

Training epoch-67 batch-121
Running loss of epoch-67 batch-121 = 2.208864316344261e-06

Training epoch-67 batch-122
Running loss of epoch-67 batch-122 = 2.225395292043686e-06

Training epoch-67 batch-123
Running loss of epoch-67 batch-123 = 1.7373822629451752e-06

Training epoch-67 batch-124
Running loss of epoch-67 batch-124 = 1.432141289114952e-06

Training epoch-67 batch-125
Running loss of epoch-67 batch-125 = 1.6265548765659332e-06

Training epoch-67 batch-126
Running loss of epoch-67 batch-126 = 2.5136396288871765e-06

Training epoch-67 batch-127
Running loss of epoch-67 batch-127 = 2.71480530500412e-06

Training epoch-67 batch-128
Running loss of epoch-67 batch-128 = 2.8114300221204758e-06

Training epoch-67 batch-129
Running loss of epoch-67 batch-129 = 2.8442591428756714e-06

Training epoch-67 batch-130
Running loss of epoch-67 batch-130 = 2.4319160729646683e-06

Training epoch-67 batch-131
Running loss of epoch-67 batch-131 = 2.011191099882126e-06

Training epoch-67 batch-132
Running loss of epoch-67 batch-132 = 1.537846401333809e-06

Training epoch-67 batch-133
Running loss of epoch-67 batch-133 = 2.1078158169984818e-06

Training epoch-67 batch-134
Running loss of epoch-67 batch-134 = 2.0419247448444366e-06

Training epoch-67 batch-135
Running loss of epoch-67 batch-135 = 1.0898802429437637e-06

Training epoch-67 batch-136
Running loss of epoch-67 batch-136 = 1.7781276255846024e-06

Training epoch-67 batch-137
Running loss of epoch-67 batch-137 = 1.5031546354293823e-06

Training epoch-67 batch-138
Running loss of epoch-67 batch-138 = 1.6854610294103622e-06

Training epoch-67 batch-139
Running loss of epoch-67 batch-139 = 2.1900050342082977e-06

Training epoch-67 batch-140
Running loss of epoch-67 batch-140 = 1.0021030902862549e-06

Training epoch-67 batch-141
Running loss of epoch-67 batch-141 = 1.562759280204773e-06

Training epoch-67 batch-142
Running loss of epoch-67 batch-142 = 1.6619451344013214e-06

Training epoch-67 batch-143
Running loss of epoch-67 batch-143 = 2.3385509848594666e-06

Training epoch-67 batch-144
Running loss of epoch-67 batch-144 = 1.398380845785141e-06

Training epoch-67 batch-145
Running loss of epoch-67 batch-145 = 1.6442500054836273e-06

Training epoch-67 batch-146
Running loss of epoch-67 batch-146 = 1.9650906324386597e-06

Training epoch-67 batch-147
Running loss of epoch-67 batch-147 = 1.7185229808092117e-06

Training epoch-67 batch-148
Running loss of epoch-67 batch-148 = 2.7189962565898895e-06

Training epoch-67 batch-149
Running loss of epoch-67 batch-149 = 2.000248059630394e-06

Training epoch-67 batch-150
Running loss of epoch-67 batch-150 = 2.7043279260396957e-06

Training epoch-67 batch-151
Running loss of epoch-67 batch-151 = 1.6917474567890167e-06

Training epoch-67 batch-152
Running loss of epoch-67 batch-152 = 1.843087375164032e-06

Training epoch-67 batch-153
Running loss of epoch-67 batch-153 = 1.346459612250328e-06

Training epoch-67 batch-154
Running loss of epoch-67 batch-154 = 2.353684976696968e-06

Training epoch-67 batch-155
Running loss of epoch-67 batch-155 = 1.9818544387817383e-06

Training epoch-67 batch-156
Running loss of epoch-67 batch-156 = 2.271728590130806e-06

Training epoch-67 batch-157
Running loss of epoch-67 batch-157 = 9.65222716331482e-06

Finished training epoch-67.



Average train loss at epoch-67 = 1.913154125213623e-06

Started Evaluation

Average val loss at epoch-67 = 3.210634281760768

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-68


Training epoch-68 batch-1
Running loss of epoch-68 batch-1 = 1.8060673028230667e-06

Training epoch-68 batch-2
Running loss of epoch-68 batch-2 = 1.2905802577733994e-06

Training epoch-68 batch-3
Running loss of epoch-68 batch-3 = 1.5941914170980453e-06

Training epoch-68 batch-4
Running loss of epoch-68 batch-4 = 1.983949914574623e-06

Training epoch-68 batch-5
Running loss of epoch-68 batch-5 = 1.380452886223793e-06

Training epoch-68 batch-6
Running loss of epoch-68 batch-6 = 1.303618773818016e-06

Training epoch-68 batch-7
Running loss of epoch-68 batch-7 = 2.0819716155529022e-06

Training epoch-68 batch-8
Running loss of epoch-68 batch-8 = 1.8230639398097992e-06

Training epoch-68 batch-9
Running loss of epoch-68 batch-9 = 2.054031938314438e-06

Training epoch-68 batch-10
Running loss of epoch-68 batch-10 = 2.0796433091163635e-06

Training epoch-68 batch-11
Running loss of epoch-68 batch-11 = 1.7669517546892166e-06

Training epoch-68 batch-12
Running loss of epoch-68 batch-12 = 2.1979212760925293e-06

Training epoch-68 batch-13
Running loss of epoch-68 batch-13 = 2.7099158614873886e-06

Training epoch-68 batch-14
Running loss of epoch-68 batch-14 = 1.816079020500183e-06

Training epoch-68 batch-15
Running loss of epoch-68 batch-15 = 1.5173573046922684e-06

Training epoch-68 batch-16
Running loss of epoch-68 batch-16 = 1.7446000128984451e-06

Training epoch-68 batch-17
Running loss of epoch-68 batch-17 = 2.4167820811271667e-06

Training epoch-68 batch-18
Running loss of epoch-68 batch-18 = 2.5872141122817993e-06

Training epoch-68 batch-19
Running loss of epoch-68 batch-19 = 1.893145963549614e-06

Training epoch-68 batch-20
Running loss of epoch-68 batch-20 = 1.8880236893892288e-06

Training epoch-68 batch-21
Running loss of epoch-68 batch-21 = 1.51083804666996e-06

Training epoch-68 batch-22
Running loss of epoch-68 batch-22 = 3.3937394618988037e-06

Training epoch-68 batch-23
Running loss of epoch-68 batch-23 = 1.5373807400465012e-06

Training epoch-68 batch-24
Running loss of epoch-68 batch-24 = 1.4705583453178406e-06

Training epoch-68 batch-25
Running loss of epoch-68 batch-25 = 1.7452985048294067e-06

Training epoch-68 batch-26
Running loss of epoch-68 batch-26 = 2.1560117602348328e-06

Training epoch-68 batch-27
Running loss of epoch-68 batch-27 = 2.500135451555252e-06

Training epoch-68 batch-28
Running loss of epoch-68 batch-28 = 1.2945383787155151e-06

Training epoch-68 batch-29
Running loss of epoch-68 batch-29 = 1.0617077350616455e-06

Training epoch-68 batch-30
Running loss of epoch-68 batch-30 = 1.9543804228305817e-06

Training epoch-68 batch-31
Running loss of epoch-68 batch-31 = 2.069631591439247e-06

Training epoch-68 batch-32
Running loss of epoch-68 batch-32 = 1.7718411982059479e-06

Training epoch-68 batch-33
Running loss of epoch-68 batch-33 = 1.5299301594495773e-06

Training epoch-68 batch-34
Running loss of epoch-68 batch-34 = 1.3380777090787888e-06

Training epoch-68 batch-35
Running loss of epoch-68 batch-35 = 1.607462763786316e-06

Training epoch-68 batch-36
Running loss of epoch-68 batch-36 = 1.85379758477211e-06

Training epoch-68 batch-37
Running loss of epoch-68 batch-37 = 1.6184058040380478e-06

Training epoch-68 batch-38
Running loss of epoch-68 batch-38 = 1.6922131180763245e-06

Training epoch-68 batch-39
Running loss of epoch-68 batch-39 = 1.443084329366684e-06

Training epoch-68 batch-40
Running loss of epoch-68 batch-40 = 1.3008248060941696e-06

Training epoch-68 batch-41
Running loss of epoch-68 batch-41 = 1.8852297216653824e-06

Training epoch-68 batch-42
Running loss of epoch-68 batch-42 = 2.6347115635871887e-06

Training epoch-68 batch-43
Running loss of epoch-68 batch-43 = 2.0381994545459747e-06

Training epoch-68 batch-44
Running loss of epoch-68 batch-44 = 9.499490261077881e-07

Training epoch-68 batch-45
Running loss of epoch-68 batch-45 = 1.839594915509224e-06

Training epoch-68 batch-46
Running loss of epoch-68 batch-46 = 2.1462328732013702e-06

Training epoch-68 batch-47
Running loss of epoch-68 batch-47 = 1.8512364476919174e-06

Training epoch-68 batch-48
Running loss of epoch-68 batch-48 = 1.1813826858997345e-06

Training epoch-68 batch-49
Running loss of epoch-68 batch-49 = 1.2307427823543549e-06

Training epoch-68 batch-50
Running loss of epoch-68 batch-50 = 1.5906989574432373e-06

Training epoch-68 batch-51
Running loss of epoch-68 batch-51 = 1.6009435057640076e-06

Training epoch-68 batch-52
Running loss of epoch-68 batch-52 = 1.775333657860756e-06

Training epoch-68 batch-53
Running loss of epoch-68 batch-53 = 1.8684659153223038e-06

Training epoch-68 batch-54
Running loss of epoch-68 batch-54 = 2.7674250304698944e-06

Training epoch-68 batch-55
Running loss of epoch-68 batch-55 = 1.914799213409424e-06

Training epoch-68 batch-56
Running loss of epoch-68 batch-56 = 2.2116582840681076e-06

Training epoch-68 batch-57
Running loss of epoch-68 batch-57 = 2.100598067045212e-06

Training epoch-68 batch-58
Running loss of epoch-68 batch-58 = 2.48616561293602e-06

Training epoch-68 batch-59
Running loss of epoch-68 batch-59 = 2.6333145797252655e-06

Training epoch-68 batch-60
Running loss of epoch-68 batch-60 = 1.7820857465267181e-06

Training epoch-68 batch-61
Running loss of epoch-68 batch-61 = 1.4987308531999588e-06

Training epoch-68 batch-62
Running loss of epoch-68 batch-62 = 1.9331928342580795e-06

Training epoch-68 batch-63
Running loss of epoch-68 batch-63 = 3.2633543014526367e-06

Training epoch-68 batch-64
Running loss of epoch-68 batch-64 = 2.339482307434082e-06

Training epoch-68 batch-65
Running loss of epoch-68 batch-65 = 3.0891969799995422e-06

Training epoch-68 batch-66
Running loss of epoch-68 batch-66 = 1.834239810705185e-06

Training epoch-68 batch-67
Running loss of epoch-68 batch-67 = 1.7541460692882538e-06

Training epoch-68 batch-68
Running loss of epoch-68 batch-68 = 1.6344711184501648e-06

Training epoch-68 batch-69
Running loss of epoch-68 batch-69 = 2.0007137209177017e-06

Training epoch-68 batch-70
Running loss of epoch-68 batch-70 = 1.7888378351926804e-06

Training epoch-68 batch-71
Running loss of epoch-68 batch-71 = 2.6549678295850754e-06

Training epoch-68 batch-72
Running loss of epoch-68 batch-72 = 1.9220169633626938e-06

Training epoch-68 batch-73
Running loss of epoch-68 batch-73 = 2.2226013243198395e-06

Training epoch-68 batch-74
Running loss of epoch-68 batch-74 = 1.507345587015152e-06

Training epoch-68 batch-75
Running loss of epoch-68 batch-75 = 1.7255079001188278e-06

Training epoch-68 batch-76
Running loss of epoch-68 batch-76 = 1.1548399925231934e-06

Training epoch-68 batch-77
Running loss of epoch-68 batch-77 = 1.4940742403268814e-06

Training epoch-68 batch-78
Running loss of epoch-68 batch-78 = 1.9585713744163513e-06

Training epoch-68 batch-79
Running loss of epoch-68 batch-79 = 1.6104895621538162e-06

Training epoch-68 batch-80
Running loss of epoch-68 batch-80 = 1.969980075955391e-06

Training epoch-68 batch-81
Running loss of epoch-68 batch-81 = 1.3899989426136017e-06

Training epoch-68 batch-82
Running loss of epoch-68 batch-82 = 1.7629936337471008e-06

Training epoch-68 batch-83
Running loss of epoch-68 batch-83 = 3.2116658985614777e-06

Training epoch-68 batch-84
Running loss of epoch-68 batch-84 = 2.080574631690979e-06

Training epoch-68 batch-85
Running loss of epoch-68 batch-85 = 1.662643626332283e-06

Training epoch-68 batch-86
Running loss of epoch-68 batch-86 = 2.180691808462143e-06

Training epoch-68 batch-87
Running loss of epoch-68 batch-87 = 9.094364941120148e-07

Training epoch-68 batch-88
Running loss of epoch-68 batch-88 = 2.1457672119140625e-06

Training epoch-68 batch-89
Running loss of epoch-68 batch-89 = 1.7480924725532532e-06

Training epoch-68 batch-90
Running loss of epoch-68 batch-90 = 1.3369135558605194e-06

Training epoch-68 batch-91
Running loss of epoch-68 batch-91 = 1.73947773873806e-06

Training epoch-68 batch-92
Running loss of epoch-68 batch-92 = 3.019813448190689e-06

Training epoch-68 batch-93
Running loss of epoch-68 batch-93 = 2.1622981876134872e-06

Training epoch-68 batch-94
Running loss of epoch-68 batch-94 = 2.4477485567331314e-06

Training epoch-68 batch-95
Running loss of epoch-68 batch-95 = 1.1562369763851166e-06

Training epoch-68 batch-96
Running loss of epoch-68 batch-96 = 1.5080440789461136e-06

Training epoch-68 batch-97
Running loss of epoch-68 batch-97 = 1.5243422240018845e-06

Training epoch-68 batch-98
Running loss of epoch-68 batch-98 = 2.4924520403146744e-06

Training epoch-68 batch-99
Running loss of epoch-68 batch-99 = 1.6686972230672836e-06

Training epoch-68 batch-100
Running loss of epoch-68 batch-100 = 2.530403435230255e-06

Training epoch-68 batch-101
Running loss of epoch-68 batch-101 = 2.464279532432556e-06

Training epoch-68 batch-102
Running loss of epoch-68 batch-102 = 1.7811544239521027e-06

Training epoch-68 batch-103
Running loss of epoch-68 batch-103 = 1.453561708331108e-06

Training epoch-68 batch-104
Running loss of epoch-68 batch-104 = 1.86101533472538e-06

Training epoch-68 batch-105
Running loss of epoch-68 batch-105 = 1.4551915228366852e-06

Training epoch-68 batch-106
Running loss of epoch-68 batch-106 = 1.2794043868780136e-06

Training epoch-68 batch-107
Running loss of epoch-68 batch-107 = 9.369105100631714e-07

Training epoch-68 batch-108
Running loss of epoch-68 batch-108 = 2.4221371859312057e-06

Training epoch-68 batch-109
Running loss of epoch-68 batch-109 = 2.0158477127552032e-06

Training epoch-68 batch-110
Running loss of epoch-68 batch-110 = 2.5632325559854507e-06

Training epoch-68 batch-111
Running loss of epoch-68 batch-111 = 3.06498259305954e-06

Training epoch-68 batch-112
Running loss of epoch-68 batch-112 = 1.709209755063057e-06

Training epoch-68 batch-113
Running loss of epoch-68 batch-113 = 1.548323780298233e-06

Training epoch-68 batch-114
Running loss of epoch-68 batch-114 = 1.8246937543153763e-06

Training epoch-68 batch-115
Running loss of epoch-68 batch-115 = 1.680571585893631e-06

Training epoch-68 batch-116
Running loss of epoch-68 batch-116 = 1.6365665942430496e-06

Training epoch-68 batch-117
Running loss of epoch-68 batch-117 = 1.3846438378095627e-06

Training epoch-68 batch-118
Running loss of epoch-68 batch-118 = 2.1997839212417603e-06

Training epoch-68 batch-119
Running loss of epoch-68 batch-119 = 1.5781261026859283e-06

Training epoch-68 batch-120
Running loss of epoch-68 batch-120 = 1.5425030142068863e-06

Training epoch-68 batch-121
Running loss of epoch-68 batch-121 = 1.6151461750268936e-06

Training epoch-68 batch-122
Running loss of epoch-68 batch-122 = 2.0083971321582794e-06

Training epoch-68 batch-123
Running loss of epoch-68 batch-123 = 1.8496066331863403e-06

Training epoch-68 batch-124
Running loss of epoch-68 batch-124 = 1.5723053365945816e-06

Training epoch-68 batch-125
Running loss of epoch-68 batch-125 = 2.776971086859703e-06

Training epoch-68 batch-126
Running loss of epoch-68 batch-126 = 2.2745225578546524e-06

Training epoch-68 batch-127
Running loss of epoch-68 batch-127 = 2.28220596909523e-06

Training epoch-68 batch-128
Running loss of epoch-68 batch-128 = 1.8766149878501892e-06

Training epoch-68 batch-129
Running loss of epoch-68 batch-129 = 2.4284236133098602e-06

Training epoch-68 batch-130
Running loss of epoch-68 batch-130 = 1.7506536096334457e-06

Training epoch-68 batch-131
Running loss of epoch-68 batch-131 = 1.3422686606645584e-06

Training epoch-68 batch-132
Running loss of epoch-68 batch-132 = 1.9902363419532776e-06

Training epoch-68 batch-133
Running loss of epoch-68 batch-133 = 2.359040081501007e-06

Training epoch-68 batch-134
Running loss of epoch-68 batch-134 = 1.051928848028183e-06

Training epoch-68 batch-135
Running loss of epoch-68 batch-135 = 1.964392140507698e-06

Training epoch-68 batch-136
Running loss of epoch-68 batch-136 = 1.176958903670311e-06

Training epoch-68 batch-137
Running loss of epoch-68 batch-137 = 2.482207491993904e-06

Training epoch-68 batch-138
Running loss of epoch-68 batch-138 = 2.7900096029043198e-06

Training epoch-68 batch-139
Running loss of epoch-68 batch-139 = 1.6910489648580551e-06

Training epoch-68 batch-140
Running loss of epoch-68 batch-140 = 1.3064127415418625e-06

Training epoch-68 batch-141
Running loss of epoch-68 batch-141 = 2.10409052670002e-06

Training epoch-68 batch-142
Running loss of epoch-68 batch-142 = 1.8710270524024963e-06

Training epoch-68 batch-143
Running loss of epoch-68 batch-143 = 1.7301645129919052e-06

Training epoch-68 batch-144
Running loss of epoch-68 batch-144 = 1.0756775736808777e-06

Training epoch-68 batch-145
Running loss of epoch-68 batch-145 = 1.7178244888782501e-06

Training epoch-68 batch-146
Running loss of epoch-68 batch-146 = 1.7026904970407486e-06

Training epoch-68 batch-147
Running loss of epoch-68 batch-147 = 1.0193325579166412e-06

Training epoch-68 batch-148
Running loss of epoch-68 batch-148 = 2.177199348807335e-06

Training epoch-68 batch-149
Running loss of epoch-68 batch-149 = 1.3692770153284073e-06

Training epoch-68 batch-150
Running loss of epoch-68 batch-150 = 2.1010637283325195e-06

Training epoch-68 batch-151
Running loss of epoch-68 batch-151 = 1.5650875866413116e-06

Training epoch-68 batch-152
Running loss of epoch-68 batch-152 = 1.7988495528697968e-06

Training epoch-68 batch-153
Running loss of epoch-68 batch-153 = 1.6381964087486267e-06

Training epoch-68 batch-154
Running loss of epoch-68 batch-154 = 1.7227139323949814e-06

Training epoch-68 batch-155
Running loss of epoch-68 batch-155 = 1.6246922314167023e-06

Training epoch-68 batch-156
Running loss of epoch-68 batch-156 = 1.3741664588451385e-06

Training epoch-68 batch-157
Running loss of epoch-68 batch-157 = 3.7439167499542236e-06

Finished training epoch-68.



Average train loss at epoch-68 = 1.8677040934562683e-06

Started Evaluation

Average val loss at epoch-68 = 3.21503693492789

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-69


Training epoch-69 batch-1
Running loss of epoch-69 batch-1 = 1.5972182154655457e-06

Training epoch-69 batch-2
Running loss of epoch-69 batch-2 = 1.2405216693878174e-06

Training epoch-69 batch-3
Running loss of epoch-69 batch-3 = 9.78587195277214e-07

Training epoch-69 batch-4
Running loss of epoch-69 batch-4 = 2.3567117750644684e-06

Training epoch-69 batch-5
Running loss of epoch-69 batch-5 = 2.043088898062706e-06

Training epoch-69 batch-6
Running loss of epoch-69 batch-6 = 1.5783589333295822e-06

Training epoch-69 batch-7
Running loss of epoch-69 batch-7 = 1.3294629752635956e-06

Training epoch-69 batch-8
Running loss of epoch-69 batch-8 = 1.2088567018508911e-06

Training epoch-69 batch-9
Running loss of epoch-69 batch-9 = 1.600012183189392e-06

Training epoch-69 batch-10
Running loss of epoch-69 batch-10 = 2.435874193906784e-06

Training epoch-69 batch-11
Running loss of epoch-69 batch-11 = 2.1329615265130997e-06

Training epoch-69 batch-12
Running loss of epoch-69 batch-12 = 2.280343323945999e-06

Training epoch-69 batch-13
Running loss of epoch-69 batch-13 = 1.5534460544586182e-06

Training epoch-69 batch-14
Running loss of epoch-69 batch-14 = 2.171378582715988e-06

Training epoch-69 batch-15
Running loss of epoch-69 batch-15 = 2.1604355424642563e-06

Training epoch-69 batch-16
Running loss of epoch-69 batch-16 = 1.5320256352424622e-06

Training epoch-69 batch-17
Running loss of epoch-69 batch-17 = 2.450309693813324e-06

Training epoch-69 batch-18
Running loss of epoch-69 batch-18 = 1.7755664885044098e-06

Training epoch-69 batch-19
Running loss of epoch-69 batch-19 = 2.405373379588127e-06

Training epoch-69 batch-20
Running loss of epoch-69 batch-20 = 1.989305019378662e-06

Training epoch-69 batch-21
Running loss of epoch-69 batch-21 = 1.2523960322141647e-06

Training epoch-69 batch-22
Running loss of epoch-69 batch-22 = 2.1150335669517517e-06

Training epoch-69 batch-23
Running loss of epoch-69 batch-23 = 1.8368009477853775e-06

Training epoch-69 batch-24
Running loss of epoch-69 batch-24 = 1.4689285308122635e-06

Training epoch-69 batch-25
Running loss of epoch-69 batch-25 = 1.8728896975517273e-06

Training epoch-69 batch-26
Running loss of epoch-69 batch-26 = 2.3024622350931168e-06

Training epoch-69 batch-27
Running loss of epoch-69 batch-27 = 1.9955914467573166e-06

Training epoch-69 batch-28
Running loss of epoch-69 batch-28 = 2.3262109607458115e-06

Training epoch-69 batch-29
Running loss of epoch-69 batch-29 = 1.2889504432678223e-06

Training epoch-69 batch-30
Running loss of epoch-69 batch-30 = 2.1525193005800247e-06

Training epoch-69 batch-31
Running loss of epoch-69 batch-31 = 2.0598527044057846e-06

Training epoch-69 batch-32
Running loss of epoch-69 batch-32 = 2.2388994693756104e-06

Training epoch-69 batch-33
Running loss of epoch-69 batch-33 = 1.185806468129158e-06

Training epoch-69 batch-34
Running loss of epoch-69 batch-34 = 1.7320271581411362e-06

Training epoch-69 batch-35
Running loss of epoch-69 batch-35 = 2.368353307247162e-06

Training epoch-69 batch-36
Running loss of epoch-69 batch-36 = 2.1189916878938675e-06

Training epoch-69 batch-37
Running loss of epoch-69 batch-37 = 1.701992005109787e-06

Training epoch-69 batch-38
Running loss of epoch-69 batch-38 = 1.4675315469503403e-06

Training epoch-69 batch-39
Running loss of epoch-69 batch-39 = 1.5515834093093872e-06

Training epoch-69 batch-40
Running loss of epoch-69 batch-40 = 1.1799857020378113e-06

Training epoch-69 batch-41
Running loss of epoch-69 batch-41 = 1.6454141587018967e-06

Training epoch-69 batch-42
Running loss of epoch-69 batch-42 = 1.8919818103313446e-06

Training epoch-69 batch-43
Running loss of epoch-69 batch-43 = 1.703621819615364e-06

Training epoch-69 batch-44
Running loss of epoch-69 batch-44 = 1.751817762851715e-06

Training epoch-69 batch-45
Running loss of epoch-69 batch-45 = 9.746290743350983e-07

Training epoch-69 batch-46
Running loss of epoch-69 batch-46 = 2.562766894698143e-06

Training epoch-69 batch-47
Running loss of epoch-69 batch-47 = 1.7390120774507523e-06

Training epoch-69 batch-48
Running loss of epoch-69 batch-48 = 1.8882565200328827e-06

Training epoch-69 batch-49
Running loss of epoch-69 batch-49 = 2.2586900740861893e-06

Training epoch-69 batch-50
Running loss of epoch-69 batch-50 = 2.5746412575244904e-06

Training epoch-69 batch-51
Running loss of epoch-69 batch-51 = 1.6719568520784378e-06

Training epoch-69 batch-52
Running loss of epoch-69 batch-52 = 2.1359883248806e-06

Training epoch-69 batch-53
Running loss of epoch-69 batch-53 = 1.782318577170372e-06

Training epoch-69 batch-54
Running loss of epoch-69 batch-54 = 1.757405698299408e-06

Training epoch-69 batch-55
Running loss of epoch-69 batch-55 = 1.928536221385002e-06

Training epoch-69 batch-56
Running loss of epoch-69 batch-56 = 2.501998096704483e-06

Training epoch-69 batch-57
Running loss of epoch-69 batch-57 = 2.6207417249679565e-06

Training epoch-69 batch-58
Running loss of epoch-69 batch-58 = 1.093139871954918e-06

Training epoch-69 batch-59
Running loss of epoch-69 batch-59 = 1.7392449080944061e-06

Training epoch-69 batch-60
Running loss of epoch-69 batch-60 = 2.203509211540222e-06

Training epoch-69 batch-61
Running loss of epoch-69 batch-61 = 1.8773134797811508e-06

Training epoch-69 batch-62
Running loss of epoch-69 batch-62 = 1.3236422091722488e-06

Training epoch-69 batch-63
Running loss of epoch-69 batch-63 = 1.6652047634124756e-06

Training epoch-69 batch-64
Running loss of epoch-69 batch-64 = 1.6684643924236298e-06

Training epoch-69 batch-65
Running loss of epoch-69 batch-65 = 1.876847818493843e-06

Training epoch-69 batch-66
Running loss of epoch-69 batch-66 = 1.2039672583341599e-06

Training epoch-69 batch-67
Running loss of epoch-69 batch-67 = 2.3404136300086975e-06

Training epoch-69 batch-68
Running loss of epoch-69 batch-68 = 1.7469283193349838e-06

Training epoch-69 batch-69
Running loss of epoch-69 batch-69 = 2.2386666387319565e-06

Training epoch-69 batch-70
Running loss of epoch-69 batch-70 = 1.559033989906311e-06

Training epoch-69 batch-71
Running loss of epoch-69 batch-71 = 2.1171290427446365e-06

Training epoch-69 batch-72
Running loss of epoch-69 batch-72 = 1.844950020313263e-06

Training epoch-69 batch-73
Running loss of epoch-69 batch-73 = 1.8454156816005707e-06

Training epoch-69 batch-74
Running loss of epoch-69 batch-74 = 2.2745225578546524e-06

Training epoch-69 batch-75
Running loss of epoch-69 batch-75 = 1.662643626332283e-06

Training epoch-69 batch-76
Running loss of epoch-69 batch-76 = 1.3997778296470642e-06

Training epoch-69 batch-77
Running loss of epoch-69 batch-77 = 2.2689346224069595e-06

Training epoch-69 batch-78
Running loss of epoch-69 batch-78 = 2.5227200239896774e-06

Training epoch-69 batch-79
Running loss of epoch-69 batch-79 = 2.0186416804790497e-06

Training epoch-69 batch-80
Running loss of epoch-69 batch-80 = 2.375571057200432e-06

Training epoch-69 batch-81
Running loss of epoch-69 batch-81 = 1.5986151993274689e-06

Training epoch-69 batch-82
Running loss of epoch-69 batch-82 = 1.7639249563217163e-06

Training epoch-69 batch-83
Running loss of epoch-69 batch-83 = 1.634005457162857e-06

Training epoch-69 batch-84
Running loss of epoch-69 batch-84 = 1.6093254089355469e-06

Training epoch-69 batch-85
Running loss of epoch-69 batch-85 = 1.607229933142662e-06

Training epoch-69 batch-86
Running loss of epoch-69 batch-86 = 2.870103344321251e-06

Training epoch-69 batch-87
Running loss of epoch-69 batch-87 = 1.3248063623905182e-06

Training epoch-69 batch-88
Running loss of epoch-69 batch-88 = 1.3592652976512909e-06

Training epoch-69 batch-89
Running loss of epoch-69 batch-89 = 1.755543053150177e-06

Training epoch-69 batch-90
Running loss of epoch-69 batch-90 = 2.6384368538856506e-06

Training epoch-69 batch-91
Running loss of epoch-69 batch-91 = 2.2028107196092606e-06

Training epoch-69 batch-92
Running loss of epoch-69 batch-92 = 1.5478581190109253e-06

Training epoch-69 batch-93
Running loss of epoch-69 batch-93 = 2.325046807527542e-06

Training epoch-69 batch-94
Running loss of epoch-69 batch-94 = 1.037493348121643e-06

Training epoch-69 batch-95
Running loss of epoch-69 batch-95 = 1.3259705156087875e-06

Training epoch-69 batch-96
Running loss of epoch-69 batch-96 = 1.2048985809087753e-06

Training epoch-69 batch-97
Running loss of epoch-69 batch-97 = 1.4458782970905304e-06

Training epoch-69 batch-98
Running loss of epoch-69 batch-98 = 2.210959792137146e-06

Training epoch-69 batch-99
Running loss of epoch-69 batch-99 = 1.893145963549614e-06

Training epoch-69 batch-100
Running loss of epoch-69 batch-100 = 1.8691644072532654e-06

Training epoch-69 batch-101
Running loss of epoch-69 batch-101 = 1.6333069652318954e-06

Training epoch-69 batch-102
Running loss of epoch-69 batch-102 = 1.6558915376663208e-06

Training epoch-69 batch-103
Running loss of epoch-69 batch-103 = 1.7052516341209412e-06

Training epoch-69 batch-104
Running loss of epoch-69 batch-104 = 1.5045516192913055e-06

Training epoch-69 batch-105
Running loss of epoch-69 batch-105 = 2.0940788090229034e-06

Training epoch-69 batch-106
Running loss of epoch-69 batch-106 = 1.412583515048027e-06

Training epoch-69 batch-107
Running loss of epoch-69 batch-107 = 1.8558930605649948e-06

Training epoch-69 batch-108
Running loss of epoch-69 batch-108 = 1.7927959561347961e-06

Training epoch-69 batch-109
Running loss of epoch-69 batch-109 = 1.7546117305755615e-06

Training epoch-69 batch-110
Running loss of epoch-69 batch-110 = 2.0121224224567413e-06

Training epoch-69 batch-111
Running loss of epoch-69 batch-111 = 1.6810372471809387e-06

Training epoch-69 batch-112
Running loss of epoch-69 batch-112 = 2.7099158614873886e-06

Training epoch-69 batch-113
Running loss of epoch-69 batch-113 = 9.539071470499039e-07

Training epoch-69 batch-114
Running loss of epoch-69 batch-114 = 1.8968712538480759e-06

Training epoch-69 batch-115
Running loss of epoch-69 batch-115 = 1.6584526747465134e-06

Training epoch-69 batch-116
Running loss of epoch-69 batch-116 = 1.9369181245565414e-06

Training epoch-69 batch-117
Running loss of epoch-69 batch-117 = 1.9869767129421234e-06

Training epoch-69 batch-118
Running loss of epoch-69 batch-118 = 1.389533281326294e-06

Training epoch-69 batch-119
Running loss of epoch-69 batch-119 = 1.2686941772699356e-06

Training epoch-69 batch-120
Running loss of epoch-69 batch-120 = 1.2740492820739746e-06

Training epoch-69 batch-121
Running loss of epoch-69 batch-121 = 1.5613622963428497e-06

Training epoch-69 batch-122
Running loss of epoch-69 batch-122 = 2.4263281375169754e-06

Training epoch-69 batch-123
Running loss of epoch-69 batch-123 = 2.5087501853704453e-06

Training epoch-69 batch-124
Running loss of epoch-69 batch-124 = 1.7774291336536407e-06

Training epoch-69 batch-125
Running loss of epoch-69 batch-125 = 2.1080486476421356e-06

Training epoch-69 batch-126
Running loss of epoch-69 batch-126 = 2.1206215023994446e-06

Training epoch-69 batch-127
Running loss of epoch-69 batch-127 = 1.7702113837003708e-06

Training epoch-69 batch-128
Running loss of epoch-69 batch-128 = 1.5844125300645828e-06

Training epoch-69 batch-129
Running loss of epoch-69 batch-129 = 1.5709083527326584e-06

Training epoch-69 batch-130
Running loss of epoch-69 batch-130 = 1.8633436411619186e-06

Training epoch-69 batch-131
Running loss of epoch-69 batch-131 = 2.739252522587776e-06

Training epoch-69 batch-132
Running loss of epoch-69 batch-132 = 1.7844140529632568e-06

Training epoch-69 batch-133
Running loss of epoch-69 batch-133 = 1.6237609088420868e-06

Training epoch-69 batch-134
Running loss of epoch-69 batch-134 = 1.5317928045988083e-06

Training epoch-69 batch-135
Running loss of epoch-69 batch-135 = 1.9189901649951935e-06

Training epoch-69 batch-136
Running loss of epoch-69 batch-136 = 1.3709068298339844e-06

Training epoch-69 batch-137
Running loss of epoch-69 batch-137 = 1.8116552382707596e-06

Training epoch-69 batch-138
Running loss of epoch-69 batch-138 = 9.872019290924072e-07

Training epoch-69 batch-139
Running loss of epoch-69 batch-139 = 2.60770320892334e-06

Training epoch-69 batch-140
Running loss of epoch-69 batch-140 = 1.3431999832391739e-06

Training epoch-69 batch-141
Running loss of epoch-69 batch-141 = 1.373467966914177e-06

Training epoch-69 batch-142
Running loss of epoch-69 batch-142 = 2.1515879780054092e-06

Training epoch-69 batch-143
Running loss of epoch-69 batch-143 = 1.7371494323015213e-06

Training epoch-69 batch-144
Running loss of epoch-69 batch-144 = 1.9045546650886536e-06

Training epoch-69 batch-145
Running loss of epoch-69 batch-145 = 1.4700926840305328e-06

Training epoch-69 batch-146
Running loss of epoch-69 batch-146 = 1.417938619852066e-06

Training epoch-69 batch-147
Running loss of epoch-69 batch-147 = 1.991400495171547e-06

Training epoch-69 batch-148
Running loss of epoch-69 batch-148 = 2.177199348807335e-06

Training epoch-69 batch-149
Running loss of epoch-69 batch-149 = 1.7576385289430618e-06

Training epoch-69 batch-150
Running loss of epoch-69 batch-150 = 1.8791761249303818e-06

Training epoch-69 batch-151
Running loss of epoch-69 batch-151 = 1.7615966498851776e-06

Training epoch-69 batch-152
Running loss of epoch-69 batch-152 = 1.8011778593063354e-06

Training epoch-69 batch-153
Running loss of epoch-69 batch-153 = 2.658925950527191e-06

Training epoch-69 batch-154
Running loss of epoch-69 batch-154 = 1.4870893210172653e-06

Training epoch-69 batch-155
Running loss of epoch-69 batch-155 = 1.750187948346138e-06

Training epoch-69 batch-156
Running loss of epoch-69 batch-156 = 2.1024607121944427e-06

Training epoch-69 batch-157
Running loss of epoch-69 batch-157 = 4.5746564865112305e-06

Finished training epoch-69.



Average train loss at epoch-69 = 1.832132041454315e-06

Started Evaluation

Average val loss at epoch-69 = 3.2207199711548653

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-70


Training epoch-70 batch-1
Running loss of epoch-70 batch-1 = 1.6344711184501648e-06

Training epoch-70 batch-2
Running loss of epoch-70 batch-2 = 1.1119991540908813e-06

Training epoch-70 batch-3
Running loss of epoch-70 batch-3 = 2.1669548004865646e-06

Training epoch-70 batch-4
Running loss of epoch-70 batch-4 = 1.3872049748897552e-06

Training epoch-70 batch-5
Running loss of epoch-70 batch-5 = 2.5962945073843002e-06

Training epoch-70 batch-6
Running loss of epoch-70 batch-6 = 2.115964889526367e-06

Training epoch-70 batch-7
Running loss of epoch-70 batch-7 = 1.6433186829090118e-06

Training epoch-70 batch-8
Running loss of epoch-70 batch-8 = 2.1366868168115616e-06

Training epoch-70 batch-9
Running loss of epoch-70 batch-9 = 1.6076955944299698e-06

Training epoch-70 batch-10
Running loss of epoch-70 batch-10 = 1.946697011590004e-06

Training epoch-70 batch-11
Running loss of epoch-70 batch-11 = 9.420327842235565e-07

Training epoch-70 batch-12
Running loss of epoch-70 batch-12 = 1.8510036170482635e-06

Training epoch-70 batch-13
Running loss of epoch-70 batch-13 = 2.4831388145685196e-06

Training epoch-70 batch-14
Running loss of epoch-70 batch-14 = 1.6328413039445877e-06

Training epoch-70 batch-15
Running loss of epoch-70 batch-15 = 2.2873282432556152e-06

Training epoch-70 batch-16
Running loss of epoch-70 batch-16 = 2.696877345442772e-06

Training epoch-70 batch-17
Running loss of epoch-70 batch-17 = 1.9604340195655823e-06

Training epoch-70 batch-18
Running loss of epoch-70 batch-18 = 1.0423827916383743e-06

Training epoch-70 batch-19
Running loss of epoch-70 batch-19 = 1.5897676348686218e-06

Training epoch-70 batch-20
Running loss of epoch-70 batch-20 = 2.475455403327942e-06

Training epoch-70 batch-21
Running loss of epoch-70 batch-21 = 1.271720975637436e-06

Training epoch-70 batch-22
Running loss of epoch-70 batch-22 = 2.478482201695442e-06

Training epoch-70 batch-23
Running loss of epoch-70 batch-23 = 1.7855782061815262e-06

Training epoch-70 batch-24
Running loss of epoch-70 batch-24 = 2.0300503820180893e-06

Training epoch-70 batch-25
Running loss of epoch-70 batch-25 = 1.923181116580963e-06

Training epoch-70 batch-26
Running loss of epoch-70 batch-26 = 1.9939616322517395e-06

Training epoch-70 batch-27
Running loss of epoch-70 batch-27 = 1.4370307326316833e-06

Training epoch-70 batch-28
Running loss of epoch-70 batch-28 = 1.3713724911212921e-06

Training epoch-70 batch-29
Running loss of epoch-70 batch-29 = 1.9008293747901917e-06

Training epoch-70 batch-30
Running loss of epoch-70 batch-30 = 1.5741679817438126e-06

Training epoch-70 batch-31
Running loss of epoch-70 batch-31 = 2.6756897568702698e-06

Training epoch-70 batch-32
Running loss of epoch-70 batch-32 = 1.8547289073467255e-06

Training epoch-70 batch-33
Running loss of epoch-70 batch-33 = 2.0416919142007828e-06

Training epoch-70 batch-34
Running loss of epoch-70 batch-34 = 1.2943055480718613e-06

Training epoch-70 batch-35
Running loss of epoch-70 batch-35 = 1.6402918845415115e-06

Training epoch-70 batch-36
Running loss of epoch-70 batch-36 = 2.409098669886589e-06

Training epoch-70 batch-37
Running loss of epoch-70 batch-37 = 1.7052516341209412e-06

Training epoch-70 batch-38
Running loss of epoch-70 batch-38 = 1.6749836504459381e-06

Training epoch-70 batch-39
Running loss of epoch-70 batch-39 = 1.623295247554779e-06

Training epoch-70 batch-40
Running loss of epoch-70 batch-40 = 1.4775432646274567e-06

Training epoch-70 batch-41
Running loss of epoch-70 batch-41 = 2.032378688454628e-06

Training epoch-70 batch-42
Running loss of epoch-70 batch-42 = 1.5757977962493896e-06

Training epoch-70 batch-43
Running loss of epoch-70 batch-43 = 3.034016117453575e-06

Training epoch-70 batch-44
Running loss of epoch-70 batch-44 = 1.3117678463459015e-06

Training epoch-70 batch-45
Running loss of epoch-70 batch-45 = 1.8673017621040344e-06

Training epoch-70 batch-46
Running loss of epoch-70 batch-46 = 2.562999725341797e-06

Training epoch-70 batch-47
Running loss of epoch-70 batch-47 = 1.891283318400383e-06

Training epoch-70 batch-48
Running loss of epoch-70 batch-48 = 1.3229437172412872e-06

Training epoch-70 batch-49
Running loss of epoch-70 batch-49 = 1.459149643778801e-06

Training epoch-70 batch-50
Running loss of epoch-70 batch-50 = 1.837732270359993e-06

Training epoch-70 batch-51
Running loss of epoch-70 batch-51 = 2.0954757928848267e-06

Training epoch-70 batch-52
Running loss of epoch-70 batch-52 = 2.185814082622528e-06

Training epoch-70 batch-53
Running loss of epoch-70 batch-53 = 1.3888347893953323e-06

Training epoch-70 batch-54
Running loss of epoch-70 batch-54 = 1.794658601284027e-06

Training epoch-70 batch-55
Running loss of epoch-70 batch-55 = 1.4451798051595688e-06

Training epoch-70 batch-56
Running loss of epoch-70 batch-56 = 1.6526319086551666e-06

Training epoch-70 batch-57
Running loss of epoch-70 batch-57 = 1.34296715259552e-06

Training epoch-70 batch-58
Running loss of epoch-70 batch-58 = 1.1308584362268448e-06

Training epoch-70 batch-59
Running loss of epoch-70 batch-59 = 2.2402964532375336e-06

Training epoch-70 batch-60
Running loss of epoch-70 batch-60 = 2.1776650100946426e-06

Training epoch-70 batch-61
Running loss of epoch-70 batch-61 = 1.6656704246997833e-06

Training epoch-70 batch-62
Running loss of epoch-70 batch-62 = 2.1671876311302185e-06

Training epoch-70 batch-63
Running loss of epoch-70 batch-63 = 1.5022233128547668e-06

Training epoch-70 batch-64
Running loss of epoch-70 batch-64 = 1.987675204873085e-06

Training epoch-70 batch-65
Running loss of epoch-70 batch-65 = 2.1471641957759857e-06

Training epoch-70 batch-66
Running loss of epoch-70 batch-66 = 2.4854671210050583e-06

Training epoch-70 batch-67
Running loss of epoch-70 batch-67 = 1.9315630197525024e-06

Training epoch-70 batch-68
Running loss of epoch-70 batch-68 = 1.8079299479722977e-06

Training epoch-70 batch-69
Running loss of epoch-70 batch-69 = 1.1245720088481903e-06

Training epoch-70 batch-70
Running loss of epoch-70 batch-70 = 1.835869625210762e-06

Training epoch-70 batch-71
Running loss of epoch-70 batch-71 = 1.709209755063057e-06

Training epoch-70 batch-72
Running loss of epoch-70 batch-72 = 2.083834260702133e-06

Training epoch-70 batch-73
Running loss of epoch-70 batch-73 = 1.919688656926155e-06

Training epoch-70 batch-74
Running loss of epoch-70 batch-74 = 1.7310958355665207e-06

Training epoch-70 batch-75
Running loss of epoch-70 batch-75 = 2.171378582715988e-06

Training epoch-70 batch-76
Running loss of epoch-70 batch-76 = 1.7364509403705597e-06

Training epoch-70 batch-77
Running loss of epoch-70 batch-77 = 1.7655547708272934e-06

Training epoch-70 batch-78
Running loss of epoch-70 batch-78 = 1.748325303196907e-06

Training epoch-70 batch-79
Running loss of epoch-70 batch-79 = 2.3033935576677322e-06

Training epoch-70 batch-80
Running loss of epoch-70 batch-80 = 2.458924427628517e-06

Training epoch-70 batch-81
Running loss of epoch-70 batch-81 = 1.40373595058918e-06

Training epoch-70 batch-82
Running loss of epoch-70 batch-82 = 1.7816200852394104e-06

Training epoch-70 batch-83
Running loss of epoch-70 batch-83 = 1.3255048543214798e-06

Training epoch-70 batch-84
Running loss of epoch-70 batch-84 = 1.7958227545022964e-06

Training epoch-70 batch-85
Running loss of epoch-70 batch-85 = 1.5583354979753494e-06

Training epoch-70 batch-86
Running loss of epoch-70 batch-86 = 1.5189871191978455e-06

Training epoch-70 batch-87
Running loss of epoch-70 batch-87 = 1.8908176571130753e-06

Training epoch-70 batch-88
Running loss of epoch-70 batch-88 = 1.6379635781049728e-06

Training epoch-70 batch-89
Running loss of epoch-70 batch-89 = 1.0973308235406876e-06

Training epoch-70 batch-90
Running loss of epoch-70 batch-90 = 1.346459612250328e-06

Training epoch-70 batch-91
Running loss of epoch-70 batch-91 = 2.189772203564644e-06

Training epoch-70 batch-92
Running loss of epoch-70 batch-92 = 1.3438984751701355e-06

Training epoch-70 batch-93
Running loss of epoch-70 batch-93 = 1.2307427823543549e-06

Training epoch-70 batch-94
Running loss of epoch-70 batch-94 = 2.164393663406372e-06

Training epoch-70 batch-95
Running loss of epoch-70 batch-95 = 1.786043867468834e-06

Training epoch-70 batch-96
Running loss of epoch-70 batch-96 = 2.3078173398971558e-06

Training epoch-70 batch-97
Running loss of epoch-70 batch-97 = 1.4114193618297577e-06

Training epoch-70 batch-98
Running loss of epoch-70 batch-98 = 1.8565915524959564e-06

Training epoch-70 batch-99
Running loss of epoch-70 batch-99 = 1.441221684217453e-06

Training epoch-70 batch-100
Running loss of epoch-70 batch-100 = 1.801876351237297e-06

Training epoch-70 batch-101
Running loss of epoch-70 batch-101 = 1.2614764273166656e-06

Training epoch-70 batch-102
Running loss of epoch-70 batch-102 = 2.904096618294716e-06

Training epoch-70 batch-103
Running loss of epoch-70 batch-103 = 6.915070116519928e-07

Training epoch-70 batch-104
Running loss of epoch-70 batch-104 = 1.4386605471372604e-06

Training epoch-70 batch-105
Running loss of epoch-70 batch-105 = 2.3685861378908157e-06

Training epoch-70 batch-106
Running loss of epoch-70 batch-106 = 2.2617168724536896e-06

Training epoch-70 batch-107
Running loss of epoch-70 batch-107 = 1.6060657799243927e-06

Training epoch-70 batch-108
Running loss of epoch-70 batch-108 = 1.4898832887411118e-06

Training epoch-70 batch-109
Running loss of epoch-70 batch-109 = 1.5005934983491898e-06

Training epoch-70 batch-110
Running loss of epoch-70 batch-110 = 1.8267892301082611e-06

Training epoch-70 batch-111
Running loss of epoch-70 batch-111 = 1.4666002243757248e-06

Training epoch-70 batch-112
Running loss of epoch-70 batch-112 = 2.719694748520851e-06

Training epoch-70 batch-113
Running loss of epoch-70 batch-113 = 1.4402903616428375e-06

Training epoch-70 batch-114
Running loss of epoch-70 batch-114 = 1.3867393136024475e-06

Training epoch-70 batch-115
Running loss of epoch-70 batch-115 = 2.046581357717514e-06

Training epoch-70 batch-116
Running loss of epoch-70 batch-116 = 1.8321443349123001e-06

Training epoch-70 batch-117
Running loss of epoch-70 batch-117 = 1.2486707419157028e-06

Training epoch-70 batch-118
Running loss of epoch-70 batch-118 = 1.8419232219457626e-06

Training epoch-70 batch-119
Running loss of epoch-70 batch-119 = 1.8593855202198029e-06

Training epoch-70 batch-120
Running loss of epoch-70 batch-120 = 1.544831320643425e-06

Training epoch-70 batch-121
Running loss of epoch-70 batch-121 = 2.2868625819683075e-06

Training epoch-70 batch-122
Running loss of epoch-70 batch-122 = 1.2656673789024353e-06

Training epoch-70 batch-123
Running loss of epoch-70 batch-123 = 1.1068768799304962e-06

Training epoch-70 batch-124
Running loss of epoch-70 batch-124 = 1.8784776329994202e-06

Training epoch-70 batch-125
Running loss of epoch-70 batch-125 = 1.6039703041315079e-06

Training epoch-70 batch-126
Running loss of epoch-70 batch-126 = 2.18232162296772e-06

Training epoch-70 batch-127
Running loss of epoch-70 batch-127 = 2.3406464606523514e-06

Training epoch-70 batch-128
Running loss of epoch-70 batch-128 = 1.9990839064121246e-06

Training epoch-70 batch-129
Running loss of epoch-70 batch-129 = 1.6023404896259308e-06

Training epoch-70 batch-130
Running loss of epoch-70 batch-130 = 2.041459083557129e-06

Training epoch-70 batch-131
Running loss of epoch-70 batch-131 = 1.8263235688209534e-06

Training epoch-70 batch-132
Running loss of epoch-70 batch-132 = 1.834239810705185e-06

Training epoch-70 batch-133
Running loss of epoch-70 batch-133 = 1.2451782822608948e-06

Training epoch-70 batch-134
Running loss of epoch-70 batch-134 = 2.6314519345760345e-06

Training epoch-70 batch-135
Running loss of epoch-70 batch-135 = 1.0442454367876053e-06

Training epoch-70 batch-136
Running loss of epoch-70 batch-136 = 1.1760275810956955e-06

Training epoch-70 batch-137
Running loss of epoch-70 batch-137 = 1.9294675439596176e-06

Training epoch-70 batch-138
Running loss of epoch-70 batch-138 = 1.5189871191978455e-06

Training epoch-70 batch-139
Running loss of epoch-70 batch-139 = 1.7851125448942184e-06

Training epoch-70 batch-140
Running loss of epoch-70 batch-140 = 2.05356627702713e-06

Training epoch-70 batch-141
Running loss of epoch-70 batch-141 = 1.4938414096832275e-06

Training epoch-70 batch-142
Running loss of epoch-70 batch-142 = 1.487554982304573e-06

Training epoch-70 batch-143
Running loss of epoch-70 batch-143 = 1.905485987663269e-06

Training epoch-70 batch-144
Running loss of epoch-70 batch-144 = 1.6703270375728607e-06

Training epoch-70 batch-145
Running loss of epoch-70 batch-145 = 1.3846438378095627e-06

Training epoch-70 batch-146
Running loss of epoch-70 batch-146 = 1.766253262758255e-06

Training epoch-70 batch-147
Running loss of epoch-70 batch-147 = 2.1134037524461746e-06

Training epoch-70 batch-148
Running loss of epoch-70 batch-148 = 1.5604309737682343e-06

Training epoch-70 batch-149
Running loss of epoch-70 batch-149 = 1.386040821671486e-06

Training epoch-70 batch-150
Running loss of epoch-70 batch-150 = 1.9455328583717346e-06

Training epoch-70 batch-151
Running loss of epoch-70 batch-151 = 2.001645043492317e-06

Training epoch-70 batch-152
Running loss of epoch-70 batch-152 = 2.0042061805725098e-06

Training epoch-70 batch-153
Running loss of epoch-70 batch-153 = 2.0614825189113617e-06

Training epoch-70 batch-154
Running loss of epoch-70 batch-154 = 1.6260892152786255e-06

Training epoch-70 batch-155
Running loss of epoch-70 batch-155 = 1.8889550119638443e-06

Training epoch-70 batch-156
Running loss of epoch-70 batch-156 = 2.2167805582284927e-06

Training epoch-70 batch-157
Running loss of epoch-70 batch-157 = 4.7460198402404785e-06

Finished training epoch-70.



Average train loss at epoch-70 = 1.7970100045204162e-06

Started Evaluation

Average val loss at epoch-70 = 3.224855662960755

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-71


Training epoch-71 batch-1
Running loss of epoch-71 batch-1 = 1.3634562492370605e-06

Training epoch-71 batch-2
Running loss of epoch-71 batch-2 = 2.09687277674675e-06

Training epoch-71 batch-3
Running loss of epoch-71 batch-3 = 1.780455932021141e-06

Training epoch-71 batch-4
Running loss of epoch-71 batch-4 = 2.1478626877069473e-06

Training epoch-71 batch-5
Running loss of epoch-71 batch-5 = 1.257285475730896e-06

Training epoch-71 batch-6
Running loss of epoch-71 batch-6 = 1.4158431440591812e-06

Training epoch-71 batch-7
Running loss of epoch-71 batch-7 = 1.2672971934080124e-06

Training epoch-71 batch-8
Running loss of epoch-71 batch-8 = 1.810956746339798e-06

Training epoch-71 batch-9
Running loss of epoch-71 batch-9 = 2.138083800673485e-06

Training epoch-71 batch-10
Running loss of epoch-71 batch-10 = 1.4451798051595688e-06

Training epoch-71 batch-11
Running loss of epoch-71 batch-11 = 2.194894477725029e-06

Training epoch-71 batch-12
Running loss of epoch-71 batch-12 = 1.9283033907413483e-06

Training epoch-71 batch-13
Running loss of epoch-71 batch-13 = 1.8978025764226913e-06

Training epoch-71 batch-14
Running loss of epoch-71 batch-14 = 2.085929736495018e-06

Training epoch-71 batch-15
Running loss of epoch-71 batch-15 = 1.310836523771286e-06

Training epoch-71 batch-16
Running loss of epoch-71 batch-16 = 1.4309771358966827e-06

Training epoch-71 batch-17
Running loss of epoch-71 batch-17 = 1.2759119272232056e-06

Training epoch-71 batch-18
Running loss of epoch-71 batch-18 = 1.8987338989973068e-06

Training epoch-71 batch-19
Running loss of epoch-71 batch-19 = 1.3282988220453262e-06

Training epoch-71 batch-20
Running loss of epoch-71 batch-20 = 1.496635377407074e-06

Training epoch-71 batch-21
Running loss of epoch-71 batch-21 = 1.5071127563714981e-06

Training epoch-71 batch-22
Running loss of epoch-71 batch-22 = 1.6628764569759369e-06

Training epoch-71 batch-23
Running loss of epoch-71 batch-23 = 1.5869736671447754e-06

Training epoch-71 batch-24
Running loss of epoch-71 batch-24 = 1.4666002243757248e-06

Training epoch-71 batch-25
Running loss of epoch-71 batch-25 = 1.7653219401836395e-06

Training epoch-71 batch-26
Running loss of epoch-71 batch-26 = 2.7276109904050827e-06

Training epoch-71 batch-27
Running loss of epoch-71 batch-27 = 1.8549617379903793e-06

Training epoch-71 batch-28
Running loss of epoch-71 batch-28 = 2.3096799850463867e-06

Training epoch-71 batch-29
Running loss of epoch-71 batch-29 = 2.575572580099106e-06

Training epoch-71 batch-30
Running loss of epoch-71 batch-30 = 2.430286258459091e-06

Training epoch-71 batch-31
Running loss of epoch-71 batch-31 = 2.075452357530594e-06

Training epoch-71 batch-32
Running loss of epoch-71 batch-32 = 1.4654360711574554e-06

Training epoch-71 batch-33
Running loss of epoch-71 batch-33 = 1.4530960470438004e-06

Training epoch-71 batch-34
Running loss of epoch-71 batch-34 = 1.1739321053028107e-06

Training epoch-71 batch-35
Running loss of epoch-71 batch-35 = 1.650303602218628e-06

Training epoch-71 batch-36
Running loss of epoch-71 batch-36 = 2.5243498384952545e-06

Training epoch-71 batch-37
Running loss of epoch-71 batch-37 = 1.112697646021843e-06

Training epoch-71 batch-38
Running loss of epoch-71 batch-38 = 2.247048541903496e-06

Training epoch-71 batch-39
Running loss of epoch-71 batch-39 = 2.144603058695793e-06

Training epoch-71 batch-40
Running loss of epoch-71 batch-40 = 1.3334210962057114e-06

Training epoch-71 batch-41
Running loss of epoch-71 batch-41 = 1.5420373529195786e-06

Training epoch-71 batch-42
Running loss of epoch-71 batch-42 = 1.7578713595867157e-06

Training epoch-71 batch-43
Running loss of epoch-71 batch-43 = 1.2991949915885925e-06

Training epoch-71 batch-44
Running loss of epoch-71 batch-44 = 2.266606315970421e-06

Training epoch-71 batch-45
Running loss of epoch-71 batch-45 = 2.6531051844358444e-06

Training epoch-71 batch-46
Running loss of epoch-71 batch-46 = 1.9532162696123123e-06

Training epoch-71 batch-47
Running loss of epoch-71 batch-47 = 2.1441373974084854e-06

Training epoch-71 batch-48
Running loss of epoch-71 batch-48 = 1.7879065126180649e-06

Training epoch-71 batch-49
Running loss of epoch-71 batch-49 = 1.4035031199455261e-06

Training epoch-71 batch-50
Running loss of epoch-71 batch-50 = 2.184184268116951e-06

Training epoch-71 batch-51
Running loss of epoch-71 batch-51 = 1.9317958503961563e-06

Training epoch-71 batch-52
Running loss of epoch-71 batch-52 = 2.0710285753011703e-06

Training epoch-71 batch-53
Running loss of epoch-71 batch-53 = 1.7222482711076736e-06

Training epoch-71 batch-54
Running loss of epoch-71 batch-54 = 2.139247953891754e-06

Training epoch-71 batch-55
Running loss of epoch-71 batch-55 = 1.1031515896320343e-06

Training epoch-71 batch-56
Running loss of epoch-71 batch-56 = 2.7562491595745087e-06

Training epoch-71 batch-57
Running loss of epoch-71 batch-57 = 1.996522769331932e-06

Training epoch-71 batch-58
Running loss of epoch-71 batch-58 = 1.9867438822984695e-06

Training epoch-71 batch-59
Running loss of epoch-71 batch-59 = 1.7993152141571045e-06

Training epoch-71 batch-60
Running loss of epoch-71 batch-60 = 2.7159694582223892e-06

Training epoch-71 batch-61
Running loss of epoch-71 batch-61 = 2.346932888031006e-06

Training epoch-71 batch-62
Running loss of epoch-71 batch-62 = 1.6388949006795883e-06

Training epoch-71 batch-63
Running loss of epoch-71 batch-63 = 1.5529803931713104e-06

Training epoch-71 batch-64
Running loss of epoch-71 batch-64 = 1.3820827007293701e-06

Training epoch-71 batch-65
Running loss of epoch-71 batch-65 = 2.8996728360652924e-06

Training epoch-71 batch-66
Running loss of epoch-71 batch-66 = 1.5727709978818893e-06

Training epoch-71 batch-67
Running loss of epoch-71 batch-67 = 1.6826670616865158e-06

Training epoch-71 batch-68
Running loss of epoch-71 batch-68 = 2.5634653866291046e-06

Training epoch-71 batch-69
Running loss of epoch-71 batch-69 = 1.8407590687274933e-06

Training epoch-71 batch-70
Running loss of epoch-71 batch-70 = 1.7492566257715225e-06

Training epoch-71 batch-71
Running loss of epoch-71 batch-71 = 1.2810342013835907e-06

Training epoch-71 batch-72
Running loss of epoch-71 batch-72 = 2.4491455405950546e-06

Training epoch-71 batch-73
Running loss of epoch-71 batch-73 = 1.0074581950902939e-06

Training epoch-71 batch-74
Running loss of epoch-71 batch-74 = 1.9837170839309692e-06

Training epoch-71 batch-75
Running loss of epoch-71 batch-75 = 1.8440186977386475e-06

Training epoch-71 batch-76
Running loss of epoch-71 batch-76 = 1.3301614671945572e-06

Training epoch-71 batch-77
Running loss of epoch-71 batch-77 = 1.701992005109787e-06

Training epoch-71 batch-78
Running loss of epoch-71 batch-78 = 1.6526319086551666e-06

Training epoch-71 batch-79
Running loss of epoch-71 batch-79 = 1.4570541679859161e-06

Training epoch-71 batch-80
Running loss of epoch-71 batch-80 = 1.6386620700359344e-06

Training epoch-71 batch-81
Running loss of epoch-71 batch-81 = 2.525513991713524e-06

Training epoch-71 batch-82
Running loss of epoch-71 batch-82 = 1.5189871191978455e-06

Training epoch-71 batch-83
Running loss of epoch-71 batch-83 = 1.3045500963926315e-06

Training epoch-71 batch-84
Running loss of epoch-71 batch-84 = 2.2798776626586914e-06

Training epoch-71 batch-85
Running loss of epoch-71 batch-85 = 1.2798700481653214e-06

Training epoch-71 batch-86
Running loss of epoch-71 batch-86 = 2.0153820514678955e-06

Training epoch-71 batch-87
Running loss of epoch-71 batch-87 = 2.262648195028305e-06

Training epoch-71 batch-88
Running loss of epoch-71 batch-88 = 1.4873221516609192e-06

Training epoch-71 batch-89
Running loss of epoch-71 batch-89 = 1.273350790143013e-06

Training epoch-71 batch-90
Running loss of epoch-71 batch-90 = 1.4707911759614944e-06

Training epoch-71 batch-91
Running loss of epoch-71 batch-91 = 2.1457672119140625e-06

Training epoch-71 batch-92
Running loss of epoch-71 batch-92 = 2.4284236133098602e-06

Training epoch-71 batch-93
Running loss of epoch-71 batch-93 = 1.7844140529632568e-06

Training epoch-71 batch-94
Running loss of epoch-71 batch-94 = 1.4058314263820648e-06

Training epoch-71 batch-95
Running loss of epoch-71 batch-95 = 2.021901309490204e-06

Training epoch-71 batch-96
Running loss of epoch-71 batch-96 = 1.339241862297058e-06

Training epoch-71 batch-97
Running loss of epoch-71 batch-97 = 2.218177542090416e-06

Training epoch-71 batch-98
Running loss of epoch-71 batch-98 = 1.7497222870588303e-06

Training epoch-71 batch-99
Running loss of epoch-71 batch-99 = 1.7553102225065231e-06

Training epoch-71 batch-100
Running loss of epoch-71 batch-100 = 2.466375008225441e-06

Training epoch-71 batch-101
Running loss of epoch-71 batch-101 = 1.8114224076271057e-06

Training epoch-71 batch-102
Running loss of epoch-71 batch-102 = 1.7045531421899796e-06

Training epoch-71 batch-103
Running loss of epoch-71 batch-103 = 1.46450474858284e-06

Training epoch-71 batch-104
Running loss of epoch-71 batch-104 = 1.4798715710639954e-06

Training epoch-71 batch-105
Running loss of epoch-71 batch-105 = 8.563511073589325e-07

Training epoch-71 batch-106
Running loss of epoch-71 batch-106 = 1.4470424503087997e-06

Training epoch-71 batch-107
Running loss of epoch-71 batch-107 = 1.819804310798645e-06

Training epoch-71 batch-108
Running loss of epoch-71 batch-108 = 9.688083082437515e-07

Training epoch-71 batch-109
Running loss of epoch-71 batch-109 = 1.2458767741918564e-06

Training epoch-71 batch-110
Running loss of epoch-71 batch-110 = 1.2640375643968582e-06

Training epoch-71 batch-111
Running loss of epoch-71 batch-111 = 2.005370333790779e-06

Training epoch-71 batch-112
Running loss of epoch-71 batch-112 = 2.2980384528636932e-06

Training epoch-71 batch-113
Running loss of epoch-71 batch-113 = 1.3585668057203293e-06

Training epoch-71 batch-114
Running loss of epoch-71 batch-114 = 1.0528601706027985e-06

Training epoch-71 batch-115
Running loss of epoch-71 batch-115 = 1.9473955035209656e-06

Training epoch-71 batch-116
Running loss of epoch-71 batch-116 = 1.6386620700359344e-06

Training epoch-71 batch-117
Running loss of epoch-71 batch-117 = 1.2700911611318588e-06

Training epoch-71 batch-118
Running loss of epoch-71 batch-118 = 1.864507794380188e-06

Training epoch-71 batch-119
Running loss of epoch-71 batch-119 = 1.9222497940063477e-06

Training epoch-71 batch-120
Running loss of epoch-71 batch-120 = 9.599607437849045e-07

Training epoch-71 batch-121
Running loss of epoch-71 batch-121 = 2.0707957446575165e-06

Training epoch-71 batch-122
Running loss of epoch-71 batch-122 = 1.2882519513368607e-06

Training epoch-71 batch-123
Running loss of epoch-71 batch-123 = 1.6158446669578552e-06

Training epoch-71 batch-124
Running loss of epoch-71 batch-124 = 2.6838388293981552e-06

Training epoch-71 batch-125
Running loss of epoch-71 batch-125 = 1.7881393432617188e-06

Training epoch-71 batch-126
Running loss of epoch-71 batch-126 = 2.9257498681545258e-06

Training epoch-71 batch-127
Running loss of epoch-71 batch-127 = 1.9152648746967316e-06

Training epoch-71 batch-128
Running loss of epoch-71 batch-128 = 1.6735866665840149e-06

Training epoch-71 batch-129
Running loss of epoch-71 batch-129 = 2.8354115784168243e-06

Training epoch-71 batch-130
Running loss of epoch-71 batch-130 = 2.119923010468483e-06

Training epoch-71 batch-131
Running loss of epoch-71 batch-131 = 1.2861564755439758e-06

Training epoch-71 batch-132
Running loss of epoch-71 batch-132 = 1.58604234457016e-06

Training epoch-71 batch-133
Running loss of epoch-71 batch-133 = 2.0775478333234787e-06

Training epoch-71 batch-134
Running loss of epoch-71 batch-134 = 1.4898832887411118e-06

Training epoch-71 batch-135
Running loss of epoch-71 batch-135 = 1.9883736968040466e-06

Training epoch-71 batch-136
Running loss of epoch-71 batch-136 = 1.5883706510066986e-06

Training epoch-71 batch-137
Running loss of epoch-71 batch-137 = 1.56438909471035e-06

Training epoch-71 batch-138
Running loss of epoch-71 batch-138 = 1.0991934686899185e-06

Training epoch-71 batch-139
Running loss of epoch-71 batch-139 = 1.6686972230672836e-06

Training epoch-71 batch-140
Running loss of epoch-71 batch-140 = 1.7718411982059479e-06

Training epoch-71 batch-141
Running loss of epoch-71 batch-141 = 1.280335709452629e-06

Training epoch-71 batch-142
Running loss of epoch-71 batch-142 = 1.9422732293605804e-06

Training epoch-71 batch-143
Running loss of epoch-71 batch-143 = 1.562759280204773e-06

Training epoch-71 batch-144
Running loss of epoch-71 batch-144 = 1.5385448932647705e-06

Training epoch-71 batch-145
Running loss of epoch-71 batch-145 = 1.4356337487697601e-06

Training epoch-71 batch-146
Running loss of epoch-71 batch-146 = 2.0740553736686707e-06

Training epoch-71 batch-147
Running loss of epoch-71 batch-147 = 1.7615966498851776e-06

Training epoch-71 batch-148
Running loss of epoch-71 batch-148 = 1.5615951269865036e-06

Training epoch-71 batch-149
Running loss of epoch-71 batch-149 = 1.4272518455982208e-06

Training epoch-71 batch-150
Running loss of epoch-71 batch-150 = 1.3005919754505157e-06

Training epoch-71 batch-151
Running loss of epoch-71 batch-151 = 1.2463424354791641e-06

Training epoch-71 batch-152
Running loss of epoch-71 batch-152 = 2.002343535423279e-06

Training epoch-71 batch-153
Running loss of epoch-71 batch-153 = 1.209089532494545e-06

Training epoch-71 batch-154
Running loss of epoch-71 batch-154 = 1.0218936949968338e-06

Training epoch-71 batch-155
Running loss of epoch-71 batch-155 = 2.2188760340213776e-06

Training epoch-71 batch-156
Running loss of epoch-71 batch-156 = 1.4114193618297577e-06

Training epoch-71 batch-157
Running loss of epoch-71 batch-157 = 1.8142163753509521e-06

Finished training epoch-71.



Average train loss at epoch-71 = 1.7601743340492249e-06

Started Evaluation

Average val loss at epoch-71 = 3.229520697342722

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-72


Training epoch-72 batch-1
Running loss of epoch-72 batch-1 = 1.9816216081380844e-06

Training epoch-72 batch-2
Running loss of epoch-72 batch-2 = 2.216082066297531e-06

Training epoch-72 batch-3
Running loss of epoch-72 batch-3 = 1.9173603504896164e-06

Training epoch-72 batch-4
Running loss of epoch-72 batch-4 = 1.141335815191269e-06

Training epoch-72 batch-5
Running loss of epoch-72 batch-5 = 1.7401762306690216e-06

Training epoch-72 batch-6
Running loss of epoch-72 batch-6 = 2.3934990167617798e-06

Training epoch-72 batch-7
Running loss of epoch-72 batch-7 = 9.152572602033615e-07

Training epoch-72 batch-8
Running loss of epoch-72 batch-8 = 1.6097910702228546e-06

Training epoch-72 batch-9
Running loss of epoch-72 batch-9 = 1.4721881598234177e-06

Training epoch-72 batch-10
Running loss of epoch-72 batch-10 = 1.7685815691947937e-06

Training epoch-72 batch-11
Running loss of epoch-72 batch-11 = 1.487554982304573e-06

Training epoch-72 batch-12
Running loss of epoch-72 batch-12 = 1.3224780559539795e-06

Training epoch-72 batch-13
Running loss of epoch-72 batch-13 = 2.754153683781624e-06

Training epoch-72 batch-14
Running loss of epoch-72 batch-14 = 1.5925616025924683e-06

Training epoch-72 batch-15
Running loss of epoch-72 batch-15 = 1.8454156816005707e-06

Training epoch-72 batch-16
Running loss of epoch-72 batch-16 = 8.274801075458527e-07

Training epoch-72 batch-17
Running loss of epoch-72 batch-17 = 2.2477470338344574e-06

Training epoch-72 batch-18
Running loss of epoch-72 batch-18 = 1.4316756278276443e-06

Training epoch-72 batch-19
Running loss of epoch-72 batch-19 = 2.019573003053665e-06

Training epoch-72 batch-20
Running loss of epoch-72 batch-20 = 1.764390617609024e-06

Training epoch-72 batch-21
Running loss of epoch-72 batch-21 = 1.4384277164936066e-06

Training epoch-72 batch-22
Running loss of epoch-72 batch-22 = 1.8193386495113373e-06

Training epoch-72 batch-23
Running loss of epoch-72 batch-23 = 1.9816216081380844e-06

Training epoch-72 batch-24
Running loss of epoch-72 batch-24 = 1.5404075384140015e-06

Training epoch-72 batch-25
Running loss of epoch-72 batch-25 = 1.264270395040512e-06

Training epoch-72 batch-26
Running loss of epoch-72 batch-26 = 1.8782448023557663e-06

Training epoch-72 batch-27
Running loss of epoch-72 batch-27 = 1.8803402781486511e-06

Training epoch-72 batch-28
Running loss of epoch-72 batch-28 = 1.6065314412117004e-06

Training epoch-72 batch-29
Running loss of epoch-72 batch-29 = 1.3676472008228302e-06

Training epoch-72 batch-30
Running loss of epoch-72 batch-30 = 2.5280751287937164e-06

Training epoch-72 batch-31
Running loss of epoch-72 batch-31 = 2.3103784769773483e-06

Training epoch-72 batch-32
Running loss of epoch-72 batch-32 = 2.1117739379405975e-06

Training epoch-72 batch-33
Running loss of epoch-72 batch-33 = 2.0426232367753983e-06

Training epoch-72 batch-34
Running loss of epoch-72 batch-34 = 1.7299316823482513e-06

Training epoch-72 batch-35
Running loss of epoch-72 batch-35 = 1.7050188034772873e-06

Training epoch-72 batch-36
Running loss of epoch-72 batch-36 = 1.9581057131290436e-06

Training epoch-72 batch-37
Running loss of epoch-72 batch-37 = 1.9292347133159637e-06

Training epoch-72 batch-38
Running loss of epoch-72 batch-38 = 1.1709053069353104e-06

Training epoch-72 batch-39
Running loss of epoch-72 batch-39 = 1.7932616174221039e-06

Training epoch-72 batch-40
Running loss of epoch-72 batch-40 = 1.0342337191104889e-06

Training epoch-72 batch-41
Running loss of epoch-72 batch-41 = 1.0454095900058746e-06

Training epoch-72 batch-42
Running loss of epoch-72 batch-42 = 1.3278331607580185e-06

Training epoch-72 batch-43
Running loss of epoch-72 batch-43 = 1.3057142496109009e-06

Training epoch-72 batch-44
Running loss of epoch-72 batch-44 = 1.2165401130914688e-06

Training epoch-72 batch-45
Running loss of epoch-72 batch-45 = 2.2028107196092606e-06

Training epoch-72 batch-46
Running loss of epoch-72 batch-46 = 1.2775417417287827e-06

Training epoch-72 batch-47
Running loss of epoch-72 batch-47 = 2.3168977349996567e-06

Training epoch-72 batch-48
Running loss of epoch-72 batch-48 = 9.6089206635952e-07

Training epoch-72 batch-49
Running loss of epoch-72 batch-49 = 1.2312084436416626e-06

Training epoch-72 batch-50
Running loss of epoch-72 batch-50 = 1.000240445137024e-06

Training epoch-72 batch-51
Running loss of epoch-72 batch-51 = 1.7674174159765244e-06

Training epoch-72 batch-52
Running loss of epoch-72 batch-52 = 1.6093254089355469e-06

Training epoch-72 batch-53
Running loss of epoch-72 batch-53 = 2.0852312445640564e-06

Training epoch-72 batch-54
Running loss of epoch-72 batch-54 = 1.7040874809026718e-06

Training epoch-72 batch-55
Running loss of epoch-72 batch-55 = 1.459149643778801e-06

Training epoch-72 batch-56
Running loss of epoch-72 batch-56 = 1.4808028936386108e-06

Training epoch-72 batch-57
Running loss of epoch-72 batch-57 = 1.9071158021688461e-06

Training epoch-72 batch-58
Running loss of epoch-72 batch-58 = 1.7813872545957565e-06

Training epoch-72 batch-59
Running loss of epoch-72 batch-59 = 1.135747879743576e-06

Training epoch-72 batch-60
Running loss of epoch-72 batch-60 = 1.991167664527893e-06

Training epoch-72 batch-61
Running loss of epoch-72 batch-61 = 2.0782463252544403e-06

Training epoch-72 batch-62
Running loss of epoch-72 batch-62 = 8.218921720981598e-07

Training epoch-72 batch-63
Running loss of epoch-72 batch-63 = 2.4978071451187134e-06

Training epoch-72 batch-64
Running loss of epoch-72 batch-64 = 1.3299286365509033e-06

Training epoch-72 batch-65
Running loss of epoch-72 batch-65 = 1.6831327229738235e-06

Training epoch-72 batch-66
Running loss of epoch-72 batch-66 = 1.9294675439596176e-06

Training epoch-72 batch-67
Running loss of epoch-72 batch-67 = 2.3245811462402344e-06

Training epoch-72 batch-68
Running loss of epoch-72 batch-68 = 1.9264407455921173e-06

Training epoch-72 batch-69
Running loss of epoch-72 batch-69 = 2.0652078092098236e-06

Training epoch-72 batch-70
Running loss of epoch-72 batch-70 = 2.1352898329496384e-06

Training epoch-72 batch-71
Running loss of epoch-72 batch-71 = 1.1378433555364609e-06

Training epoch-72 batch-72
Running loss of epoch-72 batch-72 = 1.9758008420467377e-06

Training epoch-72 batch-73
Running loss of epoch-72 batch-73 = 1.898501068353653e-06

Training epoch-72 batch-74
Running loss of epoch-72 batch-74 = 1.696636900305748e-06

Training epoch-72 batch-75
Running loss of epoch-72 batch-75 = 1.9422732293605804e-06

Training epoch-72 batch-76
Running loss of epoch-72 batch-76 = 1.4656689018011093e-06

Training epoch-72 batch-77
Running loss of epoch-72 batch-77 = 2.7904752641916275e-06

Training epoch-72 batch-78
Running loss of epoch-72 batch-78 = 1.8507707864046097e-06

Training epoch-72 batch-79
Running loss of epoch-72 batch-79 = 1.734122633934021e-06

Training epoch-72 batch-80
Running loss of epoch-72 batch-80 = 2.2975727915763855e-06

Training epoch-72 batch-81
Running loss of epoch-72 batch-81 = 1.9560102373361588e-06

Training epoch-72 batch-82
Running loss of epoch-72 batch-82 = 1.748325303196907e-06

Training epoch-72 batch-83
Running loss of epoch-72 batch-83 = 1.6370322555303574e-06

Training epoch-72 batch-84
Running loss of epoch-72 batch-84 = 1.5739351511001587e-06

Training epoch-72 batch-85
Running loss of epoch-72 batch-85 = 2.6798807084560394e-06

Training epoch-72 batch-86
Running loss of epoch-72 batch-86 = 1.1897645890712738e-06

Training epoch-72 batch-87
Running loss of epoch-72 batch-87 = 1.8328428268432617e-06

Training epoch-72 batch-88
Running loss of epoch-72 batch-88 = 1.6402918845415115e-06

Training epoch-72 batch-89
Running loss of epoch-72 batch-89 = 2.1012965589761734e-06

Training epoch-72 batch-90
Running loss of epoch-72 batch-90 = 1.705717295408249e-06

Training epoch-72 batch-91
Running loss of epoch-72 batch-91 = 2.220040187239647e-06

Training epoch-72 batch-92
Running loss of epoch-72 batch-92 = 1.6957055777311325e-06

Training epoch-72 batch-93
Running loss of epoch-72 batch-93 = 1.258915290236473e-06

Training epoch-72 batch-94
Running loss of epoch-72 batch-94 = 1.4943070709705353e-06

Training epoch-72 batch-95
Running loss of epoch-72 batch-95 = 1.5862751752138138e-06

Training epoch-72 batch-96
Running loss of epoch-72 batch-96 = 1.6584526747465134e-06

Training epoch-72 batch-97
Running loss of epoch-72 batch-97 = 1.7513521015644073e-06

Training epoch-72 batch-98
Running loss of epoch-72 batch-98 = 1.6435515135526657e-06

Training epoch-72 batch-99
Running loss of epoch-72 batch-99 = 1.8111895769834518e-06

Training epoch-72 batch-100
Running loss of epoch-72 batch-100 = 1.3674143701791763e-06

Training epoch-72 batch-101
Running loss of epoch-72 batch-101 = 1.17509625852108e-06

Training epoch-72 batch-102
Running loss of epoch-72 batch-102 = 2.2908207029104233e-06

Training epoch-72 batch-103
Running loss of epoch-72 batch-103 = 1.482432708144188e-06

Training epoch-72 batch-104
Running loss of epoch-72 batch-104 = 1.3299286365509033e-06

Training epoch-72 batch-105
Running loss of epoch-72 batch-105 = 2.017710357904434e-06

Training epoch-72 batch-106
Running loss of epoch-72 batch-106 = 2.2565945982933044e-06

Training epoch-72 batch-107
Running loss of epoch-72 batch-107 = 2.375105395913124e-06

Training epoch-72 batch-108
Running loss of epoch-72 batch-108 = 1.5480909496545792e-06

Training epoch-72 batch-109
Running loss of epoch-72 batch-109 = 1.7655547708272934e-06

Training epoch-72 batch-110
Running loss of epoch-72 batch-110 = 1.5851110219955444e-06

Training epoch-72 batch-111
Running loss of epoch-72 batch-111 = 1.3611279428005219e-06

Training epoch-72 batch-112
Running loss of epoch-72 batch-112 = 1.428648829460144e-06

Training epoch-72 batch-113
Running loss of epoch-72 batch-113 = 1.2258533388376236e-06

Training epoch-72 batch-114
Running loss of epoch-72 batch-114 = 1.4386605471372604e-06

Training epoch-72 batch-115
Running loss of epoch-72 batch-115 = 1.8882565200328827e-06

Training epoch-72 batch-116
Running loss of epoch-72 batch-116 = 1.303618773818016e-06

Training epoch-72 batch-117
Running loss of epoch-72 batch-117 = 1.35018490254879e-06

Training epoch-72 batch-118
Running loss of epoch-72 batch-118 = 1.7122365534305573e-06

Training epoch-72 batch-119
Running loss of epoch-72 batch-119 = 1.827022060751915e-06

Training epoch-72 batch-120
Running loss of epoch-72 batch-120 = 1.7532147467136383e-06

Training epoch-72 batch-121
Running loss of epoch-72 batch-121 = 1.9152648746967316e-06

Training epoch-72 batch-122
Running loss of epoch-72 batch-122 = 2.0605511963367462e-06

Training epoch-72 batch-123
Running loss of epoch-72 batch-123 = 2.2777821868658066e-06

Training epoch-72 batch-124
Running loss of epoch-72 batch-124 = 1.530628651380539e-06

Training epoch-72 batch-125
Running loss of epoch-72 batch-125 = 1.3601966202259064e-06

Training epoch-72 batch-126
Running loss of epoch-72 batch-126 = 1.8540304154157639e-06

Training epoch-72 batch-127
Running loss of epoch-72 batch-127 = 2.1676532924175262e-06

Training epoch-72 batch-128
Running loss of epoch-72 batch-128 = 1.5071127563714981e-06

Training epoch-72 batch-129
Running loss of epoch-72 batch-129 = 1.6225967556238174e-06

Training epoch-72 batch-130
Running loss of epoch-72 batch-130 = 2.459390088915825e-06

Training epoch-72 batch-131
Running loss of epoch-72 batch-131 = 1.6505364328622818e-06

Training epoch-72 batch-132
Running loss of epoch-72 batch-132 = 2.480344846844673e-06

Training epoch-72 batch-133
Running loss of epoch-72 batch-133 = 1.852167770266533e-06

Training epoch-72 batch-134
Running loss of epoch-72 batch-134 = 1.1203810572624207e-06

Training epoch-72 batch-135
Running loss of epoch-72 batch-135 = 1.7748679965734482e-06

Training epoch-72 batch-136
Running loss of epoch-72 batch-136 = 1.3937242329120636e-06

Training epoch-72 batch-137
Running loss of epoch-72 batch-137 = 1.4479737728834152e-06

Training epoch-72 batch-138
Running loss of epoch-72 batch-138 = 8.670613169670105e-07

Training epoch-72 batch-139
Running loss of epoch-72 batch-139 = 1.2952368706464767e-06

Training epoch-72 batch-140
Running loss of epoch-72 batch-140 = 1.6470439732074738e-06

Training epoch-72 batch-141
Running loss of epoch-72 batch-141 = 1.2889504432678223e-06

Training epoch-72 batch-142
Running loss of epoch-72 batch-142 = 2.0526349544525146e-06

Training epoch-72 batch-143
Running loss of epoch-72 batch-143 = 1.632142812013626e-06

Training epoch-72 batch-144
Running loss of epoch-72 batch-144 = 2.430519089102745e-06

Training epoch-72 batch-145
Running loss of epoch-72 batch-145 = 2.1476298570632935e-06

Training epoch-72 batch-146
Running loss of epoch-72 batch-146 = 1.6419216990470886e-06

Training epoch-72 batch-147
Running loss of epoch-72 batch-147 = 1.5969853848218918e-06

Training epoch-72 batch-148
Running loss of epoch-72 batch-148 = 2.032611519098282e-06

Training epoch-72 batch-149
Running loss of epoch-72 batch-149 = 2.0819716155529022e-06

Training epoch-72 batch-150
Running loss of epoch-72 batch-150 = 1.3259705156087875e-06

Training epoch-72 batch-151
Running loss of epoch-72 batch-151 = 1.8603168427944183e-06

Training epoch-72 batch-152
Running loss of epoch-72 batch-152 = 1.5762634575366974e-06

Training epoch-72 batch-153
Running loss of epoch-72 batch-153 = 2.5739427655935287e-06

Training epoch-72 batch-154
Running loss of epoch-72 batch-154 = 1.3378448784351349e-06

Training epoch-72 batch-155
Running loss of epoch-72 batch-155 = 1.6319099813699722e-06

Training epoch-72 batch-156
Running loss of epoch-72 batch-156 = 1.9040890038013458e-06

Training epoch-72 batch-157
Running loss of epoch-72 batch-157 = 9.82731580734253e-06

Finished training epoch-72.



Average train loss at epoch-72 = 1.7372936010360718e-06

Started Evaluation

Average val loss at epoch-72 = 3.2339158685583818

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-73


Training epoch-73 batch-1
Running loss of epoch-73 batch-1 = 1.662643626332283e-06

Training epoch-73 batch-2
Running loss of epoch-73 batch-2 = 2.939952537417412e-06

Training epoch-73 batch-3
Running loss of epoch-73 batch-3 = 1.396751031279564e-06

Training epoch-73 batch-4
Running loss of epoch-73 batch-4 = 1.3783574104309082e-06

Training epoch-73 batch-5
Running loss of epoch-73 batch-5 = 2.2761523723602295e-06

Training epoch-73 batch-6
Running loss of epoch-73 batch-6 = 1.1182855814695358e-06

Training epoch-73 batch-7
Running loss of epoch-73 batch-7 = 1.4738179743289948e-06

Training epoch-73 batch-8
Running loss of epoch-73 batch-8 = 1.453561708331108e-06

Training epoch-73 batch-9
Running loss of epoch-73 batch-9 = 1.391395926475525e-06

Training epoch-73 batch-10
Running loss of epoch-73 batch-10 = 2.2207386791706085e-06

Training epoch-73 batch-11
Running loss of epoch-73 batch-11 = 1.4156103134155273e-06

Training epoch-73 batch-12
Running loss of epoch-73 batch-12 = 1.2782402336597443e-06

Training epoch-73 batch-13
Running loss of epoch-73 batch-13 = 1.530395820736885e-06

Training epoch-73 batch-14
Running loss of epoch-73 batch-14 = 1.185806468129158e-06

Training epoch-73 batch-15
Running loss of epoch-73 batch-15 = 1.9671861082315445e-06

Training epoch-73 batch-16
Running loss of epoch-73 batch-16 = 1.375330612063408e-06

Training epoch-73 batch-17
Running loss of epoch-73 batch-17 = 2.0978040993213654e-06

Training epoch-73 batch-18
Running loss of epoch-73 batch-18 = 1.3310927897691727e-06

Training epoch-73 batch-19
Running loss of epoch-73 batch-19 = 1.9688159227371216e-06

Training epoch-73 batch-20
Running loss of epoch-73 batch-20 = 1.6051344573497772e-06

Training epoch-73 batch-21
Running loss of epoch-73 batch-21 = 2.0351726561784744e-06

Training epoch-73 batch-22
Running loss of epoch-73 batch-22 = 2.11433507502079e-06

Training epoch-73 batch-23
Running loss of epoch-73 batch-23 = 2.1061860024929047e-06

Training epoch-73 batch-24
Running loss of epoch-73 batch-24 = 2.1497253328561783e-06

Training epoch-73 batch-25
Running loss of epoch-73 batch-25 = 1.7972197383642197e-06

Training epoch-73 batch-26
Running loss of epoch-73 batch-26 = 1.978827640414238e-06

Training epoch-73 batch-27
Running loss of epoch-73 batch-27 = 1.875218003988266e-06

Training epoch-73 batch-28
Running loss of epoch-73 batch-28 = 1.5427358448505402e-06

Training epoch-73 batch-29
Running loss of epoch-73 batch-29 = 2.375338226556778e-06

Training epoch-73 batch-30
Running loss of epoch-73 batch-30 = 2.2491440176963806e-06

Training epoch-73 batch-31
Running loss of epoch-73 batch-31 = 1.9527506083250046e-06

Training epoch-73 batch-32
Running loss of epoch-73 batch-32 = 1.4151446521282196e-06

Training epoch-73 batch-33
Running loss of epoch-73 batch-33 = 1.893145963549614e-06

Training epoch-73 batch-34
Running loss of epoch-73 batch-34 = 1.0069925338029861e-06

Training epoch-73 batch-35
Running loss of epoch-73 batch-35 = 1.6940757632255554e-06

Training epoch-73 batch-36
Running loss of epoch-73 batch-36 = 2.037500962615013e-06

Training epoch-73 batch-37
Running loss of epoch-73 batch-37 = 1.482199877500534e-06

Training epoch-73 batch-38
Running loss of epoch-73 batch-38 = 1.8773134797811508e-06

Training epoch-73 batch-39
Running loss of epoch-73 batch-39 = 1.0461080819368362e-06

Training epoch-73 batch-40
Running loss of epoch-73 batch-40 = 1.1515803635120392e-06

Training epoch-73 batch-41
Running loss of epoch-73 batch-41 = 1.4505349099636078e-06

Training epoch-73 batch-42
Running loss of epoch-73 batch-42 = 2.132263034582138e-06

Training epoch-73 batch-43
Running loss of epoch-73 batch-43 = 2.1527521312236786e-06

Training epoch-73 batch-44
Running loss of epoch-73 batch-44 = 9.550713002681732e-07

Training epoch-73 batch-45
Running loss of epoch-73 batch-45 = 1.6170088201761246e-06

Training epoch-73 batch-46
Running loss of epoch-73 batch-46 = 1.6468111425638199e-06

Training epoch-73 batch-47
Running loss of epoch-73 batch-47 = 1.371605321764946e-06

Training epoch-73 batch-48
Running loss of epoch-73 batch-48 = 1.650070771574974e-06

Training epoch-73 batch-49
Running loss of epoch-73 batch-49 = 1.8619466572999954e-06

Training epoch-73 batch-50
Running loss of epoch-73 batch-50 = 8.591450750827789e-07

Training epoch-73 batch-51
Running loss of epoch-73 batch-51 = 2.0829029381275177e-06

Training epoch-73 batch-52
Running loss of epoch-73 batch-52 = 1.7106067389249802e-06

Training epoch-73 batch-53
Running loss of epoch-73 batch-53 = 1.894775778055191e-06

Training epoch-73 batch-54
Running loss of epoch-73 batch-54 = 1.6284175217151642e-06

Training epoch-73 batch-55
Running loss of epoch-73 batch-55 = 1.959037035703659e-06

Training epoch-73 batch-56
Running loss of epoch-73 batch-56 = 2.304092049598694e-06

Training epoch-73 batch-57
Running loss of epoch-73 batch-57 = 2.346467226743698e-06

Training epoch-73 batch-58
Running loss of epoch-73 batch-58 = 1.369742676615715e-06

Training epoch-73 batch-59
Running loss of epoch-73 batch-59 = 1.4263205230236053e-06

Training epoch-73 batch-60
Running loss of epoch-73 batch-60 = 1.8202699720859528e-06

Training epoch-73 batch-61
Running loss of epoch-73 batch-61 = 2.0139850676059723e-06

Training epoch-73 batch-62
Running loss of epoch-73 batch-62 = 1.7972197383642197e-06

Training epoch-73 batch-63
Running loss of epoch-73 batch-63 = 1.814449205994606e-06

Training epoch-73 batch-64
Running loss of epoch-73 batch-64 = 1.7345882952213287e-06

Training epoch-73 batch-65
Running loss of epoch-73 batch-65 = 1.8214341253042221e-06

Training epoch-73 batch-66
Running loss of epoch-73 batch-66 = 1.3825483620166779e-06

Training epoch-73 batch-67
Running loss of epoch-73 batch-67 = 1.4649704098701477e-06

Training epoch-73 batch-68
Running loss of epoch-73 batch-68 = 1.4959368854761124e-06

Training epoch-73 batch-69
Running loss of epoch-73 batch-69 = 1.1101365089416504e-06

Training epoch-73 batch-70
Running loss of epoch-73 batch-70 = 1.0917428880929947e-06

Training epoch-73 batch-71
Running loss of epoch-73 batch-71 = 2.1564774215221405e-06

Training epoch-73 batch-72
Running loss of epoch-73 batch-72 = 1.5865080058574677e-06

Training epoch-73 batch-73
Running loss of epoch-73 batch-73 = 1.9010622054338455e-06

Training epoch-73 batch-74
Running loss of epoch-73 batch-74 = 1.6633421182632446e-06

Training epoch-73 batch-75
Running loss of epoch-73 batch-75 = 1.650303602218628e-06

Training epoch-73 batch-76
Running loss of epoch-73 batch-76 = 2.069864422082901e-06

Training epoch-73 batch-77
Running loss of epoch-73 batch-77 = 1.432374119758606e-06

Training epoch-73 batch-78
Running loss of epoch-73 batch-78 = 2.334127202630043e-06

Training epoch-73 batch-79
Running loss of epoch-73 batch-79 = 1.5317928045988083e-06

Training epoch-73 batch-80
Running loss of epoch-73 batch-80 = 1.8416903913021088e-06

Training epoch-73 batch-81
Running loss of epoch-73 batch-81 = 2.123182639479637e-06

Training epoch-73 batch-82
Running loss of epoch-73 batch-82 = 1.130392774939537e-06

Training epoch-73 batch-83
Running loss of epoch-73 batch-83 = 1.319916918873787e-06

Training epoch-73 batch-84
Running loss of epoch-73 batch-84 = 1.0156072676181793e-06

Training epoch-73 batch-85
Running loss of epoch-73 batch-85 = 1.979060471057892e-06

Training epoch-73 batch-86
Running loss of epoch-73 batch-86 = 9.939540177583694e-07

Training epoch-73 batch-87
Running loss of epoch-73 batch-87 = 1.80746428668499e-06

Training epoch-73 batch-88
Running loss of epoch-73 batch-88 = 1.3050157576799393e-06

Training epoch-73 batch-89
Running loss of epoch-73 batch-89 = 2.6712659746408463e-06

Training epoch-73 batch-90
Running loss of epoch-73 batch-90 = 1.6083940863609314e-06

Training epoch-73 batch-91
Running loss of epoch-73 batch-91 = 1.2153759598731995e-06

Training epoch-73 batch-92
Running loss of epoch-73 batch-92 = 1.507345587015152e-06

Training epoch-73 batch-93
Running loss of epoch-73 batch-93 = 1.480570062994957e-06

Training epoch-73 batch-94
Running loss of epoch-73 batch-94 = 1.280335709452629e-06

Training epoch-73 batch-95
Running loss of epoch-73 batch-95 = 1.4863908290863037e-06

Training epoch-73 batch-96
Running loss of epoch-73 batch-96 = 1.453794538974762e-06

Training epoch-73 batch-97
Running loss of epoch-73 batch-97 = 1.5532132238149643e-06

Training epoch-73 batch-98
Running loss of epoch-73 batch-98 = 1.712702214717865e-06

Training epoch-73 batch-99
Running loss of epoch-73 batch-99 = 1.7939601093530655e-06

Training epoch-73 batch-100
Running loss of epoch-73 batch-100 = 2.341112121939659e-06

Training epoch-73 batch-101
Running loss of epoch-73 batch-101 = 2.2761523723602295e-06

Training epoch-73 batch-102
Running loss of epoch-73 batch-102 = 1.3816170394420624e-06

Training epoch-73 batch-103
Running loss of epoch-73 batch-103 = 1.044943928718567e-06

Training epoch-73 batch-104
Running loss of epoch-73 batch-104 = 1.9238796085119247e-06

Training epoch-73 batch-105
Running loss of epoch-73 batch-105 = 1.7068814486265182e-06

Training epoch-73 batch-106
Running loss of epoch-73 batch-106 = 1.5511177480220795e-06

Training epoch-73 batch-107
Running loss of epoch-73 batch-107 = 1.5369150787591934e-06

Training epoch-73 batch-108
Running loss of epoch-73 batch-108 = 1.4221295714378357e-06

Training epoch-73 batch-109
Running loss of epoch-73 batch-109 = 2.1371524780988693e-06

Training epoch-73 batch-110
Running loss of epoch-73 batch-110 = 1.7150305211544037e-06

Training epoch-73 batch-111
Running loss of epoch-73 batch-111 = 1.5669502317905426e-06

Training epoch-73 batch-112
Running loss of epoch-73 batch-112 = 1.6593839973211288e-06

Training epoch-73 batch-113
Running loss of epoch-73 batch-113 = 1.2475065886974335e-06

Training epoch-73 batch-114
Running loss of epoch-73 batch-114 = 2.007000148296356e-06

Training epoch-73 batch-115
Running loss of epoch-73 batch-115 = 2.282671630382538e-06

Training epoch-73 batch-116
Running loss of epoch-73 batch-116 = 1.1401716619729996e-06

Training epoch-73 batch-117
Running loss of epoch-73 batch-117 = 2.0954757928848267e-06

Training epoch-73 batch-118
Running loss of epoch-73 batch-118 = 1.90013088285923e-06

Training epoch-73 batch-119
Running loss of epoch-73 batch-119 = 2.5241170078516006e-06

Training epoch-73 batch-120
Running loss of epoch-73 batch-120 = 1.673353835940361e-06

Training epoch-73 batch-121
Running loss of epoch-73 batch-121 = 1.3741664588451385e-06

Training epoch-73 batch-122
Running loss of epoch-73 batch-122 = 1.8726568669080734e-06

Training epoch-73 batch-123
Running loss of epoch-73 batch-123 = 9.71369445323944e-07

Training epoch-73 batch-124
Running loss of epoch-73 batch-124 = 2.4330802261829376e-06

Training epoch-73 batch-125
Running loss of epoch-73 batch-125 = 9.73232090473175e-07

Training epoch-73 batch-126
Running loss of epoch-73 batch-126 = 1.3771932572126389e-06

Training epoch-73 batch-127
Running loss of epoch-73 batch-127 = 1.2570526450872421e-06

Training epoch-73 batch-128
Running loss of epoch-73 batch-128 = 2.434244379401207e-06

Training epoch-73 batch-129
Running loss of epoch-73 batch-129 = 1.2039672583341599e-06

Training epoch-73 batch-130
Running loss of epoch-73 batch-130 = 1.996755599975586e-06

Training epoch-73 batch-131
Running loss of epoch-73 batch-131 = 1.6838312149047852e-06

Training epoch-73 batch-132
Running loss of epoch-73 batch-132 = 1.5900004655122757e-06

Training epoch-73 batch-133
Running loss of epoch-73 batch-133 = 2.1015293896198273e-06

Training epoch-73 batch-134
Running loss of epoch-73 batch-134 = 2.3152679204940796e-06

Training epoch-73 batch-135
Running loss of epoch-73 batch-135 = 9.813811630010605e-07

Training epoch-73 batch-136
Running loss of epoch-73 batch-136 = 1.7511192709207535e-06

Training epoch-73 batch-137
Running loss of epoch-73 batch-137 = 1.5385448932647705e-06

Training epoch-73 batch-138
Running loss of epoch-73 batch-138 = 1.6023404896259308e-06

Training epoch-73 batch-139
Running loss of epoch-73 batch-139 = 2.369983121752739e-06

Training epoch-73 batch-140
Running loss of epoch-73 batch-140 = 1.691281795501709e-06

Training epoch-73 batch-141
Running loss of epoch-73 batch-141 = 1.0777730494737625e-06

Training epoch-73 batch-142
Running loss of epoch-73 batch-142 = 2.9909424483776093e-06

Training epoch-73 batch-143
Running loss of epoch-73 batch-143 = 1.7853453755378723e-06

Training epoch-73 batch-144
Running loss of epoch-73 batch-144 = 1.3594981282949448e-06

Training epoch-73 batch-145
Running loss of epoch-73 batch-145 = 1.6563571989536285e-06

Training epoch-73 batch-146
Running loss of epoch-73 batch-146 = 1.959269866347313e-06

Training epoch-73 batch-147
Running loss of epoch-73 batch-147 = 1.3667158782482147e-06

Training epoch-73 batch-148
Running loss of epoch-73 batch-148 = 1.7024576663970947e-06

Training epoch-73 batch-149
Running loss of epoch-73 batch-149 = 1.4996621757745743e-06

Training epoch-73 batch-150
Running loss of epoch-73 batch-150 = 1.0030344128608704e-06

Training epoch-73 batch-151
Running loss of epoch-73 batch-151 = 2.312939614057541e-06

Training epoch-73 batch-152
Running loss of epoch-73 batch-152 = 1.9602011889219284e-06

Training epoch-73 batch-153
Running loss of epoch-73 batch-153 = 1.6344711184501648e-06

Training epoch-73 batch-154
Running loss of epoch-73 batch-154 = 1.9706785678863525e-06

Training epoch-73 batch-155
Running loss of epoch-73 batch-155 = 1.4707911759614944e-06

Training epoch-73 batch-156
Running loss of epoch-73 batch-156 = 1.5345867723226547e-06

Training epoch-73 batch-157
Running loss of epoch-73 batch-157 = 4.492700099945068e-06

Finished training epoch-73.



Average train loss at epoch-73 = 1.6990765929222107e-06

Started Evaluation

Average val loss at epoch-73 = 3.2388401596169722

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.61 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-74


Training epoch-74 batch-1
Running loss of epoch-74 batch-1 = 1.9995495676994324e-06

Training epoch-74 batch-2
Running loss of epoch-74 batch-2 = 1.703854650259018e-06

Training epoch-74 batch-3
Running loss of epoch-74 batch-3 = 1.7564743757247925e-06

Training epoch-74 batch-4
Running loss of epoch-74 batch-4 = 1.6880221664905548e-06

Training epoch-74 batch-5
Running loss of epoch-74 batch-5 = 1.6193371266126633e-06

Training epoch-74 batch-6
Running loss of epoch-74 batch-6 = 1.396751031279564e-06

Training epoch-74 batch-7
Running loss of epoch-74 batch-7 = 2.8007198125123978e-06

Training epoch-74 batch-8
Running loss of epoch-74 batch-8 = 1.3648532330989838e-06

Training epoch-74 batch-9
Running loss of epoch-74 batch-9 = 3.0617229640483856e-06

Training epoch-74 batch-10
Running loss of epoch-74 batch-10 = 1.3553071767091751e-06

Training epoch-74 batch-11
Running loss of epoch-74 batch-11 = 1.4354009181261063e-06

Training epoch-74 batch-12
Running loss of epoch-74 batch-12 = 1.285690814256668e-06

Training epoch-74 batch-13
Running loss of epoch-74 batch-13 = 1.9012950360774994e-06

Training epoch-74 batch-14
Running loss of epoch-74 batch-14 = 1.575099304318428e-06

Training epoch-74 batch-15
Running loss of epoch-74 batch-15 = 2.521555870771408e-06

Training epoch-74 batch-16
Running loss of epoch-74 batch-16 = 2.175336703658104e-06

Training epoch-74 batch-17
Running loss of epoch-74 batch-17 = 1.5653204172849655e-06

Training epoch-74 batch-18
Running loss of epoch-74 batch-18 = 1.4971010386943817e-06

Training epoch-74 batch-19
Running loss of epoch-74 batch-19 = 1.4624092727899551e-06

Training epoch-74 batch-20
Running loss of epoch-74 batch-20 = 1.7865095287561417e-06

Training epoch-74 batch-21
Running loss of epoch-74 batch-21 = 1.8076971173286438e-06

Training epoch-74 batch-22
Running loss of epoch-74 batch-22 = 1.3553071767091751e-06

Training epoch-74 batch-23
Running loss of epoch-74 batch-23 = 1.6263220459222794e-06

Training epoch-74 batch-24
Running loss of epoch-74 batch-24 = 1.3741664588451385e-06

Training epoch-74 batch-25
Running loss of epoch-74 batch-25 = 1.155305653810501e-06

Training epoch-74 batch-26
Running loss of epoch-74 batch-26 = 1.8386635929346085e-06

Training epoch-74 batch-27
Running loss of epoch-74 batch-27 = 2.149958163499832e-06

Training epoch-74 batch-28
Running loss of epoch-74 batch-28 = 1.96089968085289e-06

Training epoch-74 batch-29
Running loss of epoch-74 batch-29 = 1.689651980996132e-06

Training epoch-74 batch-30
Running loss of epoch-74 batch-30 = 2.0407605916261673e-06

Training epoch-74 batch-31
Running loss of epoch-74 batch-31 = 9.974464774131775e-07

Training epoch-74 batch-32
Running loss of epoch-74 batch-32 = 1.5594996511936188e-06

Training epoch-74 batch-33
Running loss of epoch-74 batch-33 = 1.3855751603841782e-06

Training epoch-74 batch-34
Running loss of epoch-74 batch-34 = 1.4523975551128387e-06

Training epoch-74 batch-35
Running loss of epoch-74 batch-35 = 1.4281831681728363e-06

Training epoch-74 batch-36
Running loss of epoch-74 batch-36 = 1.325272023677826e-06

Training epoch-74 batch-37
Running loss of epoch-74 batch-37 = 1.3504177331924438e-06

Training epoch-74 batch-38
Running loss of epoch-74 batch-38 = 1.2244563549757004e-06

Training epoch-74 batch-39
Running loss of epoch-74 batch-39 = 1.9685830920934677e-06

Training epoch-74 batch-40
Running loss of epoch-74 batch-40 = 2.4971086531877518e-06

Training epoch-74 batch-41
Running loss of epoch-74 batch-41 = 1.2507662177085876e-06

Training epoch-74 batch-42
Running loss of epoch-74 batch-42 = 8.353963494300842e-07

Training epoch-74 batch-43
Running loss of epoch-74 batch-43 = 1.4735851436853409e-06

Training epoch-74 batch-44
Running loss of epoch-74 batch-44 = 1.932261511683464e-06

Training epoch-74 batch-45
Running loss of epoch-74 batch-45 = 1.421663910150528e-06

Training epoch-74 batch-46
Running loss of epoch-74 batch-46 = 1.3762619346380234e-06

Training epoch-74 batch-47
Running loss of epoch-74 batch-47 = 1.6149133443832397e-06

Training epoch-74 batch-48
Running loss of epoch-74 batch-48 = 1.5629921108484268e-06

Training epoch-74 batch-49
Running loss of epoch-74 batch-49 = 1.750187948346138e-06

Training epoch-74 batch-50
Running loss of epoch-74 batch-50 = 2.0246952772140503e-06

Training epoch-74 batch-51
Running loss of epoch-74 batch-51 = 1.7567072063684464e-06

Training epoch-74 batch-52
Running loss of epoch-74 batch-52 = 1.7392449080944061e-06

Training epoch-74 batch-53
Running loss of epoch-74 batch-53 = 1.7471611499786377e-06

Training epoch-74 batch-54
Running loss of epoch-74 batch-54 = 1.3713724911212921e-06

Training epoch-74 batch-55
Running loss of epoch-74 batch-55 = 1.7827842384576797e-06

Training epoch-74 batch-56
Running loss of epoch-74 batch-56 = 1.584179699420929e-06

Training epoch-74 batch-57
Running loss of epoch-74 batch-57 = 1.7730053514242172e-06

Training epoch-74 batch-58
Running loss of epoch-74 batch-58 = 1.889653503894806e-06

Training epoch-74 batch-59
Running loss of epoch-74 batch-59 = 1.4891847968101501e-06

Training epoch-74 batch-60
Running loss of epoch-74 batch-60 = 1.3115350157022476e-06

Training epoch-74 batch-61
Running loss of epoch-74 batch-61 = 2.417946234345436e-06

Training epoch-74 batch-62
Running loss of epoch-74 batch-62 = 2.608867362141609e-06

Training epoch-74 batch-63
Running loss of epoch-74 batch-63 = 1.108972355723381e-06

Training epoch-74 batch-64
Running loss of epoch-74 batch-64 = 1.958804205060005e-06

Training epoch-74 batch-65
Running loss of epoch-74 batch-65 = 2.007465809583664e-06

Training epoch-74 batch-66
Running loss of epoch-74 batch-66 = 9.885989129543304e-07

Training epoch-74 batch-67
Running loss of epoch-74 batch-67 = 1.2991949915885925e-06

Training epoch-74 batch-68
Running loss of epoch-74 batch-68 = 1.0789372026920319e-06

Training epoch-74 batch-69
Running loss of epoch-74 batch-69 = 1.1415686458349228e-06

Training epoch-74 batch-70
Running loss of epoch-74 batch-70 = 1.7317943274974823e-06

Training epoch-74 batch-71
Running loss of epoch-74 batch-71 = 2.5117769837379456e-06

Training epoch-74 batch-72
Running loss of epoch-74 batch-72 = 1.6423873603343964e-06

Training epoch-74 batch-73
Running loss of epoch-74 batch-73 = 1.687789335846901e-06

Training epoch-74 batch-74
Running loss of epoch-74 batch-74 = 1.4121178537607193e-06

Training epoch-74 batch-75
Running loss of epoch-74 batch-75 = 1.9818544387817383e-06

Training epoch-74 batch-76
Running loss of epoch-74 batch-76 = 1.3508833944797516e-06

Training epoch-74 batch-77
Running loss of epoch-74 batch-77 = 1.646578311920166e-06

Training epoch-74 batch-78
Running loss of epoch-74 batch-78 = 2.282438799738884e-06

Training epoch-74 batch-79
Running loss of epoch-74 batch-79 = 2.295011654496193e-06

Training epoch-74 batch-80
Running loss of epoch-74 batch-80 = 1.5022233128547668e-06

Training epoch-74 batch-81
Running loss of epoch-74 batch-81 = 1.312699168920517e-06

Training epoch-74 batch-82
Running loss of epoch-74 batch-82 = 1.7224811017513275e-06

Training epoch-74 batch-83
Running loss of epoch-74 batch-83 = 1.3248063623905182e-06

Training epoch-74 batch-84
Running loss of epoch-74 batch-84 = 1.2873206287622452e-06

Training epoch-74 batch-85
Running loss of epoch-74 batch-85 = 9.222421795129776e-07

Training epoch-74 batch-86
Running loss of epoch-74 batch-86 = 1.3669487088918686e-06

Training epoch-74 batch-87
Running loss of epoch-74 batch-87 = 1.7029233276844025e-06

Training epoch-74 batch-88
Running loss of epoch-74 batch-88 = 1.2156087905168533e-06

Training epoch-74 batch-89
Running loss of epoch-74 batch-89 = 1.7399434000253677e-06

Training epoch-74 batch-90
Running loss of epoch-74 batch-90 = 1.6582198441028595e-06

Training epoch-74 batch-91
Running loss of epoch-74 batch-91 = 1.451931893825531e-06

Training epoch-74 batch-92
Running loss of epoch-74 batch-92 = 1.2670643627643585e-06

Training epoch-74 batch-93
Running loss of epoch-74 batch-93 = 2.6172492653131485e-06

Training epoch-74 batch-94
Running loss of epoch-74 batch-94 = 1.3441313058137894e-06

Training epoch-74 batch-95
Running loss of epoch-74 batch-95 = 2.075452357530594e-06

Training epoch-74 batch-96
Running loss of epoch-74 batch-96 = 1.62515789270401e-06

Training epoch-74 batch-97
Running loss of epoch-74 batch-97 = 1.319684088230133e-06

Training epoch-74 batch-98
Running loss of epoch-74 batch-98 = 2.420041710138321e-06

Training epoch-74 batch-99
Running loss of epoch-74 batch-99 = 1.6319099813699722e-06

Training epoch-74 batch-100
Running loss of epoch-74 batch-100 = 2.1150335669517517e-06

Training epoch-74 batch-101
Running loss of epoch-74 batch-101 = 1.2186355888843536e-06

Training epoch-74 batch-102
Running loss of epoch-74 batch-102 = 1.1676456779241562e-06

Training epoch-74 batch-103
Running loss of epoch-74 batch-103 = 2.051936462521553e-06

Training epoch-74 batch-104
Running loss of epoch-74 batch-104 = 1.3601966202259064e-06

Training epoch-74 batch-105
Running loss of epoch-74 batch-105 = 1.45728699862957e-06

Training epoch-74 batch-106
Running loss of epoch-74 batch-106 = 1.2207310646772385e-06

Training epoch-74 batch-107
Running loss of epoch-74 batch-107 = 1.9420403987169266e-06

Training epoch-74 batch-108
Running loss of epoch-74 batch-108 = 1.4991965144872665e-06

Training epoch-74 batch-109
Running loss of epoch-74 batch-109 = 2.362765371799469e-06

Training epoch-74 batch-110
Running loss of epoch-74 batch-110 = 1.689651980996132e-06

Training epoch-74 batch-111
Running loss of epoch-74 batch-111 = 1.6926787793636322e-06

Training epoch-74 batch-112
Running loss of epoch-74 batch-112 = 1.530395820736885e-06

Training epoch-74 batch-113
Running loss of epoch-74 batch-113 = 1.2693926692008972e-06

Training epoch-74 batch-114
Running loss of epoch-74 batch-114 = 2.369983121752739e-06

Training epoch-74 batch-115
Running loss of epoch-74 batch-115 = 1.3809185475111008e-06

Training epoch-74 batch-116
Running loss of epoch-74 batch-116 = 1.2225937098264694e-06

Training epoch-74 batch-117
Running loss of epoch-74 batch-117 = 1.3569369912147522e-06

Training epoch-74 batch-118
Running loss of epoch-74 batch-118 = 1.727137714624405e-06

Training epoch-74 batch-119
Running loss of epoch-74 batch-119 = 2.5792978703975677e-06

Training epoch-74 batch-120
Running loss of epoch-74 batch-120 = 1.4903489500284195e-06

Training epoch-74 batch-121
Running loss of epoch-74 batch-121 = 1.7955899238586426e-06

Training epoch-74 batch-122
Running loss of epoch-74 batch-122 = 1.7129350453615189e-06

Training epoch-74 batch-123
Running loss of epoch-74 batch-123 = 1.5904661267995834e-06

Training epoch-74 batch-124
Running loss of epoch-74 batch-124 = 1.7045531421899796e-06

Training epoch-74 batch-125
Running loss of epoch-74 batch-125 = 1.2635719031095505e-06

Training epoch-74 batch-126
Running loss of epoch-74 batch-126 = 1.911073923110962e-06

Training epoch-74 batch-127
Running loss of epoch-74 batch-127 = 1.0353978723287582e-06

Training epoch-74 batch-128
Running loss of epoch-74 batch-128 = 1.485925167798996e-06

Training epoch-74 batch-129
Running loss of epoch-74 batch-129 = 1.9622966647148132e-06

Training epoch-74 batch-130
Running loss of epoch-74 batch-130 = 1.1147931218147278e-06

Training epoch-74 batch-131
Running loss of epoch-74 batch-131 = 7.664784789085388e-07

Training epoch-74 batch-132
Running loss of epoch-74 batch-132 = 1.4533288776874542e-06

Training epoch-74 batch-133
Running loss of epoch-74 batch-133 = 2.353917807340622e-06

Training epoch-74 batch-134
Running loss of epoch-74 batch-134 = 1.8139835447072983e-06

Training epoch-74 batch-135
Running loss of epoch-74 batch-135 = 2.0936131477355957e-06

Training epoch-74 batch-136
Running loss of epoch-74 batch-136 = 1.444946974515915e-06

Training epoch-74 batch-137
Running loss of epoch-74 batch-137 = 1.3292301446199417e-06

Training epoch-74 batch-138
Running loss of epoch-74 batch-138 = 2.257758751511574e-06

Training epoch-74 batch-139
Running loss of epoch-74 batch-139 = 1.6391277313232422e-06

Training epoch-74 batch-140
Running loss of epoch-74 batch-140 = 2.0994339138269424e-06

Training epoch-74 batch-141
Running loss of epoch-74 batch-141 = 1.455424353480339e-06

Training epoch-74 batch-142
Running loss of epoch-74 batch-142 = 2.1471641957759857e-06

Training epoch-74 batch-143
Running loss of epoch-74 batch-143 = 1.412816345691681e-06

Training epoch-74 batch-144
Running loss of epoch-74 batch-144 = 1.7478596419095993e-06

Training epoch-74 batch-145
Running loss of epoch-74 batch-145 = 1.580454409122467e-06

Training epoch-74 batch-146
Running loss of epoch-74 batch-146 = 1.4307443052530289e-06

Training epoch-74 batch-147
Running loss of epoch-74 batch-147 = 1.9248109310865402e-06

Training epoch-74 batch-148
Running loss of epoch-74 batch-148 = 1.914799213409424e-06

Training epoch-74 batch-149
Running loss of epoch-74 batch-149 = 1.4884863048791885e-06

Training epoch-74 batch-150
Running loss of epoch-74 batch-150 = 1.62515789270401e-06

Training epoch-74 batch-151
Running loss of epoch-74 batch-151 = 1.7909333109855652e-06

Training epoch-74 batch-152
Running loss of epoch-74 batch-152 = 1.8954742699861526e-06

Training epoch-74 batch-153
Running loss of epoch-74 batch-153 = 1.6922131180763245e-06

Training epoch-74 batch-154
Running loss of epoch-74 batch-154 = 1.5832483768463135e-06

Training epoch-74 batch-155
Running loss of epoch-74 batch-155 = 1.2903474271297455e-06

Training epoch-74 batch-156
Running loss of epoch-74 batch-156 = 2.2100284695625305e-06

Training epoch-74 batch-157
Running loss of epoch-74 batch-157 = 1.0766088962554932e-05

Finished training epoch-74.



Average train loss at epoch-74 = 1.6761809587478637e-06

Started Evaluation

Average val loss at epoch-74 = 3.242576655588652

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 63.93 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 38.13 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-75


Training epoch-75 batch-1
Running loss of epoch-75 batch-1 = 1.7066486179828644e-06

Training epoch-75 batch-2
Running loss of epoch-75 batch-2 = 1.4649704098701477e-06

Training epoch-75 batch-3
Running loss of epoch-75 batch-3 = 1.7406418919563293e-06

Training epoch-75 batch-4
Running loss of epoch-75 batch-4 = 9.83942300081253e-07

Training epoch-75 batch-5
Running loss of epoch-75 batch-5 = 1.8368009477853775e-06

Training epoch-75 batch-6
Running loss of epoch-75 batch-6 = 1.9453000277280807e-06

Training epoch-75 batch-7
Running loss of epoch-75 batch-7 = 1.166248694062233e-06

Training epoch-75 batch-8
Running loss of epoch-75 batch-8 = 1.7371494323015213e-06

Training epoch-75 batch-9
Running loss of epoch-75 batch-9 = 1.2789387255907059e-06

Training epoch-75 batch-10
Running loss of epoch-75 batch-10 = 2.0766165107488632e-06

Training epoch-75 batch-11
Running loss of epoch-75 batch-11 = 2.0745210349559784e-06

Training epoch-75 batch-12
Running loss of epoch-75 batch-12 = 1.5671830624341965e-06

Training epoch-75 batch-13
Running loss of epoch-75 batch-13 = 2.3350585252046585e-06

Training epoch-75 batch-14
Running loss of epoch-75 batch-14 = 7.126946002244949e-07

Training epoch-75 batch-15
Running loss of epoch-75 batch-15 = 1.607462763786316e-06

Training epoch-75 batch-16
Running loss of epoch-75 batch-16 = 1.4908146113157272e-06

Training epoch-75 batch-17
Running loss of epoch-75 batch-17 = 2.028653398156166e-06

Training epoch-75 batch-18
Running loss of epoch-75 batch-18 = 1.0638032108545303e-06

Training epoch-75 batch-19
Running loss of epoch-75 batch-19 = 1.1003576219081879e-06

Training epoch-75 batch-20
Running loss of epoch-75 batch-20 = 1.0686926543712616e-06

Training epoch-75 batch-21
Running loss of epoch-75 batch-21 = 2.0833685994148254e-06

Training epoch-75 batch-22
Running loss of epoch-75 batch-22 = 1.8351711332798004e-06

Training epoch-75 batch-23
Running loss of epoch-75 batch-23 = 2.148561179637909e-06

Training epoch-75 batch-24
Running loss of epoch-75 batch-24 = 2.193031832575798e-06

Training epoch-75 batch-25
Running loss of epoch-75 batch-25 = 1.4542602002620697e-06

Training epoch-75 batch-26
Running loss of epoch-75 batch-26 = 2.1846499294042587e-06

Training epoch-75 batch-27
Running loss of epoch-75 batch-27 = 1.7480924725532532e-06

Training epoch-75 batch-28
Running loss of epoch-75 batch-28 = 1.412816345691681e-06

Training epoch-75 batch-29
Running loss of epoch-75 batch-29 = 1.3480894267559052e-06

Training epoch-75 batch-30
Running loss of epoch-75 batch-30 = 1.7723068594932556e-06

Training epoch-75 batch-31
Running loss of epoch-75 batch-31 = 2.091284841299057e-06

Training epoch-75 batch-32
Running loss of epoch-75 batch-32 = 1.9993167370557785e-06

Training epoch-75 batch-33
Running loss of epoch-75 batch-33 = 2.7331989258527756e-06

Training epoch-75 batch-34
Running loss of epoch-75 batch-34 = 1.3096723705530167e-06

Training epoch-75 batch-35
Running loss of epoch-75 batch-35 = 1.1639203876256943e-06

Training epoch-75 batch-36
Running loss of epoch-75 batch-36 = 2.1194573491811752e-06

Training epoch-75 batch-37
Running loss of epoch-75 batch-37 = 1.1117663234472275e-06

Training epoch-75 batch-38
Running loss of epoch-75 batch-38 = 1.3262033462524414e-06

Training epoch-75 batch-39
Running loss of epoch-75 batch-39 = 1.4312099665403366e-06

Training epoch-75 batch-40
Running loss of epoch-75 batch-40 = 1.5457626432180405e-06

Training epoch-75 batch-41
Running loss of epoch-75 batch-41 = 1.3133976608514786e-06

Training epoch-75 batch-42
Running loss of epoch-75 batch-42 = 1.5546102076768875e-06

Training epoch-75 batch-43
Running loss of epoch-75 batch-43 = 2.039829269051552e-06

Training epoch-75 batch-44
Running loss of epoch-75 batch-44 = 1.0842923074960709e-06

Training epoch-75 batch-45
Running loss of epoch-75 batch-45 = 2.4617183953523636e-06

Training epoch-75 batch-46
Running loss of epoch-75 batch-46 = 1.7175916582345963e-06

Training epoch-75 batch-47
Running loss of epoch-75 batch-47 = 1.3441313058137894e-06

Training epoch-75 batch-48
Running loss of epoch-75 batch-48 = 2.2656749933958054e-06

Training epoch-75 batch-49
Running loss of epoch-75 batch-49 = 1.6766134649515152e-06

Training epoch-75 batch-50
Running loss of epoch-75 batch-50 = 1.9783619791269302e-06

Training epoch-75 batch-51
Running loss of epoch-75 batch-51 = 1.6463454812765121e-06

Training epoch-75 batch-52
Running loss of epoch-75 batch-52 = 2.0100269466638565e-06

Training epoch-75 batch-53
Running loss of epoch-75 batch-53 = 1.852167770266533e-06

Training epoch-75 batch-54
Running loss of epoch-75 batch-54 = 1.1383090168237686e-06

Training epoch-75 batch-55
Running loss of epoch-75 batch-55 = 2.0067673176527023e-06

Training epoch-75 batch-56
Running loss of epoch-75 batch-56 = 1.7578713595867157e-06

Training epoch-75 batch-57
Running loss of epoch-75 batch-57 = 1.4547258615493774e-06

Training epoch-75 batch-58
Running loss of epoch-75 batch-58 = 1.8337741494178772e-06

Training epoch-75 batch-59
Running loss of epoch-75 batch-59 = 1.6486737877130508e-06

Training epoch-75 batch-60
Running loss of epoch-75 batch-60 = 1.612817868590355e-06

Training epoch-75 batch-61
Running loss of epoch-75 batch-61 = 2.205371856689453e-06

Training epoch-75 batch-62
Running loss of epoch-75 batch-62 = 1.1490192264318466e-06

Training epoch-75 batch-63
Running loss of epoch-75 batch-63 = 1.8046703189611435e-06

Training epoch-75 batch-64
Running loss of epoch-75 batch-64 = 1.841224730014801e-06

Training epoch-75 batch-65
Running loss of epoch-75 batch-65 = 1.6049016267061234e-06

Training epoch-75 batch-66
Running loss of epoch-75 batch-66 = 1.6123522073030472e-06

Training epoch-75 batch-67
Running loss of epoch-75 batch-67 = 1.923413947224617e-06

Training epoch-75 batch-68
Running loss of epoch-75 batch-68 = 1.4884863048791885e-06

Training epoch-75 batch-69
Running loss of epoch-75 batch-69 = 1.1711381375789642e-06

Training epoch-75 batch-70
Running loss of epoch-75 batch-70 = 1.9604340195655823e-06

Training epoch-75 batch-71
Running loss of epoch-75 batch-71 = 1.212814822793007e-06

Training epoch-75 batch-72
Running loss of epoch-75 batch-72 = 2.1350570023059845e-06

Training epoch-75 batch-73
Running loss of epoch-75 batch-73 = 1.6910489648580551e-06

Training epoch-75 batch-74
Running loss of epoch-75 batch-74 = 2.123182639479637e-06

Training epoch-75 batch-75
Running loss of epoch-75 batch-75 = 1.3373792171478271e-06

Training epoch-75 batch-76
Running loss of epoch-75 batch-76 = 1.9136350601911545e-06

Training epoch-75 batch-77
Running loss of epoch-75 batch-77 = 1.5879049897193909e-06

Training epoch-75 batch-78
Running loss of epoch-75 batch-78 = 1.2116506695747375e-06

Training epoch-75 batch-79
Running loss of epoch-75 batch-79 = 1.7101410776376724e-06

Training epoch-75 batch-80
Running loss of epoch-75 batch-80 = 1.6081612557172775e-06

Training epoch-75 batch-81
Running loss of epoch-75 batch-81 = 2.305489033460617e-06

Training epoch-75 batch-82
Running loss of epoch-75 batch-82 = 2.207234501838684e-06

Training epoch-75 batch-83
Running loss of epoch-75 batch-83 = 1.448439434170723e-06

Training epoch-75 batch-84
Running loss of epoch-75 batch-84 = 1.1527445167303085e-06

Training epoch-75 batch-85
Running loss of epoch-75 batch-85 = 1.993263140320778e-06

Training epoch-75 batch-86
Running loss of epoch-75 batch-86 = 1.4456454664468765e-06

Training epoch-75 batch-87
Running loss of epoch-75 batch-87 = 1.3755634427070618e-06

Training epoch-75 batch-88
Running loss of epoch-75 batch-88 = 1.4528632164001465e-06

Training epoch-75 batch-89
Running loss of epoch-75 batch-89 = 1.1208467185497284e-06

Training epoch-75 batch-90
Running loss of epoch-75 batch-90 = 2.375105395913124e-06

Training epoch-75 batch-91
Running loss of epoch-75 batch-91 = 1.9918661564588547e-06

Training epoch-75 batch-92
Running loss of epoch-75 batch-92 = 8.721835911273956e-07

Training epoch-75 batch-93
Running loss of epoch-75 batch-93 = 1.3944227248430252e-06

Training epoch-75 batch-94
Running loss of epoch-75 batch-94 = 1.2426171451807022e-06

Training epoch-75 batch-95
Running loss of epoch-75 batch-95 = 2.1301675587892532e-06

Training epoch-75 batch-96
Running loss of epoch-75 batch-96 = 1.4973338693380356e-06

Training epoch-75 batch-97
Running loss of epoch-75 batch-97 = 1.0826624929904938e-06

Training epoch-75 batch-98
Running loss of epoch-75 batch-98 = 1.957640051841736e-06

Training epoch-75 batch-99
Running loss of epoch-75 batch-99 = 2.1278392523527145e-06

Training epoch-75 batch-100
Running loss of epoch-75 batch-100 = 2.203509211540222e-06

Training epoch-75 batch-101
Running loss of epoch-75 batch-101 = 1.2458767741918564e-06

Training epoch-75 batch-102
Running loss of epoch-75 batch-102 = 1.0461080819368362e-06

Training epoch-75 batch-103
Running loss of epoch-75 batch-103 = 1.7809215933084488e-06

Training epoch-75 batch-104
Running loss of epoch-75 batch-104 = 1.5425030142068863e-06

Training epoch-75 batch-105
Running loss of epoch-75 batch-105 = 1.3792887330055237e-06

Training epoch-75 batch-106
Running loss of epoch-75 batch-106 = 7.245689630508423e-07

Training epoch-75 batch-107
Running loss of epoch-75 batch-107 = 1.1727679520845413e-06

Training epoch-75 batch-108
Running loss of epoch-75 batch-108 = 1.605367287993431e-06

Training epoch-75 batch-109
Running loss of epoch-75 batch-109 = 1.3094395399093628e-06

Training epoch-75 batch-110
Running loss of epoch-75 batch-110 = 1.4810357242822647e-06

Training epoch-75 batch-111
Running loss of epoch-75 batch-111 = 1.5352852642536163e-06

Training epoch-75 batch-112
Running loss of epoch-75 batch-112 = 1.2842938303947449e-06

Training epoch-75 batch-113
Running loss of epoch-75 batch-113 = 1.4370307326316833e-06

Training epoch-75 batch-114
Running loss of epoch-75 batch-114 = 1.5504192560911179e-06

Training epoch-75 batch-115
Running loss of epoch-75 batch-115 = 1.1364463716745377e-06

Training epoch-75 batch-116
Running loss of epoch-75 batch-116 = 1.2035015970468521e-06

Training epoch-75 batch-117
Running loss of epoch-75 batch-117 = 1.398380845785141e-06

Training epoch-75 batch-118
Running loss of epoch-75 batch-118 = 1.1366792023181915e-06

Training epoch-75 batch-119
Running loss of epoch-75 batch-119 = 1.4624092727899551e-06

Training epoch-75 batch-120
Running loss of epoch-75 batch-120 = 1.8703285604715347e-06

Training epoch-75 batch-121
Running loss of epoch-75 batch-121 = 1.471489667892456e-06

Training epoch-75 batch-122
Running loss of epoch-75 batch-122 = 2.1832529455423355e-06

Training epoch-75 batch-123
Running loss of epoch-75 batch-123 = 1.3222452253103256e-06

Training epoch-75 batch-124
Running loss of epoch-75 batch-124 = 2.1527521312236786e-06

Training epoch-75 batch-125
Running loss of epoch-75 batch-125 = 1.8193386495113373e-06

Training epoch-75 batch-126
Running loss of epoch-75 batch-126 = 1.6223639249801636e-06

Training epoch-75 batch-127
Running loss of epoch-75 batch-127 = 2.5136396288871765e-06

Training epoch-75 batch-128
Running loss of epoch-75 batch-128 = 9.746290743350983e-07

Training epoch-75 batch-129
Running loss of epoch-75 batch-129 = 1.1920928955078125e-06

Training epoch-75 batch-130
Running loss of epoch-75 batch-130 = 9.73464921116829e-07

Training epoch-75 batch-131
Running loss of epoch-75 batch-131 = 2.051936462521553e-06

Training epoch-75 batch-132
Running loss of epoch-75 batch-132 = 1.1494848877191544e-06

Training epoch-75 batch-133
Running loss of epoch-75 batch-133 = 2.1187588572502136e-06

Training epoch-75 batch-134
Running loss of epoch-75 batch-134 = 1.3527460396289825e-06

Training epoch-75 batch-135
Running loss of epoch-75 batch-135 = 9.618233889341354e-07

Training epoch-75 batch-136
Running loss of epoch-75 batch-136 = 1.3450626283884048e-06

Training epoch-75 batch-137
Running loss of epoch-75 batch-137 = 1.8731225281953812e-06

Training epoch-75 batch-138
Running loss of epoch-75 batch-138 = 2.3276079446077347e-06

Training epoch-75 batch-139
Running loss of epoch-75 batch-139 = 1.5494879335165024e-06

Training epoch-75 batch-140
Running loss of epoch-75 batch-140 = 1.164386048913002e-06

Training epoch-75 batch-141
Running loss of epoch-75 batch-141 = 1.6295816749334335e-06

Training epoch-75 batch-142
Running loss of epoch-75 batch-142 = 1.8328428268432617e-06

Training epoch-75 batch-143
Running loss of epoch-75 batch-143 = 1.8246937543153763e-06

Training epoch-75 batch-144
Running loss of epoch-75 batch-144 = 2.02888622879982e-06

Training epoch-75 batch-145
Running loss of epoch-75 batch-145 = 1.9262079149484634e-06

Training epoch-75 batch-146
Running loss of epoch-75 batch-146 = 1.7688143998384476e-06

Training epoch-75 batch-147
Running loss of epoch-75 batch-147 = 1.5830155462026596e-06

Training epoch-75 batch-148
Running loss of epoch-75 batch-148 = 2.1015293896198273e-06

Training epoch-75 batch-149
Running loss of epoch-75 batch-149 = 2.157408744096756e-06

Training epoch-75 batch-150
Running loss of epoch-75 batch-150 = 1.7848797142505646e-06

Training epoch-75 batch-151
Running loss of epoch-75 batch-151 = 1.5883706510066986e-06

Training epoch-75 batch-152
Running loss of epoch-75 batch-152 = 1.3909302651882172e-06

Training epoch-75 batch-153
Running loss of epoch-75 batch-153 = 1.2759119272232056e-06

Training epoch-75 batch-154
Running loss of epoch-75 batch-154 = 1.5690457075834274e-06

Training epoch-75 batch-155
Running loss of epoch-75 batch-155 = 1.969747245311737e-06

Training epoch-75 batch-156
Running loss of epoch-75 batch-156 = 1.995125785470009e-06

Training epoch-75 batch-157
Running loss of epoch-75 batch-157 = 4.969537258148193e-06

Finished training epoch-75.



Average train loss at epoch-75 = 1.6394078731536865e-06

Started Evaluation

Average val loss at epoch-75 = 3.2465005987568905

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.10 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-76


Training epoch-76 batch-1
Running loss of epoch-76 batch-1 = 1.925276592373848e-06

Training epoch-76 batch-2
Running loss of epoch-76 batch-2 = 2.3741740733385086e-06

Training epoch-76 batch-3
Running loss of epoch-76 batch-3 = 1.698266714811325e-06

Training epoch-76 batch-4
Running loss of epoch-76 batch-4 = 2.145068719983101e-06

Training epoch-76 batch-5
Running loss of epoch-76 batch-5 = 1.6351696103811264e-06

Training epoch-76 batch-6
Running loss of epoch-76 batch-6 = 1.546693965792656e-06

Training epoch-76 batch-7
Running loss of epoch-76 batch-7 = 1.5175901353359222e-06

Training epoch-76 batch-8
Running loss of epoch-76 batch-8 = 2.016080543398857e-06

Training epoch-76 batch-9
Running loss of epoch-76 batch-9 = 1.1532101780176163e-06

Training epoch-76 batch-10
Running loss of epoch-76 batch-10 = 2.14320607483387e-06

Training epoch-76 batch-11
Running loss of epoch-76 batch-11 = 1.8118880689144135e-06

Training epoch-76 batch-12
Running loss of epoch-76 batch-12 = 1.9834842532873154e-06

Training epoch-76 batch-13
Running loss of epoch-76 batch-13 = 1.0333023965358734e-06

Training epoch-76 batch-14
Running loss of epoch-76 batch-14 = 1.701992005109787e-06

Training epoch-76 batch-15
Running loss of epoch-76 batch-15 = 1.7539132386446e-06

Training epoch-76 batch-16
Running loss of epoch-76 batch-16 = 1.1273659765720367e-06

Training epoch-76 batch-17
Running loss of epoch-76 batch-17 = 1.74669548869133e-06

Training epoch-76 batch-18
Running loss of epoch-76 batch-18 = 7.615890353918076e-07

Training epoch-76 batch-19
Running loss of epoch-76 batch-19 = 2.3192260414361954e-06

Training epoch-76 batch-20
Running loss of epoch-76 batch-20 = 1.5436671674251556e-06

Training epoch-76 batch-21
Running loss of epoch-76 batch-21 = 2.5811605155467987e-06

Training epoch-76 batch-22
Running loss of epoch-76 batch-22 = 1.8291175365447998e-06

Training epoch-76 batch-23
Running loss of epoch-76 batch-23 = 1.821666955947876e-06

Training epoch-76 batch-24
Running loss of epoch-76 batch-24 = 1.7818529158830643e-06

Training epoch-76 batch-25
Running loss of epoch-76 batch-25 = 9.872019290924072e-07

Training epoch-76 batch-26
Running loss of epoch-76 batch-26 = 1.7064157873392105e-06

Training epoch-76 batch-27
Running loss of epoch-76 batch-27 = 2.032145857810974e-06

Training epoch-76 batch-28
Running loss of epoch-76 batch-28 = 1.1583324521780014e-06

Training epoch-76 batch-29
Running loss of epoch-76 batch-29 = 2.148328348994255e-06

Training epoch-76 batch-30
Running loss of epoch-76 batch-30 = 1.8326099961996078e-06

Training epoch-76 batch-31
Running loss of epoch-76 batch-31 = 1.1778902262449265e-06

Training epoch-76 batch-32
Running loss of epoch-76 batch-32 = 1.885928213596344e-06

Training epoch-76 batch-33
Running loss of epoch-76 batch-33 = 1.4668330550193787e-06

Training epoch-76 batch-34
Running loss of epoch-76 batch-34 = 1.4023389667272568e-06

Training epoch-76 batch-35
Running loss of epoch-76 batch-35 = 1.8996652215719223e-06

Training epoch-76 batch-36
Running loss of epoch-76 batch-36 = 1.6801059246063232e-06

Training epoch-76 batch-37
Running loss of epoch-76 batch-37 = 1.2777745723724365e-06

Training epoch-76 batch-38
Running loss of epoch-76 batch-38 = 1.8791761249303818e-06

Training epoch-76 batch-39
Running loss of epoch-76 batch-39 = 1.6123522073030472e-06

Training epoch-76 batch-40
Running loss of epoch-76 batch-40 = 1.6361009329557419e-06

Training epoch-76 batch-41
Running loss of epoch-76 batch-41 = 1.4889519661664963e-06

Training epoch-76 batch-42
Running loss of epoch-76 batch-42 = 1.6076955944299698e-06

Training epoch-76 batch-43
Running loss of epoch-76 batch-43 = 1.783948391675949e-06

Training epoch-76 batch-44
Running loss of epoch-76 batch-44 = 1.3294629752635956e-06

Training epoch-76 batch-45
Running loss of epoch-76 batch-45 = 2.107350155711174e-06

Training epoch-76 batch-46
Running loss of epoch-76 batch-46 = 1.6798730939626694e-06

Training epoch-76 batch-47
Running loss of epoch-76 batch-47 = 1.3459939509630203e-06

Training epoch-76 batch-48
Running loss of epoch-76 batch-48 = 1.2051314115524292e-06

Training epoch-76 batch-49
Running loss of epoch-76 batch-49 = 1.58604234457016e-06

Training epoch-76 batch-50
Running loss of epoch-76 batch-50 = 1.661013811826706e-06

Training epoch-76 batch-51
Running loss of epoch-76 batch-51 = 1.6635749489068985e-06

Training epoch-76 batch-52
Running loss of epoch-76 batch-52 = 1.5071127563714981e-06

Training epoch-76 batch-53
Running loss of epoch-76 batch-53 = 1.4347024261951447e-06

Training epoch-76 batch-54
Running loss of epoch-76 batch-54 = 1.555541530251503e-06

Training epoch-76 batch-55
Running loss of epoch-76 batch-55 = 1.4640390872955322e-06

Training epoch-76 batch-56
Running loss of epoch-76 batch-56 = 3.025168552994728e-06

Training epoch-76 batch-57
Running loss of epoch-76 batch-57 = 1.7033889889717102e-06

Training epoch-76 batch-58
Running loss of epoch-76 batch-58 = 1.0493677109479904e-06

Training epoch-76 batch-59
Running loss of epoch-76 batch-59 = 1.4433171600103378e-06

Training epoch-76 batch-60
Running loss of epoch-76 batch-60 = 1.1443626135587692e-06

Training epoch-76 batch-61
Running loss of epoch-76 batch-61 = 1.6095582395792007e-06

Training epoch-76 batch-62
Running loss of epoch-76 batch-62 = 2.1301675587892532e-06

Training epoch-76 batch-63
Running loss of epoch-76 batch-63 = 1.3494864106178284e-06

Training epoch-76 batch-64
Running loss of epoch-76 batch-64 = 1.532258465886116e-06

Training epoch-76 batch-65
Running loss of epoch-76 batch-65 = 1.6491394490003586e-06

Training epoch-76 batch-66
Running loss of epoch-76 batch-66 = 1.275213435292244e-06

Training epoch-76 batch-67
Running loss of epoch-76 batch-67 = 1.2260861694812775e-06

Training epoch-76 batch-68
Running loss of epoch-76 batch-68 = 1.6514677554368973e-06

Training epoch-76 batch-69
Running loss of epoch-76 batch-69 = 1.3790559023618698e-06

Training epoch-76 batch-70
Running loss of epoch-76 batch-70 = 1.7504207789897919e-06

Training epoch-76 batch-71
Running loss of epoch-76 batch-71 = 1.9508879631757736e-06

Training epoch-76 batch-72
Running loss of epoch-76 batch-72 = 1.3485550880432129e-06

Training epoch-76 batch-73
Running loss of epoch-76 batch-73 = 1.4784745872020721e-06

Training epoch-76 batch-74
Running loss of epoch-76 batch-74 = 1.4007091522216797e-06

Training epoch-76 batch-75
Running loss of epoch-76 batch-75 = 1.5452969819307327e-06

Training epoch-76 batch-76
Running loss of epoch-76 batch-76 = 1.8307473510503769e-06

Training epoch-76 batch-77
Running loss of epoch-76 batch-77 = 1.8456485122442245e-06

Training epoch-76 batch-78
Running loss of epoch-76 batch-78 = 1.841224730014801e-06

Training epoch-76 batch-79
Running loss of epoch-76 batch-79 = 2.0151492208242416e-06

Training epoch-76 batch-80
Running loss of epoch-76 batch-80 = 6.586778908967972e-07

Training epoch-76 batch-81
Running loss of epoch-76 batch-81 = 1.9741710275411606e-06

Training epoch-76 batch-82
Running loss of epoch-76 batch-82 = 1.8151476979255676e-06

Training epoch-76 batch-83
Running loss of epoch-76 batch-83 = 1.5527475625276566e-06

Training epoch-76 batch-84
Running loss of epoch-76 batch-84 = 1.910608261823654e-06

Training epoch-76 batch-85
Running loss of epoch-76 batch-85 = 1.8631108105182648e-06

Training epoch-76 batch-86
Running loss of epoch-76 batch-86 = 1.3888347893953323e-06

Training epoch-76 batch-87
Running loss of epoch-76 batch-87 = 1.2281816452741623e-06

Training epoch-76 batch-88
Running loss of epoch-76 batch-88 = 1.871725544333458e-06

Training epoch-76 batch-89
Running loss of epoch-76 batch-89 = 1.0076910257339478e-06

Training epoch-76 batch-90
Running loss of epoch-76 batch-90 = 1.8146820366382599e-06

Training epoch-76 batch-91
Running loss of epoch-76 batch-91 = 1.850072294473648e-06

Training epoch-76 batch-92
Running loss of epoch-76 batch-92 = 1.5157274901866913e-06

Training epoch-76 batch-93
Running loss of epoch-76 batch-93 = 1.5415716916322708e-06

Training epoch-76 batch-94
Running loss of epoch-76 batch-94 = 1.6631092876195908e-06

Training epoch-76 batch-95
Running loss of epoch-76 batch-95 = 1.484062522649765e-06

Training epoch-76 batch-96
Running loss of epoch-76 batch-96 = 1.3131648302078247e-06

Training epoch-76 batch-97
Running loss of epoch-76 batch-97 = 1.3061799108982086e-06

Training epoch-76 batch-98
Running loss of epoch-76 batch-98 = 1.2707896530628204e-06

Training epoch-76 batch-99
Running loss of epoch-76 batch-99 = 1.3373792171478271e-06

Training epoch-76 batch-100
Running loss of epoch-76 batch-100 = 1.7543788999319077e-06

Training epoch-76 batch-101
Running loss of epoch-76 batch-101 = 1.1175870895385742e-06

Training epoch-76 batch-102
Running loss of epoch-76 batch-102 = 1.9511207938194275e-06

Training epoch-76 batch-103
Running loss of epoch-76 batch-103 = 1.458916813135147e-06

Training epoch-76 batch-104
Running loss of epoch-76 batch-104 = 1.6700942069292068e-06

Training epoch-76 batch-105
Running loss of epoch-76 batch-105 = 2.257293090224266e-06

Training epoch-76 batch-106
Running loss of epoch-76 batch-106 = 1.2260861694812775e-06

Training epoch-76 batch-107
Running loss of epoch-76 batch-107 = 1.0700896382331848e-06

Training epoch-76 batch-108
Running loss of epoch-76 batch-108 = 1.5459954738616943e-06

Training epoch-76 batch-109
Running loss of epoch-76 batch-109 = 1.4309771358966827e-06

Training epoch-76 batch-110
Running loss of epoch-76 batch-110 = 2.091517671942711e-06

Training epoch-76 batch-111
Running loss of epoch-76 batch-111 = 1.4451798051595688e-06

Training epoch-76 batch-112
Running loss of epoch-76 batch-112 = 1.7618294805288315e-06

Training epoch-76 batch-113
Running loss of epoch-76 batch-113 = 1.6083940863609314e-06

Training epoch-76 batch-114
Running loss of epoch-76 batch-114 = 1.4847610145807266e-06

Training epoch-76 batch-115
Running loss of epoch-76 batch-115 = 9.913928806781769e-07

Training epoch-76 batch-116
Running loss of epoch-76 batch-116 = 1.2903474271297455e-06

Training epoch-76 batch-117
Running loss of epoch-76 batch-117 = 1.132255420088768e-06

Training epoch-76 batch-118
Running loss of epoch-76 batch-118 = 1.2335367500782013e-06

Training epoch-76 batch-119
Running loss of epoch-76 batch-119 = 2.9264483600854874e-06

Training epoch-76 batch-120
Running loss of epoch-76 batch-120 = 1.1213123798370361e-06

Training epoch-76 batch-121
Running loss of epoch-76 batch-121 = 1.8291175365447998e-06

Training epoch-76 batch-122
Running loss of epoch-76 batch-122 = 1.0146759450435638e-06

Training epoch-76 batch-123
Running loss of epoch-76 batch-123 = 2.57883220911026e-06

Training epoch-76 batch-124
Running loss of epoch-76 batch-124 = 1.9462313503026962e-06

Training epoch-76 batch-125
Running loss of epoch-76 batch-125 = 1.125270500779152e-06

Training epoch-76 batch-126
Running loss of epoch-76 batch-126 = 2.3958273231983185e-06

Training epoch-76 batch-127
Running loss of epoch-76 batch-127 = 1.653796061873436e-06

Training epoch-76 batch-128
Running loss of epoch-76 batch-128 = 1.7022248357534409e-06

Training epoch-76 batch-129
Running loss of epoch-76 batch-129 = 1.8428545445203781e-06

Training epoch-76 batch-130
Running loss of epoch-76 batch-130 = 1.6307458281517029e-06

Training epoch-76 batch-131
Running loss of epoch-76 batch-131 = 1.3632234185934067e-06

Training epoch-76 batch-132
Running loss of epoch-76 batch-132 = 1.7872080206871033e-06

Training epoch-76 batch-133
Running loss of epoch-76 batch-133 = 1.434236764907837e-06

Training epoch-76 batch-134
Running loss of epoch-76 batch-134 = 1.6652047634124756e-06

Training epoch-76 batch-135
Running loss of epoch-76 batch-135 = 1.2526288628578186e-06

Training epoch-76 batch-136
Running loss of epoch-76 batch-136 = 1.4794059097766876e-06

Training epoch-76 batch-137
Running loss of epoch-76 batch-137 = 1.4863908290863037e-06

Training epoch-76 batch-138
Running loss of epoch-76 batch-138 = 1.2873206287622452e-06

Training epoch-76 batch-139
Running loss of epoch-76 batch-139 = 1.1967495083808899e-06

Training epoch-76 batch-140
Running loss of epoch-76 batch-140 = 1.5676487237215042e-06

Training epoch-76 batch-141
Running loss of epoch-76 batch-141 = 2.5739427655935287e-06

Training epoch-76 batch-142
Running loss of epoch-76 batch-142 = 2.0381994545459747e-06

Training epoch-76 batch-143
Running loss of epoch-76 batch-143 = 1.2598466128110886e-06

Training epoch-76 batch-144
Running loss of epoch-76 batch-144 = 1.2831296771764755e-06

Training epoch-76 batch-145
Running loss of epoch-76 batch-145 = 1.2095551937818527e-06

Training epoch-76 batch-146
Running loss of epoch-76 batch-146 = 1.1741649359464645e-06

Training epoch-76 batch-147
Running loss of epoch-76 batch-147 = 2.2582244127988815e-06

Training epoch-76 batch-148
Running loss of epoch-76 batch-148 = 1.0807998478412628e-06

Training epoch-76 batch-149
Running loss of epoch-76 batch-149 = 1.1385418474674225e-06

Training epoch-76 batch-150
Running loss of epoch-76 batch-150 = 1.1955853551626205e-06

Training epoch-76 batch-151
Running loss of epoch-76 batch-151 = 1.7774291336536407e-06

Training epoch-76 batch-152
Running loss of epoch-76 batch-152 = 1.7972197383642197e-06

Training epoch-76 batch-153
Running loss of epoch-76 batch-153 = 1.9080471247434616e-06

Training epoch-76 batch-154
Running loss of epoch-76 batch-154 = 1.7078127712011337e-06

Training epoch-76 batch-155
Running loss of epoch-76 batch-155 = 1.6551930457353592e-06

Training epoch-76 batch-156
Running loss of epoch-76 batch-156 = 9.66014340519905e-07

Training epoch-76 batch-157
Running loss of epoch-76 batch-157 = 4.377216100692749e-06

Finished training epoch-76.



Average train loss at epoch-76 = 1.6104012727737426e-06

Started Evaluation

Average val loss at epoch-76 = 3.250180424828278

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-77


Training epoch-77 batch-1
Running loss of epoch-77 batch-1 = 8.002389222383499e-07

Training epoch-77 batch-2
Running loss of epoch-77 batch-2 = 2.28220596909523e-06

Training epoch-77 batch-3
Running loss of epoch-77 batch-3 = 1.6533304005861282e-06

Training epoch-77 batch-4
Running loss of epoch-77 batch-4 = 1.0812655091285706e-06

Training epoch-77 batch-5
Running loss of epoch-77 batch-5 = 1.477077603340149e-06

Training epoch-77 batch-6
Running loss of epoch-77 batch-6 = 1.1695083230733871e-06

Training epoch-77 batch-7
Running loss of epoch-77 batch-7 = 2.0440202206373215e-06

Training epoch-77 batch-8
Running loss of epoch-77 batch-8 = 1.1119991540908813e-06

Training epoch-77 batch-9
Running loss of epoch-77 batch-9 = 1.2293457984924316e-06

Training epoch-77 batch-10
Running loss of epoch-77 batch-10 = 2.164626494050026e-06

Training epoch-77 batch-11
Running loss of epoch-77 batch-11 = 2.0561274141073227e-06

Training epoch-77 batch-12
Running loss of epoch-77 batch-12 = 1.8603168427944183e-06

Training epoch-77 batch-13
Running loss of epoch-77 batch-13 = 1.3667158782482147e-06

Training epoch-77 batch-14
Running loss of epoch-77 batch-14 = 1.389533281326294e-06

Training epoch-77 batch-15
Running loss of epoch-77 batch-15 = 1.364387571811676e-06

Training epoch-77 batch-16
Running loss of epoch-77 batch-16 = 1.5960540622472763e-06

Training epoch-77 batch-17
Running loss of epoch-77 batch-17 = 1.2842938303947449e-06

Training epoch-77 batch-18
Running loss of epoch-77 batch-18 = 1.543201506137848e-06

Training epoch-77 batch-19
Running loss of epoch-77 batch-19 = 2.1033920347690582e-06

Training epoch-77 batch-20
Running loss of epoch-77 batch-20 = 1.5364494174718857e-06

Training epoch-77 batch-21
Running loss of epoch-77 batch-21 = 8.263159543275833e-07

Training epoch-77 batch-22
Running loss of epoch-77 batch-22 = 1.1813826858997345e-06

Training epoch-77 batch-23
Running loss of epoch-77 batch-23 = 1.2316741049289703e-06

Training epoch-77 batch-24
Running loss of epoch-77 batch-24 = 1.5867408365011215e-06

Training epoch-77 batch-25
Running loss of epoch-77 batch-25 = 1.1832453310489655e-06

Training epoch-77 batch-26
Running loss of epoch-77 batch-26 = 1.2831296771764755e-06

Training epoch-77 batch-27
Running loss of epoch-77 batch-27 = 1.3781245797872543e-06

Training epoch-77 batch-28
Running loss of epoch-77 batch-28 = 2.128537744283676e-06

Training epoch-77 batch-29
Running loss of epoch-77 batch-29 = 1.926906406879425e-06

Training epoch-77 batch-30
Running loss of epoch-77 batch-30 = 1.5585683286190033e-06

Training epoch-77 batch-31
Running loss of epoch-77 batch-31 = 1.9071158021688461e-06

Training epoch-77 batch-32
Running loss of epoch-77 batch-32 = 1.2153759598731995e-06

Training epoch-77 batch-33
Running loss of epoch-77 batch-33 = 1.3238750398159027e-06

Training epoch-77 batch-34
Running loss of epoch-77 batch-34 = 1.130625605583191e-06

Training epoch-77 batch-35
Running loss of epoch-77 batch-35 = 1.146690919995308e-06

Training epoch-77 batch-36
Running loss of epoch-77 batch-36 = 1.1725351214408875e-06

Training epoch-77 batch-37
Running loss of epoch-77 batch-37 = 1.3192184269428253e-06

Training epoch-77 batch-38
Running loss of epoch-77 batch-38 = 2.1352898329496384e-06

Training epoch-77 batch-39
Running loss of epoch-77 batch-39 = 1.6461126506328583e-06

Training epoch-77 batch-40
Running loss of epoch-77 batch-40 = 1.2933742254972458e-06

Training epoch-77 batch-41
Running loss of epoch-77 batch-41 = 1.3760291039943695e-06

Training epoch-77 batch-42
Running loss of epoch-77 batch-42 = 1.6738194972276688e-06

Training epoch-77 batch-43
Running loss of epoch-77 batch-43 = 1.614447683095932e-06

Training epoch-77 batch-44
Running loss of epoch-77 batch-44 = 7.846392691135406e-07

Training epoch-77 batch-45
Running loss of epoch-77 batch-45 = 1.3862736523151398e-06

Training epoch-77 batch-46
Running loss of epoch-77 batch-46 = 1.5166588127613068e-06

Training epoch-77 batch-47
Running loss of epoch-77 batch-47 = 1.7371494323015213e-06

Training epoch-77 batch-48
Running loss of epoch-77 batch-48 = 1.3406388461589813e-06

Training epoch-77 batch-49
Running loss of epoch-77 batch-49 = 1.0882504284381866e-06

Training epoch-77 batch-50
Running loss of epoch-77 batch-50 = 1.4458782970905304e-06

Training epoch-77 batch-51
Running loss of epoch-77 batch-51 = 2.1152663975954056e-06

Training epoch-77 batch-52
Running loss of epoch-77 batch-52 = 2.957414835691452e-06

Training epoch-77 batch-53
Running loss of epoch-77 batch-53 = 1.8980354070663452e-06

Training epoch-77 batch-54
Running loss of epoch-77 batch-54 = 1.2132804840803146e-06

Training epoch-77 batch-55
Running loss of epoch-77 batch-55 = 1.871492713689804e-06

Training epoch-77 batch-56
Running loss of epoch-77 batch-56 = 1.7224811017513275e-06

Training epoch-77 batch-57
Running loss of epoch-77 batch-57 = 1.4882534742355347e-06

Training epoch-77 batch-58
Running loss of epoch-77 batch-58 = 1.609092578291893e-06

Training epoch-77 batch-59
Running loss of epoch-77 batch-59 = 1.89291313290596e-06

Training epoch-77 batch-60
Running loss of epoch-77 batch-60 = 1.2796372175216675e-06

Training epoch-77 batch-61
Running loss of epoch-77 batch-61 = 1.7175916582345963e-06

Training epoch-77 batch-62
Running loss of epoch-77 batch-62 = 1.8349383026361465e-06

Training epoch-77 batch-63
Running loss of epoch-77 batch-63 = 1.662643626332283e-06

Training epoch-77 batch-64
Running loss of epoch-77 batch-64 = 1.2505333870649338e-06

Training epoch-77 batch-65
Running loss of epoch-77 batch-65 = 1.9099097698926926e-06

Training epoch-77 batch-66
Running loss of epoch-77 batch-66 = 1.293141394853592e-06

Training epoch-77 batch-67
Running loss of epoch-77 batch-67 = 1.9937288016080856e-06

Training epoch-77 batch-68
Running loss of epoch-77 batch-68 = 1.768115907907486e-06

Training epoch-77 batch-69
Running loss of epoch-77 batch-69 = 1.2016389518976212e-06

Training epoch-77 batch-70
Running loss of epoch-77 batch-70 = 1.6328413039445877e-06

Training epoch-77 batch-71
Running loss of epoch-77 batch-71 = 1.207459717988968e-06

Training epoch-77 batch-72
Running loss of epoch-77 batch-72 = 1.8726568669080734e-06

Training epoch-77 batch-73
Running loss of epoch-77 batch-73 = 2.2707972675561905e-06

Training epoch-77 batch-74
Running loss of epoch-77 batch-74 = 1.9103754311800003e-06

Training epoch-77 batch-75
Running loss of epoch-77 batch-75 = 1.3099052011966705e-06

Training epoch-77 batch-76
Running loss of epoch-77 batch-76 = 2.5513581931591034e-06

Training epoch-77 batch-77
Running loss of epoch-77 batch-77 = 1.1592637747526169e-06

Training epoch-77 batch-78
Running loss of epoch-77 batch-78 = 1.8039718270301819e-06

Training epoch-77 batch-79
Running loss of epoch-77 batch-79 = 1.2544915080070496e-06

Training epoch-77 batch-80
Running loss of epoch-77 batch-80 = 1.069856807589531e-06

Training epoch-77 batch-81
Running loss of epoch-77 batch-81 = 7.37607479095459e-07

Training epoch-77 batch-82
Running loss of epoch-77 batch-82 = 1.7879065126180649e-06

Training epoch-77 batch-83
Running loss of epoch-77 batch-83 = 2.0845327526330948e-06

Training epoch-77 batch-84
Running loss of epoch-77 batch-84 = 2.0621810108423233e-06

Training epoch-77 batch-85
Running loss of epoch-77 batch-85 = 1.1271331459283829e-06

Training epoch-77 batch-86
Running loss of epoch-77 batch-86 = 1.6991980373859406e-06

Training epoch-77 batch-87
Running loss of epoch-77 batch-87 = 1.7695128917694092e-06

Training epoch-77 batch-88
Running loss of epoch-77 batch-88 = 1.4756806194782257e-06

Training epoch-77 batch-89
Running loss of epoch-77 batch-89 = 1.9173603504896164e-06

Training epoch-77 batch-90
Running loss of epoch-77 batch-90 = 1.7725396901369095e-06

Training epoch-77 batch-91
Running loss of epoch-77 batch-91 = 1.4139804989099503e-06

Training epoch-77 batch-92
Running loss of epoch-77 batch-92 = 1.4158431440591812e-06

Training epoch-77 batch-93
Running loss of epoch-77 batch-93 = 1.5140976756811142e-06

Training epoch-77 batch-94
Running loss of epoch-77 batch-94 = 1.242849975824356e-06

Training epoch-77 batch-95
Running loss of epoch-77 batch-95 = 1.6845297068357468e-06

Training epoch-77 batch-96
Running loss of epoch-77 batch-96 = 1.7597340047359467e-06

Training epoch-77 batch-97
Running loss of epoch-77 batch-97 = 1.5962868928909302e-06

Training epoch-77 batch-98
Running loss of epoch-77 batch-98 = 1.469627022743225e-06

Training epoch-77 batch-99
Running loss of epoch-77 batch-99 = 1.9453000277280807e-06

Training epoch-77 batch-100
Running loss of epoch-77 batch-100 = 1.248437911272049e-06

Training epoch-77 batch-101
Running loss of epoch-77 batch-101 = 2.216314896941185e-06

Training epoch-77 batch-102
Running loss of epoch-77 batch-102 = 2.775108441710472e-06

Training epoch-77 batch-103
Running loss of epoch-77 batch-103 = 1.2777745723724365e-06

Training epoch-77 batch-104
Running loss of epoch-77 batch-104 = 1.1913944035768509e-06

Training epoch-77 batch-105
Running loss of epoch-77 batch-105 = 1.4391262084245682e-06

Training epoch-77 batch-106
Running loss of epoch-77 batch-106 = 1.2707896530628204e-06

Training epoch-77 batch-107
Running loss of epoch-77 batch-107 = 2.0531006157398224e-06

Training epoch-77 batch-108
Running loss of epoch-77 batch-108 = 1.961132511496544e-06

Training epoch-77 batch-109
Running loss of epoch-77 batch-109 = 1.925276592373848e-06

Training epoch-77 batch-110
Running loss of epoch-77 batch-110 = 2.1280720829963684e-06

Training epoch-77 batch-111
Running loss of epoch-77 batch-111 = 1.1865049600601196e-06

Training epoch-77 batch-112
Running loss of epoch-77 batch-112 = 1.862877979874611e-06

Training epoch-77 batch-113
Running loss of epoch-77 batch-113 = 1.2631062418222427e-06

Training epoch-77 batch-114
Running loss of epoch-77 batch-114 = 1.4437828212976456e-06

Training epoch-77 batch-115
Running loss of epoch-77 batch-115 = 2.335989847779274e-06

Training epoch-77 batch-116
Running loss of epoch-77 batch-116 = 2.5792978703975677e-06

Training epoch-77 batch-117
Running loss of epoch-77 batch-117 = 1.8826685845851898e-06

Training epoch-77 batch-118
Running loss of epoch-77 batch-118 = 1.0780058801174164e-06

Training epoch-77 batch-119
Running loss of epoch-77 batch-119 = 1.2852251529693604e-06

Training epoch-77 batch-120
Running loss of epoch-77 batch-120 = 1.896638423204422e-06

Training epoch-77 batch-121
Running loss of epoch-77 batch-121 = 1.210719347000122e-06

Training epoch-77 batch-122
Running loss of epoch-77 batch-122 = 1.6093254089355469e-06

Training epoch-77 batch-123
Running loss of epoch-77 batch-123 = 1.4672987163066864e-06

Training epoch-77 batch-124
Running loss of epoch-77 batch-124 = 1.6654375940561295e-06

Training epoch-77 batch-125
Running loss of epoch-77 batch-125 = 2.126675099134445e-06

Training epoch-77 batch-126
Running loss of epoch-77 batch-126 = 1.7452985048294067e-06

Training epoch-77 batch-127
Running loss of epoch-77 batch-127 = 1.1699739843606949e-06

Training epoch-77 batch-128
Running loss of epoch-77 batch-128 = 1.2728851288557053e-06

Training epoch-77 batch-129
Running loss of epoch-77 batch-129 = 9.771902114152908e-07

Training epoch-77 batch-130
Running loss of epoch-77 batch-130 = 1.4891847968101501e-06

Training epoch-77 batch-131
Running loss of epoch-77 batch-131 = 1.9066501408815384e-06

Training epoch-77 batch-132
Running loss of epoch-77 batch-132 = 1.2870877981185913e-06

Training epoch-77 batch-133
Running loss of epoch-77 batch-133 = 1.130392774939537e-06

Training epoch-77 batch-134
Running loss of epoch-77 batch-134 = 1.4880206435918808e-06

Training epoch-77 batch-135
Running loss of epoch-77 batch-135 = 1.682434231042862e-06

Training epoch-77 batch-136
Running loss of epoch-77 batch-136 = 2.162996679544449e-06

Training epoch-77 batch-137
Running loss of epoch-77 batch-137 = 1.2046657502651215e-06

Training epoch-77 batch-138
Running loss of epoch-77 batch-138 = 1.1152587831020355e-06

Training epoch-77 batch-139
Running loss of epoch-77 batch-139 = 1.0158400982618332e-06

Training epoch-77 batch-140
Running loss of epoch-77 batch-140 = 1.4866236597299576e-06

Training epoch-77 batch-141
Running loss of epoch-77 batch-141 = 1.1031515896320343e-06

Training epoch-77 batch-142
Running loss of epoch-77 batch-142 = 1.6670674085617065e-06

Training epoch-77 batch-143
Running loss of epoch-77 batch-143 = 1.780455932021141e-06

Training epoch-77 batch-144
Running loss of epoch-77 batch-144 = 1.8123537302017212e-06

Training epoch-77 batch-145
Running loss of epoch-77 batch-145 = 1.1902302503585815e-06

Training epoch-77 batch-146
Running loss of epoch-77 batch-146 = 1.6381964087486267e-06

Training epoch-77 batch-147
Running loss of epoch-77 batch-147 = 1.4258548617362976e-06

Training epoch-77 batch-148
Running loss of epoch-77 batch-148 = 1.0132789611816406e-06

Training epoch-77 batch-149
Running loss of epoch-77 batch-149 = 9.690411388874054e-07

Training epoch-77 batch-150
Running loss of epoch-77 batch-150 = 2.112472429871559e-06

Training epoch-77 batch-151
Running loss of epoch-77 batch-151 = 2.139247953891754e-06

Training epoch-77 batch-152
Running loss of epoch-77 batch-152 = 1.8624123185873032e-06

Training epoch-77 batch-153
Running loss of epoch-77 batch-153 = 1.593027263879776e-06

Training epoch-77 batch-154
Running loss of epoch-77 batch-154 = 1.8039718270301819e-06

Training epoch-77 batch-155
Running loss of epoch-77 batch-155 = 1.7578713595867157e-06

Training epoch-77 batch-156
Running loss of epoch-77 batch-156 = 1.9276048988103867e-06

Training epoch-77 batch-157
Running loss of epoch-77 batch-157 = 4.533678293228149e-06

Finished training epoch-77.



Average train loss at epoch-77 = 1.5826940536499024e-06

Started Evaluation

Average val loss at epoch-77 = 3.2542037556045935

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.64 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-78


Training epoch-78 batch-1
Running loss of epoch-78 batch-1 = 1.4218967407941818e-06

Training epoch-78 batch-2
Running loss of epoch-78 batch-2 = 1.4596153050661087e-06

Training epoch-78 batch-3
Running loss of epoch-78 batch-3 = 1.2814998626708984e-06

Training epoch-78 batch-4
Running loss of epoch-78 batch-4 = 1.8810387700796127e-06

Training epoch-78 batch-5
Running loss of epoch-78 batch-5 = 1.1185184121131897e-06

Training epoch-78 batch-6
Running loss of epoch-78 batch-6 = 1.8475111573934555e-06

Training epoch-78 batch-7
Running loss of epoch-78 batch-7 = 1.1587981134653091e-06

Training epoch-78 batch-8
Running loss of epoch-78 batch-8 = 1.8172431737184525e-06

Training epoch-78 batch-9
Running loss of epoch-78 batch-9 = 2.100598067045212e-06

Training epoch-78 batch-10
Running loss of epoch-78 batch-10 = 1.476844772696495e-06

Training epoch-78 batch-11
Running loss of epoch-78 batch-11 = 1.62515789270401e-06

Training epoch-78 batch-12
Running loss of epoch-78 batch-12 = 1.6978010535240173e-06

Training epoch-78 batch-13
Running loss of epoch-78 batch-13 = 1.3867393136024475e-06

Training epoch-78 batch-14
Running loss of epoch-78 batch-14 = 1.5550758689641953e-06

Training epoch-78 batch-15
Running loss of epoch-78 batch-15 = 1.2363307178020477e-06

Training epoch-78 batch-16
Running loss of epoch-78 batch-16 = 1.1550728231668472e-06

Training epoch-78 batch-17
Running loss of epoch-78 batch-17 = 1.0225921869277954e-06

Training epoch-78 batch-18
Running loss of epoch-78 batch-18 = 1.5476252883672714e-06

Training epoch-78 batch-19
Running loss of epoch-78 batch-19 = 1.8668361008167267e-06

Training epoch-78 batch-20
Running loss of epoch-78 batch-20 = 1.1937227100133896e-06

Training epoch-78 batch-21
Running loss of epoch-78 batch-21 = 1.99279747903347e-06

Training epoch-78 batch-22
Running loss of epoch-78 batch-22 = 2.455897629261017e-06

Training epoch-78 batch-23
Running loss of epoch-78 batch-23 = 1.8354039639234543e-06

Training epoch-78 batch-24
Running loss of epoch-78 batch-24 = 1.3890676200389862e-06

Training epoch-78 batch-25
Running loss of epoch-78 batch-25 = 1.723412424325943e-06

Training epoch-78 batch-26
Running loss of epoch-78 batch-26 = 1.2114178389310837e-06

Training epoch-78 batch-27
Running loss of epoch-78 batch-27 = 1.6551930457353592e-06

Training epoch-78 batch-28
Running loss of epoch-78 batch-28 = 1.4882534742355347e-06

Training epoch-78 batch-29
Running loss of epoch-78 batch-29 = 1.434003934264183e-06

Training epoch-78 batch-30
Running loss of epoch-78 batch-30 = 1.3236422091722488e-06

Training epoch-78 batch-31
Running loss of epoch-78 batch-31 = 1.314328983426094e-06

Training epoch-78 batch-32
Running loss of epoch-78 batch-32 = 1.2011732906103134e-06

Training epoch-78 batch-33
Running loss of epoch-78 batch-33 = 1.373467966914177e-06

Training epoch-78 batch-34
Running loss of epoch-78 batch-34 = 1.512700691819191e-06

Training epoch-78 batch-35
Running loss of epoch-78 batch-35 = 1.9958242774009705e-06

Training epoch-78 batch-36
Running loss of epoch-78 batch-36 = 1.1879019439220428e-06

Training epoch-78 batch-37
Running loss of epoch-78 batch-37 = 1.4670658856630325e-06

Training epoch-78 batch-38
Running loss of epoch-78 batch-38 = 1.860782504081726e-06

Training epoch-78 batch-39
Running loss of epoch-78 batch-39 = 1.432141289114952e-06

Training epoch-78 batch-40
Running loss of epoch-78 batch-40 = 1.400010660290718e-06

Training epoch-78 batch-41
Running loss of epoch-78 batch-41 = 1.5208497643470764e-06

Training epoch-78 batch-42
Running loss of epoch-78 batch-42 = 1.7064157873392105e-06

Training epoch-78 batch-43
Running loss of epoch-78 batch-43 = 1.051928848028183e-06

Training epoch-78 batch-44
Running loss of epoch-78 batch-44 = 1.08242966234684e-06

Training epoch-78 batch-45
Running loss of epoch-78 batch-45 = 1.828884705901146e-06

Training epoch-78 batch-46
Running loss of epoch-78 batch-46 = 1.396751031279564e-06

Training epoch-78 batch-47
Running loss of epoch-78 batch-47 = 1.8742866814136505e-06

Training epoch-78 batch-48
Running loss of epoch-78 batch-48 = 1.1918600648641586e-06

Training epoch-78 batch-49
Running loss of epoch-78 batch-49 = 1.8218997865915298e-06

Training epoch-78 batch-50
Running loss of epoch-78 batch-50 = 1.0416842997074127e-06

Training epoch-78 batch-51
Running loss of epoch-78 batch-51 = 1.5338882803916931e-06

Training epoch-78 batch-52
Running loss of epoch-78 batch-52 = 1.3718381524085999e-06

Training epoch-78 batch-53
Running loss of epoch-78 batch-53 = 2.307351678609848e-06

Training epoch-78 batch-54
Running loss of epoch-78 batch-54 = 1.4891847968101501e-06

Training epoch-78 batch-55
Running loss of epoch-78 batch-55 = 1.6221310943365097e-06

Training epoch-78 batch-56
Running loss of epoch-78 batch-56 = 2.257060259580612e-06

Training epoch-78 batch-57
Running loss of epoch-78 batch-57 = 1.4612451195716858e-06

Training epoch-78 batch-58
Running loss of epoch-78 batch-58 = 1.8838327378034592e-06

Training epoch-78 batch-59
Running loss of epoch-78 batch-59 = 1.4256220310926437e-06

Training epoch-78 batch-60
Running loss of epoch-78 batch-60 = 9.431969374418259e-07

Training epoch-78 batch-61
Running loss of epoch-78 batch-61 = 1.5618279576301575e-06

Training epoch-78 batch-62
Running loss of epoch-78 batch-62 = 1.4826655387878418e-06

Training epoch-78 batch-63
Running loss of epoch-78 batch-63 = 1.996289938688278e-06

Training epoch-78 batch-64
Running loss of epoch-78 batch-64 = 1.125270500779152e-06

Training epoch-78 batch-65
Running loss of epoch-78 batch-65 = 1.066364347934723e-06

Training epoch-78 batch-66
Running loss of epoch-78 batch-66 = 1.955311745405197e-06

Training epoch-78 batch-67
Running loss of epoch-78 batch-67 = 1.5473924577236176e-06

Training epoch-78 batch-68
Running loss of epoch-78 batch-68 = 1.364387571811676e-06

Training epoch-78 batch-69
Running loss of epoch-78 batch-69 = 2.009328454732895e-06

Training epoch-78 batch-70
Running loss of epoch-78 batch-70 = 1.116190105676651e-06

Training epoch-78 batch-71
Running loss of epoch-78 batch-71 = 1.5897676348686218e-06

Training epoch-78 batch-72
Running loss of epoch-78 batch-72 = 1.2880191206932068e-06

Training epoch-78 batch-73
Running loss of epoch-78 batch-73 = 1.3390090316534042e-06

Training epoch-78 batch-74
Running loss of epoch-78 batch-74 = 1.4314427971839905e-06

Training epoch-78 batch-75
Running loss of epoch-78 batch-75 = 1.5131663531064987e-06

Training epoch-78 batch-76
Running loss of epoch-78 batch-76 = 1.682434231042862e-06

Training epoch-78 batch-77
Running loss of epoch-78 batch-77 = 2.1473970264196396e-06

Training epoch-78 batch-78
Running loss of epoch-78 batch-78 = 1.7606653273105621e-06

Training epoch-78 batch-79
Running loss of epoch-78 batch-79 = 1.257285475730896e-06

Training epoch-78 batch-80
Running loss of epoch-78 batch-80 = 1.1778902262449265e-06

Training epoch-78 batch-81
Running loss of epoch-78 batch-81 = 1.621665433049202e-06

Training epoch-78 batch-82
Running loss of epoch-78 batch-82 = 1.3597309589385986e-06

Training epoch-78 batch-83
Running loss of epoch-78 batch-83 = 2.0936131477355957e-06

Training epoch-78 batch-84
Running loss of epoch-78 batch-84 = 1.596519723534584e-06

Training epoch-78 batch-85
Running loss of epoch-78 batch-85 = 1.9685830920934677e-06

Training epoch-78 batch-86
Running loss of epoch-78 batch-86 = 1.8980354070663452e-06

Training epoch-78 batch-87
Running loss of epoch-78 batch-87 = 1.0796356946229935e-06

Training epoch-78 batch-88
Running loss of epoch-78 batch-88 = 2.261251211166382e-06

Training epoch-78 batch-89
Running loss of epoch-78 batch-89 = 1.916429027915001e-06

Training epoch-78 batch-90
Running loss of epoch-78 batch-90 = 1.5164259821176529e-06

Training epoch-78 batch-91
Running loss of epoch-78 batch-91 = 1.6258563846349716e-06

Training epoch-78 batch-92
Running loss of epoch-78 batch-92 = 8.689239621162415e-07

Training epoch-78 batch-93
Running loss of epoch-78 batch-93 = 1.4919787645339966e-06

Training epoch-78 batch-94
Running loss of epoch-78 batch-94 = 1.9853468984365463e-06

Training epoch-78 batch-95
Running loss of epoch-78 batch-95 = 1.251930370926857e-06

Training epoch-78 batch-96
Running loss of epoch-78 batch-96 = 1.7203856259584427e-06

Training epoch-78 batch-97
Running loss of epoch-78 batch-97 = 2.073589712381363e-06

Training epoch-78 batch-98
Running loss of epoch-78 batch-98 = 1.552049070596695e-06

Training epoch-78 batch-99
Running loss of epoch-78 batch-99 = 1.507345587015152e-06

Training epoch-78 batch-100
Running loss of epoch-78 batch-100 = 1.8547289073467255e-06

Training epoch-78 batch-101
Running loss of epoch-78 batch-101 = 1.1781230568885803e-06

Training epoch-78 batch-102
Running loss of epoch-78 batch-102 = 1.7888378351926804e-06

Training epoch-78 batch-103
Running loss of epoch-78 batch-103 = 1.2319069355726242e-06

Training epoch-78 batch-104
Running loss of epoch-78 batch-104 = 2.6230700314044952e-06

Training epoch-78 batch-105
Running loss of epoch-78 batch-105 = 1.8747523427009583e-06

Training epoch-78 batch-106
Running loss of epoch-78 batch-106 = 1.5634577721357346e-06

Training epoch-78 batch-107
Running loss of epoch-78 batch-107 = 1.8510036170482635e-06

Training epoch-78 batch-108
Running loss of epoch-78 batch-108 = 1.1094380170106888e-06

Training epoch-78 batch-109
Running loss of epoch-78 batch-109 = 1.393025740981102e-06

Training epoch-78 batch-110
Running loss of epoch-78 batch-110 = 1.7185229808092117e-06

Training epoch-78 batch-111
Running loss of epoch-78 batch-111 = 1.8076971173286438e-06

Training epoch-78 batch-112
Running loss of epoch-78 batch-112 = 1.6617123037576675e-06

Training epoch-78 batch-113
Running loss of epoch-78 batch-113 = 1.7029233276844025e-06

Training epoch-78 batch-114
Running loss of epoch-78 batch-114 = 2.0903535187244415e-06

Training epoch-78 batch-115
Running loss of epoch-78 batch-115 = 1.3818498700857162e-06

Training epoch-78 batch-116
Running loss of epoch-78 batch-116 = 1.1981464922428131e-06

Training epoch-78 batch-117
Running loss of epoch-78 batch-117 = 2.121785655617714e-06

Training epoch-78 batch-118
Running loss of epoch-78 batch-118 = 1.7918646335601807e-06

Training epoch-78 batch-119
Running loss of epoch-78 batch-119 = 1.6137491911649704e-06

Training epoch-78 batch-120
Running loss of epoch-78 batch-120 = 1.7832498997449875e-06

Training epoch-78 batch-121
Running loss of epoch-78 batch-121 = 1.6307458281517029e-06

Training epoch-78 batch-122
Running loss of epoch-78 batch-122 = 1.5238765627145767e-06

Training epoch-78 batch-123
Running loss of epoch-78 batch-123 = 8.612405508756638e-07

Training epoch-78 batch-124
Running loss of epoch-78 batch-124 = 7.566995918750763e-07

Training epoch-78 batch-125
Running loss of epoch-78 batch-125 = 1.6668345779180527e-06

Training epoch-78 batch-126
Running loss of epoch-78 batch-126 = 1.482199877500534e-06

Training epoch-78 batch-127
Running loss of epoch-78 batch-127 = 9.466893970966339e-07

Training epoch-78 batch-128
Running loss of epoch-78 batch-128 = 1.916196197271347e-06

Training epoch-78 batch-129
Running loss of epoch-78 batch-129 = 1.4049001038074493e-06

Training epoch-78 batch-130
Running loss of epoch-78 batch-130 = 1.7741695046424866e-06

Training epoch-78 batch-131
Running loss of epoch-78 batch-131 = 9.336508810520172e-07

Training epoch-78 batch-132
Running loss of epoch-78 batch-132 = 9.904615581035614e-07

Training epoch-78 batch-133
Running loss of epoch-78 batch-133 = 1.6279518604278564e-06

Training epoch-78 batch-134
Running loss of epoch-78 batch-134 = 1.5110708773136139e-06

Training epoch-78 batch-135
Running loss of epoch-78 batch-135 = 2.1238811314105988e-06

Training epoch-78 batch-136
Running loss of epoch-78 batch-136 = 1.2710224837064743e-06

Training epoch-78 batch-137
Running loss of epoch-78 batch-137 = 1.6775447875261307e-06

Training epoch-78 batch-138
Running loss of epoch-78 batch-138 = 1.383945345878601e-06

Training epoch-78 batch-139
Running loss of epoch-78 batch-139 = 1.5452969819307327e-06

Training epoch-78 batch-140
Running loss of epoch-78 batch-140 = 2.391636371612549e-06

Training epoch-78 batch-141
Running loss of epoch-78 batch-141 = 1.1222437024116516e-06

Training epoch-78 batch-142
Running loss of epoch-78 batch-142 = 1.707347109913826e-06

Training epoch-78 batch-143
Running loss of epoch-78 batch-143 = 1.2486707419157028e-06

Training epoch-78 batch-144
Running loss of epoch-78 batch-144 = 1.6211997717618942e-06

Training epoch-78 batch-145
Running loss of epoch-78 batch-145 = 1.5231780707836151e-06

Training epoch-78 batch-146
Running loss of epoch-78 batch-146 = 1.637730747461319e-06

Training epoch-78 batch-147
Running loss of epoch-78 batch-147 = 8.619390428066254e-07

Training epoch-78 batch-148
Running loss of epoch-78 batch-148 = 1.0854564607143402e-06

Training epoch-78 batch-149
Running loss of epoch-78 batch-149 = 1.2009404599666595e-06

Training epoch-78 batch-150
Running loss of epoch-78 batch-150 = 1.41095370054245e-06

Training epoch-78 batch-151
Running loss of epoch-78 batch-151 = 1.5299301594495773e-06

Training epoch-78 batch-152
Running loss of epoch-78 batch-152 = 1.6470439732074738e-06

Training epoch-78 batch-153
Running loss of epoch-78 batch-153 = 1.7043203115463257e-06

Training epoch-78 batch-154
Running loss of epoch-78 batch-154 = 1.7667189240455627e-06

Training epoch-78 batch-155
Running loss of epoch-78 batch-155 = 1.6167759895324707e-06

Training epoch-78 batch-156
Running loss of epoch-78 batch-156 = 1.6028061509132385e-06

Training epoch-78 batch-157
Running loss of epoch-78 batch-157 = 9.655952453613281e-06

Finished training epoch-78.



Average train loss at epoch-78 = 1.5627533197402954e-06

Started Evaluation

Average val loss at epoch-78 = 3.260028566184797

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-79


Training epoch-79 batch-1
Running loss of epoch-79 batch-1 = 1.568114385008812e-06

Training epoch-79 batch-2
Running loss of epoch-79 batch-2 = 1.5636906027793884e-06

Training epoch-79 batch-3
Running loss of epoch-79 batch-3 = 1.0707881301641464e-06

Training epoch-79 batch-4
Running loss of epoch-79 batch-4 = 1.6987323760986328e-06

Training epoch-79 batch-5
Running loss of epoch-79 batch-5 = 1.9639264792203903e-06

Training epoch-79 batch-6
Running loss of epoch-79 batch-6 = 2.26474367082119e-06

Training epoch-79 batch-7
Running loss of epoch-79 batch-7 = 1.239357516169548e-06

Training epoch-79 batch-8
Running loss of epoch-79 batch-8 = 1.4784745872020721e-06

Training epoch-79 batch-9
Running loss of epoch-79 batch-9 = 8.034985512495041e-07

Training epoch-79 batch-10
Running loss of epoch-79 batch-10 = 2.2402964532375336e-06

Training epoch-79 batch-11
Running loss of epoch-79 batch-11 = 2.1101441234350204e-06

Training epoch-79 batch-12
Running loss of epoch-79 batch-12 = 1.5797559171915054e-06

Training epoch-79 batch-13
Running loss of epoch-79 batch-13 = 1.257285475730896e-06

Training epoch-79 batch-14
Running loss of epoch-79 batch-14 = 1.4633405953645706e-06

Training epoch-79 batch-15
Running loss of epoch-79 batch-15 = 1.8200371414422989e-06

Training epoch-79 batch-16
Running loss of epoch-79 batch-16 = 1.5848781913518906e-06

Training epoch-79 batch-17
Running loss of epoch-79 batch-17 = 1.1790543794631958e-06

Training epoch-79 batch-18
Running loss of epoch-79 batch-18 = 1.5522819012403488e-06

Training epoch-79 batch-19
Running loss of epoch-79 batch-19 = 1.412816345691681e-06

Training epoch-79 batch-20
Running loss of epoch-79 batch-20 = 1.6933772712945938e-06

Training epoch-79 batch-21
Running loss of epoch-79 batch-21 = 7.555354386568069e-07

Training epoch-79 batch-22
Running loss of epoch-79 batch-22 = 9.818468242883682e-07

Training epoch-79 batch-23
Running loss of epoch-79 batch-23 = 1.108972355723381e-06

Training epoch-79 batch-24
Running loss of epoch-79 batch-24 = 1.17509625852108e-06

Training epoch-79 batch-25
Running loss of epoch-79 batch-25 = 1.4693941920995712e-06

Training epoch-79 batch-26
Running loss of epoch-79 batch-26 = 1.730630174279213e-06

Training epoch-79 batch-27
Running loss of epoch-79 batch-27 = 2.155313268303871e-06

Training epoch-79 batch-28
Running loss of epoch-79 batch-28 = 2.1348241716623306e-06

Training epoch-79 batch-29
Running loss of epoch-79 batch-29 = 1.2991949915885925e-06

Training epoch-79 batch-30
Running loss of epoch-79 batch-30 = 1.584179699420929e-06

Training epoch-79 batch-31
Running loss of epoch-79 batch-31 = 1.791166141629219e-06

Training epoch-79 batch-32
Running loss of epoch-79 batch-32 = 1.4742836356163025e-06

Training epoch-79 batch-33
Running loss of epoch-79 batch-33 = 1.6789417713880539e-06

Training epoch-79 batch-34
Running loss of epoch-79 batch-34 = 1.6426201909780502e-06

Training epoch-79 batch-35
Running loss of epoch-79 batch-35 = 1.3385433703660965e-06

Training epoch-79 batch-36
Running loss of epoch-79 batch-36 = 1.691281795501709e-06

Training epoch-79 batch-37
Running loss of epoch-79 batch-37 = 1.3879034668207169e-06

Training epoch-79 batch-38
Running loss of epoch-79 batch-38 = 2.186046913266182e-06

Training epoch-79 batch-39
Running loss of epoch-79 batch-39 = 1.5387777239084244e-06

Training epoch-79 batch-40
Running loss of epoch-79 batch-40 = 1.6996636986732483e-06

Training epoch-79 batch-41
Running loss of epoch-79 batch-41 = 1.3604294508695602e-06

Training epoch-79 batch-42
Running loss of epoch-79 batch-42 = 1.7825514078140259e-06

Training epoch-79 batch-43
Running loss of epoch-79 batch-43 = 1.110835000872612e-06

Training epoch-79 batch-44
Running loss of epoch-79 batch-44 = 1.58604234457016e-06

Training epoch-79 batch-45
Running loss of epoch-79 batch-45 = 1.4866236597299576e-06

Training epoch-79 batch-46
Running loss of epoch-79 batch-46 = 1.2754462659358978e-06

Training epoch-79 batch-47
Running loss of epoch-79 batch-47 = 1.5422701835632324e-06

Training epoch-79 batch-48
Running loss of epoch-79 batch-48 = 1.923413947224617e-06

Training epoch-79 batch-49
Running loss of epoch-79 batch-49 = 2.0600855350494385e-06

Training epoch-79 batch-50
Running loss of epoch-79 batch-50 = 1.2682285159826279e-06

Training epoch-79 batch-51
Running loss of epoch-79 batch-51 = 2.039829269051552e-06

Training epoch-79 batch-52
Running loss of epoch-79 batch-52 = 1.4971010386943817e-06

Training epoch-79 batch-53
Running loss of epoch-79 batch-53 = 1.7683487385511398e-06

Training epoch-79 batch-54
Running loss of epoch-79 batch-54 = 1.4088582247495651e-06

Training epoch-79 batch-55
Running loss of epoch-79 batch-55 = 1.4633405953645706e-06

Training epoch-79 batch-56
Running loss of epoch-79 batch-56 = 1.4372635632753372e-06

Training epoch-79 batch-57
Running loss of epoch-79 batch-57 = 1.8766149878501892e-06

Training epoch-79 batch-58
Running loss of epoch-79 batch-58 = 1.164386048913002e-06

Training epoch-79 batch-59
Running loss of epoch-79 batch-59 = 1.9585713744163513e-06

Training epoch-79 batch-60
Running loss of epoch-79 batch-60 = 1.7331913113594055e-06

Training epoch-79 batch-61
Running loss of epoch-79 batch-61 = 1.9478611648082733e-06

Training epoch-79 batch-62
Running loss of epoch-79 batch-62 = 1.5669502317905426e-06

Training epoch-79 batch-63
Running loss of epoch-79 batch-63 = 1.1490192264318466e-06

Training epoch-79 batch-64
Running loss of epoch-79 batch-64 = 1.323176547884941e-06

Training epoch-79 batch-65
Running loss of epoch-79 batch-65 = 1.5578698366880417e-06

Training epoch-79 batch-66
Running loss of epoch-79 batch-66 = 1.5178229659795761e-06

Training epoch-79 batch-67
Running loss of epoch-79 batch-67 = 1.3620592653751373e-06

Training epoch-79 batch-68
Running loss of epoch-79 batch-68 = 2.2817403078079224e-06

Training epoch-79 batch-69
Running loss of epoch-79 batch-69 = 1.5890691429376602e-06

Training epoch-79 batch-70
Running loss of epoch-79 batch-70 = 1.816311851143837e-06

Training epoch-79 batch-71
Running loss of epoch-79 batch-71 = 1.6349367797374725e-06

Training epoch-79 batch-72
Running loss of epoch-79 batch-72 = 1.2344680726528168e-06

Training epoch-79 batch-73
Running loss of epoch-79 batch-73 = 1.8551945686340332e-06

Training epoch-79 batch-74
Running loss of epoch-79 batch-74 = 9.66014340519905e-07

Training epoch-79 batch-75
Running loss of epoch-79 batch-75 = 1.2614764273166656e-06

Training epoch-79 batch-76
Running loss of epoch-79 batch-76 = 1.4442484825849533e-06

Training epoch-79 batch-77
Running loss of epoch-79 batch-77 = 1.769978553056717e-06

Training epoch-79 batch-78
Running loss of epoch-79 batch-78 = 1.369975507259369e-06

Training epoch-79 batch-79
Running loss of epoch-79 batch-79 = 1.414446160197258e-06

Training epoch-79 batch-80
Running loss of epoch-79 batch-80 = 1.8044374883174896e-06

Training epoch-79 batch-81
Running loss of epoch-79 batch-81 = 1.7744023352861404e-06

Training epoch-79 batch-82
Running loss of epoch-79 batch-82 = 2.087792381644249e-06

Training epoch-79 batch-83
Running loss of epoch-79 batch-83 = 1.1476222425699234e-06

Training epoch-79 batch-84
Running loss of epoch-79 batch-84 = 1.5450641512870789e-06

Training epoch-79 batch-85
Running loss of epoch-79 batch-85 = 1.0456424206495285e-06

Training epoch-79 batch-86
Running loss of epoch-79 batch-86 = 1.6605481505393982e-06

Training epoch-79 batch-87
Running loss of epoch-79 batch-87 = 1.5906989574432373e-06

Training epoch-79 batch-88
Running loss of epoch-79 batch-88 = 1.6794074326753616e-06

Training epoch-79 batch-89
Running loss of epoch-79 batch-89 = 7.082708179950714e-07

Training epoch-79 batch-90
Running loss of epoch-79 batch-90 = 1.9078142940998077e-06

Training epoch-79 batch-91
Running loss of epoch-79 batch-91 = 8.952338248491287e-07

Training epoch-79 batch-92
Running loss of epoch-79 batch-92 = 1.3159587979316711e-06

Training epoch-79 batch-93
Running loss of epoch-79 batch-93 = 1.427019014954567e-06

Training epoch-79 batch-94
Running loss of epoch-79 batch-94 = 2.146465703845024e-06

Training epoch-79 batch-95
Running loss of epoch-79 batch-95 = 1.9762665033340454e-06

Training epoch-79 batch-96
Running loss of epoch-79 batch-96 = 1.8510036170482635e-06

Training epoch-79 batch-97
Running loss of epoch-79 batch-97 = 1.7813872545957565e-06

Training epoch-79 batch-98
Running loss of epoch-79 batch-98 = 1.2516975402832031e-06

Training epoch-79 batch-99
Running loss of epoch-79 batch-99 = 1.3993121683597565e-06

Training epoch-79 batch-100
Running loss of epoch-79 batch-100 = 1.711072400212288e-06

Training epoch-79 batch-101
Running loss of epoch-79 batch-101 = 1.796521246433258e-06

Training epoch-79 batch-102
Running loss of epoch-79 batch-102 = 1.6263220459222794e-06

Training epoch-79 batch-103
Running loss of epoch-79 batch-103 = 1.2158416211605072e-06

Training epoch-79 batch-104
Running loss of epoch-79 batch-104 = 1.8158461898565292e-06

Training epoch-79 batch-105
Running loss of epoch-79 batch-105 = 1.841224730014801e-06

Training epoch-79 batch-106
Running loss of epoch-79 batch-106 = 1.1406373232603073e-06

Training epoch-79 batch-107
Running loss of epoch-79 batch-107 = 1.446576789021492e-06

Training epoch-79 batch-108
Running loss of epoch-79 batch-108 = 1.7820857465267181e-06

Training epoch-79 batch-109
Running loss of epoch-79 batch-109 = 8.523929864168167e-07

Training epoch-79 batch-110
Running loss of epoch-79 batch-110 = 1.9310973584651947e-06

Training epoch-79 batch-111
Running loss of epoch-79 batch-111 = 1.3404060155153275e-06

Training epoch-79 batch-112
Running loss of epoch-79 batch-112 = 1.294771209359169e-06

Training epoch-79 batch-113
Running loss of epoch-79 batch-113 = 1.4004763215780258e-06

Training epoch-79 batch-114
Running loss of epoch-79 batch-114 = 1.1958181858062744e-06

Training epoch-79 batch-115
Running loss of epoch-79 batch-115 = 1.5874393284320831e-06

Training epoch-79 batch-116
Running loss of epoch-79 batch-116 = 1.3529788702726364e-06

Training epoch-79 batch-117
Running loss of epoch-79 batch-117 = 1.2940727174282074e-06

Training epoch-79 batch-118
Running loss of epoch-79 batch-118 = 1.4672987163066864e-06

Training epoch-79 batch-119
Running loss of epoch-79 batch-119 = 1.309206709265709e-06

Training epoch-79 batch-120
Running loss of epoch-79 batch-120 = 9.441282600164413e-07

Training epoch-79 batch-121
Running loss of epoch-79 batch-121 = 1.5408731997013092e-06

Training epoch-79 batch-122
Running loss of epoch-79 batch-122 = 1.3720709830522537e-06

Training epoch-79 batch-123
Running loss of epoch-79 batch-123 = 1.794891431927681e-06

Training epoch-79 batch-124
Running loss of epoch-79 batch-124 = 1.1096708476543427e-06

Training epoch-79 batch-125
Running loss of epoch-79 batch-125 = 1.7469283193349838e-06

Training epoch-79 batch-126
Running loss of epoch-79 batch-126 = 1.1383090168237686e-06

Training epoch-79 batch-127
Running loss of epoch-79 batch-127 = 1.4035031199455261e-06

Training epoch-79 batch-128
Running loss of epoch-79 batch-128 = 2.3103784769773483e-06

Training epoch-79 batch-129
Running loss of epoch-79 batch-129 = 1.4987308531999588e-06

Training epoch-79 batch-130
Running loss of epoch-79 batch-130 = 1.648208126425743e-06

Training epoch-79 batch-131
Running loss of epoch-79 batch-131 = 1.1406373232603073e-06

Training epoch-79 batch-132
Running loss of epoch-79 batch-132 = 1.0225921869277954e-06

Training epoch-79 batch-133
Running loss of epoch-79 batch-133 = 1.460779458284378e-06

Training epoch-79 batch-134
Running loss of epoch-79 batch-134 = 1.4088582247495651e-06

Training epoch-79 batch-135
Running loss of epoch-79 batch-135 = 1.5497207641601562e-06

Training epoch-79 batch-136
Running loss of epoch-79 batch-136 = 9.192153811454773e-07

Training epoch-79 batch-137
Running loss of epoch-79 batch-137 = 1.6705598682165146e-06

Training epoch-79 batch-138
Running loss of epoch-79 batch-138 = 1.3327226042747498e-06

Training epoch-79 batch-139
Running loss of epoch-79 batch-139 = 1.4973338693380356e-06

Training epoch-79 batch-140
Running loss of epoch-79 batch-140 = 2.430751919746399e-06

Training epoch-79 batch-141
Running loss of epoch-79 batch-141 = 1.4419201761484146e-06

Training epoch-79 batch-142
Running loss of epoch-79 batch-142 = 1.1827796697616577e-06

Training epoch-79 batch-143
Running loss of epoch-79 batch-143 = 1.9404105842113495e-06

Training epoch-79 batch-144
Running loss of epoch-79 batch-144 = 1.3280659914016724e-06

Training epoch-79 batch-145
Running loss of epoch-79 batch-145 = 1.2477394193410873e-06

Training epoch-79 batch-146
Running loss of epoch-79 batch-146 = 1.3550743460655212e-06

Training epoch-79 batch-147
Running loss of epoch-79 batch-147 = 1.3830140233039856e-06

Training epoch-79 batch-148
Running loss of epoch-79 batch-148 = 1.4600809663534164e-06

Training epoch-79 batch-149
Running loss of epoch-79 batch-149 = 1.991633325815201e-06

Training epoch-79 batch-150
Running loss of epoch-79 batch-150 = 1.5203841030597687e-06

Training epoch-79 batch-151
Running loss of epoch-79 batch-151 = 1.1557713150978088e-06

Training epoch-79 batch-152
Running loss of epoch-79 batch-152 = 1.2891832739114761e-06

Training epoch-79 batch-153
Running loss of epoch-79 batch-153 = 1.1022202670574188e-06

Training epoch-79 batch-154
Running loss of epoch-79 batch-154 = 1.671724021434784e-06

Training epoch-79 batch-155
Running loss of epoch-79 batch-155 = 1.387670636177063e-06

Training epoch-79 batch-156
Running loss of epoch-79 batch-156 = 1.5900004655122757e-06

Training epoch-79 batch-157
Running loss of epoch-79 batch-157 = 1.0911375284194946e-05

Finished training epoch-79.



Average train loss at epoch-79 = 1.5378519892692567e-06

Started Evaluation

Average val loss at epoch-79 = 3.263288891629169

Accuracy for classes:
Accuracy for class equals is: 75.25 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.53 %

Finished Evaluation



Started training epoch-80


Training epoch-80 batch-1
Running loss of epoch-80 batch-1 = 2.312939614057541e-06

Training epoch-80 batch-2
Running loss of epoch-80 batch-2 = 8.873175829648972e-07

Training epoch-80 batch-3
Running loss of epoch-80 batch-3 = 1.616077497601509e-06

Training epoch-80 batch-4
Running loss of epoch-80 batch-4 = 1.664506271481514e-06

Training epoch-80 batch-5
Running loss of epoch-80 batch-5 = 1.8605496734380722e-06

Training epoch-80 batch-6
Running loss of epoch-80 batch-6 = 1.7706770449876785e-06

Training epoch-80 batch-7
Running loss of epoch-80 batch-7 = 8.095521479845047e-07

Training epoch-80 batch-8
Running loss of epoch-80 batch-8 = 1.1881347745656967e-06

Training epoch-80 batch-9
Running loss of epoch-80 batch-9 = 1.2228265404701233e-06

Training epoch-80 batch-10
Running loss of epoch-80 batch-10 = 1.862645149230957e-06

Training epoch-80 batch-11
Running loss of epoch-80 batch-11 = 1.6584526747465134e-06

Training epoch-80 batch-12
Running loss of epoch-80 batch-12 = 1.4365650713443756e-06

Training epoch-80 batch-13
Running loss of epoch-80 batch-13 = 1.3727694749832153e-06

Training epoch-80 batch-14
Running loss of epoch-80 batch-14 = 1.6656704246997833e-06

Training epoch-80 batch-15
Running loss of epoch-80 batch-15 = 1.8640421330928802e-06

Training epoch-80 batch-16
Running loss of epoch-80 batch-16 = 2.382323145866394e-06

Training epoch-80 batch-17
Running loss of epoch-80 batch-17 = 1.28219835460186e-06

Training epoch-80 batch-18
Running loss of epoch-80 batch-18 = 1.7392449080944061e-06

Training epoch-80 batch-19
Running loss of epoch-80 batch-19 = 1.8279533833265305e-06

Training epoch-80 batch-20
Running loss of epoch-80 batch-20 = 1.5434343367815018e-06

Training epoch-80 batch-21
Running loss of epoch-80 batch-21 = 7.71600753068924e-07

Training epoch-80 batch-22
Running loss of epoch-80 batch-22 = 1.141102984547615e-06

Training epoch-80 batch-23
Running loss of epoch-80 batch-23 = 1.4009419828653336e-06

Training epoch-80 batch-24
Running loss of epoch-80 batch-24 = 1.451931893825531e-06

Training epoch-80 batch-25
Running loss of epoch-80 batch-25 = 1.1243391782045364e-06

Training epoch-80 batch-26
Running loss of epoch-80 batch-26 = 1.2880191206932068e-06

Training epoch-80 batch-27
Running loss of epoch-80 batch-27 = 1.664506271481514e-06

Training epoch-80 batch-28
Running loss of epoch-80 batch-28 = 1.383945345878601e-06

Training epoch-80 batch-29
Running loss of epoch-80 batch-29 = 1.3138633221387863e-06

Training epoch-80 batch-30
Running loss of epoch-80 batch-30 = 1.1674128472805023e-06

Training epoch-80 batch-31
Running loss of epoch-80 batch-31 = 1.1262018233537674e-06

Training epoch-80 batch-32
Running loss of epoch-80 batch-32 = 1.4507677406072617e-06

Training epoch-80 batch-33
Running loss of epoch-80 batch-33 = 1.5401747077703476e-06

Training epoch-80 batch-34
Running loss of epoch-80 batch-34 = 1.5834812074899673e-06

Training epoch-80 batch-35
Running loss of epoch-80 batch-35 = 1.2833625078201294e-06

Training epoch-80 batch-36
Running loss of epoch-80 batch-36 = 1.7103739082813263e-06

Training epoch-80 batch-37
Running loss of epoch-80 batch-37 = 1.8356367945671082e-06

Training epoch-80 batch-38
Running loss of epoch-80 batch-38 = 1.555541530251503e-06

Training epoch-80 batch-39
Running loss of epoch-80 batch-39 = 1.150183379650116e-06

Training epoch-80 batch-40
Running loss of epoch-80 batch-40 = 1.735752448439598e-06

Training epoch-80 batch-41
Running loss of epoch-80 batch-41 = 1.6978010535240173e-06

Training epoch-80 batch-42
Running loss of epoch-80 batch-42 = 9.55536961555481e-07

Training epoch-80 batch-43
Running loss of epoch-80 batch-43 = 1.6114208847284317e-06

Training epoch-80 batch-44
Running loss of epoch-80 batch-44 = 1.2156087905168533e-06

Training epoch-80 batch-45
Running loss of epoch-80 batch-45 = 1.2521632015705109e-06

Training epoch-80 batch-46
Running loss of epoch-80 batch-46 = 1.2326054275035858e-06

Training epoch-80 batch-47
Running loss of epoch-80 batch-47 = 9.827781468629837e-07

Training epoch-80 batch-48
Running loss of epoch-80 batch-48 = 1.3990793377161026e-06

Training epoch-80 batch-49
Running loss of epoch-80 batch-49 = 1.0961666703224182e-06

Training epoch-80 batch-50
Running loss of epoch-80 batch-50 = 1.5506520867347717e-06

Training epoch-80 batch-51
Running loss of epoch-80 batch-51 = 1.368112862110138e-06

Training epoch-80 batch-52
Running loss of epoch-80 batch-52 = 2.39652581512928e-06

Training epoch-80 batch-53
Running loss of epoch-80 batch-53 = 1.4384277164936066e-06

Training epoch-80 batch-54
Running loss of epoch-80 batch-54 = 1.2356322258710861e-06

Training epoch-80 batch-55
Running loss of epoch-80 batch-55 = 1.4319084584712982e-06

Training epoch-80 batch-56
Running loss of epoch-80 batch-56 = 9.099021553993225e-07

Training epoch-80 batch-57
Running loss of epoch-80 batch-57 = 2.091284841299057e-06

Training epoch-80 batch-58
Running loss of epoch-80 batch-58 = 1.2139789760112762e-06

Training epoch-80 batch-59
Running loss of epoch-80 batch-59 = 1.458916813135147e-06

Training epoch-80 batch-60
Running loss of epoch-80 batch-60 = 9.587965905666351e-07

Training epoch-80 batch-61
Running loss of epoch-80 batch-61 = 1.2009404599666595e-06

Training epoch-80 batch-62
Running loss of epoch-80 batch-62 = 9.427312761545181e-07

Training epoch-80 batch-63
Running loss of epoch-80 batch-63 = 1.8384307622909546e-06

Training epoch-80 batch-64
Running loss of epoch-80 batch-64 = 1.2915115803480148e-06

Training epoch-80 batch-65
Running loss of epoch-80 batch-65 = 1.544831320643425e-06

Training epoch-80 batch-66
Running loss of epoch-80 batch-66 = 1.6260892152786255e-06

Training epoch-80 batch-67
Running loss of epoch-80 batch-67 = 1.4558900147676468e-06

Training epoch-80 batch-68
Running loss of epoch-80 batch-68 = 1.4835968613624573e-06

Training epoch-80 batch-69
Running loss of epoch-80 batch-69 = 1.369742676615715e-06

Training epoch-80 batch-70
Running loss of epoch-80 batch-70 = 9.450595825910568e-07

Training epoch-80 batch-71
Running loss of epoch-80 batch-71 = 3.0328519642353058e-06

Training epoch-80 batch-72
Running loss of epoch-80 batch-72 = 1.0409858077764511e-06

Training epoch-80 batch-73
Running loss of epoch-80 batch-73 = 1.5436671674251556e-06

Training epoch-80 batch-74
Running loss of epoch-80 batch-74 = 1.948326826095581e-06

Training epoch-80 batch-75
Running loss of epoch-80 batch-75 = 1.498265191912651e-06

Training epoch-80 batch-76
Running loss of epoch-80 batch-76 = 1.2172386050224304e-06

Training epoch-80 batch-77
Running loss of epoch-80 batch-77 = 1.9073486328125e-06

Training epoch-80 batch-78
Running loss of epoch-80 batch-78 = 1.2533273547887802e-06

Training epoch-80 batch-79
Running loss of epoch-80 batch-79 = 1.6437843441963196e-06

Training epoch-80 batch-80
Running loss of epoch-80 batch-80 = 1.3317912817001343e-06

Training epoch-80 batch-81
Running loss of epoch-80 batch-81 = 1.1450611054897308e-06

Training epoch-80 batch-82
Running loss of epoch-80 batch-82 = 1.4174729585647583e-06

Training epoch-80 batch-83
Running loss of epoch-80 batch-83 = 1.7976853996515274e-06

Training epoch-80 batch-84
Running loss of epoch-80 batch-84 = 1.246575266122818e-06

Training epoch-80 batch-85
Running loss of epoch-80 batch-85 = 1.5543773770332336e-06

Training epoch-80 batch-86
Running loss of epoch-80 batch-86 = 1.519685611128807e-06

Training epoch-80 batch-87
Running loss of epoch-80 batch-87 = 8.705537766218185e-07

Training epoch-80 batch-88
Running loss of epoch-80 batch-88 = 1.512700691819191e-06

Training epoch-80 batch-89
Running loss of epoch-80 batch-89 = 1.4542602002620697e-06

Training epoch-80 batch-90
Running loss of epoch-80 batch-90 = 1.5320256352424622e-06

Training epoch-80 batch-91
Running loss of epoch-80 batch-91 = 2.203509211540222e-06

Training epoch-80 batch-92
Running loss of epoch-80 batch-92 = 1.5064142644405365e-06

Training epoch-80 batch-93
Running loss of epoch-80 batch-93 = 1.2782402336597443e-06

Training epoch-80 batch-94
Running loss of epoch-80 batch-94 = 1.948326826095581e-06

Training epoch-80 batch-95
Running loss of epoch-80 batch-95 = 1.653796061873436e-06

Training epoch-80 batch-96
Running loss of epoch-80 batch-96 = 1.0977964848279953e-06

Training epoch-80 batch-97
Running loss of epoch-80 batch-97 = 1.2496020644903183e-06

Training epoch-80 batch-98
Running loss of epoch-80 batch-98 = 1.1851079761981964e-06

Training epoch-80 batch-99
Running loss of epoch-80 batch-99 = 2.034241333603859e-06

Training epoch-80 batch-100
Running loss of epoch-80 batch-100 = 1.0807998478412628e-06

Training epoch-80 batch-101
Running loss of epoch-80 batch-101 = 2.0584557205438614e-06

Training epoch-80 batch-102
Running loss of epoch-80 batch-102 = 2.1460000425577164e-06

Training epoch-80 batch-103
Running loss of epoch-80 batch-103 = 8.528586477041245e-07

Training epoch-80 batch-104
Running loss of epoch-80 batch-104 = 1.1119991540908813e-06

Training epoch-80 batch-105
Running loss of epoch-80 batch-105 = 1.4777760952711105e-06

Training epoch-80 batch-106
Running loss of epoch-80 batch-106 = 1.9264407455921173e-06

Training epoch-80 batch-107
Running loss of epoch-80 batch-107 = 1.4177057892084122e-06

Training epoch-80 batch-108
Running loss of epoch-80 batch-108 = 1.0617077350616455e-06

Training epoch-80 batch-109
Running loss of epoch-80 batch-109 = 1.5674158930778503e-06

Training epoch-80 batch-110
Running loss of epoch-80 batch-110 = 1.2603122740983963e-06

Training epoch-80 batch-111
Running loss of epoch-80 batch-111 = 1.0028015822172165e-06

Training epoch-80 batch-112
Running loss of epoch-80 batch-112 = 1.6703270375728607e-06

Training epoch-80 batch-113
Running loss of epoch-80 batch-113 = 1.946929842233658e-06

Training epoch-80 batch-114
Running loss of epoch-80 batch-114 = 1.1953525245189667e-06

Training epoch-80 batch-115
Running loss of epoch-80 batch-115 = 1.378590241074562e-06

Training epoch-80 batch-116
Running loss of epoch-80 batch-116 = 1.2838281691074371e-06

Training epoch-80 batch-117
Running loss of epoch-80 batch-117 = 2.5439076125621796e-06

Training epoch-80 batch-118
Running loss of epoch-80 batch-118 = 1.2177042663097382e-06

Training epoch-80 batch-119
Running loss of epoch-80 batch-119 = 1.7115380614995956e-06

Training epoch-80 batch-120
Running loss of epoch-80 batch-120 = 1.1082738637924194e-06

Training epoch-80 batch-121
Running loss of epoch-80 batch-121 = 1.2526288628578186e-06

Training epoch-80 batch-122
Running loss of epoch-80 batch-122 = 1.8482096493244171e-06

Training epoch-80 batch-123
Running loss of epoch-80 batch-123 = 1.616077497601509e-06

Training epoch-80 batch-124
Running loss of epoch-80 batch-124 = 1.405365765094757e-06

Training epoch-80 batch-125
Running loss of epoch-80 batch-125 = 1.2281816452741623e-06

Training epoch-80 batch-126
Running loss of epoch-80 batch-126 = 1.6174744814634323e-06

Training epoch-80 batch-127
Running loss of epoch-80 batch-127 = 1.491047441959381e-06

Training epoch-80 batch-128
Running loss of epoch-80 batch-128 = 2.005370333790779e-06

Training epoch-80 batch-129
Running loss of epoch-80 batch-129 = 1.6223639249801636e-06

Training epoch-80 batch-130
Running loss of epoch-80 batch-130 = 2.2388994693756104e-06

Training epoch-80 batch-131
Running loss of epoch-80 batch-131 = 1.4028046280145645e-06

Training epoch-80 batch-132
Running loss of epoch-80 batch-132 = 2.2640451788902283e-06

Training epoch-80 batch-133
Running loss of epoch-80 batch-133 = 2.1869782358407974e-06

Training epoch-80 batch-134
Running loss of epoch-80 batch-134 = 1.612817868590355e-06

Training epoch-80 batch-135
Running loss of epoch-80 batch-135 = 1.1096708476543427e-06

Training epoch-80 batch-136
Running loss of epoch-80 batch-136 = 2.2728927433490753e-06

Training epoch-80 batch-137
Running loss of epoch-80 batch-137 = 1.0407529771327972e-06

Training epoch-80 batch-138
Running loss of epoch-80 batch-138 = 1.7611309885978699e-06

Training epoch-80 batch-139
Running loss of epoch-80 batch-139 = 1.5634577721357346e-06

Training epoch-80 batch-140
Running loss of epoch-80 batch-140 = 1.2114178389310837e-06

Training epoch-80 batch-141
Running loss of epoch-80 batch-141 = 1.1648517102003098e-06

Training epoch-80 batch-142
Running loss of epoch-80 batch-142 = 8.856877684593201e-07

Training epoch-80 batch-143
Running loss of epoch-80 batch-143 = 1.5310943126678467e-06

Training epoch-80 batch-144
Running loss of epoch-80 batch-144 = 1.3799872249364853e-06

Training epoch-80 batch-145
Running loss of epoch-80 batch-145 = 1.5338882803916931e-06

Training epoch-80 batch-146
Running loss of epoch-80 batch-146 = 1.889420673251152e-06

Training epoch-80 batch-147
Running loss of epoch-80 batch-147 = 1.1832453310489655e-06

Training epoch-80 batch-148
Running loss of epoch-80 batch-148 = 1.0628718882799149e-06

Training epoch-80 batch-149
Running loss of epoch-80 batch-149 = 2.2295862436294556e-06

Training epoch-80 batch-150
Running loss of epoch-80 batch-150 = 7.392372936010361e-07

Training epoch-80 batch-151
Running loss of epoch-80 batch-151 = 1.9371509552001953e-06

Training epoch-80 batch-152
Running loss of epoch-80 batch-152 = 2.369983121752739e-06

Training epoch-80 batch-153
Running loss of epoch-80 batch-153 = 1.5259720385074615e-06

Training epoch-80 batch-154
Running loss of epoch-80 batch-154 = 1.7853453755378723e-06

Training epoch-80 batch-155
Running loss of epoch-80 batch-155 = 1.5709083527326584e-06

Training epoch-80 batch-156
Running loss of epoch-80 batch-156 = 1.6789417713880539e-06

Training epoch-80 batch-157
Running loss of epoch-80 batch-157 = 1.1429190635681152e-05

Finished training epoch-80.



Average train loss at epoch-80 = 1.5140220522880553e-06

Started Evaluation

Average val loss at epoch-80 = 3.2665516812550393

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-81


Training epoch-81 batch-1
Running loss of epoch-81 batch-1 = 1.2221280485391617e-06

Training epoch-81 batch-2
Running loss of epoch-81 batch-2 = 1.3492535799741745e-06

Training epoch-81 batch-3
Running loss of epoch-81 batch-3 = 1.4738179743289948e-06

Training epoch-81 batch-4
Running loss of epoch-81 batch-4 = 1.401873305439949e-06

Training epoch-81 batch-5
Running loss of epoch-81 batch-5 = 1.058913767337799e-06

Training epoch-81 batch-6
Running loss of epoch-81 batch-6 = 1.7567072063684464e-06

Training epoch-81 batch-7
Running loss of epoch-81 batch-7 = 1.8761493265628815e-06

Training epoch-81 batch-8
Running loss of epoch-81 batch-8 = 1.4747492969036102e-06

Training epoch-81 batch-9
Running loss of epoch-81 batch-9 = 1.643085852265358e-06

Training epoch-81 batch-10
Running loss of epoch-81 batch-10 = 1.498498022556305e-06

Training epoch-81 batch-11
Running loss of epoch-81 batch-11 = 1.4472752809524536e-06

Training epoch-81 batch-12
Running loss of epoch-81 batch-12 = 2.0265579223632812e-06

Training epoch-81 batch-13
Running loss of epoch-81 batch-13 = 1.3131648302078247e-06

Training epoch-81 batch-14
Running loss of epoch-81 batch-14 = 2.5045592337846756e-06

Training epoch-81 batch-15
Running loss of epoch-81 batch-15 = 1.3324897736310959e-06

Training epoch-81 batch-16
Running loss of epoch-81 batch-16 = 1.416075974702835e-06

Training epoch-81 batch-17
Running loss of epoch-81 batch-17 = 1.353677362203598e-06

Training epoch-81 batch-18
Running loss of epoch-81 batch-18 = 1.4249235391616821e-06

Training epoch-81 batch-19
Running loss of epoch-81 batch-19 = 1.3865064829587936e-06

Training epoch-81 batch-20
Running loss of epoch-81 batch-20 = 1.7462298274040222e-06

Training epoch-81 batch-21
Running loss of epoch-81 batch-21 = 1.352047547698021e-06

Training epoch-81 batch-22
Running loss of epoch-81 batch-22 = 1.5925616025924683e-06

Training epoch-81 batch-23
Running loss of epoch-81 batch-23 = 1.1334195733070374e-06

Training epoch-81 batch-24
Running loss of epoch-81 batch-24 = 1.0700896382331848e-06

Training epoch-81 batch-25
Running loss of epoch-81 batch-25 = 1.694774255156517e-06

Training epoch-81 batch-26
Running loss of epoch-81 batch-26 = 2.159271389245987e-06

Training epoch-81 batch-27
Running loss of epoch-81 batch-27 = 4.39351424574852e-07

Training epoch-81 batch-28
Running loss of epoch-81 batch-28 = 1.5024561434984207e-06

Training epoch-81 batch-29
Running loss of epoch-81 batch-29 = 1.7706770449876785e-06

Training epoch-81 batch-30
Running loss of epoch-81 batch-30 = 1.9720755517482758e-06

Training epoch-81 batch-31
Running loss of epoch-81 batch-31 = 1.44285149872303e-06

Training epoch-81 batch-32
Running loss of epoch-81 batch-32 = 1.4177057892084122e-06

Training epoch-81 batch-33
Running loss of epoch-81 batch-33 = 1.6654375940561295e-06

Training epoch-81 batch-34
Running loss of epoch-81 batch-34 = 1.6223639249801636e-06

Training epoch-81 batch-35
Running loss of epoch-81 batch-35 = 1.6901176422834396e-06

Training epoch-81 batch-36
Running loss of epoch-81 batch-36 = 1.7618294805288315e-06

Training epoch-81 batch-37
Running loss of epoch-81 batch-37 = 9.10833477973938e-07

Training epoch-81 batch-38
Running loss of epoch-81 batch-38 = 1.930398866534233e-06

Training epoch-81 batch-39
Running loss of epoch-81 batch-39 = 1.9366852939128876e-06

Training epoch-81 batch-40
Running loss of epoch-81 batch-40 = 1.694774255156517e-06

Training epoch-81 batch-41
Running loss of epoch-81 batch-41 = 1.7178244888782501e-06

Training epoch-81 batch-42
Running loss of epoch-81 batch-42 = 9.147915989160538e-07

Training epoch-81 batch-43
Running loss of epoch-81 batch-43 = 1.200009137392044e-06

Training epoch-81 batch-44
Running loss of epoch-81 batch-44 = 1.459149643778801e-06

Training epoch-81 batch-45
Running loss of epoch-81 batch-45 = 1.041218638420105e-06

Training epoch-81 batch-46
Running loss of epoch-81 batch-46 = 1.0356307029724121e-06

Training epoch-81 batch-47
Running loss of epoch-81 batch-47 = 1.226784661412239e-06

Training epoch-81 batch-48
Running loss of epoch-81 batch-48 = 1.47172249853611e-06

Training epoch-81 batch-49
Running loss of epoch-81 batch-49 = 1.5192199498414993e-06

Training epoch-81 batch-50
Running loss of epoch-81 batch-50 = 1.1366792023181915e-06

Training epoch-81 batch-51
Running loss of epoch-81 batch-51 = 1.269625499844551e-06

Training epoch-81 batch-52
Running loss of epoch-81 batch-52 = 1.3704411685466766e-06

Training epoch-81 batch-53
Running loss of epoch-81 batch-53 = 1.3282988220453262e-06

Training epoch-81 batch-54
Running loss of epoch-81 batch-54 = 1.4763791114091873e-06

Training epoch-81 batch-55
Running loss of epoch-81 batch-55 = 1.1618249118328094e-06

Training epoch-81 batch-56
Running loss of epoch-81 batch-56 = 1.416075974702835e-06

Training epoch-81 batch-57
Running loss of epoch-81 batch-57 = 2.1173618733882904e-06

Training epoch-81 batch-58
Running loss of epoch-81 batch-58 = 1.6246922314167023e-06

Training epoch-81 batch-59
Running loss of epoch-81 batch-59 = 1.2756790965795517e-06

Training epoch-81 batch-60
Running loss of epoch-81 batch-60 = 1.4456454664468765e-06

Training epoch-81 batch-61
Running loss of epoch-81 batch-61 = 1.56438909471035e-06

Training epoch-81 batch-62
Running loss of epoch-81 batch-62 = 1.0926742106676102e-06

Training epoch-81 batch-63
Running loss of epoch-81 batch-63 = 1.6493722796440125e-06

Training epoch-81 batch-64
Running loss of epoch-81 batch-64 = 1.0705552995204926e-06

Training epoch-81 batch-65
Running loss of epoch-81 batch-65 = 1.0973308235406876e-06

Training epoch-81 batch-66
Running loss of epoch-81 batch-66 = 1.080334186553955e-06

Training epoch-81 batch-67
Running loss of epoch-81 batch-67 = 1.591397449374199e-06

Training epoch-81 batch-68
Running loss of epoch-81 batch-68 = 7.445923984050751e-07

Training epoch-81 batch-69
Running loss of epoch-81 batch-69 = 1.5273690223693848e-06

Training epoch-81 batch-70
Running loss of epoch-81 batch-70 = 2.0507723093032837e-06

Training epoch-81 batch-71
Running loss of epoch-81 batch-71 = 1.2991949915885925e-06

Training epoch-81 batch-72
Running loss of epoch-81 batch-72 = 2.2440217435359955e-06

Training epoch-81 batch-73
Running loss of epoch-81 batch-73 = 1.0640360414981842e-06

Training epoch-81 batch-74
Running loss of epoch-81 batch-74 = 2.105953171849251e-06

Training epoch-81 batch-75
Running loss of epoch-81 batch-75 = 1.8749851733446121e-06

Training epoch-81 batch-76
Running loss of epoch-81 batch-76 = 1.644715666770935e-06

Training epoch-81 batch-77
Running loss of epoch-81 batch-77 = 9.103678166866302e-07

Training epoch-81 batch-78
Running loss of epoch-81 batch-78 = 1.7255079001188278e-06

Training epoch-81 batch-79
Running loss of epoch-81 batch-79 = 1.3576354831457138e-06

Training epoch-81 batch-80
Running loss of epoch-81 batch-80 = 1.5641562640666962e-06

Training epoch-81 batch-81
Running loss of epoch-81 batch-81 = 1.7820857465267181e-06

Training epoch-81 batch-82
Running loss of epoch-81 batch-82 = 1.9918661564588547e-06

Training epoch-81 batch-83
Running loss of epoch-81 batch-83 = 1.2014061212539673e-06

Training epoch-81 batch-84
Running loss of epoch-81 batch-84 = 1.200009137392044e-06

Training epoch-81 batch-85
Running loss of epoch-81 batch-85 = 1.4379620552062988e-06

Training epoch-81 batch-86
Running loss of epoch-81 batch-86 = 1.1601950973272324e-06

Training epoch-81 batch-87
Running loss of epoch-81 batch-87 = 1.6291160136461258e-06

Training epoch-81 batch-88
Running loss of epoch-81 batch-88 = 1.466134563088417e-06

Training epoch-81 batch-89
Running loss of epoch-81 batch-89 = 1.7140991985797882e-06

Training epoch-81 batch-90
Running loss of epoch-81 batch-90 = 1.3730023056268692e-06

Training epoch-81 batch-91
Running loss of epoch-81 batch-91 = 1.964159309864044e-06

Training epoch-81 batch-92
Running loss of epoch-81 batch-92 = 1.4719553291797638e-06

Training epoch-81 batch-93
Running loss of epoch-81 batch-93 = 1.209089532494545e-06

Training epoch-81 batch-94
Running loss of epoch-81 batch-94 = 8.279457688331604e-07

Training epoch-81 batch-95
Running loss of epoch-81 batch-95 = 1.521548256278038e-06

Training epoch-81 batch-96
Running loss of epoch-81 batch-96 = 1.107342541217804e-06

Training epoch-81 batch-97
Running loss of epoch-81 batch-97 = 1.6097910702228546e-06

Training epoch-81 batch-98
Running loss of epoch-81 batch-98 = 1.0416842997074127e-06

Training epoch-81 batch-99
Running loss of epoch-81 batch-99 = 1.6486737877130508e-06

Training epoch-81 batch-100
Running loss of epoch-81 batch-100 = 1.1944212019443512e-06

Training epoch-81 batch-101
Running loss of epoch-81 batch-101 = 1.11432746052742e-06

Training epoch-81 batch-102
Running loss of epoch-81 batch-102 = 1.6195699572563171e-06

Training epoch-81 batch-103
Running loss of epoch-81 batch-103 = 1.810723915696144e-06

Training epoch-81 batch-104
Running loss of epoch-81 batch-104 = 1.500127837061882e-06

Training epoch-81 batch-105
Running loss of epoch-81 batch-105 = 1.5010591596364975e-06

Training epoch-81 batch-106
Running loss of epoch-81 batch-106 = 1.1813826858997345e-06

Training epoch-81 batch-107
Running loss of epoch-81 batch-107 = 9.620562195777893e-07

Training epoch-81 batch-108
Running loss of epoch-81 batch-108 = 1.4852266758680344e-06

Training epoch-81 batch-109
Running loss of epoch-81 batch-109 = 2.017710357904434e-06

Training epoch-81 batch-110
Running loss of epoch-81 batch-110 = 9.94885340332985e-07

Training epoch-81 batch-111
Running loss of epoch-81 batch-111 = 2.084067091345787e-06

Training epoch-81 batch-112
Running loss of epoch-81 batch-112 = 1.5650875866413116e-06

Training epoch-81 batch-113
Running loss of epoch-81 batch-113 = 1.4938414096832275e-06

Training epoch-81 batch-114
Running loss of epoch-81 batch-114 = 1.6295816749334335e-06

Training epoch-81 batch-115
Running loss of epoch-81 batch-115 = 1.7026904970407486e-06

Training epoch-81 batch-116
Running loss of epoch-81 batch-116 = 1.080334186553955e-06

Training epoch-81 batch-117
Running loss of epoch-81 batch-117 = 1.4689285308122635e-06

Training epoch-81 batch-118
Running loss of epoch-81 batch-118 = 1.7781276255846024e-06

Training epoch-81 batch-119
Running loss of epoch-81 batch-119 = 1.1583324521780014e-06

Training epoch-81 batch-120
Running loss of epoch-81 batch-120 = 1.7755664885044098e-06

Training epoch-81 batch-121
Running loss of epoch-81 batch-121 = 2.1837186068296432e-06

Training epoch-81 batch-122
Running loss of epoch-81 batch-122 = 8.665956556797028e-07

Training epoch-81 batch-123
Running loss of epoch-81 batch-123 = 1.1511147022247314e-06

Training epoch-81 batch-124
Running loss of epoch-81 batch-124 = 1.666368916630745e-06

Training epoch-81 batch-125
Running loss of epoch-81 batch-125 = 9.837094694375992e-07

Training epoch-81 batch-126
Running loss of epoch-81 batch-126 = 1.3497192412614822e-06

Training epoch-81 batch-127
Running loss of epoch-81 batch-127 = 1.6596168279647827e-06

Training epoch-81 batch-128
Running loss of epoch-81 batch-128 = 2.025160938501358e-06

Training epoch-81 batch-129
Running loss of epoch-81 batch-129 = 1.2612435966730118e-06

Training epoch-81 batch-130
Running loss of epoch-81 batch-130 = 8.698552846908569e-07

Training epoch-81 batch-131
Running loss of epoch-81 batch-131 = 1.9634608179330826e-06

Training epoch-81 batch-132
Running loss of epoch-81 batch-132 = 1.1869706213474274e-06

Training epoch-81 batch-133
Running loss of epoch-81 batch-133 = 1.994892954826355e-06

Training epoch-81 batch-134
Running loss of epoch-81 batch-134 = 1.7615966498851776e-06

Training epoch-81 batch-135
Running loss of epoch-81 batch-135 = 1.2570526450872421e-06

Training epoch-81 batch-136
Running loss of epoch-81 batch-136 = 1.2477394193410873e-06

Training epoch-81 batch-137
Running loss of epoch-81 batch-137 = 1.8083956092596054e-06

Training epoch-81 batch-138
Running loss of epoch-81 batch-138 = 1.6773119568824768e-06

Training epoch-81 batch-139
Running loss of epoch-81 batch-139 = 1.3904646039009094e-06

Training epoch-81 batch-140
Running loss of epoch-81 batch-140 = 1.8426217138767242e-06

Training epoch-81 batch-141
Running loss of epoch-81 batch-141 = 9.55536961555481e-07

Training epoch-81 batch-142
Running loss of epoch-81 batch-142 = 1.9101426005363464e-06

Training epoch-81 batch-143
Running loss of epoch-81 batch-143 = 1.3227108865976334e-06

Training epoch-81 batch-144
Running loss of epoch-81 batch-144 = 2.035871148109436e-06

Training epoch-81 batch-145
Running loss of epoch-81 batch-145 = 2.4745240807533264e-06

Training epoch-81 batch-146
Running loss of epoch-81 batch-146 = 1.5853438526391983e-06

Training epoch-81 batch-147
Running loss of epoch-81 batch-147 = 1.3951212167739868e-06

Training epoch-81 batch-148
Running loss of epoch-81 batch-148 = 1.4246907085180283e-06

Training epoch-81 batch-149
Running loss of epoch-81 batch-149 = 1.1539086699485779e-06

Training epoch-81 batch-150
Running loss of epoch-81 batch-150 = 1.2416858226060867e-06

Training epoch-81 batch-151
Running loss of epoch-81 batch-151 = 1.0882504284381866e-06

Training epoch-81 batch-152
Running loss of epoch-81 batch-152 = 1.974869519472122e-06

Training epoch-81 batch-153
Running loss of epoch-81 batch-153 = 9.038485586643219e-07

Training epoch-81 batch-154
Running loss of epoch-81 batch-154 = 9.83942300081253e-07

Training epoch-81 batch-155
Running loss of epoch-81 batch-155 = 1.500127837061882e-06

Training epoch-81 batch-156
Running loss of epoch-81 batch-156 = 1.596519723534584e-06

Training epoch-81 batch-157
Running loss of epoch-81 batch-157 = 6.563961505889893e-06

Finished training epoch-81.



Average train loss at epoch-81 = 1.4831781387329102e-06

Started Evaluation

Average val loss at epoch-81 = 3.2710947723765122

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.45 %

Finished Evaluation



Started training epoch-82


Training epoch-82 batch-1
Running loss of epoch-82 batch-1 = 1.4391262084245682e-06

Training epoch-82 batch-2
Running loss of epoch-82 batch-2 = 1.7834827303886414e-06

Training epoch-82 batch-3
Running loss of epoch-82 batch-3 = 1.1264346539974213e-06

Training epoch-82 batch-4
Running loss of epoch-82 batch-4 = 1.2391246855258942e-06

Training epoch-82 batch-5
Running loss of epoch-82 batch-5 = 1.0740477591753006e-06

Training epoch-82 batch-6
Running loss of epoch-82 batch-6 = 1.321546733379364e-06

Training epoch-82 batch-7
Running loss of epoch-82 batch-7 = 1.4426186680793762e-06

Training epoch-82 batch-8
Running loss of epoch-82 batch-8 = 1.1317897588014603e-06

Training epoch-82 batch-9
Running loss of epoch-82 batch-9 = 1.9762665033340454e-06

Training epoch-82 batch-10
Running loss of epoch-82 batch-10 = 2.146931365132332e-06

Training epoch-82 batch-11
Running loss of epoch-82 batch-11 = 1.946929842233658e-06

Training epoch-82 batch-12
Running loss of epoch-82 batch-12 = 1.8174760043621063e-06

Training epoch-82 batch-13
Running loss of epoch-82 batch-13 = 1.2756790965795517e-06

Training epoch-82 batch-14
Running loss of epoch-82 batch-14 = 1.2496020644903183e-06

Training epoch-82 batch-15
Running loss of epoch-82 batch-15 = 1.353677362203598e-06

Training epoch-82 batch-16
Running loss of epoch-82 batch-16 = 1.0507646948099136e-06

Training epoch-82 batch-17
Running loss of epoch-82 batch-17 = 1.8333084881305695e-06

Training epoch-82 batch-18
Running loss of epoch-82 batch-18 = 1.7387792468070984e-06

Training epoch-82 batch-19
Running loss of epoch-82 batch-19 = 1.423526555299759e-06

Training epoch-82 batch-20
Running loss of epoch-82 batch-20 = 1.5010591596364975e-06

Training epoch-82 batch-21
Running loss of epoch-82 batch-21 = 1.5355180948972702e-06

Training epoch-82 batch-22
Running loss of epoch-82 batch-22 = 1.5459954738616943e-06

Training epoch-82 batch-23
Running loss of epoch-82 batch-23 = 1.6761478036642075e-06

Training epoch-82 batch-24
Running loss of epoch-82 batch-24 = 1.0097865015268326e-06

Training epoch-82 batch-25
Running loss of epoch-82 batch-25 = 2.016546204686165e-06

Training epoch-82 batch-26
Running loss of epoch-82 batch-26 = 1.0181684046983719e-06

Training epoch-82 batch-27
Running loss of epoch-82 batch-27 = 1.4670658856630325e-06

Training epoch-82 batch-28
Running loss of epoch-82 batch-28 = 1.0670628398656845e-06

Training epoch-82 batch-29
Running loss of epoch-82 batch-29 = 1.228880137205124e-06

Training epoch-82 batch-30
Running loss of epoch-82 batch-30 = 1.898268237709999e-06

Training epoch-82 batch-31
Running loss of epoch-82 batch-31 = 1.4724209904670715e-06

Training epoch-82 batch-32
Running loss of epoch-82 batch-32 = 1.580454409122467e-06

Training epoch-82 batch-33
Running loss of epoch-82 batch-33 = 1.962995156645775e-06

Training epoch-82 batch-34
Running loss of epoch-82 batch-34 = 1.90013088285923e-06

Training epoch-82 batch-35
Running loss of epoch-82 batch-35 = 1.093139871954918e-06

Training epoch-82 batch-36
Running loss of epoch-82 batch-36 = 1.1182855814695358e-06

Training epoch-82 batch-37
Running loss of epoch-82 batch-37 = 9.925570338964462e-07

Training epoch-82 batch-38
Running loss of epoch-82 batch-38 = 1.244712620973587e-06

Training epoch-82 batch-39
Running loss of epoch-82 batch-39 = 1.3718381524085999e-06

Training epoch-82 batch-40
Running loss of epoch-82 batch-40 = 1.4274846762418747e-06

Training epoch-82 batch-41
Running loss of epoch-82 batch-41 = 1.598382368683815e-06

Training epoch-82 batch-42
Running loss of epoch-82 batch-42 = 9.227078408002853e-07

Training epoch-82 batch-43
Running loss of epoch-82 batch-43 = 1.4854595065116882e-06

Training epoch-82 batch-44
Running loss of epoch-82 batch-44 = 1.3960525393486023e-06

Training epoch-82 batch-45
Running loss of epoch-82 batch-45 = 1.3166572898626328e-06

Training epoch-82 batch-46
Running loss of epoch-82 batch-46 = 1.4794059097766876e-06

Training epoch-82 batch-47
Running loss of epoch-82 batch-47 = 1.3727694749832153e-06

Training epoch-82 batch-48
Running loss of epoch-82 batch-48 = 1.2966338545084e-06

Training epoch-82 batch-49
Running loss of epoch-82 batch-49 = 1.3289973139762878e-06

Training epoch-82 batch-50
Running loss of epoch-82 batch-50 = 1.0239891707897186e-06

Training epoch-82 batch-51
Running loss of epoch-82 batch-51 = 2.0207371562719345e-06

Training epoch-82 batch-52
Running loss of epoch-82 batch-52 = 2.2973399609327316e-06

Training epoch-82 batch-53
Running loss of epoch-82 batch-53 = 2.032611519098282e-06

Training epoch-82 batch-54
Running loss of epoch-82 batch-54 = 1.184176653623581e-06

Training epoch-82 batch-55
Running loss of epoch-82 batch-55 = 1.4575198292732239e-06

Training epoch-82 batch-56
Running loss of epoch-82 batch-56 = 1.3103708624839783e-06

Training epoch-82 batch-57
Running loss of epoch-82 batch-57 = 1.8058344721794128e-06

Training epoch-82 batch-58
Running loss of epoch-82 batch-58 = 1.0246876627206802e-06

Training epoch-82 batch-59
Running loss of epoch-82 batch-59 = 1.3879034668207169e-06

Training epoch-82 batch-60
Running loss of epoch-82 batch-60 = 1.285690814256668e-06

Training epoch-82 batch-61
Running loss of epoch-82 batch-61 = 1.2156087905168533e-06

Training epoch-82 batch-62
Running loss of epoch-82 batch-62 = 1.3676472008228302e-06

Training epoch-82 batch-63
Running loss of epoch-82 batch-63 = 1.1615920811891556e-06

Training epoch-82 batch-64
Running loss of epoch-82 batch-64 = 2.132495865225792e-06

Training epoch-82 batch-65
Running loss of epoch-82 batch-65 = 1.825159415602684e-06

Training epoch-82 batch-66
Running loss of epoch-82 batch-66 = 1.541338860988617e-06

Training epoch-82 batch-67
Running loss of epoch-82 batch-67 = 2.1478626877069473e-06

Training epoch-82 batch-68
Running loss of epoch-82 batch-68 = 1.207226887345314e-06

Training epoch-82 batch-69
Running loss of epoch-82 batch-69 = 1.4600809663534164e-06

Training epoch-82 batch-70
Running loss of epoch-82 batch-70 = 1.2223608791828156e-06

Training epoch-82 batch-71
Running loss of epoch-82 batch-71 = 1.1310912668704987e-06

Training epoch-82 batch-72
Running loss of epoch-82 batch-72 = 1.4153774827718735e-06

Training epoch-82 batch-73
Running loss of epoch-82 batch-73 = 1.1599622666835785e-06

Training epoch-82 batch-74
Running loss of epoch-82 batch-74 = 1.1282972991466522e-06

Training epoch-82 batch-75
Running loss of epoch-82 batch-75 = 1.6521662473678589e-06

Training epoch-82 batch-76
Running loss of epoch-82 batch-76 = 1.4577526599168777e-06

Training epoch-82 batch-77
Running loss of epoch-82 batch-77 = 1.3567041605710983e-06

Training epoch-82 batch-78
Running loss of epoch-82 batch-78 = 9.008217602968216e-07

Training epoch-82 batch-79
Running loss of epoch-82 batch-79 = 1.2551899999380112e-06

Training epoch-82 batch-80
Running loss of epoch-82 batch-80 = 9.576324373483658e-07

Training epoch-82 batch-81
Running loss of epoch-82 batch-81 = 1.2337695807218552e-06

Training epoch-82 batch-82
Running loss of epoch-82 batch-82 = 9.890645742416382e-07

Training epoch-82 batch-83
Running loss of epoch-82 batch-83 = 1.2363307178020477e-06

Training epoch-82 batch-84
Running loss of epoch-82 batch-84 = 1.6475096344947815e-06

Training epoch-82 batch-85
Running loss of epoch-82 batch-85 = 1.725275069475174e-06

Training epoch-82 batch-86
Running loss of epoch-82 batch-86 = 1.5078112483024597e-06

Training epoch-82 batch-87
Running loss of epoch-82 batch-87 = 1.8083956092596054e-06

Training epoch-82 batch-88
Running loss of epoch-82 batch-88 = 1.194654032588005e-06

Training epoch-82 batch-89
Running loss of epoch-82 batch-89 = 1.3825483620166779e-06

Training epoch-82 batch-90
Running loss of epoch-82 batch-90 = 2.291286364197731e-06

Training epoch-82 batch-91
Running loss of epoch-82 batch-91 = 2.0319130271673203e-06

Training epoch-82 batch-92
Running loss of epoch-82 batch-92 = 8.852221071720123e-07

Training epoch-82 batch-93
Running loss of epoch-82 batch-93 = 1.3445969671010971e-06

Training epoch-82 batch-94
Running loss of epoch-82 batch-94 = 1.2852251529693604e-06

Training epoch-82 batch-95
Running loss of epoch-82 batch-95 = 1.3872049748897552e-06

Training epoch-82 batch-96
Running loss of epoch-82 batch-96 = 1.305481418967247e-06

Training epoch-82 batch-97
Running loss of epoch-82 batch-97 = 1.6116537153720856e-06

Training epoch-82 batch-98
Running loss of epoch-82 batch-98 = 1.3557728379964828e-06

Training epoch-82 batch-99
Running loss of epoch-82 batch-99 = 1.5515834093093872e-06

Training epoch-82 batch-100
Running loss of epoch-82 batch-100 = 1.45728699862957e-06

Training epoch-82 batch-101
Running loss of epoch-82 batch-101 = 1.3075768947601318e-06

Training epoch-82 batch-102
Running loss of epoch-82 batch-102 = 1.9746366888284683e-06

Training epoch-82 batch-103
Running loss of epoch-82 batch-103 = 1.5764962881803513e-06

Training epoch-82 batch-104
Running loss of epoch-82 batch-104 = 1.6919802874326706e-06

Training epoch-82 batch-105
Running loss of epoch-82 batch-105 = 1.814449205994606e-06

Training epoch-82 batch-106
Running loss of epoch-82 batch-106 = 1.6817357391119003e-06

Training epoch-82 batch-107
Running loss of epoch-82 batch-107 = 1.1224765330553055e-06

Training epoch-82 batch-108
Running loss of epoch-82 batch-108 = 1.2544915080070496e-06

Training epoch-82 batch-109
Running loss of epoch-82 batch-109 = 1.36462040245533e-06

Training epoch-82 batch-110
Running loss of epoch-82 batch-110 = 1.3997778296470642e-06

Training epoch-82 batch-111
Running loss of epoch-82 batch-111 = 1.544831320643425e-06

Training epoch-82 batch-112
Running loss of epoch-82 batch-112 = 1.166015863418579e-06

Training epoch-82 batch-113
Running loss of epoch-82 batch-113 = 1.4989636838436127e-06

Training epoch-82 batch-114
Running loss of epoch-82 batch-114 = 1.0782387107610703e-06

Training epoch-82 batch-115
Running loss of epoch-82 batch-115 = 1.319916918873787e-06

Training epoch-82 batch-116
Running loss of epoch-82 batch-116 = 1.4798715710639954e-06

Training epoch-82 batch-117
Running loss of epoch-82 batch-117 = 1.3781245797872543e-06

Training epoch-82 batch-118
Running loss of epoch-82 batch-118 = 1.0349322110414505e-06

Training epoch-82 batch-119
Running loss of epoch-82 batch-119 = 1.991633325815201e-06

Training epoch-82 batch-120
Running loss of epoch-82 batch-120 = 1.8405262380838394e-06

Training epoch-82 batch-121
Running loss of epoch-82 batch-121 = 1.6402918845415115e-06

Training epoch-82 batch-122
Running loss of epoch-82 batch-122 = 1.3867393136024475e-06

Training epoch-82 batch-123
Running loss of epoch-82 batch-123 = 2.0191073417663574e-06

Training epoch-82 batch-124
Running loss of epoch-82 batch-124 = 1.0798685252666473e-06

Training epoch-82 batch-125
Running loss of epoch-82 batch-125 = 1.1399388313293457e-06

Training epoch-82 batch-126
Running loss of epoch-82 batch-126 = 1.3192184269428253e-06

Training epoch-82 batch-127
Running loss of epoch-82 batch-127 = 1.1578667908906937e-06

Training epoch-82 batch-128
Running loss of epoch-82 batch-128 = 2.1527521312236786e-06

Training epoch-82 batch-129
Running loss of epoch-82 batch-129 = 1.098262146115303e-06

Training epoch-82 batch-130
Running loss of epoch-82 batch-130 = 9.66014340519905e-07

Training epoch-82 batch-131
Running loss of epoch-82 batch-131 = 1.7113052308559418e-06

Training epoch-82 batch-132
Running loss of epoch-82 batch-132 = 1.451931893825531e-06

Training epoch-82 batch-133
Running loss of epoch-82 batch-133 = 7.341150194406509e-07

Training epoch-82 batch-134
Running loss of epoch-82 batch-134 = 7.825437933206558e-07

Training epoch-82 batch-135
Running loss of epoch-82 batch-135 = 1.1166557669639587e-06

Training epoch-82 batch-136
Running loss of epoch-82 batch-136 = 1.153675839304924e-06

Training epoch-82 batch-137
Running loss of epoch-82 batch-137 = 1.287786290049553e-06

Training epoch-82 batch-138
Running loss of epoch-82 batch-138 = 1.9010622054338455e-06

Training epoch-82 batch-139
Running loss of epoch-82 batch-139 = 1.5776604413986206e-06

Training epoch-82 batch-140
Running loss of epoch-82 batch-140 = 1.091044396162033e-06

Training epoch-82 batch-141
Running loss of epoch-82 batch-141 = 1.7152633517980576e-06

Training epoch-82 batch-142
Running loss of epoch-82 batch-142 = 1.3620592653751373e-06

Training epoch-82 batch-143
Running loss of epoch-82 batch-143 = 1.610955223441124e-06

Training epoch-82 batch-144
Running loss of epoch-82 batch-144 = 1.6689300537109375e-06

Training epoch-82 batch-145
Running loss of epoch-82 batch-145 = 1.7471611499786377e-06

Training epoch-82 batch-146
Running loss of epoch-82 batch-146 = 1.877080649137497e-06

Training epoch-82 batch-147
Running loss of epoch-82 batch-147 = 1.0221265256404877e-06

Training epoch-82 batch-148
Running loss of epoch-82 batch-148 = 2.024928107857704e-06

Training epoch-82 batch-149
Running loss of epoch-82 batch-149 = 1.7066486179828644e-06

Training epoch-82 batch-150
Running loss of epoch-82 batch-150 = 2.1257437765598297e-06

Training epoch-82 batch-151
Running loss of epoch-82 batch-151 = 1.1078082025051117e-06

Training epoch-82 batch-152
Running loss of epoch-82 batch-152 = 1.2381933629512787e-06

Training epoch-82 batch-153
Running loss of epoch-82 batch-153 = 1.7543788999319077e-06

Training epoch-82 batch-154
Running loss of epoch-82 batch-154 = 1.7674174159765244e-06

Training epoch-82 batch-155
Running loss of epoch-82 batch-155 = 1.2731179594993591e-06

Training epoch-82 batch-156
Running loss of epoch-82 batch-156 = 1.4738179743289948e-06

Training epoch-82 batch-157
Running loss of epoch-82 batch-157 = 9.641051292419434e-06

Finished training epoch-82.



Average train loss at epoch-82 = 1.4632418751716614e-06

Started Evaluation

Average val loss at epoch-82 = 3.2737978885048316

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 35.64 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-83


Training epoch-83 batch-1
Running loss of epoch-83 batch-1 = 1.2924429029226303e-06

Training epoch-83 batch-2
Running loss of epoch-83 batch-2 = 1.5480909496545792e-06

Training epoch-83 batch-3
Running loss of epoch-83 batch-3 = 1.375330612063408e-06

Training epoch-83 batch-4
Running loss of epoch-83 batch-4 = 9.424984455108643e-07

Training epoch-83 batch-5
Running loss of epoch-83 batch-5 = 1.309206709265709e-06

Training epoch-83 batch-6
Running loss of epoch-83 batch-6 = 6.211921572685242e-07

Training epoch-83 batch-7
Running loss of epoch-83 batch-7 = 2.0298175513744354e-06

Training epoch-83 batch-8
Running loss of epoch-83 batch-8 = 1.3988465070724487e-06

Training epoch-83 batch-9
Running loss of epoch-83 batch-9 = 1.9220169633626938e-06

Training epoch-83 batch-10
Running loss of epoch-83 batch-10 = 1.0943040251731873e-06

Training epoch-83 batch-11
Running loss of epoch-83 batch-11 = 1.1704396456480026e-06

Training epoch-83 batch-12
Running loss of epoch-83 batch-12 = 1.5131663531064987e-06

Training epoch-83 batch-13
Running loss of epoch-83 batch-13 = 1.841224730014801e-06

Training epoch-83 batch-14
Running loss of epoch-83 batch-14 = 1.2454111129045486e-06

Training epoch-83 batch-15
Running loss of epoch-83 batch-15 = 1.1962838470935822e-06

Training epoch-83 batch-16
Running loss of epoch-83 batch-16 = 2.139247953891754e-06

Training epoch-83 batch-17
Running loss of epoch-83 batch-17 = 1.0686926543712616e-06

Training epoch-83 batch-18
Running loss of epoch-83 batch-18 = 1.2172386050224304e-06

Training epoch-83 batch-19
Running loss of epoch-83 batch-19 = 9.16188582777977e-07

Training epoch-83 batch-20
Running loss of epoch-83 batch-20 = 1.3008248060941696e-06

Training epoch-83 batch-21
Running loss of epoch-83 batch-21 = 1.8079299479722977e-06

Training epoch-83 batch-22
Running loss of epoch-83 batch-22 = 1.8172431737184525e-06

Training epoch-83 batch-23
Running loss of epoch-83 batch-23 = 1.964159309864044e-06

Training epoch-83 batch-24
Running loss of epoch-83 batch-24 = 1.4060642570257187e-06

Training epoch-83 batch-25
Running loss of epoch-83 batch-25 = 1.58604234457016e-06

Training epoch-83 batch-26
Running loss of epoch-83 batch-26 = 1.53249129652977e-06

Training epoch-83 batch-27
Running loss of epoch-83 batch-27 = 2.2633466869592667e-06

Training epoch-83 batch-28
Running loss of epoch-83 batch-28 = 1.260777935385704e-06

Training epoch-83 batch-29
Running loss of epoch-83 batch-29 = 1.248437911272049e-06

Training epoch-83 batch-30
Running loss of epoch-83 batch-30 = 1.2442469596862793e-06

Training epoch-83 batch-31
Running loss of epoch-83 batch-31 = 8.246861398220062e-07

Training epoch-83 batch-32
Running loss of epoch-83 batch-32 = 1.4184042811393738e-06

Training epoch-83 batch-33
Running loss of epoch-83 batch-33 = 1.1774245649576187e-06

Training epoch-83 batch-34
Running loss of epoch-83 batch-34 = 1.5404075384140015e-06

Training epoch-83 batch-35
Running loss of epoch-83 batch-35 = 1.12876296043396e-06

Training epoch-83 batch-36
Running loss of epoch-83 batch-36 = 9.879004210233688e-07

Training epoch-83 batch-37
Running loss of epoch-83 batch-37 = 1.5541445463895798e-06

Training epoch-83 batch-38
Running loss of epoch-83 batch-38 = 1.4284159988164902e-06

Training epoch-83 batch-39
Running loss of epoch-83 batch-39 = 1.3988465070724487e-06

Training epoch-83 batch-40
Running loss of epoch-83 batch-40 = 1.5906989574432373e-06

Training epoch-83 batch-41
Running loss of epoch-83 batch-41 = 1.4153774827718735e-06

Training epoch-83 batch-42
Running loss of epoch-83 batch-42 = 1.5594996511936188e-06

Training epoch-83 batch-43
Running loss of epoch-83 batch-43 = 1.366250216960907e-06

Training epoch-83 batch-44
Running loss of epoch-83 batch-44 = 9.119976311922073e-07

Training epoch-83 batch-45
Running loss of epoch-83 batch-45 = 1.9560102373361588e-06

Training epoch-83 batch-46
Running loss of epoch-83 batch-46 = 1.3294629752635956e-06

Training epoch-83 batch-47
Running loss of epoch-83 batch-47 = 1.3515818864107132e-06

Training epoch-83 batch-48
Running loss of epoch-83 batch-48 = 1.5296973288059235e-06

Training epoch-83 batch-49
Running loss of epoch-83 batch-49 = 1.5692785382270813e-06

Training epoch-83 batch-50
Running loss of epoch-83 batch-50 = 1.8400605767965317e-06

Training epoch-83 batch-51
Running loss of epoch-83 batch-51 = 1.6796402633190155e-06

Training epoch-83 batch-52
Running loss of epoch-83 batch-52 = 1.7352867871522903e-06

Training epoch-83 batch-53
Running loss of epoch-83 batch-53 = 1.794658601284027e-06

Training epoch-83 batch-54
Running loss of epoch-83 batch-54 = 1.6486737877130508e-06

Training epoch-83 batch-55
Running loss of epoch-83 batch-55 = 9.32253897190094e-07

Training epoch-83 batch-56
Running loss of epoch-83 batch-56 = 1.8298160284757614e-06

Training epoch-83 batch-57
Running loss of epoch-83 batch-57 = 1.0870862752199173e-06

Training epoch-83 batch-58
Running loss of epoch-83 batch-58 = 1.5441328287124634e-06

Training epoch-83 batch-59
Running loss of epoch-83 batch-59 = 9.427312761545181e-07

Training epoch-83 batch-60
Running loss of epoch-83 batch-60 = 1.3695098459720612e-06

Training epoch-83 batch-61
Running loss of epoch-83 batch-61 = 1.730630174279213e-06

Training epoch-83 batch-62
Running loss of epoch-83 batch-62 = 1.6614794731140137e-06

Training epoch-83 batch-63
Running loss of epoch-83 batch-63 = 1.1434312909841537e-06

Training epoch-83 batch-64
Running loss of epoch-83 batch-64 = 1.8670689314603806e-06

Training epoch-83 batch-65
Running loss of epoch-83 batch-65 = 1.5811529010534286e-06

Training epoch-83 batch-66
Running loss of epoch-83 batch-66 = 1.1003576219081879e-06

Training epoch-83 batch-67
Running loss of epoch-83 batch-67 = 9.317882359027863e-07

Training epoch-83 batch-68
Running loss of epoch-83 batch-68 = 1.5331897884607315e-06

Training epoch-83 batch-69
Running loss of epoch-83 batch-69 = 1.5648547559976578e-06

Training epoch-83 batch-70
Running loss of epoch-83 batch-70 = 1.400243490934372e-06

Training epoch-83 batch-71
Running loss of epoch-83 batch-71 = 8.57282429933548e-07

Training epoch-83 batch-72
Running loss of epoch-83 batch-72 = 9.699724614620209e-07

Training epoch-83 batch-73
Running loss of epoch-83 batch-73 = 1.6558915376663208e-06

Training epoch-83 batch-74
Running loss of epoch-83 batch-74 = 2.2149179130792618e-06

Training epoch-83 batch-75
Running loss of epoch-83 batch-75 = 1.5220139175653458e-06

Training epoch-83 batch-76
Running loss of epoch-83 batch-76 = 7.781200110912323e-07

Training epoch-83 batch-77
Running loss of epoch-83 batch-77 = 1.3113021850585938e-06

Training epoch-83 batch-78
Running loss of epoch-83 batch-78 = 7.925555109977722e-07

Training epoch-83 batch-79
Running loss of epoch-83 batch-79 = 1.2814998626708984e-06

Training epoch-83 batch-80
Running loss of epoch-83 batch-80 = 1.2731179594993591e-06

Training epoch-83 batch-81
Running loss of epoch-83 batch-81 = 1.6409903764724731e-06

Training epoch-83 batch-82
Running loss of epoch-83 batch-82 = 1.2207310646772385e-06

Training epoch-83 batch-83
Running loss of epoch-83 batch-83 = 1.7478596419095993e-06

Training epoch-83 batch-84
Running loss of epoch-83 batch-84 = 1.6659032553434372e-06

Training epoch-83 batch-85
Running loss of epoch-83 batch-85 = 1.8724240362644196e-06

Training epoch-83 batch-86
Running loss of epoch-83 batch-86 = 2.0551960915327072e-06

Training epoch-83 batch-87
Running loss of epoch-83 batch-87 = 1.7383135855197906e-06

Training epoch-83 batch-88
Running loss of epoch-83 batch-88 = 1.101754605770111e-06

Training epoch-83 batch-89
Running loss of epoch-83 batch-89 = 1.4849938452243805e-06

Training epoch-83 batch-90
Running loss of epoch-83 batch-90 = 1.7024576663970947e-06

Training epoch-83 batch-91
Running loss of epoch-83 batch-91 = 1.1119991540908813e-06

Training epoch-83 batch-92
Running loss of epoch-83 batch-92 = 1.61072239279747e-06

Training epoch-83 batch-93
Running loss of epoch-83 batch-93 = 1.085922122001648e-06

Training epoch-83 batch-94
Running loss of epoch-83 batch-94 = 1.7136335372924805e-06

Training epoch-83 batch-95
Running loss of epoch-83 batch-95 = 1.428648829460144e-06

Training epoch-83 batch-96
Running loss of epoch-83 batch-96 = 1.353677362203598e-06

Training epoch-83 batch-97
Running loss of epoch-83 batch-97 = 1.4763791114091873e-06

Training epoch-83 batch-98
Running loss of epoch-83 batch-98 = 1.5723053365945816e-06

Training epoch-83 batch-99
Running loss of epoch-83 batch-99 = 1.9711442291736603e-06

Training epoch-83 batch-100
Running loss of epoch-83 batch-100 = 9.778887033462524e-07

Training epoch-83 batch-101
Running loss of epoch-83 batch-101 = 1.3706739991903305e-06

Training epoch-83 batch-102
Running loss of epoch-83 batch-102 = 1.1497177183628082e-06

Training epoch-83 batch-103
Running loss of epoch-83 batch-103 = 1.0908115655183792e-06

Training epoch-83 batch-104
Running loss of epoch-83 batch-104 = 1.360895112156868e-06

Training epoch-83 batch-105
Running loss of epoch-83 batch-105 = 8.069910109043121e-07

Training epoch-83 batch-106
Running loss of epoch-83 batch-106 = 1.5227124094963074e-06

Training epoch-83 batch-107
Running loss of epoch-83 batch-107 = 8.963979780673981e-07

Training epoch-83 batch-108
Running loss of epoch-83 batch-108 = 1.384178176522255e-06

Training epoch-83 batch-109
Running loss of epoch-83 batch-109 = 1.1210795491933823e-06

Training epoch-83 batch-110
Running loss of epoch-83 batch-110 = 1.8451828509569168e-06

Training epoch-83 batch-111
Running loss of epoch-83 batch-111 = 9.252689778804779e-07

Training epoch-83 batch-112
Running loss of epoch-83 batch-112 = 1.4167744666337967e-06

Training epoch-83 batch-113
Running loss of epoch-83 batch-113 = 1.966021955013275e-06

Training epoch-83 batch-114
Running loss of epoch-83 batch-114 = 1.3166572898626328e-06

Training epoch-83 batch-115
Running loss of epoch-83 batch-115 = 1.5276018530130386e-06

Training epoch-83 batch-116
Running loss of epoch-83 batch-116 = 1.0533258318901062e-06

Training epoch-83 batch-117
Running loss of epoch-83 batch-117 = 1.2579839676618576e-06

Training epoch-83 batch-118
Running loss of epoch-83 batch-118 = 1.5937257558107376e-06

Training epoch-83 batch-119
Running loss of epoch-83 batch-119 = 9.951181709766388e-07

Training epoch-83 batch-120
Running loss of epoch-83 batch-120 = 1.0058283805847168e-06

Training epoch-83 batch-121
Running loss of epoch-83 batch-121 = 1.2319069355726242e-06

Training epoch-83 batch-122
Running loss of epoch-83 batch-122 = 1.8421560525894165e-06

Training epoch-83 batch-123
Running loss of epoch-83 batch-123 = 1.234235242009163e-06

Training epoch-83 batch-124
Running loss of epoch-83 batch-124 = 1.2917444109916687e-06

Training epoch-83 batch-125
Running loss of epoch-83 batch-125 = 2.0263250917196274e-06

Training epoch-83 batch-126
Running loss of epoch-83 batch-126 = 1.1995434761047363e-06

Training epoch-83 batch-127
Running loss of epoch-83 batch-127 = 2.1061860024929047e-06

Training epoch-83 batch-128
Running loss of epoch-83 batch-128 = 1.6088597476482391e-06

Training epoch-83 batch-129
Running loss of epoch-83 batch-129 = 1.1385418474674225e-06

Training epoch-83 batch-130
Running loss of epoch-83 batch-130 = 1.5087425708770752e-06

Training epoch-83 batch-131
Running loss of epoch-83 batch-131 = 9.278301149606705e-07

Training epoch-83 batch-132
Running loss of epoch-83 batch-132 = 1.609092578291893e-06

Training epoch-83 batch-133
Running loss of epoch-83 batch-133 = 1.780455932021141e-06

Training epoch-83 batch-134
Running loss of epoch-83 batch-134 = 1.698499545454979e-06

Training epoch-83 batch-135
Running loss of epoch-83 batch-135 = 1.425156369805336e-06

Training epoch-83 batch-136
Running loss of epoch-83 batch-136 = 1.757405698299408e-06

Training epoch-83 batch-137
Running loss of epoch-83 batch-137 = 1.1080410331487656e-06

Training epoch-83 batch-138
Running loss of epoch-83 batch-138 = 1.251930370926857e-06

Training epoch-83 batch-139
Running loss of epoch-83 batch-139 = 9.55304130911827e-07

Training epoch-83 batch-140
Running loss of epoch-83 batch-140 = 1.5082769095897675e-06

Training epoch-83 batch-141
Running loss of epoch-83 batch-141 = 1.9704457372426987e-06

Training epoch-83 batch-142
Running loss of epoch-83 batch-142 = 1.5227124094963074e-06

Training epoch-83 batch-143
Running loss of epoch-83 batch-143 = 2.345070242881775e-06

Training epoch-83 batch-144
Running loss of epoch-83 batch-144 = 1.176958903670311e-06

Training epoch-83 batch-145
Running loss of epoch-83 batch-145 = 1.36462040245533e-06

Training epoch-83 batch-146
Running loss of epoch-83 batch-146 = 2.062413841485977e-06

Training epoch-83 batch-147
Running loss of epoch-83 batch-147 = 9.776558727025986e-07

Training epoch-83 batch-148
Running loss of epoch-83 batch-148 = 2.044718712568283e-06

Training epoch-83 batch-149
Running loss of epoch-83 batch-149 = 1.7820857465267181e-06

Training epoch-83 batch-150
Running loss of epoch-83 batch-150 = 1.2402888387441635e-06

Training epoch-83 batch-151
Running loss of epoch-83 batch-151 = 1.298496499657631e-06

Training epoch-83 batch-152
Running loss of epoch-83 batch-152 = 8.870847523212433e-07

Training epoch-83 batch-153
Running loss of epoch-83 batch-153 = 1.7189886420965195e-06

Training epoch-83 batch-154
Running loss of epoch-83 batch-154 = 1.451931893825531e-06

Training epoch-83 batch-155
Running loss of epoch-83 batch-155 = 1.1138617992401123e-06

Training epoch-83 batch-156
Running loss of epoch-83 batch-156 = 1.4309771358966827e-06

Training epoch-83 batch-157
Running loss of epoch-83 batch-157 = 9.603798389434814e-06

Finished training epoch-83.



Average train loss at epoch-83 = 1.4401674270629882e-06

Started Evaluation

Average val loss at epoch-83 = 3.278393847377677

Accuracy for classes:
Accuracy for class equals is: 75.08 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-84


Training epoch-84 batch-1
Running loss of epoch-84 batch-1 = 1.7667189240455627e-06

Training epoch-84 batch-2
Running loss of epoch-84 batch-2 = 1.2558884918689728e-06

Training epoch-84 batch-3
Running loss of epoch-84 batch-3 = 1.6444828361272812e-06

Training epoch-84 batch-4
Running loss of epoch-84 batch-4 = 1.255422830581665e-06

Training epoch-84 batch-5
Running loss of epoch-84 batch-5 = 1.4989636838436127e-06

Training epoch-84 batch-6
Running loss of epoch-84 batch-6 = 1.7280690371990204e-06

Training epoch-84 batch-7
Running loss of epoch-84 batch-7 = 1.3527460396289825e-06

Training epoch-84 batch-8
Running loss of epoch-84 batch-8 = 1.4950055629014969e-06

Training epoch-84 batch-9
Running loss of epoch-84 batch-9 = 9.634532034397125e-07

Training epoch-84 batch-10
Running loss of epoch-84 batch-10 = 1.1294614523649216e-06

Training epoch-84 batch-11
Running loss of epoch-84 batch-11 = 1.8326099961996078e-06

Training epoch-84 batch-12
Running loss of epoch-84 batch-12 = 1.455657184123993e-06

Training epoch-84 batch-13
Running loss of epoch-84 batch-13 = 1.5203841030597687e-06

Training epoch-84 batch-14
Running loss of epoch-84 batch-14 = 1.193024218082428e-06

Training epoch-84 batch-15
Running loss of epoch-84 batch-15 = 1.4218967407941818e-06

Training epoch-84 batch-16
Running loss of epoch-84 batch-16 = 1.4188699424266815e-06

Training epoch-84 batch-17
Running loss of epoch-84 batch-17 = 1.292908564209938e-06

Training epoch-84 batch-18
Running loss of epoch-84 batch-18 = 2.0030420273542404e-06

Training epoch-84 batch-19
Running loss of epoch-84 batch-19 = 1.1792872101068497e-06

Training epoch-84 batch-20
Running loss of epoch-84 batch-20 = 1.4766119420528412e-06

Training epoch-84 batch-21
Running loss of epoch-84 batch-21 = 1.455424353480339e-06

Training epoch-84 batch-22
Running loss of epoch-84 batch-22 = 1.6039703041315079e-06

Training epoch-84 batch-23
Running loss of epoch-84 batch-23 = 2.001877874135971e-06

Training epoch-84 batch-24
Running loss of epoch-84 batch-24 = 1.5436671674251556e-06

Training epoch-84 batch-25
Running loss of epoch-84 batch-25 = 8.889473974704742e-07

Training epoch-84 batch-26
Running loss of epoch-84 batch-26 = 1.780455932021141e-06

Training epoch-84 batch-27
Running loss of epoch-84 batch-27 = 1.1725351214408875e-06

Training epoch-84 batch-28
Running loss of epoch-84 batch-28 = 9.175855666399002e-07

Training epoch-84 batch-29
Running loss of epoch-84 batch-29 = 1.0665971785783768e-06

Training epoch-84 batch-30
Running loss of epoch-84 batch-30 = 8.414499461650848e-07

Training epoch-84 batch-31
Running loss of epoch-84 batch-31 = 1.076841726899147e-06

Training epoch-84 batch-32
Running loss of epoch-84 batch-32 = 1.6165431588888168e-06

Training epoch-84 batch-33
Running loss of epoch-84 batch-33 = 1.4682300388813019e-06

Training epoch-84 batch-34
Running loss of epoch-84 batch-34 = 1.589301973581314e-06

Training epoch-84 batch-35
Running loss of epoch-84 batch-35 = 8.600763976573944e-07

Training epoch-84 batch-36
Running loss of epoch-84 batch-36 = 1.6703270375728607e-06

Training epoch-84 batch-37
Running loss of epoch-84 batch-37 = 1.5730038285255432e-06

Training epoch-84 batch-38
Running loss of epoch-84 batch-38 = 1.4493707567453384e-06

Training epoch-84 batch-39
Running loss of epoch-84 batch-39 = 8.698552846908569e-07

Training epoch-84 batch-40
Running loss of epoch-84 batch-40 = 2.0205043256282806e-06

Training epoch-84 batch-41
Running loss of epoch-84 batch-41 = 1.4794059097766876e-06

Training epoch-84 batch-42
Running loss of epoch-84 batch-42 = 1.3995449990034103e-06

Training epoch-84 batch-43
Running loss of epoch-84 batch-43 = 1.623295247554779e-06

Training epoch-84 batch-44
Running loss of epoch-84 batch-44 = 7.324852049350739e-07

Training epoch-84 batch-45
Running loss of epoch-84 batch-45 = 1.3397075235843658e-06

Training epoch-84 batch-46
Running loss of epoch-84 batch-46 = 1.273350790143013e-06

Training epoch-84 batch-47
Running loss of epoch-84 batch-47 = 1.7818529158830643e-06

Training epoch-84 batch-48
Running loss of epoch-84 batch-48 = 1.6889534890651703e-06

Training epoch-84 batch-49
Running loss of epoch-84 batch-49 = 1.098262146115303e-06

Training epoch-84 batch-50
Running loss of epoch-84 batch-50 = 1.1525116860866547e-06

Training epoch-84 batch-51
Running loss of epoch-84 batch-51 = 1.3955868780612946e-06

Training epoch-84 batch-52
Running loss of epoch-84 batch-52 = 1.3811513781547546e-06

Training epoch-84 batch-53
Running loss of epoch-84 batch-53 = 1.2819655239582062e-06

Training epoch-84 batch-54
Running loss of epoch-84 batch-54 = 1.5718396753072739e-06

Training epoch-84 batch-55
Running loss of epoch-84 batch-55 = 1.9248109310865402e-06

Training epoch-84 batch-56
Running loss of epoch-84 batch-56 = 1.4891847968101501e-06

Training epoch-84 batch-57
Running loss of epoch-84 batch-57 = 1.1026859283447266e-06

Training epoch-84 batch-58
Running loss of epoch-84 batch-58 = 1.2605451047420502e-06

Training epoch-84 batch-59
Running loss of epoch-84 batch-59 = 1.6656704246997833e-06

Training epoch-84 batch-60
Running loss of epoch-84 batch-60 = 1.4568213373422623e-06

Training epoch-84 batch-61
Running loss of epoch-84 batch-61 = 1.130392774939537e-06

Training epoch-84 batch-62
Running loss of epoch-84 batch-62 = 1.9471626728773117e-06

Training epoch-84 batch-63
Running loss of epoch-84 batch-63 = 1.4915131032466888e-06

Training epoch-84 batch-64
Running loss of epoch-84 batch-64 = 9.683426469564438e-07

Training epoch-84 batch-65
Running loss of epoch-84 batch-65 = 1.986045390367508e-06

Training epoch-84 batch-66
Running loss of epoch-84 batch-66 = 1.3557728379964828e-06

Training epoch-84 batch-67
Running loss of epoch-84 batch-67 = 1.5173573046922684e-06

Training epoch-84 batch-68
Running loss of epoch-84 batch-68 = 1.6153790056705475e-06

Training epoch-84 batch-69
Running loss of epoch-84 batch-69 = 1.6633421182632446e-06

Training epoch-84 batch-70
Running loss of epoch-84 batch-70 = 9.844079613685608e-07

Training epoch-84 batch-71
Running loss of epoch-84 batch-71 = 1.49640254676342e-06

Training epoch-84 batch-72
Running loss of epoch-84 batch-72 = 1.2922100722789764e-06

Training epoch-84 batch-73
Running loss of epoch-84 batch-73 = 1.3201497495174408e-06

Training epoch-84 batch-74
Running loss of epoch-84 batch-74 = 1.4281831681728363e-06

Training epoch-84 batch-75
Running loss of epoch-84 batch-75 = 1.0011717677116394e-06

Training epoch-84 batch-76
Running loss of epoch-84 batch-76 = 8.025672286748886e-07

Training epoch-84 batch-77
Running loss of epoch-84 batch-77 = 1.005595549941063e-06

Training epoch-84 batch-78
Running loss of epoch-84 batch-78 = 1.5499535948038101e-06

Training epoch-84 batch-79
Running loss of epoch-84 batch-79 = 1.2514647096395493e-06

Training epoch-84 batch-80
Running loss of epoch-84 batch-80 = 1.56438909471035e-06

Training epoch-84 batch-81
Running loss of epoch-84 batch-81 = 1.085922122001648e-06

Training epoch-84 batch-82
Running loss of epoch-84 batch-82 = 1.5874393284320831e-06

Training epoch-84 batch-83
Running loss of epoch-84 batch-83 = 1.307111233472824e-06

Training epoch-84 batch-84
Running loss of epoch-84 batch-84 = 1.1862721294164658e-06

Training epoch-84 batch-85
Running loss of epoch-84 batch-85 = 1.2402888387441635e-06

Training epoch-84 batch-86
Running loss of epoch-84 batch-86 = 1.4407560229301453e-06

Training epoch-84 batch-87
Running loss of epoch-84 batch-87 = 1.9585713744163513e-06

Training epoch-84 batch-88
Running loss of epoch-84 batch-88 = 1.5094410628080368e-06

Training epoch-84 batch-89
Running loss of epoch-84 batch-89 = 1.448439434170723e-06

Training epoch-84 batch-90
Running loss of epoch-84 batch-90 = 1.0516960173845291e-06

Training epoch-84 batch-91
Running loss of epoch-84 batch-91 = 1.8081627786159515e-06

Training epoch-84 batch-92
Running loss of epoch-84 batch-92 = 2.0060688257217407e-06

Training epoch-84 batch-93
Running loss of epoch-84 batch-93 = 1.4677643775939941e-06

Training epoch-84 batch-94
Running loss of epoch-84 batch-94 = 1.1904630810022354e-06

Training epoch-84 batch-95
Running loss of epoch-84 batch-95 = 1.9758008420467377e-06

Training epoch-84 batch-96
Running loss of epoch-84 batch-96 = 1.2919772416353226e-06

Training epoch-84 batch-97
Running loss of epoch-84 batch-97 = 1.1208467185497284e-06

Training epoch-84 batch-98
Running loss of epoch-84 batch-98 = 2.0707957446575165e-06

Training epoch-84 batch-99
Running loss of epoch-84 batch-99 = 1.735985279083252e-06

Training epoch-84 batch-100
Running loss of epoch-84 batch-100 = 1.8854625523090363e-06

Training epoch-84 batch-101
Running loss of epoch-84 batch-101 = 1.4419201761484146e-06

Training epoch-84 batch-102
Running loss of epoch-84 batch-102 = 1.367880031466484e-06

Training epoch-84 batch-103
Running loss of epoch-84 batch-103 = 1.7238780856132507e-06

Training epoch-84 batch-104
Running loss of epoch-84 batch-104 = 1.4421530067920685e-06

Training epoch-84 batch-105
Running loss of epoch-84 batch-105 = 1.2789387255907059e-06

Training epoch-84 batch-106
Running loss of epoch-84 batch-106 = 1.5364494174718857e-06

Training epoch-84 batch-107
Running loss of epoch-84 batch-107 = 1.484062522649765e-06

Training epoch-84 batch-108
Running loss of epoch-84 batch-108 = 1.314561814069748e-06

Training epoch-84 batch-109
Running loss of epoch-84 batch-109 = 1.3664830476045609e-06

Training epoch-84 batch-110
Running loss of epoch-84 batch-110 = 1.400010660290718e-06

Training epoch-84 batch-111
Running loss of epoch-84 batch-111 = 1.6998965293169022e-06

Training epoch-84 batch-112
Running loss of epoch-84 batch-112 = 1.6184058040380478e-06

Training epoch-84 batch-113
Running loss of epoch-84 batch-113 = 1.2901145964860916e-06

Training epoch-84 batch-114
Running loss of epoch-84 batch-114 = 9.14325937628746e-07

Training epoch-84 batch-115
Running loss of epoch-84 batch-115 = 1.9741710275411606e-06

Training epoch-84 batch-116
Running loss of epoch-84 batch-116 = 1.1525116860866547e-06

Training epoch-84 batch-117
Running loss of epoch-84 batch-117 = 1.4649704098701477e-06

Training epoch-84 batch-118
Running loss of epoch-84 batch-118 = 1.96089968085289e-06

Training epoch-84 batch-119
Running loss of epoch-84 batch-119 = 7.932540029287338e-07

Training epoch-84 batch-120
Running loss of epoch-84 batch-120 = 9.348150342702866e-07

Training epoch-84 batch-121
Running loss of epoch-84 batch-121 = 1.409091055393219e-06

Training epoch-84 batch-122
Running loss of epoch-84 batch-122 = 1.6426201909780502e-06

Training epoch-84 batch-123
Running loss of epoch-84 batch-123 = 1.2882519513368607e-06

Training epoch-84 batch-124
Running loss of epoch-84 batch-124 = 1.5280675143003464e-06

Training epoch-84 batch-125
Running loss of epoch-84 batch-125 = 1.1816155165433884e-06

Training epoch-84 batch-126
Running loss of epoch-84 batch-126 = 2.062646672129631e-06

Training epoch-84 batch-127
Running loss of epoch-84 batch-127 = 1.3827811926603317e-06

Training epoch-84 batch-128
Running loss of epoch-84 batch-128 = 1.3485550880432129e-06

Training epoch-84 batch-129
Running loss of epoch-84 batch-129 = 8.873175829648972e-07

Training epoch-84 batch-130
Running loss of epoch-84 batch-130 = 1.1527445167303085e-06

Training epoch-84 batch-131
Running loss of epoch-84 batch-131 = 8.181668817996979e-07

Training epoch-84 batch-132
Running loss of epoch-84 batch-132 = 1.2312084436416626e-06

Training epoch-84 batch-133
Running loss of epoch-84 batch-133 = 1.4971010386943817e-06

Training epoch-84 batch-134
Running loss of epoch-84 batch-134 = 9.932555258274078e-07

Training epoch-84 batch-135
Running loss of epoch-84 batch-135 = 2.4386681616306305e-06

Training epoch-84 batch-136
Running loss of epoch-84 batch-136 = 7.846392691135406e-07

Training epoch-84 batch-137
Running loss of epoch-84 batch-137 = 8.426140993833542e-07

Training epoch-84 batch-138
Running loss of epoch-84 batch-138 = 1.932727172970772e-06

Training epoch-84 batch-139
Running loss of epoch-84 batch-139 = 1.3827811926603317e-06

Training epoch-84 batch-140
Running loss of epoch-84 batch-140 = 8.963979780673981e-07

Training epoch-84 batch-141
Running loss of epoch-84 batch-141 = 1.1371448636054993e-06

Training epoch-84 batch-142
Running loss of epoch-84 batch-142 = 1.4156103134155273e-06

Training epoch-84 batch-143
Running loss of epoch-84 batch-143 = 1.6398262232542038e-06

Training epoch-84 batch-144
Running loss of epoch-84 batch-144 = 1.8165446817874908e-06

Training epoch-84 batch-145
Running loss of epoch-84 batch-145 = 1.677079126238823e-06

Training epoch-84 batch-146
Running loss of epoch-84 batch-146 = 1.3313256204128265e-06

Training epoch-84 batch-147
Running loss of epoch-84 batch-147 = 1.4635734260082245e-06

Training epoch-84 batch-148
Running loss of epoch-84 batch-148 = 1.1806841939687729e-06

Training epoch-84 batch-149
Running loss of epoch-84 batch-149 = 1.5655532479286194e-06

Training epoch-84 batch-150
Running loss of epoch-84 batch-150 = 9.825453162193298e-07

Training epoch-84 batch-151
Running loss of epoch-84 batch-151 = 9.42964106798172e-07

Training epoch-84 batch-152
Running loss of epoch-84 batch-152 = 1.5757977962493896e-06

Training epoch-84 batch-153
Running loss of epoch-84 batch-153 = 1.4514662325382233e-06

Training epoch-84 batch-154
Running loss of epoch-84 batch-154 = 1.6186386346817017e-06

Training epoch-84 batch-155
Running loss of epoch-84 batch-155 = 1.1685770004987717e-06

Training epoch-84 batch-156
Running loss of epoch-84 batch-156 = 1.2780074030160904e-06

Training epoch-84 batch-157
Running loss of epoch-84 batch-157 = 5.982816219329834e-06

Finished training epoch-84.



Average train loss at epoch-84 = 1.413080096244812e-06

Started Evaluation

Average val loss at epoch-84 = 3.28192990861441

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.55 %

Finished Evaluation



Started training epoch-85


Training epoch-85 batch-1
Running loss of epoch-85 batch-1 = 2.3352913558483124e-06

Training epoch-85 batch-2
Running loss of epoch-85 batch-2 = 1.0619405657052994e-06

Training epoch-85 batch-3
Running loss of epoch-85 batch-3 = 1.5289988368749619e-06

Training epoch-85 batch-4
Running loss of epoch-85 batch-4 = 1.4023389667272568e-06

Training epoch-85 batch-5
Running loss of epoch-85 batch-5 = 1.4675315469503403e-06

Training epoch-85 batch-6
Running loss of epoch-85 batch-6 = 1.0961666703224182e-06

Training epoch-85 batch-7
Running loss of epoch-85 batch-7 = 1.1783558875322342e-06

Training epoch-85 batch-8
Running loss of epoch-85 batch-8 = 1.9955914467573166e-06

Training epoch-85 batch-9
Running loss of epoch-85 batch-9 = 1.0970979928970337e-06

Training epoch-85 batch-10
Running loss of epoch-85 batch-10 = 1.6640406101942062e-06

Training epoch-85 batch-11
Running loss of epoch-85 batch-11 = 1.6954727470874786e-06

Training epoch-85 batch-12
Running loss of epoch-85 batch-12 = 1.3031531125307083e-06

Training epoch-85 batch-13
Running loss of epoch-85 batch-13 = 1.2295786291360855e-06

Training epoch-85 batch-14
Running loss of epoch-85 batch-14 = 8.598435670137405e-07

Training epoch-85 batch-15
Running loss of epoch-85 batch-15 = 7.420312613248825e-07

Training epoch-85 batch-16
Running loss of epoch-85 batch-16 = 2.1096784621477127e-06

Training epoch-85 batch-17
Running loss of epoch-85 batch-17 = 1.910608261823654e-06

Training epoch-85 batch-18
Running loss of epoch-85 batch-18 = 1.2952368706464767e-06

Training epoch-85 batch-19
Running loss of epoch-85 batch-19 = 1.8673017621040344e-06

Training epoch-85 batch-20
Running loss of epoch-85 batch-20 = 9.620562195777893e-07

Training epoch-85 batch-21
Running loss of epoch-85 batch-21 = 2.032378688454628e-06

Training epoch-85 batch-22
Running loss of epoch-85 batch-22 = 2.023763954639435e-06

Training epoch-85 batch-23
Running loss of epoch-85 batch-23 = 1.0945368558168411e-06

Training epoch-85 batch-24
Running loss of epoch-85 batch-24 = 1.0184012353420258e-06

Training epoch-85 batch-25
Running loss of epoch-85 batch-25 = 1.6614794731140137e-06

Training epoch-85 batch-26
Running loss of epoch-85 batch-26 = 1.5122350305318832e-06

Training epoch-85 batch-27
Running loss of epoch-85 batch-27 = 1.3320241123437881e-06

Training epoch-85 batch-28
Running loss of epoch-85 batch-28 = 1.805601641535759e-06

Training epoch-85 batch-29
Running loss of epoch-85 batch-29 = 1.282431185245514e-06

Training epoch-85 batch-30
Running loss of epoch-85 batch-30 = 1.1003576219081879e-06

Training epoch-85 batch-31
Running loss of epoch-85 batch-31 = 1.8193386495113373e-06

Training epoch-85 batch-32
Running loss of epoch-85 batch-32 = 1.5057157725095749e-06

Training epoch-85 batch-33
Running loss of epoch-85 batch-33 = 1.3685785233974457e-06

Training epoch-85 batch-34
Running loss of epoch-85 batch-34 = 1.7825514078140259e-06

Training epoch-85 batch-35
Running loss of epoch-85 batch-35 = 8.593779057264328e-07

Training epoch-85 batch-36
Running loss of epoch-85 batch-36 = 1.5941914170980453e-06

Training epoch-85 batch-37
Running loss of epoch-85 batch-37 = 1.166248694062233e-06

Training epoch-85 batch-38
Running loss of epoch-85 batch-38 = 1.2677628546953201e-06

Training epoch-85 batch-39
Running loss of epoch-85 batch-39 = 1.3799872249364853e-06

Training epoch-85 batch-40
Running loss of epoch-85 batch-40 = 1.6016419976949692e-06

Training epoch-85 batch-41
Running loss of epoch-85 batch-41 = 1.5993136912584305e-06

Training epoch-85 batch-42
Running loss of epoch-85 batch-42 = 1.10710971057415e-06

Training epoch-85 batch-43
Running loss of epoch-85 batch-43 = 1.6386620700359344e-06

Training epoch-85 batch-44
Running loss of epoch-85 batch-44 = 1.410720869898796e-06

Training epoch-85 batch-45
Running loss of epoch-85 batch-45 = 1.00722536444664e-06

Training epoch-85 batch-46
Running loss of epoch-85 batch-46 = 1.237494871020317e-06

Training epoch-85 batch-47
Running loss of epoch-85 batch-47 = 1.5830155462026596e-06

Training epoch-85 batch-48
Running loss of epoch-85 batch-48 = 1.6009435057640076e-06

Training epoch-85 batch-49
Running loss of epoch-85 batch-49 = 1.3955868780612946e-06

Training epoch-85 batch-50
Running loss of epoch-85 batch-50 = 1.3522803783416748e-06

Training epoch-85 batch-51
Running loss of epoch-85 batch-51 = 1.7494894564151764e-06

Training epoch-85 batch-52
Running loss of epoch-85 batch-52 = 1.2435484677553177e-06

Training epoch-85 batch-53
Running loss of epoch-85 batch-53 = 1.2367963790893555e-06

Training epoch-85 batch-54
Running loss of epoch-85 batch-54 = 1.1613592505455017e-06

Training epoch-85 batch-55
Running loss of epoch-85 batch-55 = 1.8277205526828766e-06

Training epoch-85 batch-56
Running loss of epoch-85 batch-56 = 1.482432708144188e-06

Training epoch-85 batch-57
Running loss of epoch-85 batch-57 = 1.944834366440773e-06

Training epoch-85 batch-58
Running loss of epoch-85 batch-58 = 1.6354024410247803e-06

Training epoch-85 batch-59
Running loss of epoch-85 batch-59 = 1.264270395040512e-06

Training epoch-85 batch-60
Running loss of epoch-85 batch-60 = 1.3296958059072495e-06

Training epoch-85 batch-61
Running loss of epoch-85 batch-61 = 8.975621312856674e-07

Training epoch-85 batch-62
Running loss of epoch-85 batch-62 = 1.6656704246997833e-06

Training epoch-85 batch-63
Running loss of epoch-85 batch-63 = 1.0514631867408752e-06

Training epoch-85 batch-64
Running loss of epoch-85 batch-64 = 2.0721927285194397e-06

Training epoch-85 batch-65
Running loss of epoch-85 batch-65 = 2.074986696243286e-06

Training epoch-85 batch-66
Running loss of epoch-85 batch-66 = 1.469859853386879e-06

Training epoch-85 batch-67
Running loss of epoch-85 batch-67 = 1.439359039068222e-06

Training epoch-85 batch-68
Running loss of epoch-85 batch-68 = 1.4593824744224548e-06

Training epoch-85 batch-69
Running loss of epoch-85 batch-69 = 1.037493348121643e-06

Training epoch-85 batch-70
Running loss of epoch-85 batch-70 = 9.182840585708618e-07

Training epoch-85 batch-71
Running loss of epoch-85 batch-71 = 9.206123650074005e-07

Training epoch-85 batch-72
Running loss of epoch-85 batch-72 = 2.0456500351428986e-06

Training epoch-85 batch-73
Running loss of epoch-85 batch-73 = 1.1979136615991592e-06

Training epoch-85 batch-74
Running loss of epoch-85 batch-74 = 2.4426262825727463e-06

Training epoch-85 batch-75
Running loss of epoch-85 batch-75 = 1.7529819160699844e-06

Training epoch-85 batch-76
Running loss of epoch-85 batch-76 = 1.4617107808589935e-06

Training epoch-85 batch-77
Running loss of epoch-85 batch-77 = 1.3886019587516785e-06

Training epoch-85 batch-78
Running loss of epoch-85 batch-78 = 1.0367948561906815e-06

Training epoch-85 batch-79
Running loss of epoch-85 batch-79 = 1.1648517102003098e-06

Training epoch-85 batch-80
Running loss of epoch-85 batch-80 = 8.381903171539307e-07

Training epoch-85 batch-81
Running loss of epoch-85 batch-81 = 1.1506490409374237e-06

Training epoch-85 batch-82
Running loss of epoch-85 batch-82 = 1.5315599739551544e-06

Training epoch-85 batch-83
Running loss of epoch-85 batch-83 = 6.002373993396759e-07

Training epoch-85 batch-84
Running loss of epoch-85 batch-84 = 1.6093254089355469e-06

Training epoch-85 batch-85
Running loss of epoch-85 batch-85 = 1.8165446817874908e-06

Training epoch-85 batch-86
Running loss of epoch-85 batch-86 = 1.0996591299772263e-06

Training epoch-85 batch-87
Running loss of epoch-85 batch-87 = 1.5052501112222672e-06

Training epoch-85 batch-88
Running loss of epoch-85 batch-88 = 1.298263669013977e-06

Training epoch-85 batch-89
Running loss of epoch-85 batch-89 = 1.6423873603343964e-06

Training epoch-85 batch-90
Running loss of epoch-85 batch-90 = 1.3669487088918686e-06

Training epoch-85 batch-91
Running loss of epoch-85 batch-91 = 1.5133991837501526e-06

Training epoch-85 batch-92
Running loss of epoch-85 batch-92 = 9.958166629076004e-07

Training epoch-85 batch-93
Running loss of epoch-85 batch-93 = 1.3669487088918686e-06

Training epoch-85 batch-94
Running loss of epoch-85 batch-94 = 1.5606638044118881e-06

Training epoch-85 batch-95
Running loss of epoch-85 batch-95 = 8.984934538602829e-07

Training epoch-85 batch-96
Running loss of epoch-85 batch-96 = 1.7120037227869034e-06

Training epoch-85 batch-97
Running loss of epoch-85 batch-97 = 1.5194527804851532e-06

Training epoch-85 batch-98
Running loss of epoch-85 batch-98 = 7.969792932271957e-07

Training epoch-85 batch-99
Running loss of epoch-85 batch-99 = 7.618218660354614e-07

Training epoch-85 batch-100
Running loss of epoch-85 batch-100 = 1.889420673251152e-06

Training epoch-85 batch-101
Running loss of epoch-85 batch-101 = 1.0551884770393372e-06

Training epoch-85 batch-102
Running loss of epoch-85 batch-102 = 9.592622518539429e-07

Training epoch-85 batch-103
Running loss of epoch-85 batch-103 = 1.3210810720920563e-06

Training epoch-85 batch-104
Running loss of epoch-85 batch-104 = 1.6584526747465134e-06

Training epoch-85 batch-105
Running loss of epoch-85 batch-105 = 8.596107363700867e-07

Training epoch-85 batch-106
Running loss of epoch-85 batch-106 = 9.522773325443268e-07

Training epoch-85 batch-107
Running loss of epoch-85 batch-107 = 1.3457611203193665e-06

Training epoch-85 batch-108
Running loss of epoch-85 batch-108 = 1.3499520719051361e-06

Training epoch-85 batch-109
Running loss of epoch-85 batch-109 = 1.0058283805847168e-06

Training epoch-85 batch-110
Running loss of epoch-85 batch-110 = 1.3669487088918686e-06

Training epoch-85 batch-111
Running loss of epoch-85 batch-111 = 1.308973878622055e-06

Training epoch-85 batch-112
Running loss of epoch-85 batch-112 = 1.5178229659795761e-06

Training epoch-85 batch-113
Running loss of epoch-85 batch-113 = 9.012874215841293e-07

Training epoch-85 batch-114
Running loss of epoch-85 batch-114 = 1.419801265001297e-06

Training epoch-85 batch-115
Running loss of epoch-85 batch-115 = 8.800998330116272e-07

Training epoch-85 batch-116
Running loss of epoch-85 batch-116 = 1.4528632164001465e-06

Training epoch-85 batch-117
Running loss of epoch-85 batch-117 = 1.4528632164001465e-06

Training epoch-85 batch-118
Running loss of epoch-85 batch-118 = 1.2312084436416626e-06

Training epoch-85 batch-119
Running loss of epoch-85 batch-119 = 1.3133976608514786e-06

Training epoch-85 batch-120
Running loss of epoch-85 batch-120 = 1.3848766684532166e-06

Training epoch-85 batch-121
Running loss of epoch-85 batch-121 = 1.5387777239084244e-06

Training epoch-85 batch-122
Running loss of epoch-85 batch-122 = 1.1862721294164658e-06

Training epoch-85 batch-123
Running loss of epoch-85 batch-123 = 1.2514647096395493e-06

Training epoch-85 batch-124
Running loss of epoch-85 batch-124 = 1.0575167834758759e-06

Training epoch-85 batch-125
Running loss of epoch-85 batch-125 = 1.8132850527763367e-06

Training epoch-85 batch-126
Running loss of epoch-85 batch-126 = 1.3872049748897552e-06

Training epoch-85 batch-127
Running loss of epoch-85 batch-127 = 1.3136304914951324e-06

Training epoch-85 batch-128
Running loss of epoch-85 batch-128 = 1.5066470950841904e-06

Training epoch-85 batch-129
Running loss of epoch-85 batch-129 = 9.047798812389374e-07

Training epoch-85 batch-130
Running loss of epoch-85 batch-130 = 1.0468065738677979e-06

Training epoch-85 batch-131
Running loss of epoch-85 batch-131 = 1.542968675494194e-06

Training epoch-85 batch-132
Running loss of epoch-85 batch-132 = 1.2381933629512787e-06

Training epoch-85 batch-133
Running loss of epoch-85 batch-133 = 1.144595444202423e-06

Training epoch-85 batch-134
Running loss of epoch-85 batch-134 = 1.4950055629014969e-06

Training epoch-85 batch-135
Running loss of epoch-85 batch-135 = 1.4470424503087997e-06

Training epoch-85 batch-136
Running loss of epoch-85 batch-136 = 1.3685785233974457e-06

Training epoch-85 batch-137
Running loss of epoch-85 batch-137 = 1.2065283954143524e-06

Training epoch-85 batch-138
Running loss of epoch-85 batch-138 = 1.0656658560037613e-06

Training epoch-85 batch-139
Running loss of epoch-85 batch-139 = 1.9476283341646194e-06

Training epoch-85 batch-140
Running loss of epoch-85 batch-140 = 1.3378448784351349e-06

Training epoch-85 batch-141
Running loss of epoch-85 batch-141 = 1.7900019884109497e-06

Training epoch-85 batch-142
Running loss of epoch-85 batch-142 = 1.2707896530628204e-06

Training epoch-85 batch-143
Running loss of epoch-85 batch-143 = 1.3043172657489777e-06

Training epoch-85 batch-144
Running loss of epoch-85 batch-144 = 1.1792872101068497e-06

Training epoch-85 batch-145
Running loss of epoch-85 batch-145 = 1.3557728379964828e-06

Training epoch-85 batch-146
Running loss of epoch-85 batch-146 = 1.8256250768899918e-06

Training epoch-85 batch-147
Running loss of epoch-85 batch-147 = 1.1827796697616577e-06

Training epoch-85 batch-148
Running loss of epoch-85 batch-148 = 1.6940757632255554e-06

Training epoch-85 batch-149
Running loss of epoch-85 batch-149 = 1.5553086996078491e-06

Training epoch-85 batch-150
Running loss of epoch-85 batch-150 = 1.4309771358966827e-06

Training epoch-85 batch-151
Running loss of epoch-85 batch-151 = 1.375097781419754e-06

Training epoch-85 batch-152
Running loss of epoch-85 batch-152 = 1.3245735317468643e-06

Training epoch-85 batch-153
Running loss of epoch-85 batch-153 = 9.264331310987473e-07

Training epoch-85 batch-154
Running loss of epoch-85 batch-154 = 1.8880236893892288e-06

Training epoch-85 batch-155
Running loss of epoch-85 batch-155 = 1.1469237506389618e-06

Training epoch-85 batch-156
Running loss of epoch-85 batch-156 = 1.458916813135147e-06

Training epoch-85 batch-157
Running loss of epoch-85 batch-157 = 4.425644874572754e-06

Finished training epoch-85.



Average train loss at epoch-85 = 1.3897225260734557e-06

Started Evaluation

Average val loss at epoch-85 = 3.285146026234878

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 50.85 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-86


Training epoch-86 batch-1
Running loss of epoch-86 batch-1 = 2.292916178703308e-06

Training epoch-86 batch-2
Running loss of epoch-86 batch-2 = 1.6421545296907425e-06

Training epoch-86 batch-3
Running loss of epoch-86 batch-3 = 1.6011763364076614e-06

Training epoch-86 batch-4
Running loss of epoch-86 batch-4 = 6.984919309616089e-07

Training epoch-86 batch-5
Running loss of epoch-86 batch-5 = 1.5890691429376602e-06

Training epoch-86 batch-6
Running loss of epoch-86 batch-6 = 9.620562195777893e-07

Training epoch-86 batch-7
Running loss of epoch-86 batch-7 = 1.5927944332361221e-06

Training epoch-86 batch-8
Running loss of epoch-86 batch-8 = 1.16787850856781e-06

Training epoch-86 batch-9
Running loss of epoch-86 batch-9 = 1.2351665645837784e-06

Training epoch-86 batch-10
Running loss of epoch-86 batch-10 = 1.7201527953147888e-06

Training epoch-86 batch-11
Running loss of epoch-86 batch-11 = 1.098494976758957e-06

Training epoch-86 batch-12
Running loss of epoch-86 batch-12 = 1.0435469448566437e-06

Training epoch-86 batch-13
Running loss of epoch-86 batch-13 = 1.077074557542801e-06

Training epoch-86 batch-14
Running loss of epoch-86 batch-14 = 1.55717134475708e-06

Training epoch-86 batch-15
Running loss of epoch-86 batch-15 = 9.592622518539429e-07

Training epoch-86 batch-16
Running loss of epoch-86 batch-16 = 9.117648005485535e-07

Training epoch-86 batch-17
Running loss of epoch-86 batch-17 = 1.6067642718553543e-06

Training epoch-86 batch-18
Running loss of epoch-86 batch-18 = 1.3131648302078247e-06

Training epoch-86 batch-19
Running loss of epoch-86 batch-19 = 1.2391246855258942e-06

Training epoch-86 batch-20
Running loss of epoch-86 batch-20 = 1.6884878277778625e-06

Training epoch-86 batch-21
Running loss of epoch-86 batch-21 = 1.8014106899499893e-06

Training epoch-86 batch-22
Running loss of epoch-86 batch-22 = 1.3795215636491776e-06

Training epoch-86 batch-23
Running loss of epoch-86 batch-23 = 9.543728083372116e-07

Training epoch-86 batch-24
Running loss of epoch-86 batch-24 = 1.1275988072156906e-06

Training epoch-86 batch-25
Running loss of epoch-86 batch-25 = 1.602107658982277e-06

Training epoch-86 batch-26
Running loss of epoch-86 batch-26 = 1.2102536857128143e-06

Training epoch-86 batch-27
Running loss of epoch-86 batch-27 = 1.6477424651384354e-06

Training epoch-86 batch-28
Running loss of epoch-86 batch-28 = 1.5837140381336212e-06

Training epoch-86 batch-29
Running loss of epoch-86 batch-29 = 1.710839569568634e-06

Training epoch-86 batch-30
Running loss of epoch-86 batch-30 = 1.4766119420528412e-06

Training epoch-86 batch-31
Running loss of epoch-86 batch-31 = 1.2707896530628204e-06

Training epoch-86 batch-32
Running loss of epoch-86 batch-32 = 1.573469489812851e-06

Training epoch-86 batch-33
Running loss of epoch-86 batch-33 = 9.280629456043243e-07

Training epoch-86 batch-34
Running loss of epoch-86 batch-34 = 1.4337711036205292e-06

Training epoch-86 batch-35
Running loss of epoch-86 batch-35 = 8.356291800737381e-07

Training epoch-86 batch-36
Running loss of epoch-86 batch-36 = 1.1222437024116516e-06

Training epoch-86 batch-37
Running loss of epoch-86 batch-37 = 1.4521647244691849e-06

Training epoch-86 batch-38
Running loss of epoch-86 batch-38 = 2.159271389245987e-06

Training epoch-86 batch-39
Running loss of epoch-86 batch-39 = 1.7452985048294067e-06

Training epoch-86 batch-40
Running loss of epoch-86 batch-40 = 1.6416888684034348e-06

Training epoch-86 batch-41
Running loss of epoch-86 batch-41 = 1.1776573956012726e-06

Training epoch-86 batch-42
Running loss of epoch-86 batch-42 = 1.49640254676342e-06

Training epoch-86 batch-43
Running loss of epoch-86 batch-43 = 9.413342922925949e-07

Training epoch-86 batch-44
Running loss of epoch-86 batch-44 = 1.3888347893953323e-06

Training epoch-86 batch-45
Running loss of epoch-86 batch-45 = 1.996522769331932e-06

Training epoch-86 batch-46
Running loss of epoch-86 batch-46 = 1.4957040548324585e-06

Training epoch-86 batch-47
Running loss of epoch-86 batch-47 = 1.312699168920517e-06

Training epoch-86 batch-48
Running loss of epoch-86 batch-48 = 1.0700896382331848e-06

Training epoch-86 batch-49
Running loss of epoch-86 batch-49 = 1.368112862110138e-06

Training epoch-86 batch-50
Running loss of epoch-86 batch-50 = 1.5168916434049606e-06

Training epoch-86 batch-51
Running loss of epoch-86 batch-51 = 1.296401023864746e-06

Training epoch-86 batch-52
Running loss of epoch-86 batch-52 = 9.955838322639465e-07

Training epoch-86 batch-53
Running loss of epoch-86 batch-53 = 1.4544930309057236e-06

Training epoch-86 batch-54
Running loss of epoch-86 batch-54 = 1.3855751603841782e-06

Training epoch-86 batch-55
Running loss of epoch-86 batch-55 = 1.5408731997013092e-06

Training epoch-86 batch-56
Running loss of epoch-86 batch-56 = 1.232139766216278e-06

Training epoch-86 batch-57
Running loss of epoch-86 batch-57 = 1.0919757187366486e-06

Training epoch-86 batch-58
Running loss of epoch-86 batch-58 = 1.1334195733070374e-06

Training epoch-86 batch-59
Running loss of epoch-86 batch-59 = 1.0635703802108765e-06

Training epoch-86 batch-60
Running loss of epoch-86 batch-60 = 1.7224811017513275e-06

Training epoch-86 batch-61
Running loss of epoch-86 batch-61 = 1.3022217899560928e-06

Training epoch-86 batch-62
Running loss of epoch-86 batch-62 = 1.0919757187366486e-06

Training epoch-86 batch-63
Running loss of epoch-86 batch-63 = 1.226784661412239e-06

Training epoch-86 batch-64
Running loss of epoch-86 batch-64 = 1.2789387255907059e-06

Training epoch-86 batch-65
Running loss of epoch-86 batch-65 = 1.5855766832828522e-06

Training epoch-86 batch-66
Running loss of epoch-86 batch-66 = 1.4468096196651459e-06

Training epoch-86 batch-67
Running loss of epoch-86 batch-67 = 1.6265548765659332e-06

Training epoch-86 batch-68
Running loss of epoch-86 batch-68 = 1.2675300240516663e-06

Training epoch-86 batch-69
Running loss of epoch-86 batch-69 = 1.6759149730205536e-06

Training epoch-86 batch-70
Running loss of epoch-86 batch-70 = 1.107342541217804e-06

Training epoch-86 batch-71
Running loss of epoch-86 batch-71 = 9.462237358093262e-07

Training epoch-86 batch-72
Running loss of epoch-86 batch-72 = 1.2351665645837784e-06

Training epoch-86 batch-73
Running loss of epoch-86 batch-73 = 1.0309740900993347e-06

Training epoch-86 batch-74
Running loss of epoch-86 batch-74 = 1.3182871043682098e-06

Training epoch-86 batch-75
Running loss of epoch-86 batch-75 = 7.860362529754639e-07

Training epoch-86 batch-76
Running loss of epoch-86 batch-76 = 1.4677643775939941e-06

Training epoch-86 batch-77
Running loss of epoch-86 batch-77 = 1.5918631106615067e-06

Training epoch-86 batch-78
Running loss of epoch-86 batch-78 = 1.0801013559103012e-06

Training epoch-86 batch-79
Running loss of epoch-86 batch-79 = 1.5283003449440002e-06

Training epoch-86 batch-80
Running loss of epoch-86 batch-80 = 1.489417627453804e-06

Training epoch-86 batch-81
Running loss of epoch-86 batch-81 = 1.2058299034833908e-06

Training epoch-86 batch-82
Running loss of epoch-86 batch-82 = 1.3371463865041733e-06

Training epoch-86 batch-83
Running loss of epoch-86 batch-83 = 1.6745179891586304e-06

Training epoch-86 batch-84
Running loss of epoch-86 batch-84 = 7.133930921554565e-07

Training epoch-86 batch-85
Running loss of epoch-86 batch-85 = 1.7737038433551788e-06

Training epoch-86 batch-86
Running loss of epoch-86 batch-86 = 1.4463439583778381e-06

Training epoch-86 batch-87
Running loss of epoch-86 batch-87 = 1.6596168279647827e-06

Training epoch-86 batch-88
Running loss of epoch-86 batch-88 = 1.3615936040878296e-06

Training epoch-86 batch-89
Running loss of epoch-86 batch-89 = 1.267995685338974e-06

Training epoch-86 batch-90
Running loss of epoch-86 batch-90 = 9.646173566579819e-07

Training epoch-86 batch-91
Running loss of epoch-86 batch-91 = 1.2845266610383987e-06

Training epoch-86 batch-92
Running loss of epoch-86 batch-92 = 1.4170072972774506e-06

Training epoch-86 batch-93
Running loss of epoch-86 batch-93 = 1.1622905731201172e-06

Training epoch-86 batch-94
Running loss of epoch-86 batch-94 = 1.3150274753570557e-06

Training epoch-86 batch-95
Running loss of epoch-86 batch-95 = 2.4246983230113983e-06

Training epoch-86 batch-96
Running loss of epoch-86 batch-96 = 1.0593794286251068e-06

Training epoch-86 batch-97
Running loss of epoch-86 batch-97 = 9.550713002681732e-07

Training epoch-86 batch-98
Running loss of epoch-86 batch-98 = 1.3837125152349472e-06

Training epoch-86 batch-99
Running loss of epoch-86 batch-99 = 1.8889550119638443e-06

Training epoch-86 batch-100
Running loss of epoch-86 batch-100 = 6.188638508319855e-07

Training epoch-86 batch-101
Running loss of epoch-86 batch-101 = 1.9827857613563538e-06

Training epoch-86 batch-102
Running loss of epoch-86 batch-102 = 1.4051329344511032e-06

Training epoch-86 batch-103
Running loss of epoch-86 batch-103 = 1.6202684491872787e-06

Training epoch-86 batch-104
Running loss of epoch-86 batch-104 = 1.1811498552560806e-06

Training epoch-86 batch-105
Running loss of epoch-86 batch-105 = 1.416308805346489e-06

Training epoch-86 batch-106
Running loss of epoch-86 batch-106 = 1.28219835460186e-06

Training epoch-86 batch-107
Running loss of epoch-86 batch-107 = 1.8139835447072983e-06

Training epoch-86 batch-108
Running loss of epoch-86 batch-108 = 1.7511192709207535e-06

Training epoch-86 batch-109
Running loss of epoch-86 batch-109 = 9.979121387004852e-07

Training epoch-86 batch-110
Running loss of epoch-86 batch-110 = 1.3897661119699478e-06

Training epoch-86 batch-111
Running loss of epoch-86 batch-111 = 1.6093254089355469e-06

Training epoch-86 batch-112
Running loss of epoch-86 batch-112 = 1.4898832887411118e-06

Training epoch-86 batch-113
Running loss of epoch-86 batch-113 = 7.499475032091141e-07

Training epoch-86 batch-114
Running loss of epoch-86 batch-114 = 1.7101410776376724e-06

Training epoch-86 batch-115
Running loss of epoch-86 batch-115 = 1.9690487533807755e-06

Training epoch-86 batch-116
Running loss of epoch-86 batch-116 = 7.746275514364243e-07

Training epoch-86 batch-117
Running loss of epoch-86 batch-117 = 1.4437828212976456e-06

Training epoch-86 batch-118
Running loss of epoch-86 batch-118 = 1.4326069504022598e-06

Training epoch-86 batch-119
Running loss of epoch-86 batch-119 = 1.1329539120197296e-06

Training epoch-86 batch-120
Running loss of epoch-86 batch-120 = 1.4118850231170654e-06

Training epoch-86 batch-121
Running loss of epoch-86 batch-121 = 1.5276018530130386e-06

Training epoch-86 batch-122
Running loss of epoch-86 batch-122 = 1.2104865163564682e-06

Training epoch-86 batch-123
Running loss of epoch-86 batch-123 = 1.341104507446289e-06

Training epoch-86 batch-124
Running loss of epoch-86 batch-124 = 1.301988959312439e-06

Training epoch-86 batch-125
Running loss of epoch-86 batch-125 = 2.011191099882126e-06

Training epoch-86 batch-126
Running loss of epoch-86 batch-126 = 1.3050157576799393e-06

Training epoch-86 batch-127
Running loss of epoch-86 batch-127 = 1.911073923110962e-06

Training epoch-86 batch-128
Running loss of epoch-86 batch-128 = 1.0598450899124146e-06

Training epoch-86 batch-129
Running loss of epoch-86 batch-129 = 8.163042366504669e-07

Training epoch-86 batch-130
Running loss of epoch-86 batch-130 = 1.0526273399591446e-06

Training epoch-86 batch-131
Running loss of epoch-86 batch-131 = 1.4249235391616821e-06

Training epoch-86 batch-132
Running loss of epoch-86 batch-132 = 1.546461135149002e-06

Training epoch-86 batch-133
Running loss of epoch-86 batch-133 = 1.4312099665403366e-06

Training epoch-86 batch-134
Running loss of epoch-86 batch-134 = 1.562526449561119e-06

Training epoch-86 batch-135
Running loss of epoch-86 batch-135 = 2.054031938314438e-06

Training epoch-86 batch-136
Running loss of epoch-86 batch-136 = 8.828938007354736e-07

Training epoch-86 batch-137
Running loss of epoch-86 batch-137 = 1.232139766216278e-06

Training epoch-86 batch-138
Running loss of epoch-86 batch-138 = 1.439359039068222e-06

Training epoch-86 batch-139
Running loss of epoch-86 batch-139 = 1.5532132238149643e-06

Training epoch-86 batch-140
Running loss of epoch-86 batch-140 = 1.3553071767091751e-06

Training epoch-86 batch-141
Running loss of epoch-86 batch-141 = 1.3369135558605194e-06

Training epoch-86 batch-142
Running loss of epoch-86 batch-142 = 1.3902317732572556e-06

Training epoch-86 batch-143
Running loss of epoch-86 batch-143 = 1.768115907907486e-06

Training epoch-86 batch-144
Running loss of epoch-86 batch-144 = 1.4700926840305328e-06

Training epoch-86 batch-145
Running loss of epoch-86 batch-145 = 1.017935574054718e-06

Training epoch-86 batch-146
Running loss of epoch-86 batch-146 = 1.6731210052967072e-06

Training epoch-86 batch-147
Running loss of epoch-86 batch-147 = 1.044711098074913e-06

Training epoch-86 batch-148
Running loss of epoch-86 batch-148 = 9.150244295597076e-07

Training epoch-86 batch-149
Running loss of epoch-86 batch-149 = 1.3110693544149399e-06

Training epoch-86 batch-150
Running loss of epoch-86 batch-150 = 1.3487879186868668e-06

Training epoch-86 batch-151
Running loss of epoch-86 batch-151 = 1.3879034668207169e-06

Training epoch-86 batch-152
Running loss of epoch-86 batch-152 = 1.184176653623581e-06

Training epoch-86 batch-153
Running loss of epoch-86 batch-153 = 1.1113006621599197e-06

Training epoch-86 batch-154
Running loss of epoch-86 batch-154 = 1.5846453607082367e-06

Training epoch-86 batch-155
Running loss of epoch-86 batch-155 = 9.441282600164413e-07

Training epoch-86 batch-156
Running loss of epoch-86 batch-156 = 1.1117663234472275e-06

Training epoch-86 batch-157
Running loss of epoch-86 batch-157 = 8.109956979751587e-06

Finished training epoch-86.



Average train loss at epoch-86 = 1.3730794191360473e-06

Started Evaluation

Average val loss at epoch-86 = 3.2880915983727106

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.64 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-87


Training epoch-87 batch-1
Running loss of epoch-87 batch-1 = 1.1390075087547302e-06

Training epoch-87 batch-2
Running loss of epoch-87 batch-2 = 1.0938383638858795e-06

Training epoch-87 batch-3
Running loss of epoch-87 batch-3 = 1.630512997508049e-06

Training epoch-87 batch-4
Running loss of epoch-87 batch-4 = 1.4468096196651459e-06

Training epoch-87 batch-5
Running loss of epoch-87 batch-5 = 1.2565869837999344e-06

Training epoch-87 batch-6
Running loss of epoch-87 batch-6 = 1.5906989574432373e-06

Training epoch-87 batch-7
Running loss of epoch-87 batch-7 = 6.717164069414139e-07

Training epoch-87 batch-8
Running loss of epoch-87 batch-8 = 1.4256220310926437e-06

Training epoch-87 batch-9
Running loss of epoch-87 batch-9 = 2.0188745111227036e-06

Training epoch-87 batch-10
Running loss of epoch-87 batch-10 = 8.377246558666229e-07

Training epoch-87 batch-11
Running loss of epoch-87 batch-11 = 1.0265503078699112e-06

Training epoch-87 batch-12
Running loss of epoch-87 batch-12 = 1.276843249797821e-06

Training epoch-87 batch-13
Running loss of epoch-87 batch-13 = 1.8943101167678833e-06

Training epoch-87 batch-14
Running loss of epoch-87 batch-14 = 1.0605435818433762e-06

Training epoch-87 batch-15
Running loss of epoch-87 batch-15 = 5.657784640789032e-07

Training epoch-87 batch-16
Running loss of epoch-87 batch-16 = 1.3837125152349472e-06

Training epoch-87 batch-17
Running loss of epoch-87 batch-17 = 1.239357516169548e-06

Training epoch-87 batch-18
Running loss of epoch-87 batch-18 = 1.0458752512931824e-06

Training epoch-87 batch-19
Running loss of epoch-87 batch-19 = 1.3154931366443634e-06

Training epoch-87 batch-20
Running loss of epoch-87 batch-20 = 1.637730747461319e-06

Training epoch-87 batch-21
Running loss of epoch-87 batch-21 = 1.1667143553495407e-06

Training epoch-87 batch-22
Running loss of epoch-87 batch-22 = 1.1760275810956955e-06

Training epoch-87 batch-23
Running loss of epoch-87 batch-23 = 1.4603137969970703e-06

Training epoch-87 batch-24
Running loss of epoch-87 batch-24 = 1.2370292097330093e-06

Training epoch-87 batch-25
Running loss of epoch-87 batch-25 = 1.137610524892807e-06

Training epoch-87 batch-26
Running loss of epoch-87 batch-26 = 1.4069955796003342e-06

Training epoch-87 batch-27
Running loss of epoch-87 batch-27 = 1.4170072972774506e-06

Training epoch-87 batch-28
Running loss of epoch-87 batch-28 = 1.3492535799741745e-06

Training epoch-87 batch-29
Running loss of epoch-87 batch-29 = 8.747447282075882e-07

Training epoch-87 batch-30
Running loss of epoch-87 batch-30 = 1.1273659765720367e-06

Training epoch-87 batch-31
Running loss of epoch-87 batch-31 = 9.646173566579819e-07

Training epoch-87 batch-32
Running loss of epoch-87 batch-32 = 1.7886050045490265e-06

Training epoch-87 batch-33
Running loss of epoch-87 batch-33 = 1.375097781419754e-06

Training epoch-87 batch-34
Running loss of epoch-87 batch-34 = 1.7166603356599808e-06

Training epoch-87 batch-35
Running loss of epoch-87 batch-35 = 1.1727679520845413e-06

Training epoch-87 batch-36
Running loss of epoch-87 batch-36 = 1.5248078852891922e-06

Training epoch-87 batch-37
Running loss of epoch-87 batch-37 = 1.085689291357994e-06

Training epoch-87 batch-38
Running loss of epoch-87 batch-38 = 1.4004763215780258e-06

Training epoch-87 batch-39
Running loss of epoch-87 batch-39 = 1.2158416211605072e-06

Training epoch-87 batch-40
Running loss of epoch-87 batch-40 = 1.2093223631381989e-06

Training epoch-87 batch-41
Running loss of epoch-87 batch-41 = 1.83936208486557e-06

Training epoch-87 batch-42
Running loss of epoch-87 batch-42 = 1.359032467007637e-06

Training epoch-87 batch-43
Running loss of epoch-87 batch-43 = 1.357169821858406e-06

Training epoch-87 batch-44
Running loss of epoch-87 batch-44 = 1.5897676348686218e-06

Training epoch-87 batch-45
Running loss of epoch-87 batch-45 = 9.85804945230484e-07

Training epoch-87 batch-46
Running loss of epoch-87 batch-46 = 1.9420403987169266e-06

Training epoch-87 batch-47
Running loss of epoch-87 batch-47 = 1.2705568224191666e-06

Training epoch-87 batch-48
Running loss of epoch-87 batch-48 = 1.316191628575325e-06

Training epoch-87 batch-49
Running loss of epoch-87 batch-49 = 1.0456424206495285e-06

Training epoch-87 batch-50
Running loss of epoch-87 batch-50 = 1.44285149872303e-06

Training epoch-87 batch-51
Running loss of epoch-87 batch-51 = 1.8642749637365341e-06

Training epoch-87 batch-52
Running loss of epoch-87 batch-52 = 8.188653737306595e-07

Training epoch-87 batch-53
Running loss of epoch-87 batch-53 = 1.1546071618795395e-06

Training epoch-87 batch-54
Running loss of epoch-87 batch-54 = 1.1220108717679977e-06

Training epoch-87 batch-55
Running loss of epoch-87 batch-55 = 1.4360994100570679e-06

Training epoch-87 batch-56
Running loss of epoch-87 batch-56 = 1.3611279428005219e-06

Training epoch-87 batch-57
Running loss of epoch-87 batch-57 = 1.4880206435918808e-06

Training epoch-87 batch-58
Running loss of epoch-87 batch-58 = 1.4901161193847656e-06

Training epoch-87 batch-59
Running loss of epoch-87 batch-59 = 1.4880206435918808e-06

Training epoch-87 batch-60
Running loss of epoch-87 batch-60 = 1.5147961676120758e-06

Training epoch-87 batch-61
Running loss of epoch-87 batch-61 = 1.1343508958816528e-06

Training epoch-87 batch-62
Running loss of epoch-87 batch-62 = 1.1508818715810776e-06

Training epoch-87 batch-63
Running loss of epoch-87 batch-63 = 1.5583354979753494e-06

Training epoch-87 batch-64
Running loss of epoch-87 batch-64 = 1.328764483332634e-06

Training epoch-87 batch-65
Running loss of epoch-87 batch-65 = 1.3147946447134018e-06

Training epoch-87 batch-66
Running loss of epoch-87 batch-66 = 1.5692785382270813e-06

Training epoch-87 batch-67
Running loss of epoch-87 batch-67 = 1.1033844202756882e-06

Training epoch-87 batch-68
Running loss of epoch-87 batch-68 = 1.4917459338903427e-06

Training epoch-87 batch-69
Running loss of epoch-87 batch-69 = 2.5534536689519882e-06

Training epoch-87 batch-70
Running loss of epoch-87 batch-70 = 1.2177042663097382e-06

Training epoch-87 batch-71
Running loss of epoch-87 batch-71 = 8.377246558666229e-07

Training epoch-87 batch-72
Running loss of epoch-87 batch-72 = 1.1171214282512665e-06

Training epoch-87 batch-73
Running loss of epoch-87 batch-73 = 1.1397060006856918e-06

Training epoch-87 batch-74
Running loss of epoch-87 batch-74 = 1.57160684466362e-06

Training epoch-87 batch-75
Running loss of epoch-87 batch-75 = 9.706709533929825e-07

Training epoch-87 batch-76
Running loss of epoch-87 batch-76 = 1.139473170042038e-06

Training epoch-87 batch-77
Running loss of epoch-87 batch-77 = 9.671784937381744e-07

Training epoch-87 batch-78
Running loss of epoch-87 batch-78 = 1.1848751455545425e-06

Training epoch-87 batch-79
Running loss of epoch-87 batch-79 = 1.4058314263820648e-06

Training epoch-87 batch-80
Running loss of epoch-87 batch-80 = 1.112697646021843e-06

Training epoch-87 batch-81
Running loss of epoch-87 batch-81 = 1.716194674372673e-06

Training epoch-87 batch-82
Running loss of epoch-87 batch-82 = 1.976964995265007e-06

Training epoch-87 batch-83
Running loss of epoch-87 batch-83 = 1.8228311091661453e-06

Training epoch-87 batch-84
Running loss of epoch-87 batch-84 = 8.135102689266205e-07

Training epoch-87 batch-85
Running loss of epoch-87 batch-85 = 1.2880191206932068e-06

Training epoch-87 batch-86
Running loss of epoch-87 batch-86 = 1.6202684491872787e-06

Training epoch-87 batch-87
Running loss of epoch-87 batch-87 = 2.258922904729843e-06

Training epoch-87 batch-88
Running loss of epoch-87 batch-88 = 1.537846401333809e-06

Training epoch-87 batch-89
Running loss of epoch-87 batch-89 = 1.1529773473739624e-06

Training epoch-87 batch-90
Running loss of epoch-87 batch-90 = 9.809155017137527e-07

Training epoch-87 batch-91
Running loss of epoch-87 batch-91 = 1.1478550732135773e-06

Training epoch-87 batch-92
Running loss of epoch-87 batch-92 = 1.8866267055273056e-06

Training epoch-87 batch-93
Running loss of epoch-87 batch-93 = 1.7404090613126755e-06

Training epoch-87 batch-94
Running loss of epoch-87 batch-94 = 1.2624077498912811e-06

Training epoch-87 batch-95
Running loss of epoch-87 batch-95 = 1.171370968222618e-06

Training epoch-87 batch-96
Running loss of epoch-87 batch-96 = 1.4512334018945694e-06

Training epoch-87 batch-97
Running loss of epoch-87 batch-97 = 1.5280675143003464e-06

Training epoch-87 batch-98
Running loss of epoch-87 batch-98 = 1.3355165719985962e-06

Training epoch-87 batch-99
Running loss of epoch-87 batch-99 = 1.3294629752635956e-06

Training epoch-87 batch-100
Running loss of epoch-87 batch-100 = 1.6409903764724731e-06

Training epoch-87 batch-101
Running loss of epoch-87 batch-101 = 1.4496035873889923e-06

Training epoch-87 batch-102
Running loss of epoch-87 batch-102 = 1.6330741345882416e-06

Training epoch-87 batch-103
Running loss of epoch-87 batch-103 = 1.0621733963489532e-06

Training epoch-87 batch-104
Running loss of epoch-87 batch-104 = 1.769978553056717e-06

Training epoch-87 batch-105
Running loss of epoch-87 batch-105 = 1.3557728379964828e-06

Training epoch-87 batch-106
Running loss of epoch-87 batch-106 = 1.4852266758680344e-06

Training epoch-87 batch-107
Running loss of epoch-87 batch-107 = 1.2456439435482025e-06

Training epoch-87 batch-108
Running loss of epoch-87 batch-108 = 1.6058329492807388e-06

Training epoch-87 batch-109
Running loss of epoch-87 batch-109 = 1.3455282896757126e-06

Training epoch-87 batch-110
Running loss of epoch-87 batch-110 = 7.899943739175797e-07

Training epoch-87 batch-111
Running loss of epoch-87 batch-111 = 2.3795291781425476e-06

Training epoch-87 batch-112
Running loss of epoch-87 batch-112 = 1.6891863197088242e-06

Training epoch-87 batch-113
Running loss of epoch-87 batch-113 = 8.738134056329727e-07

Training epoch-87 batch-114
Running loss of epoch-87 batch-114 = 8.051283657550812e-07

Training epoch-87 batch-115
Running loss of epoch-87 batch-115 = 1.2204982340335846e-06

Training epoch-87 batch-116
Running loss of epoch-87 batch-116 = 1.2398231774568558e-06

Training epoch-87 batch-117
Running loss of epoch-87 batch-117 = 9.280629456043243e-07

Training epoch-87 batch-118
Running loss of epoch-87 batch-118 = 1.4672987163066864e-06

Training epoch-87 batch-119
Running loss of epoch-87 batch-119 = 1.2246891856193542e-06

Training epoch-87 batch-120
Running loss of epoch-87 batch-120 = 1.1867377907037735e-06

Training epoch-87 batch-121
Running loss of epoch-87 batch-121 = 9.185168892145157e-07

Training epoch-87 batch-122
Running loss of epoch-87 batch-122 = 1.6863923519849777e-06

Training epoch-87 batch-123
Running loss of epoch-87 batch-123 = 1.8433202058076859e-06

Training epoch-87 batch-124
Running loss of epoch-87 batch-124 = 1.1741649359464645e-06

Training epoch-87 batch-125
Running loss of epoch-87 batch-125 = 1.485925167798996e-06

Training epoch-87 batch-126
Running loss of epoch-87 batch-126 = 1.5224795788526535e-06

Training epoch-87 batch-127
Running loss of epoch-87 batch-127 = 1.34296715259552e-06

Training epoch-87 batch-128
Running loss of epoch-87 batch-128 = 1.6335397958755493e-06

Training epoch-87 batch-129
Running loss of epoch-87 batch-129 = 1.5473924577236176e-06

Training epoch-87 batch-130
Running loss of epoch-87 batch-130 = 1.986278221011162e-06

Training epoch-87 batch-131
Running loss of epoch-87 batch-131 = 1.194886863231659e-06

Training epoch-87 batch-132
Running loss of epoch-87 batch-132 = 8.9290551841259e-07

Training epoch-87 batch-133
Running loss of epoch-87 batch-133 = 1.0402873158454895e-06

Training epoch-87 batch-134
Running loss of epoch-87 batch-134 = 1.1743977665901184e-06

Training epoch-87 batch-135
Running loss of epoch-87 batch-135 = 1.4903489500284195e-06

Training epoch-87 batch-136
Running loss of epoch-87 batch-136 = 1.7348211258649826e-06

Training epoch-87 batch-137
Running loss of epoch-87 batch-137 = 1.2211967259645462e-06

Training epoch-87 batch-138
Running loss of epoch-87 batch-138 = 8.696224540472031e-07

Training epoch-87 batch-139
Running loss of epoch-87 batch-139 = 1.1192169040441513e-06

Training epoch-87 batch-140
Running loss of epoch-87 batch-140 = 1.698499545454979e-06

Training epoch-87 batch-141
Running loss of epoch-87 batch-141 = 1.278705894947052e-06

Training epoch-87 batch-142
Running loss of epoch-87 batch-142 = 1.5099067240953445e-06

Training epoch-87 batch-143
Running loss of epoch-87 batch-143 = 1.62515789270401e-06

Training epoch-87 batch-144
Running loss of epoch-87 batch-144 = 9.25036147236824e-07

Training epoch-87 batch-145
Running loss of epoch-87 batch-145 = 1.5695113688707352e-06

Training epoch-87 batch-146
Running loss of epoch-87 batch-146 = 1.0747462511062622e-06

Training epoch-87 batch-147
Running loss of epoch-87 batch-147 = 1.534121111035347e-06

Training epoch-87 batch-148
Running loss of epoch-87 batch-148 = 1.1699739843606949e-06

Training epoch-87 batch-149
Running loss of epoch-87 batch-149 = 1.1401716619729996e-06

Training epoch-87 batch-150
Running loss of epoch-87 batch-150 = 8.961651474237442e-07

Training epoch-87 batch-151
Running loss of epoch-87 batch-151 = 1.1874362826347351e-06

Training epoch-87 batch-152
Running loss of epoch-87 batch-152 = 9.955838322639465e-07

Training epoch-87 batch-153
Running loss of epoch-87 batch-153 = 1.485925167798996e-06

Training epoch-87 batch-154
Running loss of epoch-87 batch-154 = 9.208451956510544e-07

Training epoch-87 batch-155
Running loss of epoch-87 batch-155 = 1.57160684466362e-06

Training epoch-87 batch-156
Running loss of epoch-87 batch-156 = 1.3294629752635956e-06

Training epoch-87 batch-157
Running loss of epoch-87 batch-157 = 8.314847946166992e-06

Finished training epoch-87.



Average train loss at epoch-87 = 1.353028416633606e-06

Started Evaluation

Average val loss at epoch-87 = 3.2934397913907705

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-88


Training epoch-88 batch-1
Running loss of epoch-88 batch-1 = 1.634005457162857e-06

Training epoch-88 batch-2
Running loss of epoch-88 batch-2 = 1.5415716916322708e-06

Training epoch-88 batch-3
Running loss of epoch-88 batch-3 = 1.4798715710639954e-06

Training epoch-88 batch-4
Running loss of epoch-88 batch-4 = 1.0319054126739502e-06

Training epoch-88 batch-5
Running loss of epoch-88 batch-5 = 2.5425106287002563e-06

Training epoch-88 batch-6
Running loss of epoch-88 batch-6 = 1.7017591744661331e-06

Training epoch-88 batch-7
Running loss of epoch-88 batch-7 = 1.4370307326316833e-06

Training epoch-88 batch-8
Running loss of epoch-88 batch-8 = 1.7550773918628693e-06

Training epoch-88 batch-9
Running loss of epoch-88 batch-9 = 1.7369166016578674e-06

Training epoch-88 batch-10
Running loss of epoch-88 batch-10 = 1.7567072063684464e-06

Training epoch-88 batch-11
Running loss of epoch-88 batch-11 = 9.37609001994133e-07

Training epoch-88 batch-12
Running loss of epoch-88 batch-12 = 9.024515748023987e-07

Training epoch-88 batch-13
Running loss of epoch-88 batch-13 = 1.5371479094028473e-06

Training epoch-88 batch-14
Running loss of epoch-88 batch-14 = 1.8132850527763367e-06

Training epoch-88 batch-15
Running loss of epoch-88 batch-15 = 9.485520422458649e-07

Training epoch-88 batch-16
Running loss of epoch-88 batch-16 = 1.550186425447464e-06

Training epoch-88 batch-17
Running loss of epoch-88 batch-17 = 1.1990778148174286e-06

Training epoch-88 batch-18
Running loss of epoch-88 batch-18 = 1.28219835460186e-06

Training epoch-88 batch-19
Running loss of epoch-88 batch-19 = 1.542968675494194e-06

Training epoch-88 batch-20
Running loss of epoch-88 batch-20 = 9.282957762479782e-07

Training epoch-88 batch-21
Running loss of epoch-88 batch-21 = 1.205597072839737e-06

Training epoch-88 batch-22
Running loss of epoch-88 batch-22 = 1.437729224562645e-06

Training epoch-88 batch-23
Running loss of epoch-88 batch-23 = 1.4414545148611069e-06

Training epoch-88 batch-24
Running loss of epoch-88 batch-24 = 1.8014106899499893e-06

Training epoch-88 batch-25
Running loss of epoch-88 batch-25 = 1.328764483332634e-06

Training epoch-88 batch-26
Running loss of epoch-88 batch-26 = 1.1264346539974213e-06

Training epoch-88 batch-27
Running loss of epoch-88 batch-27 = 1.0414514690637589e-06

Training epoch-88 batch-28
Running loss of epoch-88 batch-28 = 1.0649673640727997e-06

Training epoch-88 batch-29
Running loss of epoch-88 batch-29 = 1.6551930457353592e-06

Training epoch-88 batch-30
Running loss of epoch-88 batch-30 = 2.169515937566757e-06

Training epoch-88 batch-31
Running loss of epoch-88 batch-31 = 9.203795343637466e-07

Training epoch-88 batch-32
Running loss of epoch-88 batch-32 = 1.341337338089943e-06

Training epoch-88 batch-33
Running loss of epoch-88 batch-33 = 1.6202684491872787e-06

Training epoch-88 batch-34
Running loss of epoch-88 batch-34 = 1.084059476852417e-06

Training epoch-88 batch-35
Running loss of epoch-88 batch-35 = 1.7313286662101746e-06

Training epoch-88 batch-36
Running loss of epoch-88 batch-36 = 1.1601950973272324e-06

Training epoch-88 batch-37
Running loss of epoch-88 batch-37 = 1.1434312909841537e-06

Training epoch-88 batch-38
Running loss of epoch-88 batch-38 = 1.5166588127613068e-06

Training epoch-88 batch-39
Running loss of epoch-88 batch-39 = 1.39651820063591e-06

Training epoch-88 batch-40
Running loss of epoch-88 batch-40 = 1.0712537914514542e-06

Training epoch-88 batch-41
Running loss of epoch-88 batch-41 = 1.3229437172412872e-06

Training epoch-88 batch-42
Running loss of epoch-88 batch-42 = 1.1264346539974213e-06

Training epoch-88 batch-43
Running loss of epoch-88 batch-43 = 1.2186355888843536e-06

Training epoch-88 batch-44
Running loss of epoch-88 batch-44 = 1.0575167834758759e-06

Training epoch-88 batch-45
Running loss of epoch-88 batch-45 = 1.7473939806222916e-06

Training epoch-88 batch-46
Running loss of epoch-88 batch-46 = 1.0530930012464523e-06

Training epoch-88 batch-47
Running loss of epoch-88 batch-47 = 1.7583370208740234e-06

Training epoch-88 batch-48
Running loss of epoch-88 batch-48 = 1.3564713299274445e-06

Training epoch-88 batch-49
Running loss of epoch-88 batch-49 = 1.1175870895385742e-06

Training epoch-88 batch-50
Running loss of epoch-88 batch-50 = 1.6149133443832397e-06

Training epoch-88 batch-51
Running loss of epoch-88 batch-51 = 1.200009137392044e-06

Training epoch-88 batch-52
Running loss of epoch-88 batch-52 = 1.0547228157520294e-06

Training epoch-88 batch-53
Running loss of epoch-88 batch-53 = 1.3813842087984085e-06

Training epoch-88 batch-54
Running loss of epoch-88 batch-54 = 9.851064532995224e-07

Training epoch-88 batch-55
Running loss of epoch-88 batch-55 = 1.1601950973272324e-06

Training epoch-88 batch-56
Running loss of epoch-88 batch-56 = 1.2058299034833908e-06

Training epoch-88 batch-57
Running loss of epoch-88 batch-57 = 9.909272193908691e-07

Training epoch-88 batch-58
Running loss of epoch-88 batch-58 = 1.8200371414422989e-06

Training epoch-88 batch-59
Running loss of epoch-88 batch-59 = 1.0079238563776016e-06

Training epoch-88 batch-60
Running loss of epoch-88 batch-60 = 1.957174390554428e-06

Training epoch-88 batch-61
Running loss of epoch-88 batch-61 = 1.4728866517543793e-06

Training epoch-88 batch-62
Running loss of epoch-88 batch-62 = 1.5583354979753494e-06

Training epoch-88 batch-63
Running loss of epoch-88 batch-63 = 1.5096738934516907e-06

Training epoch-88 batch-64
Running loss of epoch-88 batch-64 = 1.0419171303510666e-06

Training epoch-88 batch-65
Running loss of epoch-88 batch-65 = 1.382315531373024e-06

Training epoch-88 batch-66
Running loss of epoch-88 batch-66 = 1.1080410331487656e-06

Training epoch-88 batch-67
Running loss of epoch-88 batch-67 = 1.6533304005861282e-06

Training epoch-88 batch-68
Running loss of epoch-88 batch-68 = 1.5683472156524658e-06

Training epoch-88 batch-69
Running loss of epoch-88 batch-69 = 8.633360266685486e-07

Training epoch-88 batch-70
Running loss of epoch-88 batch-70 = 1.2700911611318588e-06

Training epoch-88 batch-71
Running loss of epoch-88 batch-71 = 1.153675839304924e-06

Training epoch-88 batch-72
Running loss of epoch-88 batch-72 = 1.508975401520729e-06

Training epoch-88 batch-73
Running loss of epoch-88 batch-73 = 9.541399776935577e-07

Training epoch-88 batch-74
Running loss of epoch-88 batch-74 = 1.775333657860756e-06

Training epoch-88 batch-75
Running loss of epoch-88 batch-75 = 1.057283952832222e-06

Training epoch-88 batch-76
Running loss of epoch-88 batch-76 = 7.82310962677002e-07

Training epoch-88 batch-77
Running loss of epoch-88 batch-77 = 8.826609700918198e-07

Training epoch-88 batch-78
Running loss of epoch-88 batch-78 = 1.3795215636491776e-06

Training epoch-88 batch-79
Running loss of epoch-88 batch-79 = 1.200241968035698e-06

Training epoch-88 batch-80
Running loss of epoch-88 batch-80 = 1.7401762306690216e-06

Training epoch-88 batch-81
Running loss of epoch-88 batch-81 = 6.996560841798782e-07

Training epoch-88 batch-82
Running loss of epoch-88 batch-82 = 1.4565885066986084e-06

Training epoch-88 batch-83
Running loss of epoch-88 batch-83 = 9.944196790456772e-07

Training epoch-88 batch-84
Running loss of epoch-88 batch-84 = 1.4211982488632202e-06

Training epoch-88 batch-85
Running loss of epoch-88 batch-85 = 1.914333552122116e-06

Training epoch-88 batch-86
Running loss of epoch-88 batch-86 = 1.4558900147676468e-06

Training epoch-88 batch-87
Running loss of epoch-88 batch-87 = 1.5532132238149643e-06

Training epoch-88 batch-88
Running loss of epoch-88 batch-88 = 1.5285331755876541e-06

Training epoch-88 batch-89
Running loss of epoch-88 batch-89 = 1.634005457162857e-06

Training epoch-88 batch-90
Running loss of epoch-88 batch-90 = 1.3208482414484024e-06

Training epoch-88 batch-91
Running loss of epoch-88 batch-91 = 8.910428732633591e-07

Training epoch-88 batch-92
Running loss of epoch-88 batch-92 = 1.1150259524583817e-06

Training epoch-88 batch-93
Running loss of epoch-88 batch-93 = 1.7043203115463257e-06

Training epoch-88 batch-94
Running loss of epoch-88 batch-94 = 8.966308087110519e-07

Training epoch-88 batch-95
Running loss of epoch-88 batch-95 = 1.107342541217804e-06

Training epoch-88 batch-96
Running loss of epoch-88 batch-96 = 9.848736226558685e-07

Training epoch-88 batch-97
Running loss of epoch-88 batch-97 = 9.66014340519905e-07

Training epoch-88 batch-98
Running loss of epoch-88 batch-98 = 1.6889534890651703e-06

Training epoch-88 batch-99
Running loss of epoch-88 batch-99 = 2.0996667444705963e-06

Training epoch-88 batch-100
Running loss of epoch-88 batch-100 = 1.214444637298584e-06

Training epoch-88 batch-101
Running loss of epoch-88 batch-101 = 1.2700911611318588e-06

Training epoch-88 batch-102
Running loss of epoch-88 batch-102 = 1.4908146113157272e-06

Training epoch-88 batch-103
Running loss of epoch-88 batch-103 = 1.1045485734939575e-06

Training epoch-88 batch-104
Running loss of epoch-88 batch-104 = 7.669441401958466e-07

Training epoch-88 batch-105
Running loss of epoch-88 batch-105 = 1.244945451617241e-06

Training epoch-88 batch-106
Running loss of epoch-88 batch-106 = 1.2158416211605072e-06

Training epoch-88 batch-107
Running loss of epoch-88 batch-107 = 1.5599653124809265e-06

Training epoch-88 batch-108
Running loss of epoch-88 batch-108 = 1.0188668966293335e-06

Training epoch-88 batch-109
Running loss of epoch-88 batch-109 = 1.1869706213474274e-06

Training epoch-88 batch-110
Running loss of epoch-88 batch-110 = 1.6312114894390106e-06

Training epoch-88 batch-111
Running loss of epoch-88 batch-111 = 1.1378433555364609e-06

Training epoch-88 batch-112
Running loss of epoch-88 batch-112 = 9.424984455108643e-07

Training epoch-88 batch-113
Running loss of epoch-88 batch-113 = 1.1471565812826157e-06

Training epoch-88 batch-114
Running loss of epoch-88 batch-114 = 1.2849923223257065e-06

Training epoch-88 batch-115
Running loss of epoch-88 batch-115 = 9.613577276468277e-07

Training epoch-88 batch-116
Running loss of epoch-88 batch-116 = 9.639188647270203e-07

Training epoch-88 batch-117
Running loss of epoch-88 batch-117 = 1.816311851143837e-06

Training epoch-88 batch-118
Running loss of epoch-88 batch-118 = 1.3082753866910934e-06

Training epoch-88 batch-119
Running loss of epoch-88 batch-119 = 2.0260922610759735e-06

Training epoch-88 batch-120
Running loss of epoch-88 batch-120 = 1.0759104043245316e-06

Training epoch-88 batch-121
Running loss of epoch-88 batch-121 = 1.0952353477478027e-06

Training epoch-88 batch-122
Running loss of epoch-88 batch-122 = 1.1485535651445389e-06

Training epoch-88 batch-123
Running loss of epoch-88 batch-123 = 1.5746336430311203e-06

Training epoch-88 batch-124
Running loss of epoch-88 batch-124 = 1.6279518604278564e-06

Training epoch-88 batch-125
Running loss of epoch-88 batch-125 = 1.0025687515735626e-06

Training epoch-88 batch-126
Running loss of epoch-88 batch-126 = 1.4766119420528412e-06

Training epoch-88 batch-127
Running loss of epoch-88 batch-127 = 7.592607289552689e-07

Training epoch-88 batch-128
Running loss of epoch-88 batch-128 = 8.314382284879684e-07

Training epoch-88 batch-129
Running loss of epoch-88 batch-129 = 1.0193325579166412e-06

Training epoch-88 batch-130
Running loss of epoch-88 batch-130 = 1.9976869225502014e-06

Training epoch-88 batch-131
Running loss of epoch-88 batch-131 = 9.168870747089386e-07

Training epoch-88 batch-132
Running loss of epoch-88 batch-132 = 1.0570511221885681e-06

Training epoch-88 batch-133
Running loss of epoch-88 batch-133 = 1.1364463716745377e-06

Training epoch-88 batch-134
Running loss of epoch-88 batch-134 = 1.7588026821613312e-06

Training epoch-88 batch-135
Running loss of epoch-88 batch-135 = 1.2149102985858917e-06

Training epoch-88 batch-136
Running loss of epoch-88 batch-136 = 1.6528647392988205e-06

Training epoch-88 batch-137
Running loss of epoch-88 batch-137 = 1.234235242009163e-06

Training epoch-88 batch-138
Running loss of epoch-88 batch-138 = 1.0598450899124146e-06

Training epoch-88 batch-139
Running loss of epoch-88 batch-139 = 1.0298099368810654e-06

Training epoch-88 batch-140
Running loss of epoch-88 batch-140 = 9.480863809585571e-07

Training epoch-88 batch-141
Running loss of epoch-88 batch-141 = 1.0845251381397247e-06

Training epoch-88 batch-142
Running loss of epoch-88 batch-142 = 9.757932275533676e-07

Training epoch-88 batch-143
Running loss of epoch-88 batch-143 = 1.2922100722789764e-06

Training epoch-88 batch-144
Running loss of epoch-88 batch-144 = 1.6870908439159393e-06

Training epoch-88 batch-145
Running loss of epoch-88 batch-145 = 1.8530990928411484e-06

Training epoch-88 batch-146
Running loss of epoch-88 batch-146 = 1.073349267244339e-06

Training epoch-88 batch-147
Running loss of epoch-88 batch-147 = 1.3068784028291702e-06

Training epoch-88 batch-148
Running loss of epoch-88 batch-148 = 8.435454219579697e-07

Training epoch-88 batch-149
Running loss of epoch-88 batch-149 = 9.387731552124023e-07

Training epoch-88 batch-150
Running loss of epoch-88 batch-150 = 1.7241109162569046e-06

Training epoch-88 batch-151
Running loss of epoch-88 batch-151 = 1.4230608940124512e-06

Training epoch-88 batch-152
Running loss of epoch-88 batch-152 = 1.3201497495174408e-06

Training epoch-88 batch-153
Running loss of epoch-88 batch-153 = 1.5187542885541916e-06

Training epoch-88 batch-154
Running loss of epoch-88 batch-154 = 1.6670674085617065e-06

Training epoch-88 batch-155
Running loss of epoch-88 batch-155 = 1.7709098756313324e-06

Training epoch-88 batch-156
Running loss of epoch-88 batch-156 = 1.112464815378189e-06

Training epoch-88 batch-157
Running loss of epoch-88 batch-157 = 4.123896360397339e-06

Finished training epoch-88.



Average train loss at epoch-88 = 1.327890157699585e-06

Started Evaluation

Average val loss at epoch-88 = 3.296122267058021

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-89


Training epoch-89 batch-1
Running loss of epoch-89 batch-1 = 1.3343524187803268e-06

Training epoch-89 batch-2
Running loss of epoch-89 batch-2 = 9.185168892145157e-07

Training epoch-89 batch-3
Running loss of epoch-89 batch-3 = 1.0940711945295334e-06

Training epoch-89 batch-4
Running loss of epoch-89 batch-4 = 1.7634592950344086e-06

Training epoch-89 batch-5
Running loss of epoch-89 batch-5 = 1.28219835460186e-06

Training epoch-89 batch-6
Running loss of epoch-89 batch-6 = 1.1634547263383865e-06

Training epoch-89 batch-7
Running loss of epoch-89 batch-7 = 1.3122335076332092e-06

Training epoch-89 batch-8
Running loss of epoch-89 batch-8 = 1.2617092579603195e-06

Training epoch-89 batch-9
Running loss of epoch-89 batch-9 = 1.1275988072156906e-06

Training epoch-89 batch-10
Running loss of epoch-89 batch-10 = 9.771902114152908e-07

Training epoch-89 batch-11
Running loss of epoch-89 batch-11 = 9.755603969097137e-07

Training epoch-89 batch-12
Running loss of epoch-89 batch-12 = 1.1846423149108887e-06

Training epoch-89 batch-13
Running loss of epoch-89 batch-13 = 1.4861579984426498e-06

Training epoch-89 batch-14
Running loss of epoch-89 batch-14 = 1.0826624929904938e-06

Training epoch-89 batch-15
Running loss of epoch-89 batch-15 = 1.4742836356163025e-06

Training epoch-89 batch-16
Running loss of epoch-89 batch-16 = 1.62515789270401e-06

Training epoch-89 batch-17
Running loss of epoch-89 batch-17 = 1.961132511496544e-06

Training epoch-89 batch-18
Running loss of epoch-89 batch-18 = 1.3005919754505157e-06

Training epoch-89 batch-19
Running loss of epoch-89 batch-19 = 1.287553459405899e-06

Training epoch-89 batch-20
Running loss of epoch-89 batch-20 = 1.5560071915388107e-06

Training epoch-89 batch-21
Running loss of epoch-89 batch-21 = 9.776558727025986e-07

Training epoch-89 batch-22
Running loss of epoch-89 batch-22 = 1.2244563549757004e-06

Training epoch-89 batch-23
Running loss of epoch-89 batch-23 = 1.878710463643074e-06

Training epoch-89 batch-24
Running loss of epoch-89 batch-24 = 1.7727725207805634e-06

Training epoch-89 batch-25
Running loss of epoch-89 batch-25 = 1.1746305972337723e-06

Training epoch-89 batch-26
Running loss of epoch-89 batch-26 = 1.7660204321146011e-06

Training epoch-89 batch-27
Running loss of epoch-89 batch-27 = 1.216307282447815e-06

Training epoch-89 batch-28
Running loss of epoch-89 batch-28 = 1.7441343516111374e-06

Training epoch-89 batch-29
Running loss of epoch-89 batch-29 = 1.6225967556238174e-06

Training epoch-89 batch-30
Running loss of epoch-89 batch-30 = 1.1632218956947327e-06

Training epoch-89 batch-31
Running loss of epoch-89 batch-31 = 1.578591763973236e-06

Training epoch-89 batch-32
Running loss of epoch-89 batch-32 = 1.4016404747962952e-06

Training epoch-89 batch-33
Running loss of epoch-89 batch-33 = 9.690411388874054e-07

Training epoch-89 batch-34
Running loss of epoch-89 batch-34 = 1.1802185326814651e-06

Training epoch-89 batch-35
Running loss of epoch-89 batch-35 = 1.2498348951339722e-06

Training epoch-89 batch-36
Running loss of epoch-89 batch-36 = 1.4265533536672592e-06

Training epoch-89 batch-37
Running loss of epoch-89 batch-37 = 1.352047547698021e-06

Training epoch-89 batch-38
Running loss of epoch-89 batch-38 = 1.1275988072156906e-06

Training epoch-89 batch-39
Running loss of epoch-89 batch-39 = 1.2940727174282074e-06

Training epoch-89 batch-40
Running loss of epoch-89 batch-40 = 1.3746321201324463e-06

Training epoch-89 batch-41
Running loss of epoch-89 batch-41 = 1.7343554645776749e-06

Training epoch-89 batch-42
Running loss of epoch-89 batch-42 = 1.6542617231607437e-06

Training epoch-89 batch-43
Running loss of epoch-89 batch-43 = 9.587965905666351e-07

Training epoch-89 batch-44
Running loss of epoch-89 batch-44 = 9.846407920122147e-07

Training epoch-89 batch-45
Running loss of epoch-89 batch-45 = 1.2991949915885925e-06

Training epoch-89 batch-46
Running loss of epoch-89 batch-46 = 2.0684674382209778e-06

Training epoch-89 batch-47
Running loss of epoch-89 batch-47 = 1.6186386346817017e-06

Training epoch-89 batch-48
Running loss of epoch-89 batch-48 = 9.560026228427887e-07

Training epoch-89 batch-49
Running loss of epoch-89 batch-49 = 1.212581992149353e-06

Training epoch-89 batch-50
Running loss of epoch-89 batch-50 = 1.8423888832330704e-06

Training epoch-89 batch-51
Running loss of epoch-89 batch-51 = 1.543201506137848e-06

Training epoch-89 batch-52
Running loss of epoch-89 batch-52 = 1.0409858077764511e-06

Training epoch-89 batch-53
Running loss of epoch-89 batch-53 = 1.16787850856781e-06

Training epoch-89 batch-54
Running loss of epoch-89 batch-54 = 1.3557728379964828e-06

Training epoch-89 batch-55
Running loss of epoch-89 batch-55 = 2.0461156964302063e-06

Training epoch-89 batch-56
Running loss of epoch-89 batch-56 = 6.738118827342987e-07

Training epoch-89 batch-57
Running loss of epoch-89 batch-57 = 7.450580596923828e-07

Training epoch-89 batch-58
Running loss of epoch-89 batch-58 = 1.4766119420528412e-06

Training epoch-89 batch-59
Running loss of epoch-89 batch-59 = 1.346692442893982e-06

Training epoch-89 batch-60
Running loss of epoch-89 batch-60 = 1.5068799257278442e-06

Training epoch-89 batch-61
Running loss of epoch-89 batch-61 = 1.1117663234472275e-06

Training epoch-89 batch-62
Running loss of epoch-89 batch-62 = 1.2468080967664719e-06

Training epoch-89 batch-63
Running loss of epoch-89 batch-63 = 7.816124707460403e-07

Training epoch-89 batch-64
Running loss of epoch-89 batch-64 = 1.3201497495174408e-06

Training epoch-89 batch-65
Running loss of epoch-89 batch-65 = 1.1820811778306961e-06

Training epoch-89 batch-66
Running loss of epoch-89 batch-66 = 1.3695098459720612e-06

Training epoch-89 batch-67
Running loss of epoch-89 batch-67 = 5.35743311047554e-07

Training epoch-89 batch-68
Running loss of epoch-89 batch-68 = 1.4142133295536041e-06

Training epoch-89 batch-69
Running loss of epoch-89 batch-69 = 1.242849975824356e-06

Training epoch-89 batch-70
Running loss of epoch-89 batch-70 = 1.7082784324884415e-06

Training epoch-89 batch-71
Running loss of epoch-89 batch-71 = 1.503853127360344e-06

Training epoch-89 batch-72
Running loss of epoch-89 batch-72 = 1.6605481505393982e-06

Training epoch-89 batch-73
Running loss of epoch-89 batch-73 = 1.714564859867096e-06

Training epoch-89 batch-74
Running loss of epoch-89 batch-74 = 1.7855782061815262e-06

Training epoch-89 batch-75
Running loss of epoch-89 batch-75 = 1.2370292097330093e-06

Training epoch-89 batch-76
Running loss of epoch-89 batch-76 = 1.1865049600601196e-06

Training epoch-89 batch-77
Running loss of epoch-89 batch-77 = 1.5799887478351593e-06

Training epoch-89 batch-78
Running loss of epoch-89 batch-78 = 1.2540258467197418e-06

Training epoch-89 batch-79
Running loss of epoch-89 batch-79 = 1.291045919060707e-06

Training epoch-89 batch-80
Running loss of epoch-89 batch-80 = 1.5974510461091995e-06

Training epoch-89 batch-81
Running loss of epoch-89 batch-81 = 1.6302801668643951e-06

Training epoch-89 batch-82
Running loss of epoch-89 batch-82 = 1.4961697161197662e-06

Training epoch-89 batch-83
Running loss of epoch-89 batch-83 = 9.299255907535553e-07

Training epoch-89 batch-84
Running loss of epoch-89 batch-84 = 1.0370276868343353e-06

Training epoch-89 batch-85
Running loss of epoch-89 batch-85 = 8.586794137954712e-07

Training epoch-89 batch-86
Running loss of epoch-89 batch-86 = 1.2880191206932068e-06

Training epoch-89 batch-87
Running loss of epoch-89 batch-87 = 1.5122350305318832e-06

Training epoch-89 batch-88
Running loss of epoch-89 batch-88 = 1.037726178765297e-06

Training epoch-89 batch-89
Running loss of epoch-89 batch-89 = 1.2475065886974335e-06

Training epoch-89 batch-90
Running loss of epoch-89 batch-90 = 1.0181684046983719e-06

Training epoch-89 batch-91
Running loss of epoch-89 batch-91 = 1.1741649359464645e-06

Training epoch-89 batch-92
Running loss of epoch-89 batch-92 = 1.10710971057415e-06

Training epoch-89 batch-93
Running loss of epoch-89 batch-93 = 1.1886004358530045e-06

Training epoch-89 batch-94
Running loss of epoch-89 batch-94 = 1.373235136270523e-06

Training epoch-89 batch-95
Running loss of epoch-89 batch-95 = 9.781215339899063e-07

Training epoch-89 batch-96
Running loss of epoch-89 batch-96 = 1.3792887330055237e-06

Training epoch-89 batch-97
Running loss of epoch-89 batch-97 = 4.80329617857933e-07

Training epoch-89 batch-98
Running loss of epoch-89 batch-98 = 1.2069940567016602e-06

Training epoch-89 batch-99
Running loss of epoch-89 batch-99 = 1.2048985809087753e-06

Training epoch-89 batch-100
Running loss of epoch-89 batch-100 = 1.351814717054367e-06

Training epoch-89 batch-101
Running loss of epoch-89 batch-101 = 1.2281816452741623e-06

Training epoch-89 batch-102
Running loss of epoch-89 batch-102 = 1.3138633221387863e-06

Training epoch-89 batch-103
Running loss of epoch-89 batch-103 = 1.4440156519412994e-06

Training epoch-89 batch-104
Running loss of epoch-89 batch-104 = 1.386040821671486e-06

Training epoch-89 batch-105
Running loss of epoch-89 batch-105 = 1.5906989574432373e-06

Training epoch-89 batch-106
Running loss of epoch-89 batch-106 = 8.912757039070129e-07

Training epoch-89 batch-107
Running loss of epoch-89 batch-107 = 1.484062522649765e-06

Training epoch-89 batch-108
Running loss of epoch-89 batch-108 = 1.44285149872303e-06

Training epoch-89 batch-109
Running loss of epoch-89 batch-109 = 8.980277925729752e-07

Training epoch-89 batch-110
Running loss of epoch-89 batch-110 = 1.1860392987728119e-06

Training epoch-89 batch-111
Running loss of epoch-89 batch-111 = 7.783528417348862e-07

Training epoch-89 batch-112
Running loss of epoch-89 batch-112 = 9.348150342702866e-07

Training epoch-89 batch-113
Running loss of epoch-89 batch-113 = 1.212814822793007e-06

Training epoch-89 batch-114
Running loss of epoch-89 batch-114 = 1.4316756278276443e-06

Training epoch-89 batch-115
Running loss of epoch-89 batch-115 = 1.485925167798996e-06

Training epoch-89 batch-116
Running loss of epoch-89 batch-116 = 9.355135262012482e-07

Training epoch-89 batch-117
Running loss of epoch-89 batch-117 = 7.664784789085388e-07

Training epoch-89 batch-118
Running loss of epoch-89 batch-118 = 1.628650352358818e-06

Training epoch-89 batch-119
Running loss of epoch-89 batch-119 = 1.0421499609947205e-06

Training epoch-89 batch-120
Running loss of epoch-89 batch-120 = 1.4649704098701477e-06

Training epoch-89 batch-121
Running loss of epoch-89 batch-121 = 1.0977964848279953e-06

Training epoch-89 batch-122
Running loss of epoch-89 batch-122 = 1.5792902559041977e-06

Training epoch-89 batch-123
Running loss of epoch-89 batch-123 = 1.461012288928032e-06

Training epoch-89 batch-124
Running loss of epoch-89 batch-124 = 1.6398262232542038e-06

Training epoch-89 batch-125
Running loss of epoch-89 batch-125 = 1.378823071718216e-06

Training epoch-89 batch-126
Running loss of epoch-89 batch-126 = 1.7005950212478638e-06

Training epoch-89 batch-127
Running loss of epoch-89 batch-127 = 1.2936070561408997e-06

Training epoch-89 batch-128
Running loss of epoch-89 batch-128 = 1.1569354683160782e-06

Training epoch-89 batch-129
Running loss of epoch-89 batch-129 = 1.4351680874824524e-06

Training epoch-89 batch-130
Running loss of epoch-89 batch-130 = 1.5406403690576553e-06

Training epoch-89 batch-131
Running loss of epoch-89 batch-131 = 9.185168892145157e-07

Training epoch-89 batch-132
Running loss of epoch-89 batch-132 = 1.6225967556238174e-06

Training epoch-89 batch-133
Running loss of epoch-89 batch-133 = 1.5541445463895798e-06

Training epoch-89 batch-134
Running loss of epoch-89 batch-134 = 1.33574940264225e-06

Training epoch-89 batch-135
Running loss of epoch-89 batch-135 = 1.2863893061876297e-06

Training epoch-89 batch-136
Running loss of epoch-89 batch-136 = 1.4777760952711105e-06

Training epoch-89 batch-137
Running loss of epoch-89 batch-137 = 1.0838266462087631e-06

Training epoch-89 batch-138
Running loss of epoch-89 batch-138 = 1.2586824595928192e-06

Training epoch-89 batch-139
Running loss of epoch-89 batch-139 = 1.4123506844043732e-06

Training epoch-89 batch-140
Running loss of epoch-89 batch-140 = 9.252689778804779e-07

Training epoch-89 batch-141
Running loss of epoch-89 batch-141 = 9.87667590379715e-07

Training epoch-89 batch-142
Running loss of epoch-89 batch-142 = 9.702052921056747e-07

Training epoch-89 batch-143
Running loss of epoch-89 batch-143 = 9.89297404885292e-07

Training epoch-89 batch-144
Running loss of epoch-89 batch-144 = 1.339474692940712e-06

Training epoch-89 batch-145
Running loss of epoch-89 batch-145 = 9.273644536733627e-07

Training epoch-89 batch-146
Running loss of epoch-89 batch-146 = 1.1259689927101135e-06

Training epoch-89 batch-147
Running loss of epoch-89 batch-147 = 1.107342541217804e-06

Training epoch-89 batch-148
Running loss of epoch-89 batch-148 = 1.4514662325382233e-06

Training epoch-89 batch-149
Running loss of epoch-89 batch-149 = 1.3276003301143646e-06

Training epoch-89 batch-150
Running loss of epoch-89 batch-150 = 1.0062940418720245e-06

Training epoch-89 batch-151
Running loss of epoch-89 batch-151 = 1.7711427062749863e-06

Training epoch-89 batch-152
Running loss of epoch-89 batch-152 = 1.7315614968538284e-06

Training epoch-89 batch-153
Running loss of epoch-89 batch-153 = 1.4370307326316833e-06

Training epoch-89 batch-154
Running loss of epoch-89 batch-154 = 1.3227108865976334e-06

Training epoch-89 batch-155
Running loss of epoch-89 batch-155 = 1.4670658856630325e-06

Training epoch-89 batch-156
Running loss of epoch-89 batch-156 = 1.109205186367035e-06

Training epoch-89 batch-157
Running loss of epoch-89 batch-157 = 1.385807991027832e-05

Finished training epoch-89.



Average train loss at epoch-89 = 1.3207092881202697e-06

Started Evaluation

Average val loss at epoch-89 = 3.2988136755792716

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-90


Training epoch-90 batch-1
Running loss of epoch-90 batch-1 = 1.0200310498476028e-06

Training epoch-90 batch-2
Running loss of epoch-90 batch-2 = 6.835907697677612e-07

Training epoch-90 batch-3
Running loss of epoch-90 batch-3 = 8.870847523212433e-07

Training epoch-90 batch-4
Running loss of epoch-90 batch-4 = 1.394888386130333e-06

Training epoch-90 batch-5
Running loss of epoch-90 batch-5 = 1.3504177331924438e-06

Training epoch-90 batch-6
Running loss of epoch-90 batch-6 = 1.8281862139701843e-06

Training epoch-90 batch-7
Running loss of epoch-90 batch-7 = 1.5045516192913055e-06

Training epoch-90 batch-8
Running loss of epoch-90 batch-8 = 1.3257376849651337e-06

Training epoch-90 batch-9
Running loss of epoch-90 batch-9 = 8.621718734502792e-07

Training epoch-90 batch-10
Running loss of epoch-90 batch-10 = 1.6691628843545914e-06

Training epoch-90 batch-11
Running loss of epoch-90 batch-11 = 1.6149133443832397e-06

Training epoch-90 batch-12
Running loss of epoch-90 batch-12 = 1.2256205081939697e-06

Training epoch-90 batch-13
Running loss of epoch-90 batch-13 = 1.194886863231659e-06

Training epoch-90 batch-14
Running loss of epoch-90 batch-14 = 1.4973338693380356e-06

Training epoch-90 batch-15
Running loss of epoch-90 batch-15 = 7.762573659420013e-07

Training epoch-90 batch-16
Running loss of epoch-90 batch-16 = 1.4677643775939941e-06

Training epoch-90 batch-17
Running loss of epoch-90 batch-17 = 1.5022233128547668e-06

Training epoch-90 batch-18
Running loss of epoch-90 batch-18 = 7.757917046546936e-07

Training epoch-90 batch-19
Running loss of epoch-90 batch-19 = 9.939540177583694e-07

Training epoch-90 batch-20
Running loss of epoch-90 batch-20 = 9.075738489627838e-07

Training epoch-90 batch-21
Running loss of epoch-90 batch-21 = 8.82195308804512e-07

Training epoch-90 batch-22
Running loss of epoch-90 batch-22 = 1.4763791114091873e-06

Training epoch-90 batch-23
Running loss of epoch-90 batch-23 = 1.4973338693380356e-06

Training epoch-90 batch-24
Running loss of epoch-90 batch-24 = 7.348135113716125e-07

Training epoch-90 batch-25
Running loss of epoch-90 batch-25 = 1.4291144907474518e-06

Training epoch-90 batch-26
Running loss of epoch-90 batch-26 = 1.0530930012464523e-06

Training epoch-90 batch-27
Running loss of epoch-90 batch-27 = 1.3830140233039856e-06

Training epoch-90 batch-28
Running loss of epoch-90 batch-28 = 1.1150259524583817e-06

Training epoch-90 batch-29
Running loss of epoch-90 batch-29 = 1.7371494323015213e-06

Training epoch-90 batch-30
Running loss of epoch-90 batch-30 = 9.557697921991348e-07

Training epoch-90 batch-31
Running loss of epoch-90 batch-31 = 1.6815029084682465e-06

Training epoch-90 batch-32
Running loss of epoch-90 batch-32 = 1.9210856407880783e-06

Training epoch-90 batch-33
Running loss of epoch-90 batch-33 = 1.0200310498476028e-06

Training epoch-90 batch-34
Running loss of epoch-90 batch-34 = 1.5916302800178528e-06

Training epoch-90 batch-35
Running loss of epoch-90 batch-35 = 1.1015217751264572e-06

Training epoch-90 batch-36
Running loss of epoch-90 batch-36 = 1.4889519661664963e-06

Training epoch-90 batch-37
Running loss of epoch-90 batch-37 = 1.103850081562996e-06

Training epoch-90 batch-38
Running loss of epoch-90 batch-38 = 1.4707911759614944e-06

Training epoch-90 batch-39
Running loss of epoch-90 batch-39 = 9.778887033462524e-07

Training epoch-90 batch-40
Running loss of epoch-90 batch-40 = 1.7248094081878662e-06

Training epoch-90 batch-41
Running loss of epoch-90 batch-41 = 1.2384261935949326e-06

Training epoch-90 batch-42
Running loss of epoch-90 batch-42 = 1.0654330253601074e-06

Training epoch-90 batch-43
Running loss of epoch-90 batch-43 = 1.3476237654685974e-06

Training epoch-90 batch-44
Running loss of epoch-90 batch-44 = 1.241220161318779e-06

Training epoch-90 batch-45
Running loss of epoch-90 batch-45 = 1.253560185432434e-06

Training epoch-90 batch-46
Running loss of epoch-90 batch-46 = 1.0132789611816406e-06

Training epoch-90 batch-47
Running loss of epoch-90 batch-47 = 1.3667158782482147e-06

Training epoch-90 batch-48
Running loss of epoch-90 batch-48 = 1.0351650416851044e-06

Training epoch-90 batch-49
Running loss of epoch-90 batch-49 = 1.1960510164499283e-06

Training epoch-90 batch-50
Running loss of epoch-90 batch-50 = 1.014210283756256e-06

Training epoch-90 batch-51
Running loss of epoch-90 batch-51 = 1.6952399164438248e-06

Training epoch-90 batch-52
Running loss of epoch-90 batch-52 = 8.905772119760513e-07

Training epoch-90 batch-53
Running loss of epoch-90 batch-53 = 2.109445631504059e-06

Training epoch-90 batch-54
Running loss of epoch-90 batch-54 = 1.212814822793007e-06

Training epoch-90 batch-55
Running loss of epoch-90 batch-55 = 6.703194230794907e-07

Training epoch-90 batch-56
Running loss of epoch-90 batch-56 = 1.164386048913002e-06

Training epoch-90 batch-57
Running loss of epoch-90 batch-57 = 1.3529788702726364e-06

Training epoch-90 batch-58
Running loss of epoch-90 batch-58 = 1.0062940418720245e-06

Training epoch-90 batch-59
Running loss of epoch-90 batch-59 = 1.6312114894390106e-06

Training epoch-90 batch-60
Running loss of epoch-90 batch-60 = 1.073349267244339e-06

Training epoch-90 batch-61
Running loss of epoch-90 batch-61 = 8.242204785346985e-07

Training epoch-90 batch-62
Running loss of epoch-90 batch-62 = 1.2652017176151276e-06

Training epoch-90 batch-63
Running loss of epoch-90 batch-63 = 8.933711796998978e-07

Training epoch-90 batch-64
Running loss of epoch-90 batch-64 = 1.3834796845912933e-06

Training epoch-90 batch-65
Running loss of epoch-90 batch-65 = 1.2596137821674347e-06

Training epoch-90 batch-66
Running loss of epoch-90 batch-66 = 1.2889504432678223e-06

Training epoch-90 batch-67
Running loss of epoch-90 batch-67 = 9.133946150541306e-07

Training epoch-90 batch-68
Running loss of epoch-90 batch-68 = 1.0230578482151031e-06

Training epoch-90 batch-69
Running loss of epoch-90 batch-69 = 1.2970995157957077e-06

Training epoch-90 batch-70
Running loss of epoch-90 batch-70 = 1.1718366295099258e-06

Training epoch-90 batch-71
Running loss of epoch-90 batch-71 = 9.387731552124023e-07

Training epoch-90 batch-72
Running loss of epoch-90 batch-72 = 2.2756867110729218e-06

Training epoch-90 batch-73
Running loss of epoch-90 batch-73 = 9.881332516670227e-07

Training epoch-90 batch-74
Running loss of epoch-90 batch-74 = 8.211936801671982e-07

Training epoch-90 batch-75
Running loss of epoch-90 batch-75 = 1.5676487237215042e-06

Training epoch-90 batch-76
Running loss of epoch-90 batch-76 = 1.6530975699424744e-06

Training epoch-90 batch-77
Running loss of epoch-90 batch-77 = 1.2470409274101257e-06

Training epoch-90 batch-78
Running loss of epoch-90 batch-78 = 1.4191027730703354e-06

Training epoch-90 batch-79
Running loss of epoch-90 batch-79 = 1.6046687960624695e-06

Training epoch-90 batch-80
Running loss of epoch-90 batch-80 = 9.459909051656723e-07

Training epoch-90 batch-81
Running loss of epoch-90 batch-81 = 1.3825483620166779e-06

Training epoch-90 batch-82
Running loss of epoch-90 batch-82 = 1.098494976758957e-06

Training epoch-90 batch-83
Running loss of epoch-90 batch-83 = 1.2656673789024353e-06

Training epoch-90 batch-84
Running loss of epoch-90 batch-84 = 1.5152618288993835e-06

Training epoch-90 batch-85
Running loss of epoch-90 batch-85 = 9.762588888406754e-07

Training epoch-90 batch-86
Running loss of epoch-90 batch-86 = 1.4153774827718735e-06

Training epoch-90 batch-87
Running loss of epoch-90 batch-87 = 9.35746356844902e-07

Training epoch-90 batch-88
Running loss of epoch-90 batch-88 = 9.0012326836586e-07

Training epoch-90 batch-89
Running loss of epoch-90 batch-89 = 1.3140961527824402e-06

Training epoch-90 batch-90
Running loss of epoch-90 batch-90 = 1.2372620403766632e-06

Training epoch-90 batch-91
Running loss of epoch-90 batch-91 = 1.7727725207805634e-06

Training epoch-90 batch-92
Running loss of epoch-90 batch-92 = 1.6230624169111252e-06

Training epoch-90 batch-93
Running loss of epoch-90 batch-93 = 9.080395102500916e-07

Training epoch-90 batch-94
Running loss of epoch-90 batch-94 = 9.452924132347107e-07

Training epoch-90 batch-95
Running loss of epoch-90 batch-95 = 1.4882534742355347e-06

Training epoch-90 batch-96
Running loss of epoch-90 batch-96 = 1.3594981282949448e-06

Training epoch-90 batch-97
Running loss of epoch-90 batch-97 = 1.4114193618297577e-06

Training epoch-90 batch-98
Running loss of epoch-90 batch-98 = 1.2356322258710861e-06

Training epoch-90 batch-99
Running loss of epoch-90 batch-99 = 1.4204997569322586e-06

Training epoch-90 batch-100
Running loss of epoch-90 batch-100 = 1.8279533833265305e-06

Training epoch-90 batch-101
Running loss of epoch-90 batch-101 = 1.9085127860307693e-06

Training epoch-90 batch-102
Running loss of epoch-90 batch-102 = 1.7879065126180649e-06

Training epoch-90 batch-103
Running loss of epoch-90 batch-103 = 1.2682285159826279e-06

Training epoch-90 batch-104
Running loss of epoch-90 batch-104 = 1.4437828212976456e-06

Training epoch-90 batch-105
Running loss of epoch-90 batch-105 = 1.4230608940124512e-06

Training epoch-90 batch-106
Running loss of epoch-90 batch-106 = 9.37609001994133e-07

Training epoch-90 batch-107
Running loss of epoch-90 batch-107 = 1.412816345691681e-06

Training epoch-90 batch-108
Running loss of epoch-90 batch-108 = 1.3264361768960953e-06

Training epoch-90 batch-109
Running loss of epoch-90 batch-109 = 1.4149118214845657e-06

Training epoch-90 batch-110
Running loss of epoch-90 batch-110 = 9.35746356844902e-07

Training epoch-90 batch-111
Running loss of epoch-90 batch-111 = 1.0719522833824158e-06

Training epoch-90 batch-112
Running loss of epoch-90 batch-112 = 1.3292301446199417e-06

Training epoch-90 batch-113
Running loss of epoch-90 batch-113 = 1.480337232351303e-06

Training epoch-90 batch-114
Running loss of epoch-90 batch-114 = 1.2265518307685852e-06

Training epoch-90 batch-115
Running loss of epoch-90 batch-115 = 1.3890676200389862e-06

Training epoch-90 batch-116
Running loss of epoch-90 batch-116 = 9.71602275967598e-07

Training epoch-90 batch-117
Running loss of epoch-90 batch-117 = 1.3757962733507156e-06

Training epoch-90 batch-118
Running loss of epoch-90 batch-118 = 1.4011748135089874e-06

Training epoch-90 batch-119
Running loss of epoch-90 batch-119 = 1.2691598385572433e-06

Training epoch-90 batch-120
Running loss of epoch-90 batch-120 = 1.5264376997947693e-06

Training epoch-90 batch-121
Running loss of epoch-90 batch-121 = 1.305481418967247e-06

Training epoch-90 batch-122
Running loss of epoch-90 batch-122 = 8.919741958379745e-07

Training epoch-90 batch-123
Running loss of epoch-90 batch-123 = 1.4968682080507278e-06

Training epoch-90 batch-124
Running loss of epoch-90 batch-124 = 1.1289957910776138e-06

Training epoch-90 batch-125
Running loss of epoch-90 batch-125 = 1.317821443080902e-06

Training epoch-90 batch-126
Running loss of epoch-90 batch-126 = 1.4863908290863037e-06

Training epoch-90 batch-127
Running loss of epoch-90 batch-127 = 1.6959384083747864e-06

Training epoch-90 batch-128
Running loss of epoch-90 batch-128 = 1.2123491615056992e-06

Training epoch-90 batch-129
Running loss of epoch-90 batch-129 = 1.8926803022623062e-06

Training epoch-90 batch-130
Running loss of epoch-90 batch-130 = 1.2223608791828156e-06

Training epoch-90 batch-131
Running loss of epoch-90 batch-131 = 1.7371494323015213e-06

Training epoch-90 batch-132
Running loss of epoch-90 batch-132 = 1.0523945093154907e-06

Training epoch-90 batch-133
Running loss of epoch-90 batch-133 = 9.390059858560562e-07

Training epoch-90 batch-134
Running loss of epoch-90 batch-134 = 1.1886004358530045e-06

Training epoch-90 batch-135
Running loss of epoch-90 batch-135 = 1.4558900147676468e-06

Training epoch-90 batch-136
Running loss of epoch-90 batch-136 = 1.2468080967664719e-06

Training epoch-90 batch-137
Running loss of epoch-90 batch-137 = 1.2149102985858917e-06

Training epoch-90 batch-138
Running loss of epoch-90 batch-138 = 1.212814822793007e-06

Training epoch-90 batch-139
Running loss of epoch-90 batch-139 = 1.237494871020317e-06

Training epoch-90 batch-140
Running loss of epoch-90 batch-140 = 8.291099220514297e-07

Training epoch-90 batch-141
Running loss of epoch-90 batch-141 = 1.3457611203193665e-06

Training epoch-90 batch-142
Running loss of epoch-90 batch-142 = 1.657288521528244e-06

Training epoch-90 batch-143
Running loss of epoch-90 batch-143 = 1.3350509107112885e-06

Training epoch-90 batch-144
Running loss of epoch-90 batch-144 = 1.4707911759614944e-06

Training epoch-90 batch-145
Running loss of epoch-90 batch-145 = 1.3131648302078247e-06

Training epoch-90 batch-146
Running loss of epoch-90 batch-146 = 1.3078097254037857e-06

Training epoch-90 batch-147
Running loss of epoch-90 batch-147 = 1.2575183063745499e-06

Training epoch-90 batch-148
Running loss of epoch-90 batch-148 = 8.956994861364365e-07

Training epoch-90 batch-149
Running loss of epoch-90 batch-149 = 1.6712583601474762e-06

Training epoch-90 batch-150
Running loss of epoch-90 batch-150 = 1.25030055642128e-06

Training epoch-90 batch-151
Running loss of epoch-90 batch-151 = 1.5709083527326584e-06

Training epoch-90 batch-152
Running loss of epoch-90 batch-152 = 1.1827796697616577e-06

Training epoch-90 batch-153
Running loss of epoch-90 batch-153 = 1.1210795491933823e-06

Training epoch-90 batch-154
Running loss of epoch-90 batch-154 = 1.2307427823543549e-06

Training epoch-90 batch-155
Running loss of epoch-90 batch-155 = 1.4724209904670715e-06

Training epoch-90 batch-156
Running loss of epoch-90 batch-156 = 1.523410901427269e-06

Training epoch-90 batch-157
Running loss of epoch-90 batch-157 = 4.854053258895874e-06

Finished training epoch-90.



Average train loss at epoch-90 = 1.2909188866615295e-06

Started Evaluation

Average val loss at epoch-90 = 3.303706142463182

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.50 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-91


Training epoch-91 batch-1
Running loss of epoch-91 batch-1 = 1.6854610294103622e-06

Training epoch-91 batch-2
Running loss of epoch-91 batch-2 = 8.309725672006607e-07

Training epoch-91 batch-3
Running loss of epoch-91 batch-3 = 1.4628749340772629e-06

Training epoch-91 batch-4
Running loss of epoch-91 batch-4 = 1.3581011444330215e-06

Training epoch-91 batch-5
Running loss of epoch-91 batch-5 = 2.0759180188179016e-06

Training epoch-91 batch-6
Running loss of epoch-91 batch-6 = 8.086208254098892e-07

Training epoch-91 batch-7
Running loss of epoch-91 batch-7 = 1.0265503078699112e-06

Training epoch-91 batch-8
Running loss of epoch-91 batch-8 = 1.267995685338974e-06

Training epoch-91 batch-9
Running loss of epoch-91 batch-9 = 1.3650860637426376e-06

Training epoch-91 batch-10
Running loss of epoch-91 batch-10 = 1.9453000277280807e-06

Training epoch-91 batch-11
Running loss of epoch-91 batch-11 = 1.6954727470874786e-06

Training epoch-91 batch-12
Running loss of epoch-91 batch-12 = 1.6123522073030472e-06

Training epoch-91 batch-13
Running loss of epoch-91 batch-13 = 1.1334195733070374e-06

Training epoch-91 batch-14
Running loss of epoch-91 batch-14 = 9.923242032527924e-07

Training epoch-91 batch-15
Running loss of epoch-91 batch-15 = 1.6523990780115128e-06

Training epoch-91 batch-16
Running loss of epoch-91 batch-16 = 1.8249265849590302e-06

Training epoch-91 batch-17
Running loss of epoch-91 batch-17 = 9.145587682723999e-07

Training epoch-91 batch-18
Running loss of epoch-91 batch-18 = 8.831266313791275e-07

Training epoch-91 batch-19
Running loss of epoch-91 batch-19 = 1.3497192412614822e-06

Training epoch-91 batch-20
Running loss of epoch-91 batch-20 = 1.1727679520845413e-06

Training epoch-91 batch-21
Running loss of epoch-91 batch-21 = 1.6528647392988205e-06

Training epoch-91 batch-22
Running loss of epoch-91 batch-22 = 1.4076940715312958e-06

Training epoch-91 batch-23
Running loss of epoch-91 batch-23 = 1.3327226042747498e-06

Training epoch-91 batch-24
Running loss of epoch-91 batch-24 = 1.0121148079633713e-06

Training epoch-91 batch-25
Running loss of epoch-91 batch-25 = 1.285923644900322e-06

Training epoch-91 batch-26
Running loss of epoch-91 batch-26 = 1.4968682080507278e-06

Training epoch-91 batch-27
Running loss of epoch-91 batch-27 = 1.4994293451309204e-06

Training epoch-91 batch-28
Running loss of epoch-91 batch-28 = 1.1471565812826157e-06

Training epoch-91 batch-29
Running loss of epoch-91 batch-29 = 1.107342541217804e-06

Training epoch-91 batch-30
Running loss of epoch-91 batch-30 = 9.350478649139404e-07

Training epoch-91 batch-31
Running loss of epoch-91 batch-31 = 1.3855751603841782e-06

Training epoch-91 batch-32
Running loss of epoch-91 batch-32 = 1.1348165571689606e-06

Training epoch-91 batch-33
Running loss of epoch-91 batch-33 = 1.1585652828216553e-06

Training epoch-91 batch-34
Running loss of epoch-91 batch-34 = 1.7061829566955566e-06

Training epoch-91 batch-35
Running loss of epoch-91 batch-35 = 9.14325937628746e-07

Training epoch-91 batch-36
Running loss of epoch-91 batch-36 = 1.2936070561408997e-06

Training epoch-91 batch-37
Running loss of epoch-91 batch-37 = 9.154900908470154e-07

Training epoch-91 batch-38
Running loss of epoch-91 batch-38 = 1.6987323760986328e-06

Training epoch-91 batch-39
Running loss of epoch-91 batch-39 = 1.0181684046983719e-06

Training epoch-91 batch-40
Running loss of epoch-91 batch-40 = 1.02166086435318e-06

Training epoch-91 batch-41
Running loss of epoch-91 batch-41 = 1.4593824744224548e-06

Training epoch-91 batch-42
Running loss of epoch-91 batch-42 = 9.259674698114395e-07

Training epoch-91 batch-43
Running loss of epoch-91 batch-43 = 8.819624781608582e-07

Training epoch-91 batch-44
Running loss of epoch-91 batch-44 = 1.24308280646801e-06

Training epoch-91 batch-45
Running loss of epoch-91 batch-45 = 6.903428584337234e-07

Training epoch-91 batch-46
Running loss of epoch-91 batch-46 = 1.5005934983491898e-06

Training epoch-91 batch-47
Running loss of epoch-91 batch-47 = 1.1308584362268448e-06

Training epoch-91 batch-48
Running loss of epoch-91 batch-48 = 1.373467966914177e-06

Training epoch-91 batch-49
Running loss of epoch-91 batch-49 = 1.2356322258710861e-06

Training epoch-91 batch-50
Running loss of epoch-91 batch-50 = 2.2419262677431107e-06

Training epoch-91 batch-51
Running loss of epoch-91 batch-51 = 1.6076955944299698e-06

Training epoch-91 batch-52
Running loss of epoch-91 batch-52 = 1.1385418474674225e-06

Training epoch-91 batch-53
Running loss of epoch-91 batch-53 = 7.024500519037247e-07

Training epoch-91 batch-54
Running loss of epoch-91 batch-54 = 1.2889504432678223e-06

Training epoch-91 batch-55
Running loss of epoch-91 batch-55 = 1.0668300092220306e-06

Training epoch-91 batch-56
Running loss of epoch-91 batch-56 = 1.6898848116397858e-06

Training epoch-91 batch-57
Running loss of epoch-91 batch-57 = 1.2621749192476273e-06

Training epoch-91 batch-58
Running loss of epoch-91 batch-58 = 9.774230420589447e-07

Training epoch-91 batch-59
Running loss of epoch-91 batch-59 = 1.2700911611318588e-06

Training epoch-91 batch-60
Running loss of epoch-91 batch-60 = 1.1869706213474274e-06

Training epoch-91 batch-61
Running loss of epoch-91 batch-61 = 1.448439434170723e-06

Training epoch-91 batch-62
Running loss of epoch-91 batch-62 = 1.2835953384637833e-06

Training epoch-91 batch-63
Running loss of epoch-91 batch-63 = 1.5222467482089996e-06

Training epoch-91 batch-64
Running loss of epoch-91 batch-64 = 1.1045485734939575e-06

Training epoch-91 batch-65
Running loss of epoch-91 batch-65 = 1.7439015209674835e-06

Training epoch-91 batch-66
Running loss of epoch-91 batch-66 = 1.1951196938753128e-06

Training epoch-91 batch-67
Running loss of epoch-91 batch-67 = 1.4579854905605316e-06

Training epoch-91 batch-68
Running loss of epoch-91 batch-68 = 8.139759302139282e-07

Training epoch-91 batch-69
Running loss of epoch-91 batch-69 = 1.1324882507324219e-06

Training epoch-91 batch-70
Running loss of epoch-91 batch-70 = 1.1723022907972336e-06

Training epoch-91 batch-71
Running loss of epoch-91 batch-71 = 1.2081582099199295e-06

Training epoch-91 batch-72
Running loss of epoch-91 batch-72 = 1.0100193321704865e-06

Training epoch-91 batch-73
Running loss of epoch-91 batch-73 = 1.5096738934516907e-06

Training epoch-91 batch-74
Running loss of epoch-91 batch-74 = 9.262003004550934e-07

Training epoch-91 batch-75
Running loss of epoch-91 batch-75 = 8.130446076393127e-07

Training epoch-91 batch-76
Running loss of epoch-91 batch-76 = 1.2419186532497406e-06

Training epoch-91 batch-77
Running loss of epoch-91 batch-77 = 1.7832498997449875e-06

Training epoch-91 batch-78
Running loss of epoch-91 batch-78 = 1.1387746781110764e-06

Training epoch-91 batch-79
Running loss of epoch-91 batch-79 = 1.1872034519910812e-06

Training epoch-91 batch-80
Running loss of epoch-91 batch-80 = 1.3136304914951324e-06

Training epoch-91 batch-81
Running loss of epoch-91 batch-81 = 1.334119588136673e-06

Training epoch-91 batch-82
Running loss of epoch-91 batch-82 = 1.1366792023181915e-06

Training epoch-91 batch-83
Running loss of epoch-91 batch-83 = 1.1441297829151154e-06

Training epoch-91 batch-84
Running loss of epoch-91 batch-84 = 1.391395926475525e-06

Training epoch-91 batch-85
Running loss of epoch-91 batch-85 = 1.5925616025924683e-06

Training epoch-91 batch-86
Running loss of epoch-91 batch-86 = 1.1404044926166534e-06

Training epoch-91 batch-87
Running loss of epoch-91 batch-87 = 1.4335382729768753e-06

Training epoch-91 batch-88
Running loss of epoch-91 batch-88 = 1.3345852494239807e-06

Training epoch-91 batch-89
Running loss of epoch-91 batch-89 = 1.257285475730896e-06

Training epoch-91 batch-90
Running loss of epoch-91 batch-90 = 1.4710240066051483e-06

Training epoch-91 batch-91
Running loss of epoch-91 batch-91 = 9.527429938316345e-07

Training epoch-91 batch-92
Running loss of epoch-91 batch-92 = 1.0782387107610703e-06

Training epoch-91 batch-93
Running loss of epoch-91 batch-93 = 1.1727679520845413e-06

Training epoch-91 batch-94
Running loss of epoch-91 batch-94 = 9.986106306314468e-07

Training epoch-91 batch-95
Running loss of epoch-91 batch-95 = 1.2142118066549301e-06

Training epoch-91 batch-96
Running loss of epoch-91 batch-96 = 1.1995434761047363e-06

Training epoch-91 batch-97
Running loss of epoch-91 batch-97 = 9.585637599229813e-07

Training epoch-91 batch-98
Running loss of epoch-91 batch-98 = 1.4726538211107254e-06

Training epoch-91 batch-99
Running loss of epoch-91 batch-99 = 1.3262033462524414e-06

Training epoch-91 batch-100
Running loss of epoch-91 batch-100 = 8.605420589447021e-07

Training epoch-91 batch-101
Running loss of epoch-91 batch-101 = 1.4614779502153397e-06

Training epoch-91 batch-102
Running loss of epoch-91 batch-102 = 1.1818483471870422e-06

Training epoch-91 batch-103
Running loss of epoch-91 batch-103 = 1.5767291188240051e-06

Training epoch-91 batch-104
Running loss of epoch-91 batch-104 = 1.4686957001686096e-06

Training epoch-91 batch-105
Running loss of epoch-91 batch-105 = 1.196516677737236e-06

Training epoch-91 batch-106
Running loss of epoch-91 batch-106 = 1.6149133443832397e-06

Training epoch-91 batch-107
Running loss of epoch-91 batch-107 = 1.6244594007730484e-06

Training epoch-91 batch-108
Running loss of epoch-91 batch-108 = 1.0384246706962585e-06

Training epoch-91 batch-109
Running loss of epoch-91 batch-109 = 1.1045485734939575e-06

Training epoch-91 batch-110
Running loss of epoch-91 batch-110 = 8.98260623216629e-07

Training epoch-91 batch-111
Running loss of epoch-91 batch-111 = 9.98144969344139e-07

Training epoch-91 batch-112
Running loss of epoch-91 batch-112 = 1.416075974702835e-06

Training epoch-91 batch-113
Running loss of epoch-91 batch-113 = 9.040813893079758e-07

Training epoch-91 batch-114
Running loss of epoch-91 batch-114 = 7.98376277089119e-07

Training epoch-91 batch-115
Running loss of epoch-91 batch-115 = 1.2638047337532043e-06

Training epoch-91 batch-116
Running loss of epoch-91 batch-116 = 1.1811498552560806e-06

Training epoch-91 batch-117
Running loss of epoch-91 batch-117 = 9.813811630010605e-07

Training epoch-91 batch-118
Running loss of epoch-91 batch-118 = 1.4407560229301453e-06

Training epoch-91 batch-119
Running loss of epoch-91 batch-119 = 1.389533281326294e-06

Training epoch-91 batch-120
Running loss of epoch-91 batch-120 = 1.7702113837003708e-06

Training epoch-91 batch-121
Running loss of epoch-91 batch-121 = 9.834766387939453e-07

Training epoch-91 batch-122
Running loss of epoch-91 batch-122 = 1.123175024986267e-06

Training epoch-91 batch-123
Running loss of epoch-91 batch-123 = 2.2493768483400345e-06

Training epoch-91 batch-124
Running loss of epoch-91 batch-124 = 5.282927304506302e-07

Training epoch-91 batch-125
Running loss of epoch-91 batch-125 = 1.6498379409313202e-06

Training epoch-91 batch-126
Running loss of epoch-91 batch-126 = 9.927898645401e-07

Training epoch-91 batch-127
Running loss of epoch-91 batch-127 = 1.3075768947601318e-06

Training epoch-91 batch-128
Running loss of epoch-91 batch-128 = 1.155305653810501e-06

Training epoch-91 batch-129
Running loss of epoch-91 batch-129 = 8.910428732633591e-07

Training epoch-91 batch-130
Running loss of epoch-91 batch-130 = 1.376960426568985e-06

Training epoch-91 batch-131
Running loss of epoch-91 batch-131 = 1.4463439583778381e-06

Training epoch-91 batch-132
Running loss of epoch-91 batch-132 = 9.140931069850922e-07

Training epoch-91 batch-133
Running loss of epoch-91 batch-133 = 2.0421575754880905e-06

Training epoch-91 batch-134
Running loss of epoch-91 batch-134 = 1.0558869689702988e-06

Training epoch-91 batch-135
Running loss of epoch-91 batch-135 = 1.2458767741918564e-06

Training epoch-91 batch-136
Running loss of epoch-91 batch-136 = 1.0344665497541428e-06

Training epoch-91 batch-137
Running loss of epoch-91 batch-137 = 1.4058314263820648e-06

Training epoch-91 batch-138
Running loss of epoch-91 batch-138 = 1.4069955796003342e-06

Training epoch-91 batch-139
Running loss of epoch-91 batch-139 = 1.5543773770332336e-06

Training epoch-91 batch-140
Running loss of epoch-91 batch-140 = 8.379574865102768e-07

Training epoch-91 batch-141
Running loss of epoch-91 batch-141 = 1.4388933777809143e-06

Training epoch-91 batch-142
Running loss of epoch-91 batch-142 = 1.466134563088417e-06

Training epoch-91 batch-143
Running loss of epoch-91 batch-143 = 1.1189840734004974e-06

Training epoch-91 batch-144
Running loss of epoch-91 batch-144 = 7.846392691135406e-07

Training epoch-91 batch-145
Running loss of epoch-91 batch-145 = 1.0153744369745255e-06

Training epoch-91 batch-146
Running loss of epoch-91 batch-146 = 1.2165401130914688e-06

Training epoch-91 batch-147
Running loss of epoch-91 batch-147 = 9.35746356844902e-07

Training epoch-91 batch-148
Running loss of epoch-91 batch-148 = 1.5594996511936188e-06

Training epoch-91 batch-149
Running loss of epoch-91 batch-149 = 1.4633405953645706e-06

Training epoch-91 batch-150
Running loss of epoch-91 batch-150 = 1.627020537853241e-06

Training epoch-91 batch-151
Running loss of epoch-91 batch-151 = 9.203795343637466e-07

Training epoch-91 batch-152
Running loss of epoch-91 batch-152 = 1.059146597981453e-06

Training epoch-91 batch-153
Running loss of epoch-91 batch-153 = 1.2337695807218552e-06

Training epoch-91 batch-154
Running loss of epoch-91 batch-154 = 1.5248078852891922e-06

Training epoch-91 batch-155
Running loss of epoch-91 batch-155 = 1.5855766832828522e-06

Training epoch-91 batch-156
Running loss of epoch-91 batch-156 = 1.6936101019382477e-06

Training epoch-91 batch-157
Running loss of epoch-91 batch-157 = 4.030764102935791e-06

Finished training epoch-91.



Average train loss at epoch-91 = 1.2716487050056457e-06

Started Evaluation

Average val loss at epoch-91 = 3.3054627870258533

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-92


Training epoch-92 batch-1
Running loss of epoch-92 batch-1 = 1.7364509403705597e-06

Training epoch-92 batch-2
Running loss of epoch-92 batch-2 = 1.2884847819805145e-06

Training epoch-92 batch-3
Running loss of epoch-92 batch-3 = 1.0703224688768387e-06

Training epoch-92 batch-4
Running loss of epoch-92 batch-4 = 1.2426171451807022e-06

Training epoch-92 batch-5
Running loss of epoch-92 batch-5 = 1.2628734111785889e-06

Training epoch-92 batch-6
Running loss of epoch-92 batch-6 = 1.1529773473739624e-06

Training epoch-92 batch-7
Running loss of epoch-92 batch-7 = 1.1580996215343475e-06

Training epoch-92 batch-8
Running loss of epoch-92 batch-8 = 9.699724614620209e-07

Training epoch-92 batch-9
Running loss of epoch-92 batch-9 = 1.5157274901866913e-06

Training epoch-92 batch-10
Running loss of epoch-92 batch-10 = 1.0891817510128021e-06

Training epoch-92 batch-11
Running loss of epoch-92 batch-11 = 1.4042016118764877e-06

Training epoch-92 batch-12
Running loss of epoch-92 batch-12 = 1.0551884770393372e-06

Training epoch-92 batch-13
Running loss of epoch-92 batch-13 = 1.6603153198957443e-06

Training epoch-92 batch-14
Running loss of epoch-92 batch-14 = 1.1830125004053116e-06

Training epoch-92 batch-15
Running loss of epoch-92 batch-15 = 1.1834781616926193e-06

Training epoch-92 batch-16
Running loss of epoch-92 batch-16 = 1.2582167983055115e-06

Training epoch-92 batch-17
Running loss of epoch-92 batch-17 = 8.083879947662354e-07

Training epoch-92 batch-18
Running loss of epoch-92 batch-18 = 1.5369150787591934e-06

Training epoch-92 batch-19
Running loss of epoch-92 batch-19 = 1.8058344721794128e-06

Training epoch-92 batch-20
Running loss of epoch-92 batch-20 = 1.8498394638299942e-06

Training epoch-92 batch-21
Running loss of epoch-92 batch-21 = 2.2496096789836884e-06

Training epoch-92 batch-22
Running loss of epoch-92 batch-22 = 8.540228009223938e-07

Training epoch-92 batch-23
Running loss of epoch-92 batch-23 = 1.082196831703186e-06

Training epoch-92 batch-24
Running loss of epoch-92 batch-24 = 1.6416888684034348e-06

Training epoch-92 batch-25
Running loss of epoch-92 batch-25 = 1.3927929103374481e-06

Training epoch-92 batch-26
Running loss of epoch-92 batch-26 = 9.825453162193298e-07

Training epoch-92 batch-27
Running loss of epoch-92 batch-27 = 1.0326039046049118e-06

Training epoch-92 batch-28
Running loss of epoch-92 batch-28 = 9.19681042432785e-07

Training epoch-92 batch-29
Running loss of epoch-92 batch-29 = 1.1636875569820404e-06

Training epoch-92 batch-30
Running loss of epoch-92 batch-30 = 1.1366792023181915e-06

Training epoch-92 batch-31
Running loss of epoch-92 batch-31 = 9.30391252040863e-07

Training epoch-92 batch-32
Running loss of epoch-92 batch-32 = 1.3925600796937943e-06

Training epoch-92 batch-33
Running loss of epoch-92 batch-33 = 1.1806841939687729e-06

Training epoch-92 batch-34
Running loss of epoch-92 batch-34 = 1.108972355723381e-06

Training epoch-92 batch-35
Running loss of epoch-92 batch-35 = 1.4365650713443756e-06

Training epoch-92 batch-36
Running loss of epoch-92 batch-36 = 1.7408747225999832e-06

Training epoch-92 batch-37
Running loss of epoch-92 batch-37 = 1.0407529771327972e-06

Training epoch-92 batch-38
Running loss of epoch-92 batch-38 = 8.344650268554688e-07

Training epoch-92 batch-39
Running loss of epoch-92 batch-39 = 1.314561814069748e-06

Training epoch-92 batch-40
Running loss of epoch-92 batch-40 = 6.926711648702621e-07

Training epoch-92 batch-41
Running loss of epoch-92 batch-41 = 1.5310943126678467e-06

Training epoch-92 batch-42
Running loss of epoch-92 batch-42 = 1.0917428880929947e-06

Training epoch-92 batch-43
Running loss of epoch-92 batch-43 = 1.8335413187742233e-06

Training epoch-92 batch-44
Running loss of epoch-92 batch-44 = 6.575137376785278e-07

Training epoch-92 batch-45
Running loss of epoch-92 batch-45 = 1.078704372048378e-06

Training epoch-92 batch-46
Running loss of epoch-92 batch-46 = 1.5676487237215042e-06

Training epoch-92 batch-47
Running loss of epoch-92 batch-47 = 1.482199877500534e-06

Training epoch-92 batch-48
Running loss of epoch-92 batch-48 = 1.7397105693817139e-06

Training epoch-92 batch-49
Running loss of epoch-92 batch-49 = 8.26781615614891e-07

Training epoch-92 batch-50
Running loss of epoch-92 batch-50 = 1.6780104488134384e-06

Training epoch-92 batch-51
Running loss of epoch-92 batch-51 = 1.605600118637085e-06

Training epoch-92 batch-52
Running loss of epoch-92 batch-52 = 1.8470454961061478e-06

Training epoch-92 batch-53
Running loss of epoch-92 batch-53 = 9.275972843170166e-07

Training epoch-92 batch-54
Running loss of epoch-92 batch-54 = 1.607462763786316e-06

Training epoch-92 batch-55
Running loss of epoch-92 batch-55 = 6.628688424825668e-07

Training epoch-92 batch-56
Running loss of epoch-92 batch-56 = 1.2775417417287827e-06

Training epoch-92 batch-57
Running loss of epoch-92 batch-57 = 1.1008232831954956e-06

Training epoch-92 batch-58
Running loss of epoch-92 batch-58 = 1.607229933142662e-06

Training epoch-92 batch-59
Running loss of epoch-92 batch-59 = 1.3685785233974457e-06

Training epoch-92 batch-60
Running loss of epoch-92 batch-60 = 6.938353180885315e-07

Training epoch-92 batch-61
Running loss of epoch-92 batch-61 = 1.4510005712509155e-06

Training epoch-92 batch-62
Running loss of epoch-92 batch-62 = 1.3138633221387863e-06

Training epoch-92 batch-63
Running loss of epoch-92 batch-63 = 1.6223639249801636e-06

Training epoch-92 batch-64
Running loss of epoch-92 batch-64 = 1.3282988220453262e-06

Training epoch-92 batch-65
Running loss of epoch-92 batch-65 = 1.2586824595928192e-06

Training epoch-92 batch-66
Running loss of epoch-92 batch-66 = 8.645001798868179e-07

Training epoch-92 batch-67
Running loss of epoch-92 batch-67 = 1.0882504284381866e-06

Training epoch-92 batch-68
Running loss of epoch-92 batch-68 = 9.75094735622406e-07

Training epoch-92 batch-69
Running loss of epoch-92 batch-69 = 7.925555109977722e-07

Training epoch-92 batch-70
Running loss of epoch-92 batch-70 = 9.520445019006729e-07

Training epoch-92 batch-71
Running loss of epoch-92 batch-71 = 1.692911610007286e-06

Training epoch-92 batch-72
Running loss of epoch-92 batch-72 = 1.4915131032466888e-06

Training epoch-92 batch-73
Running loss of epoch-92 batch-73 = 1.4766119420528412e-06

Training epoch-92 batch-74
Running loss of epoch-92 batch-74 = 9.450595825910568e-07

Training epoch-92 batch-75
Running loss of epoch-92 batch-75 = 1.2845266610383987e-06

Training epoch-92 batch-76
Running loss of epoch-92 batch-76 = 1.3471581041812897e-06

Training epoch-92 batch-77
Running loss of epoch-92 batch-77 = 1.2104865163564682e-06

Training epoch-92 batch-78
Running loss of epoch-92 batch-78 = 1.1129304766654968e-06

Training epoch-92 batch-79
Running loss of epoch-92 batch-79 = 1.732492819428444e-06

Training epoch-92 batch-80
Running loss of epoch-92 batch-80 = 1.6081612557172775e-06

Training epoch-92 batch-81
Running loss of epoch-92 batch-81 = 8.170027285814285e-07

Training epoch-92 batch-82
Running loss of epoch-92 batch-82 = 1.155538484454155e-06

Training epoch-92 batch-83
Running loss of epoch-92 batch-83 = 7.213093340396881e-07

Training epoch-92 batch-84
Running loss of epoch-92 batch-84 = 1.753680408000946e-06

Training epoch-92 batch-85
Running loss of epoch-92 batch-85 = 1.2025702744722366e-06

Training epoch-92 batch-86
Running loss of epoch-92 batch-86 = 1.2300442904233932e-06

Training epoch-92 batch-87
Running loss of epoch-92 batch-87 = 7.820781320333481e-07

Training epoch-92 batch-88
Running loss of epoch-92 batch-88 = 8.153729140758514e-07

Training epoch-92 batch-89
Running loss of epoch-92 batch-89 = 1.4293473213911057e-06

Training epoch-92 batch-90
Running loss of epoch-92 batch-90 = 1.1615920811891556e-06

Training epoch-92 batch-91
Running loss of epoch-92 batch-91 = 9.508803486824036e-07

Training epoch-92 batch-92
Running loss of epoch-92 batch-92 = 8.605420589447021e-07

Training epoch-92 batch-93
Running loss of epoch-92 batch-93 = 1.146690919995308e-06

Training epoch-92 batch-94
Running loss of epoch-92 batch-94 = 1.373467966914177e-06

Training epoch-92 batch-95
Running loss of epoch-92 batch-95 = 1.33574940264225e-06

Training epoch-92 batch-96
Running loss of epoch-92 batch-96 = 1.5147961676120758e-06

Training epoch-92 batch-97
Running loss of epoch-92 batch-97 = 1.0977964848279953e-06

Training epoch-92 batch-98
Running loss of epoch-92 batch-98 = 1.101987436413765e-06

Training epoch-92 batch-99
Running loss of epoch-92 batch-99 = 1.6603153198957443e-06

Training epoch-92 batch-100
Running loss of epoch-92 batch-100 = 7.62520357966423e-07

Training epoch-92 batch-101
Running loss of epoch-92 batch-101 = 1.1469237506389618e-06

Training epoch-92 batch-102
Running loss of epoch-92 batch-102 = 1.4370307326316833e-06

Training epoch-92 batch-103
Running loss of epoch-92 batch-103 = 8.873175829648972e-07

Training epoch-92 batch-104
Running loss of epoch-92 batch-104 = 1.3131648302078247e-06

Training epoch-92 batch-105
Running loss of epoch-92 batch-105 = 1.5660189092159271e-06

Training epoch-92 batch-106
Running loss of epoch-92 batch-106 = 1.4207325875759125e-06

Training epoch-92 batch-107
Running loss of epoch-92 batch-107 = 1.2142118066549301e-06

Training epoch-92 batch-108
Running loss of epoch-92 batch-108 = 8.919741958379745e-07

Training epoch-92 batch-109
Running loss of epoch-92 batch-109 = 8.428469300270081e-07

Training epoch-92 batch-110
Running loss of epoch-92 batch-110 = 1.460779458284378e-06

Training epoch-92 batch-111
Running loss of epoch-92 batch-111 = 1.4372635632753372e-06

Training epoch-92 batch-112
Running loss of epoch-92 batch-112 = 1.248437911272049e-06

Training epoch-92 batch-113
Running loss of epoch-92 batch-113 = 1.2272503226995468e-06

Training epoch-92 batch-114
Running loss of epoch-92 batch-114 = 1.0351650416851044e-06

Training epoch-92 batch-115
Running loss of epoch-92 batch-115 = 1.10710971057415e-06

Training epoch-92 batch-116
Running loss of epoch-92 batch-116 = 1.885928213596344e-06

Training epoch-92 batch-117
Running loss of epoch-92 batch-117 = 1.6058329492807388e-06

Training epoch-92 batch-118
Running loss of epoch-92 batch-118 = 1.441221684217453e-06

Training epoch-92 batch-119
Running loss of epoch-92 batch-119 = 1.4936085790395737e-06

Training epoch-92 batch-120
Running loss of epoch-92 batch-120 = 1.2062955647706985e-06

Training epoch-92 batch-121
Running loss of epoch-92 batch-121 = 1.152046024799347e-06

Training epoch-92 batch-122
Running loss of epoch-92 batch-122 = 8.41217115521431e-07

Training epoch-92 batch-123
Running loss of epoch-92 batch-123 = 8.414499461650848e-07

Training epoch-92 batch-124
Running loss of epoch-92 batch-124 = 9.03615728020668e-07

Training epoch-92 batch-125
Running loss of epoch-92 batch-125 = 1.0791700333356857e-06

Training epoch-92 batch-126
Running loss of epoch-92 batch-126 = 1.3713724911212921e-06

Training epoch-92 batch-127
Running loss of epoch-92 batch-127 = 1.2291129678487778e-06

Training epoch-92 batch-128
Running loss of epoch-92 batch-128 = 1.6265548765659332e-06

Training epoch-92 batch-129
Running loss of epoch-92 batch-129 = 1.1920928955078125e-06

Training epoch-92 batch-130
Running loss of epoch-92 batch-130 = 1.4903489500284195e-06

Training epoch-92 batch-131
Running loss of epoch-92 batch-131 = 1.0277144610881805e-06

Training epoch-92 batch-132
Running loss of epoch-92 batch-132 = 1.1799857020378113e-06

Training epoch-92 batch-133
Running loss of epoch-92 batch-133 = 1.2463424354791641e-06

Training epoch-92 batch-134
Running loss of epoch-92 batch-134 = 1.1648517102003098e-06

Training epoch-92 batch-135
Running loss of epoch-92 batch-135 = 1.3601966202259064e-06

Training epoch-92 batch-136
Running loss of epoch-92 batch-136 = 1.1487863957881927e-06

Training epoch-92 batch-137
Running loss of epoch-92 batch-137 = 1.1168885976076126e-06

Training epoch-92 batch-138
Running loss of epoch-92 batch-138 = 1.257285475730896e-06

Training epoch-92 batch-139
Running loss of epoch-92 batch-139 = 1.7171259969472885e-06

Training epoch-92 batch-140
Running loss of epoch-92 batch-140 = 1.46450474858284e-06

Training epoch-92 batch-141
Running loss of epoch-92 batch-141 = 1.4191027730703354e-06

Training epoch-92 batch-142
Running loss of epoch-92 batch-142 = 1.0405201464891434e-06

Training epoch-92 batch-143
Running loss of epoch-92 batch-143 = 1.1119991540908813e-06

Training epoch-92 batch-144
Running loss of epoch-92 batch-144 = 1.4256220310926437e-06

Training epoch-92 batch-145
Running loss of epoch-92 batch-145 = 1.4225952327251434e-06

Training epoch-92 batch-146
Running loss of epoch-92 batch-146 = 1.0852236300706863e-06

Training epoch-92 batch-147
Running loss of epoch-92 batch-147 = 1.139240339398384e-06

Training epoch-92 batch-148
Running loss of epoch-92 batch-148 = 1.3280659914016724e-06

Training epoch-92 batch-149
Running loss of epoch-92 batch-149 = 1.4684628695249557e-06

Training epoch-92 batch-150
Running loss of epoch-92 batch-150 = 1.2603122740983963e-06

Training epoch-92 batch-151
Running loss of epoch-92 batch-151 = 1.1462252587080002e-06

Training epoch-92 batch-152
Running loss of epoch-92 batch-152 = 7.05476850271225e-07

Training epoch-92 batch-153
Running loss of epoch-92 batch-153 = 1.3152603060007095e-06

Training epoch-92 batch-154
Running loss of epoch-92 batch-154 = 1.6470439732074738e-06

Training epoch-92 batch-155
Running loss of epoch-92 batch-155 = 1.1813826858997345e-06

Training epoch-92 batch-156
Running loss of epoch-92 batch-156 = 8.740462362766266e-07

Training epoch-92 batch-157
Running loss of epoch-92 batch-157 = 4.999339580535889e-06

Finished training epoch-92.



Average train loss at epoch-92 = 1.2548327445983888e-06

Started Evaluation

Average val loss at epoch-92 = 3.3095080758395947

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-93


Training epoch-93 batch-1
Running loss of epoch-93 batch-1 = 1.107342541217804e-06

Training epoch-93 batch-2
Running loss of epoch-93 batch-2 = 1.957407221198082e-06

Training epoch-93 batch-3
Running loss of epoch-93 batch-3 = 1.5953555703163147e-06

Training epoch-93 batch-4
Running loss of epoch-93 batch-4 = 1.7550773918628693e-06

Training epoch-93 batch-5
Running loss of epoch-93 batch-5 = 8.908100426197052e-07

Training epoch-93 batch-6
Running loss of epoch-93 batch-6 = 1.3390090316534042e-06

Training epoch-93 batch-7
Running loss of epoch-93 batch-7 = 1.1764932423830032e-06

Training epoch-93 batch-8
Running loss of epoch-93 batch-8 = 1.4426186680793762e-06

Training epoch-93 batch-9
Running loss of epoch-93 batch-9 = 1.46450474858284e-06

Training epoch-93 batch-10
Running loss of epoch-93 batch-10 = 1.3401731848716736e-06

Training epoch-93 batch-11
Running loss of epoch-93 batch-11 = 1.0139774531126022e-06

Training epoch-93 batch-12
Running loss of epoch-93 batch-12 = 1.2309756129980087e-06

Training epoch-93 batch-13
Running loss of epoch-93 batch-13 = 1.1436641216278076e-06

Training epoch-93 batch-14
Running loss of epoch-93 batch-14 = 1.1208467185497284e-06

Training epoch-93 batch-15
Running loss of epoch-93 batch-15 = 2.2405292838811874e-06

Training epoch-93 batch-16
Running loss of epoch-93 batch-16 = 9.567011147737503e-07

Training epoch-93 batch-17
Running loss of epoch-93 batch-17 = 1.2808013707399368e-06

Training epoch-93 batch-18
Running loss of epoch-93 batch-18 = 9.4645656645298e-07

Training epoch-93 batch-19
Running loss of epoch-93 batch-19 = 1.5206169337034225e-06

Training epoch-93 batch-20
Running loss of epoch-93 batch-20 = 1.4808028936386108e-06

Training epoch-93 batch-21
Running loss of epoch-93 batch-21 = 1.1578667908906937e-06

Training epoch-93 batch-22
Running loss of epoch-93 batch-22 = 1.2069940567016602e-06

Training epoch-93 batch-23
Running loss of epoch-93 batch-23 = 8.421484380960464e-07

Training epoch-93 batch-24
Running loss of epoch-93 batch-24 = 9.902287274599075e-07

Training epoch-93 batch-25
Running loss of epoch-93 batch-25 = 1.6826670616865158e-06

Training epoch-93 batch-26
Running loss of epoch-93 batch-26 = 1.2966338545084e-06

Training epoch-93 batch-27
Running loss of epoch-93 batch-27 = 1.1706724762916565e-06

Training epoch-93 batch-28
Running loss of epoch-93 batch-28 = 1.0009389370679855e-06

Training epoch-93 batch-29
Running loss of epoch-93 batch-29 = 1.9115395843982697e-06

Training epoch-93 batch-30
Running loss of epoch-93 batch-30 = 1.1585652828216553e-06

Training epoch-93 batch-31
Running loss of epoch-93 batch-31 = 1.4184042811393738e-06

Training epoch-93 batch-32
Running loss of epoch-93 batch-32 = 1.8263235688209534e-06

Training epoch-93 batch-33
Running loss of epoch-93 batch-33 = 1.4624092727899551e-06

Training epoch-93 batch-34
Running loss of epoch-93 batch-34 = 1.1611264199018478e-06

Training epoch-93 batch-35
Running loss of epoch-93 batch-35 = 8.540228009223938e-07

Training epoch-93 batch-36
Running loss of epoch-93 batch-36 = 1.3636890798807144e-06

Training epoch-93 batch-37
Running loss of epoch-93 batch-37 = 8.647330105304718e-07

Training epoch-93 batch-38
Running loss of epoch-93 batch-38 = 1.3713724911212921e-06

Training epoch-93 batch-39
Running loss of epoch-93 batch-39 = 1.357169821858406e-06

Training epoch-93 batch-40
Running loss of epoch-93 batch-40 = 7.727649062871933e-07

Training epoch-93 batch-41
Running loss of epoch-93 batch-41 = 1.2402888387441635e-06

Training epoch-93 batch-42
Running loss of epoch-93 batch-42 = 1.1932570487260818e-06

Training epoch-93 batch-43
Running loss of epoch-93 batch-43 = 9.285286068916321e-07

Training epoch-93 batch-44
Running loss of epoch-93 batch-44 = 9.66014340519905e-07

Training epoch-93 batch-45
Running loss of epoch-93 batch-45 = 8.635688573122025e-07

Training epoch-93 batch-46
Running loss of epoch-93 batch-46 = 9.527429938316345e-07

Training epoch-93 batch-47
Running loss of epoch-93 batch-47 = 1.0624062269926071e-06

Training epoch-93 batch-48
Running loss of epoch-93 batch-48 = 1.5990808606147766e-06

Training epoch-93 batch-49
Running loss of epoch-93 batch-49 = 8.828938007354736e-07

Training epoch-93 batch-50
Running loss of epoch-93 batch-50 = 1.3541430234909058e-06

Training epoch-93 batch-51
Running loss of epoch-93 batch-51 = 9.504146873950958e-07

Training epoch-93 batch-52
Running loss of epoch-93 batch-52 = 8.780043572187424e-07

Training epoch-93 batch-53
Running loss of epoch-93 batch-53 = 1.45728699862957e-06

Training epoch-93 batch-54
Running loss of epoch-93 batch-54 = 1.273350790143013e-06

Training epoch-93 batch-55
Running loss of epoch-93 batch-55 = 1.5173573046922684e-06

Training epoch-93 batch-56
Running loss of epoch-93 batch-56 = 1.2274831533432007e-06

Training epoch-93 batch-57
Running loss of epoch-93 batch-57 = 1.477077603340149e-06

Training epoch-93 batch-58
Running loss of epoch-93 batch-58 = 1.6917474567890167e-06

Training epoch-93 batch-59
Running loss of epoch-93 batch-59 = 1.2188684195280075e-06

Training epoch-93 batch-60
Running loss of epoch-93 batch-60 = 1.5920959413051605e-06

Training epoch-93 batch-61
Running loss of epoch-93 batch-61 = 1.3695098459720612e-06

Training epoch-93 batch-62
Running loss of epoch-93 batch-62 = 7.7858567237854e-07

Training epoch-93 batch-63
Running loss of epoch-93 batch-63 = 9.504146873950958e-07

Training epoch-93 batch-64
Running loss of epoch-93 batch-64 = 1.3115350157022476e-06

Training epoch-93 batch-65
Running loss of epoch-93 batch-65 = 9.424984455108643e-07

Training epoch-93 batch-66
Running loss of epoch-93 batch-66 = 1.1220108717679977e-06

Training epoch-93 batch-67
Running loss of epoch-93 batch-67 = 7.739290595054626e-07

Training epoch-93 batch-68
Running loss of epoch-93 batch-68 = 1.3129319995641708e-06

Training epoch-93 batch-69
Running loss of epoch-93 batch-69 = 1.155538484454155e-06

Training epoch-93 batch-70
Running loss of epoch-93 batch-70 = 1.3408716768026352e-06

Training epoch-93 batch-71
Running loss of epoch-93 batch-71 = 7.101334631443024e-07

Training epoch-93 batch-72
Running loss of epoch-93 batch-72 = 9.746290743350983e-07

Training epoch-93 batch-73
Running loss of epoch-93 batch-73 = 1.194886863231659e-06

Training epoch-93 batch-74
Running loss of epoch-93 batch-74 = 7.592607289552689e-07

Training epoch-93 batch-75
Running loss of epoch-93 batch-75 = 1.4004763215780258e-06

Training epoch-93 batch-76
Running loss of epoch-93 batch-76 = 1.405365765094757e-06

Training epoch-93 batch-77
Running loss of epoch-93 batch-77 = 1.1699739843606949e-06

Training epoch-93 batch-78
Running loss of epoch-93 batch-78 = 1.5718396753072739e-06

Training epoch-93 batch-79
Running loss of epoch-93 batch-79 = 1.0693911463022232e-06

Training epoch-93 batch-80
Running loss of epoch-93 batch-80 = 1.5110708773136139e-06

Training epoch-93 batch-81
Running loss of epoch-93 batch-81 = 1.2728851288557053e-06

Training epoch-93 batch-82
Running loss of epoch-93 batch-82 = 8.721835911273956e-07

Training epoch-93 batch-83
Running loss of epoch-93 batch-83 = 1.653563231229782e-06

Training epoch-93 batch-84
Running loss of epoch-93 batch-84 = 1.0745134204626083e-06

Training epoch-93 batch-85
Running loss of epoch-93 batch-85 = 1.467997208237648e-06

Training epoch-93 batch-86
Running loss of epoch-93 batch-86 = 1.2461096048355103e-06

Training epoch-93 batch-87
Running loss of epoch-93 batch-87 = 8.800998330116272e-07

Training epoch-93 batch-88
Running loss of epoch-93 batch-88 = 1.0395888239145279e-06

Training epoch-93 batch-89
Running loss of epoch-93 batch-89 = 1.0433141142129898e-06

Training epoch-93 batch-90
Running loss of epoch-93 batch-90 = 1.0239891707897186e-06

Training epoch-93 batch-91
Running loss of epoch-93 batch-91 = 1.4833640307188034e-06

Training epoch-93 batch-92
Running loss of epoch-93 batch-92 = 1.3513490557670593e-06

Training epoch-93 batch-93
Running loss of epoch-93 batch-93 = 1.2367963790893555e-06

Training epoch-93 batch-94
Running loss of epoch-93 batch-94 = 1.1129304766654968e-06

Training epoch-93 batch-95
Running loss of epoch-93 batch-95 = 8.547212928533554e-07

Training epoch-93 batch-96
Running loss of epoch-93 batch-96 = 1.1087395250797272e-06

Training epoch-93 batch-97
Running loss of epoch-93 batch-97 = 4.728790372610092e-07

Training epoch-93 batch-98
Running loss of epoch-93 batch-98 = 1.7902348190546036e-06

Training epoch-93 batch-99
Running loss of epoch-93 batch-99 = 8.207280188798904e-07

Training epoch-93 batch-100
Running loss of epoch-93 batch-100 = 6.202608346939087e-07

Training epoch-93 batch-101
Running loss of epoch-93 batch-101 = 9.192153811454773e-07

Training epoch-93 batch-102
Running loss of epoch-93 batch-102 = 9.527429938316345e-07

Training epoch-93 batch-103
Running loss of epoch-93 batch-103 = 1.1646188795566559e-06

Training epoch-93 batch-104
Running loss of epoch-93 batch-104 = 1.4186371117830276e-06

Training epoch-93 batch-105
Running loss of epoch-93 batch-105 = 1.4747492969036102e-06

Training epoch-93 batch-106
Running loss of epoch-93 batch-106 = 1.4584511518478394e-06

Training epoch-93 batch-107
Running loss of epoch-93 batch-107 = 1.2367963790893555e-06

Training epoch-93 batch-108
Running loss of epoch-93 batch-108 = 9.364448487758636e-07

Training epoch-93 batch-109
Running loss of epoch-93 batch-109 = 9.096693247556686e-07

Training epoch-93 batch-110
Running loss of epoch-93 batch-110 = 1.0586809366941452e-06

Training epoch-93 batch-111
Running loss of epoch-93 batch-111 = 2.0491424947977066e-06

Training epoch-93 batch-112
Running loss of epoch-93 batch-112 = 7.126946002244949e-07

Training epoch-93 batch-113
Running loss of epoch-93 batch-113 = 1.6933772712945938e-06

Training epoch-93 batch-114
Running loss of epoch-93 batch-114 = 1.0333023965358734e-06

Training epoch-93 batch-115
Running loss of epoch-93 batch-115 = 1.2381933629512787e-06

Training epoch-93 batch-116
Running loss of epoch-93 batch-116 = 1.5660189092159271e-06

Training epoch-93 batch-117
Running loss of epoch-93 batch-117 = 7.338821887969971e-07

Training epoch-93 batch-118
Running loss of epoch-93 batch-118 = 9.974464774131775e-07

Training epoch-93 batch-119
Running loss of epoch-93 batch-119 = 1.2889504432678223e-06

Training epoch-93 batch-120
Running loss of epoch-93 batch-120 = 1.4351680874824524e-06

Training epoch-93 batch-121
Running loss of epoch-93 batch-121 = 1.333886757493019e-06

Training epoch-93 batch-122
Running loss of epoch-93 batch-122 = 1.6775447875261307e-06

Training epoch-93 batch-123
Running loss of epoch-93 batch-123 = 1.0933727025985718e-06

Training epoch-93 batch-124
Running loss of epoch-93 batch-124 = 1.101754605770111e-06

Training epoch-93 batch-125
Running loss of epoch-93 batch-125 = 7.159542292356491e-07

Training epoch-93 batch-126
Running loss of epoch-93 batch-126 = 1.6191042959690094e-06

Training epoch-93 batch-127
Running loss of epoch-93 batch-127 = 1.1762604117393494e-06

Training epoch-93 batch-128
Running loss of epoch-93 batch-128 = 1.4996621757745743e-06

Training epoch-93 batch-129
Running loss of epoch-93 batch-129 = 9.282957762479782e-07

Training epoch-93 batch-130
Running loss of epoch-93 batch-130 = 1.30385160446167e-06

Training epoch-93 batch-131
Running loss of epoch-93 batch-131 = 1.1310912668704987e-06

Training epoch-93 batch-132
Running loss of epoch-93 batch-132 = 8.593779057264328e-07

Training epoch-93 batch-133
Running loss of epoch-93 batch-133 = 1.2433156371116638e-06

Training epoch-93 batch-134
Running loss of epoch-93 batch-134 = 1.0705552995204926e-06

Training epoch-93 batch-135
Running loss of epoch-93 batch-135 = 1.2065283954143524e-06

Training epoch-93 batch-136
Running loss of epoch-93 batch-136 = 9.220093488693237e-07

Training epoch-93 batch-137
Running loss of epoch-93 batch-137 = 7.776543498039246e-07

Training epoch-93 batch-138
Running loss of epoch-93 batch-138 = 1.001870259642601e-06

Training epoch-93 batch-139
Running loss of epoch-93 batch-139 = 1.2649688869714737e-06

Training epoch-93 batch-140
Running loss of epoch-93 batch-140 = 1.600012183189392e-06

Training epoch-93 batch-141
Running loss of epoch-93 batch-141 = 1.5969853848218918e-06

Training epoch-93 batch-142
Running loss of epoch-93 batch-142 = 1.687556505203247e-06

Training epoch-93 batch-143
Running loss of epoch-93 batch-143 = 1.5790574252605438e-06

Training epoch-93 batch-144
Running loss of epoch-93 batch-144 = 1.223292201757431e-06

Training epoch-93 batch-145
Running loss of epoch-93 batch-145 = 1.1397060006856918e-06

Training epoch-93 batch-146
Running loss of epoch-93 batch-146 = 7.916241884231567e-07

Training epoch-93 batch-147
Running loss of epoch-93 batch-147 = 1.5650875866413116e-06

Training epoch-93 batch-148
Running loss of epoch-93 batch-148 = 1.25030055642128e-06

Training epoch-93 batch-149
Running loss of epoch-93 batch-149 = 1.7799902707338333e-06

Training epoch-93 batch-150
Running loss of epoch-93 batch-150 = 1.3420358300209045e-06

Training epoch-93 batch-151
Running loss of epoch-93 batch-151 = 1.026783138513565e-06

Training epoch-93 batch-152
Running loss of epoch-93 batch-152 = 1.703621819615364e-06

Training epoch-93 batch-153
Running loss of epoch-93 batch-153 = 1.5022233128547668e-06

Training epoch-93 batch-154
Running loss of epoch-93 batch-154 = 1.3243407011032104e-06

Training epoch-93 batch-155
Running loss of epoch-93 batch-155 = 2.08965502679348e-06

Training epoch-93 batch-156
Running loss of epoch-93 batch-156 = 1.1546071618795395e-06

Training epoch-93 batch-157
Running loss of epoch-93 batch-157 = 7.00727105140686e-06

Finished training epoch-93.



Average train loss at epoch-93 = 1.2401029467582703e-06

Started Evaluation

Average val loss at epoch-93 = 3.313195245830636

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-94


Training epoch-94 batch-1
Running loss of epoch-94 batch-1 = 1.4549586921930313e-06

Training epoch-94 batch-2
Running loss of epoch-94 batch-2 = 1.677079126238823e-06

Training epoch-94 batch-3
Running loss of epoch-94 batch-3 = 8.118804544210434e-07

Training epoch-94 batch-4
Running loss of epoch-94 batch-4 = 1.4237593859434128e-06

Training epoch-94 batch-5
Running loss of epoch-94 batch-5 = 1.1266674846410751e-06

Training epoch-94 batch-6
Running loss of epoch-94 batch-6 = 1.0451767593622208e-06

Training epoch-94 batch-7
Running loss of epoch-94 batch-7 = 9.925570338964462e-07

Training epoch-94 batch-8
Running loss of epoch-94 batch-8 = 1.042848452925682e-06

Training epoch-94 batch-9
Running loss of epoch-94 batch-9 = 9.05478373169899e-07

Training epoch-94 batch-10
Running loss of epoch-94 batch-10 = 9.98144969344139e-07

Training epoch-94 batch-11
Running loss of epoch-94 batch-11 = 1.0235235095024109e-06

Training epoch-94 batch-12
Running loss of epoch-94 batch-12 = 1.7636921256780624e-06

Training epoch-94 batch-13
Running loss of epoch-94 batch-13 = 1.176958903670311e-06

Training epoch-94 batch-14
Running loss of epoch-94 batch-14 = 9.10833477973938e-07

Training epoch-94 batch-15
Running loss of epoch-94 batch-15 = 1.5327241271734238e-06

Training epoch-94 batch-16
Running loss of epoch-94 batch-16 = 1.4614779502153397e-06

Training epoch-94 batch-17
Running loss of epoch-94 batch-17 = 7.634516805410385e-07

Training epoch-94 batch-18
Running loss of epoch-94 batch-18 = 9.441282600164413e-07

Training epoch-94 batch-19
Running loss of epoch-94 batch-19 = 1.386040821671486e-06

Training epoch-94 batch-20
Running loss of epoch-94 batch-20 = 1.3809185475111008e-06

Training epoch-94 batch-21
Running loss of epoch-94 batch-21 = 1.3594981282949448e-06

Training epoch-94 batch-22
Running loss of epoch-94 batch-22 = 1.8835999071598053e-06

Training epoch-94 batch-23
Running loss of epoch-94 batch-23 = 6.998889148235321e-07

Training epoch-94 batch-24
Running loss of epoch-94 batch-24 = 1.8524006009101868e-06

Training epoch-94 batch-25
Running loss of epoch-94 batch-25 = 1.4936085790395737e-06

Training epoch-94 batch-26
Running loss of epoch-94 batch-26 = 1.0579824447631836e-06

Training epoch-94 batch-27
Running loss of epoch-94 batch-27 = 1.6205012798309326e-06

Training epoch-94 batch-28
Running loss of epoch-94 batch-28 = 1.4069955796003342e-06

Training epoch-94 batch-29
Running loss of epoch-94 batch-29 = 1.133885234594345e-06

Training epoch-94 batch-30
Running loss of epoch-94 batch-30 = 8.221250027418137e-07

Training epoch-94 batch-31
Running loss of epoch-94 batch-31 = 1.662643626332283e-06

Training epoch-94 batch-32
Running loss of epoch-94 batch-32 = 1.4039687812328339e-06

Training epoch-94 batch-33
Running loss of epoch-94 batch-33 = 1.4707911759614944e-06

Training epoch-94 batch-34
Running loss of epoch-94 batch-34 = 1.1809170246124268e-06

Training epoch-94 batch-35
Running loss of epoch-94 batch-35 = 1.2223608791828156e-06

Training epoch-94 batch-36
Running loss of epoch-94 batch-36 = 1.0156072676181793e-06

Training epoch-94 batch-37
Running loss of epoch-94 batch-37 = 9.327195584774017e-07

Training epoch-94 batch-38
Running loss of epoch-94 batch-38 = 1.5138648450374603e-06

Training epoch-94 batch-39
Running loss of epoch-94 batch-39 = 1.4177057892084122e-06

Training epoch-94 batch-40
Running loss of epoch-94 batch-40 = 7.35744833946228e-07

Training epoch-94 batch-41
Running loss of epoch-94 batch-41 = 1.0391231626272202e-06

Training epoch-94 batch-42
Running loss of epoch-94 batch-42 = 1.4426186680793762e-06

Training epoch-94 batch-43
Running loss of epoch-94 batch-43 = 9.830109775066376e-07

Training epoch-94 batch-44
Running loss of epoch-94 batch-44 = 5.050096660852432e-07

Training epoch-94 batch-45
Running loss of epoch-94 batch-45 = 1.971842721104622e-06

Training epoch-94 batch-46
Running loss of epoch-94 batch-46 = 9.96515154838562e-07

Training epoch-94 batch-47
Running loss of epoch-94 batch-47 = 1.3848766684532166e-06

Training epoch-94 batch-48
Running loss of epoch-94 batch-48 = 1.5299301594495773e-06

Training epoch-94 batch-49
Running loss of epoch-94 batch-49 = 1.3944227248430252e-06

Training epoch-94 batch-50
Running loss of epoch-94 batch-50 = 1.4838296920061111e-06

Training epoch-94 batch-51
Running loss of epoch-94 batch-51 = 1.5229452401399612e-06

Training epoch-94 batch-52
Running loss of epoch-94 batch-52 = 6.104819476604462e-07

Training epoch-94 batch-53
Running loss of epoch-94 batch-53 = 1.3182871043682098e-06

Training epoch-94 batch-54
Running loss of epoch-94 batch-54 = 1.080567017197609e-06

Training epoch-94 batch-55
Running loss of epoch-94 batch-55 = 1.346459612250328e-06

Training epoch-94 batch-56
Running loss of epoch-94 batch-56 = 8.302740752696991e-07

Training epoch-94 batch-57
Running loss of epoch-94 batch-57 = 1.375330612063408e-06

Training epoch-94 batch-58
Running loss of epoch-94 batch-58 = 1.619802787899971e-06

Training epoch-94 batch-59
Running loss of epoch-94 batch-59 = 1.4135148376226425e-06

Training epoch-94 batch-60
Running loss of epoch-94 batch-60 = 8.314382284879684e-07

Training epoch-94 batch-61
Running loss of epoch-94 batch-61 = 9.860377758741379e-07

Training epoch-94 batch-62
Running loss of epoch-94 batch-62 = 1.530628651380539e-06

Training epoch-94 batch-63
Running loss of epoch-94 batch-63 = 1.0924413800239563e-06

Training epoch-94 batch-64
Running loss of epoch-94 batch-64 = 1.3061799108982086e-06

Training epoch-94 batch-65
Running loss of epoch-94 batch-65 = 1.0295771062374115e-06

Training epoch-94 batch-66
Running loss of epoch-94 batch-66 = 1.7061829566955566e-06

Training epoch-94 batch-67
Running loss of epoch-94 batch-67 = 1.0456424206495285e-06

Training epoch-94 batch-68
Running loss of epoch-94 batch-68 = 9.085051715373993e-07

Training epoch-94 batch-69
Running loss of epoch-94 batch-69 = 1.0300427675247192e-06

Training epoch-94 batch-70
Running loss of epoch-94 batch-70 = 9.252689778804779e-07

Training epoch-94 batch-71
Running loss of epoch-94 batch-71 = 1.1075753718614578e-06

Training epoch-94 batch-72
Running loss of epoch-94 batch-72 = 1.2475065886974335e-06

Training epoch-94 batch-73
Running loss of epoch-94 batch-73 = 1.2514647096395493e-06

Training epoch-94 batch-74
Running loss of epoch-94 batch-74 = 1.10710971057415e-06

Training epoch-94 batch-75
Running loss of epoch-94 batch-75 = 1.2284144759178162e-06

Training epoch-94 batch-76
Running loss of epoch-94 batch-76 = 9.548384696245193e-07

Training epoch-94 batch-77
Running loss of epoch-94 batch-77 = 1.3005919754505157e-06

Training epoch-94 batch-78
Running loss of epoch-94 batch-78 = 9.890645742416382e-07

Training epoch-94 batch-79
Running loss of epoch-94 batch-79 = 1.3478565961122513e-06

Training epoch-94 batch-80
Running loss of epoch-94 batch-80 = 1.3487879186868668e-06

Training epoch-94 batch-81
Running loss of epoch-94 batch-81 = 8.915085345506668e-07

Training epoch-94 batch-82
Running loss of epoch-94 batch-82 = 1.6379635781049728e-06

Training epoch-94 batch-83
Running loss of epoch-94 batch-83 = 1.3830140233039856e-06

Training epoch-94 batch-84
Running loss of epoch-94 batch-84 = 1.6370322555303574e-06

Training epoch-94 batch-85
Running loss of epoch-94 batch-85 = 1.2782402336597443e-06

Training epoch-94 batch-86
Running loss of epoch-94 batch-86 = 8.058268576860428e-07

Training epoch-94 batch-87
Running loss of epoch-94 batch-87 = 1.1117663234472275e-06

Training epoch-94 batch-88
Running loss of epoch-94 batch-88 = 6.784684956073761e-07

Training epoch-94 batch-89
Running loss of epoch-94 batch-89 = 8.01868736743927e-07

Training epoch-94 batch-90
Running loss of epoch-94 batch-90 = 9.89297404885292e-07

Training epoch-94 batch-91
Running loss of epoch-94 batch-91 = 9.71369445323944e-07

Training epoch-94 batch-92
Running loss of epoch-94 batch-92 = 1.323409378528595e-06

Training epoch-94 batch-93
Running loss of epoch-94 batch-93 = 1.2868549674749374e-06

Training epoch-94 batch-94
Running loss of epoch-94 batch-94 = 8.039642125368118e-07

Training epoch-94 batch-95
Running loss of epoch-94 batch-95 = 1.078704372048378e-06

Training epoch-94 batch-96
Running loss of epoch-94 batch-96 = 1.1278316378593445e-06

Training epoch-94 batch-97
Running loss of epoch-94 batch-97 = 1.2221280485391617e-06

Training epoch-94 batch-98
Running loss of epoch-94 batch-98 = 1.0477378964424133e-06

Training epoch-94 batch-99
Running loss of epoch-94 batch-99 = 1.0414514690637589e-06

Training epoch-94 batch-100
Running loss of epoch-94 batch-100 = 1.759268343448639e-06

Training epoch-94 batch-101
Running loss of epoch-94 batch-101 = 1.4009419828653336e-06

Training epoch-94 batch-102
Running loss of epoch-94 batch-102 = 1.1110678315162659e-06

Training epoch-94 batch-103
Running loss of epoch-94 batch-103 = 9.520445019006729e-07

Training epoch-94 batch-104
Running loss of epoch-94 batch-104 = 1.1990778148174286e-06

Training epoch-94 batch-105
Running loss of epoch-94 batch-105 = 1.333886757493019e-06

Training epoch-94 batch-106
Running loss of epoch-94 batch-106 = 1.194886863231659e-06

Training epoch-94 batch-107
Running loss of epoch-94 batch-107 = 1.5567056834697723e-06

Training epoch-94 batch-108
Running loss of epoch-94 batch-108 = 1.1688098311424255e-06

Training epoch-94 batch-109
Running loss of epoch-94 batch-109 = 9.71369445323944e-07

Training epoch-94 batch-110
Running loss of epoch-94 batch-110 = 1.7920974642038345e-06

Training epoch-94 batch-111
Running loss of epoch-94 batch-111 = 9.236391633749008e-07

Training epoch-94 batch-112
Running loss of epoch-94 batch-112 = 1.1480879038572311e-06

Training epoch-94 batch-113
Running loss of epoch-94 batch-113 = 8.167698979377747e-07

Training epoch-94 batch-114
Running loss of epoch-94 batch-114 = 1.1057127267122269e-06

Training epoch-94 batch-115
Running loss of epoch-94 batch-115 = 1.2025702744722366e-06

Training epoch-94 batch-116
Running loss of epoch-94 batch-116 = 1.3760291039943695e-06

Training epoch-94 batch-117
Running loss of epoch-94 batch-117 = 1.5655532479286194e-06

Training epoch-94 batch-118
Running loss of epoch-94 batch-118 = 7.245689630508423e-07

Training epoch-94 batch-119
Running loss of epoch-94 batch-119 = 1.3832468539476395e-06

Training epoch-94 batch-120
Running loss of epoch-94 batch-120 = 1.3406388461589813e-06

Training epoch-94 batch-121
Running loss of epoch-94 batch-121 = 1.398380845785141e-06

Training epoch-94 batch-122
Running loss of epoch-94 batch-122 = 1.393025740981102e-06

Training epoch-94 batch-123
Running loss of epoch-94 batch-123 = 1.5080440789461136e-06

Training epoch-94 batch-124
Running loss of epoch-94 batch-124 = 6.963964551687241e-07

Training epoch-94 batch-125
Running loss of epoch-94 batch-125 = 1.1564698070287704e-06

Training epoch-94 batch-126
Running loss of epoch-94 batch-126 = 1.482432708144188e-06

Training epoch-94 batch-127
Running loss of epoch-94 batch-127 = 1.0479707270860672e-06

Training epoch-94 batch-128
Running loss of epoch-94 batch-128 = 1.72504223883152e-06

Training epoch-94 batch-129
Running loss of epoch-94 batch-129 = 1.1639203876256943e-06

Training epoch-94 batch-130
Running loss of epoch-94 batch-130 = 1.773471012711525e-06

Training epoch-94 batch-131
Running loss of epoch-94 batch-131 = 1.0954681783914566e-06

Training epoch-94 batch-132
Running loss of epoch-94 batch-132 = 1.0058283805847168e-06

Training epoch-94 batch-133
Running loss of epoch-94 batch-133 = 8.505303412675858e-07

Training epoch-94 batch-134
Running loss of epoch-94 batch-134 = 9.35746356844902e-07

Training epoch-94 batch-135
Running loss of epoch-94 batch-135 = 1.752050593495369e-06

Training epoch-94 batch-136
Running loss of epoch-94 batch-136 = 1.1827796697616577e-06

Training epoch-94 batch-137
Running loss of epoch-94 batch-137 = 1.3140961527824402e-06

Training epoch-94 batch-138
Running loss of epoch-94 batch-138 = 1.1669471859931946e-06

Training epoch-94 batch-139
Running loss of epoch-94 batch-139 = 1.0707881301641464e-06

Training epoch-94 batch-140
Running loss of epoch-94 batch-140 = 1.1150259524583817e-06

Training epoch-94 batch-141
Running loss of epoch-94 batch-141 = 8.279457688331604e-07

Training epoch-94 batch-142
Running loss of epoch-94 batch-142 = 1.5713740140199661e-06

Training epoch-94 batch-143
Running loss of epoch-94 batch-143 = 1.862877979874611e-06

Training epoch-94 batch-144
Running loss of epoch-94 batch-144 = 1.1404044926166534e-06

Training epoch-94 batch-145
Running loss of epoch-94 batch-145 = 9.380746632814407e-07

Training epoch-94 batch-146
Running loss of epoch-94 batch-146 = 1.2854579836130142e-06

Training epoch-94 batch-147
Running loss of epoch-94 batch-147 = 1.391163095831871e-06

Training epoch-94 batch-148
Running loss of epoch-94 batch-148 = 1.3599637895822525e-06

Training epoch-94 batch-149
Running loss of epoch-94 batch-149 = 1.1224765330553055e-06

Training epoch-94 batch-150
Running loss of epoch-94 batch-150 = 7.9302117228508e-07

Training epoch-94 batch-151
Running loss of epoch-94 batch-151 = 1.519685611128807e-06

Training epoch-94 batch-152
Running loss of epoch-94 batch-152 = 1.269625499844551e-06

Training epoch-94 batch-153
Running loss of epoch-94 batch-153 = 1.1012889444828033e-06

Training epoch-94 batch-154
Running loss of epoch-94 batch-154 = 1.0239891707897186e-06

Training epoch-94 batch-155
Running loss of epoch-94 batch-155 = 1.139240339398384e-06

Training epoch-94 batch-156
Running loss of epoch-94 batch-156 = 1.3003591448068619e-06

Training epoch-94 batch-157
Running loss of epoch-94 batch-157 = 5.289912223815918e-06

Finished training epoch-94.



Average train loss at epoch-94 = 1.2210071086883544e-06

Started Evaluation

Average val loss at epoch-94 = 3.316397273226788

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.43 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.51 %

Finished Evaluation



Started training epoch-95


Training epoch-95 batch-1
Running loss of epoch-95 batch-1 = 9.217765182256699e-07

Training epoch-95 batch-2
Running loss of epoch-95 batch-2 = 1.0691583156585693e-06

Training epoch-95 batch-3
Running loss of epoch-95 batch-3 = 6.041955202817917e-07

Training epoch-95 batch-4
Running loss of epoch-95 batch-4 = 1.0302755981683731e-06

Training epoch-95 batch-5
Running loss of epoch-95 batch-5 = 1.3532117009162903e-06

Training epoch-95 batch-6
Running loss of epoch-95 batch-6 = 1.1192169040441513e-06

Training epoch-95 batch-7
Running loss of epoch-95 batch-7 = 1.5825498849153519e-06

Training epoch-95 batch-8
Running loss of epoch-95 batch-8 = 1.50362029671669e-06

Training epoch-95 batch-9
Running loss of epoch-95 batch-9 = 9.159557521343231e-07

Training epoch-95 batch-10
Running loss of epoch-95 batch-10 = 1.1941883713006973e-06

Training epoch-95 batch-11
Running loss of epoch-95 batch-11 = 1.1837109923362732e-06

Training epoch-95 batch-12
Running loss of epoch-95 batch-12 = 1.5266705304384232e-06

Training epoch-95 batch-13
Running loss of epoch-95 batch-13 = 1.385807991027832e-06

Training epoch-95 batch-14
Running loss of epoch-95 batch-14 = 1.2298114597797394e-06

Training epoch-95 batch-15
Running loss of epoch-95 batch-15 = 1.1580996215343475e-06

Training epoch-95 batch-16
Running loss of epoch-95 batch-16 = 7.390044629573822e-07

Training epoch-95 batch-17
Running loss of epoch-95 batch-17 = 6.440095603466034e-07

Training epoch-95 batch-18
Running loss of epoch-95 batch-18 = 1.3960525393486023e-06

Training epoch-95 batch-19
Running loss of epoch-95 batch-19 = 7.394701242446899e-07

Training epoch-95 batch-20
Running loss of epoch-95 batch-20 = 1.2118835002183914e-06

Training epoch-95 batch-21
Running loss of epoch-95 batch-21 = 1.3059470802545547e-06

Training epoch-95 batch-22
Running loss of epoch-95 batch-22 = 1.1390075087547302e-06

Training epoch-95 batch-23
Running loss of epoch-95 batch-23 = 1.2246891856193542e-06

Training epoch-95 batch-24
Running loss of epoch-95 batch-24 = 7.008202373981476e-07

Training epoch-95 batch-25
Running loss of epoch-95 batch-25 = 1.3746321201324463e-06

Training epoch-95 batch-26
Running loss of epoch-95 batch-26 = 8.430797606706619e-07

Training epoch-95 batch-27
Running loss of epoch-95 batch-27 = 9.362120181322098e-07

Training epoch-95 batch-28
Running loss of epoch-95 batch-28 = 1.2300442904233932e-06

Training epoch-95 batch-29
Running loss of epoch-95 batch-29 = 8.742790669202805e-07

Training epoch-95 batch-30
Running loss of epoch-95 batch-30 = 1.257285475730896e-06

Training epoch-95 batch-31
Running loss of epoch-95 batch-31 = 8.321367204189301e-07

Training epoch-95 batch-32
Running loss of epoch-95 batch-32 = 1.2561213225126266e-06

Training epoch-95 batch-33
Running loss of epoch-95 batch-33 = 1.2069940567016602e-06

Training epoch-95 batch-34
Running loss of epoch-95 batch-34 = 1.2586824595928192e-06

Training epoch-95 batch-35
Running loss of epoch-95 batch-35 = 1.0670628398656845e-06

Training epoch-95 batch-36
Running loss of epoch-95 batch-36 = 5.674082785844803e-07

Training epoch-95 batch-37
Running loss of epoch-95 batch-37 = 1.4745164662599564e-06

Training epoch-95 batch-38
Running loss of epoch-95 batch-38 = 1.7113052308559418e-06

Training epoch-95 batch-39
Running loss of epoch-95 batch-39 = 1.1818483471870422e-06

Training epoch-95 batch-40
Running loss of epoch-95 batch-40 = 1.2628734111785889e-06

Training epoch-95 batch-41
Running loss of epoch-95 batch-41 = 1.5252735465765e-06

Training epoch-95 batch-42
Running loss of epoch-95 batch-42 = 1.2295786291360855e-06

Training epoch-95 batch-43
Running loss of epoch-95 batch-43 = 9.587965905666351e-07

Training epoch-95 batch-44
Running loss of epoch-95 batch-44 = 9.89530235528946e-07

Training epoch-95 batch-45
Running loss of epoch-95 batch-45 = 7.110647857189178e-07

Training epoch-95 batch-46
Running loss of epoch-95 batch-46 = 1.1327210813760757e-06

Training epoch-95 batch-47
Running loss of epoch-95 batch-47 = 1.0351650416851044e-06

Training epoch-95 batch-48
Running loss of epoch-95 batch-48 = 1.0491348803043365e-06

Training epoch-95 batch-49
Running loss of epoch-95 batch-49 = 1.2079253792762756e-06

Training epoch-95 batch-50
Running loss of epoch-95 batch-50 = 1.0745134204626083e-06

Training epoch-95 batch-51
Running loss of epoch-95 batch-51 = 1.071486622095108e-06

Training epoch-95 batch-52
Running loss of epoch-95 batch-52 = 1.234235242009163e-06

Training epoch-95 batch-53
Running loss of epoch-95 batch-53 = 1.1557713150978088e-06

Training epoch-95 batch-54
Running loss of epoch-95 batch-54 = 1.7709098756313324e-06

Training epoch-95 batch-55
Running loss of epoch-95 batch-55 = 1.05355866253376e-06

Training epoch-95 batch-56
Running loss of epoch-95 batch-56 = 1.178588718175888e-06

Training epoch-95 batch-57
Running loss of epoch-95 batch-57 = 1.3264361768960953e-06

Training epoch-95 batch-58
Running loss of epoch-95 batch-58 = 7.345806807279587e-07

Training epoch-95 batch-59
Running loss of epoch-95 batch-59 = 9.408686310052872e-07

Training epoch-95 batch-60
Running loss of epoch-95 batch-60 = 2.0542647689580917e-06

Training epoch-95 batch-61
Running loss of epoch-95 batch-61 = 1.5771947801113129e-06

Training epoch-95 batch-62
Running loss of epoch-95 batch-62 = 1.3224780559539795e-06

Training epoch-95 batch-63
Running loss of epoch-95 batch-63 = 8.398201316595078e-07

Training epoch-95 batch-64
Running loss of epoch-95 batch-64 = 1.2244563549757004e-06

Training epoch-95 batch-65
Running loss of epoch-95 batch-65 = 9.87434759736061e-07

Training epoch-95 batch-66
Running loss of epoch-95 batch-66 = 1.0032672435045242e-06

Training epoch-95 batch-67
Running loss of epoch-95 batch-67 = 1.5974510461091995e-06

Training epoch-95 batch-68
Running loss of epoch-95 batch-68 = 1.3620592653751373e-06

Training epoch-95 batch-69
Running loss of epoch-95 batch-69 = 1.467997208237648e-06

Training epoch-95 batch-70
Running loss of epoch-95 batch-70 = 1.605600118637085e-06

Training epoch-95 batch-71
Running loss of epoch-95 batch-71 = 1.1133961379528046e-06

Training epoch-95 batch-72
Running loss of epoch-95 batch-72 = 1.9292347133159637e-06

Training epoch-95 batch-73
Running loss of epoch-95 batch-73 = 1.6996636986732483e-06

Training epoch-95 batch-74
Running loss of epoch-95 batch-74 = 9.445939213037491e-07

Training epoch-95 batch-75
Running loss of epoch-95 batch-75 = 1.4100223779678345e-06

Training epoch-95 batch-76
Running loss of epoch-95 batch-76 = 1.339241862297058e-06

Training epoch-95 batch-77
Running loss of epoch-95 batch-77 = 1.1369120329618454e-06

Training epoch-95 batch-78
Running loss of epoch-95 batch-78 = 1.6940757632255554e-06

Training epoch-95 batch-79
Running loss of epoch-95 batch-79 = 1.2009404599666595e-06

Training epoch-95 batch-80
Running loss of epoch-95 batch-80 = 1.4319084584712982e-06

Training epoch-95 batch-81
Running loss of epoch-95 batch-81 = 4.55416738986969e-07

Training epoch-95 batch-82
Running loss of epoch-95 batch-82 = 9.171199053525925e-07

Training epoch-95 batch-83
Running loss of epoch-95 batch-83 = 1.4491379261016846e-06

Training epoch-95 batch-84
Running loss of epoch-95 batch-84 = 1.0742805898189545e-06

Training epoch-95 batch-85
Running loss of epoch-95 batch-85 = 9.071081876754761e-07

Training epoch-95 batch-86
Running loss of epoch-95 batch-86 = 1.1776573956012726e-06

Training epoch-95 batch-87
Running loss of epoch-95 batch-87 = 1.335982233285904e-06

Training epoch-95 batch-88
Running loss of epoch-95 batch-88 = 1.6584526747465134e-06

Training epoch-95 batch-89
Running loss of epoch-95 batch-89 = 1.4177057892084122e-06

Training epoch-95 batch-90
Running loss of epoch-95 batch-90 = 1.1012889444828033e-06

Training epoch-95 batch-91
Running loss of epoch-95 batch-91 = 7.522758096456528e-07

Training epoch-95 batch-92
Running loss of epoch-95 batch-92 = 1.1834781616926193e-06

Training epoch-95 batch-93
Running loss of epoch-95 batch-93 = 1.4081597328186035e-06

Training epoch-95 batch-94
Running loss of epoch-95 batch-94 = 1.0435469448566437e-06

Training epoch-95 batch-95
Running loss of epoch-95 batch-95 = 1.0316725820302963e-06

Training epoch-95 batch-96
Running loss of epoch-95 batch-96 = 1.248437911272049e-06

Training epoch-95 batch-97
Running loss of epoch-95 batch-97 = 1.2097880244255066e-06

Training epoch-95 batch-98
Running loss of epoch-95 batch-98 = 8.780043572187424e-07

Training epoch-95 batch-99
Running loss of epoch-95 batch-99 = 1.4479737728834152e-06

Training epoch-95 batch-100
Running loss of epoch-95 batch-100 = 1.1832453310489655e-06

Training epoch-95 batch-101
Running loss of epoch-95 batch-101 = 1.943204551935196e-06

Training epoch-95 batch-102
Running loss of epoch-95 batch-102 = 1.3818498700857162e-06

Training epoch-95 batch-103
Running loss of epoch-95 batch-103 = 1.3727694749832153e-06

Training epoch-95 batch-104
Running loss of epoch-95 batch-104 = 1.098262146115303e-06

Training epoch-95 batch-105
Running loss of epoch-95 batch-105 = 1.562759280204773e-06

Training epoch-95 batch-106
Running loss of epoch-95 batch-106 = 1.5748664736747742e-06

Training epoch-95 batch-107
Running loss of epoch-95 batch-107 = 1.214444637298584e-06

Training epoch-95 batch-108
Running loss of epoch-95 batch-108 = 9.75094735622406e-07

Training epoch-95 batch-109
Running loss of epoch-95 batch-109 = 9.71369445323944e-07

Training epoch-95 batch-110
Running loss of epoch-95 batch-110 = 1.23586505651474e-06

Training epoch-95 batch-111
Running loss of epoch-95 batch-111 = 1.1147931218147278e-06

Training epoch-95 batch-112
Running loss of epoch-95 batch-112 = 1.5515834093093872e-06

Training epoch-95 batch-113
Running loss of epoch-95 batch-113 = 1.1583324521780014e-06

Training epoch-95 batch-114
Running loss of epoch-95 batch-114 = 8.66129994392395e-07

Training epoch-95 batch-115
Running loss of epoch-95 batch-115 = 1.226784661412239e-06

Training epoch-95 batch-116
Running loss of epoch-95 batch-116 = 9.064096957445145e-07

Training epoch-95 batch-117
Running loss of epoch-95 batch-117 = 1.3273674994707108e-06

Training epoch-95 batch-118
Running loss of epoch-95 batch-118 = 1.4491379261016846e-06

Training epoch-95 batch-119
Running loss of epoch-95 batch-119 = 6.640329957008362e-07

Training epoch-95 batch-120
Running loss of epoch-95 batch-120 = 1.091277226805687e-06

Training epoch-95 batch-121
Running loss of epoch-95 batch-121 = 1.387670636177063e-06

Training epoch-95 batch-122
Running loss of epoch-95 batch-122 = 1.5189871191978455e-06

Training epoch-95 batch-123
Running loss of epoch-95 batch-123 = 8.351635187864304e-07

Training epoch-95 batch-124
Running loss of epoch-95 batch-124 = 1.085689291357994e-06

Training epoch-95 batch-125
Running loss of epoch-95 batch-125 = 9.168870747089386e-07

Training epoch-95 batch-126
Running loss of epoch-95 batch-126 = 7.459893822669983e-07

Training epoch-95 batch-127
Running loss of epoch-95 batch-127 = 1.0454095900058746e-06

Training epoch-95 batch-128
Running loss of epoch-95 batch-128 = 1.5222467482089996e-06

Training epoch-95 batch-129
Running loss of epoch-95 batch-129 = 1.2118835002183914e-06

Training epoch-95 batch-130
Running loss of epoch-95 batch-130 = 1.4444813132286072e-06

Training epoch-95 batch-131
Running loss of epoch-95 batch-131 = 1.405598595738411e-06

Training epoch-95 batch-132
Running loss of epoch-95 batch-132 = 1.385807991027832e-06

Training epoch-95 batch-133
Running loss of epoch-95 batch-133 = 1.4798715710639954e-06

Training epoch-95 batch-134
Running loss of epoch-95 batch-134 = 1.2211967259645462e-06

Training epoch-95 batch-135
Running loss of epoch-95 batch-135 = 1.7797574400901794e-06

Training epoch-95 batch-136
Running loss of epoch-95 batch-136 = 1.4347024261951447e-06

Training epoch-95 batch-137
Running loss of epoch-95 batch-137 = 1.3168901205062866e-06

Training epoch-95 batch-138
Running loss of epoch-95 batch-138 = 9.257346391677856e-07

Training epoch-95 batch-139
Running loss of epoch-95 batch-139 = 1.5352852642536163e-06

Training epoch-95 batch-140
Running loss of epoch-95 batch-140 = 1.0298099368810654e-06

Training epoch-95 batch-141
Running loss of epoch-95 batch-141 = 1.0633375495672226e-06

Training epoch-95 batch-142
Running loss of epoch-95 batch-142 = 1.428648829460144e-06

Training epoch-95 batch-143
Running loss of epoch-95 batch-143 = 1.1040829122066498e-06

Training epoch-95 batch-144
Running loss of epoch-95 batch-144 = 1.1862721294164658e-06

Training epoch-95 batch-145
Running loss of epoch-95 batch-145 = 1.3445969671010971e-06

Training epoch-95 batch-146
Running loss of epoch-95 batch-146 = 7.017515599727631e-07

Training epoch-95 batch-147
Running loss of epoch-95 batch-147 = 1.5906989574432373e-06

Training epoch-95 batch-148
Running loss of epoch-95 batch-148 = 1.402106136083603e-06

Training epoch-95 batch-149
Running loss of epoch-95 batch-149 = 1.1383090168237686e-06

Training epoch-95 batch-150
Running loss of epoch-95 batch-150 = 9.776558727025986e-07

Training epoch-95 batch-151
Running loss of epoch-95 batch-151 = 1.0754447430372238e-06

Training epoch-95 batch-152
Running loss of epoch-95 batch-152 = 7.734633982181549e-07

Training epoch-95 batch-153
Running loss of epoch-95 batch-153 = 1.1604279279708862e-06

Training epoch-95 batch-154
Running loss of epoch-95 batch-154 = 9.937211871147156e-07

Training epoch-95 batch-155
Running loss of epoch-95 batch-155 = 1.2477394193410873e-06

Training epoch-95 batch-156
Running loss of epoch-95 batch-156 = 1.1422671377658844e-06

Training epoch-95 batch-157
Running loss of epoch-95 batch-157 = 8.206814527511597e-06

Finished training epoch-95.



Average train loss at epoch-95 = 1.2077704071998596e-06

Started Evaluation

Average val loss at epoch-95 = 3.3192531670394696

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.47 %

Finished Evaluation



Started training epoch-96


Training epoch-96 batch-1
Running loss of epoch-96 batch-1 = 1.3932585716247559e-06

Training epoch-96 batch-2
Running loss of epoch-96 batch-2 = 1.3383105397224426e-06

Training epoch-96 batch-3
Running loss of epoch-96 batch-3 = 1.1050142347812653e-06

Training epoch-96 batch-4
Running loss of epoch-96 batch-4 = 1.1452939361333847e-06

Training epoch-96 batch-5
Running loss of epoch-96 batch-5 = 1.72504223883152e-06

Training epoch-96 batch-6
Running loss of epoch-96 batch-6 = 9.292270988225937e-07

Training epoch-96 batch-7
Running loss of epoch-96 batch-7 = 1.4274846762418747e-06

Training epoch-96 batch-8
Running loss of epoch-96 batch-8 = 1.0638032108545303e-06

Training epoch-96 batch-9
Running loss of epoch-96 batch-9 = 1.3604294508695602e-06

Training epoch-96 batch-10
Running loss of epoch-96 batch-10 = 9.729992598295212e-07

Training epoch-96 batch-11
Running loss of epoch-96 batch-11 = 1.6242265701293945e-06

Training epoch-96 batch-12
Running loss of epoch-96 batch-12 = 1.1676456779241562e-06

Training epoch-96 batch-13
Running loss of epoch-96 batch-13 = 5.855690687894821e-07

Training epoch-96 batch-14
Running loss of epoch-96 batch-14 = 1.5550758689641953e-06

Training epoch-96 batch-15
Running loss of epoch-96 batch-15 = 1.9585713744163513e-06

Training epoch-96 batch-16
Running loss of epoch-96 batch-16 = 9.920913726091385e-07

Training epoch-96 batch-17
Running loss of epoch-96 batch-17 = 1.1548399925231934e-06

Training epoch-96 batch-18
Running loss of epoch-96 batch-18 = 1.285690814256668e-06

Training epoch-96 batch-19
Running loss of epoch-96 batch-19 = 1.1299271136522293e-06

Training epoch-96 batch-20
Running loss of epoch-96 batch-20 = 8.365605026483536e-07

Training epoch-96 batch-21
Running loss of epoch-96 batch-21 = 1.380452886223793e-06

Training epoch-96 batch-22
Running loss of epoch-96 batch-22 = 1.1886004358530045e-06

Training epoch-96 batch-23
Running loss of epoch-96 batch-23 = 1.0530930012464523e-06

Training epoch-96 batch-24
Running loss of epoch-96 batch-24 = 1.0861549526453018e-06

Training epoch-96 batch-25
Running loss of epoch-96 batch-25 = 1.4142133295536041e-06

Training epoch-96 batch-26
Running loss of epoch-96 batch-26 = 1.2596137821674347e-06

Training epoch-96 batch-27
Running loss of epoch-96 batch-27 = 1.2007076293230057e-06

Training epoch-96 batch-28
Running loss of epoch-96 batch-28 = 1.2079253792762756e-06

Training epoch-96 batch-29
Running loss of epoch-96 batch-29 = 7.343478500843048e-07

Training epoch-96 batch-30
Running loss of epoch-96 batch-30 = 1.5688128769397736e-06

Training epoch-96 batch-31
Running loss of epoch-96 batch-31 = 1.6193371266126633e-06

Training epoch-96 batch-32
Running loss of epoch-96 batch-32 = 1.2526288628578186e-06

Training epoch-96 batch-33
Running loss of epoch-96 batch-33 = 9.413342922925949e-07

Training epoch-96 batch-34
Running loss of epoch-96 batch-34 = 1.3310927897691727e-06

Training epoch-96 batch-35
Running loss of epoch-96 batch-35 = 1.402106136083603e-06

Training epoch-96 batch-36
Running loss of epoch-96 batch-36 = 1.4491379261016846e-06

Training epoch-96 batch-37
Running loss of epoch-96 batch-37 = 1.8563587218523026e-06

Training epoch-96 batch-38
Running loss of epoch-96 batch-38 = 1.1967495083808899e-06

Training epoch-96 batch-39
Running loss of epoch-96 batch-39 = 1.3343524187803268e-06

Training epoch-96 batch-40
Running loss of epoch-96 batch-40 = 1.639360561966896e-06

Training epoch-96 batch-41
Running loss of epoch-96 batch-41 = 1.6111880540847778e-06

Training epoch-96 batch-42
Running loss of epoch-96 batch-42 = 1.1094380170106888e-06

Training epoch-96 batch-43
Running loss of epoch-96 batch-43 = 9.497161954641342e-07

Training epoch-96 batch-44
Running loss of epoch-96 batch-44 = 1.0349322110414505e-06

Training epoch-96 batch-45
Running loss of epoch-96 batch-45 = 9.797513484954834e-07

Training epoch-96 batch-46
Running loss of epoch-96 batch-46 = 1.0582152754068375e-06

Training epoch-96 batch-47
Running loss of epoch-96 batch-47 = 9.159557521343231e-07

Training epoch-96 batch-48
Running loss of epoch-96 batch-48 = 1.412583515048027e-06

Training epoch-96 batch-49
Running loss of epoch-96 batch-49 = 1.105479896068573e-06

Training epoch-96 batch-50
Running loss of epoch-96 batch-50 = 9.294599294662476e-07

Training epoch-96 batch-51
Running loss of epoch-96 batch-51 = 1.839594915509224e-06

Training epoch-96 batch-52
Running loss of epoch-96 batch-52 = 1.6207341104745865e-06

Training epoch-96 batch-53
Running loss of epoch-96 batch-53 = 1.5206169337034225e-06

Training epoch-96 batch-54
Running loss of epoch-96 batch-54 = 1.1492520570755005e-06

Training epoch-96 batch-55
Running loss of epoch-96 batch-55 = 9.615905582904816e-07

Training epoch-96 batch-56
Running loss of epoch-96 batch-56 = 1.4924444258213043e-06

Training epoch-96 batch-57
Running loss of epoch-96 batch-57 = 1.0819640010595322e-06

Training epoch-96 batch-58
Running loss of epoch-96 batch-58 = 1.4407560229301453e-06

Training epoch-96 batch-59
Running loss of epoch-96 batch-59 = 1.1213123798370361e-06

Training epoch-96 batch-60
Running loss of epoch-96 batch-60 = 1.1203810572624207e-06

Training epoch-96 batch-61
Running loss of epoch-96 batch-61 = 1.0789372026920319e-06

Training epoch-96 batch-62
Running loss of epoch-96 batch-62 = 1.4831312000751495e-06

Training epoch-96 batch-63
Running loss of epoch-96 batch-63 = 6.137415766716003e-07

Training epoch-96 batch-64
Running loss of epoch-96 batch-64 = 7.615890353918076e-07

Training epoch-96 batch-65
Running loss of epoch-96 batch-65 = 8.253846317529678e-07

Training epoch-96 batch-66
Running loss of epoch-96 batch-66 = 1.173466444015503e-06

Training epoch-96 batch-67
Running loss of epoch-96 batch-67 = 1.4223624020814896e-06

Training epoch-96 batch-68
Running loss of epoch-96 batch-68 = 1.1580996215343475e-06

Training epoch-96 batch-69
Running loss of epoch-96 batch-69 = 1.2631062418222427e-06

Training epoch-96 batch-70
Running loss of epoch-96 batch-70 = 9.960494935512543e-07

Training epoch-96 batch-71
Running loss of epoch-96 batch-71 = 1.2619420886039734e-06

Training epoch-96 batch-72
Running loss of epoch-96 batch-72 = 1.3937242329120636e-06

Training epoch-96 batch-73
Running loss of epoch-96 batch-73 = 1.239590346813202e-06

Training epoch-96 batch-74
Running loss of epoch-96 batch-74 = 1.1723022907972336e-06

Training epoch-96 batch-75
Running loss of epoch-96 batch-75 = 6.870832294225693e-07

Training epoch-96 batch-76
Running loss of epoch-96 batch-76 = 1.5285331755876541e-06

Training epoch-96 batch-77
Running loss of epoch-96 batch-77 = 9.520445019006729e-07

Training epoch-96 batch-78
Running loss of epoch-96 batch-78 = 5.466863512992859e-07

Training epoch-96 batch-79
Running loss of epoch-96 batch-79 = 1.0188668966293335e-06

Training epoch-96 batch-80
Running loss of epoch-96 batch-80 = 9.278301149606705e-07

Training epoch-96 batch-81
Running loss of epoch-96 batch-81 = 9.723007678985596e-07

Training epoch-96 batch-82
Running loss of epoch-96 batch-82 = 1.0470394045114517e-06

Training epoch-96 batch-83
Running loss of epoch-96 batch-83 = 1.3266690075397491e-06

Training epoch-96 batch-84
Running loss of epoch-96 batch-84 = 9.017530828714371e-07

Training epoch-96 batch-85
Running loss of epoch-96 batch-85 = 1.223525032401085e-06

Training epoch-96 batch-86
Running loss of epoch-96 batch-86 = 8.530914783477783e-07

Training epoch-96 batch-87
Running loss of epoch-96 batch-87 = 1.0861549526453018e-06

Training epoch-96 batch-88
Running loss of epoch-96 batch-88 = 1.0908115655183792e-06

Training epoch-96 batch-89
Running loss of epoch-96 batch-89 = 1.6454141587018967e-06

Training epoch-96 batch-90
Running loss of epoch-96 batch-90 = 1.3378448784351349e-06

Training epoch-96 batch-91
Running loss of epoch-96 batch-91 = 7.951166480779648e-07

Training epoch-96 batch-92
Running loss of epoch-96 batch-92 = 1.0603107511997223e-06

Training epoch-96 batch-93
Running loss of epoch-96 batch-93 = 1.035863533616066e-06

Training epoch-96 batch-94
Running loss of epoch-96 batch-94 = 1.16787850856781e-06

Training epoch-96 batch-95
Running loss of epoch-96 batch-95 = 8.579809218645096e-07

Training epoch-96 batch-96
Running loss of epoch-96 batch-96 = 1.4901161193847656e-06

Training epoch-96 batch-97
Running loss of epoch-96 batch-97 = 1.4647375792264938e-06

Training epoch-96 batch-98
Running loss of epoch-96 batch-98 = 8.526258170604706e-07

Training epoch-96 batch-99
Running loss of epoch-96 batch-99 = 1.0237563401460648e-06

Training epoch-96 batch-100
Running loss of epoch-96 batch-100 = 1.044711098074913e-06

Training epoch-96 batch-101
Running loss of epoch-96 batch-101 = 7.182825356721878e-07

Training epoch-96 batch-102
Running loss of epoch-96 batch-102 = 6.575137376785278e-07

Training epoch-96 batch-103
Running loss of epoch-96 batch-103 = 8.905772119760513e-07

Training epoch-96 batch-104
Running loss of epoch-96 batch-104 = 1.103850081562996e-06

Training epoch-96 batch-105
Running loss of epoch-96 batch-105 = 1.0211952030658722e-06

Training epoch-96 batch-106
Running loss of epoch-96 batch-106 = 8.533243089914322e-07

Training epoch-96 batch-107
Running loss of epoch-96 batch-107 = 5.534384399652481e-07

Training epoch-96 batch-108
Running loss of epoch-96 batch-108 = 1.1548399925231934e-06

Training epoch-96 batch-109
Running loss of epoch-96 batch-109 = 9.620562195777893e-07

Training epoch-96 batch-110
Running loss of epoch-96 batch-110 = 1.5762634575366974e-06

Training epoch-96 batch-111
Running loss of epoch-96 batch-111 = 1.3632234185934067e-06

Training epoch-96 batch-112
Running loss of epoch-96 batch-112 = 9.438954293727875e-07

Training epoch-96 batch-113
Running loss of epoch-96 batch-113 = 1.3343524187803268e-06

Training epoch-96 batch-114
Running loss of epoch-96 batch-114 = 1.1795200407505035e-06

Training epoch-96 batch-115
Running loss of epoch-96 batch-115 = 9.888317435979843e-07

Training epoch-96 batch-116
Running loss of epoch-96 batch-116 = 1.2798700481653214e-06

Training epoch-96 batch-117
Running loss of epoch-96 batch-117 = 8.30506905913353e-07

Training epoch-96 batch-118
Running loss of epoch-96 batch-118 = 1.0668300092220306e-06

Training epoch-96 batch-119
Running loss of epoch-96 batch-119 = 1.3783574104309082e-06

Training epoch-96 batch-120
Running loss of epoch-96 batch-120 = 1.0211952030658722e-06

Training epoch-96 batch-121
Running loss of epoch-96 batch-121 = 9.534414857625961e-07

Training epoch-96 batch-122
Running loss of epoch-96 batch-122 = 7.646158337593079e-07

Training epoch-96 batch-123
Running loss of epoch-96 batch-123 = 1.1476222425699234e-06

Training epoch-96 batch-124
Running loss of epoch-96 batch-124 = 1.0423827916383743e-06

Training epoch-96 batch-125
Running loss of epoch-96 batch-125 = 1.1567026376724243e-06

Training epoch-96 batch-126
Running loss of epoch-96 batch-126 = 1.1657830327749252e-06

Training epoch-96 batch-127
Running loss of epoch-96 batch-127 = 9.415671229362488e-07

Training epoch-96 batch-128
Running loss of epoch-96 batch-128 = 8.454080671072006e-07

Training epoch-96 batch-129
Running loss of epoch-96 batch-129 = 1.3527460396289825e-06

Training epoch-96 batch-130
Running loss of epoch-96 batch-130 = 9.4645656645298e-07

Training epoch-96 batch-131
Running loss of epoch-96 batch-131 = 1.3336539268493652e-06

Training epoch-96 batch-132
Running loss of epoch-96 batch-132 = 9.133946150541306e-07

Training epoch-96 batch-133
Running loss of epoch-96 batch-133 = 9.91160050034523e-07

Training epoch-96 batch-134
Running loss of epoch-96 batch-134 = 1.0700896382331848e-06

Training epoch-96 batch-135
Running loss of epoch-96 batch-135 = 8.693896234035492e-07

Training epoch-96 batch-136
Running loss of epoch-96 batch-136 = 1.0458752512931824e-06

Training epoch-96 batch-137
Running loss of epoch-96 batch-137 = 1.527136191725731e-06

Training epoch-96 batch-138
Running loss of epoch-96 batch-138 = 1.734122633934021e-06

Training epoch-96 batch-139
Running loss of epoch-96 batch-139 = 1.0381918400526047e-06

Training epoch-96 batch-140
Running loss of epoch-96 batch-140 = 1.550186425447464e-06

Training epoch-96 batch-141
Running loss of epoch-96 batch-141 = 1.1795200407505035e-06

Training epoch-96 batch-142
Running loss of epoch-96 batch-142 = 1.6654375940561295e-06

Training epoch-96 batch-143
Running loss of epoch-96 batch-143 = 6.766058504581451e-07

Training epoch-96 batch-144
Running loss of epoch-96 batch-144 = 1.7345882952213287e-06

Training epoch-96 batch-145
Running loss of epoch-96 batch-145 = 1.1511147022247314e-06

Training epoch-96 batch-146
Running loss of epoch-96 batch-146 = 1.3597309589385986e-06

Training epoch-96 batch-147
Running loss of epoch-96 batch-147 = 1.6312114894390106e-06

Training epoch-96 batch-148
Running loss of epoch-96 batch-148 = 1.1364463716745377e-06

Training epoch-96 batch-149
Running loss of epoch-96 batch-149 = 1.3969838619232178e-06

Training epoch-96 batch-150
Running loss of epoch-96 batch-150 = 1.5294644981622696e-06

Training epoch-96 batch-151
Running loss of epoch-96 batch-151 = 1.1578667908906937e-06

Training epoch-96 batch-152
Running loss of epoch-96 batch-152 = 1.3611279428005219e-06

Training epoch-96 batch-153
Running loss of epoch-96 batch-153 = 1.4204997569322586e-06

Training epoch-96 batch-154
Running loss of epoch-96 batch-154 = 1.932261511683464e-06

Training epoch-96 batch-155
Running loss of epoch-96 batch-155 = 1.632142812013626e-06

Training epoch-96 batch-156
Running loss of epoch-96 batch-156 = 9.879004210233688e-07

Training epoch-96 batch-157
Running loss of epoch-96 batch-157 = 6.0014426708221436e-06

Finished training epoch-96.



Average train loss at epoch-96 = 1.189287006855011e-06

Started Evaluation

Average val loss at epoch-96 = 3.321879190833945

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-97


Training epoch-97 batch-1
Running loss of epoch-97 batch-1 = 1.3313256204128265e-06

Training epoch-97 batch-2
Running loss of epoch-97 batch-2 = 1.1899974197149277e-06

Training epoch-97 batch-3
Running loss of epoch-97 batch-3 = 5.632173269987106e-07

Training epoch-97 batch-4
Running loss of epoch-97 batch-4 = 6.603077054023743e-07

Training epoch-97 batch-5
Running loss of epoch-97 batch-5 = 1.1639203876256943e-06

Training epoch-97 batch-6
Running loss of epoch-97 batch-6 = 9.217765182256699e-07

Training epoch-97 batch-7
Running loss of epoch-97 batch-7 = 9.834766387939453e-07

Training epoch-97 batch-8
Running loss of epoch-97 batch-8 = 1.375097781419754e-06

Training epoch-97 batch-9
Running loss of epoch-97 batch-9 = 1.1690426617860794e-06

Training epoch-97 batch-10
Running loss of epoch-97 batch-10 = 1.5201512724161148e-06

Training epoch-97 batch-11
Running loss of epoch-97 batch-11 = 7.739290595054626e-07

Training epoch-97 batch-12
Running loss of epoch-97 batch-12 = 9.771902114152908e-07

Training epoch-97 batch-13
Running loss of epoch-97 batch-13 = 9.003560990095139e-07

Training epoch-97 batch-14
Running loss of epoch-97 batch-14 = 1.0628718882799149e-06

Training epoch-97 batch-15
Running loss of epoch-97 batch-15 = 9.685754776000977e-07

Training epoch-97 batch-16
Running loss of epoch-97 batch-16 = 1.4763791114091873e-06

Training epoch-97 batch-17
Running loss of epoch-97 batch-17 = 1.271488144993782e-06

Training epoch-97 batch-18
Running loss of epoch-97 batch-18 = 9.66247171163559e-07

Training epoch-97 batch-19
Running loss of epoch-97 batch-19 = 1.07521191239357e-06

Training epoch-97 batch-20
Running loss of epoch-97 batch-20 = 8.570495992898941e-07

Training epoch-97 batch-21
Running loss of epoch-97 batch-21 = 9.245704859495163e-07

Training epoch-97 batch-22
Running loss of epoch-97 batch-22 = 1.0242220014333725e-06

Training epoch-97 batch-23
Running loss of epoch-97 batch-23 = 1.492910087108612e-06

Training epoch-97 batch-24
Running loss of epoch-97 batch-24 = 1.234235242009163e-06

Training epoch-97 batch-25
Running loss of epoch-97 batch-25 = 1.4295801520347595e-06

Training epoch-97 batch-26
Running loss of epoch-97 batch-26 = 1.0184012353420258e-06

Training epoch-97 batch-27
Running loss of epoch-97 batch-27 = 1.4831312000751495e-06

Training epoch-97 batch-28
Running loss of epoch-97 batch-28 = 1.1711381375789642e-06

Training epoch-97 batch-29
Running loss of epoch-97 batch-29 = 7.352791726589203e-07

Training epoch-97 batch-30
Running loss of epoch-97 batch-30 = 1.1506490409374237e-06

Training epoch-97 batch-31
Running loss of epoch-97 batch-31 = 1.1296942830085754e-06

Training epoch-97 batch-32
Running loss of epoch-97 batch-32 = 8.780043572187424e-07

Training epoch-97 batch-33
Running loss of epoch-97 batch-33 = 8.645001798868179e-07

Training epoch-97 batch-34
Running loss of epoch-97 batch-34 = 1.4987308531999588e-06

Training epoch-97 batch-35
Running loss of epoch-97 batch-35 = 1.394888386130333e-06

Training epoch-97 batch-36
Running loss of epoch-97 batch-36 = 1.2738164514303207e-06

Training epoch-97 batch-37
Running loss of epoch-97 batch-37 = 1.4547258615493774e-06

Training epoch-97 batch-38
Running loss of epoch-97 batch-38 = 1.298263669013977e-06

Training epoch-97 batch-39
Running loss of epoch-97 batch-39 = 1.1294614523649216e-06

Training epoch-97 batch-40
Running loss of epoch-97 batch-40 = 8.672941476106644e-07

Training epoch-97 batch-41
Running loss of epoch-97 batch-41 = 8.798670023679733e-07

Training epoch-97 batch-42
Running loss of epoch-97 batch-42 = 8.952338248491287e-07

Training epoch-97 batch-43
Running loss of epoch-97 batch-43 = 7.543712854385376e-07

Training epoch-97 batch-44
Running loss of epoch-97 batch-44 = 8.782371878623962e-07

Training epoch-97 batch-45
Running loss of epoch-97 batch-45 = 1.0600779205560684e-06

Training epoch-97 batch-46
Running loss of epoch-97 batch-46 = 1.44285149872303e-06

Training epoch-97 batch-47
Running loss of epoch-97 batch-47 = 1.28219835460186e-06

Training epoch-97 batch-48
Running loss of epoch-97 batch-48 = 1.7709098756313324e-06

Training epoch-97 batch-49
Running loss of epoch-97 batch-49 = 1.8065329641103745e-06

Training epoch-97 batch-50
Running loss of epoch-97 batch-50 = 1.244712620973587e-06

Training epoch-97 batch-51
Running loss of epoch-97 batch-51 = 1.4414545148611069e-06

Training epoch-97 batch-52
Running loss of epoch-97 batch-52 = 1.3695098459720612e-06

Training epoch-97 batch-53
Running loss of epoch-97 batch-53 = 1.0847579687833786e-06

Training epoch-97 batch-54
Running loss of epoch-97 batch-54 = 9.578652679920197e-07

Training epoch-97 batch-55
Running loss of epoch-97 batch-55 = 1.0605435818433762e-06

Training epoch-97 batch-56
Running loss of epoch-97 batch-56 = 9.047798812389374e-07

Training epoch-97 batch-57
Running loss of epoch-97 batch-57 = 1.580454409122467e-06

Training epoch-97 batch-58
Running loss of epoch-97 batch-58 = 1.5902332961559296e-06

Training epoch-97 batch-59
Running loss of epoch-97 batch-59 = 1.317821443080902e-06

Training epoch-97 batch-60
Running loss of epoch-97 batch-60 = 1.5052501112222672e-06

Training epoch-97 batch-61
Running loss of epoch-97 batch-61 = 1.1639203876256943e-06

Training epoch-97 batch-62
Running loss of epoch-97 batch-62 = 1.4731194823980331e-06

Training epoch-97 batch-63
Running loss of epoch-97 batch-63 = 1.3813842087984085e-06

Training epoch-97 batch-64
Running loss of epoch-97 batch-64 = 9.101349860429764e-07

Training epoch-97 batch-65
Running loss of epoch-97 batch-65 = 1.2309756129980087e-06

Training epoch-97 batch-66
Running loss of epoch-97 batch-66 = 8.55419784784317e-07

Training epoch-97 batch-67
Running loss of epoch-97 batch-67 = 7.301568984985352e-07

Training epoch-97 batch-68
Running loss of epoch-97 batch-68 = 9.799841791391373e-07

Training epoch-97 batch-69
Running loss of epoch-97 batch-69 = 1.2563541531562805e-06

Training epoch-97 batch-70
Running loss of epoch-97 batch-70 = 1.2677628546953201e-06

Training epoch-97 batch-71
Running loss of epoch-97 batch-71 = 1.082196831703186e-06

Training epoch-97 batch-72
Running loss of epoch-97 batch-72 = 1.1511147022247314e-06

Training epoch-97 batch-73
Running loss of epoch-97 batch-73 = 8.509960025548935e-07

Training epoch-97 batch-74
Running loss of epoch-97 batch-74 = 9.504146873950958e-07

Training epoch-97 batch-75
Running loss of epoch-97 batch-75 = 1.427019014954567e-06

Training epoch-97 batch-76
Running loss of epoch-97 batch-76 = 1.341337338089943e-06

Training epoch-97 batch-77
Running loss of epoch-97 batch-77 = 1.149950549006462e-06

Training epoch-97 batch-78
Running loss of epoch-97 batch-78 = 1.205597072839737e-06

Training epoch-97 batch-79
Running loss of epoch-97 batch-79 = 9.564682841300964e-07

Training epoch-97 batch-80
Running loss of epoch-97 batch-80 = 9.292270988225937e-07

Training epoch-97 batch-81
Running loss of epoch-97 batch-81 = 1.421663910150528e-06

Training epoch-97 batch-82
Running loss of epoch-97 batch-82 = 1.1208467185497284e-06

Training epoch-97 batch-83
Running loss of epoch-97 batch-83 = 1.0672956705093384e-06

Training epoch-97 batch-84
Running loss of epoch-97 batch-84 = 1.0945368558168411e-06

Training epoch-97 batch-85
Running loss of epoch-97 batch-85 = 6.097834557294846e-07

Training epoch-97 batch-86
Running loss of epoch-97 batch-86 = 1.042848452925682e-06

Training epoch-97 batch-87
Running loss of epoch-97 batch-87 = 1.534121111035347e-06

Training epoch-97 batch-88
Running loss of epoch-97 batch-88 = 1.3264361768960953e-06

Training epoch-97 batch-89
Running loss of epoch-97 batch-89 = 1.4887191355228424e-06

Training epoch-97 batch-90
Running loss of epoch-97 batch-90 = 9.75094735622406e-07

Training epoch-97 batch-91
Running loss of epoch-97 batch-91 = 1.310836523771286e-06

Training epoch-97 batch-92
Running loss of epoch-97 batch-92 = 9.615905582904816e-07

Training epoch-97 batch-93
Running loss of epoch-97 batch-93 = 1.1387746781110764e-06

Training epoch-97 batch-94
Running loss of epoch-97 batch-94 = 9.73464921116829e-07

Training epoch-97 batch-95
Running loss of epoch-97 batch-95 = 1.2509990483522415e-06

Training epoch-97 batch-96
Running loss of epoch-97 batch-96 = 1.4242250472307205e-06

Training epoch-97 batch-97
Running loss of epoch-97 batch-97 = 5.781184881925583e-07

Training epoch-97 batch-98
Running loss of epoch-97 batch-98 = 8.323695510625839e-07

Training epoch-97 batch-99
Running loss of epoch-97 batch-99 = 1.146458089351654e-06

Training epoch-97 batch-100
Running loss of epoch-97 batch-100 = 1.2419186532497406e-06

Training epoch-97 batch-101
Running loss of epoch-97 batch-101 = 1.2277159839868546e-06

Training epoch-97 batch-102
Running loss of epoch-97 batch-102 = 1.141102984547615e-06

Training epoch-97 batch-103
Running loss of epoch-97 batch-103 = 1.8202699720859528e-06

Training epoch-97 batch-104
Running loss of epoch-97 batch-104 = 1.5154946595430374e-06

Training epoch-97 batch-105
Running loss of epoch-97 batch-105 = 1.0794028639793396e-06

Training epoch-97 batch-106
Running loss of epoch-97 batch-106 = 9.327195584774017e-07

Training epoch-97 batch-107
Running loss of epoch-97 batch-107 = 1.2542586773633957e-06

Training epoch-97 batch-108
Running loss of epoch-97 batch-108 = 1.0158400982618332e-06

Training epoch-97 batch-109
Running loss of epoch-97 batch-109 = 1.1224765330553055e-06

Training epoch-97 batch-110
Running loss of epoch-97 batch-110 = 1.153675839304924e-06

Training epoch-97 batch-111
Running loss of epoch-97 batch-111 = 8.367933332920074e-07

Training epoch-97 batch-112
Running loss of epoch-97 batch-112 = 1.5757977962493896e-06

Training epoch-97 batch-113
Running loss of epoch-97 batch-113 = 1.0423827916383743e-06

Training epoch-97 batch-114
Running loss of epoch-97 batch-114 = 7.692724466323853e-07

Training epoch-97 batch-115
Running loss of epoch-97 batch-115 = 1.1334195733070374e-06

Training epoch-97 batch-116
Running loss of epoch-97 batch-116 = 1.1266674846410751e-06

Training epoch-97 batch-117
Running loss of epoch-97 batch-117 = 1.4174729585647583e-06

Training epoch-97 batch-118
Running loss of epoch-97 batch-118 = 1.8461141735315323e-06

Training epoch-97 batch-119
Running loss of epoch-97 batch-119 = 1.3795215636491776e-06

Training epoch-97 batch-120
Running loss of epoch-97 batch-120 = 7.71600753068924e-07

Training epoch-97 batch-121
Running loss of epoch-97 batch-121 = 1.2989621609449387e-06

Training epoch-97 batch-122
Running loss of epoch-97 batch-122 = 1.021428033709526e-06

Training epoch-97 batch-123
Running loss of epoch-97 batch-123 = 2.184184268116951e-06

Training epoch-97 batch-124
Running loss of epoch-97 batch-124 = 8.069910109043121e-07

Training epoch-97 batch-125
Running loss of epoch-97 batch-125 = 1.2207310646772385e-06

Training epoch-97 batch-126
Running loss of epoch-97 batch-126 = 1.1348165571689606e-06

Training epoch-97 batch-127
Running loss of epoch-97 batch-127 = 1.8016435205936432e-06

Training epoch-97 batch-128
Running loss of epoch-97 batch-128 = 1.6123522073030472e-06

Training epoch-97 batch-129
Running loss of epoch-97 batch-129 = 1.0246876627206802e-06

Training epoch-97 batch-130
Running loss of epoch-97 batch-130 = 1.0789372026920319e-06

Training epoch-97 batch-131
Running loss of epoch-97 batch-131 = 7.017515599727631e-07

Training epoch-97 batch-132
Running loss of epoch-97 batch-132 = 9.399373084306717e-07

Training epoch-97 batch-133
Running loss of epoch-97 batch-133 = 1.1648517102003098e-06

Training epoch-97 batch-134
Running loss of epoch-97 batch-134 = 7.664784789085388e-07

Training epoch-97 batch-135
Running loss of epoch-97 batch-135 = 1.375097781419754e-06

Training epoch-97 batch-136
Running loss of epoch-97 batch-136 = 1.0791700333356857e-06

Training epoch-97 batch-137
Running loss of epoch-97 batch-137 = 9.578652679920197e-07

Training epoch-97 batch-138
Running loss of epoch-97 batch-138 = 1.0402873158454895e-06

Training epoch-97 batch-139
Running loss of epoch-97 batch-139 = 1.0759104043245316e-06

Training epoch-97 batch-140
Running loss of epoch-97 batch-140 = 1.437496393918991e-06

Training epoch-97 batch-141
Running loss of epoch-97 batch-141 = 1.5245750546455383e-06

Training epoch-97 batch-142
Running loss of epoch-97 batch-142 = 1.334119588136673e-06

Training epoch-97 batch-143
Running loss of epoch-97 batch-143 = 1.3685785233974457e-06

Training epoch-97 batch-144
Running loss of epoch-97 batch-144 = 9.760260581970215e-07

Training epoch-97 batch-145
Running loss of epoch-97 batch-145 = 1.0605435818433762e-06

Training epoch-97 batch-146
Running loss of epoch-97 batch-146 = 1.384178176522255e-06

Training epoch-97 batch-147
Running loss of epoch-97 batch-147 = 1.4039687812328339e-06

Training epoch-97 batch-148
Running loss of epoch-97 batch-148 = 1.1343508958816528e-06

Training epoch-97 batch-149
Running loss of epoch-97 batch-149 = 1.2051314115524292e-06

Training epoch-97 batch-150
Running loss of epoch-97 batch-150 = 1.0207295417785645e-06

Training epoch-97 batch-151
Running loss of epoch-97 batch-151 = 1.325272023677826e-06

Training epoch-97 batch-152
Running loss of epoch-97 batch-152 = 1.3746321201324463e-06

Training epoch-97 batch-153
Running loss of epoch-97 batch-153 = 1.0945368558168411e-06

Training epoch-97 batch-154
Running loss of epoch-97 batch-154 = 1.2263190001249313e-06

Training epoch-97 batch-155
Running loss of epoch-97 batch-155 = 1.2819655239582062e-06

Training epoch-97 batch-156
Running loss of epoch-97 batch-156 = 9.811483323574066e-07

Training epoch-97 batch-157
Running loss of epoch-97 batch-157 = 5.170702934265137e-06

Finished training epoch-97.



Average train loss at epoch-97 = 1.1724591255187988e-06

Started Evaluation

Average val loss at epoch-97 = 3.3262245498205485

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-98


Training epoch-98 batch-1
Running loss of epoch-98 batch-1 = 9.87667590379715e-07

Training epoch-98 batch-2
Running loss of epoch-98 batch-2 = 8.281785994768143e-07

Training epoch-98 batch-3
Running loss of epoch-98 batch-3 = 1.1939555406570435e-06

Training epoch-98 batch-4
Running loss of epoch-98 batch-4 = 1.0346993803977966e-06

Training epoch-98 batch-5
Running loss of epoch-98 batch-5 = 1.2924429029226303e-06

Training epoch-98 batch-6
Running loss of epoch-98 batch-6 = 9.632203727960587e-07

Training epoch-98 batch-7
Running loss of epoch-98 batch-7 = 1.4272518455982208e-06

Training epoch-98 batch-8
Running loss of epoch-98 batch-8 = 1.1182855814695358e-06

Training epoch-98 batch-9
Running loss of epoch-98 batch-9 = 1.1802185326814651e-06

Training epoch-98 batch-10
Running loss of epoch-98 batch-10 = 1.7504207789897919e-06

Training epoch-98 batch-11
Running loss of epoch-98 batch-11 = 1.4174729585647583e-06

Training epoch-98 batch-12
Running loss of epoch-98 batch-12 = 1.071486622095108e-06

Training epoch-98 batch-13
Running loss of epoch-98 batch-13 = 1.3224780559539795e-06

Training epoch-98 batch-14
Running loss of epoch-98 batch-14 = 1.308973878622055e-06

Training epoch-98 batch-15
Running loss of epoch-98 batch-15 = 1.4174729585647583e-06

Training epoch-98 batch-16
Running loss of epoch-98 batch-16 = 8.759088814258575e-07

Training epoch-98 batch-17
Running loss of epoch-98 batch-17 = 9.73464921116829e-07

Training epoch-98 batch-18
Running loss of epoch-98 batch-18 = 1.2866221368312836e-06

Training epoch-98 batch-19
Running loss of epoch-98 batch-19 = 1.1080410331487656e-06

Training epoch-98 batch-20
Running loss of epoch-98 batch-20 = 1.1816155165433884e-06

Training epoch-98 batch-21
Running loss of epoch-98 batch-21 = 8.111819624900818e-07

Training epoch-98 batch-22
Running loss of epoch-98 batch-22 = 9.443610906600952e-07

Training epoch-98 batch-23
Running loss of epoch-98 batch-23 = 7.988419383764267e-07

Training epoch-98 batch-24
Running loss of epoch-98 batch-24 = 1.134118065237999e-06

Training epoch-98 batch-25
Running loss of epoch-98 batch-25 = 1.402106136083603e-06

Training epoch-98 batch-26
Running loss of epoch-98 batch-26 = 1.1490192264318466e-06

Training epoch-98 batch-27
Running loss of epoch-98 batch-27 = 9.580980986356735e-07

Training epoch-98 batch-28
Running loss of epoch-98 batch-28 = 7.618218660354614e-07

Training epoch-98 batch-29
Running loss of epoch-98 batch-29 = 9.69739630818367e-07

Training epoch-98 batch-30
Running loss of epoch-98 batch-30 = 6.42845407128334e-07

Training epoch-98 batch-31
Running loss of epoch-98 batch-31 = 9.80449840426445e-07

Training epoch-98 batch-32
Running loss of epoch-98 batch-32 = 1.3862736523151398e-06

Training epoch-98 batch-33
Running loss of epoch-98 batch-33 = 8.468050509691238e-07

Training epoch-98 batch-34
Running loss of epoch-98 batch-34 = 9.469222277402878e-07

Training epoch-98 batch-35
Running loss of epoch-98 batch-35 = 1.200009137392044e-06

Training epoch-98 batch-36
Running loss of epoch-98 batch-36 = 6.842892616987228e-07

Training epoch-98 batch-37
Running loss of epoch-98 batch-37 = 1.3262033462524414e-06

Training epoch-98 batch-38
Running loss of epoch-98 batch-38 = 1.1671800166368484e-06

Training epoch-98 batch-39
Running loss of epoch-98 batch-39 = 6.258487701416016e-07

Training epoch-98 batch-40
Running loss of epoch-98 batch-40 = 1.08242966234684e-06

Training epoch-98 batch-41
Running loss of epoch-98 batch-41 = 1.4619436115026474e-06

Training epoch-98 batch-42
Running loss of epoch-98 batch-42 = 1.2051314115524292e-06

Training epoch-98 batch-43
Running loss of epoch-98 batch-43 = 1.150183379650116e-06

Training epoch-98 batch-44
Running loss of epoch-98 batch-44 = 1.0998919606208801e-06

Training epoch-98 batch-45
Running loss of epoch-98 batch-45 = 1.1385418474674225e-06

Training epoch-98 batch-46
Running loss of epoch-98 batch-46 = 7.739290595054626e-07

Training epoch-98 batch-47
Running loss of epoch-98 batch-47 = 1.277076080441475e-06

Training epoch-98 batch-48
Running loss of epoch-98 batch-48 = 1.0726507753133774e-06

Training epoch-98 batch-49
Running loss of epoch-98 batch-49 = 1.6707926988601685e-06

Training epoch-98 batch-50
Running loss of epoch-98 batch-50 = 1.1327210813760757e-06

Training epoch-98 batch-51
Running loss of epoch-98 batch-51 = 6.223563104867935e-07

Training epoch-98 batch-52
Running loss of epoch-98 batch-52 = 9.736977517604828e-07

Training epoch-98 batch-53
Running loss of epoch-98 batch-53 = 1.8060673028230667e-06

Training epoch-98 batch-54
Running loss of epoch-98 batch-54 = 1.1166557669639587e-06

Training epoch-98 batch-55
Running loss of epoch-98 batch-55 = 1.9364524632692337e-06

Training epoch-98 batch-56
Running loss of epoch-98 batch-56 = 9.632203727960587e-07

Training epoch-98 batch-57
Running loss of epoch-98 batch-57 = 1.1755619198083878e-06

Training epoch-98 batch-58
Running loss of epoch-98 batch-58 = 1.3248063623905182e-06

Training epoch-98 batch-59
Running loss of epoch-98 batch-59 = 1.584179699420929e-06

Training epoch-98 batch-60
Running loss of epoch-98 batch-60 = 1.5755649656057358e-06

Training epoch-98 batch-61
Running loss of epoch-98 batch-61 = 8.963979780673981e-07

Training epoch-98 batch-62
Running loss of epoch-98 batch-62 = 1.3171229511499405e-06

Training epoch-98 batch-63
Running loss of epoch-98 batch-63 = 1.4510005712509155e-06

Training epoch-98 batch-64
Running loss of epoch-98 batch-64 = 1.07521191239357e-06

Training epoch-98 batch-65
Running loss of epoch-98 batch-65 = 1.1052470654249191e-06

Training epoch-98 batch-66
Running loss of epoch-98 batch-66 = 1.1208467185497284e-06

Training epoch-98 batch-67
Running loss of epoch-98 batch-67 = 4.291068762540817e-07

Training epoch-98 batch-68
Running loss of epoch-98 batch-68 = 1.1755619198083878e-06

Training epoch-98 batch-69
Running loss of epoch-98 batch-69 = 1.1122319847345352e-06

Training epoch-98 batch-70
Running loss of epoch-98 batch-70 = 1.5783589333295822e-06

Training epoch-98 batch-71
Running loss of epoch-98 batch-71 = 7.702037692070007e-07

Training epoch-98 batch-72
Running loss of epoch-98 batch-72 = 1.2759119272232056e-06

Training epoch-98 batch-73
Running loss of epoch-98 batch-73 = 1.0237563401460648e-06

Training epoch-98 batch-74
Running loss of epoch-98 batch-74 = 9.799841791391373e-07

Training epoch-98 batch-75
Running loss of epoch-98 batch-75 = 1.042848452925682e-06

Training epoch-98 batch-76
Running loss of epoch-98 batch-76 = 8.684583008289337e-07

Training epoch-98 batch-77
Running loss of epoch-98 batch-77 = 9.264331310987473e-07

Training epoch-98 batch-78
Running loss of epoch-98 batch-78 = 8.074566721916199e-07

Training epoch-98 batch-79
Running loss of epoch-98 batch-79 = 1.3175886124372482e-06

Training epoch-98 batch-80
Running loss of epoch-98 batch-80 = 9.117648005485535e-07

Training epoch-98 batch-81
Running loss of epoch-98 batch-81 = 8.537899702787399e-07

Training epoch-98 batch-82
Running loss of epoch-98 batch-82 = 1.0526273399591446e-06

Training epoch-98 batch-83
Running loss of epoch-98 batch-83 = 1.1755619198083878e-06

Training epoch-98 batch-84
Running loss of epoch-98 batch-84 = 1.1408701539039612e-06

Training epoch-98 batch-85
Running loss of epoch-98 batch-85 = 1.0705552995204926e-06

Training epoch-98 batch-86
Running loss of epoch-98 batch-86 = 1.5581026673316956e-06

Training epoch-98 batch-87
Running loss of epoch-98 batch-87 = 9.557697921991348e-07

Training epoch-98 batch-88
Running loss of epoch-98 batch-88 = 1.1962838470935822e-06

Training epoch-98 batch-89
Running loss of epoch-98 batch-89 = 7.362104952335358e-07

Training epoch-98 batch-90
Running loss of epoch-98 batch-90 = 1.2728851288557053e-06

Training epoch-98 batch-91
Running loss of epoch-98 batch-91 = 2.0079314708709717e-06

Training epoch-98 batch-92
Running loss of epoch-98 batch-92 = 1.4991965144872665e-06

Training epoch-98 batch-93
Running loss of epoch-98 batch-93 = 1.600012183189392e-06

Training epoch-98 batch-94
Running loss of epoch-98 batch-94 = 1.2172386050224304e-06

Training epoch-98 batch-95
Running loss of epoch-98 batch-95 = 1.2388918548822403e-06

Training epoch-98 batch-96
Running loss of epoch-98 batch-96 = 1.232139766216278e-06

Training epoch-98 batch-97
Running loss of epoch-98 batch-97 = 4.866160452365875e-07

Training epoch-98 batch-98
Running loss of epoch-98 batch-98 = 1.3315584510564804e-06

Training epoch-98 batch-99
Running loss of epoch-98 batch-99 = 1.1941883713006973e-06

Training epoch-98 batch-100
Running loss of epoch-98 batch-100 = 1.0388903319835663e-06

Training epoch-98 batch-101
Running loss of epoch-98 batch-101 = 6.475020200014114e-07

Training epoch-98 batch-102
Running loss of epoch-98 batch-102 = 1.1932570487260818e-06

Training epoch-98 batch-103
Running loss of epoch-98 batch-103 = 8.011702448129654e-07

Training epoch-98 batch-104
Running loss of epoch-98 batch-104 = 1.7499551177024841e-06

Training epoch-98 batch-105
Running loss of epoch-98 batch-105 = 1.2780074030160904e-06

Training epoch-98 batch-106
Running loss of epoch-98 batch-106 = 1.3066455721855164e-06

Training epoch-98 batch-107
Running loss of epoch-98 batch-107 = 1.419801265001297e-06

Training epoch-98 batch-108
Running loss of epoch-98 batch-108 = 1.2558884918689728e-06

Training epoch-98 batch-109
Running loss of epoch-98 batch-109 = 7.718335837125778e-07

Training epoch-98 batch-110
Running loss of epoch-98 batch-110 = 8.568167686462402e-07

Training epoch-98 batch-111
Running loss of epoch-98 batch-111 = 1.6666017472743988e-06

Training epoch-98 batch-112
Running loss of epoch-98 batch-112 = 1.0414514690637589e-06

Training epoch-98 batch-113
Running loss of epoch-98 batch-113 = 8.228234946727753e-07

Training epoch-98 batch-114
Running loss of epoch-98 batch-114 = 1.1667143553495407e-06

Training epoch-98 batch-115
Running loss of epoch-98 batch-115 = 1.1636875569820404e-06

Training epoch-98 batch-116
Running loss of epoch-98 batch-116 = 1.1636875569820404e-06

Training epoch-98 batch-117
Running loss of epoch-98 batch-117 = 1.1043157428503036e-06

Training epoch-98 batch-118
Running loss of epoch-98 batch-118 = 1.1888332664966583e-06

Training epoch-98 batch-119
Running loss of epoch-98 batch-119 = 8.682254701852798e-07

Training epoch-98 batch-120
Running loss of epoch-98 batch-120 = 1.7851125448942184e-06

Training epoch-98 batch-121
Running loss of epoch-98 batch-121 = 1.2759119272232056e-06

Training epoch-98 batch-122
Running loss of epoch-98 batch-122 = 1.0628718882799149e-06

Training epoch-98 batch-123
Running loss of epoch-98 batch-123 = 1.1599622666835785e-06

Training epoch-98 batch-124
Running loss of epoch-98 batch-124 = 8.684583008289337e-07

Training epoch-98 batch-125
Running loss of epoch-98 batch-125 = 1.3003591448068619e-06

Training epoch-98 batch-126
Running loss of epoch-98 batch-126 = 1.5757977962493896e-06

Training epoch-98 batch-127
Running loss of epoch-98 batch-127 = 1.5844125300645828e-06

Training epoch-98 batch-128
Running loss of epoch-98 batch-128 = 1.0277144610881805e-06

Training epoch-98 batch-129
Running loss of epoch-98 batch-129 = 1.0915100574493408e-06

Training epoch-98 batch-130
Running loss of epoch-98 batch-130 = 1.3587996363639832e-06

Training epoch-98 batch-131
Running loss of epoch-98 batch-131 = 9.720679372549057e-07

Training epoch-98 batch-132
Running loss of epoch-98 batch-132 = 1.9005965441465378e-06

Training epoch-98 batch-133
Running loss of epoch-98 batch-133 = 1.2707896530628204e-06

Training epoch-98 batch-134
Running loss of epoch-98 batch-134 = 1.3490207493305206e-06

Training epoch-98 batch-135
Running loss of epoch-98 batch-135 = 8.477363735437393e-07

Training epoch-98 batch-136
Running loss of epoch-98 batch-136 = 1.2211967259645462e-06

Training epoch-98 batch-137
Running loss of epoch-98 batch-137 = 1.027015969157219e-06

Training epoch-98 batch-138
Running loss of epoch-98 batch-138 = 9.099021553993225e-07

Training epoch-98 batch-139
Running loss of epoch-98 batch-139 = 1.2384261935949326e-06

Training epoch-98 batch-140
Running loss of epoch-98 batch-140 = 6.353948265314102e-07

Training epoch-98 batch-141
Running loss of epoch-98 batch-141 = 8.835922926664352e-07

Training epoch-98 batch-142
Running loss of epoch-98 batch-142 = 9.860377758741379e-07

Training epoch-98 batch-143
Running loss of epoch-98 batch-143 = 1.3976823538541794e-06

Training epoch-98 batch-144
Running loss of epoch-98 batch-144 = 1.062639057636261e-06

Training epoch-98 batch-145
Running loss of epoch-98 batch-145 = 1.0244548320770264e-06

Training epoch-98 batch-146
Running loss of epoch-98 batch-146 = 1.005595549941063e-06

Training epoch-98 batch-147
Running loss of epoch-98 batch-147 = 1.2577511370182037e-06

Training epoch-98 batch-148
Running loss of epoch-98 batch-148 = 1.6095582395792007e-06

Training epoch-98 batch-149
Running loss of epoch-98 batch-149 = 1.846812665462494e-06

Training epoch-98 batch-150
Running loss of epoch-98 batch-150 = 8.82195308804512e-07

Training epoch-98 batch-151
Running loss of epoch-98 batch-151 = 1.0081566870212555e-06

Training epoch-98 batch-152
Running loss of epoch-98 batch-152 = 1.1301599442958832e-06

Training epoch-98 batch-153
Running loss of epoch-98 batch-153 = 1.6081612557172775e-06

Training epoch-98 batch-154
Running loss of epoch-98 batch-154 = 1.025153324007988e-06

Training epoch-98 batch-155
Running loss of epoch-98 batch-155 = 1.0030344128608704e-06

Training epoch-98 batch-156
Running loss of epoch-98 batch-156 = 1.3604294508695602e-06

Training epoch-98 batch-157
Running loss of epoch-98 batch-157 = 2.4475157260894775e-06

Finished training epoch-98.



Average train loss at epoch-98 = 1.1539086699485779e-06

Started Evaluation

Average val loss at epoch-98 = 3.3283030202514245

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-99


Training epoch-99 batch-1
Running loss of epoch-99 batch-1 = 8.218921720981598e-07

Training epoch-99 batch-2
Running loss of epoch-99 batch-2 = 1.2936070561408997e-06

Training epoch-99 batch-3
Running loss of epoch-99 batch-3 = 8.074566721916199e-07

Training epoch-99 batch-4
Running loss of epoch-99 batch-4 = 1.1024530977010727e-06

Training epoch-99 batch-5
Running loss of epoch-99 batch-5 = 1.1692754924297333e-06

Training epoch-99 batch-6
Running loss of epoch-99 batch-6 = 1.5855766832828522e-06

Training epoch-99 batch-7
Running loss of epoch-99 batch-7 = 1.1832453310489655e-06

Training epoch-99 batch-8
Running loss of epoch-99 batch-8 = 8.950009942054749e-07

Training epoch-99 batch-9
Running loss of epoch-99 batch-9 = 1.1674128472805023e-06

Training epoch-99 batch-10
Running loss of epoch-99 batch-10 = 7.813796401023865e-07

Training epoch-99 batch-11
Running loss of epoch-99 batch-11 = 9.599607437849045e-07

Training epoch-99 batch-12
Running loss of epoch-99 batch-12 = 9.171199053525925e-07

Training epoch-99 batch-13
Running loss of epoch-99 batch-13 = 1.3918615877628326e-06

Training epoch-99 batch-14
Running loss of epoch-99 batch-14 = 9.427312761545181e-07

Training epoch-99 batch-15
Running loss of epoch-99 batch-15 = 8.87550413608551e-07

Training epoch-99 batch-16
Running loss of epoch-99 batch-16 = 1.3792887330055237e-06

Training epoch-99 batch-17
Running loss of epoch-99 batch-17 = 1.6617123037576675e-06

Training epoch-99 batch-18
Running loss of epoch-99 batch-18 = 9.776558727025986e-07

Training epoch-99 batch-19
Running loss of epoch-99 batch-19 = 1.1997763067483902e-06

Training epoch-99 batch-20
Running loss of epoch-99 batch-20 = 7.848720997571945e-07

Training epoch-99 batch-21
Running loss of epoch-99 batch-21 = 1.2330710887908936e-06

Training epoch-99 batch-22
Running loss of epoch-99 batch-22 = 1.1548399925231934e-06

Training epoch-99 batch-23
Running loss of epoch-99 batch-23 = 1.2987293303012848e-06

Training epoch-99 batch-24
Running loss of epoch-99 batch-24 = 1.335982233285904e-06

Training epoch-99 batch-25
Running loss of epoch-99 batch-25 = 1.323409378528595e-06

Training epoch-99 batch-26
Running loss of epoch-99 batch-26 = 1.8887221813201904e-06

Training epoch-99 batch-27
Running loss of epoch-99 batch-27 = 1.5222467482089996e-06

Training epoch-99 batch-28
Running loss of epoch-99 batch-28 = 1.2076925486326218e-06

Training epoch-99 batch-29
Running loss of epoch-99 batch-29 = 8.849892765283585e-07

Training epoch-99 batch-30
Running loss of epoch-99 batch-30 = 8.952338248491287e-07

Training epoch-99 batch-31
Running loss of epoch-99 batch-31 = 1.1255033314228058e-06

Training epoch-99 batch-32
Running loss of epoch-99 batch-32 = 1.1280644685029984e-06

Training epoch-99 batch-33
Running loss of epoch-99 batch-33 = 1.12876296043396e-06

Training epoch-99 batch-34
Running loss of epoch-99 batch-34 = 8.142087608575821e-07

Training epoch-99 batch-35
Running loss of epoch-99 batch-35 = 1.4272518455982208e-06

Training epoch-99 batch-36
Running loss of epoch-99 batch-36 = 1.2763775885105133e-06

Training epoch-99 batch-37
Running loss of epoch-99 batch-37 = 1.2586824595928192e-06

Training epoch-99 batch-38
Running loss of epoch-99 batch-38 = 1.098262146115303e-06

Training epoch-99 batch-39
Running loss of epoch-99 batch-39 = 1.2852251529693604e-06

Training epoch-99 batch-40
Running loss of epoch-99 batch-40 = 1.541338860988617e-06

Training epoch-99 batch-41
Running loss of epoch-99 batch-41 = 8.353963494300842e-07

Training epoch-99 batch-42
Running loss of epoch-99 batch-42 = 1.7932616174221039e-06

Training epoch-99 batch-43
Running loss of epoch-99 batch-43 = 7.448252290487289e-07

Training epoch-99 batch-44
Running loss of epoch-99 batch-44 = 1.0621733963489532e-06

Training epoch-99 batch-45
Running loss of epoch-99 batch-45 = 9.94652509689331e-07

Training epoch-99 batch-46
Running loss of epoch-99 batch-46 = 1.1550728231668472e-06

Training epoch-99 batch-47
Running loss of epoch-99 batch-47 = 9.704381227493286e-07

Training epoch-99 batch-48
Running loss of epoch-99 batch-48 = 1.11432746052742e-06

Training epoch-99 batch-49
Running loss of epoch-99 batch-49 = 7.234048098325729e-07

Training epoch-99 batch-50
Running loss of epoch-99 batch-50 = 8.505303412675858e-07

Training epoch-99 batch-51
Running loss of epoch-99 batch-51 = 1.0498333722352982e-06

Training epoch-99 batch-52
Running loss of epoch-99 batch-52 = 1.1173542588949203e-06

Training epoch-99 batch-53
Running loss of epoch-99 batch-53 = 7.543712854385376e-07

Training epoch-99 batch-54
Running loss of epoch-99 batch-54 = 1.4086253941059113e-06

Training epoch-99 batch-55
Running loss of epoch-99 batch-55 = 4.814937710762024e-07

Training epoch-99 batch-56
Running loss of epoch-99 batch-56 = 7.431954145431519e-07

Training epoch-99 batch-57
Running loss of epoch-99 batch-57 = 7.830094546079636e-07

Training epoch-99 batch-58
Running loss of epoch-99 batch-58 = 8.956994861364365e-07

Training epoch-99 batch-59
Running loss of epoch-99 batch-59 = 7.462222129106522e-07

Training epoch-99 batch-60
Running loss of epoch-99 batch-60 = 8.055940270423889e-07

Training epoch-99 batch-61
Running loss of epoch-99 batch-61 = 6.789341568946838e-07

Training epoch-99 batch-62
Running loss of epoch-99 batch-62 = 1.1688098311424255e-06

Training epoch-99 batch-63
Running loss of epoch-99 batch-63 = 1.4619436115026474e-06

Training epoch-99 batch-64
Running loss of epoch-99 batch-64 = 1.2521632015705109e-06

Training epoch-99 batch-65
Running loss of epoch-99 batch-65 = 1.2631062418222427e-06

Training epoch-99 batch-66
Running loss of epoch-99 batch-66 = 1.269858330488205e-06

Training epoch-99 batch-67
Running loss of epoch-99 batch-67 = 1.6097910702228546e-06

Training epoch-99 batch-68
Running loss of epoch-99 batch-68 = 1.0435469448566437e-06

Training epoch-99 batch-69
Running loss of epoch-99 batch-69 = 9.34116542339325e-07

Training epoch-99 batch-70
Running loss of epoch-99 batch-70 = 8.705537766218185e-07

Training epoch-99 batch-71
Running loss of epoch-99 batch-71 = 9.906943887472153e-07

Training epoch-99 batch-72
Running loss of epoch-99 batch-72 = 9.592622518539429e-07

Training epoch-99 batch-73
Running loss of epoch-99 batch-73 = 1.1932570487260818e-06

Training epoch-99 batch-74
Running loss of epoch-99 batch-74 = 9.299255907535553e-07

Training epoch-99 batch-75
Running loss of epoch-99 batch-75 = 8.114147931337357e-07

Training epoch-99 batch-76
Running loss of epoch-99 batch-76 = 1.2950040400028229e-06

Training epoch-99 batch-77
Running loss of epoch-99 batch-77 = 9.424984455108643e-07

Training epoch-99 batch-78
Running loss of epoch-99 batch-78 = 1.417938619852066e-06

Training epoch-99 batch-79
Running loss of epoch-99 batch-79 = 1.0957010090351105e-06

Training epoch-99 batch-80
Running loss of epoch-99 batch-80 = 1.239357516169548e-06

Training epoch-99 batch-81
Running loss of epoch-99 batch-81 = 1.8009450286626816e-06

Training epoch-99 batch-82
Running loss of epoch-99 batch-82 = 1.3113021850585938e-06

Training epoch-99 batch-83
Running loss of epoch-99 batch-83 = 1.0165385901927948e-06

Training epoch-99 batch-84
Running loss of epoch-99 batch-84 = 1.1113006621599197e-06

Training epoch-99 batch-85
Running loss of epoch-99 batch-85 = 7.986091077327728e-07

Training epoch-99 batch-86
Running loss of epoch-99 batch-86 = 1.9364524632692337e-06

Training epoch-99 batch-87
Running loss of epoch-99 batch-87 = 1.3504177331924438e-06

Training epoch-99 batch-88
Running loss of epoch-99 batch-88 = 8.30506905913353e-07

Training epoch-99 batch-89
Running loss of epoch-99 batch-89 = 1.0721851140260696e-06

Training epoch-99 batch-90
Running loss of epoch-99 batch-90 = 8.081551641225815e-07

Training epoch-99 batch-91
Running loss of epoch-99 batch-91 = 1.7476268112659454e-06

Training epoch-99 batch-92
Running loss of epoch-99 batch-92 = 7.252674549818039e-07

Training epoch-99 batch-93
Running loss of epoch-99 batch-93 = 1.2456439435482025e-06

Training epoch-99 batch-94
Running loss of epoch-99 batch-94 = 1.0989606380462646e-06

Training epoch-99 batch-95
Running loss of epoch-99 batch-95 = 7.967464625835419e-07

Training epoch-99 batch-96
Running loss of epoch-99 batch-96 = 1.4656689018011093e-06

Training epoch-99 batch-97
Running loss of epoch-99 batch-97 = 1.4866236597299576e-06

Training epoch-99 batch-98
Running loss of epoch-99 batch-98 = 1.4989636838436127e-06

Training epoch-99 batch-99
Running loss of epoch-99 batch-99 = 2.1443702280521393e-06

Training epoch-99 batch-100
Running loss of epoch-99 batch-100 = 1.255422830581665e-06

Training epoch-99 batch-101
Running loss of epoch-99 batch-101 = 2.0870938897132874e-06

Training epoch-99 batch-102
Running loss of epoch-99 batch-102 = 6.996560841798782e-07

Training epoch-99 batch-103
Running loss of epoch-99 batch-103 = 1.3075768947601318e-06

Training epoch-99 batch-104
Running loss of epoch-99 batch-104 = 1.1576339602470398e-06

Training epoch-99 batch-105
Running loss of epoch-99 batch-105 = 1.2398231774568558e-06

Training epoch-99 batch-106
Running loss of epoch-99 batch-106 = 1.2209638953208923e-06

Training epoch-99 batch-107
Running loss of epoch-99 batch-107 = 1.0137446224689484e-06

Training epoch-99 batch-108
Running loss of epoch-99 batch-108 = 1.1562369763851166e-06

Training epoch-99 batch-109
Running loss of epoch-99 batch-109 = 8.523929864168167e-07

Training epoch-99 batch-110
Running loss of epoch-99 batch-110 = 7.217749953269958e-07

Training epoch-99 batch-111
Running loss of epoch-99 batch-111 = 8.300412446260452e-07

Training epoch-99 batch-112
Running loss of epoch-99 batch-112 = 9.399373084306717e-07

Training epoch-99 batch-113
Running loss of epoch-99 batch-113 = 7.818453013896942e-07

Training epoch-99 batch-114
Running loss of epoch-99 batch-114 = 1.346692442893982e-06

Training epoch-99 batch-115
Running loss of epoch-99 batch-115 = 1.0742805898189545e-06

Training epoch-99 batch-116
Running loss of epoch-99 batch-116 = 9.185168892145157e-07

Training epoch-99 batch-117
Running loss of epoch-99 batch-117 = 1.2025702744722366e-06

Training epoch-99 batch-118
Running loss of epoch-99 batch-118 = 1.053791493177414e-06

Training epoch-99 batch-119
Running loss of epoch-99 batch-119 = 8.349306881427765e-07

Training epoch-99 batch-120
Running loss of epoch-99 batch-120 = 1.1052470654249191e-06

Training epoch-99 batch-121
Running loss of epoch-99 batch-121 = 1.333886757493019e-06

Training epoch-99 batch-122
Running loss of epoch-99 batch-122 = 1.2647360563278198e-06

Training epoch-99 batch-123
Running loss of epoch-99 batch-123 = 1.2707896530628204e-06

Training epoch-99 batch-124
Running loss of epoch-99 batch-124 = 1.5124678611755371e-06

Training epoch-99 batch-125
Running loss of epoch-99 batch-125 = 1.727137714624405e-06

Training epoch-99 batch-126
Running loss of epoch-99 batch-126 = 1.108972355723381e-06

Training epoch-99 batch-127
Running loss of epoch-99 batch-127 = 1.196516677737236e-06

Training epoch-99 batch-128
Running loss of epoch-99 batch-128 = 1.1690426617860794e-06

Training epoch-99 batch-129
Running loss of epoch-99 batch-129 = 1.009088009595871e-06

Training epoch-99 batch-130
Running loss of epoch-99 batch-130 = 1.2528616935014725e-06

Training epoch-99 batch-131
Running loss of epoch-99 batch-131 = 1.009088009595871e-06

Training epoch-99 batch-132
Running loss of epoch-99 batch-132 = 1.0761432349681854e-06

Training epoch-99 batch-133
Running loss of epoch-99 batch-133 = 8.454080671072006e-07

Training epoch-99 batch-134
Running loss of epoch-99 batch-134 = 8.041970431804657e-07

Training epoch-99 batch-135
Running loss of epoch-99 batch-135 = 1.4116521924734116e-06

Training epoch-99 batch-136
Running loss of epoch-99 batch-136 = 9.012874215841293e-07

Training epoch-99 batch-137
Running loss of epoch-99 batch-137 = 1.0200310498476028e-06

Training epoch-99 batch-138
Running loss of epoch-99 batch-138 = 1.5150289982557297e-06

Training epoch-99 batch-139
Running loss of epoch-99 batch-139 = 6.952323019504547e-07

Training epoch-99 batch-140
Running loss of epoch-99 batch-140 = 1.3248063623905182e-06

Training epoch-99 batch-141
Running loss of epoch-99 batch-141 = 8.82195308804512e-07

Training epoch-99 batch-142
Running loss of epoch-99 batch-142 = 8.628703653812408e-07

Training epoch-99 batch-143
Running loss of epoch-99 batch-143 = 1.2421514838933945e-06

Training epoch-99 batch-144
Running loss of epoch-99 batch-144 = 1.1578667908906937e-06

Training epoch-99 batch-145
Running loss of epoch-99 batch-145 = 1.057283952832222e-06

Training epoch-99 batch-146
Running loss of epoch-99 batch-146 = 1.2062955647706985e-06

Training epoch-99 batch-147
Running loss of epoch-99 batch-147 = 1.462642103433609e-06

Training epoch-99 batch-148
Running loss of epoch-99 batch-148 = 1.0994262993335724e-06

Training epoch-99 batch-149
Running loss of epoch-99 batch-149 = 5.112960934638977e-07

Training epoch-99 batch-150
Running loss of epoch-99 batch-150 = 1.5152618288993835e-06

Training epoch-99 batch-151
Running loss of epoch-99 batch-151 = 1.4649704098701477e-06

Training epoch-99 batch-152
Running loss of epoch-99 batch-152 = 1.4940742403268814e-06

Training epoch-99 batch-153
Running loss of epoch-99 batch-153 = 1.0177027434110641e-06

Training epoch-99 batch-154
Running loss of epoch-99 batch-154 = 1.2207310646772385e-06

Training epoch-99 batch-155
Running loss of epoch-99 batch-155 = 1.3532117009162903e-06

Training epoch-99 batch-156
Running loss of epoch-99 batch-156 = 9.650830179452896e-07

Training epoch-99 batch-157
Running loss of epoch-99 batch-157 = 4.708766937255859e-06

Finished training epoch-99.



Average train loss at epoch-99 = 1.1416375637054444e-06

Started Evaluation

Average val loss at epoch-99 = 3.332052428471415

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 36.15 %

Overall Accuracy = 54.49 %

Finished Evaluation



Started training epoch-100


Training epoch-100 batch-1
Running loss of epoch-100 batch-1 = 1.4675315469503403e-06

Training epoch-100 batch-2
Running loss of epoch-100 batch-2 = 1.1243391782045364e-06

Training epoch-100 batch-3
Running loss of epoch-100 batch-3 = 8.579809218645096e-07

Training epoch-100 batch-4
Running loss of epoch-100 batch-4 = 5.48316165804863e-07

Training epoch-100 batch-5
Running loss of epoch-100 batch-5 = 1.0551884770393372e-06

Training epoch-100 batch-6
Running loss of epoch-100 batch-6 = 1.3266690075397491e-06

Training epoch-100 batch-7
Running loss of epoch-100 batch-7 = 7.450580596923828e-07

Training epoch-100 batch-8
Running loss of epoch-100 batch-8 = 1.2302771210670471e-06

Training epoch-100 batch-9
Running loss of epoch-100 batch-9 = 8.202623575925827e-07

Training epoch-100 batch-10
Running loss of epoch-100 batch-10 = 1.0328367352485657e-06

Training epoch-100 batch-11
Running loss of epoch-100 batch-11 = 9.746290743350983e-07

Training epoch-100 batch-12
Running loss of epoch-100 batch-12 = 1.423293724656105e-06

Training epoch-100 batch-13
Running loss of epoch-100 batch-13 = 1.4659017324447632e-06

Training epoch-100 batch-14
Running loss of epoch-100 batch-14 = 9.636860340833664e-07

Training epoch-100 batch-15
Running loss of epoch-100 batch-15 = 8.242204785346985e-07

Training epoch-100 batch-16
Running loss of epoch-100 batch-16 = 1.2922100722789764e-06

Training epoch-100 batch-17
Running loss of epoch-100 batch-17 = 1.0819640010595322e-06

Training epoch-100 batch-18
Running loss of epoch-100 batch-18 = 6.756745278835297e-07

Training epoch-100 batch-19
Running loss of epoch-100 batch-19 = 8.835922926664352e-07

Training epoch-100 batch-20
Running loss of epoch-100 batch-20 = 1.0111834853887558e-06

Training epoch-100 batch-21
Running loss of epoch-100 batch-21 = 1.0863877832889557e-06

Training epoch-100 batch-22
Running loss of epoch-100 batch-22 = 1.1154916137456894e-06

Training epoch-100 batch-23
Running loss of epoch-100 batch-23 = 1.200009137392044e-06

Training epoch-100 batch-24
Running loss of epoch-100 batch-24 = 1.260777935385704e-06

Training epoch-100 batch-25
Running loss of epoch-100 batch-25 = 1.3052485883235931e-06

Training epoch-100 batch-26
Running loss of epoch-100 batch-26 = 1.4649704098701477e-06

Training epoch-100 batch-27
Running loss of epoch-100 batch-27 = 1.1313240975141525e-06

Training epoch-100 batch-28
Running loss of epoch-100 batch-28 = 1.1366792023181915e-06

Training epoch-100 batch-29
Running loss of epoch-100 batch-29 = 1.037493348121643e-06

Training epoch-100 batch-30
Running loss of epoch-100 batch-30 = 1.085922122001648e-06

Training epoch-100 batch-31
Running loss of epoch-100 batch-31 = 1.5855766832828522e-06

Training epoch-100 batch-32
Running loss of epoch-100 batch-32 = 7.888302206993103e-07

Training epoch-100 batch-33
Running loss of epoch-100 batch-33 = 1.4577526599168777e-06

Training epoch-100 batch-34
Running loss of epoch-100 batch-34 = 1.2584496289491653e-06

Training epoch-100 batch-35
Running loss of epoch-100 batch-35 = 1.4924444258213043e-06

Training epoch-100 batch-36
Running loss of epoch-100 batch-36 = 1.3050157576799393e-06

Training epoch-100 batch-37
Running loss of epoch-100 batch-37 = 5.930196493864059e-07

Training epoch-100 batch-38
Running loss of epoch-100 batch-38 = 9.66014340519905e-07

Training epoch-100 batch-39
Running loss of epoch-100 batch-39 = 1.0093208402395248e-06

Training epoch-100 batch-40
Running loss of epoch-100 batch-40 = 9.210780262947083e-07

Training epoch-100 batch-41
Running loss of epoch-100 batch-41 = 1.0509975254535675e-06

Training epoch-100 batch-42
Running loss of epoch-100 batch-42 = 1.071719452738762e-06

Training epoch-100 batch-43
Running loss of epoch-100 batch-43 = 1.1329539120197296e-06

Training epoch-100 batch-44
Running loss of epoch-100 batch-44 = 1.398380845785141e-06

Training epoch-100 batch-45
Running loss of epoch-100 batch-45 = 8.773058652877808e-07

Training epoch-100 batch-46
Running loss of epoch-100 batch-46 = 1.0256189852952957e-06

Training epoch-100 batch-47
Running loss of epoch-100 batch-47 = 1.687789335846901e-06

Training epoch-100 batch-48
Running loss of epoch-100 batch-48 = 1.5213154256343842e-06

Training epoch-100 batch-49
Running loss of epoch-100 batch-49 = 7.527414709329605e-07

Training epoch-100 batch-50
Running loss of epoch-100 batch-50 = 1.646578311920166e-06

Training epoch-100 batch-51
Running loss of epoch-100 batch-51 = 1.1310912668704987e-06

Training epoch-100 batch-52
Running loss of epoch-100 batch-52 = 1.5487894415855408e-06

Training epoch-100 batch-53
Running loss of epoch-100 batch-53 = 1.0272487998008728e-06

Training epoch-100 batch-54
Running loss of epoch-100 batch-54 = 1.2051314115524292e-06

Training epoch-100 batch-55
Running loss of epoch-100 batch-55 = 1.2523960322141647e-06

Training epoch-100 batch-56
Running loss of epoch-100 batch-56 = 1.0395888239145279e-06

Training epoch-100 batch-57
Running loss of epoch-100 batch-57 = 7.995404303073883e-07

Training epoch-100 batch-58
Running loss of epoch-100 batch-58 = 1.1774245649576187e-06

Training epoch-100 batch-59
Running loss of epoch-100 batch-59 = 1.159030944108963e-06

Training epoch-100 batch-60
Running loss of epoch-100 batch-60 = 9.743962436914444e-07

Training epoch-100 batch-61
Running loss of epoch-100 batch-61 = 7.89295881986618e-07

Training epoch-100 batch-62
Running loss of epoch-100 batch-62 = 9.522773325443268e-07

Training epoch-100 batch-63
Running loss of epoch-100 batch-63 = 9.39471647143364e-07

Training epoch-100 batch-64
Running loss of epoch-100 batch-64 = 1.0209623724222183e-06

Training epoch-100 batch-65
Running loss of epoch-100 batch-65 = 1.0493677109479904e-06

Training epoch-100 batch-66
Running loss of epoch-100 batch-66 = 1.544831320643425e-06

Training epoch-100 batch-67
Running loss of epoch-100 batch-67 = 9.974464774131775e-07

Training epoch-100 batch-68
Running loss of epoch-100 batch-68 = 1.0009389370679855e-06

Training epoch-100 batch-69
Running loss of epoch-100 batch-69 = 1.1355150490999222e-06

Training epoch-100 batch-70
Running loss of epoch-100 batch-70 = 1.0908115655183792e-06

Training epoch-100 batch-71
Running loss of epoch-100 batch-71 = 9.320210665464401e-07

Training epoch-100 batch-72
Running loss of epoch-100 batch-72 = 1.7671845853328705e-06

Training epoch-100 batch-73
Running loss of epoch-100 batch-73 = 9.03615728020668e-07

Training epoch-100 batch-74
Running loss of epoch-100 batch-74 = 9.508803486824036e-07

Training epoch-100 batch-75
Running loss of epoch-100 batch-75 = 1.0521616786718369e-06

Training epoch-100 batch-76
Running loss of epoch-100 batch-76 = 1.0314397513866425e-06

Training epoch-100 batch-77
Running loss of epoch-100 batch-77 = 1.032371073961258e-06

Training epoch-100 batch-78
Running loss of epoch-100 batch-78 = 1.4388933777809143e-06

Training epoch-100 batch-79
Running loss of epoch-100 batch-79 = 1.048436388373375e-06

Training epoch-100 batch-80
Running loss of epoch-100 batch-80 = 1.319684088230133e-06

Training epoch-100 batch-81
Running loss of epoch-100 batch-81 = 7.641501724720001e-07

Training epoch-100 batch-82
Running loss of epoch-100 batch-82 = 1.3737007975578308e-06

Training epoch-100 batch-83
Running loss of epoch-100 batch-83 = 1.2414529919624329e-06

Training epoch-100 batch-84
Running loss of epoch-100 batch-84 = 8.584465831518173e-07

Training epoch-100 batch-85
Running loss of epoch-100 batch-85 = 1.237727701663971e-06

Training epoch-100 batch-86
Running loss of epoch-100 batch-86 = 1.3299286365509033e-06

Training epoch-100 batch-87
Running loss of epoch-100 batch-87 = 8.55419784784317e-07

Training epoch-100 batch-88
Running loss of epoch-100 batch-88 = 1.0528601706027985e-06

Training epoch-100 batch-89
Running loss of epoch-100 batch-89 = 1.2384261935949326e-06

Training epoch-100 batch-90
Running loss of epoch-100 batch-90 = 9.795185178518295e-07

Training epoch-100 batch-91
Running loss of epoch-100 batch-91 = 9.958166629076004e-07

Training epoch-100 batch-92
Running loss of epoch-100 batch-92 = 1.0961666703224182e-06

Training epoch-100 batch-93
Running loss of epoch-100 batch-93 = 1.264270395040512e-06

Training epoch-100 batch-94
Running loss of epoch-100 batch-94 = 1.137610524892807e-06

Training epoch-100 batch-95
Running loss of epoch-100 batch-95 = 9.934883564710617e-07

Training epoch-100 batch-96
Running loss of epoch-100 batch-96 = 8.887145668268204e-07

Training epoch-100 batch-97
Running loss of epoch-100 batch-97 = 7.944181561470032e-07

Training epoch-100 batch-98
Running loss of epoch-100 batch-98 = 1.1336524039506912e-06

Training epoch-100 batch-99
Running loss of epoch-100 batch-99 = 1.0700896382331848e-06

Training epoch-100 batch-100
Running loss of epoch-100 batch-100 = 1.2624077498912811e-06

Training epoch-100 batch-101
Running loss of epoch-100 batch-101 = 1.1166557669639587e-06

Training epoch-100 batch-102
Running loss of epoch-100 batch-102 = 9.986106306314468e-07

Training epoch-100 batch-103
Running loss of epoch-100 batch-103 = 9.87667590379715e-07

Training epoch-100 batch-104
Running loss of epoch-100 batch-104 = 1.4423858374357224e-06

Training epoch-100 batch-105
Running loss of epoch-100 batch-105 = 8.582137525081635e-07

Training epoch-100 batch-106
Running loss of epoch-100 batch-106 = 1.5851110219955444e-06

Training epoch-100 batch-107
Running loss of epoch-100 batch-107 = 9.669456630945206e-07

Training epoch-100 batch-108
Running loss of epoch-100 batch-108 = 1.2996606528759003e-06

Training epoch-100 batch-109
Running loss of epoch-100 batch-109 = 1.7113052308559418e-06

Training epoch-100 batch-110
Running loss of epoch-100 batch-110 = 1.1315569281578064e-06

Training epoch-100 batch-111
Running loss of epoch-100 batch-111 = 9.932555258274078e-07

Training epoch-100 batch-112
Running loss of epoch-100 batch-112 = 1.0582152754068375e-06

Training epoch-100 batch-113
Running loss of epoch-100 batch-113 = 8.225906640291214e-07

Training epoch-100 batch-114
Running loss of epoch-100 batch-114 = 1.0230578482151031e-06

Training epoch-100 batch-115
Running loss of epoch-100 batch-115 = 8.761417120695114e-07

Training epoch-100 batch-116
Running loss of epoch-100 batch-116 = 8.374918252229691e-07

Training epoch-100 batch-117
Running loss of epoch-100 batch-117 = 1.3925600796937943e-06

Training epoch-100 batch-118
Running loss of epoch-100 batch-118 = 1.5138648450374603e-06

Training epoch-100 batch-119
Running loss of epoch-100 batch-119 = 8.968636393547058e-07

Training epoch-100 batch-120
Running loss of epoch-100 batch-120 = 6.735790520906448e-07

Training epoch-100 batch-121
Running loss of epoch-100 batch-121 = 1.318054273724556e-06

Training epoch-100 batch-122
Running loss of epoch-100 batch-122 = 1.4130491763353348e-06

Training epoch-100 batch-123
Running loss of epoch-100 batch-123 = 1.2617092579603195e-06

Training epoch-100 batch-124
Running loss of epoch-100 batch-124 = 8.17934051156044e-07

Training epoch-100 batch-125
Running loss of epoch-100 batch-125 = 1.3292301446199417e-06

Training epoch-100 batch-126
Running loss of epoch-100 batch-126 = 1.5497207641601562e-06

Training epoch-100 batch-127
Running loss of epoch-100 batch-127 = 1.6051344573497772e-06

Training epoch-100 batch-128
Running loss of epoch-100 batch-128 = 1.5588011592626572e-06

Training epoch-100 batch-129
Running loss of epoch-100 batch-129 = 1.45728699862957e-06

Training epoch-100 batch-130
Running loss of epoch-100 batch-130 = 1.1043157428503036e-06

Training epoch-100 batch-131
Running loss of epoch-100 batch-131 = 1.0568182915449142e-06

Training epoch-100 batch-132
Running loss of epoch-100 batch-132 = 1.0202638804912567e-06

Training epoch-100 batch-133
Running loss of epoch-100 batch-133 = 1.2312084436416626e-06

Training epoch-100 batch-134
Running loss of epoch-100 batch-134 = 1.0775402188301086e-06

Training epoch-100 batch-135
Running loss of epoch-100 batch-135 = 8.514616638422012e-07

Training epoch-100 batch-136
Running loss of epoch-100 batch-136 = 1.093139871954918e-06

Training epoch-100 batch-137
Running loss of epoch-100 batch-137 = 7.939524948596954e-07

Training epoch-100 batch-138
Running loss of epoch-100 batch-138 = 9.005889296531677e-07

Training epoch-100 batch-139
Running loss of epoch-100 batch-139 = 9.832438081502914e-07

Training epoch-100 batch-140
Running loss of epoch-100 batch-140 = 5.955807864665985e-07

Training epoch-100 batch-141
Running loss of epoch-100 batch-141 = 9.98377799987793e-07

Training epoch-100 batch-142
Running loss of epoch-100 batch-142 = 1.1404044926166534e-06

Training epoch-100 batch-143
Running loss of epoch-100 batch-143 = 1.073349267244339e-06

Training epoch-100 batch-144
Running loss of epoch-100 batch-144 = 1.1899974197149277e-06

Training epoch-100 batch-145
Running loss of epoch-100 batch-145 = 1.427019014954567e-06

Training epoch-100 batch-146
Running loss of epoch-100 batch-146 = 1.1560041457414627e-06

Training epoch-100 batch-147
Running loss of epoch-100 batch-147 = 1.1241063475608826e-06

Training epoch-100 batch-148
Running loss of epoch-100 batch-148 = 1.4617107808589935e-06

Training epoch-100 batch-149
Running loss of epoch-100 batch-149 = 1.1802185326814651e-06

Training epoch-100 batch-150
Running loss of epoch-100 batch-150 = 1.4652032405138016e-06

Training epoch-100 batch-151
Running loss of epoch-100 batch-151 = 8.542556315660477e-07

Training epoch-100 batch-152
Running loss of epoch-100 batch-152 = 1.2086238712072372e-06

Training epoch-100 batch-153
Running loss of epoch-100 batch-153 = 9.427312761545181e-07

Training epoch-100 batch-154
Running loss of epoch-100 batch-154 = 1.307111233472824e-06

Training epoch-100 batch-155
Running loss of epoch-100 batch-155 = 1.1378433555364609e-06

Training epoch-100 batch-156
Running loss of epoch-100 batch-156 = 9.392388164997101e-07

Training epoch-100 batch-157
Running loss of epoch-100 batch-157 = 5.245208740234375e-06

Finished training epoch-100.



Average train loss at epoch-100 = 1.1275187134742737e-06

Started Evaluation

Average val loss at epoch-100 = 3.3343494440379895

Accuracy for classes:
Accuracy for class equals is: 75.41 %
Accuracy for class main is: 64.26 %
Accuracy for class setUp is: 63.44 %
Accuracy for class onCreate is: 56.61 %
Accuracy for class toString is: 51.19 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 30.04 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 54.47 %

Finished Evaluation

