
DataLoader:

Started trainloader.
trainset = #10000
Finished trainloader.
Started valloader.
valset = #4847
Finished valloader.

Configuration:

DB_NAME = java-top10, IMG_TYPE = reformat_literal, TRANS_SIZE = 1024
MAX_EPOCH = 100, BATCH_SIZE = 64

device = cuda:0



Started training epoch-1


Training epoch-1 batch-1
Running loss of epoch-1 batch-1 = 0.03611278906464577

Training epoch-1 batch-2
Running loss of epoch-1 batch-2 = 0.03619184345006943

Training epoch-1 batch-3
Running loss of epoch-1 batch-3 = 0.03601083531975746

Training epoch-1 batch-4
Running loss of epoch-1 batch-4 = 0.03593555465340614

Training epoch-1 batch-5
Running loss of epoch-1 batch-5 = 0.036091070622205734

Training epoch-1 batch-6
Running loss of epoch-1 batch-6 = 0.03591625392436981

Training epoch-1 batch-7
Running loss of epoch-1 batch-7 = 0.03611818328499794

Training epoch-1 batch-8
Running loss of epoch-1 batch-8 = 0.03581343963742256

Training epoch-1 batch-9
Running loss of epoch-1 batch-9 = 0.036054015159606934

Training epoch-1 batch-10
Running loss of epoch-1 batch-10 = 0.03578167036175728

Training epoch-1 batch-11
Running loss of epoch-1 batch-11 = 0.03599880635738373

Training epoch-1 batch-12
Running loss of epoch-1 batch-12 = 0.0358918234705925

Training epoch-1 batch-13
Running loss of epoch-1 batch-13 = 0.03587285429239273

Training epoch-1 batch-14
Running loss of epoch-1 batch-14 = 0.03597533330321312

Training epoch-1 batch-15
Running loss of epoch-1 batch-15 = 0.035890839993953705

Training epoch-1 batch-16
Running loss of epoch-1 batch-16 = 0.0358104333281517

Training epoch-1 batch-17
Running loss of epoch-1 batch-17 = 0.03557286784052849

Training epoch-1 batch-18
Running loss of epoch-1 batch-18 = 0.03569825366139412

Training epoch-1 batch-19
Running loss of epoch-1 batch-19 = 0.035741209983825684

Training epoch-1 batch-20
Running loss of epoch-1 batch-20 = 0.03597557172179222

Training epoch-1 batch-21
Running loss of epoch-1 batch-21 = 0.03576948121190071

Training epoch-1 batch-22
Running loss of epoch-1 batch-22 = 0.036003176122903824

Training epoch-1 batch-23
Running loss of epoch-1 batch-23 = 0.03577512130141258

Training epoch-1 batch-24
Running loss of epoch-1 batch-24 = 0.0356883741915226

Training epoch-1 batch-25
Running loss of epoch-1 batch-25 = 0.03582406044006348

Training epoch-1 batch-26
Running loss of epoch-1 batch-26 = 0.035844575613737106

Training epoch-1 batch-27
Running loss of epoch-1 batch-27 = 0.03570086136460304

Training epoch-1 batch-28
Running loss of epoch-1 batch-28 = 0.03559689223766327

Training epoch-1 batch-29
Running loss of epoch-1 batch-29 = 0.03566734492778778

Training epoch-1 batch-30
Running loss of epoch-1 batch-30 = 0.03544103726744652

Training epoch-1 batch-31
Running loss of epoch-1 batch-31 = 0.03549418970942497

Training epoch-1 batch-32
Running loss of epoch-1 batch-32 = 0.03545326367020607

Training epoch-1 batch-33
Running loss of epoch-1 batch-33 = 0.03559393435716629

Training epoch-1 batch-34
Running loss of epoch-1 batch-34 = 0.03559744358062744

Training epoch-1 batch-35
Running loss of epoch-1 batch-35 = 0.03566722944378853

Training epoch-1 batch-36
Running loss of epoch-1 batch-36 = 0.03545593470335007

Training epoch-1 batch-37
Running loss of epoch-1 batch-37 = 0.03563196584582329

Training epoch-1 batch-38
Running loss of epoch-1 batch-38 = 0.035620953887701035

Training epoch-1 batch-39
Running loss of epoch-1 batch-39 = 0.03558916226029396

Training epoch-1 batch-40
Running loss of epoch-1 batch-40 = 0.03548995032906532

Training epoch-1 batch-41
Running loss of epoch-1 batch-41 = 0.03559793531894684

Training epoch-1 batch-42
Running loss of epoch-1 batch-42 = 0.03538494184613228

Training epoch-1 batch-43
Running loss of epoch-1 batch-43 = 0.03527453914284706

Training epoch-1 batch-44
Running loss of epoch-1 batch-44 = 0.03533618897199631

Training epoch-1 batch-45
Running loss of epoch-1 batch-45 = 0.03544323518872261

Training epoch-1 batch-46
Running loss of epoch-1 batch-46 = 0.03502815216779709

Training epoch-1 batch-47
Running loss of epoch-1 batch-47 = 0.035258181393146515

Training epoch-1 batch-48
Running loss of epoch-1 batch-48 = 0.035034723579883575

Training epoch-1 batch-49
Running loss of epoch-1 batch-49 = 0.035042885690927505

Training epoch-1 batch-50
Running loss of epoch-1 batch-50 = 0.03564652055501938

Training epoch-1 batch-51
Running loss of epoch-1 batch-51 = 0.03513163700699806

Training epoch-1 batch-52
Running loss of epoch-1 batch-52 = 0.03500320762395859

Training epoch-1 batch-53
Running loss of epoch-1 batch-53 = 0.03500637412071228

Training epoch-1 batch-54
Running loss of epoch-1 batch-54 = 0.035128481686115265

Training epoch-1 batch-55
Running loss of epoch-1 batch-55 = 0.034914467483758926

Training epoch-1 batch-56
Running loss of epoch-1 batch-56 = 0.0351276770234108

Training epoch-1 batch-57
Running loss of epoch-1 batch-57 = 0.03514067456126213

Training epoch-1 batch-58
Running loss of epoch-1 batch-58 = 0.034964270889759064

Training epoch-1 batch-59
Running loss of epoch-1 batch-59 = 0.035149406641721725

Training epoch-1 batch-60
Running loss of epoch-1 batch-60 = 0.03489251434803009

Training epoch-1 batch-61
Running loss of epoch-1 batch-61 = 0.034747906029224396

Training epoch-1 batch-62
Running loss of epoch-1 batch-62 = 0.03477231785655022

Training epoch-1 batch-63
Running loss of epoch-1 batch-63 = 0.035157084465026855

Training epoch-1 batch-64
Running loss of epoch-1 batch-64 = 0.03500625863671303

Training epoch-1 batch-65
Running loss of epoch-1 batch-65 = 0.034762293100357056

Training epoch-1 batch-66
Running loss of epoch-1 batch-66 = 0.034910332411527634

Training epoch-1 batch-67
Running loss of epoch-1 batch-67 = 0.03509516268968582

Training epoch-1 batch-68
Running loss of epoch-1 batch-68 = 0.03449787572026253

Training epoch-1 batch-69
Running loss of epoch-1 batch-69 = 0.03460047394037247

Training epoch-1 batch-70
Running loss of epoch-1 batch-70 = 0.03452065587043762

Training epoch-1 batch-71
Running loss of epoch-1 batch-71 = 0.03457987308502197

Training epoch-1 batch-72
Running loss of epoch-1 batch-72 = 0.034326814115047455

Training epoch-1 batch-73
Running loss of epoch-1 batch-73 = 0.034516554325819016

Training epoch-1 batch-74
Running loss of epoch-1 batch-74 = 0.03429878130555153

Training epoch-1 batch-75
Running loss of epoch-1 batch-75 = 0.035181690007448196

Training epoch-1 batch-76
Running loss of epoch-1 batch-76 = 0.034662049263715744

Training epoch-1 batch-77
Running loss of epoch-1 batch-77 = 0.03436165675520897

Training epoch-1 batch-78
Running loss of epoch-1 batch-78 = 0.03423461318016052

Training epoch-1 batch-79
Running loss of epoch-1 batch-79 = 0.03410301357507706

Training epoch-1 batch-80
Running loss of epoch-1 batch-80 = 0.03403092548251152

Training epoch-1 batch-81
Running loss of epoch-1 batch-81 = 0.03432738035917282

Training epoch-1 batch-82
Running loss of epoch-1 batch-82 = 0.03425440192222595

Training epoch-1 batch-83
Running loss of epoch-1 batch-83 = 0.03423026576638222

Training epoch-1 batch-84
Running loss of epoch-1 batch-84 = 0.033966101706027985

Training epoch-1 batch-85
Running loss of epoch-1 batch-85 = 0.03440861403942108

Training epoch-1 batch-86
Running loss of epoch-1 batch-86 = 0.03396117314696312

Training epoch-1 batch-87
Running loss of epoch-1 batch-87 = 0.03345756232738495

Training epoch-1 batch-88
Running loss of epoch-1 batch-88 = 0.03366291522979736

Training epoch-1 batch-89
Running loss of epoch-1 batch-89 = 0.033689599484205246

Training epoch-1 batch-90
Running loss of epoch-1 batch-90 = 0.03406280651688576

Training epoch-1 batch-91
Running loss of epoch-1 batch-91 = 0.034276485443115234

Training epoch-1 batch-92
Running loss of epoch-1 batch-92 = 0.033323612064123154

Training epoch-1 batch-93
Running loss of epoch-1 batch-93 = 0.03404858708381653

Training epoch-1 batch-94
Running loss of epoch-1 batch-94 = 0.03372776135802269

Training epoch-1 batch-95
Running loss of epoch-1 batch-95 = 0.03437446430325508

Training epoch-1 batch-96
Running loss of epoch-1 batch-96 = 0.033869195729494095

Training epoch-1 batch-97
Running loss of epoch-1 batch-97 = 0.033595576882362366

Training epoch-1 batch-98
Running loss of epoch-1 batch-98 = 0.032959192991256714

Training epoch-1 batch-99
Running loss of epoch-1 batch-99 = 0.03323861211538315

Training epoch-1 batch-100
Running loss of epoch-1 batch-100 = 0.03350643068552017

Training epoch-1 batch-101
Running loss of epoch-1 batch-101 = 0.033332131803035736

Training epoch-1 batch-102
Running loss of epoch-1 batch-102 = 0.033854275941848755

Training epoch-1 batch-103
Running loss of epoch-1 batch-103 = 0.033267125487327576

Training epoch-1 batch-104
Running loss of epoch-1 batch-104 = 0.0325239859521389

Training epoch-1 batch-105
Running loss of epoch-1 batch-105 = 0.033178605139255524

Training epoch-1 batch-106
Running loss of epoch-1 batch-106 = 0.03281743451952934

Training epoch-1 batch-107
Running loss of epoch-1 batch-107 = 0.03283514454960823

Training epoch-1 batch-108
Running loss of epoch-1 batch-108 = 0.0331161804497242

Training epoch-1 batch-109
Running loss of epoch-1 batch-109 = 0.032965876162052155

Training epoch-1 batch-110
Running loss of epoch-1 batch-110 = 0.03319476917386055

Training epoch-1 batch-111
Running loss of epoch-1 batch-111 = 0.032792434096336365

Training epoch-1 batch-112
Running loss of epoch-1 batch-112 = 0.0322488397359848

Training epoch-1 batch-113
Running loss of epoch-1 batch-113 = 0.0326017364859581

Training epoch-1 batch-114
Running loss of epoch-1 batch-114 = 0.03179576247930527

Training epoch-1 batch-115
Running loss of epoch-1 batch-115 = 0.03190646693110466

Training epoch-1 batch-116
Running loss of epoch-1 batch-116 = 0.03269702196121216

Training epoch-1 batch-117
Running loss of epoch-1 batch-117 = 0.03195841237902641

Training epoch-1 batch-118
Running loss of epoch-1 batch-118 = 0.03173239156603813

Training epoch-1 batch-119
Running loss of epoch-1 batch-119 = 0.031045755371451378

Training epoch-1 batch-120
Running loss of epoch-1 batch-120 = 0.031456660479307175

Training epoch-1 batch-121
Running loss of epoch-1 batch-121 = 0.03142780065536499

Training epoch-1 batch-122
Running loss of epoch-1 batch-122 = 0.03163275867700577

Training epoch-1 batch-123
Running loss of epoch-1 batch-123 = 0.03066488541662693

Training epoch-1 batch-124
Running loss of epoch-1 batch-124 = 0.03069273568689823

Training epoch-1 batch-125
Running loss of epoch-1 batch-125 = 0.0316779762506485

Training epoch-1 batch-126
Running loss of epoch-1 batch-126 = 0.03061535581946373

Training epoch-1 batch-127
Running loss of epoch-1 batch-127 = 0.031489912420511246

Training epoch-1 batch-128
Running loss of epoch-1 batch-128 = 0.029780516400933266

Training epoch-1 batch-129
Running loss of epoch-1 batch-129 = 0.030614854767918587

Training epoch-1 batch-130
Running loss of epoch-1 batch-130 = 0.031250644475221634

Training epoch-1 batch-131
Running loss of epoch-1 batch-131 = 0.030648576095700264

Training epoch-1 batch-132
Running loss of epoch-1 batch-132 = 0.03030330128967762

Training epoch-1 batch-133
Running loss of epoch-1 batch-133 = 0.028727561235427856

Training epoch-1 batch-134
Running loss of epoch-1 batch-134 = 0.029454706236720085

Training epoch-1 batch-135
Running loss of epoch-1 batch-135 = 0.029643245041370392

Training epoch-1 batch-136
Running loss of epoch-1 batch-136 = 0.029808931052684784

Training epoch-1 batch-137
Running loss of epoch-1 batch-137 = 0.029685769230127335

Training epoch-1 batch-138
Running loss of epoch-1 batch-138 = 0.0294451005756855

Training epoch-1 batch-139
Running loss of epoch-1 batch-139 = 0.029354190453886986

Training epoch-1 batch-140
Running loss of epoch-1 batch-140 = 0.02884906530380249

Training epoch-1 batch-141
Running loss of epoch-1 batch-141 = 0.028709542006254196

Training epoch-1 batch-142
Running loss of epoch-1 batch-142 = 0.028928767889738083

Training epoch-1 batch-143
Running loss of epoch-1 batch-143 = 0.028972623869776726

Training epoch-1 batch-144
Running loss of epoch-1 batch-144 = 0.028453689068555832

Training epoch-1 batch-145
Running loss of epoch-1 batch-145 = 0.029377087950706482

Training epoch-1 batch-146
Running loss of epoch-1 batch-146 = 0.027725938707590103

Training epoch-1 batch-147
Running loss of epoch-1 batch-147 = 0.02612605132162571

Training epoch-1 batch-148
Running loss of epoch-1 batch-148 = 0.027679262682795525

Training epoch-1 batch-149
Running loss of epoch-1 batch-149 = 0.028442183509469032

Training epoch-1 batch-150
Running loss of epoch-1 batch-150 = 0.028343209996819496

Training epoch-1 batch-151
Running loss of epoch-1 batch-151 = 0.0269736647605896

Training epoch-1 batch-152
Running loss of epoch-1 batch-152 = 0.026408309116959572

Training epoch-1 batch-153
Running loss of epoch-1 batch-153 = 0.02570742554962635

Training epoch-1 batch-154
Running loss of epoch-1 batch-154 = 0.02667134813964367

Training epoch-1 batch-155
Running loss of epoch-1 batch-155 = 0.02505861409008503

Training epoch-1 batch-156
Running loss of epoch-1 batch-156 = 0.02535965107381344

Training epoch-1 batch-157
Running loss of epoch-1 batch-157 = 0.10487402975559235

Finished training epoch-1.



Average train loss at epoch-1 = 0.03344906536340714

Started Evaluation

Average val loss at epoch-1 = 1.5556141632167917

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 98.69 %
Accuracy for class setUp is: 80.98 %
Accuracy for class onCreate is: 83.90 %
Accuracy for class toString is: 60.75 %
Accuracy for class run is: 25.80 %
Accuracy for class hashCode is: 92.51 %
Accuracy for class init is: 5.38 %
Accuracy for class execute is: 2.01 %
Accuracy for class get is: 12.31 %

Overall Accuracy = 60.90 %


Best Accuracy = 60.90 % at Epoch-1
Saving model after best epoch-1

Finished Evaluation



Started training epoch-2


Training epoch-2 batch-1
Running loss of epoch-2 batch-1 = 0.025630855932831764

Training epoch-2 batch-2
Running loss of epoch-2 batch-2 = 0.025508087128400803

Training epoch-2 batch-3
Running loss of epoch-2 batch-3 = 0.027016159147024155

Training epoch-2 batch-4
Running loss of epoch-2 batch-4 = 0.027534645050764084

Training epoch-2 batch-5
Running loss of epoch-2 batch-5 = 0.023143617436289787

Training epoch-2 batch-6
Running loss of epoch-2 batch-6 = 0.024549037218093872

Training epoch-2 batch-7
Running loss of epoch-2 batch-7 = 0.02651200070977211

Training epoch-2 batch-8
Running loss of epoch-2 batch-8 = 0.027595702558755875

Training epoch-2 batch-9
Running loss of epoch-2 batch-9 = 0.024109363555908203

Training epoch-2 batch-10
Running loss of epoch-2 batch-10 = 0.025386221706867218

Training epoch-2 batch-11
Running loss of epoch-2 batch-11 = 0.0215553417801857

Training epoch-2 batch-12
Running loss of epoch-2 batch-12 = 0.023504510521888733

Training epoch-2 batch-13
Running loss of epoch-2 batch-13 = 0.02514677494764328

Training epoch-2 batch-14
Running loss of epoch-2 batch-14 = 0.023421498015522957

Training epoch-2 batch-15
Running loss of epoch-2 batch-15 = 0.02585485763847828

Training epoch-2 batch-16
Running loss of epoch-2 batch-16 = 0.02443457581102848

Training epoch-2 batch-17
Running loss of epoch-2 batch-17 = 0.01951785944402218

Training epoch-2 batch-18
Running loss of epoch-2 batch-18 = 0.022442515939474106

Training epoch-2 batch-19
Running loss of epoch-2 batch-19 = 0.02123843878507614

Training epoch-2 batch-20
Running loss of epoch-2 batch-20 = 0.02197457104921341

Training epoch-2 batch-21
Running loss of epoch-2 batch-21 = 0.022205298766493797

Training epoch-2 batch-22
Running loss of epoch-2 batch-22 = 0.02278715930879116

Training epoch-2 batch-23
Running loss of epoch-2 batch-23 = 0.022033538669347763

Training epoch-2 batch-24
Running loss of epoch-2 batch-24 = 0.02131015807390213

Training epoch-2 batch-25
Running loss of epoch-2 batch-25 = 0.021681534126400948

Training epoch-2 batch-26
Running loss of epoch-2 batch-26 = 0.020770752802491188

Training epoch-2 batch-27
Running loss of epoch-2 batch-27 = 0.02094198949635029

Training epoch-2 batch-28
Running loss of epoch-2 batch-28 = 0.021038249135017395

Training epoch-2 batch-29
Running loss of epoch-2 batch-29 = 0.0210186168551445

Training epoch-2 batch-30
Running loss of epoch-2 batch-30 = 0.02082117274403572

Training epoch-2 batch-31
Running loss of epoch-2 batch-31 = 0.021787747740745544

Training epoch-2 batch-32
Running loss of epoch-2 batch-32 = 0.021013986319303513

Training epoch-2 batch-33
Running loss of epoch-2 batch-33 = 0.023087991401553154

Training epoch-2 batch-34
Running loss of epoch-2 batch-34 = 0.020655367523431778

Training epoch-2 batch-35
Running loss of epoch-2 batch-35 = 0.01877150870859623

Training epoch-2 batch-36
Running loss of epoch-2 batch-36 = 0.019185006618499756

Training epoch-2 batch-37
Running loss of epoch-2 batch-37 = 0.019458746537566185

Training epoch-2 batch-38
Running loss of epoch-2 batch-38 = 0.018718834966421127

Training epoch-2 batch-39
Running loss of epoch-2 batch-39 = 0.018252138048410416

Training epoch-2 batch-40
Running loss of epoch-2 batch-40 = 0.02128039486706257

Training epoch-2 batch-41
Running loss of epoch-2 batch-41 = 0.01861622929573059

Training epoch-2 batch-42
Running loss of epoch-2 batch-42 = 0.01760266162455082

Training epoch-2 batch-43
Running loss of epoch-2 batch-43 = 0.018214993178844452

Training epoch-2 batch-44
Running loss of epoch-2 batch-44 = 0.0169951394200325

Training epoch-2 batch-45
Running loss of epoch-2 batch-45 = 0.019672313705086708

Training epoch-2 batch-46
Running loss of epoch-2 batch-46 = 0.017946673557162285

Training epoch-2 batch-47
Running loss of epoch-2 batch-47 = 0.019876791164278984

Training epoch-2 batch-48
Running loss of epoch-2 batch-48 = 0.01860814541578293

Training epoch-2 batch-49
Running loss of epoch-2 batch-49 = 0.0190091822296381

Training epoch-2 batch-50
Running loss of epoch-2 batch-50 = 0.017005009576678276

Training epoch-2 batch-51
Running loss of epoch-2 batch-51 = 0.01804102398455143

Training epoch-2 batch-52
Running loss of epoch-2 batch-52 = 0.019383331760764122

Training epoch-2 batch-53
Running loss of epoch-2 batch-53 = 0.017903992906212807

Training epoch-2 batch-54
Running loss of epoch-2 batch-54 = 0.017421772703528404

Training epoch-2 batch-55
Running loss of epoch-2 batch-55 = 0.016578374430537224

Training epoch-2 batch-56
Running loss of epoch-2 batch-56 = 0.018030039966106415

Training epoch-2 batch-57
Running loss of epoch-2 batch-57 = 0.018115626648068428

Training epoch-2 batch-58
Running loss of epoch-2 batch-58 = 0.017777787521481514

Training epoch-2 batch-59
Running loss of epoch-2 batch-59 = 0.015088245272636414

Training epoch-2 batch-60
Running loss of epoch-2 batch-60 = 0.019172674044966698

Training epoch-2 batch-61
Running loss of epoch-2 batch-61 = 0.018626326695084572

Training epoch-2 batch-62
Running loss of epoch-2 batch-62 = 0.01632614992558956

Training epoch-2 batch-63
Running loss of epoch-2 batch-63 = 0.01553213968873024

Training epoch-2 batch-64
Running loss of epoch-2 batch-64 = 0.017969032749533653

Training epoch-2 batch-65
Running loss of epoch-2 batch-65 = 0.01771366037428379

Training epoch-2 batch-66
Running loss of epoch-2 batch-66 = 0.018430037423968315

Training epoch-2 batch-67
Running loss of epoch-2 batch-67 = 0.017885278910398483

Training epoch-2 batch-68
Running loss of epoch-2 batch-68 = 0.018539799377322197

Training epoch-2 batch-69
Running loss of epoch-2 batch-69 = 0.013160821981728077

Training epoch-2 batch-70
Running loss of epoch-2 batch-70 = 0.014491151086986065

Training epoch-2 batch-71
Running loss of epoch-2 batch-71 = 0.01782696694135666

Training epoch-2 batch-72
Running loss of epoch-2 batch-72 = 0.019701853394508362

Training epoch-2 batch-73
Running loss of epoch-2 batch-73 = 0.01655239425599575

Training epoch-2 batch-74
Running loss of epoch-2 batch-74 = 0.01463669165968895

Training epoch-2 batch-75
Running loss of epoch-2 batch-75 = 0.015609771013259888

Training epoch-2 batch-76
Running loss of epoch-2 batch-76 = 0.016831163316965103

Training epoch-2 batch-77
Running loss of epoch-2 batch-77 = 0.015182607807219028

Training epoch-2 batch-78
Running loss of epoch-2 batch-78 = 0.017974181100726128

Training epoch-2 batch-79
Running loss of epoch-2 batch-79 = 0.016344862058758736

Training epoch-2 batch-80
Running loss of epoch-2 batch-80 = 0.01324518397450447

Training epoch-2 batch-81
Running loss of epoch-2 batch-81 = 0.016443483531475067

Training epoch-2 batch-82
Running loss of epoch-2 batch-82 = 0.01440044492483139

Training epoch-2 batch-83
Running loss of epoch-2 batch-83 = 0.015781188383698463

Training epoch-2 batch-84
Running loss of epoch-2 batch-84 = 0.012827045284211636

Training epoch-2 batch-85
Running loss of epoch-2 batch-85 = 0.015866827219724655

Training epoch-2 batch-86
Running loss of epoch-2 batch-86 = 0.016346674412488937

Training epoch-2 batch-87
Running loss of epoch-2 batch-87 = 0.016889553517103195

Training epoch-2 batch-88
Running loss of epoch-2 batch-88 = 0.012039517052471638

Training epoch-2 batch-89
Running loss of epoch-2 batch-89 = 0.011337014846503735

Training epoch-2 batch-90
Running loss of epoch-2 batch-90 = 0.015026271343231201

Training epoch-2 batch-91
Running loss of epoch-2 batch-91 = 0.016261450946331024

Training epoch-2 batch-92
Running loss of epoch-2 batch-92 = 0.012426383793354034

Training epoch-2 batch-93
Running loss of epoch-2 batch-93 = 0.019396347925066948

Training epoch-2 batch-94
Running loss of epoch-2 batch-94 = 0.013941843062639236

Training epoch-2 batch-95
Running loss of epoch-2 batch-95 = 0.01324943546205759

Training epoch-2 batch-96
Running loss of epoch-2 batch-96 = 0.01390643510967493

Training epoch-2 batch-97
Running loss of epoch-2 batch-97 = 0.013072613626718521

Training epoch-2 batch-98
Running loss of epoch-2 batch-98 = 0.013650297187268734

Training epoch-2 batch-99
Running loss of epoch-2 batch-99 = 0.01437170896679163

Training epoch-2 batch-100
Running loss of epoch-2 batch-100 = 0.018361462280154228

Training epoch-2 batch-101
Running loss of epoch-2 batch-101 = 0.016462329775094986

Training epoch-2 batch-102
Running loss of epoch-2 batch-102 = 0.016925590112805367

Training epoch-2 batch-103
Running loss of epoch-2 batch-103 = 0.01072652731090784

Training epoch-2 batch-104
Running loss of epoch-2 batch-104 = 0.015979818999767303

Training epoch-2 batch-105
Running loss of epoch-2 batch-105 = 0.017626235261559486

Training epoch-2 batch-106
Running loss of epoch-2 batch-106 = 0.01776978373527527

Training epoch-2 batch-107
Running loss of epoch-2 batch-107 = 0.014934154227375984

Training epoch-2 batch-108
Running loss of epoch-2 batch-108 = 0.015199354849755764

Training epoch-2 batch-109
Running loss of epoch-2 batch-109 = 0.01636340841650963

Training epoch-2 batch-110
Running loss of epoch-2 batch-110 = 0.016961945220828056

Training epoch-2 batch-111
Running loss of epoch-2 batch-111 = 0.013663486577570438

Training epoch-2 batch-112
Running loss of epoch-2 batch-112 = 0.009709901176393032

Training epoch-2 batch-113
Running loss of epoch-2 batch-113 = 0.013347220607101917

Training epoch-2 batch-114
Running loss of epoch-2 batch-114 = 0.012982473708689213

Training epoch-2 batch-115
Running loss of epoch-2 batch-115 = 0.015593362972140312

Training epoch-2 batch-116
Running loss of epoch-2 batch-116 = 0.013137874193489552

Training epoch-2 batch-117
Running loss of epoch-2 batch-117 = 0.014760363847017288

Training epoch-2 batch-118
Running loss of epoch-2 batch-118 = 0.012732528150081635

Training epoch-2 batch-119
Running loss of epoch-2 batch-119 = 0.011746753938496113

Training epoch-2 batch-120
Running loss of epoch-2 batch-120 = 0.010692374780774117

Training epoch-2 batch-121
Running loss of epoch-2 batch-121 = 0.013282324187457561

Training epoch-2 batch-122
Running loss of epoch-2 batch-122 = 0.011588823050260544

Training epoch-2 batch-123
Running loss of epoch-2 batch-123 = 0.013549679890275002

Training epoch-2 batch-124
Running loss of epoch-2 batch-124 = 0.009672741405665874

Training epoch-2 batch-125
Running loss of epoch-2 batch-125 = 0.012776260264217854

Training epoch-2 batch-126
Running loss of epoch-2 batch-126 = 0.00925200991332531

Training epoch-2 batch-127
Running loss of epoch-2 batch-127 = 0.010051430203020573

Training epoch-2 batch-128
Running loss of epoch-2 batch-128 = 0.011420756578445435

Training epoch-2 batch-129
Running loss of epoch-2 batch-129 = 0.013189240358769894

Training epoch-2 batch-130
Running loss of epoch-2 batch-130 = 0.012582138180732727

Training epoch-2 batch-131
Running loss of epoch-2 batch-131 = 0.014347911812365055

Training epoch-2 batch-132
Running loss of epoch-2 batch-132 = 0.01027033943682909

Training epoch-2 batch-133
Running loss of epoch-2 batch-133 = 0.010729996487498283

Training epoch-2 batch-134
Running loss of epoch-2 batch-134 = 0.01703573949635029

Training epoch-2 batch-135
Running loss of epoch-2 batch-135 = 0.013018300756812096

Training epoch-2 batch-136
Running loss of epoch-2 batch-136 = 0.0125853531062603

Training epoch-2 batch-137
Running loss of epoch-2 batch-137 = 0.010888321325182915

Training epoch-2 batch-138
Running loss of epoch-2 batch-138 = 0.01664401963353157

Training epoch-2 batch-139
Running loss of epoch-2 batch-139 = 0.013681484386324883

Training epoch-2 batch-140
Running loss of epoch-2 batch-140 = 0.01766660064458847

Training epoch-2 batch-141
Running loss of epoch-2 batch-141 = 0.012607033364474773

Training epoch-2 batch-142
Running loss of epoch-2 batch-142 = 0.011717000976204872

Training epoch-2 batch-143
Running loss of epoch-2 batch-143 = 0.016110282391309738

Training epoch-2 batch-144
Running loss of epoch-2 batch-144 = 0.012155221775174141

Training epoch-2 batch-145
Running loss of epoch-2 batch-145 = 0.015172421000897884

Training epoch-2 batch-146
Running loss of epoch-2 batch-146 = 0.014682510867714882

Training epoch-2 batch-147
Running loss of epoch-2 batch-147 = 0.014586751349270344

Training epoch-2 batch-148
Running loss of epoch-2 batch-148 = 0.012931372039020061

Training epoch-2 batch-149
Running loss of epoch-2 batch-149 = 0.013242073357105255

Training epoch-2 batch-150
Running loss of epoch-2 batch-150 = 0.01348709873855114

Training epoch-2 batch-151
Running loss of epoch-2 batch-151 = 0.011891805566847324

Training epoch-2 batch-152
Running loss of epoch-2 batch-152 = 0.014822843484580517

Training epoch-2 batch-153
Running loss of epoch-2 batch-153 = 0.010484077036380768

Training epoch-2 batch-154
Running loss of epoch-2 batch-154 = 0.012150813825428486

Training epoch-2 batch-155
Running loss of epoch-2 batch-155 = 0.014802600257098675

Training epoch-2 batch-156
Running loss of epoch-2 batch-156 = 0.012102398090064526

Training epoch-2 batch-157
Running loss of epoch-2 batch-157 = 0.057955071330070496

Finished training epoch-2.



Average train loss at epoch-2 = 0.017031148409843443

Started Evaluation

Average val loss at epoch-2 = 0.7491499009591184

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 98.20 %
Accuracy for class setUp is: 94.59 %
Accuracy for class onCreate is: 79.85 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 55.94 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 42.60 %
Accuracy for class execute is: 23.69 %
Accuracy for class get is: 64.87 %

Overall Accuracy = 77.47 %


Best Accuracy = 77.47 % at Epoch-2
Saving model after best epoch-2

Finished Evaluation



Started training epoch-3


Training epoch-3 batch-1
Running loss of epoch-3 batch-1 = 0.01368026714771986

Training epoch-3 batch-2
Running loss of epoch-3 batch-2 = 0.010283106938004494

Training epoch-3 batch-3
Running loss of epoch-3 batch-3 = 0.013654150068759918

Training epoch-3 batch-4
Running loss of epoch-3 batch-4 = 0.011571694165468216

Training epoch-3 batch-5
Running loss of epoch-3 batch-5 = 0.009844901971518993

Training epoch-3 batch-6
Running loss of epoch-3 batch-6 = 0.009514464996755123

Training epoch-3 batch-7
Running loss of epoch-3 batch-7 = 0.012023253366351128

Training epoch-3 batch-8
Running loss of epoch-3 batch-8 = 0.009739160537719727

Training epoch-3 batch-9
Running loss of epoch-3 batch-9 = 0.01167407538741827

Training epoch-3 batch-10
Running loss of epoch-3 batch-10 = 0.009614921174943447

Training epoch-3 batch-11
Running loss of epoch-3 batch-11 = 0.012585421092808247

Training epoch-3 batch-12
Running loss of epoch-3 batch-12 = 0.012190252542495728

Training epoch-3 batch-13
Running loss of epoch-3 batch-13 = 0.010005283169448376

Training epoch-3 batch-14
Running loss of epoch-3 batch-14 = 0.010862491093575954

Training epoch-3 batch-15
Running loss of epoch-3 batch-15 = 0.009403727017343044

Training epoch-3 batch-16
Running loss of epoch-3 batch-16 = 0.009886377491056919

Training epoch-3 batch-17
Running loss of epoch-3 batch-17 = 0.009613394737243652

Training epoch-3 batch-18
Running loss of epoch-3 batch-18 = 0.008749923668801785

Training epoch-3 batch-19
Running loss of epoch-3 batch-19 = 0.011488392949104309

Training epoch-3 batch-20
Running loss of epoch-3 batch-20 = 0.010971099138259888

Training epoch-3 batch-21
Running loss of epoch-3 batch-21 = 0.010416049510240555

Training epoch-3 batch-22
Running loss of epoch-3 batch-22 = 0.011579964309930801

Training epoch-3 batch-23
Running loss of epoch-3 batch-23 = 0.009543615393340588

Training epoch-3 batch-24
Running loss of epoch-3 batch-24 = 0.012280828319489956

Training epoch-3 batch-25
Running loss of epoch-3 batch-25 = 0.010715020820498466

Training epoch-3 batch-26
Running loss of epoch-3 batch-26 = 0.010521534830331802

Training epoch-3 batch-27
Running loss of epoch-3 batch-27 = 0.012680827639997005

Training epoch-3 batch-28
Running loss of epoch-3 batch-28 = 0.006919114384800196

Training epoch-3 batch-29
Running loss of epoch-3 batch-29 = 0.011592749506235123

Training epoch-3 batch-30
Running loss of epoch-3 batch-30 = 0.00928038451820612

Training epoch-3 batch-31
Running loss of epoch-3 batch-31 = 0.012635979801416397

Training epoch-3 batch-32
Running loss of epoch-3 batch-32 = 0.011203188449144363

Training epoch-3 batch-33
Running loss of epoch-3 batch-33 = 0.010865207761526108

Training epoch-3 batch-34
Running loss of epoch-3 batch-34 = 0.009364886209368706

Training epoch-3 batch-35
Running loss of epoch-3 batch-35 = 0.010236982256174088

Training epoch-3 batch-36
Running loss of epoch-3 batch-36 = 0.012489218264818192

Training epoch-3 batch-37
Running loss of epoch-3 batch-37 = 0.01480766013264656

Training epoch-3 batch-38
Running loss of epoch-3 batch-38 = 0.011800691485404968

Training epoch-3 batch-39
Running loss of epoch-3 batch-39 = 0.013309204950928688

Training epoch-3 batch-40
Running loss of epoch-3 batch-40 = 0.01085087563842535

Training epoch-3 batch-41
Running loss of epoch-3 batch-41 = 0.00838402658700943

Training epoch-3 batch-42
Running loss of epoch-3 batch-42 = 0.01074258517473936

Training epoch-3 batch-43
Running loss of epoch-3 batch-43 = 0.012590509839355946

Training epoch-3 batch-44
Running loss of epoch-3 batch-44 = 0.012739001773297787

Training epoch-3 batch-45
Running loss of epoch-3 batch-45 = 0.010368229821324348

Training epoch-3 batch-46
Running loss of epoch-3 batch-46 = 0.009107203222811222

Training epoch-3 batch-47
Running loss of epoch-3 batch-47 = 0.00876642670482397

Training epoch-3 batch-48
Running loss of epoch-3 batch-48 = 0.009433222934603691

Training epoch-3 batch-49
Running loss of epoch-3 batch-49 = 0.010297693312168121

Training epoch-3 batch-50
Running loss of epoch-3 batch-50 = 0.01416643988341093

Training epoch-3 batch-51
Running loss of epoch-3 batch-51 = 0.008145560510456562

Training epoch-3 batch-52
Running loss of epoch-3 batch-52 = 0.010404466651380062

Training epoch-3 batch-53
Running loss of epoch-3 batch-53 = 0.011498834937810898

Training epoch-3 batch-54
Running loss of epoch-3 batch-54 = 0.00845625065267086

Training epoch-3 batch-55
Running loss of epoch-3 batch-55 = 0.011042417958378792

Training epoch-3 batch-56
Running loss of epoch-3 batch-56 = 0.009075785987079144

Training epoch-3 batch-57
Running loss of epoch-3 batch-57 = 0.013334834016859531

Training epoch-3 batch-58
Running loss of epoch-3 batch-58 = 0.009866778738796711

Training epoch-3 batch-59
Running loss of epoch-3 batch-59 = 0.00860785972326994

Training epoch-3 batch-60
Running loss of epoch-3 batch-60 = 0.012303216382861137

Training epoch-3 batch-61
Running loss of epoch-3 batch-61 = 0.015493321232497692

Training epoch-3 batch-62
Running loss of epoch-3 batch-62 = 0.014621741138398647

Training epoch-3 batch-63
Running loss of epoch-3 batch-63 = 0.009251857176423073

Training epoch-3 batch-64
Running loss of epoch-3 batch-64 = 0.008232087828218937

Training epoch-3 batch-65
Running loss of epoch-3 batch-65 = 0.009983998723328114

Training epoch-3 batch-66
Running loss of epoch-3 batch-66 = 0.012298512272536755

Training epoch-3 batch-67
Running loss of epoch-3 batch-67 = 0.012127016671001911

Training epoch-3 batch-68
Running loss of epoch-3 batch-68 = 0.009150322526693344

Training epoch-3 batch-69
Running loss of epoch-3 batch-69 = 0.008349156007170677

Training epoch-3 batch-70
Running loss of epoch-3 batch-70 = 0.010508351027965546

Training epoch-3 batch-71
Running loss of epoch-3 batch-71 = 0.008730994537472725

Training epoch-3 batch-72
Running loss of epoch-3 batch-72 = 0.010274442844092846

Training epoch-3 batch-73
Running loss of epoch-3 batch-73 = 0.009894751012325287

Training epoch-3 batch-74
Running loss of epoch-3 batch-74 = 0.008922304958105087

Training epoch-3 batch-75
Running loss of epoch-3 batch-75 = 0.011091037653386593

Training epoch-3 batch-76
Running loss of epoch-3 batch-76 = 0.007291818968951702

Training epoch-3 batch-77
Running loss of epoch-3 batch-77 = 0.011497965082526207

Training epoch-3 batch-78
Running loss of epoch-3 batch-78 = 0.009591402485966682

Training epoch-3 batch-79
Running loss of epoch-3 batch-79 = 0.010597440414130688

Training epoch-3 batch-80
Running loss of epoch-3 batch-80 = 0.013363019563257694

Training epoch-3 batch-81
Running loss of epoch-3 batch-81 = 0.007603097707033157

Training epoch-3 batch-82
Running loss of epoch-3 batch-82 = 0.008135155774652958

Training epoch-3 batch-83
Running loss of epoch-3 batch-83 = 0.008629310876131058

Training epoch-3 batch-84
Running loss of epoch-3 batch-84 = 0.007987908087670803

Training epoch-3 batch-85
Running loss of epoch-3 batch-85 = 0.008172422647476196

Training epoch-3 batch-86
Running loss of epoch-3 batch-86 = 0.010656476020812988

Training epoch-3 batch-87
Running loss of epoch-3 batch-87 = 0.005988176446408033

Training epoch-3 batch-88
Running loss of epoch-3 batch-88 = 0.010681858286261559

Training epoch-3 batch-89
Running loss of epoch-3 batch-89 = 0.009575039148330688

Training epoch-3 batch-90
Running loss of epoch-3 batch-90 = 0.01259522046893835

Training epoch-3 batch-91
Running loss of epoch-3 batch-91 = 0.009383889846503735

Training epoch-3 batch-92
Running loss of epoch-3 batch-92 = 0.015207367949187756

Training epoch-3 batch-93
Running loss of epoch-3 batch-93 = 0.009228172712028027

Training epoch-3 batch-94
Running loss of epoch-3 batch-94 = 0.01163703203201294

Training epoch-3 batch-95
Running loss of epoch-3 batch-95 = 0.009876199066638947

Training epoch-3 batch-96
Running loss of epoch-3 batch-96 = 0.009691702201962471

Training epoch-3 batch-97
Running loss of epoch-3 batch-97 = 0.008012448437511921

Training epoch-3 batch-98
Running loss of epoch-3 batch-98 = 0.006933670025318861

Training epoch-3 batch-99
Running loss of epoch-3 batch-99 = 0.013260981999337673

Training epoch-3 batch-100
Running loss of epoch-3 batch-100 = 0.009149300865828991

Training epoch-3 batch-101
Running loss of epoch-3 batch-101 = 0.006880677305161953

Training epoch-3 batch-102
Running loss of epoch-3 batch-102 = 0.009765858761966228

Training epoch-3 batch-103
Running loss of epoch-3 batch-103 = 0.012562346644699574

Training epoch-3 batch-104
Running loss of epoch-3 batch-104 = 0.01182863861322403

Training epoch-3 batch-105
Running loss of epoch-3 batch-105 = 0.008659071289002895

Training epoch-3 batch-106
Running loss of epoch-3 batch-106 = 0.010509129613637924

Training epoch-3 batch-107
Running loss of epoch-3 batch-107 = 0.01232755184173584

Training epoch-3 batch-108
Running loss of epoch-3 batch-108 = 0.01090167835354805

Training epoch-3 batch-109
Running loss of epoch-3 batch-109 = 0.00823582336306572

Training epoch-3 batch-110
Running loss of epoch-3 batch-110 = 0.009072885848581791

Training epoch-3 batch-111
Running loss of epoch-3 batch-111 = 0.010830452665686607

Training epoch-3 batch-112
Running loss of epoch-3 batch-112 = 0.008645176887512207

Training epoch-3 batch-113
Running loss of epoch-3 batch-113 = 0.008881313726305962

Training epoch-3 batch-114
Running loss of epoch-3 batch-114 = 0.010499414056539536

Training epoch-3 batch-115
Running loss of epoch-3 batch-115 = 0.009401768445968628

Training epoch-3 batch-116
Running loss of epoch-3 batch-116 = 0.011963147670030594

Training epoch-3 batch-117
Running loss of epoch-3 batch-117 = 0.01025338750332594

Training epoch-3 batch-118
Running loss of epoch-3 batch-118 = 0.009821741841733456

Training epoch-3 batch-119
Running loss of epoch-3 batch-119 = 0.010579693131148815

Training epoch-3 batch-120
Running loss of epoch-3 batch-120 = 0.010570124723017216

Training epoch-3 batch-121
Running loss of epoch-3 batch-121 = 0.013085626997053623

Training epoch-3 batch-122
Running loss of epoch-3 batch-122 = 0.009284245781600475

Training epoch-3 batch-123
Running loss of epoch-3 batch-123 = 0.010444022715091705

Training epoch-3 batch-124
Running loss of epoch-3 batch-124 = 0.009189440868794918

Training epoch-3 batch-125
Running loss of epoch-3 batch-125 = 0.009753248654305935

Training epoch-3 batch-126
Running loss of epoch-3 batch-126 = 0.01238258183002472

Training epoch-3 batch-127
Running loss of epoch-3 batch-127 = 0.01292812917381525

Training epoch-3 batch-128
Running loss of epoch-3 batch-128 = 0.011439640074968338

Training epoch-3 batch-129
Running loss of epoch-3 batch-129 = 0.0064781238324940205

Training epoch-3 batch-130
Running loss of epoch-3 batch-130 = 0.009844578802585602

Training epoch-3 batch-131
Running loss of epoch-3 batch-131 = 0.007529433351010084

Training epoch-3 batch-132
Running loss of epoch-3 batch-132 = 0.007754898630082607

Training epoch-3 batch-133
Running loss of epoch-3 batch-133 = 0.011192080564796925

Training epoch-3 batch-134
Running loss of epoch-3 batch-134 = 0.01058177836239338

Training epoch-3 batch-135
Running loss of epoch-3 batch-135 = 0.009209861047565937

Training epoch-3 batch-136
Running loss of epoch-3 batch-136 = 0.010577555745840073

Training epoch-3 batch-137
Running loss of epoch-3 batch-137 = 0.011278423480689526

Training epoch-3 batch-138
Running loss of epoch-3 batch-138 = 0.010301061905920506

Training epoch-3 batch-139
Running loss of epoch-3 batch-139 = 0.009609612636268139

Training epoch-3 batch-140
Running loss of epoch-3 batch-140 = 0.011669096536934376

Training epoch-3 batch-141
Running loss of epoch-3 batch-141 = 0.009295358322560787

Training epoch-3 batch-142
Running loss of epoch-3 batch-142 = 0.009213371202349663

Training epoch-3 batch-143
Running loss of epoch-3 batch-143 = 0.009971264749765396

Training epoch-3 batch-144
Running loss of epoch-3 batch-144 = 0.01227059680968523

Training epoch-3 batch-145
Running loss of epoch-3 batch-145 = 0.006505758501589298

Training epoch-3 batch-146
Running loss of epoch-3 batch-146 = 0.007755167782306671

Training epoch-3 batch-147
Running loss of epoch-3 batch-147 = 0.009674642235040665

Training epoch-3 batch-148
Running loss of epoch-3 batch-148 = 0.010997317731380463

Training epoch-3 batch-149
Running loss of epoch-3 batch-149 = 0.011187763884663582

Training epoch-3 batch-150
Running loss of epoch-3 batch-150 = 0.009172624908387661

Training epoch-3 batch-151
Running loss of epoch-3 batch-151 = 0.01181192509829998

Training epoch-3 batch-152
Running loss of epoch-3 batch-152 = 0.009004364721477032

Training epoch-3 batch-153
Running loss of epoch-3 batch-153 = 0.01068841852247715

Training epoch-3 batch-154
Running loss of epoch-3 batch-154 = 0.010863462463021278

Training epoch-3 batch-155
Running loss of epoch-3 batch-155 = 0.010988081805408001

Training epoch-3 batch-156
Running loss of epoch-3 batch-156 = 0.007959164679050446

Training epoch-3 batch-157
Running loss of epoch-3 batch-157 = 0.04044615849852562

Finished training epoch-3.



Average train loss at epoch-3 = 0.010416977962851525

Started Evaluation

Average val loss at epoch-3 = 0.6323431280539616

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.20 %
Accuracy for class onCreate is: 85.71 %
Accuracy for class toString is: 86.01 %
Accuracy for class run is: 56.39 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 61.88 %
Accuracy for class execute is: 47.39 %
Accuracy for class get is: 41.28 %

Overall Accuracy = 79.00 %


Best Accuracy = 79.00 % at Epoch-3
Saving model after best epoch-3

Finished Evaluation



Started training epoch-4


Training epoch-4 batch-1
Running loss of epoch-4 batch-1 = 0.005717706400901079

Training epoch-4 batch-2
Running loss of epoch-4 batch-2 = 0.007132175378501415

Training epoch-4 batch-3
Running loss of epoch-4 batch-3 = 0.010463410057127476

Training epoch-4 batch-4
Running loss of epoch-4 batch-4 = 0.009977540001273155

Training epoch-4 batch-5
Running loss of epoch-4 batch-5 = 0.006592161953449249

Training epoch-4 batch-6
Running loss of epoch-4 batch-6 = 0.010525621473789215

Training epoch-4 batch-7
Running loss of epoch-4 batch-7 = 0.008245405741035938

Training epoch-4 batch-8
Running loss of epoch-4 batch-8 = 0.008692211471498013

Training epoch-4 batch-9
Running loss of epoch-4 batch-9 = 0.0083407461643219

Training epoch-4 batch-10
Running loss of epoch-4 batch-10 = 0.006573708727955818

Training epoch-4 batch-11
Running loss of epoch-4 batch-11 = 0.008136240765452385

Training epoch-4 batch-12
Running loss of epoch-4 batch-12 = 0.006722243037074804

Training epoch-4 batch-13
Running loss of epoch-4 batch-13 = 0.005488204304128885

Training epoch-4 batch-14
Running loss of epoch-4 batch-14 = 0.009812655858695507

Training epoch-4 batch-15
Running loss of epoch-4 batch-15 = 0.007705253548920155

Training epoch-4 batch-16
Running loss of epoch-4 batch-16 = 0.008658238686621189

Training epoch-4 batch-17
Running loss of epoch-4 batch-17 = 0.008679409511387348

Training epoch-4 batch-18
Running loss of epoch-4 batch-18 = 0.006074399687349796

Training epoch-4 batch-19
Running loss of epoch-4 batch-19 = 0.0077814701944589615

Training epoch-4 batch-20
Running loss of epoch-4 batch-20 = 0.007741966750472784

Training epoch-4 batch-21
Running loss of epoch-4 batch-21 = 0.007437089923769236

Training epoch-4 batch-22
Running loss of epoch-4 batch-22 = 0.006399970501661301

Training epoch-4 batch-23
Running loss of epoch-4 batch-23 = 0.009314215742051601

Training epoch-4 batch-24
Running loss of epoch-4 batch-24 = 0.00874806847423315

Training epoch-4 batch-25
Running loss of epoch-4 batch-25 = 0.0065484121441841125

Training epoch-4 batch-26
Running loss of epoch-4 batch-26 = 0.007008317857980728

Training epoch-4 batch-27
Running loss of epoch-4 batch-27 = 0.00811056513339281

Training epoch-4 batch-28
Running loss of epoch-4 batch-28 = 0.008466320112347603

Training epoch-4 batch-29
Running loss of epoch-4 batch-29 = 0.008276409469544888

Training epoch-4 batch-30
Running loss of epoch-4 batch-30 = 0.007286103442311287

Training epoch-4 batch-31
Running loss of epoch-4 batch-31 = 0.00649862689897418

Training epoch-4 batch-32
Running loss of epoch-4 batch-32 = 0.008743508718907833

Training epoch-4 batch-33
Running loss of epoch-4 batch-33 = 0.006888324394822121

Training epoch-4 batch-34
Running loss of epoch-4 batch-34 = 0.008741180412471294

Training epoch-4 batch-35
Running loss of epoch-4 batch-35 = 0.006653917953372002

Training epoch-4 batch-36
Running loss of epoch-4 batch-36 = 0.008599539287388325

Training epoch-4 batch-37
Running loss of epoch-4 batch-37 = 0.006335445214062929

Training epoch-4 batch-38
Running loss of epoch-4 batch-38 = 0.009720556437969208

Training epoch-4 batch-39
Running loss of epoch-4 batch-39 = 0.010589992627501488

Training epoch-4 batch-40
Running loss of epoch-4 batch-40 = 0.006287083961069584

Training epoch-4 batch-41
Running loss of epoch-4 batch-41 = 0.008566737174987793

Training epoch-4 batch-42
Running loss of epoch-4 batch-42 = 0.0077749197371304035

Training epoch-4 batch-43
Running loss of epoch-4 batch-43 = 0.005375179462134838

Training epoch-4 batch-44
Running loss of epoch-4 batch-44 = 0.008820702321827412

Training epoch-4 batch-45
Running loss of epoch-4 batch-45 = 0.0049924603663384914

Training epoch-4 batch-46
Running loss of epoch-4 batch-46 = 0.008374913595616817

Training epoch-4 batch-47
Running loss of epoch-4 batch-47 = 0.006886166986078024

Training epoch-4 batch-48
Running loss of epoch-4 batch-48 = 0.007642919197678566

Training epoch-4 batch-49
Running loss of epoch-4 batch-49 = 0.00833227951079607

Training epoch-4 batch-50
Running loss of epoch-4 batch-50 = 0.007789312861859798

Training epoch-4 batch-51
Running loss of epoch-4 batch-51 = 0.008072559721767902

Training epoch-4 batch-52
Running loss of epoch-4 batch-52 = 0.007613791152834892

Training epoch-4 batch-53
Running loss of epoch-4 batch-53 = 0.008433951996266842

Training epoch-4 batch-54
Running loss of epoch-4 batch-54 = 0.009351837448775768

Training epoch-4 batch-55
Running loss of epoch-4 batch-55 = 0.007291979156434536

Training epoch-4 batch-56
Running loss of epoch-4 batch-56 = 0.009812898002564907

Training epoch-4 batch-57
Running loss of epoch-4 batch-57 = 0.004405819345265627

Training epoch-4 batch-58
Running loss of epoch-4 batch-58 = 0.008770491927862167

Training epoch-4 batch-59
Running loss of epoch-4 batch-59 = 0.007179311476647854

Training epoch-4 batch-60
Running loss of epoch-4 batch-60 = 0.009535838849842548

Training epoch-4 batch-61
Running loss of epoch-4 batch-61 = 0.004639978986233473

Training epoch-4 batch-62
Running loss of epoch-4 batch-62 = 0.00554514629766345

Training epoch-4 batch-63
Running loss of epoch-4 batch-63 = 0.005933886393904686

Training epoch-4 batch-64
Running loss of epoch-4 batch-64 = 0.008446354418992996

Training epoch-4 batch-65
Running loss of epoch-4 batch-65 = 0.00760568818077445

Training epoch-4 batch-66
Running loss of epoch-4 batch-66 = 0.007343129720538855

Training epoch-4 batch-67
Running loss of epoch-4 batch-67 = 0.006536413915455341

Training epoch-4 batch-68
Running loss of epoch-4 batch-68 = 0.008415380492806435

Training epoch-4 batch-69
Running loss of epoch-4 batch-69 = 0.008254889398813248

Training epoch-4 batch-70
Running loss of epoch-4 batch-70 = 0.005628685466945171

Training epoch-4 batch-71
Running loss of epoch-4 batch-71 = 0.008566388860344887

Training epoch-4 batch-72
Running loss of epoch-4 batch-72 = 0.01077873446047306

Training epoch-4 batch-73
Running loss of epoch-4 batch-73 = 0.00996699370443821

Training epoch-4 batch-74
Running loss of epoch-4 batch-74 = 0.008157910779118538

Training epoch-4 batch-75
Running loss of epoch-4 batch-75 = 0.012903410941362381

Training epoch-4 batch-76
Running loss of epoch-4 batch-76 = 0.008628873154520988

Training epoch-4 batch-77
Running loss of epoch-4 batch-77 = 0.006985987536609173

Training epoch-4 batch-78
Running loss of epoch-4 batch-78 = 0.004657142795622349

Training epoch-4 batch-79
Running loss of epoch-4 batch-79 = 0.0066901003010571

Training epoch-4 batch-80
Running loss of epoch-4 batch-80 = 0.00676159979775548

Training epoch-4 batch-81
Running loss of epoch-4 batch-81 = 0.00617524329572916

Training epoch-4 batch-82
Running loss of epoch-4 batch-82 = 0.011733494699001312

Training epoch-4 batch-83
Running loss of epoch-4 batch-83 = 0.006823025643825531

Training epoch-4 batch-84
Running loss of epoch-4 batch-84 = 0.01013067364692688

Training epoch-4 batch-85
Running loss of epoch-4 batch-85 = 0.005547508131712675

Training epoch-4 batch-86
Running loss of epoch-4 batch-86 = 0.009983701631426811

Training epoch-4 batch-87
Running loss of epoch-4 batch-87 = 0.0073222508653998375

Training epoch-4 batch-88
Running loss of epoch-4 batch-88 = 0.0042144195176661015

Training epoch-4 batch-89
Running loss of epoch-4 batch-89 = 0.004710850305855274

Training epoch-4 batch-90
Running loss of epoch-4 batch-90 = 0.008087635971605778

Training epoch-4 batch-91
Running loss of epoch-4 batch-91 = 0.011581111699342728

Training epoch-4 batch-92
Running loss of epoch-4 batch-92 = 0.008361798711121082

Training epoch-4 batch-93
Running loss of epoch-4 batch-93 = 0.007932517677545547

Training epoch-4 batch-94
Running loss of epoch-4 batch-94 = 0.006370902992784977

Training epoch-4 batch-95
Running loss of epoch-4 batch-95 = 0.008082765154540539

Training epoch-4 batch-96
Running loss of epoch-4 batch-96 = 0.008073561824858189

Training epoch-4 batch-97
Running loss of epoch-4 batch-97 = 0.010450500063598156

Training epoch-4 batch-98
Running loss of epoch-4 batch-98 = 0.0081032644957304

Training epoch-4 batch-99
Running loss of epoch-4 batch-99 = 0.007551727816462517

Training epoch-4 batch-100
Running loss of epoch-4 batch-100 = 0.006821318529546261

Training epoch-4 batch-101
Running loss of epoch-4 batch-101 = 0.006993051618337631

Training epoch-4 batch-102
Running loss of epoch-4 batch-102 = 0.006447636988013983

Training epoch-4 batch-103
Running loss of epoch-4 batch-103 = 0.005928667727857828

Training epoch-4 batch-104
Running loss of epoch-4 batch-104 = 0.010324914008378983

Training epoch-4 batch-105
Running loss of epoch-4 batch-105 = 0.007750426419079304

Training epoch-4 batch-106
Running loss of epoch-4 batch-106 = 0.008348505944013596

Training epoch-4 batch-107
Running loss of epoch-4 batch-107 = 0.007265504449605942

Training epoch-4 batch-108
Running loss of epoch-4 batch-108 = 0.005333418026566505

Training epoch-4 batch-109
Running loss of epoch-4 batch-109 = 0.004303854424506426

Training epoch-4 batch-110
Running loss of epoch-4 batch-110 = 0.007857742719352245

Training epoch-4 batch-111
Running loss of epoch-4 batch-111 = 0.01130897831171751

Training epoch-4 batch-112
Running loss of epoch-4 batch-112 = 0.010305339470505714

Training epoch-4 batch-113
Running loss of epoch-4 batch-113 = 0.0077834175899624825

Training epoch-4 batch-114
Running loss of epoch-4 batch-114 = 0.005311035085469484

Training epoch-4 batch-115
Running loss of epoch-4 batch-115 = 0.009162263944745064

Training epoch-4 batch-116
Running loss of epoch-4 batch-116 = 0.012074033729732037

Training epoch-4 batch-117
Running loss of epoch-4 batch-117 = 0.007108849473297596

Training epoch-4 batch-118
Running loss of epoch-4 batch-118 = 0.008821530267596245

Training epoch-4 batch-119
Running loss of epoch-4 batch-119 = 0.007643093820661306

Training epoch-4 batch-120
Running loss of epoch-4 batch-120 = 0.009115959517657757

Training epoch-4 batch-121
Running loss of epoch-4 batch-121 = 0.009892554953694344

Training epoch-4 batch-122
Running loss of epoch-4 batch-122 = 0.0072866384871304035

Training epoch-4 batch-123
Running loss of epoch-4 batch-123 = 0.00680500129237771

Training epoch-4 batch-124
Running loss of epoch-4 batch-124 = 0.006305500399321318

Training epoch-4 batch-125
Running loss of epoch-4 batch-125 = 0.007719295099377632

Training epoch-4 batch-126
Running loss of epoch-4 batch-126 = 0.009068900719285011

Training epoch-4 batch-127
Running loss of epoch-4 batch-127 = 0.007410741411149502

Training epoch-4 batch-128
Running loss of epoch-4 batch-128 = 0.005790551193058491

Training epoch-4 batch-129
Running loss of epoch-4 batch-129 = 0.00865977257490158

Training epoch-4 batch-130
Running loss of epoch-4 batch-130 = 0.006201025098562241

Training epoch-4 batch-131
Running loss of epoch-4 batch-131 = 0.006995869800448418

Training epoch-4 batch-132
Running loss of epoch-4 batch-132 = 0.011255196295678616

Training epoch-4 batch-133
Running loss of epoch-4 batch-133 = 0.009431576356291771

Training epoch-4 batch-134
Running loss of epoch-4 batch-134 = 0.007233317010104656

Training epoch-4 batch-135
Running loss of epoch-4 batch-135 = 0.00786050595343113

Training epoch-4 batch-136
Running loss of epoch-4 batch-136 = 0.008678674697875977

Training epoch-4 batch-137
Running loss of epoch-4 batch-137 = 0.005244327709078789

Training epoch-4 batch-138
Running loss of epoch-4 batch-138 = 0.007725380826741457

Training epoch-4 batch-139
Running loss of epoch-4 batch-139 = 0.009811650030314922

Training epoch-4 batch-140
Running loss of epoch-4 batch-140 = 0.007461178116500378

Training epoch-4 batch-141
Running loss of epoch-4 batch-141 = 0.005514964461326599

Training epoch-4 batch-142
Running loss of epoch-4 batch-142 = 0.007117260247468948

Training epoch-4 batch-143
Running loss of epoch-4 batch-143 = 0.006882767658680677

Training epoch-4 batch-144
Running loss of epoch-4 batch-144 = 0.007691084407269955

Training epoch-4 batch-145
Running loss of epoch-4 batch-145 = 0.009352657943964005

Training epoch-4 batch-146
Running loss of epoch-4 batch-146 = 0.006369812414050102

Training epoch-4 batch-147
Running loss of epoch-4 batch-147 = 0.0072937156073749065

Training epoch-4 batch-148
Running loss of epoch-4 batch-148 = 0.007928827777504921

Training epoch-4 batch-149
Running loss of epoch-4 batch-149 = 0.0072890399023890495

Training epoch-4 batch-150
Running loss of epoch-4 batch-150 = 0.006382460240274668

Training epoch-4 batch-151
Running loss of epoch-4 batch-151 = 0.010570906102657318

Training epoch-4 batch-152
Running loss of epoch-4 batch-152 = 0.005560156423598528

Training epoch-4 batch-153
Running loss of epoch-4 batch-153 = 0.0074528479017317295

Training epoch-4 batch-154
Running loss of epoch-4 batch-154 = 0.004035715479403734

Training epoch-4 batch-155
Running loss of epoch-4 batch-155 = 0.009526731446385384

Training epoch-4 batch-156
Running loss of epoch-4 batch-156 = 0.007859764620661736

Training epoch-4 batch-157
Running loss of epoch-4 batch-157 = 0.0638776645064354

Finished training epoch-4.



Average train loss at epoch-4 = 0.007872431790828706

Started Evaluation

Average val loss at epoch-4 = 0.6153949461999888

Accuracy for classes:
Accuracy for class equals is: 94.22 %
Accuracy for class main is: 97.54 %
Accuracy for class setUp is: 70.16 %
Accuracy for class onCreate is: 89.13 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 56.62 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.95 %
Accuracy for class execute is: 34.54 %
Accuracy for class get is: 90.51 %

Overall Accuracy = 79.99 %


Best Accuracy = 79.99 % at Epoch-4
Saving model after best epoch-4

Finished Evaluation



Started training epoch-5


Training epoch-5 batch-1
Running loss of epoch-5 batch-1 = 0.005965524818748236

Training epoch-5 batch-2
Running loss of epoch-5 batch-2 = 0.004796730820089579

Training epoch-5 batch-3
Running loss of epoch-5 batch-3 = 0.008899832144379616

Training epoch-5 batch-4
Running loss of epoch-5 batch-4 = 0.0083961496129632

Training epoch-5 batch-5
Running loss of epoch-5 batch-5 = 0.006180087104439735

Training epoch-5 batch-6
Running loss of epoch-5 batch-6 = 0.006010870449244976

Training epoch-5 batch-7
Running loss of epoch-5 batch-7 = 0.005319019313901663

Training epoch-5 batch-8
Running loss of epoch-5 batch-8 = 0.005040922202169895

Training epoch-5 batch-9
Running loss of epoch-5 batch-9 = 0.006222344934940338

Training epoch-5 batch-10
Running loss of epoch-5 batch-10 = 0.007083030417561531

Training epoch-5 batch-11
Running loss of epoch-5 batch-11 = 0.007951231673359871

Training epoch-5 batch-12
Running loss of epoch-5 batch-12 = 0.004602872766554356

Training epoch-5 batch-13
Running loss of epoch-5 batch-13 = 0.007305977400392294

Training epoch-5 batch-14
Running loss of epoch-5 batch-14 = 0.00689710071310401

Training epoch-5 batch-15
Running loss of epoch-5 batch-15 = 0.005859042517840862

Training epoch-5 batch-16
Running loss of epoch-5 batch-16 = 0.005720604211091995

Training epoch-5 batch-17
Running loss of epoch-5 batch-17 = 0.009155327454209328

Training epoch-5 batch-18
Running loss of epoch-5 batch-18 = 0.007406321354210377

Training epoch-5 batch-19
Running loss of epoch-5 batch-19 = 0.007929079234600067

Training epoch-5 batch-20
Running loss of epoch-5 batch-20 = 0.0036353233736008406

Training epoch-5 batch-21
Running loss of epoch-5 batch-21 = 0.005916088819503784

Training epoch-5 batch-22
Running loss of epoch-5 batch-22 = 0.00811021588742733

Training epoch-5 batch-23
Running loss of epoch-5 batch-23 = 0.0065946755930781364

Training epoch-5 batch-24
Running loss of epoch-5 batch-24 = 0.006240224931389093

Training epoch-5 batch-25
Running loss of epoch-5 batch-25 = 0.006536630913615227

Training epoch-5 batch-26
Running loss of epoch-5 batch-26 = 0.006362141575664282

Training epoch-5 batch-27
Running loss of epoch-5 batch-27 = 0.006135123316198587

Training epoch-5 batch-28
Running loss of epoch-5 batch-28 = 0.007463398389518261

Training epoch-5 batch-29
Running loss of epoch-5 batch-29 = 0.005734696984291077

Training epoch-5 batch-30
Running loss of epoch-5 batch-30 = 0.006274665240198374

Training epoch-5 batch-31
Running loss of epoch-5 batch-31 = 0.00502012437209487

Training epoch-5 batch-32
Running loss of epoch-5 batch-32 = 0.00439881207421422

Training epoch-5 batch-33
Running loss of epoch-5 batch-33 = 0.009900945238769054

Training epoch-5 batch-34
Running loss of epoch-5 batch-34 = 0.005444913171231747

Training epoch-5 batch-35
Running loss of epoch-5 batch-35 = 0.008883367292582989

Training epoch-5 batch-36
Running loss of epoch-5 batch-36 = 0.006643432192504406

Training epoch-5 batch-37
Running loss of epoch-5 batch-37 = 0.0059526762925088406

Training epoch-5 batch-38
Running loss of epoch-5 batch-38 = 0.007537424564361572

Training epoch-5 batch-39
Running loss of epoch-5 batch-39 = 0.006355196703225374

Training epoch-5 batch-40
Running loss of epoch-5 batch-40 = 0.005563157610595226

Training epoch-5 batch-41
Running loss of epoch-5 batch-41 = 0.005419641733169556

Training epoch-5 batch-42
Running loss of epoch-5 batch-42 = 0.0047695208340883255

Training epoch-5 batch-43
Running loss of epoch-5 batch-43 = 0.004883031360805035

Training epoch-5 batch-44
Running loss of epoch-5 batch-44 = 0.00651501165702939

Training epoch-5 batch-45
Running loss of epoch-5 batch-45 = 0.007051168940961361

Training epoch-5 batch-46
Running loss of epoch-5 batch-46 = 0.006745859980583191

Training epoch-5 batch-47
Running loss of epoch-5 batch-47 = 0.009236442856490612

Training epoch-5 batch-48
Running loss of epoch-5 batch-48 = 0.006603708956390619

Training epoch-5 batch-49
Running loss of epoch-5 batch-49 = 0.005516481585800648

Training epoch-5 batch-50
Running loss of epoch-5 batch-50 = 0.005009918473660946

Training epoch-5 batch-51
Running loss of epoch-5 batch-51 = 0.0053705014288425446

Training epoch-5 batch-52
Running loss of epoch-5 batch-52 = 0.009047269821166992

Training epoch-5 batch-53
Running loss of epoch-5 batch-53 = 0.004291495308279991

Training epoch-5 batch-54
Running loss of epoch-5 batch-54 = 0.007485012523829937

Training epoch-5 batch-55
Running loss of epoch-5 batch-55 = 0.003642578376457095

Training epoch-5 batch-56
Running loss of epoch-5 batch-56 = 0.007258275523781776

Training epoch-5 batch-57
Running loss of epoch-5 batch-57 = 0.007537193596363068

Training epoch-5 batch-58
Running loss of epoch-5 batch-58 = 0.005930266343057156

Training epoch-5 batch-59
Running loss of epoch-5 batch-59 = 0.0039621032774448395

Training epoch-5 batch-60
Running loss of epoch-5 batch-60 = 0.005990568082779646

Training epoch-5 batch-61
Running loss of epoch-5 batch-61 = 0.006807303987443447

Training epoch-5 batch-62
Running loss of epoch-5 batch-62 = 0.006165374536067247

Training epoch-5 batch-63
Running loss of epoch-5 batch-63 = 0.0031139422208070755

Training epoch-5 batch-64
Running loss of epoch-5 batch-64 = 0.005192432552576065

Training epoch-5 batch-65
Running loss of epoch-5 batch-65 = 0.0060378024354577065

Training epoch-5 batch-66
Running loss of epoch-5 batch-66 = 0.007048001512885094

Training epoch-5 batch-67
Running loss of epoch-5 batch-67 = 0.005226480308920145

Training epoch-5 batch-68
Running loss of epoch-5 batch-68 = 0.005598945543169975

Training epoch-5 batch-69
Running loss of epoch-5 batch-69 = 0.0062816329300403595

Training epoch-5 batch-70
Running loss of epoch-5 batch-70 = 0.007629155181348324

Training epoch-5 batch-71
Running loss of epoch-5 batch-71 = 0.008262896910309792

Training epoch-5 batch-72
Running loss of epoch-5 batch-72 = 0.008864007890224457

Training epoch-5 batch-73
Running loss of epoch-5 batch-73 = 0.009032472968101501

Training epoch-5 batch-74
Running loss of epoch-5 batch-74 = 0.007247741799801588

Training epoch-5 batch-75
Running loss of epoch-5 batch-75 = 0.00736610684543848

Training epoch-5 batch-76
Running loss of epoch-5 batch-76 = 0.009270982816815376

Training epoch-5 batch-77
Running loss of epoch-5 batch-77 = 0.007839079946279526

Training epoch-5 batch-78
Running loss of epoch-5 batch-78 = 0.005800820887088776

Training epoch-5 batch-79
Running loss of epoch-5 batch-79 = 0.005453248508274555

Training epoch-5 batch-80
Running loss of epoch-5 batch-80 = 0.006599086336791515

Training epoch-5 batch-81
Running loss of epoch-5 batch-81 = 0.007001667283475399

Training epoch-5 batch-82
Running loss of epoch-5 batch-82 = 0.006233926862478256

Training epoch-5 batch-83
Running loss of epoch-5 batch-83 = 0.005753010045737028

Training epoch-5 batch-84
Running loss of epoch-5 batch-84 = 0.00909754354506731

Training epoch-5 batch-85
Running loss of epoch-5 batch-85 = 0.006678808946162462

Training epoch-5 batch-86
Running loss of epoch-5 batch-86 = 0.007363500073552132

Training epoch-5 batch-87
Running loss of epoch-5 batch-87 = 0.007149083539843559

Training epoch-5 batch-88
Running loss of epoch-5 batch-88 = 0.007750936783850193

Training epoch-5 batch-89
Running loss of epoch-5 batch-89 = 0.006230194587260485

Training epoch-5 batch-90
Running loss of epoch-5 batch-90 = 0.004634178709238768

Training epoch-5 batch-91
Running loss of epoch-5 batch-91 = 0.011921655386686325

Training epoch-5 batch-92
Running loss of epoch-5 batch-92 = 0.0069708386436104774

Training epoch-5 batch-93
Running loss of epoch-5 batch-93 = 0.004927959758788347

Training epoch-5 batch-94
Running loss of epoch-5 batch-94 = 0.00804469920694828

Training epoch-5 batch-95
Running loss of epoch-5 batch-95 = 0.005137033760547638

Training epoch-5 batch-96
Running loss of epoch-5 batch-96 = 0.006954962387681007

Training epoch-5 batch-97
Running loss of epoch-5 batch-97 = 0.006638886407017708

Training epoch-5 batch-98
Running loss of epoch-5 batch-98 = 0.007544454652816057

Training epoch-5 batch-99
Running loss of epoch-5 batch-99 = 0.005234862212091684

Training epoch-5 batch-100
Running loss of epoch-5 batch-100 = 0.005582770332694054

Training epoch-5 batch-101
Running loss of epoch-5 batch-101 = 0.004875736776739359

Training epoch-5 batch-102
Running loss of epoch-5 batch-102 = 0.0069407131522893906

Training epoch-5 batch-103
Running loss of epoch-5 batch-103 = 0.005979685112833977

Training epoch-5 batch-104
Running loss of epoch-5 batch-104 = 0.007533457595854998

Training epoch-5 batch-105
Running loss of epoch-5 batch-105 = 0.005795139819383621

Training epoch-5 batch-106
Running loss of epoch-5 batch-106 = 0.003247091080993414

Training epoch-5 batch-107
Running loss of epoch-5 batch-107 = 0.008406952023506165

Training epoch-5 batch-108
Running loss of epoch-5 batch-108 = 0.007503245957195759

Training epoch-5 batch-109
Running loss of epoch-5 batch-109 = 0.004865278489887714

Training epoch-5 batch-110
Running loss of epoch-5 batch-110 = 0.006983254104852676

Training epoch-5 batch-111
Running loss of epoch-5 batch-111 = 0.004566507413983345

Training epoch-5 batch-112
Running loss of epoch-5 batch-112 = 0.006870755925774574

Training epoch-5 batch-113
Running loss of epoch-5 batch-113 = 0.011311037465929985

Training epoch-5 batch-114
Running loss of epoch-5 batch-114 = 0.0038457296323031187

Training epoch-5 batch-115
Running loss of epoch-5 batch-115 = 0.002427485305815935

Training epoch-5 batch-116
Running loss of epoch-5 batch-116 = 0.006923561915755272

Training epoch-5 batch-117
Running loss of epoch-5 batch-117 = 0.005497460253536701

Training epoch-5 batch-118
Running loss of epoch-5 batch-118 = 0.006598966661840677

Training epoch-5 batch-119
Running loss of epoch-5 batch-119 = 0.0067296684719622135

Training epoch-5 batch-120
Running loss of epoch-5 batch-120 = 0.004865000490099192

Training epoch-5 batch-121
Running loss of epoch-5 batch-121 = 0.005867072846740484

Training epoch-5 batch-122
Running loss of epoch-5 batch-122 = 0.003893129760399461

Training epoch-5 batch-123
Running loss of epoch-5 batch-123 = 0.004854832775890827

Training epoch-5 batch-124
Running loss of epoch-5 batch-124 = 0.0054981904104352

Training epoch-5 batch-125
Running loss of epoch-5 batch-125 = 0.0037180555518716574

Training epoch-5 batch-126
Running loss of epoch-5 batch-126 = 0.006865649949759245

Training epoch-5 batch-127
Running loss of epoch-5 batch-127 = 0.004321250133216381

Training epoch-5 batch-128
Running loss of epoch-5 batch-128 = 0.0064355419017374516

Training epoch-5 batch-129
Running loss of epoch-5 batch-129 = 0.0036144170444458723

Training epoch-5 batch-130
Running loss of epoch-5 batch-130 = 0.006750520784407854

Training epoch-5 batch-131
Running loss of epoch-5 batch-131 = 0.008380013518035412

Training epoch-5 batch-132
Running loss of epoch-5 batch-132 = 0.004950398113578558

Training epoch-5 batch-133
Running loss of epoch-5 batch-133 = 0.008222655393183231

Training epoch-5 batch-134
Running loss of epoch-5 batch-134 = 0.006647966802120209

Training epoch-5 batch-135
Running loss of epoch-5 batch-135 = 0.007506552617996931

Training epoch-5 batch-136
Running loss of epoch-5 batch-136 = 0.0043576364405453205

Training epoch-5 batch-137
Running loss of epoch-5 batch-137 = 0.004634574521332979

Training epoch-5 batch-138
Running loss of epoch-5 batch-138 = 0.005310832988470793

Training epoch-5 batch-139
Running loss of epoch-5 batch-139 = 0.004359248094260693

Training epoch-5 batch-140
Running loss of epoch-5 batch-140 = 0.004530876874923706

Training epoch-5 batch-141
Running loss of epoch-5 batch-141 = 0.0050070821307599545

Training epoch-5 batch-142
Running loss of epoch-5 batch-142 = 0.005748137831687927

Training epoch-5 batch-143
Running loss of epoch-5 batch-143 = 0.00737536558881402

Training epoch-5 batch-144
Running loss of epoch-5 batch-144 = 0.0039100428111851215

Training epoch-5 batch-145
Running loss of epoch-5 batch-145 = 0.009076053276658058

Training epoch-5 batch-146
Running loss of epoch-5 batch-146 = 0.005944899749010801

Training epoch-5 batch-147
Running loss of epoch-5 batch-147 = 0.005401957780122757

Training epoch-5 batch-148
Running loss of epoch-5 batch-148 = 0.003289232263341546

Training epoch-5 batch-149
Running loss of epoch-5 batch-149 = 0.0035785569343715906

Training epoch-5 batch-150
Running loss of epoch-5 batch-150 = 0.0065153432078659534

Training epoch-5 batch-151
Running loss of epoch-5 batch-151 = 0.007521441671997309

Training epoch-5 batch-152
Running loss of epoch-5 batch-152 = 0.005431341007351875

Training epoch-5 batch-153
Running loss of epoch-5 batch-153 = 0.004805892240256071

Training epoch-5 batch-154
Running loss of epoch-5 batch-154 = 0.004875576123595238

Training epoch-5 batch-155
Running loss of epoch-5 batch-155 = 0.0052958098240196705

Training epoch-5 batch-156
Running loss of epoch-5 batch-156 = 0.0083091976121068

Training epoch-5 batch-157
Running loss of epoch-5 batch-157 = 0.007973536849021912

Finished training epoch-5.



Average train loss at epoch-5 = 0.006284577932953834

Started Evaluation

Average val loss at epoch-5 = 0.5724428308588502

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 95.90 %
Accuracy for class onCreate is: 86.99 %
Accuracy for class toString is: 86.01 %
Accuracy for class run is: 68.95 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 35.87 %
Accuracy for class execute is: 45.78 %
Accuracy for class get is: 74.36 %

Overall Accuracy = 81.47 %


Best Accuracy = 81.47 % at Epoch-5
Saving model after best epoch-5

Finished Evaluation



Started training epoch-6


Training epoch-6 batch-1
Running loss of epoch-6 batch-1 = 0.003842670703306794

Training epoch-6 batch-2
Running loss of epoch-6 batch-2 = 0.005329422652721405

Training epoch-6 batch-3
Running loss of epoch-6 batch-3 = 0.0052973129786551

Training epoch-6 batch-4
Running loss of epoch-6 batch-4 = 0.0035013570450246334

Training epoch-6 batch-5
Running loss of epoch-6 batch-5 = 0.002427717437967658

Training epoch-6 batch-6
Running loss of epoch-6 batch-6 = 0.003689027624204755

Training epoch-6 batch-7
Running loss of epoch-6 batch-7 = 0.011182300746440887

Training epoch-6 batch-8
Running loss of epoch-6 batch-8 = 0.004702718462795019

Training epoch-6 batch-9
Running loss of epoch-6 batch-9 = 0.0057757580652832985

Training epoch-6 batch-10
Running loss of epoch-6 batch-10 = 0.0048911976628005505

Training epoch-6 batch-11
Running loss of epoch-6 batch-11 = 0.006933895871043205

Training epoch-6 batch-12
Running loss of epoch-6 batch-12 = 0.004649706184864044

Training epoch-6 batch-13
Running loss of epoch-6 batch-13 = 0.004522519186139107

Training epoch-6 batch-14
Running loss of epoch-6 batch-14 = 0.005135948769748211

Training epoch-6 batch-15
Running loss of epoch-6 batch-15 = 0.0073087201453745365

Training epoch-6 batch-16
Running loss of epoch-6 batch-16 = 0.0024818507954478264

Training epoch-6 batch-17
Running loss of epoch-6 batch-17 = 0.004984044935554266

Training epoch-6 batch-18
Running loss of epoch-6 batch-18 = 0.004337077960371971

Training epoch-6 batch-19
Running loss of epoch-6 batch-19 = 0.004017610102891922

Training epoch-6 batch-20
Running loss of epoch-6 batch-20 = 0.004222521558403969

Training epoch-6 batch-21
Running loss of epoch-6 batch-21 = 0.005532334093004465

Training epoch-6 batch-22
Running loss of epoch-6 batch-22 = 0.004054202698171139

Training epoch-6 batch-23
Running loss of epoch-6 batch-23 = 0.0050560724921524525

Training epoch-6 batch-24
Running loss of epoch-6 batch-24 = 0.004283653572201729

Training epoch-6 batch-25
Running loss of epoch-6 batch-25 = 0.0044137174263596535

Training epoch-6 batch-26
Running loss of epoch-6 batch-26 = 0.0047190929763019085

Training epoch-6 batch-27
Running loss of epoch-6 batch-27 = 0.004863368812948465

Training epoch-6 batch-28
Running loss of epoch-6 batch-28 = 0.008223330602049828

Training epoch-6 batch-29
Running loss of epoch-6 batch-29 = 0.00545174814760685

Training epoch-6 batch-30
Running loss of epoch-6 batch-30 = 0.005058294162154198

Training epoch-6 batch-31
Running loss of epoch-6 batch-31 = 0.007142180576920509

Training epoch-6 batch-32
Running loss of epoch-6 batch-32 = 0.005219553597271442

Training epoch-6 batch-33
Running loss of epoch-6 batch-33 = 0.004841248504817486

Training epoch-6 batch-34
Running loss of epoch-6 batch-34 = 0.003407735377550125

Training epoch-6 batch-35
Running loss of epoch-6 batch-35 = 0.004770437255501747

Training epoch-6 batch-36
Running loss of epoch-6 batch-36 = 0.0048390962183475494

Training epoch-6 batch-37
Running loss of epoch-6 batch-37 = 0.004712223540991545

Training epoch-6 batch-38
Running loss of epoch-6 batch-38 = 0.004234849940985441

Training epoch-6 batch-39
Running loss of epoch-6 batch-39 = 0.003471328876912594

Training epoch-6 batch-40
Running loss of epoch-6 batch-40 = 0.003095317166298628

Training epoch-6 batch-41
Running loss of epoch-6 batch-41 = 0.005666990764439106

Training epoch-6 batch-42
Running loss of epoch-6 batch-42 = 0.005369712132960558

Training epoch-6 batch-43
Running loss of epoch-6 batch-43 = 0.0029105478897690773

Training epoch-6 batch-44
Running loss of epoch-6 batch-44 = 0.0024373680353164673

Training epoch-6 batch-45
Running loss of epoch-6 batch-45 = 0.0043111275881528854

Training epoch-6 batch-46
Running loss of epoch-6 batch-46 = 0.0056391251273453236

Training epoch-6 batch-47
Running loss of epoch-6 batch-47 = 0.0034856796264648438

Training epoch-6 batch-48
Running loss of epoch-6 batch-48 = 0.005779017694294453

Training epoch-6 batch-49
Running loss of epoch-6 batch-49 = 0.004449470434337854

Training epoch-6 batch-50
Running loss of epoch-6 batch-50 = 0.004363922867923975

Training epoch-6 batch-51
Running loss of epoch-6 batch-51 = 0.006052570883184671

Training epoch-6 batch-52
Running loss of epoch-6 batch-52 = 0.004321370739489794

Training epoch-6 batch-53
Running loss of epoch-6 batch-53 = 0.002241286914795637

Training epoch-6 batch-54
Running loss of epoch-6 batch-54 = 0.00743867876008153

Training epoch-6 batch-55
Running loss of epoch-6 batch-55 = 0.0033510311041027308

Training epoch-6 batch-56
Running loss of epoch-6 batch-56 = 0.005324762314558029

Training epoch-6 batch-57
Running loss of epoch-6 batch-57 = 0.005987033247947693

Training epoch-6 batch-58
Running loss of epoch-6 batch-58 = 0.007322144694626331

Training epoch-6 batch-59
Running loss of epoch-6 batch-59 = 0.003091201651841402

Training epoch-6 batch-60
Running loss of epoch-6 batch-60 = 0.004659320693463087

Training epoch-6 batch-61
Running loss of epoch-6 batch-61 = 0.00598485954105854

Training epoch-6 batch-62
Running loss of epoch-6 batch-62 = 0.004001016728579998

Training epoch-6 batch-63
Running loss of epoch-6 batch-63 = 0.004359923303127289

Training epoch-6 batch-64
Running loss of epoch-6 batch-64 = 0.004149128682911396

Training epoch-6 batch-65
Running loss of epoch-6 batch-65 = 0.004606776405125856

Training epoch-6 batch-66
Running loss of epoch-6 batch-66 = 0.005889881867915392

Training epoch-6 batch-67
Running loss of epoch-6 batch-67 = 0.006008080206811428

Training epoch-6 batch-68
Running loss of epoch-6 batch-68 = 0.004410420078784227

Training epoch-6 batch-69
Running loss of epoch-6 batch-69 = 0.0026350708212703466

Training epoch-6 batch-70
Running loss of epoch-6 batch-70 = 0.004015136510133743

Training epoch-6 batch-71
Running loss of epoch-6 batch-71 = 0.003264580387622118

Training epoch-6 batch-72
Running loss of epoch-6 batch-72 = 0.0036182664334774017

Training epoch-6 batch-73
Running loss of epoch-6 batch-73 = 0.004298275802284479

Training epoch-6 batch-74
Running loss of epoch-6 batch-74 = 0.002914372133091092

Training epoch-6 batch-75
Running loss of epoch-6 batch-75 = 0.006844439078122377

Training epoch-6 batch-76
Running loss of epoch-6 batch-76 = 0.00552972499281168

Training epoch-6 batch-77
Running loss of epoch-6 batch-77 = 0.0055760848335921764

Training epoch-6 batch-78
Running loss of epoch-6 batch-78 = 0.004229417536407709

Training epoch-6 batch-79
Running loss of epoch-6 batch-79 = 0.003364535514265299

Training epoch-6 batch-80
Running loss of epoch-6 batch-80 = 0.004930262453854084

Training epoch-6 batch-81
Running loss of epoch-6 batch-81 = 0.003889731364324689

Training epoch-6 batch-82
Running loss of epoch-6 batch-82 = 0.004710289649665356

Training epoch-6 batch-83
Running loss of epoch-6 batch-83 = 0.003439825028181076

Training epoch-6 batch-84
Running loss of epoch-6 batch-84 = 0.0048339939676225185

Training epoch-6 batch-85
Running loss of epoch-6 batch-85 = 0.005846451967954636

Training epoch-6 batch-86
Running loss of epoch-6 batch-86 = 0.0034779557026922703

Training epoch-6 batch-87
Running loss of epoch-6 batch-87 = 0.005654982756823301

Training epoch-6 batch-88
Running loss of epoch-6 batch-88 = 0.004832901060581207

Training epoch-6 batch-89
Running loss of epoch-6 batch-89 = 0.007264227606356144

Training epoch-6 batch-90
Running loss of epoch-6 batch-90 = 0.00432817917317152

Training epoch-6 batch-91
Running loss of epoch-6 batch-91 = 0.006288580596446991

Training epoch-6 batch-92
Running loss of epoch-6 batch-92 = 0.00490668136626482

Training epoch-6 batch-93
Running loss of epoch-6 batch-93 = 0.005157792009413242

Training epoch-6 batch-94
Running loss of epoch-6 batch-94 = 0.00527701573446393

Training epoch-6 batch-95
Running loss of epoch-6 batch-95 = 0.004499137867242098

Training epoch-6 batch-96
Running loss of epoch-6 batch-96 = 0.008317061699926853

Training epoch-6 batch-97
Running loss of epoch-6 batch-97 = 0.004549506586045027

Training epoch-6 batch-98
Running loss of epoch-6 batch-98 = 0.0034294244833290577

Training epoch-6 batch-99
Running loss of epoch-6 batch-99 = 0.0057519604451954365

Training epoch-6 batch-100
Running loss of epoch-6 batch-100 = 0.00578490924090147

Training epoch-6 batch-101
Running loss of epoch-6 batch-101 = 0.004985759500414133

Training epoch-6 batch-102
Running loss of epoch-6 batch-102 = 0.004841592162847519

Training epoch-6 batch-103
Running loss of epoch-6 batch-103 = 0.002999140415340662

Training epoch-6 batch-104
Running loss of epoch-6 batch-104 = 0.003185899928212166

Training epoch-6 batch-105
Running loss of epoch-6 batch-105 = 0.004247291944921017

Training epoch-6 batch-106
Running loss of epoch-6 batch-106 = 0.0041025010868906975

Training epoch-6 batch-107
Running loss of epoch-6 batch-107 = 0.0036252611316740513

Training epoch-6 batch-108
Running loss of epoch-6 batch-108 = 0.0031819199211895466

Training epoch-6 batch-109
Running loss of epoch-6 batch-109 = 0.00273328460752964

Training epoch-6 batch-110
Running loss of epoch-6 batch-110 = 0.0022482597269117832

Training epoch-6 batch-111
Running loss of epoch-6 batch-111 = 0.006716427393257618

Training epoch-6 batch-112
Running loss of epoch-6 batch-112 = 0.006898876745253801

Training epoch-6 batch-113
Running loss of epoch-6 batch-113 = 0.005313485860824585

Training epoch-6 batch-114
Running loss of epoch-6 batch-114 = 0.0032197386026382446

Training epoch-6 batch-115
Running loss of epoch-6 batch-115 = 0.004659684374928474

Training epoch-6 batch-116
Running loss of epoch-6 batch-116 = 0.00464877113699913

Training epoch-6 batch-117
Running loss of epoch-6 batch-117 = 0.0029468638822436333

Training epoch-6 batch-118
Running loss of epoch-6 batch-118 = 0.0031000147573649883

Training epoch-6 batch-119
Running loss of epoch-6 batch-119 = 0.006999601610004902

Training epoch-6 batch-120
Running loss of epoch-6 batch-120 = 0.005324885714799166

Training epoch-6 batch-121
Running loss of epoch-6 batch-121 = 0.003600495867431164

Training epoch-6 batch-122
Running loss of epoch-6 batch-122 = 0.003945391625165939

Training epoch-6 batch-123
Running loss of epoch-6 batch-123 = 0.00332025159150362

Training epoch-6 batch-124
Running loss of epoch-6 batch-124 = 0.005411052145063877

Training epoch-6 batch-125
Running loss of epoch-6 batch-125 = 0.004563404247164726

Training epoch-6 batch-126
Running loss of epoch-6 batch-126 = 0.005362122319638729

Training epoch-6 batch-127
Running loss of epoch-6 batch-127 = 0.004476066213101149

Training epoch-6 batch-128
Running loss of epoch-6 batch-128 = 0.004129406064748764

Training epoch-6 batch-129
Running loss of epoch-6 batch-129 = 0.004302762448787689

Training epoch-6 batch-130
Running loss of epoch-6 batch-130 = 0.006642763502895832

Training epoch-6 batch-131
Running loss of epoch-6 batch-131 = 0.0052724918350577354

Training epoch-6 batch-132
Running loss of epoch-6 batch-132 = 0.003442560788244009

Training epoch-6 batch-133
Running loss of epoch-6 batch-133 = 0.006023143418133259

Training epoch-6 batch-134
Running loss of epoch-6 batch-134 = 0.004527635872364044

Training epoch-6 batch-135
Running loss of epoch-6 batch-135 = 0.003089191857725382

Training epoch-6 batch-136
Running loss of epoch-6 batch-136 = 0.0049964869394898415

Training epoch-6 batch-137
Running loss of epoch-6 batch-137 = 0.007913663983345032

Training epoch-6 batch-138
Running loss of epoch-6 batch-138 = 0.005181554239243269

Training epoch-6 batch-139
Running loss of epoch-6 batch-139 = 0.004986818879842758

Training epoch-6 batch-140
Running loss of epoch-6 batch-140 = 0.005335464607924223

Training epoch-6 batch-141
Running loss of epoch-6 batch-141 = 0.003817785531282425

Training epoch-6 batch-142
Running loss of epoch-6 batch-142 = 0.008226292207837105

Training epoch-6 batch-143
Running loss of epoch-6 batch-143 = 0.006259262561798096

Training epoch-6 batch-144
Running loss of epoch-6 batch-144 = 0.004927597939968109

Training epoch-6 batch-145
Running loss of epoch-6 batch-145 = 0.005930679850280285

Training epoch-6 batch-146
Running loss of epoch-6 batch-146 = 0.004824141040444374

Training epoch-6 batch-147
Running loss of epoch-6 batch-147 = 0.005678547080606222

Training epoch-6 batch-148
Running loss of epoch-6 batch-148 = 0.00986060407012701

Training epoch-6 batch-149
Running loss of epoch-6 batch-149 = 0.00589941069483757

Training epoch-6 batch-150
Running loss of epoch-6 batch-150 = 0.0039364006370306015

Training epoch-6 batch-151
Running loss of epoch-6 batch-151 = 0.001881895586848259

Training epoch-6 batch-152
Running loss of epoch-6 batch-152 = 0.00402167160063982

Training epoch-6 batch-153
Running loss of epoch-6 batch-153 = 0.00711845513433218

Training epoch-6 batch-154
Running loss of epoch-6 batch-154 = 0.0068638864904642105

Training epoch-6 batch-155
Running loss of epoch-6 batch-155 = 0.007111936807632446

Training epoch-6 batch-156
Running loss of epoch-6 batch-156 = 0.0036103264428675175

Training epoch-6 batch-157
Running loss of epoch-6 batch-157 = 0.021471237763762474

Finished training epoch-6.



Average train loss at epoch-6 = 0.004873824523389339

Started Evaluation

Average val loss at epoch-6 = 0.5095376167170654

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.87 %
Accuracy for class setUp is: 92.13 %
Accuracy for class onCreate is: 91.04 %
Accuracy for class toString is: 85.67 %
Accuracy for class run is: 59.59 %
Accuracy for class hashCode is: 99.25 %
Accuracy for class init is: 64.13 %
Accuracy for class execute is: 40.96 %
Accuracy for class get is: 75.13 %

Overall Accuracy = 83.62 %


Best Accuracy = 83.62 % at Epoch-6
Saving model after best epoch-6

Finished Evaluation



Started training epoch-7


Training epoch-7 batch-1
Running loss of epoch-7 batch-1 = 0.005349960643798113

Training epoch-7 batch-2
Running loss of epoch-7 batch-2 = 0.003311936743557453

Training epoch-7 batch-3
Running loss of epoch-7 batch-3 = 0.004236202221363783

Training epoch-7 batch-4
Running loss of epoch-7 batch-4 = 0.003049082588404417

Training epoch-7 batch-5
Running loss of epoch-7 batch-5 = 0.003380929119884968

Training epoch-7 batch-6
Running loss of epoch-7 batch-6 = 0.0042256759479641914

Training epoch-7 batch-7
Running loss of epoch-7 batch-7 = 0.005430491175502539

Training epoch-7 batch-8
Running loss of epoch-7 batch-8 = 0.004818110726773739

Training epoch-7 batch-9
Running loss of epoch-7 batch-9 = 0.0049074809066951275

Training epoch-7 batch-10
Running loss of epoch-7 batch-10 = 0.0037747006863355637

Training epoch-7 batch-11
Running loss of epoch-7 batch-11 = 0.0027996108401566744

Training epoch-7 batch-12
Running loss of epoch-7 batch-12 = 0.0025089627597481012

Training epoch-7 batch-13
Running loss of epoch-7 batch-13 = 0.005862851161509752

Training epoch-7 batch-14
Running loss of epoch-7 batch-14 = 0.004751141183078289

Training epoch-7 batch-15
Running loss of epoch-7 batch-15 = 0.0054086120799183846

Training epoch-7 batch-16
Running loss of epoch-7 batch-16 = 0.0013930662535130978

Training epoch-7 batch-17
Running loss of epoch-7 batch-17 = 0.0032726917415857315

Training epoch-7 batch-18
Running loss of epoch-7 batch-18 = 0.003685356117784977

Training epoch-7 batch-19
Running loss of epoch-7 batch-19 = 0.001689523458480835

Training epoch-7 batch-20
Running loss of epoch-7 batch-20 = 0.0034321381244808435

Training epoch-7 batch-21
Running loss of epoch-7 batch-21 = 0.004548436962068081

Training epoch-7 batch-22
Running loss of epoch-7 batch-22 = 0.004559306427836418

Training epoch-7 batch-23
Running loss of epoch-7 batch-23 = 0.002555358689278364

Training epoch-7 batch-24
Running loss of epoch-7 batch-24 = 0.004989949055016041

Training epoch-7 batch-25
Running loss of epoch-7 batch-25 = 0.002636510878801346

Training epoch-7 batch-26
Running loss of epoch-7 batch-26 = 0.003814949654042721

Training epoch-7 batch-27
Running loss of epoch-7 batch-27 = 0.0032246382907032967

Training epoch-7 batch-28
Running loss of epoch-7 batch-28 = 0.002399606164544821

Training epoch-7 batch-29
Running loss of epoch-7 batch-29 = 0.004629211034625769

Training epoch-7 batch-30
Running loss of epoch-7 batch-30 = 0.0034589250572025776

Training epoch-7 batch-31
Running loss of epoch-7 batch-31 = 0.0034134758170694113

Training epoch-7 batch-32
Running loss of epoch-7 batch-32 = 0.0031638387590646744

Training epoch-7 batch-33
Running loss of epoch-7 batch-33 = 0.005210595205426216

Training epoch-7 batch-34
Running loss of epoch-7 batch-34 = 0.004151341505348682

Training epoch-7 batch-35
Running loss of epoch-7 batch-35 = 0.0034483312629163265

Training epoch-7 batch-36
Running loss of epoch-7 batch-36 = 0.003555279690772295

Training epoch-7 batch-37
Running loss of epoch-7 batch-37 = 0.005893238354474306

Training epoch-7 batch-38
Running loss of epoch-7 batch-38 = 0.004223544150590897

Training epoch-7 batch-39
Running loss of epoch-7 batch-39 = 0.0037957890890538692

Training epoch-7 batch-40
Running loss of epoch-7 batch-40 = 0.0023579259868711233

Training epoch-7 batch-41
Running loss of epoch-7 batch-41 = 0.004107731394469738

Training epoch-7 batch-42
Running loss of epoch-7 batch-42 = 0.003668764140456915

Training epoch-7 batch-43
Running loss of epoch-7 batch-43 = 0.002276843646541238

Training epoch-7 batch-44
Running loss of epoch-7 batch-44 = 0.003174018347635865

Training epoch-7 batch-45
Running loss of epoch-7 batch-45 = 0.0028990760911256075

Training epoch-7 batch-46
Running loss of epoch-7 batch-46 = 0.002978346776217222

Training epoch-7 batch-47
Running loss of epoch-7 batch-47 = 0.002554061356931925

Training epoch-7 batch-48
Running loss of epoch-7 batch-48 = 0.0034597336780279875

Training epoch-7 batch-49
Running loss of epoch-7 batch-49 = 0.004036767408251762

Training epoch-7 batch-50
Running loss of epoch-7 batch-50 = 0.0038417838513851166

Training epoch-7 batch-51
Running loss of epoch-7 batch-51 = 0.003150281962007284

Training epoch-7 batch-52
Running loss of epoch-7 batch-52 = 0.0021567377261817455

Training epoch-7 batch-53
Running loss of epoch-7 batch-53 = 0.004441061057150364

Training epoch-7 batch-54
Running loss of epoch-7 batch-54 = 0.0033911671489477158

Training epoch-7 batch-55
Running loss of epoch-7 batch-55 = 0.0033190969843417406

Training epoch-7 batch-56
Running loss of epoch-7 batch-56 = 0.005405439995229244

Training epoch-7 batch-57
Running loss of epoch-7 batch-57 = 0.004581290762871504

Training epoch-7 batch-58
Running loss of epoch-7 batch-58 = 0.0033988526556640863

Training epoch-7 batch-59
Running loss of epoch-7 batch-59 = 0.004727490711957216

Training epoch-7 batch-60
Running loss of epoch-7 batch-60 = 0.004655060358345509

Training epoch-7 batch-61
Running loss of epoch-7 batch-61 = 0.003448231378570199

Training epoch-7 batch-62
Running loss of epoch-7 batch-62 = 0.0022825924679636955

Training epoch-7 batch-63
Running loss of epoch-7 batch-63 = 0.0018732529133558273

Training epoch-7 batch-64
Running loss of epoch-7 batch-64 = 0.004680583253502846

Training epoch-7 batch-65
Running loss of epoch-7 batch-65 = 0.0038070054724812508

Training epoch-7 batch-66
Running loss of epoch-7 batch-66 = 0.003886590711772442

Training epoch-7 batch-67
Running loss of epoch-7 batch-67 = 0.002842770889401436

Training epoch-7 batch-68
Running loss of epoch-7 batch-68 = 0.005775952711701393

Training epoch-7 batch-69
Running loss of epoch-7 batch-69 = 0.0028170854784548283

Training epoch-7 batch-70
Running loss of epoch-7 batch-70 = 0.0018693482270464301

Training epoch-7 batch-71
Running loss of epoch-7 batch-71 = 0.005981280468404293

Training epoch-7 batch-72
Running loss of epoch-7 batch-72 = 0.0034562963992357254

Training epoch-7 batch-73
Running loss of epoch-7 batch-73 = 0.0016788066131994128

Training epoch-7 batch-74
Running loss of epoch-7 batch-74 = 0.003958563786000013

Training epoch-7 batch-75
Running loss of epoch-7 batch-75 = 0.0027216365560889244

Training epoch-7 batch-76
Running loss of epoch-7 batch-76 = 0.0051642851904034615

Training epoch-7 batch-77
Running loss of epoch-7 batch-77 = 0.004590083844959736

Training epoch-7 batch-78
Running loss of epoch-7 batch-78 = 0.002754023065790534

Training epoch-7 batch-79
Running loss of epoch-7 batch-79 = 0.0025520981289446354

Training epoch-7 batch-80
Running loss of epoch-7 batch-80 = 0.003849784843623638

Training epoch-7 batch-81
Running loss of epoch-7 batch-81 = 0.004593970719724894

Training epoch-7 batch-82
Running loss of epoch-7 batch-82 = 0.003931524697691202

Training epoch-7 batch-83
Running loss of epoch-7 batch-83 = 0.0033699176274240017

Training epoch-7 batch-84
Running loss of epoch-7 batch-84 = 0.004429941065609455

Training epoch-7 batch-85
Running loss of epoch-7 batch-85 = 0.003792899427935481

Training epoch-7 batch-86
Running loss of epoch-7 batch-86 = 0.0031865467317402363

Training epoch-7 batch-87
Running loss of epoch-7 batch-87 = 0.00213004183024168

Training epoch-7 batch-88
Running loss of epoch-7 batch-88 = 0.00671774847432971

Training epoch-7 batch-89
Running loss of epoch-7 batch-89 = 0.0014184040483087301

Training epoch-7 batch-90
Running loss of epoch-7 batch-90 = 0.00654932064935565

Training epoch-7 batch-91
Running loss of epoch-7 batch-91 = 0.003905042540282011

Training epoch-7 batch-92
Running loss of epoch-7 batch-92 = 0.00321973767131567

Training epoch-7 batch-93
Running loss of epoch-7 batch-93 = 0.0034987791441380978

Training epoch-7 batch-94
Running loss of epoch-7 batch-94 = 0.005477914586663246

Training epoch-7 batch-95
Running loss of epoch-7 batch-95 = 0.0038055675104260445

Training epoch-7 batch-96
Running loss of epoch-7 batch-96 = 0.00499824108555913

Training epoch-7 batch-97
Running loss of epoch-7 batch-97 = 0.004417388699948788

Training epoch-7 batch-98
Running loss of epoch-7 batch-98 = 0.005456062965095043

Training epoch-7 batch-99
Running loss of epoch-7 batch-99 = 0.0033906670287251472

Training epoch-7 batch-100
Running loss of epoch-7 batch-100 = 0.004574735648930073

Training epoch-7 batch-101
Running loss of epoch-7 batch-101 = 0.0027166332583874464

Training epoch-7 batch-102
Running loss of epoch-7 batch-102 = 0.005183948203921318

Training epoch-7 batch-103
Running loss of epoch-7 batch-103 = 0.00427800789475441

Training epoch-7 batch-104
Running loss of epoch-7 batch-104 = 0.0037901359610259533

Training epoch-7 batch-105
Running loss of epoch-7 batch-105 = 0.0037781440187245607

Training epoch-7 batch-106
Running loss of epoch-7 batch-106 = 0.003961803857237101

Training epoch-7 batch-107
Running loss of epoch-7 batch-107 = 0.0032509276643395424

Training epoch-7 batch-108
Running loss of epoch-7 batch-108 = 0.004652164876461029

Training epoch-7 batch-109
Running loss of epoch-7 batch-109 = 0.0024586294312030077

Training epoch-7 batch-110
Running loss of epoch-7 batch-110 = 0.004487353842705488

Training epoch-7 batch-111
Running loss of epoch-7 batch-111 = 0.003279913915321231

Training epoch-7 batch-112
Running loss of epoch-7 batch-112 = 0.004072400741279125

Training epoch-7 batch-113
Running loss of epoch-7 batch-113 = 0.004026071168482304

Training epoch-7 batch-114
Running loss of epoch-7 batch-114 = 0.0028881090693175793

Training epoch-7 batch-115
Running loss of epoch-7 batch-115 = 0.004850410856306553

Training epoch-7 batch-116
Running loss of epoch-7 batch-116 = 0.002197540830820799

Training epoch-7 batch-117
Running loss of epoch-7 batch-117 = 0.006575499661266804

Training epoch-7 batch-118
Running loss of epoch-7 batch-118 = 0.0043741012923419476

Training epoch-7 batch-119
Running loss of epoch-7 batch-119 = 0.005847749300301075

Training epoch-7 batch-120
Running loss of epoch-7 batch-120 = 0.0034535217564553022

Training epoch-7 batch-121
Running loss of epoch-7 batch-121 = 0.0037825494073331356

Training epoch-7 batch-122
Running loss of epoch-7 batch-122 = 0.002802368486300111

Training epoch-7 batch-123
Running loss of epoch-7 batch-123 = 0.003549247747287154

Training epoch-7 batch-124
Running loss of epoch-7 batch-124 = 0.0030315101612359285

Training epoch-7 batch-125
Running loss of epoch-7 batch-125 = 0.004246287979185581

Training epoch-7 batch-126
Running loss of epoch-7 batch-126 = 0.004379285033792257

Training epoch-7 batch-127
Running loss of epoch-7 batch-127 = 0.0013481987407431006

Training epoch-7 batch-128
Running loss of epoch-7 batch-128 = 0.004431494046002626

Training epoch-7 batch-129
Running loss of epoch-7 batch-129 = 0.0025473828427493572

Training epoch-7 batch-130
Running loss of epoch-7 batch-130 = 0.005234995856881142

Training epoch-7 batch-131
Running loss of epoch-7 batch-131 = 0.004087129607796669

Training epoch-7 batch-132
Running loss of epoch-7 batch-132 = 0.0035620806738734245

Training epoch-7 batch-133
Running loss of epoch-7 batch-133 = 0.0025739469565451145

Training epoch-7 batch-134
Running loss of epoch-7 batch-134 = 0.0029966256115585566

Training epoch-7 batch-135
Running loss of epoch-7 batch-135 = 0.004075247328728437

Training epoch-7 batch-136
Running loss of epoch-7 batch-136 = 0.0027289637364447117

Training epoch-7 batch-137
Running loss of epoch-7 batch-137 = 0.0034998173359781504

Training epoch-7 batch-138
Running loss of epoch-7 batch-138 = 0.0027171922847628593

Training epoch-7 batch-139
Running loss of epoch-7 batch-139 = 0.0033609953243285418

Training epoch-7 batch-140
Running loss of epoch-7 batch-140 = 0.0023142783902585506

Training epoch-7 batch-141
Running loss of epoch-7 batch-141 = 0.00577194057404995

Training epoch-7 batch-142
Running loss of epoch-7 batch-142 = 0.004300034139305353

Training epoch-7 batch-143
Running loss of epoch-7 batch-143 = 0.0034214919432997704

Training epoch-7 batch-144
Running loss of epoch-7 batch-144 = 0.0017638035351410508

Training epoch-7 batch-145
Running loss of epoch-7 batch-145 = 0.0038015455938875675

Training epoch-7 batch-146
Running loss of epoch-7 batch-146 = 0.003244337160140276

Training epoch-7 batch-147
Running loss of epoch-7 batch-147 = 0.004309661686420441

Training epoch-7 batch-148
Running loss of epoch-7 batch-148 = 0.004093145485967398

Training epoch-7 batch-149
Running loss of epoch-7 batch-149 = 0.0041125062853097916

Training epoch-7 batch-150
Running loss of epoch-7 batch-150 = 0.004000310320407152

Training epoch-7 batch-151
Running loss of epoch-7 batch-151 = 0.00375507608987391

Training epoch-7 batch-152
Running loss of epoch-7 batch-152 = 0.005446244962513447

Training epoch-7 batch-153
Running loss of epoch-7 batch-153 = 0.004937984514981508

Training epoch-7 batch-154
Running loss of epoch-7 batch-154 = 0.0030486828181892633

Training epoch-7 batch-155
Running loss of epoch-7 batch-155 = 0.0028068898245692253

Training epoch-7 batch-156
Running loss of epoch-7 batch-156 = 0.005010437685996294

Training epoch-7 batch-157
Running loss of epoch-7 batch-157 = 0.017056843265891075

Finished training epoch-7.



Average train loss at epoch-7 = 0.0038008196845650674

Started Evaluation

Average val loss at epoch-7 = 0.5256351902866491

Accuracy for classes:
Accuracy for class equals is: 97.03 %
Accuracy for class main is: 97.21 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 92.11 %
Accuracy for class toString is: 86.35 %
Accuracy for class run is: 71.69 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 58.30 %
Accuracy for class execute is: 56.22 %
Accuracy for class get is: 57.69 %

Overall Accuracy = 83.68 %


Best Accuracy = 83.68 % at Epoch-7
Saving model after best epoch-7

Finished Evaluation



Started training epoch-8


Training epoch-8 batch-1
Running loss of epoch-8 batch-1 = 0.002382262609899044

Training epoch-8 batch-2
Running loss of epoch-8 batch-2 = 0.004132059868425131

Training epoch-8 batch-3
Running loss of epoch-8 batch-3 = 0.0027767254505306482

Training epoch-8 batch-4
Running loss of epoch-8 batch-4 = 0.004759186878800392

Training epoch-8 batch-5
Running loss of epoch-8 batch-5 = 0.0026021143421530724

Training epoch-8 batch-6
Running loss of epoch-8 batch-6 = 0.0024806237779557705

Training epoch-8 batch-7
Running loss of epoch-8 batch-7 = 0.0029355413280427456

Training epoch-8 batch-8
Running loss of epoch-8 batch-8 = 0.002167103346437216

Training epoch-8 batch-9
Running loss of epoch-8 batch-9 = 0.003892982378602028

Training epoch-8 batch-10
Running loss of epoch-8 batch-10 = 0.0031014448031783104

Training epoch-8 batch-11
Running loss of epoch-8 batch-11 = 0.0037636226043105125

Training epoch-8 batch-12
Running loss of epoch-8 batch-12 = 0.002541733905673027

Training epoch-8 batch-13
Running loss of epoch-8 batch-13 = 0.0020667812786996365

Training epoch-8 batch-14
Running loss of epoch-8 batch-14 = 0.003653021063655615

Training epoch-8 batch-15
Running loss of epoch-8 batch-15 = 0.0023342096246778965

Training epoch-8 batch-16
Running loss of epoch-8 batch-16 = 0.002474105916917324

Training epoch-8 batch-17
Running loss of epoch-8 batch-17 = 0.003710151184350252

Training epoch-8 batch-18
Running loss of epoch-8 batch-18 = 0.002381999045610428

Training epoch-8 batch-19
Running loss of epoch-8 batch-19 = 0.001828746753744781

Training epoch-8 batch-20
Running loss of epoch-8 batch-20 = 0.001629458274692297

Training epoch-8 batch-21
Running loss of epoch-8 batch-21 = 0.0031562086660414934

Training epoch-8 batch-22
Running loss of epoch-8 batch-22 = 0.0029506601858884096

Training epoch-8 batch-23
Running loss of epoch-8 batch-23 = 0.0044729337096214294

Training epoch-8 batch-24
Running loss of epoch-8 batch-24 = 0.0023235762491822243

Training epoch-8 batch-25
Running loss of epoch-8 batch-25 = 0.002918173559010029

Training epoch-8 batch-26
Running loss of epoch-8 batch-26 = 0.0016845978097990155

Training epoch-8 batch-27
Running loss of epoch-8 batch-27 = 0.0032028083223849535

Training epoch-8 batch-28
Running loss of epoch-8 batch-28 = 0.002577331382781267

Training epoch-8 batch-29
Running loss of epoch-8 batch-29 = 0.0024001861456781626

Training epoch-8 batch-30
Running loss of epoch-8 batch-30 = 0.002116513904184103

Training epoch-8 batch-31
Running loss of epoch-8 batch-31 = 0.0026253978721797466

Training epoch-8 batch-32
Running loss of epoch-8 batch-32 = 0.004093671217560768

Training epoch-8 batch-33
Running loss of epoch-8 batch-33 = 0.003989312797784805

Training epoch-8 batch-34
Running loss of epoch-8 batch-34 = 0.0017083882121369243

Training epoch-8 batch-35
Running loss of epoch-8 batch-35 = 0.00406948896124959

Training epoch-8 batch-36
Running loss of epoch-8 batch-36 = 0.0021067061461508274

Training epoch-8 batch-37
Running loss of epoch-8 batch-37 = 0.00201589148491621

Training epoch-8 batch-38
Running loss of epoch-8 batch-38 = 0.0022942291107028723

Training epoch-8 batch-39
Running loss of epoch-8 batch-39 = 0.005511336959898472

Training epoch-8 batch-40
Running loss of epoch-8 batch-40 = 0.0033146291971206665

Training epoch-8 batch-41
Running loss of epoch-8 batch-41 = 0.0022528942208737135

Training epoch-8 batch-42
Running loss of epoch-8 batch-42 = 0.0023688001092523336

Training epoch-8 batch-43
Running loss of epoch-8 batch-43 = 0.0038366105873137712

Training epoch-8 batch-44
Running loss of epoch-8 batch-44 = 0.0019226789008826017

Training epoch-8 batch-45
Running loss of epoch-8 batch-45 = 0.002303842455148697

Training epoch-8 batch-46
Running loss of epoch-8 batch-46 = 0.0020840088836848736

Training epoch-8 batch-47
Running loss of epoch-8 batch-47 = 0.0023661134764552116

Training epoch-8 batch-48
Running loss of epoch-8 batch-48 = 0.0024032348301261663

Training epoch-8 batch-49
Running loss of epoch-8 batch-49 = 0.0034478751476854086

Training epoch-8 batch-50
Running loss of epoch-8 batch-50 = 0.001961843576282263

Training epoch-8 batch-51
Running loss of epoch-8 batch-51 = 0.0018760159146040678

Training epoch-8 batch-52
Running loss of epoch-8 batch-52 = 0.002450781874358654

Training epoch-8 batch-53
Running loss of epoch-8 batch-53 = 0.0025520066265016794

Training epoch-8 batch-54
Running loss of epoch-8 batch-54 = 0.0021127609070390463

Training epoch-8 batch-55
Running loss of epoch-8 batch-55 = 0.0020552012138068676

Training epoch-8 batch-56
Running loss of epoch-8 batch-56 = 0.005410731304436922

Training epoch-8 batch-57
Running loss of epoch-8 batch-57 = 0.0018241514917463064

Training epoch-8 batch-58
Running loss of epoch-8 batch-58 = 0.0029472524765878916

Training epoch-8 batch-59
Running loss of epoch-8 batch-59 = 0.0036848809104412794

Training epoch-8 batch-60
Running loss of epoch-8 batch-60 = 0.003790462389588356

Training epoch-8 batch-61
Running loss of epoch-8 batch-61 = 0.004235372878611088

Training epoch-8 batch-62
Running loss of epoch-8 batch-62 = 0.004430969245731831

Training epoch-8 batch-63
Running loss of epoch-8 batch-63 = 0.002199052833020687

Training epoch-8 batch-64
Running loss of epoch-8 batch-64 = 0.002277175895869732

Training epoch-8 batch-65
Running loss of epoch-8 batch-65 = 0.0025539076887071133

Training epoch-8 batch-66
Running loss of epoch-8 batch-66 = 0.003896160749718547

Training epoch-8 batch-67
Running loss of epoch-8 batch-67 = 0.0033460017293691635

Training epoch-8 batch-68
Running loss of epoch-8 batch-68 = 0.0019919653423130512

Training epoch-8 batch-69
Running loss of epoch-8 batch-69 = 0.004892065189778805

Training epoch-8 batch-70
Running loss of epoch-8 batch-70 = 0.004770494066178799

Training epoch-8 batch-71
Running loss of epoch-8 batch-71 = 0.002355985576286912

Training epoch-8 batch-72
Running loss of epoch-8 batch-72 = 0.0019588982686400414

Training epoch-8 batch-73
Running loss of epoch-8 batch-73 = 0.00609950115904212

Training epoch-8 batch-74
Running loss of epoch-8 batch-74 = 0.0016834248090162873

Training epoch-8 batch-75
Running loss of epoch-8 batch-75 = 0.0022839256562292576

Training epoch-8 batch-76
Running loss of epoch-8 batch-76 = 0.0021180305629968643

Training epoch-8 batch-77
Running loss of epoch-8 batch-77 = 0.002765763783827424

Training epoch-8 batch-78
Running loss of epoch-8 batch-78 = 0.001604415476322174

Training epoch-8 batch-79
Running loss of epoch-8 batch-79 = 0.0029398491606116295

Training epoch-8 batch-80
Running loss of epoch-8 batch-80 = 0.0036853128112852573

Training epoch-8 batch-81
Running loss of epoch-8 batch-81 = 0.002991683781147003

Training epoch-8 batch-82
Running loss of epoch-8 batch-82 = 0.00214101723395288

Training epoch-8 batch-83
Running loss of epoch-8 batch-83 = 0.0035992241464555264

Training epoch-8 batch-84
Running loss of epoch-8 batch-84 = 0.0033406761940568686

Training epoch-8 batch-85
Running loss of epoch-8 batch-85 = 0.003738959087058902

Training epoch-8 batch-86
Running loss of epoch-8 batch-86 = 0.0033627774100750685

Training epoch-8 batch-87
Running loss of epoch-8 batch-87 = 0.0015981763135641813

Training epoch-8 batch-88
Running loss of epoch-8 batch-88 = 0.0036449299659579992

Training epoch-8 batch-89
Running loss of epoch-8 batch-89 = 0.002201144816353917

Training epoch-8 batch-90
Running loss of epoch-8 batch-90 = 0.0030464385636150837

Training epoch-8 batch-91
Running loss of epoch-8 batch-91 = 0.002465085592120886

Training epoch-8 batch-92
Running loss of epoch-8 batch-92 = 0.003386596217751503

Training epoch-8 batch-93
Running loss of epoch-8 batch-93 = 0.0020967847667634487

Training epoch-8 batch-94
Running loss of epoch-8 batch-94 = 0.001671202015131712

Training epoch-8 batch-95
Running loss of epoch-8 batch-95 = 0.002777287270873785

Training epoch-8 batch-96
Running loss of epoch-8 batch-96 = 0.0024809548631310463

Training epoch-8 batch-97
Running loss of epoch-8 batch-97 = 0.0021604616194963455

Training epoch-8 batch-98
Running loss of epoch-8 batch-98 = 0.00439502764493227

Training epoch-8 batch-99
Running loss of epoch-8 batch-99 = 0.001619138172827661

Training epoch-8 batch-100
Running loss of epoch-8 batch-100 = 0.002018322702497244

Training epoch-8 batch-101
Running loss of epoch-8 batch-101 = 0.0030026016756892204

Training epoch-8 batch-102
Running loss of epoch-8 batch-102 = 0.002316445577889681

Training epoch-8 batch-103
Running loss of epoch-8 batch-103 = 0.003186759538948536

Training epoch-8 batch-104
Running loss of epoch-8 batch-104 = 0.003266611136496067

Training epoch-8 batch-105
Running loss of epoch-8 batch-105 = 0.0021005813032388687

Training epoch-8 batch-106
Running loss of epoch-8 batch-106 = 0.0029390091076493263

Training epoch-8 batch-107
Running loss of epoch-8 batch-107 = 0.003465986344963312

Training epoch-8 batch-108
Running loss of epoch-8 batch-108 = 0.0018490601796656847

Training epoch-8 batch-109
Running loss of epoch-8 batch-109 = 0.0024215257726609707

Training epoch-8 batch-110
Running loss of epoch-8 batch-110 = 0.004178350791335106

Training epoch-8 batch-111
Running loss of epoch-8 batch-111 = 0.0030183412600308657

Training epoch-8 batch-112
Running loss of epoch-8 batch-112 = 0.0015123370103538036

Training epoch-8 batch-113
Running loss of epoch-8 batch-113 = 0.0010196503717452288

Training epoch-8 batch-114
Running loss of epoch-8 batch-114 = 0.0026321406476199627

Training epoch-8 batch-115
Running loss of epoch-8 batch-115 = 0.003615389112383127

Training epoch-8 batch-116
Running loss of epoch-8 batch-116 = 0.004477345384657383

Training epoch-8 batch-117
Running loss of epoch-8 batch-117 = 0.003011066932231188

Training epoch-8 batch-118
Running loss of epoch-8 batch-118 = 0.003031487576663494

Training epoch-8 batch-119
Running loss of epoch-8 batch-119 = 0.0038277343846857548

Training epoch-8 batch-120
Running loss of epoch-8 batch-120 = 0.005096925422549248

Training epoch-8 batch-121
Running loss of epoch-8 batch-121 = 0.004475724883377552

Training epoch-8 batch-122
Running loss of epoch-8 batch-122 = 0.0027816402725875378

Training epoch-8 batch-123
Running loss of epoch-8 batch-123 = 0.00473420787602663

Training epoch-8 batch-124
Running loss of epoch-8 batch-124 = 0.0016905872616916895

Training epoch-8 batch-125
Running loss of epoch-8 batch-125 = 0.004710402339696884

Training epoch-8 batch-126
Running loss of epoch-8 batch-126 = 0.00472835311666131

Training epoch-8 batch-127
Running loss of epoch-8 batch-127 = 0.0031454875133931637

Training epoch-8 batch-128
Running loss of epoch-8 batch-128 = 0.0021781576797366142

Training epoch-8 batch-129
Running loss of epoch-8 batch-129 = 0.0023988860193639994

Training epoch-8 batch-130
Running loss of epoch-8 batch-130 = 0.002591937780380249

Training epoch-8 batch-131
Running loss of epoch-8 batch-131 = 0.003346706973388791

Training epoch-8 batch-132
Running loss of epoch-8 batch-132 = 0.004295486491173506

Training epoch-8 batch-133
Running loss of epoch-8 batch-133 = 0.00210944889113307

Training epoch-8 batch-134
Running loss of epoch-8 batch-134 = 0.0024855034425854683

Training epoch-8 batch-135
Running loss of epoch-8 batch-135 = 0.004382277838885784

Training epoch-8 batch-136
Running loss of epoch-8 batch-136 = 0.002256931969895959

Training epoch-8 batch-137
Running loss of epoch-8 batch-137 = 0.004080870188772678

Training epoch-8 batch-138
Running loss of epoch-8 batch-138 = 0.002629895694553852

Training epoch-8 batch-139
Running loss of epoch-8 batch-139 = 0.002337485319003463

Training epoch-8 batch-140
Running loss of epoch-8 batch-140 = 0.005170251242816448

Training epoch-8 batch-141
Running loss of epoch-8 batch-141 = 0.005866908002644777

Training epoch-8 batch-142
Running loss of epoch-8 batch-142 = 0.002409174805507064

Training epoch-8 batch-143
Running loss of epoch-8 batch-143 = 0.002806534292176366

Training epoch-8 batch-144
Running loss of epoch-8 batch-144 = 0.001806330867111683

Training epoch-8 batch-145
Running loss of epoch-8 batch-145 = 0.003190059447661042

Training epoch-8 batch-146
Running loss of epoch-8 batch-146 = 0.002708595944568515

Training epoch-8 batch-147
Running loss of epoch-8 batch-147 = 0.006896928884088993

Training epoch-8 batch-148
Running loss of epoch-8 batch-148 = 0.0016851426335051656

Training epoch-8 batch-149
Running loss of epoch-8 batch-149 = 0.003333739936351776

Training epoch-8 batch-150
Running loss of epoch-8 batch-150 = 0.001810054061934352

Training epoch-8 batch-151
Running loss of epoch-8 batch-151 = 0.0026082531549036503

Training epoch-8 batch-152
Running loss of epoch-8 batch-152 = 0.004451916553080082

Training epoch-8 batch-153
Running loss of epoch-8 batch-153 = 0.004393337294459343

Training epoch-8 batch-154
Running loss of epoch-8 batch-154 = 0.003510228591039777

Training epoch-8 batch-155
Running loss of epoch-8 batch-155 = 0.003636391833424568

Training epoch-8 batch-156
Running loss of epoch-8 batch-156 = 0.0029571079649031162

Training epoch-8 batch-157
Running loss of epoch-8 batch-157 = 0.003047298640012741

Finished training epoch-8.



Average train loss at epoch-8 = 0.0030013078153133393

Started Evaluation

Average val loss at epoch-8 = 0.5846857046310202

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 94.59 %
Accuracy for class setUp is: 92.13 %
Accuracy for class onCreate is: 86.78 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 56.85 %
Accuracy for class hashCode is: 99.25 %
Accuracy for class init is: 71.08 %
Accuracy for class execute is: 26.51 %
Accuracy for class get is: 76.15 %

Overall Accuracy = 82.30 %

Finished Evaluation



Started training epoch-9


Training epoch-9 batch-1
Running loss of epoch-9 batch-1 = 0.0019052455900236964

Training epoch-9 batch-2
Running loss of epoch-9 batch-2 = 0.0027341688983142376

Training epoch-9 batch-3
Running loss of epoch-9 batch-3 = 0.00316444830968976

Training epoch-9 batch-4
Running loss of epoch-9 batch-4 = 0.0025675399228930473

Training epoch-9 batch-5
Running loss of epoch-9 batch-5 = 0.0017178364796563983

Training epoch-9 batch-6
Running loss of epoch-9 batch-6 = 0.001257793395780027

Training epoch-9 batch-7
Running loss of epoch-9 batch-7 = 0.0020862433593720198

Training epoch-9 batch-8
Running loss of epoch-9 batch-8 = 0.0018243803642690182

Training epoch-9 batch-9
Running loss of epoch-9 batch-9 = 0.0013668732717633247

Training epoch-9 batch-10
Running loss of epoch-9 batch-10 = 0.0023225112818181515

Training epoch-9 batch-11
Running loss of epoch-9 batch-11 = 0.0014110803604125977

Training epoch-9 batch-12
Running loss of epoch-9 batch-12 = 0.0024212312418967485

Training epoch-9 batch-13
Running loss of epoch-9 batch-13 = 0.002558597829192877

Training epoch-9 batch-14
Running loss of epoch-9 batch-14 = 0.0009078684961423278

Training epoch-9 batch-15
Running loss of epoch-9 batch-15 = 0.0012208983534947038

Training epoch-9 batch-16
Running loss of epoch-9 batch-16 = 0.001810119953006506

Training epoch-9 batch-17
Running loss of epoch-9 batch-17 = 0.0032805735245347023

Training epoch-9 batch-18
Running loss of epoch-9 batch-18 = 0.002180467825382948

Training epoch-9 batch-19
Running loss of epoch-9 batch-19 = 0.003185834502801299

Training epoch-9 batch-20
Running loss of epoch-9 batch-20 = 0.0020682220347225666

Training epoch-9 batch-21
Running loss of epoch-9 batch-21 = 0.0009537700098007917

Training epoch-9 batch-22
Running loss of epoch-9 batch-22 = 0.002495851833373308

Training epoch-9 batch-23
Running loss of epoch-9 batch-23 = 0.0013733175583183765

Training epoch-9 batch-24
Running loss of epoch-9 batch-24 = 0.0018148787785321474

Training epoch-9 batch-25
Running loss of epoch-9 batch-25 = 0.002189305145293474

Training epoch-9 batch-26
Running loss of epoch-9 batch-26 = 0.0012515783309936523

Training epoch-9 batch-27
Running loss of epoch-9 batch-27 = 0.002463941927999258

Training epoch-9 batch-28
Running loss of epoch-9 batch-28 = 0.0014573618536815047

Training epoch-9 batch-29
Running loss of epoch-9 batch-29 = 0.0014402454253286123

Training epoch-9 batch-30
Running loss of epoch-9 batch-30 = 0.0021034451201558113

Training epoch-9 batch-31
Running loss of epoch-9 batch-31 = 0.001361228059977293

Training epoch-9 batch-32
Running loss of epoch-9 batch-32 = 0.002192780841141939

Training epoch-9 batch-33
Running loss of epoch-9 batch-33 = 0.0020665025804191828

Training epoch-9 batch-34
Running loss of epoch-9 batch-34 = 0.0019886044319719076

Training epoch-9 batch-35
Running loss of epoch-9 batch-35 = 0.0016074342420324683

Training epoch-9 batch-36
Running loss of epoch-9 batch-36 = 0.0027888892218470573

Training epoch-9 batch-37
Running loss of epoch-9 batch-37 = 0.002279277192428708

Training epoch-9 batch-38
Running loss of epoch-9 batch-38 = 0.0011959996772930026

Training epoch-9 batch-39
Running loss of epoch-9 batch-39 = 0.0028161779046058655

Training epoch-9 batch-40
Running loss of epoch-9 batch-40 = 0.002251876052469015

Training epoch-9 batch-41
Running loss of epoch-9 batch-41 = 0.0015040466096252203

Training epoch-9 batch-42
Running loss of epoch-9 batch-42 = 0.001087264739908278

Training epoch-9 batch-43
Running loss of epoch-9 batch-43 = 0.0033161425963044167

Training epoch-9 batch-44
Running loss of epoch-9 batch-44 = 0.0009266496053896844

Training epoch-9 batch-45
Running loss of epoch-9 batch-45 = 0.002828855300322175

Training epoch-9 batch-46
Running loss of epoch-9 batch-46 = 0.002708249492570758

Training epoch-9 batch-47
Running loss of epoch-9 batch-47 = 0.0012403130531311035

Training epoch-9 batch-48
Running loss of epoch-9 batch-48 = 0.0022183633409440517

Training epoch-9 batch-49
Running loss of epoch-9 batch-49 = 0.0019188907463103533

Training epoch-9 batch-50
Running loss of epoch-9 batch-50 = 0.0015486411284655333

Training epoch-9 batch-51
Running loss of epoch-9 batch-51 = 0.0017263107001781464

Training epoch-9 batch-52
Running loss of epoch-9 batch-52 = 0.0032885405234992504

Training epoch-9 batch-53
Running loss of epoch-9 batch-53 = 0.001586239319294691

Training epoch-9 batch-54
Running loss of epoch-9 batch-54 = 0.003219589591026306

Training epoch-9 batch-55
Running loss of epoch-9 batch-55 = 0.0025042505003511906

Training epoch-9 batch-56
Running loss of epoch-9 batch-56 = 0.0016705680172890425

Training epoch-9 batch-57
Running loss of epoch-9 batch-57 = 0.0026614342350512743

Training epoch-9 batch-58
Running loss of epoch-9 batch-58 = 0.000908822228666395

Training epoch-9 batch-59
Running loss of epoch-9 batch-59 = 0.001960530411452055

Training epoch-9 batch-60
Running loss of epoch-9 batch-60 = 0.0035531255416572094

Training epoch-9 batch-61
Running loss of epoch-9 batch-61 = 0.002872813493013382

Training epoch-9 batch-62
Running loss of epoch-9 batch-62 = 0.0010214203502982855

Training epoch-9 batch-63
Running loss of epoch-9 batch-63 = 0.0017695120768621564

Training epoch-9 batch-64
Running loss of epoch-9 batch-64 = 0.0033958149142563343

Training epoch-9 batch-65
Running loss of epoch-9 batch-65 = 0.005146821495145559

Training epoch-9 batch-66
Running loss of epoch-9 batch-66 = 0.002376293996348977

Training epoch-9 batch-67
Running loss of epoch-9 batch-67 = 0.003050554543733597

Training epoch-9 batch-68
Running loss of epoch-9 batch-68 = 0.0020897150970995426

Training epoch-9 batch-69
Running loss of epoch-9 batch-69 = 0.003519152756780386

Training epoch-9 batch-70
Running loss of epoch-9 batch-70 = 0.002973285038024187

Training epoch-9 batch-71
Running loss of epoch-9 batch-71 = 0.0036589535884559155

Training epoch-9 batch-72
Running loss of epoch-9 batch-72 = 0.001496434211730957

Training epoch-9 batch-73
Running loss of epoch-9 batch-73 = 0.0026183095760643482

Training epoch-9 batch-74
Running loss of epoch-9 batch-74 = 0.003763218643143773

Training epoch-9 batch-75
Running loss of epoch-9 batch-75 = 0.00257112761028111

Training epoch-9 batch-76
Running loss of epoch-9 batch-76 = 0.002480492927134037

Training epoch-9 batch-77
Running loss of epoch-9 batch-77 = 0.0008947442402131855

Training epoch-9 batch-78
Running loss of epoch-9 batch-78 = 0.0030481519643217325

Training epoch-9 batch-79
Running loss of epoch-9 batch-79 = 0.0012429546331986785

Training epoch-9 batch-80
Running loss of epoch-9 batch-80 = 0.004001290537416935

Training epoch-9 batch-81
Running loss of epoch-9 batch-81 = 0.0010183724807575345

Training epoch-9 batch-82
Running loss of epoch-9 batch-82 = 0.0020515944343060255

Training epoch-9 batch-83
Running loss of epoch-9 batch-83 = 0.0016690291231498122

Training epoch-9 batch-84
Running loss of epoch-9 batch-84 = 0.0022045013029128313

Training epoch-9 batch-85
Running loss of epoch-9 batch-85 = 0.0027360301464796066

Training epoch-9 batch-86
Running loss of epoch-9 batch-86 = 0.001657650456763804

Training epoch-9 batch-87
Running loss of epoch-9 batch-87 = 0.0028412691317498684

Training epoch-9 batch-88
Running loss of epoch-9 batch-88 = 0.002228165976703167

Training epoch-9 batch-89
Running loss of epoch-9 batch-89 = 0.002417849376797676

Training epoch-9 batch-90
Running loss of epoch-9 batch-90 = 0.0026491875760257244

Training epoch-9 batch-91
Running loss of epoch-9 batch-91 = 0.0024376879446208477

Training epoch-9 batch-92
Running loss of epoch-9 batch-92 = 0.0023395426105707884

Training epoch-9 batch-93
Running loss of epoch-9 batch-93 = 0.0013837205478921533

Training epoch-9 batch-94
Running loss of epoch-9 batch-94 = 0.0008868012810125947

Training epoch-9 batch-95
Running loss of epoch-9 batch-95 = 0.00207693362608552

Training epoch-9 batch-96
Running loss of epoch-9 batch-96 = 0.003820332931354642

Training epoch-9 batch-97
Running loss of epoch-9 batch-97 = 0.0020274589769542217

Training epoch-9 batch-98
Running loss of epoch-9 batch-98 = 0.0035042092204093933

Training epoch-9 batch-99
Running loss of epoch-9 batch-99 = 0.0033536129631102085

Training epoch-9 batch-100
Running loss of epoch-9 batch-100 = 0.003024541772902012

Training epoch-9 batch-101
Running loss of epoch-9 batch-101 = 0.0016707857139408588

Training epoch-9 batch-102
Running loss of epoch-9 batch-102 = 0.001633455976843834

Training epoch-9 batch-103
Running loss of epoch-9 batch-103 = 0.0014575815293937922

Training epoch-9 batch-104
Running loss of epoch-9 batch-104 = 0.0025384591426700354

Training epoch-9 batch-105
Running loss of epoch-9 batch-105 = 0.001731189782731235

Training epoch-9 batch-106
Running loss of epoch-9 batch-106 = 0.002563806949183345

Training epoch-9 batch-107
Running loss of epoch-9 batch-107 = 0.0014589192578569055

Training epoch-9 batch-108
Running loss of epoch-9 batch-108 = 0.002179037779569626

Training epoch-9 batch-109
Running loss of epoch-9 batch-109 = 0.0023245257325470448

Training epoch-9 batch-110
Running loss of epoch-9 batch-110 = 0.00215247948653996

Training epoch-9 batch-111
Running loss of epoch-9 batch-111 = 0.0031530391424894333

Training epoch-9 batch-112
Running loss of epoch-9 batch-112 = 0.0008009446319192648

Training epoch-9 batch-113
Running loss of epoch-9 batch-113 = 0.0018655970925465226

Training epoch-9 batch-114
Running loss of epoch-9 batch-114 = 0.001222108956426382

Training epoch-9 batch-115
Running loss of epoch-9 batch-115 = 0.002245125127956271

Training epoch-9 batch-116
Running loss of epoch-9 batch-116 = 0.0011159020941704512

Training epoch-9 batch-117
Running loss of epoch-9 batch-117 = 0.0012629423290491104

Training epoch-9 batch-118
Running loss of epoch-9 batch-118 = 0.0035445354878902435

Training epoch-9 batch-119
Running loss of epoch-9 batch-119 = 0.0027555953711271286

Training epoch-9 batch-120
Running loss of epoch-9 batch-120 = 0.0014515486545860767

Training epoch-9 batch-121
Running loss of epoch-9 batch-121 = 0.0013709361664950848

Training epoch-9 batch-122
Running loss of epoch-9 batch-122 = 0.0018089567311108112

Training epoch-9 batch-123
Running loss of epoch-9 batch-123 = 0.0012224274687469006

Training epoch-9 batch-124
Running loss of epoch-9 batch-124 = 0.00347749050706625

Training epoch-9 batch-125
Running loss of epoch-9 batch-125 = 0.0015675547765567899

Training epoch-9 batch-126
Running loss of epoch-9 batch-126 = 0.002717837691307068

Training epoch-9 batch-127
Running loss of epoch-9 batch-127 = 0.0021835220977663994

Training epoch-9 batch-128
Running loss of epoch-9 batch-128 = 0.002625228837132454

Training epoch-9 batch-129
Running loss of epoch-9 batch-129 = 0.002109275199472904

Training epoch-9 batch-130
Running loss of epoch-9 batch-130 = 0.0023480933159589767

Training epoch-9 batch-131
Running loss of epoch-9 batch-131 = 0.002998125273734331

Training epoch-9 batch-132
Running loss of epoch-9 batch-132 = 0.0013530480209738016

Training epoch-9 batch-133
Running loss of epoch-9 batch-133 = 0.002761363983154297

Training epoch-9 batch-134
Running loss of epoch-9 batch-134 = 0.0023533636704087257

Training epoch-9 batch-135
Running loss of epoch-9 batch-135 = 0.0007915285532362759

Training epoch-9 batch-136
Running loss of epoch-9 batch-136 = 0.0022008586674928665

Training epoch-9 batch-137
Running loss of epoch-9 batch-137 = 0.001720459433272481

Training epoch-9 batch-138
Running loss of epoch-9 batch-138 = 0.0019981353543698788

Training epoch-9 batch-139
Running loss of epoch-9 batch-139 = 0.0011252998374402523

Training epoch-9 batch-140
Running loss of epoch-9 batch-140 = 0.0006360526895150542

Training epoch-9 batch-141
Running loss of epoch-9 batch-141 = 0.0014295016881078482

Training epoch-9 batch-142
Running loss of epoch-9 batch-142 = 0.0020914683118462563

Training epoch-9 batch-143
Running loss of epoch-9 batch-143 = 0.0018664407543838024

Training epoch-9 batch-144
Running loss of epoch-9 batch-144 = 0.002950036898255348

Training epoch-9 batch-145
Running loss of epoch-9 batch-145 = 0.0037387507036328316

Training epoch-9 batch-146
Running loss of epoch-9 batch-146 = 0.004547665826976299

Training epoch-9 batch-147
Running loss of epoch-9 batch-147 = 0.0021872702054679394

Training epoch-9 batch-148
Running loss of epoch-9 batch-148 = 0.0009366701706312597

Training epoch-9 batch-149
Running loss of epoch-9 batch-149 = 0.003458579070866108

Training epoch-9 batch-150
Running loss of epoch-9 batch-150 = 0.0015284564578905702

Training epoch-9 batch-151
Running loss of epoch-9 batch-151 = 0.0020723463967442513

Training epoch-9 batch-152
Running loss of epoch-9 batch-152 = 0.0019751505460590124

Training epoch-9 batch-153
Running loss of epoch-9 batch-153 = 0.0017811197321861982

Training epoch-9 batch-154
Running loss of epoch-9 batch-154 = 0.0013751470251008868

Training epoch-9 batch-155
Running loss of epoch-9 batch-155 = 0.0017713385168462992

Training epoch-9 batch-156
Running loss of epoch-9 batch-156 = 0.0009339170064777136

Training epoch-9 batch-157
Running loss of epoch-9 batch-157 = 0.002915823832154274

Finished training epoch-9.



Average train loss at epoch-9 = 0.0021545647118240596

Started Evaluation

Average val loss at epoch-9 = 0.6282377175743213

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 94.92 %
Accuracy for class onCreate is: 91.04 %
Accuracy for class toString is: 89.76 %
Accuracy for class run is: 57.08 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 50.45 %
Accuracy for class execute is: 59.84 %
Accuracy for class get is: 62.31 %

Overall Accuracy = 82.59 %

Finished Evaluation



Started training epoch-10


Training epoch-10 batch-1
Running loss of epoch-10 batch-1 = 0.0009687200654298067

Training epoch-10 batch-2
Running loss of epoch-10 batch-2 = 0.0017472842009738088

Training epoch-10 batch-3
Running loss of epoch-10 batch-3 = 0.000865883775986731

Training epoch-10 batch-4
Running loss of epoch-10 batch-4 = 0.00071283511351794

Training epoch-10 batch-5
Running loss of epoch-10 batch-5 = 0.0011834950419142842

Training epoch-10 batch-6
Running loss of epoch-10 batch-6 = 0.0017679091542959213

Training epoch-10 batch-7
Running loss of epoch-10 batch-7 = 0.0018697564955800772

Training epoch-10 batch-8
Running loss of epoch-10 batch-8 = 0.0016104161040857434

Training epoch-10 batch-9
Running loss of epoch-10 batch-9 = 0.001226151129230857

Training epoch-10 batch-10
Running loss of epoch-10 batch-10 = 0.0012671055737882853

Training epoch-10 batch-11
Running loss of epoch-10 batch-11 = 0.0018177032470703125

Training epoch-10 batch-12
Running loss of epoch-10 batch-12 = 0.0012575163273140788

Training epoch-10 batch-13
Running loss of epoch-10 batch-13 = 0.0019282940775156021

Training epoch-10 batch-14
Running loss of epoch-10 batch-14 = 0.0024389000609517097

Training epoch-10 batch-15
Running loss of epoch-10 batch-15 = 0.001373555976897478

Training epoch-10 batch-16
Running loss of epoch-10 batch-16 = 0.0023424355313181877

Training epoch-10 batch-17
Running loss of epoch-10 batch-17 = 0.0009294407209381461

Training epoch-10 batch-18
Running loss of epoch-10 batch-18 = 0.0012958034640178084

Training epoch-10 batch-19
Running loss of epoch-10 batch-19 = 0.001555248280055821

Training epoch-10 batch-20
Running loss of epoch-10 batch-20 = 0.0019117413321509957

Training epoch-10 batch-21
Running loss of epoch-10 batch-21 = 0.0009620439959689975

Training epoch-10 batch-22
Running loss of epoch-10 batch-22 = 0.0015851326752454042

Training epoch-10 batch-23
Running loss of epoch-10 batch-23 = 0.0007720352150499821

Training epoch-10 batch-24
Running loss of epoch-10 batch-24 = 0.00043367419857531786

Training epoch-10 batch-25
Running loss of epoch-10 batch-25 = 0.0016195419011637568

Training epoch-10 batch-26
Running loss of epoch-10 batch-26 = 0.0023316764272749424

Training epoch-10 batch-27
Running loss of epoch-10 batch-27 = 0.001941273920238018

Training epoch-10 batch-28
Running loss of epoch-10 batch-28 = 0.0005943101132288575

Training epoch-10 batch-29
Running loss of epoch-10 batch-29 = 0.0017087490996345878

Training epoch-10 batch-30
Running loss of epoch-10 batch-30 = 0.0014133926015347242

Training epoch-10 batch-31
Running loss of epoch-10 batch-31 = 0.0022768215276300907

Training epoch-10 batch-32
Running loss of epoch-10 batch-32 = 0.0021492682863026857

Training epoch-10 batch-33
Running loss of epoch-10 batch-33 = 0.0007954816101118922

Training epoch-10 batch-34
Running loss of epoch-10 batch-34 = 0.0006023145979270339

Training epoch-10 batch-35
Running loss of epoch-10 batch-35 = 0.002077616984024644

Training epoch-10 batch-36
Running loss of epoch-10 batch-36 = 0.0017551523633301258

Training epoch-10 batch-37
Running loss of epoch-10 batch-37 = 0.0021695855539292097

Training epoch-10 batch-38
Running loss of epoch-10 batch-38 = 0.0027096986304968596

Training epoch-10 batch-39
Running loss of epoch-10 batch-39 = 0.0015230093849822879

Training epoch-10 batch-40
Running loss of epoch-10 batch-40 = 0.0011056000366806984

Training epoch-10 batch-41
Running loss of epoch-10 batch-41 = 0.0013741092989221215

Training epoch-10 batch-42
Running loss of epoch-10 batch-42 = 0.0032137874513864517

Training epoch-10 batch-43
Running loss of epoch-10 batch-43 = 0.0008644997142255306

Training epoch-10 batch-44
Running loss of epoch-10 batch-44 = 0.001370652113109827

Training epoch-10 batch-45
Running loss of epoch-10 batch-45 = 0.001878349925391376

Training epoch-10 batch-46
Running loss of epoch-10 batch-46 = 0.0009358993265777826

Training epoch-10 batch-47
Running loss of epoch-10 batch-47 = 0.0027326380368322134

Training epoch-10 batch-48
Running loss of epoch-10 batch-48 = 0.001555270398966968

Training epoch-10 batch-49
Running loss of epoch-10 batch-49 = 0.0023824626114219427

Training epoch-10 batch-50
Running loss of epoch-10 batch-50 = 0.0012439674464985728

Training epoch-10 batch-51
Running loss of epoch-10 batch-51 = 0.00118171120993793

Training epoch-10 batch-52
Running loss of epoch-10 batch-52 = 0.001507344888523221

Training epoch-10 batch-53
Running loss of epoch-10 batch-53 = 0.0011352926958352327

Training epoch-10 batch-54
Running loss of epoch-10 batch-54 = 0.0009128518286161125

Training epoch-10 batch-55
Running loss of epoch-10 batch-55 = 0.0013374881818890572

Training epoch-10 batch-56
Running loss of epoch-10 batch-56 = 0.002288471907377243

Training epoch-10 batch-57
Running loss of epoch-10 batch-57 = 0.0017963055288419127

Training epoch-10 batch-58
Running loss of epoch-10 batch-58 = 0.0011829814175143838

Training epoch-10 batch-59
Running loss of epoch-10 batch-59 = 0.002058458747342229

Training epoch-10 batch-60
Running loss of epoch-10 batch-60 = 0.0008386046392843127

Training epoch-10 batch-61
Running loss of epoch-10 batch-61 = 0.0009883248712867498

Training epoch-10 batch-62
Running loss of epoch-10 batch-62 = 0.0013628575252369046

Training epoch-10 batch-63
Running loss of epoch-10 batch-63 = 0.0020651279482990503

Training epoch-10 batch-64
Running loss of epoch-10 batch-64 = 0.0013373044785112143

Training epoch-10 batch-65
Running loss of epoch-10 batch-65 = 0.0009797350503504276

Training epoch-10 batch-66
Running loss of epoch-10 batch-66 = 0.0012390508782118559

Training epoch-10 batch-67
Running loss of epoch-10 batch-67 = 0.0010629869066178799

Training epoch-10 batch-68
Running loss of epoch-10 batch-68 = 0.0018970449455082417

Training epoch-10 batch-69
Running loss of epoch-10 batch-69 = 0.0018615645822137594

Training epoch-10 batch-70
Running loss of epoch-10 batch-70 = 0.0014784276718273759

Training epoch-10 batch-71
Running loss of epoch-10 batch-71 = 0.0011779092019423842

Training epoch-10 batch-72
Running loss of epoch-10 batch-72 = 0.0012297927169129252

Training epoch-10 batch-73
Running loss of epoch-10 batch-73 = 0.0014344677329063416

Training epoch-10 batch-74
Running loss of epoch-10 batch-74 = 0.0023244398180395365

Training epoch-10 batch-75
Running loss of epoch-10 batch-75 = 0.0008773229783400893

Training epoch-10 batch-76
Running loss of epoch-10 batch-76 = 0.0031153021845966578

Training epoch-10 batch-77
Running loss of epoch-10 batch-77 = 0.0011446225689724088

Training epoch-10 batch-78
Running loss of epoch-10 batch-78 = 0.001092794700525701

Training epoch-10 batch-79
Running loss of epoch-10 batch-79 = 0.0017194428946822882

Training epoch-10 batch-80
Running loss of epoch-10 batch-80 = 0.0019511013524606824

Training epoch-10 batch-81
Running loss of epoch-10 batch-81 = 0.0008339236956089735

Training epoch-10 batch-82
Running loss of epoch-10 batch-82 = 0.001679291483014822

Training epoch-10 batch-83
Running loss of epoch-10 batch-83 = 0.00136321980971843

Training epoch-10 batch-84
Running loss of epoch-10 batch-84 = 0.0018459436250850558

Training epoch-10 batch-85
Running loss of epoch-10 batch-85 = 0.0015657193725928664

Training epoch-10 batch-86
Running loss of epoch-10 batch-86 = 0.0019023438217118382

Training epoch-10 batch-87
Running loss of epoch-10 batch-87 = 0.0013653754722326994

Training epoch-10 batch-88
Running loss of epoch-10 batch-88 = 0.0007325030164793134

Training epoch-10 batch-89
Running loss of epoch-10 batch-89 = 0.0010447975946590304

Training epoch-10 batch-90
Running loss of epoch-10 batch-90 = 0.0016450326656922698

Training epoch-10 batch-91
Running loss of epoch-10 batch-91 = 0.001191871939226985

Training epoch-10 batch-92
Running loss of epoch-10 batch-92 = 0.0008091347408480942

Training epoch-10 batch-93
Running loss of epoch-10 batch-93 = 0.0012909488286823034

Training epoch-10 batch-94
Running loss of epoch-10 batch-94 = 0.000768546131439507

Training epoch-10 batch-95
Running loss of epoch-10 batch-95 = 0.0018781365361064672

Training epoch-10 batch-96
Running loss of epoch-10 batch-96 = 0.001652199076488614

Training epoch-10 batch-97
Running loss of epoch-10 batch-97 = 0.003388497047126293

Training epoch-10 batch-98
Running loss of epoch-10 batch-98 = 0.003194712335243821

Training epoch-10 batch-99
Running loss of epoch-10 batch-99 = 0.0004870581906288862

Training epoch-10 batch-100
Running loss of epoch-10 batch-100 = 0.0036591116804629564

Training epoch-10 batch-101
Running loss of epoch-10 batch-101 = 0.0010518425842747092

Training epoch-10 batch-102
Running loss of epoch-10 batch-102 = 0.0019955039024353027

Training epoch-10 batch-103
Running loss of epoch-10 batch-103 = 0.001469059963710606

Training epoch-10 batch-104
Running loss of epoch-10 batch-104 = 0.0009568756213411689

Training epoch-10 batch-105
Running loss of epoch-10 batch-105 = 0.0018603993812575936

Training epoch-10 batch-106
Running loss of epoch-10 batch-106 = 0.0013991646701470017

Training epoch-10 batch-107
Running loss of epoch-10 batch-107 = 0.0014564298326149583

Training epoch-10 batch-108
Running loss of epoch-10 batch-108 = 0.0022657481022179127

Training epoch-10 batch-109
Running loss of epoch-10 batch-109 = 0.00032105238642543554

Training epoch-10 batch-110
Running loss of epoch-10 batch-110 = 0.0016841660253703594

Training epoch-10 batch-111
Running loss of epoch-10 batch-111 = 0.0008262896444648504

Training epoch-10 batch-112
Running loss of epoch-10 batch-112 = 0.0009975231951102614

Training epoch-10 batch-113
Running loss of epoch-10 batch-113 = 0.0014321886701509356

Training epoch-10 batch-114
Running loss of epoch-10 batch-114 = 0.0013120367657393217

Training epoch-10 batch-115
Running loss of epoch-10 batch-115 = 0.002167545258998871

Training epoch-10 batch-116
Running loss of epoch-10 batch-116 = 0.0008405434200540185

Training epoch-10 batch-117
Running loss of epoch-10 batch-117 = 0.0014326287200674415

Training epoch-10 batch-118
Running loss of epoch-10 batch-118 = 0.0024556780699640512

Training epoch-10 batch-119
Running loss of epoch-10 batch-119 = 0.0010623743291944265

Training epoch-10 batch-120
Running loss of epoch-10 batch-120 = 0.001816692529246211

Training epoch-10 batch-121
Running loss of epoch-10 batch-121 = 0.0015144208446145058

Training epoch-10 batch-122
Running loss of epoch-10 batch-122 = 0.001344299060292542

Training epoch-10 batch-123
Running loss of epoch-10 batch-123 = 0.0018596852896735072

Training epoch-10 batch-124
Running loss of epoch-10 batch-124 = 0.0006142305210232735

Training epoch-10 batch-125
Running loss of epoch-10 batch-125 = 0.000932981027290225

Training epoch-10 batch-126
Running loss of epoch-10 batch-126 = 0.0007101272931322455

Training epoch-10 batch-127
Running loss of epoch-10 batch-127 = 0.0014667683281004429

Training epoch-10 batch-128
Running loss of epoch-10 batch-128 = 0.001718454877845943

Training epoch-10 batch-129
Running loss of epoch-10 batch-129 = 0.001809141831472516

Training epoch-10 batch-130
Running loss of epoch-10 batch-130 = 0.0021476007532328367

Training epoch-10 batch-131
Running loss of epoch-10 batch-131 = 0.0019385884515941143

Training epoch-10 batch-132
Running loss of epoch-10 batch-132 = 0.0021425550803542137

Training epoch-10 batch-133
Running loss of epoch-10 batch-133 = 0.0024324278347194195

Training epoch-10 batch-134
Running loss of epoch-10 batch-134 = 0.0012623462826013565

Training epoch-10 batch-135
Running loss of epoch-10 batch-135 = 0.0011863921536132693

Training epoch-10 batch-136
Running loss of epoch-10 batch-136 = 0.002048903377726674

Training epoch-10 batch-137
Running loss of epoch-10 batch-137 = 0.0008161530131474137

Training epoch-10 batch-138
Running loss of epoch-10 batch-138 = 0.0010157751385122538

Training epoch-10 batch-139
Running loss of epoch-10 batch-139 = 0.0018878461560234427

Training epoch-10 batch-140
Running loss of epoch-10 batch-140 = 0.0011434269836172462

Training epoch-10 batch-141
Running loss of epoch-10 batch-141 = 0.0010401287581771612

Training epoch-10 batch-142
Running loss of epoch-10 batch-142 = 0.0010060709901154041

Training epoch-10 batch-143
Running loss of epoch-10 batch-143 = 0.0008709381800144911

Training epoch-10 batch-144
Running loss of epoch-10 batch-144 = 0.0009560197358950973

Training epoch-10 batch-145
Running loss of epoch-10 batch-145 = 0.0020807995460927486

Training epoch-10 batch-146
Running loss of epoch-10 batch-146 = 0.0005995798856019974

Training epoch-10 batch-147
Running loss of epoch-10 batch-147 = 0.0012101419270038605

Training epoch-10 batch-148
Running loss of epoch-10 batch-148 = 0.0007555341580882668

Training epoch-10 batch-149
Running loss of epoch-10 batch-149 = 0.00029327848460525274

Training epoch-10 batch-150
Running loss of epoch-10 batch-150 = 0.0011519810650497675

Training epoch-10 batch-151
Running loss of epoch-10 batch-151 = 0.001678026164881885

Training epoch-10 batch-152
Running loss of epoch-10 batch-152 = 0.000972093315795064

Training epoch-10 batch-153
Running loss of epoch-10 batch-153 = 0.001331653562374413

Training epoch-10 batch-154
Running loss of epoch-10 batch-154 = 0.0015210461569949985

Training epoch-10 batch-155
Running loss of epoch-10 batch-155 = 0.001368036144413054

Training epoch-10 batch-156
Running loss of epoch-10 batch-156 = 0.0020209315698593855

Training epoch-10 batch-157
Running loss of epoch-10 batch-157 = 0.003107300028204918

Finished training epoch-10.



Average train loss at epoch-10 = 0.0014941098384559153

Started Evaluation

Average val loss at epoch-10 = 0.6144948883440137

Accuracy for classes:
Accuracy for class equals is: 96.53 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.98 %
Accuracy for class onCreate is: 92.32 %
Accuracy for class toString is: 89.76 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 54.62 %
Accuracy for class get is: 60.77 %

Overall Accuracy = 82.90 %

Finished Evaluation



Started training epoch-11


Training epoch-11 batch-1
Running loss of epoch-11 batch-1 = 0.0004097259370610118

Training epoch-11 batch-2
Running loss of epoch-11 batch-2 = 0.0008364025852642953

Training epoch-11 batch-3
Running loss of epoch-11 batch-3 = 0.0006550010293722153

Training epoch-11 batch-4
Running loss of epoch-11 batch-4 = 0.000894500408321619

Training epoch-11 batch-5
Running loss of epoch-11 batch-5 = 0.0015873750671744347

Training epoch-11 batch-6
Running loss of epoch-11 batch-6 = 0.0010496322065591812

Training epoch-11 batch-7
Running loss of epoch-11 batch-7 = 0.0009414855157956481

Training epoch-11 batch-8
Running loss of epoch-11 batch-8 = 0.0009259694488719106

Training epoch-11 batch-9
Running loss of epoch-11 batch-9 = 0.0011476383078843355

Training epoch-11 batch-10
Running loss of epoch-11 batch-10 = 0.0007729713106527925

Training epoch-11 batch-11
Running loss of epoch-11 batch-11 = 0.0010164770064875484

Training epoch-11 batch-12
Running loss of epoch-11 batch-12 = 0.0008470645407214761

Training epoch-11 batch-13
Running loss of epoch-11 batch-13 = 0.0038329726085066795

Training epoch-11 batch-14
Running loss of epoch-11 batch-14 = 0.0010394497076049447

Training epoch-11 batch-15
Running loss of epoch-11 batch-15 = 0.0006748752202838659

Training epoch-11 batch-16
Running loss of epoch-11 batch-16 = 0.0010918916668742895

Training epoch-11 batch-17
Running loss of epoch-11 batch-17 = 0.001461005536839366

Training epoch-11 batch-18
Running loss of epoch-11 batch-18 = 0.0013977715279906988

Training epoch-11 batch-19
Running loss of epoch-11 batch-19 = 0.00032237451523542404

Training epoch-11 batch-20
Running loss of epoch-11 batch-20 = 0.0008271130500361323

Training epoch-11 batch-21
Running loss of epoch-11 batch-21 = 0.00038651167415082455

Training epoch-11 batch-22
Running loss of epoch-11 batch-22 = 0.0004865986993536353

Training epoch-11 batch-23
Running loss of epoch-11 batch-23 = 0.0006467746570706367

Training epoch-11 batch-24
Running loss of epoch-11 batch-24 = 0.0018469804199412465

Training epoch-11 batch-25
Running loss of epoch-11 batch-25 = 0.0011902463156729937

Training epoch-11 batch-26
Running loss of epoch-11 batch-26 = 0.0005391211016103625

Training epoch-11 batch-27
Running loss of epoch-11 batch-27 = 0.0017452719621360302

Training epoch-11 batch-28
Running loss of epoch-11 batch-28 = 0.000644893734715879

Training epoch-11 batch-29
Running loss of epoch-11 batch-29 = 0.0016588614089414477

Training epoch-11 batch-30
Running loss of epoch-11 batch-30 = 0.0007543133106082678

Training epoch-11 batch-31
Running loss of epoch-11 batch-31 = 0.0006158823380246758

Training epoch-11 batch-32
Running loss of epoch-11 batch-32 = 0.0008261694456450641

Training epoch-11 batch-33
Running loss of epoch-11 batch-33 = 0.0016421613981947303

Training epoch-11 batch-34
Running loss of epoch-11 batch-34 = 0.0004314589314162731

Training epoch-11 batch-35
Running loss of epoch-11 batch-35 = 0.0008563472656533122

Training epoch-11 batch-36
Running loss of epoch-11 batch-36 = 0.0015208718832582235

Training epoch-11 batch-37
Running loss of epoch-11 batch-37 = 0.0008818346541374922

Training epoch-11 batch-38
Running loss of epoch-11 batch-38 = 0.000814550556242466

Training epoch-11 batch-39
Running loss of epoch-11 batch-39 = 0.0012758176308125257

Training epoch-11 batch-40
Running loss of epoch-11 batch-40 = 0.000749245984479785

Training epoch-11 batch-41
Running loss of epoch-11 batch-41 = 0.0004849861143156886

Training epoch-11 batch-42
Running loss of epoch-11 batch-42 = 0.0018282142700627446

Training epoch-11 batch-43
Running loss of epoch-11 batch-43 = 0.0025002132169902325

Training epoch-11 batch-44
Running loss of epoch-11 batch-44 = 0.0015166241209954023

Training epoch-11 batch-45
Running loss of epoch-11 batch-45 = 0.0013790923403576016

Training epoch-11 batch-46
Running loss of epoch-11 batch-46 = 0.0017528706230223179

Training epoch-11 batch-47
Running loss of epoch-11 batch-47 = 0.0016503450460731983

Training epoch-11 batch-48
Running loss of epoch-11 batch-48 = 0.0007248574402183294

Training epoch-11 batch-49
Running loss of epoch-11 batch-49 = 0.0009180622873827815

Training epoch-11 batch-50
Running loss of epoch-11 batch-50 = 0.0014836383052170277

Training epoch-11 batch-51
Running loss of epoch-11 batch-51 = 0.001027300488203764

Training epoch-11 batch-52
Running loss of epoch-11 batch-52 = 0.0012653704034164548

Training epoch-11 batch-53
Running loss of epoch-11 batch-53 = 0.0010896928142756224

Training epoch-11 batch-54
Running loss of epoch-11 batch-54 = 0.0009423429146409035

Training epoch-11 batch-55
Running loss of epoch-11 batch-55 = 0.0006829296471551061

Training epoch-11 batch-56
Running loss of epoch-11 batch-56 = 0.0005926425219513476

Training epoch-11 batch-57
Running loss of epoch-11 batch-57 = 0.0009330075117759407

Training epoch-11 batch-58
Running loss of epoch-11 batch-58 = 0.001516027026809752

Training epoch-11 batch-59
Running loss of epoch-11 batch-59 = 0.0005254690768197179

Training epoch-11 batch-60
Running loss of epoch-11 batch-60 = 0.0007750783115625381

Training epoch-11 batch-61
Running loss of epoch-11 batch-61 = 0.0004870183765888214

Training epoch-11 batch-62
Running loss of epoch-11 batch-62 = 0.0008531402563676238

Training epoch-11 batch-63
Running loss of epoch-11 batch-63 = 0.0010587078286334872

Training epoch-11 batch-64
Running loss of epoch-11 batch-64 = 0.000991852954030037

Training epoch-11 batch-65
Running loss of epoch-11 batch-65 = 0.001912083476781845

Training epoch-11 batch-66
Running loss of epoch-11 batch-66 = 0.0007215577643364668

Training epoch-11 batch-67
Running loss of epoch-11 batch-67 = 0.0006289539160206914

Training epoch-11 batch-68
Running loss of epoch-11 batch-68 = 0.00043173821177333593

Training epoch-11 batch-69
Running loss of epoch-11 batch-69 = 0.0007028672844171524

Training epoch-11 batch-70
Running loss of epoch-11 batch-70 = 0.000874366145581007

Training epoch-11 batch-71
Running loss of epoch-11 batch-71 = 0.0009581937920302153

Training epoch-11 batch-72
Running loss of epoch-11 batch-72 = 0.0008230985840782523

Training epoch-11 batch-73
Running loss of epoch-11 batch-73 = 0.001285002683289349

Training epoch-11 batch-74
Running loss of epoch-11 batch-74 = 0.0009376669768244028

Training epoch-11 batch-75
Running loss of epoch-11 batch-75 = 0.0011620006989687681

Training epoch-11 batch-76
Running loss of epoch-11 batch-76 = 0.00056114187464118

Training epoch-11 batch-77
Running loss of epoch-11 batch-77 = 0.0003985952353104949

Training epoch-11 batch-78
Running loss of epoch-11 batch-78 = 0.0005838748766109347

Training epoch-11 batch-79
Running loss of epoch-11 batch-79 = 0.0006975956493988633

Training epoch-11 batch-80
Running loss of epoch-11 batch-80 = 0.0011956916423514485

Training epoch-11 batch-81
Running loss of epoch-11 batch-81 = 0.0007927449187263846

Training epoch-11 batch-82
Running loss of epoch-11 batch-82 = 0.0018625798402354121

Training epoch-11 batch-83
Running loss of epoch-11 batch-83 = 0.00042946578469127417

Training epoch-11 batch-84
Running loss of epoch-11 batch-84 = 0.0005052428459748626

Training epoch-11 batch-85
Running loss of epoch-11 batch-85 = 0.0008656741119921207

Training epoch-11 batch-86
Running loss of epoch-11 batch-86 = 0.0011803905945271254

Training epoch-11 batch-87
Running loss of epoch-11 batch-87 = 0.0007895197486504912

Training epoch-11 batch-88
Running loss of epoch-11 batch-88 = 0.0003186023095622659

Training epoch-11 batch-89
Running loss of epoch-11 batch-89 = 0.0014912921469658613

Training epoch-11 batch-90
Running loss of epoch-11 batch-90 = 0.0010446326341480017

Training epoch-11 batch-91
Running loss of epoch-11 batch-91 = 0.0010844572680070996

Training epoch-11 batch-92
Running loss of epoch-11 batch-92 = 0.0006979749305173755

Training epoch-11 batch-93
Running loss of epoch-11 batch-93 = 0.0012067521456629038

Training epoch-11 batch-94
Running loss of epoch-11 batch-94 = 0.0011265738867223263

Training epoch-11 batch-95
Running loss of epoch-11 batch-95 = 0.0005738018080592155

Training epoch-11 batch-96
Running loss of epoch-11 batch-96 = 0.00192675378639251

Training epoch-11 batch-97
Running loss of epoch-11 batch-97 = 0.0006411061622202396

Training epoch-11 batch-98
Running loss of epoch-11 batch-98 = 0.0002494227373972535

Training epoch-11 batch-99
Running loss of epoch-11 batch-99 = 0.0009185222443193197

Training epoch-11 batch-100
Running loss of epoch-11 batch-100 = 0.0010213804198428988

Training epoch-11 batch-101
Running loss of epoch-11 batch-101 = 0.0016184455016627908

Training epoch-11 batch-102
Running loss of epoch-11 batch-102 = 0.001360832597129047

Training epoch-11 batch-103
Running loss of epoch-11 batch-103 = 0.0007503652013838291

Training epoch-11 batch-104
Running loss of epoch-11 batch-104 = 0.0016054820735007524

Training epoch-11 batch-105
Running loss of epoch-11 batch-105 = 0.000528021133504808

Training epoch-11 batch-106
Running loss of epoch-11 batch-106 = 0.0005990457721054554

Training epoch-11 batch-107
Running loss of epoch-11 batch-107 = 0.001197677105665207

Training epoch-11 batch-108
Running loss of epoch-11 batch-108 = 0.0006640577339567244

Training epoch-11 batch-109
Running loss of epoch-11 batch-109 = 0.000691187335178256

Training epoch-11 batch-110
Running loss of epoch-11 batch-110 = 0.0007838672026991844

Training epoch-11 batch-111
Running loss of epoch-11 batch-111 = 0.0015401786658912897

Training epoch-11 batch-112
Running loss of epoch-11 batch-112 = 0.0009868144989013672

Training epoch-11 batch-113
Running loss of epoch-11 batch-113 = 0.0016497476026415825

Training epoch-11 batch-114
Running loss of epoch-11 batch-114 = 0.0014011553721502423

Training epoch-11 batch-115
Running loss of epoch-11 batch-115 = 0.0011618785792961717

Training epoch-11 batch-116
Running loss of epoch-11 batch-116 = 0.0004275385872460902

Training epoch-11 batch-117
Running loss of epoch-11 batch-117 = 0.0003871035296469927

Training epoch-11 batch-118
Running loss of epoch-11 batch-118 = 0.002475912682712078

Training epoch-11 batch-119
Running loss of epoch-11 batch-119 = 0.002727986080572009

Training epoch-11 batch-120
Running loss of epoch-11 batch-120 = 0.0013239509426057339

Training epoch-11 batch-121
Running loss of epoch-11 batch-121 = 0.001489020069129765

Training epoch-11 batch-122
Running loss of epoch-11 batch-122 = 0.000676877680234611

Training epoch-11 batch-123
Running loss of epoch-11 batch-123 = 0.0015755241038277745

Training epoch-11 batch-124
Running loss of epoch-11 batch-124 = 0.000895956763997674

Training epoch-11 batch-125
Running loss of epoch-11 batch-125 = 0.001636976725421846

Training epoch-11 batch-126
Running loss of epoch-11 batch-126 = 0.0008285132935270667

Training epoch-11 batch-127
Running loss of epoch-11 batch-127 = 0.001437571831047535

Training epoch-11 batch-128
Running loss of epoch-11 batch-128 = 0.0010073449229821563

Training epoch-11 batch-129
Running loss of epoch-11 batch-129 = 0.0007384255295619369

Training epoch-11 batch-130
Running loss of epoch-11 batch-130 = 0.0009282850078307092

Training epoch-11 batch-131
Running loss of epoch-11 batch-131 = 0.0007972039747983217

Training epoch-11 batch-132
Running loss of epoch-11 batch-132 = 0.0011705377837643027

Training epoch-11 batch-133
Running loss of epoch-11 batch-133 = 0.001221760525368154

Training epoch-11 batch-134
Running loss of epoch-11 batch-134 = 0.0005670977989211679

Training epoch-11 batch-135
Running loss of epoch-11 batch-135 = 0.0008619958534836769

Training epoch-11 batch-136
Running loss of epoch-11 batch-136 = 0.0008424645056948066

Training epoch-11 batch-137
Running loss of epoch-11 batch-137 = 0.0010739814024418592

Training epoch-11 batch-138
Running loss of epoch-11 batch-138 = 0.0021838443353772163

Training epoch-11 batch-139
Running loss of epoch-11 batch-139 = 0.00031104893423616886

Training epoch-11 batch-140
Running loss of epoch-11 batch-140 = 0.0009247113484889269

Training epoch-11 batch-141
Running loss of epoch-11 batch-141 = 0.0009600961348041892

Training epoch-11 batch-142
Running loss of epoch-11 batch-142 = 0.0010917134350165725

Training epoch-11 batch-143
Running loss of epoch-11 batch-143 = 0.0006017814739607275

Training epoch-11 batch-144
Running loss of epoch-11 batch-144 = 0.0005667535588145256

Training epoch-11 batch-145
Running loss of epoch-11 batch-145 = 0.0011526887537911534

Training epoch-11 batch-146
Running loss of epoch-11 batch-146 = 0.0014204982435330749

Training epoch-11 batch-147
Running loss of epoch-11 batch-147 = 0.0009753295453265309

Training epoch-11 batch-148
Running loss of epoch-11 batch-148 = 0.0015526837669312954

Training epoch-11 batch-149
Running loss of epoch-11 batch-149 = 0.00042930198833346367

Training epoch-11 batch-150
Running loss of epoch-11 batch-150 = 0.0007161784451454878

Training epoch-11 batch-151
Running loss of epoch-11 batch-151 = 0.0003640905488282442

Training epoch-11 batch-152
Running loss of epoch-11 batch-152 = 0.0010500767966732383

Training epoch-11 batch-153
Running loss of epoch-11 batch-153 = 0.0007114284671843052

Training epoch-11 batch-154
Running loss of epoch-11 batch-154 = 0.0011201429879292846

Training epoch-11 batch-155
Running loss of epoch-11 batch-155 = 0.0010391378309577703

Training epoch-11 batch-156
Running loss of epoch-11 batch-156 = 0.0008232292020693421

Training epoch-11 batch-157
Running loss of epoch-11 batch-157 = 0.008316969498991966

Finished training epoch-11.



Average train loss at epoch-11 = 0.0010401995375752449

Started Evaluation

Average val loss at epoch-11 = 0.7121171861386086

Accuracy for classes:
Accuracy for class equals is: 95.87 %
Accuracy for class main is: 95.57 %
Accuracy for class setUp is: 89.02 %
Accuracy for class onCreate is: 89.66 %
Accuracy for class toString is: 86.69 %
Accuracy for class run is: 76.48 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 43.50 %
Accuracy for class execute is: 52.61 %
Accuracy for class get is: 69.49 %

Overall Accuracy = 82.44 %

Finished Evaluation



Started training epoch-12


Training epoch-12 batch-1
Running loss of epoch-12 batch-1 = 0.000367810542229563

Training epoch-12 batch-2
Running loss of epoch-12 batch-2 = 0.0005830659065395594

Training epoch-12 batch-3
Running loss of epoch-12 batch-3 = 0.0014989126939326525

Training epoch-12 batch-4
Running loss of epoch-12 batch-4 = 0.0003959111636504531

Training epoch-12 batch-5
Running loss of epoch-12 batch-5 = 0.0007384506752714515

Training epoch-12 batch-6
Running loss of epoch-12 batch-6 = 0.0004454598529264331

Training epoch-12 batch-7
Running loss of epoch-12 batch-7 = 0.0003127774689346552

Training epoch-12 batch-8
Running loss of epoch-12 batch-8 = 0.0008433808106929064

Training epoch-12 batch-9
Running loss of epoch-12 batch-9 = 0.002039012499153614

Training epoch-12 batch-10
Running loss of epoch-12 batch-10 = 0.0005198442377150059

Training epoch-12 batch-11
Running loss of epoch-12 batch-11 = 0.00031163357198238373

Training epoch-12 batch-12
Running loss of epoch-12 batch-12 = 0.0014474596828222275

Training epoch-12 batch-13
Running loss of epoch-12 batch-13 = 0.00069316232111305

Training epoch-12 batch-14
Running loss of epoch-12 batch-14 = 0.00033268891274929047

Training epoch-12 batch-15
Running loss of epoch-12 batch-15 = 0.0011862140381708741

Training epoch-12 batch-16
Running loss of epoch-12 batch-16 = 0.0009007129119709134

Training epoch-12 batch-17
Running loss of epoch-12 batch-17 = 0.0005509803304448724

Training epoch-12 batch-18
Running loss of epoch-12 batch-18 = 0.0009247616399079561

Training epoch-12 batch-19
Running loss of epoch-12 batch-19 = 0.0007854586001485586

Training epoch-12 batch-20
Running loss of epoch-12 batch-20 = 0.000520327826961875

Training epoch-12 batch-21
Running loss of epoch-12 batch-21 = 0.0011858835350722075

Training epoch-12 batch-22
Running loss of epoch-12 batch-22 = 0.0004085521213710308

Training epoch-12 batch-23
Running loss of epoch-12 batch-23 = 0.0006684984546154737

Training epoch-12 batch-24
Running loss of epoch-12 batch-24 = 0.00039633247070014477

Training epoch-12 batch-25
Running loss of epoch-12 batch-25 = 0.00044302898459136486

Training epoch-12 batch-26
Running loss of epoch-12 batch-26 = 0.0008776221657171845

Training epoch-12 batch-27
Running loss of epoch-12 batch-27 = 0.00039969629142433405

Training epoch-12 batch-28
Running loss of epoch-12 batch-28 = 0.00064054224640131

Training epoch-12 batch-29
Running loss of epoch-12 batch-29 = 0.0004579629749059677

Training epoch-12 batch-30
Running loss of epoch-12 batch-30 = 0.0008115098462440073

Training epoch-12 batch-31
Running loss of epoch-12 batch-31 = 0.0003048931248486042

Training epoch-12 batch-32
Running loss of epoch-12 batch-32 = 0.0015264238463714719

Training epoch-12 batch-33
Running loss of epoch-12 batch-33 = 0.0004971006419509649

Training epoch-12 batch-34
Running loss of epoch-12 batch-34 = 0.0010629792232066393

Training epoch-12 batch-35
Running loss of epoch-12 batch-35 = 0.0002554862294346094

Training epoch-12 batch-36
Running loss of epoch-12 batch-36 = 0.0005067554302513599

Training epoch-12 batch-37
Running loss of epoch-12 batch-37 = 0.0007376448484137654

Training epoch-12 batch-38
Running loss of epoch-12 batch-38 = 0.0009211812866851687

Training epoch-12 batch-39
Running loss of epoch-12 batch-39 = 0.00033890409395098686

Training epoch-12 batch-40
Running loss of epoch-12 batch-40 = 0.0009571816772222519

Training epoch-12 batch-41
Running loss of epoch-12 batch-41 = 0.000882836407981813

Training epoch-12 batch-42
Running loss of epoch-12 batch-42 = 0.0007898554904386401

Training epoch-12 batch-43
Running loss of epoch-12 batch-43 = 0.0009329470922239125

Training epoch-12 batch-44
Running loss of epoch-12 batch-44 = 0.0007090364815667272

Training epoch-12 batch-45
Running loss of epoch-12 batch-45 = 0.0005616124253720045

Training epoch-12 batch-46
Running loss of epoch-12 batch-46 = 0.0002749915001913905

Training epoch-12 batch-47
Running loss of epoch-12 batch-47 = 0.00043451401870697737

Training epoch-12 batch-48
Running loss of epoch-12 batch-48 = 0.0005601143930107355

Training epoch-12 batch-49
Running loss of epoch-12 batch-49 = 0.00019185489509254694

Training epoch-12 batch-50
Running loss of epoch-12 batch-50 = 0.0006116048898547888

Training epoch-12 batch-51
Running loss of epoch-12 batch-51 = 0.0010049768025055528

Training epoch-12 batch-52
Running loss of epoch-12 batch-52 = 0.0005875634960830212

Training epoch-12 batch-53
Running loss of epoch-12 batch-53 = 0.0007464004447683692

Training epoch-12 batch-54
Running loss of epoch-12 batch-54 = 0.001218883553519845

Training epoch-12 batch-55
Running loss of epoch-12 batch-55 = 0.0003567957319319248

Training epoch-12 batch-56
Running loss of epoch-12 batch-56 = 0.0008259424939751625

Training epoch-12 batch-57
Running loss of epoch-12 batch-57 = 0.0005554766394197941

Training epoch-12 batch-58
Running loss of epoch-12 batch-58 = 0.0003340967232361436

Training epoch-12 batch-59
Running loss of epoch-12 batch-59 = 0.0010083887027576566

Training epoch-12 batch-60
Running loss of epoch-12 batch-60 = 0.0004977661883458495

Training epoch-12 batch-61
Running loss of epoch-12 batch-61 = 0.001013349392451346

Training epoch-12 batch-62
Running loss of epoch-12 batch-62 = 0.0005582231096923351

Training epoch-12 batch-63
Running loss of epoch-12 batch-63 = 0.0004881849745288491

Training epoch-12 batch-64
Running loss of epoch-12 batch-64 = 0.0002569440985098481

Training epoch-12 batch-65
Running loss of epoch-12 batch-65 = 0.0007326164050027728

Training epoch-12 batch-66
Running loss of epoch-12 batch-66 = 0.0004365593194961548

Training epoch-12 batch-67
Running loss of epoch-12 batch-67 = 0.0006396456155925989

Training epoch-12 batch-68
Running loss of epoch-12 batch-68 = 0.0006472537061199546

Training epoch-12 batch-69
Running loss of epoch-12 batch-69 = 0.0016286334721371531

Training epoch-12 batch-70
Running loss of epoch-12 batch-70 = 0.0013715320965275168

Training epoch-12 batch-71
Running loss of epoch-12 batch-71 = 0.0010371574899181724

Training epoch-12 batch-72
Running loss of epoch-12 batch-72 = 0.0003054429544135928

Training epoch-12 batch-73
Running loss of epoch-12 batch-73 = 0.000829508644528687

Training epoch-12 batch-74
Running loss of epoch-12 batch-74 = 0.000441688927821815

Training epoch-12 batch-75
Running loss of epoch-12 batch-75 = 0.000718297204002738

Training epoch-12 batch-76
Running loss of epoch-12 batch-76 = 0.00042495818343013525

Training epoch-12 batch-77
Running loss of epoch-12 batch-77 = 0.0007647064048796892

Training epoch-12 batch-78
Running loss of epoch-12 batch-78 = 0.0005009995074942708

Training epoch-12 batch-79
Running loss of epoch-12 batch-79 = 0.0004025408998131752

Training epoch-12 batch-80
Running loss of epoch-12 batch-80 = 0.0006855962565168738

Training epoch-12 batch-81
Running loss of epoch-12 batch-81 = 0.0011538683902472258

Training epoch-12 batch-82
Running loss of epoch-12 batch-82 = 0.0007200206164270639

Training epoch-12 batch-83
Running loss of epoch-12 batch-83 = 0.00036650989204645157

Training epoch-12 batch-84
Running loss of epoch-12 batch-84 = 0.0007214953657239676

Training epoch-12 batch-85
Running loss of epoch-12 batch-85 = 0.0003378643887117505

Training epoch-12 batch-86
Running loss of epoch-12 batch-86 = 0.0003195252502337098

Training epoch-12 batch-87
Running loss of epoch-12 batch-87 = 0.00021527090575546026

Training epoch-12 batch-88
Running loss of epoch-12 batch-88 = 0.00039628741797059774

Training epoch-12 batch-89
Running loss of epoch-12 batch-89 = 0.0006622683722525835

Training epoch-12 batch-90
Running loss of epoch-12 batch-90 = 0.0015403958968818188

Training epoch-12 batch-91
Running loss of epoch-12 batch-91 = 0.0003433343954384327

Training epoch-12 batch-92
Running loss of epoch-12 batch-92 = 0.0008082124404609203

Training epoch-12 batch-93
Running loss of epoch-12 batch-93 = 0.00089395756367594

Training epoch-12 batch-94
Running loss of epoch-12 batch-94 = 0.0007887249812483788

Training epoch-12 batch-95
Running loss of epoch-12 batch-95 = 0.0009726495482027531

Training epoch-12 batch-96
Running loss of epoch-12 batch-96 = 0.0008291943813674152

Training epoch-12 batch-97
Running loss of epoch-12 batch-97 = 0.0007990790763869882

Training epoch-12 batch-98
Running loss of epoch-12 batch-98 = 0.0008024671697057784

Training epoch-12 batch-99
Running loss of epoch-12 batch-99 = 0.0003926575882360339

Training epoch-12 batch-100
Running loss of epoch-12 batch-100 = 0.00028257013764232397

Training epoch-12 batch-101
Running loss of epoch-12 batch-101 = 0.0014232194516807795

Training epoch-12 batch-102
Running loss of epoch-12 batch-102 = 0.0016635236097499728

Training epoch-12 batch-103
Running loss of epoch-12 batch-103 = 0.0006795252556912601

Training epoch-12 batch-104
Running loss of epoch-12 batch-104 = 0.0008305007359012961

Training epoch-12 batch-105
Running loss of epoch-12 batch-105 = 0.0009025016333907843

Training epoch-12 batch-106
Running loss of epoch-12 batch-106 = 0.00042641471372917295

Training epoch-12 batch-107
Running loss of epoch-12 batch-107 = 0.0007674447260797024

Training epoch-12 batch-108
Running loss of epoch-12 batch-108 = 0.0016417987644672394

Training epoch-12 batch-109
Running loss of epoch-12 batch-109 = 0.0009374732617288828

Training epoch-12 batch-110
Running loss of epoch-12 batch-110 = 0.0007478640181943774

Training epoch-12 batch-111
Running loss of epoch-12 batch-111 = 0.0005356696783564985

Training epoch-12 batch-112
Running loss of epoch-12 batch-112 = 0.0005689926911145449

Training epoch-12 batch-113
Running loss of epoch-12 batch-113 = 0.00011988217011094093

Training epoch-12 batch-114
Running loss of epoch-12 batch-114 = 0.0008540551643818617

Training epoch-12 batch-115
Running loss of epoch-12 batch-115 = 0.0017956886440515518

Training epoch-12 batch-116
Running loss of epoch-12 batch-116 = 0.0010657438542693853

Training epoch-12 batch-117
Running loss of epoch-12 batch-117 = 0.0013681914424523711

Training epoch-12 batch-118
Running loss of epoch-12 batch-118 = 0.0005528563633561134

Training epoch-12 batch-119
Running loss of epoch-12 batch-119 = 0.0009490269003435969

Training epoch-12 batch-120
Running loss of epoch-12 batch-120 = 0.0012403219006955624

Training epoch-12 batch-121
Running loss of epoch-12 batch-121 = 0.0008224467746913433

Training epoch-12 batch-122
Running loss of epoch-12 batch-122 = 0.00024550966918468475

Training epoch-12 batch-123
Running loss of epoch-12 batch-123 = 0.000894333585165441

Training epoch-12 batch-124
Running loss of epoch-12 batch-124 = 0.0006393087678588927

Training epoch-12 batch-125
Running loss of epoch-12 batch-125 = 0.00039764412213116884

Training epoch-12 batch-126
Running loss of epoch-12 batch-126 = 0.0009775691432878375

Training epoch-12 batch-127
Running loss of epoch-12 batch-127 = 0.0011448698351159692

Training epoch-12 batch-128
Running loss of epoch-12 batch-128 = 0.0010131666203960776

Training epoch-12 batch-129
Running loss of epoch-12 batch-129 = 0.0007452454883605242

Training epoch-12 batch-130
Running loss of epoch-12 batch-130 = 0.0006948725786060095

Training epoch-12 batch-131
Running loss of epoch-12 batch-131 = 0.0002806875854730606

Training epoch-12 batch-132
Running loss of epoch-12 batch-132 = 0.0002105318708345294

Training epoch-12 batch-133
Running loss of epoch-12 batch-133 = 0.0005581580335274339

Training epoch-12 batch-134
Running loss of epoch-12 batch-134 = 0.00036599417217075825

Training epoch-12 batch-135
Running loss of epoch-12 batch-135 = 0.0007767126662656665

Training epoch-12 batch-136
Running loss of epoch-12 batch-136 = 0.0009198506595566869

Training epoch-12 batch-137
Running loss of epoch-12 batch-137 = 0.0004313905956223607

Training epoch-12 batch-138
Running loss of epoch-12 batch-138 = 0.0007147579453885555

Training epoch-12 batch-139
Running loss of epoch-12 batch-139 = 0.00025117932818830013

Training epoch-12 batch-140
Running loss of epoch-12 batch-140 = 0.0008741633500903845

Training epoch-12 batch-141
Running loss of epoch-12 batch-141 = 0.0006947527290321887

Training epoch-12 batch-142
Running loss of epoch-12 batch-142 = 0.0011114994995296001

Training epoch-12 batch-143
Running loss of epoch-12 batch-143 = 0.0006901387823745608

Training epoch-12 batch-144
Running loss of epoch-12 batch-144 = 0.0006185849197208881

Training epoch-12 batch-145
Running loss of epoch-12 batch-145 = 0.0006524673663079739

Training epoch-12 batch-146
Running loss of epoch-12 batch-146 = 0.0004918774939142168

Training epoch-12 batch-147
Running loss of epoch-12 batch-147 = 0.0006362099666148424

Training epoch-12 batch-148
Running loss of epoch-12 batch-148 = 0.0005396375199779868

Training epoch-12 batch-149
Running loss of epoch-12 batch-149 = 0.0012975153513252735

Training epoch-12 batch-150
Running loss of epoch-12 batch-150 = 0.00034419039729982615

Training epoch-12 batch-151
Running loss of epoch-12 batch-151 = 0.0002749114064499736

Training epoch-12 batch-152
Running loss of epoch-12 batch-152 = 0.0027902175206691027

Training epoch-12 batch-153
Running loss of epoch-12 batch-153 = 0.0008021295652724802

Training epoch-12 batch-154
Running loss of epoch-12 batch-154 = 0.0006186375394463539

Training epoch-12 batch-155
Running loss of epoch-12 batch-155 = 0.0008248480153270066

Training epoch-12 batch-156
Running loss of epoch-12 batch-156 = 0.0006089637754485011

Training epoch-12 batch-157
Running loss of epoch-12 batch-157 = 0.004411393776535988

Finished training epoch-12.



Average train loss at epoch-12 = 0.0007375396739691496

Started Evaluation

Average val loss at epoch-12 = 0.7020370753422245

Accuracy for classes:
Accuracy for class equals is: 95.71 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 92.96 %
Accuracy for class toString is: 89.42 %
Accuracy for class run is: 52.05 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 62.56 %
Accuracy for class execute is: 54.62 %
Accuracy for class get is: 73.85 %

Overall Accuracy = 83.33 %

Finished Evaluation



Started training epoch-13


Training epoch-13 batch-1
Running loss of epoch-13 batch-1 = 0.0003017829731106758

Training epoch-13 batch-2
Running loss of epoch-13 batch-2 = 0.00030181603506207466

Training epoch-13 batch-3
Running loss of epoch-13 batch-3 = 0.0002614114200696349

Training epoch-13 batch-4
Running loss of epoch-13 batch-4 = 0.0006174740847200155

Training epoch-13 batch-5
Running loss of epoch-13 batch-5 = 0.0004988984437659383

Training epoch-13 batch-6
Running loss of epoch-13 batch-6 = 0.0005769507260993123

Training epoch-13 batch-7
Running loss of epoch-13 batch-7 = 0.00022094487212598324

Training epoch-13 batch-8
Running loss of epoch-13 batch-8 = 0.000689624110236764

Training epoch-13 batch-9
Running loss of epoch-13 batch-9 = 0.0005558137781918049

Training epoch-13 batch-10
Running loss of epoch-13 batch-10 = 0.0004882522043772042

Training epoch-13 batch-11
Running loss of epoch-13 batch-11 = 0.0010420390171930194

Training epoch-13 batch-12
Running loss of epoch-13 batch-12 = 0.0003311312757432461

Training epoch-13 batch-13
Running loss of epoch-13 batch-13 = 0.00048305827658623457

Training epoch-13 batch-14
Running loss of epoch-13 batch-14 = 0.00018674193415790796

Training epoch-13 batch-15
Running loss of epoch-13 batch-15 = 0.00033049657940864563

Training epoch-13 batch-16
Running loss of epoch-13 batch-16 = 0.0003126634983345866

Training epoch-13 batch-17
Running loss of epoch-13 batch-17 = 0.00033673655707389116

Training epoch-13 batch-18
Running loss of epoch-13 batch-18 = 0.00017168838530778885

Training epoch-13 batch-19
Running loss of epoch-13 batch-19 = 0.00044391839765012264

Training epoch-13 batch-20
Running loss of epoch-13 batch-20 = 0.00032423832453787327

Training epoch-13 batch-21
Running loss of epoch-13 batch-21 = 0.00020108255557715893

Training epoch-13 batch-22
Running loss of epoch-13 batch-22 = 0.00035659619607031345

Training epoch-13 batch-23
Running loss of epoch-13 batch-23 = 0.0006634696619585156

Training epoch-13 batch-24
Running loss of epoch-13 batch-24 = 0.000462191179394722

Training epoch-13 batch-25
Running loss of epoch-13 batch-25 = 0.00023820577189326286

Training epoch-13 batch-26
Running loss of epoch-13 batch-26 = 0.0005547059699892998

Training epoch-13 batch-27
Running loss of epoch-13 batch-27 = 0.00020833464805036783

Training epoch-13 batch-28
Running loss of epoch-13 batch-28 = 0.00037144043017178774

Training epoch-13 batch-29
Running loss of epoch-13 batch-29 = 0.0010444547515362501

Training epoch-13 batch-30
Running loss of epoch-13 batch-30 = 0.00014536327216774225

Training epoch-13 batch-31
Running loss of epoch-13 batch-31 = 0.0002319180639460683

Training epoch-13 batch-32
Running loss of epoch-13 batch-32 = 0.00019808392971754074

Training epoch-13 batch-33
Running loss of epoch-13 batch-33 = 0.00040623301174491644

Training epoch-13 batch-34
Running loss of epoch-13 batch-34 = 0.00045389251317828894

Training epoch-13 batch-35
Running loss of epoch-13 batch-35 = 0.00019887264352291822

Training epoch-13 batch-36
Running loss of epoch-13 batch-36 = 0.00025672081392258406

Training epoch-13 batch-37
Running loss of epoch-13 batch-37 = 0.00038805813528597355

Training epoch-13 batch-38
Running loss of epoch-13 batch-38 = 0.00027158460579812527

Training epoch-13 batch-39
Running loss of epoch-13 batch-39 = 0.0005327141843736172

Training epoch-13 batch-40
Running loss of epoch-13 batch-40 = 0.0003864717436954379

Training epoch-13 batch-41
Running loss of epoch-13 batch-41 = 0.00039286480750888586

Training epoch-13 batch-42
Running loss of epoch-13 batch-42 = 0.0003806373570114374

Training epoch-13 batch-43
Running loss of epoch-13 batch-43 = 0.0012253484455868602

Training epoch-13 batch-44
Running loss of epoch-13 batch-44 = 0.0008254015119746327

Training epoch-13 batch-45
Running loss of epoch-13 batch-45 = 0.00028633023612201214

Training epoch-13 batch-46
Running loss of epoch-13 batch-46 = 0.0003668548306450248

Training epoch-13 batch-47
Running loss of epoch-13 batch-47 = 0.0003472242970019579

Training epoch-13 batch-48
Running loss of epoch-13 batch-48 = 0.00022596982307732105

Training epoch-13 batch-49
Running loss of epoch-13 batch-49 = 0.0002090295311063528

Training epoch-13 batch-50
Running loss of epoch-13 batch-50 = 0.0011452067410573363

Training epoch-13 batch-51
Running loss of epoch-13 batch-51 = 0.0004515567561611533

Training epoch-13 batch-52
Running loss of epoch-13 batch-52 = 0.00035231339279562235

Training epoch-13 batch-53
Running loss of epoch-13 batch-53 = 8.035905193537474e-05

Training epoch-13 batch-54
Running loss of epoch-13 batch-54 = 0.001617314643226564

Training epoch-13 batch-55
Running loss of epoch-13 batch-55 = 0.00038853706791996956

Training epoch-13 batch-56
Running loss of epoch-13 batch-56 = 0.00015597022138535976

Training epoch-13 batch-57
Running loss of epoch-13 batch-57 = 0.0005347660044208169

Training epoch-13 batch-58
Running loss of epoch-13 batch-58 = 0.0005221187602728605

Training epoch-13 batch-59
Running loss of epoch-13 batch-59 = 0.0006481162272393703

Training epoch-13 batch-60
Running loss of epoch-13 batch-60 = 0.00030222354689612985

Training epoch-13 batch-61
Running loss of epoch-13 batch-61 = 0.0010830401442945004

Training epoch-13 batch-62
Running loss of epoch-13 batch-62 = 0.0009027642663568258

Training epoch-13 batch-63
Running loss of epoch-13 batch-63 = 0.002264904323965311

Training epoch-13 batch-64
Running loss of epoch-13 batch-64 = 0.0001612295163795352

Training epoch-13 batch-65
Running loss of epoch-13 batch-65 = 0.0011481395922601223

Training epoch-13 batch-66
Running loss of epoch-13 batch-66 = 0.00047381874173879623

Training epoch-13 batch-67
Running loss of epoch-13 batch-67 = 0.0007131063612177968

Training epoch-13 batch-68
Running loss of epoch-13 batch-68 = 0.0005193407414481044

Training epoch-13 batch-69
Running loss of epoch-13 batch-69 = 0.0002445002319291234

Training epoch-13 batch-70
Running loss of epoch-13 batch-70 = 0.0001974842743948102

Training epoch-13 batch-71
Running loss of epoch-13 batch-71 = 0.0002387474523857236

Training epoch-13 batch-72
Running loss of epoch-13 batch-72 = 0.0003254114417359233

Training epoch-13 batch-73
Running loss of epoch-13 batch-73 = 0.0002058440586552024

Training epoch-13 batch-74
Running loss of epoch-13 batch-74 = 0.00043778284452855587

Training epoch-13 batch-75
Running loss of epoch-13 batch-75 = 0.000554009573534131

Training epoch-13 batch-76
Running loss of epoch-13 batch-76 = 0.0005778538761660457

Training epoch-13 batch-77
Running loss of epoch-13 batch-77 = 0.0003902579192072153

Training epoch-13 batch-78
Running loss of epoch-13 batch-78 = 0.00030122394673526287

Training epoch-13 batch-79
Running loss of epoch-13 batch-79 = 0.0007101690862327814

Training epoch-13 batch-80
Running loss of epoch-13 batch-80 = 0.000899480190128088

Training epoch-13 batch-81
Running loss of epoch-13 batch-81 = 0.0010453377617523074

Training epoch-13 batch-82
Running loss of epoch-13 batch-82 = 0.00045167794451117516

Training epoch-13 batch-83
Running loss of epoch-13 batch-83 = 0.00037972256541252136

Training epoch-13 batch-84
Running loss of epoch-13 batch-84 = 0.001051802420988679

Training epoch-13 batch-85
Running loss of epoch-13 batch-85 = 0.00039021961856633425

Training epoch-13 batch-86
Running loss of epoch-13 batch-86 = 0.0007809347007423639

Training epoch-13 batch-87
Running loss of epoch-13 batch-87 = 0.0013438744936138391

Training epoch-13 batch-88
Running loss of epoch-13 batch-88 = 0.0006287283031269908

Training epoch-13 batch-89
Running loss of epoch-13 batch-89 = 0.0008908759336918592

Training epoch-13 batch-90
Running loss of epoch-13 batch-90 = 0.0011377936461940408

Training epoch-13 batch-91
Running loss of epoch-13 batch-91 = 0.0006031780503690243

Training epoch-13 batch-92
Running loss of epoch-13 batch-92 = 0.0008979002013802528

Training epoch-13 batch-93
Running loss of epoch-13 batch-93 = 0.00057893933262676

Training epoch-13 batch-94
Running loss of epoch-13 batch-94 = 0.00019873352721333504

Training epoch-13 batch-95
Running loss of epoch-13 batch-95 = 0.00096176314400509

Training epoch-13 batch-96
Running loss of epoch-13 batch-96 = 0.0003910285886377096

Training epoch-13 batch-97
Running loss of epoch-13 batch-97 = 0.0004811013350263238

Training epoch-13 batch-98
Running loss of epoch-13 batch-98 = 0.0006965305656194687

Training epoch-13 batch-99
Running loss of epoch-13 batch-99 = 0.0002450819592922926

Training epoch-13 batch-100
Running loss of epoch-13 batch-100 = 0.0006710465531796217

Training epoch-13 batch-101
Running loss of epoch-13 batch-101 = 0.00015067716594785452

Training epoch-13 batch-102
Running loss of epoch-13 batch-102 = 0.0005262010381557047

Training epoch-13 batch-103
Running loss of epoch-13 batch-103 = 0.00034883071202784777

Training epoch-13 batch-104
Running loss of epoch-13 batch-104 = 0.0007241526618599892

Training epoch-13 batch-105
Running loss of epoch-13 batch-105 = 0.0010390160605311394

Training epoch-13 batch-106
Running loss of epoch-13 batch-106 = 0.000443575088866055

Training epoch-13 batch-107
Running loss of epoch-13 batch-107 = 0.000535222003236413

Training epoch-13 batch-108
Running loss of epoch-13 batch-108 = 0.0004412869457155466

Training epoch-13 batch-109
Running loss of epoch-13 batch-109 = 0.0011019138619303703

Training epoch-13 batch-110
Running loss of epoch-13 batch-110 = 0.0005342448130249977

Training epoch-13 batch-111
Running loss of epoch-13 batch-111 = 0.00045332976151257753

Training epoch-13 batch-112
Running loss of epoch-13 batch-112 = 0.0007427172968164086

Training epoch-13 batch-113
Running loss of epoch-13 batch-113 = 0.00020782114006578922

Training epoch-13 batch-114
Running loss of epoch-13 batch-114 = 0.0002083552535623312

Training epoch-13 batch-115
Running loss of epoch-13 batch-115 = 0.00024530605878680944

Training epoch-13 batch-116
Running loss of epoch-13 batch-116 = 0.0006248555146157742

Training epoch-13 batch-117
Running loss of epoch-13 batch-117 = 0.0010402867337688804

Training epoch-13 batch-118
Running loss of epoch-13 batch-118 = 0.000683015096001327

Training epoch-13 batch-119
Running loss of epoch-13 batch-119 = 0.0001268937485292554

Training epoch-13 batch-120
Running loss of epoch-13 batch-120 = 0.0005820970982313156

Training epoch-13 batch-121
Running loss of epoch-13 batch-121 = 0.0005660793976858258

Training epoch-13 batch-122
Running loss of epoch-13 batch-122 = 0.00029565009754151106

Training epoch-13 batch-123
Running loss of epoch-13 batch-123 = 0.0005156799452379346

Training epoch-13 batch-124
Running loss of epoch-13 batch-124 = 0.00034527794923633337

Training epoch-13 batch-125
Running loss of epoch-13 batch-125 = 0.000535452738404274

Training epoch-13 batch-126
Running loss of epoch-13 batch-126 = 0.0006231979932636023

Training epoch-13 batch-127
Running loss of epoch-13 batch-127 = 0.0003913138061761856

Training epoch-13 batch-128
Running loss of epoch-13 batch-128 = 0.0011981300776824355

Training epoch-13 batch-129
Running loss of epoch-13 batch-129 = 0.00032502051908522844

Training epoch-13 batch-130
Running loss of epoch-13 batch-130 = 0.00010119250509887934

Training epoch-13 batch-131
Running loss of epoch-13 batch-131 = 0.0002459714887663722

Training epoch-13 batch-132
Running loss of epoch-13 batch-132 = 0.00014542578719556332

Training epoch-13 batch-133
Running loss of epoch-13 batch-133 = 0.00041078473441302776

Training epoch-13 batch-134
Running loss of epoch-13 batch-134 = 0.0004120309604331851

Training epoch-13 batch-135
Running loss of epoch-13 batch-135 = 0.00036287284456193447

Training epoch-13 batch-136
Running loss of epoch-13 batch-136 = 0.0003609025152400136

Training epoch-13 batch-137
Running loss of epoch-13 batch-137 = 0.0003742590779438615

Training epoch-13 batch-138
Running loss of epoch-13 batch-138 = 0.00019931735005229712

Training epoch-13 batch-139
Running loss of epoch-13 batch-139 = 0.0001280957367271185

Training epoch-13 batch-140
Running loss of epoch-13 batch-140 = 0.00033366959542036057

Training epoch-13 batch-141
Running loss of epoch-13 batch-141 = 0.0008419547230005264

Training epoch-13 batch-142
Running loss of epoch-13 batch-142 = 0.0004828112432733178

Training epoch-13 batch-143
Running loss of epoch-13 batch-143 = 0.00022711523342877626

Training epoch-13 batch-144
Running loss of epoch-13 batch-144 = 0.0014021660899743438

Training epoch-13 batch-145
Running loss of epoch-13 batch-145 = 0.00037415255792438984

Training epoch-13 batch-146
Running loss of epoch-13 batch-146 = 0.0005403668619692326

Training epoch-13 batch-147
Running loss of epoch-13 batch-147 = 0.0006201014039106667

Training epoch-13 batch-148
Running loss of epoch-13 batch-148 = 0.0008127120672725141

Training epoch-13 batch-149
Running loss of epoch-13 batch-149 = 0.0005163233727216721

Training epoch-13 batch-150
Running loss of epoch-13 batch-150 = 0.0003266757121309638

Training epoch-13 batch-151
Running loss of epoch-13 batch-151 = 7.392861880362034e-05

Training epoch-13 batch-152
Running loss of epoch-13 batch-152 = 0.0010114234173670411

Training epoch-13 batch-153
Running loss of epoch-13 batch-153 = 0.0003178810002282262

Training epoch-13 batch-154
Running loss of epoch-13 batch-154 = 9.281898383051157e-05

Training epoch-13 batch-155
Running loss of epoch-13 batch-155 = 0.0006610006093978882

Training epoch-13 batch-156
Running loss of epoch-13 batch-156 = 0.0003956519067287445

Training epoch-13 batch-157
Running loss of epoch-13 batch-157 = 0.0002599954605102539

Finished training epoch-13.



Average train loss at epoch-13 = 0.0005124595411121845

Started Evaluation

Average val loss at epoch-13 = 0.7255404923935681

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 86.07 %
Accuracy for class onCreate is: 92.64 %
Accuracy for class toString is: 86.01 %
Accuracy for class run is: 63.70 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 53.82 %
Accuracy for class get is: 68.97 %

Overall Accuracy = 82.53 %

Finished Evaluation



Started training epoch-14


Training epoch-14 batch-1
Running loss of epoch-14 batch-1 = 0.0003423058660700917

Training epoch-14 batch-2
Running loss of epoch-14 batch-2 = 0.0001748714130371809

Training epoch-14 batch-3
Running loss of epoch-14 batch-3 = 0.00039012974593788385

Training epoch-14 batch-4
Running loss of epoch-14 batch-4 = 0.00015950610395520926

Training epoch-14 batch-5
Running loss of epoch-14 batch-5 = 0.0002576294355094433

Training epoch-14 batch-6
Running loss of epoch-14 batch-6 = 0.00022793514654040337

Training epoch-14 batch-7
Running loss of epoch-14 batch-7 = 0.0004760103765875101

Training epoch-14 batch-8
Running loss of epoch-14 batch-8 = 0.00021510326769202948

Training epoch-14 batch-9
Running loss of epoch-14 batch-9 = 0.00018000788986682892

Training epoch-14 batch-10
Running loss of epoch-14 batch-10 = 0.0003968543605878949

Training epoch-14 batch-11
Running loss of epoch-14 batch-11 = 0.00028146698605269194

Training epoch-14 batch-12
Running loss of epoch-14 batch-12 = 0.00012083398178219795

Training epoch-14 batch-13
Running loss of epoch-14 batch-13 = 0.00024040706921368837

Training epoch-14 batch-14
Running loss of epoch-14 batch-14 = 0.00021358835510909557

Training epoch-14 batch-15
Running loss of epoch-14 batch-15 = 1.745321787893772e-05

Training epoch-14 batch-16
Running loss of epoch-14 batch-16 = 0.00019751186482608318

Training epoch-14 batch-17
Running loss of epoch-14 batch-17 = 0.0002391032176092267

Training epoch-14 batch-18
Running loss of epoch-14 batch-18 = 3.466685302555561e-05

Training epoch-14 batch-19
Running loss of epoch-14 batch-19 = 0.00021751411259174347

Training epoch-14 batch-20
Running loss of epoch-14 batch-20 = 0.00017838552594184875

Training epoch-14 batch-21
Running loss of epoch-14 batch-21 = 0.000554516795091331

Training epoch-14 batch-22
Running loss of epoch-14 batch-22 = 0.00022340822033584118

Training epoch-14 batch-23
Running loss of epoch-14 batch-23 = 0.0002711457200348377

Training epoch-14 batch-24
Running loss of epoch-14 batch-24 = 0.0003180759958922863

Training epoch-14 batch-25
Running loss of epoch-14 batch-25 = 0.00016393896657973528

Training epoch-14 batch-26
Running loss of epoch-14 batch-26 = 0.00022581149823963642

Training epoch-14 batch-27
Running loss of epoch-14 batch-27 = 0.0008857341017574072

Training epoch-14 batch-28
Running loss of epoch-14 batch-28 = 0.00032482785172760487

Training epoch-14 batch-29
Running loss of epoch-14 batch-29 = 0.00018877268303185701

Training epoch-14 batch-30
Running loss of epoch-14 batch-30 = 0.00012894743122160435

Training epoch-14 batch-31
Running loss of epoch-14 batch-31 = 0.00028010783717036247

Training epoch-14 batch-32
Running loss of epoch-14 batch-32 = 0.00029561808332800865

Training epoch-14 batch-33
Running loss of epoch-14 batch-33 = 7.462745998054743e-05

Training epoch-14 batch-34
Running loss of epoch-14 batch-34 = 0.00030309136491268873

Training epoch-14 batch-35
Running loss of epoch-14 batch-35 = 0.0014601306756958365

Training epoch-14 batch-36
Running loss of epoch-14 batch-36 = 7.340940646827221e-05

Training epoch-14 batch-37
Running loss of epoch-14 batch-37 = 0.0001745247282087803

Training epoch-14 batch-38
Running loss of epoch-14 batch-38 = 0.00035466859117150307

Training epoch-14 batch-39
Running loss of epoch-14 batch-39 = 0.00017783360090106726

Training epoch-14 batch-40
Running loss of epoch-14 batch-40 = 0.0004459540359675884

Training epoch-14 batch-41
Running loss of epoch-14 batch-41 = 0.0003205382963642478

Training epoch-14 batch-42
Running loss of epoch-14 batch-42 = 0.0001520662335678935

Training epoch-14 batch-43
Running loss of epoch-14 batch-43 = 8.837634231895208e-05

Training epoch-14 batch-44
Running loss of epoch-14 batch-44 = 0.00021569617092609406

Training epoch-14 batch-45
Running loss of epoch-14 batch-45 = 0.0003521000035107136

Training epoch-14 batch-46
Running loss of epoch-14 batch-46 = 0.0001759969163686037

Training epoch-14 batch-47
Running loss of epoch-14 batch-47 = 0.0002434145426377654

Training epoch-14 batch-48
Running loss of epoch-14 batch-48 = 0.0006464902544394135

Training epoch-14 batch-49
Running loss of epoch-14 batch-49 = 0.0010536217596381903

Training epoch-14 batch-50
Running loss of epoch-14 batch-50 = 0.00020298163872212172

Training epoch-14 batch-51
Running loss of epoch-14 batch-51 = 0.00017585046589374542

Training epoch-14 batch-52
Running loss of epoch-14 batch-52 = 0.00024412234779447317

Training epoch-14 batch-53
Running loss of epoch-14 batch-53 = 0.00016552454326301813

Training epoch-14 batch-54
Running loss of epoch-14 batch-54 = 0.00040763686411082745

Training epoch-14 batch-55
Running loss of epoch-14 batch-55 = 0.0002349258866161108

Training epoch-14 batch-56
Running loss of epoch-14 batch-56 = 0.00024096062406897545

Training epoch-14 batch-57
Running loss of epoch-14 batch-57 = 0.00027020531706511974

Training epoch-14 batch-58
Running loss of epoch-14 batch-58 = 0.00034413987305015326

Training epoch-14 batch-59
Running loss of epoch-14 batch-59 = 0.0006444633472710848

Training epoch-14 batch-60
Running loss of epoch-14 batch-60 = 0.0003390145720914006

Training epoch-14 batch-61
Running loss of epoch-14 batch-61 = 0.0006871817167848349

Training epoch-14 batch-62
Running loss of epoch-14 batch-62 = 0.00040241144597530365

Training epoch-14 batch-63
Running loss of epoch-14 batch-63 = 0.00039427296724170446

Training epoch-14 batch-64
Running loss of epoch-14 batch-64 = 0.0013836284633725882

Training epoch-14 batch-65
Running loss of epoch-14 batch-65 = 0.0003500833408907056

Training epoch-14 batch-66
Running loss of epoch-14 batch-66 = 0.00041308230720460415

Training epoch-14 batch-67
Running loss of epoch-14 batch-67 = 0.00023598549887537956

Training epoch-14 batch-68
Running loss of epoch-14 batch-68 = 9.067461360245943e-05

Training epoch-14 batch-69
Running loss of epoch-14 batch-69 = 0.00014059385284781456

Training epoch-14 batch-70
Running loss of epoch-14 batch-70 = 0.00027904706075787544

Training epoch-14 batch-71
Running loss of epoch-14 batch-71 = 0.0001513924216851592

Training epoch-14 batch-72
Running loss of epoch-14 batch-72 = 0.0002975382376462221

Training epoch-14 batch-73
Running loss of epoch-14 batch-73 = 0.0004852021229453385

Training epoch-14 batch-74
Running loss of epoch-14 batch-74 = 0.0007961663650348783

Training epoch-14 batch-75
Running loss of epoch-14 batch-75 = 0.0005157744744792581

Training epoch-14 batch-76
Running loss of epoch-14 batch-76 = 0.00016163731925189495

Training epoch-14 batch-77
Running loss of epoch-14 batch-77 = 0.0003835174720734358

Training epoch-14 batch-78
Running loss of epoch-14 batch-78 = 0.0003077704459428787

Training epoch-14 batch-79
Running loss of epoch-14 batch-79 = 0.0006304184207692742

Training epoch-14 batch-80
Running loss of epoch-14 batch-80 = 0.00013507006224244833

Training epoch-14 batch-81
Running loss of epoch-14 batch-81 = 0.0004525135736912489

Training epoch-14 batch-82
Running loss of epoch-14 batch-82 = 0.0004953842144459486

Training epoch-14 batch-83
Running loss of epoch-14 batch-83 = 0.00012138020247220993

Training epoch-14 batch-84
Running loss of epoch-14 batch-84 = 0.00034304638393223286

Training epoch-14 batch-85
Running loss of epoch-14 batch-85 = 0.00021726859267801046

Training epoch-14 batch-86
Running loss of epoch-14 batch-86 = 0.00031290703918784857

Training epoch-14 batch-87
Running loss of epoch-14 batch-87 = 0.0002816410269588232

Training epoch-14 batch-88
Running loss of epoch-14 batch-88 = 0.00012569793034344912

Training epoch-14 batch-89
Running loss of epoch-14 batch-89 = 7.552013266831636e-05

Training epoch-14 batch-90
Running loss of epoch-14 batch-90 = 0.0002977154217660427

Training epoch-14 batch-91
Running loss of epoch-14 batch-91 = 0.0001460507046431303

Training epoch-14 batch-92
Running loss of epoch-14 batch-92 = 5.9451907873153687e-05

Training epoch-14 batch-93
Running loss of epoch-14 batch-93 = 0.00042388984002172947

Training epoch-14 batch-94
Running loss of epoch-14 batch-94 = 0.00018071942031383514

Training epoch-14 batch-95
Running loss of epoch-14 batch-95 = 0.00011777994222939014

Training epoch-14 batch-96
Running loss of epoch-14 batch-96 = 0.00012161640916019678

Training epoch-14 batch-97
Running loss of epoch-14 batch-97 = 0.0005082057323306799

Training epoch-14 batch-98
Running loss of epoch-14 batch-98 = 0.00029057939536869526

Training epoch-14 batch-99
Running loss of epoch-14 batch-99 = 0.00013889826368540525

Training epoch-14 batch-100
Running loss of epoch-14 batch-100 = 0.00037434184923768044

Training epoch-14 batch-101
Running loss of epoch-14 batch-101 = 0.00016890792176127434

Training epoch-14 batch-102
Running loss of epoch-14 batch-102 = 0.00018258043564856052

Training epoch-14 batch-103
Running loss of epoch-14 batch-103 = 0.00012529641389846802

Training epoch-14 batch-104
Running loss of epoch-14 batch-104 = 0.00021500897128134966

Training epoch-14 batch-105
Running loss of epoch-14 batch-105 = 0.0003485253546386957

Training epoch-14 batch-106
Running loss of epoch-14 batch-106 = 0.0001860177144408226

Training epoch-14 batch-107
Running loss of epoch-14 batch-107 = 0.0006745153805240989

Training epoch-14 batch-108
Running loss of epoch-14 batch-108 = 0.00027903751470148563

Training epoch-14 batch-109
Running loss of epoch-14 batch-109 = 0.0003346449229866266

Training epoch-14 batch-110
Running loss of epoch-14 batch-110 = 0.0002832043683156371

Training epoch-14 batch-111
Running loss of epoch-14 batch-111 = 0.00018152676057070494

Training epoch-14 batch-112
Running loss of epoch-14 batch-112 = 0.0002299757907167077

Training epoch-14 batch-113
Running loss of epoch-14 batch-113 = 0.00014608062338083982

Training epoch-14 batch-114
Running loss of epoch-14 batch-114 = 0.00011477654334157705

Training epoch-14 batch-115
Running loss of epoch-14 batch-115 = 0.00032790133263915777

Training epoch-14 batch-116
Running loss of epoch-14 batch-116 = 0.0006775661604478955

Training epoch-14 batch-117
Running loss of epoch-14 batch-117 = 0.00013336632400751114

Training epoch-14 batch-118
Running loss of epoch-14 batch-118 = 0.00021831714548170567

Training epoch-14 batch-119
Running loss of epoch-14 batch-119 = 0.0001596916699782014

Training epoch-14 batch-120
Running loss of epoch-14 batch-120 = 0.0004751092637889087

Training epoch-14 batch-121
Running loss of epoch-14 batch-121 = 0.0002883173292502761

Training epoch-14 batch-122
Running loss of epoch-14 batch-122 = 0.00045632472028955817

Training epoch-14 batch-123
Running loss of epoch-14 batch-123 = 9.836407843977213e-05

Training epoch-14 batch-124
Running loss of epoch-14 batch-124 = 0.0003103057388216257

Training epoch-14 batch-125
Running loss of epoch-14 batch-125 = 0.0004058395279571414

Training epoch-14 batch-126
Running loss of epoch-14 batch-126 = 0.00041386077646166086

Training epoch-14 batch-127
Running loss of epoch-14 batch-127 = 0.0004223737632855773

Training epoch-14 batch-128
Running loss of epoch-14 batch-128 = 9.622273501008749e-05

Training epoch-14 batch-129
Running loss of epoch-14 batch-129 = 0.00026178767438977957

Training epoch-14 batch-130
Running loss of epoch-14 batch-130 = 0.00020811182912439108

Training epoch-14 batch-131
Running loss of epoch-14 batch-131 = 0.0004724186146631837

Training epoch-14 batch-132
Running loss of epoch-14 batch-132 = 9.766267612576485e-05

Training epoch-14 batch-133
Running loss of epoch-14 batch-133 = 0.000946131709497422

Training epoch-14 batch-134
Running loss of epoch-14 batch-134 = 0.00030131638050079346

Training epoch-14 batch-135
Running loss of epoch-14 batch-135 = 0.00019154418259859085

Training epoch-14 batch-136
Running loss of epoch-14 batch-136 = 0.00037357816472649574

Training epoch-14 batch-137
Running loss of epoch-14 batch-137 = 0.0005836783675476909

Training epoch-14 batch-138
Running loss of epoch-14 batch-138 = 0.00023318117018789053

Training epoch-14 batch-139
Running loss of epoch-14 batch-139 = 0.00019601685926318169

Training epoch-14 batch-140
Running loss of epoch-14 batch-140 = 0.00018250581342726946

Training epoch-14 batch-141
Running loss of epoch-14 batch-141 = 0.00027784821577370167

Training epoch-14 batch-142
Running loss of epoch-14 batch-142 = 0.00018555298447608948

Training epoch-14 batch-143
Running loss of epoch-14 batch-143 = 0.00010784459300339222

Training epoch-14 batch-144
Running loss of epoch-14 batch-144 = 0.00013518647756427526

Training epoch-14 batch-145
Running loss of epoch-14 batch-145 = 0.0007309830980375409

Training epoch-14 batch-146
Running loss of epoch-14 batch-146 = 0.00012198765762150288

Training epoch-14 batch-147
Running loss of epoch-14 batch-147 = 0.0013883740175515413

Training epoch-14 batch-148
Running loss of epoch-14 batch-148 = 0.00047073932364583015

Training epoch-14 batch-149
Running loss of epoch-14 batch-149 = 0.000109831802546978

Training epoch-14 batch-150
Running loss of epoch-14 batch-150 = 0.0002493971260264516

Training epoch-14 batch-151
Running loss of epoch-14 batch-151 = 0.00046582031063735485

Training epoch-14 batch-152
Running loss of epoch-14 batch-152 = 0.00035486381966620684

Training epoch-14 batch-153
Running loss of epoch-14 batch-153 = 0.0006198931951075792

Training epoch-14 batch-154
Running loss of epoch-14 batch-154 = 0.000450453138910234

Training epoch-14 batch-155
Running loss of epoch-14 batch-155 = 0.00023720855824649334

Training epoch-14 batch-156
Running loss of epoch-14 batch-156 = 0.000280996086075902

Training epoch-14 batch-157
Running loss of epoch-14 batch-157 = 0.007964501157402992

Finished training epoch-14.



Average train loss at epoch-14 = 0.00032966598942875863

Started Evaluation

Average val loss at epoch-14 = 0.7445711792279326

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 91.48 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 73.08 %

Overall Accuracy = 83.49 %

Finished Evaluation



Started training epoch-15


Training epoch-15 batch-1
Running loss of epoch-15 batch-1 = 0.0004448414547368884

Training epoch-15 batch-2
Running loss of epoch-15 batch-2 = 9.345507714897394e-05

Training epoch-15 batch-3
Running loss of epoch-15 batch-3 = 0.000869864015839994

Training epoch-15 batch-4
Running loss of epoch-15 batch-4 = 8.442869875580072e-05

Training epoch-15 batch-5
Running loss of epoch-15 batch-5 = 0.00027046864852309227

Training epoch-15 batch-6
Running loss of epoch-15 batch-6 = 0.00017613719683140516

Training epoch-15 batch-7
Running loss of epoch-15 batch-7 = 0.00018205447122454643

Training epoch-15 batch-8
Running loss of epoch-15 batch-8 = 0.00023433123715221882

Training epoch-15 batch-9
Running loss of epoch-15 batch-9 = 0.00019855680875480175

Training epoch-15 batch-10
Running loss of epoch-15 batch-10 = 0.0006187362596392632

Training epoch-15 batch-11
Running loss of epoch-15 batch-11 = 0.00018757465295493603

Training epoch-15 batch-12
Running loss of epoch-15 batch-12 = 0.00013998337090015411

Training epoch-15 batch-13
Running loss of epoch-15 batch-13 = 0.00014306732919067144

Training epoch-15 batch-14
Running loss of epoch-15 batch-14 = 8.775200694799423e-05

Training epoch-15 batch-15
Running loss of epoch-15 batch-15 = 0.00012181850615888834

Training epoch-15 batch-16
Running loss of epoch-15 batch-16 = 0.0003493494587019086

Training epoch-15 batch-17
Running loss of epoch-15 batch-17 = 0.0011358532356098294

Training epoch-15 batch-18
Running loss of epoch-15 batch-18 = 0.0001720486907288432

Training epoch-15 batch-19
Running loss of epoch-15 batch-19 = 0.0004264714661985636

Training epoch-15 batch-20
Running loss of epoch-15 batch-20 = 0.0004627993330359459

Training epoch-15 batch-21
Running loss of epoch-15 batch-21 = 0.00014359643682837486

Training epoch-15 batch-22
Running loss of epoch-15 batch-22 = 0.00024260429199784994

Training epoch-15 batch-23
Running loss of epoch-15 batch-23 = 0.0006129236426204443

Training epoch-15 batch-24
Running loss of epoch-15 batch-24 = 0.0005145651521161199

Training epoch-15 batch-25
Running loss of epoch-15 batch-25 = 0.0002018007216975093

Training epoch-15 batch-26
Running loss of epoch-15 batch-26 = 0.00023230724036693573

Training epoch-15 batch-27
Running loss of epoch-15 batch-27 = 0.0001604035496711731

Training epoch-15 batch-28
Running loss of epoch-15 batch-28 = 0.00023049849551171064

Training epoch-15 batch-29
Running loss of epoch-15 batch-29 = 0.00012334424536675215

Training epoch-15 batch-30
Running loss of epoch-15 batch-30 = 6.986188236624002e-05

Training epoch-15 batch-31
Running loss of epoch-15 batch-31 = 4.805403295904398e-05

Training epoch-15 batch-32
Running loss of epoch-15 batch-32 = 4.742085002362728e-05

Training epoch-15 batch-33
Running loss of epoch-15 batch-33 = 0.00031522882636636496

Training epoch-15 batch-34
Running loss of epoch-15 batch-34 = 0.00021605542860925198

Training epoch-15 batch-35
Running loss of epoch-15 batch-35 = 0.00028936576563864946

Training epoch-15 batch-36
Running loss of epoch-15 batch-36 = 0.0002674541901797056

Training epoch-15 batch-37
Running loss of epoch-15 batch-37 = 5.572591908276081e-05

Training epoch-15 batch-38
Running loss of epoch-15 batch-38 = 0.00010909605771303177

Training epoch-15 batch-39
Running loss of epoch-15 batch-39 = 7.396098226308823e-05

Training epoch-15 batch-40
Running loss of epoch-15 batch-40 = 0.000243297778069973

Training epoch-15 batch-41
Running loss of epoch-15 batch-41 = 0.00023321725893765688

Training epoch-15 batch-42
Running loss of epoch-15 batch-42 = 0.00010272604413330555

Training epoch-15 batch-43
Running loss of epoch-15 batch-43 = 5.898764356970787e-05

Training epoch-15 batch-44
Running loss of epoch-15 batch-44 = 9.284692350775003e-05

Training epoch-15 batch-45
Running loss of epoch-15 batch-45 = 0.00014570169150829315

Training epoch-15 batch-46
Running loss of epoch-15 batch-46 = 0.00020636606495827436

Training epoch-15 batch-47
Running loss of epoch-15 batch-47 = 9.53505514189601e-05

Training epoch-15 batch-48
Running loss of epoch-15 batch-48 = 0.00015162955969572067

Training epoch-15 batch-49
Running loss of epoch-15 batch-49 = 0.0002825689734891057

Training epoch-15 batch-50
Running loss of epoch-15 batch-50 = 4.1413004510104656e-05

Training epoch-15 batch-51
Running loss of epoch-15 batch-51 = 0.00012377824168652296

Training epoch-15 batch-52
Running loss of epoch-15 batch-52 = 6.668490823358297e-05

Training epoch-15 batch-53
Running loss of epoch-15 batch-53 = 0.00028856215067207813

Training epoch-15 batch-54
Running loss of epoch-15 batch-54 = 0.00032181257847696543

Training epoch-15 batch-55
Running loss of epoch-15 batch-55 = 0.00014444312546402216

Training epoch-15 batch-56
Running loss of epoch-15 batch-56 = 0.00019678007811307907

Training epoch-15 batch-57
Running loss of epoch-15 batch-57 = 0.00011486618313938379

Training epoch-15 batch-58
Running loss of epoch-15 batch-58 = 0.00026391365099698305

Training epoch-15 batch-59
Running loss of epoch-15 batch-59 = 0.00040709704626351595

Training epoch-15 batch-60
Running loss of epoch-15 batch-60 = 0.0001781294122338295

Training epoch-15 batch-61
Running loss of epoch-15 batch-61 = 0.0003193856682628393

Training epoch-15 batch-62
Running loss of epoch-15 batch-62 = 0.00014905782882124186

Training epoch-15 batch-63
Running loss of epoch-15 batch-63 = 0.0010877656750380993

Training epoch-15 batch-64
Running loss of epoch-15 batch-64 = 9.961903560906649e-05

Training epoch-15 batch-65
Running loss of epoch-15 batch-65 = 0.00023554288782179356

Training epoch-15 batch-66
Running loss of epoch-15 batch-66 = 0.0001148035516962409

Training epoch-15 batch-67
Running loss of epoch-15 batch-67 = 0.0003200770588591695

Training epoch-15 batch-68
Running loss of epoch-15 batch-68 = 7.961178198456764e-05

Training epoch-15 batch-69
Running loss of epoch-15 batch-69 = 0.00020892114844173193

Training epoch-15 batch-70
Running loss of epoch-15 batch-70 = 0.00021405378356575966

Training epoch-15 batch-71
Running loss of epoch-15 batch-71 = 7.358822040259838e-05

Training epoch-15 batch-72
Running loss of epoch-15 batch-72 = 0.00015789817553013563

Training epoch-15 batch-73
Running loss of epoch-15 batch-73 = 0.00011564604938030243

Training epoch-15 batch-74
Running loss of epoch-15 batch-74 = 0.0008416849886998534

Training epoch-15 batch-75
Running loss of epoch-15 batch-75 = 0.000581003725528717

Training epoch-15 batch-76
Running loss of epoch-15 batch-76 = 8.307083044201136e-05

Training epoch-15 batch-77
Running loss of epoch-15 batch-77 = 0.00016061950009316206

Training epoch-15 batch-78
Running loss of epoch-15 batch-78 = 8.883955888450146e-05

Training epoch-15 batch-79
Running loss of epoch-15 batch-79 = 0.0002030084142461419

Training epoch-15 batch-80
Running loss of epoch-15 batch-80 = 0.0003827604232355952

Training epoch-15 batch-81
Running loss of epoch-15 batch-81 = 5.3987838327884674e-05

Training epoch-15 batch-82
Running loss of epoch-15 batch-82 = 0.00015801156405359507

Training epoch-15 batch-83
Running loss of epoch-15 batch-83 = 0.00011048896703869104

Training epoch-15 batch-84
Running loss of epoch-15 batch-84 = 0.0003243776736781001

Training epoch-15 batch-85
Running loss of epoch-15 batch-85 = 0.00011139200069010258

Training epoch-15 batch-86
Running loss of epoch-15 batch-86 = 0.00027190835680812597

Training epoch-15 batch-87
Running loss of epoch-15 batch-87 = 0.00019800569862127304

Training epoch-15 batch-88
Running loss of epoch-15 batch-88 = 5.903246346861124e-05

Training epoch-15 batch-89
Running loss of epoch-15 batch-89 = 0.0002518423134461045

Training epoch-15 batch-90
Running loss of epoch-15 batch-90 = 0.00013797113206237555

Training epoch-15 batch-91
Running loss of epoch-15 batch-91 = 0.0007694700034335256

Training epoch-15 batch-92
Running loss of epoch-15 batch-92 = 0.0001552157336845994

Training epoch-15 batch-93
Running loss of epoch-15 batch-93 = 7.828394882380962e-05

Training epoch-15 batch-94
Running loss of epoch-15 batch-94 = 0.00023705954663455486

Training epoch-15 batch-95
Running loss of epoch-15 batch-95 = 0.0001773760886862874

Training epoch-15 batch-96
Running loss of epoch-15 batch-96 = 0.0004845898365601897

Training epoch-15 batch-97
Running loss of epoch-15 batch-97 = 0.00022768171038478613

Training epoch-15 batch-98
Running loss of epoch-15 batch-98 = 0.0004031779244542122

Training epoch-15 batch-99
Running loss of epoch-15 batch-99 = 0.00023449398577213287

Training epoch-15 batch-100
Running loss of epoch-15 batch-100 = 0.0010966878617182374

Training epoch-15 batch-101
Running loss of epoch-15 batch-101 = 0.0002061051782220602

Training epoch-15 batch-102
Running loss of epoch-15 batch-102 = 0.0002853741170838475

Training epoch-15 batch-103
Running loss of epoch-15 batch-103 = 0.00023318501189351082

Training epoch-15 batch-104
Running loss of epoch-15 batch-104 = 0.00034180283546447754

Training epoch-15 batch-105
Running loss of epoch-15 batch-105 = 0.00034782185684889555

Training epoch-15 batch-106
Running loss of epoch-15 batch-106 = 0.000335914664901793

Training epoch-15 batch-107
Running loss of epoch-15 batch-107 = 0.00018081662710756063

Training epoch-15 batch-108
Running loss of epoch-15 batch-108 = 0.00033492303919047117

Training epoch-15 batch-109
Running loss of epoch-15 batch-109 = 0.00013228959869593382

Training epoch-15 batch-110
Running loss of epoch-15 batch-110 = 0.0001819393364712596

Training epoch-15 batch-111
Running loss of epoch-15 batch-111 = 0.00014441530220210552

Training epoch-15 batch-112
Running loss of epoch-15 batch-112 = 0.00041800213512033224

Training epoch-15 batch-113
Running loss of epoch-15 batch-113 = 0.00014063797425478697

Training epoch-15 batch-114
Running loss of epoch-15 batch-114 = 0.00014005263801664114

Training epoch-15 batch-115
Running loss of epoch-15 batch-115 = 0.0008439039811491966

Training epoch-15 batch-116
Running loss of epoch-15 batch-116 = 0.00046332040801644325

Training epoch-15 batch-117
Running loss of epoch-15 batch-117 = 0.0002567667979747057

Training epoch-15 batch-118
Running loss of epoch-15 batch-118 = 0.00018569803796708584

Training epoch-15 batch-119
Running loss of epoch-15 batch-119 = 0.0003462492022663355

Training epoch-15 batch-120
Running loss of epoch-15 batch-120 = 0.00035981088876724243

Training epoch-15 batch-121
Running loss of epoch-15 batch-121 = 0.00043621950317174196

Training epoch-15 batch-122
Running loss of epoch-15 batch-122 = 0.0006480677984654903

Training epoch-15 batch-123
Running loss of epoch-15 batch-123 = 0.00016375456470996141

Training epoch-15 batch-124
Running loss of epoch-15 batch-124 = 0.00014834280591458082

Training epoch-15 batch-125
Running loss of epoch-15 batch-125 = 0.0003545926883816719

Training epoch-15 batch-126
Running loss of epoch-15 batch-126 = 0.00032956781797111034

Training epoch-15 batch-127
Running loss of epoch-15 batch-127 = 0.00014311005361378193

Training epoch-15 batch-128
Running loss of epoch-15 batch-128 = 0.00027728022541850805

Training epoch-15 batch-129
Running loss of epoch-15 batch-129 = 0.00011452683247625828

Training epoch-15 batch-130
Running loss of epoch-15 batch-130 = 0.0002358983037993312

Training epoch-15 batch-131
Running loss of epoch-15 batch-131 = 0.0004472790751606226

Training epoch-15 batch-132
Running loss of epoch-15 batch-132 = 9.85199585556984e-05

Training epoch-15 batch-133
Running loss of epoch-15 batch-133 = 0.0003407836193218827

Training epoch-15 batch-134
Running loss of epoch-15 batch-134 = 0.00021233537700027227

Training epoch-15 batch-135
Running loss of epoch-15 batch-135 = 0.000554080936126411

Training epoch-15 batch-136
Running loss of epoch-15 batch-136 = 0.00016614282503724098

Training epoch-15 batch-137
Running loss of epoch-15 batch-137 = 0.00011148687917739153

Training epoch-15 batch-138
Running loss of epoch-15 batch-138 = 0.00023027881979942322

Training epoch-15 batch-139
Running loss of epoch-15 batch-139 = 0.00010529777500778437

Training epoch-15 batch-140
Running loss of epoch-15 batch-140 = 0.0005346178077161312

Training epoch-15 batch-141
Running loss of epoch-15 batch-141 = 4.6503497287631035e-05

Training epoch-15 batch-142
Running loss of epoch-15 batch-142 = 0.0010583584662526846

Training epoch-15 batch-143
Running loss of epoch-15 batch-143 = 0.00024972809478640556

Training epoch-15 batch-144
Running loss of epoch-15 batch-144 = 0.00018338405061513186

Training epoch-15 batch-145
Running loss of epoch-15 batch-145 = 4.4056796468794346e-05

Training epoch-15 batch-146
Running loss of epoch-15 batch-146 = 0.00045407633297145367

Training epoch-15 batch-147
Running loss of epoch-15 batch-147 = 0.0005139476852491498

Training epoch-15 batch-148
Running loss of epoch-15 batch-148 = 0.00014992523938417435

Training epoch-15 batch-149
Running loss of epoch-15 batch-149 = 0.0001707688206806779

Training epoch-15 batch-150
Running loss of epoch-15 batch-150 = 0.00030449090991169214

Training epoch-15 batch-151
Running loss of epoch-15 batch-151 = 0.0003725665155798197

Training epoch-15 batch-152
Running loss of epoch-15 batch-152 = 0.0006039476720616221

Training epoch-15 batch-153
Running loss of epoch-15 batch-153 = 0.00023336801677942276

Training epoch-15 batch-154
Running loss of epoch-15 batch-154 = 0.00026979466201737523

Training epoch-15 batch-155
Running loss of epoch-15 batch-155 = 0.00038081256207078695

Training epoch-15 batch-156
Running loss of epoch-15 batch-156 = 0.00013153045438230038

Training epoch-15 batch-157
Running loss of epoch-15 batch-157 = 0.00011099874973297119

Finished training epoch-15.



Average train loss at epoch-15 = 0.00026814891658723353

Started Evaluation

Average val loss at epoch-15 = 0.7419560244843045

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 90.82 %
Accuracy for class onCreate is: 92.64 %
Accuracy for class toString is: 89.76 %
Accuracy for class run is: 70.78 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 59.19 %
Accuracy for class execute is: 36.55 %
Accuracy for class get is: 73.85 %

Overall Accuracy = 84.03 %


Best Accuracy = 84.03 % at Epoch-15
Saving model after best epoch-15

Finished Evaluation



Started training epoch-16


Training epoch-16 batch-1
Running loss of epoch-16 batch-1 = 0.0010165271814912558

Training epoch-16 batch-2
Running loss of epoch-16 batch-2 = 6.289128214120865e-05

Training epoch-16 batch-3
Running loss of epoch-16 batch-3 = 0.00012304820120334625

Training epoch-16 batch-4
Running loss of epoch-16 batch-4 = 9.046075865626335e-05

Training epoch-16 batch-5
Running loss of epoch-16 batch-5 = 0.0003302997210994363

Training epoch-16 batch-6
Running loss of epoch-16 batch-6 = 0.00011386082042008638

Training epoch-16 batch-7
Running loss of epoch-16 batch-7 = 0.00010634027421474457

Training epoch-16 batch-8
Running loss of epoch-16 batch-8 = 0.00010228529572486877

Training epoch-16 batch-9
Running loss of epoch-16 batch-9 = 0.00056076655164361

Training epoch-16 batch-10
Running loss of epoch-16 batch-10 = 0.0002359363716095686

Training epoch-16 batch-11
Running loss of epoch-16 batch-11 = 0.00037289131432771683

Training epoch-16 batch-12
Running loss of epoch-16 batch-12 = 0.00016450718976557255

Training epoch-16 batch-13
Running loss of epoch-16 batch-13 = 0.0001853625290095806

Training epoch-16 batch-14
Running loss of epoch-16 batch-14 = 0.0001337685389444232

Training epoch-16 batch-15
Running loss of epoch-16 batch-15 = 0.00014321820344775915

Training epoch-16 batch-16
Running loss of epoch-16 batch-16 = 6.119860336184502e-05

Training epoch-16 batch-17
Running loss of epoch-16 batch-17 = 0.00025454582646489143

Training epoch-16 batch-18
Running loss of epoch-16 batch-18 = 2.9996037483215332e-05

Training epoch-16 batch-19
Running loss of epoch-16 batch-19 = 0.0004129603039473295

Training epoch-16 batch-20
Running loss of epoch-16 batch-20 = 9.68442764133215e-05

Training epoch-16 batch-21
Running loss of epoch-16 batch-21 = 6.845023017376661e-05

Training epoch-16 batch-22
Running loss of epoch-16 batch-22 = 0.0001592931803315878

Training epoch-16 batch-23
Running loss of epoch-16 batch-23 = 0.00014540262054651976

Training epoch-16 batch-24
Running loss of epoch-16 batch-24 = 0.0002104592276737094

Training epoch-16 batch-25
Running loss of epoch-16 batch-25 = 6.256811320781708e-05

Training epoch-16 batch-26
Running loss of epoch-16 batch-26 = 8.876901119947433e-05

Training epoch-16 batch-27
Running loss of epoch-16 batch-27 = 0.0001340706367045641

Training epoch-16 batch-28
Running loss of epoch-16 batch-28 = 0.0001523077953606844

Training epoch-16 batch-29
Running loss of epoch-16 batch-29 = 0.00016727345064282417

Training epoch-16 batch-30
Running loss of epoch-16 batch-30 = 0.00010946020483970642

Training epoch-16 batch-31
Running loss of epoch-16 batch-31 = 0.00027973006945103407

Training epoch-16 batch-32
Running loss of epoch-16 batch-32 = 5.579914432018995e-05

Training epoch-16 batch-33
Running loss of epoch-16 batch-33 = 4.402431659400463e-05

Training epoch-16 batch-34
Running loss of epoch-16 batch-34 = 0.0006166545208543539

Training epoch-16 batch-35
Running loss of epoch-16 batch-35 = 7.668882608413696e-05

Training epoch-16 batch-36
Running loss of epoch-16 batch-36 = 6.522214971482754e-05

Training epoch-16 batch-37
Running loss of epoch-16 batch-37 = 0.0002025774447247386

Training epoch-16 batch-38
Running loss of epoch-16 batch-38 = 0.00028695561923086643

Training epoch-16 batch-39
Running loss of epoch-16 batch-39 = 5.6342221796512604e-05

Training epoch-16 batch-40
Running loss of epoch-16 batch-40 = 0.0001228773035109043

Training epoch-16 batch-41
Running loss of epoch-16 batch-41 = 7.23517732694745e-05

Training epoch-16 batch-42
Running loss of epoch-16 batch-42 = 0.00017991848289966583

Training epoch-16 batch-43
Running loss of epoch-16 batch-43 = 4.6421075239777565e-05

Training epoch-16 batch-44
Running loss of epoch-16 batch-44 = 4.923739470541477e-05

Training epoch-16 batch-45
Running loss of epoch-16 batch-45 = 0.00018072454258799553

Training epoch-16 batch-46
Running loss of epoch-16 batch-46 = 5.941372364759445e-05

Training epoch-16 batch-47
Running loss of epoch-16 batch-47 = 0.00012699561193585396

Training epoch-16 batch-48
Running loss of epoch-16 batch-48 = 6.220955401659012e-05

Training epoch-16 batch-49
Running loss of epoch-16 batch-49 = 9.990646503865719e-05

Training epoch-16 batch-50
Running loss of epoch-16 batch-50 = 4.1474588215351105e-05

Training epoch-16 batch-51
Running loss of epoch-16 batch-51 = 6.962928455322981e-05

Training epoch-16 batch-52
Running loss of epoch-16 batch-52 = 0.00011445337440818548

Training epoch-16 batch-53
Running loss of epoch-16 batch-53 = 0.00010070146527141333

Training epoch-16 batch-54
Running loss of epoch-16 batch-54 = 6.041373126208782e-05

Training epoch-16 batch-55
Running loss of epoch-16 batch-55 = 7.317622657865286e-05

Training epoch-16 batch-56
Running loss of epoch-16 batch-56 = 0.00014971650671213865

Training epoch-16 batch-57
Running loss of epoch-16 batch-57 = 8.180597797036171e-05

Training epoch-16 batch-58
Running loss of epoch-16 batch-58 = 0.00011368724517524242

Training epoch-16 batch-59
Running loss of epoch-16 batch-59 = 7.187516894191504e-05

Training epoch-16 batch-60
Running loss of epoch-16 batch-60 = 0.00041409453842788935

Training epoch-16 batch-61
Running loss of epoch-16 batch-61 = 7.323385216295719e-05

Training epoch-16 batch-62
Running loss of epoch-16 batch-62 = 0.00013557937927544117

Training epoch-16 batch-63
Running loss of epoch-16 batch-63 = 0.0006692681927233934

Training epoch-16 batch-64
Running loss of epoch-16 batch-64 = 0.00013553968165069818

Training epoch-16 batch-65
Running loss of epoch-16 batch-65 = 0.00014702265616506338

Training epoch-16 batch-66
Running loss of epoch-16 batch-66 = 7.460510823875666e-05

Training epoch-16 batch-67
Running loss of epoch-16 batch-67 = 0.0003356820670887828

Training epoch-16 batch-68
Running loss of epoch-16 batch-68 = 0.00032631796784698963

Training epoch-16 batch-69
Running loss of epoch-16 batch-69 = 6.064004264771938e-05

Training epoch-16 batch-70
Running loss of epoch-16 batch-70 = 8.175813127309084e-05

Training epoch-16 batch-71
Running loss of epoch-16 batch-71 = 0.0002781940856948495

Training epoch-16 batch-72
Running loss of epoch-16 batch-72 = 0.00012415449600666761

Training epoch-16 batch-73
Running loss of epoch-16 batch-73 = 0.00020375358872115612

Training epoch-16 batch-74
Running loss of epoch-16 batch-74 = 8.546537719666958e-05

Training epoch-16 batch-75
Running loss of epoch-16 batch-75 = 0.0001610959880053997

Training epoch-16 batch-76
Running loss of epoch-16 batch-76 = 0.00020105682779103518

Training epoch-16 batch-77
Running loss of epoch-16 batch-77 = 0.00024102802854031324

Training epoch-16 batch-78
Running loss of epoch-16 batch-78 = 0.00011988065671175718

Training epoch-16 batch-79
Running loss of epoch-16 batch-79 = 8.358387276530266e-05

Training epoch-16 batch-80
Running loss of epoch-16 batch-80 = 8.840789087116718e-05

Training epoch-16 batch-81
Running loss of epoch-16 batch-81 = 0.00012002699077129364

Training epoch-16 batch-82
Running loss of epoch-16 batch-82 = 0.00015072687529027462

Training epoch-16 batch-83
Running loss of epoch-16 batch-83 = 0.00021722179371863604

Training epoch-16 batch-84
Running loss of epoch-16 batch-84 = 0.000385296531021595

Training epoch-16 batch-85
Running loss of epoch-16 batch-85 = 0.00042165047489106655

Training epoch-16 batch-86
Running loss of epoch-16 batch-86 = 5.283229984343052e-05

Training epoch-16 batch-87
Running loss of epoch-16 batch-87 = 0.00029843172524124384

Training epoch-16 batch-88
Running loss of epoch-16 batch-88 = 0.0001655801897868514

Training epoch-16 batch-89
Running loss of epoch-16 batch-89 = 0.00024385459255427122

Training epoch-16 batch-90
Running loss of epoch-16 batch-90 = 0.00012502167373895645

Training epoch-16 batch-91
Running loss of epoch-16 batch-91 = 0.0008073392091318965

Training epoch-16 batch-92
Running loss of epoch-16 batch-92 = 0.00020077044609934092

Training epoch-16 batch-93
Running loss of epoch-16 batch-93 = 0.0006405093008652329

Training epoch-16 batch-94
Running loss of epoch-16 batch-94 = 0.0001813252456486225

Training epoch-16 batch-95
Running loss of epoch-16 batch-95 = 0.00013115431647747755

Training epoch-16 batch-96
Running loss of epoch-16 batch-96 = 0.00018580257892608643

Training epoch-16 batch-97
Running loss of epoch-16 batch-97 = 0.00023185485042631626

Training epoch-16 batch-98
Running loss of epoch-16 batch-98 = 0.0002335545141249895

Training epoch-16 batch-99
Running loss of epoch-16 batch-99 = 6.265065167099237e-05

Training epoch-16 batch-100
Running loss of epoch-16 batch-100 = 0.00027746474370360374

Training epoch-16 batch-101
Running loss of epoch-16 batch-101 = 0.00014397874474525452

Training epoch-16 batch-102
Running loss of epoch-16 batch-102 = 0.00031276338268071413

Training epoch-16 batch-103
Running loss of epoch-16 batch-103 = 2.819439396262169e-05

Training epoch-16 batch-104
Running loss of epoch-16 batch-104 = 9.898107964545488e-05

Training epoch-16 batch-105
Running loss of epoch-16 batch-105 = 0.0002518274122849107

Training epoch-16 batch-106
Running loss of epoch-16 batch-106 = 0.00014685641508549452

Training epoch-16 batch-107
Running loss of epoch-16 batch-107 = 0.00012672098819166422

Training epoch-16 batch-108
Running loss of epoch-16 batch-108 = 9.584333747625351e-05

Training epoch-16 batch-109
Running loss of epoch-16 batch-109 = 7.711793296039104e-05

Training epoch-16 batch-110
Running loss of epoch-16 batch-110 = 0.00025400612503290176

Training epoch-16 batch-111
Running loss of epoch-16 batch-111 = 0.0002948896726593375

Training epoch-16 batch-112
Running loss of epoch-16 batch-112 = 8.736050222069025e-05

Training epoch-16 batch-113
Running loss of epoch-16 batch-113 = 0.0001639809925109148

Training epoch-16 batch-114
Running loss of epoch-16 batch-114 = 0.0001961638918146491

Training epoch-16 batch-115
Running loss of epoch-16 batch-115 = 0.0003765253350138664

Training epoch-16 batch-116
Running loss of epoch-16 batch-116 = 0.00024117191787809134

Training epoch-16 batch-117
Running loss of epoch-16 batch-117 = 0.00013458530884236097

Training epoch-16 batch-118
Running loss of epoch-16 batch-118 = 0.00010809337254613638

Training epoch-16 batch-119
Running loss of epoch-16 batch-119 = 0.00020179501734673977

Training epoch-16 batch-120
Running loss of epoch-16 batch-120 = 0.00011449144221842289

Training epoch-16 batch-121
Running loss of epoch-16 batch-121 = 6.171385757625103e-05

Training epoch-16 batch-122
Running loss of epoch-16 batch-122 = 0.00013269553892314434

Training epoch-16 batch-123
Running loss of epoch-16 batch-123 = 9.15004638954997e-05

Training epoch-16 batch-124
Running loss of epoch-16 batch-124 = 0.00017743417993187904

Training epoch-16 batch-125
Running loss of epoch-16 batch-125 = 7.863692007958889e-05

Training epoch-16 batch-126
Running loss of epoch-16 batch-126 = 0.0003420114517211914

Training epoch-16 batch-127
Running loss of epoch-16 batch-127 = 6.022921297699213e-05

Training epoch-16 batch-128
Running loss of epoch-16 batch-128 = 8.649309165775776e-05

Training epoch-16 batch-129
Running loss of epoch-16 batch-129 = 0.00015577301383018494

Training epoch-16 batch-130
Running loss of epoch-16 batch-130 = 8.179049473255873e-05

Training epoch-16 batch-131
Running loss of epoch-16 batch-131 = 0.00011185032781213522

Training epoch-16 batch-132
Running loss of epoch-16 batch-132 = 0.0001808138331398368

Training epoch-16 batch-133
Running loss of epoch-16 batch-133 = 6.644474342465401e-05

Training epoch-16 batch-134
Running loss of epoch-16 batch-134 = 7.663259748369455e-05

Training epoch-16 batch-135
Running loss of epoch-16 batch-135 = 4.84163174405694e-05

Training epoch-16 batch-136
Running loss of epoch-16 batch-136 = 0.00019718159455806017

Training epoch-16 batch-137
Running loss of epoch-16 batch-137 = 0.00011317979078739882

Training epoch-16 batch-138
Running loss of epoch-16 batch-138 = 0.00011251482646912336

Training epoch-16 batch-139
Running loss of epoch-16 batch-139 = 4.392210394144058e-05

Training epoch-16 batch-140
Running loss of epoch-16 batch-140 = 7.229438051581383e-05

Training epoch-16 batch-141
Running loss of epoch-16 batch-141 = 8.571019861847162e-05

Training epoch-16 batch-142
Running loss of epoch-16 batch-142 = 0.00010759872384369373

Training epoch-16 batch-143
Running loss of epoch-16 batch-143 = 0.00012178358156234026

Training epoch-16 batch-144
Running loss of epoch-16 batch-144 = 6.453110836446285e-05

Training epoch-16 batch-145
Running loss of epoch-16 batch-145 = 0.00014019303489476442

Training epoch-16 batch-146
Running loss of epoch-16 batch-146 = 0.00014229165390133858

Training epoch-16 batch-147
Running loss of epoch-16 batch-147 = 1.590640749782324e-05

Training epoch-16 batch-148
Running loss of epoch-16 batch-148 = 0.00018187100067734718

Training epoch-16 batch-149
Running loss of epoch-16 batch-149 = 0.00026878586504608393

Training epoch-16 batch-150
Running loss of epoch-16 batch-150 = 0.00013008457608520985

Training epoch-16 batch-151
Running loss of epoch-16 batch-151 = 7.958174683153629e-05

Training epoch-16 batch-152
Running loss of epoch-16 batch-152 = 8.082727435976267e-05

Training epoch-16 batch-153
Running loss of epoch-16 batch-153 = 0.00015451223589479923

Training epoch-16 batch-154
Running loss of epoch-16 batch-154 = 0.00023873394820839167

Training epoch-16 batch-155
Running loss of epoch-16 batch-155 = 0.00018824299331754446

Training epoch-16 batch-156
Running loss of epoch-16 batch-156 = 0.00010280637070536613

Training epoch-16 batch-157
Running loss of epoch-16 batch-157 = 0.00032784976065158844

Finished training epoch-16.



Average train loss at epoch-16 = 0.000169999860227108

Started Evaluation

Average val loss at epoch-16 = 0.8028351779030345

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 86.72 %
Accuracy for class onCreate is: 93.18 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 59.36 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 60.99 %
Accuracy for class execute is: 46.59 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.08 %

Finished Evaluation



Started training epoch-17


Training epoch-17 batch-1
Running loss of epoch-17 batch-1 = 7.127888966351748e-05

Training epoch-17 batch-2
Running loss of epoch-17 batch-2 = 7.819337770342827e-05

Training epoch-17 batch-3
Running loss of epoch-17 batch-3 = 0.00013902015052735806

Training epoch-17 batch-4
Running loss of epoch-17 batch-4 = 0.00020692870020866394

Training epoch-17 batch-5
Running loss of epoch-17 batch-5 = 0.00010532676242291927

Training epoch-17 batch-6
Running loss of epoch-17 batch-6 = 4.3462030589580536e-05

Training epoch-17 batch-7
Running loss of epoch-17 batch-7 = 0.00010367773938924074

Training epoch-17 batch-8
Running loss of epoch-17 batch-8 = 0.00012534146662801504

Training epoch-17 batch-9
Running loss of epoch-17 batch-9 = 0.00031717971432954073

Training epoch-17 batch-10
Running loss of epoch-17 batch-10 = 3.898551221936941e-05

Training epoch-17 batch-11
Running loss of epoch-17 batch-11 = 5.791441071778536e-05

Training epoch-17 batch-12
Running loss of epoch-17 batch-12 = 8.712871931493282e-05

Training epoch-17 batch-13
Running loss of epoch-17 batch-13 = 0.00014517537783831358

Training epoch-17 batch-14
Running loss of epoch-17 batch-14 = 0.00018721586093306541

Training epoch-17 batch-15
Running loss of epoch-17 batch-15 = 0.00011065416038036346

Training epoch-17 batch-16
Running loss of epoch-17 batch-16 = 9.26790526136756e-05

Training epoch-17 batch-17
Running loss of epoch-17 batch-17 = 0.00023750204127281904

Training epoch-17 batch-18
Running loss of epoch-17 batch-18 = 6.439152639359236e-05

Training epoch-17 batch-19
Running loss of epoch-17 batch-19 = 0.00026020139921456575

Training epoch-17 batch-20
Running loss of epoch-17 batch-20 = 0.00010451872367411852

Training epoch-17 batch-21
Running loss of epoch-17 batch-21 = 0.00013577321078628302

Training epoch-17 batch-22
Running loss of epoch-17 batch-22 = 0.0001494457246735692

Training epoch-17 batch-23
Running loss of epoch-17 batch-23 = 4.733819514513016e-05

Training epoch-17 batch-24
Running loss of epoch-17 batch-24 = 8.777482435107231e-05

Training epoch-17 batch-25
Running loss of epoch-17 batch-25 = 0.00012291292659938335

Training epoch-17 batch-26
Running loss of epoch-17 batch-26 = 0.0001162281259894371

Training epoch-17 batch-27
Running loss of epoch-17 batch-27 = 0.0008155168034136295

Training epoch-17 batch-28
Running loss of epoch-17 batch-28 = 3.8575148209929466e-05

Training epoch-17 batch-29
Running loss of epoch-17 batch-29 = 0.00011721521150320768

Training epoch-17 batch-30
Running loss of epoch-17 batch-30 = 6.319384556263685e-05

Training epoch-17 batch-31
Running loss of epoch-17 batch-31 = 0.00017865723930299282

Training epoch-17 batch-32
Running loss of epoch-17 batch-32 = 0.00010483257938176394

Training epoch-17 batch-33
Running loss of epoch-17 batch-33 = 0.00017968250904232264

Training epoch-17 batch-34
Running loss of epoch-17 batch-34 = 0.00010951014701277018

Training epoch-17 batch-35
Running loss of epoch-17 batch-35 = 3.761181142181158e-05

Training epoch-17 batch-36
Running loss of epoch-17 batch-36 = 6.27095578238368e-05

Training epoch-17 batch-37
Running loss of epoch-17 batch-37 = 4.2356899939477444e-05

Training epoch-17 batch-38
Running loss of epoch-17 batch-38 = 7.592304609715939e-05

Training epoch-17 batch-39
Running loss of epoch-17 batch-39 = 7.350521627813578e-05

Training epoch-17 batch-40
Running loss of epoch-17 batch-40 = 7.206364534795284e-05

Training epoch-17 batch-41
Running loss of epoch-17 batch-41 = 0.00011314149014651775

Training epoch-17 batch-42
Running loss of epoch-17 batch-42 = 0.0006223453674465418

Training epoch-17 batch-43
Running loss of epoch-17 batch-43 = 0.00017176906112581491

Training epoch-17 batch-44
Running loss of epoch-17 batch-44 = 8.987355977296829e-05

Training epoch-17 batch-45
Running loss of epoch-17 batch-45 = 7.772201206535101e-05

Training epoch-17 batch-46
Running loss of epoch-17 batch-46 = 0.00012223876547068357

Training epoch-17 batch-47
Running loss of epoch-17 batch-47 = 0.0002176270354539156

Training epoch-17 batch-48
Running loss of epoch-17 batch-48 = 7.35818175598979e-05

Training epoch-17 batch-49
Running loss of epoch-17 batch-49 = 0.0002198361326009035

Training epoch-17 batch-50
Running loss of epoch-17 batch-50 = 0.0003550110850483179

Training epoch-17 batch-51
Running loss of epoch-17 batch-51 = 0.00020619400311261415

Training epoch-17 batch-52
Running loss of epoch-17 batch-52 = 0.00018924148753285408

Training epoch-17 batch-53
Running loss of epoch-17 batch-53 = 0.0001516067422926426

Training epoch-17 batch-54
Running loss of epoch-17 batch-54 = 0.00014843628741800785

Training epoch-17 batch-55
Running loss of epoch-17 batch-55 = 0.00010590057354420424

Training epoch-17 batch-56
Running loss of epoch-17 batch-56 = 6.975710857659578e-05

Training epoch-17 batch-57
Running loss of epoch-17 batch-57 = 6.359629333019257e-05

Training epoch-17 batch-58
Running loss of epoch-17 batch-58 = 6.630108691751957e-05

Training epoch-17 batch-59
Running loss of epoch-17 batch-59 = 0.00013085955288261175

Training epoch-17 batch-60
Running loss of epoch-17 batch-60 = 2.9396964237093925e-05

Training epoch-17 batch-61
Running loss of epoch-17 batch-61 = 5.7811615988612175e-05

Training epoch-17 batch-62
Running loss of epoch-17 batch-62 = 0.00022981897927820683

Training epoch-17 batch-63
Running loss of epoch-17 batch-63 = 0.00017764733638614416

Training epoch-17 batch-64
Running loss of epoch-17 batch-64 = 4.6345870941877365e-05

Training epoch-17 batch-65
Running loss of epoch-17 batch-65 = 3.138801548629999e-05

Training epoch-17 batch-66
Running loss of epoch-17 batch-66 = 6.91533787176013e-05

Training epoch-17 batch-67
Running loss of epoch-17 batch-67 = 0.0002382146194577217

Training epoch-17 batch-68
Running loss of epoch-17 batch-68 = 3.4863478504121304e-05

Training epoch-17 batch-69
Running loss of epoch-17 batch-69 = 5.3817988373339176e-05

Training epoch-17 batch-70
Running loss of epoch-17 batch-70 = 5.6507415138185024e-05

Training epoch-17 batch-71
Running loss of epoch-17 batch-71 = 3.4787459298968315e-05

Training epoch-17 batch-72
Running loss of epoch-17 batch-72 = 7.569161243736744e-05

Training epoch-17 batch-73
Running loss of epoch-17 batch-73 = 0.0001451818970963359

Training epoch-17 batch-74
Running loss of epoch-17 batch-74 = 7.085234392434359e-05

Training epoch-17 batch-75
Running loss of epoch-17 batch-75 = 0.00010925554670393467

Training epoch-17 batch-76
Running loss of epoch-17 batch-76 = 7.435842417180538e-05

Training epoch-17 batch-77
Running loss of epoch-17 batch-77 = 0.0001689885975793004

Training epoch-17 batch-78
Running loss of epoch-17 batch-78 = 4.731048829853535e-05

Training epoch-17 batch-79
Running loss of epoch-17 batch-79 = 5.3687021136283875e-05

Training epoch-17 batch-80
Running loss of epoch-17 batch-80 = 0.0001272503286600113

Training epoch-17 batch-81
Running loss of epoch-17 batch-81 = 6.081676110625267e-05

Training epoch-17 batch-82
Running loss of epoch-17 batch-82 = 4.315003752708435e-05

Training epoch-17 batch-83
Running loss of epoch-17 batch-83 = 0.0008017163490876555

Training epoch-17 batch-84
Running loss of epoch-17 batch-84 = 8.478097151964903e-05

Training epoch-17 batch-85
Running loss of epoch-17 batch-85 = 0.00013625540304929018

Training epoch-17 batch-86
Running loss of epoch-17 batch-86 = 8.779414929449558e-05

Training epoch-17 batch-87
Running loss of epoch-17 batch-87 = 0.0001629367470741272

Training epoch-17 batch-88
Running loss of epoch-17 batch-88 = 0.0002018880331888795

Training epoch-17 batch-89
Running loss of epoch-17 batch-89 = 0.00011114112567156553

Training epoch-17 batch-90
Running loss of epoch-17 batch-90 = 0.00015395577065646648

Training epoch-17 batch-91
Running loss of epoch-17 batch-91 = 8.167652413249016e-05

Training epoch-17 batch-92
Running loss of epoch-17 batch-92 = 9.94496513158083e-05

Training epoch-17 batch-93
Running loss of epoch-17 batch-93 = 6.231700535863638e-05

Training epoch-17 batch-94
Running loss of epoch-17 batch-94 = 0.0002840895904228091

Training epoch-17 batch-95
Running loss of epoch-17 batch-95 = 9.774335194379091e-05

Training epoch-17 batch-96
Running loss of epoch-17 batch-96 = 0.0001948352437466383

Training epoch-17 batch-97
Running loss of epoch-17 batch-97 = 0.0003063648473471403

Training epoch-17 batch-98
Running loss of epoch-17 batch-98 = 8.222414180636406e-05

Training epoch-17 batch-99
Running loss of epoch-17 batch-99 = 5.3470139391720295e-05

Training epoch-17 batch-100
Running loss of epoch-17 batch-100 = 7.253920193761587e-05

Training epoch-17 batch-101
Running loss of epoch-17 batch-101 = 0.00022769125644117594

Training epoch-17 batch-102
Running loss of epoch-17 batch-102 = 9.008531924337149e-05

Training epoch-17 batch-103
Running loss of epoch-17 batch-103 = 3.4826574847102165e-05

Training epoch-17 batch-104
Running loss of epoch-17 batch-104 = 4.688452463597059e-05

Training epoch-17 batch-105
Running loss of epoch-17 batch-105 = 0.0005257156444713473

Training epoch-17 batch-106
Running loss of epoch-17 batch-106 = 5.24851493537426e-05

Training epoch-17 batch-107
Running loss of epoch-17 batch-107 = 0.00011760788038372993

Training epoch-17 batch-108
Running loss of epoch-17 batch-108 = 0.00012219080235809088

Training epoch-17 batch-109
Running loss of epoch-17 batch-109 = 8.881592657417059e-05

Training epoch-17 batch-110
Running loss of epoch-17 batch-110 = 0.0002792824525386095

Training epoch-17 batch-111
Running loss of epoch-17 batch-111 = 8.294777944684029e-05

Training epoch-17 batch-112
Running loss of epoch-17 batch-112 = 0.0001736197154968977

Training epoch-17 batch-113
Running loss of epoch-17 batch-113 = 0.0001775100827217102

Training epoch-17 batch-114
Running loss of epoch-17 batch-114 = 5.690031684935093e-05

Training epoch-17 batch-115
Running loss of epoch-17 batch-115 = 0.0003872431116178632

Training epoch-17 batch-116
Running loss of epoch-17 batch-116 = 0.0001059254864230752

Training epoch-17 batch-117
Running loss of epoch-17 batch-117 = 8.762977086007595e-05

Training epoch-17 batch-118
Running loss of epoch-17 batch-118 = 0.00013402930926531553

Training epoch-17 batch-119
Running loss of epoch-17 batch-119 = 5.564303137362003e-05

Training epoch-17 batch-120
Running loss of epoch-17 batch-120 = 6.184540688991547e-05

Training epoch-17 batch-121
Running loss of epoch-17 batch-121 = 0.00028128456324338913

Training epoch-17 batch-122
Running loss of epoch-17 batch-122 = 6.704218685626984e-05

Training epoch-17 batch-123
Running loss of epoch-17 batch-123 = 0.00013633293565362692

Training epoch-17 batch-124
Running loss of epoch-17 batch-124 = 9.840400889515877e-05

Training epoch-17 batch-125
Running loss of epoch-17 batch-125 = 2.0746374502778053e-05

Training epoch-17 batch-126
Running loss of epoch-17 batch-126 = 0.00014555745292454958

Training epoch-17 batch-127
Running loss of epoch-17 batch-127 = 0.00010646367445588112

Training epoch-17 batch-128
Running loss of epoch-17 batch-128 = 5.702720955014229e-05

Training epoch-17 batch-129
Running loss of epoch-17 batch-129 = 7.283419836312532e-05

Training epoch-17 batch-130
Running loss of epoch-17 batch-130 = 7.713667582720518e-05

Training epoch-17 batch-131
Running loss of epoch-17 batch-131 = 9.22126928344369e-05

Training epoch-17 batch-132
Running loss of epoch-17 batch-132 = 0.0002373112365603447

Training epoch-17 batch-133
Running loss of epoch-17 batch-133 = 0.00021288683637976646

Training epoch-17 batch-134
Running loss of epoch-17 batch-134 = 8.13300721347332e-05

Training epoch-17 batch-135
Running loss of epoch-17 batch-135 = 0.00017344835214316845

Training epoch-17 batch-136
Running loss of epoch-17 batch-136 = 0.00022654910571873188

Training epoch-17 batch-137
Running loss of epoch-17 batch-137 = 0.00029347557574510574

Training epoch-17 batch-138
Running loss of epoch-17 batch-138 = 5.976483225822449e-05

Training epoch-17 batch-139
Running loss of epoch-17 batch-139 = 0.00011344847735017538

Training epoch-17 batch-140
Running loss of epoch-17 batch-140 = 0.0002199026057496667

Training epoch-17 batch-141
Running loss of epoch-17 batch-141 = 6.583193317055702e-05

Training epoch-17 batch-142
Running loss of epoch-17 batch-142 = 0.0002491917693987489

Training epoch-17 batch-143
Running loss of epoch-17 batch-143 = 0.00022951269056648016

Training epoch-17 batch-144
Running loss of epoch-17 batch-144 = 0.00015578814782202244

Training epoch-17 batch-145
Running loss of epoch-17 batch-145 = 0.00012468232307583094

Training epoch-17 batch-146
Running loss of epoch-17 batch-146 = 6.150046829134226e-05

Training epoch-17 batch-147
Running loss of epoch-17 batch-147 = 0.00018863985314965248

Training epoch-17 batch-148
Running loss of epoch-17 batch-148 = 0.00019679416436702013

Training epoch-17 batch-149
Running loss of epoch-17 batch-149 = 6.855418905615807e-05

Training epoch-17 batch-150
Running loss of epoch-17 batch-150 = 6.784196011722088e-05

Training epoch-17 batch-151
Running loss of epoch-17 batch-151 = 0.00022174278274178505

Training epoch-17 batch-152
Running loss of epoch-17 batch-152 = 0.00010486692190170288

Training epoch-17 batch-153
Running loss of epoch-17 batch-153 = 0.00014217407442629337

Training epoch-17 batch-154
Running loss of epoch-17 batch-154 = 3.555719740688801e-05

Training epoch-17 batch-155
Running loss of epoch-17 batch-155 = 9.183527436107397e-05

Training epoch-17 batch-156
Running loss of epoch-17 batch-156 = 0.0001308718929067254

Training epoch-17 batch-157
Running loss of epoch-17 batch-157 = 0.0001476667821407318

Finished training epoch-17.



Average train loss at epoch-17 = 0.00013675812557339669

Started Evaluation

Average val loss at epoch-17 = 0.8483483884081963

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 88.85 %
Accuracy for class onCreate is: 92.96 %
Accuracy for class toString is: 87.03 %
Accuracy for class run is: 62.10 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 50.45 %
Accuracy for class execute is: 51.00 %
Accuracy for class get is: 73.33 %

Overall Accuracy = 82.88 %

Finished Evaluation



Started training epoch-18


Training epoch-18 batch-1
Running loss of epoch-18 batch-1 = 0.00018688198179006577

Training epoch-18 batch-2
Running loss of epoch-18 batch-2 = 5.1405164413154125e-05

Training epoch-18 batch-3
Running loss of epoch-18 batch-3 = 9.894347749650478e-05

Training epoch-18 batch-4
Running loss of epoch-18 batch-4 = 6.451783701777458e-05

Training epoch-18 batch-5
Running loss of epoch-18 batch-5 = 6.878061685711145e-05

Training epoch-18 batch-6
Running loss of epoch-18 batch-6 = 0.00010063883382827044

Training epoch-18 batch-7
Running loss of epoch-18 batch-7 = 0.00015409907791763544

Training epoch-18 batch-8
Running loss of epoch-18 batch-8 = 0.0002449937164783478

Training epoch-18 batch-9
Running loss of epoch-18 batch-9 = 8.599634747952223e-05

Training epoch-18 batch-10
Running loss of epoch-18 batch-10 = 4.9245310947299004e-05

Training epoch-18 batch-11
Running loss of epoch-18 batch-11 = 0.0001371544785797596

Training epoch-18 batch-12
Running loss of epoch-18 batch-12 = 7.373071275651455e-05

Training epoch-18 batch-13
Running loss of epoch-18 batch-13 = 3.429723437875509e-05

Training epoch-18 batch-14
Running loss of epoch-18 batch-14 = 3.3494201488792896e-05

Training epoch-18 batch-15
Running loss of epoch-18 batch-15 = 0.00010516168549656868

Training epoch-18 batch-16
Running loss of epoch-18 batch-16 = 4.1915103793144226e-05

Training epoch-18 batch-17
Running loss of epoch-18 batch-17 = 0.00010361208114773035

Training epoch-18 batch-18
Running loss of epoch-18 batch-18 = 9.094295091927052e-05

Training epoch-18 batch-19
Running loss of epoch-18 batch-19 = 7.70248007029295e-05

Training epoch-18 batch-20
Running loss of epoch-18 batch-20 = 4.804693162441254e-05

Training epoch-18 batch-21
Running loss of epoch-18 batch-21 = 4.475691821426153e-05

Training epoch-18 batch-22
Running loss of epoch-18 batch-22 = 0.0007498804479837418

Training epoch-18 batch-23
Running loss of epoch-18 batch-23 = 2.60982196778059e-05

Training epoch-18 batch-24
Running loss of epoch-18 batch-24 = 9.076984133571386e-05

Training epoch-18 batch-25
Running loss of epoch-18 batch-25 = 5.987670738250017e-05

Training epoch-18 batch-26
Running loss of epoch-18 batch-26 = 6.147439125925303e-05

Training epoch-18 batch-27
Running loss of epoch-18 batch-27 = 5.0713191740214825e-05

Training epoch-18 batch-28
Running loss of epoch-18 batch-28 = 0.0001728041097521782

Training epoch-18 batch-29
Running loss of epoch-18 batch-29 = 6.0783582739531994e-05

Training epoch-18 batch-30
Running loss of epoch-18 batch-30 = 0.0002004812704399228

Training epoch-18 batch-31
Running loss of epoch-18 batch-31 = 0.00014387816190719604

Training epoch-18 batch-32
Running loss of epoch-18 batch-32 = 0.00015270698349922895

Training epoch-18 batch-33
Running loss of epoch-18 batch-33 = 4.142883699387312e-05

Training epoch-18 batch-34
Running loss of epoch-18 batch-34 = 2.1573156118392944e-05

Training epoch-18 batch-35
Running loss of epoch-18 batch-35 = 0.00010986765846610069

Training epoch-18 batch-36
Running loss of epoch-18 batch-36 = 8.78780847415328e-05

Training epoch-18 batch-37
Running loss of epoch-18 batch-37 = 0.00018953171093016863

Training epoch-18 batch-38
Running loss of epoch-18 batch-38 = 0.00016315223183482885

Training epoch-18 batch-39
Running loss of epoch-18 batch-39 = 0.0002952942159026861

Training epoch-18 batch-40
Running loss of epoch-18 batch-40 = 0.00010805693455040455

Training epoch-18 batch-41
Running loss of epoch-18 batch-41 = 4.28343191742897e-05

Training epoch-18 batch-42
Running loss of epoch-18 batch-42 = 9.118858724832535e-05

Training epoch-18 batch-43
Running loss of epoch-18 batch-43 = 0.0001922458177432418

Training epoch-18 batch-44
Running loss of epoch-18 batch-44 = 5.8885314501821995e-05

Training epoch-18 batch-45
Running loss of epoch-18 batch-45 = 0.00010567484423518181

Training epoch-18 batch-46
Running loss of epoch-18 batch-46 = 9.49300592765212e-05

Training epoch-18 batch-47
Running loss of epoch-18 batch-47 = 8.965120650827885e-05

Training epoch-18 batch-48
Running loss of epoch-18 batch-48 = 0.00014502147678285837

Training epoch-18 batch-49
Running loss of epoch-18 batch-49 = 2.9756920412182808e-05

Training epoch-18 batch-50
Running loss of epoch-18 batch-50 = 9.08892834559083e-05

Training epoch-18 batch-51
Running loss of epoch-18 batch-51 = 0.0001476978650316596

Training epoch-18 batch-52
Running loss of epoch-18 batch-52 = 0.00015043409075587988

Training epoch-18 batch-53
Running loss of epoch-18 batch-53 = 8.528283797204494e-05

Training epoch-18 batch-54
Running loss of epoch-18 batch-54 = 0.0001224856823682785

Training epoch-18 batch-55
Running loss of epoch-18 batch-55 = 0.00024241360370069742

Training epoch-18 batch-56
Running loss of epoch-18 batch-56 = 4.569056909531355e-05

Training epoch-18 batch-57
Running loss of epoch-18 batch-57 = 4.701747093349695e-05

Training epoch-18 batch-58
Running loss of epoch-18 batch-58 = 3.5838689655065536e-05

Training epoch-18 batch-59
Running loss of epoch-18 batch-59 = 4.787812940776348e-05

Training epoch-18 batch-60
Running loss of epoch-18 batch-60 = 0.00012063339818269014

Training epoch-18 batch-61
Running loss of epoch-18 batch-61 = 7.145630661398172e-05

Training epoch-18 batch-62
Running loss of epoch-18 batch-62 = 8.168094791471958e-05

Training epoch-18 batch-63
Running loss of epoch-18 batch-63 = 4.378287121653557e-05

Training epoch-18 batch-64
Running loss of epoch-18 batch-64 = 5.9558311477303505e-05

Training epoch-18 batch-65
Running loss of epoch-18 batch-65 = 7.671851199120283e-05

Training epoch-18 batch-66
Running loss of epoch-18 batch-66 = 5.5464799515902996e-05

Training epoch-18 batch-67
Running loss of epoch-18 batch-67 = 0.0004247050965204835

Training epoch-18 batch-68
Running loss of epoch-18 batch-68 = 0.0001047676196321845

Training epoch-18 batch-69
Running loss of epoch-18 batch-69 = 7.167365401983261e-05

Training epoch-18 batch-70
Running loss of epoch-18 batch-70 = 9.536370635032654e-05

Training epoch-18 batch-71
Running loss of epoch-18 batch-71 = 8.580123540014029e-05

Training epoch-18 batch-72
Running loss of epoch-18 batch-72 = 0.00023456860799342394

Training epoch-18 batch-73
Running loss of epoch-18 batch-73 = 0.00010499265044927597

Training epoch-18 batch-74
Running loss of epoch-18 batch-74 = 8.54905229061842e-05

Training epoch-18 batch-75
Running loss of epoch-18 batch-75 = 7.962225936353207e-05

Training epoch-18 batch-76
Running loss of epoch-18 batch-76 = 0.0001308494247496128

Training epoch-18 batch-77
Running loss of epoch-18 batch-77 = 5.040771793574095e-05

Training epoch-18 batch-78
Running loss of epoch-18 batch-78 = 0.0002872259356081486

Training epoch-18 batch-79
Running loss of epoch-18 batch-79 = 6.710388697683811e-05

Training epoch-18 batch-80
Running loss of epoch-18 batch-80 = 0.00021486100740730762

Training epoch-18 batch-81
Running loss of epoch-18 batch-81 = 7.338938303291798e-05

Training epoch-18 batch-82
Running loss of epoch-18 batch-82 = 5.13644190505147e-05

Training epoch-18 batch-83
Running loss of epoch-18 batch-83 = 0.00015112850815057755

Training epoch-18 batch-84
Running loss of epoch-18 batch-84 = 3.4902943298220634e-05

Training epoch-18 batch-85
Running loss of epoch-18 batch-85 = 3.323087003082037e-05

Training epoch-18 batch-86
Running loss of epoch-18 batch-86 = 0.00035051966551691294

Training epoch-18 batch-87
Running loss of epoch-18 batch-87 = 0.0001298751449212432

Training epoch-18 batch-88
Running loss of epoch-18 batch-88 = 0.00011346442624926567

Training epoch-18 batch-89
Running loss of epoch-18 batch-89 = 6.44349493086338e-05

Training epoch-18 batch-90
Running loss of epoch-18 batch-90 = 0.00010780722368508577

Training epoch-18 batch-91
Running loss of epoch-18 batch-91 = 9.217741899192333e-05

Training epoch-18 batch-92
Running loss of epoch-18 batch-92 = 0.0001064774114638567

Training epoch-18 batch-93
Running loss of epoch-18 batch-93 = 6.500142626464367e-05

Training epoch-18 batch-94
Running loss of epoch-18 batch-94 = 5.07852528244257e-05

Training epoch-18 batch-95
Running loss of epoch-18 batch-95 = 5.9862504713237286e-05

Training epoch-18 batch-96
Running loss of epoch-18 batch-96 = 0.0003949857782572508

Training epoch-18 batch-97
Running loss of epoch-18 batch-97 = 0.00013845611829310656

Training epoch-18 batch-98
Running loss of epoch-18 batch-98 = 8.575199171900749e-05

Training epoch-18 batch-99
Running loss of epoch-18 batch-99 = 5.8287871070206165e-05

Training epoch-18 batch-100
Running loss of epoch-18 batch-100 = 8.803093805909157e-05

Training epoch-18 batch-101
Running loss of epoch-18 batch-101 = 4.04618913307786e-05

Training epoch-18 batch-102
Running loss of epoch-18 batch-102 = 2.9043061658740044e-05

Training epoch-18 batch-103
Running loss of epoch-18 batch-103 = 0.00011404533870518208

Training epoch-18 batch-104
Running loss of epoch-18 batch-104 = 3.907224163413048e-05

Training epoch-18 batch-105
Running loss of epoch-18 batch-105 = 5.977996625006199e-05

Training epoch-18 batch-106
Running loss of epoch-18 batch-106 = 0.0001913286978378892

Training epoch-18 batch-107
Running loss of epoch-18 batch-107 = 3.804126754403114e-05

Training epoch-18 batch-108
Running loss of epoch-18 batch-108 = 5.422777030616999e-05

Training epoch-18 batch-109
Running loss of epoch-18 batch-109 = 4.4660293497145176e-05

Training epoch-18 batch-110
Running loss of epoch-18 batch-110 = 0.00013749056961387396

Training epoch-18 batch-111
Running loss of epoch-18 batch-111 = 3.995303995907307e-05

Training epoch-18 batch-112
Running loss of epoch-18 batch-112 = 9.332236368209124e-05

Training epoch-18 batch-113
Running loss of epoch-18 batch-113 = 8.960138075053692e-05

Training epoch-18 batch-114
Running loss of epoch-18 batch-114 = 3.742845728993416e-05

Training epoch-18 batch-115
Running loss of epoch-18 batch-115 = 0.00041310745291411877

Training epoch-18 batch-116
Running loss of epoch-18 batch-116 = 6.160978227853775e-05

Training epoch-18 batch-117
Running loss of epoch-18 batch-117 = 7.594237104058266e-05

Training epoch-18 batch-118
Running loss of epoch-18 batch-118 = 0.00019016314763575792

Training epoch-18 batch-119
Running loss of epoch-18 batch-119 = 4.266854375600815e-05

Training epoch-18 batch-120
Running loss of epoch-18 batch-120 = 3.49909532815218e-05

Training epoch-18 batch-121
Running loss of epoch-18 batch-121 = 7.77154928073287e-05

Training epoch-18 batch-122
Running loss of epoch-18 batch-122 = 0.00010909431148320436

Training epoch-18 batch-123
Running loss of epoch-18 batch-123 = 0.00032516696956008673

Training epoch-18 batch-124
Running loss of epoch-18 batch-124 = 4.893355071544647e-05

Training epoch-18 batch-125
Running loss of epoch-18 batch-125 = 7.862003985792398e-05

Training epoch-18 batch-126
Running loss of epoch-18 batch-126 = 9.839888662099838e-05

Training epoch-18 batch-127
Running loss of epoch-18 batch-127 = 3.348919562995434e-05

Training epoch-18 batch-128
Running loss of epoch-18 batch-128 = 8.62685265019536e-05

Training epoch-18 batch-129
Running loss of epoch-18 batch-129 = 5.736714228987694e-05

Training epoch-18 batch-130
Running loss of epoch-18 batch-130 = 5.5903103202581406e-05

Training epoch-18 batch-131
Running loss of epoch-18 batch-131 = 0.00023318862076848745

Training epoch-18 batch-132
Running loss of epoch-18 batch-132 = 8.675362914800644e-05

Training epoch-18 batch-133
Running loss of epoch-18 batch-133 = 0.00015610340051352978

Training epoch-18 batch-134
Running loss of epoch-18 batch-134 = 0.00010073196608573198

Training epoch-18 batch-135
Running loss of epoch-18 batch-135 = 6.858911365270615e-05

Training epoch-18 batch-136
Running loss of epoch-18 batch-136 = 9.966734796762466e-05

Training epoch-18 batch-137
Running loss of epoch-18 batch-137 = 6.967177614569664e-05

Training epoch-18 batch-138
Running loss of epoch-18 batch-138 = 0.00020255532581359148

Training epoch-18 batch-139
Running loss of epoch-18 batch-139 = 7.288577035069466e-05

Training epoch-18 batch-140
Running loss of epoch-18 batch-140 = 9.389722254127264e-05

Training epoch-18 batch-141
Running loss of epoch-18 batch-141 = 3.566802479326725e-05

Training epoch-18 batch-142
Running loss of epoch-18 batch-142 = 0.00010641373228281736

Training epoch-18 batch-143
Running loss of epoch-18 batch-143 = 8.50579235702753e-05

Training epoch-18 batch-144
Running loss of epoch-18 batch-144 = 4.7645880840718746e-05

Training epoch-18 batch-145
Running loss of epoch-18 batch-145 = 0.00013663095887750387

Training epoch-18 batch-146
Running loss of epoch-18 batch-146 = 0.00012982042971998453

Training epoch-18 batch-147
Running loss of epoch-18 batch-147 = 0.00010204652789980173

Training epoch-18 batch-148
Running loss of epoch-18 batch-148 = 4.969013389199972e-05

Training epoch-18 batch-149
Running loss of epoch-18 batch-149 = 8.854037150740623e-05

Training epoch-18 batch-150
Running loss of epoch-18 batch-150 = 6.43234234303236e-05

Training epoch-18 batch-151
Running loss of epoch-18 batch-151 = 7.250183261930943e-05

Training epoch-18 batch-152
Running loss of epoch-18 batch-152 = 4.6307919546961784e-05

Training epoch-18 batch-153
Running loss of epoch-18 batch-153 = 8.292112033814192e-05

Training epoch-18 batch-154
Running loss of epoch-18 batch-154 = 7.640896365046501e-05

Training epoch-18 batch-155
Running loss of epoch-18 batch-155 = 4.431686829775572e-05

Training epoch-18 batch-156
Running loss of epoch-18 batch-156 = 4.1073188185691833e-05

Training epoch-18 batch-157
Running loss of epoch-18 batch-157 = 0.0003760680556297302

Finished training epoch-18.



Average train loss at epoch-18 = 0.00010643003508448601

Started Evaluation

Average val loss at epoch-18 = 0.8467568242798447

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 93.28 %
Accuracy for class onCreate is: 92.75 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 69.18 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 47.09 %
Accuracy for class execute is: 43.78 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.21 %

Finished Evaluation



Started training epoch-19


Training epoch-19 batch-1
Running loss of epoch-19 batch-1 = 3.1193019822239876e-05

Training epoch-19 batch-2
Running loss of epoch-19 batch-2 = 3.398978151381016e-05

Training epoch-19 batch-3
Running loss of epoch-19 batch-3 = 4.405633080750704e-05

Training epoch-19 batch-4
Running loss of epoch-19 batch-4 = 0.0004640669794753194

Training epoch-19 batch-5
Running loss of epoch-19 batch-5 = 0.0001350500388070941

Training epoch-19 batch-6
Running loss of epoch-19 batch-6 = 3.631063736975193e-05

Training epoch-19 batch-7
Running loss of epoch-19 batch-7 = 6.442260928452015e-05

Training epoch-19 batch-8
Running loss of epoch-19 batch-8 = 0.00011169083882123232

Training epoch-19 batch-9
Running loss of epoch-19 batch-9 = 6.272038444876671e-05

Training epoch-19 batch-10
Running loss of epoch-19 batch-10 = 4.899222403764725e-05

Training epoch-19 batch-11
Running loss of epoch-19 batch-11 = 7.282721344381571e-05

Training epoch-19 batch-12
Running loss of epoch-19 batch-12 = 5.627179052680731e-05

Training epoch-19 batch-13
Running loss of epoch-19 batch-13 = 5.739857442677021e-05

Training epoch-19 batch-14
Running loss of epoch-19 batch-14 = 5.7526747696101665e-05

Training epoch-19 batch-15
Running loss of epoch-19 batch-15 = 7.399928290396929e-05

Training epoch-19 batch-16
Running loss of epoch-19 batch-16 = 0.00023940799292176962

Training epoch-19 batch-17
Running loss of epoch-19 batch-17 = 5.693512503057718e-05

Training epoch-19 batch-18
Running loss of epoch-19 batch-18 = 4.232942592352629e-05

Training epoch-19 batch-19
Running loss of epoch-19 batch-19 = 9.19080339372158e-05

Training epoch-19 batch-20
Running loss of epoch-19 batch-20 = 8.467817679047585e-05

Training epoch-19 batch-21
Running loss of epoch-19 batch-21 = 0.00010850338730961084

Training epoch-19 batch-22
Running loss of epoch-19 batch-22 = 0.00010075920727103949

Training epoch-19 batch-23
Running loss of epoch-19 batch-23 = 0.00010389275848865509

Training epoch-19 batch-24
Running loss of epoch-19 batch-24 = 0.00010334711987525225

Training epoch-19 batch-25
Running loss of epoch-19 batch-25 = 4.818267188966274e-05

Training epoch-19 batch-26
Running loss of epoch-19 batch-26 = 7.601012475788593e-05

Training epoch-19 batch-27
Running loss of epoch-19 batch-27 = 0.00020753988064825535

Training epoch-19 batch-28
Running loss of epoch-19 batch-28 = 7.991353049874306e-05

Training epoch-19 batch-29
Running loss of epoch-19 batch-29 = 3.7194229662418365e-05

Training epoch-19 batch-30
Running loss of epoch-19 batch-30 = 0.00031643605325371027

Training epoch-19 batch-31
Running loss of epoch-19 batch-31 = 6.590562406927347e-05

Training epoch-19 batch-32
Running loss of epoch-19 batch-32 = 7.038970943540335e-05

Training epoch-19 batch-33
Running loss of epoch-19 batch-33 = 6.851321086287498e-05

Training epoch-19 batch-34
Running loss of epoch-19 batch-34 = 7.818150334060192e-05

Training epoch-19 batch-35
Running loss of epoch-19 batch-35 = 3.7020305171608925e-05

Training epoch-19 batch-36
Running loss of epoch-19 batch-36 = 0.00013353582471609116

Training epoch-19 batch-37
Running loss of epoch-19 batch-37 = 6.566918455064297e-05

Training epoch-19 batch-38
Running loss of epoch-19 batch-38 = 3.3303978852927685e-05

Training epoch-19 batch-39
Running loss of epoch-19 batch-39 = 0.00013030075933784246

Training epoch-19 batch-40
Running loss of epoch-19 batch-40 = 4.045118112117052e-05

Training epoch-19 batch-41
Running loss of epoch-19 batch-41 = 0.00015468162018805742

Training epoch-19 batch-42
Running loss of epoch-19 batch-42 = 0.0001067769480869174

Training epoch-19 batch-43
Running loss of epoch-19 batch-43 = 4.567450378090143e-05

Training epoch-19 batch-44
Running loss of epoch-19 batch-44 = 7.184804417192936e-05

Training epoch-19 batch-45
Running loss of epoch-19 batch-45 = 3.747094888240099e-05

Training epoch-19 batch-46
Running loss of epoch-19 batch-46 = 3.5477220080792904e-05

Training epoch-19 batch-47
Running loss of epoch-19 batch-47 = 8.492264896631241e-05

Training epoch-19 batch-48
Running loss of epoch-19 batch-48 = 5.821941886097193e-05

Training epoch-19 batch-49
Running loss of epoch-19 batch-49 = 3.599689807742834e-05

Training epoch-19 batch-50
Running loss of epoch-19 batch-50 = 0.00011622405145317316

Training epoch-19 batch-51
Running loss of epoch-19 batch-51 = 0.0001274145906791091

Training epoch-19 batch-52
Running loss of epoch-19 batch-52 = 0.00010393140837550163

Training epoch-19 batch-53
Running loss of epoch-19 batch-53 = 1.240801066160202e-05

Training epoch-19 batch-54
Running loss of epoch-19 batch-54 = 8.341949433088303e-05

Training epoch-19 batch-55
Running loss of epoch-19 batch-55 = 0.00020349794067442417

Training epoch-19 batch-56
Running loss of epoch-19 batch-56 = 6.981147453188896e-05

Training epoch-19 batch-57
Running loss of epoch-19 batch-57 = 6.815348751842976e-05

Training epoch-19 batch-58
Running loss of epoch-19 batch-58 = 0.0001797765726223588

Training epoch-19 batch-59
Running loss of epoch-19 batch-59 = 7.434561848640442e-05

Training epoch-19 batch-60
Running loss of epoch-19 batch-60 = 5.5040931329131126e-05

Training epoch-19 batch-61
Running loss of epoch-19 batch-61 = 0.00012420862913131714

Training epoch-19 batch-62
Running loss of epoch-19 batch-62 = 6.967934314161539e-05

Training epoch-19 batch-63
Running loss of epoch-19 batch-63 = 5.7628611102700233e-05

Training epoch-19 batch-64
Running loss of epoch-19 batch-64 = 6.0428050346672535e-05

Training epoch-19 batch-65
Running loss of epoch-19 batch-65 = 4.5141554437577724e-05

Training epoch-19 batch-66
Running loss of epoch-19 batch-66 = 0.0001241639256477356

Training epoch-19 batch-67
Running loss of epoch-19 batch-67 = 6.163737270981073e-05

Training epoch-19 batch-68
Running loss of epoch-19 batch-68 = 0.0001221114071086049

Training epoch-19 batch-69
Running loss of epoch-19 batch-69 = 2.8480542823672295e-05

Training epoch-19 batch-70
Running loss of epoch-19 batch-70 = 3.705231938511133e-05

Training epoch-19 batch-71
Running loss of epoch-19 batch-71 = 4.914519377052784e-05

Training epoch-19 batch-72
Running loss of epoch-19 batch-72 = 7.714726962149143e-05

Training epoch-19 batch-73
Running loss of epoch-19 batch-73 = 9.038846474140882e-05

Training epoch-19 batch-74
Running loss of epoch-19 batch-74 = 9.887933265417814e-05

Training epoch-19 batch-75
Running loss of epoch-19 batch-75 = 1.8480815924704075e-05

Training epoch-19 batch-76
Running loss of epoch-19 batch-76 = 0.00034202809911221266

Training epoch-19 batch-77
Running loss of epoch-19 batch-77 = 5.2965711802244186e-05

Training epoch-19 batch-78
Running loss of epoch-19 batch-78 = 8.617178536951542e-05

Training epoch-19 batch-79
Running loss of epoch-19 batch-79 = 6.5077212639153e-05

Training epoch-19 batch-80
Running loss of epoch-19 batch-80 = 0.0005031090695410967

Training epoch-19 batch-81
Running loss of epoch-19 batch-81 = 9.358150418847799e-05

Training epoch-19 batch-82
Running loss of epoch-19 batch-82 = 0.00010217376984655857

Training epoch-19 batch-83
Running loss of epoch-19 batch-83 = 9.931519161909819e-05

Training epoch-19 batch-84
Running loss of epoch-19 batch-84 = 6.668083369731903e-05

Training epoch-19 batch-85
Running loss of epoch-19 batch-85 = 4.369474481791258e-05

Training epoch-19 batch-86
Running loss of epoch-19 batch-86 = 8.457177318632603e-05

Training epoch-19 batch-87
Running loss of epoch-19 batch-87 = 6.587442476302385e-05

Training epoch-19 batch-88
Running loss of epoch-19 batch-88 = 8.52562952786684e-05

Training epoch-19 batch-89
Running loss of epoch-19 batch-89 = 5.657470319420099e-05

Training epoch-19 batch-90
Running loss of epoch-19 batch-90 = 4.25814650952816e-05

Training epoch-19 batch-91
Running loss of epoch-19 batch-91 = 6.387289613485336e-05

Training epoch-19 batch-92
Running loss of epoch-19 batch-92 = 5.444977432489395e-05

Training epoch-19 batch-93
Running loss of epoch-19 batch-93 = 5.8025121688842773e-05

Training epoch-19 batch-94
Running loss of epoch-19 batch-94 = 8.86318739503622e-05

Training epoch-19 batch-95
Running loss of epoch-19 batch-95 = 4.973425529897213e-05

Training epoch-19 batch-96
Running loss of epoch-19 batch-96 = 0.00010541430674493313

Training epoch-19 batch-97
Running loss of epoch-19 batch-97 = 2.3947679437696934e-05

Training epoch-19 batch-98
Running loss of epoch-19 batch-98 = 2.4191802367568016e-05

Training epoch-19 batch-99
Running loss of epoch-19 batch-99 = 9.326660074293613e-05

Training epoch-19 batch-100
Running loss of epoch-19 batch-100 = 3.345427103340626e-05

Training epoch-19 batch-101
Running loss of epoch-19 batch-101 = 5.1662675105035305e-05

Training epoch-19 batch-102
Running loss of epoch-19 batch-102 = 9.172724094241858e-05

Training epoch-19 batch-103
Running loss of epoch-19 batch-103 = 4.2719533666968346e-05

Training epoch-19 batch-104
Running loss of epoch-19 batch-104 = 1.1055730283260345e-05

Training epoch-19 batch-105
Running loss of epoch-19 batch-105 = 0.00014595966786146164

Training epoch-19 batch-106
Running loss of epoch-19 batch-106 = 4.357961006462574e-05

Training epoch-19 batch-107
Running loss of epoch-19 batch-107 = 4.101113881915808e-05

Training epoch-19 batch-108
Running loss of epoch-19 batch-108 = 6.333645433187485e-05

Training epoch-19 batch-109
Running loss of epoch-19 batch-109 = 0.00013646029401570559

Training epoch-19 batch-110
Running loss of epoch-19 batch-110 = 5.933281499892473e-05

Training epoch-19 batch-111
Running loss of epoch-19 batch-111 = 6.475613918155432e-05

Training epoch-19 batch-112
Running loss of epoch-19 batch-112 = 0.00016854354180395603

Training epoch-19 batch-113
Running loss of epoch-19 batch-113 = 0.00018082908354699612

Training epoch-19 batch-114
Running loss of epoch-19 batch-114 = 4.1698687709867954e-05

Training epoch-19 batch-115
Running loss of epoch-19 batch-115 = 0.00010973215103149414

Training epoch-19 batch-116
Running loss of epoch-19 batch-116 = 3.645161632448435e-05

Training epoch-19 batch-117
Running loss of epoch-19 batch-117 = 4.662561696022749e-05

Training epoch-19 batch-118
Running loss of epoch-19 batch-118 = 7.163430564105511e-05

Training epoch-19 batch-119
Running loss of epoch-19 batch-119 = 0.0002501609269529581

Training epoch-19 batch-120
Running loss of epoch-19 batch-120 = 1.7977552488446236e-05

Training epoch-19 batch-121
Running loss of epoch-19 batch-121 = 7.603748235851526e-05

Training epoch-19 batch-122
Running loss of epoch-19 batch-122 = 7.334514521062374e-05

Training epoch-19 batch-123
Running loss of epoch-19 batch-123 = 3.484915941953659e-05

Training epoch-19 batch-124
Running loss of epoch-19 batch-124 = 0.0001124073751270771

Training epoch-19 batch-125
Running loss of epoch-19 batch-125 = 0.00025519379414618015

Training epoch-19 batch-126
Running loss of epoch-19 batch-126 = 3.94945964217186e-05

Training epoch-19 batch-127
Running loss of epoch-19 batch-127 = 0.00016925146337598562

Training epoch-19 batch-128
Running loss of epoch-19 batch-128 = 5.442090332508087e-05

Training epoch-19 batch-129
Running loss of epoch-19 batch-129 = 3.493158146739006e-05

Training epoch-19 batch-130
Running loss of epoch-19 batch-130 = 7.735507097095251e-05

Training epoch-19 batch-131
Running loss of epoch-19 batch-131 = 2.2829975932836533e-05

Training epoch-19 batch-132
Running loss of epoch-19 batch-132 = 8.961372077465057e-05

Training epoch-19 batch-133
Running loss of epoch-19 batch-133 = 6.299535743892193e-05

Training epoch-19 batch-134
Running loss of epoch-19 batch-134 = 0.00012814102228730917

Training epoch-19 batch-135
Running loss of epoch-19 batch-135 = 7.164967246353626e-05

Training epoch-19 batch-136
Running loss of epoch-19 batch-136 = 3.024563193321228e-05

Training epoch-19 batch-137
Running loss of epoch-19 batch-137 = 0.0001037440961226821

Training epoch-19 batch-138
Running loss of epoch-19 batch-138 = 5.437224172055721e-05

Training epoch-19 batch-139
Running loss of epoch-19 batch-139 = 4.414888098835945e-05

Training epoch-19 batch-140
Running loss of epoch-19 batch-140 = 8.56474507600069e-05

Training epoch-19 batch-141
Running loss of epoch-19 batch-141 = 4.996731877326965e-05

Training epoch-19 batch-142
Running loss of epoch-19 batch-142 = 7.87830213084817e-05

Training epoch-19 batch-143
Running loss of epoch-19 batch-143 = 3.406987525522709e-05

Training epoch-19 batch-144
Running loss of epoch-19 batch-144 = 1.892691943794489e-05

Training epoch-19 batch-145
Running loss of epoch-19 batch-145 = 3.4179771319031715e-05

Training epoch-19 batch-146
Running loss of epoch-19 batch-146 = 2.3039523512125015e-05

Training epoch-19 batch-147
Running loss of epoch-19 batch-147 = 5.1870825700461864e-05

Training epoch-19 batch-148
Running loss of epoch-19 batch-148 = 8.102075662463903e-05

Training epoch-19 batch-149
Running loss of epoch-19 batch-149 = 5.506549496203661e-05

Training epoch-19 batch-150
Running loss of epoch-19 batch-150 = 3.7161633372306824e-05

Training epoch-19 batch-151
Running loss of epoch-19 batch-151 = 6.755348294973373e-05

Training epoch-19 batch-152
Running loss of epoch-19 batch-152 = 7.114186882972717e-05

Training epoch-19 batch-153
Running loss of epoch-19 batch-153 = 4.357227589935064e-05

Training epoch-19 batch-154
Running loss of epoch-19 batch-154 = 2.4566426873207092e-05

Training epoch-19 batch-155
Running loss of epoch-19 batch-155 = 0.0006031586090102792

Training epoch-19 batch-156
Running loss of epoch-19 batch-156 = 4.173722118139267e-05

Training epoch-19 batch-157
Running loss of epoch-19 batch-157 = 0.0005825161933898926

Finished training epoch-19.



Average train loss at epoch-19 = 8.769605606794358e-05

Started Evaluation

Average val loss at epoch-19 = 0.8880435331864072

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 90.16 %
Accuracy for class onCreate is: 94.24 %
Accuracy for class toString is: 89.76 %
Accuracy for class run is: 58.68 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 55.82 %
Accuracy for class get is: 66.92 %

Overall Accuracy = 83.00 %

Finished Evaluation



Started training epoch-20


Training epoch-20 batch-1
Running loss of epoch-20 batch-1 = 5.698623135685921e-05

Training epoch-20 batch-2
Running loss of epoch-20 batch-2 = 8.497782982885838e-05

Training epoch-20 batch-3
Running loss of epoch-20 batch-3 = 0.00012120557948946953

Training epoch-20 batch-4
Running loss of epoch-20 batch-4 = 8.20704735815525e-05

Training epoch-20 batch-5
Running loss of epoch-20 batch-5 = 2.8491951525211334e-05

Training epoch-20 batch-6
Running loss of epoch-20 batch-6 = 8.717982564121485e-05

Training epoch-20 batch-7
Running loss of epoch-20 batch-7 = 1.8040882423520088e-05

Training epoch-20 batch-8
Running loss of epoch-20 batch-8 = 3.4194206818938255e-05

Training epoch-20 batch-9
Running loss of epoch-20 batch-9 = 0.00015981635078787804

Training epoch-20 batch-10
Running loss of epoch-20 batch-10 = 4.126632120460272e-05

Training epoch-20 batch-11
Running loss of epoch-20 batch-11 = 5.842756945639849e-05

Training epoch-20 batch-12
Running loss of epoch-20 batch-12 = 3.363611176609993e-05

Training epoch-20 batch-13
Running loss of epoch-20 batch-13 = 2.0556850358843803e-05

Training epoch-20 batch-14
Running loss of epoch-20 batch-14 = 3.8660247810184956e-05

Training epoch-20 batch-15
Running loss of epoch-20 batch-15 = 4.5598018914461136e-05

Training epoch-20 batch-16
Running loss of epoch-20 batch-16 = 2.0235078409314156e-05

Training epoch-20 batch-17
Running loss of epoch-20 batch-17 = 0.0001603533746674657

Training epoch-20 batch-18
Running loss of epoch-20 batch-18 = 4.028039984405041e-05

Training epoch-20 batch-19
Running loss of epoch-20 batch-19 = 0.00017611775547266006

Training epoch-20 batch-20
Running loss of epoch-20 batch-20 = 0.00012763706035912037

Training epoch-20 batch-21
Running loss of epoch-20 batch-21 = 0.0001095915213227272

Training epoch-20 batch-22
Running loss of epoch-20 batch-22 = 0.00012063281610608101

Training epoch-20 batch-23
Running loss of epoch-20 batch-23 = 5.917833186686039e-05

Training epoch-20 batch-24
Running loss of epoch-20 batch-24 = 3.168662078678608e-05

Training epoch-20 batch-25
Running loss of epoch-20 batch-25 = 3.0875904485583305e-05

Training epoch-20 batch-26
Running loss of epoch-20 batch-26 = 3.1169503927230835e-05

Training epoch-20 batch-27
Running loss of epoch-20 batch-27 = 3.345671575516462e-05

Training epoch-20 batch-28
Running loss of epoch-20 batch-28 = 3.605312667787075e-05

Training epoch-20 batch-29
Running loss of epoch-20 batch-29 = 0.00017939542885869741

Training epoch-20 batch-30
Running loss of epoch-20 batch-30 = 6.099708843976259e-05

Training epoch-20 batch-31
Running loss of epoch-20 batch-31 = 6.68687280267477e-05

Training epoch-20 batch-32
Running loss of epoch-20 batch-32 = 5.305325612425804e-05

Training epoch-20 batch-33
Running loss of epoch-20 batch-33 = 6.691529415547848e-05

Training epoch-20 batch-34
Running loss of epoch-20 batch-34 = 5.1101204007864e-05

Training epoch-20 batch-35
Running loss of epoch-20 batch-35 = 1.0117888450622559e-05

Training epoch-20 batch-36
Running loss of epoch-20 batch-36 = 3.434019163250923e-05

Training epoch-20 batch-37
Running loss of epoch-20 batch-37 = 4.945346154272556e-05

Training epoch-20 batch-38
Running loss of epoch-20 batch-38 = 7.548253051936626e-05

Training epoch-20 batch-39
Running loss of epoch-20 batch-39 = 3.3885007724165916e-05

Training epoch-20 batch-40
Running loss of epoch-20 batch-40 = 7.41342082619667e-05

Training epoch-20 batch-41
Running loss of epoch-20 batch-41 = 0.00011991150677204132

Training epoch-20 batch-42
Running loss of epoch-20 batch-42 = 2.3987493477761745e-05

Training epoch-20 batch-43
Running loss of epoch-20 batch-43 = 3.701203968375921e-05

Training epoch-20 batch-44
Running loss of epoch-20 batch-44 = 4.8312474973499775e-05

Training epoch-20 batch-45
Running loss of epoch-20 batch-45 = 4.036771133542061e-05

Training epoch-20 batch-46
Running loss of epoch-20 batch-46 = 7.135106716305017e-05

Training epoch-20 batch-47
Running loss of epoch-20 batch-47 = 4.6291621401906013e-05

Training epoch-20 batch-48
Running loss of epoch-20 batch-48 = 7.255352102220058e-05

Training epoch-20 batch-49
Running loss of epoch-20 batch-49 = 2.208608202636242e-05

Training epoch-20 batch-50
Running loss of epoch-20 batch-50 = 4.9031456001102924e-05

Training epoch-20 batch-51
Running loss of epoch-20 batch-51 = 3.703380934894085e-05

Training epoch-20 batch-52
Running loss of epoch-20 batch-52 = 3.785744775086641e-05

Training epoch-20 batch-53
Running loss of epoch-20 batch-53 = 6.026867777109146e-05

Training epoch-20 batch-54
Running loss of epoch-20 batch-54 = 0.00015525054186582565

Training epoch-20 batch-55
Running loss of epoch-20 batch-55 = 5.703361239284277e-05

Training epoch-20 batch-56
Running loss of epoch-20 batch-56 = 4.543410614132881e-05

Training epoch-20 batch-57
Running loss of epoch-20 batch-57 = 3.3176038414239883e-05

Training epoch-20 batch-58
Running loss of epoch-20 batch-58 = 9.098718874156475e-05

Training epoch-20 batch-59
Running loss of epoch-20 batch-59 = 5.0642527639865875e-05

Training epoch-20 batch-60
Running loss of epoch-20 batch-60 = 4.100729711353779e-05

Training epoch-20 batch-61
Running loss of epoch-20 batch-61 = 2.254906576126814e-05

Training epoch-20 batch-62
Running loss of epoch-20 batch-62 = 0.00026629806961864233

Training epoch-20 batch-63
Running loss of epoch-20 batch-63 = 4.894228186458349e-05

Training epoch-20 batch-64
Running loss of epoch-20 batch-64 = 6.556324660778046e-05

Training epoch-20 batch-65
Running loss of epoch-20 batch-65 = 0.00010410160757601261

Training epoch-20 batch-66
Running loss of epoch-20 batch-66 = 0.00011031574103981256

Training epoch-20 batch-67
Running loss of epoch-20 batch-67 = 7.45158176869154e-05

Training epoch-20 batch-68
Running loss of epoch-20 batch-68 = 8.448690641671419e-05

Training epoch-20 batch-69
Running loss of epoch-20 batch-69 = 8.268491365015507e-05

Training epoch-20 batch-70
Running loss of epoch-20 batch-70 = 2.082623541355133e-05

Training epoch-20 batch-71
Running loss of epoch-20 batch-71 = 5.878438241779804e-05

Training epoch-20 batch-72
Running loss of epoch-20 batch-72 = 6.001046858727932e-05

Training epoch-20 batch-73
Running loss of epoch-20 batch-73 = 0.00015542027540504932

Training epoch-20 batch-74
Running loss of epoch-20 batch-74 = 0.0001416949089616537

Training epoch-20 batch-75
Running loss of epoch-20 batch-75 = 0.0001281040022149682

Training epoch-20 batch-76
Running loss of epoch-20 batch-76 = 3.761798143386841e-05

Training epoch-20 batch-77
Running loss of epoch-20 batch-77 = 0.00010608218144625425

Training epoch-20 batch-78
Running loss of epoch-20 batch-78 = 9.022769518196583e-05

Training epoch-20 batch-79
Running loss of epoch-20 batch-79 = 4.3456326238811016e-05

Training epoch-20 batch-80
Running loss of epoch-20 batch-80 = 7.387041114270687e-05

Training epoch-20 batch-81
Running loss of epoch-20 batch-81 = 5.784479435533285e-05

Training epoch-20 batch-82
Running loss of epoch-20 batch-82 = 2.80656386166811e-05

Training epoch-20 batch-83
Running loss of epoch-20 batch-83 = 3.331073094159365e-05

Training epoch-20 batch-84
Running loss of epoch-20 batch-84 = 7.744692265987396e-05

Training epoch-20 batch-85
Running loss of epoch-20 batch-85 = 5.2428338676691055e-05

Training epoch-20 batch-86
Running loss of epoch-20 batch-86 = 2.820766530930996e-05

Training epoch-20 batch-87
Running loss of epoch-20 batch-87 = 1.2369826436042786e-05

Training epoch-20 batch-88
Running loss of epoch-20 batch-88 = 5.684909410774708e-05

Training epoch-20 batch-89
Running loss of epoch-20 batch-89 = 4.6495464630424976e-05

Training epoch-20 batch-90
Running loss of epoch-20 batch-90 = 2.3023225367069244e-05

Training epoch-20 batch-91
Running loss of epoch-20 batch-91 = 3.7232646718621254e-05

Training epoch-20 batch-92
Running loss of epoch-20 batch-92 = 6.309256423264742e-05

Training epoch-20 batch-93
Running loss of epoch-20 batch-93 = 7.902167271822691e-05

Training epoch-20 batch-94
Running loss of epoch-20 batch-94 = 5.3024617955088615e-05

Training epoch-20 batch-95
Running loss of epoch-20 batch-95 = 6.11630966886878e-05

Training epoch-20 batch-96
Running loss of epoch-20 batch-96 = 2.5125453248620033e-05

Training epoch-20 batch-97
Running loss of epoch-20 batch-97 = 5.26264775544405e-05

Training epoch-20 batch-98
Running loss of epoch-20 batch-98 = 4.517706111073494e-05

Training epoch-20 batch-99
Running loss of epoch-20 batch-99 = 2.980814315378666e-05

Training epoch-20 batch-100
Running loss of epoch-20 batch-100 = 8.333474397659302e-06

Training epoch-20 batch-101
Running loss of epoch-20 batch-101 = 3.253738395869732e-05

Training epoch-20 batch-102
Running loss of epoch-20 batch-102 = 3.59807163476944e-05

Training epoch-20 batch-103
Running loss of epoch-20 batch-103 = 6.654125172644854e-05

Training epoch-20 batch-104
Running loss of epoch-20 batch-104 = 4.416459705680609e-05

Training epoch-20 batch-105
Running loss of epoch-20 batch-105 = 0.0003474853001534939

Training epoch-20 batch-106
Running loss of epoch-20 batch-106 = 6.130116526037455e-05

Training epoch-20 batch-107
Running loss of epoch-20 batch-107 = 4.5160530135035515e-05

Training epoch-20 batch-108
Running loss of epoch-20 batch-108 = 4.10319771617651e-05

Training epoch-20 batch-109
Running loss of epoch-20 batch-109 = 6.770715117454529e-05

Training epoch-20 batch-110
Running loss of epoch-20 batch-110 = 0.0001249860506504774

Training epoch-20 batch-111
Running loss of epoch-20 batch-111 = 1.7469050362706184e-05

Training epoch-20 batch-112
Running loss of epoch-20 batch-112 = 4.3325359001755714e-05

Training epoch-20 batch-113
Running loss of epoch-20 batch-113 = 0.00010195805225521326

Training epoch-20 batch-114
Running loss of epoch-20 batch-114 = 3.113923594355583e-05

Training epoch-20 batch-115
Running loss of epoch-20 batch-115 = 6.572471465915442e-05

Training epoch-20 batch-116
Running loss of epoch-20 batch-116 = 2.548936754465103e-05

Training epoch-20 batch-117
Running loss of epoch-20 batch-117 = 5.490053445100784e-05

Training epoch-20 batch-118
Running loss of epoch-20 batch-118 = 0.0001411351840943098

Training epoch-20 batch-119
Running loss of epoch-20 batch-119 = 0.00018812983762472868

Training epoch-20 batch-120
Running loss of epoch-20 batch-120 = 7.566844578832388e-05

Training epoch-20 batch-121
Running loss of epoch-20 batch-121 = 1.4273682609200478e-05

Training epoch-20 batch-122
Running loss of epoch-20 batch-122 = 6.233924068510532e-05

Training epoch-20 batch-123
Running loss of epoch-20 batch-123 = 0.00012642343062907457

Training epoch-20 batch-124
Running loss of epoch-20 batch-124 = 5.12079568579793e-05

Training epoch-20 batch-125
Running loss of epoch-20 batch-125 = 1.851026900112629e-05

Training epoch-20 batch-126
Running loss of epoch-20 batch-126 = 7.859175093472004e-05

Training epoch-20 batch-127
Running loss of epoch-20 batch-127 = 7.100217044353485e-05

Training epoch-20 batch-128
Running loss of epoch-20 batch-128 = 0.00019280100241303444

Training epoch-20 batch-129
Running loss of epoch-20 batch-129 = 3.9096106775105e-05

Training epoch-20 batch-130
Running loss of epoch-20 batch-130 = 6.918853614479303e-05

Training epoch-20 batch-131
Running loss of epoch-20 batch-131 = 9.450339712202549e-05

Training epoch-20 batch-132
Running loss of epoch-20 batch-132 = 7.035944145172834e-05

Training epoch-20 batch-133
Running loss of epoch-20 batch-133 = 4.582770634442568e-05

Training epoch-20 batch-134
Running loss of epoch-20 batch-134 = 4.957395140081644e-05

Training epoch-20 batch-135
Running loss of epoch-20 batch-135 = 3.450131043791771e-05

Training epoch-20 batch-136
Running loss of epoch-20 batch-136 = 5.066255107522011e-05

Training epoch-20 batch-137
Running loss of epoch-20 batch-137 = 4.2729428969323635e-05

Training epoch-20 batch-138
Running loss of epoch-20 batch-138 = 1.1640368029475212e-05

Training epoch-20 batch-139
Running loss of epoch-20 batch-139 = 5.339144263416529e-05

Training epoch-20 batch-140
Running loss of epoch-20 batch-140 = 3.708270378410816e-05

Training epoch-20 batch-141
Running loss of epoch-20 batch-141 = 4.842423368245363e-05

Training epoch-20 batch-142
Running loss of epoch-20 batch-142 = 1.8898979760706425e-05

Training epoch-20 batch-143
Running loss of epoch-20 batch-143 = 2.7044443413615227e-05

Training epoch-20 batch-144
Running loss of epoch-20 batch-144 = 2.6645604521036148e-05

Training epoch-20 batch-145
Running loss of epoch-20 batch-145 = 9.252165909856558e-05

Training epoch-20 batch-146
Running loss of epoch-20 batch-146 = 0.00022565259132534266

Training epoch-20 batch-147
Running loss of epoch-20 batch-147 = 5.3857103921473026e-05

Training epoch-20 batch-148
Running loss of epoch-20 batch-148 = 2.8418959118425846e-05

Training epoch-20 batch-149
Running loss of epoch-20 batch-149 = 3.876350820064545e-05

Training epoch-20 batch-150
Running loss of epoch-20 batch-150 = 6.83940015733242e-05

Training epoch-20 batch-151
Running loss of epoch-20 batch-151 = 0.00015739374794065952

Training epoch-20 batch-152
Running loss of epoch-20 batch-152 = 1.901423092931509e-05

Training epoch-20 batch-153
Running loss of epoch-20 batch-153 = 9.17934812605381e-05

Training epoch-20 batch-154
Running loss of epoch-20 batch-154 = 3.578397445380688e-05

Training epoch-20 batch-155
Running loss of epoch-20 batch-155 = 5.1502836868166924e-05

Training epoch-20 batch-156
Running loss of epoch-20 batch-156 = 6.426998879760504e-05

Training epoch-20 batch-157
Running loss of epoch-20 batch-157 = 5.232915282249451e-05

Finished training epoch-20.



Average train loss at epoch-20 = 6.602829173207282e-05

Started Evaluation

Average val loss at epoch-20 = 0.8662141596436604

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 89.34 %
Accuracy for class onCreate is: 93.28 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 60.73 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 58.07 %
Accuracy for class execute is: 41.37 %
Accuracy for class get is: 73.85 %

Overall Accuracy = 83.16 %

Finished Evaluation



Started training epoch-21


Training epoch-21 batch-1
Running loss of epoch-21 batch-1 = 1.844600774347782e-05

Training epoch-21 batch-2
Running loss of epoch-21 batch-2 = 2.8611975722014904e-05

Training epoch-21 batch-3
Running loss of epoch-21 batch-3 = 7.879501208662987e-05

Training epoch-21 batch-4
Running loss of epoch-21 batch-4 = 3.309024032205343e-05

Training epoch-21 batch-5
Running loss of epoch-21 batch-5 = 4.368601366877556e-05

Training epoch-21 batch-6
Running loss of epoch-21 batch-6 = 2.752244472503662e-05

Training epoch-21 batch-7
Running loss of epoch-21 batch-7 = 6.090686656534672e-05

Training epoch-21 batch-8
Running loss of epoch-21 batch-8 = 7.368042133748531e-05

Training epoch-21 batch-9
Running loss of epoch-21 batch-9 = 6.939517334103584e-06

Training epoch-21 batch-10
Running loss of epoch-21 batch-10 = 4.0375860407948494e-05

Training epoch-21 batch-11
Running loss of epoch-21 batch-11 = 3.608572296798229e-05

Training epoch-21 batch-12
Running loss of epoch-21 batch-12 = 3.512273542582989e-05

Training epoch-21 batch-13
Running loss of epoch-21 batch-13 = 9.23575134947896e-05

Training epoch-21 batch-14
Running loss of epoch-21 batch-14 = 3.253400791436434e-05

Training epoch-21 batch-15
Running loss of epoch-21 batch-15 = 4.8510730266571045e-05

Training epoch-21 batch-16
Running loss of epoch-21 batch-16 = 5.7835597544908524e-05

Training epoch-21 batch-17
Running loss of epoch-21 batch-17 = 0.00013983971439301968

Training epoch-21 batch-18
Running loss of epoch-21 batch-18 = 4.3218256905674934e-05

Training epoch-21 batch-19
Running loss of epoch-21 batch-19 = 4.207459278404713e-05

Training epoch-21 batch-20
Running loss of epoch-21 batch-20 = 3.389548510313034e-05

Training epoch-21 batch-21
Running loss of epoch-21 batch-21 = 3.41503182426095e-05

Training epoch-21 batch-22
Running loss of epoch-21 batch-22 = 3.776117227971554e-05

Training epoch-21 batch-23
Running loss of epoch-21 batch-23 = 4.126434214413166e-05

Training epoch-21 batch-24
Running loss of epoch-21 batch-24 = 5.370797589421272e-05

Training epoch-21 batch-25
Running loss of epoch-21 batch-25 = 9.383400902152061e-05

Training epoch-21 batch-26
Running loss of epoch-21 batch-26 = 3.926234785467386e-05

Training epoch-21 batch-27
Running loss of epoch-21 batch-27 = 3.2991752959787846e-05

Training epoch-21 batch-28
Running loss of epoch-21 batch-28 = 3.230152651667595e-05

Training epoch-21 batch-29
Running loss of epoch-21 batch-29 = 5.113240331411362e-05

Training epoch-21 batch-30
Running loss of epoch-21 batch-30 = 2.6159221306443214e-05

Training epoch-21 batch-31
Running loss of epoch-21 batch-31 = 3.325904253870249e-05

Training epoch-21 batch-32
Running loss of epoch-21 batch-32 = 1.7915386706590652e-05

Training epoch-21 batch-33
Running loss of epoch-21 batch-33 = 6.249174475669861e-05

Training epoch-21 batch-34
Running loss of epoch-21 batch-34 = 3.491179086267948e-05

Training epoch-21 batch-35
Running loss of epoch-21 batch-35 = 4.4103944674134254e-05

Training epoch-21 batch-36
Running loss of epoch-21 batch-36 = 1.963519025593996e-05

Training epoch-21 batch-37
Running loss of epoch-21 batch-37 = 6.098300218582153e-05

Training epoch-21 batch-38
Running loss of epoch-21 batch-38 = 4.517927300184965e-05

Training epoch-21 batch-39
Running loss of epoch-21 batch-39 = 7.772666867822409e-05

Training epoch-21 batch-40
Running loss of epoch-21 batch-40 = 6.465846672654152e-05

Training epoch-21 batch-41
Running loss of epoch-21 batch-41 = 4.1546416468918324e-05

Training epoch-21 batch-42
Running loss of epoch-21 batch-42 = 3.246788401156664e-05

Training epoch-21 batch-43
Running loss of epoch-21 batch-43 = 2.467213198542595e-05

Training epoch-21 batch-44
Running loss of epoch-21 batch-44 = 8.219853043556213e-06

Training epoch-21 batch-45
Running loss of epoch-21 batch-45 = 6.832962390035391e-05

Training epoch-21 batch-46
Running loss of epoch-21 batch-46 = 7.732037920504808e-05

Training epoch-21 batch-47
Running loss of epoch-21 batch-47 = 3.818655386567116e-05

Training epoch-21 batch-48
Running loss of epoch-21 batch-48 = 5.685770884156227e-05

Training epoch-21 batch-49
Running loss of epoch-21 batch-49 = 2.9882648959755898e-05

Training epoch-21 batch-50
Running loss of epoch-21 batch-50 = 4.0185521356761456e-05

Training epoch-21 batch-51
Running loss of epoch-21 batch-51 = 2.3554544895887375e-05

Training epoch-21 batch-52
Running loss of epoch-21 batch-52 = 4.1696708649396896e-05

Training epoch-21 batch-53
Running loss of epoch-21 batch-53 = 2.957531251013279e-05

Training epoch-21 batch-54
Running loss of epoch-21 batch-54 = 9.40150348469615e-05

Training epoch-21 batch-55
Running loss of epoch-21 batch-55 = 1.6645994037389755e-05

Training epoch-21 batch-56
Running loss of epoch-21 batch-56 = 7.134559564292431e-05

Training epoch-21 batch-57
Running loss of epoch-21 batch-57 = 4.2045838199555874e-05

Training epoch-21 batch-58
Running loss of epoch-21 batch-58 = 4.440336488187313e-05

Training epoch-21 batch-59
Running loss of epoch-21 batch-59 = 0.00014109443873167038

Training epoch-21 batch-60
Running loss of epoch-21 batch-60 = 7.560523226857185e-05

Training epoch-21 batch-61
Running loss of epoch-21 batch-61 = 3.99639829993248e-05

Training epoch-21 batch-62
Running loss of epoch-21 batch-62 = 2.0951032638549805e-05

Training epoch-21 batch-63
Running loss of epoch-21 batch-63 = 0.00014575989916920662

Training epoch-21 batch-64
Running loss of epoch-21 batch-64 = 2.514338120818138e-05

Training epoch-21 batch-65
Running loss of epoch-21 batch-65 = 9.623437654227018e-05

Training epoch-21 batch-66
Running loss of epoch-21 batch-66 = 3.14096687361598e-05

Training epoch-21 batch-67
Running loss of epoch-21 batch-67 = 3.44316940754652e-05

Training epoch-21 batch-68
Running loss of epoch-21 batch-68 = 3.1670439057052135e-05

Training epoch-21 batch-69
Running loss of epoch-21 batch-69 = 0.00016984588000923395

Training epoch-21 batch-70
Running loss of epoch-21 batch-70 = 2.817157655954361e-05

Training epoch-21 batch-71
Running loss of epoch-21 batch-71 = 4.630081821233034e-05

Training epoch-21 batch-72
Running loss of epoch-21 batch-72 = 2.9835617169737816e-05

Training epoch-21 batch-73
Running loss of epoch-21 batch-73 = 2.7246074751019478e-05

Training epoch-21 batch-74
Running loss of epoch-21 batch-74 = 5.244638305157423e-05

Training epoch-21 batch-75
Running loss of epoch-21 batch-75 = 3.62517312169075e-05

Training epoch-21 batch-76
Running loss of epoch-21 batch-76 = 4.5615481212735176e-05

Training epoch-21 batch-77
Running loss of epoch-21 batch-77 = 3.95849347114563e-05

Training epoch-21 batch-78
Running loss of epoch-21 batch-78 = 0.00011789018753916025

Training epoch-21 batch-79
Running loss of epoch-21 batch-79 = 2.7262140065431595e-05

Training epoch-21 batch-80
Running loss of epoch-21 batch-80 = 5.350611172616482e-05

Training epoch-21 batch-81
Running loss of epoch-21 batch-81 = 4.5405467972159386e-05

Training epoch-21 batch-82
Running loss of epoch-21 batch-82 = 0.00024247344117611647

Training epoch-21 batch-83
Running loss of epoch-21 batch-83 = 3.515416756272316e-05

Training epoch-21 batch-84
Running loss of epoch-21 batch-84 = 0.00013947323895990849

Training epoch-21 batch-85
Running loss of epoch-21 batch-85 = 6.253540050238371e-05

Training epoch-21 batch-86
Running loss of epoch-21 batch-86 = 3.0544702894985676e-05

Training epoch-21 batch-87
Running loss of epoch-21 batch-87 = 4.001439083367586e-05

Training epoch-21 batch-88
Running loss of epoch-21 batch-88 = 6.951531395316124e-05

Training epoch-21 batch-89
Running loss of epoch-21 batch-89 = 1.545972190797329e-05

Training epoch-21 batch-90
Running loss of epoch-21 batch-90 = 4.230113700032234e-05

Training epoch-21 batch-91
Running loss of epoch-21 batch-91 = 5.5298907682299614e-05

Training epoch-21 batch-92
Running loss of epoch-21 batch-92 = 1.3896729797124863e-05

Training epoch-21 batch-93
Running loss of epoch-21 batch-93 = 0.00017276662401854992

Training epoch-21 batch-94
Running loss of epoch-21 batch-94 = 3.0224910005927086e-05

Training epoch-21 batch-95
Running loss of epoch-21 batch-95 = 3.08426097035408e-05

Training epoch-21 batch-96
Running loss of epoch-21 batch-96 = 7.879897020757198e-05

Training epoch-21 batch-97
Running loss of epoch-21 batch-97 = 2.1378742530941963e-05

Training epoch-21 batch-98
Running loss of epoch-21 batch-98 = 3.365462180227041e-05

Training epoch-21 batch-99
Running loss of epoch-21 batch-99 = 2.001808024942875e-05

Training epoch-21 batch-100
Running loss of epoch-21 batch-100 = 4.238146357238293e-05

Training epoch-21 batch-101
Running loss of epoch-21 batch-101 = 4.710652865469456e-05

Training epoch-21 batch-102
Running loss of epoch-21 batch-102 = 5.226035136729479e-05

Training epoch-21 batch-103
Running loss of epoch-21 batch-103 = 6.58741919323802e-05

Training epoch-21 batch-104
Running loss of epoch-21 batch-104 = 2.3251865059137344e-05

Training epoch-21 batch-105
Running loss of epoch-21 batch-105 = 7.491966243833303e-05

Training epoch-21 batch-106
Running loss of epoch-21 batch-106 = 7.352931424975395e-05

Training epoch-21 batch-107
Running loss of epoch-21 batch-107 = 7.670046761631966e-05

Training epoch-21 batch-108
Running loss of epoch-21 batch-108 = 2.063752617686987e-05

Training epoch-21 batch-109
Running loss of epoch-21 batch-109 = 5.908671300858259e-05

Training epoch-21 batch-110
Running loss of epoch-21 batch-110 = 1.7104903236031532e-05

Training epoch-21 batch-111
Running loss of epoch-21 batch-111 = 2.195383422076702e-05

Training epoch-21 batch-112
Running loss of epoch-21 batch-112 = 1.625274308025837e-05

Training epoch-21 batch-113
Running loss of epoch-21 batch-113 = 5.295046139508486e-05

Training epoch-21 batch-114
Running loss of epoch-21 batch-114 = 3.066146746277809e-05

Training epoch-21 batch-115
Running loss of epoch-21 batch-115 = 6.697326898574829e-05

Training epoch-21 batch-116
Running loss of epoch-21 batch-116 = 0.00014563265722244978

Training epoch-21 batch-117
Running loss of epoch-21 batch-117 = 3.030337393283844e-05

Training epoch-21 batch-118
Running loss of epoch-21 batch-118 = 0.00010427180677652359

Training epoch-21 batch-119
Running loss of epoch-21 batch-119 = 7.085117977112532e-05

Training epoch-21 batch-120
Running loss of epoch-21 batch-120 = 3.944407217204571e-05

Training epoch-21 batch-121
Running loss of epoch-21 batch-121 = 2.9150047339498997e-05

Training epoch-21 batch-122
Running loss of epoch-21 batch-122 = 5.235930439084768e-05

Training epoch-21 batch-123
Running loss of epoch-21 batch-123 = 7.111311424523592e-05

Training epoch-21 batch-124
Running loss of epoch-21 batch-124 = 0.00013333884999155998

Training epoch-21 batch-125
Running loss of epoch-21 batch-125 = 2.069445326924324e-05

Training epoch-21 batch-126
Running loss of epoch-21 batch-126 = 1.828954555094242e-05

Training epoch-21 batch-127
Running loss of epoch-21 batch-127 = 5.277479067444801e-05

Training epoch-21 batch-128
Running loss of epoch-21 batch-128 = 3.687804564833641e-05

Training epoch-21 batch-129
Running loss of epoch-21 batch-129 = 0.00011802185326814651

Training epoch-21 batch-130
Running loss of epoch-21 batch-130 = 4.8743211664259434e-05

Training epoch-21 batch-131
Running loss of epoch-21 batch-131 = 1.74851156771183e-05

Training epoch-21 batch-132
Running loss of epoch-21 batch-132 = 3.7120538763701916e-05

Training epoch-21 batch-133
Running loss of epoch-21 batch-133 = 3.7721008993685246e-05

Training epoch-21 batch-134
Running loss of epoch-21 batch-134 = 1.4467397704720497e-05

Training epoch-21 batch-135
Running loss of epoch-21 batch-135 = 5.354906897991896e-05

Training epoch-21 batch-136
Running loss of epoch-21 batch-136 = 2.214149571955204e-05

Training epoch-21 batch-137
Running loss of epoch-21 batch-137 = 5.646306090056896e-05

Training epoch-21 batch-138
Running loss of epoch-21 batch-138 = 6.47923443466425e-05

Training epoch-21 batch-139
Running loss of epoch-21 batch-139 = 2.787308767437935e-05

Training epoch-21 batch-140
Running loss of epoch-21 batch-140 = 6.252026651054621e-05

Training epoch-21 batch-141
Running loss of epoch-21 batch-141 = 0.00011016684584319592

Training epoch-21 batch-142
Running loss of epoch-21 batch-142 = 3.0426308512687683e-05

Training epoch-21 batch-143
Running loss of epoch-21 batch-143 = 4.3014646507799625e-05

Training epoch-21 batch-144
Running loss of epoch-21 batch-144 = 7.129332516342402e-05

Training epoch-21 batch-145
Running loss of epoch-21 batch-145 = 3.097380977123976e-05

Training epoch-21 batch-146
Running loss of epoch-21 batch-146 = 8.798111230134964e-05

Training epoch-21 batch-147
Running loss of epoch-21 batch-147 = 4.769663792103529e-05

Training epoch-21 batch-148
Running loss of epoch-21 batch-148 = 6.354902870953083e-05

Training epoch-21 batch-149
Running loss of epoch-21 batch-149 = 4.2408471927046776e-05

Training epoch-21 batch-150
Running loss of epoch-21 batch-150 = 4.54967375844717e-05

Training epoch-21 batch-151
Running loss of epoch-21 batch-151 = 2.4055596441030502e-05

Training epoch-21 batch-152
Running loss of epoch-21 batch-152 = 1.8172082491219044e-05

Training epoch-21 batch-153
Running loss of epoch-21 batch-153 = 4.4711166992783546e-05

Training epoch-21 batch-154
Running loss of epoch-21 batch-154 = 4.613935016095638e-05

Training epoch-21 batch-155
Running loss of epoch-21 batch-155 = 4.8564630560576916e-05

Training epoch-21 batch-156
Running loss of epoch-21 batch-156 = 6.427231710404158e-05

Training epoch-21 batch-157
Running loss of epoch-21 batch-157 = 2.064928412437439e-05

Finished training epoch-21.



Average train loss at epoch-21 = 5.174794569611549e-05

Started Evaluation

Average val loss at epoch-21 = 0.8882519557629214

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.21 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 87.37 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 44.58 %
Accuracy for class get is: 73.33 %

Overall Accuracy = 83.21 %

Finished Evaluation



Started training epoch-22


Training epoch-22 batch-1
Running loss of epoch-22 batch-1 = 8.227466605603695e-05

Training epoch-22 batch-2
Running loss of epoch-22 batch-2 = 0.0002079048426821828

Training epoch-22 batch-3
Running loss of epoch-22 batch-3 = 7.204106077551842e-05

Training epoch-22 batch-4
Running loss of epoch-22 batch-4 = 5.515501834452152e-05

Training epoch-22 batch-5
Running loss of epoch-22 batch-5 = 0.00010350835509598255

Training epoch-22 batch-6
Running loss of epoch-22 batch-6 = 2.6685185730457306e-05

Training epoch-22 batch-7
Running loss of epoch-22 batch-7 = 0.00012243865057826042

Training epoch-22 batch-8
Running loss of epoch-22 batch-8 = 5.80355990678072e-05

Training epoch-22 batch-9
Running loss of epoch-22 batch-9 = 1.2546894140541553e-05

Training epoch-22 batch-10
Running loss of epoch-22 batch-10 = 3.203796222805977e-05

Training epoch-22 batch-11
Running loss of epoch-22 batch-11 = 4.21272125095129e-05

Training epoch-22 batch-12
Running loss of epoch-22 batch-12 = 7.49367754906416e-05

Training epoch-22 batch-13
Running loss of epoch-22 batch-13 = 1.1734897270798683e-05

Training epoch-22 batch-14
Running loss of epoch-22 batch-14 = 5.3733354434370995e-05

Training epoch-22 batch-15
Running loss of epoch-22 batch-15 = 0.00010091648437082767

Training epoch-22 batch-16
Running loss of epoch-22 batch-16 = 7.73412175476551e-05

Training epoch-22 batch-17
Running loss of epoch-22 batch-17 = 3.962777554988861e-05

Training epoch-22 batch-18
Running loss of epoch-22 batch-18 = 2.63899564743042e-05

Training epoch-22 batch-19
Running loss of epoch-22 batch-19 = 5.447177682071924e-05

Training epoch-22 batch-20
Running loss of epoch-22 batch-20 = 1.5309546142816544e-05

Training epoch-22 batch-21
Running loss of epoch-22 batch-21 = 2.6323250494897366e-05

Training epoch-22 batch-22
Running loss of epoch-22 batch-22 = 6.0227932408452034e-05

Training epoch-22 batch-23
Running loss of epoch-22 batch-23 = 3.7325313314795494e-05

Training epoch-22 batch-24
Running loss of epoch-22 batch-24 = 2.089270856231451e-05

Training epoch-22 batch-25
Running loss of epoch-22 batch-25 = 8.311506826430559e-05

Training epoch-22 batch-26
Running loss of epoch-22 batch-26 = 4.904065281152725e-05

Training epoch-22 batch-27
Running loss of epoch-22 batch-27 = 2.805120311677456e-05

Training epoch-22 batch-28
Running loss of epoch-22 batch-28 = 5.675037391483784e-05

Training epoch-22 batch-29
Running loss of epoch-22 batch-29 = 5.0984323024749756e-05

Training epoch-22 batch-30
Running loss of epoch-22 batch-30 = 1.1942582204937935e-05

Training epoch-22 batch-31
Running loss of epoch-22 batch-31 = 3.5686884075403214e-05

Training epoch-22 batch-32
Running loss of epoch-22 batch-32 = 2.9971706680953503e-05

Training epoch-22 batch-33
Running loss of epoch-22 batch-33 = 1.0399846360087395e-05

Training epoch-22 batch-34
Running loss of epoch-22 batch-34 = 2.197851426899433e-05

Training epoch-22 batch-35
Running loss of epoch-22 batch-35 = 1.3742130249738693e-05

Training epoch-22 batch-36
Running loss of epoch-22 batch-36 = 8.149060886353254e-05

Training epoch-22 batch-37
Running loss of epoch-22 batch-37 = 2.728705294430256e-05

Training epoch-22 batch-38
Running loss of epoch-22 batch-38 = 3.750878386199474e-05

Training epoch-22 batch-39
Running loss of epoch-22 batch-39 = 3.9738952182233334e-05

Training epoch-22 batch-40
Running loss of epoch-22 batch-40 = 2.8664711862802505e-05

Training epoch-22 batch-41
Running loss of epoch-22 batch-41 = 5.710078403353691e-05

Training epoch-22 batch-42
Running loss of epoch-22 batch-42 = 8.768972475081682e-05

Training epoch-22 batch-43
Running loss of epoch-22 batch-43 = 4.344526678323746e-05

Training epoch-22 batch-44
Running loss of epoch-22 batch-44 = 0.00016479974146932364

Training epoch-22 batch-45
Running loss of epoch-22 batch-45 = 3.2135751098394394e-05

Training epoch-22 batch-46
Running loss of epoch-22 batch-46 = 2.365885302424431e-05

Training epoch-22 batch-47
Running loss of epoch-22 batch-47 = 8.509727194905281e-06

Training epoch-22 batch-48
Running loss of epoch-22 batch-48 = 4.874938167631626e-05

Training epoch-22 batch-49
Running loss of epoch-22 batch-49 = 4.1936058551073074e-05

Training epoch-22 batch-50
Running loss of epoch-22 batch-50 = 3.031129017472267e-05

Training epoch-22 batch-51
Running loss of epoch-22 batch-51 = 2.754712477326393e-05

Training epoch-22 batch-52
Running loss of epoch-22 batch-52 = 4.656135570257902e-05

Training epoch-22 batch-53
Running loss of epoch-22 batch-53 = 4.553759936243296e-05

Training epoch-22 batch-54
Running loss of epoch-22 batch-54 = 9.268219582736492e-05

Training epoch-22 batch-55
Running loss of epoch-22 batch-55 = 1.772027462720871e-05

Training epoch-22 batch-56
Running loss of epoch-22 batch-56 = 2.309901174157858e-05

Training epoch-22 batch-57
Running loss of epoch-22 batch-57 = 4.066666588187218e-05

Training epoch-22 batch-58
Running loss of epoch-22 batch-58 = 5.4170144721865654e-05

Training epoch-22 batch-59
Running loss of epoch-22 batch-59 = 1.1036871001124382e-05

Training epoch-22 batch-60
Running loss of epoch-22 batch-60 = 6.622076034545898e-05

Training epoch-22 batch-61
Running loss of epoch-22 batch-61 = 4.370301030576229e-05

Training epoch-22 batch-62
Running loss of epoch-22 batch-62 = 2.8405804187059402e-05

Training epoch-22 batch-63
Running loss of epoch-22 batch-63 = 2.8653303161263466e-05

Training epoch-22 batch-64
Running loss of epoch-22 batch-64 = 4.1283899918198586e-05

Training epoch-22 batch-65
Running loss of epoch-22 batch-65 = 6.454368121922016e-05

Training epoch-22 batch-66
Running loss of epoch-22 batch-66 = 1.5563913621008396e-05

Training epoch-22 batch-67
Running loss of epoch-22 batch-67 = 0.00014120724517852068

Training epoch-22 batch-68
Running loss of epoch-22 batch-68 = 2.6438385248184204e-05

Training epoch-22 batch-69
Running loss of epoch-22 batch-69 = 0.0001357354922220111

Training epoch-22 batch-70
Running loss of epoch-22 batch-70 = 4.809326492249966e-05

Training epoch-22 batch-71
Running loss of epoch-22 batch-71 = 3.59776895493269e-05

Training epoch-22 batch-72
Running loss of epoch-22 batch-72 = 4.6890927478671074e-05

Training epoch-22 batch-73
Running loss of epoch-22 batch-73 = 0.00012414716184139252

Training epoch-22 batch-74
Running loss of epoch-22 batch-74 = 2.7867499738931656e-05

Training epoch-22 batch-75
Running loss of epoch-22 batch-75 = 3.4199911169707775e-05

Training epoch-22 batch-76
Running loss of epoch-22 batch-76 = 2.0916340872645378e-05

Training epoch-22 batch-77
Running loss of epoch-22 batch-77 = 3.675010520964861e-05

Training epoch-22 batch-78
Running loss of epoch-22 batch-78 = 6.563973147422075e-05

Training epoch-22 batch-79
Running loss of epoch-22 batch-79 = 1.978408545255661e-05

Training epoch-22 batch-80
Running loss of epoch-22 batch-80 = 5.3464435040950775e-05

Training epoch-22 batch-81
Running loss of epoch-22 batch-81 = 1.7390819266438484e-05

Training epoch-22 batch-82
Running loss of epoch-22 batch-82 = 4.715810064226389e-05

Training epoch-22 batch-83
Running loss of epoch-22 batch-83 = 2.4594366550445557e-05

Training epoch-22 batch-84
Running loss of epoch-22 batch-84 = 3.527337685227394e-05

Training epoch-22 batch-85
Running loss of epoch-22 batch-85 = 4.0210550650954247e-05

Training epoch-22 batch-86
Running loss of epoch-22 batch-86 = 5.3078169003129005e-05

Training epoch-22 batch-87
Running loss of epoch-22 batch-87 = 3.1710369512438774e-05

Training epoch-22 batch-88
Running loss of epoch-22 batch-88 = 8.190353401005268e-05

Training epoch-22 batch-89
Running loss of epoch-22 batch-89 = 3.8371188566088676e-05

Training epoch-22 batch-90
Running loss of epoch-22 batch-90 = 1.011858694255352e-05

Training epoch-22 batch-91
Running loss of epoch-22 batch-91 = 1.6321777366101742e-05

Training epoch-22 batch-92
Running loss of epoch-22 batch-92 = 9.325752034783363e-05

Training epoch-22 batch-93
Running loss of epoch-22 batch-93 = 2.9743649065494537e-05

Training epoch-22 batch-94
Running loss of epoch-22 batch-94 = 3.6590383388102055e-05

Training epoch-22 batch-95
Running loss of epoch-22 batch-95 = 2.3363041691482067e-05

Training epoch-22 batch-96
Running loss of epoch-22 batch-96 = 6.898492574691772e-05

Training epoch-22 batch-97
Running loss of epoch-22 batch-97 = 3.858108539134264e-05

Training epoch-22 batch-98
Running loss of epoch-22 batch-98 = 4.197540692985058e-05

Training epoch-22 batch-99
Running loss of epoch-22 batch-99 = 6.02679792791605e-05

Training epoch-22 batch-100
Running loss of epoch-22 batch-100 = 2.693571150302887e-05

Training epoch-22 batch-101
Running loss of epoch-22 batch-101 = 2.933409996330738e-05

Training epoch-22 batch-102
Running loss of epoch-22 batch-102 = 2.950802445411682e-05

Training epoch-22 batch-103
Running loss of epoch-22 batch-103 = 1.1611147783696651e-05

Training epoch-22 batch-104
Running loss of epoch-22 batch-104 = 2.826121635735035e-05

Training epoch-22 batch-105
Running loss of epoch-22 batch-105 = 4.961912054568529e-05

Training epoch-22 batch-106
Running loss of epoch-22 batch-106 = 2.805911935865879e-05

Training epoch-22 batch-107
Running loss of epoch-22 batch-107 = 7.341289892792702e-05

Training epoch-22 batch-108
Running loss of epoch-22 batch-108 = 5.958648398518562e-05

Training epoch-22 batch-109
Running loss of epoch-22 batch-109 = 5.6001823395490646e-05

Training epoch-22 batch-110
Running loss of epoch-22 batch-110 = 3.173248842358589e-05

Training epoch-22 batch-111
Running loss of epoch-22 batch-111 = 6.696733180433512e-05

Training epoch-22 batch-112
Running loss of epoch-22 batch-112 = 2.6531284675002098e-05

Training epoch-22 batch-113
Running loss of epoch-22 batch-113 = 9.380874689668417e-05

Training epoch-22 batch-114
Running loss of epoch-22 batch-114 = 2.3260479792952538e-05

Training epoch-22 batch-115
Running loss of epoch-22 batch-115 = 9.95788723230362e-05

Training epoch-22 batch-116
Running loss of epoch-22 batch-116 = 1.7500482499599457e-05

Training epoch-22 batch-117
Running loss of epoch-22 batch-117 = 2.566492184996605e-05

Training epoch-22 batch-118
Running loss of epoch-22 batch-118 = 7.265142630785704e-05

Training epoch-22 batch-119
Running loss of epoch-22 batch-119 = 3.922183532267809e-05

Training epoch-22 batch-120
Running loss of epoch-22 batch-120 = 2.5774934329092503e-05

Training epoch-22 batch-121
Running loss of epoch-22 batch-121 = 2.0499341189861298e-05

Training epoch-22 batch-122
Running loss of epoch-22 batch-122 = 1.6064848750829697e-05

Training epoch-22 batch-123
Running loss of epoch-22 batch-123 = 6.287568248808384e-05

Training epoch-22 batch-124
Running loss of epoch-22 batch-124 = 2.3672357201576233e-05

Training epoch-22 batch-125
Running loss of epoch-22 batch-125 = 2.7570058591663837e-05

Training epoch-22 batch-126
Running loss of epoch-22 batch-126 = 7.464026566594839e-05

Training epoch-22 batch-127
Running loss of epoch-22 batch-127 = 4.313676618039608e-05

Training epoch-22 batch-128
Running loss of epoch-22 batch-128 = 2.9200222343206406e-05

Training epoch-22 batch-129
Running loss of epoch-22 batch-129 = 1.753761898726225e-05

Training epoch-22 batch-130
Running loss of epoch-22 batch-130 = 5.043414421379566e-05

Training epoch-22 batch-131
Running loss of epoch-22 batch-131 = 1.7436454072594643e-05

Training epoch-22 batch-132
Running loss of epoch-22 batch-132 = 1.7182668671011925e-05

Training epoch-22 batch-133
Running loss of epoch-22 batch-133 = 1.1949916370213032e-05

Training epoch-22 batch-134
Running loss of epoch-22 batch-134 = 2.910662442445755e-05

Training epoch-22 batch-135
Running loss of epoch-22 batch-135 = 2.8648413717746735e-05

Training epoch-22 batch-136
Running loss of epoch-22 batch-136 = 2.2318679839372635e-05

Training epoch-22 batch-137
Running loss of epoch-22 batch-137 = 2.0741019397974014e-05

Training epoch-22 batch-138
Running loss of epoch-22 batch-138 = 4.921876825392246e-05

Training epoch-22 batch-139
Running loss of epoch-22 batch-139 = 3.175961319357157e-05

Training epoch-22 batch-140
Running loss of epoch-22 batch-140 = 0.0003345876466482878

Training epoch-22 batch-141
Running loss of epoch-22 batch-141 = 2.5775516405701637e-05

Training epoch-22 batch-142
Running loss of epoch-22 batch-142 = 2.251635305583477e-05

Training epoch-22 batch-143
Running loss of epoch-22 batch-143 = 2.5640358217060566e-05

Training epoch-22 batch-144
Running loss of epoch-22 batch-144 = 6.315193604677916e-05

Training epoch-22 batch-145
Running loss of epoch-22 batch-145 = 5.7026511058211327e-05

Training epoch-22 batch-146
Running loss of epoch-22 batch-146 = 4.381546750664711e-05

Training epoch-22 batch-147
Running loss of epoch-22 batch-147 = 3.3874064683914185e-05

Training epoch-22 batch-148
Running loss of epoch-22 batch-148 = 1.468416303396225e-05

Training epoch-22 batch-149
Running loss of epoch-22 batch-149 = 8.665909990668297e-05

Training epoch-22 batch-150
Running loss of epoch-22 batch-150 = 4.239380359649658e-05

Training epoch-22 batch-151
Running loss of epoch-22 batch-151 = 1.9965926185250282e-05

Training epoch-22 batch-152
Running loss of epoch-22 batch-152 = 4.329101648181677e-05

Training epoch-22 batch-153
Running loss of epoch-22 batch-153 = 2.7357833459973335e-05

Training epoch-22 batch-154
Running loss of epoch-22 batch-154 = 5.266768857836723e-05

Training epoch-22 batch-155
Running loss of epoch-22 batch-155 = 3.8258498534560204e-05

Training epoch-22 batch-156
Running loss of epoch-22 batch-156 = 4.888547118753195e-05

Training epoch-22 batch-157
Running loss of epoch-22 batch-157 = 1.3310462236404419e-05

Finished training epoch-22.



Average train loss at epoch-22 = 4.675183370709419e-05

Started Evaluation

Average val loss at epoch-22 = 0.9004471018812948

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 50.45 %
Accuracy for class execute is: 45.78 %
Accuracy for class get is: 67.44 %

Overall Accuracy = 82.96 %

Finished Evaluation



Started training epoch-23


Training epoch-23 batch-1
Running loss of epoch-23 batch-1 = 5.410728044807911e-05

Training epoch-23 batch-2
Running loss of epoch-23 batch-2 = 3.3206306397914886e-05

Training epoch-23 batch-3
Running loss of epoch-23 batch-3 = 1.5382422134280205e-05

Training epoch-23 batch-4
Running loss of epoch-23 batch-4 = 3.45578882843256e-05

Training epoch-23 batch-5
Running loss of epoch-23 batch-5 = 5.180446896702051e-05

Training epoch-23 batch-6
Running loss of epoch-23 batch-6 = 5.5906944908201694e-05

Training epoch-23 batch-7
Running loss of epoch-23 batch-7 = 0.00012093177065253258

Training epoch-23 batch-8
Running loss of epoch-23 batch-8 = 5.185278132557869e-05

Training epoch-23 batch-9
Running loss of epoch-23 batch-9 = 4.937266930937767e-05

Training epoch-23 batch-10
Running loss of epoch-23 batch-10 = 4.6315137296915054e-05

Training epoch-23 batch-11
Running loss of epoch-23 batch-11 = 2.5892164558172226e-05

Training epoch-23 batch-12
Running loss of epoch-23 batch-12 = 4.5676715672016144e-05

Training epoch-23 batch-13
Running loss of epoch-23 batch-13 = 5.318061448633671e-05

Training epoch-23 batch-14
Running loss of epoch-23 batch-14 = 3.483961336314678e-05

Training epoch-23 batch-15
Running loss of epoch-23 batch-15 = 4.3109990656375885e-05

Training epoch-23 batch-16
Running loss of epoch-23 batch-16 = 4.155898932367563e-05

Training epoch-23 batch-17
Running loss of epoch-23 batch-17 = 6.0137128457427025e-05

Training epoch-23 batch-18
Running loss of epoch-23 batch-18 = 2.9265880584716797e-05

Training epoch-23 batch-19
Running loss of epoch-23 batch-19 = 6.955326534807682e-05

Training epoch-23 batch-20
Running loss of epoch-23 batch-20 = 1.048319973051548e-05

Training epoch-23 batch-21
Running loss of epoch-23 batch-21 = 0.00011258909944444895

Training epoch-23 batch-22
Running loss of epoch-23 batch-22 = 3.3531803637742996e-05

Training epoch-23 batch-23
Running loss of epoch-23 batch-23 = 6.418838165700436e-05

Training epoch-23 batch-24
Running loss of epoch-23 batch-24 = 2.406095154583454e-05

Training epoch-23 batch-25
Running loss of epoch-23 batch-25 = 1.6743550077080727e-05

Training epoch-23 batch-26
Running loss of epoch-23 batch-26 = 4.126806743443012e-05

Training epoch-23 batch-27
Running loss of epoch-23 batch-27 = 3.642262890934944e-05

Training epoch-23 batch-28
Running loss of epoch-23 batch-28 = 2.7186935767531395e-05

Training epoch-23 batch-29
Running loss of epoch-23 batch-29 = 1.25060323625803e-05

Training epoch-23 batch-30
Running loss of epoch-23 batch-30 = 2.4957582354545593e-05

Training epoch-23 batch-31
Running loss of epoch-23 batch-31 = 3.804848529398441e-05

Training epoch-23 batch-32
Running loss of epoch-23 batch-32 = 3.7161516956985e-05

Training epoch-23 batch-33
Running loss of epoch-23 batch-33 = 2.131727524101734e-05

Training epoch-23 batch-34
Running loss of epoch-23 batch-34 = 3.155902959406376e-05

Training epoch-23 batch-35
Running loss of epoch-23 batch-35 = 3.5309698432683945e-05

Training epoch-23 batch-36
Running loss of epoch-23 batch-36 = 5.080364644527435e-05

Training epoch-23 batch-37
Running loss of epoch-23 batch-37 = 4.793400876224041e-05

Training epoch-23 batch-38
Running loss of epoch-23 batch-38 = 1.7982441931962967e-05

Training epoch-23 batch-39
Running loss of epoch-23 batch-39 = 4.0588551200926304e-05

Training epoch-23 batch-40
Running loss of epoch-23 batch-40 = 2.475979272276163e-05

Training epoch-23 batch-41
Running loss of epoch-23 batch-41 = 1.7264625057578087e-05

Training epoch-23 batch-42
Running loss of epoch-23 batch-42 = 3.291107714176178e-05

Training epoch-23 batch-43
Running loss of epoch-23 batch-43 = 3.599701449275017e-05

Training epoch-23 batch-44
Running loss of epoch-23 batch-44 = 5.6147342547774315e-05

Training epoch-23 batch-45
Running loss of epoch-23 batch-45 = 0.0001038906630128622

Training epoch-23 batch-46
Running loss of epoch-23 batch-46 = 7.142755202949047e-05

Training epoch-23 batch-47
Running loss of epoch-23 batch-47 = 1.5585683286190033e-05

Training epoch-23 batch-48
Running loss of epoch-23 batch-48 = 0.0001942003145813942

Training epoch-23 batch-49
Running loss of epoch-23 batch-49 = 3.2946933060884476e-05

Training epoch-23 batch-50
Running loss of epoch-23 batch-50 = 5.2641611546278e-05

Training epoch-23 batch-51
Running loss of epoch-23 batch-51 = 1.1003809049725533e-05

Training epoch-23 batch-52
Running loss of epoch-23 batch-52 = 2.6256893761456013e-05

Training epoch-23 batch-53
Running loss of epoch-23 batch-53 = 2.9579270631074905e-05

Training epoch-23 batch-54
Running loss of epoch-23 batch-54 = 1.7852289602160454e-05

Training epoch-23 batch-55
Running loss of epoch-23 batch-55 = 3.127066884189844e-05

Training epoch-23 batch-56
Running loss of epoch-23 batch-56 = 2.089235931634903e-05

Training epoch-23 batch-57
Running loss of epoch-23 batch-57 = 3.113178536295891e-05

Training epoch-23 batch-58
Running loss of epoch-23 batch-58 = 1.2347474694252014e-05

Training epoch-23 batch-59
Running loss of epoch-23 batch-59 = 2.5718589313328266e-05

Training epoch-23 batch-60
Running loss of epoch-23 batch-60 = 3.905233461409807e-05

Training epoch-23 batch-61
Running loss of epoch-23 batch-61 = 5.3997617214918137e-05

Training epoch-23 batch-62
Running loss of epoch-23 batch-62 = 2.8198817744851112e-05

Training epoch-23 batch-63
Running loss of epoch-23 batch-63 = 4.7598034143447876e-05

Training epoch-23 batch-64
Running loss of epoch-23 batch-64 = 1.788313966244459e-05

Training epoch-23 batch-65
Running loss of epoch-23 batch-65 = 3.810471389442682e-05

Training epoch-23 batch-66
Running loss of epoch-23 batch-66 = 1.825089566409588e-05

Training epoch-23 batch-67
Running loss of epoch-23 batch-67 = 3.0828407034277916e-05

Training epoch-23 batch-68
Running loss of epoch-23 batch-68 = 6.839819252490997e-05

Training epoch-23 batch-69
Running loss of epoch-23 batch-69 = 3.4537981264293194e-05

Training epoch-23 batch-70
Running loss of epoch-23 batch-70 = 4.021089989691973e-05

Training epoch-23 batch-71
Running loss of epoch-23 batch-71 = 3.7526246160268784e-05

Training epoch-23 batch-72
Running loss of epoch-23 batch-72 = 1.484330277889967e-05

Training epoch-23 batch-73
Running loss of epoch-23 batch-73 = 2.3503554984927177e-05

Training epoch-23 batch-74
Running loss of epoch-23 batch-74 = 1.6880454495549202e-05

Training epoch-23 batch-75
Running loss of epoch-23 batch-75 = 3.1441450119018555e-05

Training epoch-23 batch-76
Running loss of epoch-23 batch-76 = 8.589413482695818e-05

Training epoch-23 batch-77
Running loss of epoch-23 batch-77 = 4.7798966988921165e-05

Training epoch-23 batch-78
Running loss of epoch-23 batch-78 = 6.558210588991642e-05

Training epoch-23 batch-79
Running loss of epoch-23 batch-79 = 5.0622737035155296e-05

Training epoch-23 batch-80
Running loss of epoch-23 batch-80 = 2.3133354261517525e-05

Training epoch-23 batch-81
Running loss of epoch-23 batch-81 = 3.8709258660674095e-05

Training epoch-23 batch-82
Running loss of epoch-23 batch-82 = 1.4367513358592987e-05

Training epoch-23 batch-83
Running loss of epoch-23 batch-83 = 2.108374610543251e-05

Training epoch-23 batch-84
Running loss of epoch-23 batch-84 = 2.2125430405139923e-05

Training epoch-23 batch-85
Running loss of epoch-23 batch-85 = 2.4981098249554634e-05

Training epoch-23 batch-86
Running loss of epoch-23 batch-86 = 3.0425842851400375e-05

Training epoch-23 batch-87
Running loss of epoch-23 batch-87 = 1.8684426322579384e-05

Training epoch-23 batch-88
Running loss of epoch-23 batch-88 = 2.338760532438755e-05

Training epoch-23 batch-89
Running loss of epoch-23 batch-89 = 1.7765210941433907e-05

Training epoch-23 batch-90
Running loss of epoch-23 batch-90 = 3.602623473852873e-05

Training epoch-23 batch-91
Running loss of epoch-23 batch-91 = 3.1582312658429146e-05

Training epoch-23 batch-92
Running loss of epoch-23 batch-92 = 1.6735168173909187e-05

Training epoch-23 batch-93
Running loss of epoch-23 batch-93 = 4.63689211755991e-05

Training epoch-23 batch-94
Running loss of epoch-23 batch-94 = 4.110555164515972e-05

Training epoch-23 batch-95
Running loss of epoch-23 batch-95 = 1.3597309589385986e-05

Training epoch-23 batch-96
Running loss of epoch-23 batch-96 = 2.371089067310095e-05

Training epoch-23 batch-97
Running loss of epoch-23 batch-97 = 4.1995663195848465e-05

Training epoch-23 batch-98
Running loss of epoch-23 batch-98 = 2.0282110199332237e-05

Training epoch-23 batch-99
Running loss of epoch-23 batch-99 = 3.6468496546149254e-05

Training epoch-23 batch-100
Running loss of epoch-23 batch-100 = 3.128941170871258e-05

Training epoch-23 batch-101
Running loss of epoch-23 batch-101 = 1.977081410586834e-05

Training epoch-23 batch-102
Running loss of epoch-23 batch-102 = 5.591590888798237e-05

Training epoch-23 batch-103
Running loss of epoch-23 batch-103 = 8.576305117458105e-05

Training epoch-23 batch-104
Running loss of epoch-23 batch-104 = 1.8608057871460915e-05

Training epoch-23 batch-105
Running loss of epoch-23 batch-105 = 2.7301721274852753e-05

Training epoch-23 batch-106
Running loss of epoch-23 batch-106 = 5.437969230115414e-05

Training epoch-23 batch-107
Running loss of epoch-23 batch-107 = 1.1877389624714851e-05

Training epoch-23 batch-108
Running loss of epoch-23 batch-108 = 3.668980207294226e-05

Training epoch-23 batch-109
Running loss of epoch-23 batch-109 = 1.9262428395450115e-05

Training epoch-23 batch-110
Running loss of epoch-23 batch-110 = 3.06152505800128e-05

Training epoch-23 batch-111
Running loss of epoch-23 batch-111 = 2.6823836378753185e-05

Training epoch-23 batch-112
Running loss of epoch-23 batch-112 = 1.9091065041720867e-05

Training epoch-23 batch-113
Running loss of epoch-23 batch-113 = 1.713423989713192e-05

Training epoch-23 batch-114
Running loss of epoch-23 batch-114 = 3.8544414564967155e-05

Training epoch-23 batch-115
Running loss of epoch-23 batch-115 = 2.047186717391014e-05

Training epoch-23 batch-116
Running loss of epoch-23 batch-116 = 5.41668850928545e-05

Training epoch-23 batch-117
Running loss of epoch-23 batch-117 = 5.4117292165756226e-05

Training epoch-23 batch-118
Running loss of epoch-23 batch-118 = 0.00015678827185183764

Training epoch-23 batch-119
Running loss of epoch-23 batch-119 = 6.610201671719551e-05

Training epoch-23 batch-120
Running loss of epoch-23 batch-120 = 6.13213051110506e-05

Training epoch-23 batch-121
Running loss of epoch-23 batch-121 = 2.4704495444893837e-05

Training epoch-23 batch-122
Running loss of epoch-23 batch-122 = 1.870165579020977e-05

Training epoch-23 batch-123
Running loss of epoch-23 batch-123 = 5.21752517670393e-05

Training epoch-23 batch-124
Running loss of epoch-23 batch-124 = 4.62669413536787e-05

Training epoch-23 batch-125
Running loss of epoch-23 batch-125 = 1.4249933883547783e-05

Training epoch-23 batch-126
Running loss of epoch-23 batch-126 = 2.499343827366829e-05

Training epoch-23 batch-127
Running loss of epoch-23 batch-127 = 9.107869118452072e-06

Training epoch-23 batch-128
Running loss of epoch-23 batch-128 = 1.8255319446325302e-05

Training epoch-23 batch-129
Running loss of epoch-23 batch-129 = 3.899121657013893e-05

Training epoch-23 batch-130
Running loss of epoch-23 batch-130 = 0.00011495989747345448

Training epoch-23 batch-131
Running loss of epoch-23 batch-131 = 2.3660017177462578e-05

Training epoch-23 batch-132
Running loss of epoch-23 batch-132 = 0.0001134221674874425

Training epoch-23 batch-133
Running loss of epoch-23 batch-133 = 3.973557613790035e-05

Training epoch-23 batch-134
Running loss of epoch-23 batch-134 = 1.1405209079384804e-05

Training epoch-23 batch-135
Running loss of epoch-23 batch-135 = 2.6388908736407757e-05

Training epoch-23 batch-136
Running loss of epoch-23 batch-136 = 2.395431511104107e-05

Training epoch-23 batch-137
Running loss of epoch-23 batch-137 = 4.377821460366249e-05

Training epoch-23 batch-138
Running loss of epoch-23 batch-138 = 1.8214574083685875e-05

Training epoch-23 batch-139
Running loss of epoch-23 batch-139 = 3.94466333091259e-05

Training epoch-23 batch-140
Running loss of epoch-23 batch-140 = 4.6430621296167374e-05

Training epoch-23 batch-141
Running loss of epoch-23 batch-141 = 3.7905992940068245e-05

Training epoch-23 batch-142
Running loss of epoch-23 batch-142 = 4.3270643800497055e-05

Training epoch-23 batch-143
Running loss of epoch-23 batch-143 = 2.0131468772888184e-05

Training epoch-23 batch-144
Running loss of epoch-23 batch-144 = 1.4874967746436596e-05

Training epoch-23 batch-145
Running loss of epoch-23 batch-145 = 2.7207774110138416e-05

Training epoch-23 batch-146
Running loss of epoch-23 batch-146 = 3.836583346128464e-05

Training epoch-23 batch-147
Running loss of epoch-23 batch-147 = 4.852074198424816e-05

Training epoch-23 batch-148
Running loss of epoch-23 batch-148 = 2.0029256120324135e-05

Training epoch-23 batch-149
Running loss of epoch-23 batch-149 = 5.4047442972660065e-05

Training epoch-23 batch-150
Running loss of epoch-23 batch-150 = 2.116314135491848e-05

Training epoch-23 batch-151
Running loss of epoch-23 batch-151 = 2.3350119590759277e-05

Training epoch-23 batch-152
Running loss of epoch-23 batch-152 = 1.6806297935545444e-05

Training epoch-23 batch-153
Running loss of epoch-23 batch-153 = 2.234126441180706e-05

Training epoch-23 batch-154
Running loss of epoch-23 batch-154 = 3.667676355689764e-05

Training epoch-23 batch-155
Running loss of epoch-23 batch-155 = 4.136469215154648e-05

Training epoch-23 batch-156
Running loss of epoch-23 batch-156 = 4.341022577136755e-05

Training epoch-23 batch-157
Running loss of epoch-23 batch-157 = 0.00018418952822685242

Finished training epoch-23.



Average train loss at epoch-23 = 3.858689144253731e-05

Started Evaluation

Average val loss at epoch-23 = 0.9168480307978251

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.82 %
Accuracy for class onCreate is: 94.14 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 61.87 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 51.57 %
Accuracy for class execute is: 46.59 %
Accuracy for class get is: 67.95 %

Overall Accuracy = 82.92 %

Finished Evaluation



Started training epoch-24


Training epoch-24 batch-1
Running loss of epoch-24 batch-1 = 2.7937348932027817e-05

Training epoch-24 batch-2
Running loss of epoch-24 batch-2 = 3.5831588320434093e-05

Training epoch-24 batch-3
Running loss of epoch-24 batch-3 = 3.126845695078373e-05

Training epoch-24 batch-4
Running loss of epoch-24 batch-4 = 2.532161306589842e-05

Training epoch-24 batch-5
Running loss of epoch-24 batch-5 = 4.53311949968338e-05

Training epoch-24 batch-6
Running loss of epoch-24 batch-6 = 4.548020660877228e-05

Training epoch-24 batch-7
Running loss of epoch-24 batch-7 = 3.4083263017237186e-05

Training epoch-24 batch-8
Running loss of epoch-24 batch-8 = 9.659212082624435e-06

Training epoch-24 batch-9
Running loss of epoch-24 batch-9 = 4.685355816036463e-05

Training epoch-24 batch-10
Running loss of epoch-24 batch-10 = 3.9885868318378925e-05

Training epoch-24 batch-11
Running loss of epoch-24 batch-11 = 2.0910752937197685e-05

Training epoch-24 batch-12
Running loss of epoch-24 batch-12 = 4.386750515550375e-05

Training epoch-24 batch-13
Running loss of epoch-24 batch-13 = 2.725282683968544e-05

Training epoch-24 batch-14
Running loss of epoch-24 batch-14 = 3.3638207241892815e-05

Training epoch-24 batch-15
Running loss of epoch-24 batch-15 = 3.028206992894411e-05

Training epoch-24 batch-16
Running loss of epoch-24 batch-16 = 6.371771451085806e-05

Training epoch-24 batch-17
Running loss of epoch-24 batch-17 = 4.473770968616009e-05

Training epoch-24 batch-18
Running loss of epoch-24 batch-18 = 1.5158206224441528e-05

Training epoch-24 batch-19
Running loss of epoch-24 batch-19 = 3.4420518204569817e-05

Training epoch-24 batch-20
Running loss of epoch-24 batch-20 = 1.4353310689330101e-05

Training epoch-24 batch-21
Running loss of epoch-24 batch-21 = 2.071796916425228e-05

Training epoch-24 batch-22
Running loss of epoch-24 batch-22 = 4.548381548374891e-05

Training epoch-24 batch-23
Running loss of epoch-24 batch-23 = 6.64611579850316e-05

Training epoch-24 batch-24
Running loss of epoch-24 batch-24 = 2.5425339117646217e-05

Training epoch-24 batch-25
Running loss of epoch-24 batch-25 = 5.835830233991146e-05

Training epoch-24 batch-26
Running loss of epoch-24 batch-26 = 3.239070065319538e-05

Training epoch-24 batch-27
Running loss of epoch-24 batch-27 = 1.8597114831209183e-05

Training epoch-24 batch-28
Running loss of epoch-24 batch-28 = 1.9788509234786034e-05

Training epoch-24 batch-29
Running loss of epoch-24 batch-29 = 1.8955441191792488e-05

Training epoch-24 batch-30
Running loss of epoch-24 batch-30 = 4.132278263568878e-05

Training epoch-24 batch-31
Running loss of epoch-24 batch-31 = 3.9718812331557274e-05

Training epoch-24 batch-32
Running loss of epoch-24 batch-32 = 7.703900337219238e-06

Training epoch-24 batch-33
Running loss of epoch-24 batch-33 = 2.2382941097021103e-05

Training epoch-24 batch-34
Running loss of epoch-24 batch-34 = 1.3704877346754074e-05

Training epoch-24 batch-35
Running loss of epoch-24 batch-35 = 2.6721972972154617e-05

Training epoch-24 batch-36
Running loss of epoch-24 batch-36 = 2.7853529900312424e-05

Training epoch-24 batch-37
Running loss of epoch-24 batch-37 = 2.6343739591538906e-05

Training epoch-24 batch-38
Running loss of epoch-24 batch-38 = 1.4044344425201416e-05

Training epoch-24 batch-39
Running loss of epoch-24 batch-39 = 8.743535727262497e-05

Training epoch-24 batch-40
Running loss of epoch-24 batch-40 = 1.2303236871957779e-05

Training epoch-24 batch-41
Running loss of epoch-24 batch-41 = 6.484310142695904e-05

Training epoch-24 batch-42
Running loss of epoch-24 batch-42 = 3.812415525317192e-05

Training epoch-24 batch-43
Running loss of epoch-24 batch-43 = 3.7128920666873455e-05

Training epoch-24 batch-44
Running loss of epoch-24 batch-44 = 1.8352060578763485e-05

Training epoch-24 batch-45
Running loss of epoch-24 batch-45 = 3.501959145069122e-05

Training epoch-24 batch-46
Running loss of epoch-24 batch-46 = 3.328395541757345e-05

Training epoch-24 batch-47
Running loss of epoch-24 batch-47 = 3.4976634196937084e-05

Training epoch-24 batch-48
Running loss of epoch-24 batch-48 = 2.0299688912928104e-05

Training epoch-24 batch-49
Running loss of epoch-24 batch-49 = 1.2763077393174171e-05

Training epoch-24 batch-50
Running loss of epoch-24 batch-50 = 5.69615513086319e-05

Training epoch-24 batch-51
Running loss of epoch-24 batch-51 = 1.0001473128795624e-05

Training epoch-24 batch-52
Running loss of epoch-24 batch-52 = 3.678072243928909e-05

Training epoch-24 batch-53
Running loss of epoch-24 batch-53 = 0.00015891785733401775

Training epoch-24 batch-54
Running loss of epoch-24 batch-54 = 3.2599433325231075e-05

Training epoch-24 batch-55
Running loss of epoch-24 batch-55 = 3.603717777878046e-05

Training epoch-24 batch-56
Running loss of epoch-24 batch-56 = 1.888175029307604e-05

Training epoch-24 batch-57
Running loss of epoch-24 batch-57 = 4.2645493522286415e-05

Training epoch-24 batch-58
Running loss of epoch-24 batch-58 = 1.1599855497479439e-05

Training epoch-24 batch-59
Running loss of epoch-24 batch-59 = 3.8990750908851624e-05

Training epoch-24 batch-60
Running loss of epoch-24 batch-60 = 3.695627674460411e-05

Training epoch-24 batch-61
Running loss of epoch-24 batch-61 = 1.7867423593997955e-05

Training epoch-24 batch-62
Running loss of epoch-24 batch-62 = 6.941938772797585e-05

Training epoch-24 batch-63
Running loss of epoch-24 batch-63 = 3.1663570553064346e-05

Training epoch-24 batch-64
Running loss of epoch-24 batch-64 = 8.811242878437042e-06

Training epoch-24 batch-65
Running loss of epoch-24 batch-65 = 4.71663661301136e-05

Training epoch-24 batch-66
Running loss of epoch-24 batch-66 = 3.6183279007673264e-05

Training epoch-24 batch-67
Running loss of epoch-24 batch-67 = 1.9643455743789673e-05

Training epoch-24 batch-68
Running loss of epoch-24 batch-68 = 2.3868982680141926e-05

Training epoch-24 batch-69
Running loss of epoch-24 batch-69 = 3.784301225095987e-05

Training epoch-24 batch-70
Running loss of epoch-24 batch-70 = 4.5268330723047256e-05

Training epoch-24 batch-71
Running loss of epoch-24 batch-71 = 2.086954191327095e-05

Training epoch-24 batch-72
Running loss of epoch-24 batch-72 = 7.232720963656902e-05

Training epoch-24 batch-73
Running loss of epoch-24 batch-73 = 1.7974060028791428e-05

Training epoch-24 batch-74
Running loss of epoch-24 batch-74 = 2.303591463714838e-05

Training epoch-24 batch-75
Running loss of epoch-24 batch-75 = 2.7289381250739098e-05

Training epoch-24 batch-76
Running loss of epoch-24 batch-76 = 5.17495209351182e-05

Training epoch-24 batch-77
Running loss of epoch-24 batch-77 = 1.877988688647747e-05

Training epoch-24 batch-78
Running loss of epoch-24 batch-78 = 2.4359440430998802e-05

Training epoch-24 batch-79
Running loss of epoch-24 batch-79 = 3.183167427778244e-05

Training epoch-24 batch-80
Running loss of epoch-24 batch-80 = 3.434298560023308e-05

Training epoch-24 batch-81
Running loss of epoch-24 batch-81 = 2.571963705122471e-05

Training epoch-24 batch-82
Running loss of epoch-24 batch-82 = 3.125588409602642e-05

Training epoch-24 batch-83
Running loss of epoch-24 batch-83 = 1.5225610695779324e-05

Training epoch-24 batch-84
Running loss of epoch-24 batch-84 = 5.659880116581917e-06

Training epoch-24 batch-85
Running loss of epoch-24 batch-85 = 9.124400094151497e-06

Training epoch-24 batch-86
Running loss of epoch-24 batch-86 = 4.325562622398138e-05

Training epoch-24 batch-87
Running loss of epoch-24 batch-87 = 1.761014573276043e-05

Training epoch-24 batch-88
Running loss of epoch-24 batch-88 = 3.576604649424553e-05

Training epoch-24 batch-89
Running loss of epoch-24 batch-89 = 6.561190821230412e-05

Training epoch-24 batch-90
Running loss of epoch-24 batch-90 = 4.203850403428078e-05

Training epoch-24 batch-91
Running loss of epoch-24 batch-91 = 2.733978908509016e-05

Training epoch-24 batch-92
Running loss of epoch-24 batch-92 = 1.9491417333483696e-05

Training epoch-24 batch-93
Running loss of epoch-24 batch-93 = 1.3951677829027176e-05

Training epoch-24 batch-94
Running loss of epoch-24 batch-94 = 4.792725667357445e-05

Training epoch-24 batch-95
Running loss of epoch-24 batch-95 = 1.316494308412075e-05

Training epoch-24 batch-96
Running loss of epoch-24 batch-96 = 3.4158467315137386e-05

Training epoch-24 batch-97
Running loss of epoch-24 batch-97 = 1.691468060016632e-05

Training epoch-24 batch-98
Running loss of epoch-24 batch-98 = 3.211689181625843e-05

Training epoch-24 batch-99
Running loss of epoch-24 batch-99 = 0.00010236748494207859

Training epoch-24 batch-100
Running loss of epoch-24 batch-100 = 1.499522477388382e-05

Training epoch-24 batch-101
Running loss of epoch-24 batch-101 = 5.1013194024562836e-05

Training epoch-24 batch-102
Running loss of epoch-24 batch-102 = 3.167090471833944e-05

Training epoch-24 batch-103
Running loss of epoch-24 batch-103 = 2.9373331926763058e-05

Training epoch-24 batch-104
Running loss of epoch-24 batch-104 = 3.465660847723484e-05

Training epoch-24 batch-105
Running loss of epoch-24 batch-105 = 6.920972373336554e-05

Training epoch-24 batch-106
Running loss of epoch-24 batch-106 = 9.864335879683495e-06

Training epoch-24 batch-107
Running loss of epoch-24 batch-107 = 3.29453032463789e-05

Training epoch-24 batch-108
Running loss of epoch-24 batch-108 = 4.769209772348404e-05

Training epoch-24 batch-109
Running loss of epoch-24 batch-109 = 3.4937867894768715e-05

Training epoch-24 batch-110
Running loss of epoch-24 batch-110 = 1.6588950529694557e-05

Training epoch-24 batch-111
Running loss of epoch-24 batch-111 = 3.903405740857124e-05

Training epoch-24 batch-112
Running loss of epoch-24 batch-112 = 4.509533755481243e-05

Training epoch-24 batch-113
Running loss of epoch-24 batch-113 = 1.6619102098047733e-05

Training epoch-24 batch-114
Running loss of epoch-24 batch-114 = 1.7390353605151176e-05

Training epoch-24 batch-115
Running loss of epoch-24 batch-115 = 6.26689288765192e-05

Training epoch-24 batch-116
Running loss of epoch-24 batch-116 = 2.8174370527267456e-05

Training epoch-24 batch-117
Running loss of epoch-24 batch-117 = 3.5460456274449825e-05

Training epoch-24 batch-118
Running loss of epoch-24 batch-118 = 1.2215692549943924e-05

Training epoch-24 batch-119
Running loss of epoch-24 batch-119 = 2.464931458234787e-05

Training epoch-24 batch-120
Running loss of epoch-24 batch-120 = 1.3101845979690552e-05

Training epoch-24 batch-121
Running loss of epoch-24 batch-121 = 1.3164244592189789e-05

Training epoch-24 batch-122
Running loss of epoch-24 batch-122 = 9.181734640151262e-05

Training epoch-24 batch-123
Running loss of epoch-24 batch-123 = 1.835636794567108e-05

Training epoch-24 batch-124
Running loss of epoch-24 batch-124 = 8.808344136923552e-05

Training epoch-24 batch-125
Running loss of epoch-24 batch-125 = 3.0536437407135963e-05

Training epoch-24 batch-126
Running loss of epoch-24 batch-126 = 1.8140533939003944e-05

Training epoch-24 batch-127
Running loss of epoch-24 batch-127 = 5.4066069424152374e-05

Training epoch-24 batch-128
Running loss of epoch-24 batch-128 = 1.5075085684657097e-05

Training epoch-24 batch-129
Running loss of epoch-24 batch-129 = 9.324250277131796e-05

Training epoch-24 batch-130
Running loss of epoch-24 batch-130 = 5.104125011712313e-05

Training epoch-24 batch-131
Running loss of epoch-24 batch-131 = 3.063422627747059e-05

Training epoch-24 batch-132
Running loss of epoch-24 batch-132 = 4.565739072859287e-05

Training epoch-24 batch-133
Running loss of epoch-24 batch-133 = 3.135309088975191e-05

Training epoch-24 batch-134
Running loss of epoch-24 batch-134 = 2.1097715944051743e-05

Training epoch-24 batch-135
Running loss of epoch-24 batch-135 = 2.5551067665219307e-05

Training epoch-24 batch-136
Running loss of epoch-24 batch-136 = 1.2816628441214561e-05

Training epoch-24 batch-137
Running loss of epoch-24 batch-137 = 3.053015097975731e-05

Training epoch-24 batch-138
Running loss of epoch-24 batch-138 = 7.495773024857044e-05

Training epoch-24 batch-139
Running loss of epoch-24 batch-139 = 1.5256693586707115e-05

Training epoch-24 batch-140
Running loss of epoch-24 batch-140 = 3.1261006370186806e-05

Training epoch-24 batch-141
Running loss of epoch-24 batch-141 = 3.556895535439253e-05

Training epoch-24 batch-142
Running loss of epoch-24 batch-142 = 9.114923886954784e-05

Training epoch-24 batch-143
Running loss of epoch-24 batch-143 = 5.636829882860184e-06

Training epoch-24 batch-144
Running loss of epoch-24 batch-144 = 8.930684998631477e-06

Training epoch-24 batch-145
Running loss of epoch-24 batch-145 = 4.987930878996849e-05

Training epoch-24 batch-146
Running loss of epoch-24 batch-146 = 3.903370816260576e-05

Training epoch-24 batch-147
Running loss of epoch-24 batch-147 = 2.7173664420843124e-05

Training epoch-24 batch-148
Running loss of epoch-24 batch-148 = 2.838950604200363e-05

Training epoch-24 batch-149
Running loss of epoch-24 batch-149 = 1.245248131453991e-05

Training epoch-24 batch-150
Running loss of epoch-24 batch-150 = 0.0001388828968629241

Training epoch-24 batch-151
Running loss of epoch-24 batch-151 = 4.0833139792084694e-05

Training epoch-24 batch-152
Running loss of epoch-24 batch-152 = 1.6892910934984684e-05

Training epoch-24 batch-153
Running loss of epoch-24 batch-153 = 4.612468183040619e-05

Training epoch-24 batch-154
Running loss of epoch-24 batch-154 = 2.6969006285071373e-05

Training epoch-24 batch-155
Running loss of epoch-24 batch-155 = 1.8133316189050674e-05

Training epoch-24 batch-156
Running loss of epoch-24 batch-156 = 2.6128487661480904e-05

Training epoch-24 batch-157
Running loss of epoch-24 batch-157 = 0.00014886260032653809

Finished training epoch-24.



Average train loss at epoch-24 = 3.481122478842735e-05

Started Evaluation

Average val loss at epoch-24 = 0.9147769121390842

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.23 %

Finished Evaluation



Started training epoch-25


Training epoch-25 batch-1
Running loss of epoch-25 batch-1 = 3.093108534812927e-05

Training epoch-25 batch-2
Running loss of epoch-25 batch-2 = 1.708907075226307e-05

Training epoch-25 batch-3
Running loss of epoch-25 batch-3 = 2.091587521135807e-05

Training epoch-25 batch-4
Running loss of epoch-25 batch-4 = 2.5775283575057983e-05

Training epoch-25 batch-5
Running loss of epoch-25 batch-5 = 2.3403321392834187e-05

Training epoch-25 batch-6
Running loss of epoch-25 batch-6 = 3.9050355553627014e-05

Training epoch-25 batch-7
Running loss of epoch-25 batch-7 = 3.046332858502865e-05

Training epoch-25 batch-8
Running loss of epoch-25 batch-8 = 4.030158743262291e-05

Training epoch-25 batch-9
Running loss of epoch-25 batch-9 = 2.4281907826662064e-05

Training epoch-25 batch-10
Running loss of epoch-25 batch-10 = 2.3752683773636818e-05

Training epoch-25 batch-11
Running loss of epoch-25 batch-11 = 2.8980080969631672e-05

Training epoch-25 batch-12
Running loss of epoch-25 batch-12 = 4.318053834140301e-05

Training epoch-25 batch-13
Running loss of epoch-25 batch-13 = 2.0136823877692223e-05

Training epoch-25 batch-14
Running loss of epoch-25 batch-14 = 2.378108911216259e-05

Training epoch-25 batch-15
Running loss of epoch-25 batch-15 = 2.0448584109544754e-05

Training epoch-25 batch-16
Running loss of epoch-25 batch-16 = 4.361150786280632e-05

Training epoch-25 batch-17
Running loss of epoch-25 batch-17 = 3.195321187376976e-05

Training epoch-25 batch-18
Running loss of epoch-25 batch-18 = 2.9592309147119522e-05

Training epoch-25 batch-19
Running loss of epoch-25 batch-19 = 1.6931677237153053e-05

Training epoch-25 batch-20
Running loss of epoch-25 batch-20 = 3.476382698863745e-05

Training epoch-25 batch-21
Running loss of epoch-25 batch-21 = 9.588012471795082e-05

Training epoch-25 batch-22
Running loss of epoch-25 batch-22 = 2.598739229142666e-05

Training epoch-25 batch-23
Running loss of epoch-25 batch-23 = 2.0168372429907322e-05

Training epoch-25 batch-24
Running loss of epoch-25 batch-24 = 2.071121707558632e-05

Training epoch-25 batch-25
Running loss of epoch-25 batch-25 = 4.0455255657434464e-05

Training epoch-25 batch-26
Running loss of epoch-25 batch-26 = 2.2986438125371933e-05

Training epoch-25 batch-27
Running loss of epoch-25 batch-27 = 4.6998728066682816e-05

Training epoch-25 batch-28
Running loss of epoch-25 batch-28 = 2.382625825703144e-05

Training epoch-25 batch-29
Running loss of epoch-25 batch-29 = 2.3199128918349743e-05

Training epoch-25 batch-30
Running loss of epoch-25 batch-30 = 9.350012987852097e-06

Training epoch-25 batch-31
Running loss of epoch-25 batch-31 = 3.5140663385391235e-05

Training epoch-25 batch-32
Running loss of epoch-25 batch-32 = 1.2515462003648281e-05

Training epoch-25 batch-33
Running loss of epoch-25 batch-33 = 6.263505201786757e-05

Training epoch-25 batch-34
Running loss of epoch-25 batch-34 = 1.8152175471186638e-05

Training epoch-25 batch-35
Running loss of epoch-25 batch-35 = 1.1113239452242851e-05

Training epoch-25 batch-36
Running loss of epoch-25 batch-36 = 1.5467172488570213e-05

Training epoch-25 batch-37
Running loss of epoch-25 batch-37 = 9.262480307370424e-05

Training epoch-25 batch-38
Running loss of epoch-25 batch-38 = 1.736974809318781e-05

Training epoch-25 batch-39
Running loss of epoch-25 batch-39 = 1.334119588136673e-05

Training epoch-25 batch-40
Running loss of epoch-25 batch-40 = 1.0250834748148918e-05

Training epoch-25 batch-41
Running loss of epoch-25 batch-41 = 2.3919157683849335e-05

Training epoch-25 batch-42
Running loss of epoch-25 batch-42 = 2.4216482415795326e-05

Training epoch-25 batch-43
Running loss of epoch-25 batch-43 = 4.049297422170639e-05

Training epoch-25 batch-44
Running loss of epoch-25 batch-44 = 2.3367581889033318e-05

Training epoch-25 batch-45
Running loss of epoch-25 batch-45 = 9.201234206557274e-06

Training epoch-25 batch-46
Running loss of epoch-25 batch-46 = 5.3437077440321445e-05

Training epoch-25 batch-47
Running loss of epoch-25 batch-47 = 2.230028621852398e-05

Training epoch-25 batch-48
Running loss of epoch-25 batch-48 = 1.6162870451807976e-05

Training epoch-25 batch-49
Running loss of epoch-25 batch-49 = 2.060865517705679e-05

Training epoch-25 batch-50
Running loss of epoch-25 batch-50 = 5.023565609008074e-05

Training epoch-25 batch-51
Running loss of epoch-25 batch-51 = 3.2644253224134445e-05

Training epoch-25 batch-52
Running loss of epoch-25 batch-52 = 1.9604573026299477e-05

Training epoch-25 batch-53
Running loss of epoch-25 batch-53 = 3.631680738180876e-05

Training epoch-25 batch-54
Running loss of epoch-25 batch-54 = 2.4211592972278595e-05

Training epoch-25 batch-55
Running loss of epoch-25 batch-55 = 4.105211701244116e-05

Training epoch-25 batch-56
Running loss of epoch-25 batch-56 = 4.7950190491974354e-05

Training epoch-25 batch-57
Running loss of epoch-25 batch-57 = 1.4729797840118408e-05

Training epoch-25 batch-58
Running loss of epoch-25 batch-58 = 2.6127789169549942e-05

Training epoch-25 batch-59
Running loss of epoch-25 batch-59 = 1.2167729437351227e-05

Training epoch-25 batch-60
Running loss of epoch-25 batch-60 = 2.4532899260520935e-05

Training epoch-25 batch-61
Running loss of epoch-25 batch-61 = 3.0456576496362686e-05

Training epoch-25 batch-62
Running loss of epoch-25 batch-62 = 9.318813681602478e-06

Training epoch-25 batch-63
Running loss of epoch-25 batch-63 = 1.2997770681977272e-05

Training epoch-25 batch-64
Running loss of epoch-25 batch-64 = 4.00240533053875e-05

Training epoch-25 batch-65
Running loss of epoch-25 batch-65 = 6.556278094649315e-06

Training epoch-25 batch-66
Running loss of epoch-25 batch-66 = 1.85349490493536e-05

Training epoch-25 batch-67
Running loss of epoch-25 batch-67 = 2.2461870685219765e-05

Training epoch-25 batch-68
Running loss of epoch-25 batch-68 = 3.011804074048996e-05

Training epoch-25 batch-69
Running loss of epoch-25 batch-69 = 2.6208581402897835e-05

Training epoch-25 batch-70
Running loss of epoch-25 batch-70 = 1.6252044588327408e-05

Training epoch-25 batch-71
Running loss of epoch-25 batch-71 = 2.4031614884734154e-05

Training epoch-25 batch-72
Running loss of epoch-25 batch-72 = 8.172588422894478e-06

Training epoch-25 batch-73
Running loss of epoch-25 batch-73 = 2.854980994015932e-05

Training epoch-25 batch-74
Running loss of epoch-25 batch-74 = 2.8676004149019718e-05

Training epoch-25 batch-75
Running loss of epoch-25 batch-75 = 2.0102481357753277e-05

Training epoch-25 batch-76
Running loss of epoch-25 batch-76 = 4.894204903393984e-05

Training epoch-25 batch-77
Running loss of epoch-25 batch-77 = 2.8593698516488075e-05

Training epoch-25 batch-78
Running loss of epoch-25 batch-78 = 2.0892592146992683e-05

Training epoch-25 batch-79
Running loss of epoch-25 batch-79 = 1.9214116036891937e-05

Training epoch-25 batch-80
Running loss of epoch-25 batch-80 = 1.959642395377159e-05

Training epoch-25 batch-81
Running loss of epoch-25 batch-81 = 2.4590175598859787e-05

Training epoch-25 batch-82
Running loss of epoch-25 batch-82 = 2.0668841898441315e-05

Training epoch-25 batch-83
Running loss of epoch-25 batch-83 = 3.778655081987381e-05

Training epoch-25 batch-84
Running loss of epoch-25 batch-84 = 2.1523097530007362e-05

Training epoch-25 batch-85
Running loss of epoch-25 batch-85 = 6.784917786717415e-05

Training epoch-25 batch-86
Running loss of epoch-25 batch-86 = 2.2195512428879738e-05

Training epoch-25 batch-87
Running loss of epoch-25 batch-87 = 2.3482833057641983e-05

Training epoch-25 batch-88
Running loss of epoch-25 batch-88 = 5.0274888053536415e-05

Training epoch-25 batch-89
Running loss of epoch-25 batch-89 = 4.286319017410278e-05

Training epoch-25 batch-90
Running loss of epoch-25 batch-90 = 9.694136679172516e-06

Training epoch-25 batch-91
Running loss of epoch-25 batch-91 = 1.8898630514740944e-05

Training epoch-25 batch-92
Running loss of epoch-25 batch-92 = 3.471213858574629e-05

Training epoch-25 batch-93
Running loss of epoch-25 batch-93 = 1.0870862752199173e-05

Training epoch-25 batch-94
Running loss of epoch-25 batch-94 = 5.583721213042736e-05

Training epoch-25 batch-95
Running loss of epoch-25 batch-95 = 1.5434809029102325e-05

Training epoch-25 batch-96
Running loss of epoch-25 batch-96 = 2.9218848794698715e-05

Training epoch-25 batch-97
Running loss of epoch-25 batch-97 = 1.6216887161135674e-05

Training epoch-25 batch-98
Running loss of epoch-25 batch-98 = 3.6197714507579803e-05

Training epoch-25 batch-99
Running loss of epoch-25 batch-99 = 1.2743053957819939e-05

Training epoch-25 batch-100
Running loss of epoch-25 batch-100 = 1.6084755770862103e-05

Training epoch-25 batch-101
Running loss of epoch-25 batch-101 = 4.084501415491104e-05

Training epoch-25 batch-102
Running loss of epoch-25 batch-102 = 4.0591112338006496e-05

Training epoch-25 batch-103
Running loss of epoch-25 batch-103 = 2.9107904992997646e-05

Training epoch-25 batch-104
Running loss of epoch-25 batch-104 = 3.567140083760023e-05

Training epoch-25 batch-105
Running loss of epoch-25 batch-105 = 5.3946743719279766e-05

Training epoch-25 batch-106
Running loss of epoch-25 batch-106 = 7.171742618083954e-05

Training epoch-25 batch-107
Running loss of epoch-25 batch-107 = 3.725267015397549e-05

Training epoch-25 batch-108
Running loss of epoch-25 batch-108 = 1.4236662536859512e-05

Training epoch-25 batch-109
Running loss of epoch-25 batch-109 = 4.226702731102705e-05

Training epoch-25 batch-110
Running loss of epoch-25 batch-110 = 1.3362383469939232e-05

Training epoch-25 batch-111
Running loss of epoch-25 batch-111 = 5.710730329155922e-05

Training epoch-25 batch-112
Running loss of epoch-25 batch-112 = 3.228010609745979e-05

Training epoch-25 batch-113
Running loss of epoch-25 batch-113 = 3.514043055474758e-05

Training epoch-25 batch-114
Running loss of epoch-25 batch-114 = 2.7582980692386627e-05

Training epoch-25 batch-115
Running loss of epoch-25 batch-115 = 3.159989137202501e-05

Training epoch-25 batch-116
Running loss of epoch-25 batch-116 = 2.500938717275858e-05

Training epoch-25 batch-117
Running loss of epoch-25 batch-117 = 1.69405248016119e-05

Training epoch-25 batch-118
Running loss of epoch-25 batch-118 = 5.0398288294672966e-05

Training epoch-25 batch-119
Running loss of epoch-25 batch-119 = 2.5687157176434994e-05

Training epoch-25 batch-120
Running loss of epoch-25 batch-120 = 0.00013439462054520845

Training epoch-25 batch-121
Running loss of epoch-25 batch-121 = 3.919098526239395e-05

Training epoch-25 batch-122
Running loss of epoch-25 batch-122 = 3.45397274941206e-05

Training epoch-25 batch-123
Running loss of epoch-25 batch-123 = 1.9312836229801178e-05

Training epoch-25 batch-124
Running loss of epoch-25 batch-124 = 4.107225686311722e-05

Training epoch-25 batch-125
Running loss of epoch-25 batch-125 = 6.353715434670448e-06

Training epoch-25 batch-126
Running loss of epoch-25 batch-126 = 4.5847613364458084e-05

Training epoch-25 batch-127
Running loss of epoch-25 batch-127 = 1.3866927474737167e-05

Training epoch-25 batch-128
Running loss of epoch-25 batch-128 = 3.544625360518694e-05

Training epoch-25 batch-129
Running loss of epoch-25 batch-129 = 2.289155963808298e-05

Training epoch-25 batch-130
Running loss of epoch-25 batch-130 = 1.7505837604403496e-05

Training epoch-25 batch-131
Running loss of epoch-25 batch-131 = 5.126220639795065e-05

Training epoch-25 batch-132
Running loss of epoch-25 batch-132 = 1.7466489225625992e-05

Training epoch-25 batch-133
Running loss of epoch-25 batch-133 = 3.230804577469826e-05

Training epoch-25 batch-134
Running loss of epoch-25 batch-134 = 2.316921018064022e-05

Training epoch-25 batch-135
Running loss of epoch-25 batch-135 = 2.5041052140295506e-05

Training epoch-25 batch-136
Running loss of epoch-25 batch-136 = 2.1469080820679665e-05

Training epoch-25 batch-137
Running loss of epoch-25 batch-137 = 5.182693712413311e-05

Training epoch-25 batch-138
Running loss of epoch-25 batch-138 = 2.1748477593064308e-05

Training epoch-25 batch-139
Running loss of epoch-25 batch-139 = 1.5626195818185806e-05

Training epoch-25 batch-140
Running loss of epoch-25 batch-140 = 4.802364856004715e-05

Training epoch-25 batch-141
Running loss of epoch-25 batch-141 = 1.7452286556363106e-05

Training epoch-25 batch-142
Running loss of epoch-25 batch-142 = 1.208554022014141e-05

Training epoch-25 batch-143
Running loss of epoch-25 batch-143 = 3.14636854454875e-05

Training epoch-25 batch-144
Running loss of epoch-25 batch-144 = 1.81559007614851e-05

Training epoch-25 batch-145
Running loss of epoch-25 batch-145 = 1.0592397302389145e-05

Training epoch-25 batch-146
Running loss of epoch-25 batch-146 = 0.0001261983998119831

Training epoch-25 batch-147
Running loss of epoch-25 batch-147 = 3.0039343982934952e-05

Training epoch-25 batch-148
Running loss of epoch-25 batch-148 = 2.2026244550943375e-05

Training epoch-25 batch-149
Running loss of epoch-25 batch-149 = 5.783303640782833e-05

Training epoch-25 batch-150
Running loss of epoch-25 batch-150 = 4.2490195482969284e-05

Training epoch-25 batch-151
Running loss of epoch-25 batch-151 = 8.523697033524513e-06

Training epoch-25 batch-152
Running loss of epoch-25 batch-152 = 5.338178016245365e-05

Training epoch-25 batch-153
Running loss of epoch-25 batch-153 = 9.9588418379426e-05

Training epoch-25 batch-154
Running loss of epoch-25 batch-154 = 2.1071056835353374e-05

Training epoch-25 batch-155
Running loss of epoch-25 batch-155 = 2.6609515771269798e-05

Training epoch-25 batch-156
Running loss of epoch-25 batch-156 = 1.1082272976636887e-05

Training epoch-25 batch-157
Running loss of epoch-25 batch-157 = 2.388283610343933e-05

Finished training epoch-25.



Average train loss at epoch-25 = 3.053640574216843e-05

Started Evaluation

Average val loss at epoch-25 = 0.9291564410509876

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.66 %
Accuracy for class onCreate is: 93.39 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 63.24 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.59 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 70.26 %

Overall Accuracy = 83.02 %

Finished Evaluation



Started training epoch-26


Training epoch-26 batch-1
Running loss of epoch-26 batch-1 = 2.4226494133472443e-05

Training epoch-26 batch-2
Running loss of epoch-26 batch-2 = 2.6886118575930595e-05

Training epoch-26 batch-3
Running loss of epoch-26 batch-3 = 3.152433782815933e-05

Training epoch-26 batch-4
Running loss of epoch-26 batch-4 = 4.341220483183861e-05

Training epoch-26 batch-5
Running loss of epoch-26 batch-5 = 1.5786616131663322e-05

Training epoch-26 batch-6
Running loss of epoch-26 batch-6 = 1.7901882529258728e-05

Training epoch-26 batch-7
Running loss of epoch-26 batch-7 = 2.5585992261767387e-05

Training epoch-26 batch-8
Running loss of epoch-26 batch-8 = 3.9907870814204216e-05

Training epoch-26 batch-9
Running loss of epoch-26 batch-9 = 3.255857154726982e-05

Training epoch-26 batch-10
Running loss of epoch-26 batch-10 = 5.9329322539269924e-05

Training epoch-26 batch-11
Running loss of epoch-26 batch-11 = 1.4888588339090347e-05

Training epoch-26 batch-12
Running loss of epoch-26 batch-12 = 1.892796717584133e-05

Training epoch-26 batch-13
Running loss of epoch-26 batch-13 = 2.006813883781433e-05

Training epoch-26 batch-14
Running loss of epoch-26 batch-14 = 1.4396850019693375e-05

Training epoch-26 batch-15
Running loss of epoch-26 batch-15 = 2.3903092369437218e-05

Training epoch-26 batch-16
Running loss of epoch-26 batch-16 = 3.042409662157297e-05

Training epoch-26 batch-17
Running loss of epoch-26 batch-17 = 1.1653639376163483e-05

Training epoch-26 batch-18
Running loss of epoch-26 batch-18 = 7.777358405292034e-06

Training epoch-26 batch-19
Running loss of epoch-26 batch-19 = 1.3296026736497879e-05

Training epoch-26 batch-20
Running loss of epoch-26 batch-20 = 4.451349377632141e-05

Training epoch-26 batch-21
Running loss of epoch-26 batch-21 = 1.9116559997200966e-05

Training epoch-26 batch-22
Running loss of epoch-26 batch-22 = 1.260777935385704e-05

Training epoch-26 batch-23
Running loss of epoch-26 batch-23 = 1.6541453078389168e-05

Training epoch-26 batch-24
Running loss of epoch-26 batch-24 = 9.3081034719944e-06

Training epoch-26 batch-25
Running loss of epoch-26 batch-25 = 4.282640293240547e-05

Training epoch-26 batch-26
Running loss of epoch-26 batch-26 = 1.1893687769770622e-05

Training epoch-26 batch-27
Running loss of epoch-26 batch-27 = 3.620062489062548e-05

Training epoch-26 batch-28
Running loss of epoch-26 batch-28 = 1.6682548448443413e-05

Training epoch-26 batch-29
Running loss of epoch-26 batch-29 = 1.3631302863359451e-05

Training epoch-26 batch-30
Running loss of epoch-26 batch-30 = 4.2208004742860794e-05

Training epoch-26 batch-31
Running loss of epoch-26 batch-31 = 2.469378523528576e-05

Training epoch-26 batch-32
Running loss of epoch-26 batch-32 = 1.088087446987629e-05

Training epoch-26 batch-33
Running loss of epoch-26 batch-33 = 1.630280166864395e-05

Training epoch-26 batch-34
Running loss of epoch-26 batch-34 = 7.748370990157127e-06

Training epoch-26 batch-35
Running loss of epoch-26 batch-35 = 1.8418882973492146e-05

Training epoch-26 batch-36
Running loss of epoch-26 batch-36 = 4.821876063942909e-05

Training epoch-26 batch-37
Running loss of epoch-26 batch-37 = 1.932866871356964e-05

Training epoch-26 batch-38
Running loss of epoch-26 batch-38 = 4.798069130629301e-05

Training epoch-26 batch-39
Running loss of epoch-26 batch-39 = 2.4298671633005142e-05

Training epoch-26 batch-40
Running loss of epoch-26 batch-40 = 2.173543907701969e-05

Training epoch-26 batch-41
Running loss of epoch-26 batch-41 = 4.045967943966389e-05

Training epoch-26 batch-42
Running loss of epoch-26 batch-42 = 3.411236684769392e-05

Training epoch-26 batch-43
Running loss of epoch-26 batch-43 = 2.3653730750083923e-05

Training epoch-26 batch-44
Running loss of epoch-26 batch-44 = 3.419781569391489e-05

Training epoch-26 batch-45
Running loss of epoch-26 batch-45 = 2.1238578483462334e-05

Training epoch-26 batch-46
Running loss of epoch-26 batch-46 = 1.984555274248123e-05

Training epoch-26 batch-47
Running loss of epoch-26 batch-47 = 2.445559948682785e-05

Training epoch-26 batch-48
Running loss of epoch-26 batch-48 = 2.2526364773511887e-05

Training epoch-26 batch-49
Running loss of epoch-26 batch-49 = 9.59392637014389e-05

Training epoch-26 batch-50
Running loss of epoch-26 batch-50 = 3.528641536831856e-05

Training epoch-26 batch-51
Running loss of epoch-26 batch-51 = 4.9068592488765717e-05

Training epoch-26 batch-52
Running loss of epoch-26 batch-52 = 1.902901567518711e-05

Training epoch-26 batch-53
Running loss of epoch-26 batch-53 = 4.710385110229254e-05

Training epoch-26 batch-54
Running loss of epoch-26 batch-54 = 3.267754800617695e-05

Training epoch-26 batch-55
Running loss of epoch-26 batch-55 = 1.7413636669516563e-05

Training epoch-26 batch-56
Running loss of epoch-26 batch-56 = 1.671037171036005e-05

Training epoch-26 batch-57
Running loss of epoch-26 batch-57 = 6.647210102528334e-05

Training epoch-26 batch-58
Running loss of epoch-26 batch-58 = 1.1481344699859619e-05

Training epoch-26 batch-59
Running loss of epoch-26 batch-59 = 2.7173664420843124e-05

Training epoch-26 batch-60
Running loss of epoch-26 batch-60 = 7.073092274367809e-05

Training epoch-26 batch-61
Running loss of epoch-26 batch-61 = 2.2867927327752113e-05

Training epoch-26 batch-62
Running loss of epoch-26 batch-62 = 1.1464115232229233e-05

Training epoch-26 batch-63
Running loss of epoch-26 batch-63 = 2.675980795174837e-05

Training epoch-26 batch-64
Running loss of epoch-26 batch-64 = 2.4268636479973793e-05

Training epoch-26 batch-65
Running loss of epoch-26 batch-65 = 1.5310128219425678e-05

Training epoch-26 batch-66
Running loss of epoch-26 batch-66 = 1.841224730014801e-05

Training epoch-26 batch-67
Running loss of epoch-26 batch-67 = 2.443208359181881e-05

Training epoch-26 batch-68
Running loss of epoch-26 batch-68 = 9.188067633658648e-05

Training epoch-26 batch-69
Running loss of epoch-26 batch-69 = 1.6147270798683167e-05

Training epoch-26 batch-70
Running loss of epoch-26 batch-70 = 1.1984026059508324e-05

Training epoch-26 batch-71
Running loss of epoch-26 batch-71 = 2.3719388991594315e-05

Training epoch-26 batch-72
Running loss of epoch-26 batch-72 = 3.766501322388649e-05

Training epoch-26 batch-73
Running loss of epoch-26 batch-73 = 2.3802276700735092e-05

Training epoch-26 batch-74
Running loss of epoch-26 batch-74 = 3.4626107662916183e-05

Training epoch-26 batch-75
Running loss of epoch-26 batch-75 = 4.109751898795366e-05

Training epoch-26 batch-76
Running loss of epoch-26 batch-76 = 3.967992961406708e-05

Training epoch-26 batch-77
Running loss of epoch-26 batch-77 = 2.4002976715564728e-05

Training epoch-26 batch-78
Running loss of epoch-26 batch-78 = 1.8249033018946648e-05

Training epoch-26 batch-79
Running loss of epoch-26 batch-79 = 4.431046545505524e-05

Training epoch-26 batch-80
Running loss of epoch-26 batch-80 = 1.81589275598526e-05

Training epoch-26 batch-81
Running loss of epoch-26 batch-81 = 1.2193107977509499e-05

Training epoch-26 batch-82
Running loss of epoch-26 batch-82 = 5.412241443991661e-05

Training epoch-26 batch-83
Running loss of epoch-26 batch-83 = 2.190493978559971e-05

Training epoch-26 batch-84
Running loss of epoch-26 batch-84 = 2.0065810531377792e-05

Training epoch-26 batch-85
Running loss of epoch-26 batch-85 = 2.447993028908968e-05

Training epoch-26 batch-86
Running loss of epoch-26 batch-86 = 1.7100712284445763e-05

Training epoch-26 batch-87
Running loss of epoch-26 batch-87 = 1.715822145342827e-05

Training epoch-26 batch-88
Running loss of epoch-26 batch-88 = 1.3238750398159027e-05

Training epoch-26 batch-89
Running loss of epoch-26 batch-89 = 3.9126258343458176e-05

Training epoch-26 batch-90
Running loss of epoch-26 batch-90 = 3.665138501673937e-05

Training epoch-26 batch-91
Running loss of epoch-26 batch-91 = 4.0862709283828735e-05

Training epoch-26 batch-92
Running loss of epoch-26 batch-92 = 1.6459496691823006e-05

Training epoch-26 batch-93
Running loss of epoch-26 batch-93 = 2.4655601009726524e-05

Training epoch-26 batch-94
Running loss of epoch-26 batch-94 = 8.704257197678089e-06

Training epoch-26 batch-95
Running loss of epoch-26 batch-95 = 2.2185733541846275e-05

Training epoch-26 batch-96
Running loss of epoch-26 batch-96 = 1.848861575126648e-05

Training epoch-26 batch-97
Running loss of epoch-26 batch-97 = 6.10321294516325e-05

Training epoch-26 batch-98
Running loss of epoch-26 batch-98 = 1.9238097593188286e-05

Training epoch-26 batch-99
Running loss of epoch-26 batch-99 = 1.4277524314820766e-05

Training epoch-26 batch-100
Running loss of epoch-26 batch-100 = 1.2060161679983139e-05

Training epoch-26 batch-101
Running loss of epoch-26 batch-101 = 3.77545366063714e-05

Training epoch-26 batch-102
Running loss of epoch-26 batch-102 = 2.1112151443958282e-05

Training epoch-26 batch-103
Running loss of epoch-26 batch-103 = 4.0155136957764626e-05

Training epoch-26 batch-104
Running loss of epoch-26 batch-104 = 1.4524091966450214e-05

Training epoch-26 batch-105
Running loss of epoch-26 batch-105 = 3.3242395147681236e-05

Training epoch-26 batch-106
Running loss of epoch-26 batch-106 = 1.70036219060421e-05

Training epoch-26 batch-107
Running loss of epoch-26 batch-107 = 1.0616844519972801e-05

Training epoch-26 batch-108
Running loss of epoch-26 batch-108 = 3.2411422580480576e-05

Training epoch-26 batch-109
Running loss of epoch-26 batch-109 = 1.9364641048014164e-05

Training epoch-26 batch-110
Running loss of epoch-26 batch-110 = 2.2634165361523628e-05

Training epoch-26 batch-111
Running loss of epoch-26 batch-111 = 4.819594323635101e-05

Training epoch-26 batch-112
Running loss of epoch-26 batch-112 = 2.093985676765442e-05

Training epoch-26 batch-113
Running loss of epoch-26 batch-113 = 1.1081807315349579e-05

Training epoch-26 batch-114
Running loss of epoch-26 batch-114 = 1.1699972674250603e-05

Training epoch-26 batch-115
Running loss of epoch-26 batch-115 = 1.903856173157692e-05

Training epoch-26 batch-116
Running loss of epoch-26 batch-116 = 3.503577318042517e-05

Training epoch-26 batch-117
Running loss of epoch-26 batch-117 = 2.603442408144474e-05

Training epoch-26 batch-118
Running loss of epoch-26 batch-118 = 5.429796874523163e-05

Training epoch-26 batch-119
Running loss of epoch-26 batch-119 = 1.8234597519040108e-05

Training epoch-26 batch-120
Running loss of epoch-26 batch-120 = 1.567043364048004e-05

Training epoch-26 batch-121
Running loss of epoch-26 batch-121 = 1.2378906831145287e-05

Training epoch-26 batch-122
Running loss of epoch-26 batch-122 = 2.4198321625590324e-05

Training epoch-26 batch-123
Running loss of epoch-26 batch-123 = 3.15103679895401e-05

Training epoch-26 batch-124
Running loss of epoch-26 batch-124 = 3.255775664001703e-05

Training epoch-26 batch-125
Running loss of epoch-26 batch-125 = 6.374181248247623e-05

Training epoch-26 batch-126
Running loss of epoch-26 batch-126 = 1.734576653689146e-05

Training epoch-26 batch-127
Running loss of epoch-26 batch-127 = 2.1795043721795082e-05

Training epoch-26 batch-128
Running loss of epoch-26 batch-128 = 2.8415466658771038e-05

Training epoch-26 batch-129
Running loss of epoch-26 batch-129 = 3.796583041548729e-05

Training epoch-26 batch-130
Running loss of epoch-26 batch-130 = 7.297727279365063e-06

Training epoch-26 batch-131
Running loss of epoch-26 batch-131 = 1.936778426170349e-05

Training epoch-26 batch-132
Running loss of epoch-26 batch-132 = 3.1807925552129745e-05

Training epoch-26 batch-133
Running loss of epoch-26 batch-133 = 2.8871232643723488e-05

Training epoch-26 batch-134
Running loss of epoch-26 batch-134 = 1.842784695327282e-05

Training epoch-26 batch-135
Running loss of epoch-26 batch-135 = 3.21617117151618e-05

Training epoch-26 batch-136
Running loss of epoch-26 batch-136 = 1.8102815374732018e-05

Training epoch-26 batch-137
Running loss of epoch-26 batch-137 = 2.9585789889097214e-05

Training epoch-26 batch-138
Running loss of epoch-26 batch-138 = 1.9177095964550972e-05

Training epoch-26 batch-139
Running loss of epoch-26 batch-139 = 1.491699367761612e-05

Training epoch-26 batch-140
Running loss of epoch-26 batch-140 = 2.960232086479664e-05

Training epoch-26 batch-141
Running loss of epoch-26 batch-141 = 4.9680471420288086e-05

Training epoch-26 batch-142
Running loss of epoch-26 batch-142 = 2.0818086341023445e-05

Training epoch-26 batch-143
Running loss of epoch-26 batch-143 = 1.4269724488258362e-05

Training epoch-26 batch-144
Running loss of epoch-26 batch-144 = 2.3802858777344227e-05

Training epoch-26 batch-145
Running loss of epoch-26 batch-145 = 2.4857930839061737e-05

Training epoch-26 batch-146
Running loss of epoch-26 batch-146 = 1.3580545783042908e-05

Training epoch-26 batch-147
Running loss of epoch-26 batch-147 = 2.5591463781893253e-05

Training epoch-26 batch-148
Running loss of epoch-26 batch-148 = 1.9659986719489098e-05

Training epoch-26 batch-149
Running loss of epoch-26 batch-149 = 4.129786975681782e-05

Training epoch-26 batch-150
Running loss of epoch-26 batch-150 = 1.8422724679112434e-05

Training epoch-26 batch-151
Running loss of epoch-26 batch-151 = 3.622262738645077e-05

Training epoch-26 batch-152
Running loss of epoch-26 batch-152 = 2.3626023903489113e-05

Training epoch-26 batch-153
Running loss of epoch-26 batch-153 = 1.580186653882265e-05

Training epoch-26 batch-154
Running loss of epoch-26 batch-154 = 1.3387762010097504e-05

Training epoch-26 batch-155
Running loss of epoch-26 batch-155 = 3.6732759326696396e-05

Training epoch-26 batch-156
Running loss of epoch-26 batch-156 = 1.5927711501717567e-05

Training epoch-26 batch-157
Running loss of epoch-26 batch-157 = 8.687376976013184e-06

Finished training epoch-26.



Average train loss at epoch-26 = 2.671450898051262e-05

Started Evaluation

Average val loss at epoch-26 = 0.9608759966402096

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 49.55 %
Accuracy for class execute is: 44.18 %
Accuracy for class get is: 72.31 %

Overall Accuracy = 82.86 %

Finished Evaluation



Started training epoch-27


Training epoch-27 batch-1
Running loss of epoch-27 batch-1 = 2.4266308173537254e-05

Training epoch-27 batch-2
Running loss of epoch-27 batch-2 = 2.1311454474925995e-05

Training epoch-27 batch-3
Running loss of epoch-27 batch-3 = 2.149166539311409e-05

Training epoch-27 batch-4
Running loss of epoch-27 batch-4 = 9.696464985609055e-06

Training epoch-27 batch-5
Running loss of epoch-27 batch-5 = 8.172867819666862e-05

Training epoch-27 batch-6
Running loss of epoch-27 batch-6 = 7.331417873501778e-05

Training epoch-27 batch-7
Running loss of epoch-27 batch-7 = 1.7781276255846024e-05

Training epoch-27 batch-8
Running loss of epoch-27 batch-8 = 2.5497516617178917e-05

Training epoch-27 batch-9
Running loss of epoch-27 batch-9 = 1.3100681826472282e-05

Training epoch-27 batch-10
Running loss of epoch-27 batch-10 = 2.4955254048109055e-05

Training epoch-27 batch-11
Running loss of epoch-27 batch-11 = 1.8383609130978584e-05

Training epoch-27 batch-12
Running loss of epoch-27 batch-12 = 2.6652589440345764e-05

Training epoch-27 batch-13
Running loss of epoch-27 batch-13 = 2.7569476515054703e-05

Training epoch-27 batch-14
Running loss of epoch-27 batch-14 = 2.6189954951405525e-05

Training epoch-27 batch-15
Running loss of epoch-27 batch-15 = 2.4943728931248188e-05

Training epoch-27 batch-16
Running loss of epoch-27 batch-16 = 5.70942647755146e-05

Training epoch-27 batch-17
Running loss of epoch-27 batch-17 = 5.609774962067604e-05

Training epoch-27 batch-18
Running loss of epoch-27 batch-18 = 2.0442414097487926e-05

Training epoch-27 batch-19
Running loss of epoch-27 batch-19 = 1.8904567696154118e-05

Training epoch-27 batch-20
Running loss of epoch-27 batch-20 = 3.60831618309021e-05

Training epoch-27 batch-21
Running loss of epoch-27 batch-21 = 1.0877382010221481e-05

Training epoch-27 batch-22
Running loss of epoch-27 batch-22 = 7.447833195328712e-05

Training epoch-27 batch-23
Running loss of epoch-27 batch-23 = 3.255135379731655e-05

Training epoch-27 batch-24
Running loss of epoch-27 batch-24 = 1.9999919459223747e-05

Training epoch-27 batch-25
Running loss of epoch-27 batch-25 = 1.6210833564400673e-05

Training epoch-27 batch-26
Running loss of epoch-27 batch-26 = 9.783077985048294e-06

Training epoch-27 batch-27
Running loss of epoch-27 batch-27 = 2.061622217297554e-05

Training epoch-27 batch-28
Running loss of epoch-27 batch-28 = 1.9341823644936085e-05

Training epoch-27 batch-29
Running loss of epoch-27 batch-29 = 1.3297423720359802e-05

Training epoch-27 batch-30
Running loss of epoch-27 batch-30 = 3.314157947897911e-05

Training epoch-27 batch-31
Running loss of epoch-27 batch-31 = 3.7416466511785984e-05

Training epoch-27 batch-32
Running loss of epoch-27 batch-32 = 3.340095281600952e-05

Training epoch-27 batch-33
Running loss of epoch-27 batch-33 = 2.1524261683225632e-05

Training epoch-27 batch-34
Running loss of epoch-27 batch-34 = 3.475986886769533e-05

Training epoch-27 batch-35
Running loss of epoch-27 batch-35 = 1.2915348634123802e-05

Training epoch-27 batch-36
Running loss of epoch-27 batch-36 = 1.3097422197461128e-05

Training epoch-27 batch-37
Running loss of epoch-27 batch-37 = 1.2985896319150925e-05

Training epoch-27 batch-38
Running loss of epoch-27 batch-38 = 1.8173595890402794e-05

Training epoch-27 batch-39
Running loss of epoch-27 batch-39 = 1.651630736887455e-05

Training epoch-27 batch-40
Running loss of epoch-27 batch-40 = 8.67689959704876e-06

Training epoch-27 batch-41
Running loss of epoch-27 batch-41 = 1.6580335795879364e-05

Training epoch-27 batch-42
Running loss of epoch-27 batch-42 = 2.4899374693632126e-05

Training epoch-27 batch-43
Running loss of epoch-27 batch-43 = 1.7747748643159866e-05

Training epoch-27 batch-44
Running loss of epoch-27 batch-44 = 8.725328370928764e-06

Training epoch-27 batch-45
Running loss of epoch-27 batch-45 = 3.719027154147625e-05

Training epoch-27 batch-46
Running loss of epoch-27 batch-46 = 8.007511496543884e-06

Training epoch-27 batch-47
Running loss of epoch-27 batch-47 = 2.0100269466638565e-05

Training epoch-27 batch-48
Running loss of epoch-27 batch-48 = 2.3640692234039307e-05

Training epoch-27 batch-49
Running loss of epoch-27 batch-49 = 1.3742828741669655e-05

Training epoch-27 batch-50
Running loss of epoch-27 batch-50 = 2.309447154402733e-05

Training epoch-27 batch-51
Running loss of epoch-27 batch-51 = 2.2178050130605698e-05

Training epoch-27 batch-52
Running loss of epoch-27 batch-52 = 2.3600878193974495e-05

Training epoch-27 batch-53
Running loss of epoch-27 batch-53 = 1.2369593605399132e-05

Training epoch-27 batch-54
Running loss of epoch-27 batch-54 = 1.849280670285225e-05

Training epoch-27 batch-55
Running loss of epoch-27 batch-55 = 2.6370631530880928e-05

Training epoch-27 batch-56
Running loss of epoch-27 batch-56 = 2.101925201714039e-05

Training epoch-27 batch-57
Running loss of epoch-27 batch-57 = 3.948435187339783e-05

Training epoch-27 batch-58
Running loss of epoch-27 batch-58 = 1.4445977285504341e-05

Training epoch-27 batch-59
Running loss of epoch-27 batch-59 = 1.4726771041750908e-05

Training epoch-27 batch-60
Running loss of epoch-27 batch-60 = 3.328756429255009e-05

Training epoch-27 batch-61
Running loss of epoch-27 batch-61 = 2.442905679345131e-05

Training epoch-27 batch-62
Running loss of epoch-27 batch-62 = 2.5235582143068314e-05

Training epoch-27 batch-63
Running loss of epoch-27 batch-63 = 6.907351780682802e-05

Training epoch-27 batch-64
Running loss of epoch-27 batch-64 = 2.055475488305092e-05

Training epoch-27 batch-65
Running loss of epoch-27 batch-65 = 2.8971931897103786e-05

Training epoch-27 batch-66
Running loss of epoch-27 batch-66 = 5.521927960216999e-05

Training epoch-27 batch-67
Running loss of epoch-27 batch-67 = 1.7452402971684933e-05

Training epoch-27 batch-68
Running loss of epoch-27 batch-68 = 1.8965103663504124e-05

Training epoch-27 batch-69
Running loss of epoch-27 batch-69 = 5.558435805141926e-05

Training epoch-27 batch-70
Running loss of epoch-27 batch-70 = 5.62276691198349e-05

Training epoch-27 batch-71
Running loss of epoch-27 batch-71 = 2.3010186851024628e-05

Training epoch-27 batch-72
Running loss of epoch-27 batch-72 = 7.51507468521595e-06

Training epoch-27 batch-73
Running loss of epoch-27 batch-73 = 1.6241217963397503e-05

Training epoch-27 batch-74
Running loss of epoch-27 batch-74 = 2.2395863197743893e-05

Training epoch-27 batch-75
Running loss of epoch-27 batch-75 = 2.0991545170545578e-05

Training epoch-27 batch-76
Running loss of epoch-27 batch-76 = 1.8772901967167854e-05

Training epoch-27 batch-77
Running loss of epoch-27 batch-77 = 3.125052899122238e-05

Training epoch-27 batch-78
Running loss of epoch-27 batch-78 = 2.5867484509944916e-05

Training epoch-27 batch-79
Running loss of epoch-27 batch-79 = 1.685053575783968e-05

Training epoch-27 batch-80
Running loss of epoch-27 batch-80 = 3.237207420170307e-05

Training epoch-27 batch-81
Running loss of epoch-27 batch-81 = 2.2326945327222347e-05

Training epoch-27 batch-82
Running loss of epoch-27 batch-82 = 3.0673108994960785e-05

Training epoch-27 batch-83
Running loss of epoch-27 batch-83 = 4.9192807637155056e-05

Training epoch-27 batch-84
Running loss of epoch-27 batch-84 = 1.5478581190109253e-05

Training epoch-27 batch-85
Running loss of epoch-27 batch-85 = 3.4329481422901154e-05

Training epoch-27 batch-86
Running loss of epoch-27 batch-86 = 1.9610626623034477e-05

Training epoch-27 batch-87
Running loss of epoch-27 batch-87 = 1.1166324838995934e-05

Training epoch-27 batch-88
Running loss of epoch-27 batch-88 = 5.498458631336689e-05

Training epoch-27 batch-89
Running loss of epoch-27 batch-89 = 1.1675525456666946e-05

Training epoch-27 batch-90
Running loss of epoch-27 batch-90 = 9.644078090786934e-06

Training epoch-27 batch-91
Running loss of epoch-27 batch-91 = 9.152106940746307e-06

Training epoch-27 batch-92
Running loss of epoch-27 batch-92 = 2.3775268346071243e-05

Training epoch-27 batch-93
Running loss of epoch-27 batch-93 = 3.219069913029671e-05

Training epoch-27 batch-94
Running loss of epoch-27 batch-94 = 1.6816426068544388e-05

Training epoch-27 batch-95
Running loss of epoch-27 batch-95 = 1.3032578863203526e-05

Training epoch-27 batch-96
Running loss of epoch-27 batch-96 = 2.8451671823859215e-05

Training epoch-27 batch-97
Running loss of epoch-27 batch-97 = 1.892796717584133e-05

Training epoch-27 batch-98
Running loss of epoch-27 batch-98 = 7.53854401409626e-05

Training epoch-27 batch-99
Running loss of epoch-27 batch-99 = 4.6753790229558945e-05

Training epoch-27 batch-100
Running loss of epoch-27 batch-100 = 2.7574598789215088e-05

Training epoch-27 batch-101
Running loss of epoch-27 batch-101 = 2.331961877644062e-05

Training epoch-27 batch-102
Running loss of epoch-27 batch-102 = 3.443052992224693e-05

Training epoch-27 batch-103
Running loss of epoch-27 batch-103 = 2.4732667952775955e-05

Training epoch-27 batch-104
Running loss of epoch-27 batch-104 = 1.558242365717888e-05

Training epoch-27 batch-105
Running loss of epoch-27 batch-105 = 1.4515710063278675e-05

Training epoch-27 batch-106
Running loss of epoch-27 batch-106 = 1.7590937204658985e-05

Training epoch-27 batch-107
Running loss of epoch-27 batch-107 = 1.970096491277218e-05

Training epoch-27 batch-108
Running loss of epoch-27 batch-108 = 4.365737549960613e-05

Training epoch-27 batch-109
Running loss of epoch-27 batch-109 = 8.725561201572418e-06

Training epoch-27 batch-110
Running loss of epoch-27 batch-110 = 1.9254046492278576e-05

Training epoch-27 batch-111
Running loss of epoch-27 batch-111 = 3.229733556509018e-05

Training epoch-27 batch-112
Running loss of epoch-27 batch-112 = 3.319629468023777e-05

Training epoch-27 batch-113
Running loss of epoch-27 batch-113 = 8.405884727835655e-06

Training epoch-27 batch-114
Running loss of epoch-27 batch-114 = 2.8299284167587757e-05

Training epoch-27 batch-115
Running loss of epoch-27 batch-115 = 1.1773314327001572e-05

Training epoch-27 batch-116
Running loss of epoch-27 batch-116 = 3.698025830090046e-05

Training epoch-27 batch-117
Running loss of epoch-27 batch-117 = 8.579343557357788e-06

Training epoch-27 batch-118
Running loss of epoch-27 batch-118 = 4.4067855924367905e-06

Training epoch-27 batch-119
Running loss of epoch-27 batch-119 = 1.0988791473209858e-05

Training epoch-27 batch-120
Running loss of epoch-27 batch-120 = 2.8876704163849354e-05

Training epoch-27 batch-121
Running loss of epoch-27 batch-121 = 1.4118384569883347e-05

Training epoch-27 batch-122
Running loss of epoch-27 batch-122 = 2.6725465431809425e-05

Training epoch-27 batch-123
Running loss of epoch-27 batch-123 = 2.4151639081537724e-05

Training epoch-27 batch-124
Running loss of epoch-27 batch-124 = 2.2423802874982357e-05

Training epoch-27 batch-125
Running loss of epoch-27 batch-125 = 1.3006618246436119e-05

Training epoch-27 batch-126
Running loss of epoch-27 batch-126 = 2.6980647817254066e-05

Training epoch-27 batch-127
Running loss of epoch-27 batch-127 = 3.437150735408068e-05

Training epoch-27 batch-128
Running loss of epoch-27 batch-128 = 3.9740465581417084e-05

Training epoch-27 batch-129
Running loss of epoch-27 batch-129 = 1.8123071640729904e-05

Training epoch-27 batch-130
Running loss of epoch-27 batch-130 = 4.161405377089977e-05

Training epoch-27 batch-131
Running loss of epoch-27 batch-131 = 7.138587534427643e-06

Training epoch-27 batch-132
Running loss of epoch-27 batch-132 = 1.5644822269678116e-05

Training epoch-27 batch-133
Running loss of epoch-27 batch-133 = 2.2893422283232212e-05

Training epoch-27 batch-134
Running loss of epoch-27 batch-134 = 1.025572419166565e-05

Training epoch-27 batch-135
Running loss of epoch-27 batch-135 = 3.453390672802925e-05

Training epoch-27 batch-136
Running loss of epoch-27 batch-136 = 2.799497451633215e-05

Training epoch-27 batch-137
Running loss of epoch-27 batch-137 = 2.159539144486189e-05

Training epoch-27 batch-138
Running loss of epoch-27 batch-138 = 2.3575383238494396e-05

Training epoch-27 batch-139
Running loss of epoch-27 batch-139 = 3.234052564948797e-05

Training epoch-27 batch-140
Running loss of epoch-27 batch-140 = 1.2962846085429192e-05

Training epoch-27 batch-141
Running loss of epoch-27 batch-141 = 2.6490655727684498e-05

Training epoch-27 batch-142
Running loss of epoch-27 batch-142 = 1.5352852642536163e-05

Training epoch-27 batch-143
Running loss of epoch-27 batch-143 = 1.3023382052779198e-05

Training epoch-27 batch-144
Running loss of epoch-27 batch-144 = 1.817755401134491e-05

Training epoch-27 batch-145
Running loss of epoch-27 batch-145 = 1.9816099666059017e-05

Training epoch-27 batch-146
Running loss of epoch-27 batch-146 = 1.505785621702671e-05

Training epoch-27 batch-147
Running loss of epoch-27 batch-147 = 1.1491123586893082e-05

Training epoch-27 batch-148
Running loss of epoch-27 batch-148 = 2.0237057469785213e-05

Training epoch-27 batch-149
Running loss of epoch-27 batch-149 = 2.4927081540226936e-05

Training epoch-27 batch-150
Running loss of epoch-27 batch-150 = 3.0020251870155334e-05

Training epoch-27 batch-151
Running loss of epoch-27 batch-151 = 1.5034573152661324e-05

Training epoch-27 batch-152
Running loss of epoch-27 batch-152 = 2.23678071051836e-05

Training epoch-27 batch-153
Running loss of epoch-27 batch-153 = 1.4435150660574436e-05

Training epoch-27 batch-154
Running loss of epoch-27 batch-154 = 9.850598871707916e-06

Training epoch-27 batch-155
Running loss of epoch-27 batch-155 = 2.353684976696968e-05

Training epoch-27 batch-156
Running loss of epoch-27 batch-156 = 2.8016744181513786e-05

Training epoch-27 batch-157
Running loss of epoch-27 batch-157 = 6.0364603996276855e-05

Finished training epoch-27.



Average train loss at epoch-27 = 2.4713793396949768e-05

Started Evaluation

Average val loss at epoch-27 = 0.9524683984668867

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 94.14 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 63.47 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.59 %
Accuracy for class execute is: 44.58 %
Accuracy for class get is: 70.51 %

Overall Accuracy = 83.27 %

Finished Evaluation



Started training epoch-28


Training epoch-28 batch-1
Running loss of epoch-28 batch-1 = 1.1241878382861614e-05

Training epoch-28 batch-2
Running loss of epoch-28 batch-2 = 0.00010715913958847523

Training epoch-28 batch-3
Running loss of epoch-28 batch-3 = 1.4864373952150345e-05

Training epoch-28 batch-4
Running loss of epoch-28 batch-4 = 1.447414979338646e-05

Training epoch-28 batch-5
Running loss of epoch-28 batch-5 = 4.9629947170615196e-05

Training epoch-28 batch-6
Running loss of epoch-28 batch-6 = 1.9496772438287735e-05

Training epoch-28 batch-7
Running loss of epoch-28 batch-7 = 7.990282028913498e-06

Training epoch-28 batch-8
Running loss of epoch-28 batch-8 = 3.854185342788696e-05

Training epoch-28 batch-9
Running loss of epoch-28 batch-9 = 1.734844408929348e-05

Training epoch-28 batch-10
Running loss of epoch-28 batch-10 = 2.9019196517765522e-05

Training epoch-28 batch-11
Running loss of epoch-28 batch-11 = 1.5361583791673183e-05

Training epoch-28 batch-12
Running loss of epoch-28 batch-12 = 1.8730061128735542e-05

Training epoch-28 batch-13
Running loss of epoch-28 batch-13 = 5.691545084118843e-06

Training epoch-28 batch-14
Running loss of epoch-28 batch-14 = 6.253889296203852e-05

Training epoch-28 batch-15
Running loss of epoch-28 batch-15 = 1.5963800251483917e-05

Training epoch-28 batch-16
Running loss of epoch-28 batch-16 = 7.999362424015999e-06

Training epoch-28 batch-17
Running loss of epoch-28 batch-17 = 8.07945616543293e-06

Training epoch-28 batch-18
Running loss of epoch-28 batch-18 = 2.2903084754943848e-05

Training epoch-28 batch-19
Running loss of epoch-28 batch-19 = 2.447119913995266e-05

Training epoch-28 batch-20
Running loss of epoch-28 batch-20 = 1.3075536116957664e-05

Training epoch-28 batch-21
Running loss of epoch-28 batch-21 = 1.2946780771017075e-05

Training epoch-28 batch-22
Running loss of epoch-28 batch-22 = 3.5141012631356716e-05

Training epoch-28 batch-23
Running loss of epoch-28 batch-23 = 1.5829456970095634e-05

Training epoch-28 batch-24
Running loss of epoch-28 batch-24 = 2.006778959184885e-05

Training epoch-28 batch-25
Running loss of epoch-28 batch-25 = 1.5832018107175827e-05

Training epoch-28 batch-26
Running loss of epoch-28 batch-26 = 2.5229062885046005e-05

Training epoch-28 batch-27
Running loss of epoch-28 batch-27 = 2.1918443962931633e-05

Training epoch-28 batch-28
Running loss of epoch-28 batch-28 = 1.2532109394669533e-05

Training epoch-28 batch-29
Running loss of epoch-28 batch-29 = 4.854332655668259e-05

Training epoch-28 batch-30
Running loss of epoch-28 batch-30 = 5.024508573114872e-05

Training epoch-28 batch-31
Running loss of epoch-28 batch-31 = 4.257913678884506e-05

Training epoch-28 batch-32
Running loss of epoch-28 batch-32 = 1.9282102584838867e-05

Training epoch-28 batch-33
Running loss of epoch-28 batch-33 = 7.999362424015999e-06

Training epoch-28 batch-34
Running loss of epoch-28 batch-34 = 1.372629776597023e-05

Training epoch-28 batch-35
Running loss of epoch-28 batch-35 = 3.3115269616246223e-05

Training epoch-28 batch-36
Running loss of epoch-28 batch-36 = 8.320435881614685e-06

Training epoch-28 batch-37
Running loss of epoch-28 batch-37 = 1.5056692063808441e-05

Training epoch-28 batch-38
Running loss of epoch-28 batch-38 = 8.556060492992401e-06

Training epoch-28 batch-39
Running loss of epoch-28 batch-39 = 1.4249933883547783e-05

Training epoch-28 batch-40
Running loss of epoch-28 batch-40 = 5.209119990468025e-06

Training epoch-28 batch-41
Running loss of epoch-28 batch-41 = 1.4911172911524773e-05

Training epoch-28 batch-42
Running loss of epoch-28 batch-42 = 2.528168261051178e-05

Training epoch-28 batch-43
Running loss of epoch-28 batch-43 = 2.51084566116333e-05

Training epoch-28 batch-44
Running loss of epoch-28 batch-44 = 3.859051503241062e-05

Training epoch-28 batch-45
Running loss of epoch-28 batch-45 = 1.5012803487479687e-05

Training epoch-28 batch-46
Running loss of epoch-28 batch-46 = 1.568906009197235e-05

Training epoch-28 batch-47
Running loss of epoch-28 batch-47 = 1.4543766155838966e-05

Training epoch-28 batch-48
Running loss of epoch-28 batch-48 = 1.6161473467946053e-05

Training epoch-28 batch-49
Running loss of epoch-28 batch-49 = 1.4879507943987846e-05

Training epoch-28 batch-50
Running loss of epoch-28 batch-50 = 2.5009969249367714e-05

Training epoch-28 batch-51
Running loss of epoch-28 batch-51 = 2.1948013454675674e-05

Training epoch-28 batch-52
Running loss of epoch-28 batch-52 = 2.3521482944488525e-05

Training epoch-28 batch-53
Running loss of epoch-28 batch-53 = 1.5041092410683632e-05

Training epoch-28 batch-54
Running loss of epoch-28 batch-54 = 1.2857140973210335e-05

Training epoch-28 batch-55
Running loss of epoch-28 batch-55 = 1.728837378323078e-05

Training epoch-28 batch-56
Running loss of epoch-28 batch-56 = 1.006200909614563e-05

Training epoch-28 batch-57
Running loss of epoch-28 batch-57 = 2.393685281276703e-05

Training epoch-28 batch-58
Running loss of epoch-28 batch-58 = 2.314150333404541e-05

Training epoch-28 batch-59
Running loss of epoch-28 batch-59 = 5.470169708132744e-05

Training epoch-28 batch-60
Running loss of epoch-28 batch-60 = 1.2030359357595444e-05

Training epoch-28 batch-61
Running loss of epoch-28 batch-61 = 2.954038791358471e-05

Training epoch-28 batch-62
Running loss of epoch-28 batch-62 = 1.1728610843420029e-05

Training epoch-28 batch-63
Running loss of epoch-28 batch-63 = 2.3543834686279297e-05

Training epoch-28 batch-64
Running loss of epoch-28 batch-64 = 1.1250842362642288e-05

Training epoch-28 batch-65
Running loss of epoch-28 batch-65 = 2.3114844225347042e-05

Training epoch-28 batch-66
Running loss of epoch-28 batch-66 = 1.7887679859995842e-05

Training epoch-28 batch-67
Running loss of epoch-28 batch-67 = 1.5323515981435776e-05

Training epoch-28 batch-68
Running loss of epoch-28 batch-68 = 8.952571079134941e-06

Training epoch-28 batch-69
Running loss of epoch-28 batch-69 = 2.395128831267357e-05

Training epoch-28 batch-70
Running loss of epoch-28 batch-70 = 2.0661624148488045e-05

Training epoch-28 batch-71
Running loss of epoch-28 batch-71 = 4.630303010344505e-06

Training epoch-28 batch-72
Running loss of epoch-28 batch-72 = 1.565949060022831e-05

Training epoch-28 batch-73
Running loss of epoch-28 batch-73 = 2.176966518163681e-05

Training epoch-28 batch-74
Running loss of epoch-28 batch-74 = 3.2013049349188805e-05

Training epoch-28 batch-75
Running loss of epoch-28 batch-75 = 1.816614530980587e-05

Training epoch-28 batch-76
Running loss of epoch-28 batch-76 = 6.11462164670229e-05

Training epoch-28 batch-77
Running loss of epoch-28 batch-77 = 1.1380529031157494e-05

Training epoch-28 batch-78
Running loss of epoch-28 batch-78 = 3.168685361742973e-05

Training epoch-28 batch-79
Running loss of epoch-28 batch-79 = 2.3847445845603943e-05

Training epoch-28 batch-80
Running loss of epoch-28 batch-80 = 2.1291663870215416e-05

Training epoch-28 batch-81
Running loss of epoch-28 batch-81 = 2.0907726138830185e-05

Training epoch-28 batch-82
Running loss of epoch-28 batch-82 = 2.7498463168740273e-05

Training epoch-28 batch-83
Running loss of epoch-28 batch-83 = 4.066620022058487e-06

Training epoch-28 batch-84
Running loss of epoch-28 batch-84 = 2.8442475013434887e-05

Training epoch-28 batch-85
Running loss of epoch-28 batch-85 = 8.300412446260452e-06

Training epoch-28 batch-86
Running loss of epoch-28 batch-86 = 1.1822907254099846e-05

Training epoch-28 batch-87
Running loss of epoch-28 batch-87 = 2.2260239347815514e-05

Training epoch-28 batch-88
Running loss of epoch-28 batch-88 = 3.2881973311305046e-05

Training epoch-28 batch-89
Running loss of epoch-28 batch-89 = 2.856459468603134e-05

Training epoch-28 batch-90
Running loss of epoch-28 batch-90 = 1.772446557879448e-05

Training epoch-28 batch-91
Running loss of epoch-28 batch-91 = 6.280036177486181e-05

Training epoch-28 batch-92
Running loss of epoch-28 batch-92 = 2.4122069589793682e-05

Training epoch-28 batch-93
Running loss of epoch-28 batch-93 = 4.823529161512852e-05

Training epoch-28 batch-94
Running loss of epoch-28 batch-94 = 4.044896923005581e-05

Training epoch-28 batch-95
Running loss of epoch-28 batch-95 = 3.0337832868099213e-05

Training epoch-28 batch-96
Running loss of epoch-28 batch-96 = 1.9454630091786385e-05

Training epoch-28 batch-97
Running loss of epoch-28 batch-97 = 1.0818126611411572e-05

Training epoch-28 batch-98
Running loss of epoch-28 batch-98 = 1.54995359480381e-05

Training epoch-28 batch-99
Running loss of epoch-28 batch-99 = 2.84191919490695e-05

Training epoch-28 batch-100
Running loss of epoch-28 batch-100 = 2.4154549464583397e-05

Training epoch-28 batch-101
Running loss of epoch-28 batch-101 = 3.199512138962746e-05

Training epoch-28 batch-102
Running loss of epoch-28 batch-102 = 2.59672524407506e-05

Training epoch-28 batch-103
Running loss of epoch-28 batch-103 = 1.360173337161541e-05

Training epoch-28 batch-104
Running loss of epoch-28 batch-104 = 9.841052815318108e-06

Training epoch-28 batch-105
Running loss of epoch-28 batch-105 = 1.5363097190856934e-05

Training epoch-28 batch-106
Running loss of epoch-28 batch-106 = 1.0504620149731636e-05

Training epoch-28 batch-107
Running loss of epoch-28 batch-107 = 4.9398280680179596e-05

Training epoch-28 batch-108
Running loss of epoch-28 batch-108 = 1.6450416296720505e-05

Training epoch-28 batch-109
Running loss of epoch-28 batch-109 = 1.3872049748897552e-05

Training epoch-28 batch-110
Running loss of epoch-28 batch-110 = 4.208856262266636e-05

Training epoch-28 batch-111
Running loss of epoch-28 batch-111 = 1.3424316421151161e-05

Training epoch-28 batch-112
Running loss of epoch-28 batch-112 = 1.5796860679984093e-05

Training epoch-28 batch-113
Running loss of epoch-28 batch-113 = 9.115086868405342e-06

Training epoch-28 batch-114
Running loss of epoch-28 batch-114 = 1.0581454262137413e-05

Training epoch-28 batch-115
Running loss of epoch-28 batch-115 = 1.742970198392868e-05

Training epoch-28 batch-116
Running loss of epoch-28 batch-116 = 2.381065860390663e-05

Training epoch-28 batch-117
Running loss of epoch-28 batch-117 = 1.4900928363204002e-05

Training epoch-28 batch-118
Running loss of epoch-28 batch-118 = 5.804270040243864e-05

Training epoch-28 batch-119
Running loss of epoch-28 batch-119 = 3.213132731616497e-05

Training epoch-28 batch-120
Running loss of epoch-28 batch-120 = 9.79122705757618e-06

Training epoch-28 batch-121
Running loss of epoch-28 batch-121 = 1.7445418052375317e-05

Training epoch-28 batch-122
Running loss of epoch-28 batch-122 = 2.501392737030983e-05

Training epoch-28 batch-123
Running loss of epoch-28 batch-123 = 1.802109181880951e-05

Training epoch-28 batch-124
Running loss of epoch-28 batch-124 = 1.4116289094090462e-05

Training epoch-28 batch-125
Running loss of epoch-28 batch-125 = 1.658813562244177e-05

Training epoch-28 batch-126
Running loss of epoch-28 batch-126 = 1.4392426237463951e-05

Training epoch-28 batch-127
Running loss of epoch-28 batch-127 = 1.4526303857564926e-05

Training epoch-28 batch-128
Running loss of epoch-28 batch-128 = 1.3819197192788124e-05

Training epoch-28 batch-129
Running loss of epoch-28 batch-129 = 3.139371983706951e-05

Training epoch-28 batch-130
Running loss of epoch-28 batch-130 = 1.8034828826785088e-05

Training epoch-28 batch-131
Running loss of epoch-28 batch-131 = 9.477371349930763e-06

Training epoch-28 batch-132
Running loss of epoch-28 batch-132 = 1.742178574204445e-05

Training epoch-28 batch-133
Running loss of epoch-28 batch-133 = 2.6838621124625206e-05

Training epoch-28 batch-134
Running loss of epoch-28 batch-134 = 3.583519719541073e-05

Training epoch-28 batch-135
Running loss of epoch-28 batch-135 = 4.8269168473780155e-05

Training epoch-28 batch-136
Running loss of epoch-28 batch-136 = 6.864732131361961e-05

Training epoch-28 batch-137
Running loss of epoch-28 batch-137 = 1.390255056321621e-05

Training epoch-28 batch-138
Running loss of epoch-28 batch-138 = 8.096452802419662e-06

Training epoch-28 batch-139
Running loss of epoch-28 batch-139 = 6.762286648154259e-05

Training epoch-28 batch-140
Running loss of epoch-28 batch-140 = 5.8647594414651394e-05

Training epoch-28 batch-141
Running loss of epoch-28 batch-141 = 1.7930520698428154e-05

Training epoch-28 batch-142
Running loss of epoch-28 batch-142 = 1.0427553206682205e-05

Training epoch-28 batch-143
Running loss of epoch-28 batch-143 = 7.486669346690178e-06

Training epoch-28 batch-144
Running loss of epoch-28 batch-144 = 2.090982161462307e-05

Training epoch-28 batch-145
Running loss of epoch-28 batch-145 = 2.692360430955887e-05

Training epoch-28 batch-146
Running loss of epoch-28 batch-146 = 2.9469607397913933e-05

Training epoch-28 batch-147
Running loss of epoch-28 batch-147 = 8.91042873263359e-06

Training epoch-28 batch-148
Running loss of epoch-28 batch-148 = 1.0535004548728466e-05

Training epoch-28 batch-149
Running loss of epoch-28 batch-149 = 2.1140906028449535e-05

Training epoch-28 batch-150
Running loss of epoch-28 batch-150 = 2.079969272017479e-05

Training epoch-28 batch-151
Running loss of epoch-28 batch-151 = 1.5955185517668724e-05

Training epoch-28 batch-152
Running loss of epoch-28 batch-152 = 3.101385664194822e-05

Training epoch-28 batch-153
Running loss of epoch-28 batch-153 = 2.133520320057869e-05

Training epoch-28 batch-154
Running loss of epoch-28 batch-154 = 1.5251338481903076e-05

Training epoch-28 batch-155
Running loss of epoch-28 batch-155 = 1.641036942601204e-05

Training epoch-28 batch-156
Running loss of epoch-28 batch-156 = 1.5174038708209991e-05

Training epoch-28 batch-157
Running loss of epoch-28 batch-157 = 1.8436461687088013e-05

Finished training epoch-28.



Average train loss at epoch-28 = 2.272949516773224e-05

Started Evaluation

Average val loss at epoch-28 = 0.957200088777977

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 90.66 %
Accuracy for class onCreate is: 93.28 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 66.44 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.14 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.31 %

Finished Evaluation



Started training epoch-29


Training epoch-29 batch-1
Running loss of epoch-29 batch-1 = 1.265411265194416e-05

Training epoch-29 batch-2
Running loss of epoch-29 batch-2 = 1.89200509339571e-05

Training epoch-29 batch-3
Running loss of epoch-29 batch-3 = 3.61825805157423e-05

Training epoch-29 batch-4
Running loss of epoch-29 batch-4 = 1.9157538190484047e-05

Training epoch-29 batch-5
Running loss of epoch-29 batch-5 = 1.2938166037201881e-05

Training epoch-29 batch-6
Running loss of epoch-29 batch-6 = 1.9249855540692806e-05

Training epoch-29 batch-7
Running loss of epoch-29 batch-7 = 1.688161864876747e-05

Training epoch-29 batch-8
Running loss of epoch-29 batch-8 = 1.4224322512745857e-05

Training epoch-29 batch-9
Running loss of epoch-29 batch-9 = 1.2111850082874298e-05

Training epoch-29 batch-10
Running loss of epoch-29 batch-10 = 5.806563422083855e-06

Training epoch-29 batch-11
Running loss of epoch-29 batch-11 = 1.2049218639731407e-05

Training epoch-29 batch-12
Running loss of epoch-29 batch-12 = 8.88605136424303e-05

Training epoch-29 batch-13
Running loss of epoch-29 batch-13 = 2.9724324122071266e-05

Training epoch-29 batch-14
Running loss of epoch-29 batch-14 = 1.5145284123718739e-05

Training epoch-29 batch-15
Running loss of epoch-29 batch-15 = 3.0405004508793354e-05

Training epoch-29 batch-16
Running loss of epoch-29 batch-16 = 2.2991793230175972e-05

Training epoch-29 batch-17
Running loss of epoch-29 batch-17 = 1.2373784556984901e-05

Training epoch-29 batch-18
Running loss of epoch-29 batch-18 = 1.5084515325725079e-05

Training epoch-29 batch-19
Running loss of epoch-29 batch-19 = 8.172588422894478e-06

Training epoch-29 batch-20
Running loss of epoch-29 batch-20 = 1.4238525182008743e-05

Training epoch-29 batch-21
Running loss of epoch-29 batch-21 = 3.462121821939945e-05

Training epoch-29 batch-22
Running loss of epoch-29 batch-22 = 2.981489524245262e-05

Training epoch-29 batch-23
Running loss of epoch-29 batch-23 = 1.1363066732883453e-05

Training epoch-29 batch-24
Running loss of epoch-29 batch-24 = 1.7187092453241348e-05

Training epoch-29 batch-25
Running loss of epoch-29 batch-25 = 1.0313931852579117e-05

Training epoch-29 batch-26
Running loss of epoch-29 batch-26 = 5.3307623602449894e-05

Training epoch-29 batch-27
Running loss of epoch-29 batch-27 = 1.2195203453302383e-05

Training epoch-29 batch-28
Running loss of epoch-29 batch-28 = 2.4089938960969448e-05

Training epoch-29 batch-29
Running loss of epoch-29 batch-29 = 2.3082713596522808e-05

Training epoch-29 batch-30
Running loss of epoch-29 batch-30 = 1.7442507669329643e-05

Training epoch-29 batch-31
Running loss of epoch-29 batch-31 = 1.800060272216797e-05

Training epoch-29 batch-32
Running loss of epoch-29 batch-32 = 9.230803698301315e-06

Training epoch-29 batch-33
Running loss of epoch-29 batch-33 = 4.865298978984356e-05

Training epoch-29 batch-34
Running loss of epoch-29 batch-34 = 1.4966470189392567e-05

Training epoch-29 batch-35
Running loss of epoch-29 batch-35 = 1.7951708287000656e-05

Training epoch-29 batch-36
Running loss of epoch-29 batch-36 = 2.5284942239522934e-05

Training epoch-29 batch-37
Running loss of epoch-29 batch-37 = 5.168840289115906e-05

Training epoch-29 batch-38
Running loss of epoch-29 batch-38 = 1.109694130718708e-05

Training epoch-29 batch-39
Running loss of epoch-29 batch-39 = 1.581525430083275e-05

Training epoch-29 batch-40
Running loss of epoch-29 batch-40 = 3.82403377443552e-05

Training epoch-29 batch-41
Running loss of epoch-29 batch-41 = 1.9045080989599228e-05

Training epoch-29 batch-42
Running loss of epoch-29 batch-42 = 5.432870239019394e-06

Training epoch-29 batch-43
Running loss of epoch-29 batch-43 = 3.12726479023695e-05

Training epoch-29 batch-44
Running loss of epoch-29 batch-44 = 2.8178677894175053e-05

Training epoch-29 batch-45
Running loss of epoch-29 batch-45 = 1.3107783161103725e-05

Training epoch-29 batch-46
Running loss of epoch-29 batch-46 = 1.748744398355484e-05

Training epoch-29 batch-47
Running loss of epoch-29 batch-47 = 1.22864730656147e-05

Training epoch-29 batch-48
Running loss of epoch-29 batch-48 = 2.7752015739679337e-05

Training epoch-29 batch-49
Running loss of epoch-29 batch-49 = 1.4082295820116997e-05

Training epoch-29 batch-50
Running loss of epoch-29 batch-50 = 1.8061604350805283e-05

Training epoch-29 batch-51
Running loss of epoch-29 batch-51 = 1.527508720755577e-05

Training epoch-29 batch-52
Running loss of epoch-29 batch-52 = 2.831779420375824e-05

Training epoch-29 batch-53
Running loss of epoch-29 batch-53 = 2.3326254449784756e-05

Training epoch-29 batch-54
Running loss of epoch-29 batch-54 = 1.7051352187991142e-05

Training epoch-29 batch-55
Running loss of epoch-29 batch-55 = 1.5053432434797287e-05

Training epoch-29 batch-56
Running loss of epoch-29 batch-56 = 3.552739508450031e-05

Training epoch-29 batch-57
Running loss of epoch-29 batch-57 = 1.4535384252667427e-05

Training epoch-29 batch-58
Running loss of epoch-29 batch-58 = 9.440584108233452e-06

Training epoch-29 batch-59
Running loss of epoch-29 batch-59 = 2.4885404855012894e-05

Training epoch-29 batch-60
Running loss of epoch-29 batch-60 = 1.0033370926976204e-05

Training epoch-29 batch-61
Running loss of epoch-29 batch-61 = 3.943569026887417e-05

Training epoch-29 batch-62
Running loss of epoch-29 batch-62 = 4.973262548446655e-06

Training epoch-29 batch-63
Running loss of epoch-29 batch-63 = 1.2599979527294636e-05

Training epoch-29 batch-64
Running loss of epoch-29 batch-64 = 3.597536124289036e-05

Training epoch-29 batch-65
Running loss of epoch-29 batch-65 = 1.8367310985922813e-05

Training epoch-29 batch-66
Running loss of epoch-29 batch-66 = 2.0777108147740364e-05

Training epoch-29 batch-67
Running loss of epoch-29 batch-67 = 1.16666778922081e-05

Training epoch-29 batch-68
Running loss of epoch-29 batch-68 = 1.3584038242697716e-05

Training epoch-29 batch-69
Running loss of epoch-29 batch-69 = 3.130082041025162e-05

Training epoch-29 batch-70
Running loss of epoch-29 batch-70 = 2.1402724087238312e-05

Training epoch-29 batch-71
Running loss of epoch-29 batch-71 = 2.3695058189332485e-05

Training epoch-29 batch-72
Running loss of epoch-29 batch-72 = 8.58725979924202e-06

Training epoch-29 batch-73
Running loss of epoch-29 batch-73 = 8.821254596114159e-06

Training epoch-29 batch-74
Running loss of epoch-29 batch-74 = 1.3756798580288887e-05

Training epoch-29 batch-75
Running loss of epoch-29 batch-75 = 4.5669497922062874e-05

Training epoch-29 batch-76
Running loss of epoch-29 batch-76 = 2.2381311282515526e-05

Training epoch-29 batch-77
Running loss of epoch-29 batch-77 = 2.6949448511004448e-05

Training epoch-29 batch-78
Running loss of epoch-29 batch-78 = 1.760898157954216e-05

Training epoch-29 batch-79
Running loss of epoch-29 batch-79 = 3.3049844205379486e-05

Training epoch-29 batch-80
Running loss of epoch-29 batch-80 = 2.7179019525647163e-05

Training epoch-29 batch-81
Running loss of epoch-29 batch-81 = 9.039649739861488e-06

Training epoch-29 batch-82
Running loss of epoch-29 batch-82 = 2.0010164007544518e-05

Training epoch-29 batch-83
Running loss of epoch-29 batch-83 = 1.0717194527387619e-05

Training epoch-29 batch-84
Running loss of epoch-29 batch-84 = 2.9921531677246094e-05

Training epoch-29 batch-85
Running loss of epoch-29 batch-85 = 2.818868961185217e-05

Training epoch-29 batch-86
Running loss of epoch-29 batch-86 = 4.069996066391468e-05

Training epoch-29 batch-87
Running loss of epoch-29 batch-87 = 5.604233592748642e-06

Training epoch-29 batch-88
Running loss of epoch-29 batch-88 = 4.8824469558894634e-05

Training epoch-29 batch-89
Running loss of epoch-29 batch-89 = 2.2310297936201096e-05

Training epoch-29 batch-90
Running loss of epoch-29 batch-90 = 5.507469177246094e-05

Training epoch-29 batch-91
Running loss of epoch-29 batch-91 = 1.5031080693006516e-05

Training epoch-29 batch-92
Running loss of epoch-29 batch-92 = 1.8507824279367924e-05

Training epoch-29 batch-93
Running loss of epoch-29 batch-93 = 1.6739126294851303e-05

Training epoch-29 batch-94
Running loss of epoch-29 batch-94 = 1.7950544133782387e-05

Training epoch-29 batch-95
Running loss of epoch-29 batch-95 = 1.3339333236217499e-05

Training epoch-29 batch-96
Running loss of epoch-29 batch-96 = 2.6950379833579063e-05

Training epoch-29 batch-97
Running loss of epoch-29 batch-97 = 1.275702379643917e-05

Training epoch-29 batch-98
Running loss of epoch-29 batch-98 = 2.9344693757593632e-05

Training epoch-29 batch-99
Running loss of epoch-29 batch-99 = 2.2415886633098125e-05

Training epoch-29 batch-100
Running loss of epoch-29 batch-100 = 1.2363656423985958e-05

Training epoch-29 batch-101
Running loss of epoch-29 batch-101 = 1.1912314221262932e-05

Training epoch-29 batch-102
Running loss of epoch-29 batch-102 = 2.0377570763230324e-05

Training epoch-29 batch-103
Running loss of epoch-29 batch-103 = 1.055723987519741e-05

Training epoch-29 batch-104
Running loss of epoch-29 batch-104 = 1.277984119951725e-05

Training epoch-29 batch-105
Running loss of epoch-29 batch-105 = 1.9472558051347733e-05

Training epoch-29 batch-106
Running loss of epoch-29 batch-106 = 2.444267738610506e-05

Training epoch-29 batch-107
Running loss of epoch-29 batch-107 = 1.3850163668394089e-05

Training epoch-29 batch-108
Running loss of epoch-29 batch-108 = 3.154447767883539e-05

Training epoch-29 batch-109
Running loss of epoch-29 batch-109 = 1.4082062989473343e-05

Training epoch-29 batch-110
Running loss of epoch-29 batch-110 = 9.43685881793499e-06

Training epoch-29 batch-111
Running loss of epoch-29 batch-111 = 2.5306595489382744e-05

Training epoch-29 batch-112
Running loss of epoch-29 batch-112 = 9.80752520263195e-06

Training epoch-29 batch-113
Running loss of epoch-29 batch-113 = 2.8198817744851112e-05

Training epoch-29 batch-114
Running loss of epoch-29 batch-114 = 2.371612936258316e-05

Training epoch-29 batch-115
Running loss of epoch-29 batch-115 = 8.349306881427765e-06

Training epoch-29 batch-116
Running loss of epoch-29 batch-116 = 2.1612620912492275e-05

Training epoch-29 batch-117
Running loss of epoch-29 batch-117 = 1.2263190001249313e-05

Training epoch-29 batch-118
Running loss of epoch-29 batch-118 = 1.2014992535114288e-05

Training epoch-29 batch-119
Running loss of epoch-29 batch-119 = 1.7486978322267532e-05

Training epoch-29 batch-120
Running loss of epoch-29 batch-120 = 1.725798938423395e-05

Training epoch-29 batch-121
Running loss of epoch-29 batch-121 = 1.0072486475110054e-05

Training epoch-29 batch-122
Running loss of epoch-29 batch-122 = 2.9956689104437828e-05

Training epoch-29 batch-123
Running loss of epoch-29 batch-123 = 2.2279447875916958e-05

Training epoch-29 batch-124
Running loss of epoch-29 batch-124 = 4.161789547652006e-05

Training epoch-29 batch-125
Running loss of epoch-29 batch-125 = 3.139069303870201e-05

Training epoch-29 batch-126
Running loss of epoch-29 batch-126 = 1.5825731679797173e-05

Training epoch-29 batch-127
Running loss of epoch-29 batch-127 = 1.46450474858284e-05

Training epoch-29 batch-128
Running loss of epoch-29 batch-128 = 8.273986168205738e-06

Training epoch-29 batch-129
Running loss of epoch-29 batch-129 = 2.1282117813825607e-05

Training epoch-29 batch-130
Running loss of epoch-29 batch-130 = 7.225200533866882e-06

Training epoch-29 batch-131
Running loss of epoch-29 batch-131 = 2.5979476049542427e-05

Training epoch-29 batch-132
Running loss of epoch-29 batch-132 = 2.9753195121884346e-05

Training epoch-29 batch-133
Running loss of epoch-29 batch-133 = 3.8654543459415436e-05

Training epoch-29 batch-134
Running loss of epoch-29 batch-134 = 2.6769819669425488e-05

Training epoch-29 batch-135
Running loss of epoch-29 batch-135 = 1.669023185968399e-05

Training epoch-29 batch-136
Running loss of epoch-29 batch-136 = 2.484337892383337e-05

Training epoch-29 batch-137
Running loss of epoch-29 batch-137 = 1.549883745610714e-05

Training epoch-29 batch-138
Running loss of epoch-29 batch-138 = 1.1525116860866547e-05

Training epoch-29 batch-139
Running loss of epoch-29 batch-139 = 1.3540731742978096e-05

Training epoch-29 batch-140
Running loss of epoch-29 batch-140 = 1.3996148481965065e-05

Training epoch-29 batch-141
Running loss of epoch-29 batch-141 = 1.3792887330055237e-05

Training epoch-29 batch-142
Running loss of epoch-29 batch-142 = 1.7161481082439423e-05

Training epoch-29 batch-143
Running loss of epoch-29 batch-143 = 3.0330847948789597e-06

Training epoch-29 batch-144
Running loss of epoch-29 batch-144 = 1.758011057972908e-05

Training epoch-29 batch-145
Running loss of epoch-29 batch-145 = 1.6903621144592762e-05

Training epoch-29 batch-146
Running loss of epoch-29 batch-146 = 7.377471774816513e-06

Training epoch-29 batch-147
Running loss of epoch-29 batch-147 = 3.436766564846039e-05

Training epoch-29 batch-148
Running loss of epoch-29 batch-148 = 4.025641828775406e-06

Training epoch-29 batch-149
Running loss of epoch-29 batch-149 = 9.687652345746756e-05

Training epoch-29 batch-150
Running loss of epoch-29 batch-150 = 2.7723610401153564e-05

Training epoch-29 batch-151
Running loss of epoch-29 batch-151 = 1.7126090824604034e-05

Training epoch-29 batch-152
Running loss of epoch-29 batch-152 = 1.561851240694523e-05

Training epoch-29 batch-153
Running loss of epoch-29 batch-153 = 7.88806937634945e-06

Training epoch-29 batch-154
Running loss of epoch-29 batch-154 = 1.1823605746030807e-05

Training epoch-29 batch-155
Running loss of epoch-29 batch-155 = 2.6903115212917328e-05

Training epoch-29 batch-156
Running loss of epoch-29 batch-156 = 1.6258331015706062e-05

Training epoch-29 batch-157
Running loss of epoch-29 batch-157 = 1.5284866094589233e-05

Finished training epoch-29.



Average train loss at epoch-29 = 2.097801864147186e-05

Started Evaluation

Average val loss at epoch-29 = 1.0059200689445453

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.49 %
Accuracy for class onCreate is: 94.14 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 60.96 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 50.00 %
Accuracy for class execute is: 48.59 %
Accuracy for class get is: 70.26 %

Overall Accuracy = 82.90 %

Finished Evaluation



Started training epoch-30


Training epoch-30 batch-1
Running loss of epoch-30 batch-1 = 1.3939454220235348e-05

Training epoch-30 batch-2
Running loss of epoch-30 batch-2 = 8.211703971028328e-06

Training epoch-30 batch-3
Running loss of epoch-30 batch-3 = 2.0111678168177605e-05

Training epoch-30 batch-4
Running loss of epoch-30 batch-4 = 3.248150460422039e-05

Training epoch-30 batch-5
Running loss of epoch-30 batch-5 = 4.161195829510689e-05

Training epoch-30 batch-6
Running loss of epoch-30 batch-6 = 1.9956030882894993e-05

Training epoch-30 batch-7
Running loss of epoch-30 batch-7 = 3.1945062801241875e-05

Training epoch-30 batch-8
Running loss of epoch-30 batch-8 = 1.3676704838871956e-05

Training epoch-30 batch-9
Running loss of epoch-30 batch-9 = 1.4811288565397263e-05

Training epoch-30 batch-10
Running loss of epoch-30 batch-10 = 2.4787848815321922e-05

Training epoch-30 batch-11
Running loss of epoch-30 batch-11 = 3.0362047255039215e-05

Training epoch-30 batch-12
Running loss of epoch-30 batch-12 = 4.279008135199547e-05

Training epoch-30 batch-13
Running loss of epoch-30 batch-13 = 1.1854339390993118e-05

Training epoch-30 batch-14
Running loss of epoch-30 batch-14 = 2.4220207706093788e-05

Training epoch-30 batch-15
Running loss of epoch-30 batch-15 = 1.2582400813698769e-05

Training epoch-30 batch-16
Running loss of epoch-30 batch-16 = 9.329989552497864e-06

Training epoch-30 batch-17
Running loss of epoch-30 batch-17 = 1.824740320444107e-05

Training epoch-30 batch-18
Running loss of epoch-30 batch-18 = 1.9382452592253685e-05

Training epoch-30 batch-19
Running loss of epoch-30 batch-19 = 1.9720057025551796e-05

Training epoch-30 batch-20
Running loss of epoch-30 batch-20 = 2.679484896361828e-05

Training epoch-30 batch-21
Running loss of epoch-30 batch-21 = 3.170187119394541e-05

Training epoch-30 batch-22
Running loss of epoch-30 batch-22 = 1.556333154439926e-05

Training epoch-30 batch-23
Running loss of epoch-30 batch-23 = 1.537846401333809e-05

Training epoch-30 batch-24
Running loss of epoch-30 batch-24 = 1.6203499399125576e-05

Training epoch-30 batch-25
Running loss of epoch-30 batch-25 = 3.274972550570965e-05

Training epoch-30 batch-26
Running loss of epoch-30 batch-26 = 1.4371005818247795e-05

Training epoch-30 batch-27
Running loss of epoch-30 batch-27 = 2.017989754676819e-05

Training epoch-30 batch-28
Running loss of epoch-30 batch-28 = 7.94162042438984e-06

Training epoch-30 batch-29
Running loss of epoch-30 batch-29 = 2.374278847128153e-05

Training epoch-30 batch-30
Running loss of epoch-30 batch-30 = 1.9076280295848846e-05

Training epoch-30 batch-31
Running loss of epoch-30 batch-31 = 1.4684395864605904e-05

Training epoch-30 batch-32
Running loss of epoch-30 batch-32 = 1.3746554031968117e-05

Training epoch-30 batch-33
Running loss of epoch-30 batch-33 = 2.8181122615933418e-05

Training epoch-30 batch-34
Running loss of epoch-30 batch-34 = 1.506321132183075e-05

Training epoch-30 batch-35
Running loss of epoch-30 batch-35 = 1.1831405572593212e-05

Training epoch-30 batch-36
Running loss of epoch-30 batch-36 = 1.3877172023057938e-05

Training epoch-30 batch-37
Running loss of epoch-30 batch-37 = 1.5793368220329285e-05

Training epoch-30 batch-38
Running loss of epoch-30 batch-38 = 4.864414222538471e-06

Training epoch-30 batch-39
Running loss of epoch-30 batch-39 = 1.5868572518229485e-05

Training epoch-30 batch-40
Running loss of epoch-30 batch-40 = 1.0096002370119095e-05

Training epoch-30 batch-41
Running loss of epoch-30 batch-41 = 9.050359949469566e-06

Training epoch-30 batch-42
Running loss of epoch-30 batch-42 = 7.35861249268055e-06

Training epoch-30 batch-43
Running loss of epoch-30 batch-43 = 2.0867562852799892e-05

Training epoch-30 batch-44
Running loss of epoch-30 batch-44 = 7.848255336284637e-06

Training epoch-30 batch-45
Running loss of epoch-30 batch-45 = 6.931810639798641e-05

Training epoch-30 batch-46
Running loss of epoch-30 batch-46 = 1.2973090633749962e-05

Training epoch-30 batch-47
Running loss of epoch-30 batch-47 = 1.2364936992526054e-05

Training epoch-30 batch-48
Running loss of epoch-30 batch-48 = 1.6028760001063347e-05

Training epoch-30 batch-49
Running loss of epoch-30 batch-49 = 2.7315225452184677e-05

Training epoch-30 batch-50
Running loss of epoch-30 batch-50 = 1.6880454495549202e-05

Training epoch-30 batch-51
Running loss of epoch-30 batch-51 = 2.2161751985549927e-05

Training epoch-30 batch-52
Running loss of epoch-30 batch-52 = 6.719725206494331e-06

Training epoch-30 batch-53
Running loss of epoch-30 batch-53 = 4.325597546994686e-05

Training epoch-30 batch-54
Running loss of epoch-30 batch-54 = 1.1059921234846115e-05

Training epoch-30 batch-55
Running loss of epoch-30 batch-55 = 1.1727912351489067e-05

Training epoch-30 batch-56
Running loss of epoch-30 batch-56 = 4.285690374672413e-05

Training epoch-30 batch-57
Running loss of epoch-30 batch-57 = 1.5452736988663673e-05

Training epoch-30 batch-58
Running loss of epoch-30 batch-58 = 1.0089948773384094e-05

Training epoch-30 batch-59
Running loss of epoch-30 batch-59 = 2.8499634936451912e-05

Training epoch-30 batch-60
Running loss of epoch-30 batch-60 = 1.4811055734753609e-05

Training epoch-30 batch-61
Running loss of epoch-30 batch-61 = 4.1318126022815704e-06

Training epoch-30 batch-62
Running loss of epoch-30 batch-62 = 1.8525170162320137e-05

Training epoch-30 batch-63
Running loss of epoch-30 batch-63 = 3.3583492040634155e-06

Training epoch-30 batch-64
Running loss of epoch-30 batch-64 = 6.926245987415314e-06

Training epoch-30 batch-65
Running loss of epoch-30 batch-65 = 1.232675276696682e-05

Training epoch-30 batch-66
Running loss of epoch-30 batch-66 = 1.7097569070756435e-05

Training epoch-30 batch-67
Running loss of epoch-30 batch-67 = 1.4915247447788715e-05

Training epoch-30 batch-68
Running loss of epoch-30 batch-68 = 1.4241435565054417e-05

Training epoch-30 batch-69
Running loss of epoch-30 batch-69 = 1.982506364583969e-05

Training epoch-30 batch-70
Running loss of epoch-30 batch-70 = 8.519529365003109e-05

Training epoch-30 batch-71
Running loss of epoch-30 batch-71 = 1.9759172573685646e-05

Training epoch-30 batch-72
Running loss of epoch-30 batch-72 = 2.9370887205004692e-05

Training epoch-30 batch-73
Running loss of epoch-30 batch-73 = 8.403556421399117e-06

Training epoch-30 batch-74
Running loss of epoch-30 batch-74 = 8.649658411741257e-06

Training epoch-30 batch-75
Running loss of epoch-30 batch-75 = 1.3991259038448334e-05

Training epoch-30 batch-76
Running loss of epoch-30 batch-76 = 1.1662254109978676e-05

Training epoch-30 batch-77
Running loss of epoch-30 batch-77 = 1.3550976291298866e-05

Training epoch-30 batch-78
Running loss of epoch-30 batch-78 = 2.4682492949068546e-05

Training epoch-30 batch-79
Running loss of epoch-30 batch-79 = 6.67572021484375e-06

Training epoch-30 batch-80
Running loss of epoch-30 batch-80 = 1.0437332093715668e-05

Training epoch-30 batch-81
Running loss of epoch-30 batch-81 = 1.3438751921057701e-05

Training epoch-30 batch-82
Running loss of epoch-30 batch-82 = 2.180039882659912e-05

Training epoch-30 batch-83
Running loss of epoch-30 batch-83 = 4.188157618045807e-05

Training epoch-30 batch-84
Running loss of epoch-30 batch-84 = 2.2455351427197456e-05

Training epoch-30 batch-85
Running loss of epoch-30 batch-85 = 1.994241029024124e-05

Training epoch-30 batch-86
Running loss of epoch-30 batch-86 = 1.3690674677491188e-05

Training epoch-30 batch-87
Running loss of epoch-30 batch-87 = 2.563372254371643e-05

Training epoch-30 batch-88
Running loss of epoch-30 batch-88 = 2.5605782866477966e-05

Training epoch-30 batch-89
Running loss of epoch-30 batch-89 = 2.4592038244009018e-05

Training epoch-30 batch-90
Running loss of epoch-30 batch-90 = 1.4757737517356873e-05

Training epoch-30 batch-91
Running loss of epoch-30 batch-91 = 8.871080353856087e-06

Training epoch-30 batch-92
Running loss of epoch-30 batch-92 = 9.530922397971153e-06

Training epoch-30 batch-93
Running loss of epoch-30 batch-93 = 1.4039687812328339e-05

Training epoch-30 batch-94
Running loss of epoch-30 batch-94 = 3.342540003359318e-05

Training epoch-30 batch-95
Running loss of epoch-30 batch-95 = 3.2433075830340385e-05

Training epoch-30 batch-96
Running loss of epoch-30 batch-96 = 1.4148419722914696e-05

Training epoch-30 batch-97
Running loss of epoch-30 batch-97 = 1.495378091931343e-05

Training epoch-30 batch-98
Running loss of epoch-30 batch-98 = 1.8057413399219513e-05

Training epoch-30 batch-99
Running loss of epoch-30 batch-99 = 1.4869263395667076e-05

Training epoch-30 batch-100
Running loss of epoch-30 batch-100 = 3.829807974398136e-05

Training epoch-30 batch-101
Running loss of epoch-30 batch-101 = 2.9647722840309143e-05

Training epoch-30 batch-102
Running loss of epoch-30 batch-102 = 1.9500264897942543e-05

Training epoch-30 batch-103
Running loss of epoch-30 batch-103 = 2.613011747598648e-05

Training epoch-30 batch-104
Running loss of epoch-30 batch-104 = 1.3593817129731178e-05

Training epoch-30 batch-105
Running loss of epoch-30 batch-105 = 1.054513268172741e-05

Training epoch-30 batch-106
Running loss of epoch-30 batch-106 = 2.8663547709584236e-05

Training epoch-30 batch-107
Running loss of epoch-30 batch-107 = 2.530938945710659e-05

Training epoch-30 batch-108
Running loss of epoch-30 batch-108 = 1.737079583108425e-05

Training epoch-30 batch-109
Running loss of epoch-30 batch-109 = 5.967216566205025e-06

Training epoch-30 batch-110
Running loss of epoch-30 batch-110 = 2.134544774889946e-05

Training epoch-30 batch-111
Running loss of epoch-30 batch-111 = 1.3207551091909409e-05

Training epoch-30 batch-112
Running loss of epoch-30 batch-112 = 9.39890742301941e-06

Training epoch-30 batch-113
Running loss of epoch-30 batch-113 = 4.210870247334242e-05

Training epoch-30 batch-114
Running loss of epoch-30 batch-114 = 3.5358709283173084e-05

Training epoch-30 batch-115
Running loss of epoch-30 batch-115 = 1.6997684724628925e-05

Training epoch-30 batch-116
Running loss of epoch-30 batch-116 = 9.415438398718834e-06

Training epoch-30 batch-117
Running loss of epoch-30 batch-117 = 9.177601896226406e-06

Training epoch-30 batch-118
Running loss of epoch-30 batch-118 = 1.7928890883922577e-05

Training epoch-30 batch-119
Running loss of epoch-30 batch-119 = 8.441274985671043e-06

Training epoch-30 batch-120
Running loss of epoch-30 batch-120 = 9.63779166340828e-06

Training epoch-30 batch-121
Running loss of epoch-30 batch-121 = 1.1980067938566208e-05

Training epoch-30 batch-122
Running loss of epoch-30 batch-122 = 2.2174092009663582e-05

Training epoch-30 batch-123
Running loss of epoch-30 batch-123 = 1.0572373867034912e-05

Training epoch-30 batch-124
Running loss of epoch-30 batch-124 = 5.494104698300362e-06

Training epoch-30 batch-125
Running loss of epoch-30 batch-125 = 3.247696440666914e-05

Training epoch-30 batch-126
Running loss of epoch-30 batch-126 = 1.5815487131476402e-05

Training epoch-30 batch-127
Running loss of epoch-30 batch-127 = 1.2904871255159378e-05

Training epoch-30 batch-128
Running loss of epoch-30 batch-128 = 5.694595165550709e-05

Training epoch-30 batch-129
Running loss of epoch-30 batch-129 = 2.9843300580978394e-05

Training epoch-30 batch-130
Running loss of epoch-30 batch-130 = 2.7770642191171646e-05

Training epoch-30 batch-131
Running loss of epoch-30 batch-131 = 2.4750595912337303e-05

Training epoch-30 batch-132
Running loss of epoch-30 batch-132 = 2.091936767101288e-05

Training epoch-30 batch-133
Running loss of epoch-30 batch-133 = 1.2916047126054764e-05

Training epoch-30 batch-134
Running loss of epoch-30 batch-134 = 1.7570797353982925e-05

Training epoch-30 batch-135
Running loss of epoch-30 batch-135 = 1.0045710951089859e-05

Training epoch-30 batch-136
Running loss of epoch-30 batch-136 = 1.7683720216155052e-05

Training epoch-30 batch-137
Running loss of epoch-30 batch-137 = 2.2452790290117264e-05

Training epoch-30 batch-138
Running loss of epoch-30 batch-138 = 2.9692426323890686e-05

Training epoch-30 batch-139
Running loss of epoch-30 batch-139 = 8.988659828901291e-06

Training epoch-30 batch-140
Running loss of epoch-30 batch-140 = 2.6088091544806957e-05

Training epoch-30 batch-141
Running loss of epoch-30 batch-141 = 1.6043311916291714e-05

Training epoch-30 batch-142
Running loss of epoch-30 batch-142 = 1.0181451216340065e-05

Training epoch-30 batch-143
Running loss of epoch-30 batch-143 = 2.3367232643067837e-05

Training epoch-30 batch-144
Running loss of epoch-30 batch-144 = 2.0685838535428047e-05

Training epoch-30 batch-145
Running loss of epoch-30 batch-145 = 9.181676432490349e-06

Training epoch-30 batch-146
Running loss of epoch-30 batch-146 = 3.4485943615436554e-05

Training epoch-30 batch-147
Running loss of epoch-30 batch-147 = 1.3972516171634197e-05

Training epoch-30 batch-148
Running loss of epoch-30 batch-148 = 1.1180993169546127e-05

Training epoch-30 batch-149
Running loss of epoch-30 batch-149 = 1.3420707546174526e-05

Training epoch-30 batch-150
Running loss of epoch-30 batch-150 = 1.800060272216797e-05

Training epoch-30 batch-151
Running loss of epoch-30 batch-151 = 1.3057608157396317e-05

Training epoch-30 batch-152
Running loss of epoch-30 batch-152 = 1.8328544683754444e-05

Training epoch-30 batch-153
Running loss of epoch-30 batch-153 = 8.274801075458527e-06

Training epoch-30 batch-154
Running loss of epoch-30 batch-154 = 1.62667129188776e-05

Training epoch-30 batch-155
Running loss of epoch-30 batch-155 = 8.77375714480877e-06

Training epoch-30 batch-156
Running loss of epoch-30 batch-156 = 9.455252438783646e-06

Training epoch-30 batch-157
Running loss of epoch-30 batch-157 = 8.478015661239624e-05

Finished training epoch-30.



Average train loss at epoch-30 = 1.9220662862062456e-05

Started Evaluation

Average val loss at epoch-30 = 0.9782646760717829

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.49 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.59 %
Accuracy for class execute is: 43.78 %
Accuracy for class get is: 70.26 %

Overall Accuracy = 83.21 %

Finished Evaluation



Started training epoch-31


Training epoch-31 batch-1
Running loss of epoch-31 batch-1 = 5.602370947599411e-06

Training epoch-31 batch-2
Running loss of epoch-31 batch-2 = 1.0897871106863022e-05

Training epoch-31 batch-3
Running loss of epoch-31 batch-3 = 7.4678100645542145e-06

Training epoch-31 batch-4
Running loss of epoch-31 batch-4 = 4.3262261897325516e-06

Training epoch-31 batch-5
Running loss of epoch-31 batch-5 = 2.6407651603221893e-05

Training epoch-31 batch-6
Running loss of epoch-31 batch-6 = 2.085091546177864e-05

Training epoch-31 batch-7
Running loss of epoch-31 batch-7 = 3.292900510132313e-05

Training epoch-31 batch-8
Running loss of epoch-31 batch-8 = 5.66430389881134e-06

Training epoch-31 batch-9
Running loss of epoch-31 batch-9 = 2.2113090381026268e-05

Training epoch-31 batch-10
Running loss of epoch-31 batch-10 = 1.900852657854557e-05

Training epoch-31 batch-11
Running loss of epoch-31 batch-11 = 2.4588080123066902e-05

Training epoch-31 batch-12
Running loss of epoch-31 batch-12 = 1.4981254935264587e-05

Training epoch-31 batch-13
Running loss of epoch-31 batch-13 = 1.0576331987977028e-05

Training epoch-31 batch-14
Running loss of epoch-31 batch-14 = 1.5246681869029999e-05

Training epoch-31 batch-15
Running loss of epoch-31 batch-15 = 1.9813422113656998e-05

Training epoch-31 batch-16
Running loss of epoch-31 batch-16 = 4.3810345232486725e-05

Training epoch-31 batch-17
Running loss of epoch-31 batch-17 = 1.7700716853141785e-05

Training epoch-31 batch-18
Running loss of epoch-31 batch-18 = 6.072688847780228e-06

Training epoch-31 batch-19
Running loss of epoch-31 batch-19 = 2.1547311916947365e-05

Training epoch-31 batch-20
Running loss of epoch-31 batch-20 = 1.3796146959066391e-05

Training epoch-31 batch-21
Running loss of epoch-31 batch-21 = 3.8103084079921246e-05

Training epoch-31 batch-22
Running loss of epoch-31 batch-22 = 1.224735751748085e-05

Training epoch-31 batch-23
Running loss of epoch-31 batch-23 = 1.5543773770332336e-05

Training epoch-31 batch-24
Running loss of epoch-31 batch-24 = 2.176733687520027e-05

Training epoch-31 batch-25
Running loss of epoch-31 batch-25 = 1.21269840747118e-05

Training epoch-31 batch-26
Running loss of epoch-31 batch-26 = 2.0405743271112442e-05

Training epoch-31 batch-27
Running loss of epoch-31 batch-27 = 4.095444455742836e-05

Training epoch-31 batch-28
Running loss of epoch-31 batch-28 = 1.2774718925356865e-05

Training epoch-31 batch-29
Running loss of epoch-31 batch-29 = 8.264090865850449e-06

Training epoch-31 batch-30
Running loss of epoch-31 batch-30 = 1.3457727618515491e-05

Training epoch-31 batch-31
Running loss of epoch-31 batch-31 = 1.7874175682663918e-05

Training epoch-31 batch-32
Running loss of epoch-31 batch-32 = 4.4759130105376244e-05

Training epoch-31 batch-33
Running loss of epoch-31 batch-33 = 2.808426506817341e-05

Training epoch-31 batch-34
Running loss of epoch-31 batch-34 = 2.1242070943117142e-05

Training epoch-31 batch-35
Running loss of epoch-31 batch-35 = 1.0018469765782356e-05

Training epoch-31 batch-36
Running loss of epoch-31 batch-36 = 1.387705560773611e-05

Training epoch-31 batch-37
Running loss of epoch-31 batch-37 = 3.7726713344454765e-05

Training epoch-31 batch-38
Running loss of epoch-31 batch-38 = 2.9040384106338024e-05

Training epoch-31 batch-39
Running loss of epoch-31 batch-39 = 6.112968549132347e-06

Training epoch-31 batch-40
Running loss of epoch-31 batch-40 = 1.4540040865540504e-05

Training epoch-31 batch-41
Running loss of epoch-31 batch-41 = 1.2120930477976799e-05

Training epoch-31 batch-42
Running loss of epoch-31 batch-42 = 2.437247894704342e-05

Training epoch-31 batch-43
Running loss of epoch-31 batch-43 = 3.847386687994003e-05

Training epoch-31 batch-44
Running loss of epoch-31 batch-44 = 1.9070925191044807e-05

Training epoch-31 batch-45
Running loss of epoch-31 batch-45 = 1.107039861381054e-05

Training epoch-31 batch-46
Running loss of epoch-31 batch-46 = 9.485753253102303e-06

Training epoch-31 batch-47
Running loss of epoch-31 batch-47 = 5.669891834259033e-06

Training epoch-31 batch-48
Running loss of epoch-31 batch-48 = 1.672562211751938e-05

Training epoch-31 batch-49
Running loss of epoch-31 batch-49 = 1.8028542399406433e-05

Training epoch-31 batch-50
Running loss of epoch-31 batch-50 = 1.345551572740078e-05

Training epoch-31 batch-51
Running loss of epoch-31 batch-51 = 1.7012236639857292e-05

Training epoch-31 batch-52
Running loss of epoch-31 batch-52 = 2.0162435248494148e-05

Training epoch-31 batch-53
Running loss of epoch-31 batch-53 = 1.750071533024311e-05

Training epoch-31 batch-54
Running loss of epoch-31 batch-54 = 1.0341638699173927e-05

Training epoch-31 batch-55
Running loss of epoch-31 batch-55 = 2.454756759107113e-05

Training epoch-31 batch-56
Running loss of epoch-31 batch-56 = 1.2157135643064976e-05

Training epoch-31 batch-57
Running loss of epoch-31 batch-57 = 1.7055543139576912e-05

Training epoch-31 batch-58
Running loss of epoch-31 batch-58 = 2.8776703402400017e-05

Training epoch-31 batch-59
Running loss of epoch-31 batch-59 = 8.328352123498917e-06

Training epoch-31 batch-60
Running loss of epoch-31 batch-60 = 1.511559821665287e-05

Training epoch-31 batch-61
Running loss of epoch-31 batch-61 = 1.6096513718366623e-05

Training epoch-31 batch-62
Running loss of epoch-31 batch-62 = 1.8984312191605568e-05

Training epoch-31 batch-63
Running loss of epoch-31 batch-63 = 1.552235335111618e-05

Training epoch-31 batch-64
Running loss of epoch-31 batch-64 = 2.1980260498821735e-05

Training epoch-31 batch-65
Running loss of epoch-31 batch-65 = 1.592037733644247e-05

Training epoch-31 batch-66
Running loss of epoch-31 batch-66 = 8.765608072280884e-06

Training epoch-31 batch-67
Running loss of epoch-31 batch-67 = 2.1169427782297134e-05

Training epoch-31 batch-68
Running loss of epoch-31 batch-68 = 1.7672195099294186e-05

Training epoch-31 batch-69
Running loss of epoch-31 batch-69 = 7.287133485078812e-06

Training epoch-31 batch-70
Running loss of epoch-31 batch-70 = 2.8071575798094273e-05

Training epoch-31 batch-71
Running loss of epoch-31 batch-71 = 1.7372658476233482e-05

Training epoch-31 batch-72
Running loss of epoch-31 batch-72 = 1.0164454579353333e-05

Training epoch-31 batch-73
Running loss of epoch-31 batch-73 = 9.252224117517471e-06

Training epoch-31 batch-74
Running loss of epoch-31 batch-74 = 1.7852289602160454e-05

Training epoch-31 batch-75
Running loss of epoch-31 batch-75 = 1.0677613317966461e-05

Training epoch-31 batch-76
Running loss of epoch-31 batch-76 = 3.4942757338285446e-05

Training epoch-31 batch-77
Running loss of epoch-31 batch-77 = 1.1013820767402649e-05

Training epoch-31 batch-78
Running loss of epoch-31 batch-78 = 8.695060387253761e-06

Training epoch-31 batch-79
Running loss of epoch-31 batch-79 = 1.7179176211357117e-05

Training epoch-31 batch-80
Running loss of epoch-31 batch-80 = 6.6712964326143265e-06

Training epoch-31 batch-81
Running loss of epoch-31 batch-81 = 1.22458441182971e-05

Training epoch-31 batch-82
Running loss of epoch-31 batch-82 = 2.464966382831335e-05

Training epoch-31 batch-83
Running loss of epoch-31 batch-83 = 1.1135358363389969e-05

Training epoch-31 batch-84
Running loss of epoch-31 batch-84 = 3.4864526242017746e-05

Training epoch-31 batch-85
Running loss of epoch-31 batch-85 = 1.567206345498562e-05

Training epoch-31 batch-86
Running loss of epoch-31 batch-86 = 1.1454103514552116e-05

Training epoch-31 batch-87
Running loss of epoch-31 batch-87 = 1.2523145414888859e-05

Training epoch-31 batch-88
Running loss of epoch-31 batch-88 = 2.286233939230442e-05

Training epoch-31 batch-89
Running loss of epoch-31 batch-89 = 2.106593456119299e-05

Training epoch-31 batch-90
Running loss of epoch-31 batch-90 = 1.0500429198145866e-05

Training epoch-31 batch-91
Running loss of epoch-31 batch-91 = 5.5390410125255585e-06

Training epoch-31 batch-92
Running loss of epoch-31 batch-92 = 6.6461507230997086e-06

Training epoch-31 batch-93
Running loss of epoch-31 batch-93 = 8.152681402862072e-06

Training epoch-31 batch-94
Running loss of epoch-31 batch-94 = 7.24010169506073e-06

Training epoch-31 batch-95
Running loss of epoch-31 batch-95 = 2.4714041501283646e-05

Training epoch-31 batch-96
Running loss of epoch-31 batch-96 = 1.823226921260357e-05

Training epoch-31 batch-97
Running loss of epoch-31 batch-97 = 1.9724247977137566e-05

Training epoch-31 batch-98
Running loss of epoch-31 batch-98 = 2.1697254851460457e-05

Training epoch-31 batch-99
Running loss of epoch-31 batch-99 = 1.2936419807374477e-05

Training epoch-31 batch-100
Running loss of epoch-31 batch-100 = 2.2602267563343048e-05

Training epoch-31 batch-101
Running loss of epoch-31 batch-101 = 2.6986701413989067e-05

Training epoch-31 batch-102
Running loss of epoch-31 batch-102 = 2.041948027908802e-05

Training epoch-31 batch-103
Running loss of epoch-31 batch-103 = 1.0905787348747253e-05

Training epoch-31 batch-104
Running loss of epoch-31 batch-104 = 1.97594054043293e-05

Training epoch-31 batch-105
Running loss of epoch-31 batch-105 = 2.5334302335977554e-06

Training epoch-31 batch-106
Running loss of epoch-31 batch-106 = 1.558044459670782e-05

Training epoch-31 batch-107
Running loss of epoch-31 batch-107 = 1.2252014130353928e-05

Training epoch-31 batch-108
Running loss of epoch-31 batch-108 = 1.2304633855819702e-05

Training epoch-31 batch-109
Running loss of epoch-31 batch-109 = 1.0523712262511253e-05

Training epoch-31 batch-110
Running loss of epoch-31 batch-110 = 1.5967292711138725e-05

Training epoch-31 batch-111
Running loss of epoch-31 batch-111 = 6.941845640540123e-06

Training epoch-31 batch-112
Running loss of epoch-31 batch-112 = 1.0098563507199287e-05

Training epoch-31 batch-113
Running loss of epoch-31 batch-113 = 1.475634053349495e-05

Training epoch-31 batch-114
Running loss of epoch-31 batch-114 = 1.4050863683223724e-05

Training epoch-31 batch-115
Running loss of epoch-31 batch-115 = 1.2511154636740685e-05

Training epoch-31 batch-116
Running loss of epoch-31 batch-116 = 1.9444618374109268e-05

Training epoch-31 batch-117
Running loss of epoch-31 batch-117 = 1.1073192581534386e-05

Training epoch-31 batch-118
Running loss of epoch-31 batch-118 = 2.681789919734001e-05

Training epoch-31 batch-119
Running loss of epoch-31 batch-119 = 2.3424974642693996e-05

Training epoch-31 batch-120
Running loss of epoch-31 batch-120 = 1.9071274437010288e-05

Training epoch-31 batch-121
Running loss of epoch-31 batch-121 = 9.024050086736679e-06

Training epoch-31 batch-122
Running loss of epoch-31 batch-122 = 3.4199096262454987e-05

Training epoch-31 batch-123
Running loss of epoch-31 batch-123 = 2.418295480310917e-05

Training epoch-31 batch-124
Running loss of epoch-31 batch-124 = 1.1865748092532158e-05

Training epoch-31 batch-125
Running loss of epoch-31 batch-125 = 2.8892653062939644e-05

Training epoch-31 batch-126
Running loss of epoch-31 batch-126 = 1.257588155567646e-05

Training epoch-31 batch-127
Running loss of epoch-31 batch-127 = 9.549083188176155e-06

Training epoch-31 batch-128
Running loss of epoch-31 batch-128 = 1.925393007695675e-05

Training epoch-31 batch-129
Running loss of epoch-31 batch-129 = 2.2097723558545113e-05

Training epoch-31 batch-130
Running loss of epoch-31 batch-130 = 2.2005056962370872e-05

Training epoch-31 batch-131
Running loss of epoch-31 batch-131 = 1.0286225005984306e-05

Training epoch-31 batch-132
Running loss of epoch-31 batch-132 = 6.9141387939453125e-06

Training epoch-31 batch-133
Running loss of epoch-31 batch-133 = 2.2857333533465862e-05

Training epoch-31 batch-134
Running loss of epoch-31 batch-134 = 4.497612826526165e-05

Training epoch-31 batch-135
Running loss of epoch-31 batch-135 = 2.759275957942009e-05

Training epoch-31 batch-136
Running loss of epoch-31 batch-136 = 1.151359174400568e-05

Training epoch-31 batch-137
Running loss of epoch-31 batch-137 = 1.645972952246666e-05

Training epoch-31 batch-138
Running loss of epoch-31 batch-138 = 1.0343617759644985e-05

Training epoch-31 batch-139
Running loss of epoch-31 batch-139 = 6.980029866099358e-06

Training epoch-31 batch-140
Running loss of epoch-31 batch-140 = 8.531846106052399e-06

Training epoch-31 batch-141
Running loss of epoch-31 batch-141 = 2.4848151952028275e-05

Training epoch-31 batch-142
Running loss of epoch-31 batch-142 = 1.667579635977745e-05

Training epoch-31 batch-143
Running loss of epoch-31 batch-143 = 1.2701842933893204e-05

Training epoch-31 batch-144
Running loss of epoch-31 batch-144 = 1.5020603314042091e-05

Training epoch-31 batch-145
Running loss of epoch-31 batch-145 = 6.539514288306236e-06

Training epoch-31 batch-146
Running loss of epoch-31 batch-146 = 1.4834804460406303e-05

Training epoch-31 batch-147
Running loss of epoch-31 batch-147 = 2.8699636459350586e-05

Training epoch-31 batch-148
Running loss of epoch-31 batch-148 = 8.746981620788574e-06

Training epoch-31 batch-149
Running loss of epoch-31 batch-149 = 1.6471371054649353e-05

Training epoch-31 batch-150
Running loss of epoch-31 batch-150 = 2.077198587357998e-05

Training epoch-31 batch-151
Running loss of epoch-31 batch-151 = 1.1202646419405937e-05

Training epoch-31 batch-152
Running loss of epoch-31 batch-152 = 1.1963769793510437e-05

Training epoch-31 batch-153
Running loss of epoch-31 batch-153 = 1.8195947632193565e-05

Training epoch-31 batch-154
Running loss of epoch-31 batch-154 = 7.235934026539326e-05

Training epoch-31 batch-155
Running loss of epoch-31 batch-155 = 1.8245656974613667e-05

Training epoch-31 batch-156
Running loss of epoch-31 batch-156 = 2.111680805683136e-05

Training epoch-31 batch-157
Running loss of epoch-31 batch-157 = 3.1225383281707764e-05

Finished training epoch-31.



Average train loss at epoch-31 = 1.749291867017746e-05

Started Evaluation

Average val loss at epoch-31 = 0.9867346857475624

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 90.16 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 63.01 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.10 %

Finished Evaluation



Started training epoch-32


Training epoch-32 batch-1
Running loss of epoch-32 batch-1 = 5.912734195590019e-06

Training epoch-32 batch-2
Running loss of epoch-32 batch-2 = 5.77140599489212e-06

Training epoch-32 batch-3
Running loss of epoch-32 batch-3 = 1.435750164091587e-05

Training epoch-32 batch-4
Running loss of epoch-32 batch-4 = 1.0367250069975853e-05

Training epoch-32 batch-5
Running loss of epoch-32 batch-5 = 9.631272405385971e-06

Training epoch-32 batch-6
Running loss of epoch-32 batch-6 = 1.1056894436478615e-05

Training epoch-32 batch-7
Running loss of epoch-32 batch-7 = 6.470596417784691e-06

Training epoch-32 batch-8
Running loss of epoch-32 batch-8 = 3.420421853661537e-05

Training epoch-32 batch-9
Running loss of epoch-32 batch-9 = 2.2580381482839584e-05

Training epoch-32 batch-10
Running loss of epoch-32 batch-10 = 9.495764970779419e-06

Training epoch-32 batch-11
Running loss of epoch-32 batch-11 = 8.873874321579933e-06

Training epoch-32 batch-12
Running loss of epoch-32 batch-12 = 6.039859727025032e-06

Training epoch-32 batch-13
Running loss of epoch-32 batch-13 = 3.6117155104875565e-05

Training epoch-32 batch-14
Running loss of epoch-32 batch-14 = 2.8335489332675934e-06

Training epoch-32 batch-15
Running loss of epoch-32 batch-15 = 9.944895282387733e-06

Training epoch-32 batch-16
Running loss of epoch-32 batch-16 = 1.7677899450063705e-05

Training epoch-32 batch-17
Running loss of epoch-32 batch-17 = 1.3886252418160439e-05

Training epoch-32 batch-18
Running loss of epoch-32 batch-18 = 2.0295381546020508e-05

Training epoch-32 batch-19
Running loss of epoch-32 batch-19 = 1.887604594230652e-05

Training epoch-32 batch-20
Running loss of epoch-32 batch-20 = 2.611265517771244e-05

Training epoch-32 batch-21
Running loss of epoch-32 batch-21 = 9.297393262386322e-06

Training epoch-32 batch-22
Running loss of epoch-32 batch-22 = 2.3030675947666168e-05

Training epoch-32 batch-23
Running loss of epoch-32 batch-23 = 8.70577059686184e-06

Training epoch-32 batch-24
Running loss of epoch-32 batch-24 = 1.766020432114601e-05

Training epoch-32 batch-25
Running loss of epoch-32 batch-25 = 1.8710270524024963e-05

Training epoch-32 batch-26
Running loss of epoch-32 batch-26 = 9.624985978007317e-06

Training epoch-32 batch-27
Running loss of epoch-32 batch-27 = 8.281553164124489e-06

Training epoch-32 batch-28
Running loss of epoch-32 batch-28 = 1.670815981924534e-05

Training epoch-32 batch-29
Running loss of epoch-32 batch-29 = 9.238487109541893e-06

Training epoch-32 batch-30
Running loss of epoch-32 batch-30 = 1.4026416465640068e-05

Training epoch-32 batch-31
Running loss of epoch-32 batch-31 = 8.881324902176857e-06

Training epoch-32 batch-32
Running loss of epoch-32 batch-32 = 4.582758992910385e-05

Training epoch-32 batch-33
Running loss of epoch-32 batch-33 = 6.841961294412613e-06

Training epoch-32 batch-34
Running loss of epoch-32 batch-34 = 1.7895596101880074e-05

Training epoch-32 batch-35
Running loss of epoch-32 batch-35 = 6.2461476773023605e-06

Training epoch-32 batch-36
Running loss of epoch-32 batch-36 = 1.158728264272213e-05

Training epoch-32 batch-37
Running loss of epoch-32 batch-37 = 1.6633886843919754e-05

Training epoch-32 batch-38
Running loss of epoch-32 batch-38 = 1.2482982128858566e-05

Training epoch-32 batch-39
Running loss of epoch-32 batch-39 = 9.3772541731596e-06

Training epoch-32 batch-40
Running loss of epoch-32 batch-40 = 1.2062955647706985e-05

Training epoch-32 batch-41
Running loss of epoch-32 batch-41 = 1.2998469173908234e-05

Training epoch-32 batch-42
Running loss of epoch-32 batch-42 = 1.0920688509941101e-05

Training epoch-32 batch-43
Running loss of epoch-32 batch-43 = 1.1355383321642876e-05

Training epoch-32 batch-44
Running loss of epoch-32 batch-44 = 2.633756957948208e-05

Training epoch-32 batch-45
Running loss of epoch-32 batch-45 = 1.197238452732563e-05

Training epoch-32 batch-46
Running loss of epoch-32 batch-46 = 1.406203955411911e-05

Training epoch-32 batch-47
Running loss of epoch-32 batch-47 = 1.8073944374918938e-05

Training epoch-32 batch-48
Running loss of epoch-32 batch-48 = 8.588889613747597e-06

Training epoch-32 batch-49
Running loss of epoch-32 batch-49 = 1.0792864486575127e-05

Training epoch-32 batch-50
Running loss of epoch-32 batch-50 = 2.9387883841991425e-06

Training epoch-32 batch-51
Running loss of epoch-32 batch-51 = 2.1373620256781578e-05

Training epoch-32 batch-52
Running loss of epoch-32 batch-52 = 9.588664397597313e-06

Training epoch-32 batch-53
Running loss of epoch-32 batch-53 = 2.1123560145497322e-05

Training epoch-32 batch-54
Running loss of epoch-32 batch-54 = 1.5354249626398087e-05

Training epoch-32 batch-55
Running loss of epoch-32 batch-55 = 2.0945211872458458e-05

Training epoch-32 batch-56
Running loss of epoch-32 batch-56 = 1.5666126273572445e-05

Training epoch-32 batch-57
Running loss of epoch-32 batch-57 = 1.7949147149920464e-05

Training epoch-32 batch-58
Running loss of epoch-32 batch-58 = 9.423820301890373e-06

Training epoch-32 batch-59
Running loss of epoch-32 batch-59 = 1.4982419088482857e-05

Training epoch-32 batch-60
Running loss of epoch-32 batch-60 = 1.9258353859186172e-05

Training epoch-32 batch-61
Running loss of epoch-32 batch-61 = 1.2614764273166656e-05

Training epoch-32 batch-62
Running loss of epoch-32 batch-62 = 2.045975998044014e-05

Training epoch-32 batch-63
Running loss of epoch-32 batch-63 = 4.7188601456582546e-05

Training epoch-32 batch-64
Running loss of epoch-32 batch-64 = 1.1450611054897308e-05

Training epoch-32 batch-65
Running loss of epoch-32 batch-65 = 1.9696541130542755e-05

Training epoch-32 batch-66
Running loss of epoch-32 batch-66 = 2.1351035684347153e-05

Training epoch-32 batch-67
Running loss of epoch-32 batch-67 = 1.852423883974552e-05

Training epoch-32 batch-68
Running loss of epoch-32 batch-68 = 1.3184035196900368e-05

Training epoch-32 batch-69
Running loss of epoch-32 batch-69 = 2.6912661269307137e-05

Training epoch-32 batch-70
Running loss of epoch-32 batch-70 = 8.801929652690887e-06

Training epoch-32 batch-71
Running loss of epoch-32 batch-71 = 1.561117824167013e-05

Training epoch-32 batch-72
Running loss of epoch-32 batch-72 = 1.3619894161820412e-05

Training epoch-32 batch-73
Running loss of epoch-32 batch-73 = 7.201917469501495e-06

Training epoch-32 batch-74
Running loss of epoch-32 batch-74 = 1.7180340364575386e-05

Training epoch-32 batch-75
Running loss of epoch-32 batch-75 = 2.145557664334774e-05

Training epoch-32 batch-76
Running loss of epoch-32 batch-76 = 3.3452874049544334e-05

Training epoch-32 batch-77
Running loss of epoch-32 batch-77 = 2.0678387954831123e-05

Training epoch-32 batch-78
Running loss of epoch-32 batch-78 = 1.4623161405324936e-05

Training epoch-32 batch-79
Running loss of epoch-32 batch-79 = 1.93172600120306e-05

Training epoch-32 batch-80
Running loss of epoch-32 batch-80 = 1.1175638064742088e-05

Training epoch-32 batch-81
Running loss of epoch-32 batch-81 = 3.121630288660526e-05

Training epoch-32 batch-82
Running loss of epoch-32 batch-82 = 1.679663546383381e-05

Training epoch-32 batch-83
Running loss of epoch-32 batch-83 = 1.1597992852330208e-05

Training epoch-32 batch-84
Running loss of epoch-32 batch-84 = 1.8985243514180183e-05

Training epoch-32 batch-85
Running loss of epoch-32 batch-85 = 5.369773134589195e-06

Training epoch-32 batch-86
Running loss of epoch-32 batch-86 = 1.048017293214798e-05

Training epoch-32 batch-87
Running loss of epoch-32 batch-87 = 1.546391285955906e-05

Training epoch-32 batch-88
Running loss of epoch-32 batch-88 = 1.543457619845867e-05

Training epoch-32 batch-89
Running loss of epoch-32 batch-89 = 5.273649003356695e-05

Training epoch-32 batch-90
Running loss of epoch-32 batch-90 = 1.4743302017450333e-05

Training epoch-32 batch-91
Running loss of epoch-32 batch-91 = 1.823343336582184e-05

Training epoch-32 batch-92
Running loss of epoch-32 batch-92 = 2.367282286286354e-05

Training epoch-32 batch-93
Running loss of epoch-32 batch-93 = 1.1623604223132133e-05

Training epoch-32 batch-94
Running loss of epoch-32 batch-94 = 1.3804994523525238e-05

Training epoch-32 batch-95
Running loss of epoch-32 batch-95 = 1.6216421499848366e-05

Training epoch-32 batch-96
Running loss of epoch-32 batch-96 = 2.4249311536550522e-05

Training epoch-32 batch-97
Running loss of epoch-32 batch-97 = 8.298316970467567e-06

Training epoch-32 batch-98
Running loss of epoch-32 batch-98 = 7.302034646272659e-06

Training epoch-32 batch-99
Running loss of epoch-32 batch-99 = 1.1623604223132133e-05

Training epoch-32 batch-100
Running loss of epoch-32 batch-100 = 2.460484392940998e-05

Training epoch-32 batch-101
Running loss of epoch-32 batch-101 = 1.7772894352674484e-05

Training epoch-32 batch-102
Running loss of epoch-32 batch-102 = 1.1642230674624443e-05

Training epoch-32 batch-103
Running loss of epoch-32 batch-103 = 2.2067571990191936e-05

Training epoch-32 batch-104
Running loss of epoch-32 batch-104 = 1.2364005669951439e-05

Training epoch-32 batch-105
Running loss of epoch-32 batch-105 = 6.346812006086111e-05

Training epoch-32 batch-106
Running loss of epoch-32 batch-106 = 2.5377143174409866e-05

Training epoch-32 batch-107
Running loss of epoch-32 batch-107 = 7.425202056765556e-06

Training epoch-32 batch-108
Running loss of epoch-32 batch-108 = 1.437612809240818e-05

Training epoch-32 batch-109
Running loss of epoch-32 batch-109 = 2.8937123715877533e-05

Training epoch-32 batch-110
Running loss of epoch-32 batch-110 = 3.0189286917448044e-05

Training epoch-32 batch-111
Running loss of epoch-32 batch-111 = 8.355127647519112e-06

Training epoch-32 batch-112
Running loss of epoch-32 batch-112 = 1.2023258022964e-05

Training epoch-32 batch-113
Running loss of epoch-32 batch-113 = 1.2239906936883926e-05

Training epoch-32 batch-114
Running loss of epoch-32 batch-114 = 2.1135667338967323e-05

Training epoch-32 batch-115
Running loss of epoch-32 batch-115 = 1.1901487596333027e-05

Training epoch-32 batch-116
Running loss of epoch-32 batch-116 = 1.6463221982121468e-05

Training epoch-32 batch-117
Running loss of epoch-32 batch-117 = 9.085051715373993e-06

Training epoch-32 batch-118
Running loss of epoch-32 batch-118 = 1.7177080735564232e-05

Training epoch-32 batch-119
Running loss of epoch-32 batch-119 = 2.1837418898940086e-05

Training epoch-32 batch-120
Running loss of epoch-32 batch-120 = 1.8393388018012047e-05

Training epoch-32 batch-121
Running loss of epoch-32 batch-121 = 2.016429789364338e-05

Training epoch-32 batch-122
Running loss of epoch-32 batch-122 = 9.792158380150795e-06

Training epoch-32 batch-123
Running loss of epoch-32 batch-123 = 1.632561907172203e-05

Training epoch-32 batch-124
Running loss of epoch-32 batch-124 = 1.4349818229675293e-05

Training epoch-32 batch-125
Running loss of epoch-32 batch-125 = 2.87891598418355e-05

Training epoch-32 batch-126
Running loss of epoch-32 batch-126 = 1.4093471691012383e-05

Training epoch-32 batch-127
Running loss of epoch-32 batch-127 = 1.0124407708644867e-05

Training epoch-32 batch-128
Running loss of epoch-32 batch-128 = 2.6313471607863903e-05

Training epoch-32 batch-129
Running loss of epoch-32 batch-129 = 1.2327451258897781e-05

Training epoch-32 batch-130
Running loss of epoch-32 batch-130 = 1.4127232134342194e-05

Training epoch-32 batch-131
Running loss of epoch-32 batch-131 = 9.003793820738792e-06

Training epoch-32 batch-132
Running loss of epoch-32 batch-132 = 6.611226126551628e-06

Training epoch-32 batch-133
Running loss of epoch-32 batch-133 = 7.71181657910347e-06

Training epoch-32 batch-134
Running loss of epoch-32 batch-134 = 1.3451091945171356e-05

Training epoch-32 batch-135
Running loss of epoch-32 batch-135 = 9.2389527708292e-06

Training epoch-32 batch-136
Running loss of epoch-32 batch-136 = 2.7314876206219196e-05

Training epoch-32 batch-137
Running loss of epoch-32 batch-137 = 1.3310695067048073e-05

Training epoch-32 batch-138
Running loss of epoch-32 batch-138 = 1.3253884389996529e-05

Training epoch-32 batch-139
Running loss of epoch-32 batch-139 = 3.057881258428097e-05

Training epoch-32 batch-140
Running loss of epoch-32 batch-140 = 3.580283373594284e-05

Training epoch-32 batch-141
Running loss of epoch-32 batch-141 = 1.3472279533743858e-05

Training epoch-32 batch-142
Running loss of epoch-32 batch-142 = 8.038943633437157e-06

Training epoch-32 batch-143
Running loss of epoch-32 batch-143 = 9.690877050161362e-06

Training epoch-32 batch-144
Running loss of epoch-32 batch-144 = 1.1268770322203636e-05

Training epoch-32 batch-145
Running loss of epoch-32 batch-145 = 2.8149341233074665e-05

Training epoch-32 batch-146
Running loss of epoch-32 batch-146 = 2.3264670744538307e-05

Training epoch-32 batch-147
Running loss of epoch-32 batch-147 = 9.238719940185547e-06

Training epoch-32 batch-148
Running loss of epoch-32 batch-148 = 1.0176096111536026e-05

Training epoch-32 batch-149
Running loss of epoch-32 batch-149 = 1.1004740372300148e-05

Training epoch-32 batch-150
Running loss of epoch-32 batch-150 = 1.896300818771124e-05

Training epoch-32 batch-151
Running loss of epoch-32 batch-151 = 2.4058041162788868e-05

Training epoch-32 batch-152
Running loss of epoch-32 batch-152 = 8.334871381521225e-06

Training epoch-32 batch-153
Running loss of epoch-32 batch-153 = 1.3386830687522888e-05

Training epoch-32 batch-154
Running loss of epoch-32 batch-154 = 1.0596821084618568e-05

Training epoch-32 batch-155
Running loss of epoch-32 batch-155 = 1.5478581190109253e-05

Training epoch-32 batch-156
Running loss of epoch-32 batch-156 = 2.3277709260582924e-05

Training epoch-32 batch-157
Running loss of epoch-32 batch-157 = 0.00014139339327812195

Finished training epoch-32.



Average train loss at epoch-32 = 1.653089225292206e-05

Started Evaluation

Average val loss at epoch-32 = 1.0069207831493738

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.10 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 45.38 %
Accuracy for class get is: 69.49 %

Overall Accuracy = 82.81 %

Finished Evaluation



Started training epoch-33


Training epoch-33 batch-1
Running loss of epoch-33 batch-1 = 2.093566581606865e-05

Training epoch-33 batch-2
Running loss of epoch-33 batch-2 = 1.6562524251639843e-05

Training epoch-33 batch-3
Running loss of epoch-33 batch-3 = 5.355570465326309e-06

Training epoch-33 batch-4
Running loss of epoch-33 batch-4 = 1.5731900930404663e-05

Training epoch-33 batch-5
Running loss of epoch-33 batch-5 = 1.1327909305691719e-05

Training epoch-33 batch-6
Running loss of epoch-33 batch-6 = 4.381872713565826e-06

Training epoch-33 batch-7
Running loss of epoch-33 batch-7 = 3.602704964578152e-05

Training epoch-33 batch-8
Running loss of epoch-33 batch-8 = 1.3088574633002281e-05

Training epoch-33 batch-9
Running loss of epoch-33 batch-9 = 5.976296961307526e-06

Training epoch-33 batch-10
Running loss of epoch-33 batch-10 = 1.4730729162693024e-05

Training epoch-33 batch-11
Running loss of epoch-33 batch-11 = 1.77617184817791e-05

Training epoch-33 batch-12
Running loss of epoch-33 batch-12 = 2.6609981432557106e-05

Training epoch-33 batch-13
Running loss of epoch-33 batch-13 = 1.3943295925855637e-05

Training epoch-33 batch-14
Running loss of epoch-33 batch-14 = 1.3110227882862091e-05

Training epoch-33 batch-15
Running loss of epoch-33 batch-15 = 1.4537130482494831e-05

Training epoch-33 batch-16
Running loss of epoch-33 batch-16 = 7.1784015744924545e-06

Training epoch-33 batch-17
Running loss of epoch-33 batch-17 = 7.534166797995567e-06

Training epoch-33 batch-18
Running loss of epoch-33 batch-18 = 3.288709558546543e-05

Training epoch-33 batch-19
Running loss of epoch-33 batch-19 = 2.159224823117256e-05

Training epoch-33 batch-20
Running loss of epoch-33 batch-20 = 6.110174581408501e-06

Training epoch-33 batch-21
Running loss of epoch-33 batch-21 = 1.0483898222446442e-05

Training epoch-33 batch-22
Running loss of epoch-33 batch-22 = 1.3946788385510445e-05

Training epoch-33 batch-23
Running loss of epoch-33 batch-23 = 9.967712685465813e-06

Training epoch-33 batch-24
Running loss of epoch-33 batch-24 = 2.1979212760925293e-05

Training epoch-33 batch-25
Running loss of epoch-33 batch-25 = 1.6493024304509163e-05

Training epoch-33 batch-26
Running loss of epoch-33 batch-26 = 4.949048161506653e-06

Training epoch-33 batch-27
Running loss of epoch-33 batch-27 = 1.074490137398243e-05

Training epoch-33 batch-28
Running loss of epoch-33 batch-28 = 9.08784568309784e-06

Training epoch-33 batch-29
Running loss of epoch-33 batch-29 = 1.5842029824852943e-05

Training epoch-33 batch-30
Running loss of epoch-33 batch-30 = 2.3883534595370293e-05

Training epoch-33 batch-31
Running loss of epoch-33 batch-31 = 1.0485760867595673e-05

Training epoch-33 batch-32
Running loss of epoch-33 batch-32 = 1.0221032425761223e-05

Training epoch-33 batch-33
Running loss of epoch-33 batch-33 = 1.3556564226746559e-05

Training epoch-33 batch-34
Running loss of epoch-33 batch-34 = 1.2269709259271622e-05

Training epoch-33 batch-35
Running loss of epoch-33 batch-35 = 8.470844477415085e-06

Training epoch-33 batch-36
Running loss of epoch-33 batch-36 = 1.4902791008353233e-05

Training epoch-33 batch-37
Running loss of epoch-33 batch-37 = 1.8439488485455513e-05

Training epoch-33 batch-38
Running loss of epoch-33 batch-38 = 8.578412234783173e-06

Training epoch-33 batch-39
Running loss of epoch-33 batch-39 = 1.120707020163536e-05

Training epoch-33 batch-40
Running loss of epoch-33 batch-40 = 1.1450261808931828e-05

Training epoch-33 batch-41
Running loss of epoch-33 batch-41 = 1.771003007888794e-05

Training epoch-33 batch-42
Running loss of epoch-33 batch-42 = 8.897040970623493e-06

Training epoch-33 batch-43
Running loss of epoch-33 batch-43 = 1.6327714547514915e-05

Training epoch-33 batch-44
Running loss of epoch-33 batch-44 = 1.219648402184248e-05

Training epoch-33 batch-45
Running loss of epoch-33 batch-45 = 5.460577085614204e-06

Training epoch-33 batch-46
Running loss of epoch-33 batch-46 = 1.049460843205452e-05

Training epoch-33 batch-47
Running loss of epoch-33 batch-47 = 1.4767982065677643e-05

Training epoch-33 batch-48
Running loss of epoch-33 batch-48 = 1.3934215530753136e-05

Training epoch-33 batch-49
Running loss of epoch-33 batch-49 = 6.409594789147377e-06

Training epoch-33 batch-50
Running loss of epoch-33 batch-50 = 3.1781382858753204e-06

Training epoch-33 batch-51
Running loss of epoch-33 batch-51 = 1.737079583108425e-05

Training epoch-33 batch-52
Running loss of epoch-33 batch-52 = 2.1134736016392708e-05

Training epoch-33 batch-53
Running loss of epoch-33 batch-53 = 1.7906073480844498e-05

Training epoch-33 batch-54
Running loss of epoch-33 batch-54 = 2.9584974981844425e-05

Training epoch-33 batch-55
Running loss of epoch-33 batch-55 = 2.5122426450252533e-06

Training epoch-33 batch-56
Running loss of epoch-33 batch-56 = 3.6079436540603638e-06

Training epoch-33 batch-57
Running loss of epoch-33 batch-57 = 5.833141040056944e-05

Training epoch-33 batch-58
Running loss of epoch-33 batch-58 = 1.683156006038189e-05

Training epoch-33 batch-59
Running loss of epoch-33 batch-59 = 1.8946127966046333e-05

Training epoch-33 batch-60
Running loss of epoch-33 batch-60 = 9.631039574742317e-06

Training epoch-33 batch-61
Running loss of epoch-33 batch-61 = 5.218898877501488e-06

Training epoch-33 batch-62
Running loss of epoch-33 batch-62 = 2.223229967057705e-05

Training epoch-33 batch-63
Running loss of epoch-33 batch-63 = 1.7013633623719215e-05

Training epoch-33 batch-64
Running loss of epoch-33 batch-64 = 2.2732187062501907e-05

Training epoch-33 batch-65
Running loss of epoch-33 batch-65 = 6.128102540969849e-06

Training epoch-33 batch-66
Running loss of epoch-33 batch-66 = 1.565762795507908e-05

Training epoch-33 batch-67
Running loss of epoch-33 batch-67 = 1.1231633834540844e-05

Training epoch-33 batch-68
Running loss of epoch-33 batch-68 = 1.5225610695779324e-05

Training epoch-33 batch-69
Running loss of epoch-33 batch-69 = 2.3180851712822914e-05

Training epoch-33 batch-70
Running loss of epoch-33 batch-70 = 9.381445124745369e-06

Training epoch-33 batch-71
Running loss of epoch-33 batch-71 = 1.2886011973023415e-05

Training epoch-33 batch-72
Running loss of epoch-33 batch-72 = 2.9298244044184685e-05

Training epoch-33 batch-73
Running loss of epoch-33 batch-73 = 1.5459954738616943e-05

Training epoch-33 batch-74
Running loss of epoch-33 batch-74 = 1.642177812755108e-05

Training epoch-33 batch-75
Running loss of epoch-33 batch-75 = 1.7643673345446587e-05

Training epoch-33 batch-76
Running loss of epoch-33 batch-76 = 9.055715054273605e-06

Training epoch-33 batch-77
Running loss of epoch-33 batch-77 = 8.430099114775658e-06

Training epoch-33 batch-78
Running loss of epoch-33 batch-78 = 1.1895550414919853e-05

Training epoch-33 batch-79
Running loss of epoch-33 batch-79 = 9.96817834675312e-06

Training epoch-33 batch-80
Running loss of epoch-33 batch-80 = 5.7462602853775024e-06

Training epoch-33 batch-81
Running loss of epoch-33 batch-81 = 8.198898285627365e-06

Training epoch-33 batch-82
Running loss of epoch-33 batch-82 = 2.390798181295395e-05

Training epoch-33 batch-83
Running loss of epoch-33 batch-83 = 2.105371095240116e-05

Training epoch-33 batch-84
Running loss of epoch-33 batch-84 = 2.000213135033846e-05

Training epoch-33 batch-85
Running loss of epoch-33 batch-85 = 8.372124284505844e-06

Training epoch-33 batch-86
Running loss of epoch-33 batch-86 = 5.313276778906584e-05

Training epoch-33 batch-87
Running loss of epoch-33 batch-87 = 7.716706022620201e-06

Training epoch-33 batch-88
Running loss of epoch-33 batch-88 = 1.3231765478849411e-05

Training epoch-33 batch-89
Running loss of epoch-33 batch-89 = 7.865484803915024e-06

Training epoch-33 batch-90
Running loss of epoch-33 batch-90 = 1.7778482288122177e-05

Training epoch-33 batch-91
Running loss of epoch-33 batch-91 = 1.4019664376974106e-05

Training epoch-33 batch-92
Running loss of epoch-33 batch-92 = 9.396811947226524e-06

Training epoch-33 batch-93
Running loss of epoch-33 batch-93 = 2.6825815439224243e-05

Training epoch-33 batch-94
Running loss of epoch-33 batch-94 = 1.5006400644779205e-05

Training epoch-33 batch-95
Running loss of epoch-33 batch-95 = 1.0797404684126377e-05

Training epoch-33 batch-96
Running loss of epoch-33 batch-96 = 1.3369601219892502e-05

Training epoch-33 batch-97
Running loss of epoch-33 batch-97 = 8.980976417660713e-06

Training epoch-33 batch-98
Running loss of epoch-33 batch-98 = 1.827091909945011e-05

Training epoch-33 batch-99
Running loss of epoch-33 batch-99 = 9.129522368311882e-06

Training epoch-33 batch-100
Running loss of epoch-33 batch-100 = 1.2355856597423553e-05

Training epoch-33 batch-101
Running loss of epoch-33 batch-101 = 1.117517240345478e-05

Training epoch-33 batch-102
Running loss of epoch-33 batch-102 = 1.96872279047966e-05

Training epoch-33 batch-103
Running loss of epoch-33 batch-103 = 2.332148142158985e-05

Training epoch-33 batch-104
Running loss of epoch-33 batch-104 = 8.654315024614334e-06

Training epoch-33 batch-105
Running loss of epoch-33 batch-105 = 2.079736441373825e-05

Training epoch-33 batch-106
Running loss of epoch-33 batch-106 = 4.1454448364675045e-05

Training epoch-33 batch-107
Running loss of epoch-33 batch-107 = 1.0530464351177216e-05

Training epoch-33 batch-108
Running loss of epoch-33 batch-108 = 8.465489372611046e-06

Training epoch-33 batch-109
Running loss of epoch-33 batch-109 = 1.8063816241919994e-05

Training epoch-33 batch-110
Running loss of epoch-33 batch-110 = 1.3108947314321995e-05

Training epoch-33 batch-111
Running loss of epoch-33 batch-111 = 3.2338895834982395e-05

Training epoch-33 batch-112
Running loss of epoch-33 batch-112 = 2.2877240553498268e-05

Training epoch-33 batch-113
Running loss of epoch-33 batch-113 = 3.6893878132104874e-05

Training epoch-33 batch-114
Running loss of epoch-33 batch-114 = 1.5822937712073326e-05

Training epoch-33 batch-115
Running loss of epoch-33 batch-115 = 5.265465006232262e-06

Training epoch-33 batch-116
Running loss of epoch-33 batch-116 = 1.263967715203762e-05

Training epoch-33 batch-117
Running loss of epoch-33 batch-117 = 8.498085662722588e-06

Training epoch-33 batch-118
Running loss of epoch-33 batch-118 = 9.171664714813232e-06

Training epoch-33 batch-119
Running loss of epoch-33 batch-119 = 1.341826282441616e-05

Training epoch-33 batch-120
Running loss of epoch-33 batch-120 = 1.2953067198395729e-05

Training epoch-33 batch-121
Running loss of epoch-33 batch-121 = 6.624963134527206e-06

Training epoch-33 batch-122
Running loss of epoch-33 batch-122 = 4.695495590567589e-06

Training epoch-33 batch-123
Running loss of epoch-33 batch-123 = 3.3252639696002007e-05

Training epoch-33 batch-124
Running loss of epoch-33 batch-124 = 8.614035323262215e-06

Training epoch-33 batch-125
Running loss of epoch-33 batch-125 = 1.4899764209985733e-05

Training epoch-33 batch-126
Running loss of epoch-33 batch-126 = 9.642913937568665e-06

Training epoch-33 batch-127
Running loss of epoch-33 batch-127 = 1.6397330909967422e-05

Training epoch-33 batch-128
Running loss of epoch-33 batch-128 = 8.864793926477432e-06

Training epoch-33 batch-129
Running loss of epoch-33 batch-129 = 2.9834453016519547e-05

Training epoch-33 batch-130
Running loss of epoch-33 batch-130 = 1.4747376553714275e-05

Training epoch-33 batch-131
Running loss of epoch-33 batch-131 = 6.359070539474487e-06

Training epoch-33 batch-132
Running loss of epoch-33 batch-132 = 2.5038840249180794e-05

Training epoch-33 batch-133
Running loss of epoch-33 batch-133 = 4.909466952085495e-06

Training epoch-33 batch-134
Running loss of epoch-33 batch-134 = 1.0649673640727997e-05

Training epoch-33 batch-135
Running loss of epoch-33 batch-135 = 3.243936225771904e-05

Training epoch-33 batch-136
Running loss of epoch-33 batch-136 = 1.5908852219581604e-05

Training epoch-33 batch-137
Running loss of epoch-33 batch-137 = 1.8165213987231255e-05

Training epoch-33 batch-138
Running loss of epoch-33 batch-138 = 1.7603160813450813e-05

Training epoch-33 batch-139
Running loss of epoch-33 batch-139 = 9.09389927983284e-06

Training epoch-33 batch-140
Running loss of epoch-33 batch-140 = 1.427740789949894e-05

Training epoch-33 batch-141
Running loss of epoch-33 batch-141 = 1.4164485037326813e-05

Training epoch-33 batch-142
Running loss of epoch-33 batch-142 = 1.9826926290988922e-05

Training epoch-33 batch-143
Running loss of epoch-33 batch-143 = 7.753144018352032e-05

Training epoch-33 batch-144
Running loss of epoch-33 batch-144 = 1.7987098544836044e-05

Training epoch-33 batch-145
Running loss of epoch-33 batch-145 = 5.14625571668148e-06

Training epoch-33 batch-146
Running loss of epoch-33 batch-146 = 1.1197756975889206e-05

Training epoch-33 batch-147
Running loss of epoch-33 batch-147 = 1.0205432772636414e-05

Training epoch-33 batch-148
Running loss of epoch-33 batch-148 = 9.630457498133183e-06

Training epoch-33 batch-149
Running loss of epoch-33 batch-149 = 1.8136808648705482e-05

Training epoch-33 batch-150
Running loss of epoch-33 batch-150 = 2.7421978302299976e-05

Training epoch-33 batch-151
Running loss of epoch-33 batch-151 = 1.2913253158330917e-05

Training epoch-33 batch-152
Running loss of epoch-33 batch-152 = 1.5889760106801987e-05

Training epoch-33 batch-153
Running loss of epoch-33 batch-153 = 2.2058840841054916e-05

Training epoch-33 batch-154
Running loss of epoch-33 batch-154 = 1.695984974503517e-05

Training epoch-33 batch-155
Running loss of epoch-33 batch-155 = 8.10530036687851e-06

Training epoch-33 batch-156
Running loss of epoch-33 batch-156 = 1.2409873306751251e-05

Training epoch-33 batch-157
Running loss of epoch-33 batch-157 = 5.5573880672454834e-05

Finished training epoch-33.



Average train loss at epoch-33 = 1.5567565709352493e-05

Started Evaluation

Average val loss at epoch-33 = 1.0083990847515931

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.05 %
Accuracy for class run is: 61.19 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.59 %
Accuracy for class execute is: 44.98 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 82.94 %

Finished Evaluation



Started training epoch-34


Training epoch-34 batch-1
Running loss of epoch-34 batch-1 = 2.5912420824170113e-05

Training epoch-34 batch-2
Running loss of epoch-34 batch-2 = 1.6570091247558594e-05

Training epoch-34 batch-3
Running loss of epoch-34 batch-3 = 1.3534212484955788e-05

Training epoch-34 batch-4
Running loss of epoch-34 batch-4 = 9.50181856751442e-06

Training epoch-34 batch-5
Running loss of epoch-34 batch-5 = 5.211681127548218e-06

Training epoch-34 batch-6
Running loss of epoch-34 batch-6 = 2.5964342057704926e-05

Training epoch-34 batch-7
Running loss of epoch-34 batch-7 = 1.1298339813947678e-05

Training epoch-34 batch-8
Running loss of epoch-34 batch-8 = 7.19225499778986e-06

Training epoch-34 batch-9
Running loss of epoch-34 batch-9 = 8.42241570353508e-06

Training epoch-34 batch-10
Running loss of epoch-34 batch-10 = 2.0229024812579155e-05

Training epoch-34 batch-11
Running loss of epoch-34 batch-11 = 1.084362156689167e-05

Training epoch-34 batch-12
Running loss of epoch-34 batch-12 = 1.0133488103747368e-05

Training epoch-34 batch-13
Running loss of epoch-34 batch-13 = 1.9990839064121246e-05

Training epoch-34 batch-14
Running loss of epoch-34 batch-14 = 1.2689386494457722e-05

Training epoch-34 batch-15
Running loss of epoch-34 batch-15 = 6.3658226281404495e-06

Training epoch-34 batch-16
Running loss of epoch-34 batch-16 = 1.4862976968288422e-05

Training epoch-34 batch-17
Running loss of epoch-34 batch-17 = 3.650551661849022e-06

Training epoch-34 batch-18
Running loss of epoch-34 batch-18 = 3.822776488959789e-05

Training epoch-34 batch-19
Running loss of epoch-34 batch-19 = 2.022460103034973e-05

Training epoch-34 batch-20
Running loss of epoch-34 batch-20 = 1.123594120144844e-05

Training epoch-34 batch-21
Running loss of epoch-34 batch-21 = 1.7270678654313087e-05

Training epoch-34 batch-22
Running loss of epoch-34 batch-22 = 2.4634413421154022e-05

Training epoch-34 batch-23
Running loss of epoch-34 batch-23 = 1.7941929399967194e-05

Training epoch-34 batch-24
Running loss of epoch-34 batch-24 = 5.739741027355194e-06

Training epoch-34 batch-25
Running loss of epoch-34 batch-25 = 4.3803825974464417e-05

Training epoch-34 batch-26
Running loss of epoch-34 batch-26 = 2.394896000623703e-06

Training epoch-34 batch-27
Running loss of epoch-34 batch-27 = 3.874592948704958e-05

Training epoch-34 batch-28
Running loss of epoch-34 batch-28 = 1.1372845619916916e-05

Training epoch-34 batch-29
Running loss of epoch-34 batch-29 = 2.24369578063488e-05

Training epoch-34 batch-30
Running loss of epoch-34 batch-30 = 2.2018561139702797e-05

Training epoch-34 batch-31
Running loss of epoch-34 batch-31 = 5.8549921959638596e-06

Training epoch-34 batch-32
Running loss of epoch-34 batch-32 = 4.787696525454521e-06

Training epoch-34 batch-33
Running loss of epoch-34 batch-33 = 1.467694528400898e-05

Training epoch-34 batch-34
Running loss of epoch-34 batch-34 = 9.011011570692062e-06

Training epoch-34 batch-35
Running loss of epoch-34 batch-35 = 7.957685738801956e-06

Training epoch-34 batch-36
Running loss of epoch-34 batch-36 = 8.536968380212784e-06

Training epoch-34 batch-37
Running loss of epoch-34 batch-37 = 1.1858530342578888e-05

Training epoch-34 batch-38
Running loss of epoch-34 batch-38 = 9.199604392051697e-06

Training epoch-34 batch-39
Running loss of epoch-34 batch-39 = 1.221662387251854e-05

Training epoch-34 batch-40
Running loss of epoch-34 batch-40 = 1.095910556614399e-05

Training epoch-34 batch-41
Running loss of epoch-34 batch-41 = 1.777568832039833e-05

Training epoch-34 batch-42
Running loss of epoch-34 batch-42 = 1.8960097804665565e-05

Training epoch-34 batch-43
Running loss of epoch-34 batch-43 = 2.057943493127823e-05

Training epoch-34 batch-44
Running loss of epoch-34 batch-44 = 1.1421740055084229e-05

Training epoch-34 batch-45
Running loss of epoch-34 batch-45 = 6.700633093714714e-06

Training epoch-34 batch-46
Running loss of epoch-34 batch-46 = 1.5502097085118294e-05

Training epoch-34 batch-47
Running loss of epoch-34 batch-47 = 1.0349554941058159e-05

Training epoch-34 batch-48
Running loss of epoch-34 batch-48 = 4.996312782168388e-06

Training epoch-34 batch-49
Running loss of epoch-34 batch-49 = 8.640345185995102e-06

Training epoch-34 batch-50
Running loss of epoch-34 batch-50 = 2.1921820007264614e-05

Training epoch-34 batch-51
Running loss of epoch-34 batch-51 = 1.3574259355664253e-05

Training epoch-34 batch-52
Running loss of epoch-34 batch-52 = 4.798639565706253e-06

Training epoch-34 batch-53
Running loss of epoch-34 batch-53 = 1.003919169306755e-05

Training epoch-34 batch-54
Running loss of epoch-34 batch-54 = 1.9789906218647957e-05

Training epoch-34 batch-55
Running loss of epoch-34 batch-55 = 8.734059520065784e-06

Training epoch-34 batch-56
Running loss of epoch-34 batch-56 = 1.173466444015503e-05

Training epoch-34 batch-57
Running loss of epoch-34 batch-57 = 1.844833604991436e-05

Training epoch-34 batch-58
Running loss of epoch-34 batch-58 = 4.011974669992924e-05

Training epoch-34 batch-59
Running loss of epoch-34 batch-59 = 4.98676672577858e-06

Training epoch-34 batch-60
Running loss of epoch-34 batch-60 = 7.811235263943672e-06

Training epoch-34 batch-61
Running loss of epoch-34 batch-61 = 1.410418190062046e-05

Training epoch-34 batch-62
Running loss of epoch-34 batch-62 = 5.7711731642484665e-06

Training epoch-34 batch-63
Running loss of epoch-34 batch-63 = 3.3772317692637444e-05

Training epoch-34 batch-64
Running loss of epoch-34 batch-64 = 7.414491847157478e-06

Training epoch-34 batch-65
Running loss of epoch-34 batch-65 = 1.055235043168068e-05

Training epoch-34 batch-66
Running loss of epoch-34 batch-66 = 1.174979843199253e-05

Training epoch-34 batch-67
Running loss of epoch-34 batch-67 = 4.938570782542229e-06

Training epoch-34 batch-68
Running loss of epoch-34 batch-68 = 2.227630466222763e-05

Training epoch-34 batch-69
Running loss of epoch-34 batch-69 = 9.73767600953579e-06

Training epoch-34 batch-70
Running loss of epoch-34 batch-70 = 9.82615165412426e-06

Training epoch-34 batch-71
Running loss of epoch-34 batch-71 = 4.3532345443964005e-06

Training epoch-34 batch-72
Running loss of epoch-34 batch-72 = 1.6235513612627983e-05

Training epoch-34 batch-73
Running loss of epoch-34 batch-73 = 9.888317435979843e-06

Training epoch-34 batch-74
Running loss of epoch-34 batch-74 = 2.6641180738806725e-05

Training epoch-34 batch-75
Running loss of epoch-34 batch-75 = 2.375105395913124e-05

Training epoch-34 batch-76
Running loss of epoch-34 batch-76 = 9.127426892518997e-06

Training epoch-34 batch-77
Running loss of epoch-34 batch-77 = 1.1609401553869247e-05

Training epoch-34 batch-78
Running loss of epoch-34 batch-78 = 1.8724938854575157e-05

Training epoch-34 batch-79
Running loss of epoch-34 batch-79 = 1.7193611711263657e-05

Training epoch-34 batch-80
Running loss of epoch-34 batch-80 = 1.8107471987605095e-05

Training epoch-34 batch-81
Running loss of epoch-34 batch-81 = 1.573888584971428e-05

Training epoch-34 batch-82
Running loss of epoch-34 batch-82 = 2.306443639099598e-05

Training epoch-34 batch-83
Running loss of epoch-34 batch-83 = 8.412869647145271e-06

Training epoch-34 batch-84
Running loss of epoch-34 batch-84 = 1.1002179235219955e-05

Training epoch-34 batch-85
Running loss of epoch-34 batch-85 = 1.626717858016491e-05

Training epoch-34 batch-86
Running loss of epoch-34 batch-86 = 1.5002908185124397e-05

Training epoch-34 batch-87
Running loss of epoch-34 batch-87 = 2.288899850100279e-05

Training epoch-34 batch-88
Running loss of epoch-34 batch-88 = 1.0709278285503387e-05

Training epoch-34 batch-89
Running loss of epoch-34 batch-89 = 2.6754802092909813e-05

Training epoch-34 batch-90
Running loss of epoch-34 batch-90 = 1.609092578291893e-05

Training epoch-34 batch-91
Running loss of epoch-34 batch-91 = 1.027202233672142e-05

Training epoch-34 batch-92
Running loss of epoch-34 batch-92 = 8.965958841145039e-06

Training epoch-34 batch-93
Running loss of epoch-34 batch-93 = 9.662238880991936e-06

Training epoch-34 batch-94
Running loss of epoch-34 batch-94 = 8.448027074337006e-06

Training epoch-34 batch-95
Running loss of epoch-34 batch-95 = 1.0810792446136475e-05

Training epoch-34 batch-96
Running loss of epoch-34 batch-96 = 5.0067901611328125e-06

Training epoch-34 batch-97
Running loss of epoch-34 batch-97 = 1.9587576389312744e-05

Training epoch-34 batch-98
Running loss of epoch-34 batch-98 = 9.199138730764389e-06

Training epoch-34 batch-99
Running loss of epoch-34 batch-99 = 2.276059240102768e-05

Training epoch-34 batch-100
Running loss of epoch-34 batch-100 = 1.2679374776780605e-05

Training epoch-34 batch-101
Running loss of epoch-34 batch-101 = 2.9406044632196426e-05

Training epoch-34 batch-102
Running loss of epoch-34 batch-102 = 1.73856969922781e-05

Training epoch-34 batch-103
Running loss of epoch-34 batch-103 = 2.213194966316223e-05

Training epoch-34 batch-104
Running loss of epoch-34 batch-104 = 1.4682067558169365e-05

Training epoch-34 batch-105
Running loss of epoch-34 batch-105 = 2.134614624083042e-05

Training epoch-34 batch-106
Running loss of epoch-34 batch-106 = 1.540116500109434e-05

Training epoch-34 batch-107
Running loss of epoch-34 batch-107 = 1.2682750821113586e-05

Training epoch-34 batch-108
Running loss of epoch-34 batch-108 = 1.4666235074400902e-05

Training epoch-34 batch-109
Running loss of epoch-34 batch-109 = 1.5156110748648643e-05

Training epoch-34 batch-110
Running loss of epoch-34 batch-110 = 6.1050523072481155e-06

Training epoch-34 batch-111
Running loss of epoch-34 batch-111 = 1.0181218385696411e-05

Training epoch-34 batch-112
Running loss of epoch-34 batch-112 = 9.966548532247543e-06

Training epoch-34 batch-113
Running loss of epoch-34 batch-113 = 2.8463546186685562e-05

Training epoch-34 batch-114
Running loss of epoch-34 batch-114 = 1.765531487762928e-05

Training epoch-34 batch-115
Running loss of epoch-34 batch-115 = 9.467359632253647e-06

Training epoch-34 batch-116
Running loss of epoch-34 batch-116 = 5.865469574928284e-06

Training epoch-34 batch-117
Running loss of epoch-34 batch-117 = 2.0183390006422997e-05

Training epoch-34 batch-118
Running loss of epoch-34 batch-118 = 1.1574709787964821e-05

Training epoch-34 batch-119
Running loss of epoch-34 batch-119 = 1.1999392881989479e-05

Training epoch-34 batch-120
Running loss of epoch-34 batch-120 = 1.90704595297575e-05

Training epoch-34 batch-121
Running loss of epoch-34 batch-121 = 1.25812366604805e-05

Training epoch-34 batch-122
Running loss of epoch-34 batch-122 = 1.0715564712882042e-05

Training epoch-34 batch-123
Running loss of epoch-34 batch-123 = 8.541624993085861e-06

Training epoch-34 batch-124
Running loss of epoch-34 batch-124 = 1.1102762073278427e-05

Training epoch-34 batch-125
Running loss of epoch-34 batch-125 = 1.287786290049553e-05

Training epoch-34 batch-126
Running loss of epoch-34 batch-126 = 9.656883776187897e-06

Training epoch-34 batch-127
Running loss of epoch-34 batch-127 = 7.71298073232174e-06

Training epoch-34 batch-128
Running loss of epoch-34 batch-128 = 1.8279999494552612e-05

Training epoch-34 batch-129
Running loss of epoch-34 batch-129 = 1.0295771062374115e-05

Training epoch-34 batch-130
Running loss of epoch-34 batch-130 = 1.906626857817173e-05

Training epoch-34 batch-131
Running loss of epoch-34 batch-131 = 9.107054211199284e-06

Training epoch-34 batch-132
Running loss of epoch-34 batch-132 = 1.7114216461777687e-05

Training epoch-34 batch-133
Running loss of epoch-34 batch-133 = 3.800727427005768e-06

Training epoch-34 batch-134
Running loss of epoch-34 batch-134 = 4.017958417534828e-06

Training epoch-34 batch-135
Running loss of epoch-34 batch-135 = 1.1502066627144814e-05

Training epoch-34 batch-136
Running loss of epoch-34 batch-136 = 1.8278369680047035e-05

Training epoch-34 batch-137
Running loss of epoch-34 batch-137 = 1.2122327461838722e-05

Training epoch-34 batch-138
Running loss of epoch-34 batch-138 = 1.3470649719238281e-05

Training epoch-34 batch-139
Running loss of epoch-34 batch-139 = 2.5517772883176804e-05

Training epoch-34 batch-140
Running loss of epoch-34 batch-140 = 1.4493009075522423e-05

Training epoch-34 batch-141
Running loss of epoch-34 batch-141 = 7.724156603217125e-06

Training epoch-34 batch-142
Running loss of epoch-34 batch-142 = 1.547904685139656e-05

Training epoch-34 batch-143
Running loss of epoch-34 batch-143 = 8.956529200077057e-06

Training epoch-34 batch-144
Running loss of epoch-34 batch-144 = 1.9916333258152008e-05

Training epoch-34 batch-145
Running loss of epoch-34 batch-145 = 2.6250840164721012e-05

Training epoch-34 batch-146
Running loss of epoch-34 batch-146 = 8.088303729891777e-06

Training epoch-34 batch-147
Running loss of epoch-34 batch-147 = 4.8795249313116074e-05

Training epoch-34 batch-148
Running loss of epoch-34 batch-148 = 1.9561382941901684e-05

Training epoch-34 batch-149
Running loss of epoch-34 batch-149 = 7.547903805971146e-06

Training epoch-34 batch-150
Running loss of epoch-34 batch-150 = 8.051516488194466e-06

Training epoch-34 batch-151
Running loss of epoch-34 batch-151 = 6.054062396287918e-06

Training epoch-34 batch-152
Running loss of epoch-34 batch-152 = 9.921379387378693e-06

Training epoch-34 batch-153
Running loss of epoch-34 batch-153 = 1.676846295595169e-05

Training epoch-34 batch-154
Running loss of epoch-34 batch-154 = 8.394476026296616e-06

Training epoch-34 batch-155
Running loss of epoch-34 batch-155 = 1.8788035959005356e-05

Training epoch-34 batch-156
Running loss of epoch-34 batch-156 = 9.868526831269264e-06

Training epoch-34 batch-157
Running loss of epoch-34 batch-157 = 2.4810433387756348e-06

Finished training epoch-34.



Average train loss at epoch-34 = 1.4360943436622619e-05

Started Evaluation

Average val loss at epoch-34 = 1.003741768378524

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.60 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 63.01 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 70.00 %

Overall Accuracy = 82.98 %

Finished Evaluation



Started training epoch-35


Training epoch-35 batch-1
Running loss of epoch-35 batch-1 = 9.26479697227478e-06

Training epoch-35 batch-2
Running loss of epoch-35 batch-2 = 1.0203104466199875e-05

Training epoch-35 batch-3
Running loss of epoch-35 batch-3 = 6.515299901366234e-06

Training epoch-35 batch-4
Running loss of epoch-35 batch-4 = 9.892275556921959e-06

Training epoch-35 batch-5
Running loss of epoch-35 batch-5 = 9.733717888593674e-06

Training epoch-35 batch-6
Running loss of epoch-35 batch-6 = 3.477046266198158e-05

Training epoch-35 batch-7
Running loss of epoch-35 batch-7 = 1.2960517778992653e-05

Training epoch-35 batch-8
Running loss of epoch-35 batch-8 = 1.9572791643440723e-05

Training epoch-35 batch-9
Running loss of epoch-35 batch-9 = 2.3770378902554512e-05

Training epoch-35 batch-10
Running loss of epoch-35 batch-10 = 1.1281576007604599e-05

Training epoch-35 batch-11
Running loss of epoch-35 batch-11 = 2.763001248240471e-05

Training epoch-35 batch-12
Running loss of epoch-35 batch-12 = 1.0495306923985481e-05

Training epoch-35 batch-13
Running loss of epoch-35 batch-13 = 1.763971522450447e-05

Training epoch-35 batch-14
Running loss of epoch-35 batch-14 = 1.0008923709392548e-05

Training epoch-35 batch-15
Running loss of epoch-35 batch-15 = 1.5464378520846367e-05

Training epoch-35 batch-16
Running loss of epoch-35 batch-16 = 1.4665769413113594e-05

Training epoch-35 batch-17
Running loss of epoch-35 batch-17 = 1.257285475730896e-05

Training epoch-35 batch-18
Running loss of epoch-35 batch-18 = 1.7633428797125816e-05

Training epoch-35 batch-19
Running loss of epoch-35 batch-19 = 2.1584564819931984e-05

Training epoch-35 batch-20
Running loss of epoch-35 batch-20 = 1.8516555428504944e-05

Training epoch-35 batch-21
Running loss of epoch-35 batch-21 = 5.770474672317505e-06

Training epoch-35 batch-22
Running loss of epoch-35 batch-22 = 6.2228646129369736e-06

Training epoch-35 batch-23
Running loss of epoch-35 batch-23 = 1.2323493137955666e-05

Training epoch-35 batch-24
Running loss of epoch-35 batch-24 = 1.3306038454174995e-05

Training epoch-35 batch-25
Running loss of epoch-35 batch-25 = 2.0246952772140503e-05

Training epoch-35 batch-26
Running loss of epoch-35 batch-26 = 1.1123716831207275e-05

Training epoch-35 batch-27
Running loss of epoch-35 batch-27 = 8.683884516358376e-06

Training epoch-35 batch-28
Running loss of epoch-35 batch-28 = 1.088273711502552e-05

Training epoch-35 batch-29
Running loss of epoch-35 batch-29 = 2.311798743903637e-05

Training epoch-35 batch-30
Running loss of epoch-35 batch-30 = 1.636054366827011e-05

Training epoch-35 batch-31
Running loss of epoch-35 batch-31 = 1.2560747563838959e-05

Training epoch-35 batch-32
Running loss of epoch-35 batch-32 = 1.0107411071658134e-05

Training epoch-35 batch-33
Running loss of epoch-35 batch-33 = 1.2988923117518425e-05

Training epoch-35 batch-34
Running loss of epoch-35 batch-34 = 1.2571457773447037e-05

Training epoch-35 batch-35
Running loss of epoch-35 batch-35 = 1.287856139242649e-05

Training epoch-35 batch-36
Running loss of epoch-35 batch-36 = 6.51658046990633e-06

Training epoch-35 batch-37
Running loss of epoch-35 batch-37 = 5.052657797932625e-06

Training epoch-35 batch-38
Running loss of epoch-35 batch-38 = 1.4294404536485672e-05

Training epoch-35 batch-39
Running loss of epoch-35 batch-39 = 1.2409640476107597e-05

Training epoch-35 batch-40
Running loss of epoch-35 batch-40 = 1.6715610399842262e-05

Training epoch-35 batch-41
Running loss of epoch-35 batch-41 = 2.2836262360215187e-05

Training epoch-35 batch-42
Running loss of epoch-35 batch-42 = 8.167233318090439e-06

Training epoch-35 batch-43
Running loss of epoch-35 batch-43 = 9.582610800862312e-06

Training epoch-35 batch-44
Running loss of epoch-35 batch-44 = 8.828938007354736e-06

Training epoch-35 batch-45
Running loss of epoch-35 batch-45 = 1.2805219739675522e-05

Training epoch-35 batch-46
Running loss of epoch-35 batch-46 = 8.680857717990875e-06

Training epoch-35 batch-47
Running loss of epoch-35 batch-47 = 2.464628778398037e-05

Training epoch-35 batch-48
Running loss of epoch-35 batch-48 = 8.078757673501968e-06

Training epoch-35 batch-49
Running loss of epoch-35 batch-49 = 1.736101694405079e-05

Training epoch-35 batch-50
Running loss of epoch-35 batch-50 = 1.401267945766449e-05

Training epoch-35 batch-51
Running loss of epoch-35 batch-51 = 1.2205448001623154e-05

Training epoch-35 batch-52
Running loss of epoch-35 batch-52 = 1.5844125300645828e-05

Training epoch-35 batch-53
Running loss of epoch-35 batch-53 = 1.7549609765410423e-05

Training epoch-35 batch-54
Running loss of epoch-35 batch-54 = 1.359335146844387e-05

Training epoch-35 batch-55
Running loss of epoch-35 batch-55 = 1.2360513210296631e-05

Training epoch-35 batch-56
Running loss of epoch-35 batch-56 = 1.987791620194912e-05

Training epoch-35 batch-57
Running loss of epoch-35 batch-57 = 1.7988495528697968e-06

Training epoch-35 batch-58
Running loss of epoch-35 batch-58 = 1.6244826838374138e-05

Training epoch-35 batch-59
Running loss of epoch-35 batch-59 = 1.6415957361459732e-05

Training epoch-35 batch-60
Running loss of epoch-35 batch-60 = 4.6473927795886993e-05

Training epoch-35 batch-61
Running loss of epoch-35 batch-61 = 1.5802448615431786e-05

Training epoch-35 batch-62
Running loss of epoch-35 batch-62 = 1.1463183909654617e-05

Training epoch-35 batch-63
Running loss of epoch-35 batch-63 = 3.015110269188881e-05

Training epoch-35 batch-64
Running loss of epoch-35 batch-64 = 1.025339588522911e-05

Training epoch-35 batch-65
Running loss of epoch-35 batch-65 = 7.401453331112862e-06

Training epoch-35 batch-66
Running loss of epoch-35 batch-66 = 4.455447196960449e-06

Training epoch-35 batch-67
Running loss of epoch-35 batch-67 = 1.0960735380649567e-05

Training epoch-35 batch-68
Running loss of epoch-35 batch-68 = 3.7779100239276886e-06

Training epoch-35 batch-69
Running loss of epoch-35 batch-69 = 1.3409648090600967e-05

Training epoch-35 batch-70
Running loss of epoch-35 batch-70 = 2.0576873794198036e-05

Training epoch-35 batch-71
Running loss of epoch-35 batch-71 = 1.0304385796189308e-05

Training epoch-35 batch-72
Running loss of epoch-35 batch-72 = 1.588510349392891e-05

Training epoch-35 batch-73
Running loss of epoch-35 batch-73 = 1.3234559446573257e-05

Training epoch-35 batch-74
Running loss of epoch-35 batch-74 = 8.190050721168518e-06

Training epoch-35 batch-75
Running loss of epoch-35 batch-75 = 1.7371028661727905e-05

Training epoch-35 batch-76
Running loss of epoch-35 batch-76 = 1.3730721548199654e-05

Training epoch-35 batch-77
Running loss of epoch-35 batch-77 = 1.8285703845322132e-05

Training epoch-35 batch-78
Running loss of epoch-35 batch-78 = 1.7506303265690804e-05

Training epoch-35 batch-79
Running loss of epoch-35 batch-79 = 6.5409112721681595e-06

Training epoch-35 batch-80
Running loss of epoch-35 batch-80 = 1.2245262041687965e-05

Training epoch-35 batch-81
Running loss of epoch-35 batch-81 = 1.2029195204377174e-05

Training epoch-35 batch-82
Running loss of epoch-35 batch-82 = 2.432079054415226e-05

Training epoch-35 batch-83
Running loss of epoch-35 batch-83 = 1.1353986337780952e-05

Training epoch-35 batch-84
Running loss of epoch-35 batch-84 = 1.5639234334230423e-05

Training epoch-35 batch-85
Running loss of epoch-35 batch-85 = 6.13208394497633e-05

Training epoch-35 batch-86
Running loss of epoch-35 batch-86 = 1.6187899746000767e-05

Training epoch-35 batch-87
Running loss of epoch-35 batch-87 = 1.3249111361801624e-05

Training epoch-35 batch-88
Running loss of epoch-35 batch-88 = 5.35743311047554e-06

Training epoch-35 batch-89
Running loss of epoch-35 batch-89 = 1.5889876522123814e-05

Training epoch-35 batch-90
Running loss of epoch-35 batch-90 = 7.772818207740784e-06

Training epoch-35 batch-91
Running loss of epoch-35 batch-91 = 1.168833114206791e-05

Training epoch-35 batch-92
Running loss of epoch-35 batch-92 = 1.2298114597797394e-05

Training epoch-35 batch-93
Running loss of epoch-35 batch-93 = 1.4637713320553303e-05

Training epoch-35 batch-94
Running loss of epoch-35 batch-94 = 7.402384653687477e-06

Training epoch-35 batch-95
Running loss of epoch-35 batch-95 = 2.3960601538419724e-05

Training epoch-35 batch-96
Running loss of epoch-35 batch-96 = 4.485715180635452e-06

Training epoch-35 batch-97
Running loss of epoch-35 batch-97 = 8.252449333667755e-06

Training epoch-35 batch-98
Running loss of epoch-35 batch-98 = 8.04639421403408e-06

Training epoch-35 batch-99
Running loss of epoch-35 batch-99 = 6.070826202630997e-06

Training epoch-35 batch-100
Running loss of epoch-35 batch-100 = 1.4485092833638191e-05

Training epoch-35 batch-101
Running loss of epoch-35 batch-101 = 1.301988959312439e-05

Training epoch-35 batch-102
Running loss of epoch-35 batch-102 = 1.4245393685996532e-05

Training epoch-35 batch-103
Running loss of epoch-35 batch-103 = 1.2761913239955902e-05

Training epoch-35 batch-104
Running loss of epoch-35 batch-104 = 8.663395419716835e-06

Training epoch-35 batch-105
Running loss of epoch-35 batch-105 = 6.890157237648964e-06

Training epoch-35 batch-106
Running loss of epoch-35 batch-106 = 4.5923516154289246e-06

Training epoch-35 batch-107
Running loss of epoch-35 batch-107 = 1.2468080967664719e-05

Training epoch-35 batch-108
Running loss of epoch-35 batch-108 = 7.463386282324791e-06

Training epoch-35 batch-109
Running loss of epoch-35 batch-109 = 3.441469743847847e-06

Training epoch-35 batch-110
Running loss of epoch-35 batch-110 = 3.6333221942186356e-06

Training epoch-35 batch-111
Running loss of epoch-35 batch-111 = 1.0232441127300262e-05

Training epoch-35 batch-112
Running loss of epoch-35 batch-112 = 9.3081034719944e-06

Training epoch-35 batch-113
Running loss of epoch-35 batch-113 = 1.0888557881116867e-05

Training epoch-35 batch-114
Running loss of epoch-35 batch-114 = 2.270366530865431e-05

Training epoch-35 batch-115
Running loss of epoch-35 batch-115 = 8.35862010717392e-06

Training epoch-35 batch-116
Running loss of epoch-35 batch-116 = 8.653150871396065e-06

Training epoch-35 batch-117
Running loss of epoch-35 batch-117 = 4.2496249079704285e-06

Training epoch-35 batch-118
Running loss of epoch-35 batch-118 = 7.31111504137516e-06

Training epoch-35 batch-119
Running loss of epoch-35 batch-119 = 2.0625884644687176e-05

Training epoch-35 batch-120
Running loss of epoch-35 batch-120 = 5.8514997363090515e-06

Training epoch-35 batch-121
Running loss of epoch-35 batch-121 = 6.516464054584503e-06

Training epoch-35 batch-122
Running loss of epoch-35 batch-122 = 5.229376256465912e-06

Training epoch-35 batch-123
Running loss of epoch-35 batch-123 = 8.539529517292976e-06

Training epoch-35 batch-124
Running loss of epoch-35 batch-124 = 1.3624085113406181e-05

Training epoch-35 batch-125
Running loss of epoch-35 batch-125 = 1.1963769793510437e-05

Training epoch-35 batch-126
Running loss of epoch-35 batch-126 = 8.995644748210907e-06

Training epoch-35 batch-127
Running loss of epoch-35 batch-127 = 2.6294146664440632e-05

Training epoch-35 batch-128
Running loss of epoch-35 batch-128 = 2.989312633872032e-06

Training epoch-35 batch-129
Running loss of epoch-35 batch-129 = 1.8371036276221275e-05

Training epoch-35 batch-130
Running loss of epoch-35 batch-130 = 2.9341084882616997e-05

Training epoch-35 batch-131
Running loss of epoch-35 batch-131 = 3.0511291697621346e-05

Training epoch-35 batch-132
Running loss of epoch-35 batch-132 = 1.0431278496980667e-05

Training epoch-35 batch-133
Running loss of epoch-35 batch-133 = 9.444775059819221e-06

Training epoch-35 batch-134
Running loss of epoch-35 batch-134 = 2.6129651814699173e-05

Training epoch-35 batch-135
Running loss of epoch-35 batch-135 = 7.75395892560482e-06

Training epoch-35 batch-136
Running loss of epoch-35 batch-136 = 1.442059874534607e-05

Training epoch-35 batch-137
Running loss of epoch-35 batch-137 = 1.1438969522714615e-05

Training epoch-35 batch-138
Running loss of epoch-35 batch-138 = 1.5213387086987495e-05

Training epoch-35 batch-139
Running loss of epoch-35 batch-139 = 7.82310962677002e-06

Training epoch-35 batch-140
Running loss of epoch-35 batch-140 = 3.965524956583977e-05

Training epoch-35 batch-141
Running loss of epoch-35 batch-141 = 1.6866950318217278e-05

Training epoch-35 batch-142
Running loss of epoch-35 batch-142 = 1.6038771718740463e-05

Training epoch-35 batch-143
Running loss of epoch-35 batch-143 = 1.0953051969408989e-05

Training epoch-35 batch-144
Running loss of epoch-35 batch-144 = 4.639849066734314e-06

Training epoch-35 batch-145
Running loss of epoch-35 batch-145 = 1.2188917025923729e-05

Training epoch-35 batch-146
Running loss of epoch-35 batch-146 = 1.0799150913953781e-05

Training epoch-35 batch-147
Running loss of epoch-35 batch-147 = 2.3090746253728867e-05

Training epoch-35 batch-148
Running loss of epoch-35 batch-148 = 1.8274644389748573e-05

Training epoch-35 batch-149
Running loss of epoch-35 batch-149 = 1.2141885235905647e-05

Training epoch-35 batch-150
Running loss of epoch-35 batch-150 = 2.2733118385076523e-05

Training epoch-35 batch-151
Running loss of epoch-35 batch-151 = 4.873145371675491e-06

Training epoch-35 batch-152
Running loss of epoch-35 batch-152 = 9.08365473151207e-06

Training epoch-35 batch-153
Running loss of epoch-35 batch-153 = 1.7626676708459854e-05

Training epoch-35 batch-154
Running loss of epoch-35 batch-154 = 9.376322850584984e-06

Training epoch-35 batch-155
Running loss of epoch-35 batch-155 = 7.648253813385963e-06

Training epoch-35 batch-156
Running loss of epoch-35 batch-156 = 1.828698441386223e-05

Training epoch-35 batch-157
Running loss of epoch-35 batch-157 = 2.30446457862854e-05

Finished training epoch-35.



Average train loss at epoch-35 = 1.3709456473588944e-05

Started Evaluation

Average val loss at epoch-35 = 1.0386332075844693

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.34 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 61.42 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 50.90 %
Accuracy for class execute is: 46.99 %
Accuracy for class get is: 72.82 %

Overall Accuracy = 82.90 %

Finished Evaluation



Started training epoch-36


Training epoch-36 batch-1
Running loss of epoch-36 batch-1 = 8.458271622657776e-06

Training epoch-36 batch-2
Running loss of epoch-36 batch-2 = 2.3788074031472206e-05

Training epoch-36 batch-3
Running loss of epoch-36 batch-3 = 7.543480023741722e-06

Training epoch-36 batch-4
Running loss of epoch-36 batch-4 = 1.3450859114527702e-05

Training epoch-36 batch-5
Running loss of epoch-36 batch-5 = 1.3185548596084118e-05

Training epoch-36 batch-6
Running loss of epoch-36 batch-6 = 1.5141093172132969e-05

Training epoch-36 batch-7
Running loss of epoch-36 batch-7 = 5.081645213067532e-06

Training epoch-36 batch-8
Running loss of epoch-36 batch-8 = 3.172201104462147e-05

Training epoch-36 batch-9
Running loss of epoch-36 batch-9 = 8.703907951712608e-06

Training epoch-36 batch-10
Running loss of epoch-36 batch-10 = 7.439753971993923e-06

Training epoch-36 batch-11
Running loss of epoch-36 batch-11 = 4.026596434414387e-05

Training epoch-36 batch-12
Running loss of epoch-36 batch-12 = 1.972564496099949e-05

Training epoch-36 batch-13
Running loss of epoch-36 batch-13 = 3.918074071407318e-06

Training epoch-36 batch-14
Running loss of epoch-36 batch-14 = 1.5100231394171715e-05

Training epoch-36 batch-15
Running loss of epoch-36 batch-15 = 2.3979227989912033e-06

Training epoch-36 batch-16
Running loss of epoch-36 batch-16 = 9.485287591814995e-06

Training epoch-36 batch-17
Running loss of epoch-36 batch-17 = 1.7224811017513275e-05

Training epoch-36 batch-18
Running loss of epoch-36 batch-18 = 1.4272984117269516e-05

Training epoch-36 batch-19
Running loss of epoch-36 batch-19 = 2.1071871742606163e-05

Training epoch-36 batch-20
Running loss of epoch-36 batch-20 = 9.151874110102654e-06

Training epoch-36 batch-21
Running loss of epoch-36 batch-21 = 5.752313882112503e-06

Training epoch-36 batch-22
Running loss of epoch-36 batch-22 = 1.0687857866287231e-05

Training epoch-36 batch-23
Running loss of epoch-36 batch-23 = 2.2548716515302658e-05

Training epoch-36 batch-24
Running loss of epoch-36 batch-24 = 8.944189175963402e-06

Training epoch-36 batch-25
Running loss of epoch-36 batch-25 = 1.5699537470936775e-05

Training epoch-36 batch-26
Running loss of epoch-36 batch-26 = 9.047333151102066e-06

Training epoch-36 batch-27
Running loss of epoch-36 batch-27 = 2.9948889277875423e-05

Training epoch-36 batch-28
Running loss of epoch-36 batch-28 = 8.404720574617386e-06

Training epoch-36 batch-29
Running loss of epoch-36 batch-29 = 1.5074852854013443e-05

Training epoch-36 batch-30
Running loss of epoch-36 batch-30 = 7.3802657425403595e-06

Training epoch-36 batch-31
Running loss of epoch-36 batch-31 = 5.452195182442665e-06

Training epoch-36 batch-32
Running loss of epoch-36 batch-32 = 4.799105226993561e-06

Training epoch-36 batch-33
Running loss of epoch-36 batch-33 = 1.1422671377658844e-05

Training epoch-36 batch-34
Running loss of epoch-36 batch-34 = 1.0648742318153381e-05

Training epoch-36 batch-35
Running loss of epoch-36 batch-35 = 2.1232524886727333e-05

Training epoch-36 batch-36
Running loss of epoch-36 batch-36 = 1.4514895156025887e-05

Training epoch-36 batch-37
Running loss of epoch-36 batch-37 = 2.1221116185188293e-05

Training epoch-36 batch-38
Running loss of epoch-36 batch-38 = 1.8785474821925163e-05

Training epoch-36 batch-39
Running loss of epoch-36 batch-39 = 1.589721068739891e-05

Training epoch-36 batch-40
Running loss of epoch-36 batch-40 = 1.5133293345570564e-05

Training epoch-36 batch-41
Running loss of epoch-36 batch-41 = 1.5602214261889458e-05

Training epoch-36 batch-42
Running loss of epoch-36 batch-42 = 4.682457074522972e-06

Training epoch-36 batch-43
Running loss of epoch-36 batch-43 = 7.801689207553864e-06

Training epoch-36 batch-44
Running loss of epoch-36 batch-44 = 1.3195909559726715e-05

Training epoch-36 batch-45
Running loss of epoch-36 batch-45 = 1.5990808606147766e-05

Training epoch-36 batch-46
Running loss of epoch-36 batch-46 = 1.7522135749459267e-05

Training epoch-36 batch-47
Running loss of epoch-36 batch-47 = 8.221948519349098e-06

Training epoch-36 batch-48
Running loss of epoch-36 batch-48 = 9.88692045211792e-06

Training epoch-36 batch-49
Running loss of epoch-36 batch-49 = 7.683411240577698e-06

Training epoch-36 batch-50
Running loss of epoch-36 batch-50 = 2.2673048079013824e-05

Training epoch-36 batch-51
Running loss of epoch-36 batch-51 = 1.2056902050971985e-05

Training epoch-36 batch-52
Running loss of epoch-36 batch-52 = 1.5598954632878304e-05

Training epoch-36 batch-53
Running loss of epoch-36 batch-53 = 8.777249604463577e-06

Training epoch-36 batch-54
Running loss of epoch-36 batch-54 = 4.607159644365311e-05

Training epoch-36 batch-55
Running loss of epoch-36 batch-55 = 1.1249212548136711e-05

Training epoch-36 batch-56
Running loss of epoch-36 batch-56 = 1.3298937119543552e-05

Training epoch-36 batch-57
Running loss of epoch-36 batch-57 = 9.457580745220184e-06

Training epoch-36 batch-58
Running loss of epoch-36 batch-58 = 8.198898285627365e-06

Training epoch-36 batch-59
Running loss of epoch-36 batch-59 = 7.409835234284401e-06

Training epoch-36 batch-60
Running loss of epoch-36 batch-60 = 5.7220458984375e-06

Training epoch-36 batch-61
Running loss of epoch-36 batch-61 = 1.4174729585647583e-05

Training epoch-36 batch-62
Running loss of epoch-36 batch-62 = 1.9673025235533714e-05

Training epoch-36 batch-63
Running loss of epoch-36 batch-63 = 1.7933547496795654e-05

Training epoch-36 batch-64
Running loss of epoch-36 batch-64 = 6.192130967974663e-06

Training epoch-36 batch-65
Running loss of epoch-36 batch-65 = 1.2066448107361794e-05

Training epoch-36 batch-66
Running loss of epoch-36 batch-66 = 9.420793503522873e-06

Training epoch-36 batch-67
Running loss of epoch-36 batch-67 = 7.4570998549461365e-06

Training epoch-36 batch-68
Running loss of epoch-36 batch-68 = 2.1785730496048927e-05

Training epoch-36 batch-69
Running loss of epoch-36 batch-69 = 1.4468329027295113e-05

Training epoch-36 batch-70
Running loss of epoch-36 batch-70 = 9.979819878935814e-06

Training epoch-36 batch-71
Running loss of epoch-36 batch-71 = 6.707850843667984e-06

Training epoch-36 batch-72
Running loss of epoch-36 batch-72 = 1.3188226148486137e-05

Training epoch-36 batch-73
Running loss of epoch-36 batch-73 = 4.349276423454285e-06

Training epoch-36 batch-74
Running loss of epoch-36 batch-74 = 4.650559276342392e-06

Training epoch-36 batch-75
Running loss of epoch-36 batch-75 = 7.3427800089120865e-06

Training epoch-36 batch-76
Running loss of epoch-36 batch-76 = 9.261304512619972e-06

Training epoch-36 batch-77
Running loss of epoch-36 batch-77 = 1.2972275726497173e-05

Training epoch-36 batch-78
Running loss of epoch-36 batch-78 = 9.685871191322803e-06

Training epoch-36 batch-79
Running loss of epoch-36 batch-79 = 8.726026862859726e-06

Training epoch-36 batch-80
Running loss of epoch-36 batch-80 = 2.0173145458102226e-05

Training epoch-36 batch-81
Running loss of epoch-36 batch-81 = 1.0784133337438107e-05

Training epoch-36 batch-82
Running loss of epoch-36 batch-82 = 5.273264832794666e-06

Training epoch-36 batch-83
Running loss of epoch-36 batch-83 = 7.992144674062729e-06

Training epoch-36 batch-84
Running loss of epoch-36 batch-84 = 4.380941390991211e-06

Training epoch-36 batch-85
Running loss of epoch-36 batch-85 = 1.1703930795192719e-05

Training epoch-36 batch-86
Running loss of epoch-36 batch-86 = 9.405426681041718e-06

Training epoch-36 batch-87
Running loss of epoch-36 batch-87 = 8.325092494487762e-06

Training epoch-36 batch-88
Running loss of epoch-36 batch-88 = 9.126029908657074e-06

Training epoch-36 batch-89
Running loss of epoch-36 batch-89 = 1.0307878255844116e-05

Training epoch-36 batch-90
Running loss of epoch-36 batch-90 = 2.5866087526082993e-05

Training epoch-36 batch-91
Running loss of epoch-36 batch-91 = 6.773741915822029e-06

Training epoch-36 batch-92
Running loss of epoch-36 batch-92 = 1.8573831766843796e-05

Training epoch-36 batch-93
Running loss of epoch-36 batch-93 = 2.2463267669081688e-05

Training epoch-36 batch-94
Running loss of epoch-36 batch-94 = 6.499234586954117e-06

Training epoch-36 batch-95
Running loss of epoch-36 batch-95 = 1.2874137610197067e-05

Training epoch-36 batch-96
Running loss of epoch-36 batch-96 = 2.523651346564293e-06

Training epoch-36 batch-97
Running loss of epoch-36 batch-97 = 1.361314207315445e-05

Training epoch-36 batch-98
Running loss of epoch-36 batch-98 = 2.147338818758726e-05

Training epoch-36 batch-99
Running loss of epoch-36 batch-99 = 2.839183434844017e-05

Training epoch-36 batch-100
Running loss of epoch-36 batch-100 = 1.6698148101568222e-05

Training epoch-36 batch-101
Running loss of epoch-36 batch-101 = 1.2890319339931011e-05

Training epoch-36 batch-102
Running loss of epoch-36 batch-102 = 1.2241536751389503e-05

Training epoch-36 batch-103
Running loss of epoch-36 batch-103 = 5.75813464820385e-06

Training epoch-36 batch-104
Running loss of epoch-36 batch-104 = 1.8583377823233604e-05

Training epoch-36 batch-105
Running loss of epoch-36 batch-105 = 1.0510440915822983e-05

Training epoch-36 batch-106
Running loss of epoch-36 batch-106 = 1.001113560050726e-05

Training epoch-36 batch-107
Running loss of epoch-36 batch-107 = 2.9395567253232002e-05

Training epoch-36 batch-108
Running loss of epoch-36 batch-108 = 5.686655640602112e-06

Training epoch-36 batch-109
Running loss of epoch-36 batch-109 = 1.1388212442398071e-05

Training epoch-36 batch-110
Running loss of epoch-36 batch-110 = 1.826114021241665e-05

Training epoch-36 batch-111
Running loss of epoch-36 batch-111 = 1.3035489246249199e-05

Training epoch-36 batch-112
Running loss of epoch-36 batch-112 = 1.3586715795099735e-05

Training epoch-36 batch-113
Running loss of epoch-36 batch-113 = 9.228009730577469e-06

Training epoch-36 batch-114
Running loss of epoch-36 batch-114 = 1.0099029168486595e-05

Training epoch-36 batch-115
Running loss of epoch-36 batch-115 = 2.6401132345199585e-05

Training epoch-36 batch-116
Running loss of epoch-36 batch-116 = 7.112976163625717e-06

Training epoch-36 batch-117
Running loss of epoch-36 batch-117 = 8.426373824477196e-06

Training epoch-36 batch-118
Running loss of epoch-36 batch-118 = 9.149778634309769e-06

Training epoch-36 batch-119
Running loss of epoch-36 batch-119 = 6.782938726246357e-06

Training epoch-36 batch-120
Running loss of epoch-36 batch-120 = 5.667097866535187e-06

Training epoch-36 batch-121
Running loss of epoch-36 batch-121 = 1.567695289850235e-05

Training epoch-36 batch-122
Running loss of epoch-36 batch-122 = 2.0636478438973427e-05

Training epoch-36 batch-123
Running loss of epoch-36 batch-123 = 1.2625474482774734e-05

Training epoch-36 batch-124
Running loss of epoch-36 batch-124 = 7.540336810052395e-06

Training epoch-36 batch-125
Running loss of epoch-36 batch-125 = 8.662696927785873e-06

Training epoch-36 batch-126
Running loss of epoch-36 batch-126 = 2.2348016500473022e-05

Training epoch-36 batch-127
Running loss of epoch-36 batch-127 = 6.6284555941820145e-06

Training epoch-36 batch-128
Running loss of epoch-36 batch-128 = 2.44856346398592e-05

Training epoch-36 batch-129
Running loss of epoch-36 batch-129 = 9.025679901242256e-06

Training epoch-36 batch-130
Running loss of epoch-36 batch-130 = 1.6202684491872787e-06

Training epoch-36 batch-131
Running loss of epoch-36 batch-131 = 6.564892828464508e-06

Training epoch-36 batch-132
Running loss of epoch-36 batch-132 = 1.4610588550567627e-05

Training epoch-36 batch-133
Running loss of epoch-36 batch-133 = 1.9003869965672493e-05

Training epoch-36 batch-134
Running loss of epoch-36 batch-134 = 8.889124728739262e-06

Training epoch-36 batch-135
Running loss of epoch-36 batch-135 = 2.585328184068203e-05

Training epoch-36 batch-136
Running loss of epoch-36 batch-136 = 4.087807610630989e-06

Training epoch-36 batch-137
Running loss of epoch-36 batch-137 = 1.3431767001748085e-05

Training epoch-36 batch-138
Running loss of epoch-36 batch-138 = 1.6775913536548615e-05

Training epoch-36 batch-139
Running loss of epoch-36 batch-139 = 4.043779335916042e-05

Training epoch-36 batch-140
Running loss of epoch-36 batch-140 = 1.854030415415764e-05

Training epoch-36 batch-141
Running loss of epoch-36 batch-141 = 1.3920245692133904e-05

Training epoch-36 batch-142
Running loss of epoch-36 batch-142 = 1.9685830920934677e-05

Training epoch-36 batch-143
Running loss of epoch-36 batch-143 = 7.408671081066132e-06

Training epoch-36 batch-144
Running loss of epoch-36 batch-144 = 6.401212885975838e-06

Training epoch-36 batch-145
Running loss of epoch-36 batch-145 = 9.228941053152084e-06

Training epoch-36 batch-146
Running loss of epoch-36 batch-146 = 1.3608601875603199e-05

Training epoch-36 batch-147
Running loss of epoch-36 batch-147 = 3.795372322201729e-06

Training epoch-36 batch-148
Running loss of epoch-36 batch-148 = 1.6936566680669785e-05

Training epoch-36 batch-149
Running loss of epoch-36 batch-149 = 8.465605787932873e-06

Training epoch-36 batch-150
Running loss of epoch-36 batch-150 = 1.0195537470281124e-05

Training epoch-36 batch-151
Running loss of epoch-36 batch-151 = 8.799601346254349e-06

Training epoch-36 batch-152
Running loss of epoch-36 batch-152 = 1.1349795386195183e-05

Training epoch-36 batch-153
Running loss of epoch-36 batch-153 = 4.776753485202789e-06

Training epoch-36 batch-154
Running loss of epoch-36 batch-154 = 6.4461492002010345e-06

Training epoch-36 batch-155
Running loss of epoch-36 batch-155 = 7.545109838247299e-06

Training epoch-36 batch-156
Running loss of epoch-36 batch-156 = 7.757451385259628e-06

Training epoch-36 batch-157
Running loss of epoch-36 batch-157 = 1.632794737815857e-05

Finished training epoch-36.



Average train loss at epoch-36 = 1.2839360535144806e-05

Started Evaluation

Average val loss at epoch-36 = 1.0204345319305885

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.16 %
Accuracy for class onCreate is: 94.03 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.91 %
Accuracy for class execute is: 44.18 %
Accuracy for class get is: 70.77 %

Overall Accuracy = 83.10 %

Finished Evaluation



Started training epoch-37


Training epoch-37 batch-1
Running loss of epoch-37 batch-1 = 1.107039861381054e-05

Training epoch-37 batch-2
Running loss of epoch-37 batch-2 = 2.4945009499788284e-05

Training epoch-37 batch-3
Running loss of epoch-37 batch-3 = 5.9925951063632965e-06

Training epoch-37 batch-4
Running loss of epoch-37 batch-4 = 1.5423167496919632e-05

Training epoch-37 batch-5
Running loss of epoch-37 batch-5 = 1.8398044630885124e-05

Training epoch-37 batch-6
Running loss of epoch-37 batch-6 = 7.0356763899326324e-06

Training epoch-37 batch-7
Running loss of epoch-37 batch-7 = 4.754168912768364e-06

Training epoch-37 batch-8
Running loss of epoch-37 batch-8 = 1.760968007147312e-05

Training epoch-37 batch-9
Running loss of epoch-37 batch-9 = 7.976777851581573e-06

Training epoch-37 batch-10
Running loss of epoch-37 batch-10 = 5.709705874323845e-06

Training epoch-37 batch-11
Running loss of epoch-37 batch-11 = 8.909963071346283e-06

Training epoch-37 batch-12
Running loss of epoch-37 batch-12 = 2.1439045667648315e-06

Training epoch-37 batch-13
Running loss of epoch-37 batch-13 = 2.2351741790771484e-06

Training epoch-37 batch-14
Running loss of epoch-37 batch-14 = 1.0319752618670464e-05

Training epoch-37 batch-15
Running loss of epoch-37 batch-15 = 5.4694246500730515e-06

Training epoch-37 batch-16
Running loss of epoch-37 batch-16 = 1.1706491932272911e-05

Training epoch-37 batch-17
Running loss of epoch-37 batch-17 = 1.0766088962554932e-05

Training epoch-37 batch-18
Running loss of epoch-37 batch-18 = 1.4541205018758774e-05

Training epoch-37 batch-19
Running loss of epoch-37 batch-19 = 1.2693926692008972e-05

Training epoch-37 batch-20
Running loss of epoch-37 batch-20 = 1.426867675036192e-05

Training epoch-37 batch-21
Running loss of epoch-37 batch-21 = 1.0186107829213142e-05

Training epoch-37 batch-22
Running loss of epoch-37 batch-22 = 7.424270734190941e-06

Training epoch-37 batch-23
Running loss of epoch-37 batch-23 = 8.533243089914322e-06

Training epoch-37 batch-24
Running loss of epoch-37 batch-24 = 8.059665560722351e-06

Training epoch-37 batch-25
Running loss of epoch-37 batch-25 = 6.996560841798782e-06

Training epoch-37 batch-26
Running loss of epoch-37 batch-26 = 4.670582711696625e-06

Training epoch-37 batch-27
Running loss of epoch-37 batch-27 = 9.638024494051933e-06

Training epoch-37 batch-28
Running loss of epoch-37 batch-28 = 5.006324499845505e-06

Training epoch-37 batch-29
Running loss of epoch-37 batch-29 = 8.066650480031967e-06

Training epoch-37 batch-30
Running loss of epoch-37 batch-30 = 8.683186024427414e-06

Training epoch-37 batch-31
Running loss of epoch-37 batch-31 = 8.917879313230515e-06

Training epoch-37 batch-32
Running loss of epoch-37 batch-32 = 7.239403203129768e-06

Training epoch-37 batch-33
Running loss of epoch-37 batch-33 = 1.2227101251482964e-05

Training epoch-37 batch-34
Running loss of epoch-37 batch-34 = 2.662651240825653e-06

Training epoch-37 batch-35
Running loss of epoch-37 batch-35 = 1.6780104488134384e-05

Training epoch-37 batch-36
Running loss of epoch-37 batch-36 = 1.733098179101944e-05

Training epoch-37 batch-37
Running loss of epoch-37 batch-37 = 5.588401108980179e-06

Training epoch-37 batch-38
Running loss of epoch-37 batch-38 = 8.362578228116035e-06

Training epoch-37 batch-39
Running loss of epoch-37 batch-39 = 1.149158924818039e-05

Training epoch-37 batch-40
Running loss of epoch-37 batch-40 = 6.413087248802185e-06

Training epoch-37 batch-41
Running loss of epoch-37 batch-41 = 1.2102536857128143e-05

Training epoch-37 batch-42
Running loss of epoch-37 batch-42 = 2.5070970878005028e-05

Training epoch-37 batch-43
Running loss of epoch-37 batch-43 = 1.3674143701791763e-05

Training epoch-37 batch-44
Running loss of epoch-37 batch-44 = 1.3044336810708046e-05

Training epoch-37 batch-45
Running loss of epoch-37 batch-45 = 1.5727709978818893e-05

Training epoch-37 batch-46
Running loss of epoch-37 batch-46 = 9.25757922232151e-06

Training epoch-37 batch-47
Running loss of epoch-37 batch-47 = 1.5452271327376366e-05

Training epoch-37 batch-48
Running loss of epoch-37 batch-48 = 1.4079036191105843e-05

Training epoch-37 batch-49
Running loss of epoch-37 batch-49 = 7.549766451120377e-06

Training epoch-37 batch-50
Running loss of epoch-37 batch-50 = 8.02706927061081e-06

Training epoch-37 batch-51
Running loss of epoch-37 batch-51 = 9.878305718302727e-06

Training epoch-37 batch-52
Running loss of epoch-37 batch-52 = 3.256474155932665e-05

Training epoch-37 batch-53
Running loss of epoch-37 batch-53 = 1.6692327335476875e-05

Training epoch-37 batch-54
Running loss of epoch-37 batch-54 = 7.576541975140572e-06

Training epoch-37 batch-55
Running loss of epoch-37 batch-55 = 2.786284312605858e-06

Training epoch-37 batch-56
Running loss of epoch-37 batch-56 = 4.505505785346031e-06

Training epoch-37 batch-57
Running loss of epoch-37 batch-57 = 3.846059553325176e-05

Training epoch-37 batch-58
Running loss of epoch-37 batch-58 = 1.0010553523898125e-05

Training epoch-37 batch-59
Running loss of epoch-37 batch-59 = 1.1089025065302849e-05

Training epoch-37 batch-60
Running loss of epoch-37 batch-60 = 9.712530300021172e-06

Training epoch-37 batch-61
Running loss of epoch-37 batch-61 = 1.948513090610504e-05

Training epoch-37 batch-62
Running loss of epoch-37 batch-62 = 1.363246701657772e-05

Training epoch-37 batch-63
Running loss of epoch-37 batch-63 = 1.1124182492494583e-05

Training epoch-37 batch-64
Running loss of epoch-37 batch-64 = 1.5585217624902725e-05

Training epoch-37 batch-65
Running loss of epoch-37 batch-65 = 9.508104994893074e-06

Training epoch-37 batch-66
Running loss of epoch-37 batch-66 = 1.1462019756436348e-05

Training epoch-37 batch-67
Running loss of epoch-37 batch-67 = 1.7760787159204483e-05

Training epoch-37 batch-68
Running loss of epoch-37 batch-68 = 8.617411367595196e-06

Training epoch-37 batch-69
Running loss of epoch-37 batch-69 = 2.882210537791252e-06

Training epoch-37 batch-70
Running loss of epoch-37 batch-70 = 1.1905096471309662e-05

Training epoch-37 batch-71
Running loss of epoch-37 batch-71 = 2.0977342501282692e-05

Training epoch-37 batch-72
Running loss of epoch-37 batch-72 = 1.111859455704689e-05

Training epoch-37 batch-73
Running loss of epoch-37 batch-73 = 1.1268537491559982e-05

Training epoch-37 batch-74
Running loss of epoch-37 batch-74 = 1.09807588160038e-05

Training epoch-37 batch-75
Running loss of epoch-37 batch-75 = 1.4674384146928787e-05

Training epoch-37 batch-76
Running loss of epoch-37 batch-76 = 2.5896471925079823e-05

Training epoch-37 batch-77
Running loss of epoch-37 batch-77 = 6.513670086860657e-06

Training epoch-37 batch-78
Running loss of epoch-37 batch-78 = 1.2455042451620102e-05

Training epoch-37 batch-79
Running loss of epoch-37 batch-79 = 2.2433116100728512e-05

Training epoch-37 batch-80
Running loss of epoch-37 batch-80 = 1.3253418728709221e-05

Training epoch-37 batch-81
Running loss of epoch-37 batch-81 = 8.02893191576004e-06

Training epoch-37 batch-82
Running loss of epoch-37 batch-82 = 1.3825949281454086e-05

Training epoch-37 batch-83
Running loss of epoch-37 batch-83 = 4.395609721541405e-06

Training epoch-37 batch-84
Running loss of epoch-37 batch-84 = 1.7328420653939247e-05

Training epoch-37 batch-85
Running loss of epoch-37 batch-85 = 2.3035332560539246e-05

Training epoch-37 batch-86
Running loss of epoch-37 batch-86 = 5.3979456424713135e-06

Training epoch-37 batch-87
Running loss of epoch-37 batch-87 = 7.41216354072094e-06

Training epoch-37 batch-88
Running loss of epoch-37 batch-88 = 1.965416595339775e-05

Training epoch-37 batch-89
Running loss of epoch-37 batch-89 = 3.7292949855327606e-05

Training epoch-37 batch-90
Running loss of epoch-37 batch-90 = 8.951174095273018e-06

Training epoch-37 batch-91
Running loss of epoch-37 batch-91 = 4.588160663843155e-06

Training epoch-37 batch-92
Running loss of epoch-37 batch-92 = 7.794937118887901e-06

Training epoch-37 batch-93
Running loss of epoch-37 batch-93 = 5.7220458984375e-06

Training epoch-37 batch-94
Running loss of epoch-37 batch-94 = 1.5186844393610954e-05

Training epoch-37 batch-95
Running loss of epoch-37 batch-95 = 2.2785039618611336e-05

Training epoch-37 batch-96
Running loss of epoch-37 batch-96 = 1.0405899956822395e-05

Training epoch-37 batch-97
Running loss of epoch-37 batch-97 = 1.2310221791267395e-05

Training epoch-37 batch-98
Running loss of epoch-37 batch-98 = 7.45849683880806e-06

Training epoch-37 batch-99
Running loss of epoch-37 batch-99 = 7.770257070660591e-06

Training epoch-37 batch-100
Running loss of epoch-37 batch-100 = 1.0871561244130135e-05

Training epoch-37 batch-101
Running loss of epoch-37 batch-101 = 9.917188435792923e-06

Training epoch-37 batch-102
Running loss of epoch-37 batch-102 = 6.534392014145851e-06

Training epoch-37 batch-103
Running loss of epoch-37 batch-103 = 2.7474015951156616e-06

Training epoch-37 batch-104
Running loss of epoch-37 batch-104 = 1.225341111421585e-05

Training epoch-37 batch-105
Running loss of epoch-37 batch-105 = 9.195879101753235e-06

Training epoch-37 batch-106
Running loss of epoch-37 batch-106 = 1.1498341336846352e-05

Training epoch-37 batch-107
Running loss of epoch-37 batch-107 = 3.2635871320962906e-06

Training epoch-37 batch-108
Running loss of epoch-37 batch-108 = 6.591202691197395e-06

Training epoch-37 batch-109
Running loss of epoch-37 batch-109 = 1.7844606190919876e-05

Training epoch-37 batch-110
Running loss of epoch-37 batch-110 = 2.0175473764538765e-05

Training epoch-37 batch-111
Running loss of epoch-37 batch-111 = 9.297044016420841e-06

Training epoch-37 batch-112
Running loss of epoch-37 batch-112 = 1.4868564903736115e-05

Training epoch-37 batch-113
Running loss of epoch-37 batch-113 = 1.9805273041129112e-05

Training epoch-37 batch-114
Running loss of epoch-37 batch-114 = 2.566864714026451e-05

Training epoch-37 batch-115
Running loss of epoch-37 batch-115 = 1.83966476470232e-05

Training epoch-37 batch-116
Running loss of epoch-37 batch-116 = 9.7277807071805e-06

Training epoch-37 batch-117
Running loss of epoch-37 batch-117 = 8.436385542154312e-06

Training epoch-37 batch-118
Running loss of epoch-37 batch-118 = 9.339768439531326e-06

Training epoch-37 batch-119
Running loss of epoch-37 batch-119 = 3.620050847530365e-05

Training epoch-37 batch-120
Running loss of epoch-37 batch-120 = 6.3213519752025604e-06

Training epoch-37 batch-121
Running loss of epoch-37 batch-121 = 1.558731310069561e-05

Training epoch-37 batch-122
Running loss of epoch-37 batch-122 = 9.957235306501389e-06

Training epoch-37 batch-123
Running loss of epoch-37 batch-123 = 9.583658538758755e-06

Training epoch-37 batch-124
Running loss of epoch-37 batch-124 = 6.936490535736084e-06

Training epoch-37 batch-125
Running loss of epoch-37 batch-125 = 8.385395631194115e-06

Training epoch-37 batch-126
Running loss of epoch-37 batch-126 = 8.52346420288086e-06

Training epoch-37 batch-127
Running loss of epoch-37 batch-127 = 9.441981092095375e-06

Training epoch-37 batch-128
Running loss of epoch-37 batch-128 = 7.439404726028442e-06

Training epoch-37 batch-129
Running loss of epoch-37 batch-129 = 6.303191184997559e-06

Training epoch-37 batch-130
Running loss of epoch-37 batch-130 = 7.746974006295204e-06

Training epoch-37 batch-131
Running loss of epoch-37 batch-131 = 1.1887634173035622e-05

Training epoch-37 batch-132
Running loss of epoch-37 batch-132 = 2.0415522158145905e-05

Training epoch-37 batch-133
Running loss of epoch-37 batch-133 = 1.0796589776873589e-05

Training epoch-37 batch-134
Running loss of epoch-37 batch-134 = 2.715294249355793e-05

Training epoch-37 batch-135
Running loss of epoch-37 batch-135 = 7.351860404014587e-06

Training epoch-37 batch-136
Running loss of epoch-37 batch-136 = 8.441973477602005e-06

Training epoch-37 batch-137
Running loss of epoch-37 batch-137 = 7.469207048416138e-06

Training epoch-37 batch-138
Running loss of epoch-37 batch-138 = 8.386094123125076e-06

Training epoch-37 batch-139
Running loss of epoch-37 batch-139 = 6.49341382086277e-06

Training epoch-37 batch-140
Running loss of epoch-37 batch-140 = 8.58306884765625e-06

Training epoch-37 batch-141
Running loss of epoch-37 batch-141 = 1.3310927897691727e-05

Training epoch-37 batch-142
Running loss of epoch-37 batch-142 = 7.785158231854439e-06

Training epoch-37 batch-143
Running loss of epoch-37 batch-143 = 4.454934969544411e-05

Training epoch-37 batch-144
Running loss of epoch-37 batch-144 = 1.4754245057702065e-05

Training epoch-37 batch-145
Running loss of epoch-37 batch-145 = 1.527252607047558e-05

Training epoch-37 batch-146
Running loss of epoch-37 batch-146 = 9.924639016389847e-06

Training epoch-37 batch-147
Running loss of epoch-37 batch-147 = 9.399140253663063e-06

Training epoch-37 batch-148
Running loss of epoch-37 batch-148 = 6.022397428750992e-06

Training epoch-37 batch-149
Running loss of epoch-37 batch-149 = 2.316548489034176e-05

Training epoch-37 batch-150
Running loss of epoch-37 batch-150 = 1.2901145964860916e-05

Training epoch-37 batch-151
Running loss of epoch-37 batch-151 = 1.3597076758742332e-05

Training epoch-37 batch-152
Running loss of epoch-37 batch-152 = 7.102498784661293e-06

Training epoch-37 batch-153
Running loss of epoch-37 batch-153 = 3.0099647119641304e-05

Training epoch-37 batch-154
Running loss of epoch-37 batch-154 = 8.85198824107647e-06

Training epoch-37 batch-155
Running loss of epoch-37 batch-155 = 7.645459845662117e-06

Training epoch-37 batch-156
Running loss of epoch-37 batch-156 = 1.2790784239768982e-05

Training epoch-37 batch-157
Running loss of epoch-37 batch-157 = 0.0001996271312236786

Finished training epoch-37.



Average train loss at epoch-37 = 1.239876002073288e-05

Started Evaluation

Average val loss at epoch-37 = 1.0253871661545513

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 89.42 %
Accuracy for class run is: 61.64 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 55.61 %
Accuracy for class execute is: 44.18 %
Accuracy for class get is: 68.72 %

Overall Accuracy = 83.04 %

Finished Evaluation



Started training epoch-38


Training epoch-38 batch-1
Running loss of epoch-38 batch-1 = 2.963002771139145e-06

Training epoch-38 batch-2
Running loss of epoch-38 batch-2 = 3.3098855055868626e-05

Training epoch-38 batch-3
Running loss of epoch-38 batch-3 = 7.980503141880035e-06

Training epoch-38 batch-4
Running loss of epoch-38 batch-4 = 1.2416858226060867e-05

Training epoch-38 batch-5
Running loss of epoch-38 batch-5 = 1.0034535080194473e-05

Training epoch-38 batch-6
Running loss of epoch-38 batch-6 = 8.83336178958416e-06

Training epoch-38 batch-7
Running loss of epoch-38 batch-7 = 9.655021131038666e-06

Training epoch-38 batch-8
Running loss of epoch-38 batch-8 = 2.023449633270502e-05

Training epoch-38 batch-9
Running loss of epoch-38 batch-9 = 9.893206879496574e-06

Training epoch-38 batch-10
Running loss of epoch-38 batch-10 = 8.52532684803009e-06

Training epoch-38 batch-11
Running loss of epoch-38 batch-11 = 1.0521383956074715e-05

Training epoch-38 batch-12
Running loss of epoch-38 batch-12 = 1.6406644135713577e-05

Training epoch-38 batch-13
Running loss of epoch-38 batch-13 = 1.981784589588642e-05

Training epoch-38 batch-14
Running loss of epoch-38 batch-14 = 1.4508026652038097e-05

Training epoch-38 batch-15
Running loss of epoch-38 batch-15 = 1.8911203369498253e-05

Training epoch-38 batch-16
Running loss of epoch-38 batch-16 = 1.085689291357994e-05

Training epoch-38 batch-17
Running loss of epoch-38 batch-17 = 2.350611612200737e-05

Training epoch-38 batch-18
Running loss of epoch-38 batch-18 = 7.26897269487381e-06

Training epoch-38 batch-19
Running loss of epoch-38 batch-19 = 2.317677717655897e-05

Training epoch-38 batch-20
Running loss of epoch-38 batch-20 = 1.3462966307997704e-05

Training epoch-38 batch-21
Running loss of epoch-38 batch-21 = 7.040100172162056e-06

Training epoch-38 batch-22
Running loss of epoch-38 batch-22 = 1.7788494005799294e-05

Training epoch-38 batch-23
Running loss of epoch-38 batch-23 = 8.19074921309948e-06

Training epoch-38 batch-24
Running loss of epoch-38 batch-24 = 8.594710379838943e-06

Training epoch-38 batch-25
Running loss of epoch-38 batch-25 = 1.4747492969036102e-05

Training epoch-38 batch-26
Running loss of epoch-38 batch-26 = 4.884321242570877e-06

Training epoch-38 batch-27
Running loss of epoch-38 batch-27 = 7.465248927474022e-06

Training epoch-38 batch-28
Running loss of epoch-38 batch-28 = 2.9923394322395325e-06

Training epoch-38 batch-29
Running loss of epoch-38 batch-29 = 9.43080522119999e-06

Training epoch-38 batch-30
Running loss of epoch-38 batch-30 = 6.7818909883499146e-06

Training epoch-38 batch-31
Running loss of epoch-38 batch-31 = 5.775131285190582e-06

Training epoch-38 batch-32
Running loss of epoch-38 batch-32 = 1.0886695235967636e-05

Training epoch-38 batch-33
Running loss of epoch-38 batch-33 = 6.705988198518753e-06

Training epoch-38 batch-34
Running loss of epoch-38 batch-34 = 2.7594156563282013e-05

Training epoch-38 batch-35
Running loss of epoch-38 batch-35 = 1.1940719559788704e-05

Training epoch-38 batch-36
Running loss of epoch-38 batch-36 = 1.4192191883921623e-05

Training epoch-38 batch-37
Running loss of epoch-38 batch-37 = 2.03100498765707e-05

Training epoch-38 batch-38
Running loss of epoch-38 batch-38 = 5.71180135011673e-06

Training epoch-38 batch-39
Running loss of epoch-38 batch-39 = 1.741712912917137e-05

Training epoch-38 batch-40
Running loss of epoch-38 batch-40 = 8.833827450871468e-06

Training epoch-38 batch-41
Running loss of epoch-38 batch-41 = 8.907169103622437e-06

Training epoch-38 batch-42
Running loss of epoch-38 batch-42 = 1.100881490856409e-05

Training epoch-38 batch-43
Running loss of epoch-38 batch-43 = 1.1975993402302265e-05

Training epoch-38 batch-44
Running loss of epoch-38 batch-44 = 1.136469654738903e-05

Training epoch-38 batch-45
Running loss of epoch-38 batch-45 = 4.4442713260650635e-06

Training epoch-38 batch-46
Running loss of epoch-38 batch-46 = 1.0844552889466286e-05

Training epoch-38 batch-47
Running loss of epoch-38 batch-47 = 1.0119052603840828e-05

Training epoch-38 batch-48
Running loss of epoch-38 batch-48 = 1.1268537491559982e-05

Training epoch-38 batch-49
Running loss of epoch-38 batch-49 = 1.2712785974144936e-05

Training epoch-38 batch-50
Running loss of epoch-38 batch-50 = 1.6062520444393158e-05

Training epoch-38 batch-51
Running loss of epoch-38 batch-51 = 7.664086297154427e-06

Training epoch-38 batch-52
Running loss of epoch-38 batch-52 = 1.5588710084557533e-05

Training epoch-38 batch-53
Running loss of epoch-38 batch-53 = 8.570961654186249e-06

Training epoch-38 batch-54
Running loss of epoch-38 batch-54 = 1.4048069715499878e-05

Training epoch-38 batch-55
Running loss of epoch-38 batch-55 = 7.645692676305771e-06

Training epoch-38 batch-56
Running loss of epoch-38 batch-56 = 4.385830834507942e-06

Training epoch-38 batch-57
Running loss of epoch-38 batch-57 = 9.213574230670929e-06

Training epoch-38 batch-58
Running loss of epoch-38 batch-58 = 1.219380646944046e-05

Training epoch-38 batch-59
Running loss of epoch-38 batch-59 = 2.4626264348626137e-05

Training epoch-38 batch-60
Running loss of epoch-38 batch-60 = 1.2786826118826866e-05

Training epoch-38 batch-61
Running loss of epoch-38 batch-61 = 2.427026629447937e-05

Training epoch-38 batch-62
Running loss of epoch-38 batch-62 = 1.0646996088325977e-05

Training epoch-38 batch-63
Running loss of epoch-38 batch-63 = 1.3609416782855988e-05

Training epoch-38 batch-64
Running loss of epoch-38 batch-64 = 1.3336073607206345e-05

Training epoch-38 batch-65
Running loss of epoch-38 batch-65 = 1.963670365512371e-05

Training epoch-38 batch-66
Running loss of epoch-38 batch-66 = 2.2508669644594193e-05

Training epoch-38 batch-67
Running loss of epoch-38 batch-67 = 1.0692747309803963e-05

Training epoch-38 batch-68
Running loss of epoch-38 batch-68 = 1.9353115931153297e-05

Training epoch-38 batch-69
Running loss of epoch-38 batch-69 = 1.116073690354824e-05

Training epoch-38 batch-70
Running loss of epoch-38 batch-70 = 1.0707415640354156e-05

Training epoch-38 batch-71
Running loss of epoch-38 batch-71 = 7.204478606581688e-06

Training epoch-38 batch-72
Running loss of epoch-38 batch-72 = 1.8682098016142845e-05

Training epoch-38 batch-73
Running loss of epoch-38 batch-73 = 5.336245521903038e-06

Training epoch-38 batch-74
Running loss of epoch-38 batch-74 = 7.77142122387886e-06

Training epoch-38 batch-75
Running loss of epoch-38 batch-75 = 8.736737072467804e-06

Training epoch-38 batch-76
Running loss of epoch-38 batch-76 = 8.32974910736084e-06

Training epoch-38 batch-77
Running loss of epoch-38 batch-77 = 1.8578488379716873e-05

Training epoch-38 batch-78
Running loss of epoch-38 batch-78 = 3.3816322684288025e-06

Training epoch-38 batch-79
Running loss of epoch-38 batch-79 = 1.035328023135662e-05

Training epoch-38 batch-80
Running loss of epoch-38 batch-80 = 1.6147736459970474e-05

Training epoch-38 batch-81
Running loss of epoch-38 batch-81 = 1.1733034625649452e-05

Training epoch-38 batch-82
Running loss of epoch-38 batch-82 = 1.0234536603093147e-05

Training epoch-38 batch-83
Running loss of epoch-38 batch-83 = 4.99282032251358e-06

Training epoch-38 batch-84
Running loss of epoch-38 batch-84 = 7.416121661663055e-06

Training epoch-38 batch-85
Running loss of epoch-38 batch-85 = 8.72625969350338e-06

Training epoch-38 batch-86
Running loss of epoch-38 batch-86 = 4.448229447007179e-06

Training epoch-38 batch-87
Running loss of epoch-38 batch-87 = 1.0226154699921608e-05

Training epoch-38 batch-88
Running loss of epoch-38 batch-88 = 8.958857506513596e-06

Training epoch-38 batch-89
Running loss of epoch-38 batch-89 = 1.2273667380213737e-05

Training epoch-38 batch-90
Running loss of epoch-38 batch-90 = 2.047233283519745e-05

Training epoch-38 batch-91
Running loss of epoch-38 batch-91 = 6.732763722538948e-06

Training epoch-38 batch-92
Running loss of epoch-38 batch-92 = 8.763046935200691e-06

Training epoch-38 batch-93
Running loss of epoch-38 batch-93 = 6.965361535549164e-06

Training epoch-38 batch-94
Running loss of epoch-38 batch-94 = 2.945307642221451e-06

Training epoch-38 batch-95
Running loss of epoch-38 batch-95 = 1.3696029782295227e-05

Training epoch-38 batch-96
Running loss of epoch-38 batch-96 = 4.242639988660812e-06

Training epoch-38 batch-97
Running loss of epoch-38 batch-97 = 1.2137927114963531e-05

Training epoch-38 batch-98
Running loss of epoch-38 batch-98 = 8.424976840615273e-06

Training epoch-38 batch-99
Running loss of epoch-38 batch-99 = 5.440087988972664e-06

Training epoch-38 batch-100
Running loss of epoch-38 batch-100 = 1.3161101378500462e-05

Training epoch-38 batch-101
Running loss of epoch-38 batch-101 = 1.0429183021187782e-05

Training epoch-38 batch-102
Running loss of epoch-38 batch-102 = 6.513437256217003e-06

Training epoch-38 batch-103
Running loss of epoch-38 batch-103 = 1.3000681065022945e-05

Training epoch-38 batch-104
Running loss of epoch-38 batch-104 = 2.3137079551815987e-05

Training epoch-38 batch-105
Running loss of epoch-38 batch-105 = 8.755596354603767e-06

Training epoch-38 batch-106
Running loss of epoch-38 batch-106 = 9.477604180574417e-06

Training epoch-38 batch-107
Running loss of epoch-38 batch-107 = 1.4686491340398788e-05

Training epoch-38 batch-108
Running loss of epoch-38 batch-108 = 4.424946382641792e-06

Training epoch-38 batch-109
Running loss of epoch-38 batch-109 = 1.1245021596550941e-05

Training epoch-38 batch-110
Running loss of epoch-38 batch-110 = 8.157221600413322e-06

Training epoch-38 batch-111
Running loss of epoch-38 batch-111 = 2.0998064428567886e-05

Training epoch-38 batch-112
Running loss of epoch-38 batch-112 = 1.5123281627893448e-05

Training epoch-38 batch-113
Running loss of epoch-38 batch-113 = 3.928318619728088e-06

Training epoch-38 batch-114
Running loss of epoch-38 batch-114 = 1.0945135727524757e-05

Training epoch-38 batch-115
Running loss of epoch-38 batch-115 = 2.6870984584093094e-05

Training epoch-38 batch-116
Running loss of epoch-38 batch-116 = 1.6840174794197083e-05

Training epoch-38 batch-117
Running loss of epoch-38 batch-117 = 4.473607987165451e-06

Training epoch-38 batch-118
Running loss of epoch-38 batch-118 = 5.666399374604225e-06

Training epoch-38 batch-119
Running loss of epoch-38 batch-119 = 1.5264376997947693e-05

Training epoch-38 batch-120
Running loss of epoch-38 batch-120 = 1.838942989706993e-05

Training epoch-38 batch-121
Running loss of epoch-38 batch-121 = 5.9506855905056e-06

Training epoch-38 batch-122
Running loss of epoch-38 batch-122 = 8.43266025185585e-06

Training epoch-38 batch-123
Running loss of epoch-38 batch-123 = 1.0230112820863724e-05

Training epoch-38 batch-124
Running loss of epoch-38 batch-124 = 9.560026228427887e-06

Training epoch-38 batch-125
Running loss of epoch-38 batch-125 = 5.8426521718502045e-06

Training epoch-38 batch-126
Running loss of epoch-38 batch-126 = 9.22568142414093e-06

Training epoch-38 batch-127
Running loss of epoch-38 batch-127 = 8.826609700918198e-06

Training epoch-38 batch-128
Running loss of epoch-38 batch-128 = 7.67270103096962e-06

Training epoch-38 batch-129
Running loss of epoch-38 batch-129 = 6.874324753880501e-06

Training epoch-38 batch-130
Running loss of epoch-38 batch-130 = 5.839159712195396e-06

Training epoch-38 batch-131
Running loss of epoch-38 batch-131 = 8.424511179327965e-06

Training epoch-38 batch-132
Running loss of epoch-38 batch-132 = 2.719089388847351e-05

Training epoch-38 batch-133
Running loss of epoch-38 batch-133 = 1.4628050848841667e-05

Training epoch-38 batch-134
Running loss of epoch-38 batch-134 = 9.055482223629951e-06

Training epoch-38 batch-135
Running loss of epoch-38 batch-135 = 5.177222192287445e-06

Training epoch-38 batch-136
Running loss of epoch-38 batch-136 = 5.344627425074577e-06

Training epoch-38 batch-137
Running loss of epoch-38 batch-137 = 1.1328142136335373e-05

Training epoch-38 batch-138
Running loss of epoch-38 batch-138 = 5.554873496294022e-06

Training epoch-38 batch-139
Running loss of epoch-38 batch-139 = 7.455470040440559e-06

Training epoch-38 batch-140
Running loss of epoch-38 batch-140 = 1.4308607205748558e-05

Training epoch-38 batch-141
Running loss of epoch-38 batch-141 = 1.1461088433861732e-05

Training epoch-38 batch-142
Running loss of epoch-38 batch-142 = 5.903886631131172e-06

Training epoch-38 batch-143
Running loss of epoch-38 batch-143 = 6.79423101246357e-06

Training epoch-38 batch-144
Running loss of epoch-38 batch-144 = 1.5970319509506226e-05

Training epoch-38 batch-145
Running loss of epoch-38 batch-145 = 1.1317548342049122e-05

Training epoch-38 batch-146
Running loss of epoch-38 batch-146 = 2.1921005100011826e-05

Training epoch-38 batch-147
Running loss of epoch-38 batch-147 = 1.9312137737870216e-05

Training epoch-38 batch-148
Running loss of epoch-38 batch-148 = 8.132541552186012e-06

Training epoch-38 batch-149
Running loss of epoch-38 batch-149 = 1.4827121049165726e-05

Training epoch-38 batch-150
Running loss of epoch-38 batch-150 = 1.3690907508134842e-05

Training epoch-38 batch-151
Running loss of epoch-38 batch-151 = 4.519708454608917e-06

Training epoch-38 batch-152
Running loss of epoch-38 batch-152 = 2.9531540349125862e-05

Training epoch-38 batch-153
Running loss of epoch-38 batch-153 = 1.1388445273041725e-05

Training epoch-38 batch-154
Running loss of epoch-38 batch-154 = 4.738220013678074e-06

Training epoch-38 batch-155
Running loss of epoch-38 batch-155 = 8.292379789054394e-06

Training epoch-38 batch-156
Running loss of epoch-38 batch-156 = 9.107636287808418e-06

Training epoch-38 batch-157
Running loss of epoch-38 batch-157 = 3.593415021896362e-05

Finished training epoch-38.



Average train loss at epoch-38 = 1.1573536694049836e-05

Started Evaluation

Average val loss at epoch-38 = 1.0317825909458909

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 42.17 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.29 %

Finished Evaluation



Started training epoch-39


Training epoch-39 batch-1
Running loss of epoch-39 batch-1 = 1.4824792742729187e-05

Training epoch-39 batch-2
Running loss of epoch-39 batch-2 = 9.047798812389374e-06

Training epoch-39 batch-3
Running loss of epoch-39 batch-3 = 1.2412900105118752e-05

Training epoch-39 batch-4
Running loss of epoch-39 batch-4 = 7.004011422395706e-06

Training epoch-39 batch-5
Running loss of epoch-39 batch-5 = 1.0579125955700874e-05

Training epoch-39 batch-6
Running loss of epoch-39 batch-6 = 5.17512671649456e-06

Training epoch-39 batch-7
Running loss of epoch-39 batch-7 = 1.1842697858810425e-05

Training epoch-39 batch-8
Running loss of epoch-39 batch-8 = 1.0586809366941452e-05

Training epoch-39 batch-9
Running loss of epoch-39 batch-9 = 8.916249498724937e-06

Training epoch-39 batch-10
Running loss of epoch-39 batch-10 = 1.3348180800676346e-05

Training epoch-39 batch-11
Running loss of epoch-39 batch-11 = 9.639887139201164e-06

Training epoch-39 batch-12
Running loss of epoch-39 batch-12 = 1.4811055734753609e-05

Training epoch-39 batch-13
Running loss of epoch-39 batch-13 = 8.916482329368591e-06

Training epoch-39 batch-14
Running loss of epoch-39 batch-14 = 3.6973506212234497e-06

Training epoch-39 batch-15
Running loss of epoch-39 batch-15 = 1.1174473911523819e-05

Training epoch-39 batch-16
Running loss of epoch-39 batch-16 = 5.628680810332298e-06

Training epoch-39 batch-17
Running loss of epoch-39 batch-17 = 1.6119563952088356e-05

Training epoch-39 batch-18
Running loss of epoch-39 batch-18 = 1.4676479622721672e-05

Training epoch-39 batch-19
Running loss of epoch-39 batch-19 = 7.727066986262798e-06

Training epoch-39 batch-20
Running loss of epoch-39 batch-20 = 7.388880476355553e-06

Training epoch-39 batch-21
Running loss of epoch-39 batch-21 = 6.9765374064445496e-06

Training epoch-39 batch-22
Running loss of epoch-39 batch-22 = 9.812414646148682e-06

Training epoch-39 batch-23
Running loss of epoch-39 batch-23 = 1.928163692355156e-05

Training epoch-39 batch-24
Running loss of epoch-39 batch-24 = 7.693422958254814e-06

Training epoch-39 batch-25
Running loss of epoch-39 batch-25 = 1.0764924809336662e-05

Training epoch-39 batch-26
Running loss of epoch-39 batch-26 = 1.298799179494381e-05

Training epoch-39 batch-27
Running loss of epoch-39 batch-27 = 5.715526640415192e-06

Training epoch-39 batch-28
Running loss of epoch-39 batch-28 = 1.1089490726590157e-05

Training epoch-39 batch-29
Running loss of epoch-39 batch-29 = 1.1506956070661545e-05

Training epoch-39 batch-30
Running loss of epoch-39 batch-30 = 9.835464879870415e-06

Training epoch-39 batch-31
Running loss of epoch-39 batch-31 = 3.950903192162514e-06

Training epoch-39 batch-32
Running loss of epoch-39 batch-32 = 5.135079845786095e-06

Training epoch-39 batch-33
Running loss of epoch-39 batch-33 = 1.2006144970655441e-05

Training epoch-39 batch-34
Running loss of epoch-39 batch-34 = 1.685740426182747e-05

Training epoch-39 batch-35
Running loss of epoch-39 batch-35 = 2.6152003556489944e-05

Training epoch-39 batch-36
Running loss of epoch-39 batch-36 = 1.167692244052887e-05

Training epoch-39 batch-37
Running loss of epoch-39 batch-37 = 5.357665941119194e-06

Training epoch-39 batch-38
Running loss of epoch-39 batch-38 = 9.009381756186485e-06

Training epoch-39 batch-39
Running loss of epoch-39 batch-39 = 7.521593943238258e-06

Training epoch-39 batch-40
Running loss of epoch-39 batch-40 = 8.472474291920662e-06

Training epoch-39 batch-41
Running loss of epoch-39 batch-41 = 7.509952411055565e-06

Training epoch-39 batch-42
Running loss of epoch-39 batch-42 = 1.4938763342797756e-05

Training epoch-39 batch-43
Running loss of epoch-39 batch-43 = 1.4370772987604141e-05

Training epoch-39 batch-44
Running loss of epoch-39 batch-44 = 2.2573163732886314e-05

Training epoch-39 batch-45
Running loss of epoch-39 batch-45 = 2.5243032723665237e-05

Training epoch-39 batch-46
Running loss of epoch-39 batch-46 = 1.7861369997262955e-05

Training epoch-39 batch-47
Running loss of epoch-39 batch-47 = 5.757436156272888e-06

Training epoch-39 batch-48
Running loss of epoch-39 batch-48 = 2.130598295480013e-05

Training epoch-39 batch-49
Running loss of epoch-39 batch-49 = 1.2966804206371307e-05

Training epoch-39 batch-50
Running loss of epoch-39 batch-50 = 1.782202161848545e-05

Training epoch-39 batch-51
Running loss of epoch-39 batch-51 = 1.2299977242946625e-05

Training epoch-39 batch-52
Running loss of epoch-39 batch-52 = 9.80449840426445e-06

Training epoch-39 batch-53
Running loss of epoch-39 batch-53 = 9.932206012308598e-06

Training epoch-39 batch-54
Running loss of epoch-39 batch-54 = 7.579335942864418e-06

Training epoch-39 batch-55
Running loss of epoch-39 batch-55 = 1.3364013284444809e-05

Training epoch-39 batch-56
Running loss of epoch-39 batch-56 = 3.9711594581604e-06

Training epoch-39 batch-57
Running loss of epoch-39 batch-57 = 9.577954187989235e-06

Training epoch-39 batch-58
Running loss of epoch-39 batch-58 = 4.007015377283096e-06

Training epoch-39 batch-59
Running loss of epoch-39 batch-59 = 3.996537998318672e-06

Training epoch-39 batch-60
Running loss of epoch-39 batch-60 = 1.3915589079260826e-05

Training epoch-39 batch-61
Running loss of epoch-39 batch-61 = 1.0799616575241089e-05

Training epoch-39 batch-62
Running loss of epoch-39 batch-62 = 3.875698894262314e-06

Training epoch-39 batch-63
Running loss of epoch-39 batch-63 = 1.5592435374855995e-05

Training epoch-39 batch-64
Running loss of epoch-39 batch-64 = 1.471606083214283e-05

Training epoch-39 batch-65
Running loss of epoch-39 batch-65 = 6.632879376411438e-06

Training epoch-39 batch-66
Running loss of epoch-39 batch-66 = 5.351146683096886e-06

Training epoch-39 batch-67
Running loss of epoch-39 batch-67 = 4.7495122998952866e-06

Training epoch-39 batch-68
Running loss of epoch-39 batch-68 = 1.2499280273914337e-05

Training epoch-39 batch-69
Running loss of epoch-39 batch-69 = 1.128343865275383e-05

Training epoch-39 batch-70
Running loss of epoch-39 batch-70 = 1.4043645933270454e-05

Training epoch-39 batch-71
Running loss of epoch-39 batch-71 = 2.0020175725221634e-05

Training epoch-39 batch-72
Running loss of epoch-39 batch-72 = 8.766073733568192e-06

Training epoch-39 batch-73
Running loss of epoch-39 batch-73 = 1.8040183931589127e-05

Training epoch-39 batch-74
Running loss of epoch-39 batch-74 = 2.9023736715316772e-05

Training epoch-39 batch-75
Running loss of epoch-39 batch-75 = 1.921551302075386e-05

Training epoch-39 batch-76
Running loss of epoch-39 batch-76 = 6.8407971411943436e-06

Training epoch-39 batch-77
Running loss of epoch-39 batch-77 = 5.049863830208778e-06

Training epoch-39 batch-78
Running loss of epoch-39 batch-78 = 4.538102075457573e-06

Training epoch-39 batch-79
Running loss of epoch-39 batch-79 = 5.211913958191872e-06

Training epoch-39 batch-80
Running loss of epoch-39 batch-80 = 6.237532943487167e-06

Training epoch-39 batch-81
Running loss of epoch-39 batch-81 = 8.71601514518261e-06

Training epoch-39 batch-82
Running loss of epoch-39 batch-82 = 1.7448095604777336e-05

Training epoch-39 batch-83
Running loss of epoch-39 batch-83 = 7.805414497852325e-06

Training epoch-39 batch-84
Running loss of epoch-39 batch-84 = 1.4111632481217384e-05

Training epoch-39 batch-85
Running loss of epoch-39 batch-85 = 8.911825716495514e-06

Training epoch-39 batch-86
Running loss of epoch-39 batch-86 = 6.878282874822617e-06

Training epoch-39 batch-87
Running loss of epoch-39 batch-87 = 1.0515563189983368e-05

Training epoch-39 batch-88
Running loss of epoch-39 batch-88 = 1.306470949202776e-05

Training epoch-39 batch-89
Running loss of epoch-39 batch-89 = 1.3128388673067093e-05

Training epoch-39 batch-90
Running loss of epoch-39 batch-90 = 6.281072273850441e-06

Training epoch-39 batch-91
Running loss of epoch-39 batch-91 = 1.077982597053051e-05

Training epoch-39 batch-92
Running loss of epoch-39 batch-92 = 7.064430974423885e-06

Training epoch-39 batch-93
Running loss of epoch-39 batch-93 = 9.181909263134003e-06

Training epoch-39 batch-94
Running loss of epoch-39 batch-94 = 9.687384590506554e-06

Training epoch-39 batch-95
Running loss of epoch-39 batch-95 = 4.564179107546806e-06

Training epoch-39 batch-96
Running loss of epoch-39 batch-96 = 3.5616103559732437e-06

Training epoch-39 batch-97
Running loss of epoch-39 batch-97 = 1.6153440810739994e-05

Training epoch-39 batch-98
Running loss of epoch-39 batch-98 = 5.808891728520393e-06

Training epoch-39 batch-99
Running loss of epoch-39 batch-99 = 1.3792188838124275e-05

Training epoch-39 batch-100
Running loss of epoch-39 batch-100 = 1.0555842891335487e-05

Training epoch-39 batch-101
Running loss of epoch-39 batch-101 = 1.2478558346629143e-05

Training epoch-39 batch-102
Running loss of epoch-39 batch-102 = 7.597962394356728e-06

Training epoch-39 batch-103
Running loss of epoch-39 batch-103 = 8.722534403204918e-06

Training epoch-39 batch-104
Running loss of epoch-39 batch-104 = 6.451038643717766e-06

Training epoch-39 batch-105
Running loss of epoch-39 batch-105 = 7.797498255968094e-06

Training epoch-39 batch-106
Running loss of epoch-39 batch-106 = 1.0193558409810066e-05

Training epoch-39 batch-107
Running loss of epoch-39 batch-107 = 8.670613169670105e-06

Training epoch-39 batch-108
Running loss of epoch-39 batch-108 = 6.8354420363903046e-06

Training epoch-39 batch-109
Running loss of epoch-39 batch-109 = 9.006587788462639e-06

Training epoch-39 batch-110
Running loss of epoch-39 batch-110 = 5.619262810796499e-05

Training epoch-39 batch-111
Running loss of epoch-39 batch-111 = 1.1021969839930534e-05

Training epoch-39 batch-112
Running loss of epoch-39 batch-112 = 7.463851943612099e-06

Training epoch-39 batch-113
Running loss of epoch-39 batch-113 = 5.853129550814629e-06

Training epoch-39 batch-114
Running loss of epoch-39 batch-114 = 3.783963620662689e-06

Training epoch-39 batch-115
Running loss of epoch-39 batch-115 = 9.448500350117683e-06

Training epoch-39 batch-116
Running loss of epoch-39 batch-116 = 2.7537811547517776e-05

Training epoch-39 batch-117
Running loss of epoch-39 batch-117 = 9.95909795165062e-06

Training epoch-39 batch-118
Running loss of epoch-39 batch-118 = 1.0421965271234512e-05

Training epoch-39 batch-119
Running loss of epoch-39 batch-119 = 1.9016209989786148e-05

Training epoch-39 batch-120
Running loss of epoch-39 batch-120 = 7.906230166554451e-06

Training epoch-39 batch-121
Running loss of epoch-39 batch-121 = 3.791414201259613e-06

Training epoch-39 batch-122
Running loss of epoch-39 batch-122 = 8.179107680916786e-06

Training epoch-39 batch-123
Running loss of epoch-39 batch-123 = 7.026828825473785e-06

Training epoch-39 batch-124
Running loss of epoch-39 batch-124 = 1.050787977874279e-05

Training epoch-39 batch-125
Running loss of epoch-39 batch-125 = 5.746958777308464e-06

Training epoch-39 batch-126
Running loss of epoch-39 batch-126 = 7.620779797434807e-06

Training epoch-39 batch-127
Running loss of epoch-39 batch-127 = 7.527880370616913e-06

Training epoch-39 batch-128
Running loss of epoch-39 batch-128 = 1.0335119441151619e-05

Training epoch-39 batch-129
Running loss of epoch-39 batch-129 = 1.1463416740298271e-05

Training epoch-39 batch-130
Running loss of epoch-39 batch-130 = 3.696349449455738e-05

Training epoch-39 batch-131
Running loss of epoch-39 batch-131 = 3.6857090890407562e-06

Training epoch-39 batch-132
Running loss of epoch-39 batch-132 = 5.8389268815517426e-06

Training epoch-39 batch-133
Running loss of epoch-39 batch-133 = 3.523426130414009e-06

Training epoch-39 batch-134
Running loss of epoch-39 batch-134 = 1.0012183338403702e-05

Training epoch-39 batch-135
Running loss of epoch-39 batch-135 = 3.205845132470131e-06

Training epoch-39 batch-136
Running loss of epoch-39 batch-136 = 4.402711056172848e-06

Training epoch-39 batch-137
Running loss of epoch-39 batch-137 = 9.718816727399826e-06

Training epoch-39 batch-138
Running loss of epoch-39 batch-138 = 2.324022352695465e-05

Training epoch-39 batch-139
Running loss of epoch-39 batch-139 = 2.250075340270996e-06

Training epoch-39 batch-140
Running loss of epoch-39 batch-140 = 1.2992648407816887e-05

Training epoch-39 batch-141
Running loss of epoch-39 batch-141 = 3.1088246032595634e-05

Training epoch-39 batch-142
Running loss of epoch-39 batch-142 = 1.9704457372426987e-05

Training epoch-39 batch-143
Running loss of epoch-39 batch-143 = 5.788635462522507e-06

Training epoch-39 batch-144
Running loss of epoch-39 batch-144 = 4.079658538103104e-06

Training epoch-39 batch-145
Running loss of epoch-39 batch-145 = 4.8156362026929855e-06

Training epoch-39 batch-146
Running loss of epoch-39 batch-146 = 1.1458294466137886e-05

Training epoch-39 batch-147
Running loss of epoch-39 batch-147 = 6.024260073900223e-06

Training epoch-39 batch-148
Running loss of epoch-39 batch-148 = 1.1945143342018127e-05

Training epoch-39 batch-149
Running loss of epoch-39 batch-149 = 1.3355864211916924e-05

Training epoch-39 batch-150
Running loss of epoch-39 batch-150 = 5.457550287246704e-06

Training epoch-39 batch-151
Running loss of epoch-39 batch-151 = 5.801208317279816e-06

Training epoch-39 batch-152
Running loss of epoch-39 batch-152 = 1.640990376472473e-05

Training epoch-39 batch-153
Running loss of epoch-39 batch-153 = 1.249765045940876e-05

Training epoch-39 batch-154
Running loss of epoch-39 batch-154 = 1.4233868569135666e-05

Training epoch-39 batch-155
Running loss of epoch-39 batch-155 = 2.3372704163193703e-05

Training epoch-39 batch-156
Running loss of epoch-39 batch-156 = 8.200528100132942e-06

Training epoch-39 batch-157
Running loss of epoch-39 batch-157 = 3.15643846988678e-05

Finished training epoch-39.



Average train loss at epoch-39 = 1.1015262454748154e-05

Started Evaluation

Average val loss at epoch-39 = 1.0331751111434762

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 63.24 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.19 %

Finished Evaluation



Started training epoch-40


Training epoch-40 batch-1
Running loss of epoch-40 batch-1 = 6.6318316385149956e-06

Training epoch-40 batch-2
Running loss of epoch-40 batch-2 = 5.2584800869226456e-06

Training epoch-40 batch-3
Running loss of epoch-40 batch-3 = 3.857305273413658e-06

Training epoch-40 batch-4
Running loss of epoch-40 batch-4 = 1.1706724762916565e-05

Training epoch-40 batch-5
Running loss of epoch-40 batch-5 = 4.0351878851652145e-06

Training epoch-40 batch-6
Running loss of epoch-40 batch-6 = 9.096460416913033e-06

Training epoch-40 batch-7
Running loss of epoch-40 batch-7 = 1.0111834853887558e-05

Training epoch-40 batch-8
Running loss of epoch-40 batch-8 = 1.2601492926478386e-05

Training epoch-40 batch-9
Running loss of epoch-40 batch-9 = 5.688751116394997e-06

Training epoch-40 batch-10
Running loss of epoch-40 batch-10 = 8.22450965642929e-06

Training epoch-40 batch-11
Running loss of epoch-40 batch-11 = 1.598196104168892e-05

Training epoch-40 batch-12
Running loss of epoch-40 batch-12 = 3.1371600925922394e-06

Training epoch-40 batch-13
Running loss of epoch-40 batch-13 = 1.196330413222313e-05

Training epoch-40 batch-14
Running loss of epoch-40 batch-14 = 4.146946594119072e-06

Training epoch-40 batch-15
Running loss of epoch-40 batch-15 = 6.97956420481205e-06

Training epoch-40 batch-16
Running loss of epoch-40 batch-16 = 2.2303545847535133e-05

Training epoch-40 batch-17
Running loss of epoch-40 batch-17 = 1.238333061337471e-05

Training epoch-40 batch-18
Running loss of epoch-40 batch-18 = 6.276881322264671e-06

Training epoch-40 batch-19
Running loss of epoch-40 batch-19 = 3.466382622718811e-06

Training epoch-40 batch-20
Running loss of epoch-40 batch-20 = 8.029630407691002e-06

Training epoch-40 batch-21
Running loss of epoch-40 batch-21 = 2.795504406094551e-05

Training epoch-40 batch-22
Running loss of epoch-40 batch-22 = 1.547718420624733e-05

Training epoch-40 batch-23
Running loss of epoch-40 batch-23 = 1.901201903820038e-05

Training epoch-40 batch-24
Running loss of epoch-40 batch-24 = 5.6960852816700935e-06

Training epoch-40 batch-25
Running loss of epoch-40 batch-25 = 6.978167220950127e-06

Training epoch-40 batch-26
Running loss of epoch-40 batch-26 = 4.233326762914658e-06

Training epoch-40 batch-27
Running loss of epoch-40 batch-27 = 2.1247658878564835e-05

Training epoch-40 batch-28
Running loss of epoch-40 batch-28 = 5.458248779177666e-06

Training epoch-40 batch-29
Running loss of epoch-40 batch-29 = 8.660485036671162e-06

Training epoch-40 batch-30
Running loss of epoch-40 batch-30 = 3.5858480259776115e-05

Training epoch-40 batch-31
Running loss of epoch-40 batch-31 = 7.029855623841286e-06

Training epoch-40 batch-32
Running loss of epoch-40 batch-32 = 2.1360348910093307e-05

Training epoch-40 batch-33
Running loss of epoch-40 batch-33 = 4.289485514163971e-05

Training epoch-40 batch-34
Running loss of epoch-40 batch-34 = 7.419846951961517e-06

Training epoch-40 batch-35
Running loss of epoch-40 batch-35 = 2.1524028852581978e-05

Training epoch-40 batch-36
Running loss of epoch-40 batch-36 = 3.227265551686287e-06

Training epoch-40 batch-37
Running loss of epoch-40 batch-37 = 6.577465683221817e-06

Training epoch-40 batch-38
Running loss of epoch-40 batch-38 = 7.320893928408623e-06

Training epoch-40 batch-39
Running loss of epoch-40 batch-39 = 2.460367977619171e-05

Training epoch-40 batch-40
Running loss of epoch-40 batch-40 = 4.129484295845032e-06

Training epoch-40 batch-41
Running loss of epoch-40 batch-41 = 9.099254384636879e-06

Training epoch-40 batch-42
Running loss of epoch-40 batch-42 = 4.274770617485046e-06

Training epoch-40 batch-43
Running loss of epoch-40 batch-43 = 5.662906914949417e-06

Training epoch-40 batch-44
Running loss of epoch-40 batch-44 = 9.085750207304955e-06

Training epoch-40 batch-45
Running loss of epoch-40 batch-45 = 9.617535397410393e-06

Training epoch-40 batch-46
Running loss of epoch-40 batch-46 = 6.6622160375118256e-06

Training epoch-40 batch-47
Running loss of epoch-40 batch-47 = 1.651630736887455e-05

Training epoch-40 batch-48
Running loss of epoch-40 batch-48 = 8.919276297092438e-06

Training epoch-40 batch-49
Running loss of epoch-40 batch-49 = 7.351161912083626e-06

Training epoch-40 batch-50
Running loss of epoch-40 batch-50 = 1.0842457413673401e-05

Training epoch-40 batch-51
Running loss of epoch-40 batch-51 = 6.305519491434097e-06

Training epoch-40 batch-52
Running loss of epoch-40 batch-52 = 1.254281960427761e-05

Training epoch-40 batch-53
Running loss of epoch-40 batch-53 = 1.750071533024311e-05

Training epoch-40 batch-54
Running loss of epoch-40 batch-54 = 3.8011930882930756e-06

Training epoch-40 batch-55
Running loss of epoch-40 batch-55 = 1.0179821401834488e-05

Training epoch-40 batch-56
Running loss of epoch-40 batch-56 = 5.5031850934028625e-06

Training epoch-40 batch-57
Running loss of epoch-40 batch-57 = 1.3469019904732704e-05

Training epoch-40 batch-58
Running loss of epoch-40 batch-58 = 4.054047167301178e-06

Training epoch-40 batch-59
Running loss of epoch-40 batch-59 = 8.688890375196934e-06

Training epoch-40 batch-60
Running loss of epoch-40 batch-60 = 7.666880264878273e-06

Training epoch-40 batch-61
Running loss of epoch-40 batch-61 = 5.66127710044384e-06

Training epoch-40 batch-62
Running loss of epoch-40 batch-62 = 1.2143515050411224e-05

Training epoch-40 batch-63
Running loss of epoch-40 batch-63 = 8.696457371115685e-06

Training epoch-40 batch-64
Running loss of epoch-40 batch-64 = 1.4533521607518196e-05

Training epoch-40 batch-65
Running loss of epoch-40 batch-65 = 7.777009159326553e-06

Training epoch-40 batch-66
Running loss of epoch-40 batch-66 = 8.634757250547409e-06

Training epoch-40 batch-67
Running loss of epoch-40 batch-67 = 5.799345672130585e-06

Training epoch-40 batch-68
Running loss of epoch-40 batch-68 = 7.87968747317791e-06

Training epoch-40 batch-69
Running loss of epoch-40 batch-69 = 1.0172603651881218e-05

Training epoch-40 batch-70
Running loss of epoch-40 batch-70 = 1.4074146747589111e-05

Training epoch-40 batch-71
Running loss of epoch-40 batch-71 = 1.3948185369372368e-05

Training epoch-40 batch-72
Running loss of epoch-40 batch-72 = 8.563045412302017e-06

Training epoch-40 batch-73
Running loss of epoch-40 batch-73 = 6.696907803416252e-06

Training epoch-40 batch-74
Running loss of epoch-40 batch-74 = 1.0245712473988533e-05

Training epoch-40 batch-75
Running loss of epoch-40 batch-75 = 1.5253433957695961e-05

Training epoch-40 batch-76
Running loss of epoch-40 batch-76 = 3.8691796362400055e-06

Training epoch-40 batch-77
Running loss of epoch-40 batch-77 = 6.897607818245888e-06

Training epoch-40 batch-78
Running loss of epoch-40 batch-78 = 1.2233853340148926e-05

Training epoch-40 batch-79
Running loss of epoch-40 batch-79 = 6.11552968621254e-06

Training epoch-40 batch-80
Running loss of epoch-40 batch-80 = 9.017763659358025e-06

Training epoch-40 batch-81
Running loss of epoch-40 batch-81 = 3.1203962862491608e-06

Training epoch-40 batch-82
Running loss of epoch-40 batch-82 = 7.177935913205147e-06

Training epoch-40 batch-83
Running loss of epoch-40 batch-83 = 4.490837454795837e-06

Training epoch-40 batch-84
Running loss of epoch-40 batch-84 = 9.434996172785759e-06

Training epoch-40 batch-85
Running loss of epoch-40 batch-85 = 4.178611561655998e-06

Training epoch-40 batch-86
Running loss of epoch-40 batch-86 = 5.645910277962685e-06

Training epoch-40 batch-87
Running loss of epoch-40 batch-87 = 5.164183676242828e-06

Training epoch-40 batch-88
Running loss of epoch-40 batch-88 = 1.037190668284893e-05

Training epoch-40 batch-89
Running loss of epoch-40 batch-89 = 1.1874712072312832e-05

Training epoch-40 batch-90
Running loss of epoch-40 batch-90 = 8.354661986231804e-06

Training epoch-40 batch-91
Running loss of epoch-40 batch-91 = 1.0251067578792572e-05

Training epoch-40 batch-92
Running loss of epoch-40 batch-92 = 8.22450965642929e-06

Training epoch-40 batch-93
Running loss of epoch-40 batch-93 = 7.778173312544823e-06

Training epoch-40 batch-94
Running loss of epoch-40 batch-94 = 6.550224497914314e-06

Training epoch-40 batch-95
Running loss of epoch-40 batch-95 = 7.979338988661766e-06

Training epoch-40 batch-96
Running loss of epoch-40 batch-96 = 9.136274456977844e-06

Training epoch-40 batch-97
Running loss of epoch-40 batch-97 = 1.278705894947052e-05

Training epoch-40 batch-98
Running loss of epoch-40 batch-98 = 6.659887731075287e-06

Training epoch-40 batch-99
Running loss of epoch-40 batch-99 = 1.7658574506640434e-05

Training epoch-40 batch-100
Running loss of epoch-40 batch-100 = 5.069654434919357e-06

Training epoch-40 batch-101
Running loss of epoch-40 batch-101 = 1.7080223187804222e-05

Training epoch-40 batch-102
Running loss of epoch-40 batch-102 = 1.1833617463707924e-05

Training epoch-40 batch-103
Running loss of epoch-40 batch-103 = 1.0200543329119682e-05

Training epoch-40 batch-104
Running loss of epoch-40 batch-104 = 2.2919615730643272e-05

Training epoch-40 batch-105
Running loss of epoch-40 batch-105 = 1.1974014341831207e-05

Training epoch-40 batch-106
Running loss of epoch-40 batch-106 = 7.61914998292923e-06

Training epoch-40 batch-107
Running loss of epoch-40 batch-107 = 1.1920928955078125e-05

Training epoch-40 batch-108
Running loss of epoch-40 batch-108 = 7.522990927100182e-06

Training epoch-40 batch-109
Running loss of epoch-40 batch-109 = 4.287576302886009e-06

Training epoch-40 batch-110
Running loss of epoch-40 batch-110 = 1.3320241123437881e-05

Training epoch-40 batch-111
Running loss of epoch-40 batch-111 = 5.03193587064743e-06

Training epoch-40 batch-112
Running loss of epoch-40 batch-112 = 1.338939182460308e-05

Training epoch-40 batch-113
Running loss of epoch-40 batch-113 = 7.273629307746887e-06

Training epoch-40 batch-114
Running loss of epoch-40 batch-114 = 8.181901648640633e-06

Training epoch-40 batch-115
Running loss of epoch-40 batch-115 = 4.428671672940254e-06

Training epoch-40 batch-116
Running loss of epoch-40 batch-116 = 7.276423275470734e-06

Training epoch-40 batch-117
Running loss of epoch-40 batch-117 = 9.709969162940979e-06

Training epoch-40 batch-118
Running loss of epoch-40 batch-118 = 1.2867618352174759e-05

Training epoch-40 batch-119
Running loss of epoch-40 batch-119 = 1.058122143149376e-05

Training epoch-40 batch-120
Running loss of epoch-40 batch-120 = 1.1287163943052292e-05

Training epoch-40 batch-121
Running loss of epoch-40 batch-121 = 1.1132797226309776e-05

Training epoch-40 batch-122
Running loss of epoch-40 batch-122 = 2.360367216169834e-05

Training epoch-40 batch-123
Running loss of epoch-40 batch-123 = 1.6427133232355118e-05

Training epoch-40 batch-124
Running loss of epoch-40 batch-124 = 1.5850644558668137e-05

Training epoch-40 batch-125
Running loss of epoch-40 batch-125 = 1.0671094059944153e-05

Training epoch-40 batch-126
Running loss of epoch-40 batch-126 = 8.171889930963516e-06

Training epoch-40 batch-127
Running loss of epoch-40 batch-127 = 8.489005267620087e-06

Training epoch-40 batch-128
Running loss of epoch-40 batch-128 = 2.800486981868744e-06

Training epoch-40 batch-129
Running loss of epoch-40 batch-129 = 2.8127804398536682e-05

Training epoch-40 batch-130
Running loss of epoch-40 batch-130 = 1.2660166248679161e-05

Training epoch-40 batch-131
Running loss of epoch-40 batch-131 = 6.2354374676942825e-06

Training epoch-40 batch-132
Running loss of epoch-40 batch-132 = 4.58606518805027e-06

Training epoch-40 batch-133
Running loss of epoch-40 batch-133 = 1.3958662748336792e-05

Training epoch-40 batch-134
Running loss of epoch-40 batch-134 = 5.076872184872627e-06

Training epoch-40 batch-135
Running loss of epoch-40 batch-135 = 7.572118192911148e-06

Training epoch-40 batch-136
Running loss of epoch-40 batch-136 = 7.072463631629944e-06

Training epoch-40 batch-137
Running loss of epoch-40 batch-137 = 4.365341737866402e-06

Training epoch-40 batch-138
Running loss of epoch-40 batch-138 = 8.645933121442795e-06

Training epoch-40 batch-139
Running loss of epoch-40 batch-139 = 6.116461008787155e-06

Training epoch-40 batch-140
Running loss of epoch-40 batch-140 = 1.8843915313482285e-05

Training epoch-40 batch-141
Running loss of epoch-40 batch-141 = 9.328126907348633e-06

Training epoch-40 batch-142
Running loss of epoch-40 batch-142 = 7.703201845288277e-06

Training epoch-40 batch-143
Running loss of epoch-40 batch-143 = 1.1560274288058281e-05

Training epoch-40 batch-144
Running loss of epoch-40 batch-144 = 1.319916918873787e-05

Training epoch-40 batch-145
Running loss of epoch-40 batch-145 = 7.298542186617851e-06

Training epoch-40 batch-146
Running loss of epoch-40 batch-146 = 1.0952353477478027e-05

Training epoch-40 batch-147
Running loss of epoch-40 batch-147 = 2.5419285520911217e-05

Training epoch-40 batch-148
Running loss of epoch-40 batch-148 = 7.978174835443497e-06

Training epoch-40 batch-149
Running loss of epoch-40 batch-149 = 7.803086191415787e-06

Training epoch-40 batch-150
Running loss of epoch-40 batch-150 = 4.675285890698433e-05

Training epoch-40 batch-151
Running loss of epoch-40 batch-151 = 4.994915798306465e-06

Training epoch-40 batch-152
Running loss of epoch-40 batch-152 = 8.272938430309296e-06

Training epoch-40 batch-153
Running loss of epoch-40 batch-153 = 1.2604519724845886e-05

Training epoch-40 batch-154
Running loss of epoch-40 batch-154 = 1.1011026799678802e-05

Training epoch-40 batch-155
Running loss of epoch-40 batch-155 = 4.123663529753685e-06

Training epoch-40 batch-156
Running loss of epoch-40 batch-156 = 5.788635462522507e-06

Training epoch-40 batch-157
Running loss of epoch-40 batch-157 = 1.4342367649078369e-05

Finished training epoch-40.



Average train loss at epoch-40 = 1.039297953248024e-05

Started Evaluation

Average val loss at epoch-40 = 1.0465327697451787

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 60.73 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 55.83 %
Accuracy for class execute is: 44.58 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.14 %

Finished Evaluation



Started training epoch-41


Training epoch-41 batch-1
Running loss of epoch-41 batch-1 = 9.801238775253296e-06

Training epoch-41 batch-2
Running loss of epoch-41 batch-2 = 1.4238758012652397e-05

Training epoch-41 batch-3
Running loss of epoch-41 batch-3 = 4.990492016077042e-06

Training epoch-41 batch-4
Running loss of epoch-41 batch-4 = 5.615642294287682e-06

Training epoch-41 batch-5
Running loss of epoch-41 batch-5 = 9.565846994519234e-06

Training epoch-41 batch-6
Running loss of epoch-41 batch-6 = 1.494772732257843e-05

Training epoch-41 batch-7
Running loss of epoch-41 batch-7 = 5.74672594666481e-06

Training epoch-41 batch-8
Running loss of epoch-41 batch-8 = 9.533949196338654e-06

Training epoch-41 batch-9
Running loss of epoch-41 batch-9 = 7.059890776872635e-06

Training epoch-41 batch-10
Running loss of epoch-41 batch-10 = 9.643146768212318e-06

Training epoch-41 batch-11
Running loss of epoch-41 batch-11 = 5.6622084230184555e-06

Training epoch-41 batch-12
Running loss of epoch-41 batch-12 = 1.5311874449253082e-05

Training epoch-41 batch-13
Running loss of epoch-41 batch-13 = 8.07340256869793e-06

Training epoch-41 batch-14
Running loss of epoch-41 batch-14 = 2.0687002688646317e-05

Training epoch-41 batch-15
Running loss of epoch-41 batch-15 = 3.822147846221924e-06

Training epoch-41 batch-16
Running loss of epoch-41 batch-16 = 8.902046829462051e-06

Training epoch-41 batch-17
Running loss of epoch-41 batch-17 = 6.685731932520866e-06

Training epoch-41 batch-18
Running loss of epoch-41 batch-18 = 1.3354700058698654e-05

Training epoch-41 batch-19
Running loss of epoch-41 batch-19 = 6.317393854260445e-06

Training epoch-41 batch-20
Running loss of epoch-41 batch-20 = 7.998663932085037e-06

Training epoch-41 batch-21
Running loss of epoch-41 batch-21 = 2.5900546461343765e-05

Training epoch-41 batch-22
Running loss of epoch-41 batch-22 = 8.41403380036354e-06

Training epoch-41 batch-23
Running loss of epoch-41 batch-23 = 1.938524655997753e-05

Training epoch-41 batch-24
Running loss of epoch-41 batch-24 = 1.3248296454548836e-05

Training epoch-41 batch-25
Running loss of epoch-41 batch-25 = 1.1825701221823692e-05

Training epoch-41 batch-26
Running loss of epoch-41 batch-26 = 3.232387825846672e-06

Training epoch-41 batch-27
Running loss of epoch-41 batch-27 = 5.032401531934738e-06

Training epoch-41 batch-28
Running loss of epoch-41 batch-28 = 1.0756542906165123e-05

Training epoch-41 batch-29
Running loss of epoch-41 batch-29 = 6.177928298711777e-06

Training epoch-41 batch-30
Running loss of epoch-41 batch-30 = 3.6533456295728683e-06

Training epoch-41 batch-31
Running loss of epoch-41 batch-31 = 6.793532520532608e-06

Training epoch-41 batch-32
Running loss of epoch-41 batch-32 = 2.803746610879898e-06

Training epoch-41 batch-33
Running loss of epoch-41 batch-33 = 1.3627344742417336e-05

Training epoch-41 batch-34
Running loss of epoch-41 batch-34 = 1.1147232726216316e-05

Training epoch-41 batch-35
Running loss of epoch-41 batch-35 = 8.742325007915497e-06

Training epoch-41 batch-36
Running loss of epoch-41 batch-36 = 5.660112947225571e-06

Training epoch-41 batch-37
Running loss of epoch-41 batch-37 = 1.4328397810459137e-05

Training epoch-41 batch-38
Running loss of epoch-41 batch-38 = 9.798677638173103e-06

Training epoch-41 batch-39
Running loss of epoch-41 batch-39 = 1.5744706615805626e-05

Training epoch-41 batch-40
Running loss of epoch-41 batch-40 = 7.336260750889778e-06

Training epoch-41 batch-41
Running loss of epoch-41 batch-41 = 7.484806701540947e-06

Training epoch-41 batch-42
Running loss of epoch-41 batch-42 = 8.095521479845047e-06

Training epoch-41 batch-43
Running loss of epoch-41 batch-43 = 8.231028914451599e-06

Training epoch-41 batch-44
Running loss of epoch-41 batch-44 = 5.138805136084557e-06

Training epoch-41 batch-45
Running loss of epoch-41 batch-45 = 1.766742207109928e-05

Training epoch-41 batch-46
Running loss of epoch-41 batch-46 = 9.793555364012718e-06

Training epoch-41 batch-47
Running loss of epoch-41 batch-47 = 4.871981218457222e-06

Training epoch-41 batch-48
Running loss of epoch-41 batch-48 = 4.395376890897751e-06

Training epoch-41 batch-49
Running loss of epoch-41 batch-49 = 5.057314410805702e-06

Training epoch-41 batch-50
Running loss of epoch-41 batch-50 = 6.2445178627967834e-06

Training epoch-41 batch-51
Running loss of epoch-41 batch-51 = 9.602867066860199e-06

Training epoch-41 batch-52
Running loss of epoch-41 batch-52 = 7.817288860678673e-06

Training epoch-41 batch-53
Running loss of epoch-41 batch-53 = 2.598739229142666e-05

Training epoch-41 batch-54
Running loss of epoch-41 batch-54 = 4.578614607453346e-06

Training epoch-41 batch-55
Running loss of epoch-41 batch-55 = 4.308763891458511e-06

Training epoch-41 batch-56
Running loss of epoch-41 batch-56 = 5.398644134402275e-06

Training epoch-41 batch-57
Running loss of epoch-41 batch-57 = 1.1580530554056168e-05

Training epoch-41 batch-58
Running loss of epoch-41 batch-58 = 9.948154911398888e-06

Training epoch-41 batch-59
Running loss of epoch-41 batch-59 = 6.525078788399696e-06

Training epoch-41 batch-60
Running loss of epoch-41 batch-60 = 4.92902472615242e-06

Training epoch-41 batch-61
Running loss of epoch-41 batch-61 = 6.866641342639923e-06

Training epoch-41 batch-62
Running loss of epoch-41 batch-62 = 4.782341420650482e-06

Training epoch-41 batch-63
Running loss of epoch-41 batch-63 = 1.181475818157196e-05

Training epoch-41 batch-64
Running loss of epoch-41 batch-64 = 7.764669135212898e-06

Training epoch-41 batch-65
Running loss of epoch-41 batch-65 = 5.713198333978653e-06

Training epoch-41 batch-66
Running loss of epoch-41 batch-66 = 8.421717211604118e-06

Training epoch-41 batch-67
Running loss of epoch-41 batch-67 = 8.535338565707207e-06

Training epoch-41 batch-68
Running loss of epoch-41 batch-68 = 2.0335428416728973e-06

Training epoch-41 batch-69
Running loss of epoch-41 batch-69 = 7.016817107796669e-06

Training epoch-41 batch-70
Running loss of epoch-41 batch-70 = 1.0735355317592621e-05

Training epoch-41 batch-71
Running loss of epoch-41 batch-71 = 6.694812327623367e-06

Training epoch-41 batch-72
Running loss of epoch-41 batch-72 = 9.58726741373539e-06

Training epoch-41 batch-73
Running loss of epoch-41 batch-73 = 5.137873813509941e-06

Training epoch-41 batch-74
Running loss of epoch-41 batch-74 = 1.2443400919437408e-05

Training epoch-41 batch-75
Running loss of epoch-41 batch-75 = 7.160124368965626e-06

Training epoch-41 batch-76
Running loss of epoch-41 batch-76 = 6.575603038072586e-06

Training epoch-41 batch-77
Running loss of epoch-41 batch-77 = 1.074071042239666e-05

Training epoch-41 batch-78
Running loss of epoch-41 batch-78 = 2.5780173018574715e-05

Training epoch-41 batch-79
Running loss of epoch-41 batch-79 = 6.886664777994156e-06

Training epoch-41 batch-80
Running loss of epoch-41 batch-80 = 1.2142583727836609e-05

Training epoch-41 batch-81
Running loss of epoch-41 batch-81 = 1.794658601284027e-05

Training epoch-41 batch-82
Running loss of epoch-41 batch-82 = 6.957445293664932e-06

Training epoch-41 batch-83
Running loss of epoch-41 batch-83 = 9.360257536172867e-06

Training epoch-41 batch-84
Running loss of epoch-41 batch-84 = 6.097601726651192e-06

Training epoch-41 batch-85
Running loss of epoch-41 batch-85 = 1.1427560821175575e-05

Training epoch-41 batch-86
Running loss of epoch-41 batch-86 = 1.6397330909967422e-05

Training epoch-41 batch-87
Running loss of epoch-41 batch-87 = 9.020324796438217e-06

Training epoch-41 batch-88
Running loss of epoch-41 batch-88 = 7.313443347811699e-06

Training epoch-41 batch-89
Running loss of epoch-41 batch-89 = 4.143454134464264e-06

Training epoch-41 batch-90
Running loss of epoch-41 batch-90 = 1.0756775736808777e-05

Training epoch-41 batch-91
Running loss of epoch-41 batch-91 = 2.7058180421590805e-05

Training epoch-41 batch-92
Running loss of epoch-41 batch-92 = 1.1303694918751717e-05

Training epoch-41 batch-93
Running loss of epoch-41 batch-93 = 1.787184737622738e-05

Training epoch-41 batch-94
Running loss of epoch-41 batch-94 = 1.098797656595707e-05

Training epoch-41 batch-95
Running loss of epoch-41 batch-95 = 4.161614924669266e-06

Training epoch-41 batch-96
Running loss of epoch-41 batch-96 = 8.180970326066017e-06

Training epoch-41 batch-97
Running loss of epoch-41 batch-97 = 3.5923440009355545e-06

Training epoch-41 batch-98
Running loss of epoch-41 batch-98 = 2.0432518795132637e-05

Training epoch-41 batch-99
Running loss of epoch-41 batch-99 = 1.7801998183131218e-05

Training epoch-41 batch-100
Running loss of epoch-41 batch-100 = 6.552785634994507e-06

Training epoch-41 batch-101
Running loss of epoch-41 batch-101 = 1.0729883797466755e-05

Training epoch-41 batch-102
Running loss of epoch-41 batch-102 = 1.5274621546268463e-05

Training epoch-41 batch-103
Running loss of epoch-41 batch-103 = 2.6854686439037323e-06

Training epoch-41 batch-104
Running loss of epoch-41 batch-104 = 2.1476298570632935e-05

Training epoch-41 batch-105
Running loss of epoch-41 batch-105 = 9.811483323574066e-06

Training epoch-41 batch-106
Running loss of epoch-41 batch-106 = 9.94652509689331e-06

Training epoch-41 batch-107
Running loss of epoch-41 batch-107 = 7.547670975327492e-06

Training epoch-41 batch-108
Running loss of epoch-41 batch-108 = 2.562534064054489e-06

Training epoch-41 batch-109
Running loss of epoch-41 batch-109 = 1.7794081941246986e-05

Training epoch-41 batch-110
Running loss of epoch-41 batch-110 = 1.17418821901083e-05

Training epoch-41 batch-111
Running loss of epoch-41 batch-111 = 9.424518793821335e-06

Training epoch-41 batch-112
Running loss of epoch-41 batch-112 = 2.389354631304741e-05

Training epoch-41 batch-113
Running loss of epoch-41 batch-113 = 1.5218975022435188e-05

Training epoch-41 batch-114
Running loss of epoch-41 batch-114 = 1.2683449313044548e-05

Training epoch-41 batch-115
Running loss of epoch-41 batch-115 = 9.461306035518646e-06

Training epoch-41 batch-116
Running loss of epoch-41 batch-116 = 2.9040267691016197e-05

Training epoch-41 batch-117
Running loss of epoch-41 batch-117 = 6.493180990219116e-06

Training epoch-41 batch-118
Running loss of epoch-41 batch-118 = 3.118067979812622e-06

Training epoch-41 batch-119
Running loss of epoch-41 batch-119 = 8.964678272604942e-06

Training epoch-41 batch-120
Running loss of epoch-41 batch-120 = 7.691560313105583e-06

Training epoch-41 batch-121
Running loss of epoch-41 batch-121 = 1.6267993487417698e-05

Training epoch-41 batch-122
Running loss of epoch-41 batch-122 = 8.641742169857025e-06

Training epoch-41 batch-123
Running loss of epoch-41 batch-123 = 2.7085188776254654e-06

Training epoch-41 batch-124
Running loss of epoch-41 batch-124 = 8.319271728396416e-06

Training epoch-41 batch-125
Running loss of epoch-41 batch-125 = 1.116073690354824e-05

Training epoch-41 batch-126
Running loss of epoch-41 batch-126 = 9.009148925542831e-06

Training epoch-41 batch-127
Running loss of epoch-41 batch-127 = 1.721736043691635e-05

Training epoch-41 batch-128
Running loss of epoch-41 batch-128 = 5.912734195590019e-06

Training epoch-41 batch-129
Running loss of epoch-41 batch-129 = 1.206202432513237e-05

Training epoch-41 batch-130
Running loss of epoch-41 batch-130 = 2.133217640221119e-05

Training epoch-41 batch-131
Running loss of epoch-41 batch-131 = 5.340436473488808e-06

Training epoch-41 batch-132
Running loss of epoch-41 batch-132 = 2.5562942028045654e-05

Training epoch-41 batch-133
Running loss of epoch-41 batch-133 = 4.420056939125061e-06

Training epoch-41 batch-134
Running loss of epoch-41 batch-134 = 9.810319170355797e-06

Training epoch-41 batch-135
Running loss of epoch-41 batch-135 = 7.857102900743484e-06

Training epoch-41 batch-136
Running loss of epoch-41 batch-136 = 1.3372162356972694e-05

Training epoch-41 batch-137
Running loss of epoch-41 batch-137 = 5.209119990468025e-06

Training epoch-41 batch-138
Running loss of epoch-41 batch-138 = 1.4805467799305916e-05

Training epoch-41 batch-139
Running loss of epoch-41 batch-139 = 8.333707228302956e-06

Training epoch-41 batch-140
Running loss of epoch-41 batch-140 = 8.28644260764122e-06

Training epoch-41 batch-141
Running loss of epoch-41 batch-141 = 1.432409044355154e-05

Training epoch-41 batch-142
Running loss of epoch-41 batch-142 = 1.975905615836382e-05

Training epoch-41 batch-143
Running loss of epoch-41 batch-143 = 6.0549937188625336e-06

Training epoch-41 batch-144
Running loss of epoch-41 batch-144 = 4.52226959168911e-06

Training epoch-41 batch-145
Running loss of epoch-41 batch-145 = 5.703652277588844e-06

Training epoch-41 batch-146
Running loss of epoch-41 batch-146 = 7.868977263569832e-06

Training epoch-41 batch-147
Running loss of epoch-41 batch-147 = 6.368616595864296e-06

Training epoch-41 batch-148
Running loss of epoch-41 batch-148 = 8.52113589644432e-06

Training epoch-41 batch-149
Running loss of epoch-41 batch-149 = 6.181187927722931e-06

Training epoch-41 batch-150
Running loss of epoch-41 batch-150 = 1.2340722605586052e-05

Training epoch-41 batch-151
Running loss of epoch-41 batch-151 = 5.037756636738777e-06

Training epoch-41 batch-152
Running loss of epoch-41 batch-152 = 5.780253559350967e-06

Training epoch-41 batch-153
Running loss of epoch-41 batch-153 = 7.541617378592491e-06

Training epoch-41 batch-154
Running loss of epoch-41 batch-154 = 5.427980795502663e-06

Training epoch-41 batch-155
Running loss of epoch-41 batch-155 = 1.680990681052208e-05

Training epoch-41 batch-156
Running loss of epoch-41 batch-156 = 4.689441993832588e-06

Training epoch-41 batch-157
Running loss of epoch-41 batch-157 = 8.948519825935364e-05

Finished training epoch-41.



Average train loss at epoch-41 = 1.007305458188057e-05

Started Evaluation

Average val loss at epoch-41 = 1.0429878536644872

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.05 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 55.38 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 72.56 %

Overall Accuracy = 83.21 %

Finished Evaluation



Started training epoch-42


Training epoch-42 batch-1
Running loss of epoch-42 batch-1 = 1.1052470654249191e-05

Training epoch-42 batch-2
Running loss of epoch-42 batch-2 = 1.1695316061377525e-05

Training epoch-42 batch-3
Running loss of epoch-42 batch-3 = 8.327653631567955e-06

Training epoch-42 batch-4
Running loss of epoch-42 batch-4 = 1.0604038834571838e-05

Training epoch-42 batch-5
Running loss of epoch-42 batch-5 = 5.857553333044052e-06

Training epoch-42 batch-6
Running loss of epoch-42 batch-6 = 7.617520168423653e-06

Training epoch-42 batch-7
Running loss of epoch-42 batch-7 = 2.0971987396478653e-05

Training epoch-42 batch-8
Running loss of epoch-42 batch-8 = 1.2156087905168533e-05

Training epoch-42 batch-9
Running loss of epoch-42 batch-9 = 1.5357742086052895e-05

Training epoch-42 batch-10
Running loss of epoch-42 batch-10 = 9.839655831456184e-06

Training epoch-42 batch-11
Running loss of epoch-42 batch-11 = 2.0365696400403976e-06

Training epoch-42 batch-12
Running loss of epoch-42 batch-12 = 8.174218237400055e-06

Training epoch-42 batch-13
Running loss of epoch-42 batch-13 = 3.752065822482109e-06

Training epoch-42 batch-14
Running loss of epoch-42 batch-14 = 1.0212883353233337e-05

Training epoch-42 batch-15
Running loss of epoch-42 batch-15 = 4.651956260204315e-06

Training epoch-42 batch-16
Running loss of epoch-42 batch-16 = 7.79842957854271e-06

Training epoch-42 batch-17
Running loss of epoch-42 batch-17 = 7.109483703970909e-06

Training epoch-42 batch-18
Running loss of epoch-42 batch-18 = 8.213799446821213e-06

Training epoch-42 batch-19
Running loss of epoch-42 batch-19 = 4.9388036131858826e-06

Training epoch-42 batch-20
Running loss of epoch-42 batch-20 = 4.6137720346450806e-06

Training epoch-42 batch-21
Running loss of epoch-42 batch-21 = 7.773516699671745e-06

Training epoch-42 batch-22
Running loss of epoch-42 batch-22 = 4.843808710575104e-06

Training epoch-42 batch-23
Running loss of epoch-42 batch-23 = 6.031244993209839e-06

Training epoch-42 batch-24
Running loss of epoch-42 batch-24 = 1.4211051166057587e-05

Training epoch-42 batch-25
Running loss of epoch-42 batch-25 = 1.183070708066225e-05

Training epoch-42 batch-26
Running loss of epoch-42 batch-26 = 1.7124461010098457e-05

Training epoch-42 batch-27
Running loss of epoch-42 batch-27 = 2.866378054022789e-06

Training epoch-42 batch-28
Running loss of epoch-42 batch-28 = 1.4677410945296288e-05

Training epoch-42 batch-29
Running loss of epoch-42 batch-29 = 7.700640708208084e-06

Training epoch-42 batch-30
Running loss of epoch-42 batch-30 = 4.522036761045456e-06

Training epoch-42 batch-31
Running loss of epoch-42 batch-31 = 3.6098062992095947e-06

Training epoch-42 batch-32
Running loss of epoch-42 batch-32 = 8.378061465919018e-06

Training epoch-42 batch-33
Running loss of epoch-42 batch-33 = 5.479669198393822e-06

Training epoch-42 batch-34
Running loss of epoch-42 batch-34 = 4.4959597289562225e-06

Training epoch-42 batch-35
Running loss of epoch-42 batch-35 = 7.706927135586739e-06

Training epoch-42 batch-36
Running loss of epoch-42 batch-36 = 7.468974217772484e-06

Training epoch-42 batch-37
Running loss of epoch-42 batch-37 = 7.72508792579174e-06

Training epoch-42 batch-38
Running loss of epoch-42 batch-38 = 2.5431858375668526e-05

Training epoch-42 batch-39
Running loss of epoch-42 batch-39 = 2.8242357075214386e-06

Training epoch-42 batch-40
Running loss of epoch-42 batch-40 = 6.1070313677191734e-06

Training epoch-42 batch-41
Running loss of epoch-42 batch-41 = 8.814968168735504e-06

Training epoch-42 batch-42
Running loss of epoch-42 batch-42 = 3.3578835427761078e-06

Training epoch-42 batch-43
Running loss of epoch-42 batch-43 = 7.3390547186136246e-06

Training epoch-42 batch-44
Running loss of epoch-42 batch-44 = 1.0248972102999687e-05

Training epoch-42 batch-45
Running loss of epoch-42 batch-45 = 1.296098344027996e-05

Training epoch-42 batch-46
Running loss of epoch-42 batch-46 = 5.284324288368225e-06

Training epoch-42 batch-47
Running loss of epoch-42 batch-47 = 6.8927183747291565e-06

Training epoch-42 batch-48
Running loss of epoch-42 batch-48 = 8.980045095086098e-06

Training epoch-42 batch-49
Running loss of epoch-42 batch-49 = 5.457084625959396e-06

Training epoch-42 batch-50
Running loss of epoch-42 batch-50 = 7.206108421087265e-06

Training epoch-42 batch-51
Running loss of epoch-42 batch-51 = 5.1693059504032135e-06

Training epoch-42 batch-52
Running loss of epoch-42 batch-52 = 4.824018105864525e-06

Training epoch-42 batch-53
Running loss of epoch-42 batch-53 = 1.2377044185996056e-05

Training epoch-42 batch-54
Running loss of epoch-42 batch-54 = 5.181413143873215e-06

Training epoch-42 batch-55
Running loss of epoch-42 batch-55 = 6.128102540969849e-06

Training epoch-42 batch-56
Running loss of epoch-42 batch-56 = 4.2887404561042786e-06

Training epoch-42 batch-57
Running loss of epoch-42 batch-57 = 8.544884622097015e-06

Training epoch-42 batch-58
Running loss of epoch-42 batch-58 = 6.787478923797607e-06

Training epoch-42 batch-59
Running loss of epoch-42 batch-59 = 1.1887168511748314e-05

Training epoch-42 batch-60
Running loss of epoch-42 batch-60 = 1.7801299691200256e-05

Training epoch-42 batch-61
Running loss of epoch-42 batch-61 = 9.34838317334652e-06

Training epoch-42 batch-62
Running loss of epoch-42 batch-62 = 2.0145438611507416e-05

Training epoch-42 batch-63
Running loss of epoch-42 batch-63 = 1.171790063381195e-05

Training epoch-42 batch-64
Running loss of epoch-42 batch-64 = 1.4605466276407242e-05

Training epoch-42 batch-65
Running loss of epoch-42 batch-65 = 1.1121854186058044e-05

Training epoch-42 batch-66
Running loss of epoch-42 batch-66 = 7.253256626427174e-06

Training epoch-42 batch-67
Running loss of epoch-42 batch-67 = 1.1580530554056168e-05

Training epoch-42 batch-68
Running loss of epoch-42 batch-68 = 6.688060238957405e-06

Training epoch-42 batch-69
Running loss of epoch-42 batch-69 = 1.0557007044553757e-05

Training epoch-42 batch-70
Running loss of epoch-42 batch-70 = 8.299946784973145e-06

Training epoch-42 batch-71
Running loss of epoch-42 batch-71 = 1.659034751355648e-05

Training epoch-42 batch-72
Running loss of epoch-42 batch-72 = 1.5627825632691383e-05

Training epoch-42 batch-73
Running loss of epoch-42 batch-73 = 6.807735189795494e-06

Training epoch-42 batch-74
Running loss of epoch-42 batch-74 = 7.302500307559967e-06

Training epoch-42 batch-75
Running loss of epoch-42 batch-75 = 5.943933501839638e-06

Training epoch-42 batch-76
Running loss of epoch-42 batch-76 = 9.171664714813232e-06

Training epoch-42 batch-77
Running loss of epoch-42 batch-77 = 1.3141543604433537e-05

Training epoch-42 batch-78
Running loss of epoch-42 batch-78 = 7.6016876846551895e-06

Training epoch-42 batch-79
Running loss of epoch-42 batch-79 = 4.241941496729851e-06

Training epoch-42 batch-80
Running loss of epoch-42 batch-80 = 6.852205842733383e-06

Training epoch-42 batch-81
Running loss of epoch-42 batch-81 = 1.0232208296656609e-05

Training epoch-42 batch-82
Running loss of epoch-42 batch-82 = 5.8724544942379e-06

Training epoch-42 batch-83
Running loss of epoch-42 batch-83 = 1.3836659491062164e-05

Training epoch-42 batch-84
Running loss of epoch-42 batch-84 = 1.8192455172538757e-05

Training epoch-42 batch-85
Running loss of epoch-42 batch-85 = 1.2220116332173347e-05

Training epoch-42 batch-86
Running loss of epoch-42 batch-86 = 2.4551991373300552e-06

Training epoch-42 batch-87
Running loss of epoch-42 batch-87 = 2.8384383767843246e-06

Training epoch-42 batch-88
Running loss of epoch-42 batch-88 = 4.63088508695364e-06

Training epoch-42 batch-89
Running loss of epoch-42 batch-89 = 1.4204299077391624e-05

Training epoch-42 batch-90
Running loss of epoch-42 batch-90 = 7.650814950466156e-06

Training epoch-42 batch-91
Running loss of epoch-42 batch-91 = 1.6730977222323418e-05

Training epoch-42 batch-92
Running loss of epoch-42 batch-92 = 4.631001502275467e-06

Training epoch-42 batch-93
Running loss of epoch-42 batch-93 = 6.986083462834358e-06

Training epoch-42 batch-94
Running loss of epoch-42 batch-94 = 9.383773431181908e-06

Training epoch-42 batch-95
Running loss of epoch-42 batch-95 = 8.206116035580635e-06

Training epoch-42 batch-96
Running loss of epoch-42 batch-96 = 3.948807716369629e-06

Training epoch-42 batch-97
Running loss of epoch-42 batch-97 = 1.0046642273664474e-05

Training epoch-42 batch-98
Running loss of epoch-42 batch-98 = 4.198169335722923e-06

Training epoch-42 batch-99
Running loss of epoch-42 batch-99 = 3.6046840250492096e-06

Training epoch-42 batch-100
Running loss of epoch-42 batch-100 = 1.1531170457601547e-05

Training epoch-42 batch-101
Running loss of epoch-42 batch-101 = 1.7710844986140728e-05

Training epoch-42 batch-102
Running loss of epoch-42 batch-102 = 3.977213054895401e-06

Training epoch-42 batch-103
Running loss of epoch-42 batch-103 = 1.5110243111848831e-05

Training epoch-42 batch-104
Running loss of epoch-42 batch-104 = 1.1338386684656143e-05

Training epoch-42 batch-105
Running loss of epoch-42 batch-105 = 9.0824905782938e-06

Training epoch-42 batch-106
Running loss of epoch-42 batch-106 = 3.854045644402504e-06

Training epoch-42 batch-107
Running loss of epoch-42 batch-107 = 3.7015415728092194e-06

Training epoch-42 batch-108
Running loss of epoch-42 batch-108 = 1.7841346561908722e-05

Training epoch-42 batch-109
Running loss of epoch-42 batch-109 = 2.0591774955391884e-05

Training epoch-42 batch-110
Running loss of epoch-42 batch-110 = 8.083181455731392e-06

Training epoch-42 batch-111
Running loss of epoch-42 batch-111 = 1.9505852833390236e-05

Training epoch-42 batch-112
Running loss of epoch-42 batch-112 = 8.831731975078583e-06

Training epoch-42 batch-113
Running loss of epoch-42 batch-113 = 5.275942385196686e-06

Training epoch-42 batch-114
Running loss of epoch-42 batch-114 = 5.742302164435387e-06

Training epoch-42 batch-115
Running loss of epoch-42 batch-115 = 7.1069225668907166e-06

Training epoch-42 batch-116
Running loss of epoch-42 batch-116 = 7.514376193284988e-06

Training epoch-42 batch-117
Running loss of epoch-42 batch-117 = 1.092907041311264e-05

Training epoch-42 batch-118
Running loss of epoch-42 batch-118 = 3.5658013075590134e-06

Training epoch-42 batch-119
Running loss of epoch-42 batch-119 = 6.5909698605537415e-06

Training epoch-42 batch-120
Running loss of epoch-42 batch-120 = 2.1665822714567184e-05

Training epoch-42 batch-121
Running loss of epoch-42 batch-121 = 9.981449693441391e-06

Training epoch-42 batch-122
Running loss of epoch-42 batch-122 = 1.6223755665123463e-05

Training epoch-42 batch-123
Running loss of epoch-42 batch-123 = 2.6584602892398834e-06

Training epoch-42 batch-124
Running loss of epoch-42 batch-124 = 5.7604629546403885e-06

Training epoch-42 batch-125
Running loss of epoch-42 batch-125 = 1.3420823961496353e-05

Training epoch-42 batch-126
Running loss of epoch-42 batch-126 = 1.0767136700451374e-05

Training epoch-42 batch-127
Running loss of epoch-42 batch-127 = 8.291332051157951e-06

Training epoch-42 batch-128
Running loss of epoch-42 batch-128 = 5.854293704032898e-06

Training epoch-42 batch-129
Running loss of epoch-42 batch-129 = 8.620554581284523e-06

Training epoch-42 batch-130
Running loss of epoch-42 batch-130 = 2.012890763580799e-05

Training epoch-42 batch-131
Running loss of epoch-42 batch-131 = 2.1821819245815277e-05

Training epoch-42 batch-132
Running loss of epoch-42 batch-132 = 8.21962021291256e-06

Training epoch-42 batch-133
Running loss of epoch-42 batch-133 = 8.389586582779884e-06

Training epoch-42 batch-134
Running loss of epoch-42 batch-134 = 1.1293450370430946e-05

Training epoch-42 batch-135
Running loss of epoch-42 batch-135 = 1.6293488442897797e-05

Training epoch-42 batch-136
Running loss of epoch-42 batch-136 = 1.710350625216961e-05

Training epoch-42 batch-137
Running loss of epoch-42 batch-137 = 1.3052020221948624e-05

Training epoch-42 batch-138
Running loss of epoch-42 batch-138 = 5.47594390809536e-06

Training epoch-42 batch-139
Running loss of epoch-42 batch-139 = 5.7944562286138535e-06

Training epoch-42 batch-140
Running loss of epoch-42 batch-140 = 5.331588909029961e-06

Training epoch-42 batch-141
Running loss of epoch-42 batch-141 = 5.2121467888355255e-06

Training epoch-42 batch-142
Running loss of epoch-42 batch-142 = 5.333451554179192e-06

Training epoch-42 batch-143
Running loss of epoch-42 batch-143 = 2.0960113033652306e-05

Training epoch-42 batch-144
Running loss of epoch-42 batch-144 = 3.286730498075485e-05

Training epoch-42 batch-145
Running loss of epoch-42 batch-145 = 1.0914169251918793e-05

Training epoch-42 batch-146
Running loss of epoch-42 batch-146 = 3.9227306842803955e-06

Training epoch-42 batch-147
Running loss of epoch-42 batch-147 = 1.112394966185093e-05

Training epoch-42 batch-148
Running loss of epoch-42 batch-148 = 5.135545507073402e-06

Training epoch-42 batch-149
Running loss of epoch-42 batch-149 = 5.0656963139772415e-06

Training epoch-42 batch-150
Running loss of epoch-42 batch-150 = 1.2638745829463005e-05

Training epoch-42 batch-151
Running loss of epoch-42 batch-151 = 6.95069320499897e-06

Training epoch-42 batch-152
Running loss of epoch-42 batch-152 = 9.161420166492462e-06

Training epoch-42 batch-153
Running loss of epoch-42 batch-153 = 1.034117303788662e-05

Training epoch-42 batch-154
Running loss of epoch-42 batch-154 = 1.2835953384637833e-05

Training epoch-42 batch-155
Running loss of epoch-42 batch-155 = 9.927432984113693e-06

Training epoch-42 batch-156
Running loss of epoch-42 batch-156 = 1.2282747775316238e-05

Training epoch-42 batch-157
Running loss of epoch-42 batch-157 = 6.274133920669556e-05

Finished training epoch-42.



Average train loss at epoch-42 = 9.56277623772621e-06

Started Evaluation

Average val loss at epoch-42 = 1.0684508355463884

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 94.03 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 61.19 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 51.12 %
Accuracy for class execute is: 46.59 %
Accuracy for class get is: 69.23 %

Overall Accuracy = 82.83 %

Finished Evaluation



Started training epoch-43


Training epoch-43 batch-1
Running loss of epoch-43 batch-1 = 6.26174733042717e-06

Training epoch-43 batch-2
Running loss of epoch-43 batch-2 = 1.6898149624466896e-05

Training epoch-43 batch-3
Running loss of epoch-43 batch-3 = 8.564209565520287e-06

Training epoch-43 batch-4
Running loss of epoch-43 batch-4 = 5.885958671569824e-06

Training epoch-43 batch-5
Running loss of epoch-43 batch-5 = 9.471084922552109e-06

Training epoch-43 batch-6
Running loss of epoch-43 batch-6 = 9.038485586643219e-06

Training epoch-43 batch-7
Running loss of epoch-43 batch-7 = 4.2441533878445625e-06

Training epoch-43 batch-8
Running loss of epoch-43 batch-8 = 6.766524165868759e-06

Training epoch-43 batch-9
Running loss of epoch-43 batch-9 = 6.6997017711400986e-06

Training epoch-43 batch-10
Running loss of epoch-43 batch-10 = 9.890645742416382e-06

Training epoch-43 batch-11
Running loss of epoch-43 batch-11 = 7.06082209944725e-06

Training epoch-43 batch-12
Running loss of epoch-43 batch-12 = 9.925803169608116e-06

Training epoch-43 batch-13
Running loss of epoch-43 batch-13 = 1.0086456313729286e-05

Training epoch-43 batch-14
Running loss of epoch-43 batch-14 = 1.0657357051968575e-05

Training epoch-43 batch-15
Running loss of epoch-43 batch-15 = 1.5340978279709816e-05

Training epoch-43 batch-16
Running loss of epoch-43 batch-16 = 1.0513002052903175e-05

Training epoch-43 batch-17
Running loss of epoch-43 batch-17 = 7.1409158408641815e-06

Training epoch-43 batch-18
Running loss of epoch-43 batch-18 = 4.220521077513695e-06

Training epoch-43 batch-19
Running loss of epoch-43 batch-19 = 4.161614924669266e-06

Training epoch-43 batch-20
Running loss of epoch-43 batch-20 = 1.6051344573497772e-06

Training epoch-43 batch-21
Running loss of epoch-43 batch-21 = 3.3455435186624527e-06

Training epoch-43 batch-22
Running loss of epoch-43 batch-22 = 4.56138513982296e-06

Training epoch-43 batch-23
Running loss of epoch-43 batch-23 = 1.1070864275097847e-05

Training epoch-43 batch-24
Running loss of epoch-43 batch-24 = 1.4061108231544495e-05

Training epoch-43 batch-25
Running loss of epoch-43 batch-25 = 1.188390888273716e-05

Training epoch-43 batch-26
Running loss of epoch-43 batch-26 = 1.110415905714035e-05

Training epoch-43 batch-27
Running loss of epoch-43 batch-27 = 9.584706276655197e-06

Training epoch-43 batch-28
Running loss of epoch-43 batch-28 = 6.760237738490105e-06

Training epoch-43 batch-29
Running loss of epoch-43 batch-29 = 2.5631394237279892e-05

Training epoch-43 batch-30
Running loss of epoch-43 batch-30 = 9.829294867813587e-06

Training epoch-43 batch-31
Running loss of epoch-43 batch-31 = 9.055482223629951e-06

Training epoch-43 batch-32
Running loss of epoch-43 batch-32 = 7.081544026732445e-06

Training epoch-43 batch-33
Running loss of epoch-43 batch-33 = 4.98560257256031e-06

Training epoch-43 batch-34
Running loss of epoch-43 batch-34 = 8.415896445512772e-06

Training epoch-43 batch-35
Running loss of epoch-43 batch-35 = 1.3731652870774269e-05

Training epoch-43 batch-36
Running loss of epoch-43 batch-36 = 9.489012882113457e-06

Training epoch-43 batch-37
Running loss of epoch-43 batch-37 = 9.2268455773592e-06

Training epoch-43 batch-38
Running loss of epoch-43 batch-38 = 1.0033836588263512e-05

Training epoch-43 batch-39
Running loss of epoch-43 batch-39 = 1.2774718925356865e-05

Training epoch-43 batch-40
Running loss of epoch-43 batch-40 = 2.400553785264492e-05

Training epoch-43 batch-41
Running loss of epoch-43 batch-41 = 7.3248520493507385e-06

Training epoch-43 batch-42
Running loss of epoch-43 batch-42 = 6.638234481215477e-06

Training epoch-43 batch-43
Running loss of epoch-43 batch-43 = 4.601664841175079e-06

Training epoch-43 batch-44
Running loss of epoch-43 batch-44 = 1.5000347048044205e-05

Training epoch-43 batch-45
Running loss of epoch-43 batch-45 = 1.5551806427538395e-05

Training epoch-43 batch-46
Running loss of epoch-43 batch-46 = 6.312038749456406e-06

Training epoch-43 batch-47
Running loss of epoch-43 batch-47 = 1.5511643141508102e-05

Training epoch-43 batch-48
Running loss of epoch-43 batch-48 = 3.568129613995552e-06

Training epoch-43 batch-49
Running loss of epoch-43 batch-49 = 3.3623073250055313e-06

Training epoch-43 batch-50
Running loss of epoch-43 batch-50 = 8.771196007728577e-06

Training epoch-43 batch-51
Running loss of epoch-43 batch-51 = 8.242670446634293e-06

Training epoch-43 batch-52
Running loss of epoch-43 batch-52 = 3.6794226616621017e-06

Training epoch-43 batch-53
Running loss of epoch-43 batch-53 = 8.126255124807358e-06

Training epoch-43 batch-54
Running loss of epoch-43 batch-54 = 9.245704859495163e-06

Training epoch-43 batch-55
Running loss of epoch-43 batch-55 = 6.408430635929108e-06

Training epoch-43 batch-56
Running loss of epoch-43 batch-56 = 6.2747858464717865e-06

Training epoch-43 batch-57
Running loss of epoch-43 batch-57 = 8.397269994020462e-06

Training epoch-43 batch-58
Running loss of epoch-43 batch-58 = 4.6351924538612366e-06

Training epoch-43 batch-59
Running loss of epoch-43 batch-59 = 4.534493200480938e-06

Training epoch-43 batch-60
Running loss of epoch-43 batch-60 = 6.061047315597534e-06

Training epoch-43 batch-61
Running loss of epoch-43 batch-61 = 1.0706251487135887e-05

Training epoch-43 batch-62
Running loss of epoch-43 batch-62 = 4.609813913702965e-06

Training epoch-43 batch-63
Running loss of epoch-43 batch-63 = 1.32822897285223e-05

Training epoch-43 batch-64
Running loss of epoch-43 batch-64 = 9.444309398531914e-06

Training epoch-43 batch-65
Running loss of epoch-43 batch-65 = 1.51745043694973e-05

Training epoch-43 batch-66
Running loss of epoch-43 batch-66 = 1.2079603038728237e-05

Training epoch-43 batch-67
Running loss of epoch-43 batch-67 = 8.256640285253525e-06

Training epoch-43 batch-68
Running loss of epoch-43 batch-68 = 1.616682857275009e-05

Training epoch-43 batch-69
Running loss of epoch-43 batch-69 = 1.0677380487322807e-05

Training epoch-43 batch-70
Running loss of epoch-43 batch-70 = 1.4978460967540741e-05

Training epoch-43 batch-71
Running loss of epoch-43 batch-71 = 3.102235496044159e-06

Training epoch-43 batch-72
Running loss of epoch-43 batch-72 = 2.891756594181061e-06

Training epoch-43 batch-73
Running loss of epoch-43 batch-73 = 1.921551302075386e-06

Training epoch-43 batch-74
Running loss of epoch-43 batch-74 = 2.647191286087036e-05

Training epoch-43 batch-75
Running loss of epoch-43 batch-75 = 8.990289643406868e-06

Training epoch-43 batch-76
Running loss of epoch-43 batch-76 = 1.020100899040699e-05

Training epoch-43 batch-77
Running loss of epoch-43 batch-77 = 1.714529935270548e-05

Training epoch-43 batch-78
Running loss of epoch-43 batch-78 = 3.793509677052498e-06

Training epoch-43 batch-79
Running loss of epoch-43 batch-79 = 6.6396314650774e-06

Training epoch-43 batch-80
Running loss of epoch-43 batch-80 = 4.080589860677719e-06

Training epoch-43 batch-81
Running loss of epoch-43 batch-81 = 4.784204065799713e-06

Training epoch-43 batch-82
Running loss of epoch-43 batch-82 = 8.340226486325264e-06

Training epoch-43 batch-83
Running loss of epoch-43 batch-83 = 9.365146979689598e-06

Training epoch-43 batch-84
Running loss of epoch-43 batch-84 = 2.227257937192917e-06

Training epoch-43 batch-85
Running loss of epoch-43 batch-85 = 5.027279257774353e-06

Training epoch-43 batch-86
Running loss of epoch-43 batch-86 = 1.6374513506889343e-05

Training epoch-43 batch-87
Running loss of epoch-43 batch-87 = 6.629154086112976e-06

Training epoch-43 batch-88
Running loss of epoch-43 batch-88 = 9.831273928284645e-06

Training epoch-43 batch-89
Running loss of epoch-43 batch-89 = 1.09085813164711e-05

Training epoch-43 batch-90
Running loss of epoch-43 batch-90 = 4.7548674046993256e-06

Training epoch-43 batch-91
Running loss of epoch-43 batch-91 = 1.616845838725567e-05

Training epoch-43 batch-92
Running loss of epoch-43 batch-92 = 1.1138850823044777e-05

Training epoch-43 batch-93
Running loss of epoch-43 batch-93 = 9.71602275967598e-06

Training epoch-43 batch-94
Running loss of epoch-43 batch-94 = 5.7157594710588455e-06

Training epoch-43 batch-95
Running loss of epoch-43 batch-95 = 3.964174538850784e-06

Training epoch-43 batch-96
Running loss of epoch-43 batch-96 = 9.462004527449608e-06

Training epoch-43 batch-97
Running loss of epoch-43 batch-97 = 6.61914236843586e-06

Training epoch-43 batch-98
Running loss of epoch-43 batch-98 = 1.0818475857377052e-05

Training epoch-43 batch-99
Running loss of epoch-43 batch-99 = 5.535781383514404e-06

Training epoch-43 batch-100
Running loss of epoch-43 batch-100 = 1.8636230379343033e-05

Training epoch-43 batch-101
Running loss of epoch-43 batch-101 = 6.037065759301186e-06

Training epoch-43 batch-102
Running loss of epoch-43 batch-102 = 1.1563533917069435e-05

Training epoch-43 batch-103
Running loss of epoch-43 batch-103 = 9.522540494799614e-06

Training epoch-43 batch-104
Running loss of epoch-43 batch-104 = 3.1993258744478226e-06

Training epoch-43 batch-105
Running loss of epoch-43 batch-105 = 9.051524102687836e-06

Training epoch-43 batch-106
Running loss of epoch-43 batch-106 = 4.713190719485283e-06

Training epoch-43 batch-107
Running loss of epoch-43 batch-107 = 6.904127076268196e-06

Training epoch-43 batch-108
Running loss of epoch-43 batch-108 = 2.8028152883052826e-06

Training epoch-43 batch-109
Running loss of epoch-43 batch-109 = 7.0070382207632065e-06

Training epoch-43 batch-110
Running loss of epoch-43 batch-110 = 4.465458914637566e-06

Training epoch-43 batch-111
Running loss of epoch-43 batch-111 = 1.1186348274350166e-05

Training epoch-43 batch-112
Running loss of epoch-43 batch-112 = 1.2130942195653915e-05

Training epoch-43 batch-113
Running loss of epoch-43 batch-113 = 4.221219569444656e-06

Training epoch-43 batch-114
Running loss of epoch-43 batch-114 = 1.0890653356909752e-05

Training epoch-43 batch-115
Running loss of epoch-43 batch-115 = 1.3107899576425552e-05

Training epoch-43 batch-116
Running loss of epoch-43 batch-116 = 4.3353065848350525e-06

Training epoch-43 batch-117
Running loss of epoch-43 batch-117 = 1.4146789908409119e-05

Training epoch-43 batch-118
Running loss of epoch-43 batch-118 = 7.917173206806183e-06

Training epoch-43 batch-119
Running loss of epoch-43 batch-119 = 6.2373001128435135e-06

Training epoch-43 batch-120
Running loss of epoch-43 batch-120 = 5.352776497602463e-06

Training epoch-43 batch-121
Running loss of epoch-43 batch-121 = 1.0210089385509491e-05

Training epoch-43 batch-122
Running loss of epoch-43 batch-122 = 3.6046840250492096e-06

Training epoch-43 batch-123
Running loss of epoch-43 batch-123 = 3.196229226887226e-05

Training epoch-43 batch-124
Running loss of epoch-43 batch-124 = 3.747176378965378e-06

Training epoch-43 batch-125
Running loss of epoch-43 batch-125 = 4.07826155424118e-06

Training epoch-43 batch-126
Running loss of epoch-43 batch-126 = 5.596783012151718e-06

Training epoch-43 batch-127
Running loss of epoch-43 batch-127 = 2.0751729607582092e-05

Training epoch-43 batch-128
Running loss of epoch-43 batch-128 = 1.2042000889778137e-05

Training epoch-43 batch-129
Running loss of epoch-43 batch-129 = 1.499941572546959e-05

Training epoch-43 batch-130
Running loss of epoch-43 batch-130 = 6.872694939374924e-06

Training epoch-43 batch-131
Running loss of epoch-43 batch-131 = 3.468478098511696e-06

Training epoch-43 batch-132
Running loss of epoch-43 batch-132 = 7.344875484704971e-06

Training epoch-43 batch-133
Running loss of epoch-43 batch-133 = 2.777809277176857e-05

Training epoch-43 batch-134
Running loss of epoch-43 batch-134 = 1.1088093742728233e-05

Training epoch-43 batch-135
Running loss of epoch-43 batch-135 = 4.22564335167408e-06

Training epoch-43 batch-136
Running loss of epoch-43 batch-136 = 1.1421740055084229e-05

Training epoch-43 batch-137
Running loss of epoch-43 batch-137 = 3.214692696928978e-06

Training epoch-43 batch-138
Running loss of epoch-43 batch-138 = 1.5129218809306622e-05

Training epoch-43 batch-139
Running loss of epoch-43 batch-139 = 4.801899194717407e-06

Training epoch-43 batch-140
Running loss of epoch-43 batch-140 = 4.0463637560606e-06

Training epoch-43 batch-141
Running loss of epoch-43 batch-141 = 6.563728675246239e-06

Training epoch-43 batch-142
Running loss of epoch-43 batch-142 = 1.5000579878687859e-05

Training epoch-43 batch-143
Running loss of epoch-43 batch-143 = 6.821472197771072e-06

Training epoch-43 batch-144
Running loss of epoch-43 batch-144 = 1.4944467693567276e-05

Training epoch-43 batch-145
Running loss of epoch-43 batch-145 = 4.3208710849285126e-06

Training epoch-43 batch-146
Running loss of epoch-43 batch-146 = 1.235981471836567e-05

Training epoch-43 batch-147
Running loss of epoch-43 batch-147 = 7.526017725467682e-06

Training epoch-43 batch-148
Running loss of epoch-43 batch-148 = 3.82494181394577e-06

Training epoch-43 batch-149
Running loss of epoch-43 batch-149 = 1.757778227329254e-05

Training epoch-43 batch-150
Running loss of epoch-43 batch-150 = 1.2124422937631607e-05

Training epoch-43 batch-151
Running loss of epoch-43 batch-151 = 1.026480458676815e-05

Training epoch-43 batch-152
Running loss of epoch-43 batch-152 = 6.4980704337358475e-06

Training epoch-43 batch-153
Running loss of epoch-43 batch-153 = 9.858282282948494e-06

Training epoch-43 batch-154
Running loss of epoch-43 batch-154 = 5.808891728520393e-06

Training epoch-43 batch-155
Running loss of epoch-43 batch-155 = 5.818204954266548e-06

Training epoch-43 batch-156
Running loss of epoch-43 batch-156 = 1.1210329830646515e-05

Training epoch-43 batch-157
Running loss of epoch-43 batch-157 = 0.00010730326175689697

Finished training epoch-43.



Average train loss at epoch-43 = 9.270211309194564e-06

Started Evaluation

Average val loss at epoch-43 = 1.060021005342031

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 63.70 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 72.31 %

Overall Accuracy = 83.16 %

Finished Evaluation



Started training epoch-44


Training epoch-44 batch-1
Running loss of epoch-44 batch-1 = 1.1691008694469929e-05

Training epoch-44 batch-2
Running loss of epoch-44 batch-2 = 1.0825460776686668e-05

Training epoch-44 batch-3
Running loss of epoch-44 batch-3 = 9.331852197647095e-06

Training epoch-44 batch-4
Running loss of epoch-44 batch-4 = 2.7136411517858505e-06

Training epoch-44 batch-5
Running loss of epoch-44 batch-5 = 1.0133720934391022e-05

Training epoch-44 batch-6
Running loss of epoch-44 batch-6 = 5.22867776453495e-06

Training epoch-44 batch-7
Running loss of epoch-44 batch-7 = 1.972983591258526e-05

Training epoch-44 batch-8
Running loss of epoch-44 batch-8 = 3.600027412176132e-06

Training epoch-44 batch-9
Running loss of epoch-44 batch-9 = 6.257090717554092e-06

Training epoch-44 batch-10
Running loss of epoch-44 batch-10 = 6.0908496379852295e-06

Training epoch-44 batch-11
Running loss of epoch-44 batch-11 = 6.606802344322205e-06

Training epoch-44 batch-12
Running loss of epoch-44 batch-12 = 8.801929652690887e-06

Training epoch-44 batch-13
Running loss of epoch-44 batch-13 = 7.707159966230392e-06

Training epoch-44 batch-14
Running loss of epoch-44 batch-14 = 4.995614290237427e-06

Training epoch-44 batch-15
Running loss of epoch-44 batch-15 = 5.482463166117668e-06

Training epoch-44 batch-16
Running loss of epoch-44 batch-16 = 6.987014785408974e-06

Training epoch-44 batch-17
Running loss of epoch-44 batch-17 = 1.2049451470375061e-05

Training epoch-44 batch-18
Running loss of epoch-44 batch-18 = 1.28129031509161e-05

Training epoch-44 batch-19
Running loss of epoch-44 batch-19 = 6.353715434670448e-06

Training epoch-44 batch-20
Running loss of epoch-44 batch-20 = 7.172813639044762e-06

Training epoch-44 batch-21
Running loss of epoch-44 batch-21 = 2.533942461013794e-05

Training epoch-44 batch-22
Running loss of epoch-44 batch-22 = 1.1438969522714615e-05

Training epoch-44 batch-23
Running loss of epoch-44 batch-23 = 1.1987751349806786e-05

Training epoch-44 batch-24
Running loss of epoch-44 batch-24 = 5.819601938128471e-06

Training epoch-44 batch-25
Running loss of epoch-44 batch-25 = 2.131727524101734e-05

Training epoch-44 batch-26
Running loss of epoch-44 batch-26 = 7.025431841611862e-06

Training epoch-44 batch-27
Running loss of epoch-44 batch-27 = 1.6438309103250504e-05

Training epoch-44 batch-28
Running loss of epoch-44 batch-28 = 1.2109288945794106e-05

Training epoch-44 batch-29
Running loss of epoch-44 batch-29 = 1.1736294254660606e-05

Training epoch-44 batch-30
Running loss of epoch-44 batch-30 = 8.073169738054276e-06

Training epoch-44 batch-31
Running loss of epoch-44 batch-31 = 5.591660737991333e-06

Training epoch-44 batch-32
Running loss of epoch-44 batch-32 = 4.039378836750984e-06

Training epoch-44 batch-33
Running loss of epoch-44 batch-33 = 1.1113006621599197e-05

Training epoch-44 batch-34
Running loss of epoch-44 batch-34 = 1.5310710296034813e-05

Training epoch-44 batch-35
Running loss of epoch-44 batch-35 = 4.59328293800354e-06

Training epoch-44 batch-36
Running loss of epoch-44 batch-36 = 4.64986078441143e-06

Training epoch-44 batch-37
Running loss of epoch-44 batch-37 = 8.468981832265854e-06

Training epoch-44 batch-38
Running loss of epoch-44 batch-38 = 4.750792868435383e-06

Training epoch-44 batch-39
Running loss of epoch-44 batch-39 = 2.590147778391838e-05

Training epoch-44 batch-40
Running loss of epoch-44 batch-40 = 1.0231509804725647e-05

Training epoch-44 batch-41
Running loss of epoch-44 batch-41 = 9.526265785098076e-06

Training epoch-44 batch-42
Running loss of epoch-44 batch-42 = 9.02626197785139e-06

Training epoch-44 batch-43
Running loss of epoch-44 batch-43 = 6.257789209485054e-06

Training epoch-44 batch-44
Running loss of epoch-44 batch-44 = 2.805609256029129e-06

Training epoch-44 batch-45
Running loss of epoch-44 batch-45 = 1.0812422260642052e-05

Training epoch-44 batch-46
Running loss of epoch-44 batch-46 = 7.450347766280174e-06

Training epoch-44 batch-47
Running loss of epoch-44 batch-47 = 8.506933227181435e-06

Training epoch-44 batch-48
Running loss of epoch-44 batch-48 = 1.6960082575678825e-05

Training epoch-44 batch-49
Running loss of epoch-44 batch-49 = 1.1316966265439987e-05

Training epoch-44 batch-50
Running loss of epoch-44 batch-50 = 3.2531097531318665e-06

Training epoch-44 batch-51
Running loss of epoch-44 batch-51 = 5.3693074733018875e-06

Training epoch-44 batch-52
Running loss of epoch-44 batch-52 = 1.2606615200638771e-05

Training epoch-44 batch-53
Running loss of epoch-44 batch-53 = 1.6298610717058182e-05

Training epoch-44 batch-54
Running loss of epoch-44 batch-54 = 6.950227543711662e-06

Training epoch-44 batch-55
Running loss of epoch-44 batch-55 = 8.78889113664627e-06

Training epoch-44 batch-56
Running loss of epoch-44 batch-56 = 8.87131318449974e-06

Training epoch-44 batch-57
Running loss of epoch-44 batch-57 = 1.3037584722042084e-05

Training epoch-44 batch-58
Running loss of epoch-44 batch-58 = 1.7028534784913063e-05

Training epoch-44 batch-59
Running loss of epoch-44 batch-59 = 1.7496058717370033e-05

Training epoch-44 batch-60
Running loss of epoch-44 batch-60 = 1.7283950001001358e-05

Training epoch-44 batch-61
Running loss of epoch-44 batch-61 = 5.76535239815712e-06

Training epoch-44 batch-62
Running loss of epoch-44 batch-62 = 6.957445293664932e-06

Training epoch-44 batch-63
Running loss of epoch-44 batch-63 = 7.980968803167343e-06

Training epoch-44 batch-64
Running loss of epoch-44 batch-64 = 5.9192534536123276e-06

Training epoch-44 batch-65
Running loss of epoch-44 batch-65 = 4.079192876815796e-06

Training epoch-44 batch-66
Running loss of epoch-44 batch-66 = 3.4871045500040054e-06

Training epoch-44 batch-67
Running loss of epoch-44 batch-67 = 1.6979407519102097e-05

Training epoch-44 batch-68
Running loss of epoch-44 batch-68 = 1.061195507645607e-05

Training epoch-44 batch-69
Running loss of epoch-44 batch-69 = 4.237750545144081e-06

Training epoch-44 batch-70
Running loss of epoch-44 batch-70 = 1.0860618203878403e-05

Training epoch-44 batch-71
Running loss of epoch-44 batch-71 = 4.0477607399225235e-06

Training epoch-44 batch-72
Running loss of epoch-44 batch-72 = 2.151820808649063e-06

Training epoch-44 batch-73
Running loss of epoch-44 batch-73 = 1.1269468814134598e-05

Training epoch-44 batch-74
Running loss of epoch-44 batch-74 = 1.0489486157894135e-05

Training epoch-44 batch-75
Running loss of epoch-44 batch-75 = 4.462897777557373e-06

Training epoch-44 batch-76
Running loss of epoch-44 batch-76 = 6.637768819928169e-06

Training epoch-44 batch-77
Running loss of epoch-44 batch-77 = 7.848720997571945e-06

Training epoch-44 batch-78
Running loss of epoch-44 batch-78 = 3.5096891224384308e-06

Training epoch-44 batch-79
Running loss of epoch-44 batch-79 = 7.065478712320328e-06

Training epoch-44 batch-80
Running loss of epoch-44 batch-80 = 4.109460860490799e-06

Training epoch-44 batch-81
Running loss of epoch-44 batch-81 = 6.763031706213951e-06

Training epoch-44 batch-82
Running loss of epoch-44 batch-82 = 1.0256189852952957e-05

Training epoch-44 batch-83
Running loss of epoch-44 batch-83 = 5.066394805908203e-06

Training epoch-44 batch-84
Running loss of epoch-44 batch-84 = 1.0408461093902588e-05

Training epoch-44 batch-85
Running loss of epoch-44 batch-85 = 4.351604729890823e-06

Training epoch-44 batch-86
Running loss of epoch-44 batch-86 = 4.098983481526375e-06

Training epoch-44 batch-87
Running loss of epoch-44 batch-87 = 8.509261533617973e-06

Training epoch-44 batch-88
Running loss of epoch-44 batch-88 = 6.1818864196538925e-06

Training epoch-44 batch-89
Running loss of epoch-44 batch-89 = 4.934379830956459e-06

Training epoch-44 batch-90
Running loss of epoch-44 batch-90 = 1.0498450137674809e-05

Training epoch-44 batch-91
Running loss of epoch-44 batch-91 = 1.1130468919873238e-05

Training epoch-44 batch-92
Running loss of epoch-44 batch-92 = 4.531582817435265e-06

Training epoch-44 batch-93
Running loss of epoch-44 batch-93 = 8.103088475763798e-06

Training epoch-44 batch-94
Running loss of epoch-44 batch-94 = 8.803093805909157e-06

Training epoch-44 batch-95
Running loss of epoch-44 batch-95 = 4.190485924482346e-06

Training epoch-44 batch-96
Running loss of epoch-44 batch-96 = 8.595758117735386e-06

Training epoch-44 batch-97
Running loss of epoch-44 batch-97 = 8.585397154092789e-06

Training epoch-44 batch-98
Running loss of epoch-44 batch-98 = 1.185503788292408e-05

Training epoch-44 batch-99
Running loss of epoch-44 batch-99 = 3.947177901864052e-06

Training epoch-44 batch-100
Running loss of epoch-44 batch-100 = 6.768153980374336e-06

Training epoch-44 batch-101
Running loss of epoch-44 batch-101 = 7.977476343512535e-06

Training epoch-44 batch-102
Running loss of epoch-44 batch-102 = 8.86223278939724e-06

Training epoch-44 batch-103
Running loss of epoch-44 batch-103 = 3.264285624027252e-06

Training epoch-44 batch-104
Running loss of epoch-44 batch-104 = 6.965361535549164e-06

Training epoch-44 batch-105
Running loss of epoch-44 batch-105 = 1.1345837265253067e-05

Training epoch-44 batch-106
Running loss of epoch-44 batch-106 = 4.402128979563713e-06

Training epoch-44 batch-107
Running loss of epoch-44 batch-107 = 4.0673185139894485e-06

Training epoch-44 batch-108
Running loss of epoch-44 batch-108 = 5.037290975451469e-06

Training epoch-44 batch-109
Running loss of epoch-44 batch-109 = 6.737653166055679e-06

Training epoch-44 batch-110
Running loss of epoch-44 batch-110 = 6.312038749456406e-06

Training epoch-44 batch-111
Running loss of epoch-44 batch-111 = 1.0001007467508316e-05

Training epoch-44 batch-112
Running loss of epoch-44 batch-112 = 3.3995602279901505e-06

Training epoch-44 batch-113
Running loss of epoch-44 batch-113 = 1.824740320444107e-05

Training epoch-44 batch-114
Running loss of epoch-44 batch-114 = 8.399831131100655e-06

Training epoch-44 batch-115
Running loss of epoch-44 batch-115 = 1.0597286745905876e-05

Training epoch-44 batch-116
Running loss of epoch-44 batch-116 = 8.895061910152435e-06

Training epoch-44 batch-117
Running loss of epoch-44 batch-117 = 8.231494575738907e-06

Training epoch-44 batch-118
Running loss of epoch-44 batch-118 = 8.660834282636642e-06

Training epoch-44 batch-119
Running loss of epoch-44 batch-119 = 1.0616611689329147e-05

Training epoch-44 batch-120
Running loss of epoch-44 batch-120 = 9.541865438222885e-06

Training epoch-44 batch-121
Running loss of epoch-44 batch-121 = 1.259450800716877e-05

Training epoch-44 batch-122
Running loss of epoch-44 batch-122 = 6.902031600475311e-06

Training epoch-44 batch-123
Running loss of epoch-44 batch-123 = 1.3676472008228302e-05

Training epoch-44 batch-124
Running loss of epoch-44 batch-124 = 4.643574357032776e-06

Training epoch-44 batch-125
Running loss of epoch-44 batch-125 = 9.354669600725174e-06

Training epoch-44 batch-126
Running loss of epoch-44 batch-126 = 8.83219763636589e-06

Training epoch-44 batch-127
Running loss of epoch-44 batch-127 = 3.5482924431562424e-05

Training epoch-44 batch-128
Running loss of epoch-44 batch-128 = 9.608222171664238e-06

Training epoch-44 batch-129
Running loss of epoch-44 batch-129 = 4.8549845814704895e-06

Training epoch-44 batch-130
Running loss of epoch-44 batch-130 = 8.41100700199604e-06

Training epoch-44 batch-131
Running loss of epoch-44 batch-131 = 1.1655967682600021e-05

Training epoch-44 batch-132
Running loss of epoch-44 batch-132 = 1.0272604413330555e-05

Training epoch-44 batch-133
Running loss of epoch-44 batch-133 = 5.918554961681366e-06

Training epoch-44 batch-134
Running loss of epoch-44 batch-134 = 7.861759513616562e-06

Training epoch-44 batch-135
Running loss of epoch-44 batch-135 = 4.766741767525673e-06

Training epoch-44 batch-136
Running loss of epoch-44 batch-136 = 9.107403457164764e-06

Training epoch-44 batch-137
Running loss of epoch-44 batch-137 = 8.778879418969154e-06

Training epoch-44 batch-138
Running loss of epoch-44 batch-138 = 9.831739589571953e-06

Training epoch-44 batch-139
Running loss of epoch-44 batch-139 = 5.736714228987694e-06

Training epoch-44 batch-140
Running loss of epoch-44 batch-140 = 2.6423949748277664e-06

Training epoch-44 batch-141
Running loss of epoch-44 batch-141 = 1.3379612937569618e-05

Training epoch-44 batch-142
Running loss of epoch-44 batch-142 = 2.991640940308571e-06

Training epoch-44 batch-143
Running loss of epoch-44 batch-143 = 7.979804649949074e-06

Training epoch-44 batch-144
Running loss of epoch-44 batch-144 = 6.366753950715065e-06

Training epoch-44 batch-145
Running loss of epoch-44 batch-145 = 1.825043000280857e-05

Training epoch-44 batch-146
Running loss of epoch-44 batch-146 = 6.597023457288742e-06

Training epoch-44 batch-147
Running loss of epoch-44 batch-147 = 5.95790334045887e-06

Training epoch-44 batch-148
Running loss of epoch-44 batch-148 = 2.1157320588827133e-06

Training epoch-44 batch-149
Running loss of epoch-44 batch-149 = 7.1125105023384094e-06

Training epoch-44 batch-150
Running loss of epoch-44 batch-150 = 8.105998858809471e-06

Training epoch-44 batch-151
Running loss of epoch-44 batch-151 = 7.960479706525803e-06

Training epoch-44 batch-152
Running loss of epoch-44 batch-152 = 6.05778768658638e-06

Training epoch-44 batch-153
Running loss of epoch-44 batch-153 = 1.1648982763290405e-05

Training epoch-44 batch-154
Running loss of epoch-44 batch-154 = 8.034054189920425e-06

Training epoch-44 batch-155
Running loss of epoch-44 batch-155 = 8.19226261228323e-06

Training epoch-44 batch-156
Running loss of epoch-44 batch-156 = 1.1042808182537556e-05

Training epoch-44 batch-157
Running loss of epoch-44 batch-157 = 5.6877732276916504e-05

Finished training epoch-44.



Average train loss at epoch-44 = 8.862174302339554e-06

Started Evaluation

Average val loss at epoch-44 = 1.0563160857334783

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.93 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.23 %

Finished Evaluation



Started training epoch-45


Training epoch-45 batch-1
Running loss of epoch-45 batch-1 = 4.3264590203762054e-06

Training epoch-45 batch-2
Running loss of epoch-45 batch-2 = 4.476169124245644e-06

Training epoch-45 batch-3
Running loss of epoch-45 batch-3 = 8.82311724126339e-06

Training epoch-45 batch-4
Running loss of epoch-45 batch-4 = 4.800735041499138e-06

Training epoch-45 batch-5
Running loss of epoch-45 batch-5 = 8.637318387627602e-06

Training epoch-45 batch-6
Running loss of epoch-45 batch-6 = 4.576984792947769e-06

Training epoch-45 batch-7
Running loss of epoch-45 batch-7 = 9.072013199329376e-06

Training epoch-45 batch-8
Running loss of epoch-45 batch-8 = 1.526903361082077e-05

Training epoch-45 batch-9
Running loss of epoch-45 batch-9 = 4.351139068603516e-06

Training epoch-45 batch-10
Running loss of epoch-45 batch-10 = 1.1089257895946503e-05

Training epoch-45 batch-11
Running loss of epoch-45 batch-11 = 1.0253163054585457e-05

Training epoch-45 batch-12
Running loss of epoch-45 batch-12 = 5.834037438035011e-06

Training epoch-45 batch-13
Running loss of epoch-45 batch-13 = 2.9015354812145233e-06

Training epoch-45 batch-14
Running loss of epoch-45 batch-14 = 7.265713065862656e-06

Training epoch-45 batch-15
Running loss of epoch-45 batch-15 = 8.556060492992401e-06

Training epoch-45 batch-16
Running loss of epoch-45 batch-16 = 8.753268048167229e-06

Training epoch-45 batch-17
Running loss of epoch-45 batch-17 = 1.0138261131942272e-05

Training epoch-45 batch-18
Running loss of epoch-45 batch-18 = 3.6710407584905624e-06

Training epoch-45 batch-19
Running loss of epoch-45 batch-19 = 1.285923644900322e-05

Training epoch-45 batch-20
Running loss of epoch-45 batch-20 = 8.563278242945671e-06

Training epoch-45 batch-21
Running loss of epoch-45 batch-21 = 9.602867066860199e-06

Training epoch-45 batch-22
Running loss of epoch-45 batch-22 = 6.64382241666317e-06

Training epoch-45 batch-23
Running loss of epoch-45 batch-23 = 7.598428055644035e-06

Training epoch-45 batch-24
Running loss of epoch-45 batch-24 = 6.83218240737915e-06

Training epoch-45 batch-25
Running loss of epoch-45 batch-25 = 7.473863661289215e-06

Training epoch-45 batch-26
Running loss of epoch-45 batch-26 = 7.161404937505722e-06

Training epoch-45 batch-27
Running loss of epoch-45 batch-27 = 7.320893928408623e-06

Training epoch-45 batch-28
Running loss of epoch-45 batch-28 = 4.045199602842331e-06

Training epoch-45 batch-29
Running loss of epoch-45 batch-29 = 5.675479769706726e-06

Training epoch-45 batch-30
Running loss of epoch-45 batch-30 = 6.621005013585091e-06

Training epoch-45 batch-31
Running loss of epoch-45 batch-31 = 5.173031240701675e-06

Training epoch-45 batch-32
Running loss of epoch-45 batch-32 = 7.254304364323616e-06

Training epoch-45 batch-33
Running loss of epoch-45 batch-33 = 6.702728569507599e-06

Training epoch-45 batch-34
Running loss of epoch-45 batch-34 = 6.630783900618553e-06

Training epoch-45 batch-35
Running loss of epoch-45 batch-35 = 1.0892516002058983e-05

Training epoch-45 batch-36
Running loss of epoch-45 batch-36 = 6.6802604123950005e-06

Training epoch-45 batch-37
Running loss of epoch-45 batch-37 = 8.658738806843758e-06

Training epoch-45 batch-38
Running loss of epoch-45 batch-38 = 7.080379873514175e-06

Training epoch-45 batch-39
Running loss of epoch-45 batch-39 = 8.148839697241783e-06

Training epoch-45 batch-40
Running loss of epoch-45 batch-40 = 4.842178896069527e-06

Training epoch-45 batch-41
Running loss of epoch-45 batch-41 = 6.009358912706375e-06

Training epoch-45 batch-42
Running loss of epoch-45 batch-42 = 8.376315236091614e-06

Training epoch-45 batch-43
Running loss of epoch-45 batch-43 = 1.4104414731264114e-05

Training epoch-45 batch-44
Running loss of epoch-45 batch-44 = 9.111827239394188e-06

Training epoch-45 batch-45
Running loss of epoch-45 batch-45 = 1.3820827007293701e-06

Training epoch-45 batch-46
Running loss of epoch-45 batch-46 = 7.138820365071297e-06

Training epoch-45 batch-47
Running loss of epoch-45 batch-47 = 3.91458161175251e-06

Training epoch-45 batch-48
Running loss of epoch-45 batch-48 = 5.677342414855957e-06

Training epoch-45 batch-49
Running loss of epoch-45 batch-49 = 7.168389856815338e-06

Training epoch-45 batch-50
Running loss of epoch-45 batch-50 = 1.1795898899435997e-05

Training epoch-45 batch-51
Running loss of epoch-45 batch-51 = 1.8691178411245346e-05

Training epoch-45 batch-52
Running loss of epoch-45 batch-52 = 4.3960753828287125e-06

Training epoch-45 batch-53
Running loss of epoch-45 batch-53 = 1.11341942101717e-05

Training epoch-45 batch-54
Running loss of epoch-45 batch-54 = 6.2461476773023605e-06

Training epoch-45 batch-55
Running loss of epoch-45 batch-55 = 5.459412932395935e-06

Training epoch-45 batch-56
Running loss of epoch-45 batch-56 = 1.4293473213911057e-05

Training epoch-45 batch-57
Running loss of epoch-45 batch-57 = 2.37906351685524e-06

Training epoch-45 batch-58
Running loss of epoch-45 batch-58 = 6.6570937633514404e-06

Training epoch-45 batch-59
Running loss of epoch-45 batch-59 = 1.5743542462587357e-05

Training epoch-45 batch-60
Running loss of epoch-45 batch-60 = 9.411945939064026e-06

Training epoch-45 batch-61
Running loss of epoch-45 batch-61 = 6.220769137144089e-06

Training epoch-45 batch-62
Running loss of epoch-45 batch-62 = 3.3816322684288025e-06

Training epoch-45 batch-63
Running loss of epoch-45 batch-63 = 5.039619281888008e-06

Training epoch-45 batch-64
Running loss of epoch-45 batch-64 = 4.7406647354364395e-06

Training epoch-45 batch-65
Running loss of epoch-45 batch-65 = 6.819143891334534e-06

Training epoch-45 batch-66
Running loss of epoch-45 batch-66 = 4.278495907783508e-06

Training epoch-45 batch-67
Running loss of epoch-45 batch-67 = 5.485606379806995e-06

Training epoch-45 batch-68
Running loss of epoch-45 batch-68 = 6.139511242508888e-06

Training epoch-45 batch-69
Running loss of epoch-45 batch-69 = 9.989598765969276e-06

Training epoch-45 batch-70
Running loss of epoch-45 batch-70 = 6.074318662285805e-06

Training epoch-45 batch-71
Running loss of epoch-45 batch-71 = 6.155576556921005e-06

Training epoch-45 batch-72
Running loss of epoch-45 batch-72 = 4.338333383202553e-06

Training epoch-45 batch-73
Running loss of epoch-45 batch-73 = 6.5320637077093124e-06

Training epoch-45 batch-74
Running loss of epoch-45 batch-74 = 1.8076971173286438e-06

Training epoch-45 batch-75
Running loss of epoch-45 batch-75 = 2.607237547636032e-06

Training epoch-45 batch-76
Running loss of epoch-45 batch-76 = 1.4295103028416634e-05

Training epoch-45 batch-77
Running loss of epoch-45 batch-77 = 1.3663899153470993e-05

Training epoch-45 batch-78
Running loss of epoch-45 batch-78 = 7.718568667769432e-06

Training epoch-45 batch-79
Running loss of epoch-45 batch-79 = 1.0514166206121445e-05

Training epoch-45 batch-80
Running loss of epoch-45 batch-80 = 7.051043212413788e-06

Training epoch-45 batch-81
Running loss of epoch-45 batch-81 = 7.741386070847511e-06

Training epoch-45 batch-82
Running loss of epoch-45 batch-82 = 2.104812301695347e-05

Training epoch-45 batch-83
Running loss of epoch-45 batch-83 = 9.70764085650444e-06

Training epoch-45 batch-84
Running loss of epoch-45 batch-84 = 5.410052835941315e-06

Training epoch-45 batch-85
Running loss of epoch-45 batch-85 = 8.394941687583923e-06

Training epoch-45 batch-86
Running loss of epoch-45 batch-86 = 2.2424152120947838e-05

Training epoch-45 batch-87
Running loss of epoch-45 batch-87 = 9.351992048323154e-06

Training epoch-45 batch-88
Running loss of epoch-45 batch-88 = 1.0299962013959885e-05

Training epoch-45 batch-89
Running loss of epoch-45 batch-89 = 9.660143405199051e-06

Training epoch-45 batch-90
Running loss of epoch-45 batch-90 = 1.231650821864605e-05

Training epoch-45 batch-91
Running loss of epoch-45 batch-91 = 1.041637733578682e-05

Training epoch-45 batch-92
Running loss of epoch-45 batch-92 = 5.803536623716354e-06

Training epoch-45 batch-93
Running loss of epoch-45 batch-93 = 6.235670298337936e-06

Training epoch-45 batch-94
Running loss of epoch-45 batch-94 = 5.704350769519806e-06

Training epoch-45 batch-95
Running loss of epoch-45 batch-95 = 5.843816325068474e-06

Training epoch-45 batch-96
Running loss of epoch-45 batch-96 = 7.276423275470734e-06

Training epoch-45 batch-97
Running loss of epoch-45 batch-97 = 9.027658961713314e-06

Training epoch-45 batch-98
Running loss of epoch-45 batch-98 = 1.7694896087050438e-05

Training epoch-45 batch-99
Running loss of epoch-45 batch-99 = 7.997499778866768e-06

Training epoch-45 batch-100
Running loss of epoch-45 batch-100 = 6.2230974435806274e-06

Training epoch-45 batch-101
Running loss of epoch-45 batch-101 = 1.0519754141569138e-05

Training epoch-45 batch-102
Running loss of epoch-45 batch-102 = 6.215181201696396e-06

Training epoch-45 batch-103
Running loss of epoch-45 batch-103 = 7.694587111473083e-06

Training epoch-45 batch-104
Running loss of epoch-45 batch-104 = 3.7974677979946136e-06

Training epoch-45 batch-105
Running loss of epoch-45 batch-105 = 1.173082273453474e-05

Training epoch-45 batch-106
Running loss of epoch-45 batch-106 = 8.021946996450424e-06

Training epoch-45 batch-107
Running loss of epoch-45 batch-107 = 1.8905848264694214e-06

Training epoch-45 batch-108
Running loss of epoch-45 batch-108 = 1.4627818018198013e-05

Training epoch-45 batch-109
Running loss of epoch-45 batch-109 = 1.0021263733506203e-05

Training epoch-45 batch-110
Running loss of epoch-45 batch-110 = 9.39657911658287e-06

Training epoch-45 batch-111
Running loss of epoch-45 batch-111 = 4.827510565519333e-06

Training epoch-45 batch-112
Running loss of epoch-45 batch-112 = 5.680602043867111e-06

Training epoch-45 batch-113
Running loss of epoch-45 batch-113 = 6.144633516669273e-06

Training epoch-45 batch-114
Running loss of epoch-45 batch-114 = 1.2569362297654152e-05

Training epoch-45 batch-115
Running loss of epoch-45 batch-115 = 3.956025466322899e-06

Training epoch-45 batch-116
Running loss of epoch-45 batch-116 = 9.565846994519234e-06

Training epoch-45 batch-117
Running loss of epoch-45 batch-117 = 1.1068535968661308e-05

Training epoch-45 batch-118
Running loss of epoch-45 batch-118 = 4.129251465201378e-06

Training epoch-45 batch-119
Running loss of epoch-45 batch-119 = 2.334429882466793e-05

Training epoch-45 batch-120
Running loss of epoch-45 batch-120 = 1.6037840396165848e-05

Training epoch-45 batch-121
Running loss of epoch-45 batch-121 = 9.398208931088448e-06

Training epoch-45 batch-122
Running loss of epoch-45 batch-122 = 5.516223609447479e-06

Training epoch-45 batch-123
Running loss of epoch-45 batch-123 = 5.927402526140213e-06

Training epoch-45 batch-124
Running loss of epoch-45 batch-124 = 6.888061761856079e-06

Training epoch-45 batch-125
Running loss of epoch-45 batch-125 = 9.75397415459156e-06

Training epoch-45 batch-126
Running loss of epoch-45 batch-126 = 5.220761522650719e-06

Training epoch-45 batch-127
Running loss of epoch-45 batch-127 = 2.235453575849533e-05

Training epoch-45 batch-128
Running loss of epoch-45 batch-128 = 5.835201591253281e-06

Training epoch-45 batch-129
Running loss of epoch-45 batch-129 = 7.822876796126366e-06

Training epoch-45 batch-130
Running loss of epoch-45 batch-130 = 5.059177055954933e-06

Training epoch-45 batch-131
Running loss of epoch-45 batch-131 = 3.930181264877319e-06

Training epoch-45 batch-132
Running loss of epoch-45 batch-132 = 5.923211574554443e-06

Training epoch-45 batch-133
Running loss of epoch-45 batch-133 = 4.935776814818382e-06

Training epoch-45 batch-134
Running loss of epoch-45 batch-134 = 5.896901711821556e-06

Training epoch-45 batch-135
Running loss of epoch-45 batch-135 = 1.119193620979786e-05

Training epoch-45 batch-136
Running loss of epoch-45 batch-136 = 8.468050509691238e-06

Training epoch-45 batch-137
Running loss of epoch-45 batch-137 = 6.576301530003548e-06

Training epoch-45 batch-138
Running loss of epoch-45 batch-138 = 1.1578667908906937e-05

Training epoch-45 batch-139
Running loss of epoch-45 batch-139 = 4.7979410737752914e-06

Training epoch-45 batch-140
Running loss of epoch-45 batch-140 = 9.898096323013306e-06

Training epoch-45 batch-141
Running loss of epoch-45 batch-141 = 4.531582817435265e-06

Training epoch-45 batch-142
Running loss of epoch-45 batch-142 = 2.6011839509010315e-06

Training epoch-45 batch-143
Running loss of epoch-45 batch-143 = 8.961185812950134e-06

Training epoch-45 batch-144
Running loss of epoch-45 batch-144 = 5.17931766808033e-06

Training epoch-45 batch-145
Running loss of epoch-45 batch-145 = 2.0800158381462097e-05

Training epoch-45 batch-146
Running loss of epoch-45 batch-146 = 8.005881682038307e-06

Training epoch-45 batch-147
Running loss of epoch-45 batch-147 = 8.356291800737381e-06

Training epoch-45 batch-148
Running loss of epoch-45 batch-148 = 1.8777325749397278e-05

Training epoch-45 batch-149
Running loss of epoch-45 batch-149 = 1.2312782928347588e-05

Training epoch-45 batch-150
Running loss of epoch-45 batch-150 = 1.126178540289402e-05

Training epoch-45 batch-151
Running loss of epoch-45 batch-151 = 4.115980118513107e-06

Training epoch-45 batch-152
Running loss of epoch-45 batch-152 = 7.609371095895767e-06

Training epoch-45 batch-153
Running loss of epoch-45 batch-153 = 9.092502295970917e-06

Training epoch-45 batch-154
Running loss of epoch-45 batch-154 = 9.457813575863838e-06

Training epoch-45 batch-155
Running loss of epoch-45 batch-155 = 2.3768749088048935e-05

Training epoch-45 batch-156
Running loss of epoch-45 batch-156 = 9.986339136958122e-06

Training epoch-45 batch-157
Running loss of epoch-45 batch-157 = 2.171844244003296e-06

Finished training epoch-45.



Average train loss at epoch-45 = 8.315864205360413e-06

Started Evaluation

Average val loss at epoch-45 = 1.0712650296656623

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.49 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 63.70 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.69 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 72.56 %

Overall Accuracy = 83.21 %

Finished Evaluation



Started training epoch-46


Training epoch-46 batch-1
Running loss of epoch-46 batch-1 = 5.4747797548770905e-06

Training epoch-46 batch-2
Running loss of epoch-46 batch-2 = 7.61798582971096e-06

Training epoch-46 batch-3
Running loss of epoch-46 batch-3 = 7.592141628265381e-06

Training epoch-46 batch-4
Running loss of epoch-46 batch-4 = 6.987014785408974e-06

Training epoch-46 batch-5
Running loss of epoch-46 batch-5 = 8.849427103996277e-06

Training epoch-46 batch-6
Running loss of epoch-46 batch-6 = 1.707533374428749e-05

Training epoch-46 batch-7
Running loss of epoch-46 batch-7 = 6.810063496232033e-06

Training epoch-46 batch-8
Running loss of epoch-46 batch-8 = 5.39352186024189e-06

Training epoch-46 batch-9
Running loss of epoch-46 batch-9 = 9.476207196712494e-06

Training epoch-46 batch-10
Running loss of epoch-46 batch-10 = 1.9121915102005005e-05

Training epoch-46 batch-11
Running loss of epoch-46 batch-11 = 9.236857295036316e-06

Training epoch-46 batch-12
Running loss of epoch-46 batch-12 = 4.823319613933563e-06

Training epoch-46 batch-13
Running loss of epoch-46 batch-13 = 1.2637348845601082e-05

Training epoch-46 batch-14
Running loss of epoch-46 batch-14 = 1.0139308869838715e-05

Training epoch-46 batch-15
Running loss of epoch-46 batch-15 = 5.340436473488808e-06

Training epoch-46 batch-16
Running loss of epoch-46 batch-16 = 4.320638254284859e-06

Training epoch-46 batch-17
Running loss of epoch-46 batch-17 = 2.2100284695625305e-05

Training epoch-46 batch-18
Running loss of epoch-46 batch-18 = 9.644078090786934e-06

Training epoch-46 batch-19
Running loss of epoch-46 batch-19 = 3.891764208674431e-06

Training epoch-46 batch-20
Running loss of epoch-46 batch-20 = 5.368143320083618e-06

Training epoch-46 batch-21
Running loss of epoch-46 batch-21 = 9.877141565084457e-06

Training epoch-46 batch-22
Running loss of epoch-46 batch-22 = 1.8231570720672607e-05

Training epoch-46 batch-23
Running loss of epoch-46 batch-23 = 6.320653483271599e-06

Training epoch-46 batch-24
Running loss of epoch-46 batch-24 = 4.964182153344154e-06

Training epoch-46 batch-25
Running loss of epoch-46 batch-25 = 9.891809895634651e-06

Training epoch-46 batch-26
Running loss of epoch-46 batch-26 = 1.1450611054897308e-05

Training epoch-46 batch-27
Running loss of epoch-46 batch-27 = 5.730893462896347e-06

Training epoch-46 batch-28
Running loss of epoch-46 batch-28 = 3.3408869057893753e-06

Training epoch-46 batch-29
Running loss of epoch-46 batch-29 = 7.077469490468502e-06

Training epoch-46 batch-30
Running loss of epoch-46 batch-30 = 2.919929102063179e-06

Training epoch-46 batch-31
Running loss of epoch-46 batch-31 = 4.977453500032425e-06

Training epoch-46 batch-32
Running loss of epoch-46 batch-32 = 1.3015465810894966e-05

Training epoch-46 batch-33
Running loss of epoch-46 batch-33 = 9.05897468328476e-06

Training epoch-46 batch-34
Running loss of epoch-46 batch-34 = 3.823079168796539e-06

Training epoch-46 batch-35
Running loss of epoch-46 batch-35 = 2.8102658689022064e-06

Training epoch-46 batch-36
Running loss of epoch-46 batch-36 = 5.556503310799599e-06

Training epoch-46 batch-37
Running loss of epoch-46 batch-37 = 6.919726729393005e-06

Training epoch-46 batch-38
Running loss of epoch-46 batch-38 = 5.300622433423996e-06

Training epoch-46 batch-39
Running loss of epoch-46 batch-39 = 2.286536619067192e-05

Training epoch-46 batch-40
Running loss of epoch-46 batch-40 = 7.355120033025742e-06

Training epoch-46 batch-41
Running loss of epoch-46 batch-41 = 4.4673215597867966e-06

Training epoch-46 batch-42
Running loss of epoch-46 batch-42 = 7.844064384698868e-06

Training epoch-46 batch-43
Running loss of epoch-46 batch-43 = 3.995141014456749e-06

Training epoch-46 batch-44
Running loss of epoch-46 batch-44 = 6.727874279022217e-06

Training epoch-46 batch-45
Running loss of epoch-46 batch-45 = 7.591908797621727e-06

Training epoch-46 batch-46
Running loss of epoch-46 batch-46 = 5.304347723722458e-06

Training epoch-46 batch-47
Running loss of epoch-46 batch-47 = 1.6022007912397385e-05

Training epoch-46 batch-48
Running loss of epoch-46 batch-48 = 2.8689391911029816e-06

Training epoch-46 batch-49
Running loss of epoch-46 batch-49 = 1.203198917210102e-05

Training epoch-46 batch-50
Running loss of epoch-46 batch-50 = 1.3442011550068855e-05

Training epoch-46 batch-51
Running loss of epoch-46 batch-51 = 6.460351869463921e-06

Training epoch-46 batch-52
Running loss of epoch-46 batch-52 = 8.214730769395828e-06

Training epoch-46 batch-53
Running loss of epoch-46 batch-53 = 3.420282155275345e-06

Training epoch-46 batch-54
Running loss of epoch-46 batch-54 = 7.155817002058029e-06

Training epoch-46 batch-55
Running loss of epoch-46 batch-55 = 1.0468065738677979e-05

Training epoch-46 batch-56
Running loss of epoch-46 batch-56 = 1.1960277333855629e-05

Training epoch-46 batch-57
Running loss of epoch-46 batch-57 = 8.322764188051224e-06

Training epoch-46 batch-58
Running loss of epoch-46 batch-58 = 1.7622020095586777e-05

Training epoch-46 batch-59
Running loss of epoch-46 batch-59 = 3.535766154527664e-06

Training epoch-46 batch-60
Running loss of epoch-46 batch-60 = 9.966781362891197e-06

Training epoch-46 batch-61
Running loss of epoch-46 batch-61 = 3.3578835427761078e-06

Training epoch-46 batch-62
Running loss of epoch-46 batch-62 = 1.0743271559476852e-05

Training epoch-46 batch-63
Running loss of epoch-46 batch-63 = 9.273062460124493e-06

Training epoch-46 batch-64
Running loss of epoch-46 batch-64 = 9.473413228988647e-06

Training epoch-46 batch-65
Running loss of epoch-46 batch-65 = 8.973060175776482e-06

Training epoch-46 batch-66
Running loss of epoch-46 batch-66 = 3.114575520157814e-06

Training epoch-46 batch-67
Running loss of epoch-46 batch-67 = 4.417030140757561e-06

Training epoch-46 batch-68
Running loss of epoch-46 batch-68 = 1.1962605640292168e-05

Training epoch-46 batch-69
Running loss of epoch-46 batch-69 = 5.5031850934028625e-06

Training epoch-46 batch-70
Running loss of epoch-46 batch-70 = 3.90433706343174e-06

Training epoch-46 batch-71
Running loss of epoch-46 batch-71 = 1.7613638192415237e-06

Training epoch-46 batch-72
Running loss of epoch-46 batch-72 = 6.0426536947488785e-06

Training epoch-46 batch-73
Running loss of epoch-46 batch-73 = 4.62215393781662e-06

Training epoch-46 batch-74
Running loss of epoch-46 batch-74 = 5.86523674428463e-06

Training epoch-46 batch-75
Running loss of epoch-46 batch-75 = 3.9529986679553986e-06

Training epoch-46 batch-76
Running loss of epoch-46 batch-76 = 1.1893222108483315e-05

Training epoch-46 batch-77
Running loss of epoch-46 batch-77 = 9.038019925355911e-06

Training epoch-46 batch-78
Running loss of epoch-46 batch-78 = 3.988388925790787e-06

Training epoch-46 batch-79
Running loss of epoch-46 batch-79 = 3.8691796362400055e-06

Training epoch-46 batch-80
Running loss of epoch-46 batch-80 = 3.287568688392639e-06

Training epoch-46 batch-81
Running loss of epoch-46 batch-81 = 1.2131640687584877e-05

Training epoch-46 batch-82
Running loss of epoch-46 batch-82 = 5.4978299885988235e-06

Training epoch-46 batch-83
Running loss of epoch-46 batch-83 = 7.488997653126717e-06

Training epoch-46 batch-84
Running loss of epoch-46 batch-84 = 1.0942807421088219e-05

Training epoch-46 batch-85
Running loss of epoch-46 batch-85 = 6.1248429119586945e-06

Training epoch-46 batch-86
Running loss of epoch-46 batch-86 = 1.1350959539413452e-05

Training epoch-46 batch-87
Running loss of epoch-46 batch-87 = 3.416556864976883e-06

Training epoch-46 batch-88
Running loss of epoch-46 batch-88 = 4.148809239268303e-06

Training epoch-46 batch-89
Running loss of epoch-46 batch-89 = 1.1336291208863258e-05

Training epoch-46 batch-90
Running loss of epoch-46 batch-90 = 3.284541890025139e-06

Training epoch-46 batch-91
Running loss of epoch-46 batch-91 = 4.970235750079155e-06

Training epoch-46 batch-92
Running loss of epoch-46 batch-92 = 1.0024290531873703e-05

Training epoch-46 batch-93
Running loss of epoch-46 batch-93 = 8.605653420090675e-06

Training epoch-46 batch-94
Running loss of epoch-46 batch-94 = 1.8170569092035294e-05

Training epoch-46 batch-95
Running loss of epoch-46 batch-95 = 7.183291018009186e-06

Training epoch-46 batch-96
Running loss of epoch-46 batch-96 = 5.3283292800188065e-06

Training epoch-46 batch-97
Running loss of epoch-46 batch-97 = 3.925757482647896e-06

Training epoch-46 batch-98
Running loss of epoch-46 batch-98 = 1.4853663742542267e-05

Training epoch-46 batch-99
Running loss of epoch-46 batch-99 = 4.891771823167801e-06

Training epoch-46 batch-100
Running loss of epoch-46 batch-100 = 1.3265293091535568e-05

Training epoch-46 batch-101
Running loss of epoch-46 batch-101 = 4.594679921865463e-06

Training epoch-46 batch-102
Running loss of epoch-46 batch-102 = 4.760688170790672e-06

Training epoch-46 batch-103
Running loss of epoch-46 batch-103 = 1.0563991963863373e-05

Training epoch-46 batch-104
Running loss of epoch-46 batch-104 = 6.307847797870636e-06

Training epoch-46 batch-105
Running loss of epoch-46 batch-105 = 5.638226866722107e-06

Training epoch-46 batch-106
Running loss of epoch-46 batch-106 = 1.5070196241140366e-05

Training epoch-46 batch-107
Running loss of epoch-46 batch-107 = 2.746935933828354e-06

Training epoch-46 batch-108
Running loss of epoch-46 batch-108 = 8.557457476854324e-06

Training epoch-46 batch-109
Running loss of epoch-46 batch-109 = 6.320886313915253e-06

Training epoch-46 batch-110
Running loss of epoch-46 batch-110 = 7.519032806158066e-06

Training epoch-46 batch-111
Running loss of epoch-46 batch-111 = 1.1596828699111938e-05

Training epoch-46 batch-112
Running loss of epoch-46 batch-112 = 4.513189196586609e-06

Training epoch-46 batch-113
Running loss of epoch-46 batch-113 = 7.2552356868982315e-06

Training epoch-46 batch-114
Running loss of epoch-46 batch-114 = 1.9853468984365463e-06

Training epoch-46 batch-115
Running loss of epoch-46 batch-115 = 6.724614650011063e-06

Training epoch-46 batch-116
Running loss of epoch-46 batch-116 = 1.0358402505517006e-05

Training epoch-46 batch-117
Running loss of epoch-46 batch-117 = 3.748573362827301e-06

Training epoch-46 batch-118
Running loss of epoch-46 batch-118 = 1.321360468864441e-05

Training epoch-46 batch-119
Running loss of epoch-46 batch-119 = 8.371425792574883e-06

Training epoch-46 batch-120
Running loss of epoch-46 batch-120 = 1.1022202670574188e-05

Training epoch-46 batch-121
Running loss of epoch-46 batch-121 = 1.4099059626460075e-05

Training epoch-46 batch-122
Running loss of epoch-46 batch-122 = 7.41099938750267e-06

Training epoch-46 batch-123
Running loss of epoch-46 batch-123 = 7.26478174328804e-06

Training epoch-46 batch-124
Running loss of epoch-46 batch-124 = 4.830304533243179e-06

Training epoch-46 batch-125
Running loss of epoch-46 batch-125 = 5.481531843543053e-06

Training epoch-46 batch-126
Running loss of epoch-46 batch-126 = 8.230796083807945e-06

Training epoch-46 batch-127
Running loss of epoch-46 batch-127 = 8.287373930215836e-06

Training epoch-46 batch-128
Running loss of epoch-46 batch-128 = 9.385636076331139e-06

Training epoch-46 batch-129
Running loss of epoch-46 batch-129 = 1.3795914128422737e-05

Training epoch-46 batch-130
Running loss of epoch-46 batch-130 = 7.644062861800194e-06

Training epoch-46 batch-131
Running loss of epoch-46 batch-131 = 3.67150641977787e-06

Training epoch-46 batch-132
Running loss of epoch-46 batch-132 = 2.487795427441597e-06

Training epoch-46 batch-133
Running loss of epoch-46 batch-133 = 6.645219400525093e-06

Training epoch-46 batch-134
Running loss of epoch-46 batch-134 = 6.256392225623131e-06

Training epoch-46 batch-135
Running loss of epoch-46 batch-135 = 1.099705696105957e-05

Training epoch-46 batch-136
Running loss of epoch-46 batch-136 = 1.53491273522377e-05

Training epoch-46 batch-137
Running loss of epoch-46 batch-137 = 3.990950062870979e-06

Training epoch-46 batch-138
Running loss of epoch-46 batch-138 = 6.873393431305885e-06

Training epoch-46 batch-139
Running loss of epoch-46 batch-139 = 8.78400169312954e-06

Training epoch-46 batch-140
Running loss of epoch-46 batch-140 = 1.628044992685318e-05

Training epoch-46 batch-141
Running loss of epoch-46 batch-141 = 1.1697178706526756e-05

Training epoch-46 batch-142
Running loss of epoch-46 batch-142 = 8.099246770143509e-06

Training epoch-46 batch-143
Running loss of epoch-46 batch-143 = 5.4533593356609344e-06

Training epoch-46 batch-144
Running loss of epoch-46 batch-144 = 5.816807970404625e-06

Training epoch-46 batch-145
Running loss of epoch-46 batch-145 = 7.38177914172411e-06

Training epoch-46 batch-146
Running loss of epoch-46 batch-146 = 2.0990846678614616e-05

Training epoch-46 batch-147
Running loss of epoch-46 batch-147 = 7.513212040066719e-06

Training epoch-46 batch-148
Running loss of epoch-46 batch-148 = 5.5676791816949844e-06

Training epoch-46 batch-149
Running loss of epoch-46 batch-149 = 7.053138688206673e-06

Training epoch-46 batch-150
Running loss of epoch-46 batch-150 = 5.834270268678665e-06

Training epoch-46 batch-151
Running loss of epoch-46 batch-151 = 9.919865988194942e-06

Training epoch-46 batch-152
Running loss of epoch-46 batch-152 = 4.693400114774704e-06

Training epoch-46 batch-153
Running loss of epoch-46 batch-153 = 5.964189767837524e-06

Training epoch-46 batch-154
Running loss of epoch-46 batch-154 = 6.517977453768253e-06

Training epoch-46 batch-155
Running loss of epoch-46 batch-155 = 5.5907294154167175e-06

Training epoch-46 batch-156
Running loss of epoch-46 batch-156 = 1.058611087501049e-05

Training epoch-46 batch-157
Running loss of epoch-46 batch-157 = 4.344061017036438e-05

Finished training epoch-46.



Average train loss at epoch-46 = 8.070219308137894e-06

Started Evaluation

Average val loss at epoch-46 = 1.0655859750597039

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.60 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.71 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.27 %

Finished Evaluation



Started training epoch-47


Training epoch-47 batch-1
Running loss of epoch-47 batch-1 = 4.552304744720459e-06

Training epoch-47 batch-2
Running loss of epoch-47 batch-2 = 4.576984792947769e-06

Training epoch-47 batch-3
Running loss of epoch-47 batch-3 = 1.0449672117829323e-05

Training epoch-47 batch-4
Running loss of epoch-47 batch-4 = 1.0538613423705101e-05

Training epoch-47 batch-5
Running loss of epoch-47 batch-5 = 1.1399853974580765e-05

Training epoch-47 batch-6
Running loss of epoch-47 batch-6 = 5.539273843169212e-06

Training epoch-47 batch-7
Running loss of epoch-47 batch-7 = 5.396781489253044e-06

Training epoch-47 batch-8
Running loss of epoch-47 batch-8 = 7.416121661663055e-06

Training epoch-47 batch-9
Running loss of epoch-47 batch-9 = 8.351169526576996e-06

Training epoch-47 batch-10
Running loss of epoch-47 batch-10 = 5.375826731324196e-06

Training epoch-47 batch-11
Running loss of epoch-47 batch-11 = 3.932742401957512e-06

Training epoch-47 batch-12
Running loss of epoch-47 batch-12 = 4.952540621161461e-06

Training epoch-47 batch-13
Running loss of epoch-47 batch-13 = 6.246380507946014e-06

Training epoch-47 batch-14
Running loss of epoch-47 batch-14 = 4.4084154069423676e-06

Training epoch-47 batch-15
Running loss of epoch-47 batch-15 = 5.668029189109802e-06

Training epoch-47 batch-16
Running loss of epoch-47 batch-16 = 6.280606612563133e-06

Training epoch-47 batch-17
Running loss of epoch-47 batch-17 = 3.766268491744995e-06

Training epoch-47 batch-18
Running loss of epoch-47 batch-18 = 1.0475050657987595e-05

Training epoch-47 batch-19
Running loss of epoch-47 batch-19 = 1.5421188436448574e-05

Training epoch-47 batch-20
Running loss of epoch-47 batch-20 = 2.1602027118206024e-05

Training epoch-47 batch-21
Running loss of epoch-47 batch-21 = 7.943715900182724e-06

Training epoch-47 batch-22
Running loss of epoch-47 batch-22 = 7.566297426819801e-06

Training epoch-47 batch-23
Running loss of epoch-47 batch-23 = 4.087807610630989e-06

Training epoch-47 batch-24
Running loss of epoch-47 batch-24 = 2.7283094823360443e-06

Training epoch-47 batch-25
Running loss of epoch-47 batch-25 = 4.410510882735252e-06

Training epoch-47 batch-26
Running loss of epoch-47 batch-26 = 6.6785141825675964e-06

Training epoch-47 batch-27
Running loss of epoch-47 batch-27 = 2.4447916075587273e-05

Training epoch-47 batch-28
Running loss of epoch-47 batch-28 = 9.686686098575592e-06

Training epoch-47 batch-29
Running loss of epoch-47 batch-29 = 1.0040123015642166e-05

Training epoch-47 batch-30
Running loss of epoch-47 batch-30 = 1.1627096682786942e-05

Training epoch-47 batch-31
Running loss of epoch-47 batch-31 = 5.97233884036541e-06

Training epoch-47 batch-32
Running loss of epoch-47 batch-32 = 1.0799849405884743e-05

Training epoch-47 batch-33
Running loss of epoch-47 batch-33 = 5.45731745660305e-06

Training epoch-47 batch-34
Running loss of epoch-47 batch-34 = 1.2733740732073784e-05

Training epoch-47 batch-35
Running loss of epoch-47 batch-35 = 6.563495844602585e-06

Training epoch-47 batch-36
Running loss of epoch-47 batch-36 = 9.154900908470154e-06

Training epoch-47 batch-37
Running loss of epoch-47 batch-37 = 8.668284863233566e-06

Training epoch-47 batch-38
Running loss of epoch-47 batch-38 = 6.379559636116028e-06

Training epoch-47 batch-39
Running loss of epoch-47 batch-39 = 1.6689300537109375e-06

Training epoch-47 batch-40
Running loss of epoch-47 batch-40 = 9.640585631132126e-06

Training epoch-47 batch-41
Running loss of epoch-47 batch-41 = 9.510666131973267e-06

Training epoch-47 batch-42
Running loss of epoch-47 batch-42 = 5.658948794007301e-06

Training epoch-47 batch-43
Running loss of epoch-47 batch-43 = 5.419133231043816e-06

Training epoch-47 batch-44
Running loss of epoch-47 batch-44 = 6.979797035455704e-06

Training epoch-47 batch-45
Running loss of epoch-47 batch-45 = 1.4060176908969879e-05

Training epoch-47 batch-46
Running loss of epoch-47 batch-46 = 7.876893505454063e-06

Training epoch-47 batch-47
Running loss of epoch-47 batch-47 = 2.8908252716064453e-06

Training epoch-47 batch-48
Running loss of epoch-47 batch-48 = 1.0911142453551292e-05

Training epoch-47 batch-49
Running loss of epoch-47 batch-49 = 4.049856215715408e-06

Training epoch-47 batch-50
Running loss of epoch-47 batch-50 = 9.42847691476345e-06

Training epoch-47 batch-51
Running loss of epoch-47 batch-51 = 6.5052881836891174e-06

Training epoch-47 batch-52
Running loss of epoch-47 batch-52 = 9.217532351613045e-06

Training epoch-47 batch-53
Running loss of epoch-47 batch-53 = 6.954418495297432e-06

Training epoch-47 batch-54
Running loss of epoch-47 batch-54 = 4.7299545258283615e-06

Training epoch-47 batch-55
Running loss of epoch-47 batch-55 = 8.716946467757225e-06

Training epoch-47 batch-56
Running loss of epoch-47 batch-56 = 4.308763891458511e-06

Training epoch-47 batch-57
Running loss of epoch-47 batch-57 = 6.7148357629776e-06

Training epoch-47 batch-58
Running loss of epoch-47 batch-58 = 6.023445166647434e-06

Training epoch-47 batch-59
Running loss of epoch-47 batch-59 = 9.36770811676979e-06

Training epoch-47 batch-60
Running loss of epoch-47 batch-60 = 5.106208845973015e-06

Training epoch-47 batch-61
Running loss of epoch-47 batch-61 = 1.4739343896508217e-05

Training epoch-47 batch-62
Running loss of epoch-47 batch-62 = 1.3006734661757946e-05

Training epoch-47 batch-63
Running loss of epoch-47 batch-63 = 1.07521191239357e-05

Training epoch-47 batch-64
Running loss of epoch-47 batch-64 = 6.120651960372925e-06

Training epoch-47 batch-65
Running loss of epoch-47 batch-65 = 8.691800758242607e-06

Training epoch-47 batch-66
Running loss of epoch-47 batch-66 = 8.070841431617737e-06

Training epoch-47 batch-67
Running loss of epoch-47 batch-67 = 5.24311326444149e-06

Training epoch-47 batch-68
Running loss of epoch-47 batch-68 = 7.657799869775772e-06

Training epoch-47 batch-69
Running loss of epoch-47 batch-69 = 1.1350028216838837e-05

Training epoch-47 batch-70
Running loss of epoch-47 batch-70 = 6.792834028601646e-06

Training epoch-47 batch-71
Running loss of epoch-47 batch-71 = 7.737427949905396e-06

Training epoch-47 batch-72
Running loss of epoch-47 batch-72 = 4.922039806842804e-06

Training epoch-47 batch-73
Running loss of epoch-47 batch-73 = 6.469665095210075e-06

Training epoch-47 batch-74
Running loss of epoch-47 batch-74 = 7.446855306625366e-06

Training epoch-47 batch-75
Running loss of epoch-47 batch-75 = 7.532769814133644e-06

Training epoch-47 batch-76
Running loss of epoch-47 batch-76 = 6.955815479159355e-06

Training epoch-47 batch-77
Running loss of epoch-47 batch-77 = 5.554873496294022e-06

Training epoch-47 batch-78
Running loss of epoch-47 batch-78 = 8.923932909965515e-06

Training epoch-47 batch-79
Running loss of epoch-47 batch-79 = 5.166279152035713e-06

Training epoch-47 batch-80
Running loss of epoch-47 batch-80 = 8.790520951151848e-06

Training epoch-47 batch-81
Running loss of epoch-47 batch-81 = 6.050104275345802e-06

Training epoch-47 batch-82
Running loss of epoch-47 batch-82 = 8.775154128670692e-06

Training epoch-47 batch-83
Running loss of epoch-47 batch-83 = 3.898050636053085e-06

Training epoch-47 batch-84
Running loss of epoch-47 batch-84 = 5.29061071574688e-06

Training epoch-47 batch-85
Running loss of epoch-47 batch-85 = 9.860843420028687e-06

Training epoch-47 batch-86
Running loss of epoch-47 batch-86 = 7.644528523087502e-06

Training epoch-47 batch-87
Running loss of epoch-47 batch-87 = 6.532995030283928e-06

Training epoch-47 batch-88
Running loss of epoch-47 batch-88 = 4.602828994393349e-06

Training epoch-47 batch-89
Running loss of epoch-47 batch-89 = 5.955575034022331e-06

Training epoch-47 batch-90
Running loss of epoch-47 batch-90 = 2.9115471988916397e-06

Training epoch-47 batch-91
Running loss of epoch-47 batch-91 = 2.3366883397102356e-06

Training epoch-47 batch-92
Running loss of epoch-47 batch-92 = 4.542293027043343e-06

Training epoch-47 batch-93
Running loss of epoch-47 batch-93 = 3.1155068427324295e-06

Training epoch-47 batch-94
Running loss of epoch-47 batch-94 = 2.7085538022220135e-05

Training epoch-47 batch-95
Running loss of epoch-47 batch-95 = 1.175818033516407e-05

Training epoch-47 batch-96
Running loss of epoch-47 batch-96 = 1.1831289157271385e-05

Training epoch-47 batch-97
Running loss of epoch-47 batch-97 = 5.44707290828228e-06

Training epoch-47 batch-98
Running loss of epoch-47 batch-98 = 5.9371814131736755e-06

Training epoch-47 batch-99
Running loss of epoch-47 batch-99 = 4.211440682411194e-06

Training epoch-47 batch-100
Running loss of epoch-47 batch-100 = 7.533002644777298e-06

Training epoch-47 batch-101
Running loss of epoch-47 batch-101 = 1.7858808860182762e-05

Training epoch-47 batch-102
Running loss of epoch-47 batch-102 = 5.7085417211055756e-06

Training epoch-47 batch-103
Running loss of epoch-47 batch-103 = 8.980277925729752e-06

Training epoch-47 batch-104
Running loss of epoch-47 batch-104 = 1.986767165362835e-05

Training epoch-47 batch-105
Running loss of epoch-47 batch-105 = 5.783280357718468e-06

Training epoch-47 batch-106
Running loss of epoch-47 batch-106 = 1.1278782039880753e-05

Training epoch-47 batch-107
Running loss of epoch-47 batch-107 = 8.945586159825325e-06

Training epoch-47 batch-108
Running loss of epoch-47 batch-108 = 6.434042006731033e-06

Training epoch-47 batch-109
Running loss of epoch-47 batch-109 = 1.3715471141040325e-05

Training epoch-47 batch-110
Running loss of epoch-47 batch-110 = 7.327646017074585e-06

Training epoch-47 batch-111
Running loss of epoch-47 batch-111 = 6.112968549132347e-06

Training epoch-47 batch-112
Running loss of epoch-47 batch-112 = 6.139744073152542e-06

Training epoch-47 batch-113
Running loss of epoch-47 batch-113 = 5.659181624650955e-06

Training epoch-47 batch-114
Running loss of epoch-47 batch-114 = 3.6228448152542114e-06

Training epoch-47 batch-115
Running loss of epoch-47 batch-115 = 1.4397315680980682e-05

Training epoch-47 batch-116
Running loss of epoch-47 batch-116 = 3.3550895750522614e-06

Training epoch-47 batch-117
Running loss of epoch-47 batch-117 = 1.0278308764100075e-05

Training epoch-47 batch-118
Running loss of epoch-47 batch-118 = 1.2326287105679512e-05

Training epoch-47 batch-119
Running loss of epoch-47 batch-119 = 6.281537935137749e-06

Training epoch-47 batch-120
Running loss of epoch-47 batch-120 = 6.766756996512413e-06

Training epoch-47 batch-121
Running loss of epoch-47 batch-121 = 1.9476283341646194e-06

Training epoch-47 batch-122
Running loss of epoch-47 batch-122 = 7.728580385446548e-06

Training epoch-47 batch-123
Running loss of epoch-47 batch-123 = 2.0470470190048218e-05

Training epoch-47 batch-124
Running loss of epoch-47 batch-124 = 2.9641669243574142e-06

Training epoch-47 batch-125
Running loss of epoch-47 batch-125 = 8.717179298400879e-06

Training epoch-47 batch-126
Running loss of epoch-47 batch-126 = 1.0533025488257408e-05

Training epoch-47 batch-127
Running loss of epoch-47 batch-127 = 2.334360033273697e-06

Training epoch-47 batch-128
Running loss of epoch-47 batch-128 = 3.364402800798416e-06

Training epoch-47 batch-129
Running loss of epoch-47 batch-129 = 3.487570211291313e-06

Training epoch-47 batch-130
Running loss of epoch-47 batch-130 = 1.1358177289366722e-05

Training epoch-47 batch-131
Running loss of epoch-47 batch-131 = 4.793982952833176e-06

Training epoch-47 batch-132
Running loss of epoch-47 batch-132 = 3.9904844015836716e-06

Training epoch-47 batch-133
Running loss of epoch-47 batch-133 = 4.414701834321022e-06

Training epoch-47 batch-134
Running loss of epoch-47 batch-134 = 5.588401108980179e-06

Training epoch-47 batch-135
Running loss of epoch-47 batch-135 = 7.207505404949188e-06

Training epoch-47 batch-136
Running loss of epoch-47 batch-136 = 4.189321771264076e-06

Training epoch-47 batch-137
Running loss of epoch-47 batch-137 = 1.3520941138267517e-05

Training epoch-47 batch-138
Running loss of epoch-47 batch-138 = 6.817514076828957e-06

Training epoch-47 batch-139
Running loss of epoch-47 batch-139 = 3.7690624594688416e-06

Training epoch-47 batch-140
Running loss of epoch-47 batch-140 = 6.573973223567009e-06

Training epoch-47 batch-141
Running loss of epoch-47 batch-141 = 6.0766469687223434e-06

Training epoch-47 batch-142
Running loss of epoch-47 batch-142 = 3.155320882797241e-06

Training epoch-47 batch-143
Running loss of epoch-47 batch-143 = 7.910886779427528e-06

Training epoch-47 batch-144
Running loss of epoch-47 batch-144 = 1.2442469596862793e-05

Training epoch-47 batch-145
Running loss of epoch-47 batch-145 = 8.902745321393013e-06

Training epoch-47 batch-146
Running loss of epoch-47 batch-146 = 2.007465809583664e-06

Training epoch-47 batch-147
Running loss of epoch-47 batch-147 = 5.6587159633636475e-06

Training epoch-47 batch-148
Running loss of epoch-47 batch-148 = 1.0842224583029747e-05

Training epoch-47 batch-149
Running loss of epoch-47 batch-149 = 5.123903974890709e-06

Training epoch-47 batch-150
Running loss of epoch-47 batch-150 = 7.598660886287689e-06

Training epoch-47 batch-151
Running loss of epoch-47 batch-151 = 7.945811375975609e-06

Training epoch-47 batch-152
Running loss of epoch-47 batch-152 = 7.926253601908684e-06

Training epoch-47 batch-153
Running loss of epoch-47 batch-153 = 8.827075362205505e-06

Training epoch-47 batch-154
Running loss of epoch-47 batch-154 = 7.743015885353088e-06

Training epoch-47 batch-155
Running loss of epoch-47 batch-155 = 1.0152580216526985e-05

Training epoch-47 batch-156
Running loss of epoch-47 batch-156 = 1.8513761460781097e-05

Training epoch-47 batch-157
Running loss of epoch-47 batch-157 = 1.4942139387130737e-05

Finished training epoch-47.



Average train loss at epoch-47 = 7.830127328634262e-06

Started Evaluation

Average val loss at epoch-47 = 1.0846444968969393

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.49 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 61.42 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.91 %
Accuracy for class execute is: 44.98 %
Accuracy for class get is: 70.00 %

Overall Accuracy = 83.00 %

Finished Evaluation



Started training epoch-48


Training epoch-48 batch-1
Running loss of epoch-48 batch-1 = 1.0848743841052055e-05

Training epoch-48 batch-2
Running loss of epoch-48 batch-2 = 2.4547334760427475e-06

Training epoch-48 batch-3
Running loss of epoch-48 batch-3 = 5.281995981931686e-06

Training epoch-48 batch-4
Running loss of epoch-48 batch-4 = 1.6344012692570686e-05

Training epoch-48 batch-5
Running loss of epoch-48 batch-5 = 5.137640982866287e-06

Training epoch-48 batch-6
Running loss of epoch-48 batch-6 = 7.5730495154857635e-06

Training epoch-48 batch-7
Running loss of epoch-48 batch-7 = 6.159767508506775e-06

Training epoch-48 batch-8
Running loss of epoch-48 batch-8 = 7.099006325006485e-06

Training epoch-48 batch-9
Running loss of epoch-48 batch-9 = 7.669441401958466e-06

Training epoch-48 batch-10
Running loss of epoch-48 batch-10 = 8.108792826533318e-06

Training epoch-48 batch-11
Running loss of epoch-48 batch-11 = 4.06801700592041e-06

Training epoch-48 batch-12
Running loss of epoch-48 batch-12 = 1.5361234545707703e-05

Training epoch-48 batch-13
Running loss of epoch-48 batch-13 = 8.789356797933578e-06

Training epoch-48 batch-14
Running loss of epoch-48 batch-14 = 1.1953990906476974e-05

Training epoch-48 batch-15
Running loss of epoch-48 batch-15 = 8.599832653999329e-06

Training epoch-48 batch-16
Running loss of epoch-48 batch-16 = 5.333218723535538e-06

Training epoch-48 batch-17
Running loss of epoch-48 batch-17 = 6.112735718488693e-06

Training epoch-48 batch-18
Running loss of epoch-48 batch-18 = 5.1907263696193695e-06

Training epoch-48 batch-19
Running loss of epoch-48 batch-19 = 1.0765856131911278e-05

Training epoch-48 batch-20
Running loss of epoch-48 batch-20 = 6.365589797496796e-06

Training epoch-48 batch-21
Running loss of epoch-48 batch-21 = 2.1883752197027206e-06

Training epoch-48 batch-22
Running loss of epoch-48 batch-22 = 6.699236109852791e-06

Training epoch-48 batch-23
Running loss of epoch-48 batch-23 = 4.889210686087608e-06

Training epoch-48 batch-24
Running loss of epoch-48 batch-24 = 7.643364369869232e-06

Training epoch-48 batch-25
Running loss of epoch-48 batch-25 = 6.242888048291206e-06

Training epoch-48 batch-26
Running loss of epoch-48 batch-26 = 1.9769417122006416e-05

Training epoch-48 batch-27
Running loss of epoch-48 batch-27 = 8.901581168174744e-06

Training epoch-48 batch-28
Running loss of epoch-48 batch-28 = 3.1387899070978165e-06

Training epoch-48 batch-29
Running loss of epoch-48 batch-29 = 6.406800821423531e-06

Training epoch-48 batch-30
Running loss of epoch-48 batch-30 = 4.102010279893875e-06

Training epoch-48 batch-31
Running loss of epoch-48 batch-31 = 5.245208740234375e-06

Training epoch-48 batch-32
Running loss of epoch-48 batch-32 = 4.466855898499489e-06

Training epoch-48 batch-33
Running loss of epoch-48 batch-33 = 1.5175668522715569e-05

Training epoch-48 batch-34
Running loss of epoch-48 batch-34 = 7.194699719548225e-06

Training epoch-48 batch-35
Running loss of epoch-48 batch-35 = 4.001893103122711e-06

Training epoch-48 batch-36
Running loss of epoch-48 batch-36 = 4.98979352414608e-06

Training epoch-48 batch-37
Running loss of epoch-48 batch-37 = 2.7692876756191254e-06

Training epoch-48 batch-38
Running loss of epoch-48 batch-38 = 6.424030289053917e-06

Training epoch-48 batch-39
Running loss of epoch-48 batch-39 = 8.013099431991577e-06

Training epoch-48 batch-40
Running loss of epoch-48 batch-40 = 3.670109435915947e-06

Training epoch-48 batch-41
Running loss of epoch-48 batch-41 = 9.089126251637936e-06

Training epoch-48 batch-42
Running loss of epoch-48 batch-42 = 5.32856211066246e-06

Training epoch-48 batch-43
Running loss of epoch-48 batch-43 = 3.4568365663290024e-06

Training epoch-48 batch-44
Running loss of epoch-48 batch-44 = 5.330890417098999e-06

Training epoch-48 batch-45
Running loss of epoch-48 batch-45 = 8.690869435667992e-06

Training epoch-48 batch-46
Running loss of epoch-48 batch-46 = 3.4065451472997665e-06

Training epoch-48 batch-47
Running loss of epoch-48 batch-47 = 1.6680685803294182e-05

Training epoch-48 batch-48
Running loss of epoch-48 batch-48 = 6.718561053276062e-06

Training epoch-48 batch-49
Running loss of epoch-48 batch-49 = 6.353016942739487e-06

Training epoch-48 batch-50
Running loss of epoch-48 batch-50 = 4.074303433299065e-06

Training epoch-48 batch-51
Running loss of epoch-48 batch-51 = 8.260132744908333e-06

Training epoch-48 batch-52
Running loss of epoch-48 batch-52 = 3.4803524613380432e-06

Training epoch-48 batch-53
Running loss of epoch-48 batch-53 = 5.309004336595535e-06

Training epoch-48 batch-54
Running loss of epoch-48 batch-54 = 1.4626653864979744e-05

Training epoch-48 batch-55
Running loss of epoch-48 batch-55 = 9.693903848528862e-06

Training epoch-48 batch-56
Running loss of epoch-48 batch-56 = 6.729969754815102e-06

Training epoch-48 batch-57
Running loss of epoch-48 batch-57 = 5.130423232913017e-06

Training epoch-48 batch-58
Running loss of epoch-48 batch-58 = 1.153792254626751e-05

Training epoch-48 batch-59
Running loss of epoch-48 batch-59 = 4.828907549381256e-06

Training epoch-48 batch-60
Running loss of epoch-48 batch-60 = 7.581431418657303e-06

Training epoch-48 batch-61
Running loss of epoch-48 batch-61 = 1.7923302948474884e-06

Training epoch-48 batch-62
Running loss of epoch-48 batch-62 = 9.209616109728813e-06

Training epoch-48 batch-63
Running loss of epoch-48 batch-63 = 7.369089871644974e-06

Training epoch-48 batch-64
Running loss of epoch-48 batch-64 = 6.869900971651077e-06

Training epoch-48 batch-65
Running loss of epoch-48 batch-65 = 5.611218512058258e-06

Training epoch-48 batch-66
Running loss of epoch-48 batch-66 = 5.468027666211128e-06

Training epoch-48 batch-67
Running loss of epoch-48 batch-67 = 3.276858478784561e-06

Training epoch-48 batch-68
Running loss of epoch-48 batch-68 = 1.776008866727352e-05

Training epoch-48 batch-69
Running loss of epoch-48 batch-69 = 3.815162926912308e-06

Training epoch-48 batch-70
Running loss of epoch-48 batch-70 = 9.997747838497162e-06

Training epoch-48 batch-71
Running loss of epoch-48 batch-71 = 1.0104384273290634e-05

Training epoch-48 batch-72
Running loss of epoch-48 batch-72 = 1.566787250339985e-05

Training epoch-48 batch-73
Running loss of epoch-48 batch-73 = 3.711087629199028e-06

Training epoch-48 batch-74
Running loss of epoch-48 batch-74 = 4.051486030220985e-06

Training epoch-48 batch-75
Running loss of epoch-48 batch-75 = 8.713104762136936e-06

Training epoch-48 batch-76
Running loss of epoch-48 batch-76 = 4.622386768460274e-06

Training epoch-48 batch-77
Running loss of epoch-48 batch-77 = 7.867347449064255e-06

Training epoch-48 batch-78
Running loss of epoch-48 batch-78 = 1.3076001778244972e-05

Training epoch-48 batch-79
Running loss of epoch-48 batch-79 = 1.0615447536110878e-05

Training epoch-48 batch-80
Running loss of epoch-48 batch-80 = 3.4440308809280396e-06

Training epoch-48 batch-81
Running loss of epoch-48 batch-81 = 1.2095319107174873e-05

Training epoch-48 batch-82
Running loss of epoch-48 batch-82 = 5.966518074274063e-06

Training epoch-48 batch-83
Running loss of epoch-48 batch-83 = 3.842636942863464e-06

Training epoch-48 batch-84
Running loss of epoch-48 batch-84 = 5.891080945730209e-06

Training epoch-48 batch-85
Running loss of epoch-48 batch-85 = 5.535082891583443e-06

Training epoch-48 batch-86
Running loss of epoch-48 batch-86 = 1.0831281542778015e-05

Training epoch-48 batch-87
Running loss of epoch-48 batch-87 = 7.1052927523851395e-06

Training epoch-48 batch-88
Running loss of epoch-48 batch-88 = 6.984220817685127e-06

Training epoch-48 batch-89
Running loss of epoch-48 batch-89 = 1.154397614300251e-05

Training epoch-48 batch-90
Running loss of epoch-48 batch-90 = 7.6070427894592285e-06

Training epoch-48 batch-91
Running loss of epoch-48 batch-91 = 5.929963663220406e-06

Training epoch-48 batch-92
Running loss of epoch-48 batch-92 = 3.896886482834816e-06

Training epoch-48 batch-93
Running loss of epoch-48 batch-93 = 2.5002285838127136e-05

Training epoch-48 batch-94
Running loss of epoch-48 batch-94 = 3.5457778722047806e-06

Training epoch-48 batch-95
Running loss of epoch-48 batch-95 = 3.312714397907257e-06

Training epoch-48 batch-96
Running loss of epoch-48 batch-96 = 8.050817996263504e-06

Training epoch-48 batch-97
Running loss of epoch-48 batch-97 = 6.998889148235321e-06

Training epoch-48 batch-98
Running loss of epoch-48 batch-98 = 6.997375749051571e-06

Training epoch-48 batch-99
Running loss of epoch-48 batch-99 = 2.789543941617012e-06

Training epoch-48 batch-100
Running loss of epoch-48 batch-100 = 6.513437256217003e-06

Training epoch-48 batch-101
Running loss of epoch-48 batch-101 = 7.455935701727867e-06

Training epoch-48 batch-102
Running loss of epoch-48 batch-102 = 6.560236215591431e-06

Training epoch-48 batch-103
Running loss of epoch-48 batch-103 = 5.746958777308464e-06

Training epoch-48 batch-104
Running loss of epoch-48 batch-104 = 8.791452273726463e-06

Training epoch-48 batch-105
Running loss of epoch-48 batch-105 = 9.281560778617859e-06

Training epoch-48 batch-106
Running loss of epoch-48 batch-106 = 2.167397178709507e-05

Training epoch-48 batch-107
Running loss of epoch-48 batch-107 = 5.414243787527084e-06

Training epoch-48 batch-108
Running loss of epoch-48 batch-108 = 1.6079284250736237e-05

Training epoch-48 batch-109
Running loss of epoch-48 batch-109 = 1.4791963621973991e-05

Training epoch-48 batch-110
Running loss of epoch-48 batch-110 = 7.271068170666695e-06

Training epoch-48 batch-111
Running loss of epoch-48 batch-111 = 1.1301715858280659e-05

Training epoch-48 batch-112
Running loss of epoch-48 batch-112 = 7.438473403453827e-06

Training epoch-48 batch-113
Running loss of epoch-48 batch-113 = 6.804009899497032e-06

Training epoch-48 batch-114
Running loss of epoch-48 batch-114 = 3.360910341143608e-06

Training epoch-48 batch-115
Running loss of epoch-48 batch-115 = 5.00120222568512e-06

Training epoch-48 batch-116
Running loss of epoch-48 batch-116 = 4.291068762540817e-06

Training epoch-48 batch-117
Running loss of epoch-48 batch-117 = 5.6158751249313354e-06

Training epoch-48 batch-118
Running loss of epoch-48 batch-118 = 4.185596480965614e-06

Training epoch-48 batch-119
Running loss of epoch-48 batch-119 = 6.415648385882378e-06

Training epoch-48 batch-120
Running loss of epoch-48 batch-120 = 1.036585308611393e-05

Training epoch-48 batch-121
Running loss of epoch-48 batch-121 = 4.683155566453934e-06

Training epoch-48 batch-122
Running loss of epoch-48 batch-122 = 1.717754639685154e-05

Training epoch-48 batch-123
Running loss of epoch-48 batch-123 = 7.444759830832481e-06

Training epoch-48 batch-124
Running loss of epoch-48 batch-124 = 5.913432687520981e-06

Training epoch-48 batch-125
Running loss of epoch-48 batch-125 = 6.289919838309288e-06

Training epoch-48 batch-126
Running loss of epoch-48 batch-126 = 6.11436553299427e-06

Training epoch-48 batch-127
Running loss of epoch-48 batch-127 = 4.379311576485634e-06

Training epoch-48 batch-128
Running loss of epoch-48 batch-128 = 3.557885065674782e-06

Training epoch-48 batch-129
Running loss of epoch-48 batch-129 = 4.373025149106979e-06

Training epoch-48 batch-130
Running loss of epoch-48 batch-130 = 8.947914466261864e-06

Training epoch-48 batch-131
Running loss of epoch-48 batch-131 = 2.0544975996017456e-06

Training epoch-48 batch-132
Running loss of epoch-48 batch-132 = 5.074078217148781e-06

Training epoch-48 batch-133
Running loss of epoch-48 batch-133 = 4.305271431803703e-06

Training epoch-48 batch-134
Running loss of epoch-48 batch-134 = 9.450595825910568e-06

Training epoch-48 batch-135
Running loss of epoch-48 batch-135 = 4.941131919622421e-06

Training epoch-48 batch-136
Running loss of epoch-48 batch-136 = 6.557907909154892e-06

Training epoch-48 batch-137
Running loss of epoch-48 batch-137 = 4.118890501558781e-06

Training epoch-48 batch-138
Running loss of epoch-48 batch-138 = 9.293202310800552e-06

Training epoch-48 batch-139
Running loss of epoch-48 batch-139 = 1.1269468814134598e-05

Training epoch-48 batch-140
Running loss of epoch-48 batch-140 = 6.902497261762619e-06

Training epoch-48 batch-141
Running loss of epoch-48 batch-141 = 3.4496188163757324e-06

Training epoch-48 batch-142
Running loss of epoch-48 batch-142 = 5.887588486075401e-06

Training epoch-48 batch-143
Running loss of epoch-48 batch-143 = 8.595641702413559e-06

Training epoch-48 batch-144
Running loss of epoch-48 batch-144 = 6.082933396100998e-06

Training epoch-48 batch-145
Running loss of epoch-48 batch-145 = 1.484481617808342e-05

Training epoch-48 batch-146
Running loss of epoch-48 batch-146 = 4.952540621161461e-06

Training epoch-48 batch-147
Running loss of epoch-48 batch-147 = 1.6247620806097984e-05

Training epoch-48 batch-148
Running loss of epoch-48 batch-148 = 3.0007213354110718e-06

Training epoch-48 batch-149
Running loss of epoch-48 batch-149 = 1.091882586479187e-05

Training epoch-48 batch-150
Running loss of epoch-48 batch-150 = 9.329640306532383e-06

Training epoch-48 batch-151
Running loss of epoch-48 batch-151 = 4.0817540138959885e-06

Training epoch-48 batch-152
Running loss of epoch-48 batch-152 = 1.6484875231981277e-05

Training epoch-48 batch-153
Running loss of epoch-48 batch-153 = 6.906688213348389e-06

Training epoch-48 batch-154
Running loss of epoch-48 batch-154 = 5.30388206243515e-06

Training epoch-48 batch-155
Running loss of epoch-48 batch-155 = 3.7446152418851852e-06

Training epoch-48 batch-156
Running loss of epoch-48 batch-156 = 3.995606675744057e-06

Training epoch-48 batch-157
Running loss of epoch-48 batch-157 = 1.944601535797119e-05

Finished training epoch-48.



Average train loss at epoch-48 = 7.490088045597076e-06

Started Evaluation

Average val loss at epoch-48 = 1.078368203264685

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 63.70 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 70.77 %

Overall Accuracy = 83.19 %

Finished Evaluation



Started training epoch-49


Training epoch-49 batch-1
Running loss of epoch-49 batch-1 = 7.684342563152313e-06

Training epoch-49 batch-2
Running loss of epoch-49 batch-2 = 1.5012919902801514e-05

Training epoch-49 batch-3
Running loss of epoch-49 batch-3 = 1.0188436135649681e-05

Training epoch-49 batch-4
Running loss of epoch-49 batch-4 = 4.947185516357422e-06

Training epoch-49 batch-5
Running loss of epoch-49 batch-5 = 8.176546543836594e-06

Training epoch-49 batch-6
Running loss of epoch-49 batch-6 = 5.450565367937088e-06

Training epoch-49 batch-7
Running loss of epoch-49 batch-7 = 7.535330951213837e-06

Training epoch-49 batch-8
Running loss of epoch-49 batch-8 = 5.031470209360123e-06

Training epoch-49 batch-9
Running loss of epoch-49 batch-9 = 9.556068107485771e-06

Training epoch-49 batch-10
Running loss of epoch-49 batch-10 = 8.092029020190239e-06

Training epoch-49 batch-11
Running loss of epoch-49 batch-11 = 7.522990927100182e-06

Training epoch-49 batch-12
Running loss of epoch-49 batch-12 = 7.370486855506897e-06

Training epoch-49 batch-13
Running loss of epoch-49 batch-13 = 2.9953662306070328e-06

Training epoch-49 batch-14
Running loss of epoch-49 batch-14 = 3.3336691558361053e-06

Training epoch-49 batch-15
Running loss of epoch-49 batch-15 = 8.917413651943207e-06

Training epoch-49 batch-16
Running loss of epoch-49 batch-16 = 4.9120280891656876e-06

Training epoch-49 batch-17
Running loss of epoch-49 batch-17 = 4.27616760134697e-06

Training epoch-49 batch-18
Running loss of epoch-49 batch-18 = 4.013534635305405e-06

Training epoch-49 batch-19
Running loss of epoch-49 batch-19 = 3.844965249300003e-06

Training epoch-49 batch-20
Running loss of epoch-49 batch-20 = 4.667323082685471e-06

Training epoch-49 batch-21
Running loss of epoch-49 batch-21 = 1.2455973774194717e-05

Training epoch-49 batch-22
Running loss of epoch-49 batch-22 = 3.703869879245758e-06

Training epoch-49 batch-23
Running loss of epoch-49 batch-23 = 6.271060556173325e-06

Training epoch-49 batch-24
Running loss of epoch-49 batch-24 = 6.5802596509456635e-06

Training epoch-49 batch-25
Running loss of epoch-49 batch-25 = 6.702495738863945e-06

Training epoch-49 batch-26
Running loss of epoch-49 batch-26 = 6.0212332755327225e-06

Training epoch-49 batch-27
Running loss of epoch-49 batch-27 = 5.579087883234024e-06

Training epoch-49 batch-28
Running loss of epoch-49 batch-28 = 5.960697308182716e-06

Training epoch-49 batch-29
Running loss of epoch-49 batch-29 = 5.481066182255745e-06

Training epoch-49 batch-30
Running loss of epoch-49 batch-30 = 1.1905096471309662e-05

Training epoch-49 batch-31
Running loss of epoch-49 batch-31 = 1.2861564755439758e-05

Training epoch-49 batch-32
Running loss of epoch-49 batch-32 = 2.2728927433490753e-06

Training epoch-49 batch-33
Running loss of epoch-49 batch-33 = 8.758390322327614e-06

Training epoch-49 batch-34
Running loss of epoch-49 batch-34 = 4.394678398966789e-06

Training epoch-49 batch-35
Running loss of epoch-49 batch-35 = 1.0587042197585106e-05

Training epoch-49 batch-36
Running loss of epoch-49 batch-36 = 3.5241246223449707e-06

Training epoch-49 batch-37
Running loss of epoch-49 batch-37 = 1.2106029316782951e-05

Training epoch-49 batch-38
Running loss of epoch-49 batch-38 = 4.90737147629261e-06

Training epoch-49 batch-39
Running loss of epoch-49 batch-39 = 7.032882422208786e-06

Training epoch-49 batch-40
Running loss of epoch-49 batch-40 = 9.852694347500801e-06

Training epoch-49 batch-41
Running loss of epoch-49 batch-41 = 2.032378688454628e-06

Training epoch-49 batch-42
Running loss of epoch-49 batch-42 = 4.342058673501015e-06

Training epoch-49 batch-43
Running loss of epoch-49 batch-43 = 1.7980113625526428e-05

Training epoch-49 batch-44
Running loss of epoch-49 batch-44 = 1.9033439457416534e-05

Training epoch-49 batch-45
Running loss of epoch-49 batch-45 = 5.636829882860184e-06

Training epoch-49 batch-46
Running loss of epoch-49 batch-46 = 8.046859875321388e-06

Training epoch-49 batch-47
Running loss of epoch-49 batch-47 = 3.1066592782735825e-06

Training epoch-49 batch-48
Running loss of epoch-49 batch-48 = 5.879439413547516e-06

Training epoch-49 batch-49
Running loss of epoch-49 batch-49 = 1.0759569704532623e-05

Training epoch-49 batch-50
Running loss of epoch-49 batch-50 = 5.79003244638443e-06

Training epoch-49 batch-51
Running loss of epoch-49 batch-51 = 8.287373930215836e-06

Training epoch-49 batch-52
Running loss of epoch-49 batch-52 = 6.334856152534485e-06

Training epoch-49 batch-53
Running loss of epoch-49 batch-53 = 7.294351235032082e-06

Training epoch-49 batch-54
Running loss of epoch-49 batch-54 = 9.538372978568077e-06

Training epoch-49 batch-55
Running loss of epoch-49 batch-55 = 3.594905138015747e-06

Training epoch-49 batch-56
Running loss of epoch-49 batch-56 = 4.4335611164569855e-06

Training epoch-49 batch-57
Running loss of epoch-49 batch-57 = 3.794906660914421e-06

Training epoch-49 batch-58
Running loss of epoch-49 batch-58 = 9.789131581783295e-06

Training epoch-49 batch-59
Running loss of epoch-49 batch-59 = 8.356524631381035e-06

Training epoch-49 batch-60
Running loss of epoch-49 batch-60 = 4.94765117764473e-06

Training epoch-49 batch-61
Running loss of epoch-49 batch-61 = 6.173970177769661e-06

Training epoch-49 batch-62
Running loss of epoch-49 batch-62 = 3.7997961044311523e-06

Training epoch-49 batch-63
Running loss of epoch-49 batch-63 = 2.745981328189373e-05

Training epoch-49 batch-64
Running loss of epoch-49 batch-64 = 2.7779024094343185e-06

Training epoch-49 batch-65
Running loss of epoch-49 batch-65 = 9.296461939811707e-06

Training epoch-49 batch-66
Running loss of epoch-49 batch-66 = 5.5818818509578705e-06

Training epoch-49 batch-67
Running loss of epoch-49 batch-67 = 5.693407729268074e-06

Training epoch-49 batch-68
Running loss of epoch-49 batch-68 = 3.4014228731393814e-06

Training epoch-49 batch-69
Running loss of epoch-49 batch-69 = 8.698785677552223e-06

Training epoch-49 batch-70
Running loss of epoch-49 batch-70 = 4.605390131473541e-06

Training epoch-49 batch-71
Running loss of epoch-49 batch-71 = 3.867084160447121e-06

Training epoch-49 batch-72
Running loss of epoch-49 batch-72 = 8.30390490591526e-06

Training epoch-49 batch-73
Running loss of epoch-49 batch-73 = 7.592607289552689e-06

Training epoch-49 batch-74
Running loss of epoch-49 batch-74 = 7.5998250395059586e-06

Training epoch-49 batch-75
Running loss of epoch-49 batch-75 = 5.032168701291084e-06

Training epoch-49 batch-76
Running loss of epoch-49 batch-76 = 9.960029274225235e-06

Training epoch-49 batch-77
Running loss of epoch-49 batch-77 = 2.484302967786789e-06

Training epoch-49 batch-78
Running loss of epoch-49 batch-78 = 9.839190170168877e-06

Training epoch-49 batch-79
Running loss of epoch-49 batch-79 = 7.429393008351326e-06

Training epoch-49 batch-80
Running loss of epoch-49 batch-80 = 2.5248154997825623e-06

Training epoch-49 batch-81
Running loss of epoch-49 batch-81 = 9.563402272760868e-06

Training epoch-49 batch-82
Running loss of epoch-49 batch-82 = 9.6841249614954e-06

Training epoch-49 batch-83
Running loss of epoch-49 batch-83 = 4.36045229434967e-06

Training epoch-49 batch-84
Running loss of epoch-49 batch-84 = 1.8491409718990326e-06

Training epoch-49 batch-85
Running loss of epoch-49 batch-85 = 3.3925753086805344e-06

Training epoch-49 batch-86
Running loss of epoch-49 batch-86 = 3.5692937672138214e-06

Training epoch-49 batch-87
Running loss of epoch-49 batch-87 = 9.989598765969276e-06

Training epoch-49 batch-88
Running loss of epoch-49 batch-88 = 5.5907294154167175e-06

Training epoch-49 batch-89
Running loss of epoch-49 batch-89 = 4.987930878996849e-06

Training epoch-49 batch-90
Running loss of epoch-49 batch-90 = 6.501330062747002e-06

Training epoch-49 batch-91
Running loss of epoch-49 batch-91 = 9.324168786406517e-06

Training epoch-49 batch-92
Running loss of epoch-49 batch-92 = 1.549231819808483e-05

Training epoch-49 batch-93
Running loss of epoch-49 batch-93 = 3.541354089975357e-06

Training epoch-49 batch-94
Running loss of epoch-49 batch-94 = 8.143018931150436e-06

Training epoch-49 batch-95
Running loss of epoch-49 batch-95 = 4.519708454608917e-06

Training epoch-49 batch-96
Running loss of epoch-49 batch-96 = 1.2174015864729881e-05

Training epoch-49 batch-97
Running loss of epoch-49 batch-97 = 7.1283429861068726e-06

Training epoch-49 batch-98
Running loss of epoch-49 batch-98 = 7.790280506014824e-06

Training epoch-49 batch-99
Running loss of epoch-49 batch-99 = 6.051268428564072e-06

Training epoch-49 batch-100
Running loss of epoch-49 batch-100 = 6.057322025299072e-06

Training epoch-49 batch-101
Running loss of epoch-49 batch-101 = 1.096515916287899e-05

Training epoch-49 batch-102
Running loss of epoch-49 batch-102 = 1.0929303243756294e-05

Training epoch-49 batch-103
Running loss of epoch-49 batch-103 = 2.048932947218418e-05

Training epoch-49 batch-104
Running loss of epoch-49 batch-104 = 5.987007170915604e-06

Training epoch-49 batch-105
Running loss of epoch-49 batch-105 = 2.048676833510399e-06

Training epoch-49 batch-106
Running loss of epoch-49 batch-106 = 1.2495787814259529e-05

Training epoch-49 batch-107
Running loss of epoch-49 batch-107 = 4.508299753069878e-06

Training epoch-49 batch-108
Running loss of epoch-49 batch-108 = 2.6193447411060333e-06

Training epoch-49 batch-109
Running loss of epoch-49 batch-109 = 5.210051313042641e-06

Training epoch-49 batch-110
Running loss of epoch-49 batch-110 = 3.5106204450130463e-06

Training epoch-49 batch-111
Running loss of epoch-49 batch-111 = 8.3243940025568e-06

Training epoch-49 batch-112
Running loss of epoch-49 batch-112 = 1.2012314982712269e-05

Training epoch-49 batch-113
Running loss of epoch-49 batch-113 = 9.6180010586977e-06

Training epoch-49 batch-114
Running loss of epoch-49 batch-114 = 3.99397686123848e-06

Training epoch-49 batch-115
Running loss of epoch-49 batch-115 = 3.225170075893402e-06

Training epoch-49 batch-116
Running loss of epoch-49 batch-116 = 3.087799996137619e-06

Training epoch-49 batch-117
Running loss of epoch-49 batch-117 = 8.435919880867004e-06

Training epoch-49 batch-118
Running loss of epoch-49 batch-118 = 4.0351878851652145e-06

Training epoch-49 batch-119
Running loss of epoch-49 batch-119 = 5.532056093215942e-06

Training epoch-49 batch-120
Running loss of epoch-49 batch-120 = 4.9513764679431915e-06

Training epoch-49 batch-121
Running loss of epoch-49 batch-121 = 8.701113983988762e-06

Training epoch-49 batch-122
Running loss of epoch-49 batch-122 = 1.1992175132036209e-05

Training epoch-49 batch-123
Running loss of epoch-49 batch-123 = 9.420560672879219e-06

Training epoch-49 batch-124
Running loss of epoch-49 batch-124 = 1.2571457773447037e-05

Training epoch-49 batch-125
Running loss of epoch-49 batch-125 = 8.360017091035843e-06

Training epoch-49 batch-126
Running loss of epoch-49 batch-126 = 3.2458920031785965e-06

Training epoch-49 batch-127
Running loss of epoch-49 batch-127 = 6.654532626271248e-06

Training epoch-49 batch-128
Running loss of epoch-49 batch-128 = 8.100876584649086e-06

Training epoch-49 batch-129
Running loss of epoch-49 batch-129 = 6.875256076455116e-06

Training epoch-49 batch-130
Running loss of epoch-49 batch-130 = 1.6655772924423218e-05

Training epoch-49 batch-131
Running loss of epoch-49 batch-131 = 1.3082753866910934e-05

Training epoch-49 batch-132
Running loss of epoch-49 batch-132 = 6.129033863544464e-06

Training epoch-49 batch-133
Running loss of epoch-49 batch-133 = 5.773967131972313e-06

Training epoch-49 batch-134
Running loss of epoch-49 batch-134 = 3.5725533962249756e-06

Training epoch-49 batch-135
Running loss of epoch-49 batch-135 = 5.987472832202911e-06

Training epoch-49 batch-136
Running loss of epoch-49 batch-136 = 1.4928984455764294e-05

Training epoch-49 batch-137
Running loss of epoch-49 batch-137 = 1.0855495929718018e-05

Training epoch-49 batch-138
Running loss of epoch-49 batch-138 = 5.1050446927547455e-06

Training epoch-49 batch-139
Running loss of epoch-49 batch-139 = 3.486406058073044e-06

Training epoch-49 batch-140
Running loss of epoch-49 batch-140 = 5.00422902405262e-06

Training epoch-49 batch-141
Running loss of epoch-49 batch-141 = 4.728790372610092e-06

Training epoch-49 batch-142
Running loss of epoch-49 batch-142 = 4.7835055738687515e-06

Training epoch-49 batch-143
Running loss of epoch-49 batch-143 = 7.7106524258852e-06

Training epoch-49 batch-144
Running loss of epoch-49 batch-144 = 3.8726720958948135e-06

Training epoch-49 batch-145
Running loss of epoch-49 batch-145 = 6.258953362703323e-06

Training epoch-49 batch-146
Running loss of epoch-49 batch-146 = 1.4498597010970116e-05

Training epoch-49 batch-147
Running loss of epoch-49 batch-147 = 1.0551419109106064e-05

Training epoch-49 batch-148
Running loss of epoch-49 batch-148 = 4.720641300082207e-06

Training epoch-49 batch-149
Running loss of epoch-49 batch-149 = 6.1516184359788895e-06

Training epoch-49 batch-150
Running loss of epoch-49 batch-150 = 4.288041964173317e-06

Training epoch-49 batch-151
Running loss of epoch-49 batch-151 = 2.7050264179706573e-06

Training epoch-49 batch-152
Running loss of epoch-49 batch-152 = 6.743008270859718e-06

Training epoch-49 batch-153
Running loss of epoch-49 batch-153 = 6.216578185558319e-06

Training epoch-49 batch-154
Running loss of epoch-49 batch-154 = 5.867332220077515e-06

Training epoch-49 batch-155
Running loss of epoch-49 batch-155 = 4.8318179324269295e-06

Training epoch-49 batch-156
Running loss of epoch-49 batch-156 = 5.772104486823082e-06

Training epoch-49 batch-157
Running loss of epoch-49 batch-157 = 4.494190216064453e-05

Finished training epoch-49.



Average train loss at epoch-49 = 7.2365984320640566e-06

Started Evaluation

Average val loss at epoch-49 = 1.078591881870065

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.71 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.71 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.10 %

Finished Evaluation



Started training epoch-50


Training epoch-50 batch-1
Running loss of epoch-50 batch-1 = 1.0409392416477203e-05

Training epoch-50 batch-2
Running loss of epoch-50 batch-2 = 7.4536073952913284e-06

Training epoch-50 batch-3
Running loss of epoch-50 batch-3 = 3.6370474845170975e-06

Training epoch-50 batch-4
Running loss of epoch-50 batch-4 = 2.739951014518738e-06

Training epoch-50 batch-5
Running loss of epoch-50 batch-5 = 3.916444256901741e-06

Training epoch-50 batch-6
Running loss of epoch-50 batch-6 = 2.5497283786535263e-06

Training epoch-50 batch-7
Running loss of epoch-50 batch-7 = 4.346249625086784e-06

Training epoch-50 batch-8
Running loss of epoch-50 batch-8 = 9.112292900681496e-06

Training epoch-50 batch-9
Running loss of epoch-50 batch-9 = 3.807246685028076e-06

Training epoch-50 batch-10
Running loss of epoch-50 batch-10 = 9.024050086736679e-06

Training epoch-50 batch-11
Running loss of epoch-50 batch-11 = 6.275484338402748e-06

Training epoch-50 batch-12
Running loss of epoch-50 batch-12 = 3.0493829399347305e-06

Training epoch-50 batch-13
Running loss of epoch-50 batch-13 = 8.595176041126251e-06

Training epoch-50 batch-14
Running loss of epoch-50 batch-14 = 2.421438694000244e-06

Training epoch-50 batch-15
Running loss of epoch-50 batch-15 = 6.2247272580862045e-06

Training epoch-50 batch-16
Running loss of epoch-50 batch-16 = 5.487585440278053e-06

Training epoch-50 batch-17
Running loss of epoch-50 batch-17 = 7.594935595989227e-07

Training epoch-50 batch-18
Running loss of epoch-50 batch-18 = 5.0335656851530075e-06

Training epoch-50 batch-19
Running loss of epoch-50 batch-19 = 5.785143002867699e-06

Training epoch-50 batch-20
Running loss of epoch-50 batch-20 = 3.1711533665657043e-06

Training epoch-50 batch-21
Running loss of epoch-50 batch-21 = 6.954651325941086e-06

Training epoch-50 batch-22
Running loss of epoch-50 batch-22 = 6.4049381762743e-06

Training epoch-50 batch-23
Running loss of epoch-50 batch-23 = 7.642200216650963e-06

Training epoch-50 batch-24
Running loss of epoch-50 batch-24 = 1.9407249055802822e-05

Training epoch-50 batch-25
Running loss of epoch-50 batch-25 = 1.3995449990034103e-05

Training epoch-50 batch-26
Running loss of epoch-50 batch-26 = 5.643581971526146e-06

Training epoch-50 batch-27
Running loss of epoch-50 batch-27 = 3.988156095147133e-06

Training epoch-50 batch-28
Running loss of epoch-50 batch-28 = 4.61796298623085e-06

Training epoch-50 batch-29
Running loss of epoch-50 batch-29 = 9.160488843917847e-06

Training epoch-50 batch-30
Running loss of epoch-50 batch-30 = 6.582122296094894e-06

Training epoch-50 batch-31
Running loss of epoch-50 batch-31 = 9.041046723723412e-06

Training epoch-50 batch-32
Running loss of epoch-50 batch-32 = 3.7511344999074936e-06

Training epoch-50 batch-33
Running loss of epoch-50 batch-33 = 9.517301805317402e-06

Training epoch-50 batch-34
Running loss of epoch-50 batch-34 = 8.021015673875809e-06

Training epoch-50 batch-35
Running loss of epoch-50 batch-35 = 7.48620368540287e-06

Training epoch-50 batch-36
Running loss of epoch-50 batch-36 = 8.250819519162178e-06

Training epoch-50 batch-37
Running loss of epoch-50 batch-37 = 7.497845217585564e-06

Training epoch-50 batch-38
Running loss of epoch-50 batch-38 = 5.58153260499239e-06

Training epoch-50 batch-39
Running loss of epoch-50 batch-39 = 3.839842975139618e-06

Training epoch-50 batch-40
Running loss of epoch-50 batch-40 = 6.7302025854587555e-06

Training epoch-50 batch-41
Running loss of epoch-50 batch-41 = 3.3937394618988037e-06

Training epoch-50 batch-42
Running loss of epoch-50 batch-42 = 7.492024451494217e-06

Training epoch-50 batch-43
Running loss of epoch-50 batch-43 = 1.2271339073777199e-05

Training epoch-50 batch-44
Running loss of epoch-50 batch-44 = 6.430549547076225e-06

Training epoch-50 batch-45
Running loss of epoch-50 batch-45 = 5.305279046297073e-06

Training epoch-50 batch-46
Running loss of epoch-50 batch-46 = 2.9134098440408707e-06

Training epoch-50 batch-47
Running loss of epoch-50 batch-47 = 3.998167812824249e-06

Training epoch-50 batch-48
Running loss of epoch-50 batch-48 = 3.953929990530014e-06

Training epoch-50 batch-49
Running loss of epoch-50 batch-49 = 8.22567380964756e-06

Training epoch-50 batch-50
Running loss of epoch-50 batch-50 = 4.016794264316559e-06

Training epoch-50 batch-51
Running loss of epoch-50 batch-51 = 8.397502824664116e-06

Training epoch-50 batch-52
Running loss of epoch-50 batch-52 = 3.969762474298477e-06

Training epoch-50 batch-53
Running loss of epoch-50 batch-53 = 1.1885073035955429e-05

Training epoch-50 batch-54
Running loss of epoch-50 batch-54 = 1.388578675687313e-05

Training epoch-50 batch-55
Running loss of epoch-50 batch-55 = 1.9267667084932327e-05

Training epoch-50 batch-56
Running loss of epoch-50 batch-56 = 3.8244761526584625e-06

Training epoch-50 batch-57
Running loss of epoch-50 batch-57 = 3.8691796362400055e-06

Training epoch-50 batch-58
Running loss of epoch-50 batch-58 = 4.529254510998726e-06

Training epoch-50 batch-59
Running loss of epoch-50 batch-59 = 3.7669669836759567e-06

Training epoch-50 batch-60
Running loss of epoch-50 batch-60 = 8.640112355351448e-06

Training epoch-50 batch-61
Running loss of epoch-50 batch-61 = 7.178168743848801e-06

Training epoch-50 batch-62
Running loss of epoch-50 batch-62 = 7.4587296694517136e-06

Training epoch-50 batch-63
Running loss of epoch-50 batch-63 = 2.673128619790077e-06

Training epoch-50 batch-64
Running loss of epoch-50 batch-64 = 4.838220775127411e-06

Training epoch-50 batch-65
Running loss of epoch-50 batch-65 = 3.470107913017273e-06

Training epoch-50 batch-66
Running loss of epoch-50 batch-66 = 1.3310229405760765e-05

Training epoch-50 batch-67
Running loss of epoch-50 batch-67 = 4.723668098449707e-06

Training epoch-50 batch-68
Running loss of epoch-50 batch-68 = 4.1422899812459946e-06

Training epoch-50 batch-69
Running loss of epoch-50 batch-69 = 5.56139275431633e-06

Training epoch-50 batch-70
Running loss of epoch-50 batch-70 = 7.604248821735382e-06

Training epoch-50 batch-71
Running loss of epoch-50 batch-71 = 7.380032911896706e-06

Training epoch-50 batch-72
Running loss of epoch-50 batch-72 = 3.5832636058330536e-06

Training epoch-50 batch-73
Running loss of epoch-50 batch-73 = 9.045004844665527e-06

Training epoch-50 batch-74
Running loss of epoch-50 batch-74 = 1.0647345334291458e-05

Training epoch-50 batch-75
Running loss of epoch-50 batch-75 = 7.643597200512886e-06

Training epoch-50 batch-76
Running loss of epoch-50 batch-76 = 7.1604736149311066e-06

Training epoch-50 batch-77
Running loss of epoch-50 batch-77 = 8.332077413797379e-06

Training epoch-50 batch-78
Running loss of epoch-50 batch-78 = 3.968598321080208e-06

Training epoch-50 batch-79
Running loss of epoch-50 batch-79 = 2.0065344870090485e-06

Training epoch-50 batch-80
Running loss of epoch-50 batch-80 = 5.09270466864109e-06

Training epoch-50 batch-81
Running loss of epoch-50 batch-81 = 5.952781066298485e-06

Training epoch-50 batch-82
Running loss of epoch-50 batch-82 = 5.486421287059784e-06

Training epoch-50 batch-83
Running loss of epoch-50 batch-83 = 4.088506102561951e-06

Training epoch-50 batch-84
Running loss of epoch-50 batch-84 = 1.3334676623344421e-05

Training epoch-50 batch-85
Running loss of epoch-50 batch-85 = 7.921596989035606e-06

Training epoch-50 batch-86
Running loss of epoch-50 batch-86 = 6.3444022089242935e-06

Training epoch-50 batch-87
Running loss of epoch-50 batch-87 = 2.885935828089714e-06

Training epoch-50 batch-88
Running loss of epoch-50 batch-88 = 5.77140599489212e-06

Training epoch-50 batch-89
Running loss of epoch-50 batch-89 = 1.705181784927845e-05

Training epoch-50 batch-90
Running loss of epoch-50 batch-90 = 5.58304600417614e-06

Training epoch-50 batch-91
Running loss of epoch-50 batch-91 = 8.814502507448196e-06

Training epoch-50 batch-92
Running loss of epoch-50 batch-92 = 4.585599526762962e-06

Training epoch-50 batch-93
Running loss of epoch-50 batch-93 = 4.330417141318321e-06

Training epoch-50 batch-94
Running loss of epoch-50 batch-94 = 2.0065344870090485e-06

Training epoch-50 batch-95
Running loss of epoch-50 batch-95 = 3.1248200684785843e-06

Training epoch-50 batch-96
Running loss of epoch-50 batch-96 = 1.2363540008664131e-05

Training epoch-50 batch-97
Running loss of epoch-50 batch-97 = 6.177695468068123e-06

Training epoch-50 batch-98
Running loss of epoch-50 batch-98 = 6.1749015003442764e-06

Training epoch-50 batch-99
Running loss of epoch-50 batch-99 = 7.346505299210548e-06

Training epoch-50 batch-100
Running loss of epoch-50 batch-100 = 2.184370532631874e-05

Training epoch-50 batch-101
Running loss of epoch-50 batch-101 = 3.8142316043376923e-06

Training epoch-50 batch-102
Running loss of epoch-50 batch-102 = 1.5322118997573853e-05

Training epoch-50 batch-103
Running loss of epoch-50 batch-103 = 2.311309799551964e-06

Training epoch-50 batch-104
Running loss of epoch-50 batch-104 = 4.71482053399086e-06

Training epoch-50 batch-105
Running loss of epoch-50 batch-105 = 7.679685950279236e-06

Training epoch-50 batch-106
Running loss of epoch-50 batch-106 = 1.227017492055893e-05

Training epoch-50 batch-107
Running loss of epoch-50 batch-107 = 9.835930541157722e-06

Training epoch-50 batch-108
Running loss of epoch-50 batch-108 = 7.761875167489052e-06

Training epoch-50 batch-109
Running loss of epoch-50 batch-109 = 1.2953532859683037e-05

Training epoch-50 batch-110
Running loss of epoch-50 batch-110 = 9.187962859869003e-06

Training epoch-50 batch-111
Running loss of epoch-50 batch-111 = 8.524628356099129e-06

Training epoch-50 batch-112
Running loss of epoch-50 batch-112 = 4.940899088978767e-06

Training epoch-50 batch-113
Running loss of epoch-50 batch-113 = 5.027744919061661e-06

Training epoch-50 batch-114
Running loss of epoch-50 batch-114 = 1.5075784176588058e-06

Training epoch-50 batch-115
Running loss of epoch-50 batch-115 = 5.292007699608803e-06

Training epoch-50 batch-116
Running loss of epoch-50 batch-116 = 1.1656899005174637e-05

Training epoch-50 batch-117
Running loss of epoch-50 batch-117 = 5.800742655992508e-06

Training epoch-50 batch-118
Running loss of epoch-50 batch-118 = 1.2729084119200706e-05

Training epoch-50 batch-119
Running loss of epoch-50 batch-119 = 8.474336937069893e-06

Training epoch-50 batch-120
Running loss of epoch-50 batch-120 = 2.372940070927143e-05

Training epoch-50 batch-121
Running loss of epoch-50 batch-121 = 6.28945417702198e-06

Training epoch-50 batch-122
Running loss of epoch-50 batch-122 = 1.5181489288806915e-05

Training epoch-50 batch-123
Running loss of epoch-50 batch-123 = 3.974186256527901e-06

Training epoch-50 batch-124
Running loss of epoch-50 batch-124 = 2.3653265088796616e-06

Training epoch-50 batch-125
Running loss of epoch-50 batch-125 = 1.3272976502776146e-05

Training epoch-50 batch-126
Running loss of epoch-50 batch-126 = 4.9551017582416534e-06

Training epoch-50 batch-127
Running loss of epoch-50 batch-127 = 8.177943527698517e-06

Training epoch-50 batch-128
Running loss of epoch-50 batch-128 = 8.8477972894907e-06

Training epoch-50 batch-129
Running loss of epoch-50 batch-129 = 5.988869816064835e-06

Training epoch-50 batch-130
Running loss of epoch-50 batch-130 = 9.761657565832138e-06

Training epoch-50 batch-131
Running loss of epoch-50 batch-131 = 6.7069195210933685e-06

Training epoch-50 batch-132
Running loss of epoch-50 batch-132 = 1.2370757758617401e-05

Training epoch-50 batch-133
Running loss of epoch-50 batch-133 = 3.7420541048049927e-06

Training epoch-50 batch-134
Running loss of epoch-50 batch-134 = 4.711560904979706e-06

Training epoch-50 batch-135
Running loss of epoch-50 batch-135 = 4.36161644756794e-06

Training epoch-50 batch-136
Running loss of epoch-50 batch-136 = 1.1066440492868423e-06

Training epoch-50 batch-137
Running loss of epoch-50 batch-137 = 2.4084001779556274e-06

Training epoch-50 batch-138
Running loss of epoch-50 batch-138 = 4.967907443642616e-06

Training epoch-50 batch-139
Running loss of epoch-50 batch-139 = 2.4617183953523636e-06

Training epoch-50 batch-140
Running loss of epoch-50 batch-140 = 1.032254658639431e-05

Training epoch-50 batch-141
Running loss of epoch-50 batch-141 = 7.392140105366707e-06

Training epoch-50 batch-142
Running loss of epoch-50 batch-142 = 5.679670721292496e-06

Training epoch-50 batch-143
Running loss of epoch-50 batch-143 = 3.4936238080263138e-06

Training epoch-50 batch-144
Running loss of epoch-50 batch-144 = 5.566747859120369e-06

Training epoch-50 batch-145
Running loss of epoch-50 batch-145 = 1.1689728125929832e-05

Training epoch-50 batch-146
Running loss of epoch-50 batch-146 = 2.7904752641916275e-06

Training epoch-50 batch-147
Running loss of epoch-50 batch-147 = 7.30506144464016e-06

Training epoch-50 batch-148
Running loss of epoch-50 batch-148 = 7.696682587265968e-06

Training epoch-50 batch-149
Running loss of epoch-50 batch-149 = 5.7104043662548065e-06

Training epoch-50 batch-150
Running loss of epoch-50 batch-150 = 6.46873377263546e-06

Training epoch-50 batch-151
Running loss of epoch-50 batch-151 = 6.448943167924881e-06

Training epoch-50 batch-152
Running loss of epoch-50 batch-152 = 3.3891992643475533e-06

Training epoch-50 batch-153
Running loss of epoch-50 batch-153 = 2.952292561531067e-06

Training epoch-50 batch-154
Running loss of epoch-50 batch-154 = 6.387708708643913e-06

Training epoch-50 batch-155
Running loss of epoch-50 batch-155 = 5.032401531934738e-06

Training epoch-50 batch-156
Running loss of epoch-50 batch-156 = 1.0543502867221832e-05

Training epoch-50 batch-157
Running loss of epoch-50 batch-157 = 7.029250264167786e-05

Finished training epoch-50.



Average train loss at epoch-50 = 7.018166780471802e-06

Started Evaluation

Average val loss at epoch-50 = 1.0840668884815816

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 63.47 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.27 %

Finished Evaluation



Started training epoch-51


Training epoch-51 batch-1
Running loss of epoch-51 batch-1 = 5.152309313416481e-06

Training epoch-51 batch-2
Running loss of epoch-51 batch-2 = 8.112052455544472e-06

Training epoch-51 batch-3
Running loss of epoch-51 batch-3 = 8.273869752883911e-06

Training epoch-51 batch-4
Running loss of epoch-51 batch-4 = 3.930646926164627e-06

Training epoch-51 batch-5
Running loss of epoch-51 batch-5 = 9.0734101831913e-06

Training epoch-51 batch-6
Running loss of epoch-51 batch-6 = 4.510162398219109e-06

Training epoch-51 batch-7
Running loss of epoch-51 batch-7 = 9.286915883421898e-06

Training epoch-51 batch-8
Running loss of epoch-51 batch-8 = 6.321584805846214e-06

Training epoch-51 batch-9
Running loss of epoch-51 batch-9 = 6.63392711430788e-06

Training epoch-51 batch-10
Running loss of epoch-51 batch-10 = 6.8086665123701096e-06

Training epoch-51 batch-11
Running loss of epoch-51 batch-11 = 8.190050721168518e-06

Training epoch-51 batch-12
Running loss of epoch-51 batch-12 = 4.666624590754509e-06

Training epoch-51 batch-13
Running loss of epoch-51 batch-13 = 6.304820999503136e-06

Training epoch-51 batch-14
Running loss of epoch-51 batch-14 = 6.408197805285454e-06

Training epoch-51 batch-15
Running loss of epoch-51 batch-15 = 4.342757165431976e-06

Training epoch-51 batch-16
Running loss of epoch-51 batch-16 = 8.652685210108757e-06

Training epoch-51 batch-17
Running loss of epoch-51 batch-17 = 1.735985279083252e-06

Training epoch-51 batch-18
Running loss of epoch-51 batch-18 = 3.960682079195976e-06

Training epoch-51 batch-19
Running loss of epoch-51 batch-19 = 2.08243727684021e-06

Training epoch-51 batch-20
Running loss of epoch-51 batch-20 = 5.952781066298485e-06

Training epoch-51 batch-21
Running loss of epoch-51 batch-21 = 1.7583370208740234e-06

Training epoch-51 batch-22
Running loss of epoch-51 batch-22 = 9.713461622595787e-06

Training epoch-51 batch-23
Running loss of epoch-51 batch-23 = 2.82912515103817e-06

Training epoch-51 batch-24
Running loss of epoch-51 batch-24 = 3.3830292522907257e-06

Training epoch-51 batch-25
Running loss of epoch-51 batch-25 = 5.427282303571701e-06

Training epoch-51 batch-26
Running loss of epoch-51 batch-26 = 1.3158423826098442e-05

Training epoch-51 batch-27
Running loss of epoch-51 batch-27 = 3.096414729952812e-06

Training epoch-51 batch-28
Running loss of epoch-51 batch-28 = 6.362562999129295e-06

Training epoch-51 batch-29
Running loss of epoch-51 batch-29 = 1.5059718862175941e-05

Training epoch-51 batch-30
Running loss of epoch-51 batch-30 = 1.262070145457983e-05

Training epoch-51 batch-31
Running loss of epoch-51 batch-31 = 4.278728738427162e-06

Training epoch-51 batch-32
Running loss of epoch-51 batch-32 = 3.5194680094718933e-06

Training epoch-51 batch-33
Running loss of epoch-51 batch-33 = 4.125293344259262e-06

Training epoch-51 batch-34
Running loss of epoch-51 batch-34 = 1.2801261618733406e-05

Training epoch-51 batch-35
Running loss of epoch-51 batch-35 = 6.698304787278175e-06

Training epoch-51 batch-36
Running loss of epoch-51 batch-36 = 2.5068875402212143e-06

Training epoch-51 batch-37
Running loss of epoch-51 batch-37 = 7.338356226682663e-06

Training epoch-51 batch-38
Running loss of epoch-51 batch-38 = 4.849396646022797e-06

Training epoch-51 batch-39
Running loss of epoch-51 batch-39 = 5.0996895879507065e-06

Training epoch-51 batch-40
Running loss of epoch-51 batch-40 = 5.787704139947891e-06

Training epoch-51 batch-41
Running loss of epoch-51 batch-41 = 7.3676928877830505e-06

Training epoch-51 batch-42
Running loss of epoch-51 batch-42 = 1.260894350707531e-05

Training epoch-51 batch-43
Running loss of epoch-51 batch-43 = 5.922280251979828e-06

Training epoch-51 batch-44
Running loss of epoch-51 batch-44 = 8.76537524163723e-06

Training epoch-51 batch-45
Running loss of epoch-51 batch-45 = 1.2935372069478035e-05

Training epoch-51 batch-46
Running loss of epoch-51 batch-46 = 1.534237526357174e-05

Training epoch-51 batch-47
Running loss of epoch-51 batch-47 = 7.218914106488228e-06

Training epoch-51 batch-48
Running loss of epoch-51 batch-48 = 7.180729880928993e-06

Training epoch-51 batch-49
Running loss of epoch-51 batch-49 = 1.1993106454610825e-06

Training epoch-51 batch-50
Running loss of epoch-51 batch-50 = 4.981411620974541e-06

Training epoch-51 batch-51
Running loss of epoch-51 batch-51 = 4.752771928906441e-06

Training epoch-51 batch-52
Running loss of epoch-51 batch-52 = 2.280808985233307e-06

Training epoch-51 batch-53
Running loss of epoch-51 batch-53 = 9.453389793634415e-06

Training epoch-51 batch-54
Running loss of epoch-51 batch-54 = 6.321584805846214e-06

Training epoch-51 batch-55
Running loss of epoch-51 batch-55 = 1.7262063920497894e-06

Training epoch-51 batch-56
Running loss of epoch-51 batch-56 = 7.768161594867706e-06

Training epoch-51 batch-57
Running loss of epoch-51 batch-57 = 2.3821252398192883e-05

Training epoch-51 batch-58
Running loss of epoch-51 batch-58 = 1.0061543434858322e-05

Training epoch-51 batch-59
Running loss of epoch-51 batch-59 = 4.509231075644493e-06

Training epoch-51 batch-60
Running loss of epoch-51 batch-60 = 1.6936566680669785e-05

Training epoch-51 batch-61
Running loss of epoch-51 batch-61 = 4.269415512681007e-06

Training epoch-51 batch-62
Running loss of epoch-51 batch-62 = 3.216089680790901e-06

Training epoch-51 batch-63
Running loss of epoch-51 batch-63 = 3.3085234463214874e-06

Training epoch-51 batch-64
Running loss of epoch-51 batch-64 = 1.7579528503119946e-05

Training epoch-51 batch-65
Running loss of epoch-51 batch-65 = 1.1798692867159843e-05

Training epoch-51 batch-66
Running loss of epoch-51 batch-66 = 7.799128070473671e-06

Training epoch-51 batch-67
Running loss of epoch-51 batch-67 = 3.8673169910907745e-06

Training epoch-51 batch-68
Running loss of epoch-51 batch-68 = 7.020076736807823e-06

Training epoch-51 batch-69
Running loss of epoch-51 batch-69 = 7.026828825473785e-06

Training epoch-51 batch-70
Running loss of epoch-51 batch-70 = 8.322764188051224e-06

Training epoch-51 batch-71
Running loss of epoch-51 batch-71 = 9.388197213411331e-06

Training epoch-51 batch-72
Running loss of epoch-51 batch-72 = 8.011236786842346e-06

Training epoch-51 batch-73
Running loss of epoch-51 batch-73 = 6.580492481589317e-06

Training epoch-51 batch-74
Running loss of epoch-51 batch-74 = 4.9173831939697266e-06

Training epoch-51 batch-75
Running loss of epoch-51 batch-75 = 4.173256456851959e-06

Training epoch-51 batch-76
Running loss of epoch-51 batch-76 = 4.9371737986803055e-06

Training epoch-51 batch-77
Running loss of epoch-51 batch-77 = 4.795845597982407e-06

Training epoch-51 batch-78
Running loss of epoch-51 batch-78 = 1.4228280633687973e-05

Training epoch-51 batch-79
Running loss of epoch-51 batch-79 = 4.162546247243881e-06

Training epoch-51 batch-80
Running loss of epoch-51 batch-80 = 3.957422450184822e-06

Training epoch-51 batch-81
Running loss of epoch-51 batch-81 = 4.92902472615242e-06

Training epoch-51 batch-82
Running loss of epoch-51 batch-82 = 4.069879651069641e-06

Training epoch-51 batch-83
Running loss of epoch-51 batch-83 = 9.044073522090912e-06

Training epoch-51 batch-84
Running loss of epoch-51 batch-84 = 3.492925316095352e-06

Training epoch-51 batch-85
Running loss of epoch-51 batch-85 = 4.367204383015633e-06

Training epoch-51 batch-86
Running loss of epoch-51 batch-86 = 1.2253876775503159e-05

Training epoch-51 batch-87
Running loss of epoch-51 batch-87 = 7.129507139325142e-06

Training epoch-51 batch-88
Running loss of epoch-51 batch-88 = 6.838003173470497e-06

Training epoch-51 batch-89
Running loss of epoch-51 batch-89 = 1.0179704986512661e-05

Training epoch-51 batch-90
Running loss of epoch-51 batch-90 = 6.01145438849926e-06

Training epoch-51 batch-91
Running loss of epoch-51 batch-91 = 1.0177027434110641e-05

Training epoch-51 batch-92
Running loss of epoch-51 batch-92 = 6.6908542066812515e-06

Training epoch-51 batch-93
Running loss of epoch-51 batch-93 = 1.9744038581848145e-06

Training epoch-51 batch-94
Running loss of epoch-51 batch-94 = 9.00577288120985e-06

Training epoch-51 batch-95
Running loss of epoch-51 batch-95 = 5.854526534676552e-06

Training epoch-51 batch-96
Running loss of epoch-51 batch-96 = 5.595386028289795e-06

Training epoch-51 batch-97
Running loss of epoch-51 batch-97 = 7.82730057835579e-06

Training epoch-51 batch-98
Running loss of epoch-51 batch-98 = 4.912959411740303e-06

Training epoch-51 batch-99
Running loss of epoch-51 batch-99 = 6.9534871727228165e-06

Training epoch-51 batch-100
Running loss of epoch-51 batch-100 = 5.6468416005373e-06

Training epoch-51 batch-101
Running loss of epoch-51 batch-101 = 4.285946488380432e-06

Training epoch-51 batch-102
Running loss of epoch-51 batch-102 = 4.690838977694511e-06

Training epoch-51 batch-103
Running loss of epoch-51 batch-103 = 4.572095349431038e-06

Training epoch-51 batch-104
Running loss of epoch-51 batch-104 = 9.641516953706741e-06

Training epoch-51 batch-105
Running loss of epoch-51 batch-105 = 3.91458161175251e-06

Training epoch-51 batch-106
Running loss of epoch-51 batch-106 = 2.7462374418973923e-06

Training epoch-51 batch-107
Running loss of epoch-51 batch-107 = 9.668990969657898e-06

Training epoch-51 batch-108
Running loss of epoch-51 batch-108 = 5.787704139947891e-06

Training epoch-51 batch-109
Running loss of epoch-51 batch-109 = 9.661074727773666e-06

Training epoch-51 batch-110
Running loss of epoch-51 batch-110 = 4.786765202879906e-06

Training epoch-51 batch-111
Running loss of epoch-51 batch-111 = 3.4098047763109207e-06

Training epoch-51 batch-112
Running loss of epoch-51 batch-112 = 3.9495062083005905e-06

Training epoch-51 batch-113
Running loss of epoch-51 batch-113 = 7.415656000375748e-06

Training epoch-51 batch-114
Running loss of epoch-51 batch-114 = 6.468268111348152e-06

Training epoch-51 batch-115
Running loss of epoch-51 batch-115 = 4.190485924482346e-06

Training epoch-51 batch-116
Running loss of epoch-51 batch-116 = 6.22146762907505e-06

Training epoch-51 batch-117
Running loss of epoch-51 batch-117 = 5.037523806095123e-06

Training epoch-51 batch-118
Running loss of epoch-51 batch-118 = 4.759873263537884e-06

Training epoch-51 batch-119
Running loss of epoch-51 batch-119 = 1.124129630625248e-05

Training epoch-51 batch-120
Running loss of epoch-51 batch-120 = 2.741580829024315e-06

Training epoch-51 batch-121
Running loss of epoch-51 batch-121 = 3.1481031328439713e-06

Training epoch-51 batch-122
Running loss of epoch-51 batch-122 = 3.073830157518387e-06

Training epoch-51 batch-123
Running loss of epoch-51 batch-123 = 8.254079148173332e-06

Training epoch-51 batch-124
Running loss of epoch-51 batch-124 = 3.066146746277809e-06

Training epoch-51 batch-125
Running loss of epoch-51 batch-125 = 9.949086233973503e-06

Training epoch-51 batch-126
Running loss of epoch-51 batch-126 = 2.3301690816879272e-06

Training epoch-51 batch-127
Running loss of epoch-51 batch-127 = 5.16488216817379e-06

Training epoch-51 batch-128
Running loss of epoch-51 batch-128 = 6.462796591222286e-06

Training epoch-51 batch-129
Running loss of epoch-51 batch-129 = 1.3460638001561165e-05

Training epoch-51 batch-130
Running loss of epoch-51 batch-130 = 2.8354115784168243e-06

Training epoch-51 batch-131
Running loss of epoch-51 batch-131 = 1.7205718904733658e-05

Training epoch-51 batch-132
Running loss of epoch-51 batch-132 = 3.900611773133278e-06

Training epoch-51 batch-133
Running loss of epoch-51 batch-133 = 6.236601620912552e-06

Training epoch-51 batch-134
Running loss of epoch-51 batch-134 = 4.6156346797943115e-06

Training epoch-51 batch-135
Running loss of epoch-51 batch-135 = 5.393289029598236e-06

Training epoch-51 batch-136
Running loss of epoch-51 batch-136 = 4.337402060627937e-06

Training epoch-51 batch-137
Running loss of epoch-51 batch-137 = 1.4172634109854698e-05

Training epoch-51 batch-138
Running loss of epoch-51 batch-138 = 5.336245521903038e-06

Training epoch-51 batch-139
Running loss of epoch-51 batch-139 = 4.825880751013756e-06

Training epoch-51 batch-140
Running loss of epoch-51 batch-140 = 5.757203325629234e-06

Training epoch-51 batch-141
Running loss of epoch-51 batch-141 = 6.943708285689354e-06

Training epoch-51 batch-142
Running loss of epoch-51 batch-142 = 8.22683796286583e-06

Training epoch-51 batch-143
Running loss of epoch-51 batch-143 = 4.417961463332176e-06

Training epoch-51 batch-144
Running loss of epoch-51 batch-144 = 6.076181307435036e-06

Training epoch-51 batch-145
Running loss of epoch-51 batch-145 = 9.319279342889786e-06

Training epoch-51 batch-146
Running loss of epoch-51 batch-146 = 4.888745024800301e-06

Training epoch-51 batch-147
Running loss of epoch-51 batch-147 = 7.18235969543457e-06

Training epoch-51 batch-148
Running loss of epoch-51 batch-148 = 6.223563104867935e-06

Training epoch-51 batch-149
Running loss of epoch-51 batch-149 = 8.345348760485649e-06

Training epoch-51 batch-150
Running loss of epoch-51 batch-150 = 4.2689498513937e-06

Training epoch-51 batch-151
Running loss of epoch-51 batch-151 = 5.895504727959633e-06

Training epoch-51 batch-152
Running loss of epoch-51 batch-152 = 9.565148502588272e-06

Training epoch-51 batch-153
Running loss of epoch-51 batch-153 = 3.739725798368454e-06

Training epoch-51 batch-154
Running loss of epoch-51 batch-154 = 1.0941410437226295e-05

Training epoch-51 batch-155
Running loss of epoch-51 batch-155 = 6.913207471370697e-06

Training epoch-51 batch-156
Running loss of epoch-51 batch-156 = 7.300404831767082e-06

Training epoch-51 batch-157
Running loss of epoch-51 batch-157 = 1.4930963516235352e-05

Finished training epoch-51.



Average train loss at epoch-51 = 6.739082932472229e-06

Started Evaluation

Average val loss at epoch-51 = 1.0982497020232513

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 63.70 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 44.18 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.12 %

Finished Evaluation



Started training epoch-52


Training epoch-52 batch-1
Running loss of epoch-52 batch-1 = 5.0764065235853195e-06

Training epoch-52 batch-2
Running loss of epoch-52 batch-2 = 5.2193645387887955e-06

Training epoch-52 batch-3
Running loss of epoch-52 batch-3 = 1.210719347000122e-06

Training epoch-52 batch-4
Running loss of epoch-52 batch-4 = 1.1530006304383278e-05

Training epoch-52 batch-5
Running loss of epoch-52 batch-5 = 5.268026143312454e-06

Training epoch-52 batch-6
Running loss of epoch-52 batch-6 = 6.0622114688158035e-06

Training epoch-52 batch-7
Running loss of epoch-52 batch-7 = 3.4759286791086197e-06

Training epoch-52 batch-8
Running loss of epoch-52 batch-8 = 2.486654557287693e-05

Training epoch-52 batch-9
Running loss of epoch-52 batch-9 = 5.9710582718253136e-06

Training epoch-52 batch-10
Running loss of epoch-52 batch-10 = 5.2102841436862946e-06

Training epoch-52 batch-11
Running loss of epoch-52 batch-11 = 3.6284327507019043e-06

Training epoch-52 batch-12
Running loss of epoch-52 batch-12 = 6.15045428276062e-06

Training epoch-52 batch-13
Running loss of epoch-52 batch-13 = 7.187714800238609e-06

Training epoch-52 batch-14
Running loss of epoch-52 batch-14 = 9.96515154838562e-06

Training epoch-52 batch-15
Running loss of epoch-52 batch-15 = 3.7373974919319153e-06

Training epoch-52 batch-16
Running loss of epoch-52 batch-16 = 8.564209565520287e-06

Training epoch-52 batch-17
Running loss of epoch-52 batch-17 = 3.3813994377851486e-06

Training epoch-52 batch-18
Running loss of epoch-52 batch-18 = 6.077345460653305e-06

Training epoch-52 batch-19
Running loss of epoch-52 batch-19 = 9.093666449189186e-06

Training epoch-52 batch-20
Running loss of epoch-52 batch-20 = 2.2973399609327316e-06

Training epoch-52 batch-21
Running loss of epoch-52 batch-21 = 8.148374035954475e-06

Training epoch-52 batch-22
Running loss of epoch-52 batch-22 = 6.427988409996033e-06

Training epoch-52 batch-23
Running loss of epoch-52 batch-23 = 7.790746167302132e-06

Training epoch-52 batch-24
Running loss of epoch-52 batch-24 = 3.516441211104393e-06

Training epoch-52 batch-25
Running loss of epoch-52 batch-25 = 5.27198426425457e-06

Training epoch-52 batch-26
Running loss of epoch-52 batch-26 = 5.771755240857601e-06

Training epoch-52 batch-27
Running loss of epoch-52 batch-27 = 9.317649528384209e-06

Training epoch-52 batch-28
Running loss of epoch-52 batch-28 = 1.5627127140760422e-05

Training epoch-52 batch-29
Running loss of epoch-52 batch-29 = 2.1787709556519985e-05

Training epoch-52 batch-30
Running loss of epoch-52 batch-30 = 2.1760351955890656e-06

Training epoch-52 batch-31
Running loss of epoch-52 batch-31 = 5.365582183003426e-06

Training epoch-52 batch-32
Running loss of epoch-52 batch-32 = 4.875240847468376e-06

Training epoch-52 batch-33
Running loss of epoch-52 batch-33 = 8.222181349992752e-06

Training epoch-52 batch-34
Running loss of epoch-52 batch-34 = 5.216803401708603e-06

Training epoch-52 batch-35
Running loss of epoch-52 batch-35 = 1.0712537914514542e-05

Training epoch-52 batch-36
Running loss of epoch-52 batch-36 = 3.473367542028427e-06

Training epoch-52 batch-37
Running loss of epoch-52 batch-37 = 7.687835022807121e-06

Training epoch-52 batch-38
Running loss of epoch-52 batch-38 = 1.0250834748148918e-05

Training epoch-52 batch-39
Running loss of epoch-52 batch-39 = 5.118781700730324e-06

Training epoch-52 batch-40
Running loss of epoch-52 batch-40 = 4.327390342950821e-06

Training epoch-52 batch-41
Running loss of epoch-52 batch-41 = 6.3676852732896805e-06

Training epoch-52 batch-42
Running loss of epoch-52 batch-42 = 5.508074536919594e-06

Training epoch-52 batch-43
Running loss of epoch-52 batch-43 = 4.173256456851959e-06

Training epoch-52 batch-44
Running loss of epoch-52 batch-44 = 6.666756235063076e-06

Training epoch-52 batch-45
Running loss of epoch-52 batch-45 = 3.211433067917824e-06

Training epoch-52 batch-46
Running loss of epoch-52 batch-46 = 5.931593477725983e-06

Training epoch-52 batch-47
Running loss of epoch-52 batch-47 = 3.400491550564766e-06

Training epoch-52 batch-48
Running loss of epoch-52 batch-48 = 4.54392284154892e-06

Training epoch-52 batch-49
Running loss of epoch-52 batch-49 = 1.2170057743787766e-06

Training epoch-52 batch-50
Running loss of epoch-52 batch-50 = 6.909016519784927e-06

Training epoch-52 batch-51
Running loss of epoch-52 batch-51 = 6.993068382143974e-06

Training epoch-52 batch-52
Running loss of epoch-52 batch-52 = 4.556961357593536e-06

Training epoch-52 batch-53
Running loss of epoch-52 batch-53 = 3.933673724532127e-06

Training epoch-52 batch-54
Running loss of epoch-52 batch-54 = 5.280831828713417e-06

Training epoch-52 batch-55
Running loss of epoch-52 batch-55 = 1.7655547708272934e-06

Training epoch-52 batch-56
Running loss of epoch-52 batch-56 = 2.2170133888721466e-06

Training epoch-52 batch-57
Running loss of epoch-52 batch-57 = 2.566492184996605e-06

Training epoch-52 batch-58
Running loss of epoch-52 batch-58 = 8.022645488381386e-06

Training epoch-52 batch-59
Running loss of epoch-52 batch-59 = 3.5553239285945892e-06

Training epoch-52 batch-60
Running loss of epoch-52 batch-60 = 1.152954064309597e-05

Training epoch-52 batch-61
Running loss of epoch-52 batch-61 = 8.29971395432949e-06

Training epoch-52 batch-62
Running loss of epoch-52 batch-62 = 4.945322871208191e-06

Training epoch-52 batch-63
Running loss of epoch-52 batch-63 = 3.0007213354110718e-06

Training epoch-52 batch-64
Running loss of epoch-52 batch-64 = 4.633096978068352e-06

Training epoch-52 batch-65
Running loss of epoch-52 batch-65 = 8.245930075645447e-06

Training epoch-52 batch-66
Running loss of epoch-52 batch-66 = 9.110895916819572e-06

Training epoch-52 batch-67
Running loss of epoch-52 batch-67 = 4.045665264129639e-06

Training epoch-52 batch-68
Running loss of epoch-52 batch-68 = 7.816357538104057e-06

Training epoch-52 batch-69
Running loss of epoch-52 batch-69 = 5.638226866722107e-06

Training epoch-52 batch-70
Running loss of epoch-52 batch-70 = 1.0567368008196354e-05

Training epoch-52 batch-71
Running loss of epoch-52 batch-71 = 3.5332050174474716e-06

Training epoch-52 batch-72
Running loss of epoch-52 batch-72 = 5.526002496480942e-06

Training epoch-52 batch-73
Running loss of epoch-52 batch-73 = 4.838220775127411e-06

Training epoch-52 batch-74
Running loss of epoch-52 batch-74 = 3.5210978239774704e-06

Training epoch-52 batch-75
Running loss of epoch-52 batch-75 = 6.451969966292381e-06

Training epoch-52 batch-76
Running loss of epoch-52 batch-76 = 5.214940756559372e-06

Training epoch-52 batch-77
Running loss of epoch-52 batch-77 = 1.3950048014521599e-05

Training epoch-52 batch-78
Running loss of epoch-52 batch-78 = 4.499219357967377e-06

Training epoch-52 batch-79
Running loss of epoch-52 batch-79 = 8.95792618393898e-06

Training epoch-52 batch-80
Running loss of epoch-52 batch-80 = 4.564644768834114e-06

Training epoch-52 batch-81
Running loss of epoch-52 batch-81 = 1.3630371540784836e-05

Training epoch-52 batch-82
Running loss of epoch-52 batch-82 = 8.024275302886963e-06

Training epoch-52 batch-83
Running loss of epoch-52 batch-83 = 7.832655683159828e-06

Training epoch-52 batch-84
Running loss of epoch-52 batch-84 = 7.549533620476723e-06

Training epoch-52 batch-85
Running loss of epoch-52 batch-85 = 7.655704393982887e-06

Training epoch-52 batch-86
Running loss of epoch-52 batch-86 = 1.9356142729520798e-05

Training epoch-52 batch-87
Running loss of epoch-52 batch-87 = 5.417270585894585e-06

Training epoch-52 batch-88
Running loss of epoch-52 batch-88 = 2.494547516107559e-06

Training epoch-52 batch-89
Running loss of epoch-52 batch-89 = 6.135087460279465e-06

Training epoch-52 batch-90
Running loss of epoch-52 batch-90 = 4.9336813390254974e-06

Training epoch-52 batch-91
Running loss of epoch-52 batch-91 = 4.172557964920998e-06

Training epoch-52 batch-92
Running loss of epoch-52 batch-92 = 5.116919055581093e-06

Training epoch-52 batch-93
Running loss of epoch-52 batch-93 = 2.9425136744976044e-06

Training epoch-52 batch-94
Running loss of epoch-52 batch-94 = 5.878740921616554e-06

Training epoch-52 batch-95
Running loss of epoch-52 batch-95 = 7.139984518289566e-06

Training epoch-52 batch-96
Running loss of epoch-52 batch-96 = 4.943227395415306e-06

Training epoch-52 batch-97
Running loss of epoch-52 batch-97 = 1.6565201804041862e-05

Training epoch-52 batch-98
Running loss of epoch-52 batch-98 = 2.5727786123752594e-06

Training epoch-52 batch-99
Running loss of epoch-52 batch-99 = 6.392830982804298e-06

Training epoch-52 batch-100
Running loss of epoch-52 batch-100 = 7.722992449998856e-06

Training epoch-52 batch-101
Running loss of epoch-52 batch-101 = 2.304092049598694e-06

Training epoch-52 batch-102
Running loss of epoch-52 batch-102 = 4.575354978442192e-06

Training epoch-52 batch-103
Running loss of epoch-52 batch-103 = 1.2033618986606598e-05

Training epoch-52 batch-104
Running loss of epoch-52 batch-104 = 8.281320333480835e-06

Training epoch-52 batch-105
Running loss of epoch-52 batch-105 = 8.863862603902817e-06

Training epoch-52 batch-106
Running loss of epoch-52 batch-106 = 2.792803570628166e-06

Training epoch-52 batch-107
Running loss of epoch-52 batch-107 = 5.590030923485756e-06

Training epoch-52 batch-108
Running loss of epoch-52 batch-108 = 3.867084160447121e-06

Training epoch-52 batch-109
Running loss of epoch-52 batch-109 = 7.206341251730919e-06

Training epoch-52 batch-110
Running loss of epoch-52 batch-110 = 1.9832514226436615e-06

Training epoch-52 batch-111
Running loss of epoch-52 batch-111 = 4.532979801297188e-06

Training epoch-52 batch-112
Running loss of epoch-52 batch-112 = 1.619686372578144e-05

Training epoch-52 batch-113
Running loss of epoch-52 batch-113 = 4.249159246683121e-06

Training epoch-52 batch-114
Running loss of epoch-52 batch-114 = 3.5853590816259384e-06

Training epoch-52 batch-115
Running loss of epoch-52 batch-115 = 7.6035503298044205e-06

Training epoch-52 batch-116
Running loss of epoch-52 batch-116 = 4.781410098075867e-06

Training epoch-52 batch-117
Running loss of epoch-52 batch-117 = 5.0640664994716644e-06

Training epoch-52 batch-118
Running loss of epoch-52 batch-118 = 6.345333531498909e-06

Training epoch-52 batch-119
Running loss of epoch-52 batch-119 = 4.491768777370453e-06

Training epoch-52 batch-120
Running loss of epoch-52 batch-120 = 1.0754098184406757e-05

Training epoch-52 batch-121
Running loss of epoch-52 batch-121 = 1.121428795158863e-05

Training epoch-52 batch-122
Running loss of epoch-52 batch-122 = 7.503782398998737e-06

Training epoch-52 batch-123
Running loss of epoch-52 batch-123 = 5.345558747649193e-06

Training epoch-52 batch-124
Running loss of epoch-52 batch-124 = 9.78016760200262e-06

Training epoch-52 batch-125
Running loss of epoch-52 batch-125 = 1.3486715033650398e-05

Training epoch-52 batch-126
Running loss of epoch-52 batch-126 = 5.288748070597649e-06

Training epoch-52 batch-127
Running loss of epoch-52 batch-127 = 4.233093932271004e-06

Training epoch-52 batch-128
Running loss of epoch-52 batch-128 = 5.356967449188232e-06

Training epoch-52 batch-129
Running loss of epoch-52 batch-129 = 8.006114512681961e-06

Training epoch-52 batch-130
Running loss of epoch-52 batch-130 = 5.537411198019981e-06

Training epoch-52 batch-131
Running loss of epoch-52 batch-131 = 3.773253411054611e-06

Training epoch-52 batch-132
Running loss of epoch-52 batch-132 = 1.91456638276577e-06

Training epoch-52 batch-133
Running loss of epoch-52 batch-133 = 4.017027094960213e-06

Training epoch-52 batch-134
Running loss of epoch-52 batch-134 = 7.833121344447136e-06

Training epoch-52 batch-135
Running loss of epoch-52 batch-135 = 6.0445163398981094e-06

Training epoch-52 batch-136
Running loss of epoch-52 batch-136 = 9.61916521191597e-06

Training epoch-52 batch-137
Running loss of epoch-52 batch-137 = 5.934154614806175e-06

Training epoch-52 batch-138
Running loss of epoch-52 batch-138 = 4.16068360209465e-06

Training epoch-52 batch-139
Running loss of epoch-52 batch-139 = 5.181180313229561e-06

Training epoch-52 batch-140
Running loss of epoch-52 batch-140 = 2.4044420570135117e-06

Training epoch-52 batch-141
Running loss of epoch-52 batch-141 = 1.1473661288619041e-05

Training epoch-52 batch-142
Running loss of epoch-52 batch-142 = 6.020301952958107e-06

Training epoch-52 batch-143
Running loss of epoch-52 batch-143 = 3.821216523647308e-06

Training epoch-52 batch-144
Running loss of epoch-52 batch-144 = 6.404239684343338e-06

Training epoch-52 batch-145
Running loss of epoch-52 batch-145 = 3.939494490623474e-06

Training epoch-52 batch-146
Running loss of epoch-52 batch-146 = 7.078051567077637e-06

Training epoch-52 batch-147
Running loss of epoch-52 batch-147 = 8.588191121816635e-06

Training epoch-52 batch-148
Running loss of epoch-52 batch-148 = 6.5749045461416245e-06

Training epoch-52 batch-149
Running loss of epoch-52 batch-149 = 4.642875865101814e-06

Training epoch-52 batch-150
Running loss of epoch-52 batch-150 = 8.989591151475906e-06

Training epoch-52 batch-151
Running loss of epoch-52 batch-151 = 8.478527888655663e-06

Training epoch-52 batch-152
Running loss of epoch-52 batch-152 = 6.311573088169098e-06

Training epoch-52 batch-153
Running loss of epoch-52 batch-153 = 3.6999117583036423e-06

Training epoch-52 batch-154
Running loss of epoch-52 batch-154 = 3.6014243960380554e-06

Training epoch-52 batch-155
Running loss of epoch-52 batch-155 = 7.055932655930519e-06

Training epoch-52 batch-156
Running loss of epoch-52 batch-156 = 5.679205060005188e-06

Training epoch-52 batch-157
Running loss of epoch-52 batch-157 = 2.637505531311035e-06

Finished training epoch-52.



Average train loss at epoch-52 = 6.47682398557663e-06

Started Evaluation

Average val loss at epoch-52 = 1.0906011159706013

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.48 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 83.16 %

Finished Evaluation



Started training epoch-53


Training epoch-53 batch-1
Running loss of epoch-53 batch-1 = 3.025168552994728e-06

Training epoch-53 batch-2
Running loss of epoch-53 batch-2 = 4.593050107359886e-06

Training epoch-53 batch-3
Running loss of epoch-53 batch-3 = 4.9746595323085785e-06

Training epoch-53 batch-4
Running loss of epoch-53 batch-4 = 5.403067916631699e-06

Training epoch-53 batch-5
Running loss of epoch-53 batch-5 = 7.138354703783989e-06

Training epoch-53 batch-6
Running loss of epoch-53 batch-6 = 2.72504985332489e-06

Training epoch-53 batch-7
Running loss of epoch-53 batch-7 = 4.237284883856773e-06

Training epoch-53 batch-8
Running loss of epoch-53 batch-8 = 5.168374627828598e-06

Training epoch-53 batch-9
Running loss of epoch-53 batch-9 = 1.9237399101257324e-05

Training epoch-53 batch-10
Running loss of epoch-53 batch-10 = 3.6905985325574875e-06

Training epoch-53 batch-11
Running loss of epoch-53 batch-11 = 4.447298124432564e-06

Training epoch-53 batch-12
Running loss of epoch-53 batch-12 = 6.539514288306236e-06

Training epoch-53 batch-13
Running loss of epoch-53 batch-13 = 9.029405191540718e-06

Training epoch-53 batch-14
Running loss of epoch-53 batch-14 = 4.265923053026199e-06

Training epoch-53 batch-15
Running loss of epoch-53 batch-15 = 1.4003366231918335e-05

Training epoch-53 batch-16
Running loss of epoch-53 batch-16 = 3.782333806157112e-06

Training epoch-53 batch-17
Running loss of epoch-53 batch-17 = 7.970957085490227e-06

Training epoch-53 batch-18
Running loss of epoch-53 batch-18 = 3.2547395676374435e-06

Training epoch-53 batch-19
Running loss of epoch-53 batch-19 = 6.634509190917015e-06

Training epoch-53 batch-20
Running loss of epoch-53 batch-20 = 6.006215699017048e-06

Training epoch-53 batch-21
Running loss of epoch-53 batch-21 = 7.759546861052513e-06

Training epoch-53 batch-22
Running loss of epoch-53 batch-22 = 8.064555004239082e-06

Training epoch-53 batch-23
Running loss of epoch-53 batch-23 = 5.403999239206314e-06

Training epoch-53 batch-24
Running loss of epoch-53 batch-24 = 2.669636160135269e-06

Training epoch-53 batch-25
Running loss of epoch-53 batch-25 = 6.608897820115089e-06

Training epoch-53 batch-26
Running loss of epoch-53 batch-26 = 5.721813067793846e-06

Training epoch-53 batch-27
Running loss of epoch-53 batch-27 = 8.138595148921013e-06

Training epoch-53 batch-28
Running loss of epoch-53 batch-28 = 6.5658241510391235e-06

Training epoch-53 batch-29
Running loss of epoch-53 batch-29 = 4.424015060067177e-06

Training epoch-53 batch-30
Running loss of epoch-53 batch-30 = 5.855457857251167e-06

Training epoch-53 batch-31
Running loss of epoch-53 batch-31 = 5.9604644775390625e-06

Training epoch-53 batch-32
Running loss of epoch-53 batch-32 = 1.1620111763477325e-05

Training epoch-53 batch-33
Running loss of epoch-53 batch-33 = 7.0089008659124374e-06

Training epoch-53 batch-34
Running loss of epoch-53 batch-34 = 7.557449862360954e-06

Training epoch-53 batch-35
Running loss of epoch-53 batch-35 = 7.011927664279938e-06

Training epoch-53 batch-36
Running loss of epoch-53 batch-36 = 4.391418769955635e-06

Training epoch-53 batch-37
Running loss of epoch-53 batch-37 = 3.7830322980880737e-06

Training epoch-53 batch-38
Running loss of epoch-53 batch-38 = 9.210314601659775e-06

Training epoch-53 batch-39
Running loss of epoch-53 batch-39 = 1.378590241074562e-05

Training epoch-53 batch-40
Running loss of epoch-53 batch-40 = 2.5241170078516006e-06

Training epoch-53 batch-41
Running loss of epoch-53 batch-41 = 1.0006129741668701e-05

Training epoch-53 batch-42
Running loss of epoch-53 batch-42 = 4.742061719298363e-06

Training epoch-53 batch-43
Running loss of epoch-53 batch-43 = 6.058253347873688e-06

Training epoch-53 batch-44
Running loss of epoch-53 batch-44 = 5.036359652876854e-06

Training epoch-53 batch-45
Running loss of epoch-53 batch-45 = 7.005641236901283e-06

Training epoch-53 batch-46
Running loss of epoch-53 batch-46 = 7.187947630882263e-06

Training epoch-53 batch-47
Running loss of epoch-53 batch-47 = 1.8543796613812447e-05

Training epoch-53 batch-48
Running loss of epoch-53 batch-48 = 8.382368832826614e-06

Training epoch-53 batch-49
Running loss of epoch-53 batch-49 = 6.434740498661995e-06

Training epoch-53 batch-50
Running loss of epoch-53 batch-50 = 7.470836862921715e-06

Training epoch-53 batch-51
Running loss of epoch-53 batch-51 = 5.051260814070702e-06

Training epoch-53 batch-52
Running loss of epoch-53 batch-52 = 9.398441761732101e-06

Training epoch-53 batch-53
Running loss of epoch-53 batch-53 = 2.6980414986610413e-06

Training epoch-53 batch-54
Running loss of epoch-53 batch-54 = 7.427763193845749e-06

Training epoch-53 batch-55
Running loss of epoch-53 batch-55 = 4.4871121644973755e-06

Training epoch-53 batch-56
Running loss of epoch-53 batch-56 = 1.0210787877440453e-05

Training epoch-53 batch-57
Running loss of epoch-53 batch-57 = 3.899913281202316e-06

Training epoch-53 batch-58
Running loss of epoch-53 batch-58 = 6.260816007852554e-06

Training epoch-53 batch-59
Running loss of epoch-53 batch-59 = 2.6742927730083466e-06

Training epoch-53 batch-60
Running loss of epoch-53 batch-60 = 4.649395123124123e-06

Training epoch-53 batch-61
Running loss of epoch-53 batch-61 = 1.3416167348623276e-05

Training epoch-53 batch-62
Running loss of epoch-53 batch-62 = 4.562782123684883e-06

Training epoch-53 batch-63
Running loss of epoch-53 batch-63 = 1.2881588190793991e-05

Training epoch-53 batch-64
Running loss of epoch-53 batch-64 = 6.808433681726456e-06

Training epoch-53 batch-65
Running loss of epoch-53 batch-65 = 2.5711487978696823e-06

Training epoch-53 batch-66
Running loss of epoch-53 batch-66 = 2.396758645772934e-06

Training epoch-53 batch-67
Running loss of epoch-53 batch-67 = 3.0908267945051193e-06

Training epoch-53 batch-68
Running loss of epoch-53 batch-68 = 7.292488589882851e-06

Training epoch-53 batch-69
Running loss of epoch-53 batch-69 = 4.466623067855835e-06

Training epoch-53 batch-70
Running loss of epoch-53 batch-70 = 8.476199582219124e-06

Training epoch-53 batch-71
Running loss of epoch-53 batch-71 = 3.9495062083005905e-06

Training epoch-53 batch-72
Running loss of epoch-53 batch-72 = 9.870855137705803e-06

Training epoch-53 batch-73
Running loss of epoch-53 batch-73 = 3.873137757182121e-06

Training epoch-53 batch-74
Running loss of epoch-53 batch-74 = 3.897352144122124e-06

Training epoch-53 batch-75
Running loss of epoch-53 batch-75 = 8.699949830770493e-06

Training epoch-53 batch-76
Running loss of epoch-53 batch-76 = 2.541113644838333e-06

Training epoch-53 batch-77
Running loss of epoch-53 batch-77 = 7.1462709456682205e-06

Training epoch-53 batch-78
Running loss of epoch-53 batch-78 = 3.5725533962249756e-06

Training epoch-53 batch-79
Running loss of epoch-53 batch-79 = 4.209112375974655e-06

Training epoch-53 batch-80
Running loss of epoch-53 batch-80 = 3.019929863512516e-06

Training epoch-53 batch-81
Running loss of epoch-53 batch-81 = 3.848923370242119e-06

Training epoch-53 batch-82
Running loss of epoch-53 batch-82 = 3.369757905602455e-06

Training epoch-53 batch-83
Running loss of epoch-53 batch-83 = 3.6782585084438324e-06

Training epoch-53 batch-84
Running loss of epoch-53 batch-84 = 5.293171852827072e-06

Training epoch-53 batch-85
Running loss of epoch-53 batch-85 = 3.677560016512871e-06

Training epoch-53 batch-86
Running loss of epoch-53 batch-86 = 3.5706907510757446e-06

Training epoch-53 batch-87
Running loss of epoch-53 batch-87 = 3.863126039505005e-06

Training epoch-53 batch-88
Running loss of epoch-53 batch-88 = 5.5890996009111404e-06

Training epoch-53 batch-89
Running loss of epoch-53 batch-89 = 7.227994501590729e-06

Training epoch-53 batch-90
Running loss of epoch-53 batch-90 = 7.80867412686348e-06

Training epoch-53 batch-91
Running loss of epoch-53 batch-91 = 7.78166577219963e-06

Training epoch-53 batch-92
Running loss of epoch-53 batch-92 = 5.1888637244701385e-06

Training epoch-53 batch-93
Running loss of epoch-53 batch-93 = 3.553694114089012e-06

Training epoch-53 batch-94
Running loss of epoch-53 batch-94 = 3.6954879760742188e-06

Training epoch-53 batch-95
Running loss of epoch-53 batch-95 = 4.357192665338516e-06

Training epoch-53 batch-96
Running loss of epoch-53 batch-96 = 2.3203901946544647e-06

Training epoch-53 batch-97
Running loss of epoch-53 batch-97 = 5.892245098948479e-06

Training epoch-53 batch-98
Running loss of epoch-53 batch-98 = 8.574919775128365e-06

Training epoch-53 batch-99
Running loss of epoch-53 batch-99 = 4.407484084367752e-06

Training epoch-53 batch-100
Running loss of epoch-53 batch-100 = 6.826827302575111e-06

Training epoch-53 batch-101
Running loss of epoch-53 batch-101 = 3.743451088666916e-06

Training epoch-53 batch-102
Running loss of epoch-53 batch-102 = 4.614703357219696e-06

Training epoch-53 batch-103
Running loss of epoch-53 batch-103 = 2.3064203560352325e-06

Training epoch-53 batch-104
Running loss of epoch-53 batch-104 = 9.450362995266914e-06

Training epoch-53 batch-105
Running loss of epoch-53 batch-105 = 7.945345714688301e-06

Training epoch-53 batch-106
Running loss of epoch-53 batch-106 = 1.0725692845880985e-05

Training epoch-53 batch-107
Running loss of epoch-53 batch-107 = 4.6477653086185455e-06

Training epoch-53 batch-108
Running loss of epoch-53 batch-108 = 1.047714613378048e-05

Training epoch-53 batch-109
Running loss of epoch-53 batch-109 = 4.300381988286972e-06

Training epoch-53 batch-110
Running loss of epoch-53 batch-110 = 7.713213562965393e-06

Training epoch-53 batch-111
Running loss of epoch-53 batch-111 = 2.489890903234482e-06

Training epoch-53 batch-112
Running loss of epoch-53 batch-112 = 5.255453288555145e-06

Training epoch-53 batch-113
Running loss of epoch-53 batch-113 = 1.1504162102937698e-05

Training epoch-53 batch-114
Running loss of epoch-53 batch-114 = 8.506933227181435e-06

Training epoch-53 batch-115
Running loss of epoch-53 batch-115 = 6.121816113591194e-06

Training epoch-53 batch-116
Running loss of epoch-53 batch-116 = 7.593072950839996e-06

Training epoch-53 batch-117
Running loss of epoch-53 batch-117 = 1.3201497495174408e-06

Training epoch-53 batch-118
Running loss of epoch-53 batch-118 = 3.4209806472063065e-06

Training epoch-53 batch-119
Running loss of epoch-53 batch-119 = 3.3369287848472595e-06

Training epoch-53 batch-120
Running loss of epoch-53 batch-120 = 4.203524440526962e-06

Training epoch-53 batch-121
Running loss of epoch-53 batch-121 = 3.302237018942833e-06

Training epoch-53 batch-122
Running loss of epoch-53 batch-122 = 6.346730515360832e-06

Training epoch-53 batch-123
Running loss of epoch-53 batch-123 = 5.896436050534248e-06

Training epoch-53 batch-124
Running loss of epoch-53 batch-124 = 5.409820005297661e-06

Training epoch-53 batch-125
Running loss of epoch-53 batch-125 = 8.567702025175095e-06

Training epoch-53 batch-126
Running loss of epoch-53 batch-126 = 8.747680112719536e-06

Training epoch-53 batch-127
Running loss of epoch-53 batch-127 = 2.9497314244508743e-06

Training epoch-53 batch-128
Running loss of epoch-53 batch-128 = 2.9206275939941406e-06

Training epoch-53 batch-129
Running loss of epoch-53 batch-129 = 3.752531483769417e-06

Training epoch-53 batch-130
Running loss of epoch-53 batch-130 = 1.6145757399499416e-05

Training epoch-53 batch-131
Running loss of epoch-53 batch-131 = 3.4783734008669853e-06

Training epoch-53 batch-132
Running loss of epoch-53 batch-132 = 3.061257302761078e-06

Training epoch-53 batch-133
Running loss of epoch-53 batch-133 = 3.207707777619362e-06

Training epoch-53 batch-134
Running loss of epoch-53 batch-134 = 2.3348256945610046e-06

Training epoch-53 batch-135
Running loss of epoch-53 batch-135 = 4.10410575568676e-06

Training epoch-53 batch-136
Running loss of epoch-53 batch-136 = 6.105750799179077e-06

Training epoch-53 batch-137
Running loss of epoch-53 batch-137 = 9.417301043868065e-06

Training epoch-53 batch-138
Running loss of epoch-53 batch-138 = 9.537208825349808e-06

Training epoch-53 batch-139
Running loss of epoch-53 batch-139 = 2.078711986541748e-06

Training epoch-53 batch-140
Running loss of epoch-53 batch-140 = 6.7979563027620316e-06

Training epoch-53 batch-141
Running loss of epoch-53 batch-141 = 4.915054887533188e-06

Training epoch-53 batch-142
Running loss of epoch-53 batch-142 = 5.686422809958458e-06

Training epoch-53 batch-143
Running loss of epoch-53 batch-143 = 6.8678054958581924e-06

Training epoch-53 batch-144
Running loss of epoch-53 batch-144 = 6.213784217834473e-06

Training epoch-53 batch-145
Running loss of epoch-53 batch-145 = 3.7329737097024918e-06

Training epoch-53 batch-146
Running loss of epoch-53 batch-146 = 8.165603503584862e-06

Training epoch-53 batch-147
Running loss of epoch-53 batch-147 = 1.1413590982556343e-05

Training epoch-53 batch-148
Running loss of epoch-53 batch-148 = 5.943351425230503e-06

Training epoch-53 batch-149
Running loss of epoch-53 batch-149 = 1.1215917766094208e-05

Training epoch-53 batch-150
Running loss of epoch-53 batch-150 = 1.1928146705031395e-05

Training epoch-53 batch-151
Running loss of epoch-53 batch-151 = 1.2098345905542374e-05

Training epoch-53 batch-152
Running loss of epoch-53 batch-152 = 1.1119525879621506e-05

Training epoch-53 batch-153
Running loss of epoch-53 batch-153 = 4.7497451305389404e-06

Training epoch-53 batch-154
Running loss of epoch-53 batch-154 = 1.5718862414360046e-05

Training epoch-53 batch-155
Running loss of epoch-53 batch-155 = 1.8936116248369217e-06

Training epoch-53 batch-156
Running loss of epoch-53 batch-156 = 8.293427526950836e-06

Training epoch-53 batch-157
Running loss of epoch-53 batch-157 = 3.606081008911133e-05

Finished training epoch-53.



Average train loss at epoch-53 = 6.322555243968964e-06

Started Evaluation

Average val loss at epoch-53 = 1.1100345232572228

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 94.03 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 62.10 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 44.98 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.06 %

Finished Evaluation



Started training epoch-54


Training epoch-54 batch-1
Running loss of epoch-54 batch-1 = 4.876870661973953e-06

Training epoch-54 batch-2
Running loss of epoch-54 batch-2 = 2.2070016711950302e-06

Training epoch-54 batch-3
Running loss of epoch-54 batch-3 = 8.933944627642632e-06

Training epoch-54 batch-4
Running loss of epoch-54 batch-4 = 1.1288328096270561e-05

Training epoch-54 batch-5
Running loss of epoch-54 batch-5 = 4.341127350926399e-06

Training epoch-54 batch-6
Running loss of epoch-54 batch-6 = 5.462905392050743e-06

Training epoch-54 batch-7
Running loss of epoch-54 batch-7 = 8.536968380212784e-06

Training epoch-54 batch-8
Running loss of epoch-54 batch-8 = 5.007954314351082e-06

Training epoch-54 batch-9
Running loss of epoch-54 batch-9 = 7.455935701727867e-06

Training epoch-54 batch-10
Running loss of epoch-54 batch-10 = 1.183338463306427e-05

Training epoch-54 batch-11
Running loss of epoch-54 batch-11 = 7.247319445014e-06

Training epoch-54 batch-12
Running loss of epoch-54 batch-12 = 5.777925252914429e-06

Training epoch-54 batch-13
Running loss of epoch-54 batch-13 = 7.063848897814751e-06

Training epoch-54 batch-14
Running loss of epoch-54 batch-14 = 4.370696842670441e-06

Training epoch-54 batch-15
Running loss of epoch-54 batch-15 = 1.4013145118951797e-05

Training epoch-54 batch-16
Running loss of epoch-54 batch-16 = 8.78702849149704e-06

Training epoch-54 batch-17
Running loss of epoch-54 batch-17 = 3.709457814693451e-06

Training epoch-54 batch-18
Running loss of epoch-54 batch-18 = 8.725794032216072e-06

Training epoch-54 batch-19
Running loss of epoch-54 batch-19 = 9.414739906787872e-06

Training epoch-54 batch-20
Running loss of epoch-54 batch-20 = 9.831972420215607e-06

Training epoch-54 batch-21
Running loss of epoch-54 batch-21 = 4.362780600786209e-06

Training epoch-54 batch-22
Running loss of epoch-54 batch-22 = 2.459390088915825e-06

Training epoch-54 batch-23
Running loss of epoch-54 batch-23 = 5.336478352546692e-06

Training epoch-54 batch-24
Running loss of epoch-54 batch-24 = 6.4515043050050735e-06

Training epoch-54 batch-25
Running loss of epoch-54 batch-25 = 1.517520286142826e-05

Training epoch-54 batch-26
Running loss of epoch-54 batch-26 = 3.0132941901683807e-06

Training epoch-54 batch-27
Running loss of epoch-54 batch-27 = 8.007744327187538e-06

Training epoch-54 batch-28
Running loss of epoch-54 batch-28 = 5.150679498910904e-06

Training epoch-54 batch-29
Running loss of epoch-54 batch-29 = 5.058012902736664e-06

Training epoch-54 batch-30
Running loss of epoch-54 batch-30 = 4.430999979376793e-06

Training epoch-54 batch-31
Running loss of epoch-54 batch-31 = 9.696930646896362e-06

Training epoch-54 batch-32
Running loss of epoch-54 batch-32 = 2.680579200387001e-06

Training epoch-54 batch-33
Running loss of epoch-54 batch-33 = 1.0236166417598724e-05

Training epoch-54 batch-34
Running loss of epoch-54 batch-34 = 4.090834408998489e-06

Training epoch-54 batch-35
Running loss of epoch-54 batch-35 = 2.7955975383520126e-06

Training epoch-54 batch-36
Running loss of epoch-54 batch-36 = 9.62638296186924e-06

Training epoch-54 batch-37
Running loss of epoch-54 batch-37 = 5.94346784055233e-06

Training epoch-54 batch-38
Running loss of epoch-54 batch-38 = 2.739718183875084e-06

Training epoch-54 batch-39
Running loss of epoch-54 batch-39 = 6.431015208363533e-06

Training epoch-54 batch-40
Running loss of epoch-54 batch-40 = 8.039642125368118e-06

Training epoch-54 batch-41
Running loss of epoch-54 batch-41 = 7.825903594493866e-06

Training epoch-54 batch-42
Running loss of epoch-54 batch-42 = 6.934395059943199e-06

Training epoch-54 batch-43
Running loss of epoch-54 batch-43 = 5.229609087109566e-06

Training epoch-54 batch-44
Running loss of epoch-54 batch-44 = 1.1083437129855156e-05

Training epoch-54 batch-45
Running loss of epoch-54 batch-45 = 6.873859092593193e-06

Training epoch-54 batch-46
Running loss of epoch-54 batch-46 = 1.1557014659047127e-05

Training epoch-54 batch-47
Running loss of epoch-54 batch-47 = 2.1558254957199097e-05

Training epoch-54 batch-48
Running loss of epoch-54 batch-48 = 7.126946002244949e-06

Training epoch-54 batch-49
Running loss of epoch-54 batch-49 = 1.042126677930355e-05

Training epoch-54 batch-50
Running loss of epoch-54 batch-50 = 5.727866664528847e-06

Training epoch-54 batch-51
Running loss of epoch-54 batch-51 = 6.1749015003442764e-06

Training epoch-54 batch-52
Running loss of epoch-54 batch-52 = 2.753688022494316e-06

Training epoch-54 batch-53
Running loss of epoch-54 batch-53 = 5.2694231271743774e-06

Training epoch-54 batch-54
Running loss of epoch-54 batch-54 = 1.606927253305912e-05

Training epoch-54 batch-55
Running loss of epoch-54 batch-55 = 6.127404049038887e-06

Training epoch-54 batch-56
Running loss of epoch-54 batch-56 = 4.543224349617958e-06

Training epoch-54 batch-57
Running loss of epoch-54 batch-57 = 6.383052095770836e-06

Training epoch-54 batch-58
Running loss of epoch-54 batch-58 = 1.1532101780176163e-06

Training epoch-54 batch-59
Running loss of epoch-54 batch-59 = 5.9390440583229065e-06

Training epoch-54 batch-60
Running loss of epoch-54 batch-60 = 8.514616638422012e-06

Training epoch-54 batch-61
Running loss of epoch-54 batch-61 = 4.0710438042879105e-06

Training epoch-54 batch-62
Running loss of epoch-54 batch-62 = 8.213566616177559e-06

Training epoch-54 batch-63
Running loss of epoch-54 batch-63 = 3.943918272852898e-06

Training epoch-54 batch-64
Running loss of epoch-54 batch-64 = 3.852648660540581e-06

Training epoch-54 batch-65
Running loss of epoch-54 batch-65 = 2.283835783600807e-06

Training epoch-54 batch-66
Running loss of epoch-54 batch-66 = 3.037741407752037e-06

Training epoch-54 batch-67
Running loss of epoch-54 batch-67 = 2.696644514799118e-06

Training epoch-54 batch-68
Running loss of epoch-54 batch-68 = 2.9925722628831863e-06

Training epoch-54 batch-69
Running loss of epoch-54 batch-69 = 4.556262865662575e-06

Training epoch-54 batch-70
Running loss of epoch-54 batch-70 = 6.42845407128334e-07

Training epoch-54 batch-71
Running loss of epoch-54 batch-71 = 4.33274544775486e-06

Training epoch-54 batch-72
Running loss of epoch-54 batch-72 = 6.091548129916191e-06

Training epoch-54 batch-73
Running loss of epoch-54 batch-73 = 3.2889656722545624e-06

Training epoch-54 batch-74
Running loss of epoch-54 batch-74 = 3.332272171974182e-06

Training epoch-54 batch-75
Running loss of epoch-54 batch-75 = 3.45008447766304e-06

Training epoch-54 batch-76
Running loss of epoch-54 batch-76 = 2.757646143436432e-06

Training epoch-54 batch-77
Running loss of epoch-54 batch-77 = 7.77863897383213e-06

Training epoch-54 batch-78
Running loss of epoch-54 batch-78 = 4.7925859689712524e-06

Training epoch-54 batch-79
Running loss of epoch-54 batch-79 = 3.2687094062566757e-06

Training epoch-54 batch-80
Running loss of epoch-54 batch-80 = 3.888271749019623e-06

Training epoch-54 batch-81
Running loss of epoch-54 batch-81 = 1.3052253052592278e-05

Training epoch-54 batch-82
Running loss of epoch-54 batch-82 = 4.153931513428688e-06

Training epoch-54 batch-83
Running loss of epoch-54 batch-83 = 7.961643859744072e-06

Training epoch-54 batch-84
Running loss of epoch-54 batch-84 = 6.12274743616581e-06

Training epoch-54 batch-85
Running loss of epoch-54 batch-85 = 5.241716280579567e-06

Training epoch-54 batch-86
Running loss of epoch-54 batch-86 = 3.914814442396164e-06

Training epoch-54 batch-87
Running loss of epoch-54 batch-87 = 1.8740538507699966e-06

Training epoch-54 batch-88
Running loss of epoch-54 batch-88 = 6.5872445702552795e-06

Training epoch-54 batch-89
Running loss of epoch-54 batch-89 = 4.44706529378891e-06

Training epoch-54 batch-90
Running loss of epoch-54 batch-90 = 6.475485861301422e-06

Training epoch-54 batch-91
Running loss of epoch-54 batch-91 = 7.172813639044762e-06

Training epoch-54 batch-92
Running loss of epoch-54 batch-92 = 3.927387297153473e-06

Training epoch-54 batch-93
Running loss of epoch-54 batch-93 = 6.6517386585474014e-06

Training epoch-54 batch-94
Running loss of epoch-54 batch-94 = 9.295297786593437e-06

Training epoch-54 batch-95
Running loss of epoch-54 batch-95 = 1.037726178765297e-05

Training epoch-54 batch-96
Running loss of epoch-54 batch-96 = 1.1259457096457481e-05

Training epoch-54 batch-97
Running loss of epoch-54 batch-97 = 4.470348358154297e-06

Training epoch-54 batch-98
Running loss of epoch-54 batch-98 = 7.141381502151489e-06

Training epoch-54 batch-99
Running loss of epoch-54 batch-99 = 5.630543455481529e-06

Training epoch-54 batch-100
Running loss of epoch-54 batch-100 = 3.5446137189865112e-06

Training epoch-54 batch-101
Running loss of epoch-54 batch-101 = 3.6866404116153717e-06

Training epoch-54 batch-102
Running loss of epoch-54 batch-102 = 5.316920578479767e-06

Training epoch-54 batch-103
Running loss of epoch-54 batch-103 = 4.97954897582531e-06

Training epoch-54 batch-104
Running loss of epoch-54 batch-104 = 4.465924575924873e-06

Training epoch-54 batch-105
Running loss of epoch-54 batch-105 = 1.4271121472120285e-05

Training epoch-54 batch-106
Running loss of epoch-54 batch-106 = 5.243811756372452e-06

Training epoch-54 batch-107
Running loss of epoch-54 batch-107 = 6.4193736761808395e-06

Training epoch-54 batch-108
Running loss of epoch-54 batch-108 = 3.377441316843033e-06

Training epoch-54 batch-109
Running loss of epoch-54 batch-109 = 3.929249942302704e-06

Training epoch-54 batch-110
Running loss of epoch-54 batch-110 = 3.313180059194565e-06

Training epoch-54 batch-111
Running loss of epoch-54 batch-111 = 8.733011782169342e-06

Training epoch-54 batch-112
Running loss of epoch-54 batch-112 = 2.8836075216531754e-06

Training epoch-54 batch-113
Running loss of epoch-54 batch-113 = 5.285022780299187e-06

Training epoch-54 batch-114
Running loss of epoch-54 batch-114 = 7.640570402145386e-06

Training epoch-54 batch-115
Running loss of epoch-54 batch-115 = 5.135079845786095e-06

Training epoch-54 batch-116
Running loss of epoch-54 batch-116 = 4.366505891084671e-06

Training epoch-54 batch-117
Running loss of epoch-54 batch-117 = 6.289687007665634e-06

Training epoch-54 batch-118
Running loss of epoch-54 batch-118 = 7.765833288431168e-06

Training epoch-54 batch-119
Running loss of epoch-54 batch-119 = 3.81888821721077e-06

Training epoch-54 batch-120
Running loss of epoch-54 batch-120 = 9.014736860990524e-06

Training epoch-54 batch-121
Running loss of epoch-54 batch-121 = 8.416129276156425e-06

Training epoch-54 batch-122
Running loss of epoch-54 batch-122 = 4.927162081003189e-06

Training epoch-54 batch-123
Running loss of epoch-54 batch-123 = 4.233792424201965e-06

Training epoch-54 batch-124
Running loss of epoch-54 batch-124 = 5.348352715373039e-06

Training epoch-54 batch-125
Running loss of epoch-54 batch-125 = 8.363043889403343e-06

Training epoch-54 batch-126
Running loss of epoch-54 batch-126 = 5.650334060192108e-06

Training epoch-54 batch-127
Running loss of epoch-54 batch-127 = 2.7527566999197006e-06

Training epoch-54 batch-128
Running loss of epoch-54 batch-128 = 5.13601116836071e-06

Training epoch-54 batch-129
Running loss of epoch-54 batch-129 = 2.016080543398857e-06

Training epoch-54 batch-130
Running loss of epoch-54 batch-130 = 2.980697900056839e-06

Training epoch-54 batch-131
Running loss of epoch-54 batch-131 = 5.126930773258209e-06

Training epoch-54 batch-132
Running loss of epoch-54 batch-132 = 7.191905751824379e-06

Training epoch-54 batch-133
Running loss of epoch-54 batch-133 = 5.246372893452644e-06

Training epoch-54 batch-134
Running loss of epoch-54 batch-134 = 3.1711533665657043e-06

Training epoch-54 batch-135
Running loss of epoch-54 batch-135 = 3.4403055906295776e-06

Training epoch-54 batch-136
Running loss of epoch-54 batch-136 = 5.382811650633812e-06

Training epoch-54 batch-137
Running loss of epoch-54 batch-137 = 6.294809281826019e-06

Training epoch-54 batch-138
Running loss of epoch-54 batch-138 = 3.5956036299467087e-06

Training epoch-54 batch-139
Running loss of epoch-54 batch-139 = 6.312970072031021e-06

Training epoch-54 batch-140
Running loss of epoch-54 batch-140 = 7.625669240951538e-06

Training epoch-54 batch-141
Running loss of epoch-54 batch-141 = 5.080830305814743e-06

Training epoch-54 batch-142
Running loss of epoch-54 batch-142 = 5.234498530626297e-06

Training epoch-54 batch-143
Running loss of epoch-54 batch-143 = 6.677350029349327e-06

Training epoch-54 batch-144
Running loss of epoch-54 batch-144 = 5.740206688642502e-06

Training epoch-54 batch-145
Running loss of epoch-54 batch-145 = 6.070593371987343e-06

Training epoch-54 batch-146
Running loss of epoch-54 batch-146 = 9.762588888406754e-06

Training epoch-54 batch-147
Running loss of epoch-54 batch-147 = 6.511807441711426e-06

Training epoch-54 batch-148
Running loss of epoch-54 batch-148 = 6.61495141685009e-06

Training epoch-54 batch-149
Running loss of epoch-54 batch-149 = 3.94345261156559e-06

Training epoch-54 batch-150
Running loss of epoch-54 batch-150 = 3.879424184560776e-06

Training epoch-54 batch-151
Running loss of epoch-54 batch-151 = 7.027294486761093e-06

Training epoch-54 batch-152
Running loss of epoch-54 batch-152 = 3.129243850708008e-06

Training epoch-54 batch-153
Running loss of epoch-54 batch-153 = 1.268833875656128e-05

Training epoch-54 batch-154
Running loss of epoch-54 batch-154 = 4.518311470746994e-06

Training epoch-54 batch-155
Running loss of epoch-54 batch-155 = 5.56139275431633e-06

Training epoch-54 batch-156
Running loss of epoch-54 batch-156 = 3.5529956221580505e-06

Training epoch-54 batch-157
Running loss of epoch-54 batch-157 = 6.198883056640625e-06

Finished training epoch-54.



Average train loss at epoch-54 = 6.1001315712928774e-06

Started Evaluation

Average val loss at epoch-54 = 1.103974266315091

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.14 %

Finished Evaluation



Started training epoch-55


Training epoch-55 batch-1
Running loss of epoch-55 batch-1 = 3.0300579965114594e-06

Training epoch-55 batch-2
Running loss of epoch-55 batch-2 = 1.621665433049202e-06

Training epoch-55 batch-3
Running loss of epoch-55 batch-3 = 3.4926924854516983e-06

Training epoch-55 batch-4
Running loss of epoch-55 batch-4 = 4.932517185807228e-06

Training epoch-55 batch-5
Running loss of epoch-55 batch-5 = 2.9266811907291412e-06

Training epoch-55 batch-6
Running loss of epoch-55 batch-6 = 1.0279472917318344e-06

Training epoch-55 batch-7
Running loss of epoch-55 batch-7 = 7.195631042122841e-06

Training epoch-55 batch-8
Running loss of epoch-55 batch-8 = 1.8102582544088364e-06

Training epoch-55 batch-9
Running loss of epoch-55 batch-9 = 4.090135917067528e-06

Training epoch-55 batch-10
Running loss of epoch-55 batch-10 = 8.393311873078346e-06

Training epoch-55 batch-11
Running loss of epoch-55 batch-11 = 6.101327016949654e-06

Training epoch-55 batch-12
Running loss of epoch-55 batch-12 = 4.480360075831413e-06

Training epoch-55 batch-13
Running loss of epoch-55 batch-13 = 4.7388020902872086e-06

Training epoch-55 batch-14
Running loss of epoch-55 batch-14 = 5.464302375912666e-06

Training epoch-55 batch-15
Running loss of epoch-55 batch-15 = 1.162942498922348e-05

Training epoch-55 batch-16
Running loss of epoch-55 batch-16 = 9.67155210673809e-06

Training epoch-55 batch-17
Running loss of epoch-55 batch-17 = 3.2302923500537872e-06

Training epoch-55 batch-18
Running loss of epoch-55 batch-18 = 4.835659638047218e-06

Training epoch-55 batch-19
Running loss of epoch-55 batch-19 = 1.2584496289491653e-06

Training epoch-55 batch-20
Running loss of epoch-55 batch-20 = 1.4558201655745506e-05

Training epoch-55 batch-21
Running loss of epoch-55 batch-21 = 2.148328348994255e-06

Training epoch-55 batch-22
Running loss of epoch-55 batch-22 = 5.62635250389576e-06

Training epoch-55 batch-23
Running loss of epoch-55 batch-23 = 2.903630957007408e-06

Training epoch-55 batch-24
Running loss of epoch-55 batch-24 = 1.006387174129486e-05

Training epoch-55 batch-25
Running loss of epoch-55 batch-25 = 7.459893822669983e-06

Training epoch-55 batch-26
Running loss of epoch-55 batch-26 = 1.3632699847221375e-05

Training epoch-55 batch-27
Running loss of epoch-55 batch-27 = 3.7294812500476837e-06

Training epoch-55 batch-28
Running loss of epoch-55 batch-28 = 8.652685210108757e-06

Training epoch-55 batch-29
Running loss of epoch-55 batch-29 = 2.650544047355652e-06

Training epoch-55 batch-30
Running loss of epoch-55 batch-30 = 3.803987056016922e-06

Training epoch-55 batch-31
Running loss of epoch-55 batch-31 = 2.1944288164377213e-06

Training epoch-55 batch-32
Running loss of epoch-55 batch-32 = 3.4356489777565002e-06

Training epoch-55 batch-33
Running loss of epoch-55 batch-33 = 4.4102780520915985e-06

Training epoch-55 batch-34
Running loss of epoch-55 batch-34 = 3.7155114114284515e-06

Training epoch-55 batch-35
Running loss of epoch-55 batch-35 = 2.0558945834636688e-05

Training epoch-55 batch-36
Running loss of epoch-55 batch-36 = 4.368252120912075e-06

Training epoch-55 batch-37
Running loss of epoch-55 batch-37 = 2.58604995906353e-06

Training epoch-55 batch-38
Running loss of epoch-55 batch-38 = 2.901768311858177e-06

Training epoch-55 batch-39
Running loss of epoch-55 batch-39 = 3.0561350286006927e-06

Training epoch-55 batch-40
Running loss of epoch-55 batch-40 = 3.700144588947296e-06

Training epoch-55 batch-41
Running loss of epoch-55 batch-41 = 8.368166163563728e-06

Training epoch-55 batch-42
Running loss of epoch-55 batch-42 = 2.9194634407758713e-06

Training epoch-55 batch-43
Running loss of epoch-55 batch-43 = 7.573748007416725e-06

Training epoch-55 batch-44
Running loss of epoch-55 batch-44 = 5.9658195823431015e-06

Training epoch-55 batch-45
Running loss of epoch-55 batch-45 = 4.754168912768364e-06

Training epoch-55 batch-46
Running loss of epoch-55 batch-46 = 8.437782526016235e-06

Training epoch-55 batch-47
Running loss of epoch-55 batch-47 = 5.054054781794548e-06

Training epoch-55 batch-48
Running loss of epoch-55 batch-48 = 3.7814024835824966e-06

Training epoch-55 batch-49
Running loss of epoch-55 batch-49 = 2.64681875705719e-06

Training epoch-55 batch-50
Running loss of epoch-55 batch-50 = 5.823094397783279e-06

Training epoch-55 batch-51
Running loss of epoch-55 batch-51 = 3.849854692816734e-06

Training epoch-55 batch-52
Running loss of epoch-55 batch-52 = 5.311798304319382e-06

Training epoch-55 batch-53
Running loss of epoch-55 batch-53 = 9.410316124558449e-06

Training epoch-55 batch-54
Running loss of epoch-55 batch-54 = 1.1911150068044662e-05

Training epoch-55 batch-55
Running loss of epoch-55 batch-55 = 6.042886525392532e-06

Training epoch-55 batch-56
Running loss of epoch-55 batch-56 = 1.996522769331932e-06

Training epoch-55 batch-57
Running loss of epoch-55 batch-57 = 5.042646080255508e-06

Training epoch-55 batch-58
Running loss of epoch-55 batch-58 = 4.219356924295425e-06

Training epoch-55 batch-59
Running loss of epoch-55 batch-59 = 3.3704563975334167e-06

Training epoch-55 batch-60
Running loss of epoch-55 batch-60 = 1.0006828233599663e-05

Training epoch-55 batch-61
Running loss of epoch-55 batch-61 = 6.651272997260094e-06

Training epoch-55 batch-62
Running loss of epoch-55 batch-62 = 7.806578651070595e-06

Training epoch-55 batch-63
Running loss of epoch-55 batch-63 = 7.50483013689518e-06

Training epoch-55 batch-64
Running loss of epoch-55 batch-64 = 2.9548536986112595e-06

Training epoch-55 batch-65
Running loss of epoch-55 batch-65 = 6.026239134371281e-06

Training epoch-55 batch-66
Running loss of epoch-55 batch-66 = 6.119720637798309e-06

Training epoch-55 batch-67
Running loss of epoch-55 batch-67 = 4.003522917628288e-06

Training epoch-55 batch-68
Running loss of epoch-55 batch-68 = 4.7977082431316376e-06

Training epoch-55 batch-69
Running loss of epoch-55 batch-69 = 3.7981662899255753e-06

Training epoch-55 batch-70
Running loss of epoch-55 batch-70 = 4.9800146371126175e-06

Training epoch-55 batch-71
Running loss of epoch-55 batch-71 = 3.3830292522907257e-06

Training epoch-55 batch-72
Running loss of epoch-55 batch-72 = 5.4587144404649734e-06

Training epoch-55 batch-73
Running loss of epoch-55 batch-73 = 6.415299139916897e-06

Training epoch-55 batch-74
Running loss of epoch-55 batch-74 = 6.8747904151678085e-06

Training epoch-55 batch-75
Running loss of epoch-55 batch-75 = 1.2543518096208572e-05

Training epoch-55 batch-76
Running loss of epoch-55 batch-76 = 2.2139865905046463e-06

Training epoch-55 batch-77
Running loss of epoch-55 batch-77 = 2.2780150175094604e-06

Training epoch-55 batch-78
Running loss of epoch-55 batch-78 = 3.7425197660923004e-06

Training epoch-55 batch-79
Running loss of epoch-55 batch-79 = 6.023561581969261e-06

Training epoch-55 batch-80
Running loss of epoch-55 batch-80 = 1.3006385415792465e-05

Training epoch-55 batch-81
Running loss of epoch-55 batch-81 = 3.5902485251426697e-06

Training epoch-55 batch-82
Running loss of epoch-55 batch-82 = 4.544621333479881e-06

Training epoch-55 batch-83
Running loss of epoch-55 batch-83 = 5.27501106262207e-06

Training epoch-55 batch-84
Running loss of epoch-55 batch-84 = 5.797715857625008e-06

Training epoch-55 batch-85
Running loss of epoch-55 batch-85 = 5.40073961019516e-06

Training epoch-55 batch-86
Running loss of epoch-55 batch-86 = 4.3655745685100555e-06

Training epoch-55 batch-87
Running loss of epoch-55 batch-87 = 6.647780537605286e-06

Training epoch-55 batch-88
Running loss of epoch-55 batch-88 = 4.677334800362587e-06

Training epoch-55 batch-89
Running loss of epoch-55 batch-89 = 4.7031790018081665e-06

Training epoch-55 batch-90
Running loss of epoch-55 batch-90 = 3.7085264921188354e-06

Training epoch-55 batch-91
Running loss of epoch-55 batch-91 = 6.2820035964250565e-06

Training epoch-55 batch-92
Running loss of epoch-55 batch-92 = 6.1478931456804276e-06

Training epoch-55 batch-93
Running loss of epoch-55 batch-93 = 8.39773565530777e-06

Training epoch-55 batch-94
Running loss of epoch-55 batch-94 = 3.3651012927293777e-06

Training epoch-55 batch-95
Running loss of epoch-55 batch-95 = 4.119006916880608e-06

Training epoch-55 batch-96
Running loss of epoch-55 batch-96 = 2.521788701415062e-06

Training epoch-55 batch-97
Running loss of epoch-55 batch-97 = 5.877576768398285e-06

Training epoch-55 batch-98
Running loss of epoch-55 batch-98 = 3.915745764970779e-06

Training epoch-55 batch-99
Running loss of epoch-55 batch-99 = 5.679670721292496e-06

Training epoch-55 batch-100
Running loss of epoch-55 batch-100 = 4.904111847281456e-06

Training epoch-55 batch-101
Running loss of epoch-55 batch-101 = 8.750241249799728e-06

Training epoch-55 batch-102
Running loss of epoch-55 batch-102 = 2.0961975678801537e-05

Training epoch-55 batch-103
Running loss of epoch-55 batch-103 = 3.3797696232795715e-06

Training epoch-55 batch-104
Running loss of epoch-55 batch-104 = 3.312481567263603e-06

Training epoch-55 batch-105
Running loss of epoch-55 batch-105 = 2.078944817185402e-06

Training epoch-55 batch-106
Running loss of epoch-55 batch-106 = 2.891756594181061e-06

Training epoch-55 batch-107
Running loss of epoch-55 batch-107 = 6.346963346004486e-06

Training epoch-55 batch-108
Running loss of epoch-55 batch-108 = 7.832888513803482e-06

Training epoch-55 batch-109
Running loss of epoch-55 batch-109 = 3.670109435915947e-06

Training epoch-55 batch-110
Running loss of epoch-55 batch-110 = 5.754875019192696e-06

Training epoch-55 batch-111
Running loss of epoch-55 batch-111 = 5.862442776560783e-06

Training epoch-55 batch-112
Running loss of epoch-55 batch-112 = 9.633135050535202e-06

Training epoch-55 batch-113
Running loss of epoch-55 batch-113 = 4.069879651069641e-06

Training epoch-55 batch-114
Running loss of epoch-55 batch-114 = 5.711335688829422e-06

Training epoch-55 batch-115
Running loss of epoch-55 batch-115 = 9.675510227680206e-06

Training epoch-55 batch-116
Running loss of epoch-55 batch-116 = 4.6622008085250854e-06

Training epoch-55 batch-117
Running loss of epoch-55 batch-117 = 8.924165740609169e-06

Training epoch-55 batch-118
Running loss of epoch-55 batch-118 = 3.849156200885773e-06

Training epoch-55 batch-119
Running loss of epoch-55 batch-119 = 8.089933544397354e-06

Training epoch-55 batch-120
Running loss of epoch-55 batch-120 = 8.819857612252235e-06

Training epoch-55 batch-121
Running loss of epoch-55 batch-121 = 5.174893885850906e-06

Training epoch-55 batch-122
Running loss of epoch-55 batch-122 = 6.464775651693344e-06

Training epoch-55 batch-123
Running loss of epoch-55 batch-123 = 6.7069195210933685e-06

Training epoch-55 batch-124
Running loss of epoch-55 batch-124 = 5.270820111036301e-06

Training epoch-55 batch-125
Running loss of epoch-55 batch-125 = 4.250789061188698e-06

Training epoch-55 batch-126
Running loss of epoch-55 batch-126 = 4.643574357032776e-06

Training epoch-55 batch-127
Running loss of epoch-55 batch-127 = 2.0992010831832886e-06

Training epoch-55 batch-128
Running loss of epoch-55 batch-128 = 6.919261068105698e-06

Training epoch-55 batch-129
Running loss of epoch-55 batch-129 = 2.3585744202136993e-06

Training epoch-55 batch-130
Running loss of epoch-55 batch-130 = 5.8014411479234695e-06

Training epoch-55 batch-131
Running loss of epoch-55 batch-131 = 9.162118658423424e-06

Training epoch-55 batch-132
Running loss of epoch-55 batch-132 = 1.4503486454486847e-05

Training epoch-55 batch-133
Running loss of epoch-55 batch-133 = 6.260350346565247e-06

Training epoch-55 batch-134
Running loss of epoch-55 batch-134 = 1.725088804960251e-05

Training epoch-55 batch-135
Running loss of epoch-55 batch-135 = 4.460802301764488e-06

Training epoch-55 batch-136
Running loss of epoch-55 batch-136 = 4.887580871582031e-06

Training epoch-55 batch-137
Running loss of epoch-55 batch-137 = 3.6885030567646027e-06

Training epoch-55 batch-138
Running loss of epoch-55 batch-138 = 2.5837216526269913e-06

Training epoch-55 batch-139
Running loss of epoch-55 batch-139 = 9.658047929406166e-06

Training epoch-55 batch-140
Running loss of epoch-55 batch-140 = 1.0226154699921608e-05

Training epoch-55 batch-141
Running loss of epoch-55 batch-141 = 9.182607755064964e-06

Training epoch-55 batch-142
Running loss of epoch-55 batch-142 = 4.1639432311058044e-06

Training epoch-55 batch-143
Running loss of epoch-55 batch-143 = 7.941853255033493e-06

Training epoch-55 batch-144
Running loss of epoch-55 batch-144 = 4.209112375974655e-06

Training epoch-55 batch-145
Running loss of epoch-55 batch-145 = 5.316454917192459e-06

Training epoch-55 batch-146
Running loss of epoch-55 batch-146 = 3.8133002817630768e-06

Training epoch-55 batch-147
Running loss of epoch-55 batch-147 = 1.4118617400527e-05

Training epoch-55 batch-148
Running loss of epoch-55 batch-148 = 1.1496245861053467e-05

Training epoch-55 batch-149
Running loss of epoch-55 batch-149 = 4.517612978816032e-06

Training epoch-55 batch-150
Running loss of epoch-55 batch-150 = 4.458939656615257e-06

Training epoch-55 batch-151
Running loss of epoch-55 batch-151 = 3.8119032979011536e-06

Training epoch-55 batch-152
Running loss of epoch-55 batch-152 = 9.599723853170872e-06

Training epoch-55 batch-153
Running loss of epoch-55 batch-153 = 4.998175427317619e-06

Training epoch-55 batch-154
Running loss of epoch-55 batch-154 = 4.308763891458511e-06

Training epoch-55 batch-155
Running loss of epoch-55 batch-155 = 6.569083780050278e-06

Training epoch-55 batch-156
Running loss of epoch-55 batch-156 = 8.607283234596252e-06

Training epoch-55 batch-157
Running loss of epoch-55 batch-157 = 1.2531876564025879e-05

Finished training epoch-55.



Average train loss at epoch-55 = 5.926236510276794e-06

Started Evaluation

Average val loss at epoch-55 = 1.1085390038736502

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 63.24 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.21 %

Finished Evaluation



Started training epoch-56


Training epoch-56 batch-1
Running loss of epoch-56 batch-1 = 4.295026883482933e-06

Training epoch-56 batch-2
Running loss of epoch-56 batch-2 = 6.092013791203499e-06

Training epoch-56 batch-3
Running loss of epoch-56 batch-3 = 3.9797741919755936e-06

Training epoch-56 batch-4
Running loss of epoch-56 batch-4 = 6.4373016357421875e-06

Training epoch-56 batch-5
Running loss of epoch-56 batch-5 = 2.807122655212879e-06

Training epoch-56 batch-6
Running loss of epoch-56 batch-6 = 8.720438927412033e-06

Training epoch-56 batch-7
Running loss of epoch-56 batch-7 = 7.617287337779999e-06

Training epoch-56 batch-8
Running loss of epoch-56 batch-8 = 5.315057933330536e-06

Training epoch-56 batch-9
Running loss of epoch-56 batch-9 = 3.468012437224388e-06

Training epoch-56 batch-10
Running loss of epoch-56 batch-10 = 7.603317499160767e-06

Training epoch-56 batch-11
Running loss of epoch-56 batch-11 = 2.7355272322893143e-06

Training epoch-56 batch-12
Running loss of epoch-56 batch-12 = 1.3930490240454674e-05

Training epoch-56 batch-13
Running loss of epoch-56 batch-13 = 1.1319410987198353e-05

Training epoch-56 batch-14
Running loss of epoch-56 batch-14 = 3.39350663125515e-06

Training epoch-56 batch-15
Running loss of epoch-56 batch-15 = 5.233800038695335e-06

Training epoch-56 batch-16
Running loss of epoch-56 batch-16 = 2.5602057576179504e-06

Training epoch-56 batch-17
Running loss of epoch-56 batch-17 = 7.357215508818626e-06

Training epoch-56 batch-18
Running loss of epoch-56 batch-18 = 1.0785646736621857e-05

Training epoch-56 batch-19
Running loss of epoch-56 batch-19 = 3.958120942115784e-06

Training epoch-56 batch-20
Running loss of epoch-56 batch-20 = 1.5996862202882767e-05

Training epoch-56 batch-21
Running loss of epoch-56 batch-21 = 5.11854887008667e-06

Training epoch-56 batch-22
Running loss of epoch-56 batch-22 = 4.884786903858185e-06

Training epoch-56 batch-23
Running loss of epoch-56 batch-23 = 4.464061930775642e-06

Training epoch-56 batch-24
Running loss of epoch-56 batch-24 = 4.1280873119831085e-06

Training epoch-56 batch-25
Running loss of epoch-56 batch-25 = 3.287801519036293e-06

Training epoch-56 batch-26
Running loss of epoch-56 batch-26 = 5.1765237003564835e-06

Training epoch-56 batch-27
Running loss of epoch-56 batch-27 = 5.359528586268425e-06

Training epoch-56 batch-28
Running loss of epoch-56 batch-28 = 6.113201379776001e-06

Training epoch-56 batch-29
Running loss of epoch-56 batch-29 = 4.022615030407906e-06

Training epoch-56 batch-30
Running loss of epoch-56 batch-30 = 4.738336428999901e-06

Training epoch-56 batch-31
Running loss of epoch-56 batch-31 = 1.8111895769834518e-06

Training epoch-56 batch-32
Running loss of epoch-56 batch-32 = 7.527181878685951e-06

Training epoch-56 batch-33
Running loss of epoch-56 batch-33 = 3.4722033888101578e-06

Training epoch-56 batch-34
Running loss of epoch-56 batch-34 = 4.033325240015984e-06

Training epoch-56 batch-35
Running loss of epoch-56 batch-35 = 8.210539817810059e-06

Training epoch-56 batch-36
Running loss of epoch-56 batch-36 = 5.938345566391945e-06

Training epoch-56 batch-37
Running loss of epoch-56 batch-37 = 4.839850589632988e-06

Training epoch-56 batch-38
Running loss of epoch-56 batch-38 = 3.4188851714134216e-06

Training epoch-56 batch-39
Running loss of epoch-56 batch-39 = 9.221956133842468e-06

Training epoch-56 batch-40
Running loss of epoch-56 batch-40 = 1.5301629900932312e-06

Training epoch-56 batch-41
Running loss of epoch-56 batch-41 = 1.4384277164936066e-06

Training epoch-56 batch-42
Running loss of epoch-56 batch-42 = 4.205387085676193e-06

Training epoch-56 batch-43
Running loss of epoch-56 batch-43 = 1.8274877220392227e-06

Training epoch-56 batch-44
Running loss of epoch-56 batch-44 = 7.468275725841522e-06

Training epoch-56 batch-45
Running loss of epoch-56 batch-45 = 7.115770131349564e-06

Training epoch-56 batch-46
Running loss of epoch-56 batch-46 = 3.0708033591508865e-06

Training epoch-56 batch-47
Running loss of epoch-56 batch-47 = 5.6372955441474915e-06

Training epoch-56 batch-48
Running loss of epoch-56 batch-48 = 4.108762368559837e-06

Training epoch-56 batch-49
Running loss of epoch-56 batch-49 = 6.032176315784454e-06

Training epoch-56 batch-50
Running loss of epoch-56 batch-50 = 5.745561793446541e-06

Training epoch-56 batch-51
Running loss of epoch-56 batch-51 = 2.393033355474472e-06

Training epoch-56 batch-52
Running loss of epoch-56 batch-52 = 2.144603058695793e-06

Training epoch-56 batch-53
Running loss of epoch-56 batch-53 = 1.7476268112659454e-06

Training epoch-56 batch-54
Running loss of epoch-56 batch-54 = 5.062902346253395e-06

Training epoch-56 batch-55
Running loss of epoch-56 batch-55 = 8.849427103996277e-06

Training epoch-56 batch-56
Running loss of epoch-56 batch-56 = 7.031485438346863e-06

Training epoch-56 batch-57
Running loss of epoch-56 batch-57 = 7.734168320894241e-06

Training epoch-56 batch-58
Running loss of epoch-56 batch-58 = 6.675487384200096e-06

Training epoch-56 batch-59
Running loss of epoch-56 batch-59 = 1.3913260772824287e-05

Training epoch-56 batch-60
Running loss of epoch-56 batch-60 = 8.03195871412754e-06

Training epoch-56 batch-61
Running loss of epoch-56 batch-61 = 6.755348294973373e-06

Training epoch-56 batch-62
Running loss of epoch-56 batch-62 = 1.2281117960810661e-05

Training epoch-56 batch-63
Running loss of epoch-56 batch-63 = 5.391659215092659e-06

Training epoch-56 batch-64
Running loss of epoch-56 batch-64 = 4.699220880866051e-06

Training epoch-56 batch-65
Running loss of epoch-56 batch-65 = 2.978602424263954e-06

Training epoch-56 batch-66
Running loss of epoch-56 batch-66 = 6.447546184062958e-06

Training epoch-56 batch-67
Running loss of epoch-56 batch-67 = 7.33998604118824e-06

Training epoch-56 batch-68
Running loss of epoch-56 batch-68 = 2.1476298570632935e-06

Training epoch-56 batch-69
Running loss of epoch-56 batch-69 = 2.2295862436294556e-06

Training epoch-56 batch-70
Running loss of epoch-56 batch-70 = 4.027271643280983e-06

Training epoch-56 batch-71
Running loss of epoch-56 batch-71 = 4.263129085302353e-06

Training epoch-56 batch-72
Running loss of epoch-56 batch-72 = 6.15045428276062e-06

Training epoch-56 batch-73
Running loss of epoch-56 batch-73 = 5.374196916818619e-06

Training epoch-56 batch-74
Running loss of epoch-56 batch-74 = 4.1977036744356155e-06

Training epoch-56 batch-75
Running loss of epoch-56 batch-75 = 1.4538760297000408e-05

Training epoch-56 batch-76
Running loss of epoch-56 batch-76 = 2.2060703486204147e-06

Training epoch-56 batch-77
Running loss of epoch-56 batch-77 = 9.174225851893425e-06

Training epoch-56 batch-78
Running loss of epoch-56 batch-78 = 4.6889763325452805e-06

Training epoch-56 batch-79
Running loss of epoch-56 batch-79 = 3.518536686897278e-06

Training epoch-56 batch-80
Running loss of epoch-56 batch-80 = 3.237975761294365e-06

Training epoch-56 batch-81
Running loss of epoch-56 batch-81 = 7.488764822483063e-06

Training epoch-56 batch-82
Running loss of epoch-56 batch-82 = 2.8526410460472107e-06

Training epoch-56 batch-83
Running loss of epoch-56 batch-83 = 6.531132385134697e-06

Training epoch-56 batch-84
Running loss of epoch-56 batch-84 = 8.15466046333313e-06

Training epoch-56 batch-85
Running loss of epoch-56 batch-85 = 3.5457778722047806e-06

Training epoch-56 batch-86
Running loss of epoch-56 batch-86 = 3.7963036447763443e-06

Training epoch-56 batch-87
Running loss of epoch-56 batch-87 = 7.889466360211372e-06

Training epoch-56 batch-88
Running loss of epoch-56 batch-88 = 3.3578835427761078e-06

Training epoch-56 batch-89
Running loss of epoch-56 batch-89 = 9.221723303198814e-06

Training epoch-56 batch-90
Running loss of epoch-56 batch-90 = 8.413568139076233e-06

Training epoch-56 batch-91
Running loss of epoch-56 batch-91 = 3.5190023481845856e-06

Training epoch-56 batch-92
Running loss of epoch-56 batch-92 = 7.653841748833656e-06

Training epoch-56 batch-93
Running loss of epoch-56 batch-93 = 5.441717803478241e-06

Training epoch-56 batch-94
Running loss of epoch-56 batch-94 = 3.1283125281333923e-06

Training epoch-56 batch-95
Running loss of epoch-56 batch-95 = 3.0212104320526123e-06

Training epoch-56 batch-96
Running loss of epoch-56 batch-96 = 7.811235263943672e-06

Training epoch-56 batch-97
Running loss of epoch-56 batch-97 = 1.9990839064121246e-06

Training epoch-56 batch-98
Running loss of epoch-56 batch-98 = 8.18353146314621e-06

Training epoch-56 batch-99
Running loss of epoch-56 batch-99 = 6.0258898884058e-06

Training epoch-56 batch-100
Running loss of epoch-56 batch-100 = 6.778864189982414e-06

Training epoch-56 batch-101
Running loss of epoch-56 batch-101 = 2.8479844331741333e-06

Training epoch-56 batch-102
Running loss of epoch-56 batch-102 = 3.367895260453224e-06

Training epoch-56 batch-103
Running loss of epoch-56 batch-103 = 6.573274731636047e-06

Training epoch-56 batch-104
Running loss of epoch-56 batch-104 = 1.07551459223032e-05

Training epoch-56 batch-105
Running loss of epoch-56 batch-105 = 3.0118972063064575e-06

Training epoch-56 batch-106
Running loss of epoch-56 batch-106 = 2.224929630756378e-06

Training epoch-56 batch-107
Running loss of epoch-56 batch-107 = 7.94953666627407e-06

Training epoch-56 batch-108
Running loss of epoch-56 batch-108 = 3.852182999253273e-06

Training epoch-56 batch-109
Running loss of epoch-56 batch-109 = 5.112728103995323e-06

Training epoch-56 batch-110
Running loss of epoch-56 batch-110 = 3.269873559474945e-06

Training epoch-56 batch-111
Running loss of epoch-56 batch-111 = 4.0996819734573364e-06

Training epoch-56 batch-112
Running loss of epoch-56 batch-112 = 7.256167009472847e-06

Training epoch-56 batch-113
Running loss of epoch-56 batch-113 = 4.145549610257149e-06

Training epoch-56 batch-114
Running loss of epoch-56 batch-114 = 8.484115824103355e-06

Training epoch-56 batch-115
Running loss of epoch-56 batch-115 = 4.9015507102012634e-06

Training epoch-56 batch-116
Running loss of epoch-56 batch-116 = 1.6549602150917053e-06

Training epoch-56 batch-117
Running loss of epoch-56 batch-117 = 3.4940894693136215e-06

Training epoch-56 batch-118
Running loss of epoch-56 batch-118 = 3.432156518101692e-06

Training epoch-56 batch-119
Running loss of epoch-56 batch-119 = 9.337672963738441e-06

Training epoch-56 batch-120
Running loss of epoch-56 batch-120 = 2.7883797883987427e-06

Training epoch-56 batch-121
Running loss of epoch-56 batch-121 = 5.172565579414368e-06

Training epoch-56 batch-122
Running loss of epoch-56 batch-122 = 2.4605542421340942e-06

Training epoch-56 batch-123
Running loss of epoch-56 batch-123 = 5.6335702538490295e-06

Training epoch-56 batch-124
Running loss of epoch-56 batch-124 = 4.583271220326424e-06

Training epoch-56 batch-125
Running loss of epoch-56 batch-125 = 6.5497588366270065e-06

Training epoch-56 batch-126
Running loss of epoch-56 batch-126 = 3.876630216836929e-06

Training epoch-56 batch-127
Running loss of epoch-56 batch-127 = 6.264308467507362e-06

Training epoch-56 batch-128
Running loss of epoch-56 batch-128 = 6.014714017510414e-06

Training epoch-56 batch-129
Running loss of epoch-56 batch-129 = 6.618676707148552e-06

Training epoch-56 batch-130
Running loss of epoch-56 batch-130 = 6.44405372440815e-06

Training epoch-56 batch-131
Running loss of epoch-56 batch-131 = 3.3588148653507233e-06

Training epoch-56 batch-132
Running loss of epoch-56 batch-132 = 6.071524694561958e-06

Training epoch-56 batch-133
Running loss of epoch-56 batch-133 = 5.923910066485405e-06

Training epoch-56 batch-134
Running loss of epoch-56 batch-134 = 2.580927684903145e-06

Training epoch-56 batch-135
Running loss of epoch-56 batch-135 = 8.10413621366024e-06

Training epoch-56 batch-136
Running loss of epoch-56 batch-136 = 2.0507490262389183e-05

Training epoch-56 batch-137
Running loss of epoch-56 batch-137 = 5.0694216042757034e-06

Training epoch-56 batch-138
Running loss of epoch-56 batch-138 = 7.177237421274185e-06

Training epoch-56 batch-139
Running loss of epoch-56 batch-139 = 1.686043106019497e-05

Training epoch-56 batch-140
Running loss of epoch-56 batch-140 = 2.889661118388176e-06

Training epoch-56 batch-141
Running loss of epoch-56 batch-141 = 4.449859261512756e-06

Training epoch-56 batch-142
Running loss of epoch-56 batch-142 = 4.444969817996025e-06

Training epoch-56 batch-143
Running loss of epoch-56 batch-143 = 8.13719816505909e-06

Training epoch-56 batch-144
Running loss of epoch-56 batch-144 = 1.020822674036026e-05

Training epoch-56 batch-145
Running loss of epoch-56 batch-145 = 5.912501364946365e-06

Training epoch-56 batch-146
Running loss of epoch-56 batch-146 = 3.87127511203289e-06

Training epoch-56 batch-147
Running loss of epoch-56 batch-147 = 4.062661901116371e-06

Training epoch-56 batch-148
Running loss of epoch-56 batch-148 = 2.635875716805458e-06

Training epoch-56 batch-149
Running loss of epoch-56 batch-149 = 2.4479813873767853e-06

Training epoch-56 batch-150
Running loss of epoch-56 batch-150 = 4.556961357593536e-06

Training epoch-56 batch-151
Running loss of epoch-56 batch-151 = 6.3374172896146774e-06

Training epoch-56 batch-152
Running loss of epoch-56 batch-152 = 4.766741767525673e-06

Training epoch-56 batch-153
Running loss of epoch-56 batch-153 = 2.501998096704483e-06

Training epoch-56 batch-154
Running loss of epoch-56 batch-154 = 1.54722947627306e-05

Training epoch-56 batch-155
Running loss of epoch-56 batch-155 = 3.94647940993309e-06

Training epoch-56 batch-156
Running loss of epoch-56 batch-156 = 8.173519745469093e-06

Training epoch-56 batch-157
Running loss of epoch-56 batch-157 = 5.4333359003067017e-05

Finished training epoch-56.



Average train loss at epoch-56 = 5.784515291452408e-06

Started Evaluation

Average val loss at epoch-56 = 1.116032229460296

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 60.73 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.59 %
Accuracy for class execute is: 44.58 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 82.94 %

Finished Evaluation



Started training epoch-57


Training epoch-57 batch-1
Running loss of epoch-57 batch-1 = 5.419598892331123e-06

Training epoch-57 batch-2
Running loss of epoch-57 batch-2 = 6.769783794879913e-06

Training epoch-57 batch-3
Running loss of epoch-57 batch-3 = 9.684357792139053e-06

Training epoch-57 batch-4
Running loss of epoch-57 batch-4 = 5.752313882112503e-06

Training epoch-57 batch-5
Running loss of epoch-57 batch-5 = 4.345783963799477e-06

Training epoch-57 batch-6
Running loss of epoch-57 batch-6 = 1.1607538908720016e-05

Training epoch-57 batch-7
Running loss of epoch-57 batch-7 = 7.657567039132118e-06

Training epoch-57 batch-8
Running loss of epoch-57 batch-8 = 5.007022991776466e-06

Training epoch-57 batch-9
Running loss of epoch-57 batch-9 = 2.0225998014211655e-06

Training epoch-57 batch-10
Running loss of epoch-57 batch-10 = 5.539273843169212e-06

Training epoch-57 batch-11
Running loss of epoch-57 batch-11 = 8.49645584821701e-06

Training epoch-57 batch-12
Running loss of epoch-57 batch-12 = 4.042172804474831e-06

Training epoch-57 batch-13
Running loss of epoch-57 batch-13 = 4.994915798306465e-06

Training epoch-57 batch-14
Running loss of epoch-57 batch-14 = 2.3494940251111984e-06

Training epoch-57 batch-15
Running loss of epoch-57 batch-15 = 4.7765206545591354e-06

Training epoch-57 batch-16
Running loss of epoch-57 batch-16 = 2.112705260515213e-06

Training epoch-57 batch-17
Running loss of epoch-57 batch-17 = 6.9784000515937805e-06

Training epoch-57 batch-18
Running loss of epoch-57 batch-18 = 8.147675544023514e-06

Training epoch-57 batch-19
Running loss of epoch-57 batch-19 = 3.6461278796195984e-06

Training epoch-57 batch-20
Running loss of epoch-57 batch-20 = 1.2963777408003807e-05

Training epoch-57 batch-21
Running loss of epoch-57 batch-21 = 4.73950058221817e-06

Training epoch-57 batch-22
Running loss of epoch-57 batch-22 = 5.86523674428463e-06

Training epoch-57 batch-23
Running loss of epoch-57 batch-23 = 4.497356712818146e-06

Training epoch-57 batch-24
Running loss of epoch-57 batch-24 = 3.507360816001892e-06

Training epoch-57 batch-25
Running loss of epoch-57 batch-25 = 1.732748933136463e-05

Training epoch-57 batch-26
Running loss of epoch-57 batch-26 = 2.560904249548912e-06

Training epoch-57 batch-27
Running loss of epoch-57 batch-27 = 1.267693005502224e-05

Training epoch-57 batch-28
Running loss of epoch-57 batch-28 = 7.5178686529397964e-06

Training epoch-57 batch-29
Running loss of epoch-57 batch-29 = 1.695007085800171e-06

Training epoch-57 batch-30
Running loss of epoch-57 batch-30 = 3.2831449061632156e-06

Training epoch-57 batch-31
Running loss of epoch-57 batch-31 = 4.020053893327713e-06

Training epoch-57 batch-32
Running loss of epoch-57 batch-32 = 5.2622053772211075e-06

Training epoch-57 batch-33
Running loss of epoch-57 batch-33 = 1.1927913874387741e-05

Training epoch-57 batch-34
Running loss of epoch-57 batch-34 = 5.284091457724571e-06

Training epoch-57 batch-35
Running loss of epoch-57 batch-35 = 9.201234206557274e-06

Training epoch-57 batch-36
Running loss of epoch-57 batch-36 = 7.400289177894592e-06

Training epoch-57 batch-37
Running loss of epoch-57 batch-37 = 4.32250089943409e-06

Training epoch-57 batch-38
Running loss of epoch-57 batch-38 = 3.046588972210884e-06

Training epoch-57 batch-39
Running loss of epoch-57 batch-39 = 5.930894985795021e-06

Training epoch-57 batch-40
Running loss of epoch-57 batch-40 = 5.675246939063072e-06

Training epoch-57 batch-41
Running loss of epoch-57 batch-41 = 3.6819837987422943e-06

Training epoch-57 batch-42
Running loss of epoch-57 batch-42 = 5.4209958761930466e-06

Training epoch-57 batch-43
Running loss of epoch-57 batch-43 = 7.479684427380562e-06

Training epoch-57 batch-44
Running loss of epoch-57 batch-44 = 3.157183527946472e-06

Training epoch-57 batch-45
Running loss of epoch-57 batch-45 = 6.253132596611977e-06

Training epoch-57 batch-46
Running loss of epoch-57 batch-46 = 3.6391429603099823e-06

Training epoch-57 batch-47
Running loss of epoch-57 batch-47 = 2.966029569506645e-06

Training epoch-57 batch-48
Running loss of epoch-57 batch-48 = 1.0910909622907639e-05

Training epoch-57 batch-49
Running loss of epoch-57 batch-49 = 1.391430851072073e-05

Training epoch-57 batch-50
Running loss of epoch-57 batch-50 = 2.2605527192354202e-06

Training epoch-57 batch-51
Running loss of epoch-57 batch-51 = 2.61492095887661e-06

Training epoch-57 batch-52
Running loss of epoch-57 batch-52 = 3.647059202194214e-06

Training epoch-57 batch-53
Running loss of epoch-57 batch-53 = 2.917833626270294e-06

Training epoch-57 batch-54
Running loss of epoch-57 batch-54 = 6.1586033552885056e-06

Training epoch-57 batch-55
Running loss of epoch-57 batch-55 = 1.4500692486763e-05

Training epoch-57 batch-56
Running loss of epoch-57 batch-56 = 4.784902557730675e-06

Training epoch-57 batch-57
Running loss of epoch-57 batch-57 = 4.729023203253746e-06

Training epoch-57 batch-58
Running loss of epoch-57 batch-58 = 5.052657797932625e-06

Training epoch-57 batch-59
Running loss of epoch-57 batch-59 = 5.1551032811403275e-06

Training epoch-57 batch-60
Running loss of epoch-57 batch-60 = 4.846835508942604e-06

Training epoch-57 batch-61
Running loss of epoch-57 batch-61 = 4.827277734875679e-06

Training epoch-57 batch-62
Running loss of epoch-57 batch-62 = 4.0389131754636765e-06

Training epoch-57 batch-63
Running loss of epoch-57 batch-63 = 3.8070138543844223e-06

Training epoch-57 batch-64
Running loss of epoch-57 batch-64 = 4.002358764410019e-06

Training epoch-57 batch-65
Running loss of epoch-57 batch-65 = 7.90553167462349e-06

Training epoch-57 batch-66
Running loss of epoch-57 batch-66 = 3.64682637155056e-06

Training epoch-57 batch-67
Running loss of epoch-57 batch-67 = 1.3874378055334091e-06

Training epoch-57 batch-68
Running loss of epoch-57 batch-68 = 5.097594112157822e-06

Training epoch-57 batch-69
Running loss of epoch-57 batch-69 = 8.292379789054394e-06

Training epoch-57 batch-70
Running loss of epoch-57 batch-70 = 6.833113729953766e-06

Training epoch-57 batch-71
Running loss of epoch-57 batch-71 = 3.422144800424576e-06

Training epoch-57 batch-72
Running loss of epoch-57 batch-72 = 2.585817128419876e-06

Training epoch-57 batch-73
Running loss of epoch-57 batch-73 = 1.5045516192913055e-06

Training epoch-57 batch-74
Running loss of epoch-57 batch-74 = 3.193272277712822e-06

Training epoch-57 batch-75
Running loss of epoch-57 batch-75 = 7.400987669825554e-06

Training epoch-57 batch-76
Running loss of epoch-57 batch-76 = 6.174203008413315e-06

Training epoch-57 batch-77
Running loss of epoch-57 batch-77 = 4.995847120881081e-06

Training epoch-57 batch-78
Running loss of epoch-57 batch-78 = 2.926914021372795e-06

Training epoch-57 batch-79
Running loss of epoch-57 batch-79 = 5.650566890835762e-06

Training epoch-57 batch-80
Running loss of epoch-57 batch-80 = 5.352078005671501e-06

Training epoch-57 batch-81
Running loss of epoch-57 batch-81 = 3.0831433832645416e-06

Training epoch-57 batch-82
Running loss of epoch-57 batch-82 = 4.783738404512405e-06

Training epoch-57 batch-83
Running loss of epoch-57 batch-83 = 1.5388475731015205e-05

Training epoch-57 batch-84
Running loss of epoch-57 batch-84 = 1.0679941624403e-05

Training epoch-57 batch-85
Running loss of epoch-57 batch-85 = 7.885275408625603e-06

Training epoch-57 batch-86
Running loss of epoch-57 batch-86 = 4.434492439031601e-06

Training epoch-57 batch-87
Running loss of epoch-57 batch-87 = 3.2526440918445587e-06

Training epoch-57 batch-88
Running loss of epoch-57 batch-88 = 3.823079168796539e-06

Training epoch-57 batch-89
Running loss of epoch-57 batch-89 = 2.896646037697792e-06

Training epoch-57 batch-90
Running loss of epoch-57 batch-90 = 3.156019374728203e-06

Training epoch-57 batch-91
Running loss of epoch-57 batch-91 = 9.587034583091736e-06

Training epoch-57 batch-92
Running loss of epoch-57 batch-92 = 1.401873305439949e-06

Training epoch-57 batch-93
Running loss of epoch-57 batch-93 = 4.224013537168503e-06

Training epoch-57 batch-94
Running loss of epoch-57 batch-94 = 1.5974510461091995e-06

Training epoch-57 batch-95
Running loss of epoch-57 batch-95 = 2.0687002688646317e-06

Training epoch-57 batch-96
Running loss of epoch-57 batch-96 = 8.353497833013535e-06

Training epoch-57 batch-97
Running loss of epoch-57 batch-97 = 7.681548595428467e-06

Training epoch-57 batch-98
Running loss of epoch-57 batch-98 = 5.012843757867813e-06

Training epoch-57 batch-99
Running loss of epoch-57 batch-99 = 4.4496264308691025e-06

Training epoch-57 batch-100
Running loss of epoch-57 batch-100 = 2.5741755962371826e-06

Training epoch-57 batch-101
Running loss of epoch-57 batch-101 = 2.48616561293602e-06

Training epoch-57 batch-102
Running loss of epoch-57 batch-102 = 8.400995284318924e-06

Training epoch-57 batch-103
Running loss of epoch-57 batch-103 = 2.362532541155815e-06

Training epoch-57 batch-104
Running loss of epoch-57 batch-104 = 5.630776286125183e-06

Training epoch-57 batch-105
Running loss of epoch-57 batch-105 = 2.0603183656930923e-06

Training epoch-57 batch-106
Running loss of epoch-57 batch-106 = 4.520406946539879e-06

Training epoch-57 batch-107
Running loss of epoch-57 batch-107 = 5.484558641910553e-06

Training epoch-57 batch-108
Running loss of epoch-57 batch-108 = 5.9872400015592575e-06

Training epoch-57 batch-109
Running loss of epoch-57 batch-109 = 5.020061507821083e-06

Training epoch-57 batch-110
Running loss of epoch-57 batch-110 = 1.6841106116771698e-05

Training epoch-57 batch-111
Running loss of epoch-57 batch-111 = 3.1481031328439713e-06

Training epoch-57 batch-112
Running loss of epoch-57 batch-112 = 3.906199708580971e-06

Training epoch-57 batch-113
Running loss of epoch-57 batch-113 = 2.8284266591072083e-06

Training epoch-57 batch-114
Running loss of epoch-57 batch-114 = 4.109228029847145e-06

Training epoch-57 batch-115
Running loss of epoch-57 batch-115 = 2.887565642595291e-06

Training epoch-57 batch-116
Running loss of epoch-57 batch-116 = 4.512723535299301e-06

Training epoch-57 batch-117
Running loss of epoch-57 batch-117 = 5.198875442147255e-06

Training epoch-57 batch-118
Running loss of epoch-57 batch-118 = 8.28946940600872e-06

Training epoch-57 batch-119
Running loss of epoch-57 batch-119 = 6.24428503215313e-06

Training epoch-57 batch-120
Running loss of epoch-57 batch-120 = 5.695968866348267e-06

Training epoch-57 batch-121
Running loss of epoch-57 batch-121 = 8.10110941529274e-06

Training epoch-57 batch-122
Running loss of epoch-57 batch-122 = 3.4493859857320786e-06

Training epoch-57 batch-123
Running loss of epoch-57 batch-123 = 2.434477210044861e-06

Training epoch-57 batch-124
Running loss of epoch-57 batch-124 = 9.44524072110653e-06

Training epoch-57 batch-125
Running loss of epoch-57 batch-125 = 3.833789378404617e-06

Training epoch-57 batch-126
Running loss of epoch-57 batch-126 = 4.31830994784832e-06

Training epoch-57 batch-127
Running loss of epoch-57 batch-127 = 3.519700840115547e-06

Training epoch-57 batch-128
Running loss of epoch-57 batch-128 = 4.8729125410318375e-06

Training epoch-57 batch-129
Running loss of epoch-57 batch-129 = 3.3010728657245636e-06

Training epoch-57 batch-130
Running loss of epoch-57 batch-130 = 9.58004966378212e-06

Training epoch-57 batch-131
Running loss of epoch-57 batch-131 = 7.554073818027973e-06

Training epoch-57 batch-132
Running loss of epoch-57 batch-132 = 4.675239324569702e-06

Training epoch-57 batch-133
Running loss of epoch-57 batch-133 = 5.765119567513466e-06

Training epoch-57 batch-134
Running loss of epoch-57 batch-134 = 2.6244670152664185e-06

Training epoch-57 batch-135
Running loss of epoch-57 batch-135 = 6.41215592622757e-06

Training epoch-57 batch-136
Running loss of epoch-57 batch-136 = 4.0156301110982895e-06

Training epoch-57 batch-137
Running loss of epoch-57 batch-137 = 2.923421561717987e-06

Training epoch-57 batch-138
Running loss of epoch-57 batch-138 = 4.531117156147957e-06

Training epoch-57 batch-139
Running loss of epoch-57 batch-139 = 6.4587220549583435e-06

Training epoch-57 batch-140
Running loss of epoch-57 batch-140 = 8.104369044303894e-06

Training epoch-57 batch-141
Running loss of epoch-57 batch-141 = 9.587034583091736e-06

Training epoch-57 batch-142
Running loss of epoch-57 batch-142 = 8.885981515049934e-06

Training epoch-57 batch-143
Running loss of epoch-57 batch-143 = 4.037749022245407e-06

Training epoch-57 batch-144
Running loss of epoch-57 batch-144 = 2.1264422684907913e-06

Training epoch-57 batch-145
Running loss of epoch-57 batch-145 = 4.685716703534126e-06

Training epoch-57 batch-146
Running loss of epoch-57 batch-146 = 3.523426130414009e-06

Training epoch-57 batch-147
Running loss of epoch-57 batch-147 = 6.938120350241661e-06

Training epoch-57 batch-148
Running loss of epoch-57 batch-148 = 6.980961188673973e-06

Training epoch-57 batch-149
Running loss of epoch-57 batch-149 = 5.8407895267009735e-06

Training epoch-57 batch-150
Running loss of epoch-57 batch-150 = 5.440087988972664e-06

Training epoch-57 batch-151
Running loss of epoch-57 batch-151 = 6.90016895532608e-06

Training epoch-57 batch-152
Running loss of epoch-57 batch-152 = 2.7904752641916275e-06

Training epoch-57 batch-153
Running loss of epoch-57 batch-153 = 5.886424332857132e-06

Training epoch-57 batch-154
Running loss of epoch-57 batch-154 = 3.8282014429569244e-06

Training epoch-57 batch-155
Running loss of epoch-57 batch-155 = 3.868015483021736e-06

Training epoch-57 batch-156
Running loss of epoch-57 batch-156 = 6.74370676279068e-06

Training epoch-57 batch-157
Running loss of epoch-57 batch-157 = 7.059425115585327e-06

Finished training epoch-57.



Average train loss at epoch-57 = 5.5224962532520295e-06

Started Evaluation

Average val loss at epoch-57 = 1.1102602624430467

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.16 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.19 %

Finished Evaluation



Started training epoch-58


Training epoch-58 batch-1
Running loss of epoch-58 batch-1 = 4.205619916319847e-06

Training epoch-58 batch-2
Running loss of epoch-58 batch-2 = 6.233109161257744e-06

Training epoch-58 batch-3
Running loss of epoch-58 batch-3 = 3.3918768167495728e-06

Training epoch-58 batch-4
Running loss of epoch-58 batch-4 = 5.903886631131172e-06

Training epoch-58 batch-5
Running loss of epoch-58 batch-5 = 4.617730155587196e-06

Training epoch-58 batch-6
Running loss of epoch-58 batch-6 = 5.687586963176727e-06

Training epoch-58 batch-7
Running loss of epoch-58 batch-7 = 2.4978071451187134e-06

Training epoch-58 batch-8
Running loss of epoch-58 batch-8 = 2.9080547392368317e-06

Training epoch-58 batch-9
Running loss of epoch-58 batch-9 = 5.48316165804863e-06

Training epoch-58 batch-10
Running loss of epoch-58 batch-10 = 5.068490281701088e-06

Training epoch-58 batch-11
Running loss of epoch-58 batch-11 = 4.818663001060486e-06

Training epoch-58 batch-12
Running loss of epoch-58 batch-12 = 5.141133442521095e-06

Training epoch-58 batch-13
Running loss of epoch-58 batch-13 = 6.0996972024440765e-06

Training epoch-58 batch-14
Running loss of epoch-58 batch-14 = 3.1797681003808975e-06

Training epoch-58 batch-15
Running loss of epoch-58 batch-15 = 2.3622997105121613e-06

Training epoch-58 batch-16
Running loss of epoch-58 batch-16 = 5.736015737056732e-06

Training epoch-58 batch-17
Running loss of epoch-58 batch-17 = 3.970228135585785e-06

Training epoch-58 batch-18
Running loss of epoch-58 batch-18 = 3.841007128357887e-06

Training epoch-58 batch-19
Running loss of epoch-58 batch-19 = 3.922497853636742e-06

Training epoch-58 batch-20
Running loss of epoch-58 batch-20 = 7.226131856441498e-06

Training epoch-58 batch-21
Running loss of epoch-58 batch-21 = 4.8622023314237595e-06

Training epoch-58 batch-22
Running loss of epoch-58 batch-22 = 9.665731340646744e-06

Training epoch-58 batch-23
Running loss of epoch-58 batch-23 = 2.216547727584839e-06

Training epoch-58 batch-24
Running loss of epoch-58 batch-24 = 3.0829105526208878e-06

Training epoch-58 batch-25
Running loss of epoch-58 batch-25 = 1.275213435292244e-06

Training epoch-58 batch-26
Running loss of epoch-58 batch-26 = 4.801200702786446e-06

Training epoch-58 batch-27
Running loss of epoch-58 batch-27 = 5.779089406132698e-06

Training epoch-58 batch-28
Running loss of epoch-58 batch-28 = 3.7106219679117203e-06

Training epoch-58 batch-29
Running loss of epoch-58 batch-29 = 4.4191256165504456e-06

Training epoch-58 batch-30
Running loss of epoch-58 batch-30 = 7.171882316470146e-06

Training epoch-58 batch-31
Running loss of epoch-58 batch-31 = 1.1837109923362732e-05

Training epoch-58 batch-32
Running loss of epoch-58 batch-32 = 6.355810910463333e-06

Training epoch-58 batch-33
Running loss of epoch-58 batch-33 = 8.077127858996391e-06

Training epoch-58 batch-34
Running loss of epoch-58 batch-34 = 4.377681761980057e-06

Training epoch-58 batch-35
Running loss of epoch-58 batch-35 = 2.8908252716064453e-06

Training epoch-58 batch-36
Running loss of epoch-58 batch-36 = 5.448469892144203e-06

Training epoch-58 batch-37
Running loss of epoch-58 batch-37 = 1.053069718182087e-05

Training epoch-58 batch-38
Running loss of epoch-58 batch-38 = 1.9542640075087547e-05

Training epoch-58 batch-39
Running loss of epoch-58 batch-39 = 4.775123670697212e-06

Training epoch-58 batch-40
Running loss of epoch-58 batch-40 = 3.2316893339157104e-06

Training epoch-58 batch-41
Running loss of epoch-58 batch-41 = 6.5248459577560425e-06

Training epoch-58 batch-42
Running loss of epoch-58 batch-42 = 6.135320290923119e-06

Training epoch-58 batch-43
Running loss of epoch-58 batch-43 = 1.834006980061531e-06

Training epoch-58 batch-44
Running loss of epoch-58 batch-44 = 4.951609298586845e-06

Training epoch-58 batch-45
Running loss of epoch-58 batch-45 = 9.909970685839653e-06

Training epoch-58 batch-46
Running loss of epoch-58 batch-46 = 1.0562129318714142e-05

Training epoch-58 batch-47
Running loss of epoch-58 batch-47 = 4.286179319024086e-06

Training epoch-58 batch-48
Running loss of epoch-58 batch-48 = 5.723442882299423e-06

Training epoch-58 batch-49
Running loss of epoch-58 batch-49 = 3.920402377843857e-06

Training epoch-58 batch-50
Running loss of epoch-58 batch-50 = 8.769566193223e-06

Training epoch-58 batch-51
Running loss of epoch-58 batch-51 = 5.124602466821671e-07

Training epoch-58 batch-52
Running loss of epoch-58 batch-52 = 3.348337486386299e-06

Training epoch-58 batch-53
Running loss of epoch-58 batch-53 = 3.0279625207185745e-06

Training epoch-58 batch-54
Running loss of epoch-58 batch-54 = 6.963964551687241e-06

Training epoch-58 batch-55
Running loss of epoch-58 batch-55 = 8.424278348684311e-06

Training epoch-58 batch-56
Running loss of epoch-58 batch-56 = 7.984694093465805e-06

Training epoch-58 batch-57
Running loss of epoch-58 batch-57 = 4.8943329602479935e-06

Training epoch-58 batch-58
Running loss of epoch-58 batch-58 = 4.325294867157936e-06

Training epoch-58 batch-59
Running loss of epoch-58 batch-59 = 6.553018465638161e-06

Training epoch-58 batch-60
Running loss of epoch-58 batch-60 = 6.3211191445589066e-06

Training epoch-58 batch-61
Running loss of epoch-58 batch-61 = 4.959991201758385e-06

Training epoch-58 batch-62
Running loss of epoch-58 batch-62 = 5.288282409310341e-06

Training epoch-58 batch-63
Running loss of epoch-58 batch-63 = 5.213543772697449e-06

Training epoch-58 batch-64
Running loss of epoch-58 batch-64 = 1.837802119553089e-05

Training epoch-58 batch-65
Running loss of epoch-58 batch-65 = 2.600252628326416e-06

Training epoch-58 batch-66
Running loss of epoch-58 batch-66 = 3.0368100851774216e-06

Training epoch-58 batch-67
Running loss of epoch-58 batch-67 = 7.797847501933575e-06

Training epoch-58 batch-68
Running loss of epoch-58 batch-68 = 7.94464722275734e-06

Training epoch-58 batch-69
Running loss of epoch-58 batch-69 = 4.39048744738102e-06

Training epoch-58 batch-70
Running loss of epoch-58 batch-70 = 3.2975804060697556e-06

Training epoch-58 batch-71
Running loss of epoch-58 batch-71 = 2.989545464515686e-06

Training epoch-58 batch-72
Running loss of epoch-58 batch-72 = 2.693384885787964e-06

Training epoch-58 batch-73
Running loss of epoch-58 batch-73 = 3.1115487217903137e-06

Training epoch-58 batch-74
Running loss of epoch-58 batch-74 = 7.1320682764053345e-06

Training epoch-58 batch-75
Running loss of epoch-58 batch-75 = 4.767673090100288e-06

Training epoch-58 batch-76
Running loss of epoch-58 batch-76 = 8.743489161133766e-06

Training epoch-58 batch-77
Running loss of epoch-58 batch-77 = 5.166046321392059e-06

Training epoch-58 batch-78
Running loss of epoch-58 batch-78 = 3.4349504858255386e-06

Training epoch-58 batch-79
Running loss of epoch-58 batch-79 = 4.423549398779869e-06

Training epoch-58 batch-80
Running loss of epoch-58 batch-80 = 6.740214303135872e-06

Training epoch-58 batch-81
Running loss of epoch-58 batch-81 = 3.052176907658577e-06

Training epoch-58 batch-82
Running loss of epoch-58 batch-82 = 5.695037543773651e-06

Training epoch-58 batch-83
Running loss of epoch-58 batch-83 = 3.3203978091478348e-06

Training epoch-58 batch-84
Running loss of epoch-58 batch-84 = 6.1425380408763885e-06

Training epoch-58 batch-85
Running loss of epoch-58 batch-85 = 3.70759516954422e-06

Training epoch-58 batch-86
Running loss of epoch-58 batch-86 = 2.989545464515686e-06

Training epoch-58 batch-87
Running loss of epoch-58 batch-87 = 4.81167808175087e-06

Training epoch-58 batch-88
Running loss of epoch-58 batch-88 = 2.2258609533309937e-06

Training epoch-58 batch-89
Running loss of epoch-58 batch-89 = 3.1364616006612778e-06

Training epoch-58 batch-90
Running loss of epoch-58 batch-90 = 6.178626790642738e-06

Training epoch-58 batch-91
Running loss of epoch-58 batch-91 = 3.323890268802643e-06

Training epoch-58 batch-92
Running loss of epoch-58 batch-92 = 6.449408829212189e-06

Training epoch-58 batch-93
Running loss of epoch-58 batch-93 = 6.085960194468498e-06

Training epoch-58 batch-94
Running loss of epoch-58 batch-94 = 4.198867827653885e-06

Training epoch-58 batch-95
Running loss of epoch-58 batch-95 = 3.5495031625032425e-06

Training epoch-58 batch-96
Running loss of epoch-58 batch-96 = 2.957647666335106e-06

Training epoch-58 batch-97
Running loss of epoch-58 batch-97 = 1.557404175400734e-06

Training epoch-58 batch-98
Running loss of epoch-58 batch-98 = 5.270820111036301e-06

Training epoch-58 batch-99
Running loss of epoch-58 batch-99 = 3.2365787774324417e-06

Training epoch-58 batch-100
Running loss of epoch-58 batch-100 = 3.3993273973464966e-06

Training epoch-58 batch-101
Running loss of epoch-58 batch-101 = 2.9653310775756836e-06

Training epoch-58 batch-102
Running loss of epoch-58 batch-102 = 2.7890782803297043e-06

Training epoch-58 batch-103
Running loss of epoch-58 batch-103 = 2.629123628139496e-06

Training epoch-58 batch-104
Running loss of epoch-58 batch-104 = 6.01448118686676e-06

Training epoch-58 batch-105
Running loss of epoch-58 batch-105 = 5.2549876272678375e-06

Training epoch-58 batch-106
Running loss of epoch-58 batch-106 = 6.240559741854668e-06

Training epoch-58 batch-107
Running loss of epoch-58 batch-107 = 7.043592631816864e-06

Training epoch-58 batch-108
Running loss of epoch-58 batch-108 = 1.8402934074401855e-06

Training epoch-58 batch-109
Running loss of epoch-58 batch-109 = 6.466871127486229e-06

Training epoch-58 batch-110
Running loss of epoch-58 batch-110 = 2.971617504954338e-06

Training epoch-58 batch-111
Running loss of epoch-58 batch-111 = 4.673609510064125e-06

Training epoch-58 batch-112
Running loss of epoch-58 batch-112 = 2.1168962121009827e-06

Training epoch-58 batch-113
Running loss of epoch-58 batch-113 = 4.261033609509468e-06

Training epoch-58 batch-114
Running loss of epoch-58 batch-114 = 2.030283212661743e-06

Training epoch-58 batch-115
Running loss of epoch-58 batch-115 = 1.0202638804912567e-05

Training epoch-58 batch-116
Running loss of epoch-58 batch-116 = 2.4975743144750595e-06

Training epoch-58 batch-117
Running loss of epoch-58 batch-117 = 1.1470634490251541e-05

Training epoch-58 batch-118
Running loss of epoch-58 batch-118 = 3.7034042179584503e-06

Training epoch-58 batch-119
Running loss of epoch-58 batch-119 = 6.966292858123779e-06

Training epoch-58 batch-120
Running loss of epoch-58 batch-120 = 1.131650060415268e-05

Training epoch-58 batch-121
Running loss of epoch-58 batch-121 = 6.224494427442551e-06

Training epoch-58 batch-122
Running loss of epoch-58 batch-122 = 7.173279300332069e-06

Training epoch-58 batch-123
Running loss of epoch-58 batch-123 = 6.132293492555618e-06

Training epoch-58 batch-124
Running loss of epoch-58 batch-124 = 7.585156708955765e-06

Training epoch-58 batch-125
Running loss of epoch-58 batch-125 = 4.764413461089134e-06

Training epoch-58 batch-126
Running loss of epoch-58 batch-126 = 9.220791980624199e-06

Training epoch-58 batch-127
Running loss of epoch-58 batch-127 = 3.289896994829178e-06

Training epoch-58 batch-128
Running loss of epoch-58 batch-128 = 1.7287908121943474e-05

Training epoch-58 batch-129
Running loss of epoch-58 batch-129 = 6.4515043050050735e-06

Training epoch-58 batch-130
Running loss of epoch-58 batch-130 = 4.7853682190179825e-06

Training epoch-58 batch-131
Running loss of epoch-58 batch-131 = 6.400630809366703e-06

Training epoch-58 batch-132
Running loss of epoch-58 batch-132 = 4.644971340894699e-06

Training epoch-58 batch-133
Running loss of epoch-58 batch-133 = 3.912718966603279e-06

Training epoch-58 batch-134
Running loss of epoch-58 batch-134 = 8.61077569425106e-06

Training epoch-58 batch-135
Running loss of epoch-58 batch-135 = 2.9832590371370316e-06

Training epoch-58 batch-136
Running loss of epoch-58 batch-136 = 4.433328285813332e-06

Training epoch-58 batch-137
Running loss of epoch-58 batch-137 = 4.259403795003891e-06

Training epoch-58 batch-138
Running loss of epoch-58 batch-138 = 5.830777809023857e-06

Training epoch-58 batch-139
Running loss of epoch-58 batch-139 = 7.561873644590378e-06

Training epoch-58 batch-140
Running loss of epoch-58 batch-140 = 3.3902470022439957e-06

Training epoch-58 batch-141
Running loss of epoch-58 batch-141 = 6.131595000624657e-06

Training epoch-58 batch-142
Running loss of epoch-58 batch-142 = 8.731381967663765e-06

Training epoch-58 batch-143
Running loss of epoch-58 batch-143 = 3.4978147596120834e-06

Training epoch-58 batch-144
Running loss of epoch-58 batch-144 = 4.573725163936615e-06

Training epoch-58 batch-145
Running loss of epoch-58 batch-145 = 1.6675330698490143e-06

Training epoch-58 batch-146
Running loss of epoch-58 batch-146 = 6.35744072496891e-06

Training epoch-58 batch-147
Running loss of epoch-58 batch-147 = 3.5819830372929573e-06

Training epoch-58 batch-148
Running loss of epoch-58 batch-148 = 3.4298282116651535e-06

Training epoch-58 batch-149
Running loss of epoch-58 batch-149 = 5.872687324881554e-06

Training epoch-58 batch-150
Running loss of epoch-58 batch-150 = 5.619833245873451e-06

Training epoch-58 batch-151
Running loss of epoch-58 batch-151 = 8.351868018507957e-06

Training epoch-58 batch-152
Running loss of epoch-58 batch-152 = 8.010538294911385e-06

Training epoch-58 batch-153
Running loss of epoch-58 batch-153 = 3.307359293103218e-06

Training epoch-58 batch-154
Running loss of epoch-58 batch-154 = 5.833804607391357e-06

Training epoch-58 batch-155
Running loss of epoch-58 batch-155 = 1.3073906302452087e-05

Training epoch-58 batch-156
Running loss of epoch-58 batch-156 = 4.107831045985222e-06

Training epoch-58 batch-157
Running loss of epoch-58 batch-157 = 2.213194966316223e-05

Finished training epoch-58.



Average train loss at epoch-58 = 5.44511154294014e-06

Started Evaluation

Average val loss at epoch-58 = 1.1122483966671792

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.19 %

Finished Evaluation



Started training epoch-59


Training epoch-59 batch-1
Running loss of epoch-59 batch-1 = 5.782349035143852e-06

Training epoch-59 batch-2
Running loss of epoch-59 batch-2 = 3.223307430744171e-06

Training epoch-59 batch-3
Running loss of epoch-59 batch-3 = 5.509005859494209e-06

Training epoch-59 batch-4
Running loss of epoch-59 batch-4 = 4.844041541218758e-06

Training epoch-59 batch-5
Running loss of epoch-59 batch-5 = 4.4798944145441055e-06

Training epoch-59 batch-6
Running loss of epoch-59 batch-6 = 1.278286799788475e-05

Training epoch-59 batch-7
Running loss of epoch-59 batch-7 = 3.3008400350809097e-06

Training epoch-59 batch-8
Running loss of epoch-59 batch-8 = 1.7106067389249802e-06

Training epoch-59 batch-9
Running loss of epoch-59 batch-9 = 5.945097655057907e-06

Training epoch-59 batch-10
Running loss of epoch-59 batch-10 = 5.0407834351062775e-06

Training epoch-59 batch-11
Running loss of epoch-59 batch-11 = 6.295507773756981e-06

Training epoch-59 batch-12
Running loss of epoch-59 batch-12 = 6.101792678236961e-06

Training epoch-59 batch-13
Running loss of epoch-59 batch-13 = 5.801906809210777e-06

Training epoch-59 batch-14
Running loss of epoch-59 batch-14 = 8.519738912582397e-06

Training epoch-59 batch-15
Running loss of epoch-59 batch-15 = 6.93928450345993e-06

Training epoch-59 batch-16
Running loss of epoch-59 batch-16 = 1.2228265404701233e-06

Training epoch-59 batch-17
Running loss of epoch-59 batch-17 = 8.086208254098892e-06

Training epoch-59 batch-18
Running loss of epoch-59 batch-18 = 3.468245267868042e-06

Training epoch-59 batch-19
Running loss of epoch-59 batch-19 = 8.666422218084335e-06

Training epoch-59 batch-20
Running loss of epoch-59 batch-20 = 3.5713892430067062e-06

Training epoch-59 batch-21
Running loss of epoch-59 batch-21 = 4.0156301110982895e-06

Training epoch-59 batch-22
Running loss of epoch-59 batch-22 = 5.142297595739365e-06

Training epoch-59 batch-23
Running loss of epoch-59 batch-23 = 4.8477668315172195e-06

Training epoch-59 batch-24
Running loss of epoch-59 batch-24 = 7.898779585957527e-06

Training epoch-59 batch-25
Running loss of epoch-59 batch-25 = 5.848240107297897e-06

Training epoch-59 batch-26
Running loss of epoch-59 batch-26 = 5.3015537559986115e-06

Training epoch-59 batch-27
Running loss of epoch-59 batch-27 = 2.2335443645715714e-06

Training epoch-59 batch-28
Running loss of epoch-59 batch-28 = 7.118796929717064e-06

Training epoch-59 batch-29
Running loss of epoch-59 batch-29 = 3.6759302020072937e-06

Training epoch-59 batch-30
Running loss of epoch-59 batch-30 = 5.043577402830124e-06

Training epoch-59 batch-31
Running loss of epoch-59 batch-31 = 5.730311386287212e-06

Training epoch-59 batch-32
Running loss of epoch-59 batch-32 = 2.8067734092473984e-06

Training epoch-59 batch-33
Running loss of epoch-59 batch-33 = 2.005835995078087e-06

Training epoch-59 batch-34
Running loss of epoch-59 batch-34 = 6.5122731029987335e-06

Training epoch-59 batch-35
Running loss of epoch-59 batch-35 = 6.92182220518589e-06

Training epoch-59 batch-36
Running loss of epoch-59 batch-36 = 4.361849278211594e-06

Training epoch-59 batch-37
Running loss of epoch-59 batch-37 = 7.560476660728455e-06

Training epoch-59 batch-38
Running loss of epoch-59 batch-38 = 1.653563231229782e-06

Training epoch-59 batch-39
Running loss of epoch-59 batch-39 = 3.777211531996727e-06

Training epoch-59 batch-40
Running loss of epoch-59 batch-40 = 2.7015339583158493e-06

Training epoch-59 batch-41
Running loss of epoch-59 batch-41 = 2.3778993636369705e-06

Training epoch-59 batch-42
Running loss of epoch-59 batch-42 = 4.112254828214645e-06

Training epoch-59 batch-43
Running loss of epoch-59 batch-43 = 2.110609784722328e-06

Training epoch-59 batch-44
Running loss of epoch-59 batch-44 = 5.0247181206941605e-06

Training epoch-59 batch-45
Running loss of epoch-59 batch-45 = 3.705732524394989e-06

Training epoch-59 batch-46
Running loss of epoch-59 batch-46 = 5.3460244089365005e-06

Training epoch-59 batch-47
Running loss of epoch-59 batch-47 = 9.675626643002033e-06

Training epoch-59 batch-48
Running loss of epoch-59 batch-48 = 7.80331902205944e-06

Training epoch-59 batch-49
Running loss of epoch-59 batch-49 = 5.992129445075989e-06

Training epoch-59 batch-50
Running loss of epoch-59 batch-50 = 2.6812776923179626e-06

Training epoch-59 batch-51
Running loss of epoch-59 batch-51 = 1.239171251654625e-05

Training epoch-59 batch-52
Running loss of epoch-59 batch-52 = 4.472211003303528e-06

Training epoch-59 batch-53
Running loss of epoch-59 batch-53 = 4.7371722757816315e-06

Training epoch-59 batch-54
Running loss of epoch-59 batch-54 = 6.533227860927582e-06

Training epoch-59 batch-55
Running loss of epoch-59 batch-55 = 2.956017851829529e-06

Training epoch-59 batch-56
Running loss of epoch-59 batch-56 = 3.334134817123413e-06

Training epoch-59 batch-57
Running loss of epoch-59 batch-57 = 3.916211426258087e-06

Training epoch-59 batch-58
Running loss of epoch-59 batch-58 = 1.5500932931900024e-05

Training epoch-59 batch-59
Running loss of epoch-59 batch-59 = 1.8980354070663452e-06

Training epoch-59 batch-60
Running loss of epoch-59 batch-60 = 2.8049107640981674e-06

Training epoch-59 batch-61
Running loss of epoch-59 batch-61 = 2.729007974267006e-06

Training epoch-59 batch-62
Running loss of epoch-59 batch-62 = 1.0214047506451607e-05

Training epoch-59 batch-63
Running loss of epoch-59 batch-63 = 7.456867024302483e-06

Training epoch-59 batch-64
Running loss of epoch-59 batch-64 = 6.381189450621605e-06

Training epoch-59 batch-65
Running loss of epoch-59 batch-65 = 3.5087577998638153e-06

Training epoch-59 batch-66
Running loss of epoch-59 batch-66 = 3.814464434981346e-06

Training epoch-59 batch-67
Running loss of epoch-59 batch-67 = 7.716706022620201e-06

Training epoch-59 batch-68
Running loss of epoch-59 batch-68 = 3.79304401576519e-06

Training epoch-59 batch-69
Running loss of epoch-59 batch-69 = 2.761371433734894e-06

Training epoch-59 batch-70
Running loss of epoch-59 batch-70 = 6.024027243256569e-06

Training epoch-59 batch-71
Running loss of epoch-59 batch-71 = 3.2780226320028305e-06

Training epoch-59 batch-72
Running loss of epoch-59 batch-72 = 5.106208845973015e-06

Training epoch-59 batch-73
Running loss of epoch-59 batch-73 = 5.029374733567238e-06

Training epoch-59 batch-74
Running loss of epoch-59 batch-74 = 9.474111720919609e-06

Training epoch-59 batch-75
Running loss of epoch-59 batch-75 = 3.6512501537799835e-06

Training epoch-59 batch-76
Running loss of epoch-59 batch-76 = 4.112953320145607e-06

Training epoch-59 batch-77
Running loss of epoch-59 batch-77 = 5.813548341393471e-06

Training epoch-59 batch-78
Running loss of epoch-59 batch-78 = 5.117151886224747e-06

Training epoch-59 batch-79
Running loss of epoch-59 batch-79 = 1.0448507964611053e-05

Training epoch-59 batch-80
Running loss of epoch-59 batch-80 = 9.773531928658485e-06

Training epoch-59 batch-81
Running loss of epoch-59 batch-81 = 3.7313438951969147e-06

Training epoch-59 batch-82
Running loss of epoch-59 batch-82 = 4.6886270865798e-06

Training epoch-59 batch-83
Running loss of epoch-59 batch-83 = 4.515284672379494e-06

Training epoch-59 batch-84
Running loss of epoch-59 batch-84 = 3.2603275030851364e-06

Training epoch-59 batch-85
Running loss of epoch-59 batch-85 = 1.1936062946915627e-05

Training epoch-59 batch-86
Running loss of epoch-59 batch-86 = 6.216345354914665e-06

Training epoch-59 batch-87
Running loss of epoch-59 batch-87 = 7.698778063058853e-06

Training epoch-59 batch-88
Running loss of epoch-59 batch-88 = 2.5855842977762222e-06

Training epoch-59 batch-89
Running loss of epoch-59 batch-89 = 5.795154720544815e-06

Training epoch-59 batch-90
Running loss of epoch-59 batch-90 = 8.316943421959877e-06

Training epoch-59 batch-91
Running loss of epoch-59 batch-91 = 6.167450919747353e-06

Training epoch-59 batch-92
Running loss of epoch-59 batch-92 = 2.0854640752077103e-06

Training epoch-59 batch-93
Running loss of epoch-59 batch-93 = 6.360001862049103e-06

Training epoch-59 batch-94
Running loss of epoch-59 batch-94 = 4.371395334601402e-06

Training epoch-59 batch-95
Running loss of epoch-59 batch-95 = 5.624489858746529e-06

Training epoch-59 batch-96
Running loss of epoch-59 batch-96 = 2.009584568440914e-05

Training epoch-59 batch-97
Running loss of epoch-59 batch-97 = 9.329523891210556e-06

Training epoch-59 batch-98
Running loss of epoch-59 batch-98 = 5.881302058696747e-06

Training epoch-59 batch-99
Running loss of epoch-59 batch-99 = 5.4461415857076645e-06

Training epoch-59 batch-100
Running loss of epoch-59 batch-100 = 5.255686119198799e-06

Training epoch-59 batch-101
Running loss of epoch-59 batch-101 = 5.5175041779875755e-06

Training epoch-59 batch-102
Running loss of epoch-59 batch-102 = 2.2188760340213776e-06

Training epoch-59 batch-103
Running loss of epoch-59 batch-103 = 4.152301698923111e-06

Training epoch-59 batch-104
Running loss of epoch-59 batch-104 = 5.27920201420784e-06

Training epoch-59 batch-105
Running loss of epoch-59 batch-105 = 4.741828888654709e-06

Training epoch-59 batch-106
Running loss of epoch-59 batch-106 = 7.139751687645912e-06

Training epoch-59 batch-107
Running loss of epoch-59 batch-107 = 2.817949280142784e-06

Training epoch-59 batch-108
Running loss of epoch-59 batch-108 = 2.3813918232917786e-06

Training epoch-59 batch-109
Running loss of epoch-59 batch-109 = 4.425877705216408e-06

Training epoch-59 batch-110
Running loss of epoch-59 batch-110 = 5.866633728146553e-06

Training epoch-59 batch-111
Running loss of epoch-59 batch-111 = 7.668975740671158e-06

Training epoch-59 batch-112
Running loss of epoch-59 batch-112 = 6.395857781171799e-06

Training epoch-59 batch-113
Running loss of epoch-59 batch-113 = 4.354165866971016e-06

Training epoch-59 batch-114
Running loss of epoch-59 batch-114 = 4.365108907222748e-06

Training epoch-59 batch-115
Running loss of epoch-59 batch-115 = 1.3273907825350761e-05

Training epoch-59 batch-116
Running loss of epoch-59 batch-116 = 8.388189598917961e-06

Training epoch-59 batch-117
Running loss of epoch-59 batch-117 = 7.299240678548813e-07

Training epoch-59 batch-118
Running loss of epoch-59 batch-118 = 3.3543910831212997e-06

Training epoch-59 batch-119
Running loss of epoch-59 batch-119 = 3.0028168112039566e-06

Training epoch-59 batch-120
Running loss of epoch-59 batch-120 = 8.077826350927353e-06

Training epoch-59 batch-121
Running loss of epoch-59 batch-121 = 5.659181624650955e-06

Training epoch-59 batch-122
Running loss of epoch-59 batch-122 = 4.098052158951759e-06

Training epoch-59 batch-123
Running loss of epoch-59 batch-123 = 4.711095243692398e-06

Training epoch-59 batch-124
Running loss of epoch-59 batch-124 = 3.7446152418851852e-06

Training epoch-59 batch-125
Running loss of epoch-59 batch-125 = 5.536479875445366e-06

Training epoch-59 batch-126
Running loss of epoch-59 batch-126 = 8.771196007728577e-06

Training epoch-59 batch-127
Running loss of epoch-59 batch-127 = 6.220769137144089e-06

Training epoch-59 batch-128
Running loss of epoch-59 batch-128 = 9.033596143126488e-06

Training epoch-59 batch-129
Running loss of epoch-59 batch-129 = 2.5620684027671814e-06

Training epoch-59 batch-130
Running loss of epoch-59 batch-130 = 3.926455974578857e-06

Training epoch-59 batch-131
Running loss of epoch-59 batch-131 = 3.7660356611013412e-06

Training epoch-59 batch-132
Running loss of epoch-59 batch-132 = 5.875946953892708e-06

Training epoch-59 batch-133
Running loss of epoch-59 batch-133 = 4.736706614494324e-06

Training epoch-59 batch-134
Running loss of epoch-59 batch-134 = 2.2870954126119614e-06

Training epoch-59 batch-135
Running loss of epoch-59 batch-135 = 3.4514814615249634e-06

Training epoch-59 batch-136
Running loss of epoch-59 batch-136 = 3.682216629385948e-06

Training epoch-59 batch-137
Running loss of epoch-59 batch-137 = 6.4587220549583435e-06

Training epoch-59 batch-138
Running loss of epoch-59 batch-138 = 2.316432073712349e-06

Training epoch-59 batch-139
Running loss of epoch-59 batch-139 = 4.050089046359062e-06

Training epoch-59 batch-140
Running loss of epoch-59 batch-140 = 3.922265022993088e-06

Training epoch-59 batch-141
Running loss of epoch-59 batch-141 = 5.526700988411903e-06

Training epoch-59 batch-142
Running loss of epoch-59 batch-142 = 2.993270754814148e-06

Training epoch-59 batch-143
Running loss of epoch-59 batch-143 = 4.571862518787384e-06

Training epoch-59 batch-144
Running loss of epoch-59 batch-144 = 5.506677553057671e-06

Training epoch-59 batch-145
Running loss of epoch-59 batch-145 = 3.6712735891342163e-06

Training epoch-59 batch-146
Running loss of epoch-59 batch-146 = 1.5583354979753494e-06

Training epoch-59 batch-147
Running loss of epoch-59 batch-147 = 7.2766561061143875e-06

Training epoch-59 batch-148
Running loss of epoch-59 batch-148 = 4.026107490062714e-06

Training epoch-59 batch-149
Running loss of epoch-59 batch-149 = 1.3213139027357101e-06

Training epoch-59 batch-150
Running loss of epoch-59 batch-150 = 1.7236452549695969e-06

Training epoch-59 batch-151
Running loss of epoch-59 batch-151 = 3.0959490686655045e-06

Training epoch-59 batch-152
Running loss of epoch-59 batch-152 = 2.153683453798294e-06

Training epoch-59 batch-153
Running loss of epoch-59 batch-153 = 2.7276109904050827e-06

Training epoch-59 batch-154
Running loss of epoch-59 batch-154 = 4.66848723590374e-06

Training epoch-59 batch-155
Running loss of epoch-59 batch-155 = 5.763256922364235e-06

Training epoch-59 batch-156
Running loss of epoch-59 batch-156 = 2.5352928787469864e-06

Training epoch-59 batch-157
Running loss of epoch-59 batch-157 = 2.419203519821167e-05

Finished training epoch-59.



Average train loss at epoch-59 = 5.270534753799438e-06

Started Evaluation

Average val loss at epoch-59 = 1.1200078560083824

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.49 %
Accuracy for class onCreate is: 94.03 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 83.19 %

Finished Evaluation



Started training epoch-60


Training epoch-60 batch-1
Running loss of epoch-60 batch-1 = 5.2317045629024506e-06

Training epoch-60 batch-2
Running loss of epoch-60 batch-2 = 4.91156242787838e-06

Training epoch-60 batch-3
Running loss of epoch-60 batch-3 = 5.753012374043465e-06

Training epoch-60 batch-4
Running loss of epoch-60 batch-4 = 4.8978254199028015e-06

Training epoch-60 batch-5
Running loss of epoch-60 batch-5 = 3.698514774441719e-06

Training epoch-60 batch-6
Running loss of epoch-60 batch-6 = 1.0882038623094559e-05

Training epoch-60 batch-7
Running loss of epoch-60 batch-7 = 5.40376640856266e-06

Training epoch-60 batch-8
Running loss of epoch-60 batch-8 = 6.891554221510887e-06

Training epoch-60 batch-9
Running loss of epoch-60 batch-9 = 4.530884325504303e-06

Training epoch-60 batch-10
Running loss of epoch-60 batch-10 = 5.008885636925697e-06

Training epoch-60 batch-11
Running loss of epoch-60 batch-11 = 2.8742942959070206e-06

Training epoch-60 batch-12
Running loss of epoch-60 batch-12 = 5.728099495172501e-06

Training epoch-60 batch-13
Running loss of epoch-60 batch-13 = 1.553678885102272e-06

Training epoch-60 batch-14
Running loss of epoch-60 batch-14 = 3.621680662035942e-06

Training epoch-60 batch-15
Running loss of epoch-60 batch-15 = 3.5150442272424698e-06

Training epoch-60 batch-16
Running loss of epoch-60 batch-16 = 2.661021426320076e-06

Training epoch-60 batch-17
Running loss of epoch-60 batch-17 = 2.6866327971220016e-06

Training epoch-60 batch-18
Running loss of epoch-60 batch-18 = 4.449859261512756e-06

Training epoch-60 batch-19
Running loss of epoch-60 batch-19 = 6.5390486270189285e-06

Training epoch-60 batch-20
Running loss of epoch-60 batch-20 = 7.882248610258102e-06

Training epoch-60 batch-21
Running loss of epoch-60 batch-21 = 3.930181264877319e-06

Training epoch-60 batch-22
Running loss of epoch-60 batch-22 = 6.24009408056736e-06

Training epoch-60 batch-23
Running loss of epoch-60 batch-23 = 3.748573362827301e-06

Training epoch-60 batch-24
Running loss of epoch-60 batch-24 = 5.546491593122482e-06

Training epoch-60 batch-25
Running loss of epoch-60 batch-25 = 1.2973323464393616e-06

Training epoch-60 batch-26
Running loss of epoch-60 batch-26 = 6.822170689702034e-06

Training epoch-60 batch-27
Running loss of epoch-60 batch-27 = 5.477108061313629e-06

Training epoch-60 batch-28
Running loss of epoch-60 batch-28 = 1.5099067240953445e-06

Training epoch-60 batch-29
Running loss of epoch-60 batch-29 = 2.7925707399845123e-06

Training epoch-60 batch-30
Running loss of epoch-60 batch-30 = 4.731584340333939e-06

Training epoch-60 batch-31
Running loss of epoch-60 batch-31 = 2.8514768928289413e-06

Training epoch-60 batch-32
Running loss of epoch-60 batch-32 = 4.688510671257973e-06

Training epoch-60 batch-33
Running loss of epoch-60 batch-33 = 4.2174942791461945e-06

Training epoch-60 batch-34
Running loss of epoch-60 batch-34 = 9.60356555879116e-06

Training epoch-60 batch-35
Running loss of epoch-60 batch-35 = 4.476634785532951e-06

Training epoch-60 batch-36
Running loss of epoch-60 batch-36 = 5.505513399839401e-06

Training epoch-60 batch-37
Running loss of epoch-60 batch-37 = 4.023779183626175e-06

Training epoch-60 batch-38
Running loss of epoch-60 batch-38 = 6.909715011715889e-06

Training epoch-60 batch-39
Running loss of epoch-60 batch-39 = 3.304099664092064e-06

Training epoch-60 batch-40
Running loss of epoch-60 batch-40 = 1.3783574104309082e-06

Training epoch-60 batch-41
Running loss of epoch-60 batch-41 = 6.511108949780464e-06

Training epoch-60 batch-42
Running loss of epoch-60 batch-42 = 8.132308721542358e-06

Training epoch-60 batch-43
Running loss of epoch-60 batch-43 = 4.955334588885307e-06

Training epoch-60 batch-44
Running loss of epoch-60 batch-44 = 3.073597326874733e-06

Training epoch-60 batch-45
Running loss of epoch-60 batch-45 = 3.3564865589141846e-06

Training epoch-60 batch-46
Running loss of epoch-60 batch-46 = 9.388430044054985e-06

Training epoch-60 batch-47
Running loss of epoch-60 batch-47 = 6.856396794319153e-06

Training epoch-60 batch-48
Running loss of epoch-60 batch-48 = 9.151408448815346e-06

Training epoch-60 batch-49
Running loss of epoch-60 batch-49 = 6.44288957118988e-06

Training epoch-60 batch-50
Running loss of epoch-60 batch-50 = 9.864335879683495e-06

Training epoch-60 batch-51
Running loss of epoch-60 batch-51 = 6.032641977071762e-06

Training epoch-60 batch-52
Running loss of epoch-60 batch-52 = 1.1890195310115814e-05

Training epoch-60 batch-53
Running loss of epoch-60 batch-53 = 3.987690433859825e-06

Training epoch-60 batch-54
Running loss of epoch-60 batch-54 = 1.0426389053463936e-05

Training epoch-60 batch-55
Running loss of epoch-60 batch-55 = 1.0529998689889908e-05

Training epoch-60 batch-56
Running loss of epoch-60 batch-56 = 5.159527063369751e-06

Training epoch-60 batch-57
Running loss of epoch-60 batch-57 = 1.0800780728459358e-05

Training epoch-60 batch-58
Running loss of epoch-60 batch-58 = 4.858942702412605e-06

Training epoch-60 batch-59
Running loss of epoch-60 batch-59 = 3.3620744943618774e-06

Training epoch-60 batch-60
Running loss of epoch-60 batch-60 = 3.364868462085724e-06

Training epoch-60 batch-61
Running loss of epoch-60 batch-61 = 4.159985110163689e-06

Training epoch-60 batch-62
Running loss of epoch-60 batch-62 = 1.5883706510066986e-06

Training epoch-60 batch-63
Running loss of epoch-60 batch-63 = 1.2421049177646637e-05

Training epoch-60 batch-64
Running loss of epoch-60 batch-64 = 4.757195711135864e-06

Training epoch-60 batch-65
Running loss of epoch-60 batch-65 = 6.200745701789856e-06

Training epoch-60 batch-66
Running loss of epoch-60 batch-66 = 4.717148840427399e-06

Training epoch-60 batch-67
Running loss of epoch-60 batch-67 = 2.5567132979631424e-06

Training epoch-60 batch-68
Running loss of epoch-60 batch-68 = 4.7031790018081665e-06

Training epoch-60 batch-69
Running loss of epoch-60 batch-69 = 7.149530574679375e-06

Training epoch-60 batch-70
Running loss of epoch-60 batch-70 = 5.044508725404739e-06

Training epoch-60 batch-71
Running loss of epoch-60 batch-71 = 5.314359441399574e-06

Training epoch-60 batch-72
Running loss of epoch-60 batch-72 = 4.674075171351433e-06

Training epoch-60 batch-73
Running loss of epoch-60 batch-73 = 7.559079676866531e-06

Training epoch-60 batch-74
Running loss of epoch-60 batch-74 = 5.482928827404976e-06

Training epoch-60 batch-75
Running loss of epoch-60 batch-75 = 7.206806913018227e-06

Training epoch-60 batch-76
Running loss of epoch-60 batch-76 = 3.050081431865692e-06

Training epoch-60 batch-77
Running loss of epoch-60 batch-77 = 5.4354313760995865e-06

Training epoch-60 batch-78
Running loss of epoch-60 batch-78 = 4.011671990156174e-06

Training epoch-60 batch-79
Running loss of epoch-60 batch-79 = 5.24008646607399e-06

Training epoch-60 batch-80
Running loss of epoch-60 batch-80 = 1.168111339211464e-06

Training epoch-60 batch-81
Running loss of epoch-60 batch-81 = 6.482703611254692e-06

Training epoch-60 batch-82
Running loss of epoch-60 batch-82 = 3.2791867852211e-06

Training epoch-60 batch-83
Running loss of epoch-60 batch-83 = 6.274553015828133e-06

Training epoch-60 batch-84
Running loss of epoch-60 batch-84 = 6.654765456914902e-06

Training epoch-60 batch-85
Running loss of epoch-60 batch-85 = 2.2065360099077225e-06

Training epoch-60 batch-86
Running loss of epoch-60 batch-86 = 5.086534656584263e-06

Training epoch-60 batch-87
Running loss of epoch-60 batch-87 = 1.285807229578495e-05

Training epoch-60 batch-88
Running loss of epoch-60 batch-88 = 3.689434379339218e-06

Training epoch-60 batch-89
Running loss of epoch-60 batch-89 = 6.351154297590256e-06

Training epoch-60 batch-90
Running loss of epoch-60 batch-90 = 3.4205149859189987e-06

Training epoch-60 batch-91
Running loss of epoch-60 batch-91 = 7.879920303821564e-06

Training epoch-60 batch-92
Running loss of epoch-60 batch-92 = 1.4579854905605316e-06

Training epoch-60 batch-93
Running loss of epoch-60 batch-93 = 5.949288606643677e-06

Training epoch-60 batch-94
Running loss of epoch-60 batch-94 = 5.170702934265137e-06

Training epoch-60 batch-95
Running loss of epoch-60 batch-95 = 4.090135917067528e-06

Training epoch-60 batch-96
Running loss of epoch-60 batch-96 = 7.107388228178024e-06

Training epoch-60 batch-97
Running loss of epoch-60 batch-97 = 9.983312338590622e-06

Training epoch-60 batch-98
Running loss of epoch-60 batch-98 = 1.509813591837883e-05

Training epoch-60 batch-99
Running loss of epoch-60 batch-99 = 9.716255590319633e-06

Training epoch-60 batch-100
Running loss of epoch-60 batch-100 = 2.900604158639908e-06

Training epoch-60 batch-101
Running loss of epoch-60 batch-101 = 3.1869858503341675e-06

Training epoch-60 batch-102
Running loss of epoch-60 batch-102 = 1.3923272490501404e-06

Training epoch-60 batch-103
Running loss of epoch-60 batch-103 = 3.643566742539406e-06

Training epoch-60 batch-104
Running loss of epoch-60 batch-104 = 8.148374035954475e-06

Training epoch-60 batch-105
Running loss of epoch-60 batch-105 = 8.631730452179909e-06

Training epoch-60 batch-106
Running loss of epoch-60 batch-106 = 3.09688039124012e-06

Training epoch-60 batch-107
Running loss of epoch-60 batch-107 = 8.0897007137537e-06

Training epoch-60 batch-108
Running loss of epoch-60 batch-108 = 7.785391062498093e-06

Training epoch-60 batch-109
Running loss of epoch-60 batch-109 = 3.5653356462717056e-06

Training epoch-60 batch-110
Running loss of epoch-60 batch-110 = 4.405621439218521e-06

Training epoch-60 batch-111
Running loss of epoch-60 batch-111 = 6.029382348060608e-06

Training epoch-60 batch-112
Running loss of epoch-60 batch-112 = 3.264518454670906e-06

Training epoch-60 batch-113
Running loss of epoch-60 batch-113 = 2.9013026505708694e-06

Training epoch-60 batch-114
Running loss of epoch-60 batch-114 = 3.63960862159729e-06

Training epoch-60 batch-115
Running loss of epoch-60 batch-115 = 5.1192473620176315e-06

Training epoch-60 batch-116
Running loss of epoch-60 batch-116 = 1.1818483471870422e-06

Training epoch-60 batch-117
Running loss of epoch-60 batch-117 = 3.637978807091713e-06

Training epoch-60 batch-118
Running loss of epoch-60 batch-118 = 5.718786269426346e-06

Training epoch-60 batch-119
Running loss of epoch-60 batch-119 = 2.752058207988739e-06

Training epoch-60 batch-120
Running loss of epoch-60 batch-120 = 3.2139942049980164e-06

Training epoch-60 batch-121
Running loss of epoch-60 batch-121 = 6.428686901926994e-06

Training epoch-60 batch-122
Running loss of epoch-60 batch-122 = 6.650807335972786e-06

Training epoch-60 batch-123
Running loss of epoch-60 batch-123 = 4.346249625086784e-06

Training epoch-60 batch-124
Running loss of epoch-60 batch-124 = 2.5744084268808365e-06

Training epoch-60 batch-125
Running loss of epoch-60 batch-125 = 1.946697011590004e-06

Training epoch-60 batch-126
Running loss of epoch-60 batch-126 = 4.492700099945068e-06

Training epoch-60 batch-127
Running loss of epoch-60 batch-127 = 6.1749015003442764e-06

Training epoch-60 batch-128
Running loss of epoch-60 batch-128 = 8.384231477975845e-07

Training epoch-60 batch-129
Running loss of epoch-60 batch-129 = 2.044485881924629e-06

Training epoch-60 batch-130
Running loss of epoch-60 batch-130 = 5.331356078386307e-06

Training epoch-60 batch-131
Running loss of epoch-60 batch-131 = 2.1262094378471375e-06

Training epoch-60 batch-132
Running loss of epoch-60 batch-132 = 3.4370459616184235e-06

Training epoch-60 batch-133
Running loss of epoch-60 batch-133 = 5.834503099322319e-06

Training epoch-60 batch-134
Running loss of epoch-60 batch-134 = 5.457783117890358e-06

Training epoch-60 batch-135
Running loss of epoch-60 batch-135 = 4.012370482087135e-06

Training epoch-60 batch-136
Running loss of epoch-60 batch-136 = 4.176981747150421e-06

Training epoch-60 batch-137
Running loss of epoch-60 batch-137 = 9.622890502214432e-07

Training epoch-60 batch-138
Running loss of epoch-60 batch-138 = 4.063593223690987e-06

Training epoch-60 batch-139
Running loss of epoch-60 batch-139 = 4.063127562403679e-06

Training epoch-60 batch-140
Running loss of epoch-60 batch-140 = 8.242670446634293e-06

Training epoch-60 batch-141
Running loss of epoch-60 batch-141 = 1.6170088201761246e-06

Training epoch-60 batch-142
Running loss of epoch-60 batch-142 = 5.0086528062820435e-06

Training epoch-60 batch-143
Running loss of epoch-60 batch-143 = 8.186791092157364e-06

Training epoch-60 batch-144
Running loss of epoch-60 batch-144 = 4.526926204562187e-06

Training epoch-60 batch-145
Running loss of epoch-60 batch-145 = 4.403991624712944e-06

Training epoch-60 batch-146
Running loss of epoch-60 batch-146 = 6.4445193856954575e-06

Training epoch-60 batch-147
Running loss of epoch-60 batch-147 = 5.349051207304001e-06

Training epoch-60 batch-148
Running loss of epoch-60 batch-148 = 3.3723190426826477e-06

Training epoch-60 batch-149
Running loss of epoch-60 batch-149 = 8.309725672006607e-06

Training epoch-60 batch-150
Running loss of epoch-60 batch-150 = 5.386536940932274e-06

Training epoch-60 batch-151
Running loss of epoch-60 batch-151 = 4.403525963425636e-06

Training epoch-60 batch-152
Running loss of epoch-60 batch-152 = 6.712740287184715e-06

Training epoch-60 batch-153
Running loss of epoch-60 batch-153 = 5.65825030207634e-06

Training epoch-60 batch-154
Running loss of epoch-60 batch-154 = 9.576324373483658e-07

Training epoch-60 batch-155
Running loss of epoch-60 batch-155 = 1.803738996386528e-06

Training epoch-60 batch-156
Running loss of epoch-60 batch-156 = 3.369757905602455e-06

Training epoch-60 batch-157
Running loss of epoch-60 batch-157 = 6.120651960372925e-06

Finished training epoch-60.



Average train loss at epoch-60 = 5.143497139215469e-06

Started Evaluation

Average val loss at epoch-60 = 1.1220343618267634

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 63.24 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.19 %

Finished Evaluation



Started training epoch-61


Training epoch-61 batch-1
Running loss of epoch-61 batch-1 = 6.043585017323494e-06

Training epoch-61 batch-2
Running loss of epoch-61 batch-2 = 4.359753802418709e-06

Training epoch-61 batch-3
Running loss of epoch-61 batch-3 = 1.5455298125743866e-06

Training epoch-61 batch-4
Running loss of epoch-61 batch-4 = 1.9744038581848145e-06

Training epoch-61 batch-5
Running loss of epoch-61 batch-5 = 9.552109986543655e-06

Training epoch-61 batch-6
Running loss of epoch-61 batch-6 = 6.445217877626419e-06

Training epoch-61 batch-7
Running loss of epoch-61 batch-7 = 5.231006070971489e-06

Training epoch-61 batch-8
Running loss of epoch-61 batch-8 = 1.1789146810770035e-05

Training epoch-61 batch-9
Running loss of epoch-61 batch-9 = 1.1489959433674812e-05

Training epoch-61 batch-10
Running loss of epoch-61 batch-10 = 4.623318091034889e-06

Training epoch-61 batch-11
Running loss of epoch-61 batch-11 = 4.5907218009233475e-06

Training epoch-61 batch-12
Running loss of epoch-61 batch-12 = 4.613539204001427e-06

Training epoch-61 batch-13
Running loss of epoch-61 batch-13 = 6.649643182754517e-06

Training epoch-61 batch-14
Running loss of epoch-61 batch-14 = 2.0477455109357834e-06

Training epoch-61 batch-15
Running loss of epoch-61 batch-15 = 6.0086604207754135e-06

Training epoch-61 batch-16
Running loss of epoch-61 batch-16 = 5.275476723909378e-06

Training epoch-61 batch-17
Running loss of epoch-61 batch-17 = 2.164626494050026e-06

Training epoch-61 batch-18
Running loss of epoch-61 batch-18 = 5.376292392611504e-06

Training epoch-61 batch-19
Running loss of epoch-61 batch-19 = 3.1287781894207e-06

Training epoch-61 batch-20
Running loss of epoch-61 batch-20 = 5.0480011850595474e-06

Training epoch-61 batch-21
Running loss of epoch-61 batch-21 = 3.4561380743980408e-06

Training epoch-61 batch-22
Running loss of epoch-61 batch-22 = 1.5739351511001587e-06

Training epoch-61 batch-23
Running loss of epoch-61 batch-23 = 2.161134034395218e-06

Training epoch-61 batch-24
Running loss of epoch-61 batch-24 = 7.813796401023865e-06

Training epoch-61 batch-25
Running loss of epoch-61 batch-25 = 3.5997945815324783e-06

Training epoch-61 batch-26
Running loss of epoch-61 batch-26 = 5.121808499097824e-06

Training epoch-61 batch-27
Running loss of epoch-61 batch-27 = 2.09687277674675e-06

Training epoch-61 batch-28
Running loss of epoch-61 batch-28 = 3.6386772990226746e-06

Training epoch-61 batch-29
Running loss of epoch-61 batch-29 = 4.059169441461563e-06

Training epoch-61 batch-30
Running loss of epoch-61 batch-30 = 7.664551958441734e-06

Training epoch-61 batch-31
Running loss of epoch-61 batch-31 = 5.745328962802887e-06

Training epoch-61 batch-32
Running loss of epoch-61 batch-32 = 6.760936230421066e-06

Training epoch-61 batch-33
Running loss of epoch-61 batch-33 = 3.934837877750397e-06

Training epoch-61 batch-34
Running loss of epoch-61 batch-34 = 6.091315299272537e-06

Training epoch-61 batch-35
Running loss of epoch-61 batch-35 = 5.292007699608803e-06

Training epoch-61 batch-36
Running loss of epoch-61 batch-36 = 9.387731552124023e-06

Training epoch-61 batch-37
Running loss of epoch-61 batch-37 = 3.7851277738809586e-06

Training epoch-61 batch-38
Running loss of epoch-61 batch-38 = 1.9953586161136627e-06

Training epoch-61 batch-39
Running loss of epoch-61 batch-39 = 6.524962373077869e-06

Training epoch-61 batch-40
Running loss of epoch-61 batch-40 = 5.133915692567825e-06

Training epoch-61 batch-41
Running loss of epoch-61 batch-41 = 8.893664926290512e-06

Training epoch-61 batch-42
Running loss of epoch-61 batch-42 = 9.439187124371529e-06

Training epoch-61 batch-43
Running loss of epoch-61 batch-43 = 7.682479918003082e-06

Training epoch-61 batch-44
Running loss of epoch-61 batch-44 = 4.063360393047333e-06

Training epoch-61 batch-45
Running loss of epoch-61 batch-45 = 4.6510249376297e-06

Training epoch-61 batch-46
Running loss of epoch-61 batch-46 = 2.591637894511223e-06

Training epoch-61 batch-47
Running loss of epoch-61 batch-47 = 4.326924681663513e-06

Training epoch-61 batch-48
Running loss of epoch-61 batch-48 = 4.251021891832352e-06

Training epoch-61 batch-49
Running loss of epoch-61 batch-49 = 5.2300747483968735e-06

Training epoch-61 batch-50
Running loss of epoch-61 batch-50 = 1.8512364476919174e-06

Training epoch-61 batch-51
Running loss of epoch-61 batch-51 = 6.139744073152542e-06

Training epoch-61 batch-52
Running loss of epoch-61 batch-52 = 2.067768946290016e-06

Training epoch-61 batch-53
Running loss of epoch-61 batch-53 = 6.232643499970436e-06

Training epoch-61 batch-54
Running loss of epoch-61 batch-54 = 3.9513688534498215e-06

Training epoch-61 batch-55
Running loss of epoch-61 batch-55 = 3.584194928407669e-06

Training epoch-61 batch-56
Running loss of epoch-61 batch-56 = 5.843117833137512e-06

Training epoch-61 batch-57
Running loss of epoch-61 batch-57 = 3.077322617173195e-06

Training epoch-61 batch-58
Running loss of epoch-61 batch-58 = 3.1301751732826233e-06

Training epoch-61 batch-59
Running loss of epoch-61 batch-59 = 5.235197022557259e-06

Training epoch-61 batch-60
Running loss of epoch-61 batch-60 = 4.6032946556806564e-06

Training epoch-61 batch-61
Running loss of epoch-61 batch-61 = 2.6957131922245026e-06

Training epoch-61 batch-62
Running loss of epoch-61 batch-62 = 1.6365665942430496e-06

Training epoch-61 batch-63
Running loss of epoch-61 batch-63 = 5.973735824227333e-06

Training epoch-61 batch-64
Running loss of epoch-61 batch-64 = 7.1928370743989944e-06

Training epoch-61 batch-65
Running loss of epoch-61 batch-65 = 4.761619493365288e-06

Training epoch-61 batch-66
Running loss of epoch-61 batch-66 = 5.709938704967499e-06

Training epoch-61 batch-67
Running loss of epoch-61 batch-67 = 1.960666850209236e-06

Training epoch-61 batch-68
Running loss of epoch-61 batch-68 = 6.070360541343689e-06

Training epoch-61 batch-69
Running loss of epoch-61 batch-69 = 7.791444659233093e-06

Training epoch-61 batch-70
Running loss of epoch-61 batch-70 = 4.097120836377144e-06

Training epoch-61 batch-71
Running loss of epoch-61 batch-71 = 3.8547441363334656e-06

Training epoch-61 batch-72
Running loss of epoch-61 batch-72 = 4.979083314538002e-06

Training epoch-61 batch-73
Running loss of epoch-61 batch-73 = 5.282927304506302e-06

Training epoch-61 batch-74
Running loss of epoch-61 batch-74 = 3.0605588108301163e-06

Training epoch-61 batch-75
Running loss of epoch-61 batch-75 = 3.7727877497673035e-06

Training epoch-61 batch-76
Running loss of epoch-61 batch-76 = 1.200009137392044e-06

Training epoch-61 batch-77
Running loss of epoch-61 batch-77 = 1.1345837265253067e-06

Training epoch-61 batch-78
Running loss of epoch-61 batch-78 = 3.1047966331243515e-06

Training epoch-61 batch-79
Running loss of epoch-61 batch-79 = 5.3085386753082275e-06

Training epoch-61 batch-80
Running loss of epoch-61 batch-80 = 4.390254616737366e-06

Training epoch-61 batch-81
Running loss of epoch-61 batch-81 = 2.5418121367692947e-06

Training epoch-61 batch-82
Running loss of epoch-61 batch-82 = 5.00422902405262e-06

Training epoch-61 batch-83
Running loss of epoch-61 batch-83 = 2.3774337023496628e-06

Training epoch-61 batch-84
Running loss of epoch-61 batch-84 = 1.8135178834199905e-06

Training epoch-61 batch-85
Running loss of epoch-61 batch-85 = 2.5830231606960297e-06

Training epoch-61 batch-86
Running loss of epoch-61 batch-86 = 1.9671861082315445e-06

Training epoch-61 batch-87
Running loss of epoch-61 batch-87 = 7.831724360585213e-06

Training epoch-61 batch-88
Running loss of epoch-61 batch-88 = 1.069740392267704e-05

Training epoch-61 batch-89
Running loss of epoch-61 batch-89 = 7.168157026171684e-06

Training epoch-61 batch-90
Running loss of epoch-61 batch-90 = 2.1765008568763733e-06

Training epoch-61 batch-91
Running loss of epoch-61 batch-91 = 7.934635505080223e-06

Training epoch-61 batch-92
Running loss of epoch-61 batch-92 = 5.346606485545635e-06

Training epoch-61 batch-93
Running loss of epoch-61 batch-93 = 6.042188033461571e-06

Training epoch-61 batch-94
Running loss of epoch-61 batch-94 = 4.7460198402404785e-06

Training epoch-61 batch-95
Running loss of epoch-61 batch-95 = 3.7497375160455704e-06

Training epoch-61 batch-96
Running loss of epoch-61 batch-96 = 3.4964177757501602e-06

Training epoch-61 batch-97
Running loss of epoch-61 batch-97 = 2.76835635304451e-06

Training epoch-61 batch-98
Running loss of epoch-61 batch-98 = 9.729992598295212e-06

Training epoch-61 batch-99
Running loss of epoch-61 batch-99 = 3.325054422020912e-06

Training epoch-61 batch-100
Running loss of epoch-61 batch-100 = 4.627974703907967e-06

Training epoch-61 batch-101
Running loss of epoch-61 batch-101 = 5.365349352359772e-06

Training epoch-61 batch-102
Running loss of epoch-61 batch-102 = 5.223089829087257e-06

Training epoch-61 batch-103
Running loss of epoch-61 batch-103 = 4.727626219391823e-06

Training epoch-61 batch-104
Running loss of epoch-61 batch-104 = 3.2051466405391693e-06

Training epoch-61 batch-105
Running loss of epoch-61 batch-105 = 2.9494985938072205e-06

Training epoch-61 batch-106
Running loss of epoch-61 batch-106 = 4.4354237616062164e-06

Training epoch-61 batch-107
Running loss of epoch-61 batch-107 = 1.8959399312734604e-06

Training epoch-61 batch-108
Running loss of epoch-61 batch-108 = 9.511364623904228e-06

Training epoch-61 batch-109
Running loss of epoch-61 batch-109 = 5.0747767090797424e-06

Training epoch-61 batch-110
Running loss of epoch-61 batch-110 = 3.934837877750397e-06

Training epoch-61 batch-111
Running loss of epoch-61 batch-111 = 2.273125573992729e-06

Training epoch-61 batch-112
Running loss of epoch-61 batch-112 = 7.570488378405571e-06

Training epoch-61 batch-113
Running loss of epoch-61 batch-113 = 1.6299542039632797e-05

Training epoch-61 batch-114
Running loss of epoch-61 batch-114 = 4.226574674248695e-06

Training epoch-61 batch-115
Running loss of epoch-61 batch-115 = 2.575106918811798e-06

Training epoch-61 batch-116
Running loss of epoch-61 batch-116 = 3.54694202542305e-06

Training epoch-61 batch-117
Running loss of epoch-61 batch-117 = 9.872019290924072e-06

Training epoch-61 batch-118
Running loss of epoch-61 batch-118 = 4.453584551811218e-06

Training epoch-61 batch-119
Running loss of epoch-61 batch-119 = 6.135087460279465e-06

Training epoch-61 batch-120
Running loss of epoch-61 batch-120 = 2.116197720170021e-06

Training epoch-61 batch-121
Running loss of epoch-61 batch-121 = 1.4218967407941818e-06

Training epoch-61 batch-122
Running loss of epoch-61 batch-122 = 4.307366907596588e-06

Training epoch-61 batch-123
Running loss of epoch-61 batch-123 = 8.223578333854675e-07

Training epoch-61 batch-124
Running loss of epoch-61 batch-124 = 7.678987458348274e-06

Training epoch-61 batch-125
Running loss of epoch-61 batch-125 = 2.922024577856064e-06

Training epoch-61 batch-126
Running loss of epoch-61 batch-126 = 2.732500433921814e-06

Training epoch-61 batch-127
Running loss of epoch-61 batch-127 = 1.794891431927681e-06

Training epoch-61 batch-128
Running loss of epoch-61 batch-128 = 4.255445674061775e-06

Training epoch-61 batch-129
Running loss of epoch-61 batch-129 = 8.074333891272545e-06

Training epoch-61 batch-130
Running loss of epoch-61 batch-130 = 6.788410246372223e-06

Training epoch-61 batch-131
Running loss of epoch-61 batch-131 = 4.324829205870628e-06

Training epoch-61 batch-132
Running loss of epoch-61 batch-132 = 5.478737875819206e-06

Training epoch-61 batch-133
Running loss of epoch-61 batch-133 = 5.35021536052227e-06

Training epoch-61 batch-134
Running loss of epoch-61 batch-134 = 8.21123830974102e-06

Training epoch-61 batch-135
Running loss of epoch-61 batch-135 = 8.783536031842232e-06

Training epoch-61 batch-136
Running loss of epoch-61 batch-136 = 6.9874804466962814e-06

Training epoch-61 batch-137
Running loss of epoch-61 batch-137 = 5.492707714438438e-06

Training epoch-61 batch-138
Running loss of epoch-61 batch-138 = 6.656162440776825e-06

Training epoch-61 batch-139
Running loss of epoch-61 batch-139 = 5.889451131224632e-06

Training epoch-61 batch-140
Running loss of epoch-61 batch-140 = 4.611676558852196e-06

Training epoch-61 batch-141
Running loss of epoch-61 batch-141 = 4.229135811328888e-06

Training epoch-61 batch-142
Running loss of epoch-61 batch-142 = 5.261972546577454e-06

Training epoch-61 batch-143
Running loss of epoch-61 batch-143 = 6.8035442382097244e-06

Training epoch-61 batch-144
Running loss of epoch-61 batch-144 = 3.779074177145958e-06

Training epoch-61 batch-145
Running loss of epoch-61 batch-145 = 8.807983249425888e-06

Training epoch-61 batch-146
Running loss of epoch-61 batch-146 = 3.916677087545395e-06

Training epoch-61 batch-147
Running loss of epoch-61 batch-147 = 4.53554093837738e-06

Training epoch-61 batch-148
Running loss of epoch-61 batch-148 = 7.678521797060966e-06

Training epoch-61 batch-149
Running loss of epoch-61 batch-149 = 1.077260822057724e-05

Training epoch-61 batch-150
Running loss of epoch-61 batch-150 = 4.613073542714119e-06

Training epoch-61 batch-151
Running loss of epoch-61 batch-151 = 1.7017591744661331e-06

Training epoch-61 batch-152
Running loss of epoch-61 batch-152 = 8.021132089197636e-06

Training epoch-61 batch-153
Running loss of epoch-61 batch-153 = 3.849854692816734e-06

Training epoch-61 batch-154
Running loss of epoch-61 batch-154 = 1.616310328245163e-06

Training epoch-61 batch-155
Running loss of epoch-61 batch-155 = 6.436603143811226e-06

Training epoch-61 batch-156
Running loss of epoch-61 batch-156 = 4.943693056702614e-06

Training epoch-61 batch-157
Running loss of epoch-61 batch-157 = 2.2280961275100708e-05

Finished training epoch-61.



Average train loss at epoch-61 = 4.998786002397537e-06

Started Evaluation

Average val loss at epoch-61 = 1.126872784036885

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 87.71 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.10 %

Finished Evaluation



Started training epoch-62


Training epoch-62 batch-1
Running loss of epoch-62 batch-1 = 1.4528166502714157e-05

Training epoch-62 batch-2
Running loss of epoch-62 batch-2 = 6.2747858464717865e-06

Training epoch-62 batch-3
Running loss of epoch-62 batch-3 = 3.003748133778572e-06

Training epoch-62 batch-4
Running loss of epoch-62 batch-4 = 3.541121259331703e-06

Training epoch-62 batch-5
Running loss of epoch-62 batch-5 = 9.577954187989235e-06

Training epoch-62 batch-6
Running loss of epoch-62 batch-6 = 3.453576937317848e-06

Training epoch-62 batch-7
Running loss of epoch-62 batch-7 = 7.230089977383614e-06

Training epoch-62 batch-8
Running loss of epoch-62 batch-8 = 8.767005056142807e-06

Training epoch-62 batch-9
Running loss of epoch-62 batch-9 = 5.079666152596474e-06

Training epoch-62 batch-10
Running loss of epoch-62 batch-10 = 6.556278094649315e-06

Training epoch-62 batch-11
Running loss of epoch-62 batch-11 = 4.076864570379257e-06

Training epoch-62 batch-12
Running loss of epoch-62 batch-12 = 6.874557584524155e-06

Training epoch-62 batch-13
Running loss of epoch-62 batch-13 = 3.4992117434740067e-06

Training epoch-62 batch-14
Running loss of epoch-62 batch-14 = 5.05475327372551e-06

Training epoch-62 batch-15
Running loss of epoch-62 batch-15 = 1.7737038433551788e-06

Training epoch-62 batch-16
Running loss of epoch-62 batch-16 = 3.995141014456749e-06

Training epoch-62 batch-17
Running loss of epoch-62 batch-17 = 3.405613824725151e-06

Training epoch-62 batch-18
Running loss of epoch-62 batch-18 = 5.42239286005497e-06

Training epoch-62 batch-19
Running loss of epoch-62 batch-19 = 6.518559530377388e-06

Training epoch-62 batch-20
Running loss of epoch-62 batch-20 = 4.956033080816269e-06

Training epoch-62 batch-21
Running loss of epoch-62 batch-21 = 2.3872125893831253e-06

Training epoch-62 batch-22
Running loss of epoch-62 batch-22 = 7.032183930277824e-06

Training epoch-62 batch-23
Running loss of epoch-62 batch-23 = 4.727626219391823e-06

Training epoch-62 batch-24
Running loss of epoch-62 batch-24 = 9.326613508164883e-06

Training epoch-62 batch-25
Running loss of epoch-62 batch-25 = 4.254048690199852e-06

Training epoch-62 batch-26
Running loss of epoch-62 batch-26 = 3.56859527528286e-06

Training epoch-62 batch-27
Running loss of epoch-62 batch-27 = 2.8440263122320175e-06

Training epoch-62 batch-28
Running loss of epoch-62 batch-28 = 2.7937348932027817e-06

Training epoch-62 batch-29
Running loss of epoch-62 batch-29 = 4.407716915011406e-06

Training epoch-62 batch-30
Running loss of epoch-62 batch-30 = 3.714347258210182e-06

Training epoch-62 batch-31
Running loss of epoch-62 batch-31 = 4.766741767525673e-06

Training epoch-62 batch-32
Running loss of epoch-62 batch-32 = 9.492971003055573e-06

Training epoch-62 batch-33
Running loss of epoch-62 batch-33 = 1.0974938049912453e-05

Training epoch-62 batch-34
Running loss of epoch-62 batch-34 = 1.0944437235593796e-05

Training epoch-62 batch-35
Running loss of epoch-62 batch-35 = 4.118424840271473e-06

Training epoch-62 batch-36
Running loss of epoch-62 batch-36 = 4.9441587179899216e-06

Training epoch-62 batch-37
Running loss of epoch-62 batch-37 = 1.2044329196214676e-06

Training epoch-62 batch-38
Running loss of epoch-62 batch-38 = 5.0549861043691635e-06

Training epoch-62 batch-39
Running loss of epoch-62 batch-39 = 4.111090674996376e-06

Training epoch-62 batch-40
Running loss of epoch-62 batch-40 = 2.803746610879898e-06

Training epoch-62 batch-41
Running loss of epoch-62 batch-41 = 6.88689760863781e-06

Training epoch-62 batch-42
Running loss of epoch-62 batch-42 = 4.226574674248695e-06

Training epoch-62 batch-43
Running loss of epoch-62 batch-43 = 4.7709327191114426e-06

Training epoch-62 batch-44
Running loss of epoch-62 batch-44 = 1.1424534022808075e-05

Training epoch-62 batch-45
Running loss of epoch-62 batch-45 = 1.9294675439596176e-06

Training epoch-62 batch-46
Running loss of epoch-62 batch-46 = 3.0461233109235764e-06

Training epoch-62 batch-47
Running loss of epoch-62 batch-47 = 4.40794974565506e-06

Training epoch-62 batch-48
Running loss of epoch-62 batch-48 = 3.2924581319093704e-06

Training epoch-62 batch-49
Running loss of epoch-62 batch-49 = 2.198503352701664e-06

Training epoch-62 batch-50
Running loss of epoch-62 batch-50 = 1.934124156832695e-06

Training epoch-62 batch-51
Running loss of epoch-62 batch-51 = 4.050787538290024e-06

Training epoch-62 batch-52
Running loss of epoch-62 batch-52 = 1.1442229151725769e-05

Training epoch-62 batch-53
Running loss of epoch-62 batch-53 = 2.074986696243286e-06

Training epoch-62 batch-54
Running loss of epoch-62 batch-54 = 4.712957888841629e-06

Training epoch-62 batch-55
Running loss of epoch-62 batch-55 = 1.0784482583403587e-05

Training epoch-62 batch-56
Running loss of epoch-62 batch-56 = 4.533212631940842e-06

Training epoch-62 batch-57
Running loss of epoch-62 batch-57 = 5.7301949709653854e-06

Training epoch-62 batch-58
Running loss of epoch-62 batch-58 = 2.8349459171295166e-06

Training epoch-62 batch-59
Running loss of epoch-62 batch-59 = 2.349959686398506e-06

Training epoch-62 batch-60
Running loss of epoch-62 batch-60 = 8.663162589073181e-06

Training epoch-62 batch-61
Running loss of epoch-62 batch-61 = 4.41516749560833e-06

Training epoch-62 batch-62
Running loss of epoch-62 batch-62 = 2.0121224224567413e-06

Training epoch-62 batch-63
Running loss of epoch-62 batch-63 = 4.472443833947182e-06

Training epoch-62 batch-64
Running loss of epoch-62 batch-64 = 3.4656841307878494e-06

Training epoch-62 batch-65
Running loss of epoch-62 batch-65 = 8.523231372237206e-06

Training epoch-62 batch-66
Running loss of epoch-62 batch-66 = 5.163252353668213e-06

Training epoch-62 batch-67
Running loss of epoch-62 batch-67 = 5.84777444601059e-06

Training epoch-62 batch-68
Running loss of epoch-62 batch-68 = 5.345791578292847e-06

Training epoch-62 batch-69
Running loss of epoch-62 batch-69 = 7.103662937879562e-06

Training epoch-62 batch-70
Running loss of epoch-62 batch-70 = 3.4195836633443832e-06

Training epoch-62 batch-71
Running loss of epoch-62 batch-71 = 1.7334241420030594e-06

Training epoch-62 batch-72
Running loss of epoch-62 batch-72 = 2.3909378796815872e-06

Training epoch-62 batch-73
Running loss of epoch-62 batch-73 = 1.7173588275909424e-06

Training epoch-62 batch-74
Running loss of epoch-62 batch-74 = 5.4817646741867065e-06

Training epoch-62 batch-75
Running loss of epoch-62 batch-75 = 4.155328497290611e-06

Training epoch-62 batch-76
Running loss of epoch-62 batch-76 = 8.36770050227642e-06

Training epoch-62 batch-77
Running loss of epoch-62 batch-77 = 3.1476374715566635e-06

Training epoch-62 batch-78
Running loss of epoch-62 batch-78 = 2.4596229195594788e-06

Training epoch-62 batch-79
Running loss of epoch-62 batch-79 = 5.359295755624771e-06

Training epoch-62 batch-80
Running loss of epoch-62 batch-80 = 5.118781700730324e-06

Training epoch-62 batch-81
Running loss of epoch-62 batch-81 = 5.293870344758034e-06

Training epoch-62 batch-82
Running loss of epoch-62 batch-82 = 1.2004747986793518e-06

Training epoch-62 batch-83
Running loss of epoch-62 batch-83 = 5.7800207287073135e-06

Training epoch-62 batch-84
Running loss of epoch-62 batch-84 = 5.951151251792908e-06

Training epoch-62 batch-85
Running loss of epoch-62 batch-85 = 1.6510020941495895e-06

Training epoch-62 batch-86
Running loss of epoch-62 batch-86 = 4.4584739953279495e-06

Training epoch-62 batch-87
Running loss of epoch-62 batch-87 = 1.946929842233658e-06

Training epoch-62 batch-88
Running loss of epoch-62 batch-88 = 6.400980055332184e-06

Training epoch-62 batch-89
Running loss of epoch-62 batch-89 = 3.075692802667618e-06

Training epoch-62 batch-90
Running loss of epoch-62 batch-90 = 5.4049305617809296e-06

Training epoch-62 batch-91
Running loss of epoch-62 batch-91 = 3.0759256333112717e-06

Training epoch-62 batch-92
Running loss of epoch-62 batch-92 = 3.930181264877319e-06

Training epoch-62 batch-93
Running loss of epoch-62 batch-93 = 2.9494985938072205e-06

Training epoch-62 batch-94
Running loss of epoch-62 batch-94 = 7.388880476355553e-06

Training epoch-62 batch-95
Running loss of epoch-62 batch-95 = 3.725755959749222e-06

Training epoch-62 batch-96
Running loss of epoch-62 batch-96 = 4.332046955823898e-06

Training epoch-62 batch-97
Running loss of epoch-62 batch-97 = 5.080830305814743e-06

Training epoch-62 batch-98
Running loss of epoch-62 batch-98 = 3.670342266559601e-06

Training epoch-62 batch-99
Running loss of epoch-62 batch-99 = 1.725275069475174e-06

Training epoch-62 batch-100
Running loss of epoch-62 batch-100 = 4.906440153717995e-06

Training epoch-62 batch-101
Running loss of epoch-62 batch-101 = 2.90735624730587e-06

Training epoch-62 batch-102
Running loss of epoch-62 batch-102 = 4.375819116830826e-06

Training epoch-62 batch-103
Running loss of epoch-62 batch-103 = 4.3727923184633255e-06

Training epoch-62 batch-104
Running loss of epoch-62 batch-104 = 2.9865186661481857e-06

Training epoch-62 batch-105
Running loss of epoch-62 batch-105 = 6.069662049412727e-06

Training epoch-62 batch-106
Running loss of epoch-62 batch-106 = 3.7262216210365295e-06

Training epoch-62 batch-107
Running loss of epoch-62 batch-107 = 3.539491444826126e-06

Training epoch-62 batch-108
Running loss of epoch-62 batch-108 = 3.5297125577926636e-06

Training epoch-62 batch-109
Running loss of epoch-62 batch-109 = 4.4836197048425674e-06

Training epoch-62 batch-110
Running loss of epoch-62 batch-110 = 5.537411198019981e-06

Training epoch-62 batch-111
Running loss of epoch-62 batch-111 = 5.783280357718468e-06

Training epoch-62 batch-112
Running loss of epoch-62 batch-112 = 2.0300503820180893e-06

Training epoch-62 batch-113
Running loss of epoch-62 batch-113 = 6.762798875570297e-06

Training epoch-62 batch-114
Running loss of epoch-62 batch-114 = 7.402850314974785e-06

Training epoch-62 batch-115
Running loss of epoch-62 batch-115 = 4.323199391365051e-06

Training epoch-62 batch-116
Running loss of epoch-62 batch-116 = 2.450309693813324e-06

Training epoch-62 batch-117
Running loss of epoch-62 batch-117 = 3.5078264772892e-06

Training epoch-62 batch-118
Running loss of epoch-62 batch-118 = 2.9031652957201004e-06

Training epoch-62 batch-119
Running loss of epoch-62 batch-119 = 3.2105017453432083e-06

Training epoch-62 batch-120
Running loss of epoch-62 batch-120 = 3.984663635492325e-06

Training epoch-62 batch-121
Running loss of epoch-62 batch-121 = 9.223120287060738e-06

Training epoch-62 batch-122
Running loss of epoch-62 batch-122 = 1.7795246094465256e-06

Training epoch-62 batch-123
Running loss of epoch-62 batch-123 = 3.8156285881996155e-06

Training epoch-62 batch-124
Running loss of epoch-62 batch-124 = 5.7711731642484665e-06

Training epoch-62 batch-125
Running loss of epoch-62 batch-125 = 3.046821802854538e-06

Training epoch-62 batch-126
Running loss of epoch-62 batch-126 = 4.433095455169678e-06

Training epoch-62 batch-127
Running loss of epoch-62 batch-127 = 1.2265518307685852e-06

Training epoch-62 batch-128
Running loss of epoch-62 batch-128 = 5.190027877688408e-06

Training epoch-62 batch-129
Running loss of epoch-62 batch-129 = 3.98256815969944e-06

Training epoch-62 batch-130
Running loss of epoch-62 batch-130 = 4.00422140955925e-06

Training epoch-62 batch-131
Running loss of epoch-62 batch-131 = 1.6011996194720268e-05

Training epoch-62 batch-132
Running loss of epoch-62 batch-132 = 4.619592800736427e-06

Training epoch-62 batch-133
Running loss of epoch-62 batch-133 = 4.200264811515808e-06

Training epoch-62 batch-134
Running loss of epoch-62 batch-134 = 7.268274202942848e-06

Training epoch-62 batch-135
Running loss of epoch-62 batch-135 = 3.5641714930534363e-06

Training epoch-62 batch-136
Running loss of epoch-62 batch-136 = 4.748580977320671e-06

Training epoch-62 batch-137
Running loss of epoch-62 batch-137 = 3.922497853636742e-06

Training epoch-62 batch-138
Running loss of epoch-62 batch-138 = 1.1469004675745964e-05

Training epoch-62 batch-139
Running loss of epoch-62 batch-139 = 1.4453195035457611e-05

Training epoch-62 batch-140
Running loss of epoch-62 batch-140 = 2.9907096177339554e-06

Training epoch-62 batch-141
Running loss of epoch-62 batch-141 = 4.570465534925461e-06

Training epoch-62 batch-142
Running loss of epoch-62 batch-142 = 2.9585789889097214e-06

Training epoch-62 batch-143
Running loss of epoch-62 batch-143 = 3.9425212889909744e-06

Training epoch-62 batch-144
Running loss of epoch-62 batch-144 = 1.2528616935014725e-06

Training epoch-62 batch-145
Running loss of epoch-62 batch-145 = 6.483402103185654e-06

Training epoch-62 batch-146
Running loss of epoch-62 batch-146 = 5.0282105803489685e-06

Training epoch-62 batch-147
Running loss of epoch-62 batch-147 = 1.7124693840742111e-06

Training epoch-62 batch-148
Running loss of epoch-62 batch-148 = 9.618233889341354e-06

Training epoch-62 batch-149
Running loss of epoch-62 batch-149 = 3.5476405173540115e-06

Training epoch-62 batch-150
Running loss of epoch-62 batch-150 = 4.612607881426811e-06

Training epoch-62 batch-151
Running loss of epoch-62 batch-151 = 6.0212332755327225e-06

Training epoch-62 batch-152
Running loss of epoch-62 batch-152 = 4.618195816874504e-06

Training epoch-62 batch-153
Running loss of epoch-62 batch-153 = 4.0638260543346405e-06

Training epoch-62 batch-154
Running loss of epoch-62 batch-154 = 2.7548521757125854e-06

Training epoch-62 batch-155
Running loss of epoch-62 batch-155 = 3.4598633646965027e-06

Training epoch-62 batch-156
Running loss of epoch-62 batch-156 = 8.193310350179672e-07

Training epoch-62 batch-157
Running loss of epoch-62 batch-157 = 2.05114483833313e-05

Finished training epoch-62.



Average train loss at epoch-62 = 4.865314811468124e-06

Started Evaluation

Average val loss at epoch-62 = 1.1314918430433756

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.14 %

Finished Evaluation



Started training epoch-63


Training epoch-63 batch-1
Running loss of epoch-63 batch-1 = 3.940192982554436e-06

Training epoch-63 batch-2
Running loss of epoch-63 batch-2 = 9.077368304133415e-06

Training epoch-63 batch-3
Running loss of epoch-63 batch-3 = 5.720648914575577e-06

Training epoch-63 batch-4
Running loss of epoch-63 batch-4 = 8.087139576673508e-06

Training epoch-63 batch-5
Running loss of epoch-63 batch-5 = 3.5706907510757446e-06

Training epoch-63 batch-6
Running loss of epoch-63 batch-6 = 3.8016587495803833e-06

Training epoch-63 batch-7
Running loss of epoch-63 batch-7 = 6.642891094088554e-06

Training epoch-63 batch-8
Running loss of epoch-63 batch-8 = 6.3963234424591064e-06

Training epoch-63 batch-9
Running loss of epoch-63 batch-9 = 3.566266968846321e-06

Training epoch-63 batch-10
Running loss of epoch-63 batch-10 = 3.0903611332178116e-06

Training epoch-63 batch-11
Running loss of epoch-63 batch-11 = 4.500383511185646e-06

Training epoch-63 batch-12
Running loss of epoch-63 batch-12 = 7.719965651631355e-06

Training epoch-63 batch-13
Running loss of epoch-63 batch-13 = 5.3460244089365005e-06

Training epoch-63 batch-14
Running loss of epoch-63 batch-14 = 3.6403071135282516e-06

Training epoch-63 batch-15
Running loss of epoch-63 batch-15 = 2.424931153655052e-06

Training epoch-63 batch-16
Running loss of epoch-63 batch-16 = 3.4258700907230377e-06

Training epoch-63 batch-17
Running loss of epoch-63 batch-17 = 5.536247044801712e-06

Training epoch-63 batch-18
Running loss of epoch-63 batch-18 = 4.255678504705429e-06

Training epoch-63 batch-19
Running loss of epoch-63 batch-19 = 4.510628059506416e-06

Training epoch-63 batch-20
Running loss of epoch-63 batch-20 = 6.275949999690056e-06

Training epoch-63 batch-21
Running loss of epoch-63 batch-21 = 8.448725566267967e-06

Training epoch-63 batch-22
Running loss of epoch-63 batch-22 = 4.016794264316559e-06

Training epoch-63 batch-23
Running loss of epoch-63 batch-23 = 4.32552769780159e-06

Training epoch-63 batch-24
Running loss of epoch-63 batch-24 = 4.088273271918297e-06

Training epoch-63 batch-25
Running loss of epoch-63 batch-25 = 2.2309832274913788e-06

Training epoch-63 batch-26
Running loss of epoch-63 batch-26 = 1.3993121683597565e-05

Training epoch-63 batch-27
Running loss of epoch-63 batch-27 = 2.794666215777397e-06

Training epoch-63 batch-28
Running loss of epoch-63 batch-28 = 3.313180059194565e-06

Training epoch-63 batch-29
Running loss of epoch-63 batch-29 = 5.006557330489159e-06

Training epoch-63 batch-30
Running loss of epoch-63 batch-30 = 9.677605703473091e-06

Training epoch-63 batch-31
Running loss of epoch-63 batch-31 = 6.106682121753693e-06

Training epoch-63 batch-32
Running loss of epoch-63 batch-32 = 4.740431904792786e-06

Training epoch-63 batch-33
Running loss of epoch-63 batch-33 = 5.4764095693826675e-06

Training epoch-63 batch-34
Running loss of epoch-63 batch-34 = 6.2498729676008224e-06

Training epoch-63 batch-35
Running loss of epoch-63 batch-35 = 6.380490958690643e-06

Training epoch-63 batch-36
Running loss of epoch-63 batch-36 = 1.1680647730827332e-05

Training epoch-63 batch-37
Running loss of epoch-63 batch-37 = 3.489898517727852e-06

Training epoch-63 batch-38
Running loss of epoch-63 batch-38 = 3.16789373755455e-06

Training epoch-63 batch-39
Running loss of epoch-63 batch-39 = 1.9238796085119247e-06

Training epoch-63 batch-40
Running loss of epoch-63 batch-40 = 2.214685082435608e-06

Training epoch-63 batch-41
Running loss of epoch-63 batch-41 = 3.944849595427513e-06

Training epoch-63 batch-42
Running loss of epoch-63 batch-42 = 9.14442352950573e-06

Training epoch-63 batch-43
Running loss of epoch-63 batch-43 = 6.295042112469673e-06

Training epoch-63 batch-44
Running loss of epoch-63 batch-44 = 5.9853773564100266e-06

Training epoch-63 batch-45
Running loss of epoch-63 batch-45 = 8.6131040006876e-06

Training epoch-63 batch-46
Running loss of epoch-63 batch-46 = 4.892703145742416e-06

Training epoch-63 batch-47
Running loss of epoch-63 batch-47 = 5.129724740982056e-06

Training epoch-63 batch-48
Running loss of epoch-63 batch-48 = 7.177237421274185e-06

Training epoch-63 batch-49
Running loss of epoch-63 batch-49 = 3.566266968846321e-06

Training epoch-63 batch-50
Running loss of epoch-63 batch-50 = 4.253815859556198e-06

Training epoch-63 batch-51
Running loss of epoch-63 batch-51 = 6.806105375289917e-06

Training epoch-63 batch-52
Running loss of epoch-63 batch-52 = 3.848690539598465e-06

Training epoch-63 batch-53
Running loss of epoch-63 batch-53 = 3.0957162380218506e-06

Training epoch-63 batch-54
Running loss of epoch-63 batch-54 = 1.0443152859807014e-05

Training epoch-63 batch-55
Running loss of epoch-63 batch-55 = 1.8239952623844147e-06

Training epoch-63 batch-56
Running loss of epoch-63 batch-56 = 2.325279638171196e-06

Training epoch-63 batch-57
Running loss of epoch-63 batch-57 = 7.548369467258453e-06

Training epoch-63 batch-58
Running loss of epoch-63 batch-58 = 5.720881745219231e-06

Training epoch-63 batch-59
Running loss of epoch-63 batch-59 = 3.070337697863579e-06

Training epoch-63 batch-60
Running loss of epoch-63 batch-60 = 4.3280888348817825e-06

Training epoch-63 batch-61
Running loss of epoch-63 batch-61 = 3.527384251356125e-06

Training epoch-63 batch-62
Running loss of epoch-63 batch-62 = 4.580244421958923e-06

Training epoch-63 batch-63
Running loss of epoch-63 batch-63 = 1.2440141290426254e-06

Training epoch-63 batch-64
Running loss of epoch-63 batch-64 = 2.6188790798187256e-06

Training epoch-63 batch-65
Running loss of epoch-63 batch-65 = 5.0335656851530075e-06

Training epoch-63 batch-66
Running loss of epoch-63 batch-66 = 8.211005479097366e-06

Training epoch-63 batch-67
Running loss of epoch-63 batch-67 = 2.930406481027603e-06

Training epoch-63 batch-68
Running loss of epoch-63 batch-68 = 6.774673238396645e-06

Training epoch-63 batch-69
Running loss of epoch-63 batch-69 = 2.6721972972154617e-06

Training epoch-63 batch-70
Running loss of epoch-63 batch-70 = 3.007706254720688e-06

Training epoch-63 batch-71
Running loss of epoch-63 batch-71 = 2.9352959245443344e-06

Training epoch-63 batch-72
Running loss of epoch-63 batch-72 = 4.837755113840103e-06

Training epoch-63 batch-73
Running loss of epoch-63 batch-73 = 8.886330761015415e-06

Training epoch-63 batch-74
Running loss of epoch-63 batch-74 = 4.3318141251802444e-06

Training epoch-63 batch-75
Running loss of epoch-63 batch-75 = 4.284083843231201e-06

Training epoch-63 batch-76
Running loss of epoch-63 batch-76 = 5.275942385196686e-06

Training epoch-63 batch-77
Running loss of epoch-63 batch-77 = 1.28941610455513e-06

Training epoch-63 batch-78
Running loss of epoch-63 batch-78 = 3.623543307185173e-06

Training epoch-63 batch-79
Running loss of epoch-63 batch-79 = 3.5940902307629585e-06

Training epoch-63 batch-80
Running loss of epoch-63 batch-80 = 5.017733201384544e-06

Training epoch-63 batch-81
Running loss of epoch-63 batch-81 = 1.562526449561119e-06

Training epoch-63 batch-82
Running loss of epoch-63 batch-82 = 2.3818574845790863e-06

Training epoch-63 batch-83
Running loss of epoch-63 batch-83 = 1.1350493878126144e-06

Training epoch-63 batch-84
Running loss of epoch-63 batch-84 = 6.438000127673149e-06

Training epoch-63 batch-85
Running loss of epoch-63 batch-85 = 6.903195753693581e-06

Training epoch-63 batch-86
Running loss of epoch-63 batch-86 = 4.403525963425636e-06

Training epoch-63 batch-87
Running loss of epoch-63 batch-87 = 3.4335535019636154e-06

Training epoch-63 batch-88
Running loss of epoch-63 batch-88 = 4.498288035392761e-06

Training epoch-63 batch-89
Running loss of epoch-63 batch-89 = 4.0531158447265625e-06

Training epoch-63 batch-90
Running loss of epoch-63 batch-90 = 6.12274743616581e-06

Training epoch-63 batch-91
Running loss of epoch-63 batch-91 = 5.300156772136688e-06

Training epoch-63 batch-92
Running loss of epoch-63 batch-92 = 3.877095878124237e-06

Training epoch-63 batch-93
Running loss of epoch-63 batch-93 = 2.728775143623352e-06

Training epoch-63 batch-94
Running loss of epoch-63 batch-94 = 3.418419510126114e-06

Training epoch-63 batch-95
Running loss of epoch-63 batch-95 = 1.2616859748959541e-05

Training epoch-63 batch-96
Running loss of epoch-63 batch-96 = 1.7106067389249802e-06

Training epoch-63 batch-97
Running loss of epoch-63 batch-97 = 4.848232492804527e-06

Training epoch-63 batch-98
Running loss of epoch-63 batch-98 = 4.599103704094887e-06

Training epoch-63 batch-99
Running loss of epoch-63 batch-99 = 3.0745286494493484e-06

Training epoch-63 batch-100
Running loss of epoch-63 batch-100 = 4.6442728489637375e-06

Training epoch-63 batch-101
Running loss of epoch-63 batch-101 = 5.301320925354958e-06

Training epoch-63 batch-102
Running loss of epoch-63 batch-102 = 7.059425115585327e-06

Training epoch-63 batch-103
Running loss of epoch-63 batch-103 = 3.0240043997764587e-06

Training epoch-63 batch-104
Running loss of epoch-63 batch-104 = 4.536472260951996e-06

Training epoch-63 batch-105
Running loss of epoch-63 batch-105 = 5.630543455481529e-06

Training epoch-63 batch-106
Running loss of epoch-63 batch-106 = 4.875939339399338e-06

Training epoch-63 batch-107
Running loss of epoch-63 batch-107 = 2.387678250670433e-06

Training epoch-63 batch-108
Running loss of epoch-63 batch-108 = 3.0784867703914642e-06

Training epoch-63 batch-109
Running loss of epoch-63 batch-109 = 8.923932909965515e-06

Training epoch-63 batch-110
Running loss of epoch-63 batch-110 = 3.905966877937317e-06

Training epoch-63 batch-111
Running loss of epoch-63 batch-111 = 2.882210537791252e-06

Training epoch-63 batch-112
Running loss of epoch-63 batch-112 = 4.535773769021034e-06

Training epoch-63 batch-113
Running loss of epoch-63 batch-113 = 7.637077942490578e-06

Training epoch-63 batch-114
Running loss of epoch-63 batch-114 = 3.102002665400505e-06

Training epoch-63 batch-115
Running loss of epoch-63 batch-115 = 6.01448118686676e-06

Training epoch-63 batch-116
Running loss of epoch-63 batch-116 = 3.0777882784605026e-06

Training epoch-63 batch-117
Running loss of epoch-63 batch-117 = 7.061054930090904e-06

Training epoch-63 batch-118
Running loss of epoch-63 batch-118 = 5.6033022701740265e-06

Training epoch-63 batch-119
Running loss of epoch-63 batch-119 = 4.8871152102947235e-06

Training epoch-63 batch-120
Running loss of epoch-63 batch-120 = 4.522502422332764e-06

Training epoch-63 batch-121
Running loss of epoch-63 batch-121 = 2.57161445915699e-06

Training epoch-63 batch-122
Running loss of epoch-63 batch-122 = 2.9655639082193375e-06

Training epoch-63 batch-123
Running loss of epoch-63 batch-123 = 3.264518454670906e-06

Training epoch-63 batch-124
Running loss of epoch-63 batch-124 = 4.079192876815796e-06

Training epoch-63 batch-125
Running loss of epoch-63 batch-125 = 1.6384292393922806e-06

Training epoch-63 batch-126
Running loss of epoch-63 batch-126 = 8.467817679047585e-06

Training epoch-63 batch-127
Running loss of epoch-63 batch-127 = 5.606561899185181e-06

Training epoch-63 batch-128
Running loss of epoch-63 batch-128 = 2.9799994081258774e-06

Training epoch-63 batch-129
Running loss of epoch-63 batch-129 = 4.512956365942955e-06

Training epoch-63 batch-130
Running loss of epoch-63 batch-130 = 3.2957177609205246e-06

Training epoch-63 batch-131
Running loss of epoch-63 batch-131 = 4.458706825971603e-06

Training epoch-63 batch-132
Running loss of epoch-63 batch-132 = 2.3259781301021576e-06

Training epoch-63 batch-133
Running loss of epoch-63 batch-133 = 7.038936018943787e-06

Training epoch-63 batch-134
Running loss of epoch-63 batch-134 = 1.257285475730896e-06

Training epoch-63 batch-135
Running loss of epoch-63 batch-135 = 3.0903611332178116e-06

Training epoch-63 batch-136
Running loss of epoch-63 batch-136 = 6.502727046608925e-06

Training epoch-63 batch-137
Running loss of epoch-63 batch-137 = 4.7567300498485565e-06

Training epoch-63 batch-138
Running loss of epoch-63 batch-138 = 3.0517112463712692e-06

Training epoch-63 batch-139
Running loss of epoch-63 batch-139 = 4.85382042825222e-06

Training epoch-63 batch-140
Running loss of epoch-63 batch-140 = 2.4242326617240906e-06

Training epoch-63 batch-141
Running loss of epoch-63 batch-141 = 4.016561433672905e-06

Training epoch-63 batch-142
Running loss of epoch-63 batch-142 = 5.730893462896347e-06

Training epoch-63 batch-143
Running loss of epoch-63 batch-143 = 1.5401747077703476e-06

Training epoch-63 batch-144
Running loss of epoch-63 batch-144 = 3.987690433859825e-06

Training epoch-63 batch-145
Running loss of epoch-63 batch-145 = 6.921589374542236e-06

Training epoch-63 batch-146
Running loss of epoch-63 batch-146 = 2.291053533554077e-06

Training epoch-63 batch-147
Running loss of epoch-63 batch-147 = 2.787448465824127e-06

Training epoch-63 batch-148
Running loss of epoch-63 batch-148 = 4.663132131099701e-06

Training epoch-63 batch-149
Running loss of epoch-63 batch-149 = 4.335073754191399e-06

Training epoch-63 batch-150
Running loss of epoch-63 batch-150 = 4.898058250546455e-06

Training epoch-63 batch-151
Running loss of epoch-63 batch-151 = 3.7425197660923004e-06

Training epoch-63 batch-152
Running loss of epoch-63 batch-152 = 2.8435606509447098e-06

Training epoch-63 batch-153
Running loss of epoch-63 batch-153 = 4.93926927447319e-06

Training epoch-63 batch-154
Running loss of epoch-63 batch-154 = 4.034955054521561e-06

Training epoch-63 batch-155
Running loss of epoch-63 batch-155 = 8.170260116457939e-06

Training epoch-63 batch-156
Running loss of epoch-63 batch-156 = 2.7616042643785477e-06

Training epoch-63 batch-157
Running loss of epoch-63 batch-157 = 4.7266483306884766e-05

Finished training epoch-63.



Average train loss at epoch-63 = 4.786978662014007e-06

Started Evaluation

Average val loss at epoch-63 = 1.128988192607865

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.48 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 83.12 %

Finished Evaluation



Started training epoch-64


Training epoch-64 batch-1
Running loss of epoch-64 batch-1 = 4.109693691134453e-06

Training epoch-64 batch-2
Running loss of epoch-64 batch-2 = 3.3408869057893753e-06

Training epoch-64 batch-3
Running loss of epoch-64 batch-3 = 3.171851858496666e-06

Training epoch-64 batch-4
Running loss of epoch-64 batch-4 = 2.5422777980566025e-06

Training epoch-64 batch-5
Running loss of epoch-64 batch-5 = 7.146969437599182e-06

Training epoch-64 batch-6
Running loss of epoch-64 batch-6 = 2.541579306125641e-06

Training epoch-64 batch-7
Running loss of epoch-64 batch-7 = 2.7015339583158493e-06

Training epoch-64 batch-8
Running loss of epoch-64 batch-8 = 5.642417818307877e-06

Training epoch-64 batch-9
Running loss of epoch-64 batch-9 = 4.937639459967613e-06

Training epoch-64 batch-10
Running loss of epoch-64 batch-10 = 2.1385494619607925e-06

Training epoch-64 batch-11
Running loss of epoch-64 batch-11 = 3.88571061193943e-06

Training epoch-64 batch-12
Running loss of epoch-64 batch-12 = 4.956033080816269e-06

Training epoch-64 batch-13
Running loss of epoch-64 batch-13 = 2.5390181690454483e-06

Training epoch-64 batch-14
Running loss of epoch-64 batch-14 = 7.977243512868881e-06

Training epoch-64 batch-15
Running loss of epoch-64 batch-15 = 3.9942096918821335e-06

Training epoch-64 batch-16
Running loss of epoch-64 batch-16 = 7.966766133904457e-06

Training epoch-64 batch-17
Running loss of epoch-64 batch-17 = 5.489913746714592e-06

Training epoch-64 batch-18
Running loss of epoch-64 batch-18 = 2.5853514671325684e-06

Training epoch-64 batch-19
Running loss of epoch-64 batch-19 = 8.429167792201042e-06

Training epoch-64 batch-20
Running loss of epoch-64 batch-20 = 6.668269634246826e-07

Training epoch-64 batch-21
Running loss of epoch-64 batch-21 = 3.3990945667028427e-06

Training epoch-64 batch-22
Running loss of epoch-64 batch-22 = 2.154149115085602e-06

Training epoch-64 batch-23
Running loss of epoch-64 batch-23 = 5.327165126800537e-06

Training epoch-64 batch-24
Running loss of epoch-64 batch-24 = 6.8212393671274185e-06

Training epoch-64 batch-25
Running loss of epoch-64 batch-25 = 4.411675035953522e-06

Training epoch-64 batch-26
Running loss of epoch-64 batch-26 = 4.4496264308691025e-06

Training epoch-64 batch-27
Running loss of epoch-64 batch-27 = 5.193054676055908e-06

Training epoch-64 batch-28
Running loss of epoch-64 batch-28 = 7.323455065488815e-06

Training epoch-64 batch-29
Running loss of epoch-64 batch-29 = 4.930421710014343e-06

Training epoch-64 batch-30
Running loss of epoch-64 batch-30 = 9.029172360897064e-06

Training epoch-64 batch-31
Running loss of epoch-64 batch-31 = 3.4477561712265015e-06

Training epoch-64 batch-32
Running loss of epoch-64 batch-32 = 3.4957192838191986e-06

Training epoch-64 batch-33
Running loss of epoch-64 batch-33 = 2.9653310775756836e-06

Training epoch-64 batch-34
Running loss of epoch-64 batch-34 = 6.266636773943901e-06

Training epoch-64 batch-35
Running loss of epoch-64 batch-35 = 3.596534952521324e-06

Training epoch-64 batch-36
Running loss of epoch-64 batch-36 = 5.97536563873291e-06

Training epoch-64 batch-37
Running loss of epoch-64 batch-37 = 3.2726675271987915e-06

Training epoch-64 batch-38
Running loss of epoch-64 batch-38 = 6.00423663854599e-06

Training epoch-64 batch-39
Running loss of epoch-64 batch-39 = 2.791173756122589e-06

Training epoch-64 batch-40
Running loss of epoch-64 batch-40 = 2.35741026699543e-06

Training epoch-64 batch-41
Running loss of epoch-64 batch-41 = 1.0603340342640877e-05

Training epoch-64 batch-42
Running loss of epoch-64 batch-42 = 2.814456820487976e-06

Training epoch-64 batch-43
Running loss of epoch-64 batch-43 = 8.05174931883812e-06

Training epoch-64 batch-44
Running loss of epoch-64 batch-44 = 4.654284566640854e-06

Training epoch-64 batch-45
Running loss of epoch-64 batch-45 = 2.7106143534183502e-06

Training epoch-64 batch-46
Running loss of epoch-64 batch-46 = 3.62214632332325e-06

Training epoch-64 batch-47
Running loss of epoch-64 batch-47 = 3.1441450119018555e-06

Training epoch-64 batch-48
Running loss of epoch-64 batch-48 = 3.0328519642353058e-06

Training epoch-64 batch-49
Running loss of epoch-64 batch-49 = 1.8517021089792252e-06

Training epoch-64 batch-50
Running loss of epoch-64 batch-50 = 2.0854640752077103e-06

Training epoch-64 batch-51
Running loss of epoch-64 batch-51 = 3.041233867406845e-06

Training epoch-64 batch-52
Running loss of epoch-64 batch-52 = 5.248468369245529e-06

Training epoch-64 batch-53
Running loss of epoch-64 batch-53 = 9.459909051656723e-07

Training epoch-64 batch-54
Running loss of epoch-64 batch-54 = 2.2549647837877274e-06

Training epoch-64 batch-55
Running loss of epoch-64 batch-55 = 2.8426293283700943e-06

Training epoch-64 batch-56
Running loss of epoch-64 batch-56 = 2.598157152533531e-06

Training epoch-64 batch-57
Running loss of epoch-64 batch-57 = 4.415400326251984e-06

Training epoch-64 batch-58
Running loss of epoch-64 batch-58 = 5.340203642845154e-06

Training epoch-64 batch-59
Running loss of epoch-64 batch-59 = 1.741340383887291e-06

Training epoch-64 batch-60
Running loss of epoch-64 batch-60 = 9.006820619106293e-06

Training epoch-64 batch-61
Running loss of epoch-64 batch-61 = 2.505956217646599e-06

Training epoch-64 batch-62
Running loss of epoch-64 batch-62 = 3.2624229788780212e-06

Training epoch-64 batch-63
Running loss of epoch-64 batch-63 = 3.660796210169792e-06

Training epoch-64 batch-64
Running loss of epoch-64 batch-64 = 2.03610397875309e-06

Training epoch-64 batch-65
Running loss of epoch-64 batch-65 = 3.921100869774818e-06

Training epoch-64 batch-66
Running loss of epoch-64 batch-66 = 4.901783540844917e-06

Training epoch-64 batch-67
Running loss of epoch-64 batch-67 = 7.822178304195404e-06

Training epoch-64 batch-68
Running loss of epoch-64 batch-68 = 9.34535637497902e-06

Training epoch-64 batch-69
Running loss of epoch-64 batch-69 = 6.137415766716003e-06

Training epoch-64 batch-70
Running loss of epoch-64 batch-70 = 5.754409357905388e-06

Training epoch-64 batch-71
Running loss of epoch-64 batch-71 = 2.436107024550438e-06

Training epoch-64 batch-72
Running loss of epoch-64 batch-72 = 2.9942020773887634e-06

Training epoch-64 batch-73
Running loss of epoch-64 batch-73 = 6.860820576548576e-06

Training epoch-64 batch-74
Running loss of epoch-64 batch-74 = 7.404480129480362e-06

Training epoch-64 batch-75
Running loss of epoch-64 batch-75 = 3.630528226494789e-06

Training epoch-64 batch-76
Running loss of epoch-64 batch-76 = 4.292232915759087e-06

Training epoch-64 batch-77
Running loss of epoch-64 batch-77 = 4.395144060254097e-06

Training epoch-64 batch-78
Running loss of epoch-64 batch-78 = 3.0882656574249268e-06

Training epoch-64 batch-79
Running loss of epoch-64 batch-79 = 1.6123522073030472e-06

Training epoch-64 batch-80
Running loss of epoch-64 batch-80 = 2.9888469725847244e-06

Training epoch-64 batch-81
Running loss of epoch-64 batch-81 = 9.988900274038315e-06

Training epoch-64 batch-82
Running loss of epoch-64 batch-82 = 2.0568259060382843e-06

Training epoch-64 batch-83
Running loss of epoch-64 batch-83 = 2.8300564736127853e-06

Training epoch-64 batch-84
Running loss of epoch-64 batch-84 = 4.681060090661049e-06

Training epoch-64 batch-85
Running loss of epoch-64 batch-85 = 2.72202305495739e-06

Training epoch-64 batch-86
Running loss of epoch-64 batch-86 = 9.272247552871704e-06

Training epoch-64 batch-87
Running loss of epoch-64 batch-87 = 2.829008735716343e-06

Training epoch-64 batch-88
Running loss of epoch-64 batch-88 = 3.137625753879547e-06

Training epoch-64 batch-89
Running loss of epoch-64 batch-89 = 1.723412424325943e-06

Training epoch-64 batch-90
Running loss of epoch-64 batch-90 = 1.3820827007293701e-06

Training epoch-64 batch-91
Running loss of epoch-64 batch-91 = 7.30087049305439e-06

Training epoch-64 batch-92
Running loss of epoch-64 batch-92 = 4.346948117017746e-06

Training epoch-64 batch-93
Running loss of epoch-64 batch-93 = 3.583962097764015e-06

Training epoch-64 batch-94
Running loss of epoch-64 batch-94 = 3.1103845685720444e-06

Training epoch-64 batch-95
Running loss of epoch-64 batch-95 = 1.075398176908493e-05

Training epoch-64 batch-96
Running loss of epoch-64 batch-96 = 5.917157977819443e-06

Training epoch-64 batch-97
Running loss of epoch-64 batch-97 = 5.735782906413078e-06

Training epoch-64 batch-98
Running loss of epoch-64 batch-98 = 5.103647708892822e-06

Training epoch-64 batch-99
Running loss of epoch-64 batch-99 = 7.76839442551136e-06

Training epoch-64 batch-100
Running loss of epoch-64 batch-100 = 2.293149009346962e-06

Training epoch-64 batch-101
Running loss of epoch-64 batch-101 = 5.439622327685356e-06

Training epoch-64 batch-102
Running loss of epoch-64 batch-102 = 2.3655593395233154e-06

Training epoch-64 batch-103
Running loss of epoch-64 batch-103 = 4.130415618419647e-06

Training epoch-64 batch-104
Running loss of epoch-64 batch-104 = 2.262415364384651e-06

Training epoch-64 batch-105
Running loss of epoch-64 batch-105 = 5.867797881364822e-06

Training epoch-64 batch-106
Running loss of epoch-64 batch-106 = 3.6668498069047928e-06

Training epoch-64 batch-107
Running loss of epoch-64 batch-107 = 4.59630973637104e-06

Training epoch-64 batch-108
Running loss of epoch-64 batch-108 = 3.0831433832645416e-06

Training epoch-64 batch-109
Running loss of epoch-64 batch-109 = 2.8065405786037445e-06

Training epoch-64 batch-110
Running loss of epoch-64 batch-110 = 9.658746421337128e-06

Training epoch-64 batch-111
Running loss of epoch-64 batch-111 = 5.657551810145378e-06

Training epoch-64 batch-112
Running loss of epoch-64 batch-112 = 2.4759210646152496e-06

Training epoch-64 batch-113
Running loss of epoch-64 batch-113 = 3.647059202194214e-06

Training epoch-64 batch-114
Running loss of epoch-64 batch-114 = 6.895978003740311e-06

Training epoch-64 batch-115
Running loss of epoch-64 batch-115 = 4.667090252041817e-06

Training epoch-64 batch-116
Running loss of epoch-64 batch-116 = 5.1176175475120544e-06

Training epoch-64 batch-117
Running loss of epoch-64 batch-117 = 2.7008354663848877e-06

Training epoch-64 batch-118
Running loss of epoch-64 batch-118 = 2.6579946279525757e-06

Training epoch-64 batch-119
Running loss of epoch-64 batch-119 = 3.99700365960598e-06

Training epoch-64 batch-120
Running loss of epoch-64 batch-120 = 5.712034180760384e-06

Training epoch-64 batch-121
Running loss of epoch-64 batch-121 = 4.6691857278347015e-06

Training epoch-64 batch-122
Running loss of epoch-64 batch-122 = 6.090616807341576e-06

Training epoch-64 batch-123
Running loss of epoch-64 batch-123 = 6.120302714407444e-06

Training epoch-64 batch-124
Running loss of epoch-64 batch-124 = 1.6019446775317192e-05

Training epoch-64 batch-125
Running loss of epoch-64 batch-125 = 5.548587068915367e-06

Training epoch-64 batch-126
Running loss of epoch-64 batch-126 = 6.273854523897171e-06

Training epoch-64 batch-127
Running loss of epoch-64 batch-127 = 9.136972948908806e-06

Training epoch-64 batch-128
Running loss of epoch-64 batch-128 = 2.9511284083127975e-06

Training epoch-64 batch-129
Running loss of epoch-64 batch-129 = 6.8480148911476135e-06

Training epoch-64 batch-130
Running loss of epoch-64 batch-130 = 2.6531051844358444e-06

Training epoch-64 batch-131
Running loss of epoch-64 batch-131 = 6.972113624215126e-06

Training epoch-64 batch-132
Running loss of epoch-64 batch-132 = 4.643574357032776e-06

Training epoch-64 batch-133
Running loss of epoch-64 batch-133 = 8.45850445330143e-06

Training epoch-64 batch-134
Running loss of epoch-64 batch-134 = 3.044959157705307e-06

Training epoch-64 batch-135
Running loss of epoch-64 batch-135 = 2.813059836626053e-06

Training epoch-64 batch-136
Running loss of epoch-64 batch-136 = 7.71903432905674e-06

Training epoch-64 batch-137
Running loss of epoch-64 batch-137 = 2.5834888219833374e-06

Training epoch-64 batch-138
Running loss of epoch-64 batch-138 = 5.4496340453624725e-06

Training epoch-64 batch-139
Running loss of epoch-64 batch-139 = 1.9443687051534653e-06

Training epoch-64 batch-140
Running loss of epoch-64 batch-140 = 4.338100552558899e-06

Training epoch-64 batch-141
Running loss of epoch-64 batch-141 = 1.3592652976512909e-06

Training epoch-64 batch-142
Running loss of epoch-64 batch-142 = 8.01263377070427e-06

Training epoch-64 batch-143
Running loss of epoch-64 batch-143 = 6.23706728219986e-06

Training epoch-64 batch-144
Running loss of epoch-64 batch-144 = 5.084788426756859e-06

Training epoch-64 batch-145
Running loss of epoch-64 batch-145 = 3.836117684841156e-06

Training epoch-64 batch-146
Running loss of epoch-64 batch-146 = 4.272442311048508e-06

Training epoch-64 batch-147
Running loss of epoch-64 batch-147 = 4.280591383576393e-06

Training epoch-64 batch-148
Running loss of epoch-64 batch-148 = 6.222166121006012e-06

Training epoch-64 batch-149
Running loss of epoch-64 batch-149 = 4.802364856004715e-06

Training epoch-64 batch-150
Running loss of epoch-64 batch-150 = 3.1818635761737823e-06

Training epoch-64 batch-151
Running loss of epoch-64 batch-151 = 6.04032538831234e-06

Training epoch-64 batch-152
Running loss of epoch-64 batch-152 = 2.93622724711895e-06

Training epoch-64 batch-153
Running loss of epoch-64 batch-153 = 9.690411388874054e-07

Training epoch-64 batch-154
Running loss of epoch-64 batch-154 = 1.8705613911151886e-06

Training epoch-64 batch-155
Running loss of epoch-64 batch-155 = 6.915302947163582e-06

Training epoch-64 batch-156
Running loss of epoch-64 batch-156 = 3.976514562964439e-06

Training epoch-64 batch-157
Running loss of epoch-64 batch-157 = 5.862489342689514e-05

Finished training epoch-64.



Average train loss at epoch-64 = 4.684208333492279e-06

Started Evaluation

Average val loss at epoch-64 = 1.1333926280078206

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.33 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.48 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 83.14 %

Finished Evaluation



Started training epoch-65


Training epoch-65 batch-1
Running loss of epoch-65 batch-1 = 6.7730434238910675e-06

Training epoch-65 batch-2
Running loss of epoch-65 batch-2 = 6.242422387003899e-06

Training epoch-65 batch-3
Running loss of epoch-65 batch-3 = 8.50786454975605e-06

Training epoch-65 batch-4
Running loss of epoch-65 batch-4 = 8.760485798120499e-06

Training epoch-65 batch-5
Running loss of epoch-65 batch-5 = 8.081551641225815e-07

Training epoch-65 batch-6
Running loss of epoch-65 batch-6 = 5.73950819671154e-06

Training epoch-65 batch-7
Running loss of epoch-65 batch-7 = 5.391659215092659e-06

Training epoch-65 batch-8
Running loss of epoch-65 batch-8 = 3.030989319086075e-06

Training epoch-65 batch-9
Running loss of epoch-65 batch-9 = 9.752344340085983e-06

Training epoch-65 batch-10
Running loss of epoch-65 batch-10 = 2.6763882488012314e-06

Training epoch-65 batch-11
Running loss of epoch-65 batch-11 = 4.015397280454636e-06

Training epoch-65 batch-12
Running loss of epoch-65 batch-12 = 2.889428287744522e-06

Training epoch-65 batch-13
Running loss of epoch-65 batch-13 = 1.3231998309493065e-05

Training epoch-65 batch-14
Running loss of epoch-65 batch-14 = 2.677319571375847e-06

Training epoch-65 batch-15
Running loss of epoch-65 batch-15 = 5.371402949094772e-06

Training epoch-65 batch-16
Running loss of epoch-65 batch-16 = 3.931345418095589e-06

Training epoch-65 batch-17
Running loss of epoch-65 batch-17 = 2.9068905860185623e-06

Training epoch-65 batch-18
Running loss of epoch-65 batch-18 = 7.26478174328804e-06

Training epoch-65 batch-19
Running loss of epoch-65 batch-19 = 2.72924080491066e-06

Training epoch-65 batch-20
Running loss of epoch-65 batch-20 = 2.79303640127182e-06

Training epoch-65 batch-21
Running loss of epoch-65 batch-21 = 5.5960845202207565e-06

Training epoch-65 batch-22
Running loss of epoch-65 batch-22 = 2.8619542717933655e-06

Training epoch-65 batch-23
Running loss of epoch-65 batch-23 = 6.538117304444313e-06

Training epoch-65 batch-24
Running loss of epoch-65 batch-24 = 7.138354703783989e-06

Training epoch-65 batch-25
Running loss of epoch-65 batch-25 = 4.269881173968315e-06

Training epoch-65 batch-26
Running loss of epoch-65 batch-26 = 9.815441444516182e-06

Training epoch-65 batch-27
Running loss of epoch-65 batch-27 = 3.836816176772118e-06

Training epoch-65 batch-28
Running loss of epoch-65 batch-28 = 4.066852852702141e-06

Training epoch-65 batch-29
Running loss of epoch-65 batch-29 = 5.414476618170738e-06

Training epoch-65 batch-30
Running loss of epoch-65 batch-30 = 8.524628356099129e-06

Training epoch-65 batch-31
Running loss of epoch-65 batch-31 = 4.197238013148308e-06

Training epoch-65 batch-32
Running loss of epoch-65 batch-32 = 5.22867776453495e-06

Training epoch-65 batch-33
Running loss of epoch-65 batch-33 = 2.9955990612506866e-06

Training epoch-65 batch-34
Running loss of epoch-65 batch-34 = 2.0328443497419357e-06

Training epoch-65 batch-35
Running loss of epoch-65 batch-35 = 2.8177164494991302e-06

Training epoch-65 batch-36
Running loss of epoch-65 batch-36 = 3.3993273973464966e-06

Training epoch-65 batch-37
Running loss of epoch-65 batch-37 = 4.172325134277344e-06

Training epoch-65 batch-38
Running loss of epoch-65 batch-38 = 7.449416443705559e-06

Training epoch-65 batch-39
Running loss of epoch-65 batch-39 = 8.343253284692764e-06

Training epoch-65 batch-40
Running loss of epoch-65 batch-40 = 4.211673513054848e-06

Training epoch-65 batch-41
Running loss of epoch-65 batch-41 = 3.3422838896512985e-06

Training epoch-65 batch-42
Running loss of epoch-65 batch-42 = 2.83936969935894e-06

Training epoch-65 batch-43
Running loss of epoch-65 batch-43 = 4.643108695745468e-06

Training epoch-65 batch-44
Running loss of epoch-65 batch-44 = 2.2142194211483e-06

Training epoch-65 batch-45
Running loss of epoch-65 batch-45 = 3.352295607328415e-06

Training epoch-65 batch-46
Running loss of epoch-65 batch-46 = 3.3783726394176483e-06

Training epoch-65 batch-47
Running loss of epoch-65 batch-47 = 1.0826624929904938e-06

Training epoch-65 batch-48
Running loss of epoch-65 batch-48 = 3.913650289177895e-06

Training epoch-65 batch-49
Running loss of epoch-65 batch-49 = 1.0917894542217255e-05

Training epoch-65 batch-50
Running loss of epoch-65 batch-50 = 2.8258655220270157e-06

Training epoch-65 batch-51
Running loss of epoch-65 batch-51 = 5.905982106924057e-06

Training epoch-65 batch-52
Running loss of epoch-65 batch-52 = 4.5173801481723785e-06

Training epoch-65 batch-53
Running loss of epoch-65 batch-53 = 2.144835889339447e-06

Training epoch-65 batch-54
Running loss of epoch-65 batch-54 = 3.0014198273420334e-06

Training epoch-65 batch-55
Running loss of epoch-65 batch-55 = 4.747183993458748e-06

Training epoch-65 batch-56
Running loss of epoch-65 batch-56 = 3.1085219234228134e-06

Training epoch-65 batch-57
Running loss of epoch-65 batch-57 = 1.3634562492370605e-06

Training epoch-65 batch-58
Running loss of epoch-65 batch-58 = 5.224952474236488e-06

Training epoch-65 batch-59
Running loss of epoch-65 batch-59 = 4.135072231292725e-06

Training epoch-65 batch-60
Running loss of epoch-65 batch-60 = 4.630768671631813e-06

Training epoch-65 batch-61
Running loss of epoch-65 batch-61 = 4.830537363886833e-06

Training epoch-65 batch-62
Running loss of epoch-65 batch-62 = 3.491295501589775e-06

Training epoch-65 batch-63
Running loss of epoch-65 batch-63 = 1.7422717064619064e-06

Training epoch-65 batch-64
Running loss of epoch-65 batch-64 = 3.460794687271118e-06

Training epoch-65 batch-65
Running loss of epoch-65 batch-65 = 4.607252776622772e-06

Training epoch-65 batch-66
Running loss of epoch-65 batch-66 = 3.156019374728203e-06

Training epoch-65 batch-67
Running loss of epoch-65 batch-67 = 5.163485184311867e-06

Training epoch-65 batch-68
Running loss of epoch-65 batch-68 = 1.6351696103811264e-06

Training epoch-65 batch-69
Running loss of epoch-65 batch-69 = 6.69015571475029e-06

Training epoch-65 batch-70
Running loss of epoch-65 batch-70 = 5.051260814070702e-06

Training epoch-65 batch-71
Running loss of epoch-65 batch-71 = 6.114598363637924e-06

Training epoch-65 batch-72
Running loss of epoch-65 batch-72 = 9.165378287434578e-06

Training epoch-65 batch-73
Running loss of epoch-65 batch-73 = 4.267087206244469e-06

Training epoch-65 batch-74
Running loss of epoch-65 batch-74 = 2.7599744498729706e-06

Training epoch-65 batch-75
Running loss of epoch-65 batch-75 = 6.527639925479889e-06

Training epoch-65 batch-76
Running loss of epoch-65 batch-76 = 3.041466698050499e-06

Training epoch-65 batch-77
Running loss of epoch-65 batch-77 = 3.32552008330822e-06

Training epoch-65 batch-78
Running loss of epoch-65 batch-78 = 5.683396011590958e-06

Training epoch-65 batch-79
Running loss of epoch-65 batch-79 = 3.9460137486457825e-06

Training epoch-65 batch-80
Running loss of epoch-65 batch-80 = 2.0668376237154007e-06

Training epoch-65 batch-81
Running loss of epoch-65 batch-81 = 2.341112121939659e-06

Training epoch-65 batch-82
Running loss of epoch-65 batch-82 = 5.4140109568834305e-06

Training epoch-65 batch-83
Running loss of epoch-65 batch-83 = 3.958819434046745e-06

Training epoch-65 batch-84
Running loss of epoch-65 batch-84 = 6.467103958129883e-06

Training epoch-65 batch-85
Running loss of epoch-65 batch-85 = 3.0531082302331924e-06

Training epoch-65 batch-86
Running loss of epoch-65 batch-86 = 5.72926364839077e-06

Training epoch-65 batch-87
Running loss of epoch-65 batch-87 = 5.355337634682655e-06

Training epoch-65 batch-88
Running loss of epoch-65 batch-88 = 1.9222497940063477e-06

Training epoch-65 batch-89
Running loss of epoch-65 batch-89 = 1.9632279872894287e-06

Training epoch-65 batch-90
Running loss of epoch-65 batch-90 = 5.0782691687345505e-06

Training epoch-65 batch-91
Running loss of epoch-65 batch-91 = 4.148110747337341e-06

Training epoch-65 batch-92
Running loss of epoch-65 batch-92 = 3.727385774254799e-06

Training epoch-65 batch-93
Running loss of epoch-65 batch-93 = 4.964414983987808e-06

Training epoch-65 batch-94
Running loss of epoch-65 batch-94 = 2.09687277674675e-06

Training epoch-65 batch-95
Running loss of epoch-65 batch-95 = 3.4854747354984283e-06

Training epoch-65 batch-96
Running loss of epoch-65 batch-96 = 3.6603305488824844e-06

Training epoch-65 batch-97
Running loss of epoch-65 batch-97 = 6.52228482067585e-06

Training epoch-65 batch-98
Running loss of epoch-65 batch-98 = 4.3502077460289e-06

Training epoch-65 batch-99
Running loss of epoch-65 batch-99 = 4.681292921304703e-06

Training epoch-65 batch-100
Running loss of epoch-65 batch-100 = 3.077322617173195e-06

Training epoch-65 batch-101
Running loss of epoch-65 batch-101 = 5.623558536171913e-06

Training epoch-65 batch-102
Running loss of epoch-65 batch-102 = 2.437736839056015e-06

Training epoch-65 batch-103
Running loss of epoch-65 batch-103 = 1.9827857613563538e-06

Training epoch-65 batch-104
Running loss of epoch-65 batch-104 = 3.5641714930534363e-06

Training epoch-65 batch-105
Running loss of epoch-65 batch-105 = 5.224253982305527e-06

Training epoch-65 batch-106
Running loss of epoch-65 batch-106 = 8.265487849712372e-06

Training epoch-65 batch-107
Running loss of epoch-65 batch-107 = 3.254041075706482e-06

Training epoch-65 batch-108
Running loss of epoch-65 batch-108 = 1.954147592186928e-06

Training epoch-65 batch-109
Running loss of epoch-65 batch-109 = 4.05777245759964e-06

Training epoch-65 batch-110
Running loss of epoch-65 batch-110 = 7.947906851768494e-06

Training epoch-65 batch-111
Running loss of epoch-65 batch-111 = 5.560228601098061e-06

Training epoch-65 batch-112
Running loss of epoch-65 batch-112 = 5.816691555082798e-06

Training epoch-65 batch-113
Running loss of epoch-65 batch-113 = 4.4282060116529465e-06

Training epoch-65 batch-114
Running loss of epoch-65 batch-114 = 3.7692952901124954e-06

Training epoch-65 batch-115
Running loss of epoch-65 batch-115 = 3.281049430370331e-06

Training epoch-65 batch-116
Running loss of epoch-65 batch-116 = 4.468020051717758e-06

Training epoch-65 batch-117
Running loss of epoch-65 batch-117 = 2.7301721274852753e-06

Training epoch-65 batch-118
Running loss of epoch-65 batch-118 = 3.0244700610637665e-06

Training epoch-65 batch-119
Running loss of epoch-65 batch-119 = 2.515967935323715e-06

Training epoch-65 batch-120
Running loss of epoch-65 batch-120 = 2.321787178516388e-06

Training epoch-65 batch-121
Running loss of epoch-65 batch-121 = 4.088273271918297e-06

Training epoch-65 batch-122
Running loss of epoch-65 batch-122 = 4.918081685900688e-06

Training epoch-65 batch-123
Running loss of epoch-65 batch-123 = 4.2496249079704285e-06

Training epoch-65 batch-124
Running loss of epoch-65 batch-124 = 4.598638042807579e-06

Training epoch-65 batch-125
Running loss of epoch-65 batch-125 = 5.967332981526852e-06

Training epoch-65 batch-126
Running loss of epoch-65 batch-126 = 3.536231815814972e-06

Training epoch-65 batch-127
Running loss of epoch-65 batch-127 = 4.658708348870277e-06

Training epoch-65 batch-128
Running loss of epoch-65 batch-128 = 5.6873541325330734e-06

Training epoch-65 batch-129
Running loss of epoch-65 batch-129 = 2.469867467880249e-06

Training epoch-65 batch-130
Running loss of epoch-65 batch-130 = 7.275491952896118e-06

Training epoch-65 batch-131
Running loss of epoch-65 batch-131 = 1.959037035703659e-06

Training epoch-65 batch-132
Running loss of epoch-65 batch-132 = 4.925299435853958e-06

Training epoch-65 batch-133
Running loss of epoch-65 batch-133 = 1.6978010535240173e-06

Training epoch-65 batch-134
Running loss of epoch-65 batch-134 = 6.485963240265846e-06

Training epoch-65 batch-135
Running loss of epoch-65 batch-135 = 2.299901098012924e-06

Training epoch-65 batch-136
Running loss of epoch-65 batch-136 = 2.3562461137771606e-06

Training epoch-65 batch-137
Running loss of epoch-65 batch-137 = 2.400251105427742e-06

Training epoch-65 batch-138
Running loss of epoch-65 batch-138 = 4.932284355163574e-06

Training epoch-65 batch-139
Running loss of epoch-65 batch-139 = 2.5564804673194885e-06

Training epoch-65 batch-140
Running loss of epoch-65 batch-140 = 1.8905848264694214e-06

Training epoch-65 batch-141
Running loss of epoch-65 batch-141 = 8.519738912582397e-06

Training epoch-65 batch-142
Running loss of epoch-65 batch-142 = 2.6568304747343063e-06

Training epoch-65 batch-143
Running loss of epoch-65 batch-143 = 4.895264282822609e-06

Training epoch-65 batch-144
Running loss of epoch-65 batch-144 = 2.216547727584839e-06

Training epoch-65 batch-145
Running loss of epoch-65 batch-145 = 4.560686647891998e-06

Training epoch-65 batch-146
Running loss of epoch-65 batch-146 = 5.038687959313393e-06

Training epoch-65 batch-147
Running loss of epoch-65 batch-147 = 2.810731530189514e-06

Training epoch-65 batch-148
Running loss of epoch-65 batch-148 = 3.2517127692699432e-06

Training epoch-65 batch-149
Running loss of epoch-65 batch-149 = 1.780386082828045e-05

Training epoch-65 batch-150
Running loss of epoch-65 batch-150 = 3.976980224251747e-06

Training epoch-65 batch-151
Running loss of epoch-65 batch-151 = 3.142980858683586e-06

Training epoch-65 batch-152
Running loss of epoch-65 batch-152 = 5.627283826470375e-06

Training epoch-65 batch-153
Running loss of epoch-65 batch-153 = 5.3693074733018875e-06

Training epoch-65 batch-154
Running loss of epoch-65 batch-154 = 6.317859515547752e-06

Training epoch-65 batch-155
Running loss of epoch-65 batch-155 = 7.4321869760751724e-06

Training epoch-65 batch-156
Running loss of epoch-65 batch-156 = 2.757296897470951e-06

Training epoch-65 batch-157
Running loss of epoch-65 batch-157 = 9.644776582717896e-06

Finished training epoch-65.



Average train loss at epoch-65 = 4.5080728828907015e-06

Started Evaluation

Average val loss at epoch-65 = 1.139372826942367

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.06 %

Finished Evaluation



Started training epoch-66


Training epoch-66 batch-1
Running loss of epoch-66 batch-1 = 3.6514829844236374e-06

Training epoch-66 batch-2
Running loss of epoch-66 batch-2 = 1.5299301594495773e-06

Training epoch-66 batch-3
Running loss of epoch-66 batch-3 = 3.141583874821663e-06

Training epoch-66 batch-4
Running loss of epoch-66 batch-4 = 2.925051376223564e-06

Training epoch-66 batch-5
Running loss of epoch-66 batch-5 = 4.789093509316444e-06

Training epoch-66 batch-6
Running loss of epoch-66 batch-6 = 2.678483724594116e-06

Training epoch-66 batch-7
Running loss of epoch-66 batch-7 = 4.186294972896576e-06

Training epoch-66 batch-8
Running loss of epoch-66 batch-8 = 4.609348252415657e-06

Training epoch-66 batch-9
Running loss of epoch-66 batch-9 = 5.301320925354958e-06

Training epoch-66 batch-10
Running loss of epoch-66 batch-10 = 2.428889274597168e-06

Training epoch-66 batch-11
Running loss of epoch-66 batch-11 = 2.2926833480596542e-06

Training epoch-66 batch-12
Running loss of epoch-66 batch-12 = 3.707362338900566e-06

Training epoch-66 batch-13
Running loss of epoch-66 batch-13 = 5.157198756933212e-06

Training epoch-66 batch-14
Running loss of epoch-66 batch-14 = 2.4158507585525513e-06

Training epoch-66 batch-15
Running loss of epoch-66 batch-15 = 2.609565854072571e-06

Training epoch-66 batch-16
Running loss of epoch-66 batch-16 = 3.1352974474430084e-06

Training epoch-66 batch-17
Running loss of epoch-66 batch-17 = 2.423301339149475e-06

Training epoch-66 batch-18
Running loss of epoch-66 batch-18 = 4.615867510437965e-06

Training epoch-66 batch-19
Running loss of epoch-66 batch-19 = 6.118090823292732e-06

Training epoch-66 batch-20
Running loss of epoch-66 batch-20 = 7.319031283259392e-06

Training epoch-66 batch-21
Running loss of epoch-66 batch-21 = 3.2195821404457092e-06

Training epoch-66 batch-22
Running loss of epoch-66 batch-22 = 6.3604675233364105e-06

Training epoch-66 batch-23
Running loss of epoch-66 batch-23 = 3.877095878124237e-06

Training epoch-66 batch-24
Running loss of epoch-66 batch-24 = 5.201669409871101e-06

Training epoch-66 batch-25
Running loss of epoch-66 batch-25 = 4.992354661226273e-06

Training epoch-66 batch-26
Running loss of epoch-66 batch-26 = 1.930398866534233e-06

Training epoch-66 batch-27
Running loss of epoch-66 batch-27 = 4.125991836190224e-06

Training epoch-66 batch-28
Running loss of epoch-66 batch-28 = 4.428671672940254e-06

Training epoch-66 batch-29
Running loss of epoch-66 batch-29 = 2.0193401724100113e-06

Training epoch-66 batch-30
Running loss of epoch-66 batch-30 = 4.948582500219345e-06

Training epoch-66 batch-31
Running loss of epoch-66 batch-31 = 2.400251105427742e-06

Training epoch-66 batch-32
Running loss of epoch-66 batch-32 = 2.956017851829529e-06

Training epoch-66 batch-33
Running loss of epoch-66 batch-33 = 4.630303010344505e-06

Training epoch-66 batch-34
Running loss of epoch-66 batch-34 = 3.7564896047115326e-06

Training epoch-66 batch-35
Running loss of epoch-66 batch-35 = 4.9655791372060776e-06

Training epoch-66 batch-36
Running loss of epoch-66 batch-36 = 6.807269528508186e-06

Training epoch-66 batch-37
Running loss of epoch-66 batch-37 = 2.421438694000244e-06

Training epoch-66 batch-38
Running loss of epoch-66 batch-38 = 5.5746641010046005e-06

Training epoch-66 batch-39
Running loss of epoch-66 batch-39 = 4.356959834694862e-06

Training epoch-66 batch-40
Running loss of epoch-66 batch-40 = 4.239380359649658e-06

Training epoch-66 batch-41
Running loss of epoch-66 batch-41 = 5.02169132232666e-06

Training epoch-66 batch-42
Running loss of epoch-66 batch-42 = 3.573484718799591e-06

Training epoch-66 batch-43
Running loss of epoch-66 batch-43 = 3.0617229640483856e-06

Training epoch-66 batch-44
Running loss of epoch-66 batch-44 = 3.72203066945076e-06

Training epoch-66 batch-45
Running loss of epoch-66 batch-45 = 6.5283384174108505e-06

Training epoch-66 batch-46
Running loss of epoch-66 batch-46 = 5.514360964298248e-06

Training epoch-66 batch-47
Running loss of epoch-66 batch-47 = 4.540197551250458e-06

Training epoch-66 batch-48
Running loss of epoch-66 batch-48 = 3.911089152097702e-06

Training epoch-66 batch-49
Running loss of epoch-66 batch-49 = 2.1299347281455994e-06

Training epoch-66 batch-50
Running loss of epoch-66 batch-50 = 1.5350524336099625e-06

Training epoch-66 batch-51
Running loss of epoch-66 batch-51 = 2.861255779862404e-06

Training epoch-66 batch-52
Running loss of epoch-66 batch-52 = 3.6873389035463333e-06

Training epoch-66 batch-53
Running loss of epoch-66 batch-53 = 4.0996819734573364e-06

Training epoch-66 batch-54
Running loss of epoch-66 batch-54 = 3.0668452382087708e-06

Training epoch-66 batch-55
Running loss of epoch-66 batch-55 = 2.7632340788841248e-06

Training epoch-66 batch-56
Running loss of epoch-66 batch-56 = 3.284192644059658e-06

Training epoch-66 batch-57
Running loss of epoch-66 batch-57 = 6.221933290362358e-06

Training epoch-66 batch-58
Running loss of epoch-66 batch-58 = 6.638234481215477e-06

Training epoch-66 batch-59
Running loss of epoch-66 batch-59 = 5.775829777121544e-06

Training epoch-66 batch-60
Running loss of epoch-66 batch-60 = 4.1215680539608e-06

Training epoch-66 batch-61
Running loss of epoch-66 batch-61 = 5.103647708892822e-06

Training epoch-66 batch-62
Running loss of epoch-66 batch-62 = 5.244743078947067e-06

Training epoch-66 batch-63
Running loss of epoch-66 batch-63 = 3.3869873732328415e-06

Training epoch-66 batch-64
Running loss of epoch-66 batch-64 = 6.639398634433746e-06

Training epoch-66 batch-65
Running loss of epoch-66 batch-65 = 1.505902037024498e-05

Training epoch-66 batch-66
Running loss of epoch-66 batch-66 = 6.879214197397232e-06

Training epoch-66 batch-67
Running loss of epoch-66 batch-67 = 2.977205440402031e-06

Training epoch-66 batch-68
Running loss of epoch-66 batch-68 = 3.2098032534122467e-06

Training epoch-66 batch-69
Running loss of epoch-66 batch-69 = 4.754052497446537e-06

Training epoch-66 batch-70
Running loss of epoch-66 batch-70 = 6.971065886318684e-06

Training epoch-66 batch-71
Running loss of epoch-66 batch-71 = 7.942551746964455e-06

Training epoch-66 batch-72
Running loss of epoch-66 batch-72 = 3.514811396598816e-06

Training epoch-66 batch-73
Running loss of epoch-66 batch-73 = 2.301996573805809e-06

Training epoch-66 batch-74
Running loss of epoch-66 batch-74 = 5.8854930102825165e-06

Training epoch-66 batch-75
Running loss of epoch-66 batch-75 = 1.2248056009411812e-05

Training epoch-66 batch-76
Running loss of epoch-66 batch-76 = 2.132495865225792e-06

Training epoch-66 batch-77
Running loss of epoch-66 batch-77 = 3.0712690204381943e-06

Training epoch-66 batch-78
Running loss of epoch-66 batch-78 = 4.141358658671379e-06

Training epoch-66 batch-79
Running loss of epoch-66 batch-79 = 3.854511305689812e-06

Training epoch-66 batch-80
Running loss of epoch-66 batch-80 = 3.5509001463651657e-06

Training epoch-66 batch-81
Running loss of epoch-66 batch-81 = 4.505971446633339e-06

Training epoch-66 batch-82
Running loss of epoch-66 batch-82 = 5.873618647456169e-06

Training epoch-66 batch-83
Running loss of epoch-66 batch-83 = 3.198627382516861e-06

Training epoch-66 batch-84
Running loss of epoch-66 batch-84 = 5.807727575302124e-06

Training epoch-66 batch-85
Running loss of epoch-66 batch-85 = 2.628657966852188e-06

Training epoch-66 batch-86
Running loss of epoch-66 batch-86 = 6.0158781707286835e-06

Training epoch-66 batch-87
Running loss of epoch-66 batch-87 = 6.3357874751091e-06

Training epoch-66 batch-88
Running loss of epoch-66 batch-88 = 4.747183993458748e-06

Training epoch-66 batch-89
Running loss of epoch-66 batch-89 = 4.5869965106248856e-06

Training epoch-66 batch-90
Running loss of epoch-66 batch-90 = 3.579305484890938e-06

Training epoch-66 batch-91
Running loss of epoch-66 batch-91 = 2.9115471988916397e-06

Training epoch-66 batch-92
Running loss of epoch-66 batch-92 = 2.5157351046800613e-06

Training epoch-66 batch-93
Running loss of epoch-66 batch-93 = 2.323184162378311e-06

Training epoch-66 batch-94
Running loss of epoch-66 batch-94 = 3.952533006668091e-06

Training epoch-66 batch-95
Running loss of epoch-66 batch-95 = 5.256384611129761e-06

Training epoch-66 batch-96
Running loss of epoch-66 batch-96 = 6.692484021186829e-06

Training epoch-66 batch-97
Running loss of epoch-66 batch-97 = 4.507601261138916e-06

Training epoch-66 batch-98
Running loss of epoch-66 batch-98 = 4.233792424201965e-06

Training epoch-66 batch-99
Running loss of epoch-66 batch-99 = 2.080574631690979e-06

Training epoch-66 batch-100
Running loss of epoch-66 batch-100 = 6.105750799179077e-06

Training epoch-66 batch-101
Running loss of epoch-66 batch-101 = 4.542991518974304e-06

Training epoch-66 batch-102
Running loss of epoch-66 batch-102 = 2.9045622795820236e-06

Training epoch-66 batch-103
Running loss of epoch-66 batch-103 = 5.701091140508652e-06

Training epoch-66 batch-104
Running loss of epoch-66 batch-104 = 7.031019777059555e-06

Training epoch-66 batch-105
Running loss of epoch-66 batch-105 = 4.5709311962127686e-06

Training epoch-66 batch-106
Running loss of epoch-66 batch-106 = 3.1874515116214752e-06

Training epoch-66 batch-107
Running loss of epoch-66 batch-107 = 1.519685611128807e-06

Training epoch-66 batch-108
Running loss of epoch-66 batch-108 = 2.521555870771408e-06

Training epoch-66 batch-109
Running loss of epoch-66 batch-109 = 2.484535798430443e-06

Training epoch-66 batch-110
Running loss of epoch-66 batch-110 = 3.6838464438915253e-06

Training epoch-66 batch-111
Running loss of epoch-66 batch-111 = 1.8514692783355713e-06

Training epoch-66 batch-112
Running loss of epoch-66 batch-112 = 7.692025974392891e-06

Training epoch-66 batch-113
Running loss of epoch-66 batch-113 = 4.139728844165802e-06

Training epoch-66 batch-114
Running loss of epoch-66 batch-114 = 5.587935447692871e-06

Training epoch-66 batch-115
Running loss of epoch-66 batch-115 = 3.5797711461782455e-06

Training epoch-66 batch-116
Running loss of epoch-66 batch-116 = 2.9795337468385696e-06

Training epoch-66 batch-117
Running loss of epoch-66 batch-117 = 4.178844392299652e-06

Training epoch-66 batch-118
Running loss of epoch-66 batch-118 = 6.45429827272892e-06

Training epoch-66 batch-119
Running loss of epoch-66 batch-119 = 2.8836075216531754e-06

Training epoch-66 batch-120
Running loss of epoch-66 batch-120 = 8.530216291546822e-06

Training epoch-66 batch-121
Running loss of epoch-66 batch-121 = 2.3695174604654312e-06

Training epoch-66 batch-122
Running loss of epoch-66 batch-122 = 1.991167664527893e-06

Training epoch-66 batch-123
Running loss of epoch-66 batch-123 = 1.032371073961258e-06

Training epoch-66 batch-124
Running loss of epoch-66 batch-124 = 2.4819746613502502e-06

Training epoch-66 batch-125
Running loss of epoch-66 batch-125 = 6.746500730514526e-06

Training epoch-66 batch-126
Running loss of epoch-66 batch-126 = 5.336012691259384e-06

Training epoch-66 batch-127
Running loss of epoch-66 batch-127 = 3.457535058259964e-06

Training epoch-66 batch-128
Running loss of epoch-66 batch-128 = 2.504093572497368e-06

Training epoch-66 batch-129
Running loss of epoch-66 batch-129 = 2.1513551473617554e-06

Training epoch-66 batch-130
Running loss of epoch-66 batch-130 = 5.008885636925697e-06

Training epoch-66 batch-131
Running loss of epoch-66 batch-131 = 5.074311047792435e-06

Training epoch-66 batch-132
Running loss of epoch-66 batch-132 = 2.4836044758558273e-06

Training epoch-66 batch-133
Running loss of epoch-66 batch-133 = 3.1134113669395447e-06

Training epoch-66 batch-134
Running loss of epoch-66 batch-134 = 8.750008419156075e-06

Training epoch-66 batch-135
Running loss of epoch-66 batch-135 = 8.097849786281586e-06

Training epoch-66 batch-136
Running loss of epoch-66 batch-136 = 3.0403025448322296e-06

Training epoch-66 batch-137
Running loss of epoch-66 batch-137 = 7.75209628045559e-06

Training epoch-66 batch-138
Running loss of epoch-66 batch-138 = 2.3867469280958176e-06

Training epoch-66 batch-139
Running loss of epoch-66 batch-139 = 9.627779945731163e-06

Training epoch-66 batch-140
Running loss of epoch-66 batch-140 = 5.896901711821556e-06

Training epoch-66 batch-141
Running loss of epoch-66 batch-141 = 2.4812761694192886e-06

Training epoch-66 batch-142
Running loss of epoch-66 batch-142 = 7.504597306251526e-06

Training epoch-66 batch-143
Running loss of epoch-66 batch-143 = 2.4177134037017822e-06

Training epoch-66 batch-144
Running loss of epoch-66 batch-144 = 2.777436748147011e-06

Training epoch-66 batch-145
Running loss of epoch-66 batch-145 = 5.255453288555145e-06

Training epoch-66 batch-146
Running loss of epoch-66 batch-146 = 2.8477516025304794e-06

Training epoch-66 batch-147
Running loss of epoch-66 batch-147 = 4.373490810394287e-06

Training epoch-66 batch-148
Running loss of epoch-66 batch-148 = 2.6584602892398834e-06

Training epoch-66 batch-149
Running loss of epoch-66 batch-149 = 4.145549610257149e-06

Training epoch-66 batch-150
Running loss of epoch-66 batch-150 = 1.3149343430995941e-05

Training epoch-66 batch-151
Running loss of epoch-66 batch-151 = 1.7932616174221039e-06

Training epoch-66 batch-152
Running loss of epoch-66 batch-152 = 1.528766006231308e-06

Training epoch-66 batch-153
Running loss of epoch-66 batch-153 = 9.171664714813232e-06

Training epoch-66 batch-154
Running loss of epoch-66 batch-154 = 5.288282409310341e-06

Training epoch-66 batch-155
Running loss of epoch-66 batch-155 = 2.3210886865854263e-06

Training epoch-66 batch-156
Running loss of epoch-66 batch-156 = 2.784421667456627e-06

Training epoch-66 batch-157
Running loss of epoch-66 batch-157 = 1.2025237083435059e-05

Finished training epoch-66.



Average train loss at epoch-66 = 4.389546066522598e-06

Started Evaluation

Average val loss at epoch-66 = 1.1476333956695366

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 52.47 %
Accuracy for class execute is: 44.18 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.04 %

Finished Evaluation



Started training epoch-67


Training epoch-67 batch-1
Running loss of epoch-67 batch-1 = 8.346978574991226e-06

Training epoch-67 batch-2
Running loss of epoch-67 batch-2 = 3.6996789276599884e-06

Training epoch-67 batch-3
Running loss of epoch-67 batch-3 = 3.502471372485161e-06

Training epoch-67 batch-4
Running loss of epoch-67 batch-4 = 4.27919439971447e-06

Training epoch-67 batch-5
Running loss of epoch-67 batch-5 = 4.562316462397575e-06

Training epoch-67 batch-6
Running loss of epoch-67 batch-6 = 3.0673108994960785e-06

Training epoch-67 batch-7
Running loss of epoch-67 batch-7 = 2.8838403522968292e-06

Training epoch-67 batch-8
Running loss of epoch-67 batch-8 = 3.677094355225563e-06

Training epoch-67 batch-9
Running loss of epoch-67 batch-9 = 3.680819645524025e-06

Training epoch-67 batch-10
Running loss of epoch-67 batch-10 = 3.989087417721748e-06

Training epoch-67 batch-11
Running loss of epoch-67 batch-11 = 4.483852535486221e-06

Training epoch-67 batch-12
Running loss of epoch-67 batch-12 = 1.9404105842113495e-06

Training epoch-67 batch-13
Running loss of epoch-67 batch-13 = 5.120877176523209e-06

Training epoch-67 batch-14
Running loss of epoch-67 batch-14 = 4.534376785159111e-06

Training epoch-67 batch-15
Running loss of epoch-67 batch-15 = 5.205627530813217e-06

Training epoch-67 batch-16
Running loss of epoch-67 batch-16 = 3.2677780836820602e-06

Training epoch-67 batch-17
Running loss of epoch-67 batch-17 = 2.533895894885063e-06

Training epoch-67 batch-18
Running loss of epoch-67 batch-18 = 2.971617504954338e-06

Training epoch-67 batch-19
Running loss of epoch-67 batch-19 = 4.220055416226387e-06

Training epoch-67 batch-20
Running loss of epoch-67 batch-20 = 5.19677996635437e-06

Training epoch-67 batch-21
Running loss of epoch-67 batch-21 = 3.933906555175781e-06

Training epoch-67 batch-22
Running loss of epoch-67 batch-22 = 6.308313459157944e-06

Training epoch-67 batch-23
Running loss of epoch-67 batch-23 = 4.828907549381256e-06

Training epoch-67 batch-24
Running loss of epoch-67 batch-24 = 4.116678610444069e-06

Training epoch-67 batch-25
Running loss of epoch-67 batch-25 = 3.1685922294855118e-06

Training epoch-67 batch-26
Running loss of epoch-67 batch-26 = 8.76991543918848e-06

Training epoch-67 batch-27
Running loss of epoch-67 batch-27 = 3.650318831205368e-06

Training epoch-67 batch-28
Running loss of epoch-67 batch-28 = 4.739267751574516e-06

Training epoch-67 batch-29
Running loss of epoch-67 batch-29 = 3.93204391002655e-06

Training epoch-67 batch-30
Running loss of epoch-67 batch-30 = 1.9865110516548157e-06

Training epoch-67 batch-31
Running loss of epoch-67 batch-31 = 8.232193067669868e-06

Training epoch-67 batch-32
Running loss of epoch-67 batch-32 = 4.2531173676252365e-06

Training epoch-67 batch-33
Running loss of epoch-67 batch-33 = 3.1192321330308914e-06

Training epoch-67 batch-34
Running loss of epoch-67 batch-34 = 7.997266948223114e-06

Training epoch-67 batch-35
Running loss of epoch-67 batch-35 = 3.933673724532127e-06

Training epoch-67 batch-36
Running loss of epoch-67 batch-36 = 2.2726599127054214e-06

Training epoch-67 batch-37
Running loss of epoch-67 batch-37 = 3.915745764970779e-06

Training epoch-67 batch-38
Running loss of epoch-67 batch-38 = 7.956521585583687e-06

Training epoch-67 batch-39
Running loss of epoch-67 batch-39 = 2.5962945073843002e-06

Training epoch-67 batch-40
Running loss of epoch-67 batch-40 = 2.78581865131855e-06

Training epoch-67 batch-41
Running loss of epoch-67 batch-41 = 2.1476298570632935e-06

Training epoch-67 batch-42
Running loss of epoch-67 batch-42 = 1.882202923297882e-06

Training epoch-67 batch-43
Running loss of epoch-67 batch-43 = 6.975140422582626e-06

Training epoch-67 batch-44
Running loss of epoch-67 batch-44 = 4.221219569444656e-06

Training epoch-67 batch-45
Running loss of epoch-67 batch-45 = 2.289656549692154e-06

Training epoch-67 batch-46
Running loss of epoch-67 batch-46 = 3.90736386179924e-06

Training epoch-67 batch-47
Running loss of epoch-67 batch-47 = 1.3225479051470757e-05

Training epoch-67 batch-48
Running loss of epoch-67 batch-48 = 1.4624092727899551e-06

Training epoch-67 batch-49
Running loss of epoch-67 batch-49 = 6.7157670855522156e-06

Training epoch-67 batch-50
Running loss of epoch-67 batch-50 = 4.476169124245644e-06

Training epoch-67 batch-51
Running loss of epoch-67 batch-51 = 2.4512410163879395e-06

Training epoch-67 batch-52
Running loss of epoch-67 batch-52 = 4.979316145181656e-06

Training epoch-67 batch-53
Running loss of epoch-67 batch-53 = 3.6316923797130585e-06

Training epoch-67 batch-54
Running loss of epoch-67 batch-54 = 2.8794165700674057e-06

Training epoch-67 batch-55
Running loss of epoch-67 batch-55 = 3.2691750675439835e-06

Training epoch-67 batch-56
Running loss of epoch-67 batch-56 = 4.906440153717995e-06

Training epoch-67 batch-57
Running loss of epoch-67 batch-57 = 6.642192602157593e-06

Training epoch-67 batch-58
Running loss of epoch-67 batch-58 = 1.9010622054338455e-06

Training epoch-67 batch-59
Running loss of epoch-67 batch-59 = 6.556743755936623e-06

Training epoch-67 batch-60
Running loss of epoch-67 batch-60 = 3.718072548508644e-06

Training epoch-67 batch-61
Running loss of epoch-67 batch-61 = 1.9082799553871155e-06

Training epoch-67 batch-62
Running loss of epoch-67 batch-62 = 3.26475128531456e-06

Training epoch-67 batch-63
Running loss of epoch-67 batch-63 = 8.458271622657776e-06

Training epoch-67 batch-64
Running loss of epoch-67 batch-64 = 2.0300503820180893e-06

Training epoch-67 batch-65
Running loss of epoch-67 batch-65 = 3.928551450371742e-06

Training epoch-67 batch-66
Running loss of epoch-67 batch-66 = 4.532281309366226e-06

Training epoch-67 batch-67
Running loss of epoch-67 batch-67 = 5.88269904255867e-06

Training epoch-67 batch-68
Running loss of epoch-67 batch-68 = 5.96209429204464e-06

Training epoch-67 batch-69
Running loss of epoch-67 batch-69 = 4.224013537168503e-06

Training epoch-67 batch-70
Running loss of epoch-67 batch-70 = 4.173256456851959e-06

Training epoch-67 batch-71
Running loss of epoch-67 batch-71 = 3.2188836485147476e-06

Training epoch-67 batch-72
Running loss of epoch-67 batch-72 = 6.381422281265259e-06

Training epoch-67 batch-73
Running loss of epoch-67 batch-73 = 3.1706877052783966e-06

Training epoch-67 batch-74
Running loss of epoch-67 batch-74 = 3.914115950465202e-06

Training epoch-67 batch-75
Running loss of epoch-67 batch-75 = 2.0363368093967438e-06

Training epoch-67 batch-76
Running loss of epoch-67 batch-76 = 2.1078158169984818e-06

Training epoch-67 batch-77
Running loss of epoch-67 batch-77 = 2.543441951274872e-06

Training epoch-67 batch-78
Running loss of epoch-67 batch-78 = 2.330634742975235e-06

Training epoch-67 batch-79
Running loss of epoch-67 batch-79 = 5.6549906730651855e-06

Training epoch-67 batch-80
Running loss of epoch-67 batch-80 = 2.2239983081817627e-06

Training epoch-67 batch-81
Running loss of epoch-67 batch-81 = 4.0603335946798325e-06

Training epoch-67 batch-82
Running loss of epoch-67 batch-82 = 2.7837231755256653e-06

Training epoch-67 batch-83
Running loss of epoch-67 batch-83 = 7.523456588387489e-06

Training epoch-67 batch-84
Running loss of epoch-67 batch-84 = 1.0026386007666588e-05

Training epoch-67 batch-85
Running loss of epoch-67 batch-85 = 3.5874545574188232e-06

Training epoch-67 batch-86
Running loss of epoch-67 batch-86 = 4.976987838745117e-06

Training epoch-67 batch-87
Running loss of epoch-67 batch-87 = 2.57883220911026e-06

Training epoch-67 batch-88
Running loss of epoch-67 batch-88 = 4.506204277276993e-06

Training epoch-67 batch-89
Running loss of epoch-67 batch-89 = 3.9478763937950134e-06

Training epoch-67 batch-90
Running loss of epoch-67 batch-90 = 4.832400009036064e-06

Training epoch-67 batch-91
Running loss of epoch-67 batch-91 = 5.709938704967499e-06

Training epoch-67 batch-92
Running loss of epoch-67 batch-92 = 1.5080440789461136e-06

Training epoch-67 batch-93
Running loss of epoch-67 batch-93 = 1.0472722351551056e-05

Training epoch-67 batch-94
Running loss of epoch-67 batch-94 = 3.541354089975357e-06

Training epoch-67 batch-95
Running loss of epoch-67 batch-95 = 2.4940818548202515e-06

Training epoch-67 batch-96
Running loss of epoch-67 batch-96 = 1.6605481505393982e-06

Training epoch-67 batch-97
Running loss of epoch-67 batch-97 = 3.464752808213234e-06

Training epoch-67 batch-98
Running loss of epoch-67 batch-98 = 4.427973181009293e-06

Training epoch-67 batch-99
Running loss of epoch-67 batch-99 = 7.032183930277824e-06

Training epoch-67 batch-100
Running loss of epoch-67 batch-100 = 3.627035766839981e-06

Training epoch-67 batch-101
Running loss of epoch-67 batch-101 = 2.387911081314087e-06

Training epoch-67 batch-102
Running loss of epoch-67 batch-102 = 2.5171320885419846e-06

Training epoch-67 batch-103
Running loss of epoch-67 batch-103 = 4.09269705414772e-06

Training epoch-67 batch-104
Running loss of epoch-67 batch-104 = 1.1078082025051117e-06

Training epoch-67 batch-105
Running loss of epoch-67 batch-105 = 2.3150350898504257e-06

Training epoch-67 batch-106
Running loss of epoch-67 batch-106 = 1.7082784324884415e-06

Training epoch-67 batch-107
Running loss of epoch-67 batch-107 = 4.441710188984871e-06

Training epoch-67 batch-108
Running loss of epoch-67 batch-108 = 3.132270649075508e-06

Training epoch-67 batch-109
Running loss of epoch-67 batch-109 = 1.507345587015152e-06

Training epoch-67 batch-110
Running loss of epoch-67 batch-110 = 4.489906132221222e-06

Training epoch-67 batch-111
Running loss of epoch-67 batch-111 = 1.9685830920934677e-06

Training epoch-67 batch-112
Running loss of epoch-67 batch-112 = 6.865477189421654e-06

Training epoch-67 batch-113
Running loss of epoch-67 batch-113 = 6.678048521280289e-06

Training epoch-67 batch-114
Running loss of epoch-67 batch-114 = 6.117392331361771e-06

Training epoch-67 batch-115
Running loss of epoch-67 batch-115 = 2.9995571821928024e-06

Training epoch-67 batch-116
Running loss of epoch-67 batch-116 = 5.15044666826725e-06

Training epoch-67 batch-117
Running loss of epoch-67 batch-117 = 2.834480255842209e-06

Training epoch-67 batch-118
Running loss of epoch-67 batch-118 = 4.446832463145256e-06

Training epoch-67 batch-119
Running loss of epoch-67 batch-119 = 9.550247341394424e-06

Training epoch-67 batch-120
Running loss of epoch-67 batch-120 = 5.109235644340515e-06

Training epoch-67 batch-121
Running loss of epoch-67 batch-121 = 1.93924643099308e-06

Training epoch-67 batch-122
Running loss of epoch-67 batch-122 = 5.258247256278992e-06

Training epoch-67 batch-123
Running loss of epoch-67 batch-123 = 1.3582408428192139e-05

Training epoch-67 batch-124
Running loss of epoch-67 batch-124 = 3.847293555736542e-06

Training epoch-67 batch-125
Running loss of epoch-67 batch-125 = 1.6619451344013214e-06

Training epoch-67 batch-126
Running loss of epoch-67 batch-126 = 8.142087608575821e-06

Training epoch-67 batch-127
Running loss of epoch-67 batch-127 = 6.191432476043701e-06

Training epoch-67 batch-128
Running loss of epoch-67 batch-128 = 3.6205165088176727e-06

Training epoch-67 batch-129
Running loss of epoch-67 batch-129 = 1.6666017472743988e-06

Training epoch-67 batch-130
Running loss of epoch-67 batch-130 = 1.6745179891586304e-06

Training epoch-67 batch-131
Running loss of epoch-67 batch-131 = 2.8908252716064453e-06

Training epoch-67 batch-132
Running loss of epoch-67 batch-132 = 3.044726327061653e-06

Training epoch-67 batch-133
Running loss of epoch-67 batch-133 = 3.7620775401592255e-06

Training epoch-67 batch-134
Running loss of epoch-67 batch-134 = 1.2440141290426254e-06

Training epoch-67 batch-135
Running loss of epoch-67 batch-135 = 6.891787052154541e-06

Training epoch-67 batch-136
Running loss of epoch-67 batch-136 = 3.396999090909958e-06

Training epoch-67 batch-137
Running loss of epoch-67 batch-137 = 3.948574885725975e-06

Training epoch-67 batch-138
Running loss of epoch-67 batch-138 = 5.305977538228035e-06

Training epoch-67 batch-139
Running loss of epoch-67 batch-139 = 3.427034243941307e-06

Training epoch-67 batch-140
Running loss of epoch-67 batch-140 = 2.47173011302948e-06

Training epoch-67 batch-141
Running loss of epoch-67 batch-141 = 1.889653503894806e-06

Training epoch-67 batch-142
Running loss of epoch-67 batch-142 = 8.011236786842346e-06

Training epoch-67 batch-143
Running loss of epoch-67 batch-143 = 6.177462637424469e-06

Training epoch-67 batch-144
Running loss of epoch-67 batch-144 = 2.8383219614624977e-06

Training epoch-67 batch-145
Running loss of epoch-67 batch-145 = 5.111563950777054e-06

Training epoch-67 batch-146
Running loss of epoch-67 batch-146 = 2.084067091345787e-06

Training epoch-67 batch-147
Running loss of epoch-67 batch-147 = 5.32856211066246e-06

Training epoch-67 batch-148
Running loss of epoch-67 batch-148 = 9.52230766415596e-06

Training epoch-67 batch-149
Running loss of epoch-67 batch-149 = 2.3958273231983185e-06

Training epoch-67 batch-150
Running loss of epoch-67 batch-150 = 5.984213203191757e-06

Training epoch-67 batch-151
Running loss of epoch-67 batch-151 = 7.621943950653076e-06

Training epoch-67 batch-152
Running loss of epoch-67 batch-152 = 2.5725457817316055e-06

Training epoch-67 batch-153
Running loss of epoch-67 batch-153 = 1.623528078198433e-06

Training epoch-67 batch-154
Running loss of epoch-67 batch-154 = 3.7264544516801834e-06

Training epoch-67 batch-155
Running loss of epoch-67 batch-155 = 3.2687094062566757e-06

Training epoch-67 batch-156
Running loss of epoch-67 batch-156 = 6.121350452303886e-06

Training epoch-67 batch-157
Running loss of epoch-67 batch-157 = 1.4081597328186035e-06

Finished training epoch-67.



Average train loss at epoch-67 = 4.292009770870209e-06

Started Evaluation

Average val loss at epoch-67 = 1.1455219724225871

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 61.87 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.06 %

Finished Evaluation



Started training epoch-68


Training epoch-68 batch-1
Running loss of epoch-68 batch-1 = 2.7082860469818115e-06

Training epoch-68 batch-2
Running loss of epoch-68 batch-2 = 2.7420464903116226e-06

Training epoch-68 batch-3
Running loss of epoch-68 batch-3 = 2.673594281077385e-06

Training epoch-68 batch-4
Running loss of epoch-68 batch-4 = 3.1711533665657043e-06

Training epoch-68 batch-5
Running loss of epoch-68 batch-5 = 4.943227395415306e-06

Training epoch-68 batch-6
Running loss of epoch-68 batch-6 = 5.651963874697685e-06

Training epoch-68 batch-7
Running loss of epoch-68 batch-7 = 1.7273705452680588e-06

Training epoch-68 batch-8
Running loss of epoch-68 batch-8 = 4.691537469625473e-06

Training epoch-68 batch-9
Running loss of epoch-68 batch-9 = 3.359280526638031e-06

Training epoch-68 batch-10
Running loss of epoch-68 batch-10 = 4.9585942178964615e-06

Training epoch-68 batch-11
Running loss of epoch-68 batch-11 = 4.8074871301651e-06

Training epoch-68 batch-12
Running loss of epoch-68 batch-12 = 4.0619634091854095e-06

Training epoch-68 batch-13
Running loss of epoch-68 batch-13 = 1.920154318213463e-06

Training epoch-68 batch-14
Running loss of epoch-68 batch-14 = 4.443572834134102e-06

Training epoch-68 batch-15
Running loss of epoch-68 batch-15 = 4.30690124630928e-06

Training epoch-68 batch-16
Running loss of epoch-68 batch-16 = 2.932734787464142e-06

Training epoch-68 batch-17
Running loss of epoch-68 batch-17 = 4.251720383763313e-06

Training epoch-68 batch-18
Running loss of epoch-68 batch-18 = 4.015397280454636e-06

Training epoch-68 batch-19
Running loss of epoch-68 batch-19 = 8.4657222032547e-07

Training epoch-68 batch-20
Running loss of epoch-68 batch-20 = 2.2244639694690704e-06

Training epoch-68 batch-21
Running loss of epoch-68 batch-21 = 4.720874130725861e-06

Training epoch-68 batch-22
Running loss of epoch-68 batch-22 = 2.9117800295352936e-06

Training epoch-68 batch-23
Running loss of epoch-68 batch-23 = 4.509929567575455e-06

Training epoch-68 batch-24
Running loss of epoch-68 batch-24 = 6.095739081501961e-06

Training epoch-68 batch-25
Running loss of epoch-68 batch-25 = 1.1239200830459595e-05

Training epoch-68 batch-26
Running loss of epoch-68 batch-26 = 4.681060090661049e-06

Training epoch-68 batch-27
Running loss of epoch-68 batch-27 = 5.21540641784668e-06

Training epoch-68 batch-28
Running loss of epoch-68 batch-28 = 3.7527643144130707e-06

Training epoch-68 batch-29
Running loss of epoch-68 batch-29 = 4.044501110911369e-06

Training epoch-68 batch-30
Running loss of epoch-68 batch-30 = 4.740082658827305e-06

Training epoch-68 batch-31
Running loss of epoch-68 batch-31 = 3.902940079569817e-06

Training epoch-68 batch-32
Running loss of epoch-68 batch-32 = 2.418411895632744e-06

Training epoch-68 batch-33
Running loss of epoch-68 batch-33 = 4.278495907783508e-06

Training epoch-68 batch-34
Running loss of epoch-68 batch-34 = 1.7134007066488266e-06

Training epoch-68 batch-35
Running loss of epoch-68 batch-35 = 6.187008693814278e-06

Training epoch-68 batch-36
Running loss of epoch-68 batch-36 = 7.095281034708023e-06

Training epoch-68 batch-37
Running loss of epoch-68 batch-37 = 3.2831449061632156e-06

Training epoch-68 batch-38
Running loss of epoch-68 batch-38 = 3.5106204450130463e-06

Training epoch-68 batch-39
Running loss of epoch-68 batch-39 = 5.9604644775390625e-06

Training epoch-68 batch-40
Running loss of epoch-68 batch-40 = 5.549285560846329e-06

Training epoch-68 batch-41
Running loss of epoch-68 batch-41 = 2.1418090909719467e-06

Training epoch-68 batch-42
Running loss of epoch-68 batch-42 = 8.913921192288399e-06

Training epoch-68 batch-43
Running loss of epoch-68 batch-43 = 1.02166086435318e-06

Training epoch-68 batch-44
Running loss of epoch-68 batch-44 = 5.503417924046516e-06

Training epoch-68 batch-45
Running loss of epoch-68 batch-45 = 3.411434590816498e-06

Training epoch-68 batch-46
Running loss of epoch-68 batch-46 = 7.333466783165932e-06

Training epoch-68 batch-47
Running loss of epoch-68 batch-47 = 2.0258594304323196e-06

Training epoch-68 batch-48
Running loss of epoch-68 batch-48 = 2.1385494619607925e-06

Training epoch-68 batch-49
Running loss of epoch-68 batch-49 = 1.937383785843849e-06

Training epoch-68 batch-50
Running loss of epoch-68 batch-50 = 5.492242053151131e-06

Training epoch-68 batch-51
Running loss of epoch-68 batch-51 = 4.7336798161268234e-06

Training epoch-68 batch-52
Running loss of epoch-68 batch-52 = 5.005160346627235e-06

Training epoch-68 batch-53
Running loss of epoch-68 batch-53 = 1.9667204469442368e-06

Training epoch-68 batch-54
Running loss of epoch-68 batch-54 = 4.612375050783157e-06

Training epoch-68 batch-55
Running loss of epoch-68 batch-55 = 1.8742866814136505e-06

Training epoch-68 batch-56
Running loss of epoch-68 batch-56 = 2.712476998567581e-06

Training epoch-68 batch-57
Running loss of epoch-68 batch-57 = 9.790528565645218e-07

Training epoch-68 batch-58
Running loss of epoch-68 batch-58 = 3.247056156396866e-06

Training epoch-68 batch-59
Running loss of epoch-68 batch-59 = 7.607275620102882e-06

Training epoch-68 batch-60
Running loss of epoch-68 batch-60 = 6.60470686852932e-06

Training epoch-68 batch-61
Running loss of epoch-68 batch-61 = 2.3655593395233154e-06

Training epoch-68 batch-62
Running loss of epoch-68 batch-62 = 4.00724820792675e-06

Training epoch-68 batch-63
Running loss of epoch-68 batch-63 = 1.9546132534742355e-06

Training epoch-68 batch-64
Running loss of epoch-68 batch-64 = 3.935769200325012e-06

Training epoch-68 batch-65
Running loss of epoch-68 batch-65 = 6.1301980167627335e-06

Training epoch-68 batch-66
Running loss of epoch-68 batch-66 = 5.883164703845978e-06

Training epoch-68 batch-67
Running loss of epoch-68 batch-67 = 5.576293915510178e-06

Training epoch-68 batch-68
Running loss of epoch-68 batch-68 = 3.0221417546272278e-06

Training epoch-68 batch-69
Running loss of epoch-68 batch-69 = 2.296408638358116e-06

Training epoch-68 batch-70
Running loss of epoch-68 batch-70 = 3.1364616006612778e-06

Training epoch-68 batch-71
Running loss of epoch-68 batch-71 = 5.4300762712955475e-06

Training epoch-68 batch-72
Running loss of epoch-68 batch-72 = 4.100147634744644e-06

Training epoch-68 batch-73
Running loss of epoch-68 batch-73 = 6.075715646147728e-06

Training epoch-68 batch-74
Running loss of epoch-68 batch-74 = 9.39471647143364e-06

Training epoch-68 batch-75
Running loss of epoch-68 batch-75 = 2.1776650100946426e-06

Training epoch-68 batch-76
Running loss of epoch-68 batch-76 = 2.9478687793016434e-06

Training epoch-68 batch-77
Running loss of epoch-68 batch-77 = 2.392800524830818e-06

Training epoch-68 batch-78
Running loss of epoch-68 batch-78 = 4.692003130912781e-06

Training epoch-68 batch-79
Running loss of epoch-68 batch-79 = 7.100636139512062e-06

Training epoch-68 batch-80
Running loss of epoch-68 batch-80 = 8.217641152441502e-06

Training epoch-68 batch-81
Running loss of epoch-68 batch-81 = 1.284060999751091e-06

Training epoch-68 batch-82
Running loss of epoch-68 batch-82 = 4.51959203928709e-06

Training epoch-68 batch-83
Running loss of epoch-68 batch-83 = 2.8945505619049072e-06

Training epoch-68 batch-84
Running loss of epoch-68 batch-84 = 2.7467031031847e-06

Training epoch-68 batch-85
Running loss of epoch-68 batch-85 = 4.48618084192276e-06

Training epoch-68 batch-86
Running loss of epoch-68 batch-86 = 4.096655175089836e-06

Training epoch-68 batch-87
Running loss of epoch-68 batch-87 = 5.0461385399103165e-06

Training epoch-68 batch-88
Running loss of epoch-68 batch-88 = 3.420049324631691e-06

Training epoch-68 batch-89
Running loss of epoch-68 batch-89 = 1.966021955013275e-06

Training epoch-68 batch-90
Running loss of epoch-68 batch-90 = 5.036359652876854e-06

Training epoch-68 batch-91
Running loss of epoch-68 batch-91 = 5.201669409871101e-06

Training epoch-68 batch-92
Running loss of epoch-68 batch-92 = 5.834735929965973e-06

Training epoch-68 batch-93
Running loss of epoch-68 batch-93 = 3.6440324038267136e-06

Training epoch-68 batch-94
Running loss of epoch-68 batch-94 = 3.735767677426338e-06

Training epoch-68 batch-95
Running loss of epoch-68 batch-95 = 2.1872110664844513e-06

Training epoch-68 batch-96
Running loss of epoch-68 batch-96 = 3.3634714782238007e-06

Training epoch-68 batch-97
Running loss of epoch-68 batch-97 = 4.372093826532364e-06

Training epoch-68 batch-98
Running loss of epoch-68 batch-98 = 1.727137714624405e-06

Training epoch-68 batch-99
Running loss of epoch-68 batch-99 = 7.552094757556915e-06

Training epoch-68 batch-100
Running loss of epoch-68 batch-100 = 2.484070137143135e-06

Training epoch-68 batch-101
Running loss of epoch-68 batch-101 = 7.896451279520988e-06

Training epoch-68 batch-102
Running loss of epoch-68 batch-102 = 3.0514784157276154e-06

Training epoch-68 batch-103
Running loss of epoch-68 batch-103 = 2.0924489945173264e-06

Training epoch-68 batch-104
Running loss of epoch-68 batch-104 = 5.310168489813805e-06

Training epoch-68 batch-105
Running loss of epoch-68 batch-105 = 4.005385562777519e-06

Training epoch-68 batch-106
Running loss of epoch-68 batch-106 = 2.0489096641540527e-06

Training epoch-68 batch-107
Running loss of epoch-68 batch-107 = 6.0230959206819534e-06

Training epoch-68 batch-108
Running loss of epoch-68 batch-108 = 3.157416358590126e-06

Training epoch-68 batch-109
Running loss of epoch-68 batch-109 = 2.6100315153598785e-06

Training epoch-68 batch-110
Running loss of epoch-68 batch-110 = 5.157897248864174e-06

Training epoch-68 batch-111
Running loss of epoch-68 batch-111 = 2.9532238841056824e-06

Training epoch-68 batch-112
Running loss of epoch-68 batch-112 = 5.631474778056145e-06

Training epoch-68 batch-113
Running loss of epoch-68 batch-113 = 2.708984538912773e-06

Training epoch-68 batch-114
Running loss of epoch-68 batch-114 = 3.889435902237892e-06

Training epoch-68 batch-115
Running loss of epoch-68 batch-115 = 2.191402018070221e-06

Training epoch-68 batch-116
Running loss of epoch-68 batch-116 = 2.94344499707222e-06

Training epoch-68 batch-117
Running loss of epoch-68 batch-117 = 4.571396857500076e-06

Training epoch-68 batch-118
Running loss of epoch-68 batch-118 = 4.308763891458511e-06

Training epoch-68 batch-119
Running loss of epoch-68 batch-119 = 5.544861778616905e-06

Training epoch-68 batch-120
Running loss of epoch-68 batch-120 = 5.366979166865349e-06

Training epoch-68 batch-121
Running loss of epoch-68 batch-121 = 3.925757482647896e-06

Training epoch-68 batch-122
Running loss of epoch-68 batch-122 = 2.5078188627958298e-06

Training epoch-68 batch-123
Running loss of epoch-68 batch-123 = 4.194676876068115e-06

Training epoch-68 batch-124
Running loss of epoch-68 batch-124 = 2.67871655523777e-06

Training epoch-68 batch-125
Running loss of epoch-68 batch-125 = 4.0049199014902115e-06

Training epoch-68 batch-126
Running loss of epoch-68 batch-126 = 5.242181941866875e-06

Training epoch-68 batch-127
Running loss of epoch-68 batch-127 = 4.268717020750046e-06

Training epoch-68 batch-128
Running loss of epoch-68 batch-128 = 2.8721988201141357e-06

Training epoch-68 batch-129
Running loss of epoch-68 batch-129 = 1.4468096196651459e-06

Training epoch-68 batch-130
Running loss of epoch-68 batch-130 = 7.328810170292854e-06

Training epoch-68 batch-131
Running loss of epoch-68 batch-131 = 2.439366653561592e-06

Training epoch-68 batch-132
Running loss of epoch-68 batch-132 = 6.285961717367172e-06

Training epoch-68 batch-133
Running loss of epoch-68 batch-133 = 5.6817661970853806e-06

Training epoch-68 batch-134
Running loss of epoch-68 batch-134 = 8.991220965981483e-06

Training epoch-68 batch-135
Running loss of epoch-68 batch-135 = 3.85078601539135e-06

Training epoch-68 batch-136
Running loss of epoch-68 batch-136 = 3.116670995950699e-06

Training epoch-68 batch-137
Running loss of epoch-68 batch-137 = 5.842419341206551e-06

Training epoch-68 batch-138
Running loss of epoch-68 batch-138 = 2.8954818844795227e-06

Training epoch-68 batch-139
Running loss of epoch-68 batch-139 = 4.751142114400864e-06

Training epoch-68 batch-140
Running loss of epoch-68 batch-140 = 7.3052942752838135e-06

Training epoch-68 batch-141
Running loss of epoch-68 batch-141 = 5.070352926850319e-06

Training epoch-68 batch-142
Running loss of epoch-68 batch-142 = 5.477108061313629e-06

Training epoch-68 batch-143
Running loss of epoch-68 batch-143 = 3.63239087164402e-06

Training epoch-68 batch-144
Running loss of epoch-68 batch-144 = 3.521796315908432e-06

Training epoch-68 batch-145
Running loss of epoch-68 batch-145 = 4.524365067481995e-06

Training epoch-68 batch-146
Running loss of epoch-68 batch-146 = 3.1976960599422455e-06

Training epoch-68 batch-147
Running loss of epoch-68 batch-147 = 7.3695555329322815e-06

Training epoch-68 batch-148
Running loss of epoch-68 batch-148 = 1.5569385141134262e-06

Training epoch-68 batch-149
Running loss of epoch-68 batch-149 = 5.695736035704613e-06

Training epoch-68 batch-150
Running loss of epoch-68 batch-150 = 5.567912012338638e-06

Training epoch-68 batch-151
Running loss of epoch-68 batch-151 = 3.98675911128521e-06

Training epoch-68 batch-152
Running loss of epoch-68 batch-152 = 4.972098395228386e-06

Training epoch-68 batch-153
Running loss of epoch-68 batch-153 = 6.370944902300835e-06

Training epoch-68 batch-154
Running loss of epoch-68 batch-154 = 3.967666998505592e-06

Training epoch-68 batch-155
Running loss of epoch-68 batch-155 = 5.16069121658802e-06

Training epoch-68 batch-156
Running loss of epoch-68 batch-156 = 1.7082784324884415e-06

Training epoch-68 batch-157
Running loss of epoch-68 batch-157 = 1.1537224054336548e-05

Finished training epoch-68.



Average train loss at epoch-68 = 4.209313541650772e-06

Started Evaluation

Average val loss at epoch-68 = 1.1419890040842204

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.14 %

Finished Evaluation



Started training epoch-69


Training epoch-69 batch-1
Running loss of epoch-69 batch-1 = 3.487803041934967e-06

Training epoch-69 batch-2
Running loss of epoch-69 batch-2 = 3.6458950489759445e-06

Training epoch-69 batch-3
Running loss of epoch-69 batch-3 = 2.5727786123752594e-06

Training epoch-69 batch-4
Running loss of epoch-69 batch-4 = 4.790490493178368e-06

Training epoch-69 batch-5
Running loss of epoch-69 batch-5 = 1.2228265404701233e-06

Training epoch-69 batch-6
Running loss of epoch-69 batch-6 = 3.924360498785973e-06

Training epoch-69 batch-7
Running loss of epoch-69 batch-7 = 6.642192602157593e-06

Training epoch-69 batch-8
Running loss of epoch-69 batch-8 = 1.2968666851520538e-06

Training epoch-69 batch-9
Running loss of epoch-69 batch-9 = 3.4656841307878494e-06

Training epoch-69 batch-10
Running loss of epoch-69 batch-10 = 2.400483936071396e-06

Training epoch-69 batch-11
Running loss of epoch-69 batch-11 = 3.395369276404381e-06

Training epoch-69 batch-12
Running loss of epoch-69 batch-12 = 5.998183041810989e-06

Training epoch-69 batch-13
Running loss of epoch-69 batch-13 = 1.8312130123376846e-06

Training epoch-69 batch-14
Running loss of epoch-69 batch-14 = 7.590511813759804e-06

Training epoch-69 batch-15
Running loss of epoch-69 batch-15 = 9.289011359214783e-06

Training epoch-69 batch-16
Running loss of epoch-69 batch-16 = 8.475501090288162e-06

Training epoch-69 batch-17
Running loss of epoch-69 batch-17 = 5.3532421588897705e-06

Training epoch-69 batch-18
Running loss of epoch-69 batch-18 = 2.644956111907959e-06

Training epoch-69 batch-19
Running loss of epoch-69 batch-19 = 3.287801519036293e-06

Training epoch-69 batch-20
Running loss of epoch-69 batch-20 = 1.7425045371055603e-06

Training epoch-69 batch-21
Running loss of epoch-69 batch-21 = 3.614695742726326e-06

Training epoch-69 batch-22
Running loss of epoch-69 batch-22 = 2.7385540306568146e-06

Training epoch-69 batch-23
Running loss of epoch-69 batch-23 = 2.430286258459091e-06

Training epoch-69 batch-24
Running loss of epoch-69 batch-24 = 4.181172698736191e-06

Training epoch-69 batch-25
Running loss of epoch-69 batch-25 = 3.2049138098955154e-06

Training epoch-69 batch-26
Running loss of epoch-69 batch-26 = 2.978835254907608e-06

Training epoch-69 batch-27
Running loss of epoch-69 batch-27 = 1.8873251974582672e-06

Training epoch-69 batch-28
Running loss of epoch-69 batch-28 = 3.0060764402151108e-06

Training epoch-69 batch-29
Running loss of epoch-69 batch-29 = 2.5490298867225647e-06

Training epoch-69 batch-30
Running loss of epoch-69 batch-30 = 3.553694114089012e-06

Training epoch-69 batch-31
Running loss of epoch-69 batch-31 = 4.884321242570877e-06

Training epoch-69 batch-32
Running loss of epoch-69 batch-32 = 1.7152633517980576e-06

Training epoch-69 batch-33
Running loss of epoch-69 batch-33 = 5.370704457163811e-06

Training epoch-69 batch-34
Running loss of epoch-69 batch-34 = 5.583977326750755e-06

Training epoch-69 batch-35
Running loss of epoch-69 batch-35 = 7.530674338340759e-06

Training epoch-69 batch-36
Running loss of epoch-69 batch-36 = 3.752531483769417e-06

Training epoch-69 batch-37
Running loss of epoch-69 batch-37 = 2.5155022740364075e-06

Training epoch-69 batch-38
Running loss of epoch-69 batch-38 = 4.2228493839502335e-06

Training epoch-69 batch-39
Running loss of epoch-69 batch-39 = 7.597729563713074e-06

Training epoch-69 batch-40
Running loss of epoch-69 batch-40 = 1.2635719031095505e-06

Training epoch-69 batch-41
Running loss of epoch-69 batch-41 = 7.581198588013649e-06

Training epoch-69 batch-42
Running loss of epoch-69 batch-42 = 5.061738193035126e-06

Training epoch-69 batch-43
Running loss of epoch-69 batch-43 = 3.5108532756567e-06

Training epoch-69 batch-44
Running loss of epoch-69 batch-44 = 3.22563573718071e-06

Training epoch-69 batch-45
Running loss of epoch-69 batch-45 = 3.3334363251924515e-06

Training epoch-69 batch-46
Running loss of epoch-69 batch-46 = 2.4118926376104355e-06

Training epoch-69 batch-47
Running loss of epoch-69 batch-47 = 3.6587007343769073e-06

Training epoch-69 batch-48
Running loss of epoch-69 batch-48 = 4.5334454625844955e-06

Training epoch-69 batch-49
Running loss of epoch-69 batch-49 = 4.868721589446068e-06

Training epoch-69 batch-50
Running loss of epoch-69 batch-50 = 1.1102063581347466e-05

Training epoch-69 batch-51
Running loss of epoch-69 batch-51 = 3.443099558353424e-06

Training epoch-69 batch-52
Running loss of epoch-69 batch-52 = 2.6139896363019943e-06

Training epoch-69 batch-53
Running loss of epoch-69 batch-53 = 3.2889656722545624e-06

Training epoch-69 batch-54
Running loss of epoch-69 batch-54 = 1.2193340808153152e-06

Training epoch-69 batch-55
Running loss of epoch-69 batch-55 = 3.870343789458275e-06

Training epoch-69 batch-56
Running loss of epoch-69 batch-56 = 9.420327842235565e-07

Training epoch-69 batch-57
Running loss of epoch-69 batch-57 = 7.183291018009186e-06

Training epoch-69 batch-58
Running loss of epoch-69 batch-58 = 1.714564859867096e-06

Training epoch-69 batch-59
Running loss of epoch-69 batch-59 = 1.7471611499786377e-06

Training epoch-69 batch-60
Running loss of epoch-69 batch-60 = 4.073837772011757e-06

Training epoch-69 batch-61
Running loss of epoch-69 batch-61 = 1.3135373592376709e-05

Training epoch-69 batch-62
Running loss of epoch-69 batch-62 = 2.9548536986112595e-06

Training epoch-69 batch-63
Running loss of epoch-69 batch-63 = 7.032183930277824e-06

Training epoch-69 batch-64
Running loss of epoch-69 batch-64 = 3.5993289202451706e-06

Training epoch-69 batch-65
Running loss of epoch-69 batch-65 = 3.935536369681358e-06

Training epoch-69 batch-66
Running loss of epoch-69 batch-66 = 1.0948395356535912e-05

Training epoch-69 batch-67
Running loss of epoch-69 batch-67 = 3.125285729765892e-06

Training epoch-69 batch-68
Running loss of epoch-69 batch-68 = 3.047054633498192e-06

Training epoch-69 batch-69
Running loss of epoch-69 batch-69 = 2.343207597732544e-06

Training epoch-69 batch-70
Running loss of epoch-69 batch-70 = 3.6563724279403687e-06

Training epoch-69 batch-71
Running loss of epoch-69 batch-71 = 1.619802787899971e-06

Training epoch-69 batch-72
Running loss of epoch-69 batch-72 = 3.461027517914772e-06

Training epoch-69 batch-73
Running loss of epoch-69 batch-73 = 3.0300579965114594e-06

Training epoch-69 batch-74
Running loss of epoch-69 batch-74 = 1.4835968613624573e-06

Training epoch-69 batch-75
Running loss of epoch-69 batch-75 = 9.083887562155724e-06

Training epoch-69 batch-76
Running loss of epoch-69 batch-76 = 5.2370596677064896e-06

Training epoch-69 batch-77
Running loss of epoch-69 batch-77 = 3.961613401770592e-06

Training epoch-69 batch-78
Running loss of epoch-69 batch-78 = 3.5169068723917007e-06

Training epoch-69 batch-79
Running loss of epoch-69 batch-79 = 2.848682925105095e-06

Training epoch-69 batch-80
Running loss of epoch-69 batch-80 = 4.405155777931213e-06

Training epoch-69 batch-81
Running loss of epoch-69 batch-81 = 5.2247196435928345e-06

Training epoch-69 batch-82
Running loss of epoch-69 batch-82 = 3.485940396785736e-06

Training epoch-69 batch-83
Running loss of epoch-69 batch-83 = 3.475230187177658e-06

Training epoch-69 batch-84
Running loss of epoch-69 batch-84 = 5.42541965842247e-06

Training epoch-69 batch-85
Running loss of epoch-69 batch-85 = 1.923413947224617e-06

Training epoch-69 batch-86
Running loss of epoch-69 batch-86 = 4.236586391925812e-06

Training epoch-69 batch-87
Running loss of epoch-69 batch-87 = 5.175592377781868e-06

Training epoch-69 batch-88
Running loss of epoch-69 batch-88 = 1.975102350115776e-06

Training epoch-69 batch-89
Running loss of epoch-69 batch-89 = 3.4319236874580383e-06

Training epoch-69 batch-90
Running loss of epoch-69 batch-90 = 1.4472752809524536e-06

Training epoch-69 batch-91
Running loss of epoch-69 batch-91 = 3.843801096081734e-06

Training epoch-69 batch-92
Running loss of epoch-69 batch-92 = 4.6424102038145065e-06

Training epoch-69 batch-93
Running loss of epoch-69 batch-93 = 2.3352913558483124e-06

Training epoch-69 batch-94
Running loss of epoch-69 batch-94 = 1.8097925931215286e-06

Training epoch-69 batch-95
Running loss of epoch-69 batch-95 = 8.218688890337944e-06

Training epoch-69 batch-96
Running loss of epoch-69 batch-96 = 4.674308001995087e-06

Training epoch-69 batch-97
Running loss of epoch-69 batch-97 = 1.477263867855072e-05

Training epoch-69 batch-98
Running loss of epoch-69 batch-98 = 4.111090674996376e-06

Training epoch-69 batch-99
Running loss of epoch-69 batch-99 = 1.912703737616539e-06

Training epoch-69 batch-100
Running loss of epoch-69 batch-100 = 2.0854640752077103e-06

Training epoch-69 batch-101
Running loss of epoch-69 batch-101 = 3.5918783396482468e-06

Training epoch-69 batch-102
Running loss of epoch-69 batch-102 = 4.538334906101227e-06

Training epoch-69 batch-103
Running loss of epoch-69 batch-103 = 1.962762326002121e-06

Training epoch-69 batch-104
Running loss of epoch-69 batch-104 = 3.67150641977787e-06

Training epoch-69 batch-105
Running loss of epoch-69 batch-105 = 2.709217369556427e-06

Training epoch-69 batch-106
Running loss of epoch-69 batch-106 = 3.0871015042066574e-06

Training epoch-69 batch-107
Running loss of epoch-69 batch-107 = 3.6831479519605637e-06

Training epoch-69 batch-108
Running loss of epoch-69 batch-108 = 1.9657891243696213e-06

Training epoch-69 batch-109
Running loss of epoch-69 batch-109 = 2.1085143089294434e-06

Training epoch-69 batch-110
Running loss of epoch-69 batch-110 = 3.1669624149799347e-06

Training epoch-69 batch-111
Running loss of epoch-69 batch-111 = 4.751607775688171e-06

Training epoch-69 batch-112
Running loss of epoch-69 batch-112 = 3.1027011573314667e-06

Training epoch-69 batch-113
Running loss of epoch-69 batch-113 = 3.7115532904863358e-06

Training epoch-69 batch-114
Running loss of epoch-69 batch-114 = 4.824018105864525e-06

Training epoch-69 batch-115
Running loss of epoch-69 batch-115 = 4.170229658484459e-06

Training epoch-69 batch-116
Running loss of epoch-69 batch-116 = 2.4926848709583282e-06

Training epoch-69 batch-117
Running loss of epoch-69 batch-117 = 5.816575139760971e-06

Training epoch-69 batch-118
Running loss of epoch-69 batch-118 = 4.060566425323486e-06

Training epoch-69 batch-119
Running loss of epoch-69 batch-119 = 6.571877747774124e-06

Training epoch-69 batch-120
Running loss of epoch-69 batch-120 = 6.5481290221214294e-06

Training epoch-69 batch-121
Running loss of epoch-69 batch-121 = 3.464752808213234e-06

Training epoch-69 batch-122
Running loss of epoch-69 batch-122 = 2.3851171135902405e-06

Training epoch-69 batch-123
Running loss of epoch-69 batch-123 = 6.618909537792206e-06

Training epoch-69 batch-124
Running loss of epoch-69 batch-124 = 2.941349521279335e-06

Training epoch-69 batch-125
Running loss of epoch-69 batch-125 = 4.6729110181331635e-06

Training epoch-69 batch-126
Running loss of epoch-69 batch-126 = 6.371410563588142e-06

Training epoch-69 batch-127
Running loss of epoch-69 batch-127 = 3.109453245997429e-06

Training epoch-69 batch-128
Running loss of epoch-69 batch-128 = 2.8652139008045197e-06

Training epoch-69 batch-129
Running loss of epoch-69 batch-129 = 2.473825588822365e-06

Training epoch-69 batch-130
Running loss of epoch-69 batch-130 = 2.630986273288727e-06

Training epoch-69 batch-131
Running loss of epoch-69 batch-131 = 3.183959051966667e-06

Training epoch-69 batch-132
Running loss of epoch-69 batch-132 = 2.8584618121385574e-06

Training epoch-69 batch-133
Running loss of epoch-69 batch-133 = 4.1853636503219604e-06

Training epoch-69 batch-134
Running loss of epoch-69 batch-134 = 1.5562400221824646e-06

Training epoch-69 batch-135
Running loss of epoch-69 batch-135 = 4.0102750062942505e-06

Training epoch-69 batch-136
Running loss of epoch-69 batch-136 = 3.584194928407669e-06

Training epoch-69 batch-137
Running loss of epoch-69 batch-137 = 1.7300015315413475e-05

Training epoch-69 batch-138
Running loss of epoch-69 batch-138 = 4.033325240015984e-06

Training epoch-69 batch-139
Running loss of epoch-69 batch-139 = 5.533918738365173e-06

Training epoch-69 batch-140
Running loss of epoch-69 batch-140 = 3.5543926060199738e-06

Training epoch-69 batch-141
Running loss of epoch-69 batch-141 = 3.2351817935705185e-06

Training epoch-69 batch-142
Running loss of epoch-69 batch-142 = 8.381204679608345e-06

Training epoch-69 batch-143
Running loss of epoch-69 batch-143 = 2.1152663975954056e-06

Training epoch-69 batch-144
Running loss of epoch-69 batch-144 = 5.1800161600112915e-06

Training epoch-69 batch-145
Running loss of epoch-69 batch-145 = 3.08244489133358e-06

Training epoch-69 batch-146
Running loss of epoch-69 batch-146 = 5.50784170627594e-06

Training epoch-69 batch-147
Running loss of epoch-69 batch-147 = 3.3820979297161102e-06

Training epoch-69 batch-148
Running loss of epoch-69 batch-148 = 3.956956788897514e-06

Training epoch-69 batch-149
Running loss of epoch-69 batch-149 = 3.225402906537056e-06

Training epoch-69 batch-150
Running loss of epoch-69 batch-150 = 2.939486876130104e-06

Training epoch-69 batch-151
Running loss of epoch-69 batch-151 = 2.7962960302829742e-06

Training epoch-69 batch-152
Running loss of epoch-69 batch-152 = 6.833579391241074e-06

Training epoch-69 batch-153
Running loss of epoch-69 batch-153 = 1.2381933629512787e-06

Training epoch-69 batch-154
Running loss of epoch-69 batch-154 = 3.859400749206543e-06

Training epoch-69 batch-155
Running loss of epoch-69 batch-155 = 1.1688098311424255e-06

Training epoch-69 batch-156
Running loss of epoch-69 batch-156 = 4.32855449616909e-06

Training epoch-69 batch-157
Running loss of epoch-69 batch-157 = 9.510666131973267e-06

Finished training epoch-69.



Average train loss at epoch-69 = 4.10325825214386e-06

Started Evaluation

Average val loss at epoch-69 = 1.154579439123884

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 61.87 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.14 %
Accuracy for class execute is: 43.78 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.00 %

Finished Evaluation



Started training epoch-70


Training epoch-70 batch-1
Running loss of epoch-70 batch-1 = 4.244502633810043e-06

Training epoch-70 batch-2
Running loss of epoch-70 batch-2 = 2.420041710138321e-06

Training epoch-70 batch-3
Running loss of epoch-70 batch-3 = 5.721347406506538e-06

Training epoch-70 batch-4
Running loss of epoch-70 batch-4 = 3.163004294037819e-06

Training epoch-70 batch-5
Running loss of epoch-70 batch-5 = 3.2978132367134094e-06

Training epoch-70 batch-6
Running loss of epoch-70 batch-6 = 5.2372924983501434e-06

Training epoch-70 batch-7
Running loss of epoch-70 batch-7 = 1.526903361082077e-06

Training epoch-70 batch-8
Running loss of epoch-70 batch-8 = 3.8228463381528854e-06

Training epoch-70 batch-9
Running loss of epoch-70 batch-9 = 2.635642886161804e-06

Training epoch-70 batch-10
Running loss of epoch-70 batch-10 = 3.7348363548517227e-06

Training epoch-70 batch-11
Running loss of epoch-70 batch-11 = 8.077360689640045e-06

Training epoch-70 batch-12
Running loss of epoch-70 batch-12 = 5.537644028663635e-06

Training epoch-70 batch-13
Running loss of epoch-70 batch-13 = 8.58423300087452e-06

Training epoch-70 batch-14
Running loss of epoch-70 batch-14 = 2.8670765459537506e-06

Training epoch-70 batch-15
Running loss of epoch-70 batch-15 = 4.02587465941906e-06

Training epoch-70 batch-16
Running loss of epoch-70 batch-16 = 4.134839400649071e-06

Training epoch-70 batch-17
Running loss of epoch-70 batch-17 = 6.433343514800072e-06

Training epoch-70 batch-18
Running loss of epoch-70 batch-18 = 4.470115527510643e-06

Training epoch-70 batch-19
Running loss of epoch-70 batch-19 = 5.930429324507713e-06

Training epoch-70 batch-20
Running loss of epoch-70 batch-20 = 4.334840923547745e-06

Training epoch-70 batch-21
Running loss of epoch-70 batch-21 = 1.8177088350057602e-06

Training epoch-70 batch-22
Running loss of epoch-70 batch-22 = 3.2745301723480225e-06

Training epoch-70 batch-23
Running loss of epoch-70 batch-23 = 3.380933776497841e-06

Training epoch-70 batch-24
Running loss of epoch-70 batch-24 = 1.6293488442897797e-06

Training epoch-70 batch-25
Running loss of epoch-70 batch-25 = 5.697598680853844e-06

Training epoch-70 batch-26
Running loss of epoch-70 batch-26 = 2.4139881134033203e-06

Training epoch-70 batch-27
Running loss of epoch-70 batch-27 = 6.7320652306079865e-06

Training epoch-70 batch-28
Running loss of epoch-70 batch-28 = 3.7937425076961517e-06

Training epoch-70 batch-29
Running loss of epoch-70 batch-29 = 2.5941990315914154e-06

Training epoch-70 batch-30
Running loss of epoch-70 batch-30 = 3.707828000187874e-06

Training epoch-70 batch-31
Running loss of epoch-70 batch-31 = 2.8263311833143234e-06

Training epoch-70 batch-32
Running loss of epoch-70 batch-32 = 5.099456757307053e-06

Training epoch-70 batch-33
Running loss of epoch-70 batch-33 = 4.7979410737752914e-06

Training epoch-70 batch-34
Running loss of epoch-70 batch-34 = 5.533918738365173e-06

Training epoch-70 batch-35
Running loss of epoch-70 batch-35 = 1.2754928320646286e-05

Training epoch-70 batch-36
Running loss of epoch-70 batch-36 = 1.3152603060007095e-06

Training epoch-70 batch-37
Running loss of epoch-70 batch-37 = 9.329523891210556e-07

Training epoch-70 batch-38
Running loss of epoch-70 batch-38 = 3.759283572435379e-06

Training epoch-70 batch-39
Running loss of epoch-70 batch-39 = 3.564869984984398e-06

Training epoch-70 batch-40
Running loss of epoch-70 batch-40 = 3.868015483021736e-06

Training epoch-70 batch-41
Running loss of epoch-70 batch-41 = 3.6035198718309402e-06

Training epoch-70 batch-42
Running loss of epoch-70 batch-42 = 2.2405292838811874e-06

Training epoch-70 batch-43
Running loss of epoch-70 batch-43 = 2.3029278963804245e-06

Training epoch-70 batch-44
Running loss of epoch-70 batch-44 = 3.1534582376480103e-06

Training epoch-70 batch-45
Running loss of epoch-70 batch-45 = 5.56441955268383e-06

Training epoch-70 batch-46
Running loss of epoch-70 batch-46 = 1.2477394193410873e-06

Training epoch-70 batch-47
Running loss of epoch-70 batch-47 = 2.3462343961000443e-06

Training epoch-70 batch-48
Running loss of epoch-70 batch-48 = 1.469859853386879e-06

Training epoch-70 batch-49
Running loss of epoch-70 batch-49 = 3.300607204437256e-06

Training epoch-70 batch-50
Running loss of epoch-70 batch-50 = 4.844972863793373e-06

Training epoch-70 batch-51
Running loss of epoch-70 batch-51 = 3.7800054997205734e-06

Training epoch-70 batch-52
Running loss of epoch-70 batch-52 = 3.57883982360363e-06

Training epoch-70 batch-53
Running loss of epoch-70 batch-53 = 6.6265929490327835e-06

Training epoch-70 batch-54
Running loss of epoch-70 batch-54 = 1.2009404599666595e-06

Training epoch-70 batch-55
Running loss of epoch-70 batch-55 = 1.421663910150528e-06

Training epoch-70 batch-56
Running loss of epoch-70 batch-56 = 6.084330379962921e-06

Training epoch-70 batch-57
Running loss of epoch-70 batch-57 = 4.998175427317619e-06

Training epoch-70 batch-58
Running loss of epoch-70 batch-58 = 3.5227276384830475e-06

Training epoch-70 batch-59
Running loss of epoch-70 batch-59 = 3.7280842661857605e-06

Training epoch-70 batch-60
Running loss of epoch-70 batch-60 = 1.4600809663534164e-06

Training epoch-70 batch-61
Running loss of epoch-70 batch-61 = 1.8300488591194153e-06

Training epoch-70 batch-62
Running loss of epoch-70 batch-62 = 4.537636414170265e-06

Training epoch-70 batch-63
Running loss of epoch-70 batch-63 = 1.2029660865664482e-05

Training epoch-70 batch-64
Running loss of epoch-70 batch-64 = 6.512971594929695e-06

Training epoch-70 batch-65
Running loss of epoch-70 batch-65 = 4.5765191316604614e-06

Training epoch-70 batch-66
Running loss of epoch-70 batch-66 = 5.3211115300655365e-06

Training epoch-70 batch-67
Running loss of epoch-70 batch-67 = 3.6030542105436325e-06

Training epoch-70 batch-68
Running loss of epoch-70 batch-68 = 3.2151583582162857e-06

Training epoch-70 batch-69
Running loss of epoch-70 batch-69 = 6.281537935137749e-06

Training epoch-70 batch-70
Running loss of epoch-70 batch-70 = 1.6244594007730484e-06

Training epoch-70 batch-71
Running loss of epoch-70 batch-71 = 2.7692876756191254e-06

Training epoch-70 batch-72
Running loss of epoch-70 batch-72 = 2.5152694433927536e-06

Training epoch-70 batch-73
Running loss of epoch-70 batch-73 = 3.427267074584961e-06

Training epoch-70 batch-74
Running loss of epoch-70 batch-74 = 4.005152732133865e-06

Training epoch-70 batch-75
Running loss of epoch-70 batch-75 = 3.814697265625e-06

Training epoch-70 batch-76
Running loss of epoch-70 batch-76 = 2.887798473238945e-06

Training epoch-70 batch-77
Running loss of epoch-70 batch-77 = 4.557892680168152e-06

Training epoch-70 batch-78
Running loss of epoch-70 batch-78 = 3.6621931940317154e-06

Training epoch-70 batch-79
Running loss of epoch-70 batch-79 = 2.6549678295850754e-06

Training epoch-70 batch-80
Running loss of epoch-70 batch-80 = 2.6177149266004562e-06

Training epoch-70 batch-81
Running loss of epoch-70 batch-81 = 4.83333133161068e-06

Training epoch-70 batch-82
Running loss of epoch-70 batch-82 = 2.8349459171295166e-06

Training epoch-70 batch-83
Running loss of epoch-70 batch-83 = 6.729038432240486e-06

Training epoch-70 batch-84
Running loss of epoch-70 batch-84 = 4.744855687022209e-06

Training epoch-70 batch-85
Running loss of epoch-70 batch-85 = 6.215646862983704e-06

Training epoch-70 batch-86
Running loss of epoch-70 batch-86 = 3.4633558243513107e-06

Training epoch-70 batch-87
Running loss of epoch-70 batch-87 = 4.107365384697914e-06

Training epoch-70 batch-88
Running loss of epoch-70 batch-88 = 2.2619497030973434e-06

Training epoch-70 batch-89
Running loss of epoch-70 batch-89 = 4.259636625647545e-06

Training epoch-70 batch-90
Running loss of epoch-70 batch-90 = 5.013076588511467e-06

Training epoch-70 batch-91
Running loss of epoch-70 batch-91 = 4.156958311796188e-06

Training epoch-70 batch-92
Running loss of epoch-70 batch-92 = 3.3096875995397568e-06

Training epoch-70 batch-93
Running loss of epoch-70 batch-93 = 4.729721695184708e-06

Training epoch-70 batch-94
Running loss of epoch-70 batch-94 = 2.3103784769773483e-06

Training epoch-70 batch-95
Running loss of epoch-70 batch-95 = 1.4225952327251434e-06

Training epoch-70 batch-96
Running loss of epoch-70 batch-96 = 2.4994369596242905e-06

Training epoch-70 batch-97
Running loss of epoch-70 batch-97 = 3.669876605272293e-06

Training epoch-70 batch-98
Running loss of epoch-70 batch-98 = 4.391418769955635e-06

Training epoch-70 batch-99
Running loss of epoch-70 batch-99 = 3.0172523111104965e-06

Training epoch-70 batch-100
Running loss of epoch-70 batch-100 = 6.780726835131645e-06

Training epoch-70 batch-101
Running loss of epoch-70 batch-101 = 3.2975804060697556e-06

Training epoch-70 batch-102
Running loss of epoch-70 batch-102 = 3.925524652004242e-06

Training epoch-70 batch-103
Running loss of epoch-70 batch-103 = 1.6081612557172775e-06

Training epoch-70 batch-104
Running loss of epoch-70 batch-104 = 3.384426236152649e-06

Training epoch-70 batch-105
Running loss of epoch-70 batch-105 = 2.321554347872734e-06

Training epoch-70 batch-106
Running loss of epoch-70 batch-106 = 6.22984953224659e-06

Training epoch-70 batch-107
Running loss of epoch-70 batch-107 = 2.451706677675247e-06

Training epoch-70 batch-108
Running loss of epoch-70 batch-108 = 1.8121208995580673e-06

Training epoch-70 batch-109
Running loss of epoch-70 batch-109 = 5.334150046110153e-06

Training epoch-70 batch-110
Running loss of epoch-70 batch-110 = 3.248453140258789e-06

Training epoch-70 batch-111
Running loss of epoch-70 batch-111 = 1.98977068066597e-06

Training epoch-70 batch-112
Running loss of epoch-70 batch-112 = 6.0782767832279205e-06

Training epoch-70 batch-113
Running loss of epoch-70 batch-113 = 6.006564944982529e-06

Training epoch-70 batch-114
Running loss of epoch-70 batch-114 = 1.0682269930839539e-06

Training epoch-70 batch-115
Running loss of epoch-70 batch-115 = 3.5213306546211243e-06

Training epoch-70 batch-116
Running loss of epoch-70 batch-116 = 4.617031663656235e-06

Training epoch-70 batch-117
Running loss of epoch-70 batch-117 = 3.932509571313858e-06

Training epoch-70 batch-118
Running loss of epoch-70 batch-118 = 4.008878022432327e-06

Training epoch-70 batch-119
Running loss of epoch-70 batch-119 = 1.5227124094963074e-06

Training epoch-70 batch-120
Running loss of epoch-70 batch-120 = 1.089414581656456e-06

Training epoch-70 batch-121
Running loss of epoch-70 batch-121 = 4.185596480965614e-06

Training epoch-70 batch-122
Running loss of epoch-70 batch-122 = 2.86824069917202e-06

Training epoch-70 batch-123
Running loss of epoch-70 batch-123 = 6.369082257151604e-06

Training epoch-70 batch-124
Running loss of epoch-70 batch-124 = 3.866385668516159e-06

Training epoch-70 batch-125
Running loss of epoch-70 batch-125 = 8.072936907410622e-06

Training epoch-70 batch-126
Running loss of epoch-70 batch-126 = 7.808208465576172e-06

Training epoch-70 batch-127
Running loss of epoch-70 batch-127 = 7.50063918530941e-06

Training epoch-70 batch-128
Running loss of epoch-70 batch-128 = 6.591901183128357e-06

Training epoch-70 batch-129
Running loss of epoch-70 batch-129 = 5.362555384635925e-06

Training epoch-70 batch-130
Running loss of epoch-70 batch-130 = 2.1634623408317566e-06

Training epoch-70 batch-131
Running loss of epoch-70 batch-131 = 2.555549144744873e-06

Training epoch-70 batch-132
Running loss of epoch-70 batch-132 = 1.844950020313263e-06

Training epoch-70 batch-133
Running loss of epoch-70 batch-133 = 4.455447196960449e-06

Training epoch-70 batch-134
Running loss of epoch-70 batch-134 = 1.9995495676994324e-06

Training epoch-70 batch-135
Running loss of epoch-70 batch-135 = 6.095040589570999e-06

Training epoch-70 batch-136
Running loss of epoch-70 batch-136 = 3.5655684769153595e-06

Training epoch-70 batch-137
Running loss of epoch-70 batch-137 = 3.988621756434441e-06

Training epoch-70 batch-138
Running loss of epoch-70 batch-138 = 4.416331648826599e-06

Training epoch-70 batch-139
Running loss of epoch-70 batch-139 = 3.698980435729027e-06

Training epoch-70 batch-140
Running loss of epoch-70 batch-140 = 5.029607564210892e-06

Training epoch-70 batch-141
Running loss of epoch-70 batch-141 = 3.764638677239418e-06

Training epoch-70 batch-142
Running loss of epoch-70 batch-142 = 6.207963451743126e-06

Training epoch-70 batch-143
Running loss of epoch-70 batch-143 = 3.526918590068817e-06

Training epoch-70 batch-144
Running loss of epoch-70 batch-144 = 3.23285348713398e-06

Training epoch-70 batch-145
Running loss of epoch-70 batch-145 = 2.9408838599920273e-06

Training epoch-70 batch-146
Running loss of epoch-70 batch-146 = 2.6207417249679565e-06

Training epoch-70 batch-147
Running loss of epoch-70 batch-147 = 4.000961780548096e-06

Training epoch-70 batch-148
Running loss of epoch-70 batch-148 = 7.017282769083977e-06

Training epoch-70 batch-149
Running loss of epoch-70 batch-149 = 2.235639840364456e-06

Training epoch-70 batch-150
Running loss of epoch-70 batch-150 = 2.567889168858528e-06

Training epoch-70 batch-151
Running loss of epoch-70 batch-151 = 4.173954948782921e-06

Training epoch-70 batch-152
Running loss of epoch-70 batch-152 = 9.651994332671165e-06

Training epoch-70 batch-153
Running loss of epoch-70 batch-153 = 4.003522917628288e-06

Training epoch-70 batch-154
Running loss of epoch-70 batch-154 = 2.3245811462402344e-06

Training epoch-70 batch-155
Running loss of epoch-70 batch-155 = 3.591179847717285e-06

Training epoch-70 batch-156
Running loss of epoch-70 batch-156 = 3.277324140071869e-06

Training epoch-70 batch-157
Running loss of epoch-70 batch-157 = 1.6093254089355469e-06

Finished training epoch-70.



Average train loss at epoch-70 = 3.998680412769318e-06

Started Evaluation

Average val loss at epoch-70 = 1.1480769801982518

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.89 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.71 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.12 %

Finished Evaluation



Started training epoch-71


Training epoch-71 batch-1
Running loss of epoch-71 batch-1 = 3.6514829844236374e-06

Training epoch-71 batch-2
Running loss of epoch-71 batch-2 = 5.116919055581093e-06

Training epoch-71 batch-3
Running loss of epoch-71 batch-3 = 6.207963451743126e-06

Training epoch-71 batch-4
Running loss of epoch-71 batch-4 = 3.0102673918008804e-06

Training epoch-71 batch-5
Running loss of epoch-71 batch-5 = 3.996770828962326e-06

Training epoch-71 batch-6
Running loss of epoch-71 batch-6 = 2.5383196771144867e-06

Training epoch-71 batch-7
Running loss of epoch-71 batch-7 = 3.4528784453868866e-06

Training epoch-71 batch-8
Running loss of epoch-71 batch-8 = 5.644978955388069e-06

Training epoch-71 batch-9
Running loss of epoch-71 batch-9 = 5.079666152596474e-06

Training epoch-71 batch-10
Running loss of epoch-71 batch-10 = 4.696659743785858e-06

Training epoch-71 batch-11
Running loss of epoch-71 batch-11 = 2.474989742040634e-06

Training epoch-71 batch-12
Running loss of epoch-71 batch-12 = 3.3169053494930267e-06

Training epoch-71 batch-13
Running loss of epoch-71 batch-13 = 5.024252459406853e-06

Training epoch-71 batch-14
Running loss of epoch-71 batch-14 = 2.0456500351428986e-06

Training epoch-71 batch-15
Running loss of epoch-71 batch-15 = 1.6300473362207413e-06

Training epoch-71 batch-16
Running loss of epoch-71 batch-16 = 2.116430550813675e-06

Training epoch-71 batch-17
Running loss of epoch-71 batch-17 = 6.611458957195282e-06

Training epoch-71 batch-18
Running loss of epoch-71 batch-18 = 2.9371585696935654e-06

Training epoch-71 batch-19
Running loss of epoch-71 batch-19 = 1.3597309589385986e-06

Training epoch-71 batch-20
Running loss of epoch-71 batch-20 = 2.4479813873767853e-06

Training epoch-71 batch-21
Running loss of epoch-71 batch-21 = 3.964640200138092e-06

Training epoch-71 batch-22
Running loss of epoch-71 batch-22 = 5.0568487495183945e-06

Training epoch-71 batch-23
Running loss of epoch-71 batch-23 = 4.204222932457924e-06

Training epoch-71 batch-24
Running loss of epoch-71 batch-24 = 2.003973349928856e-06

Training epoch-71 batch-25
Running loss of epoch-71 batch-25 = 3.673834726214409e-06

Training epoch-71 batch-26
Running loss of epoch-71 batch-26 = 2.8335489332675934e-06

Training epoch-71 batch-27
Running loss of epoch-71 batch-27 = 4.3245963752269745e-06

Training epoch-71 batch-28
Running loss of epoch-71 batch-28 = 3.11131589114666e-06

Training epoch-71 batch-29
Running loss of epoch-71 batch-29 = 1.6437843441963196e-06

Training epoch-71 batch-30
Running loss of epoch-71 batch-30 = 6.29783608019352e-06

Training epoch-71 batch-31
Running loss of epoch-71 batch-31 = 2.3641623556613922e-06

Training epoch-71 batch-32
Running loss of epoch-71 batch-32 = 4.08245250582695e-06

Training epoch-71 batch-33
Running loss of epoch-71 batch-33 = 1.7194543033838272e-06

Training epoch-71 batch-34
Running loss of epoch-71 batch-34 = 2.182088792324066e-06

Training epoch-71 batch-35
Running loss of epoch-71 batch-35 = 3.0547380447387695e-06

Training epoch-71 batch-36
Running loss of epoch-71 batch-36 = 7.003545761108398e-07

Training epoch-71 batch-37
Running loss of epoch-71 batch-37 = 3.114575520157814e-06

Training epoch-71 batch-38
Running loss of epoch-71 batch-38 = 2.923887223005295e-06

Training epoch-71 batch-39
Running loss of epoch-71 batch-39 = 3.9208680391311646e-06

Training epoch-71 batch-40
Running loss of epoch-71 batch-40 = 3.6999117583036423e-06

Training epoch-71 batch-41
Running loss of epoch-71 batch-41 = 3.1797681003808975e-06

Training epoch-71 batch-42
Running loss of epoch-71 batch-42 = 9.799143299460411e-06

Training epoch-71 batch-43
Running loss of epoch-71 batch-43 = 3.864755854010582e-06

Training epoch-71 batch-44
Running loss of epoch-71 batch-44 = 2.07521952688694e-06

Training epoch-71 batch-45
Running loss of epoch-71 batch-45 = 2.4191103875637054e-06

Training epoch-71 batch-46
Running loss of epoch-71 batch-46 = 4.866393283009529e-06

Training epoch-71 batch-47
Running loss of epoch-71 batch-47 = 4.382571205496788e-06

Training epoch-71 batch-48
Running loss of epoch-71 batch-48 = 1.5238765627145767e-06

Training epoch-71 batch-49
Running loss of epoch-71 batch-49 = 2.3206230252981186e-06

Training epoch-71 batch-50
Running loss of epoch-71 batch-50 = 4.865927621722221e-06

Training epoch-71 batch-51
Running loss of epoch-71 batch-51 = 4.6121422201395035e-06

Training epoch-71 batch-52
Running loss of epoch-71 batch-52 = 5.138339474797249e-06

Training epoch-71 batch-53
Running loss of epoch-71 batch-53 = 3.814930096268654e-06

Training epoch-71 batch-54
Running loss of epoch-71 batch-54 = 2.942979335784912e-06

Training epoch-71 batch-55
Running loss of epoch-71 batch-55 = 8.699717000126839e-06

Training epoch-71 batch-56
Running loss of epoch-71 batch-56 = 4.92180697619915e-06

Training epoch-71 batch-57
Running loss of epoch-71 batch-57 = 1.7548445612192154e-06

Training epoch-71 batch-58
Running loss of epoch-71 batch-58 = 2.982793375849724e-06

Training epoch-71 batch-59
Running loss of epoch-71 batch-59 = 2.9257498681545258e-06

Training epoch-71 batch-60
Running loss of epoch-71 batch-60 = 4.173722118139267e-06

Training epoch-71 batch-61
Running loss of epoch-71 batch-61 = 4.7283247113227844e-06

Training epoch-71 batch-62
Running loss of epoch-71 batch-62 = 4.426576197147369e-06

Training epoch-71 batch-63
Running loss of epoch-71 batch-63 = 4.222383722662926e-06

Training epoch-71 batch-64
Running loss of epoch-71 batch-64 = 7.746974006295204e-06

Training epoch-71 batch-65
Running loss of epoch-71 batch-65 = 3.030989319086075e-06

Training epoch-71 batch-66
Running loss of epoch-71 batch-66 = 5.203764885663986e-06

Training epoch-71 batch-67
Running loss of epoch-71 batch-67 = 3.4300610423088074e-06

Training epoch-71 batch-68
Running loss of epoch-71 batch-68 = 1.939479261636734e-06

Training epoch-71 batch-69
Running loss of epoch-71 batch-69 = 1.601874828338623e-05

Training epoch-71 batch-70
Running loss of epoch-71 batch-70 = 2.08965502679348e-06

Training epoch-71 batch-71
Running loss of epoch-71 batch-71 = 4.24007885158062e-06

Training epoch-71 batch-72
Running loss of epoch-71 batch-72 = 5.144625902175903e-06

Training epoch-71 batch-73
Running loss of epoch-71 batch-73 = 2.198386937379837e-06

Training epoch-71 batch-74
Running loss of epoch-71 batch-74 = 4.149973392486572e-06

Training epoch-71 batch-75
Running loss of epoch-71 batch-75 = 3.7660356611013412e-06

Training epoch-71 batch-76
Running loss of epoch-71 batch-76 = 6.642425432801247e-06

Training epoch-71 batch-77
Running loss of epoch-71 batch-77 = 1.0442221537232399e-05

Training epoch-71 batch-78
Running loss of epoch-71 batch-78 = 4.788627848029137e-06

Training epoch-71 batch-79
Running loss of epoch-71 batch-79 = 5.224486812949181e-06

Training epoch-71 batch-80
Running loss of epoch-71 batch-80 = 3.247056156396866e-06

Training epoch-71 batch-81
Running loss of epoch-71 batch-81 = 1.8826685845851898e-06

Training epoch-71 batch-82
Running loss of epoch-71 batch-82 = 3.4035183489322662e-06

Training epoch-71 batch-83
Running loss of epoch-71 batch-83 = 2.8649810701608658e-06

Training epoch-71 batch-84
Running loss of epoch-71 batch-84 = 1.4440156519412994e-06

Training epoch-71 batch-85
Running loss of epoch-71 batch-85 = 1.2686941772699356e-06

Training epoch-71 batch-86
Running loss of epoch-71 batch-86 = 8.616363629698753e-06

Training epoch-71 batch-87
Running loss of epoch-71 batch-87 = 4.8030633479356766e-06

Training epoch-71 batch-88
Running loss of epoch-71 batch-88 = 5.3069088608026505e-06

Training epoch-71 batch-89
Running loss of epoch-71 batch-89 = 3.0174851417541504e-06

Training epoch-71 batch-90
Running loss of epoch-71 batch-90 = 2.7492642402648926e-06

Training epoch-71 batch-91
Running loss of epoch-71 batch-91 = 1.4067627489566803e-06

Training epoch-71 batch-92
Running loss of epoch-71 batch-92 = 2.9390212148427963e-06

Training epoch-71 batch-93
Running loss of epoch-71 batch-93 = 4.336470738053322e-06

Training epoch-71 batch-94
Running loss of epoch-71 batch-94 = 4.599336534738541e-06

Training epoch-71 batch-95
Running loss of epoch-71 batch-95 = 1.89291313290596e-06

Training epoch-71 batch-96
Running loss of epoch-71 batch-96 = 2.8440263122320175e-06

Training epoch-71 batch-97
Running loss of epoch-71 batch-97 = 7.823342457413673e-06

Training epoch-71 batch-98
Running loss of epoch-71 batch-98 = 1.9080471247434616e-06

Training epoch-71 batch-99
Running loss of epoch-71 batch-99 = 4.33996319770813e-06

Training epoch-71 batch-100
Running loss of epoch-71 batch-100 = 8.039642125368118e-06

Training epoch-71 batch-101
Running loss of epoch-71 batch-101 = 2.3813918232917786e-06

Training epoch-71 batch-102
Running loss of epoch-71 batch-102 = 4.236586391925812e-06

Training epoch-71 batch-103
Running loss of epoch-71 batch-103 = 2.3958273231983185e-06

Training epoch-71 batch-104
Running loss of epoch-71 batch-104 = 6.9248490035533905e-06

Training epoch-71 batch-105
Running loss of epoch-71 batch-105 = 1.009390689432621e-05

Training epoch-71 batch-106
Running loss of epoch-71 batch-106 = 4.087341949343681e-06

Training epoch-71 batch-107
Running loss of epoch-71 batch-107 = 2.5008339434862137e-06

Training epoch-71 batch-108
Running loss of epoch-71 batch-108 = 5.243578925728798e-06

Training epoch-71 batch-109
Running loss of epoch-71 batch-109 = 7.248949259519577e-06

Training epoch-71 batch-110
Running loss of epoch-71 batch-110 = 4.366505891084671e-06

Training epoch-71 batch-111
Running loss of epoch-71 batch-111 = 4.194444045424461e-06

Training epoch-71 batch-112
Running loss of epoch-71 batch-112 = 4.529254510998726e-06

Training epoch-71 batch-113
Running loss of epoch-71 batch-113 = 2.641696482896805e-06

Training epoch-71 batch-114
Running loss of epoch-71 batch-114 = 1.3259705156087875e-06

Training epoch-71 batch-115
Running loss of epoch-71 batch-115 = 3.425171598792076e-06

Training epoch-71 batch-116
Running loss of epoch-71 batch-116 = 3.0687078833580017e-06

Training epoch-71 batch-117
Running loss of epoch-71 batch-117 = 2.4633482098579407e-06

Training epoch-71 batch-118
Running loss of epoch-71 batch-118 = 4.002824425697327e-06

Training epoch-71 batch-119
Running loss of epoch-71 batch-119 = 3.209337592124939e-06

Training epoch-71 batch-120
Running loss of epoch-71 batch-120 = 7.507391273975372e-06

Training epoch-71 batch-121
Running loss of epoch-71 batch-121 = 2.384418621659279e-06

Training epoch-71 batch-122
Running loss of epoch-71 batch-122 = 3.8009602576494217e-06

Training epoch-71 batch-123
Running loss of epoch-71 batch-123 = 3.3171381801366806e-06

Training epoch-71 batch-124
Running loss of epoch-71 batch-124 = 4.160916432738304e-06

Training epoch-71 batch-125
Running loss of epoch-71 batch-125 = 2.3837201297283173e-06

Training epoch-71 batch-126
Running loss of epoch-71 batch-126 = 3.907596692442894e-06

Training epoch-71 batch-127
Running loss of epoch-71 batch-127 = 8.11740756034851e-06

Training epoch-71 batch-128
Running loss of epoch-71 batch-128 = 3.870576620101929e-06

Training epoch-71 batch-129
Running loss of epoch-71 batch-129 = 4.128320142626762e-06

Training epoch-71 batch-130
Running loss of epoch-71 batch-130 = 7.688067853450775e-07

Training epoch-71 batch-131
Running loss of epoch-71 batch-131 = 3.964640200138092e-06

Training epoch-71 batch-132
Running loss of epoch-71 batch-132 = 1.0526273399591446e-06

Training epoch-71 batch-133
Running loss of epoch-71 batch-133 = 3.4123659133911133e-06

Training epoch-71 batch-134
Running loss of epoch-71 batch-134 = 2.7103815227746964e-06

Training epoch-71 batch-135
Running loss of epoch-71 batch-135 = 4.566041752696037e-06

Training epoch-71 batch-136
Running loss of epoch-71 batch-136 = 1.5082769095897675e-06

Training epoch-71 batch-137
Running loss of epoch-71 batch-137 = 9.502051398158073e-06

Training epoch-71 batch-138
Running loss of epoch-71 batch-138 = 2.575339749455452e-06

Training epoch-71 batch-139
Running loss of epoch-71 batch-139 = 4.012370482087135e-06

Training epoch-71 batch-140
Running loss of epoch-71 batch-140 = 2.1813903003931046e-06

Training epoch-71 batch-141
Running loss of epoch-71 batch-141 = 2.5657936930656433e-06

Training epoch-71 batch-142
Running loss of epoch-71 batch-142 = 3.218185156583786e-06

Training epoch-71 batch-143
Running loss of epoch-71 batch-143 = 4.67570498585701e-06

Training epoch-71 batch-144
Running loss of epoch-71 batch-144 = 1.3094395399093628e-06

Training epoch-71 batch-145
Running loss of epoch-71 batch-145 = 2.3923348635435104e-06

Training epoch-71 batch-146
Running loss of epoch-71 batch-146 = 3.361376002430916e-06

Training epoch-71 batch-147
Running loss of epoch-71 batch-147 = 2.436107024550438e-06

Training epoch-71 batch-148
Running loss of epoch-71 batch-148 = 4.437984898686409e-06

Training epoch-71 batch-149
Running loss of epoch-71 batch-149 = 3.132503479719162e-06

Training epoch-71 batch-150
Running loss of epoch-71 batch-150 = 5.257315933704376e-06

Training epoch-71 batch-151
Running loss of epoch-71 batch-151 = 2.359505742788315e-06

Training epoch-71 batch-152
Running loss of epoch-71 batch-152 = 4.13903035223484e-06

Training epoch-71 batch-153
Running loss of epoch-71 batch-153 = 4.849862307310104e-06

Training epoch-71 batch-154
Running loss of epoch-71 batch-154 = 4.732515662908554e-06

Training epoch-71 batch-155
Running loss of epoch-71 batch-155 = 4.420755431056023e-06

Training epoch-71 batch-156
Running loss of epoch-71 batch-156 = 3.6638230085372925e-06

Training epoch-71 batch-157
Running loss of epoch-71 batch-157 = 2.734363079071045e-06

Finished training epoch-71.



Average train loss at epoch-71 = 3.926262259483338e-06

Started Evaluation

Average val loss at epoch-71 = 1.1575159116767186

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.00 %

Finished Evaluation



Started training epoch-72


Training epoch-72 batch-1
Running loss of epoch-72 batch-1 = 6.188871338963509e-06

Training epoch-72 batch-2
Running loss of epoch-72 batch-2 = 3.9353035390377045e-06

Training epoch-72 batch-3
Running loss of epoch-72 batch-3 = 3.237742930650711e-06

Training epoch-72 batch-4
Running loss of epoch-72 batch-4 = 4.627043381333351e-06

Training epoch-72 batch-5
Running loss of epoch-72 batch-5 = 5.943700671195984e-06

Training epoch-72 batch-6
Running loss of epoch-72 batch-6 = 2.993037924170494e-06

Training epoch-72 batch-7
Running loss of epoch-72 batch-7 = 4.314817488193512e-06

Training epoch-72 batch-8
Running loss of epoch-72 batch-8 = 2.7855858206748962e-06

Training epoch-72 batch-9
Running loss of epoch-72 batch-9 = 2.6975758373737335e-06

Training epoch-72 batch-10
Running loss of epoch-72 batch-10 = 2.032145857810974e-06

Training epoch-72 batch-11
Running loss of epoch-72 batch-11 = 3.638211637735367e-06

Training epoch-72 batch-12
Running loss of epoch-72 batch-12 = 2.94344499707222e-06

Training epoch-72 batch-13
Running loss of epoch-72 batch-13 = 2.441462129354477e-06

Training epoch-72 batch-14
Running loss of epoch-72 batch-14 = 2.7990899980068207e-06

Training epoch-72 batch-15
Running loss of epoch-72 batch-15 = 5.3551048040390015e-06

Training epoch-72 batch-16
Running loss of epoch-72 batch-16 = 1.0227318853139877e-05

Training epoch-72 batch-17
Running loss of epoch-72 batch-17 = 6.894813850522041e-06

Training epoch-72 batch-18
Running loss of epoch-72 batch-18 = 3.0545052140951157e-06

Training epoch-72 batch-19
Running loss of epoch-72 batch-19 = 1.0470394045114517e-06

Training epoch-72 batch-20
Running loss of epoch-72 batch-20 = 2.5371555238962173e-06

Training epoch-72 batch-21
Running loss of epoch-72 batch-21 = 2.7527566999197006e-06

Training epoch-72 batch-22
Running loss of epoch-72 batch-22 = 2.4603214114904404e-06

Training epoch-72 batch-23
Running loss of epoch-72 batch-23 = 3.6174897104501724e-06

Training epoch-72 batch-24
Running loss of epoch-72 batch-24 = 2.989312633872032e-06

Training epoch-72 batch-25
Running loss of epoch-72 batch-25 = 2.6389025151729584e-06

Training epoch-72 batch-26
Running loss of epoch-72 batch-26 = 4.027271643280983e-06

Training epoch-72 batch-27
Running loss of epoch-72 batch-27 = 1.967884600162506e-06

Training epoch-72 batch-28
Running loss of epoch-72 batch-28 = 2.8281938284635544e-06

Training epoch-72 batch-29
Running loss of epoch-72 batch-29 = 2.514803782105446e-06

Training epoch-72 batch-30
Running loss of epoch-72 batch-30 = 5.102250725030899e-06

Training epoch-72 batch-31
Running loss of epoch-72 batch-31 = 3.7977006286382675e-06

Training epoch-72 batch-32
Running loss of epoch-72 batch-32 = 7.690861821174622e-06

Training epoch-72 batch-33
Running loss of epoch-72 batch-33 = 4.407251253724098e-06

Training epoch-72 batch-34
Running loss of epoch-72 batch-34 = 3.22563573718071e-06

Training epoch-72 batch-35
Running loss of epoch-72 batch-35 = 3.2263342291116714e-06

Training epoch-72 batch-36
Running loss of epoch-72 batch-36 = 4.536006599664688e-06

Training epoch-72 batch-37
Running loss of epoch-72 batch-37 = 1.259148120880127e-06

Training epoch-72 batch-38
Running loss of epoch-72 batch-38 = 8.065486326813698e-06

Training epoch-72 batch-39
Running loss of epoch-72 batch-39 = 2.7550850063562393e-06

Training epoch-72 batch-40
Running loss of epoch-72 batch-40 = 3.350665792822838e-06

Training epoch-72 batch-41
Running loss of epoch-72 batch-41 = 2.0868610590696335e-06

Training epoch-72 batch-42
Running loss of epoch-72 batch-42 = 1.8391292542219162e-06

Training epoch-72 batch-43
Running loss of epoch-72 batch-43 = 3.6705750972032547e-06

Training epoch-72 batch-44
Running loss of epoch-72 batch-44 = 5.7998113334178925e-06

Training epoch-72 batch-45
Running loss of epoch-72 batch-45 = 4.9907248467206955e-06

Training epoch-72 batch-46
Running loss of epoch-72 batch-46 = 5.166511982679367e-06

Training epoch-72 batch-47
Running loss of epoch-72 batch-47 = 1.0470394045114517e-05

Training epoch-72 batch-48
Running loss of epoch-72 batch-48 = 1.8584541976451874e-06

Training epoch-72 batch-49
Running loss of epoch-72 batch-49 = 5.037523806095123e-06

Training epoch-72 batch-50
Running loss of epoch-72 batch-50 = 1.957407221198082e-06

Training epoch-72 batch-51
Running loss of epoch-72 batch-51 = 1.6118865460157394e-06

Training epoch-72 batch-52
Running loss of epoch-72 batch-52 = 2.4193432182073593e-06

Training epoch-72 batch-53
Running loss of epoch-72 batch-53 = 2.727145329117775e-06

Training epoch-72 batch-54
Running loss of epoch-72 batch-54 = 1.184176653623581e-06

Training epoch-72 batch-55
Running loss of epoch-72 batch-55 = 3.8370490074157715e-06

Training epoch-72 batch-56
Running loss of epoch-72 batch-56 = 3.602355718612671e-06

Training epoch-72 batch-57
Running loss of epoch-72 batch-57 = 3.5695265978574753e-06

Training epoch-72 batch-58
Running loss of epoch-72 batch-58 = 5.120877176523209e-06

Training epoch-72 batch-59
Running loss of epoch-72 batch-59 = 3.1457748264074326e-06

Training epoch-72 batch-60
Running loss of epoch-72 batch-60 = 6.913905963301659e-06

Training epoch-72 batch-61
Running loss of epoch-72 batch-61 = 3.6458950489759445e-06

Training epoch-72 batch-62
Running loss of epoch-72 batch-62 = 7.6324213296175e-06

Training epoch-72 batch-63
Running loss of epoch-72 batch-63 = 1.3434328138828278e-06

Training epoch-72 batch-64
Running loss of epoch-72 batch-64 = 2.543441951274872e-06

Training epoch-72 batch-65
Running loss of epoch-72 batch-65 = 4.459638148546219e-06

Training epoch-72 batch-66
Running loss of epoch-72 batch-66 = 2.73948535323143e-06

Training epoch-72 batch-67
Running loss of epoch-72 batch-67 = 4.338566213846207e-06

Training epoch-72 batch-68
Running loss of epoch-72 batch-68 = 3.6298297345638275e-06

Training epoch-72 batch-69
Running loss of epoch-72 batch-69 = 1.9082799553871155e-06

Training epoch-72 batch-70
Running loss of epoch-72 batch-70 = 5.148584023118019e-06

Training epoch-72 batch-71
Running loss of epoch-72 batch-71 = 3.273831680417061e-06

Training epoch-72 batch-72
Running loss of epoch-72 batch-72 = 5.44707290828228e-06

Training epoch-72 batch-73
Running loss of epoch-72 batch-73 = 3.2410025596618652e-06

Training epoch-72 batch-74
Running loss of epoch-72 batch-74 = 7.119262591004372e-06

Training epoch-72 batch-75
Running loss of epoch-72 batch-75 = 2.436107024550438e-06

Training epoch-72 batch-76
Running loss of epoch-72 batch-76 = 2.3795291781425476e-06

Training epoch-72 batch-77
Running loss of epoch-72 batch-77 = 1.989537850022316e-06

Training epoch-72 batch-78
Running loss of epoch-72 batch-78 = 1.9141007214784622e-06

Training epoch-72 batch-79
Running loss of epoch-72 batch-79 = 2.403627149760723e-06

Training epoch-72 batch-80
Running loss of epoch-72 batch-80 = 3.550434485077858e-06

Training epoch-72 batch-81
Running loss of epoch-72 batch-81 = 3.764871507883072e-06

Training epoch-72 batch-82
Running loss of epoch-72 batch-82 = 4.231464117765427e-06

Training epoch-72 batch-83
Running loss of epoch-72 batch-83 = 2.2442545741796494e-06

Training epoch-72 batch-84
Running loss of epoch-72 batch-84 = 1.6705598682165146e-06

Training epoch-72 batch-85
Running loss of epoch-72 batch-85 = 3.4922268241643906e-06

Training epoch-72 batch-86
Running loss of epoch-72 batch-86 = 2.5068875402212143e-06

Training epoch-72 batch-87
Running loss of epoch-72 batch-87 = 2.7334317564964294e-06

Training epoch-72 batch-88
Running loss of epoch-72 batch-88 = 2.017710357904434e-06

Training epoch-72 batch-89
Running loss of epoch-72 batch-89 = 5.727633833885193e-06

Training epoch-72 batch-90
Running loss of epoch-72 batch-90 = 5.021924152970314e-06

Training epoch-72 batch-91
Running loss of epoch-72 batch-91 = 4.977220669388771e-06

Training epoch-72 batch-92
Running loss of epoch-72 batch-92 = 4.719942808151245e-06

Training epoch-72 batch-93
Running loss of epoch-72 batch-93 = 4.045199602842331e-06

Training epoch-72 batch-94
Running loss of epoch-72 batch-94 = 7.753493264317513e-06

Training epoch-72 batch-95
Running loss of epoch-72 batch-95 = 1.1739786714315414e-05

Training epoch-72 batch-96
Running loss of epoch-72 batch-96 = 8.407514542341232e-07

Training epoch-72 batch-97
Running loss of epoch-72 batch-97 = 3.3499673008918762e-06

Training epoch-72 batch-98
Running loss of epoch-72 batch-98 = 5.234498530626297e-06

Training epoch-72 batch-99
Running loss of epoch-72 batch-99 = 2.939719706773758e-06

Training epoch-72 batch-100
Running loss of epoch-72 batch-100 = 3.6885030567646027e-06

Training epoch-72 batch-101
Running loss of epoch-72 batch-101 = 2.519693225622177e-06

Training epoch-72 batch-102
Running loss of epoch-72 batch-102 = 2.453569322824478e-06

Training epoch-72 batch-103
Running loss of epoch-72 batch-103 = 3.8141151890158653e-06

Training epoch-72 batch-104
Running loss of epoch-72 batch-104 = 2.8565991669893265e-06

Training epoch-72 batch-105
Running loss of epoch-72 batch-105 = 7.903669029474258e-06

Training epoch-72 batch-106
Running loss of epoch-72 batch-106 = 1.925509423017502e-06

Training epoch-72 batch-107
Running loss of epoch-72 batch-107 = 3.949739038944244e-06

Training epoch-72 batch-108
Running loss of epoch-72 batch-108 = 5.120877176523209e-06

Training epoch-72 batch-109
Running loss of epoch-72 batch-109 = 3.570225089788437e-06

Training epoch-72 batch-110
Running loss of epoch-72 batch-110 = 3.7169083952903748e-06

Training epoch-72 batch-111
Running loss of epoch-72 batch-111 = 6.529735401272774e-06

Training epoch-72 batch-112
Running loss of epoch-72 batch-112 = 3.2975804060697556e-06

Training epoch-72 batch-113
Running loss of epoch-72 batch-113 = 5.341833457350731e-06

Training epoch-72 batch-114
Running loss of epoch-72 batch-114 = 1.3979151844978333e-06

Training epoch-72 batch-115
Running loss of epoch-72 batch-115 = 3.5047996789216995e-06

Training epoch-72 batch-116
Running loss of epoch-72 batch-116 = 3.870809450745583e-06

Training epoch-72 batch-117
Running loss of epoch-72 batch-117 = 4.219589754939079e-06

Training epoch-72 batch-118
Running loss of epoch-72 batch-118 = 3.1027011573314667e-06

Training epoch-72 batch-119
Running loss of epoch-72 batch-119 = 3.728782758116722e-06

Training epoch-72 batch-120
Running loss of epoch-72 batch-120 = 3.7369318306446075e-06

Training epoch-72 batch-121
Running loss of epoch-72 batch-121 = 1.8030405044555664e-06

Training epoch-72 batch-122
Running loss of epoch-72 batch-122 = 4.857545718550682e-06

Training epoch-72 batch-123
Running loss of epoch-72 batch-123 = 4.459172487258911e-06

Training epoch-72 batch-124
Running loss of epoch-72 batch-124 = 6.758375093340874e-06

Training epoch-72 batch-125
Running loss of epoch-72 batch-125 = 5.7192519307136536e-06

Training epoch-72 batch-126
Running loss of epoch-72 batch-126 = 4.000961780548096e-06

Training epoch-72 batch-127
Running loss of epoch-72 batch-127 = 3.669876605272293e-06

Training epoch-72 batch-128
Running loss of epoch-72 batch-128 = 1.1335592716932297e-05

Training epoch-72 batch-129
Running loss of epoch-72 batch-129 = 5.566049367189407e-06

Training epoch-72 batch-130
Running loss of epoch-72 batch-130 = 1.3012904673814774e-06

Training epoch-72 batch-131
Running loss of epoch-72 batch-131 = 6.558140739798546e-06

Training epoch-72 batch-132
Running loss of epoch-72 batch-132 = 3.009568899869919e-06

Training epoch-72 batch-133
Running loss of epoch-72 batch-133 = 2.0640436559915543e-06

Training epoch-72 batch-134
Running loss of epoch-72 batch-134 = 3.45427542924881e-06

Training epoch-72 batch-135
Running loss of epoch-72 batch-135 = 2.1746382117271423e-06

Training epoch-72 batch-136
Running loss of epoch-72 batch-136 = 3.7106219679117203e-06

Training epoch-72 batch-137
Running loss of epoch-72 batch-137 = 4.405155777931213e-06

Training epoch-72 batch-138
Running loss of epoch-72 batch-138 = 4.178844392299652e-06

Training epoch-72 batch-139
Running loss of epoch-72 batch-139 = 1.735752448439598e-06

Training epoch-72 batch-140
Running loss of epoch-72 batch-140 = 1.7511192709207535e-06

Training epoch-72 batch-141
Running loss of epoch-72 batch-141 = 2.2025778889656067e-06

Training epoch-72 batch-142
Running loss of epoch-72 batch-142 = 1.0889489203691483e-06

Training epoch-72 batch-143
Running loss of epoch-72 batch-143 = 3.2791867852211e-06

Training epoch-72 batch-144
Running loss of epoch-72 batch-144 = 4.167668521404266e-06

Training epoch-72 batch-145
Running loss of epoch-72 batch-145 = 4.724832251667976e-06

Training epoch-72 batch-146
Running loss of epoch-72 batch-146 = 5.964189767837524e-06

Training epoch-72 batch-147
Running loss of epoch-72 batch-147 = 6.800750270485878e-06

Training epoch-72 batch-148
Running loss of epoch-72 batch-148 = 1.994892954826355e-06

Training epoch-72 batch-149
Running loss of epoch-72 batch-149 = 1.914799213409424e-06

Training epoch-72 batch-150
Running loss of epoch-72 batch-150 = 4.386762157082558e-06

Training epoch-72 batch-151
Running loss of epoch-72 batch-151 = 3.880821168422699e-06

Training epoch-72 batch-152
Running loss of epoch-72 batch-152 = 2.203509211540222e-06

Training epoch-72 batch-153
Running loss of epoch-72 batch-153 = 1.6260892152786255e-06

Training epoch-72 batch-154
Running loss of epoch-72 batch-154 = 2.726912498474121e-06

Training epoch-72 batch-155
Running loss of epoch-72 batch-155 = 2.3907050490379333e-06

Training epoch-72 batch-156
Running loss of epoch-72 batch-156 = 2.1650921553373337e-06

Training epoch-72 batch-157
Running loss of epoch-72 batch-157 = 4.706531763076782e-05

Finished training epoch-72.



Average train loss at epoch-72 = 3.879167139530182e-06

Started Evaluation

Average val loss at epoch-72 = 1.1513062293706573

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.48 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.16 %

Finished Evaluation



Started training epoch-73


Training epoch-73 batch-1
Running loss of epoch-73 batch-1 = 7.4766576290130615e-06

Training epoch-73 batch-2
Running loss of epoch-73 batch-2 = 2.8794165700674057e-06

Training epoch-73 batch-3
Running loss of epoch-73 batch-3 = 4.125526174902916e-06

Training epoch-73 batch-4
Running loss of epoch-73 batch-4 = 2.048211172223091e-06

Training epoch-73 batch-5
Running loss of epoch-73 batch-5 = 1.8745195120573044e-06

Training epoch-73 batch-6
Running loss of epoch-73 batch-6 = 1.0076910257339478e-06

Training epoch-73 batch-7
Running loss of epoch-73 batch-7 = 2.105953171849251e-06

Training epoch-73 batch-8
Running loss of epoch-73 batch-8 = 2.7909409254789352e-06

Training epoch-73 batch-9
Running loss of epoch-73 batch-9 = 1.1648284271359444e-05

Training epoch-73 batch-10
Running loss of epoch-73 batch-10 = 5.100853741168976e-06

Training epoch-73 batch-11
Running loss of epoch-73 batch-11 = 3.859400749206543e-06

Training epoch-73 batch-12
Running loss of epoch-73 batch-12 = 4.643108695745468e-06

Training epoch-73 batch-13
Running loss of epoch-73 batch-13 = 7.854308933019638e-06

Training epoch-73 batch-14
Running loss of epoch-73 batch-14 = 1.7415732145309448e-06

Training epoch-73 batch-15
Running loss of epoch-73 batch-15 = 3.0212104320526123e-06

Training epoch-73 batch-16
Running loss of epoch-73 batch-16 = 4.958128556609154e-06

Training epoch-73 batch-17
Running loss of epoch-73 batch-17 = 2.118060365319252e-06

Training epoch-73 batch-18
Running loss of epoch-73 batch-18 = 3.7134159356355667e-06

Training epoch-73 batch-19
Running loss of epoch-73 batch-19 = 4.918547347187996e-06

Training epoch-73 batch-20
Running loss of epoch-73 batch-20 = 3.2070092856884003e-06

Training epoch-73 batch-21
Running loss of epoch-73 batch-21 = 3.478489816188812e-07

Training epoch-73 batch-22
Running loss of epoch-73 batch-22 = 7.694121450185776e-06

Training epoch-73 batch-23
Running loss of epoch-73 batch-23 = 5.511566996574402e-06

Training epoch-73 batch-24
Running loss of epoch-73 batch-24 = 2.382323145866394e-06

Training epoch-73 batch-25
Running loss of epoch-73 batch-25 = 3.8142316043376923e-06

Training epoch-73 batch-26
Running loss of epoch-73 batch-26 = 3.5457778722047806e-06

Training epoch-73 batch-27
Running loss of epoch-73 batch-27 = 1.8035061657428741e-06

Training epoch-73 batch-28
Running loss of epoch-73 batch-28 = 4.843110218644142e-06

Training epoch-73 batch-29
Running loss of epoch-73 batch-29 = 2.884538844227791e-06

Training epoch-73 batch-30
Running loss of epoch-73 batch-30 = 1.976732164621353e-06

Training epoch-73 batch-31
Running loss of epoch-73 batch-31 = 7.869908586144447e-06

Training epoch-73 batch-32
Running loss of epoch-73 batch-32 = 3.6121346056461334e-06

Training epoch-73 batch-33
Running loss of epoch-73 batch-33 = 1.8635764718055725e-06

Training epoch-73 batch-34
Running loss of epoch-73 batch-34 = 3.955326974391937e-06

Training epoch-73 batch-35
Running loss of epoch-73 batch-35 = 2.2177118808031082e-06

Training epoch-73 batch-36
Running loss of epoch-73 batch-36 = 6.3283368945121765e-06

Training epoch-73 batch-37
Running loss of epoch-73 batch-37 = 4.07942570745945e-06

Training epoch-73 batch-38
Running loss of epoch-73 batch-38 = 3.212830051779747e-06

Training epoch-73 batch-39
Running loss of epoch-73 batch-39 = 3.760796971619129e-06

Training epoch-73 batch-40
Running loss of epoch-73 batch-40 = 4.731351509690285e-06

Training epoch-73 batch-41
Running loss of epoch-73 batch-41 = 7.0552341639995575e-06

Training epoch-73 batch-42
Running loss of epoch-73 batch-42 = 3.0333176255226135e-06

Training epoch-73 batch-43
Running loss of epoch-73 batch-43 = 3.894558176398277e-06

Training epoch-73 batch-44
Running loss of epoch-73 batch-44 = 4.735775291919708e-06

Training epoch-73 batch-45
Running loss of epoch-73 batch-45 = 2.6312191039323807e-06

Training epoch-73 batch-46
Running loss of epoch-73 batch-46 = 3.0461233109235764e-06

Training epoch-73 batch-47
Running loss of epoch-73 batch-47 = 3.536231815814972e-06

Training epoch-73 batch-48
Running loss of epoch-73 batch-48 = 4.76115383207798e-06

Training epoch-73 batch-49
Running loss of epoch-73 batch-49 = 1.2742821127176285e-06

Training epoch-73 batch-50
Running loss of epoch-73 batch-50 = 1.6994308680295944e-06

Training epoch-73 batch-51
Running loss of epoch-73 batch-51 = 1.2530945241451263e-06

Training epoch-73 batch-52
Running loss of epoch-73 batch-52 = 5.171401426196098e-06

Training epoch-73 batch-53
Running loss of epoch-73 batch-53 = 3.4801196306943893e-06

Training epoch-73 batch-54
Running loss of epoch-73 batch-54 = 3.11434268951416e-06

Training epoch-73 batch-55
Running loss of epoch-73 batch-55 = 2.369750291109085e-06

Training epoch-73 batch-56
Running loss of epoch-73 batch-56 = 5.20399771630764e-06

Training epoch-73 batch-57
Running loss of epoch-73 batch-57 = 2.5546178221702576e-06

Training epoch-73 batch-58
Running loss of epoch-73 batch-58 = 3.6158598959445953e-06

Training epoch-73 batch-59
Running loss of epoch-73 batch-59 = 3.2940879464149475e-06

Training epoch-73 batch-60
Running loss of epoch-73 batch-60 = 2.218177542090416e-06

Training epoch-73 batch-61
Running loss of epoch-73 batch-61 = 2.5203917175531387e-06

Training epoch-73 batch-62
Running loss of epoch-73 batch-62 = 3.170222043991089e-06

Training epoch-73 batch-63
Running loss of epoch-73 batch-63 = 3.6654528230428696e-06

Training epoch-73 batch-64
Running loss of epoch-73 batch-64 = 4.448229447007179e-06

Training epoch-73 batch-65
Running loss of epoch-73 batch-65 = 1.275213435292244e-06

Training epoch-73 batch-66
Running loss of epoch-73 batch-66 = 4.1157472878694534e-06

Training epoch-73 batch-67
Running loss of epoch-73 batch-67 = 3.196997568011284e-06

Training epoch-73 batch-68
Running loss of epoch-73 batch-68 = 6.361166015267372e-06

Training epoch-73 batch-69
Running loss of epoch-73 batch-69 = 5.809124559164047e-07

Training epoch-73 batch-70
Running loss of epoch-73 batch-70 = 3.39653342962265e-06

Training epoch-73 batch-71
Running loss of epoch-73 batch-71 = 4.016794264316559e-06

Training epoch-73 batch-72
Running loss of epoch-73 batch-72 = 6.294809281826019e-06

Training epoch-73 batch-73
Running loss of epoch-73 batch-73 = 8.777948096394539e-06

Training epoch-73 batch-74
Running loss of epoch-73 batch-74 = 3.941124305129051e-06

Training epoch-73 batch-75
Running loss of epoch-73 batch-75 = 2.295244485139847e-06

Training epoch-73 batch-76
Running loss of epoch-73 batch-76 = 1.5613622963428497e-06

Training epoch-73 batch-77
Running loss of epoch-73 batch-77 = 2.364860847592354e-06

Training epoch-73 batch-78
Running loss of epoch-73 batch-78 = 3.935769200325012e-06

Training epoch-73 batch-79
Running loss of epoch-73 batch-79 = 3.405613824725151e-06

Training epoch-73 batch-80
Running loss of epoch-73 batch-80 = 1.920154318213463e-06

Training epoch-73 batch-81
Running loss of epoch-73 batch-81 = 3.839610144495964e-06

Training epoch-73 batch-82
Running loss of epoch-73 batch-82 = 2.8139911592006683e-06

Training epoch-73 batch-83
Running loss of epoch-73 batch-83 = 8.436618372797966e-06

Training epoch-73 batch-84
Running loss of epoch-73 batch-84 = 1.895008608698845e-06

Training epoch-73 batch-85
Running loss of epoch-73 batch-85 = 4.727160558104515e-06

Training epoch-73 batch-86
Running loss of epoch-73 batch-86 = 5.941605195403099e-06

Training epoch-73 batch-87
Running loss of epoch-73 batch-87 = 2.7588102966547012e-06

Training epoch-73 batch-88
Running loss of epoch-73 batch-88 = 5.644513294100761e-06

Training epoch-73 batch-89
Running loss of epoch-73 batch-89 = 4.970934242010117e-06

Training epoch-73 batch-90
Running loss of epoch-73 batch-90 = 1.8046703189611435e-06

Training epoch-73 batch-91
Running loss of epoch-73 batch-91 = 4.248227924108505e-06

Training epoch-73 batch-92
Running loss of epoch-73 batch-92 = 3.6067795008420944e-06

Training epoch-73 batch-93
Running loss of epoch-73 batch-93 = 2.5096815079450607e-06

Training epoch-73 batch-94
Running loss of epoch-73 batch-94 = 2.6405323296785355e-06

Training epoch-73 batch-95
Running loss of epoch-73 batch-95 = 2.4293549358844757e-06

Training epoch-73 batch-96
Running loss of epoch-73 batch-96 = 2.543441951274872e-06

Training epoch-73 batch-97
Running loss of epoch-73 batch-97 = 1.501990482211113e-06

Training epoch-73 batch-98
Running loss of epoch-73 batch-98 = 2.485932782292366e-06

Training epoch-73 batch-99
Running loss of epoch-73 batch-99 = 3.1439121812582016e-06

Training epoch-73 batch-100
Running loss of epoch-73 batch-100 = 3.1443778425455093e-06

Training epoch-73 batch-101
Running loss of epoch-73 batch-101 = 6.177229806780815e-06

Training epoch-73 batch-102
Running loss of epoch-73 batch-102 = 2.963002771139145e-06

Training epoch-73 batch-103
Running loss of epoch-73 batch-103 = 2.010609023272991e-06

Training epoch-73 batch-104
Running loss of epoch-73 batch-104 = 3.405613824725151e-06

Training epoch-73 batch-105
Running loss of epoch-73 batch-105 = 2.6670750230550766e-06

Training epoch-73 batch-106
Running loss of epoch-73 batch-106 = 5.420064553618431e-06

Training epoch-73 batch-107
Running loss of epoch-73 batch-107 = 1.8246937543153763e-06

Training epoch-73 batch-108
Running loss of epoch-73 batch-108 = 4.222383722662926e-06

Training epoch-73 batch-109
Running loss of epoch-73 batch-109 = 4.330649971961975e-06

Training epoch-73 batch-110
Running loss of epoch-73 batch-110 = 5.9122685343027115e-06

Training epoch-73 batch-111
Running loss of epoch-73 batch-111 = 4.494795575737953e-06

Training epoch-73 batch-112
Running loss of epoch-73 batch-112 = 2.834480255842209e-06

Training epoch-73 batch-113
Running loss of epoch-73 batch-113 = 4.115281626582146e-06

Training epoch-73 batch-114
Running loss of epoch-73 batch-114 = 4.4284388422966e-06

Training epoch-73 batch-115
Running loss of epoch-73 batch-115 = 6.397021934390068e-06

Training epoch-73 batch-116
Running loss of epoch-73 batch-116 = 1.405598595738411e-06

Training epoch-73 batch-117
Running loss of epoch-73 batch-117 = 5.387701094150543e-06

Training epoch-73 batch-118
Running loss of epoch-73 batch-118 = 2.5294721126556396e-06

Training epoch-73 batch-119
Running loss of epoch-73 batch-119 = 2.126675099134445e-06

Training epoch-73 batch-120
Running loss of epoch-73 batch-120 = 2.095010131597519e-06

Training epoch-73 batch-121
Running loss of epoch-73 batch-121 = 3.4391414374113083e-06

Training epoch-73 batch-122
Running loss of epoch-73 batch-122 = 6.824033334851265e-06

Training epoch-73 batch-123
Running loss of epoch-73 batch-123 = 3.469642251729965e-06

Training epoch-73 batch-124
Running loss of epoch-73 batch-124 = 2.3888424038887024e-06

Training epoch-73 batch-125
Running loss of epoch-73 batch-125 = 2.2992026060819626e-06

Training epoch-73 batch-126
Running loss of epoch-73 batch-126 = 4.366505891084671e-06

Training epoch-73 batch-127
Running loss of epoch-73 batch-127 = 4.014931619167328e-06

Training epoch-73 batch-128
Running loss of epoch-73 batch-128 = 2.6070047169923782e-06

Training epoch-73 batch-129
Running loss of epoch-73 batch-129 = 3.1618401408195496e-06

Training epoch-73 batch-130
Running loss of epoch-73 batch-130 = 6.2980689108371735e-06

Training epoch-73 batch-131
Running loss of epoch-73 batch-131 = 1.6922131180763245e-06

Training epoch-73 batch-132
Running loss of epoch-73 batch-132 = 9.382842108607292e-06

Training epoch-73 batch-133
Running loss of epoch-73 batch-133 = 4.404457286000252e-06

Training epoch-73 batch-134
Running loss of epoch-73 batch-134 = 2.550426870584488e-06

Training epoch-73 batch-135
Running loss of epoch-73 batch-135 = 6.484799087047577e-06

Training epoch-73 batch-136
Running loss of epoch-73 batch-136 = 7.037539035081863e-06

Training epoch-73 batch-137
Running loss of epoch-73 batch-137 = 3.2866373658180237e-06

Training epoch-73 batch-138
Running loss of epoch-73 batch-138 = 5.0424132496118546e-06

Training epoch-73 batch-139
Running loss of epoch-73 batch-139 = 6.536953151226044e-06

Training epoch-73 batch-140
Running loss of epoch-73 batch-140 = 1.612817868590355e-06

Training epoch-73 batch-141
Running loss of epoch-73 batch-141 = 1.8784776329994202e-06

Training epoch-73 batch-142
Running loss of epoch-73 batch-142 = 1.1241063475608826e-06

Training epoch-73 batch-143
Running loss of epoch-73 batch-143 = 2.72504985332489e-06

Training epoch-73 batch-144
Running loss of epoch-73 batch-144 = 1.3099052011966705e-06

Training epoch-73 batch-145
Running loss of epoch-73 batch-145 = 4.565343260765076e-06

Training epoch-73 batch-146
Running loss of epoch-73 batch-146 = 3.5390257835388184e-06

Training epoch-73 batch-147
Running loss of epoch-73 batch-147 = 6.53672032058239e-06

Training epoch-73 batch-148
Running loss of epoch-73 batch-148 = 6.082933396100998e-06

Training epoch-73 batch-149
Running loss of epoch-73 batch-149 = 3.0831433832645416e-06

Training epoch-73 batch-150
Running loss of epoch-73 batch-150 = 4.156492650508881e-06

Training epoch-73 batch-151
Running loss of epoch-73 batch-151 = 1.2230593711137772e-06

Training epoch-73 batch-152
Running loss of epoch-73 batch-152 = 5.416572093963623e-06

Training epoch-73 batch-153
Running loss of epoch-73 batch-153 = 3.6028213798999786e-06

Training epoch-73 batch-154
Running loss of epoch-73 batch-154 = 1.7450656741857529e-06

Training epoch-73 batch-155
Running loss of epoch-73 batch-155 = 4.5746564865112305e-06

Training epoch-73 batch-156
Running loss of epoch-73 batch-156 = 4.4959597289562225e-06

Training epoch-73 batch-157
Running loss of epoch-73 batch-157 = 2.080574631690979e-05

Finished training epoch-73.



Average train loss at epoch-73 = 3.7805542349815368e-06

Started Evaluation

Average val loss at epoch-73 = 1.15464687418172

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.17 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.14 %

Finished Evaluation



Started training epoch-74


Training epoch-74 batch-1
Running loss of epoch-74 batch-1 = 4.47477214038372e-06

Training epoch-74 batch-2
Running loss of epoch-74 batch-2 = 1.348322257399559e-06

Training epoch-74 batch-3
Running loss of epoch-74 batch-3 = 3.2710377126932144e-06

Training epoch-74 batch-4
Running loss of epoch-74 batch-4 = 2.146698534488678e-06

Training epoch-74 batch-5
Running loss of epoch-74 batch-5 = 3.779074177145958e-06

Training epoch-74 batch-6
Running loss of epoch-74 batch-6 = 2.541346475481987e-06

Training epoch-74 batch-7
Running loss of epoch-74 batch-7 = 2.2600870579481125e-06

Training epoch-74 batch-8
Running loss of epoch-74 batch-8 = 1.6207341104745865e-06

Training epoch-74 batch-9
Running loss of epoch-74 batch-9 = 3.223540261387825e-06

Training epoch-74 batch-10
Running loss of epoch-74 batch-10 = 5.917856469750404e-06

Training epoch-74 batch-11
Running loss of epoch-74 batch-11 = 2.0833685994148254e-06

Training epoch-74 batch-12
Running loss of epoch-74 batch-12 = 5.509238690137863e-06

Training epoch-74 batch-13
Running loss of epoch-74 batch-13 = 3.9369333535432816e-06

Training epoch-74 batch-14
Running loss of epoch-74 batch-14 = 2.4188775569200516e-06

Training epoch-74 batch-15
Running loss of epoch-74 batch-15 = 5.867332220077515e-06

Training epoch-74 batch-16
Running loss of epoch-74 batch-16 = 2.200016751885414e-06

Training epoch-74 batch-17
Running loss of epoch-74 batch-17 = 3.0437950044870377e-06

Training epoch-74 batch-18
Running loss of epoch-74 batch-18 = 3.92901711165905e-06

Training epoch-74 batch-19
Running loss of epoch-74 batch-19 = 2.3988541215658188e-06

Training epoch-74 batch-20
Running loss of epoch-74 batch-20 = 2.757646143436432e-06

Training epoch-74 batch-21
Running loss of epoch-74 batch-21 = 4.832400009036064e-06

Training epoch-74 batch-22
Running loss of epoch-74 batch-22 = 3.6230776458978653e-06

Training epoch-74 batch-23
Running loss of epoch-74 batch-23 = 3.3366959542036057e-06

Training epoch-74 batch-24
Running loss of epoch-74 batch-24 = 7.949769496917725e-06

Training epoch-74 batch-25
Running loss of epoch-74 batch-25 = 3.639375790953636e-06

Training epoch-74 batch-26
Running loss of epoch-74 batch-26 = 2.582557499408722e-06

Training epoch-74 batch-27
Running loss of epoch-74 batch-27 = 1.7171259969472885e-06

Training epoch-74 batch-28
Running loss of epoch-74 batch-28 = 3.5669654607772827e-06

Training epoch-74 batch-29
Running loss of epoch-74 batch-29 = 2.9960647225379944e-06

Training epoch-74 batch-30
Running loss of epoch-74 batch-30 = 3.050314262509346e-06

Training epoch-74 batch-31
Running loss of epoch-74 batch-31 = 2.535991370677948e-06

Training epoch-74 batch-32
Running loss of epoch-74 batch-32 = 8.047092705965042e-06

Training epoch-74 batch-33
Running loss of epoch-74 batch-33 = 1.5657860785722733e-06

Training epoch-74 batch-34
Running loss of epoch-74 batch-34 = 1.8454156816005707e-06

Training epoch-74 batch-35
Running loss of epoch-74 batch-35 = 2.716435119509697e-06

Training epoch-74 batch-36
Running loss of epoch-74 batch-36 = 4.292698577046394e-06

Training epoch-74 batch-37
Running loss of epoch-74 batch-37 = 1.569977030158043e-06

Training epoch-74 batch-38
Running loss of epoch-74 batch-38 = 1.378823071718216e-06

Training epoch-74 batch-39
Running loss of epoch-74 batch-39 = 6.561866030097008e-06

Training epoch-74 batch-40
Running loss of epoch-74 batch-40 = 1.219799742102623e-06

Training epoch-74 batch-41
Running loss of epoch-74 batch-41 = 2.1972227841615677e-06

Training epoch-74 batch-42
Running loss of epoch-74 batch-42 = 5.544628947973251e-06

Training epoch-74 batch-43
Running loss of epoch-74 batch-43 = 1.6405247151851654e-06

Training epoch-74 batch-44
Running loss of epoch-74 batch-44 = 8.435221388936043e-06

Training epoch-74 batch-45
Running loss of epoch-74 batch-45 = 8.81589949131012e-06

Training epoch-74 batch-46
Running loss of epoch-74 batch-46 = 2.964399755001068e-06

Training epoch-74 batch-47
Running loss of epoch-74 batch-47 = 3.93320806324482e-06

Training epoch-74 batch-48
Running loss of epoch-74 batch-48 = 5.036825314164162e-06

Training epoch-74 batch-49
Running loss of epoch-74 batch-49 = 1.1376803740859032e-05

Training epoch-74 batch-50
Running loss of epoch-74 batch-50 = 3.934837877750397e-06

Training epoch-74 batch-51
Running loss of epoch-74 batch-51 = 3.927387297153473e-06

Training epoch-74 batch-52
Running loss of epoch-74 batch-52 = 5.6372955441474915e-06

Training epoch-74 batch-53
Running loss of epoch-74 batch-53 = 5.4049305617809296e-06

Training epoch-74 batch-54
Running loss of epoch-74 batch-54 = 5.873385816812515e-06

Training epoch-74 batch-55
Running loss of epoch-74 batch-55 = 3.3550895750522614e-06

Training epoch-74 batch-56
Running loss of epoch-74 batch-56 = 2.850312739610672e-06

Training epoch-74 batch-57
Running loss of epoch-74 batch-57 = 2.528773620724678e-06

Training epoch-74 batch-58
Running loss of epoch-74 batch-58 = 2.09687277674675e-06

Training epoch-74 batch-59
Running loss of epoch-74 batch-59 = 2.1741725504398346e-06

Training epoch-74 batch-60
Running loss of epoch-74 batch-60 = 8.789356797933578e-07

Training epoch-74 batch-61
Running loss of epoch-74 batch-61 = 6.953952834010124e-06

Training epoch-74 batch-62
Running loss of epoch-74 batch-62 = 3.2836105674505234e-06

Training epoch-74 batch-63
Running loss of epoch-74 batch-63 = 3.6193523555994034e-06

Training epoch-74 batch-64
Running loss of epoch-74 batch-64 = 2.4761538952589035e-06

Training epoch-74 batch-65
Running loss of epoch-74 batch-65 = 1.912703737616539e-06

Training epoch-74 batch-66
Running loss of epoch-74 batch-66 = 3.847526386380196e-06

Training epoch-74 batch-67
Running loss of epoch-74 batch-67 = 3.95788811147213e-06

Training epoch-74 batch-68
Running loss of epoch-74 batch-68 = 7.813796401023865e-07

Training epoch-74 batch-69
Running loss of epoch-74 batch-69 = 5.357200279831886e-06

Training epoch-74 batch-70
Running loss of epoch-74 batch-70 = 3.850553184747696e-06

Training epoch-74 batch-71
Running loss of epoch-74 batch-71 = 2.1837186068296432e-06

Training epoch-74 batch-72
Running loss of epoch-74 batch-72 = 7.682712748646736e-06

Training epoch-74 batch-73
Running loss of epoch-74 batch-73 = 3.426801413297653e-06

Training epoch-74 batch-74
Running loss of epoch-74 batch-74 = 1.8312130123376846e-06

Training epoch-74 batch-75
Running loss of epoch-74 batch-75 = 9.792856872081757e-07

Training epoch-74 batch-76
Running loss of epoch-74 batch-76 = 4.4691842049360275e-06

Training epoch-74 batch-77
Running loss of epoch-74 batch-77 = 2.6801135390996933e-06

Training epoch-74 batch-78
Running loss of epoch-74 batch-78 = 3.2924581319093704e-06

Training epoch-74 batch-79
Running loss of epoch-74 batch-79 = 4.398403689265251e-06

Training epoch-74 batch-80
Running loss of epoch-74 batch-80 = 5.444977432489395e-06

Training epoch-74 batch-81
Running loss of epoch-74 batch-81 = 3.082677721977234e-06

Training epoch-74 batch-82
Running loss of epoch-74 batch-82 = 2.144603058695793e-06

Training epoch-74 batch-83
Running loss of epoch-74 batch-83 = 4.042638465762138e-06

Training epoch-74 batch-84
Running loss of epoch-74 batch-84 = 4.392582923173904e-06

Training epoch-74 batch-85
Running loss of epoch-74 batch-85 = 5.961395800113678e-06

Training epoch-74 batch-86
Running loss of epoch-74 batch-86 = 8.889008313417435e-06

Training epoch-74 batch-87
Running loss of epoch-74 batch-87 = 2.0121224224567413e-06

Training epoch-74 batch-88
Running loss of epoch-74 batch-88 = 4.82192263007164e-06

Training epoch-74 batch-89
Running loss of epoch-74 batch-89 = 2.6100315153598785e-06

Training epoch-74 batch-90
Running loss of epoch-74 batch-90 = 3.3499673008918762e-06

Training epoch-74 batch-91
Running loss of epoch-74 batch-91 = 4.409812390804291e-06

Training epoch-74 batch-92
Running loss of epoch-74 batch-92 = 7.464783266186714e-06

Training epoch-74 batch-93
Running loss of epoch-74 batch-93 = 2.4670735001564026e-06

Training epoch-74 batch-94
Running loss of epoch-74 batch-94 = 7.0158857852220535e-06

Training epoch-74 batch-95
Running loss of epoch-74 batch-95 = 2.391170710325241e-06

Training epoch-74 batch-96
Running loss of epoch-74 batch-96 = 4.734145477414131e-06

Training epoch-74 batch-97
Running loss of epoch-74 batch-97 = 3.26475128531456e-06

Training epoch-74 batch-98
Running loss of epoch-74 batch-98 = 3.680819645524025e-06

Training epoch-74 batch-99
Running loss of epoch-74 batch-99 = 1.7774291336536407e-06

Training epoch-74 batch-100
Running loss of epoch-74 batch-100 = 3.116438165307045e-06

Training epoch-74 batch-101
Running loss of epoch-74 batch-101 = 2.807704731822014e-06

Training epoch-74 batch-102
Running loss of epoch-74 batch-102 = 4.187691956758499e-06

Training epoch-74 batch-103
Running loss of epoch-74 batch-103 = 3.086403012275696e-06

Training epoch-74 batch-104
Running loss of epoch-74 batch-104 = 4.263129085302353e-06

Training epoch-74 batch-105
Running loss of epoch-74 batch-105 = 6.92903995513916e-06

Training epoch-74 batch-106
Running loss of epoch-74 batch-106 = 1.3899989426136017e-06

Training epoch-74 batch-107
Running loss of epoch-74 batch-107 = 3.2025855034589767e-06

Training epoch-74 batch-108
Running loss of epoch-74 batch-108 = 4.381174221634865e-06

Training epoch-74 batch-109
Running loss of epoch-74 batch-109 = 1.6284175217151642e-06

Training epoch-74 batch-110
Running loss of epoch-74 batch-110 = 2.1175947040319443e-06

Training epoch-74 batch-111
Running loss of epoch-74 batch-111 = 6.173504516482353e-06

Training epoch-74 batch-112
Running loss of epoch-74 batch-112 = 1.9439030438661575e-06

Training epoch-74 batch-113
Running loss of epoch-74 batch-113 = 4.034023731946945e-06

Training epoch-74 batch-114
Running loss of epoch-74 batch-114 = 2.6614870876073837e-06

Training epoch-74 batch-115
Running loss of epoch-74 batch-115 = 2.9189977794885635e-06

Training epoch-74 batch-116
Running loss of epoch-74 batch-116 = 8.64081084728241e-06

Training epoch-74 batch-117
Running loss of epoch-74 batch-117 = 4.992354661226273e-06

Training epoch-74 batch-118
Running loss of epoch-74 batch-118 = 3.4354161471128464e-06

Training epoch-74 batch-119
Running loss of epoch-74 batch-119 = 2.321554347872734e-06

Training epoch-74 batch-120
Running loss of epoch-74 batch-120 = 3.818189725279808e-06

Training epoch-74 batch-121
Running loss of epoch-74 batch-121 = 1.7024576663970947e-06

Training epoch-74 batch-122
Running loss of epoch-74 batch-122 = 5.003763362765312e-06

Training epoch-74 batch-123
Running loss of epoch-74 batch-123 = 2.4689361453056335e-06

Training epoch-74 batch-124
Running loss of epoch-74 batch-124 = 3.621913492679596e-06

Training epoch-74 batch-125
Running loss of epoch-74 batch-125 = 3.2784882932901382e-06

Training epoch-74 batch-126
Running loss of epoch-74 batch-126 = 2.51084566116333e-06

Training epoch-74 batch-127
Running loss of epoch-74 batch-127 = 2.6691704988479614e-06

Training epoch-74 batch-128
Running loss of epoch-74 batch-128 = 1.7099082469940186e-06

Training epoch-74 batch-129
Running loss of epoch-74 batch-129 = 1.1174241080880165e-05

Training epoch-74 batch-130
Running loss of epoch-74 batch-130 = 2.786051481962204e-06

Training epoch-74 batch-131
Running loss of epoch-74 batch-131 = 2.5534536689519882e-06

Training epoch-74 batch-132
Running loss of epoch-74 batch-132 = 1.3320241123437881e-06

Training epoch-74 batch-133
Running loss of epoch-74 batch-133 = 3.61073762178421e-06

Training epoch-74 batch-134
Running loss of epoch-74 batch-134 = 4.0065497159957886e-06

Training epoch-74 batch-135
Running loss of epoch-74 batch-135 = 3.1064264476299286e-06

Training epoch-74 batch-136
Running loss of epoch-74 batch-136 = 1.7283018678426743e-06

Training epoch-74 batch-137
Running loss of epoch-74 batch-137 = 1.9131693989038467e-06

Training epoch-74 batch-138
Running loss of epoch-74 batch-138 = 1.453561708331108e-06

Training epoch-74 batch-139
Running loss of epoch-74 batch-139 = 3.484310582280159e-06

Training epoch-74 batch-140
Running loss of epoch-74 batch-140 = 6.605405360460281e-06

Training epoch-74 batch-141
Running loss of epoch-74 batch-141 = 1.928536221385002e-06

Training epoch-74 batch-142
Running loss of epoch-74 batch-142 = 2.61794775724411e-06

Training epoch-74 batch-143
Running loss of epoch-74 batch-143 = 3.901077434420586e-06

Training epoch-74 batch-144
Running loss of epoch-74 batch-144 = 4.553934559226036e-06

Training epoch-74 batch-145
Running loss of epoch-74 batch-145 = 2.794899046421051e-06

Training epoch-74 batch-146
Running loss of epoch-74 batch-146 = 2.668472006917e-06

Training epoch-74 batch-147
Running loss of epoch-74 batch-147 = 2.4060718715190887e-06

Training epoch-74 batch-148
Running loss of epoch-74 batch-148 = 2.6652123779058456e-06

Training epoch-74 batch-149
Running loss of epoch-74 batch-149 = 4.187226295471191e-06

Training epoch-74 batch-150
Running loss of epoch-74 batch-150 = 3.482447937130928e-06

Training epoch-74 batch-151
Running loss of epoch-74 batch-151 = 6.0407910495996475e-06

Training epoch-74 batch-152
Running loss of epoch-74 batch-152 = 4.291767254471779e-06

Training epoch-74 batch-153
Running loss of epoch-74 batch-153 = 4.242872819304466e-06

Training epoch-74 batch-154
Running loss of epoch-74 batch-154 = 2.389773726463318e-06

Training epoch-74 batch-155
Running loss of epoch-74 batch-155 = 3.793509677052498e-06

Training epoch-74 batch-156
Running loss of epoch-74 batch-156 = 1.9532162696123123e-06

Training epoch-74 batch-157
Running loss of epoch-74 batch-157 = 9.991228580474854e-06

Finished training epoch-74.



Average train loss at epoch-74 = 3.6767438054084777e-06

Started Evaluation

Average val loss at epoch-74 = 1.165897136086199

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.00 %

Finished Evaluation



Started training epoch-75


Training epoch-75 batch-1
Running loss of epoch-75 batch-1 = 3.526918590068817e-06

Training epoch-75 batch-2
Running loss of epoch-75 batch-2 = 3.291526809334755e-06

Training epoch-75 batch-3
Running loss of epoch-75 batch-3 = 1.567881554365158e-06

Training epoch-75 batch-4
Running loss of epoch-75 batch-4 = 5.037290975451469e-06

Training epoch-75 batch-5
Running loss of epoch-75 batch-5 = 1.06769148260355e-05

Training epoch-75 batch-6
Running loss of epoch-75 batch-6 = 1.1522788554430008e-06

Training epoch-75 batch-7
Running loss of epoch-75 batch-7 = 2.423534169793129e-06

Training epoch-75 batch-8
Running loss of epoch-75 batch-8 = 2.802349627017975e-06

Training epoch-75 batch-9
Running loss of epoch-75 batch-9 = 6.604474037885666e-06

Training epoch-75 batch-10
Running loss of epoch-75 batch-10 = 2.462649717926979e-06

Training epoch-75 batch-11
Running loss of epoch-75 batch-11 = 4.216330125927925e-06

Training epoch-75 batch-12
Running loss of epoch-75 batch-12 = 1.1185184121131897e-06

Training epoch-75 batch-13
Running loss of epoch-75 batch-13 = 5.355803295969963e-06

Training epoch-75 batch-14
Running loss of epoch-75 batch-14 = 5.554407835006714e-06

Training epoch-75 batch-15
Running loss of epoch-75 batch-15 = 3.5886187106370926e-06

Training epoch-75 batch-16
Running loss of epoch-75 batch-16 = 2.569984644651413e-06

Training epoch-75 batch-17
Running loss of epoch-75 batch-17 = 1.3045500963926315e-06

Training epoch-75 batch-18
Running loss of epoch-75 batch-18 = 2.4756882339715958e-06

Training epoch-75 batch-19
Running loss of epoch-75 batch-19 = 6.00423663854599e-06

Training epoch-75 batch-20
Running loss of epoch-75 batch-20 = 3.296881914138794e-06

Training epoch-75 batch-21
Running loss of epoch-75 batch-21 = 1.7348211258649826e-06

Training epoch-75 batch-22
Running loss of epoch-75 batch-22 = 5.8675650507211685e-06

Training epoch-75 batch-23
Running loss of epoch-75 batch-23 = 4.509463906288147e-06

Training epoch-75 batch-24
Running loss of epoch-75 batch-24 = 1.0132789611816406e-06

Training epoch-75 batch-25
Running loss of epoch-75 batch-25 = 3.7958379834890366e-06

Training epoch-75 batch-26
Running loss of epoch-75 batch-26 = 2.6638153940439224e-06

Training epoch-75 batch-27
Running loss of epoch-75 batch-27 = 2.144835889339447e-06

Training epoch-75 batch-28
Running loss of epoch-75 batch-28 = 3.346474841237068e-06

Training epoch-75 batch-29
Running loss of epoch-75 batch-29 = 3.55415977537632e-06

Training epoch-75 batch-30
Running loss of epoch-75 batch-30 = 2.525048330426216e-06

Training epoch-75 batch-31
Running loss of epoch-75 batch-31 = 3.5243574529886246e-06

Training epoch-75 batch-32
Running loss of epoch-75 batch-32 = 4.367204383015633e-06

Training epoch-75 batch-33
Running loss of epoch-75 batch-33 = 5.000270903110504e-06

Training epoch-75 batch-34
Running loss of epoch-75 batch-34 = 3.1709205359220505e-06

Training epoch-75 batch-35
Running loss of epoch-75 batch-35 = 3.7334393709897995e-06

Training epoch-75 batch-36
Running loss of epoch-75 batch-36 = 2.0728912204504013e-06

Training epoch-75 batch-37
Running loss of epoch-75 batch-37 = 2.5695189833641052e-06

Training epoch-75 batch-38
Running loss of epoch-75 batch-38 = 3.35020013153553e-06

Training epoch-75 batch-39
Running loss of epoch-75 batch-39 = 7.660593837499619e-06

Training epoch-75 batch-40
Running loss of epoch-75 batch-40 = 3.7960708141326904e-06

Training epoch-75 batch-41
Running loss of epoch-75 batch-41 = 2.5671906769275665e-06

Training epoch-75 batch-42
Running loss of epoch-75 batch-42 = 3.0172523111104965e-06

Training epoch-75 batch-43
Running loss of epoch-75 batch-43 = 3.06800939142704e-06

Training epoch-75 batch-44
Running loss of epoch-75 batch-44 = 2.2437889128923416e-06

Training epoch-75 batch-45
Running loss of epoch-75 batch-45 = 2.1934974938631058e-06

Training epoch-75 batch-46
Running loss of epoch-75 batch-46 = 2.70036980509758e-06

Training epoch-75 batch-47
Running loss of epoch-75 batch-47 = 1.258915290236473e-06

Training epoch-75 batch-48
Running loss of epoch-75 batch-48 = 3.977678716182709e-06

Training epoch-75 batch-49
Running loss of epoch-75 batch-49 = 1.8363352864980698e-06

Training epoch-75 batch-50
Running loss of epoch-75 batch-50 = 1.5392433851957321e-06

Training epoch-75 batch-51
Running loss of epoch-75 batch-51 = 4.560919478535652e-06

Training epoch-75 batch-52
Running loss of epoch-75 batch-52 = 2.4102628231048584e-06

Training epoch-75 batch-53
Running loss of epoch-75 batch-53 = 3.9422884583473206e-06

Training epoch-75 batch-54
Running loss of epoch-75 batch-54 = 2.5031622499227524e-06

Training epoch-75 batch-55
Running loss of epoch-75 batch-55 = 2.6426278054714203e-06

Training epoch-75 batch-56
Running loss of epoch-75 batch-56 = 2.2063031792640686e-06

Training epoch-75 batch-57
Running loss of epoch-75 batch-57 = 4.866858944296837e-06

Training epoch-75 batch-58
Running loss of epoch-75 batch-58 = 2.0847655832767487e-06

Training epoch-75 batch-59
Running loss of epoch-75 batch-59 = 4.217261448502541e-06

Training epoch-75 batch-60
Running loss of epoch-75 batch-60 = 1.3099052011966705e-06

Training epoch-75 batch-61
Running loss of epoch-75 batch-61 = 2.6407651603221893e-06

Training epoch-75 batch-62
Running loss of epoch-75 batch-62 = 2.2263266146183014e-06

Training epoch-75 batch-63
Running loss of epoch-75 batch-63 = 6.665708497166634e-06

Training epoch-75 batch-64
Running loss of epoch-75 batch-64 = 2.7299392968416214e-06

Training epoch-75 batch-65
Running loss of epoch-75 batch-65 = 5.5050477385520935e-06

Training epoch-75 batch-66
Running loss of epoch-75 batch-66 = 1.0272487998008728e-06

Training epoch-75 batch-67
Running loss of epoch-75 batch-67 = 3.3364631235599518e-06

Training epoch-75 batch-68
Running loss of epoch-75 batch-68 = 1.898501068353653e-06

Training epoch-75 batch-69
Running loss of epoch-75 batch-69 = 8.665025234222412e-06

Training epoch-75 batch-70
Running loss of epoch-75 batch-70 = 1.3513490557670593e-06

Training epoch-75 batch-71
Running loss of epoch-75 batch-71 = 4.026107490062714e-06

Training epoch-75 batch-72
Running loss of epoch-75 batch-72 = 3.182794898748398e-06

Training epoch-75 batch-73
Running loss of epoch-75 batch-73 = 9.043840691447258e-06

Training epoch-75 batch-74
Running loss of epoch-75 batch-74 = 3.0258670449256897e-06

Training epoch-75 batch-75
Running loss of epoch-75 batch-75 = 3.987690433859825e-06

Training epoch-75 batch-76
Running loss of epoch-75 batch-76 = 3.550434485077858e-06

Training epoch-75 batch-77
Running loss of epoch-75 batch-77 = 2.250541001558304e-06

Training epoch-75 batch-78
Running loss of epoch-75 batch-78 = 2.237968146800995e-06

Training epoch-75 batch-79
Running loss of epoch-75 batch-79 = 5.319016054272652e-06

Training epoch-75 batch-80
Running loss of epoch-75 batch-80 = 2.789543941617012e-06

Training epoch-75 batch-81
Running loss of epoch-75 batch-81 = 8.40076245367527e-06

Training epoch-75 batch-82
Running loss of epoch-75 batch-82 = 5.934387445449829e-06

Training epoch-75 batch-83
Running loss of epoch-75 batch-83 = 6.531598046422005e-06

Training epoch-75 batch-84
Running loss of epoch-75 batch-84 = 3.8899015635252e-06

Training epoch-75 batch-85
Running loss of epoch-75 batch-85 = 2.137385308742523e-06

Training epoch-75 batch-86
Running loss of epoch-75 batch-86 = 4.233792424201965e-06

Training epoch-75 batch-87
Running loss of epoch-75 batch-87 = 5.2014365792274475e-06

Training epoch-75 batch-88
Running loss of epoch-75 batch-88 = 8.356291800737381e-07

Training epoch-75 batch-89
Running loss of epoch-75 batch-89 = 1.932727172970772e-06

Training epoch-75 batch-90
Running loss of epoch-75 batch-90 = 4.801666364073753e-06

Training epoch-75 batch-91
Running loss of epoch-75 batch-91 = 3.752531483769417e-06

Training epoch-75 batch-92
Running loss of epoch-75 batch-92 = 4.63658943772316e-06

Training epoch-75 batch-93
Running loss of epoch-75 batch-93 = 2.605840563774109e-06

Training epoch-75 batch-94
Running loss of epoch-75 batch-94 = 2.71480530500412e-06

Training epoch-75 batch-95
Running loss of epoch-75 batch-95 = 4.244968295097351e-06

Training epoch-75 batch-96
Running loss of epoch-75 batch-96 = 5.827518180012703e-06

Training epoch-75 batch-97
Running loss of epoch-75 batch-97 = 4.8961956053972244e-06

Training epoch-75 batch-98
Running loss of epoch-75 batch-98 = 1.7711427062749863e-06

Training epoch-75 batch-99
Running loss of epoch-75 batch-99 = 5.888752639293671e-06

Training epoch-75 batch-100
Running loss of epoch-75 batch-100 = 1.0302755981683731e-06

Training epoch-75 batch-101
Running loss of epoch-75 batch-101 = 2.159504219889641e-06

Training epoch-75 batch-102
Running loss of epoch-75 batch-102 = 4.132045432925224e-06

Training epoch-75 batch-103
Running loss of epoch-75 batch-103 = 4.344386979937553e-06

Training epoch-75 batch-104
Running loss of epoch-75 batch-104 = 3.623543307185173e-06

Training epoch-75 batch-105
Running loss of epoch-75 batch-105 = 3.4654513001441956e-06

Training epoch-75 batch-106
Running loss of epoch-75 batch-106 = 4.998175427317619e-06

Training epoch-75 batch-107
Running loss of epoch-75 batch-107 = 1.9024591892957687e-06

Training epoch-75 batch-108
Running loss of epoch-75 batch-108 = 3.4724362194538116e-06

Training epoch-75 batch-109
Running loss of epoch-75 batch-109 = 3.0400697141885757e-06

Training epoch-75 batch-110
Running loss of epoch-75 batch-110 = 2.8437934815883636e-06

Training epoch-75 batch-111
Running loss of epoch-75 batch-111 = 2.589542418718338e-06

Training epoch-75 batch-112
Running loss of epoch-75 batch-112 = 5.990033969283104e-06

Training epoch-75 batch-113
Running loss of epoch-75 batch-113 = 2.4579931050539017e-06

Training epoch-75 batch-114
Running loss of epoch-75 batch-114 = 2.732733264565468e-06

Training epoch-75 batch-115
Running loss of epoch-75 batch-115 = 4.281988367438316e-06

Training epoch-75 batch-116
Running loss of epoch-75 batch-116 = 4.694564267992973e-06

Training epoch-75 batch-117
Running loss of epoch-75 batch-117 = 2.873362973332405e-06

Training epoch-75 batch-118
Running loss of epoch-75 batch-118 = 3.0512455850839615e-06

Training epoch-75 batch-119
Running loss of epoch-75 batch-119 = 6.796792149543762e-06

Training epoch-75 batch-120
Running loss of epoch-75 batch-120 = 1.6405247151851654e-06

Training epoch-75 batch-121
Running loss of epoch-75 batch-121 = 1.7317943274974823e-06

Training epoch-75 batch-122
Running loss of epoch-75 batch-122 = 2.4829059839248657e-06

Training epoch-75 batch-123
Running loss of epoch-75 batch-123 = 4.607951268553734e-06

Training epoch-75 batch-124
Running loss of epoch-75 batch-124 = 2.176966518163681e-06

Training epoch-75 batch-125
Running loss of epoch-75 batch-125 = 3.896653652191162e-06

Training epoch-75 batch-126
Running loss of epoch-75 batch-126 = 3.952067345380783e-06

Training epoch-75 batch-127
Running loss of epoch-75 batch-127 = 1.2810109183192253e-05

Training epoch-75 batch-128
Running loss of epoch-75 batch-128 = 3.881519660353661e-06

Training epoch-75 batch-129
Running loss of epoch-75 batch-129 = 2.8098002076148987e-06

Training epoch-75 batch-130
Running loss of epoch-75 batch-130 = 4.317844286561012e-06

Training epoch-75 batch-131
Running loss of epoch-75 batch-131 = 2.407701686024666e-06

Training epoch-75 batch-132
Running loss of epoch-75 batch-132 = 2.47894786298275e-06

Training epoch-75 batch-133
Running loss of epoch-75 batch-133 = 3.518303856253624e-06

Training epoch-75 batch-134
Running loss of epoch-75 batch-134 = 4.98560257256031e-06

Training epoch-75 batch-135
Running loss of epoch-75 batch-135 = 2.3958273231983185e-06

Training epoch-75 batch-136
Running loss of epoch-75 batch-136 = 2.069864422082901e-06

Training epoch-75 batch-137
Running loss of epoch-75 batch-137 = 3.4547410905361176e-06

Training epoch-75 batch-138
Running loss of epoch-75 batch-138 = 9.092967957258224e-06

Training epoch-75 batch-139
Running loss of epoch-75 batch-139 = 4.9103982746601105e-06

Training epoch-75 batch-140
Running loss of epoch-75 batch-140 = 2.5671906769275665e-06

Training epoch-75 batch-141
Running loss of epoch-75 batch-141 = 4.281522706151009e-06

Training epoch-75 batch-142
Running loss of epoch-75 batch-142 = 2.4014152586460114e-06

Training epoch-75 batch-143
Running loss of epoch-75 batch-143 = 3.859866410493851e-06

Training epoch-75 batch-144
Running loss of epoch-75 batch-144 = 2.6980414986610413e-06

Training epoch-75 batch-145
Running loss of epoch-75 batch-145 = 4.618777893483639e-06

Training epoch-75 batch-146
Running loss of epoch-75 batch-146 = 1.3546086847782135e-06

Training epoch-75 batch-147
Running loss of epoch-75 batch-147 = 2.3727770894765854e-06

Training epoch-75 batch-148
Running loss of epoch-75 batch-148 = 5.4903794080019e-06

Training epoch-75 batch-149
Running loss of epoch-75 batch-149 = 6.53974711894989e-06

Training epoch-75 batch-150
Running loss of epoch-75 batch-150 = 3.9299484342336655e-06

Training epoch-75 batch-151
Running loss of epoch-75 batch-151 = 4.58909198641777e-06

Training epoch-75 batch-152
Running loss of epoch-75 batch-152 = 1.4298129826784134e-06

Training epoch-75 batch-153
Running loss of epoch-75 batch-153 = 4.395376890897751e-06

Training epoch-75 batch-154
Running loss of epoch-75 batch-154 = 3.453809767961502e-06

Training epoch-75 batch-155
Running loss of epoch-75 batch-155 = 1.634005457162857e-06

Training epoch-75 batch-156
Running loss of epoch-75 batch-156 = 1.600245013833046e-06

Training epoch-75 batch-157
Running loss of epoch-75 batch-157 = 3.416091203689575e-06

Finished training epoch-75.



Average train loss at epoch-75 = 3.612477332353592e-06

Started Evaluation

Average val loss at epoch-75 = 1.1676818949246843

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.04 %

Finished Evaluation



Started training epoch-76


Training epoch-76 batch-1
Running loss of epoch-76 batch-1 = 2.684071660041809e-06

Training epoch-76 batch-2
Running loss of epoch-76 batch-2 = 3.369757905602455e-06

Training epoch-76 batch-3
Running loss of epoch-76 batch-3 = 4.896428436040878e-06

Training epoch-76 batch-4
Running loss of epoch-76 batch-4 = 1.8386635929346085e-06

Training epoch-76 batch-5
Running loss of epoch-76 batch-5 = 1.1646188795566559e-06

Training epoch-76 batch-6
Running loss of epoch-76 batch-6 = 1.741340383887291e-06

Training epoch-76 batch-7
Running loss of epoch-76 batch-7 = 3.3657997846603394e-06

Training epoch-76 batch-8
Running loss of epoch-76 batch-8 = 5.985144525766373e-06

Training epoch-76 batch-9
Running loss of epoch-76 batch-9 = 6.346032023429871e-06

Training epoch-76 batch-10
Running loss of epoch-76 batch-10 = 3.576977178454399e-06

Training epoch-76 batch-11
Running loss of epoch-76 batch-11 = 3.1674280762672424e-06

Training epoch-76 batch-12
Running loss of epoch-76 batch-12 = 6.668036803603172e-06

Training epoch-76 batch-13
Running loss of epoch-76 batch-13 = 6.888993084430695e-06

Training epoch-76 batch-14
Running loss of epoch-76 batch-14 = 4.265224561095238e-06

Training epoch-76 batch-15
Running loss of epoch-76 batch-15 = 3.177439793944359e-06

Training epoch-76 batch-16
Running loss of epoch-76 batch-16 = 5.801906809210777e-06

Training epoch-76 batch-17
Running loss of epoch-76 batch-17 = 3.221677616238594e-06

Training epoch-76 batch-18
Running loss of epoch-76 batch-18 = 3.1797681003808975e-06

Training epoch-76 batch-19
Running loss of epoch-76 batch-19 = 6.466871127486229e-06

Training epoch-76 batch-20
Running loss of epoch-76 batch-20 = 3.4708064049482346e-06

Training epoch-76 batch-21
Running loss of epoch-76 batch-21 = 2.914806827902794e-06

Training epoch-76 batch-22
Running loss of epoch-76 batch-22 = 4.301778972148895e-06

Training epoch-76 batch-23
Running loss of epoch-76 batch-23 = 2.7203932404518127e-06

Training epoch-76 batch-24
Running loss of epoch-76 batch-24 = 2.8212089091539383e-06

Training epoch-76 batch-25
Running loss of epoch-76 batch-25 = 2.0498409867286682e-06

Training epoch-76 batch-26
Running loss of epoch-76 batch-26 = 2.925284206867218e-06

Training epoch-76 batch-27
Running loss of epoch-76 batch-27 = 1.3345852494239807e-06

Training epoch-76 batch-28
Running loss of epoch-76 batch-28 = 5.2617397159338e-06

Training epoch-76 batch-29
Running loss of epoch-76 batch-29 = 3.3902470022439957e-06

Training epoch-76 batch-30
Running loss of epoch-76 batch-30 = 1.7890706658363342e-06

Training epoch-76 batch-31
Running loss of epoch-76 batch-31 = 5.439389497041702e-06

Training epoch-76 batch-32
Running loss of epoch-76 batch-32 = 1.6456469893455505e-06

Training epoch-76 batch-33
Running loss of epoch-76 batch-33 = 2.7082860469818115e-06

Training epoch-76 batch-34
Running loss of epoch-76 batch-34 = 2.716435119509697e-06

Training epoch-76 batch-35
Running loss of epoch-76 batch-35 = 4.422618076205254e-06

Training epoch-76 batch-36
Running loss of epoch-76 batch-36 = 5.761627107858658e-06

Training epoch-76 batch-37
Running loss of epoch-76 batch-37 = 4.262896254658699e-06

Training epoch-76 batch-38
Running loss of epoch-76 batch-38 = 5.357898771762848e-06

Training epoch-76 batch-39
Running loss of epoch-76 batch-39 = 4.6622008085250854e-06

Training epoch-76 batch-40
Running loss of epoch-76 batch-40 = 2.2673048079013824e-06

Training epoch-76 batch-41
Running loss of epoch-76 batch-41 = 4.5923516154289246e-06

Training epoch-76 batch-42
Running loss of epoch-76 batch-42 = 1.2032687664031982e-05

Training epoch-76 batch-43
Running loss of epoch-76 batch-43 = 4.052184522151947e-06

Training epoch-76 batch-44
Running loss of epoch-76 batch-44 = 1.7362181097269058e-06

Training epoch-76 batch-45
Running loss of epoch-76 batch-45 = 1.9704457372426987e-06

Training epoch-76 batch-46
Running loss of epoch-76 batch-46 = 1.5809200704097748e-06

Training epoch-76 batch-47
Running loss of epoch-76 batch-47 = 1.162523403763771e-06

Training epoch-76 batch-48
Running loss of epoch-76 batch-48 = 4.2782630771398544e-06

Training epoch-76 batch-49
Running loss of epoch-76 batch-49 = 3.0123628675937653e-06

Training epoch-76 batch-50
Running loss of epoch-76 batch-50 = 1.7955899238586426e-06

Training epoch-76 batch-51
Running loss of epoch-76 batch-51 = 3.0624214559793472e-06

Training epoch-76 batch-52
Running loss of epoch-76 batch-52 = 2.08965502679348e-06

Training epoch-76 batch-53
Running loss of epoch-76 batch-53 = 1.5816185623407364e-06

Training epoch-76 batch-54
Running loss of epoch-76 batch-54 = 3.4903641790151596e-06

Training epoch-76 batch-55
Running loss of epoch-76 batch-55 = 3.582332283258438e-06

Training epoch-76 batch-56
Running loss of epoch-76 batch-56 = 8.342321962118149e-07

Training epoch-76 batch-57
Running loss of epoch-76 batch-57 = 1.2796372175216675e-06

Training epoch-76 batch-58
Running loss of epoch-76 batch-58 = 2.341112121939659e-06

Training epoch-76 batch-59
Running loss of epoch-76 batch-59 = 3.518303856253624e-06

Training epoch-76 batch-60
Running loss of epoch-76 batch-60 = 3.4996774047613144e-06

Training epoch-76 batch-61
Running loss of epoch-76 batch-61 = 4.8193614929914474e-06

Training epoch-76 batch-62
Running loss of epoch-76 batch-62 = 2.993270754814148e-06

Training epoch-76 batch-63
Running loss of epoch-76 batch-63 = 7.557682693004608e-07

Training epoch-76 batch-64
Running loss of epoch-76 batch-64 = 2.241227775812149e-06

Training epoch-76 batch-65
Running loss of epoch-76 batch-65 = 3.0633527785539627e-06

Training epoch-76 batch-66
Running loss of epoch-76 batch-66 = 8.719507604837418e-07

Training epoch-76 batch-67
Running loss of epoch-76 batch-67 = 4.3334439396858215e-06

Training epoch-76 batch-68
Running loss of epoch-76 batch-68 = 2.427259460091591e-06

Training epoch-76 batch-69
Running loss of epoch-76 batch-69 = 3.364868462085724e-06

Training epoch-76 batch-70
Running loss of epoch-76 batch-70 = 1.4060642570257187e-06

Training epoch-76 batch-71
Running loss of epoch-76 batch-71 = 2.8409995138645172e-06

Training epoch-76 batch-72
Running loss of epoch-76 batch-72 = 1.700129359960556e-06

Training epoch-76 batch-73
Running loss of epoch-76 batch-73 = 3.103865310549736e-06

Training epoch-76 batch-74
Running loss of epoch-76 batch-74 = 2.996763214468956e-06

Training epoch-76 batch-75
Running loss of epoch-76 batch-75 = 1.8100254237651825e-06

Training epoch-76 batch-76
Running loss of epoch-76 batch-76 = 1.9336584955453873e-06

Training epoch-76 batch-77
Running loss of epoch-76 batch-77 = 1.8647406250238419e-06

Training epoch-76 batch-78
Running loss of epoch-76 batch-78 = 2.489425241947174e-06

Training epoch-76 batch-79
Running loss of epoch-76 batch-79 = 4.736706614494324e-06

Training epoch-76 batch-80
Running loss of epoch-76 batch-80 = 6.45010732114315e-06

Training epoch-76 batch-81
Running loss of epoch-76 batch-81 = 3.902008756995201e-06

Training epoch-76 batch-82
Running loss of epoch-76 batch-82 = 3.0137598514556885e-06

Training epoch-76 batch-83
Running loss of epoch-76 batch-83 = 7.452210411429405e-06

Training epoch-76 batch-84
Running loss of epoch-76 batch-84 = 3.2961834222078323e-06

Training epoch-76 batch-85
Running loss of epoch-76 batch-85 = 8.72812233865261e-06

Training epoch-76 batch-86
Running loss of epoch-76 batch-86 = 6.969785317778587e-06

Training epoch-76 batch-87
Running loss of epoch-76 batch-87 = 3.7085264921188354e-06

Training epoch-76 batch-88
Running loss of epoch-76 batch-88 = 3.689434379339218e-06

Training epoch-76 batch-89
Running loss of epoch-76 batch-89 = 5.502952262759209e-06

Training epoch-76 batch-90
Running loss of epoch-76 batch-90 = 2.0831357687711716e-06

Training epoch-76 batch-91
Running loss of epoch-76 batch-91 = 6.339512765407562e-06

Training epoch-76 batch-92
Running loss of epoch-76 batch-92 = 2.754153683781624e-06

Training epoch-76 batch-93
Running loss of epoch-76 batch-93 = 3.6258716136217117e-06

Training epoch-76 batch-94
Running loss of epoch-76 batch-94 = 6.651272997260094e-06

Training epoch-76 batch-95
Running loss of epoch-76 batch-95 = 2.275221049785614e-06

Training epoch-76 batch-96
Running loss of epoch-76 batch-96 = 6.920658051967621e-06

Training epoch-76 batch-97
Running loss of epoch-76 batch-97 = 4.533212631940842e-06

Training epoch-76 batch-98
Running loss of epoch-76 batch-98 = 3.5883858799934387e-06

Training epoch-76 batch-99
Running loss of epoch-76 batch-99 = 2.3632310330867767e-06

Training epoch-76 batch-100
Running loss of epoch-76 batch-100 = 1.148320734500885e-06

Training epoch-76 batch-101
Running loss of epoch-76 batch-101 = 5.757901817560196e-07

Training epoch-76 batch-102
Running loss of epoch-76 batch-102 = 2.0836014300584793e-06

Training epoch-76 batch-103
Running loss of epoch-76 batch-103 = 6.769085302948952e-06

Training epoch-76 batch-104
Running loss of epoch-76 batch-104 = 1.5618279576301575e-06

Training epoch-76 batch-105
Running loss of epoch-76 batch-105 = 2.3473985493183136e-06

Training epoch-76 batch-106
Running loss of epoch-76 batch-106 = 4.7211069613695145e-06

Training epoch-76 batch-107
Running loss of epoch-76 batch-107 = 1.8470454961061478e-06

Training epoch-76 batch-108
Running loss of epoch-76 batch-108 = 1.7227139323949814e-06

Training epoch-76 batch-109
Running loss of epoch-76 batch-109 = 4.2531173676252365e-06

Training epoch-76 batch-110
Running loss of epoch-76 batch-110 = 2.916436642408371e-06

Training epoch-76 batch-111
Running loss of epoch-76 batch-111 = 2.8426293283700943e-06

Training epoch-76 batch-112
Running loss of epoch-76 batch-112 = 5.514593794941902e-06

Training epoch-76 batch-113
Running loss of epoch-76 batch-113 = 1.9283033907413483e-06

Training epoch-76 batch-114
Running loss of epoch-76 batch-114 = 2.0598527044057846e-06

Training epoch-76 batch-115
Running loss of epoch-76 batch-115 = 3.3529940992593765e-06

Training epoch-76 batch-116
Running loss of epoch-76 batch-116 = 3.988388925790787e-06

Training epoch-76 batch-117
Running loss of epoch-76 batch-117 = 3.9904844015836716e-06

Training epoch-76 batch-118
Running loss of epoch-76 batch-118 = 2.500135451555252e-06

Training epoch-76 batch-119
Running loss of epoch-76 batch-119 = 4.3655745685100555e-06

Training epoch-76 batch-120
Running loss of epoch-76 batch-120 = 1.5187542885541916e-06

Training epoch-76 batch-121
Running loss of epoch-76 batch-121 = 1.3133976608514786e-06

Training epoch-76 batch-122
Running loss of epoch-76 batch-122 = 3.247056156396866e-06

Training epoch-76 batch-123
Running loss of epoch-76 batch-123 = 2.059387043118477e-06

Training epoch-76 batch-124
Running loss of epoch-76 batch-124 = 3.0356459319591522e-06

Training epoch-76 batch-125
Running loss of epoch-76 batch-125 = 2.627028152346611e-06

Training epoch-76 batch-126
Running loss of epoch-76 batch-126 = 3.3723190426826477e-06

Training epoch-76 batch-127
Running loss of epoch-76 batch-127 = 1.8512364476919174e-06

Training epoch-76 batch-128
Running loss of epoch-76 batch-128 = 2.546701580286026e-06

Training epoch-76 batch-129
Running loss of epoch-76 batch-129 = 3.666384145617485e-06

Training epoch-76 batch-130
Running loss of epoch-76 batch-130 = 2.8419308364391327e-06

Training epoch-76 batch-131
Running loss of epoch-76 batch-131 = 5.196547135710716e-06

Training epoch-76 batch-132
Running loss of epoch-76 batch-132 = 4.305969923734665e-06

Training epoch-76 batch-133
Running loss of epoch-76 batch-133 = 4.233326762914658e-06

Training epoch-76 batch-134
Running loss of epoch-76 batch-134 = 3.546476364135742e-06

Training epoch-76 batch-135
Running loss of epoch-76 batch-135 = 5.070120096206665e-06

Training epoch-76 batch-136
Running loss of epoch-76 batch-136 = 2.878718078136444e-06

Training epoch-76 batch-137
Running loss of epoch-76 batch-137 = 1.3292301446199417e-06

Training epoch-76 batch-138
Running loss of epoch-76 batch-138 = 4.9299560487270355e-06

Training epoch-76 batch-139
Running loss of epoch-76 batch-139 = 4.807254299521446e-06

Training epoch-76 batch-140
Running loss of epoch-76 batch-140 = 3.580935299396515e-06

Training epoch-76 batch-141
Running loss of epoch-76 batch-141 = 4.507601261138916e-06

Training epoch-76 batch-142
Running loss of epoch-76 batch-142 = 1.1889263987541199e-05

Training epoch-76 batch-143
Running loss of epoch-76 batch-143 = 3.0174851417541504e-06

Training epoch-76 batch-144
Running loss of epoch-76 batch-144 = 3.879191353917122e-06

Training epoch-76 batch-145
Running loss of epoch-76 batch-145 = 3.3923424780368805e-06

Training epoch-76 batch-146
Running loss of epoch-76 batch-146 = 6.2177423387765884e-06

Training epoch-76 batch-147
Running loss of epoch-76 batch-147 = 4.774890840053558e-06

Training epoch-76 batch-148
Running loss of epoch-76 batch-148 = 1.5762634575366974e-06

Training epoch-76 batch-149
Running loss of epoch-76 batch-149 = 2.7653295546770096e-06

Training epoch-76 batch-150
Running loss of epoch-76 batch-150 = 4.298053681850433e-06

Training epoch-76 batch-151
Running loss of epoch-76 batch-151 = 1.944601535797119e-06

Training epoch-76 batch-152
Running loss of epoch-76 batch-152 = 5.349982529878616e-06

Training epoch-76 batch-153
Running loss of epoch-76 batch-153 = 3.502238541841507e-06

Training epoch-76 batch-154
Running loss of epoch-76 batch-154 = 4.770001396536827e-06

Training epoch-76 batch-155
Running loss of epoch-76 batch-155 = 3.865920007228851e-06

Training epoch-76 batch-156
Running loss of epoch-76 batch-156 = 4.439381882548332e-06

Training epoch-76 batch-157
Running loss of epoch-76 batch-157 = 1.1663883924484253e-05

Finished training epoch-76.



Average train loss at epoch-76 = 3.562738001346588e-06

Started Evaluation

Average val loss at epoch-76 = 1.1687981777454537

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.04 %

Finished Evaluation



Started training epoch-77


Training epoch-77 batch-1
Running loss of epoch-77 batch-1 = 4.02168370783329e-06

Training epoch-77 batch-2
Running loss of epoch-77 batch-2 = 6.170244887471199e-06

Training epoch-77 batch-3
Running loss of epoch-77 batch-3 = 7.20820389688015e-06

Training epoch-77 batch-4
Running loss of epoch-77 batch-4 = 5.140434950590134e-06

Training epoch-77 batch-5
Running loss of epoch-77 batch-5 = 3.1029339879751205e-06

Training epoch-77 batch-6
Running loss of epoch-77 batch-6 = 4.112720489501953e-06

Training epoch-77 batch-7
Running loss of epoch-77 batch-7 = 4.509463906288147e-06

Training epoch-77 batch-8
Running loss of epoch-77 batch-8 = 2.330401912331581e-06

Training epoch-77 batch-9
Running loss of epoch-77 batch-9 = 2.5229528546333313e-06

Training epoch-77 batch-10
Running loss of epoch-77 batch-10 = 9.043142199516296e-07

Training epoch-77 batch-11
Running loss of epoch-77 batch-11 = 5.902722477912903e-06

Training epoch-77 batch-12
Running loss of epoch-77 batch-12 = 3.091758117079735e-06

Training epoch-77 batch-13
Running loss of epoch-77 batch-13 = 3.8673169910907745e-06

Training epoch-77 batch-14
Running loss of epoch-77 batch-14 = 4.628673195838928e-06

Training epoch-77 batch-15
Running loss of epoch-77 batch-15 = 3.2265670597553253e-06

Training epoch-77 batch-16
Running loss of epoch-77 batch-16 = 3.0833762139081955e-06

Training epoch-77 batch-17
Running loss of epoch-77 batch-17 = 5.524372681975365e-06

Training epoch-77 batch-18
Running loss of epoch-77 batch-18 = 2.8118956834077835e-06

Training epoch-77 batch-19
Running loss of epoch-77 batch-19 = 2.635875716805458e-06

Training epoch-77 batch-20
Running loss of epoch-77 batch-20 = 6.366055458784103e-06

Training epoch-77 batch-21
Running loss of epoch-77 batch-21 = 8.922070264816284e-07

Training epoch-77 batch-22
Running loss of epoch-77 batch-22 = 1.4016404747962952e-06

Training epoch-77 batch-23
Running loss of epoch-77 batch-23 = 4.198402166366577e-06

Training epoch-77 batch-24
Running loss of epoch-77 batch-24 = 3.7027057260274887e-06

Training epoch-77 batch-25
Running loss of epoch-77 batch-25 = 9.404262527823448e-06

Training epoch-77 batch-26
Running loss of epoch-77 batch-26 = 1.3727694749832153e-06

Training epoch-77 batch-27
Running loss of epoch-77 batch-27 = 1.993030309677124e-06

Training epoch-77 batch-28
Running loss of epoch-77 batch-28 = 2.3171305656433105e-06

Training epoch-77 batch-29
Running loss of epoch-77 batch-29 = 4.743458703160286e-06

Training epoch-77 batch-30
Running loss of epoch-77 batch-30 = 5.854293704032898e-06

Training epoch-77 batch-31
Running loss of epoch-77 batch-31 = 6.440561264753342e-06

Training epoch-77 batch-32
Running loss of epoch-77 batch-32 = 1.3248063623905182e-06

Training epoch-77 batch-33
Running loss of epoch-77 batch-33 = 2.159504219889641e-06

Training epoch-77 batch-34
Running loss of epoch-77 batch-34 = 2.787448465824127e-06

Training epoch-77 batch-35
Running loss of epoch-77 batch-35 = 1.72504223883152e-06

Training epoch-77 batch-36
Running loss of epoch-77 batch-36 = 4.063127562403679e-06

Training epoch-77 batch-37
Running loss of epoch-77 batch-37 = 1.7480924725532532e-06

Training epoch-77 batch-38
Running loss of epoch-77 batch-38 = 2.3634638637304306e-06

Training epoch-77 batch-39
Running loss of epoch-77 batch-39 = 2.673361450433731e-06

Training epoch-77 batch-40
Running loss of epoch-77 batch-40 = 3.6852434277534485e-06

Training epoch-77 batch-41
Running loss of epoch-77 batch-41 = 2.4097971618175507e-06

Training epoch-77 batch-42
Running loss of epoch-77 batch-42 = 2.7043279260396957e-06

Training epoch-77 batch-43
Running loss of epoch-77 batch-43 = 1.891283318400383e-06

Training epoch-77 batch-44
Running loss of epoch-77 batch-44 = 1.0600779205560684e-06

Training epoch-77 batch-45
Running loss of epoch-77 batch-45 = 8.366536349058151e-06

Training epoch-77 batch-46
Running loss of epoch-77 batch-46 = 2.273824065923691e-06

Training epoch-77 batch-47
Running loss of epoch-77 batch-47 = 1.885928213596344e-06

Training epoch-77 batch-48
Running loss of epoch-77 batch-48 = 2.7886126190423965e-06

Training epoch-77 batch-49
Running loss of epoch-77 batch-49 = 1.5476252883672714e-06

Training epoch-77 batch-50
Running loss of epoch-77 batch-50 = 2.0011793822050095e-06

Training epoch-77 batch-51
Running loss of epoch-77 batch-51 = 3.3797696232795715e-06

Training epoch-77 batch-52
Running loss of epoch-77 batch-52 = 1.7487909644842148e-06

Training epoch-77 batch-53
Running loss of epoch-77 batch-53 = 1.0944902896881104e-05

Training epoch-77 batch-54
Running loss of epoch-77 batch-54 = 2.3383181542158127e-06

Training epoch-77 batch-55
Running loss of epoch-77 batch-55 = 2.8829090297222137e-06

Training epoch-77 batch-56
Running loss of epoch-77 batch-56 = 6.092246621847153e-06

Training epoch-77 batch-57
Running loss of epoch-77 batch-57 = 3.4228432923555374e-06

Training epoch-77 batch-58
Running loss of epoch-77 batch-58 = 6.790272891521454e-06

Training epoch-77 batch-59
Running loss of epoch-77 batch-59 = 1.9490253180265427e-06

Training epoch-77 batch-60
Running loss of epoch-77 batch-60 = 4.768138751387596e-06

Training epoch-77 batch-61
Running loss of epoch-77 batch-61 = 4.182569682598114e-06

Training epoch-77 batch-62
Running loss of epoch-77 batch-62 = 2.100365236401558e-06

Training epoch-77 batch-63
Running loss of epoch-77 batch-63 = 5.54998405277729e-06

Training epoch-77 batch-64
Running loss of epoch-77 batch-64 = 2.7730129659175873e-06

Training epoch-77 batch-65
Running loss of epoch-77 batch-65 = 2.871500328183174e-06

Training epoch-77 batch-66
Running loss of epoch-77 batch-66 = 2.1359883248806e-06

Training epoch-77 batch-67
Running loss of epoch-77 batch-67 = 4.687346518039703e-06

Training epoch-77 batch-68
Running loss of epoch-77 batch-68 = 4.346482455730438e-06

Training epoch-77 batch-69
Running loss of epoch-77 batch-69 = 8.211471140384674e-06

Training epoch-77 batch-70
Running loss of epoch-77 batch-70 = 3.66847962141037e-06

Training epoch-77 batch-71
Running loss of epoch-77 batch-71 = 1.5881378203630447e-06

Training epoch-77 batch-72
Running loss of epoch-77 batch-72 = 3.747176378965378e-06

Training epoch-77 batch-73
Running loss of epoch-77 batch-73 = 6.5583735704422e-06

Training epoch-77 batch-74
Running loss of epoch-77 batch-74 = 2.4328473955392838e-06

Training epoch-77 batch-75
Running loss of epoch-77 batch-75 = 3.1208619475364685e-06

Training epoch-77 batch-76
Running loss of epoch-77 batch-76 = 2.4724286049604416e-06

Training epoch-77 batch-77
Running loss of epoch-77 batch-77 = 3.2135285437107086e-06

Training epoch-77 batch-78
Running loss of epoch-77 batch-78 = 6.763497367501259e-06

Training epoch-77 batch-79
Running loss of epoch-77 batch-79 = 3.1476374715566635e-06

Training epoch-77 batch-80
Running loss of epoch-77 batch-80 = 1.5289988368749619e-06

Training epoch-77 batch-81
Running loss of epoch-77 batch-81 = 5.86523674428463e-06

Training epoch-77 batch-82
Running loss of epoch-77 batch-82 = 1.4652032405138016e-06

Training epoch-77 batch-83
Running loss of epoch-77 batch-83 = 3.1364616006612778e-06

Training epoch-77 batch-84
Running loss of epoch-77 batch-84 = 3.1660310924053192e-06

Training epoch-77 batch-85
Running loss of epoch-77 batch-85 = 4.334840923547745e-06

Training epoch-77 batch-86
Running loss of epoch-77 batch-86 = 6.27920962870121e-06

Training epoch-77 batch-87
Running loss of epoch-77 batch-87 = 3.332272171974182e-06

Training epoch-77 batch-88
Running loss of epoch-77 batch-88 = 6.314367055892944e-06

Training epoch-77 batch-89
Running loss of epoch-77 batch-89 = 2.103857696056366e-06

Training epoch-77 batch-90
Running loss of epoch-77 batch-90 = 3.0528753995895386e-06

Training epoch-77 batch-91
Running loss of epoch-77 batch-91 = 2.4228356778621674e-06

Training epoch-77 batch-92
Running loss of epoch-77 batch-92 = 2.0354054868221283e-06

Training epoch-77 batch-93
Running loss of epoch-77 batch-93 = 3.5280827432870865e-06

Training epoch-77 batch-94
Running loss of epoch-77 batch-94 = 1.1886004358530045e-06

Training epoch-77 batch-95
Running loss of epoch-77 batch-95 = 1.39651820063591e-06

Training epoch-77 batch-96
Running loss of epoch-77 batch-96 = 2.3185275495052338e-06

Training epoch-77 batch-97
Running loss of epoch-77 batch-97 = 2.273358404636383e-06

Training epoch-77 batch-98
Running loss of epoch-77 batch-98 = 9.038485586643219e-07

Training epoch-77 batch-99
Running loss of epoch-77 batch-99 = 4.023080691695213e-06

Training epoch-77 batch-100
Running loss of epoch-77 batch-100 = 4.636123776435852e-06

Training epoch-77 batch-101
Running loss of epoch-77 batch-101 = 3.3851247280836105e-06

Training epoch-77 batch-102
Running loss of epoch-77 batch-102 = 3.216788172721863e-06

Training epoch-77 batch-103
Running loss of epoch-77 batch-103 = 4.104338586330414e-06

Training epoch-77 batch-104
Running loss of epoch-77 batch-104 = 4.982808604836464e-06

Training epoch-77 batch-105
Running loss of epoch-77 batch-105 = 2.7050264179706573e-06

Training epoch-77 batch-106
Running loss of epoch-77 batch-106 = 2.059387043118477e-06

Training epoch-77 batch-107
Running loss of epoch-77 batch-107 = 2.769753336906433e-06

Training epoch-77 batch-108
Running loss of epoch-77 batch-108 = 5.194684490561485e-06

Training epoch-77 batch-109
Running loss of epoch-77 batch-109 = 6.7909713834524155e-06

Training epoch-77 batch-110
Running loss of epoch-77 batch-110 = 4.391418769955635e-06

Training epoch-77 batch-111
Running loss of epoch-77 batch-111 = 4.13903035223484e-06

Training epoch-77 batch-112
Running loss of epoch-77 batch-112 = 1.3434328138828278e-06

Training epoch-77 batch-113
Running loss of epoch-77 batch-113 = 3.2510142773389816e-06

Training epoch-77 batch-114
Running loss of epoch-77 batch-114 = 4.986068233847618e-06

Training epoch-77 batch-115
Running loss of epoch-77 batch-115 = 4.359753802418709e-06

Training epoch-77 batch-116
Running loss of epoch-77 batch-116 = 3.3045653253793716e-06

Training epoch-77 batch-117
Running loss of epoch-77 batch-117 = 4.507135599851608e-06

Training epoch-77 batch-118
Running loss of epoch-77 batch-118 = 3.4228432923555374e-06

Training epoch-77 batch-119
Running loss of epoch-77 batch-119 = 2.8116628527641296e-06

Training epoch-77 batch-120
Running loss of epoch-77 batch-120 = 2.6691704988479614e-06

Training epoch-77 batch-121
Running loss of epoch-77 batch-121 = 1.5236437320709229e-06

Training epoch-77 batch-122
Running loss of epoch-77 batch-122 = 4.344852641224861e-06

Training epoch-77 batch-123
Running loss of epoch-77 batch-123 = 3.069872036576271e-06

Training epoch-77 batch-124
Running loss of epoch-77 batch-124 = 1.7241109162569046e-06

Training epoch-77 batch-125
Running loss of epoch-77 batch-125 = 2.487562596797943e-06

Training epoch-77 batch-126
Running loss of epoch-77 batch-126 = 1.810956746339798e-06

Training epoch-77 batch-127
Running loss of epoch-77 batch-127 = 2.525513991713524e-06

Training epoch-77 batch-128
Running loss of epoch-77 batch-128 = 4.2442698031663895e-06

Training epoch-77 batch-129
Running loss of epoch-77 batch-129 = 6.1891041696071625e-06

Training epoch-77 batch-130
Running loss of epoch-77 batch-130 = 4.540197551250458e-06

Training epoch-77 batch-131
Running loss of epoch-77 batch-131 = 9.601935744285583e-07

Training epoch-77 batch-132
Running loss of epoch-77 batch-132 = 2.1527521312236786e-06

Training epoch-77 batch-133
Running loss of epoch-77 batch-133 = 4.166038706898689e-06

Training epoch-77 batch-134
Running loss of epoch-77 batch-134 = 3.903638571500778e-06

Training epoch-77 batch-135
Running loss of epoch-77 batch-135 = 2.7155037969350815e-06

Training epoch-77 batch-136
Running loss of epoch-77 batch-136 = 4.6174973249435425e-06

Training epoch-77 batch-137
Running loss of epoch-77 batch-137 = 1.509208232164383e-06

Training epoch-77 batch-138
Running loss of epoch-77 batch-138 = 1.7213169485330582e-06

Training epoch-77 batch-139
Running loss of epoch-77 batch-139 = 4.763714969158173e-06

Training epoch-77 batch-140
Running loss of epoch-77 batch-140 = 2.312706783413887e-06

Training epoch-77 batch-141
Running loss of epoch-77 batch-141 = 3.831228241324425e-06

Training epoch-77 batch-142
Running loss of epoch-77 batch-142 = 9.452924132347107e-07

Training epoch-77 batch-143
Running loss of epoch-77 batch-143 = 2.735992893576622e-06

Training epoch-77 batch-144
Running loss of epoch-77 batch-144 = 3.329245373606682e-06

Training epoch-77 batch-145
Running loss of epoch-77 batch-145 = 3.4910626709461212e-06

Training epoch-77 batch-146
Running loss of epoch-77 batch-146 = 3.8871075958013535e-06

Training epoch-77 batch-147
Running loss of epoch-77 batch-147 = 4.348112270236015e-06

Training epoch-77 batch-148
Running loss of epoch-77 batch-148 = 2.803746610879898e-06

Training epoch-77 batch-149
Running loss of epoch-77 batch-149 = 1.40373595058918e-06

Training epoch-77 batch-150
Running loss of epoch-77 batch-150 = 4.1048042476177216e-06

Training epoch-77 batch-151
Running loss of epoch-77 batch-151 = 2.202345058321953e-06

Training epoch-77 batch-152
Running loss of epoch-77 batch-152 = 2.4943146854639053e-06

Training epoch-77 batch-153
Running loss of epoch-77 batch-153 = 1.225154846906662e-06

Training epoch-77 batch-154
Running loss of epoch-77 batch-154 = 4.14624810218811e-06

Training epoch-77 batch-155
Running loss of epoch-77 batch-155 = 5.379319190979004e-06

Training epoch-77 batch-156
Running loss of epoch-77 batch-156 = 2.2649765014648438e-06

Training epoch-77 batch-157
Running loss of epoch-77 batch-157 = 6.9141387939453125e-06

Finished training epoch-77.



Average train loss at epoch-77 = 3.469665348529816e-06

Started Evaluation

Average val loss at epoch-77 = 1.1696455439431175

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 42.57 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.08 %

Finished Evaluation



Started training epoch-78


Training epoch-78 batch-1
Running loss of epoch-78 batch-1 = 2.1453015506267548e-06

Training epoch-78 batch-2
Running loss of epoch-78 batch-2 = 2.8940849006175995e-06

Training epoch-78 batch-3
Running loss of epoch-78 batch-3 = 4.4924672693014145e-06

Training epoch-78 batch-4
Running loss of epoch-78 batch-4 = 4.214933142066002e-06

Training epoch-78 batch-5
Running loss of epoch-78 batch-5 = 2.725515514612198e-06

Training epoch-78 batch-6
Running loss of epoch-78 batch-6 = 2.084067091345787e-06

Training epoch-78 batch-7
Running loss of epoch-78 batch-7 = 4.2263418436050415e-06

Training epoch-78 batch-8
Running loss of epoch-78 batch-8 = 2.4067703634500504e-06

Training epoch-78 batch-9
Running loss of epoch-78 batch-9 = 3.6116689443588257e-06

Training epoch-78 batch-10
Running loss of epoch-78 batch-10 = 2.4980399757623672e-06

Training epoch-78 batch-11
Running loss of epoch-78 batch-11 = 3.880588337779045e-06

Training epoch-78 batch-12
Running loss of epoch-78 batch-12 = 2.680346369743347e-06

Training epoch-78 batch-13
Running loss of epoch-78 batch-13 = 3.632856532931328e-06

Training epoch-78 batch-14
Running loss of epoch-78 batch-14 = 3.160908818244934e-06

Training epoch-78 batch-15
Running loss of epoch-78 batch-15 = 2.830987796187401e-06

Training epoch-78 batch-16
Running loss of epoch-78 batch-16 = 2.669868990778923e-06

Training epoch-78 batch-17
Running loss of epoch-78 batch-17 = 4.393747076392174e-06

Training epoch-78 batch-18
Running loss of epoch-78 batch-18 = 5.604233592748642e-06

Training epoch-78 batch-19
Running loss of epoch-78 batch-19 = 2.119923010468483e-06

Training epoch-78 batch-20
Running loss of epoch-78 batch-20 = 6.488524377346039e-06

Training epoch-78 batch-21
Running loss of epoch-78 batch-21 = 1.1406373232603073e-06

Training epoch-78 batch-22
Running loss of epoch-78 batch-22 = 3.201887011528015e-06

Training epoch-78 batch-23
Running loss of epoch-78 batch-23 = 1.5771947801113129e-06

Training epoch-78 batch-24
Running loss of epoch-78 batch-24 = 3.938097506761551e-06

Training epoch-78 batch-25
Running loss of epoch-78 batch-25 = 2.618413418531418e-06

Training epoch-78 batch-26
Running loss of epoch-78 batch-26 = 4.092231392860413e-06

Training epoch-78 batch-27
Running loss of epoch-78 batch-27 = 3.4403055906295776e-06

Training epoch-78 batch-28
Running loss of epoch-78 batch-28 = 4.0263403207063675e-06

Training epoch-78 batch-29
Running loss of epoch-78 batch-29 = 1.875218003988266e-06

Training epoch-78 batch-30
Running loss of epoch-78 batch-30 = 1.7138663679361343e-06

Training epoch-78 batch-31
Running loss of epoch-78 batch-31 = 8.481554687023163e-06

Training epoch-78 batch-32
Running loss of epoch-78 batch-32 = 8.491799235343933e-06

Training epoch-78 batch-33
Running loss of epoch-78 batch-33 = 3.455672413110733e-06

Training epoch-78 batch-34
Running loss of epoch-78 batch-34 = 2.3872125893831253e-06

Training epoch-78 batch-35
Running loss of epoch-78 batch-35 = 7.929280400276184e-06

Training epoch-78 batch-36
Running loss of epoch-78 batch-36 = 1.3350509107112885e-06

Training epoch-78 batch-37
Running loss of epoch-78 batch-37 = 2.6274938136339188e-06

Training epoch-78 batch-38
Running loss of epoch-78 batch-38 = 1.7182901501655579e-06

Training epoch-78 batch-39
Running loss of epoch-78 batch-39 = 4.139961674809456e-06

Training epoch-78 batch-40
Running loss of epoch-78 batch-40 = 3.29664908349514e-06

Training epoch-78 batch-41
Running loss of epoch-78 batch-41 = 2.427026629447937e-06

Training epoch-78 batch-42
Running loss of epoch-78 batch-42 = 2.9101502150297165e-06

Training epoch-78 batch-43
Running loss of epoch-78 batch-43 = 4.0156301110982895e-06

Training epoch-78 batch-44
Running loss of epoch-78 batch-44 = 1.4083925634622574e-06

Training epoch-78 batch-45
Running loss of epoch-78 batch-45 = 1.0170042514801025e-06

Training epoch-78 batch-46
Running loss of epoch-78 batch-46 = 3.5027042031288147e-06

Training epoch-78 batch-47
Running loss of epoch-78 batch-47 = 2.7657952159643173e-06

Training epoch-78 batch-48
Running loss of epoch-78 batch-48 = 3.491761162877083e-06

Training epoch-78 batch-49
Running loss of epoch-78 batch-49 = 3.0051451176404953e-06

Training epoch-78 batch-50
Running loss of epoch-78 batch-50 = 1.6526319086551666e-06

Training epoch-78 batch-51
Running loss of epoch-78 batch-51 = 3.291526809334755e-06

Training epoch-78 batch-52
Running loss of epoch-78 batch-52 = 2.0756851881742477e-06

Training epoch-78 batch-53
Running loss of epoch-78 batch-53 = 2.1744053810834885e-06

Training epoch-78 batch-54
Running loss of epoch-78 batch-54 = 3.1958334147930145e-06

Training epoch-78 batch-55
Running loss of epoch-78 batch-55 = 4.641478881239891e-06

Training epoch-78 batch-56
Running loss of epoch-78 batch-56 = 3.684312105178833e-06

Training epoch-78 batch-57
Running loss of epoch-78 batch-57 = 1.8943101167678833e-06

Training epoch-78 batch-58
Running loss of epoch-78 batch-58 = 5.603069439530373e-06

Training epoch-78 batch-59
Running loss of epoch-78 batch-59 = 6.669666618108749e-06

Training epoch-78 batch-60
Running loss of epoch-78 batch-60 = 1.3872049748897552e-06

Training epoch-78 batch-61
Running loss of epoch-78 batch-61 = 3.5981647670269012e-06

Training epoch-78 batch-62
Running loss of epoch-78 batch-62 = 4.171626642346382e-06

Training epoch-78 batch-63
Running loss of epoch-78 batch-63 = 4.888279363512993e-06

Training epoch-78 batch-64
Running loss of epoch-78 batch-64 = 2.9739458113908768e-06

Training epoch-78 batch-65
Running loss of epoch-78 batch-65 = 4.488043487071991e-06

Training epoch-78 batch-66
Running loss of epoch-78 batch-66 = 2.1364539861679077e-06

Training epoch-78 batch-67
Running loss of epoch-78 batch-67 = 2.5639310479164124e-06

Training epoch-78 batch-68
Running loss of epoch-78 batch-68 = 4.6370550990104675e-06

Training epoch-78 batch-69
Running loss of epoch-78 batch-69 = 6.061512976884842e-06

Training epoch-78 batch-70
Running loss of epoch-78 batch-70 = 5.9998128563165665e-06

Training epoch-78 batch-71
Running loss of epoch-78 batch-71 = 5.390029400587082e-06

Training epoch-78 batch-72
Running loss of epoch-78 batch-72 = 3.5013072192668915e-06

Training epoch-78 batch-73
Running loss of epoch-78 batch-73 = 2.871733158826828e-06

Training epoch-78 batch-74
Running loss of epoch-78 batch-74 = 2.3297034204006195e-06

Training epoch-78 batch-75
Running loss of epoch-78 batch-75 = 7.485039532184601e-06

Training epoch-78 batch-76
Running loss of epoch-78 batch-76 = 4.098750650882721e-06

Training epoch-78 batch-77
Running loss of epoch-78 batch-77 = 4.855915904045105e-06

Training epoch-78 batch-78
Running loss of epoch-78 batch-78 = 5.280482582747936e-06

Training epoch-78 batch-79
Running loss of epoch-78 batch-79 = 1.8316786736249924e-06

Training epoch-78 batch-80
Running loss of epoch-78 batch-80 = 2.4689361453056335e-06

Training epoch-78 batch-81
Running loss of epoch-78 batch-81 = 4.178611561655998e-06

Training epoch-78 batch-82
Running loss of epoch-78 batch-82 = 3.5779085010290146e-06

Training epoch-78 batch-83
Running loss of epoch-78 batch-83 = 2.9064249247312546e-06

Training epoch-78 batch-84
Running loss of epoch-78 batch-84 = 4.760688170790672e-06

Training epoch-78 batch-85
Running loss of epoch-78 batch-85 = 2.5187619030475616e-06

Training epoch-78 batch-86
Running loss of epoch-78 batch-86 = 1.330627128481865e-06

Training epoch-78 batch-87
Running loss of epoch-78 batch-87 = 2.9604416340589523e-06

Training epoch-78 batch-88
Running loss of epoch-78 batch-88 = 3.953929990530014e-06

Training epoch-78 batch-89
Running loss of epoch-78 batch-89 = 1.5094410628080368e-06

Training epoch-78 batch-90
Running loss of epoch-78 batch-90 = 2.337386831641197e-06

Training epoch-78 batch-91
Running loss of epoch-78 batch-91 = 2.292916178703308e-06

Training epoch-78 batch-92
Running loss of epoch-78 batch-92 = 3.6675482988357544e-06

Training epoch-78 batch-93
Running loss of epoch-78 batch-93 = 2.0510051399469376e-06

Training epoch-78 batch-94
Running loss of epoch-78 batch-94 = 4.807952791452408e-06

Training epoch-78 batch-95
Running loss of epoch-78 batch-95 = 4.991423338651657e-06

Training epoch-78 batch-96
Running loss of epoch-78 batch-96 = 6.912741810083389e-07

Training epoch-78 batch-97
Running loss of epoch-78 batch-97 = 1.2291129678487778e-06

Training epoch-78 batch-98
Running loss of epoch-78 batch-98 = 2.541346475481987e-06

Training epoch-78 batch-99
Running loss of epoch-78 batch-99 = 2.166023477911949e-06

Training epoch-78 batch-100
Running loss of epoch-78 batch-100 = 1.878943294286728e-06

Training epoch-78 batch-101
Running loss of epoch-78 batch-101 = 4.292232915759087e-06

Training epoch-78 batch-102
Running loss of epoch-78 batch-102 = 5.7551078498363495e-06

Training epoch-78 batch-103
Running loss of epoch-78 batch-103 = 2.522021532058716e-06

Training epoch-78 batch-104
Running loss of epoch-78 batch-104 = 2.5941990315914154e-06

Training epoch-78 batch-105
Running loss of epoch-78 batch-105 = 2.8186477720737457e-06

Training epoch-78 batch-106
Running loss of epoch-78 batch-106 = 1.7951242625713348e-06

Training epoch-78 batch-107
Running loss of epoch-78 batch-107 = 1.651933416724205e-06

Training epoch-78 batch-108
Running loss of epoch-78 batch-108 = 5.036592483520508e-06

Training epoch-78 batch-109
Running loss of epoch-78 batch-109 = 4.727160558104515e-06

Training epoch-78 batch-110
Running loss of epoch-78 batch-110 = 2.6314519345760345e-06

Training epoch-78 batch-111
Running loss of epoch-78 batch-111 = 4.956033080816269e-06

Training epoch-78 batch-112
Running loss of epoch-78 batch-112 = 2.9387883841991425e-06

Training epoch-78 batch-113
Running loss of epoch-78 batch-113 = 3.0351802706718445e-06

Training epoch-78 batch-114
Running loss of epoch-78 batch-114 = 2.9299408197402954e-06

Training epoch-78 batch-115
Running loss of epoch-78 batch-115 = 3.1369272619485855e-06

Training epoch-78 batch-116
Running loss of epoch-78 batch-116 = 2.5008339434862137e-06

Training epoch-78 batch-117
Running loss of epoch-78 batch-117 = 6.252434104681015e-06

Training epoch-78 batch-118
Running loss of epoch-78 batch-118 = 3.735767677426338e-06

Training epoch-78 batch-119
Running loss of epoch-78 batch-119 = 3.607477992773056e-06

Training epoch-78 batch-120
Running loss of epoch-78 batch-120 = 4.022382199764252e-06

Training epoch-78 batch-121
Running loss of epoch-78 batch-121 = 4.883855581283569e-06

Training epoch-78 batch-122
Running loss of epoch-78 batch-122 = 4.30806539952755e-06

Training epoch-78 batch-123
Running loss of epoch-78 batch-123 = 4.186760634183884e-06

Training epoch-78 batch-124
Running loss of epoch-78 batch-124 = 8.872710168361664e-06

Training epoch-78 batch-125
Running loss of epoch-78 batch-125 = 2.6118941605091095e-06

Training epoch-78 batch-126
Running loss of epoch-78 batch-126 = 3.0493829399347305e-06

Training epoch-78 batch-127
Running loss of epoch-78 batch-127 = 7.969094440340996e-06

Training epoch-78 batch-128
Running loss of epoch-78 batch-128 = 2.5259796530008316e-06

Training epoch-78 batch-129
Running loss of epoch-78 batch-129 = 8.697621524333954e-06

Training epoch-78 batch-130
Running loss of epoch-78 batch-130 = 3.5420525819063187e-06

Training epoch-78 batch-131
Running loss of epoch-78 batch-131 = 3.6677811294794083e-06

Training epoch-78 batch-132
Running loss of epoch-78 batch-132 = 1.178821548819542e-06

Training epoch-78 batch-133
Running loss of epoch-78 batch-133 = 1.16787850856781e-06

Training epoch-78 batch-134
Running loss of epoch-78 batch-134 = 1.8677674233913422e-06

Training epoch-78 batch-135
Running loss of epoch-78 batch-135 = 3.019813448190689e-06

Training epoch-78 batch-136
Running loss of epoch-78 batch-136 = 2.73948535323143e-06

Training epoch-78 batch-137
Running loss of epoch-78 batch-137 = 4.836125299334526e-06

Training epoch-78 batch-138
Running loss of epoch-78 batch-138 = 1.2512318789958954e-06

Training epoch-78 batch-139
Running loss of epoch-78 batch-139 = 1.1897645890712738e-06

Training epoch-78 batch-140
Running loss of epoch-78 batch-140 = 1.5231780707836151e-06

Training epoch-78 batch-141
Running loss of epoch-78 batch-141 = 1.969514414668083e-06

Training epoch-78 batch-142
Running loss of epoch-78 batch-142 = 3.8139987736940384e-06

Training epoch-78 batch-143
Running loss of epoch-78 batch-143 = 3.0936207622289658e-06

Training epoch-78 batch-144
Running loss of epoch-78 batch-144 = 2.522021532058716e-06

Training epoch-78 batch-145
Running loss of epoch-78 batch-145 = 3.1352974474430084e-06

Training epoch-78 batch-146
Running loss of epoch-78 batch-146 = 3.4957192838191986e-06

Training epoch-78 batch-147
Running loss of epoch-78 batch-147 = 3.888970240950584e-06

Training epoch-78 batch-148
Running loss of epoch-78 batch-148 = 7.213093340396881e-07

Training epoch-78 batch-149
Running loss of epoch-78 batch-149 = 2.4370383471250534e-06

Training epoch-78 batch-150
Running loss of epoch-78 batch-150 = 3.438210114836693e-06

Training epoch-78 batch-151
Running loss of epoch-78 batch-151 = 3.962544724345207e-06

Training epoch-78 batch-152
Running loss of epoch-78 batch-152 = 4.470348358154297e-06

Training epoch-78 batch-153
Running loss of epoch-78 batch-153 = 1.2314412742853165e-06

Training epoch-78 batch-154
Running loss of epoch-78 batch-154 = 1.5811529010534286e-06

Training epoch-78 batch-155
Running loss of epoch-78 batch-155 = 5.289679393172264e-06

Training epoch-78 batch-156
Running loss of epoch-78 batch-156 = 2.0640436559915543e-06

Training epoch-78 batch-157
Running loss of epoch-78 batch-157 = 9.667128324508667e-06

Finished training epoch-78.



Average train loss at epoch-78 = 3.408687561750412e-06

Started Evaluation

Average val loss at epoch-78 = 1.172929891480617

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.06 %

Finished Evaluation



Started training epoch-79


Training epoch-79 batch-1
Running loss of epoch-79 batch-1 = 1.6884878277778625e-06

Training epoch-79 batch-2
Running loss of epoch-79 batch-2 = 1.0905787348747253e-06

Training epoch-79 batch-3
Running loss of epoch-79 batch-3 = 2.8829090297222137e-06

Training epoch-79 batch-4
Running loss of epoch-79 batch-4 = 2.304092049598694e-06

Training epoch-79 batch-5
Running loss of epoch-79 batch-5 = 4.920409992337227e-06

Training epoch-79 batch-6
Running loss of epoch-79 batch-6 = 2.6973430067300797e-06

Training epoch-79 batch-7
Running loss of epoch-79 batch-7 = 1.780688762664795e-06

Training epoch-79 batch-8
Running loss of epoch-79 batch-8 = 7.360009476542473e-06

Training epoch-79 batch-9
Running loss of epoch-79 batch-9 = 6.926944479346275e-06

Training epoch-79 batch-10
Running loss of epoch-79 batch-10 = 2.202112227678299e-06

Training epoch-79 batch-11
Running loss of epoch-79 batch-11 = 6.416579708456993e-06

Training epoch-79 batch-12
Running loss of epoch-79 batch-12 = 3.4803524613380432e-06

Training epoch-79 batch-13
Running loss of epoch-79 batch-13 = 4.034722223877907e-06

Training epoch-79 batch-14
Running loss of epoch-79 batch-14 = 1.2121163308620453e-06

Training epoch-79 batch-15
Running loss of epoch-79 batch-15 = 3.734137862920761e-06

Training epoch-79 batch-16
Running loss of epoch-79 batch-16 = 5.316920578479767e-06

Training epoch-79 batch-17
Running loss of epoch-79 batch-17 = 2.505956217646599e-06

Training epoch-79 batch-18
Running loss of epoch-79 batch-18 = 9.848736226558685e-07

Training epoch-79 batch-19
Running loss of epoch-79 batch-19 = 2.2426247596740723e-06

Training epoch-79 batch-20
Running loss of epoch-79 batch-20 = 3.4240074455738068e-06

Training epoch-79 batch-21
Running loss of epoch-79 batch-21 = 2.5818590074777603e-06

Training epoch-79 batch-22
Running loss of epoch-79 batch-22 = 2.7671921998262405e-06

Training epoch-79 batch-23
Running loss of epoch-79 batch-23 = 1.737847924232483e-06

Training epoch-79 batch-24
Running loss of epoch-79 batch-24 = 5.421694368124008e-06

Training epoch-79 batch-25
Running loss of epoch-79 batch-25 = 3.413064405322075e-06

Training epoch-79 batch-26
Running loss of epoch-79 batch-26 = 2.223765477538109e-06

Training epoch-79 batch-27
Running loss of epoch-79 batch-27 = 3.19676473736763e-06

Training epoch-79 batch-28
Running loss of epoch-79 batch-28 = 1.8891878426074982e-06

Training epoch-79 batch-29
Running loss of epoch-79 batch-29 = 5.2852556109428406e-06

Training epoch-79 batch-30
Running loss of epoch-79 batch-30 = 2.835877239704132e-06

Training epoch-79 batch-31
Running loss of epoch-79 batch-31 = 4.8193614929914474e-06

Training epoch-79 batch-32
Running loss of epoch-79 batch-32 = 3.539491444826126e-06

Training epoch-79 batch-33
Running loss of epoch-79 batch-33 = 8.369795978069305e-06

Training epoch-79 batch-34
Running loss of epoch-79 batch-34 = 2.9639340937137604e-06

Training epoch-79 batch-35
Running loss of epoch-79 batch-35 = 3.2691750675439835e-06

Training epoch-79 batch-36
Running loss of epoch-79 batch-36 = 2.6510097086429596e-06

Training epoch-79 batch-37
Running loss of epoch-79 batch-37 = 2.2954773157835007e-06

Training epoch-79 batch-38
Running loss of epoch-79 batch-38 = 1.051323488354683e-05

Training epoch-79 batch-39
Running loss of epoch-79 batch-39 = 1.8028076738119125e-06

Training epoch-79 batch-40
Running loss of epoch-79 batch-40 = 2.642860636115074e-06

Training epoch-79 batch-41
Running loss of epoch-79 batch-41 = 2.360669896006584e-06

Training epoch-79 batch-42
Running loss of epoch-79 batch-42 = 4.9532391130924225e-06

Training epoch-79 batch-43
Running loss of epoch-79 batch-43 = 3.589317202568054e-06

Training epoch-79 batch-44
Running loss of epoch-79 batch-44 = 4.5942142605781555e-06

Training epoch-79 batch-45
Running loss of epoch-79 batch-45 = 4.337984137237072e-06

Training epoch-79 batch-46
Running loss of epoch-79 batch-46 = 2.3026950657367706e-06

Training epoch-79 batch-47
Running loss of epoch-79 batch-47 = 2.8794165700674057e-06

Training epoch-79 batch-48
Running loss of epoch-79 batch-48 = 2.0873267203569412e-06

Training epoch-79 batch-49
Running loss of epoch-79 batch-49 = 2.216314896941185e-06

Training epoch-79 batch-50
Running loss of epoch-79 batch-50 = 5.204463377594948e-06

Training epoch-79 batch-51
Running loss of epoch-79 batch-51 = 2.028653398156166e-06

Training epoch-79 batch-52
Running loss of epoch-79 batch-52 = 4.695495590567589e-06

Training epoch-79 batch-53
Running loss of epoch-79 batch-53 = 2.8817448765039444e-06

Training epoch-79 batch-54
Running loss of epoch-79 batch-54 = 2.2621825337409973e-06

Training epoch-79 batch-55
Running loss of epoch-79 batch-55 = 4.737405106425285e-06

Training epoch-79 batch-56
Running loss of epoch-79 batch-56 = 2.869870513677597e-06

Training epoch-79 batch-57
Running loss of epoch-79 batch-57 = 5.350913852453232e-06

Training epoch-79 batch-58
Running loss of epoch-79 batch-58 = 1.644948497414589e-06

Training epoch-79 batch-59
Running loss of epoch-79 batch-59 = 2.2370368242263794e-06

Training epoch-79 batch-60
Running loss of epoch-79 batch-60 = 5.334150046110153e-06

Training epoch-79 batch-61
Running loss of epoch-79 batch-61 = 1.878710463643074e-06

Training epoch-79 batch-62
Running loss of epoch-79 batch-62 = 9.011942893266678e-06

Training epoch-79 batch-63
Running loss of epoch-79 batch-63 = 4.576984792947769e-06

Training epoch-79 batch-64
Running loss of epoch-79 batch-64 = 3.1364616006612778e-06

Training epoch-79 batch-65
Running loss of epoch-79 batch-65 = 4.050321877002716e-06

Training epoch-79 batch-66
Running loss of epoch-79 batch-66 = 4.474073648452759e-06

Training epoch-79 batch-67
Running loss of epoch-79 batch-67 = 2.772081643342972e-06

Training epoch-79 batch-68
Running loss of epoch-79 batch-68 = 3.443332388997078e-06

Training epoch-79 batch-69
Running loss of epoch-79 batch-69 = 4.7960784286260605e-06

Training epoch-79 batch-70
Running loss of epoch-79 batch-70 = 2.8382055461406708e-06

Training epoch-79 batch-71
Running loss of epoch-79 batch-71 = 1.959502696990967e-06

Training epoch-79 batch-72
Running loss of epoch-79 batch-72 = 7.58003443479538e-06

Training epoch-79 batch-73
Running loss of epoch-79 batch-73 = 1.4649704098701477e-06

Training epoch-79 batch-74
Running loss of epoch-79 batch-74 = 2.960674464702606e-06

Training epoch-79 batch-75
Running loss of epoch-79 batch-75 = 2.0710285753011703e-06

Training epoch-79 batch-76
Running loss of epoch-79 batch-76 = 2.3902393877506256e-06

Training epoch-79 batch-77
Running loss of epoch-79 batch-77 = 3.2410025596618652e-06

Training epoch-79 batch-78
Running loss of epoch-79 batch-78 = 4.264060407876968e-06

Training epoch-79 batch-79
Running loss of epoch-79 batch-79 = 3.9208680391311646e-06

Training epoch-79 batch-80
Running loss of epoch-79 batch-80 = 2.4882610887289047e-06

Training epoch-79 batch-81
Running loss of epoch-79 batch-81 = 1.3869721442461014e-06

Training epoch-79 batch-82
Running loss of epoch-79 batch-82 = 4.454748705029488e-06

Training epoch-79 batch-83
Running loss of epoch-79 batch-83 = 2.657761797308922e-06

Training epoch-79 batch-84
Running loss of epoch-79 batch-84 = 3.0167866498231888e-06

Training epoch-79 batch-85
Running loss of epoch-79 batch-85 = 1.7266720533370972e-06

Training epoch-79 batch-86
Running loss of epoch-79 batch-86 = 1.8659047782421112e-06

Training epoch-79 batch-87
Running loss of epoch-79 batch-87 = 3.5206321626901627e-06

Training epoch-79 batch-88
Running loss of epoch-79 batch-88 = 8.970499038696289e-06

Training epoch-79 batch-89
Running loss of epoch-79 batch-89 = 2.5799963623285294e-06

Training epoch-79 batch-90
Running loss of epoch-79 batch-90 = 3.822380676865578e-06

Training epoch-79 batch-91
Running loss of epoch-79 batch-91 = 3.544846549630165e-06

Training epoch-79 batch-92
Running loss of epoch-79 batch-92 = 1.6959384083747864e-06

Training epoch-79 batch-93
Running loss of epoch-79 batch-93 = 4.198402166366577e-06

Training epoch-79 batch-94
Running loss of epoch-79 batch-94 = 3.5534612834453583e-06

Training epoch-79 batch-95
Running loss of epoch-79 batch-95 = 4.7013163566589355e-06

Training epoch-79 batch-96
Running loss of epoch-79 batch-96 = 2.359505742788315e-06

Training epoch-79 batch-97
Running loss of epoch-79 batch-97 = 3.0531082302331924e-06

Training epoch-79 batch-98
Running loss of epoch-79 batch-98 = 2.6836059987545013e-06

Training epoch-79 batch-99
Running loss of epoch-79 batch-99 = 2.2391323000192642e-06

Training epoch-79 batch-100
Running loss of epoch-79 batch-100 = 1.8221326172351837e-06

Training epoch-79 batch-101
Running loss of epoch-79 batch-101 = 2.5979243218898773e-06

Training epoch-79 batch-102
Running loss of epoch-79 batch-102 = 3.064284101128578e-06

Training epoch-79 batch-103
Running loss of epoch-79 batch-103 = 3.3848918974399567e-06

Training epoch-79 batch-104
Running loss of epoch-79 batch-104 = 2.6724301278591156e-06

Training epoch-79 batch-105
Running loss of epoch-79 batch-105 = 2.364395186305046e-06

Training epoch-79 batch-106
Running loss of epoch-79 batch-106 = 1.1399388313293457e-06

Training epoch-79 batch-107
Running loss of epoch-79 batch-107 = 2.098502591252327e-06

Training epoch-79 batch-108
Running loss of epoch-79 batch-108 = 2.3418106138706207e-06

Training epoch-79 batch-109
Running loss of epoch-79 batch-109 = 3.8030557334423065e-06

Training epoch-79 batch-110
Running loss of epoch-79 batch-110 = 2.1799933165311813e-06

Training epoch-79 batch-111
Running loss of epoch-79 batch-111 = 5.7336874306201935e-06

Training epoch-79 batch-112
Running loss of epoch-79 batch-112 = 2.1264422684907913e-06

Training epoch-79 batch-113
Running loss of epoch-79 batch-113 = 4.290137439966202e-06

Training epoch-79 batch-114
Running loss of epoch-79 batch-114 = 3.837980329990387e-06

Training epoch-79 batch-115
Running loss of epoch-79 batch-115 = 2.487562596797943e-06

Training epoch-79 batch-116
Running loss of epoch-79 batch-116 = 2.3245811462402344e-06

Training epoch-79 batch-117
Running loss of epoch-79 batch-117 = 2.7068890631198883e-06

Training epoch-79 batch-118
Running loss of epoch-79 batch-118 = 8.97655263543129e-06

Training epoch-79 batch-119
Running loss of epoch-79 batch-119 = 3.920402377843857e-06

Training epoch-79 batch-120
Running loss of epoch-79 batch-120 = 2.9364600777626038e-06

Training epoch-79 batch-121
Running loss of epoch-79 batch-121 = 1.6861595213413239e-06

Training epoch-79 batch-122
Running loss of epoch-79 batch-122 = 5.191890522837639e-06

Training epoch-79 batch-123
Running loss of epoch-79 batch-123 = 2.4817418307065964e-06

Training epoch-79 batch-124
Running loss of epoch-79 batch-124 = 2.258922904729843e-06

Training epoch-79 batch-125
Running loss of epoch-79 batch-125 = 1.4274846762418747e-06

Training epoch-79 batch-126
Running loss of epoch-79 batch-126 = 4.007946699857712e-06

Training epoch-79 batch-127
Running loss of epoch-79 batch-127 = 2.1907035261392593e-06

Training epoch-79 batch-128
Running loss of epoch-79 batch-128 = 2.6763882488012314e-06

Training epoch-79 batch-129
Running loss of epoch-79 batch-129 = 1.621665433049202e-06

Training epoch-79 batch-130
Running loss of epoch-79 batch-130 = 3.4635886549949646e-06

Training epoch-79 batch-131
Running loss of epoch-79 batch-131 = 1.6302801668643951e-06

Training epoch-79 batch-132
Running loss of epoch-79 batch-132 = 3.8051512092351913e-06

Training epoch-79 batch-133
Running loss of epoch-79 batch-133 = 3.4866388887166977e-06

Training epoch-79 batch-134
Running loss of epoch-79 batch-134 = 4.2351894080638885e-06

Training epoch-79 batch-135
Running loss of epoch-79 batch-135 = 2.989545464515686e-06

Training epoch-79 batch-136
Running loss of epoch-79 batch-136 = 9.990762919187546e-07

Training epoch-79 batch-137
Running loss of epoch-79 batch-137 = 1.596519723534584e-06

Training epoch-79 batch-138
Running loss of epoch-79 batch-138 = 5.387933924794197e-06

Training epoch-79 batch-139
Running loss of epoch-79 batch-139 = 1.4230608940124512e-06

Training epoch-79 batch-140
Running loss of epoch-79 batch-140 = 3.877794370055199e-06

Training epoch-79 batch-141
Running loss of epoch-79 batch-141 = 5.514826625585556e-06

Training epoch-79 batch-142
Running loss of epoch-79 batch-142 = 1.3990793377161026e-06

Training epoch-79 batch-143
Running loss of epoch-79 batch-143 = 1.3534445315599442e-06

Training epoch-79 batch-144
Running loss of epoch-79 batch-144 = 1.6419216990470886e-06

Training epoch-79 batch-145
Running loss of epoch-79 batch-145 = 5.680834874510765e-06

Training epoch-79 batch-146
Running loss of epoch-79 batch-146 = 3.6193523555994034e-06

Training epoch-79 batch-147
Running loss of epoch-79 batch-147 = 1.3257376849651337e-06

Training epoch-79 batch-148
Running loss of epoch-79 batch-148 = 4.48920764029026e-06

Training epoch-79 batch-149
Running loss of epoch-79 batch-149 = 3.5446137189865112e-06

Training epoch-79 batch-150
Running loss of epoch-79 batch-150 = 4.6247150748968124e-06

Training epoch-79 batch-151
Running loss of epoch-79 batch-151 = 2.241227775812149e-06

Training epoch-79 batch-152
Running loss of epoch-79 batch-152 = 3.6910641938447952e-06

Training epoch-79 batch-153
Running loss of epoch-79 batch-153 = 1.762760803103447e-06

Training epoch-79 batch-154
Running loss of epoch-79 batch-154 = 2.3241154849529266e-06

Training epoch-79 batch-155
Running loss of epoch-79 batch-155 = 6.3248444348573685e-06

Training epoch-79 batch-156
Running loss of epoch-79 batch-156 = 8.433125913143158e-07

Training epoch-79 batch-157
Running loss of epoch-79 batch-157 = 1.823529601097107e-05

Finished training epoch-79.



Average train loss at epoch-79 = 3.350602835416794e-06

Started Evaluation

Average val loss at epoch-79 = 1.1744886593818364

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.06 %

Finished Evaluation



Started training epoch-80


Training epoch-80 batch-1
Running loss of epoch-80 batch-1 = 2.5674235075712204e-06

Training epoch-80 batch-2
Running loss of epoch-80 batch-2 = 3.8868747651577e-06

Training epoch-80 batch-3
Running loss of epoch-80 batch-3 = 7.077818736433983e-06

Training epoch-80 batch-4
Running loss of epoch-80 batch-4 = 1.3282988220453262e-06

Training epoch-80 batch-5
Running loss of epoch-80 batch-5 = 2.7634669095277786e-06

Training epoch-80 batch-6
Running loss of epoch-80 batch-6 = 2.746470272541046e-06

Training epoch-80 batch-7
Running loss of epoch-80 batch-7 = 2.452172338962555e-06

Training epoch-80 batch-8
Running loss of epoch-80 batch-8 = 5.653128027915955e-07

Training epoch-80 batch-9
Running loss of epoch-80 batch-9 = 6.051966920495033e-06

Training epoch-80 batch-10
Running loss of epoch-80 batch-10 = 1.4204997569322586e-06

Training epoch-80 batch-11
Running loss of epoch-80 batch-11 = 2.489658072590828e-06

Training epoch-80 batch-12
Running loss of epoch-80 batch-12 = 1.1168885976076126e-06

Training epoch-80 batch-13
Running loss of epoch-80 batch-13 = 1.6309786587953568e-06

Training epoch-80 batch-14
Running loss of epoch-80 batch-14 = 5.2140094339847565e-06

Training epoch-80 batch-15
Running loss of epoch-80 batch-15 = 2.8049107640981674e-06

Training epoch-80 batch-16
Running loss of epoch-80 batch-16 = 2.7115456759929657e-06

Training epoch-80 batch-17
Running loss of epoch-80 batch-17 = 1.9953586161136627e-06

Training epoch-80 batch-18
Running loss of epoch-80 batch-18 = 3.127381205558777e-06

Training epoch-80 batch-19
Running loss of epoch-80 batch-19 = 5.034031346440315e-06

Training epoch-80 batch-20
Running loss of epoch-80 batch-20 = 1.6584526747465134e-06

Training epoch-80 batch-21
Running loss of epoch-80 batch-21 = 6.3427723944187164e-06

Training epoch-80 batch-22
Running loss of epoch-80 batch-22 = 3.114575520157814e-06

Training epoch-80 batch-23
Running loss of epoch-80 batch-23 = 3.498746082186699e-06

Training epoch-80 batch-24
Running loss of epoch-80 batch-24 = 2.028420567512512e-06

Training epoch-80 batch-25
Running loss of epoch-80 batch-25 = 2.123182639479637e-06

Training epoch-80 batch-26
Running loss of epoch-80 batch-26 = 3.8139987736940384e-06

Training epoch-80 batch-27
Running loss of epoch-80 batch-27 = 5.611684173345566e-06

Training epoch-80 batch-28
Running loss of epoch-80 batch-28 = 5.334848538041115e-06

Training epoch-80 batch-29
Running loss of epoch-80 batch-29 = 3.2498501241207123e-06

Training epoch-80 batch-30
Running loss of epoch-80 batch-30 = 1.7457641661167145e-06

Training epoch-80 batch-31
Running loss of epoch-80 batch-31 = 2.080807462334633e-06

Training epoch-80 batch-32
Running loss of epoch-80 batch-32 = 2.491055056452751e-06

Training epoch-80 batch-33
Running loss of epoch-80 batch-33 = 3.23285348713398e-06

Training epoch-80 batch-34
Running loss of epoch-80 batch-34 = 2.7229543775320053e-06

Training epoch-80 batch-35
Running loss of epoch-80 batch-35 = 1.375330612063408e-06

Training epoch-80 batch-36
Running loss of epoch-80 batch-36 = 1.9476283341646194e-06

Training epoch-80 batch-37
Running loss of epoch-80 batch-37 = 2.1918676793575287e-06

Training epoch-80 batch-38
Running loss of epoch-80 batch-38 = 3.0570663511753082e-06

Training epoch-80 batch-39
Running loss of epoch-80 batch-39 = 2.7853529900312424e-06

Training epoch-80 batch-40
Running loss of epoch-80 batch-40 = 3.915745764970779e-06

Training epoch-80 batch-41
Running loss of epoch-80 batch-41 = 4.049856215715408e-06

Training epoch-80 batch-42
Running loss of epoch-80 batch-42 = 3.0230730772018433e-06

Training epoch-80 batch-43
Running loss of epoch-80 batch-43 = 2.908986061811447e-06

Training epoch-80 batch-44
Running loss of epoch-80 batch-44 = 4.01865690946579e-06

Training epoch-80 batch-45
Running loss of epoch-80 batch-45 = 5.328096449375153e-06

Training epoch-80 batch-46
Running loss of epoch-80 batch-46 = 2.5383196771144867e-06

Training epoch-80 batch-47
Running loss of epoch-80 batch-47 = 2.688029780983925e-06

Training epoch-80 batch-48
Running loss of epoch-80 batch-48 = 2.9066577553749084e-06

Training epoch-80 batch-49
Running loss of epoch-80 batch-49 = 2.4873297661542892e-06

Training epoch-80 batch-50
Running loss of epoch-80 batch-50 = 2.560904249548912e-06

Training epoch-80 batch-51
Running loss of epoch-80 batch-51 = 1.5639234334230423e-06

Training epoch-80 batch-52
Running loss of epoch-80 batch-52 = 3.3439137041568756e-06

Training epoch-80 batch-53
Running loss of epoch-80 batch-53 = 3.2032839953899384e-06

Training epoch-80 batch-54
Running loss of epoch-80 batch-54 = 5.8747828006744385e-06

Training epoch-80 batch-55
Running loss of epoch-80 batch-55 = 1.5511177480220795e-06

Training epoch-80 batch-56
Running loss of epoch-80 batch-56 = 2.487562596797943e-06

Training epoch-80 batch-57
Running loss of epoch-80 batch-57 = 1.0388670489192009e-05

Training epoch-80 batch-58
Running loss of epoch-80 batch-58 = 4.816800355911255e-06

Training epoch-80 batch-59
Running loss of epoch-80 batch-59 = 2.7441419661045074e-06

Training epoch-80 batch-60
Running loss of epoch-80 batch-60 = 3.881053999066353e-06

Training epoch-80 batch-61
Running loss of epoch-80 batch-61 = 2.796994522213936e-06

Training epoch-80 batch-62
Running loss of epoch-80 batch-62 = 1.964159309864044e-06

Training epoch-80 batch-63
Running loss of epoch-80 batch-63 = 2.721790224313736e-06

Training epoch-80 batch-64
Running loss of epoch-80 batch-64 = 4.496192559599876e-06

Training epoch-80 batch-65
Running loss of epoch-80 batch-65 = 2.998393028974533e-06

Training epoch-80 batch-66
Running loss of epoch-80 batch-66 = 5.179084837436676e-06

Training epoch-80 batch-67
Running loss of epoch-80 batch-67 = 1.850305125117302e-06

Training epoch-80 batch-68
Running loss of epoch-80 batch-68 = 1.7499551177024841e-06

Training epoch-80 batch-69
Running loss of epoch-80 batch-69 = 4.379777237772942e-06

Training epoch-80 batch-70
Running loss of epoch-80 batch-70 = 1.834006980061531e-06

Training epoch-80 batch-71
Running loss of epoch-80 batch-71 = 5.091540515422821e-06

Training epoch-80 batch-72
Running loss of epoch-80 batch-72 = 1.6773119568824768e-06

Training epoch-80 batch-73
Running loss of epoch-80 batch-73 = 2.175569534301758e-06

Training epoch-80 batch-74
Running loss of epoch-80 batch-74 = 9.338837116956711e-07

Training epoch-80 batch-75
Running loss of epoch-80 batch-75 = 2.462416887283325e-06

Training epoch-80 batch-76
Running loss of epoch-80 batch-76 = 2.712476998567581e-06

Training epoch-80 batch-77
Running loss of epoch-80 batch-77 = 3.7101563066244125e-06

Training epoch-80 batch-78
Running loss of epoch-80 batch-78 = 2.919929102063179e-06

Training epoch-80 batch-79
Running loss of epoch-80 batch-79 = 3.6065466701984406e-06

Training epoch-80 batch-80
Running loss of epoch-80 batch-80 = 3.1401868909597397e-06

Training epoch-80 batch-81
Running loss of epoch-80 batch-81 = 1.8605496734380722e-06

Training epoch-80 batch-82
Running loss of epoch-80 batch-82 = 2.6512425392866135e-06

Training epoch-80 batch-83
Running loss of epoch-80 batch-83 = 2.0957086235284805e-06

Training epoch-80 batch-84
Running loss of epoch-80 batch-84 = 2.968357875943184e-06

Training epoch-80 batch-85
Running loss of epoch-80 batch-85 = 5.808891728520393e-06

Training epoch-80 batch-86
Running loss of epoch-80 batch-86 = 4.373257979750633e-06

Training epoch-80 batch-87
Running loss of epoch-80 batch-87 = 2.5739427655935287e-06

Training epoch-80 batch-88
Running loss of epoch-80 batch-88 = 4.4514890760183334e-06

Training epoch-80 batch-89
Running loss of epoch-80 batch-89 = 3.6919955164194107e-06

Training epoch-80 batch-90
Running loss of epoch-80 batch-90 = 3.363005816936493e-06

Training epoch-80 batch-91
Running loss of epoch-80 batch-91 = 3.7511344999074936e-06

Training epoch-80 batch-92
Running loss of epoch-80 batch-92 = 3.927154466509819e-06

Training epoch-80 batch-93
Running loss of epoch-80 batch-93 = 3.568129613995552e-06

Training epoch-80 batch-94
Running loss of epoch-80 batch-94 = 8.769100531935692e-06

Training epoch-80 batch-95
Running loss of epoch-80 batch-95 = 2.352287992835045e-06

Training epoch-80 batch-96
Running loss of epoch-80 batch-96 = 2.686167135834694e-06

Training epoch-80 batch-97
Running loss of epoch-80 batch-97 = 1.2300442904233932e-06

Training epoch-80 batch-98
Running loss of epoch-80 batch-98 = 3.1632371246814728e-06

Training epoch-80 batch-99
Running loss of epoch-80 batch-99 = 5.772802978754044e-06

Training epoch-80 batch-100
Running loss of epoch-80 batch-100 = 2.212822437286377e-06

Training epoch-80 batch-101
Running loss of epoch-80 batch-101 = 5.1925890147686005e-06

Training epoch-80 batch-102
Running loss of epoch-80 batch-102 = 1.6193371266126633e-06

Training epoch-80 batch-103
Running loss of epoch-80 batch-103 = 2.9264483600854874e-06

Training epoch-80 batch-104
Running loss of epoch-80 batch-104 = 2.2172462195158005e-06

Training epoch-80 batch-105
Running loss of epoch-80 batch-105 = 4.664529114961624e-06

Training epoch-80 batch-106
Running loss of epoch-80 batch-106 = 2.629123628139496e-06

Training epoch-80 batch-107
Running loss of epoch-80 batch-107 = 2.2312160581350327e-06

Training epoch-80 batch-108
Running loss of epoch-80 batch-108 = 1.469859853386879e-06

Training epoch-80 batch-109
Running loss of epoch-80 batch-109 = 1.877080649137497e-06

Training epoch-80 batch-110
Running loss of epoch-80 batch-110 = 1.3879034668207169e-06

Training epoch-80 batch-111
Running loss of epoch-80 batch-111 = 2.798857167363167e-06

Training epoch-80 batch-112
Running loss of epoch-80 batch-112 = 1.2745149433612823e-06

Training epoch-80 batch-113
Running loss of epoch-80 batch-113 = 8.03195871412754e-06

Training epoch-80 batch-114
Running loss of epoch-80 batch-114 = 4.597008228302002e-06

Training epoch-80 batch-115
Running loss of epoch-80 batch-115 = 1.0421499609947205e-06

Training epoch-80 batch-116
Running loss of epoch-80 batch-116 = 4.984205588698387e-06

Training epoch-80 batch-117
Running loss of epoch-80 batch-117 = 2.454034984111786e-06

Training epoch-80 batch-118
Running loss of epoch-80 batch-118 = 4.215165972709656e-06

Training epoch-80 batch-119
Running loss of epoch-80 batch-119 = 2.0312145352363586e-06

Training epoch-80 batch-120
Running loss of epoch-80 batch-120 = 4.978850483894348e-06

Training epoch-80 batch-121
Running loss of epoch-80 batch-121 = 2.962769940495491e-06

Training epoch-80 batch-122
Running loss of epoch-80 batch-122 = 4.54275868833065e-06

Training epoch-80 batch-123
Running loss of epoch-80 batch-123 = 1.5576370060443878e-06

Training epoch-80 batch-124
Running loss of epoch-80 batch-124 = 5.099456757307053e-06

Training epoch-80 batch-125
Running loss of epoch-80 batch-125 = 4.063127562403679e-06

Training epoch-80 batch-126
Running loss of epoch-80 batch-126 = 4.221685230731964e-06

Training epoch-80 batch-127
Running loss of epoch-80 batch-127 = 4.7655776143074036e-06

Training epoch-80 batch-128
Running loss of epoch-80 batch-128 = 2.3383181542158127e-06

Training epoch-80 batch-129
Running loss of epoch-80 batch-129 = 3.3311080187559128e-06

Training epoch-80 batch-130
Running loss of epoch-80 batch-130 = 3.1532254070043564e-06

Training epoch-80 batch-131
Running loss of epoch-80 batch-131 = 1.7404090613126755e-06

Training epoch-80 batch-132
Running loss of epoch-80 batch-132 = 4.862900823354721e-06

Training epoch-80 batch-133
Running loss of epoch-80 batch-133 = 2.5280751287937164e-06

Training epoch-80 batch-134
Running loss of epoch-80 batch-134 = 6.09084963798523e-07

Training epoch-80 batch-135
Running loss of epoch-80 batch-135 = 2.8510112315416336e-06

Training epoch-80 batch-136
Running loss of epoch-80 batch-136 = 3.3618416637182236e-06

Training epoch-80 batch-137
Running loss of epoch-80 batch-137 = 4.202593117952347e-06

Training epoch-80 batch-138
Running loss of epoch-80 batch-138 = 1.6675330698490143e-06

Training epoch-80 batch-139
Running loss of epoch-80 batch-139 = 2.228887751698494e-06

Training epoch-80 batch-140
Running loss of epoch-80 batch-140 = 2.535758540034294e-06

Training epoch-80 batch-141
Running loss of epoch-80 batch-141 = 4.019821062684059e-06

Training epoch-80 batch-142
Running loss of epoch-80 batch-142 = 3.4600961953401566e-06

Training epoch-80 batch-143
Running loss of epoch-80 batch-143 = 4.349742084741592e-06

Training epoch-80 batch-144
Running loss of epoch-80 batch-144 = 1.9837170839309692e-06

Training epoch-80 batch-145
Running loss of epoch-80 batch-145 = 3.6838464438915253e-06

Training epoch-80 batch-146
Running loss of epoch-80 batch-146 = 5.962792783975601e-07

Training epoch-80 batch-147
Running loss of epoch-80 batch-147 = 3.3492688089609146e-06

Training epoch-80 batch-148
Running loss of epoch-80 batch-148 = 6.205867975950241e-06

Training epoch-80 batch-149
Running loss of epoch-80 batch-149 = 3.881053999066353e-06

Training epoch-80 batch-150
Running loss of epoch-80 batch-150 = 4.075001925230026e-06

Training epoch-80 batch-151
Running loss of epoch-80 batch-151 = 4.122965037822723e-06

Training epoch-80 batch-152
Running loss of epoch-80 batch-152 = 1.4351680874824524e-06

Training epoch-80 batch-153
Running loss of epoch-80 batch-153 = 4.273373633623123e-06

Training epoch-80 batch-154
Running loss of epoch-80 batch-154 = 2.032611519098282e-06

Training epoch-80 batch-155
Running loss of epoch-80 batch-155 = 3.177439793944359e-06

Training epoch-80 batch-156
Running loss of epoch-80 batch-156 = 9.492505341768265e-06

Training epoch-80 batch-157
Running loss of epoch-80 batch-157 = 2.689659595489502e-05

Finished training epoch-80.



Average train loss at epoch-80 = 3.2989129424095155e-06

Started Evaluation

Average val loss at epoch-80 = 1.1734132636309274

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.21 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 89.08 %
Accuracy for class run is: 62.10 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.28 %

Overall Accuracy = 83.12 %

Finished Evaluation



Started training epoch-81


Training epoch-81 batch-1
Running loss of epoch-81 batch-1 = 1.434003934264183e-06

Training epoch-81 batch-2
Running loss of epoch-81 batch-2 = 2.48616561293602e-06

Training epoch-81 batch-3
Running loss of epoch-81 batch-3 = 1.1334195733070374e-06

Training epoch-81 batch-4
Running loss of epoch-81 batch-4 = 2.4491455405950546e-06

Training epoch-81 batch-5
Running loss of epoch-81 batch-5 = 2.1418090909719467e-06

Training epoch-81 batch-6
Running loss of epoch-81 batch-6 = 2.6710331439971924e-06

Training epoch-81 batch-7
Running loss of epoch-81 batch-7 = 2.733897417783737e-06

Training epoch-81 batch-8
Running loss of epoch-81 batch-8 = 3.1422823667526245e-06

Training epoch-81 batch-9
Running loss of epoch-81 batch-9 = 3.5706907510757446e-06

Training epoch-81 batch-10
Running loss of epoch-81 batch-10 = 2.289889380335808e-06

Training epoch-81 batch-11
Running loss of epoch-81 batch-11 = 7.5409188866615295e-06

Training epoch-81 batch-12
Running loss of epoch-81 batch-12 = 2.3371540009975433e-06

Training epoch-81 batch-13
Running loss of epoch-81 batch-13 = 4.558125510811806e-06

Training epoch-81 batch-14
Running loss of epoch-81 batch-14 = 2.0854640752077103e-06

Training epoch-81 batch-15
Running loss of epoch-81 batch-15 = 3.4265685826539993e-06

Training epoch-81 batch-16
Running loss of epoch-81 batch-16 = 4.656147211790085e-06

Training epoch-81 batch-17
Running loss of epoch-81 batch-17 = 2.0461156964302063e-06

Training epoch-81 batch-18
Running loss of epoch-81 batch-18 = 1.5582190826535225e-06

Training epoch-81 batch-19
Running loss of epoch-81 batch-19 = 2.0174775272607803e-06

Training epoch-81 batch-20
Running loss of epoch-81 batch-20 = 3.6368146538734436e-06

Training epoch-81 batch-21
Running loss of epoch-81 batch-21 = 1.6579870134592056e-06

Training epoch-81 batch-22
Running loss of epoch-81 batch-22 = 5.4033007472753525e-06

Training epoch-81 batch-23
Running loss of epoch-81 batch-23 = 4.955800250172615e-06

Training epoch-81 batch-24
Running loss of epoch-81 batch-24 = 3.584660589694977e-06

Training epoch-81 batch-25
Running loss of epoch-81 batch-25 = 4.464061930775642e-06

Training epoch-81 batch-26
Running loss of epoch-81 batch-26 = 2.282671630382538e-06

Training epoch-81 batch-27
Running loss of epoch-81 batch-27 = 6.095506250858307e-06

Training epoch-81 batch-28
Running loss of epoch-81 batch-28 = 5.0361268222332e-07

Training epoch-81 batch-29
Running loss of epoch-81 batch-29 = 3.1276140362024307e-06

Training epoch-81 batch-30
Running loss of epoch-81 batch-30 = 2.203509211540222e-06

Training epoch-81 batch-31
Running loss of epoch-81 batch-31 = 1.2600794434547424e-06

Training epoch-81 batch-32
Running loss of epoch-81 batch-32 = 5.942303687334061e-06

Training epoch-81 batch-33
Running loss of epoch-81 batch-33 = 2.9834918677806854e-06

Training epoch-81 batch-34
Running loss of epoch-81 batch-34 = 1.9345898181200027e-06

Training epoch-81 batch-35
Running loss of epoch-81 batch-35 = 4.4764019548892975e-06

Training epoch-81 batch-36
Running loss of epoch-81 batch-36 = 4.26173210144043e-06

Training epoch-81 batch-37
Running loss of epoch-81 batch-37 = 2.9685907065868378e-06

Training epoch-81 batch-38
Running loss of epoch-81 batch-38 = 9.124632924795151e-07

Training epoch-81 batch-39
Running loss of epoch-81 batch-39 = 1.9301660358905792e-06

Training epoch-81 batch-40
Running loss of epoch-81 batch-40 = 4.119705408811569e-06

Training epoch-81 batch-41
Running loss of epoch-81 batch-41 = 3.166263923048973e-06

Training epoch-81 batch-42
Running loss of epoch-81 batch-42 = 4.410510882735252e-06

Training epoch-81 batch-43
Running loss of epoch-81 batch-43 = 6.692018359899521e-06

Training epoch-81 batch-44
Running loss of epoch-81 batch-44 = 2.185581251978874e-06

Training epoch-81 batch-45
Running loss of epoch-81 batch-45 = 2.6712659746408463e-06

Training epoch-81 batch-46
Running loss of epoch-81 batch-46 = 3.771623596549034e-06

Training epoch-81 batch-47
Running loss of epoch-81 batch-47 = 1.4726538211107254e-06

Training epoch-81 batch-48
Running loss of epoch-81 batch-48 = 9.462237358093262e-07

Training epoch-81 batch-49
Running loss of epoch-81 batch-49 = 3.5134144127368927e-06

Training epoch-81 batch-50
Running loss of epoch-81 batch-50 = 1.2444797903299332e-06

Training epoch-81 batch-51
Running loss of epoch-81 batch-51 = 2.141343429684639e-06

Training epoch-81 batch-52
Running loss of epoch-81 batch-52 = 2.580927684903145e-06

Training epoch-81 batch-53
Running loss of epoch-81 batch-53 = 1.4491379261016846e-06

Training epoch-81 batch-54
Running loss of epoch-81 batch-54 = 2.5709159672260284e-06

Training epoch-81 batch-55
Running loss of epoch-81 batch-55 = 3.7760473787784576e-06

Training epoch-81 batch-56
Running loss of epoch-81 batch-56 = 2.275221049785614e-06

Training epoch-81 batch-57
Running loss of epoch-81 batch-57 = 5.766749382019043e-06

Training epoch-81 batch-58
Running loss of epoch-81 batch-58 = 6.973510608077049e-06

Training epoch-81 batch-59
Running loss of epoch-81 batch-59 = 1.7532147467136383e-06

Training epoch-81 batch-60
Running loss of epoch-81 batch-60 = 4.139728844165802e-06

Training epoch-81 batch-61
Running loss of epoch-81 batch-61 = 2.7052592486143112e-06

Training epoch-81 batch-62
Running loss of epoch-81 batch-62 = 3.111083060503006e-06

Training epoch-81 batch-63
Running loss of epoch-81 batch-63 = 3.1834933906793594e-06

Training epoch-81 batch-64
Running loss of epoch-81 batch-64 = 5.0391536206007e-06

Training epoch-81 batch-65
Running loss of epoch-81 batch-65 = 1.7557758837938309e-06

Training epoch-81 batch-66
Running loss of epoch-81 batch-66 = 4.123896360397339e-06

Training epoch-81 batch-67
Running loss of epoch-81 batch-67 = 1.6826670616865158e-06

Training epoch-81 batch-68
Running loss of epoch-81 batch-68 = 1.7362181097269058e-06

Training epoch-81 batch-69
Running loss of epoch-81 batch-69 = 1.691281795501709e-06

Training epoch-81 batch-70
Running loss of epoch-81 batch-70 = 2.560904249548912e-06

Training epoch-81 batch-71
Running loss of epoch-81 batch-71 = 3.063119947910309e-06

Training epoch-81 batch-72
Running loss of epoch-81 batch-72 = 1.4489050954580307e-06

Training epoch-81 batch-73
Running loss of epoch-81 batch-73 = 3.1052622944116592e-06

Training epoch-81 batch-74
Running loss of epoch-81 batch-74 = 8.838716894388199e-06

Training epoch-81 batch-75
Running loss of epoch-81 batch-75 = 2.1990854293107986e-06

Training epoch-81 batch-76
Running loss of epoch-81 batch-76 = 2.2638123482465744e-06

Training epoch-81 batch-77
Running loss of epoch-81 batch-77 = 5.7264696806669235e-06

Training epoch-81 batch-78
Running loss of epoch-81 batch-78 = 3.7834979593753815e-06

Training epoch-81 batch-79
Running loss of epoch-81 batch-79 = 3.6014243960380554e-06

Training epoch-81 batch-80
Running loss of epoch-81 batch-80 = 1.984415575861931e-06

Training epoch-81 batch-81
Running loss of epoch-81 batch-81 = 1.878710463643074e-06

Training epoch-81 batch-82
Running loss of epoch-81 batch-82 = 3.848923370242119e-06

Training epoch-81 batch-83
Running loss of epoch-81 batch-83 = 2.7194619178771973e-06

Training epoch-81 batch-84
Running loss of epoch-81 batch-84 = 3.391411155462265e-06

Training epoch-81 batch-85
Running loss of epoch-81 batch-85 = 3.0028168112039566e-06

Training epoch-81 batch-86
Running loss of epoch-81 batch-86 = 2.6330817490816116e-06

Training epoch-81 batch-87
Running loss of epoch-81 batch-87 = 1.9827857613563538e-06

Training epoch-81 batch-88
Running loss of epoch-81 batch-88 = 4.571396857500076e-06

Training epoch-81 batch-89
Running loss of epoch-81 batch-89 = 3.4119002521038055e-06

Training epoch-81 batch-90
Running loss of epoch-81 batch-90 = 4.503875970840454e-06

Training epoch-81 batch-91
Running loss of epoch-81 batch-91 = 4.245201125741005e-06

Training epoch-81 batch-92
Running loss of epoch-81 batch-92 = 3.894791007041931e-06

Training epoch-81 batch-93
Running loss of epoch-81 batch-93 = 5.076872184872627e-06

Training epoch-81 batch-94
Running loss of epoch-81 batch-94 = 2.8815120458602905e-06

Training epoch-81 batch-95
Running loss of epoch-81 batch-95 = 2.464745193719864e-06

Training epoch-81 batch-96
Running loss of epoch-81 batch-96 = 3.8063153624534607e-06

Training epoch-81 batch-97
Running loss of epoch-81 batch-97 = 2.559507265686989e-06

Training epoch-81 batch-98
Running loss of epoch-81 batch-98 = 1.9459985196590424e-06

Training epoch-81 batch-99
Running loss of epoch-81 batch-99 = 2.185581251978874e-06

Training epoch-81 batch-100
Running loss of epoch-81 batch-100 = 3.8798898458480835e-06

Training epoch-81 batch-101
Running loss of epoch-81 batch-101 = 2.6819761842489243e-06

Training epoch-81 batch-102
Running loss of epoch-81 batch-102 = 3.3334363251924515e-06

Training epoch-81 batch-103
Running loss of epoch-81 batch-103 = 5.939509719610214e-07

Training epoch-81 batch-104
Running loss of epoch-81 batch-104 = 2.4067703634500504e-06

Training epoch-81 batch-105
Running loss of epoch-81 batch-105 = 4.995381459593773e-06

Training epoch-81 batch-106
Running loss of epoch-81 batch-106 = 3.7050340324640274e-06

Training epoch-81 batch-107
Running loss of epoch-81 batch-107 = 2.22143717110157e-06

Training epoch-81 batch-108
Running loss of epoch-81 batch-108 = 5.657318979501724e-06

Training epoch-81 batch-109
Running loss of epoch-81 batch-109 = 2.5695189833641052e-06

Training epoch-81 batch-110
Running loss of epoch-81 batch-110 = 3.416324034333229e-06

Training epoch-81 batch-111
Running loss of epoch-81 batch-111 = 4.780944436788559e-06

Training epoch-81 batch-112
Running loss of epoch-81 batch-112 = 2.86824069917202e-06

Training epoch-81 batch-113
Running loss of epoch-81 batch-113 = 3.676861524581909e-06

Training epoch-81 batch-114
Running loss of epoch-81 batch-114 = 3.872904926538467e-06

Training epoch-81 batch-115
Running loss of epoch-81 batch-115 = 2.3865140974521637e-06

Training epoch-81 batch-116
Running loss of epoch-81 batch-116 = 2.8624199330806732e-06

Training epoch-81 batch-117
Running loss of epoch-81 batch-117 = 1.9350554794073105e-06

Training epoch-81 batch-118
Running loss of epoch-81 batch-118 = 3.1942036002874374e-06

Training epoch-81 batch-119
Running loss of epoch-81 batch-119 = 4.839617758989334e-06

Training epoch-81 batch-120
Running loss of epoch-81 batch-120 = 2.9515940696001053e-06

Training epoch-81 batch-121
Running loss of epoch-81 batch-121 = 4.457077011466026e-06

Training epoch-81 batch-122
Running loss of epoch-81 batch-122 = 3.1355302780866623e-06

Training epoch-81 batch-123
Running loss of epoch-81 batch-123 = 2.6780180633068085e-06

Training epoch-81 batch-124
Running loss of epoch-81 batch-124 = 1.7648562788963318e-06

Training epoch-81 batch-125
Running loss of epoch-81 batch-125 = 2.2514723241329193e-06

Training epoch-81 batch-126
Running loss of epoch-81 batch-126 = 3.671739250421524e-06

Training epoch-81 batch-127
Running loss of epoch-81 batch-127 = 2.4263281375169754e-06

Training epoch-81 batch-128
Running loss of epoch-81 batch-128 = 6.355345249176025e-06

Training epoch-81 batch-129
Running loss of epoch-81 batch-129 = 2.0354054868221283e-06

Training epoch-81 batch-130
Running loss of epoch-81 batch-130 = 2.743443474173546e-06

Training epoch-81 batch-131
Running loss of epoch-81 batch-131 = 5.96093013882637e-06

Training epoch-81 batch-132
Running loss of epoch-81 batch-132 = 4.591653123497963e-06

Training epoch-81 batch-133
Running loss of epoch-81 batch-133 = 8.109025657176971e-06

Training epoch-81 batch-134
Running loss of epoch-81 batch-134 = 1.677079126238823e-06

Training epoch-81 batch-135
Running loss of epoch-81 batch-135 = 1.5813857316970825e-06

Training epoch-81 batch-136
Running loss of epoch-81 batch-136 = 8.44825990498066e-06

Training epoch-81 batch-137
Running loss of epoch-81 batch-137 = 1.895008608698845e-06

Training epoch-81 batch-138
Running loss of epoch-81 batch-138 = 1.210952177643776e-06

Training epoch-81 batch-139
Running loss of epoch-81 batch-139 = 2.3155007511377335e-06

Training epoch-81 batch-140
Running loss of epoch-81 batch-140 = 2.137618139386177e-06

Training epoch-81 batch-141
Running loss of epoch-81 batch-141 = 1.7904676496982574e-06

Training epoch-81 batch-142
Running loss of epoch-81 batch-142 = 6.561167538166046e-06

Training epoch-81 batch-143
Running loss of epoch-81 batch-143 = 2.4498440325260162e-06

Training epoch-81 batch-144
Running loss of epoch-81 batch-144 = 2.614222466945648e-06

Training epoch-81 batch-145
Running loss of epoch-81 batch-145 = 2.2794120013713837e-06

Training epoch-81 batch-146
Running loss of epoch-81 batch-146 = 8.09086486697197e-07

Training epoch-81 batch-147
Running loss of epoch-81 batch-147 = 7.279450073838234e-06

Training epoch-81 batch-148
Running loss of epoch-81 batch-148 = 2.010725438594818e-06

Training epoch-81 batch-149
Running loss of epoch-81 batch-149 = 2.3848842829465866e-06

Training epoch-81 batch-150
Running loss of epoch-81 batch-150 = 3.2922253012657166e-06

Training epoch-81 batch-151
Running loss of epoch-81 batch-151 = 4.7371722757816315e-06

Training epoch-81 batch-152
Running loss of epoch-81 batch-152 = 2.2670719772577286e-06

Training epoch-81 batch-153
Running loss of epoch-81 batch-153 = 1.6533304005861282e-06

Training epoch-81 batch-154
Running loss of epoch-81 batch-154 = 2.5494955480098724e-06

Training epoch-81 batch-155
Running loss of epoch-81 batch-155 = 5.073845386505127e-06

Training epoch-81 batch-156
Running loss of epoch-81 batch-156 = 2.446817234158516e-06

Training epoch-81 batch-157
Running loss of epoch-81 batch-157 = 1.3772398233413696e-05

Finished training epoch-81.



Average train loss at epoch-81 = 3.219277411699295e-06

Started Evaluation

Average val loss at epoch-81 = 1.1733571346330074

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.48 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.14 %

Finished Evaluation



Started training epoch-82


Training epoch-82 batch-1
Running loss of epoch-82 batch-1 = 6.536953151226044e-06

Training epoch-82 batch-2
Running loss of epoch-82 batch-2 = 2.1781306713819504e-06

Training epoch-82 batch-3
Running loss of epoch-82 batch-3 = 2.887798473238945e-06

Training epoch-82 batch-4
Running loss of epoch-82 batch-4 = 1.0319054126739502e-06

Training epoch-82 batch-5
Running loss of epoch-82 batch-5 = 4.0102750062942505e-06

Training epoch-82 batch-6
Running loss of epoch-82 batch-6 = 3.80747951567173e-06

Training epoch-82 batch-7
Running loss of epoch-82 batch-7 = 2.817949280142784e-06

Training epoch-82 batch-8
Running loss of epoch-82 batch-8 = 2.0654406398534775e-06

Training epoch-82 batch-9
Running loss of epoch-82 batch-9 = 2.741580829024315e-06

Training epoch-82 batch-10
Running loss of epoch-82 batch-10 = 1.448206603527069e-06

Training epoch-82 batch-11
Running loss of epoch-82 batch-11 = 1.3809185475111008e-06

Training epoch-82 batch-12
Running loss of epoch-82 batch-12 = 1.884065568447113e-06

Training epoch-82 batch-13
Running loss of epoch-82 batch-13 = 2.8836075216531754e-06

Training epoch-82 batch-14
Running loss of epoch-82 batch-14 = 2.7311034500598907e-06

Training epoch-82 batch-15
Running loss of epoch-82 batch-15 = 6.3390471041202545e-06

Training epoch-82 batch-16
Running loss of epoch-82 batch-16 = 2.225395292043686e-06

Training epoch-82 batch-17
Running loss of epoch-82 batch-17 = 3.902940079569817e-06

Training epoch-82 batch-18
Running loss of epoch-82 batch-18 = 1.9387807697057724e-06

Training epoch-82 batch-19
Running loss of epoch-82 batch-19 = 2.242857590317726e-06

Training epoch-82 batch-20
Running loss of epoch-82 batch-20 = 6.014248356223106e-06

Training epoch-82 batch-21
Running loss of epoch-82 batch-21 = 2.3602042347192764e-06

Training epoch-82 batch-22
Running loss of epoch-82 batch-22 = 6.029615178704262e-06

Training epoch-82 batch-23
Running loss of epoch-82 batch-23 = 2.5294721126556396e-06

Training epoch-82 batch-24
Running loss of epoch-82 batch-24 = 2.402346581220627e-06

Training epoch-82 batch-25
Running loss of epoch-82 batch-25 = 3.4964177757501602e-06

Training epoch-82 batch-26
Running loss of epoch-82 batch-26 = 6.13299198448658e-06

Training epoch-82 batch-27
Running loss of epoch-82 batch-27 = 2.37906351685524e-06

Training epoch-82 batch-28
Running loss of epoch-82 batch-28 = 1.9492581486701965e-06

Training epoch-82 batch-29
Running loss of epoch-82 batch-29 = 2.202112227678299e-06

Training epoch-82 batch-30
Running loss of epoch-82 batch-30 = 3.5208649933338165e-06

Training epoch-82 batch-31
Running loss of epoch-82 batch-31 = 2.754153683781624e-06

Training epoch-82 batch-32
Running loss of epoch-82 batch-32 = 3.1050294637680054e-06

Training epoch-82 batch-33
Running loss of epoch-82 batch-33 = 1.341104507446289e-06

Training epoch-82 batch-34
Running loss of epoch-82 batch-34 = 3.101769834756851e-06

Training epoch-82 batch-35
Running loss of epoch-82 batch-35 = 1.5953555703163147e-06

Training epoch-82 batch-36
Running loss of epoch-82 batch-36 = 1.9741710275411606e-06

Training epoch-82 batch-37
Running loss of epoch-82 batch-37 = 2.8114300221204758e-06

Training epoch-82 batch-38
Running loss of epoch-82 batch-38 = 3.061257302761078e-06

Training epoch-82 batch-39
Running loss of epoch-82 batch-39 = 2.463115379214287e-06

Training epoch-82 batch-40
Running loss of epoch-82 batch-40 = 2.7024652808904648e-06

Training epoch-82 batch-41
Running loss of epoch-82 batch-41 = 5.980720743536949e-06

Training epoch-82 batch-42
Running loss of epoch-82 batch-42 = 2.8063077479600906e-06

Training epoch-82 batch-43
Running loss of epoch-82 batch-43 = 1.9043218344449997e-06

Training epoch-82 batch-44
Running loss of epoch-82 batch-44 = 5.770009011030197e-06

Training epoch-82 batch-45
Running loss of epoch-82 batch-45 = 2.6100315153598785e-06

Training epoch-82 batch-46
Running loss of epoch-82 batch-46 = 3.0463561415672302e-06

Training epoch-82 batch-47
Running loss of epoch-82 batch-47 = 1.0058283805847168e-06

Training epoch-82 batch-48
Running loss of epoch-82 batch-48 = 2.9730144888162613e-06

Training epoch-82 batch-49
Running loss of epoch-82 batch-49 = 1.1061783879995346e-06

Training epoch-82 batch-50
Running loss of epoch-82 batch-50 = 5.890149623155594e-06

Training epoch-82 batch-51
Running loss of epoch-82 batch-51 = 1.9315630197525024e-06

Training epoch-82 batch-52
Running loss of epoch-82 batch-52 = 4.144385457038879e-06

Training epoch-82 batch-53
Running loss of epoch-82 batch-53 = 4.794448614120483e-06

Training epoch-82 batch-54
Running loss of epoch-82 batch-54 = 2.0724255591630936e-06

Training epoch-82 batch-55
Running loss of epoch-82 batch-55 = 3.071967512369156e-06

Training epoch-82 batch-56
Running loss of epoch-82 batch-56 = 3.3138785511255264e-06

Training epoch-82 batch-57
Running loss of epoch-82 batch-57 = 2.034008502960205e-06

Training epoch-82 batch-58
Running loss of epoch-82 batch-58 = 2.098502591252327e-06

Training epoch-82 batch-59
Running loss of epoch-82 batch-59 = 2.7818605303764343e-06

Training epoch-82 batch-60
Running loss of epoch-82 batch-60 = 6.155343726277351e-06

Training epoch-82 batch-61
Running loss of epoch-82 batch-61 = 2.586282789707184e-06

Training epoch-82 batch-62
Running loss of epoch-82 batch-62 = 1.7704442143440247e-06

Training epoch-82 batch-63
Running loss of epoch-82 batch-63 = 3.1900126487016678e-06

Training epoch-82 batch-64
Running loss of epoch-82 batch-64 = 2.4887267500162125e-06

Training epoch-82 batch-65
Running loss of epoch-82 batch-65 = 4.3050386011600494e-06

Training epoch-82 batch-66
Running loss of epoch-82 batch-66 = 1.1280644685029984e-06

Training epoch-82 batch-67
Running loss of epoch-82 batch-67 = 2.405606210231781e-06

Training epoch-82 batch-68
Running loss of epoch-82 batch-68 = 3.0158553272485733e-06

Training epoch-82 batch-69
Running loss of epoch-82 batch-69 = 5.8014411479234695e-06

Training epoch-82 batch-70
Running loss of epoch-82 batch-70 = 4.3602194637060165e-06

Training epoch-82 batch-71
Running loss of epoch-82 batch-71 = 1.7543788999319077e-06

Training epoch-82 batch-72
Running loss of epoch-82 batch-72 = 9.24128107726574e-06

Training epoch-82 batch-73
Running loss of epoch-82 batch-73 = 2.0689330995082855e-06

Training epoch-82 batch-74
Running loss of epoch-82 batch-74 = 1.7043203115463257e-06

Training epoch-82 batch-75
Running loss of epoch-82 batch-75 = 3.414694219827652e-06

Training epoch-82 batch-76
Running loss of epoch-82 batch-76 = 3.089429810643196e-06

Training epoch-82 batch-77
Running loss of epoch-82 batch-77 = 3.2391399145126343e-06

Training epoch-82 batch-78
Running loss of epoch-82 batch-78 = 2.021435648202896e-06

Training epoch-82 batch-79
Running loss of epoch-82 batch-79 = 2.3087486624717712e-06

Training epoch-82 batch-80
Running loss of epoch-82 batch-80 = 2.2924505174160004e-06

Training epoch-82 batch-81
Running loss of epoch-82 batch-81 = 2.986285835504532e-06

Training epoch-82 batch-82
Running loss of epoch-82 batch-82 = 6.223795935511589e-06

Training epoch-82 batch-83
Running loss of epoch-82 batch-83 = 4.961621016263962e-06

Training epoch-82 batch-84
Running loss of epoch-82 batch-84 = 2.7213245630264282e-06

Training epoch-82 batch-85
Running loss of epoch-82 batch-85 = 1.3802200555801392e-06

Training epoch-82 batch-86
Running loss of epoch-82 batch-86 = 2.782559022307396e-06

Training epoch-82 batch-87
Running loss of epoch-82 batch-87 = 1.312699168920517e-06

Training epoch-82 batch-88
Running loss of epoch-82 batch-88 = 1.9492581486701965e-06

Training epoch-82 batch-89
Running loss of epoch-82 batch-89 = 2.361135557293892e-06

Training epoch-82 batch-90
Running loss of epoch-82 batch-90 = 2.9301736503839493e-06

Training epoch-82 batch-91
Running loss of epoch-82 batch-91 = 1.341337338089943e-06

Training epoch-82 batch-92
Running loss of epoch-82 batch-92 = 2.292916178703308e-06

Training epoch-82 batch-93
Running loss of epoch-82 batch-93 = 5.695270374417305e-06

Training epoch-82 batch-94
Running loss of epoch-82 batch-94 = 1.5562400221824646e-06

Training epoch-82 batch-95
Running loss of epoch-82 batch-95 = 6.800750270485878e-06

Training epoch-82 batch-96
Running loss of epoch-82 batch-96 = 2.1150335669517517e-06

Training epoch-82 batch-97
Running loss of epoch-82 batch-97 = 2.185581251978874e-06

Training epoch-82 batch-98
Running loss of epoch-82 batch-98 = 4.4817570596933365e-06

Training epoch-82 batch-99
Running loss of epoch-82 batch-99 = 2.0689330995082855e-06

Training epoch-82 batch-100
Running loss of epoch-82 batch-100 = 2.050073817372322e-06

Training epoch-82 batch-101
Running loss of epoch-82 batch-101 = 2.876855432987213e-06

Training epoch-82 batch-102
Running loss of epoch-82 batch-102 = 2.9115471988916397e-06

Training epoch-82 batch-103
Running loss of epoch-82 batch-103 = 2.8542708605527878e-06

Training epoch-82 batch-104
Running loss of epoch-82 batch-104 = 2.7099158614873886e-06

Training epoch-82 batch-105
Running loss of epoch-82 batch-105 = 3.7641730159521103e-06

Training epoch-82 batch-106
Running loss of epoch-82 batch-106 = 2.0726583898067474e-06

Training epoch-82 batch-107
Running loss of epoch-82 batch-107 = 5.9194862842559814e-06

Training epoch-82 batch-108
Running loss of epoch-82 batch-108 = 2.819811925292015e-06

Training epoch-82 batch-109
Running loss of epoch-82 batch-109 = 5.326000973582268e-06

Training epoch-82 batch-110
Running loss of epoch-82 batch-110 = 2.955319359898567e-06

Training epoch-82 batch-111
Running loss of epoch-82 batch-111 = 2.2780150175094604e-06

Training epoch-82 batch-112
Running loss of epoch-82 batch-112 = 1.4067627489566803e-06

Training epoch-82 batch-113
Running loss of epoch-82 batch-113 = 3.5567209124565125e-06

Training epoch-82 batch-114
Running loss of epoch-82 batch-114 = 2.5297049432992935e-06

Training epoch-82 batch-115
Running loss of epoch-82 batch-115 = 5.819601938128471e-06

Training epoch-82 batch-116
Running loss of epoch-82 batch-116 = 2.5387853384017944e-06

Training epoch-82 batch-117
Running loss of epoch-82 batch-117 = 3.5194680094718933e-06

Training epoch-82 batch-118
Running loss of epoch-82 batch-118 = 2.3227185010910034e-06

Training epoch-82 batch-119
Running loss of epoch-82 batch-119 = 3.380468115210533e-06

Training epoch-82 batch-120
Running loss of epoch-82 batch-120 = 2.3194588720798492e-06

Training epoch-82 batch-121
Running loss of epoch-82 batch-121 = 7.4089039117097855e-06

Training epoch-82 batch-122
Running loss of epoch-82 batch-122 = 2.2137537598609924e-06

Training epoch-82 batch-123
Running loss of epoch-82 batch-123 = 3.4940894693136215e-06

Training epoch-82 batch-124
Running loss of epoch-82 batch-124 = 4.2584724724292755e-06

Training epoch-82 batch-125
Running loss of epoch-82 batch-125 = 7.566530257463455e-06

Training epoch-82 batch-126
Running loss of epoch-82 batch-126 = 3.963010385632515e-06

Training epoch-82 batch-127
Running loss of epoch-82 batch-127 = 1.69314444065094e-06

Training epoch-82 batch-128
Running loss of epoch-82 batch-128 = 2.9855873435735703e-06

Training epoch-82 batch-129
Running loss of epoch-82 batch-129 = 2.169981598854065e-06

Training epoch-82 batch-130
Running loss of epoch-82 batch-130 = 2.674991264939308e-06

Training epoch-82 batch-131
Running loss of epoch-82 batch-131 = 3.5134144127368927e-06

Training epoch-82 batch-132
Running loss of epoch-82 batch-132 = 2.5955960154533386e-06

Training epoch-82 batch-133
Running loss of epoch-82 batch-133 = 1.8214341253042221e-06

Training epoch-82 batch-134
Running loss of epoch-82 batch-134 = 3.250548616051674e-06

Training epoch-82 batch-135
Running loss of epoch-82 batch-135 = 3.2745301723480225e-06

Training epoch-82 batch-136
Running loss of epoch-82 batch-136 = 2.564862370491028e-06

Training epoch-82 batch-137
Running loss of epoch-82 batch-137 = 1.9865110516548157e-06

Training epoch-82 batch-138
Running loss of epoch-82 batch-138 = 7.756287232041359e-06

Training epoch-82 batch-139
Running loss of epoch-82 batch-139 = 3.134133294224739e-06

Training epoch-82 batch-140
Running loss of epoch-82 batch-140 = 4.675472155213356e-06

Training epoch-82 batch-141
Running loss of epoch-82 batch-141 = 1.866137608885765e-06

Training epoch-82 batch-142
Running loss of epoch-82 batch-142 = 3.064284101128578e-06

Training epoch-82 batch-143
Running loss of epoch-82 batch-143 = 1.0973308235406876e-06

Training epoch-82 batch-144
Running loss of epoch-82 batch-144 = 1.1159572750329971e-06

Training epoch-82 batch-145
Running loss of epoch-82 batch-145 = 2.6372727006673813e-06

Training epoch-82 batch-146
Running loss of epoch-82 batch-146 = 2.825632691383362e-06

Training epoch-82 batch-147
Running loss of epoch-82 batch-147 = 2.8263311833143234e-06

Training epoch-82 batch-148
Running loss of epoch-82 batch-148 = 2.8247013688087463e-06

Training epoch-82 batch-149
Running loss of epoch-82 batch-149 = 4.1262246668338776e-06

Training epoch-82 batch-150
Running loss of epoch-82 batch-150 = 4.967674612998962e-06

Training epoch-82 batch-151
Running loss of epoch-82 batch-151 = 5.426118150353432e-06

Training epoch-82 batch-152
Running loss of epoch-82 batch-152 = 3.1923409551382065e-06

Training epoch-82 batch-153
Running loss of epoch-82 batch-153 = 2.496875822544098e-06

Training epoch-82 batch-154
Running loss of epoch-82 batch-154 = 3.080349415540695e-06

Training epoch-82 batch-155
Running loss of epoch-82 batch-155 = 4.2335595935583115e-06

Training epoch-82 batch-156
Running loss of epoch-82 batch-156 = 2.2761523723602295e-06

Training epoch-82 batch-157
Running loss of epoch-82 batch-157 = 9.831041097640991e-06

Finished training epoch-82.



Average train loss at epoch-82 = 3.1632214784622194e-06

Started Evaluation

Average val loss at epoch-82 = 1.1800716189446263

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.74 %
Accuracy for class run is: 62.33 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.10 %

Finished Evaluation



Started training epoch-83


Training epoch-83 batch-1
Running loss of epoch-83 batch-1 = 2.3120082914829254e-06

Training epoch-83 batch-2
Running loss of epoch-83 batch-2 = 9.121838957071304e-06

Training epoch-83 batch-3
Running loss of epoch-83 batch-3 = 3.3408869057893753e-06

Training epoch-83 batch-4
Running loss of epoch-83 batch-4 = 3.853347152471542e-06

Training epoch-83 batch-5
Running loss of epoch-83 batch-5 = 1.634005457162857e-06

Training epoch-83 batch-6
Running loss of epoch-83 batch-6 = 1.9462313503026962e-06

Training epoch-83 batch-7
Running loss of epoch-83 batch-7 = 4.549743607640266e-06

Training epoch-83 batch-8
Running loss of epoch-83 batch-8 = 3.023771569132805e-06

Training epoch-83 batch-9
Running loss of epoch-83 batch-9 = 6.051966920495033e-06

Training epoch-83 batch-10
Running loss of epoch-83 batch-10 = 2.5972258299589157e-06

Training epoch-83 batch-11
Running loss of epoch-83 batch-11 = 9.619398042559624e-06

Training epoch-83 batch-12
Running loss of epoch-83 batch-12 = 1.74669548869133e-06

Training epoch-83 batch-13
Running loss of epoch-83 batch-13 = 8.127186447381973e-06

Training epoch-83 batch-14
Running loss of epoch-83 batch-14 = 1.937383785843849e-06

Training epoch-83 batch-15
Running loss of epoch-83 batch-15 = 2.6025809347629547e-06

Training epoch-83 batch-16
Running loss of epoch-83 batch-16 = 1.1152587831020355e-06

Training epoch-83 batch-17
Running loss of epoch-83 batch-17 = 3.334134817123413e-06

Training epoch-83 batch-18
Running loss of epoch-83 batch-18 = 4.996778443455696e-06

Training epoch-83 batch-19
Running loss of epoch-83 batch-19 = 2.8798822313547134e-06

Training epoch-83 batch-20
Running loss of epoch-83 batch-20 = 2.1345913410186768e-06

Training epoch-83 batch-21
Running loss of epoch-83 batch-21 = 3.309221938252449e-06

Training epoch-83 batch-22
Running loss of epoch-83 batch-22 = 1.8561258912086487e-06

Training epoch-83 batch-23
Running loss of epoch-83 batch-23 = 4.281522706151009e-06

Training epoch-83 batch-24
Running loss of epoch-83 batch-24 = 6.429152563214302e-06

Training epoch-83 batch-25
Running loss of epoch-83 batch-25 = 2.8226058930158615e-06

Training epoch-83 batch-26
Running loss of epoch-83 batch-26 = 3.8070138543844223e-06

Training epoch-83 batch-27
Running loss of epoch-83 batch-27 = 4.485482349991798e-06

Training epoch-83 batch-28
Running loss of epoch-83 batch-28 = 1.5010591596364975e-06

Training epoch-83 batch-29
Running loss of epoch-83 batch-29 = 1.542968675494194e-06

Training epoch-83 batch-30
Running loss of epoch-83 batch-30 = 5.311798304319382e-06

Training epoch-83 batch-31
Running loss of epoch-83 batch-31 = 2.051703631877899e-06

Training epoch-83 batch-32
Running loss of epoch-83 batch-32 = 1.7236452549695969e-06

Training epoch-83 batch-33
Running loss of epoch-83 batch-33 = 1.9459985196590424e-06

Training epoch-83 batch-34
Running loss of epoch-83 batch-34 = 2.4312175810337067e-06

Training epoch-83 batch-35
Running loss of epoch-83 batch-35 = 3.4938566386699677e-06

Training epoch-83 batch-36
Running loss of epoch-83 batch-36 = 4.1746534407138824e-06

Training epoch-83 batch-37
Running loss of epoch-83 batch-37 = 2.907123416662216e-06

Training epoch-83 batch-38
Running loss of epoch-83 batch-38 = 2.2954773157835007e-06

Training epoch-83 batch-39
Running loss of epoch-83 batch-39 = 2.1103769540786743e-06

Training epoch-83 batch-40
Running loss of epoch-83 batch-40 = 2.298620529472828e-06

Training epoch-83 batch-41
Running loss of epoch-83 batch-41 = 1.8833670765161514e-06

Training epoch-83 batch-42
Running loss of epoch-83 batch-42 = 3.7837307900190353e-06

Training epoch-83 batch-43
Running loss of epoch-83 batch-43 = 9.923242032527924e-07

Training epoch-83 batch-44
Running loss of epoch-83 batch-44 = 4.188157618045807e-06

Training epoch-83 batch-45
Running loss of epoch-83 batch-45 = 3.6205165088176727e-06

Training epoch-83 batch-46
Running loss of epoch-83 batch-46 = 2.50362791121006e-06

Training epoch-83 batch-47
Running loss of epoch-83 batch-47 = 1.8852297216653824e-06

Training epoch-83 batch-48
Running loss of epoch-83 batch-48 = 3.058928996324539e-06

Training epoch-83 batch-49
Running loss of epoch-83 batch-49 = 5.2656978368759155e-06

Training epoch-83 batch-50
Running loss of epoch-83 batch-50 = 2.9227230697870255e-06

Training epoch-83 batch-51
Running loss of epoch-83 batch-51 = 1.3918615877628326e-06

Training epoch-83 batch-52
Running loss of epoch-83 batch-52 = 4.290370270609856e-06

Training epoch-83 batch-53
Running loss of epoch-83 batch-53 = 1.9364524632692337e-06

Training epoch-83 batch-54
Running loss of epoch-83 batch-54 = 2.371380105614662e-06

Training epoch-83 batch-55
Running loss of epoch-83 batch-55 = 8.905772119760513e-07

Training epoch-83 batch-56
Running loss of epoch-83 batch-56 = 4.407484084367752e-06

Training epoch-83 batch-57
Running loss of epoch-83 batch-57 = 4.692934453487396e-06

Training epoch-83 batch-58
Running loss of epoch-83 batch-58 = 1.3986136764287949e-06

Training epoch-83 batch-59
Running loss of epoch-83 batch-59 = 6.251269951462746e-06

Training epoch-83 batch-60
Running loss of epoch-83 batch-60 = 3.5942066460847855e-06

Training epoch-83 batch-61
Running loss of epoch-83 batch-61 = 3.557885065674782e-06

Training epoch-83 batch-62
Running loss of epoch-83 batch-62 = 2.9203947633504868e-06

Training epoch-83 batch-63
Running loss of epoch-83 batch-63 = 4.0638260543346405e-06

Training epoch-83 batch-64
Running loss of epoch-83 batch-64 = 2.8221402317285538e-06

Training epoch-83 batch-65
Running loss of epoch-83 batch-65 = 4.286877810955048e-06

Training epoch-83 batch-66
Running loss of epoch-83 batch-66 = 2.1604355424642563e-06

Training epoch-83 batch-67
Running loss of epoch-83 batch-67 = 1.5760306268930435e-06

Training epoch-83 batch-68
Running loss of epoch-83 batch-68 = 2.3529864847660065e-06

Training epoch-83 batch-69
Running loss of epoch-83 batch-69 = 1.1117663234472275e-06

Training epoch-83 batch-70
Running loss of epoch-83 batch-70 = 5.053821951150894e-06

Training epoch-83 batch-71
Running loss of epoch-83 batch-71 = 1.3455282896757126e-06

Training epoch-83 batch-72
Running loss of epoch-83 batch-72 = 4.627741873264313e-06

Training epoch-83 batch-73
Running loss of epoch-83 batch-73 = 2.5918707251548767e-06

Training epoch-83 batch-74
Running loss of epoch-83 batch-74 = 2.27917917072773e-06

Training epoch-83 batch-75
Running loss of epoch-83 batch-75 = 1.6803387552499771e-06

Training epoch-83 batch-76
Running loss of epoch-83 batch-76 = 4.1441526263952255e-06

Training epoch-83 batch-77
Running loss of epoch-83 batch-77 = 3.227265551686287e-06

Training epoch-83 batch-78
Running loss of epoch-83 batch-78 = 3.963476046919823e-06

Training epoch-83 batch-79
Running loss of epoch-83 batch-79 = 2.0193401724100113e-06

Training epoch-83 batch-80
Running loss of epoch-83 batch-80 = 1.4007091522216797e-06

Training epoch-83 batch-81
Running loss of epoch-83 batch-81 = 2.6745256036520004e-06

Training epoch-83 batch-82
Running loss of epoch-83 batch-82 = 1.1289957910776138e-06

Training epoch-83 batch-83
Running loss of epoch-83 batch-83 = 2.2854655981063843e-06

Training epoch-83 batch-84
Running loss of epoch-83 batch-84 = 4.006084054708481e-06

Training epoch-83 batch-85
Running loss of epoch-83 batch-85 = 3.482447937130928e-06

Training epoch-83 batch-86
Running loss of epoch-83 batch-86 = 5.242181941866875e-06

Training epoch-83 batch-87
Running loss of epoch-83 batch-87 = 3.1981617212295532e-06

Training epoch-83 batch-88
Running loss of epoch-83 batch-88 = 3.1348317861557007e-06

Training epoch-83 batch-89
Running loss of epoch-83 batch-89 = 1.9085127860307693e-06

Training epoch-83 batch-90
Running loss of epoch-83 batch-90 = 4.997709766030312e-06

Training epoch-83 batch-91
Running loss of epoch-83 batch-91 = 2.007465809583664e-06

Training epoch-83 batch-92
Running loss of epoch-83 batch-92 = 3.2514799386262894e-06

Training epoch-83 batch-93
Running loss of epoch-83 batch-93 = 2.501998096704483e-06

Training epoch-83 batch-94
Running loss of epoch-83 batch-94 = 2.1359883248806e-06

Training epoch-83 batch-95
Running loss of epoch-83 batch-95 = 2.262648195028305e-06

Training epoch-83 batch-96
Running loss of epoch-83 batch-96 = 8.225906640291214e-07

Training epoch-83 batch-97
Running loss of epoch-83 batch-97 = 2.096407115459442e-06

Training epoch-83 batch-98
Running loss of epoch-83 batch-98 = 4.139496013522148e-06

Training epoch-83 batch-99
Running loss of epoch-83 batch-99 = 8.821487426757812e-06

Training epoch-83 batch-100
Running loss of epoch-83 batch-100 = 3.641704097390175e-06

Training epoch-83 batch-101
Running loss of epoch-83 batch-101 = 4.2496249079704285e-06

Training epoch-83 batch-102
Running loss of epoch-83 batch-102 = 4.163011908531189e-06

Training epoch-83 batch-103
Running loss of epoch-83 batch-103 = 2.5243498384952545e-06

Training epoch-83 batch-104
Running loss of epoch-83 batch-104 = 2.6170164346694946e-06

Training epoch-83 batch-105
Running loss of epoch-83 batch-105 = 3.316206857562065e-06

Training epoch-83 batch-106
Running loss of epoch-83 batch-106 = 1.0533258318901062e-06

Training epoch-83 batch-107
Running loss of epoch-83 batch-107 = 1.8274877220392227e-06

Training epoch-83 batch-108
Running loss of epoch-83 batch-108 = 1.6675330698490143e-06

Training epoch-83 batch-109
Running loss of epoch-83 batch-109 = 2.6223715394735336e-06

Training epoch-83 batch-110
Running loss of epoch-83 batch-110 = 5.433335900306702e-06

Training epoch-83 batch-111
Running loss of epoch-83 batch-111 = 3.651948645710945e-06

Training epoch-83 batch-112
Running loss of epoch-83 batch-112 = 2.6207417249679565e-06

Training epoch-83 batch-113
Running loss of epoch-83 batch-113 = 2.3865140974521637e-06

Training epoch-83 batch-114
Running loss of epoch-83 batch-114 = 2.7802307158708572e-06

Training epoch-83 batch-115
Running loss of epoch-83 batch-115 = 1.6405247151851654e-06

Training epoch-83 batch-116
Running loss of epoch-83 batch-116 = 4.914356395602226e-06

Training epoch-83 batch-117
Running loss of epoch-83 batch-117 = 1.850305125117302e-06

Training epoch-83 batch-118
Running loss of epoch-83 batch-118 = 1.6696285456418991e-06

Training epoch-83 batch-119
Running loss of epoch-83 batch-119 = 3.1266827136278152e-06

Training epoch-83 batch-120
Running loss of epoch-83 batch-120 = 1.8065329641103745e-06

Training epoch-83 batch-121
Running loss of epoch-83 batch-121 = 2.132495865225792e-06

Training epoch-83 batch-122
Running loss of epoch-83 batch-122 = 3.2314565032720566e-06

Training epoch-83 batch-123
Running loss of epoch-83 batch-123 = 1.2437812983989716e-06

Training epoch-83 batch-124
Running loss of epoch-83 batch-124 = 3.228895366191864e-06

Training epoch-83 batch-125
Running loss of epoch-83 batch-125 = 3.0423980206251144e-06

Training epoch-83 batch-126
Running loss of epoch-83 batch-126 = 3.363238647580147e-06

Training epoch-83 batch-127
Running loss of epoch-83 batch-127 = 3.3669639378786087e-06

Training epoch-83 batch-128
Running loss of epoch-83 batch-128 = 3.5176053643226624e-06

Training epoch-83 batch-129
Running loss of epoch-83 batch-129 = 2.5262124836444855e-06

Training epoch-83 batch-130
Running loss of epoch-83 batch-130 = 3.0174851417541504e-06

Training epoch-83 batch-131
Running loss of epoch-83 batch-131 = 1.8940772861242294e-06

Training epoch-83 batch-132
Running loss of epoch-83 batch-132 = 3.6726705729961395e-06

Training epoch-83 batch-133
Running loss of epoch-83 batch-133 = 1.5560071915388107e-06

Training epoch-83 batch-134
Running loss of epoch-83 batch-134 = 2.5387853384017944e-06

Training epoch-83 batch-135
Running loss of epoch-83 batch-135 = 2.401880919933319e-06

Training epoch-83 batch-136
Running loss of epoch-83 batch-136 = 2.743443474173546e-06

Training epoch-83 batch-137
Running loss of epoch-83 batch-137 = 1.92178413271904e-06

Training epoch-83 batch-138
Running loss of epoch-83 batch-138 = 2.43261456489563e-06

Training epoch-83 batch-139
Running loss of epoch-83 batch-139 = 4.6496279537677765e-06

Training epoch-83 batch-140
Running loss of epoch-83 batch-140 = 2.137618139386177e-06

Training epoch-83 batch-141
Running loss of epoch-83 batch-141 = 4.376983270049095e-06

Training epoch-83 batch-142
Running loss of epoch-83 batch-142 = 3.862660378217697e-06

Training epoch-83 batch-143
Running loss of epoch-83 batch-143 = 2.362998202443123e-06

Training epoch-83 batch-144
Running loss of epoch-83 batch-144 = 1.6489066183567047e-06

Training epoch-83 batch-145
Running loss of epoch-83 batch-145 = 2.5932677090168e-06

Training epoch-83 batch-146
Running loss of epoch-83 batch-146 = 1.6835983842611313e-06

Training epoch-83 batch-147
Running loss of epoch-83 batch-147 = 2.016080543398857e-06

Training epoch-83 batch-148
Running loss of epoch-83 batch-148 = 2.9138755053281784e-06

Training epoch-83 batch-149
Running loss of epoch-83 batch-149 = 6.269197911024094e-06

Training epoch-83 batch-150
Running loss of epoch-83 batch-150 = 3.480585291981697e-06

Training epoch-83 batch-151
Running loss of epoch-83 batch-151 = 3.3015385270118713e-06

Training epoch-83 batch-152
Running loss of epoch-83 batch-152 = 9.469222277402878e-07

Training epoch-83 batch-153
Running loss of epoch-83 batch-153 = 2.001877874135971e-06

Training epoch-83 batch-154
Running loss of epoch-83 batch-154 = 2.8565991669893265e-06

Training epoch-83 batch-155
Running loss of epoch-83 batch-155 = 1.9988510757684708e-06

Training epoch-83 batch-156
Running loss of epoch-83 batch-156 = 8.563976734876633e-06

Training epoch-83 batch-157
Running loss of epoch-83 batch-157 = 3.152340650558472e-05

Finished training epoch-83.



Average train loss at epoch-83 = 3.131260722875595e-06

Started Evaluation

Average val loss at epoch-83 = 1.1846995916637895

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.05 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 53.81 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 71.54 %

Overall Accuracy = 83.04 %

Finished Evaluation



Started training epoch-84


Training epoch-84 batch-1
Running loss of epoch-84 batch-1 = 1.5003606677055359e-06

Training epoch-84 batch-2
Running loss of epoch-84 batch-2 = 3.729015588760376e-06

Training epoch-84 batch-3
Running loss of epoch-84 batch-3 = 2.2065360099077225e-06

Training epoch-84 batch-4
Running loss of epoch-84 batch-4 = 2.4188775569200516e-06

Training epoch-84 batch-5
Running loss of epoch-84 batch-5 = 1.8025748431682587e-06

Training epoch-84 batch-6
Running loss of epoch-84 batch-6 = 2.4761538952589035e-06

Training epoch-84 batch-7
Running loss of epoch-84 batch-7 = 3.987457603216171e-06

Training epoch-84 batch-8
Running loss of epoch-84 batch-8 = 2.648448571562767e-06

Training epoch-84 batch-9
Running loss of epoch-84 batch-9 = 1.7741695046424866e-06

Training epoch-84 batch-10
Running loss of epoch-84 batch-10 = 3.1266827136278152e-06

Training epoch-84 batch-11
Running loss of epoch-84 batch-11 = 3.452179953455925e-06

Training epoch-84 batch-12
Running loss of epoch-84 batch-12 = 2.1723099052906036e-06

Training epoch-84 batch-13
Running loss of epoch-84 batch-13 = 2.2049061954021454e-06

Training epoch-84 batch-14
Running loss of epoch-84 batch-14 = 3.815162926912308e-06

Training epoch-84 batch-15
Running loss of epoch-84 batch-15 = 3.7071295082569122e-06

Training epoch-84 batch-16
Running loss of epoch-84 batch-16 = 4.220288246870041e-06

Training epoch-84 batch-17
Running loss of epoch-84 batch-17 = 4.638219252228737e-06

Training epoch-84 batch-18
Running loss of epoch-84 batch-18 = 2.5120098143815994e-06

Training epoch-84 batch-19
Running loss of epoch-84 batch-19 = 3.920635208487511e-06

Training epoch-84 batch-20
Running loss of epoch-84 batch-20 = 9.576324373483658e-07

Training epoch-84 batch-21
Running loss of epoch-84 batch-21 = 2.7972273528575897e-06

Training epoch-84 batch-22
Running loss of epoch-84 batch-22 = 1.825159415602684e-06

Training epoch-84 batch-23
Running loss of epoch-84 batch-23 = 1.7504207789897919e-06

Training epoch-84 batch-24
Running loss of epoch-84 batch-24 = 1.5848781913518906e-06

Training epoch-84 batch-25
Running loss of epoch-84 batch-25 = 3.319932147860527e-06

Training epoch-84 batch-26
Running loss of epoch-84 batch-26 = 3.2635871320962906e-06

Training epoch-84 batch-27
Running loss of epoch-84 batch-27 = 4.678498953580856e-06

Training epoch-84 batch-28
Running loss of epoch-84 batch-28 = 5.28222881257534e-06

Training epoch-84 batch-29
Running loss of epoch-84 batch-29 = 5.184905603528023e-06

Training epoch-84 batch-30
Running loss of epoch-84 batch-30 = 3.5760458558797836e-06

Training epoch-84 batch-31
Running loss of epoch-84 batch-31 = 2.73948535323143e-06

Training epoch-84 batch-32
Running loss of epoch-84 batch-32 = 2.533895894885063e-06

Training epoch-84 batch-33
Running loss of epoch-84 batch-33 = 2.1848827600479126e-06

Training epoch-84 batch-34
Running loss of epoch-84 batch-34 = 1.6796402633190155e-06

Training epoch-84 batch-35
Running loss of epoch-84 batch-35 = 4.664063453674316e-06

Training epoch-84 batch-36
Running loss of epoch-84 batch-36 = 3.8994476199150085e-06

Training epoch-84 batch-37
Running loss of epoch-84 batch-37 = 4.014931619167328e-06

Training epoch-84 batch-38
Running loss of epoch-84 batch-38 = 1.1259689927101135e-06

Training epoch-84 batch-39
Running loss of epoch-84 batch-39 = 1.3012904673814774e-06

Training epoch-84 batch-40
Running loss of epoch-84 batch-40 = 3.2600946724414825e-06

Training epoch-84 batch-41
Running loss of epoch-84 batch-41 = 3.978610038757324e-06

Training epoch-84 batch-42
Running loss of epoch-84 batch-42 = 2.336222678422928e-06

Training epoch-84 batch-43
Running loss of epoch-84 batch-43 = 1.628650352358818e-06

Training epoch-84 batch-44
Running loss of epoch-84 batch-44 = 2.7457717806100845e-06

Training epoch-84 batch-45
Running loss of epoch-84 batch-45 = 1.7362181097269058e-06

Training epoch-84 batch-46
Running loss of epoch-84 batch-46 = 2.874992787837982e-06

Training epoch-84 batch-47
Running loss of epoch-84 batch-47 = 4.180707037448883e-06

Training epoch-84 batch-48
Running loss of epoch-84 batch-48 = 2.0992010831832886e-06

Training epoch-84 batch-49
Running loss of epoch-84 batch-49 = 1.4065299183130264e-06

Training epoch-84 batch-50
Running loss of epoch-84 batch-50 = 1.7136335372924805e-06

Training epoch-84 batch-51
Running loss of epoch-84 batch-51 = 1.4137476682662964e-06

Training epoch-84 batch-52
Running loss of epoch-84 batch-52 = 3.3944379538297653e-06

Training epoch-84 batch-53
Running loss of epoch-84 batch-53 = 2.4889595806598663e-06

Training epoch-84 batch-54
Running loss of epoch-84 batch-54 = 2.3294705897569656e-06

Training epoch-84 batch-55
Running loss of epoch-84 batch-55 = 3.0421651899814606e-06

Training epoch-84 batch-56
Running loss of epoch-84 batch-56 = 2.8850045055150986e-06

Training epoch-84 batch-57
Running loss of epoch-84 batch-57 = 2.16485932469368e-06

Training epoch-84 batch-58
Running loss of epoch-84 batch-58 = 1.7636921256780624e-06

Training epoch-84 batch-59
Running loss of epoch-84 batch-59 = 9.960262104868889e-06

Training epoch-84 batch-60
Running loss of epoch-84 batch-60 = 1.2686941772699356e-06

Training epoch-84 batch-61
Running loss of epoch-84 batch-61 = 7.145572453737259e-07

Training epoch-84 batch-62
Running loss of epoch-84 batch-62 = 1.871492713689804e-06

Training epoch-84 batch-63
Running loss of epoch-84 batch-63 = 2.346932888031006e-06

Training epoch-84 batch-64
Running loss of epoch-84 batch-64 = 2.6351772248744965e-06

Training epoch-84 batch-65
Running loss of epoch-84 batch-65 = 6.412854418158531e-06

Training epoch-84 batch-66
Running loss of epoch-84 batch-66 = 1.9636936485767365e-06

Training epoch-84 batch-67
Running loss of epoch-84 batch-67 = 3.5213306546211243e-06

Training epoch-84 batch-68
Running loss of epoch-84 batch-68 = 1.2225937098264694e-06

Training epoch-84 batch-69
Running loss of epoch-84 batch-69 = 4.3353065848350525e-06

Training epoch-84 batch-70
Running loss of epoch-84 batch-70 = 3.448221832513809e-06

Training epoch-84 batch-71
Running loss of epoch-84 batch-71 = 1.8214341253042221e-06

Training epoch-84 batch-72
Running loss of epoch-84 batch-72 = 2.292916178703308e-06

Training epoch-84 batch-73
Running loss of epoch-84 batch-73 = 3.0328519642353058e-06

Training epoch-84 batch-74
Running loss of epoch-84 batch-74 = 9.718351066112518e-07

Training epoch-84 batch-75
Running loss of epoch-84 batch-75 = 1.5480909496545792e-06

Training epoch-84 batch-76
Running loss of epoch-84 batch-76 = 2.727378159761429e-06

Training epoch-84 batch-77
Running loss of epoch-84 batch-77 = 4.182802513241768e-06

Training epoch-84 batch-78
Running loss of epoch-84 batch-78 = 2.600019797682762e-06

Training epoch-84 batch-79
Running loss of epoch-84 batch-79 = 3.373948857188225e-06

Training epoch-84 batch-80
Running loss of epoch-84 batch-80 = 3.6766286939382553e-06

Training epoch-84 batch-81
Running loss of epoch-84 batch-81 = 3.7294812500476837e-06

Training epoch-84 batch-82
Running loss of epoch-84 batch-82 = 1.7995480448007584e-06

Training epoch-84 batch-83
Running loss of epoch-84 batch-83 = 4.899920895695686e-06

Training epoch-84 batch-84
Running loss of epoch-84 batch-84 = 2.078479155898094e-06

Training epoch-84 batch-85
Running loss of epoch-84 batch-85 = 3.6598648875951767e-06

Training epoch-84 batch-86
Running loss of epoch-84 batch-86 = 5.324371159076691e-06

Training epoch-84 batch-87
Running loss of epoch-84 batch-87 = 2.3692846298217773e-06

Training epoch-84 batch-88
Running loss of epoch-84 batch-88 = 1.2051314115524292e-06

Training epoch-84 batch-89
Running loss of epoch-84 batch-89 = 2.0132865756750107e-06

Training epoch-84 batch-90
Running loss of epoch-84 batch-90 = 1.4391262084245682e-06

Training epoch-84 batch-91
Running loss of epoch-84 batch-91 = 3.468012437224388e-06

Training epoch-84 batch-92
Running loss of epoch-84 batch-92 = 1.83936208486557e-06

Training epoch-84 batch-93
Running loss of epoch-84 batch-93 = 1.8833670765161514e-06

Training epoch-84 batch-94
Running loss of epoch-84 batch-94 = 1.8156133592128754e-06

Training epoch-84 batch-95
Running loss of epoch-84 batch-95 = 1.2621749192476273e-06

Training epoch-84 batch-96
Running loss of epoch-84 batch-96 = 5.0477683544158936e-06

Training epoch-84 batch-97
Running loss of epoch-84 batch-97 = 2.4205073714256287e-06

Training epoch-84 batch-98
Running loss of epoch-84 batch-98 = 3.896187990903854e-06

Training epoch-84 batch-99
Running loss of epoch-84 batch-99 = 3.4563709050416946e-06

Training epoch-84 batch-100
Running loss of epoch-84 batch-100 = 1.1962838470935822e-06

Training epoch-84 batch-101
Running loss of epoch-84 batch-101 = 2.912711352109909e-06

Training epoch-84 batch-102
Running loss of epoch-84 batch-102 = 1.462642103433609e-06

Training epoch-84 batch-103
Running loss of epoch-84 batch-103 = 8.804025128483772e-06

Training epoch-84 batch-104
Running loss of epoch-84 batch-104 = 1.786043867468834e-06

Training epoch-84 batch-105
Running loss of epoch-84 batch-105 = 2.8938520699739456e-06

Training epoch-84 batch-106
Running loss of epoch-84 batch-106 = 1.8991995602846146e-06

Training epoch-84 batch-107
Running loss of epoch-84 batch-107 = 3.251945599913597e-06

Training epoch-84 batch-108
Running loss of epoch-84 batch-108 = 4.38210554420948e-06

Training epoch-84 batch-109
Running loss of epoch-84 batch-109 = 2.3315660655498505e-06

Training epoch-84 batch-110
Running loss of epoch-84 batch-110 = 1.8882565200328827e-06

Training epoch-84 batch-111
Running loss of epoch-84 batch-111 = 7.446156814694405e-06

Training epoch-84 batch-112
Running loss of epoch-84 batch-112 = 3.5855919122695923e-06

Training epoch-84 batch-113
Running loss of epoch-84 batch-113 = 1.5050172805786133e-06

Training epoch-84 batch-114
Running loss of epoch-84 batch-114 = 3.404216840863228e-06

Training epoch-84 batch-115
Running loss of epoch-84 batch-115 = 1.9550789147615433e-06

Training epoch-84 batch-116
Running loss of epoch-84 batch-116 = 6.101327016949654e-06

Training epoch-84 batch-117
Running loss of epoch-84 batch-117 = 2.5208573788404465e-06

Training epoch-84 batch-118
Running loss of epoch-84 batch-118 = 3.8710422813892365e-06

Training epoch-84 batch-119
Running loss of epoch-84 batch-119 = 1.0798685252666473e-06

Training epoch-84 batch-120
Running loss of epoch-84 batch-120 = 2.0975712686777115e-06

Training epoch-84 batch-121
Running loss of epoch-84 batch-121 = 2.511311322450638e-06

Training epoch-84 batch-122
Running loss of epoch-84 batch-122 = 4.541827365756035e-06

Training epoch-84 batch-123
Running loss of epoch-84 batch-123 = 2.3008324205875397e-06

Training epoch-84 batch-124
Running loss of epoch-84 batch-124 = 3.744848072528839e-06

Training epoch-84 batch-125
Running loss of epoch-84 batch-125 = 5.33577986061573e-06

Training epoch-84 batch-126
Running loss of epoch-84 batch-126 = 3.5206321626901627e-06

Training epoch-84 batch-127
Running loss of epoch-84 batch-127 = 5.549518391489983e-06

Training epoch-84 batch-128
Running loss of epoch-84 batch-128 = 7.210997864603996e-06

Training epoch-84 batch-129
Running loss of epoch-84 batch-129 = 3.1532254070043564e-06

Training epoch-84 batch-130
Running loss of epoch-84 batch-130 = 2.505723387002945e-06

Training epoch-84 batch-131
Running loss of epoch-84 batch-131 = 1.5799887478351593e-06

Training epoch-84 batch-132
Running loss of epoch-84 batch-132 = 2.5695189833641052e-06

Training epoch-84 batch-133
Running loss of epoch-84 batch-133 = 4.288041964173317e-06

Training epoch-84 batch-134
Running loss of epoch-84 batch-134 = 8.678063750267029e-06

Training epoch-84 batch-135
Running loss of epoch-84 batch-135 = 6.578164175152779e-06

Training epoch-84 batch-136
Running loss of epoch-84 batch-136 = 2.130633220076561e-06

Training epoch-84 batch-137
Running loss of epoch-84 batch-137 = 2.123182639479637e-06

Training epoch-84 batch-138
Running loss of epoch-84 batch-138 = 2.258922904729843e-06

Training epoch-84 batch-139
Running loss of epoch-84 batch-139 = 3.134598955512047e-06

Training epoch-84 batch-140
Running loss of epoch-84 batch-140 = 2.0605511963367462e-06

Training epoch-84 batch-141
Running loss of epoch-84 batch-141 = 4.129717126488686e-06

Training epoch-84 batch-142
Running loss of epoch-84 batch-142 = 1.8172431737184525e-06

Training epoch-84 batch-143
Running loss of epoch-84 batch-143 = 2.762768417596817e-06

Training epoch-84 batch-144
Running loss of epoch-84 batch-144 = 6.557907909154892e-06

Training epoch-84 batch-145
Running loss of epoch-84 batch-145 = 4.32971864938736e-06

Training epoch-84 batch-146
Running loss of epoch-84 batch-146 = 2.0244624465703964e-06

Training epoch-84 batch-147
Running loss of epoch-84 batch-147 = 1.4835968613624573e-06

Training epoch-84 batch-148
Running loss of epoch-84 batch-148 = 4.339497536420822e-06

Training epoch-84 batch-149
Running loss of epoch-84 batch-149 = 2.768123522400856e-06

Training epoch-84 batch-150
Running loss of epoch-84 batch-150 = 1.9280705600976944e-06

Training epoch-84 batch-151
Running loss of epoch-84 batch-151 = 3.468478098511696e-06

Training epoch-84 batch-152
Running loss of epoch-84 batch-152 = 2.7937348932027817e-06

Training epoch-84 batch-153
Running loss of epoch-84 batch-153 = 2.436107024550438e-06

Training epoch-84 batch-154
Running loss of epoch-84 batch-154 = 4.1371677070856094e-06

Training epoch-84 batch-155
Running loss of epoch-84 batch-155 = 3.0209776014089584e-06

Training epoch-84 batch-156
Running loss of epoch-84 batch-156 = 1.8102582544088364e-06

Training epoch-84 batch-157
Running loss of epoch-84 batch-157 = 5.286186933517456e-06

Finished training epoch-84.



Average train loss at epoch-84 = 3.0396044254302978e-06

Started Evaluation

Average val loss at epoch-84 = 1.1842783183866699

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 93.92 %
Accuracy for class toString is: 88.05 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.71 %
Accuracy for class execute is: 42.97 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 83.16 %

Finished Evaluation



Started training epoch-85


Training epoch-85 batch-1
Running loss of epoch-85 batch-1 = 4.354165866971016e-06

Training epoch-85 batch-2
Running loss of epoch-85 batch-2 = 8.381903171539307e-07

Training epoch-85 batch-3
Running loss of epoch-85 batch-3 = 1.832377165555954e-06

Training epoch-85 batch-4
Running loss of epoch-85 batch-4 = 9.499490261077881e-06

Training epoch-85 batch-5
Running loss of epoch-85 batch-5 = 1.964392140507698e-06

Training epoch-85 batch-6
Running loss of epoch-85 batch-6 = 4.0978193283081055e-06

Training epoch-85 batch-7
Running loss of epoch-85 batch-7 = 1.8174760043621063e-06

Training epoch-85 batch-8
Running loss of epoch-85 batch-8 = 3.0992086976766586e-06

Training epoch-85 batch-9
Running loss of epoch-85 batch-9 = 3.3851247280836105e-06

Training epoch-85 batch-10
Running loss of epoch-85 batch-10 = 2.1224841475486755e-06

Training epoch-85 batch-11
Running loss of epoch-85 batch-11 = 4.956033080816269e-06

Training epoch-85 batch-12
Running loss of epoch-85 batch-12 = 2.4635810405015945e-06

Training epoch-85 batch-13
Running loss of epoch-85 batch-13 = 2.7676578611135483e-06

Training epoch-85 batch-14
Running loss of epoch-85 batch-14 = 2.3352913558483124e-06

Training epoch-85 batch-15
Running loss of epoch-85 batch-15 = 1.7995480448007584e-06

Training epoch-85 batch-16
Running loss of epoch-85 batch-16 = 2.655899152159691e-06

Training epoch-85 batch-17
Running loss of epoch-85 batch-17 = 3.1390227377414703e-06

Training epoch-85 batch-18
Running loss of epoch-85 batch-18 = 4.2244791984558105e-06

Training epoch-85 batch-19
Running loss of epoch-85 batch-19 = 3.6996789276599884e-06

Training epoch-85 batch-20
Running loss of epoch-85 batch-20 = 3.564637154340744e-06

Training epoch-85 batch-21
Running loss of epoch-85 batch-21 = 1.868000254034996e-06

Training epoch-85 batch-22
Running loss of epoch-85 batch-22 = 1.8882565200328827e-06

Training epoch-85 batch-23
Running loss of epoch-85 batch-23 = 3.973720595240593e-06

Training epoch-85 batch-24
Running loss of epoch-85 batch-24 = 5.9443991631269455e-06

Training epoch-85 batch-25
Running loss of epoch-85 batch-25 = 3.86824831366539e-06

Training epoch-85 batch-26
Running loss of epoch-85 batch-26 = 2.8477516025304794e-06

Training epoch-85 batch-27
Running loss of epoch-85 batch-27 = 3.2319221645593643e-06

Training epoch-85 batch-28
Running loss of epoch-85 batch-28 = 3.4885015338659286e-06

Training epoch-85 batch-29
Running loss of epoch-85 batch-29 = 2.125510945916176e-06

Training epoch-85 batch-30
Running loss of epoch-85 batch-30 = 2.9189977794885635e-06

Training epoch-85 batch-31
Running loss of epoch-85 batch-31 = 1.6891863197088242e-06

Training epoch-85 batch-32
Running loss of epoch-85 batch-32 = 2.710847184062004e-06

Training epoch-85 batch-33
Running loss of epoch-85 batch-33 = 4.867091774940491e-06

Training epoch-85 batch-34
Running loss of epoch-85 batch-34 = 2.925284206867218e-06

Training epoch-85 batch-35
Running loss of epoch-85 batch-35 = 7.702037692070007e-07

Training epoch-85 batch-36
Running loss of epoch-85 batch-36 = 3.5855919122695923e-06

Training epoch-85 batch-37
Running loss of epoch-85 batch-37 = 5.071749910712242e-06

Training epoch-85 batch-38
Running loss of epoch-85 batch-38 = 3.1744129955768585e-06

Training epoch-85 batch-39
Running loss of epoch-85 batch-39 = 2.7490314096212387e-06

Training epoch-85 batch-40
Running loss of epoch-85 batch-40 = 1.632142812013626e-06

Training epoch-85 batch-41
Running loss of epoch-85 batch-41 = 1.6915146261453629e-06

Training epoch-85 batch-42
Running loss of epoch-85 batch-42 = 2.384185791015625e-06

Training epoch-85 batch-43
Running loss of epoch-85 batch-43 = 2.4747569113969803e-06

Training epoch-85 batch-44
Running loss of epoch-85 batch-44 = 3.628898411989212e-06

Training epoch-85 batch-45
Running loss of epoch-85 batch-45 = 2.928543835878372e-06

Training epoch-85 batch-46
Running loss of epoch-85 batch-46 = 4.950212314724922e-06

Training epoch-85 batch-47
Running loss of epoch-85 batch-47 = 3.281049430370331e-06

Training epoch-85 batch-48
Running loss of epoch-85 batch-48 = 4.112487658858299e-06

Training epoch-85 batch-49
Running loss of epoch-85 batch-49 = 3.119930624961853e-06

Training epoch-85 batch-50
Running loss of epoch-85 batch-50 = 4.7674402594566345e-06

Training epoch-85 batch-51
Running loss of epoch-85 batch-51 = 1.4293473213911057e-06

Training epoch-85 batch-52
Running loss of epoch-85 batch-52 = 4.36161644756794e-06

Training epoch-85 batch-53
Running loss of epoch-85 batch-53 = 2.9348302632570267e-06

Training epoch-85 batch-54
Running loss of epoch-85 batch-54 = 3.6174897104501724e-06

Training epoch-85 batch-55
Running loss of epoch-85 batch-55 = 2.057058736681938e-06

Training epoch-85 batch-56
Running loss of epoch-85 batch-56 = 3.654276952147484e-06

Training epoch-85 batch-57
Running loss of epoch-85 batch-57 = 2.7641654014587402e-06

Training epoch-85 batch-58
Running loss of epoch-85 batch-58 = 1.764390617609024e-06

Training epoch-85 batch-59
Running loss of epoch-85 batch-59 = 2.741580829024315e-06

Training epoch-85 batch-60
Running loss of epoch-85 batch-60 = 1.4530960470438004e-06

Training epoch-85 batch-61
Running loss of epoch-85 batch-61 = 2.5918707251548767e-06

Training epoch-85 batch-62
Running loss of epoch-85 batch-62 = 2.7816276997327805e-06

Training epoch-85 batch-63
Running loss of epoch-85 batch-63 = 3.0295923352241516e-06

Training epoch-85 batch-64
Running loss of epoch-85 batch-64 = 1.646578311920166e-06

Training epoch-85 batch-65
Running loss of epoch-85 batch-65 = 7.995637133717537e-06

Training epoch-85 batch-66
Running loss of epoch-85 batch-66 = 5.5890996009111404e-06

Training epoch-85 batch-67
Running loss of epoch-85 batch-67 = 1.0973308235406876e-06

Training epoch-85 batch-68
Running loss of epoch-85 batch-68 = 5.063600838184357e-06

Training epoch-85 batch-69
Running loss of epoch-85 batch-69 = 2.6067718863487244e-06

Training epoch-85 batch-70
Running loss of epoch-85 batch-70 = 1.9173603504896164e-06

Training epoch-85 batch-71
Running loss of epoch-85 batch-71 = 1.4957040548324585e-06

Training epoch-85 batch-72
Running loss of epoch-85 batch-72 = 2.8654467314481735e-06

Training epoch-85 batch-73
Running loss of epoch-85 batch-73 = 7.862690836191177e-07

Training epoch-85 batch-74
Running loss of epoch-85 batch-74 = 3.5385601222515106e-06

Training epoch-85 batch-75
Running loss of epoch-85 batch-75 = 1.651933416724205e-06

Training epoch-85 batch-76
Running loss of epoch-85 batch-76 = 3.2887328416109085e-06

Training epoch-85 batch-77
Running loss of epoch-85 batch-77 = 2.0100269466638565e-06

Training epoch-85 batch-78
Running loss of epoch-85 batch-78 = 2.2328458726406097e-06

Training epoch-85 batch-79
Running loss of epoch-85 batch-79 = 3.6242417991161346e-06

Training epoch-85 batch-80
Running loss of epoch-85 batch-80 = 1.7669517546892166e-06

Training epoch-85 batch-81
Running loss of epoch-85 batch-81 = 3.332272171974182e-06

Training epoch-85 batch-82
Running loss of epoch-85 batch-82 = 2.317829057574272e-06

Training epoch-85 batch-83
Running loss of epoch-85 batch-83 = 3.5176053643226624e-06

Training epoch-85 batch-84
Running loss of epoch-85 batch-84 = 3.8035213947296143e-06

Training epoch-85 batch-85
Running loss of epoch-85 batch-85 = 3.092922270298004e-06

Training epoch-85 batch-86
Running loss of epoch-85 batch-86 = 3.073364496231079e-06

Training epoch-85 batch-87
Running loss of epoch-85 batch-87 = 1.750187948346138e-06

Training epoch-85 batch-88
Running loss of epoch-85 batch-88 = 1.4191027730703354e-06

Training epoch-85 batch-89
Running loss of epoch-85 batch-89 = 3.2871030271053314e-06

Training epoch-85 batch-90
Running loss of epoch-85 batch-90 = 2.485699951648712e-06

Training epoch-85 batch-91
Running loss of epoch-85 batch-91 = 5.608424544334412e-06

Training epoch-85 batch-92
Running loss of epoch-85 batch-92 = 1.6461126506328583e-06

Training epoch-85 batch-93
Running loss of epoch-85 batch-93 = 1.7960555851459503e-06

Training epoch-85 batch-94
Running loss of epoch-85 batch-94 = 1.3778917491436005e-06

Training epoch-85 batch-95
Running loss of epoch-85 batch-95 = 2.844724804162979e-06

Training epoch-85 batch-96
Running loss of epoch-85 batch-96 = 1.4882534742355347e-06

Training epoch-85 batch-97
Running loss of epoch-85 batch-97 = 2.8829090297222137e-06

Training epoch-85 batch-98
Running loss of epoch-85 batch-98 = 3.5108532756567e-06

Training epoch-85 batch-99
Running loss of epoch-85 batch-99 = 3.826804459095001e-06

Training epoch-85 batch-100
Running loss of epoch-85 batch-100 = 2.7872156351804733e-06

Training epoch-85 batch-101
Running loss of epoch-85 batch-101 = 1.0957010090351105e-06

Training epoch-85 batch-102
Running loss of epoch-85 batch-102 = 1.3238750398159027e-06

Training epoch-85 batch-103
Running loss of epoch-85 batch-103 = 2.5723129510879517e-06

Training epoch-85 batch-104
Running loss of epoch-85 batch-104 = 2.0600855350494385e-06

Training epoch-85 batch-105
Running loss of epoch-85 batch-105 = 4.410510882735252e-06

Training epoch-85 batch-106
Running loss of epoch-85 batch-106 = 3.1956005841493607e-06

Training epoch-85 batch-107
Running loss of epoch-85 batch-107 = 3.1795352697372437e-06

Training epoch-85 batch-108
Running loss of epoch-85 batch-108 = 1.1506490409374237e-06

Training epoch-85 batch-109
Running loss of epoch-85 batch-109 = 4.0174927562475204e-06

Training epoch-85 batch-110
Running loss of epoch-85 batch-110 = 4.338333383202553e-06

Training epoch-85 batch-111
Running loss of epoch-85 batch-111 = 1.1695083230733871e-06

Training epoch-85 batch-112
Running loss of epoch-85 batch-112 = 1.5711411833763123e-06

Training epoch-85 batch-113
Running loss of epoch-85 batch-113 = 4.945555701851845e-06

Training epoch-85 batch-114
Running loss of epoch-85 batch-114 = 1.1620577424764633e-06

Training epoch-85 batch-115
Running loss of epoch-85 batch-115 = 2.769753336906433e-06

Training epoch-85 batch-116
Running loss of epoch-85 batch-116 = 3.0249357223510742e-06

Training epoch-85 batch-117
Running loss of epoch-85 batch-117 = 3.195134922862053e-06

Training epoch-85 batch-118
Running loss of epoch-85 batch-118 = 3.552297130227089e-06

Training epoch-85 batch-119
Running loss of epoch-85 batch-119 = 4.307134076952934e-06

Training epoch-85 batch-120
Running loss of epoch-85 batch-120 = 1.921318471431732e-06

Training epoch-85 batch-121
Running loss of epoch-85 batch-121 = 2.0095612853765488e-06

Training epoch-85 batch-122
Running loss of epoch-85 batch-122 = 3.922497853636742e-06

Training epoch-85 batch-123
Running loss of epoch-85 batch-123 = 3.887573257088661e-06

Training epoch-85 batch-124
Running loss of epoch-85 batch-124 = 9.336508810520172e-07

Training epoch-85 batch-125
Running loss of epoch-85 batch-125 = 3.3481046557426453e-06

Training epoch-85 batch-126
Running loss of epoch-85 batch-126 = 3.3830292522907257e-06

Training epoch-85 batch-127
Running loss of epoch-85 batch-127 = 3.5171397030353546e-06

Training epoch-85 batch-128
Running loss of epoch-85 batch-128 = 2.454034984111786e-06

Training epoch-85 batch-129
Running loss of epoch-85 batch-129 = 1.920154318213463e-06

Training epoch-85 batch-130
Running loss of epoch-85 batch-130 = 1.7757993191480637e-06

Training epoch-85 batch-131
Running loss of epoch-85 batch-131 = 8.19261185824871e-06

Training epoch-85 batch-132
Running loss of epoch-85 batch-132 = 1.8049031496047974e-06

Training epoch-85 batch-133
Running loss of epoch-85 batch-133 = 2.6300549507141113e-06

Training epoch-85 batch-134
Running loss of epoch-85 batch-134 = 2.343440428376198e-06

Training epoch-85 batch-135
Running loss of epoch-85 batch-135 = 3.7173740565776825e-06

Training epoch-85 batch-136
Running loss of epoch-85 batch-136 = 3.6032870411872864e-06

Training epoch-85 batch-137
Running loss of epoch-85 batch-137 = 4.3406616896390915e-06

Training epoch-85 batch-138
Running loss of epoch-85 batch-138 = 5.795154720544815e-07

Training epoch-85 batch-139
Running loss of epoch-85 batch-139 = 3.200257197022438e-06

Training epoch-85 batch-140
Running loss of epoch-85 batch-140 = 1.891283318400383e-06

Training epoch-85 batch-141
Running loss of epoch-85 batch-141 = 2.9534567147493362e-06

Training epoch-85 batch-142
Running loss of epoch-85 batch-142 = 2.0712614059448242e-06

Training epoch-85 batch-143
Running loss of epoch-85 batch-143 = 2.2614840418100357e-06

Training epoch-85 batch-144
Running loss of epoch-85 batch-144 = 8.079223334789276e-07

Training epoch-85 batch-145
Running loss of epoch-85 batch-145 = 2.319691702723503e-06

Training epoch-85 batch-146
Running loss of epoch-85 batch-146 = 8.234987035393715e-06

Training epoch-85 batch-147
Running loss of epoch-85 batch-147 = 3.0975788831710815e-06

Training epoch-85 batch-148
Running loss of epoch-85 batch-148 = 2.1297018975019455e-06

Training epoch-85 batch-149
Running loss of epoch-85 batch-149 = 3.0694063752889633e-06

Training epoch-85 batch-150
Running loss of epoch-85 batch-150 = 2.246350049972534e-06

Training epoch-85 batch-151
Running loss of epoch-85 batch-151 = 1.3508833944797516e-06

Training epoch-85 batch-152
Running loss of epoch-85 batch-152 = 4.269648343324661e-06

Training epoch-85 batch-153
Running loss of epoch-85 batch-153 = 4.819827154278755e-06

Training epoch-85 batch-154
Running loss of epoch-85 batch-154 = 3.849156200885773e-06

Training epoch-85 batch-155
Running loss of epoch-85 batch-155 = 4.1262246668338776e-06

Training epoch-85 batch-156
Running loss of epoch-85 batch-156 = 2.301996573805809e-06

Training epoch-85 batch-157
Running loss of epoch-85 batch-157 = 1.5657395124435425e-05

Finished training epoch-85.



Average train loss at epoch-85 = 2.9935047030448915e-06

Started Evaluation

Average val loss at epoch-85 = 1.1811944178626785

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 97.05 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 54.26 %
Accuracy for class execute is: 42.17 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 83.10 %

Finished Evaluation



Started training epoch-86

