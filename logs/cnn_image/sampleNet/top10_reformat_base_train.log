
DataLoader:

Started trainloader.
trainset = #10000
Finished trainloader.
Started valloader.
valset = #4847
Finished valloader.

Configuration:

DB_NAME = java-top10, IMG_TYPE = reformat_base, TRANS_SIZE = 1024
MAX_EPOCH = 100, BATCH_SIZE = 64

device = cuda:0



Started training epoch-1


Training epoch-1 batch-1
Running loss of epoch-1 batch-1 = 0.03590795770287514

Training epoch-1 batch-2
Running loss of epoch-1 batch-2 = 0.03604823723435402

Training epoch-1 batch-3
Running loss of epoch-1 batch-3 = 0.035918693989515305

Training epoch-1 batch-4
Running loss of epoch-1 batch-4 = 0.03593435138463974

Training epoch-1 batch-5
Running loss of epoch-1 batch-5 = 0.03609459102153778

Training epoch-1 batch-6
Running loss of epoch-1 batch-6 = 0.035743165761232376

Training epoch-1 batch-7
Running loss of epoch-1 batch-7 = 0.03595554456114769

Training epoch-1 batch-8
Running loss of epoch-1 batch-8 = 0.036219965666532516

Training epoch-1 batch-9
Running loss of epoch-1 batch-9 = 0.035703904926776886

Training epoch-1 batch-10
Running loss of epoch-1 batch-10 = 0.035800300538539886

Training epoch-1 batch-11
Running loss of epoch-1 batch-11 = 0.036097366362810135

Training epoch-1 batch-12
Running loss of epoch-1 batch-12 = 0.03598248213529587

Training epoch-1 batch-13
Running loss of epoch-1 batch-13 = 0.03591183200478554

Training epoch-1 batch-14
Running loss of epoch-1 batch-14 = 0.03608507290482521

Training epoch-1 batch-15
Running loss of epoch-1 batch-15 = 0.03626842424273491

Training epoch-1 batch-16
Running loss of epoch-1 batch-16 = 0.03594919666647911

Training epoch-1 batch-17
Running loss of epoch-1 batch-17 = 0.03615766018629074

Training epoch-1 batch-18
Running loss of epoch-1 batch-18 = 0.03586937114596367

Training epoch-1 batch-19
Running loss of epoch-1 batch-19 = 0.03589703142642975

Training epoch-1 batch-20
Running loss of epoch-1 batch-20 = 0.03621096536517143

Training epoch-1 batch-21
Running loss of epoch-1 batch-21 = 0.035923413932323456

Training epoch-1 batch-22
Running loss of epoch-1 batch-22 = 0.03603814169764519

Training epoch-1 batch-23
Running loss of epoch-1 batch-23 = 0.0362473726272583

Training epoch-1 batch-24
Running loss of epoch-1 batch-24 = 0.03586600720882416

Training epoch-1 batch-25
Running loss of epoch-1 batch-25 = 0.036263786256313324

Training epoch-1 batch-26
Running loss of epoch-1 batch-26 = 0.03583038970828056

Training epoch-1 batch-27
Running loss of epoch-1 batch-27 = 0.03587063401937485

Training epoch-1 batch-28
Running loss of epoch-1 batch-28 = 0.035771530121564865

Training epoch-1 batch-29
Running loss of epoch-1 batch-29 = 0.03567781299352646

Training epoch-1 batch-30
Running loss of epoch-1 batch-30 = 0.0358949676156044

Training epoch-1 batch-31
Running loss of epoch-1 batch-31 = 0.03594503551721573

Training epoch-1 batch-32
Running loss of epoch-1 batch-32 = 0.03568468987941742

Training epoch-1 batch-33
Running loss of epoch-1 batch-33 = 0.03602856025099754

Training epoch-1 batch-34
Running loss of epoch-1 batch-34 = 0.036108750849962234

Training epoch-1 batch-35
Running loss of epoch-1 batch-35 = 0.03571140766143799

Training epoch-1 batch-36
Running loss of epoch-1 batch-36 = 0.035931020975112915

Training epoch-1 batch-37
Running loss of epoch-1 batch-37 = 0.03589744120836258

Training epoch-1 batch-38
Running loss of epoch-1 batch-38 = 0.03612819314002991

Training epoch-1 batch-39
Running loss of epoch-1 batch-39 = 0.03631560504436493

Training epoch-1 batch-40
Running loss of epoch-1 batch-40 = 0.03590280935168266

Training epoch-1 batch-41
Running loss of epoch-1 batch-41 = 0.03592055290937424

Training epoch-1 batch-42
Running loss of epoch-1 batch-42 = 0.036131568253040314

Training epoch-1 batch-43
Running loss of epoch-1 batch-43 = 0.03570571541786194

Training epoch-1 batch-44
Running loss of epoch-1 batch-44 = 0.03586811572313309

Training epoch-1 batch-45
Running loss of epoch-1 batch-45 = 0.03601257503032684

Training epoch-1 batch-46
Running loss of epoch-1 batch-46 = 0.03560316935181618

Training epoch-1 batch-47
Running loss of epoch-1 batch-47 = 0.03589647635817528

Training epoch-1 batch-48
Running loss of epoch-1 batch-48 = 0.03586191684007645

Training epoch-1 batch-49
Running loss of epoch-1 batch-49 = 0.035810284316539764

Training epoch-1 batch-50
Running loss of epoch-1 batch-50 = 0.035651177167892456

Training epoch-1 batch-51
Running loss of epoch-1 batch-51 = 0.03580038622021675

Training epoch-1 batch-52
Running loss of epoch-1 batch-52 = 0.03571959212422371

Training epoch-1 batch-53
Running loss of epoch-1 batch-53 = 0.03561084344983101

Training epoch-1 batch-54
Running loss of epoch-1 batch-54 = 0.03596770018339157

Training epoch-1 batch-55
Running loss of epoch-1 batch-55 = 0.03568370267748833

Training epoch-1 batch-56
Running loss of epoch-1 batch-56 = 0.0358085110783577

Training epoch-1 batch-57
Running loss of epoch-1 batch-57 = 0.03586714714765549

Training epoch-1 batch-58
Running loss of epoch-1 batch-58 = 0.03595485910773277

Training epoch-1 batch-59
Running loss of epoch-1 batch-59 = 0.03597201034426689

Training epoch-1 batch-60
Running loss of epoch-1 batch-60 = 0.035845134407281876

Training epoch-1 batch-61
Running loss of epoch-1 batch-61 = 0.035714078694581985

Training epoch-1 batch-62
Running loss of epoch-1 batch-62 = 0.03577939048409462

Training epoch-1 batch-63
Running loss of epoch-1 batch-63 = 0.0357784666121006

Training epoch-1 batch-64
Running loss of epoch-1 batch-64 = 0.03551835939288139

Training epoch-1 batch-65
Running loss of epoch-1 batch-65 = 0.03571406006813049

Training epoch-1 batch-66
Running loss of epoch-1 batch-66 = 0.0356297641992569

Training epoch-1 batch-67
Running loss of epoch-1 batch-67 = 0.03584212437272072

Training epoch-1 batch-68
Running loss of epoch-1 batch-68 = 0.03568728268146515

Training epoch-1 batch-69
Running loss of epoch-1 batch-69 = 0.035786375403404236

Training epoch-1 batch-70
Running loss of epoch-1 batch-70 = 0.03573857992887497

Training epoch-1 batch-71
Running loss of epoch-1 batch-71 = 0.035707663744688034

Training epoch-1 batch-72
Running loss of epoch-1 batch-72 = 0.035954609513282776

Training epoch-1 batch-73
Running loss of epoch-1 batch-73 = 0.03541509434580803

Training epoch-1 batch-74
Running loss of epoch-1 batch-74 = 0.035534653812646866

Training epoch-1 batch-75
Running loss of epoch-1 batch-75 = 0.03549814969301224

Training epoch-1 batch-76
Running loss of epoch-1 batch-76 = 0.03566712513566017

Training epoch-1 batch-77
Running loss of epoch-1 batch-77 = 0.03594279661774635

Training epoch-1 batch-78
Running loss of epoch-1 batch-78 = 0.03576367720961571

Training epoch-1 batch-79
Running loss of epoch-1 batch-79 = 0.03562621399760246

Training epoch-1 batch-80
Running loss of epoch-1 batch-80 = 0.0358874574303627

Training epoch-1 batch-81
Running loss of epoch-1 batch-81 = 0.035632967948913574

Training epoch-1 batch-82
Running loss of epoch-1 batch-82 = 0.035705603659152985

Training epoch-1 batch-83
Running loss of epoch-1 batch-83 = 0.03573404252529144

Training epoch-1 batch-84
Running loss of epoch-1 batch-84 = 0.03577036038041115

Training epoch-1 batch-85
Running loss of epoch-1 batch-85 = 0.035585951060056686

Training epoch-1 batch-86
Running loss of epoch-1 batch-86 = 0.03514387458562851

Training epoch-1 batch-87
Running loss of epoch-1 batch-87 = 0.03563912957906723

Training epoch-1 batch-88
Running loss of epoch-1 batch-88 = 0.03557378798723221

Training epoch-1 batch-89
Running loss of epoch-1 batch-89 = 0.03558037430047989

Training epoch-1 batch-90
Running loss of epoch-1 batch-90 = 0.03573746234178543

Training epoch-1 batch-91
Running loss of epoch-1 batch-91 = 0.03557781130075455

Training epoch-1 batch-92
Running loss of epoch-1 batch-92 = 0.03558962419629097

Training epoch-1 batch-93
Running loss of epoch-1 batch-93 = 0.035610657185316086

Training epoch-1 batch-94
Running loss of epoch-1 batch-94 = 0.035865120589733124

Training epoch-1 batch-95
Running loss of epoch-1 batch-95 = 0.035766277462244034

Training epoch-1 batch-96
Running loss of epoch-1 batch-96 = 0.03551192581653595

Training epoch-1 batch-97
Running loss of epoch-1 batch-97 = 0.03537352383136749

Training epoch-1 batch-98
Running loss of epoch-1 batch-98 = 0.03531718999147415

Training epoch-1 batch-99
Running loss of epoch-1 batch-99 = 0.035734571516513824

Training epoch-1 batch-100
Running loss of epoch-1 batch-100 = 0.03547632321715355

Training epoch-1 batch-101
Running loss of epoch-1 batch-101 = 0.03593733534216881

Training epoch-1 batch-102
Running loss of epoch-1 batch-102 = 0.03523613512516022

Training epoch-1 batch-103
Running loss of epoch-1 batch-103 = 0.035475827753543854

Training epoch-1 batch-104
Running loss of epoch-1 batch-104 = 0.03528374433517456

Training epoch-1 batch-105
Running loss of epoch-1 batch-105 = 0.03568703681230545

Training epoch-1 batch-106
Running loss of epoch-1 batch-106 = 0.03556433692574501

Training epoch-1 batch-107
Running loss of epoch-1 batch-107 = 0.035231947898864746

Training epoch-1 batch-108
Running loss of epoch-1 batch-108 = 0.03542887419462204

Training epoch-1 batch-109
Running loss of epoch-1 batch-109 = 0.03504837676882744

Training epoch-1 batch-110
Running loss of epoch-1 batch-110 = 0.03519217297434807

Training epoch-1 batch-111
Running loss of epoch-1 batch-111 = 0.03557002916932106

Training epoch-1 batch-112
Running loss of epoch-1 batch-112 = 0.03490382432937622

Training epoch-1 batch-113
Running loss of epoch-1 batch-113 = 0.03500945866107941

Training epoch-1 batch-114
Running loss of epoch-1 batch-114 = 0.035690948367118835

Training epoch-1 batch-115
Running loss of epoch-1 batch-115 = 0.03544367104768753

Training epoch-1 batch-116
Running loss of epoch-1 batch-116 = 0.035284750163555145

Training epoch-1 batch-117
Running loss of epoch-1 batch-117 = 0.03540327399969101

Training epoch-1 batch-118
Running loss of epoch-1 batch-118 = 0.03532125800848007

Training epoch-1 batch-119
Running loss of epoch-1 batch-119 = 0.03558525815606117

Training epoch-1 batch-120
Running loss of epoch-1 batch-120 = 0.0352756530046463

Training epoch-1 batch-121
Running loss of epoch-1 batch-121 = 0.0354650542140007

Training epoch-1 batch-122
Running loss of epoch-1 batch-122 = 0.035396188497543335

Training epoch-1 batch-123
Running loss of epoch-1 batch-123 = 0.03550887852907181

Training epoch-1 batch-124
Running loss of epoch-1 batch-124 = 0.03527245298027992

Training epoch-1 batch-125
Running loss of epoch-1 batch-125 = 0.03528108820319176

Training epoch-1 batch-126
Running loss of epoch-1 batch-126 = 0.0352119505405426

Training epoch-1 batch-127
Running loss of epoch-1 batch-127 = 0.03501608222723007

Training epoch-1 batch-128
Running loss of epoch-1 batch-128 = 0.03520706668496132

Training epoch-1 batch-129
Running loss of epoch-1 batch-129 = 0.03543194383382797

Training epoch-1 batch-130
Running loss of epoch-1 batch-130 = 0.0347401425242424

Training epoch-1 batch-131
Running loss of epoch-1 batch-131 = 0.034937016665935516

Training epoch-1 batch-132
Running loss of epoch-1 batch-132 = 0.035032615065574646

Training epoch-1 batch-133
Running loss of epoch-1 batch-133 = 0.03520100563764572

Training epoch-1 batch-134
Running loss of epoch-1 batch-134 = 0.03451281785964966

Training epoch-1 batch-135
Running loss of epoch-1 batch-135 = 0.034721311181783676

Training epoch-1 batch-136
Running loss of epoch-1 batch-136 = 0.03509458154439926

Training epoch-1 batch-137
Running loss of epoch-1 batch-137 = 0.035455018281936646

Training epoch-1 batch-138
Running loss of epoch-1 batch-138 = 0.03476749360561371

Training epoch-1 batch-139
Running loss of epoch-1 batch-139 = 0.03604965656995773

Training epoch-1 batch-140
Running loss of epoch-1 batch-140 = 0.035316940397024155

Training epoch-1 batch-141
Running loss of epoch-1 batch-141 = 0.035434506833553314

Training epoch-1 batch-142
Running loss of epoch-1 batch-142 = 0.034510161727666855

Training epoch-1 batch-143
Running loss of epoch-1 batch-143 = 0.03527761250734329

Training epoch-1 batch-144
Running loss of epoch-1 batch-144 = 0.0341956689953804

Training epoch-1 batch-145
Running loss of epoch-1 batch-145 = 0.03487488254904747

Training epoch-1 batch-146
Running loss of epoch-1 batch-146 = 0.035013630986213684

Training epoch-1 batch-147
Running loss of epoch-1 batch-147 = 0.03513481467962265

Training epoch-1 batch-148
Running loss of epoch-1 batch-148 = 0.03459465876221657

Training epoch-1 batch-149
Running loss of epoch-1 batch-149 = 0.034975674003362656

Training epoch-1 batch-150
Running loss of epoch-1 batch-150 = 0.034774377942085266

Training epoch-1 batch-151
Running loss of epoch-1 batch-151 = 0.034413158893585205

Training epoch-1 batch-152
Running loss of epoch-1 batch-152 = 0.03483859822154045

Training epoch-1 batch-153
Running loss of epoch-1 batch-153 = 0.03462813422083855

Training epoch-1 batch-154
Running loss of epoch-1 batch-154 = 0.035069093108177185

Training epoch-1 batch-155
Running loss of epoch-1 batch-155 = 0.034742191433906555

Training epoch-1 batch-156
Running loss of epoch-1 batch-156 = 0.03494605794548988

Training epoch-1 batch-157
Running loss of epoch-1 batch-157 = 0.1400308609008789

Finished training epoch-1.



Average train loss at epoch-1 = 0.03574541940689087

Started Evaluation

Average val loss at epoch-1 = 2.2183763949494613

Accuracy for classes:
Accuracy for class equals is: 25.91 %
Accuracy for class main is: 76.89 %
Accuracy for class setUp is: 40.00 %
Accuracy for class onCreate is: 18.23 %
Accuracy for class toString is: 7.85 %
Accuracy for class run is: 0.00 %
Accuracy for class hashCode is: 80.15 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 0.00 %
Accuracy for class get is: 26.41 %

Overall Accuracy = 28.49 %


Best Accuracy = 28.49 % at Epoch-1
Saving model after best epoch-1

Finished Evaluation



Started training epoch-2


Training epoch-2 batch-1
Running loss of epoch-2 batch-1 = 0.03486381098628044

Training epoch-2 batch-2
Running loss of epoch-2 batch-2 = 0.033932678401470184

Training epoch-2 batch-3
Running loss of epoch-2 batch-3 = 0.03424173220992088

Training epoch-2 batch-4
Running loss of epoch-2 batch-4 = 0.034909654408693314

Training epoch-2 batch-5
Running loss of epoch-2 batch-5 = 0.03416584059596062

Training epoch-2 batch-6
Running loss of epoch-2 batch-6 = 0.03406799957156181

Training epoch-2 batch-7
Running loss of epoch-2 batch-7 = 0.0343930684030056

Training epoch-2 batch-8
Running loss of epoch-2 batch-8 = 0.03462950140237808

Training epoch-2 batch-9
Running loss of epoch-2 batch-9 = 0.03438417986035347

Training epoch-2 batch-10
Running loss of epoch-2 batch-10 = 0.0335378497838974

Training epoch-2 batch-11
Running loss of epoch-2 batch-11 = 0.03447069600224495

Training epoch-2 batch-12
Running loss of epoch-2 batch-12 = 0.034798186272382736

Training epoch-2 batch-13
Running loss of epoch-2 batch-13 = 0.03372013941407204

Training epoch-2 batch-14
Running loss of epoch-2 batch-14 = 0.034212157130241394

Training epoch-2 batch-15
Running loss of epoch-2 batch-15 = 0.03470896556973457

Training epoch-2 batch-16
Running loss of epoch-2 batch-16 = 0.033707838505506516

Training epoch-2 batch-17
Running loss of epoch-2 batch-17 = 0.034997373819351196

Training epoch-2 batch-18
Running loss of epoch-2 batch-18 = 0.035234350711107254

Training epoch-2 batch-19
Running loss of epoch-2 batch-19 = 0.034503236413002014

Training epoch-2 batch-20
Running loss of epoch-2 batch-20 = 0.033711791038513184

Training epoch-2 batch-21
Running loss of epoch-2 batch-21 = 0.03470652550458908

Training epoch-2 batch-22
Running loss of epoch-2 batch-22 = 0.0336029976606369

Training epoch-2 batch-23
Running loss of epoch-2 batch-23 = 0.03346780687570572

Training epoch-2 batch-24
Running loss of epoch-2 batch-24 = 0.03432495892047882

Training epoch-2 batch-25
Running loss of epoch-2 batch-25 = 0.03282005712389946

Training epoch-2 batch-26
Running loss of epoch-2 batch-26 = 0.03449242562055588

Training epoch-2 batch-27
Running loss of epoch-2 batch-27 = 0.033108778297901154

Training epoch-2 batch-28
Running loss of epoch-2 batch-28 = 0.03414679318666458

Training epoch-2 batch-29
Running loss of epoch-2 batch-29 = 0.03361709415912628

Training epoch-2 batch-30
Running loss of epoch-2 batch-30 = 0.033677831292152405

Training epoch-2 batch-31
Running loss of epoch-2 batch-31 = 0.03482469916343689

Training epoch-2 batch-32
Running loss of epoch-2 batch-32 = 0.03388284891843796

Training epoch-2 batch-33
Running loss of epoch-2 batch-33 = 0.033771343529224396

Training epoch-2 batch-34
Running loss of epoch-2 batch-34 = 0.03214755654335022

Training epoch-2 batch-35
Running loss of epoch-2 batch-35 = 0.034028343856334686

Training epoch-2 batch-36
Running loss of epoch-2 batch-36 = 0.03376680612564087

Training epoch-2 batch-37
Running loss of epoch-2 batch-37 = 0.033368322998285294

Training epoch-2 batch-38
Running loss of epoch-2 batch-38 = 0.03291003033518791

Training epoch-2 batch-39
Running loss of epoch-2 batch-39 = 0.03323230519890785

Training epoch-2 batch-40
Running loss of epoch-2 batch-40 = 0.03294728696346283

Training epoch-2 batch-41
Running loss of epoch-2 batch-41 = 0.03338916227221489

Training epoch-2 batch-42
Running loss of epoch-2 batch-42 = 0.033392153680324554

Training epoch-2 batch-43
Running loss of epoch-2 batch-43 = 0.03428259119391441

Training epoch-2 batch-44
Running loss of epoch-2 batch-44 = 0.03320607170462608

Training epoch-2 batch-45
Running loss of epoch-2 batch-45 = 0.03371499106287956

Training epoch-2 batch-46
Running loss of epoch-2 batch-46 = 0.03292061388492584

Training epoch-2 batch-47
Running loss of epoch-2 batch-47 = 0.03258414939045906

Training epoch-2 batch-48
Running loss of epoch-2 batch-48 = 0.034619543701410294

Training epoch-2 batch-49
Running loss of epoch-2 batch-49 = 0.0323779471218586

Training epoch-2 batch-50
Running loss of epoch-2 batch-50 = 0.03220616653561592

Training epoch-2 batch-51
Running loss of epoch-2 batch-51 = 0.032943785190582275

Training epoch-2 batch-52
Running loss of epoch-2 batch-52 = 0.032205794006586075

Training epoch-2 batch-53
Running loss of epoch-2 batch-53 = 0.03217083960771561

Training epoch-2 batch-54
Running loss of epoch-2 batch-54 = 0.03326445072889328

Training epoch-2 batch-55
Running loss of epoch-2 batch-55 = 0.03182267025113106

Training epoch-2 batch-56
Running loss of epoch-2 batch-56 = 0.032832879573106766

Training epoch-2 batch-57
Running loss of epoch-2 batch-57 = 0.032651387155056

Training epoch-2 batch-58
Running loss of epoch-2 batch-58 = 0.032566819339990616

Training epoch-2 batch-59
Running loss of epoch-2 batch-59 = 0.03319842368364334

Training epoch-2 batch-60
Running loss of epoch-2 batch-60 = 0.032608143985271454

Training epoch-2 batch-61
Running loss of epoch-2 batch-61 = 0.031743742525577545

Training epoch-2 batch-62
Running loss of epoch-2 batch-62 = 0.03325379639863968

Training epoch-2 batch-63
Running loss of epoch-2 batch-63 = 0.033206742256879807

Training epoch-2 batch-64
Running loss of epoch-2 batch-64 = 0.03247164189815521

Training epoch-2 batch-65
Running loss of epoch-2 batch-65 = 0.032153695821762085

Training epoch-2 batch-66
Running loss of epoch-2 batch-66 = 0.03305048123002052

Training epoch-2 batch-67
Running loss of epoch-2 batch-67 = 0.03232967481017113

Training epoch-2 batch-68
Running loss of epoch-2 batch-68 = 0.032116226851940155

Training epoch-2 batch-69
Running loss of epoch-2 batch-69 = 0.03218410909175873

Training epoch-2 batch-70
Running loss of epoch-2 batch-70 = 0.03052385151386261

Training epoch-2 batch-71
Running loss of epoch-2 batch-71 = 0.03354892507195473

Training epoch-2 batch-72
Running loss of epoch-2 batch-72 = 0.03193456679582596

Training epoch-2 batch-73
Running loss of epoch-2 batch-73 = 0.032770220190286636

Training epoch-2 batch-74
Running loss of epoch-2 batch-74 = 0.03093576431274414

Training epoch-2 batch-75
Running loss of epoch-2 batch-75 = 0.03129581734538078

Training epoch-2 batch-76
Running loss of epoch-2 batch-76 = 0.03098161146044731

Training epoch-2 batch-77
Running loss of epoch-2 batch-77 = 0.033242177218198776

Training epoch-2 batch-78
Running loss of epoch-2 batch-78 = 0.03312918171286583

Training epoch-2 batch-79
Running loss of epoch-2 batch-79 = 0.03350082039833069

Training epoch-2 batch-80
Running loss of epoch-2 batch-80 = 0.03142240270972252

Training epoch-2 batch-81
Running loss of epoch-2 batch-81 = 0.03166365250945091

Training epoch-2 batch-82
Running loss of epoch-2 batch-82 = 0.032706860452890396

Training epoch-2 batch-83
Running loss of epoch-2 batch-83 = 0.03153322637081146

Training epoch-2 batch-84
Running loss of epoch-2 batch-84 = 0.030786758288741112

Training epoch-2 batch-85
Running loss of epoch-2 batch-85 = 0.032291729003190994

Training epoch-2 batch-86
Running loss of epoch-2 batch-86 = 0.03272532299160957

Training epoch-2 batch-87
Running loss of epoch-2 batch-87 = 0.03174937516450882

Training epoch-2 batch-88
Running loss of epoch-2 batch-88 = 0.03415875509381294

Training epoch-2 batch-89
Running loss of epoch-2 batch-89 = 0.03125712275505066

Training epoch-2 batch-90
Running loss of epoch-2 batch-90 = 0.031356990337371826

Training epoch-2 batch-91
Running loss of epoch-2 batch-91 = 0.032348599284887314

Training epoch-2 batch-92
Running loss of epoch-2 batch-92 = 0.02901291288435459

Training epoch-2 batch-93
Running loss of epoch-2 batch-93 = 0.03163678199052811

Training epoch-2 batch-94
Running loss of epoch-2 batch-94 = 0.0312433373183012

Training epoch-2 batch-95
Running loss of epoch-2 batch-95 = 0.030077924951910973

Training epoch-2 batch-96
Running loss of epoch-2 batch-96 = 0.029993584379553795

Training epoch-2 batch-97
Running loss of epoch-2 batch-97 = 0.03108779527246952

Training epoch-2 batch-98
Running loss of epoch-2 batch-98 = 0.030872078612446785

Training epoch-2 batch-99
Running loss of epoch-2 batch-99 = 0.031704120337963104

Training epoch-2 batch-100
Running loss of epoch-2 batch-100 = 0.031748831272125244

Training epoch-2 batch-101
Running loss of epoch-2 batch-101 = 0.03187284991145134

Training epoch-2 batch-102
Running loss of epoch-2 batch-102 = 0.03270891681313515

Training epoch-2 batch-103
Running loss of epoch-2 batch-103 = 0.02966797724366188

Training epoch-2 batch-104
Running loss of epoch-2 batch-104 = 0.032935842871665955

Training epoch-2 batch-105
Running loss of epoch-2 batch-105 = 0.030247438699007034

Training epoch-2 batch-106
Running loss of epoch-2 batch-106 = 0.033142734318971634

Training epoch-2 batch-107
Running loss of epoch-2 batch-107 = 0.030353005975484848

Training epoch-2 batch-108
Running loss of epoch-2 batch-108 = 0.029072750359773636

Training epoch-2 batch-109
Running loss of epoch-2 batch-109 = 0.031894031912088394

Training epoch-2 batch-110
Running loss of epoch-2 batch-110 = 0.030421705916523933

Training epoch-2 batch-111
Running loss of epoch-2 batch-111 = 0.030001726001501083

Training epoch-2 batch-112
Running loss of epoch-2 batch-112 = 0.02917254902422428

Training epoch-2 batch-113
Running loss of epoch-2 batch-113 = 0.03205743059515953

Training epoch-2 batch-114
Running loss of epoch-2 batch-114 = 0.02888101153075695

Training epoch-2 batch-115
Running loss of epoch-2 batch-115 = 0.03077586553990841

Training epoch-2 batch-116
Running loss of epoch-2 batch-116 = 0.030731575563549995

Training epoch-2 batch-117
Running loss of epoch-2 batch-117 = 0.030683016404509544

Training epoch-2 batch-118
Running loss of epoch-2 batch-118 = 0.03200393542647362

Training epoch-2 batch-119
Running loss of epoch-2 batch-119 = 0.03249305486679077

Training epoch-2 batch-120
Running loss of epoch-2 batch-120 = 0.029776936396956444

Training epoch-2 batch-121
Running loss of epoch-2 batch-121 = 0.029570288956165314

Training epoch-2 batch-122
Running loss of epoch-2 batch-122 = 0.02826077491044998

Training epoch-2 batch-123
Running loss of epoch-2 batch-123 = 0.03084123320877552

Training epoch-2 batch-124
Running loss of epoch-2 batch-124 = 0.03170527145266533

Training epoch-2 batch-125
Running loss of epoch-2 batch-125 = 0.02965407632291317

Training epoch-2 batch-126
Running loss of epoch-2 batch-126 = 0.03060293197631836

Training epoch-2 batch-127
Running loss of epoch-2 batch-127 = 0.026988297700881958

Training epoch-2 batch-128
Running loss of epoch-2 batch-128 = 0.029697826132178307

Training epoch-2 batch-129
Running loss of epoch-2 batch-129 = 0.02767897956073284

Training epoch-2 batch-130
Running loss of epoch-2 batch-130 = 0.03199321776628494

Training epoch-2 batch-131
Running loss of epoch-2 batch-131 = 0.03095742128789425

Training epoch-2 batch-132
Running loss of epoch-2 batch-132 = 0.02904707007110119

Training epoch-2 batch-133
Running loss of epoch-2 batch-133 = 0.029212873429059982

Training epoch-2 batch-134
Running loss of epoch-2 batch-134 = 0.028452686965465546

Training epoch-2 batch-135
Running loss of epoch-2 batch-135 = 0.032139476388692856

Training epoch-2 batch-136
Running loss of epoch-2 batch-136 = 0.031276922672986984

Training epoch-2 batch-137
Running loss of epoch-2 batch-137 = 0.029208241030573845

Training epoch-2 batch-138
Running loss of epoch-2 batch-138 = 0.028366614133119583

Training epoch-2 batch-139
Running loss of epoch-2 batch-139 = 0.02890332229435444

Training epoch-2 batch-140
Running loss of epoch-2 batch-140 = 0.032083846628665924

Training epoch-2 batch-141
Running loss of epoch-2 batch-141 = 0.028060777112841606

Training epoch-2 batch-142
Running loss of epoch-2 batch-142 = 0.029135022312402725

Training epoch-2 batch-143
Running loss of epoch-2 batch-143 = 0.03103291615843773

Training epoch-2 batch-144
Running loss of epoch-2 batch-144 = 0.028930814936757088

Training epoch-2 batch-145
Running loss of epoch-2 batch-145 = 0.028637874871492386

Training epoch-2 batch-146
Running loss of epoch-2 batch-146 = 0.03069988265633583

Training epoch-2 batch-147
Running loss of epoch-2 batch-147 = 0.03112669102847576

Training epoch-2 batch-148
Running loss of epoch-2 batch-148 = 0.028894569724798203

Training epoch-2 batch-149
Running loss of epoch-2 batch-149 = 0.03126426041126251

Training epoch-2 batch-150
Running loss of epoch-2 batch-150 = 0.032593291252851486

Training epoch-2 batch-151
Running loss of epoch-2 batch-151 = 0.02828211896121502

Training epoch-2 batch-152
Running loss of epoch-2 batch-152 = 0.029465576633810997

Training epoch-2 batch-153
Running loss of epoch-2 batch-153 = 0.0308575127273798

Training epoch-2 batch-154
Running loss of epoch-2 batch-154 = 0.029964527115225792

Training epoch-2 batch-155
Running loss of epoch-2 batch-155 = 0.03238450363278389

Training epoch-2 batch-156
Running loss of epoch-2 batch-156 = 0.030010130256414413

Training epoch-2 batch-157
Running loss of epoch-2 batch-157 = 0.11385761946439743

Finished training epoch-2.



Average train loss at epoch-2 = 0.03212855205535889

Started Evaluation

Average val loss at epoch-2 = 1.886295326446232

Accuracy for classes:
Accuracy for class equals is: 74.59 %
Accuracy for class main is: 16.72 %
Accuracy for class setUp is: 36.72 %
Accuracy for class onCreate is: 25.59 %
Accuracy for class toString is: 30.03 %
Accuracy for class run is: 8.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 6.73 %
Accuracy for class execute is: 7.23 %
Accuracy for class get is: 37.18 %

Overall Accuracy = 32.06 %


Best Accuracy = 32.06 % at Epoch-2
Saving model after best epoch-2

Finished Evaluation



Started training epoch-3


Training epoch-3 batch-1
Running loss of epoch-3 batch-1 = 0.02911493182182312

Training epoch-3 batch-2
Running loss of epoch-3 batch-2 = 0.026903415098786354

Training epoch-3 batch-3
Running loss of epoch-3 batch-3 = 0.03085867315530777

Training epoch-3 batch-4
Running loss of epoch-3 batch-4 = 0.028197383508086205

Training epoch-3 batch-5
Running loss of epoch-3 batch-5 = 0.026430612429976463

Training epoch-3 batch-6
Running loss of epoch-3 batch-6 = 0.029518408700823784

Training epoch-3 batch-7
Running loss of epoch-3 batch-7 = 0.027424441650509834

Training epoch-3 batch-8
Running loss of epoch-3 batch-8 = 0.027619732543826103

Training epoch-3 batch-9
Running loss of epoch-3 batch-9 = 0.03093612939119339

Training epoch-3 batch-10
Running loss of epoch-3 batch-10 = 0.027044856920838356

Training epoch-3 batch-11
Running loss of epoch-3 batch-11 = 0.029243839904665947

Training epoch-3 batch-12
Running loss of epoch-3 batch-12 = 0.028619449585676193

Training epoch-3 batch-13
Running loss of epoch-3 batch-13 = 0.027608666568994522

Training epoch-3 batch-14
Running loss of epoch-3 batch-14 = 0.028682570904493332

Training epoch-3 batch-15
Running loss of epoch-3 batch-15 = 0.027904625982046127

Training epoch-3 batch-16
Running loss of epoch-3 batch-16 = 0.02791864238679409

Training epoch-3 batch-17
Running loss of epoch-3 batch-17 = 0.02969251573085785

Training epoch-3 batch-18
Running loss of epoch-3 batch-18 = 0.02641378715634346

Training epoch-3 batch-19
Running loss of epoch-3 batch-19 = 0.026329748332500458

Training epoch-3 batch-20
Running loss of epoch-3 batch-20 = 0.026612941175699234

Training epoch-3 batch-21
Running loss of epoch-3 batch-21 = 0.02902575582265854

Training epoch-3 batch-22
Running loss of epoch-3 batch-22 = 0.028324928134679794

Training epoch-3 batch-23
Running loss of epoch-3 batch-23 = 0.027336139231920242

Training epoch-3 batch-24
Running loss of epoch-3 batch-24 = 0.027243930846452713

Training epoch-3 batch-25
Running loss of epoch-3 batch-25 = 0.0278207678347826

Training epoch-3 batch-26
Running loss of epoch-3 batch-26 = 0.02583051472902298

Training epoch-3 batch-27
Running loss of epoch-3 batch-27 = 0.028067171573638916

Training epoch-3 batch-28
Running loss of epoch-3 batch-28 = 0.03048793599009514

Training epoch-3 batch-29
Running loss of epoch-3 batch-29 = 0.028668144717812538

Training epoch-3 batch-30
Running loss of epoch-3 batch-30 = 0.02914932370185852

Training epoch-3 batch-31
Running loss of epoch-3 batch-31 = 0.02722282148897648

Training epoch-3 batch-32
Running loss of epoch-3 batch-32 = 0.028874874114990234

Training epoch-3 batch-33
Running loss of epoch-3 batch-33 = 0.029072610661387444

Training epoch-3 batch-34
Running loss of epoch-3 batch-34 = 0.026896892115473747

Training epoch-3 batch-35
Running loss of epoch-3 batch-35 = 0.029229212552309036

Training epoch-3 batch-36
Running loss of epoch-3 batch-36 = 0.029517509043216705

Training epoch-3 batch-37
Running loss of epoch-3 batch-37 = 0.02770797349512577

Training epoch-3 batch-38
Running loss of epoch-3 batch-38 = 0.026148971170186996

Training epoch-3 batch-39
Running loss of epoch-3 batch-39 = 0.027607670053839684

Training epoch-3 batch-40
Running loss of epoch-3 batch-40 = 0.02660224586725235

Training epoch-3 batch-41
Running loss of epoch-3 batch-41 = 0.028465449810028076

Training epoch-3 batch-42
Running loss of epoch-3 batch-42 = 0.028412850573658943

Training epoch-3 batch-43
Running loss of epoch-3 batch-43 = 0.027914827689528465

Training epoch-3 batch-44
Running loss of epoch-3 batch-44 = 0.02859549969434738

Training epoch-3 batch-45
Running loss of epoch-3 batch-45 = 0.027013080194592476

Training epoch-3 batch-46
Running loss of epoch-3 batch-46 = 0.028567567467689514

Training epoch-3 batch-47
Running loss of epoch-3 batch-47 = 0.02785828337073326

Training epoch-3 batch-48
Running loss of epoch-3 batch-48 = 0.027621259912848473

Training epoch-3 batch-49
Running loss of epoch-3 batch-49 = 0.030230924487113953

Training epoch-3 batch-50
Running loss of epoch-3 batch-50 = 0.02926292084157467

Training epoch-3 batch-51
Running loss of epoch-3 batch-51 = 0.02867225743830204

Training epoch-3 batch-52
Running loss of epoch-3 batch-52 = 0.029984964057803154

Training epoch-3 batch-53
Running loss of epoch-3 batch-53 = 0.025102337822318077

Training epoch-3 batch-54
Running loss of epoch-3 batch-54 = 0.025358151644468307

Training epoch-3 batch-55
Running loss of epoch-3 batch-55 = 0.02709200046956539

Training epoch-3 batch-56
Running loss of epoch-3 batch-56 = 0.02841399796307087

Training epoch-3 batch-57
Running loss of epoch-3 batch-57 = 0.02680450677871704

Training epoch-3 batch-58
Running loss of epoch-3 batch-58 = 0.02788699045777321

Training epoch-3 batch-59
Running loss of epoch-3 batch-59 = 0.02729569934308529

Training epoch-3 batch-60
Running loss of epoch-3 batch-60 = 0.027206802740693092

Training epoch-3 batch-61
Running loss of epoch-3 batch-61 = 0.0278133787214756

Training epoch-3 batch-62
Running loss of epoch-3 batch-62 = 0.0253660436719656

Training epoch-3 batch-63
Running loss of epoch-3 batch-63 = 0.024340137839317322

Training epoch-3 batch-64
Running loss of epoch-3 batch-64 = 0.028124282136559486

Training epoch-3 batch-65
Running loss of epoch-3 batch-65 = 0.02777511067688465

Training epoch-3 batch-66
Running loss of epoch-3 batch-66 = 0.028173990547657013

Training epoch-3 batch-67
Running loss of epoch-3 batch-67 = 0.027874480932950974

Training epoch-3 batch-68
Running loss of epoch-3 batch-68 = 0.026704099029302597

Training epoch-3 batch-69
Running loss of epoch-3 batch-69 = 0.026160910725593567

Training epoch-3 batch-70
Running loss of epoch-3 batch-70 = 0.02615731954574585

Training epoch-3 batch-71
Running loss of epoch-3 batch-71 = 0.025933893397450447

Training epoch-3 batch-72
Running loss of epoch-3 batch-72 = 0.024640679359436035

Training epoch-3 batch-73
Running loss of epoch-3 batch-73 = 0.02919401042163372

Training epoch-3 batch-74
Running loss of epoch-3 batch-74 = 0.028888793662190437

Training epoch-3 batch-75
Running loss of epoch-3 batch-75 = 0.028747882694005966

Training epoch-3 batch-76
Running loss of epoch-3 batch-76 = 0.025077112019062042

Training epoch-3 batch-77
Running loss of epoch-3 batch-77 = 0.024903254583477974

Training epoch-3 batch-78
Running loss of epoch-3 batch-78 = 0.026136208325624466

Training epoch-3 batch-79
Running loss of epoch-3 batch-79 = 0.026867765933275223

Training epoch-3 batch-80
Running loss of epoch-3 batch-80 = 0.025926096364855766

Training epoch-3 batch-81
Running loss of epoch-3 batch-81 = 0.02530525252223015

Training epoch-3 batch-82
Running loss of epoch-3 batch-82 = 0.02491411566734314

Training epoch-3 batch-83
Running loss of epoch-3 batch-83 = 0.027008960023522377

Training epoch-3 batch-84
Running loss of epoch-3 batch-84 = 0.02653566747903824

Training epoch-3 batch-85
Running loss of epoch-3 batch-85 = 0.028173577040433884

Training epoch-3 batch-86
Running loss of epoch-3 batch-86 = 0.02947021648287773

Training epoch-3 batch-87
Running loss of epoch-3 batch-87 = 0.027213936671614647

Training epoch-3 batch-88
Running loss of epoch-3 batch-88 = 0.029419973492622375

Training epoch-3 batch-89
Running loss of epoch-3 batch-89 = 0.026411714032292366

Training epoch-3 batch-90
Running loss of epoch-3 batch-90 = 0.026023678481578827

Training epoch-3 batch-91
Running loss of epoch-3 batch-91 = 0.02782233990728855

Training epoch-3 batch-92
Running loss of epoch-3 batch-92 = 0.028577569872140884

Training epoch-3 batch-93
Running loss of epoch-3 batch-93 = 0.027856817469000816

Training epoch-3 batch-94
Running loss of epoch-3 batch-94 = 0.028021831065416336

Training epoch-3 batch-95
Running loss of epoch-3 batch-95 = 0.023274024948477745

Training epoch-3 batch-96
Running loss of epoch-3 batch-96 = 0.027525413781404495

Training epoch-3 batch-97
Running loss of epoch-3 batch-97 = 0.02439206652343273

Training epoch-3 batch-98
Running loss of epoch-3 batch-98 = 0.028787298128008842

Training epoch-3 batch-99
Running loss of epoch-3 batch-99 = 0.02820710651576519

Training epoch-3 batch-100
Running loss of epoch-3 batch-100 = 0.02600683830678463

Training epoch-3 batch-101
Running loss of epoch-3 batch-101 = 0.025080738589167595

Training epoch-3 batch-102
Running loss of epoch-3 batch-102 = 0.02676726132631302

Training epoch-3 batch-103
Running loss of epoch-3 batch-103 = 0.024730516597628593

Training epoch-3 batch-104
Running loss of epoch-3 batch-104 = 0.023928202688694

Training epoch-3 batch-105
Running loss of epoch-3 batch-105 = 0.026581929996609688

Training epoch-3 batch-106
Running loss of epoch-3 batch-106 = 0.02663305215537548

Training epoch-3 batch-107
Running loss of epoch-3 batch-107 = 0.026579108089208603

Training epoch-3 batch-108
Running loss of epoch-3 batch-108 = 0.024529090151190758

Training epoch-3 batch-109
Running loss of epoch-3 batch-109 = 0.025317303836345673

Training epoch-3 batch-110
Running loss of epoch-3 batch-110 = 0.02649412676692009

Training epoch-3 batch-111
Running loss of epoch-3 batch-111 = 0.025426743552088737

Training epoch-3 batch-112
Running loss of epoch-3 batch-112 = 0.024121252819895744

Training epoch-3 batch-113
Running loss of epoch-3 batch-113 = 0.024303195998072624

Training epoch-3 batch-114
Running loss of epoch-3 batch-114 = 0.02635015733540058

Training epoch-3 batch-115
Running loss of epoch-3 batch-115 = 0.028805401176214218

Training epoch-3 batch-116
Running loss of epoch-3 batch-116 = 0.02484702505171299

Training epoch-3 batch-117
Running loss of epoch-3 batch-117 = 0.027789752930402756

Training epoch-3 batch-118
Running loss of epoch-3 batch-118 = 0.028486374765634537

Training epoch-3 batch-119
Running loss of epoch-3 batch-119 = 0.028052233159542084

Training epoch-3 batch-120
Running loss of epoch-3 batch-120 = 0.02948630228638649

Training epoch-3 batch-121
Running loss of epoch-3 batch-121 = 0.026521150022745132

Training epoch-3 batch-122
Running loss of epoch-3 batch-122 = 0.027533892542123795

Training epoch-3 batch-123
Running loss of epoch-3 batch-123 = 0.027175212278962135

Training epoch-3 batch-124
Running loss of epoch-3 batch-124 = 0.024443402886390686

Training epoch-3 batch-125
Running loss of epoch-3 batch-125 = 0.024888329207897186

Training epoch-3 batch-126
Running loss of epoch-3 batch-126 = 0.025222932919859886

Training epoch-3 batch-127
Running loss of epoch-3 batch-127 = 0.025083715096116066

Training epoch-3 batch-128
Running loss of epoch-3 batch-128 = 0.022478876635432243

Training epoch-3 batch-129
Running loss of epoch-3 batch-129 = 0.02934158965945244

Training epoch-3 batch-130
Running loss of epoch-3 batch-130 = 0.027918562293052673

Training epoch-3 batch-131
Running loss of epoch-3 batch-131 = 0.023521317169070244

Training epoch-3 batch-132
Running loss of epoch-3 batch-132 = 0.024261701852083206

Training epoch-3 batch-133
Running loss of epoch-3 batch-133 = 0.027770008891820908

Training epoch-3 batch-134
Running loss of epoch-3 batch-134 = 0.02894929237663746

Training epoch-3 batch-135
Running loss of epoch-3 batch-135 = 0.027430471032857895

Training epoch-3 batch-136
Running loss of epoch-3 batch-136 = 0.025641191750764847

Training epoch-3 batch-137
Running loss of epoch-3 batch-137 = 0.02585563249886036

Training epoch-3 batch-138
Running loss of epoch-3 batch-138 = 0.02674569934606552

Training epoch-3 batch-139
Running loss of epoch-3 batch-139 = 0.027161508798599243

Training epoch-3 batch-140
Running loss of epoch-3 batch-140 = 0.026461446657776833

Training epoch-3 batch-141
Running loss of epoch-3 batch-141 = 0.025409763678908348

Training epoch-3 batch-142
Running loss of epoch-3 batch-142 = 0.023068202659487724

Training epoch-3 batch-143
Running loss of epoch-3 batch-143 = 0.026783199980854988

Training epoch-3 batch-144
Running loss of epoch-3 batch-144 = 0.02571258507668972

Training epoch-3 batch-145
Running loss of epoch-3 batch-145 = 0.025098787620663643

Training epoch-3 batch-146
Running loss of epoch-3 batch-146 = 0.025158138945698738

Training epoch-3 batch-147
Running loss of epoch-3 batch-147 = 0.023171929642558098

Training epoch-3 batch-148
Running loss of epoch-3 batch-148 = 0.02478083223104477

Training epoch-3 batch-149
Running loss of epoch-3 batch-149 = 0.024110566824674606

Training epoch-3 batch-150
Running loss of epoch-3 batch-150 = 0.025464314967393875

Training epoch-3 batch-151
Running loss of epoch-3 batch-151 = 0.03060251846909523

Training epoch-3 batch-152
Running loss of epoch-3 batch-152 = 0.025937087833881378

Training epoch-3 batch-153
Running loss of epoch-3 batch-153 = 0.024523891508579254

Training epoch-3 batch-154
Running loss of epoch-3 batch-154 = 0.02829410508275032

Training epoch-3 batch-155
Running loss of epoch-3 batch-155 = 0.024746589362621307

Training epoch-3 batch-156
Running loss of epoch-3 batch-156 = 0.027152329683303833

Training epoch-3 batch-157
Running loss of epoch-3 batch-157 = 0.07668255269527435

Finished training epoch-3.



Average train loss at epoch-3 = 0.027106087505817413

Started Evaluation

Average val loss at epoch-3 = 1.6364732870930119

Accuracy for classes:
Accuracy for class equals is: 59.57 %
Accuracy for class main is: 44.10 %
Accuracy for class setUp is: 55.90 %
Accuracy for class onCreate is: 45.74 %
Accuracy for class toString is: 39.25 %
Accuracy for class run is: 26.94 %
Accuracy for class hashCode is: 82.77 %
Accuracy for class init is: 17.04 %
Accuracy for class execute is: 31.33 %
Accuracy for class get is: 15.90 %

Overall Accuracy = 42.71 %


Best Accuracy = 42.71 % at Epoch-3
Saving model after best epoch-3

Finished Evaluation



Started training epoch-4


Training epoch-4 batch-1
Running loss of epoch-4 batch-1 = 0.02172582969069481

Training epoch-4 batch-2
Running loss of epoch-4 batch-2 = 0.024145597591996193

Training epoch-4 batch-3
Running loss of epoch-4 batch-3 = 0.023020686581730843

Training epoch-4 batch-4
Running loss of epoch-4 batch-4 = 0.022296248003840446

Training epoch-4 batch-5
Running loss of epoch-4 batch-5 = 0.02293083816766739

Training epoch-4 batch-6
Running loss of epoch-4 batch-6 = 0.022290991619229317

Training epoch-4 batch-7
Running loss of epoch-4 batch-7 = 0.0249667726457119

Training epoch-4 batch-8
Running loss of epoch-4 batch-8 = 0.025448612868785858

Training epoch-4 batch-9
Running loss of epoch-4 batch-9 = 0.022963518276810646

Training epoch-4 batch-10
Running loss of epoch-4 batch-10 = 0.023783180862665176

Training epoch-4 batch-11
Running loss of epoch-4 batch-11 = 0.02282675914466381

Training epoch-4 batch-12
Running loss of epoch-4 batch-12 = 0.02291427180171013

Training epoch-4 batch-13
Running loss of epoch-4 batch-13 = 0.024510109797120094

Training epoch-4 batch-14
Running loss of epoch-4 batch-14 = 0.02105262130498886

Training epoch-4 batch-15
Running loss of epoch-4 batch-15 = 0.019386740401387215

Training epoch-4 batch-16
Running loss of epoch-4 batch-16 = 0.02503236196935177

Training epoch-4 batch-17
Running loss of epoch-4 batch-17 = 0.021219536662101746

Training epoch-4 batch-18
Running loss of epoch-4 batch-18 = 0.021199874579906464

Training epoch-4 batch-19
Running loss of epoch-4 batch-19 = 0.02176978625357151

Training epoch-4 batch-20
Running loss of epoch-4 batch-20 = 0.017736367881298065

Training epoch-4 batch-21
Running loss of epoch-4 batch-21 = 0.02692268043756485

Training epoch-4 batch-22
Running loss of epoch-4 batch-22 = 0.025314750149846077

Training epoch-4 batch-23
Running loss of epoch-4 batch-23 = 0.021767714992165565

Training epoch-4 batch-24
Running loss of epoch-4 batch-24 = 0.022621383890509605

Training epoch-4 batch-25
Running loss of epoch-4 batch-25 = 0.023789964616298676

Training epoch-4 batch-26
Running loss of epoch-4 batch-26 = 0.025112025439739227

Training epoch-4 batch-27
Running loss of epoch-4 batch-27 = 0.020848171785473824

Training epoch-4 batch-28
Running loss of epoch-4 batch-28 = 0.023488610982894897

Training epoch-4 batch-29
Running loss of epoch-4 batch-29 = 0.022228840738534927

Training epoch-4 batch-30
Running loss of epoch-4 batch-30 = 0.022903520613908768

Training epoch-4 batch-31
Running loss of epoch-4 batch-31 = 0.022098200395703316

Training epoch-4 batch-32
Running loss of epoch-4 batch-32 = 0.019881363958120346

Training epoch-4 batch-33
Running loss of epoch-4 batch-33 = 0.0210099034011364

Training epoch-4 batch-34
Running loss of epoch-4 batch-34 = 0.023308496922254562

Training epoch-4 batch-35
Running loss of epoch-4 batch-35 = 0.02228659950196743

Training epoch-4 batch-36
Running loss of epoch-4 batch-36 = 0.021530091762542725

Training epoch-4 batch-37
Running loss of epoch-4 batch-37 = 0.021268902346491814

Training epoch-4 batch-38
Running loss of epoch-4 batch-38 = 0.020866375416517258

Training epoch-4 batch-39
Running loss of epoch-4 batch-39 = 0.0241676177829504

Training epoch-4 batch-40
Running loss of epoch-4 batch-40 = 0.02299707569181919

Training epoch-4 batch-41
Running loss of epoch-4 batch-41 = 0.022450104355812073

Training epoch-4 batch-42
Running loss of epoch-4 batch-42 = 0.026344425976276398

Training epoch-4 batch-43
Running loss of epoch-4 batch-43 = 0.022023718804121017

Training epoch-4 batch-44
Running loss of epoch-4 batch-44 = 0.023055274039506912

Training epoch-4 batch-45
Running loss of epoch-4 batch-45 = 0.02387712337076664

Training epoch-4 batch-46
Running loss of epoch-4 batch-46 = 0.025922222062945366

Training epoch-4 batch-47
Running loss of epoch-4 batch-47 = 0.023712649941444397

Training epoch-4 batch-48
Running loss of epoch-4 batch-48 = 0.018659483641386032

Training epoch-4 batch-49
Running loss of epoch-4 batch-49 = 0.022604605183005333

Training epoch-4 batch-50
Running loss of epoch-4 batch-50 = 0.022424383088946342

Training epoch-4 batch-51
Running loss of epoch-4 batch-51 = 0.023320607841014862

Training epoch-4 batch-52
Running loss of epoch-4 batch-52 = 0.023928213864564896

Training epoch-4 batch-53
Running loss of epoch-4 batch-53 = 0.020806781947612762

Training epoch-4 batch-54
Running loss of epoch-4 batch-54 = 0.021140357479453087

Training epoch-4 batch-55
Running loss of epoch-4 batch-55 = 0.024324772879481316

Training epoch-4 batch-56
Running loss of epoch-4 batch-56 = 0.02175055257976055

Training epoch-4 batch-57
Running loss of epoch-4 batch-57 = 0.021135466173291206

Training epoch-4 batch-58
Running loss of epoch-4 batch-58 = 0.020421942695975304

Training epoch-4 batch-59
Running loss of epoch-4 batch-59 = 0.02133004367351532

Training epoch-4 batch-60
Running loss of epoch-4 batch-60 = 0.02284415438771248

Training epoch-4 batch-61
Running loss of epoch-4 batch-61 = 0.023792585358023643

Training epoch-4 batch-62
Running loss of epoch-4 batch-62 = 0.025049498304724693

Training epoch-4 batch-63
Running loss of epoch-4 batch-63 = 0.02344640903174877

Training epoch-4 batch-64
Running loss of epoch-4 batch-64 = 0.021438663825392723

Training epoch-4 batch-65
Running loss of epoch-4 batch-65 = 0.02354271523654461

Training epoch-4 batch-66
Running loss of epoch-4 batch-66 = 0.01874546892940998

Training epoch-4 batch-67
Running loss of epoch-4 batch-67 = 0.02265850454568863

Training epoch-4 batch-68
Running loss of epoch-4 batch-68 = 0.02208203822374344

Training epoch-4 batch-69
Running loss of epoch-4 batch-69 = 0.021866146475076675

Training epoch-4 batch-70
Running loss of epoch-4 batch-70 = 0.02512325346469879

Training epoch-4 batch-71
Running loss of epoch-4 batch-71 = 0.021744798868894577

Training epoch-4 batch-72
Running loss of epoch-4 batch-72 = 0.02070699818432331

Training epoch-4 batch-73
Running loss of epoch-4 batch-73 = 0.023444294929504395

Training epoch-4 batch-74
Running loss of epoch-4 batch-74 = 0.019045041874051094

Training epoch-4 batch-75
Running loss of epoch-4 batch-75 = 0.021076904609799385

Training epoch-4 batch-76
Running loss of epoch-4 batch-76 = 0.02236286737024784

Training epoch-4 batch-77
Running loss of epoch-4 batch-77 = 0.023226294666528702

Training epoch-4 batch-78
Running loss of epoch-4 batch-78 = 0.02297845482826233

Training epoch-4 batch-79
Running loss of epoch-4 batch-79 = 0.02184404619038105

Training epoch-4 batch-80
Running loss of epoch-4 batch-80 = 0.022966155782341957

Training epoch-4 batch-81
Running loss of epoch-4 batch-81 = 0.020899346098303795

Training epoch-4 batch-82
Running loss of epoch-4 batch-82 = 0.02137574926018715

Training epoch-4 batch-83
Running loss of epoch-4 batch-83 = 0.02495465986430645

Training epoch-4 batch-84
Running loss of epoch-4 batch-84 = 0.020672092214226723

Training epoch-4 batch-85
Running loss of epoch-4 batch-85 = 0.022164853289723396

Training epoch-4 batch-86
Running loss of epoch-4 batch-86 = 0.023711038753390312

Training epoch-4 batch-87
Running loss of epoch-4 batch-87 = 0.021065808832645416

Training epoch-4 batch-88
Running loss of epoch-4 batch-88 = 0.01950179971754551

Training epoch-4 batch-89
Running loss of epoch-4 batch-89 = 0.020964495837688446

Training epoch-4 batch-90
Running loss of epoch-4 batch-90 = 0.019789734855294228

Training epoch-4 batch-91
Running loss of epoch-4 batch-91 = 0.019856048747897148

Training epoch-4 batch-92
Running loss of epoch-4 batch-92 = 0.021043673157691956

Training epoch-4 batch-93
Running loss of epoch-4 batch-93 = 0.02376825548708439

Training epoch-4 batch-94
Running loss of epoch-4 batch-94 = 0.02006673626601696

Training epoch-4 batch-95
Running loss of epoch-4 batch-95 = 0.020845485851168633

Training epoch-4 batch-96
Running loss of epoch-4 batch-96 = 0.02261730097234249

Training epoch-4 batch-97
Running loss of epoch-4 batch-97 = 0.020860880613327026

Training epoch-4 batch-98
Running loss of epoch-4 batch-98 = 0.023450858891010284

Training epoch-4 batch-99
Running loss of epoch-4 batch-99 = 0.02074284292757511

Training epoch-4 batch-100
Running loss of epoch-4 batch-100 = 0.02008087933063507

Training epoch-4 batch-101
Running loss of epoch-4 batch-101 = 0.019207529723644257

Training epoch-4 batch-102
Running loss of epoch-4 batch-102 = 0.023961065337061882

Training epoch-4 batch-103
Running loss of epoch-4 batch-103 = 0.022109514102339745

Training epoch-4 batch-104
Running loss of epoch-4 batch-104 = 0.024570118635892868

Training epoch-4 batch-105
Running loss of epoch-4 batch-105 = 0.02431359887123108

Training epoch-4 batch-106
Running loss of epoch-4 batch-106 = 0.022868148982524872

Training epoch-4 batch-107
Running loss of epoch-4 batch-107 = 0.020714066922664642

Training epoch-4 batch-108
Running loss of epoch-4 batch-108 = 0.02319074608385563

Training epoch-4 batch-109
Running loss of epoch-4 batch-109 = 0.017759215086698532

Training epoch-4 batch-110
Running loss of epoch-4 batch-110 = 0.021497247740626335

Training epoch-4 batch-111
Running loss of epoch-4 batch-111 = 0.018550066277384758

Training epoch-4 batch-112
Running loss of epoch-4 batch-112 = 0.021377060562372208

Training epoch-4 batch-113
Running loss of epoch-4 batch-113 = 0.01694081723690033

Training epoch-4 batch-114
Running loss of epoch-4 batch-114 = 0.018742993474006653

Training epoch-4 batch-115
Running loss of epoch-4 batch-115 = 0.023399338126182556

Training epoch-4 batch-116
Running loss of epoch-4 batch-116 = 0.022705577313899994

Training epoch-4 batch-117
Running loss of epoch-4 batch-117 = 0.02370304800570011

Training epoch-4 batch-118
Running loss of epoch-4 batch-118 = 0.026594219729304314

Training epoch-4 batch-119
Running loss of epoch-4 batch-119 = 0.025605322793126106

Training epoch-4 batch-120
Running loss of epoch-4 batch-120 = 0.02021343633532524

Training epoch-4 batch-121
Running loss of epoch-4 batch-121 = 0.020134015008807182

Training epoch-4 batch-122
Running loss of epoch-4 batch-122 = 0.022865261882543564

Training epoch-4 batch-123
Running loss of epoch-4 batch-123 = 0.02132503315806389

Training epoch-4 batch-124
Running loss of epoch-4 batch-124 = 0.021371064707636833

Training epoch-4 batch-125
Running loss of epoch-4 batch-125 = 0.022748388350009918

Training epoch-4 batch-126
Running loss of epoch-4 batch-126 = 0.022617070004343987

Training epoch-4 batch-127
Running loss of epoch-4 batch-127 = 0.02489001303911209

Training epoch-4 batch-128
Running loss of epoch-4 batch-128 = 0.02251621149480343

Training epoch-4 batch-129
Running loss of epoch-4 batch-129 = 0.023750226944684982

Training epoch-4 batch-130
Running loss of epoch-4 batch-130 = 0.020212505012750626

Training epoch-4 batch-131
Running loss of epoch-4 batch-131 = 0.021487951278686523

Training epoch-4 batch-132
Running loss of epoch-4 batch-132 = 0.0217741671949625

Training epoch-4 batch-133
Running loss of epoch-4 batch-133 = 0.021908478811383247

Training epoch-4 batch-134
Running loss of epoch-4 batch-134 = 0.019051918759942055

Training epoch-4 batch-135
Running loss of epoch-4 batch-135 = 0.02018692158162594

Training epoch-4 batch-136
Running loss of epoch-4 batch-136 = 0.022395307198166847

Training epoch-4 batch-137
Running loss of epoch-4 batch-137 = 0.027235398069024086

Training epoch-4 batch-138
Running loss of epoch-4 batch-138 = 0.024472255259752274

Training epoch-4 batch-139
Running loss of epoch-4 batch-139 = 0.019186336547136307

Training epoch-4 batch-140
Running loss of epoch-4 batch-140 = 0.02215898036956787

Training epoch-4 batch-141
Running loss of epoch-4 batch-141 = 0.02297702059149742

Training epoch-4 batch-142
Running loss of epoch-4 batch-142 = 0.020773995667696

Training epoch-4 batch-143
Running loss of epoch-4 batch-143 = 0.020444631576538086

Training epoch-4 batch-144
Running loss of epoch-4 batch-144 = 0.02169943042099476

Training epoch-4 batch-145
Running loss of epoch-4 batch-145 = 0.019507333636283875

Training epoch-4 batch-146
Running loss of epoch-4 batch-146 = 0.02295505441725254

Training epoch-4 batch-147
Running loss of epoch-4 batch-147 = 0.01917426288127899

Training epoch-4 batch-148
Running loss of epoch-4 batch-148 = 0.023531340062618256

Training epoch-4 batch-149
Running loss of epoch-4 batch-149 = 0.0214444138109684

Training epoch-4 batch-150
Running loss of epoch-4 batch-150 = 0.022713162004947662

Training epoch-4 batch-151
Running loss of epoch-4 batch-151 = 0.022703060880303383

Training epoch-4 batch-152
Running loss of epoch-4 batch-152 = 0.021278057247400284

Training epoch-4 batch-153
Running loss of epoch-4 batch-153 = 0.025350533425807953

Training epoch-4 batch-154
Running loss of epoch-4 batch-154 = 0.024279950186610222

Training epoch-4 batch-155
Running loss of epoch-4 batch-155 = 0.02279006689786911

Training epoch-4 batch-156
Running loss of epoch-4 batch-156 = 0.02065417170524597

Training epoch-4 batch-157
Running loss of epoch-4 batch-157 = 0.04324392229318619

Finished training epoch-4.



Average train loss at epoch-4 = 0.02225076265335083

Started Evaluation

Average val loss at epoch-4 = 1.5405054461014898

Accuracy for classes:
Accuracy for class equals is: 70.46 %
Accuracy for class main is: 51.31 %
Accuracy for class setUp is: 57.21 %
Accuracy for class onCreate is: 53.94 %
Accuracy for class toString is: 33.79 %
Accuracy for class run is: 13.70 %
Accuracy for class hashCode is: 83.15 %
Accuracy for class init is: 17.71 %
Accuracy for class execute is: 29.72 %
Accuracy for class get is: 27.69 %

Overall Accuracy = 46.15 %


Best Accuracy = 46.15 % at Epoch-4
Saving model after best epoch-4

Finished Evaluation



Started training epoch-5


Training epoch-5 batch-1
Running loss of epoch-5 batch-1 = 0.016902722418308258

Training epoch-5 batch-2
Running loss of epoch-5 batch-2 = 0.014516475610435009

Training epoch-5 batch-3
Running loss of epoch-5 batch-3 = 0.017260918393731117

Training epoch-5 batch-4
Running loss of epoch-5 batch-4 = 0.020077070221304893

Training epoch-5 batch-5
Running loss of epoch-5 batch-5 = 0.014642668887972832

Training epoch-5 batch-6
Running loss of epoch-5 batch-6 = 0.01921740733087063

Training epoch-5 batch-7
Running loss of epoch-5 batch-7 = 0.01819600723683834

Training epoch-5 batch-8
Running loss of epoch-5 batch-8 = 0.018184218555688858

Training epoch-5 batch-9
Running loss of epoch-5 batch-9 = 0.018170569092035294

Training epoch-5 batch-10
Running loss of epoch-5 batch-10 = 0.015160001814365387

Training epoch-5 batch-11
Running loss of epoch-5 batch-11 = 0.0177130289375782

Training epoch-5 batch-12
Running loss of epoch-5 batch-12 = 0.01684483326971531

Training epoch-5 batch-13
Running loss of epoch-5 batch-13 = 0.015124424360692501

Training epoch-5 batch-14
Running loss of epoch-5 batch-14 = 0.019999995827674866

Training epoch-5 batch-15
Running loss of epoch-5 batch-15 = 0.01554198656231165

Training epoch-5 batch-16
Running loss of epoch-5 batch-16 = 0.014590738341212273

Training epoch-5 batch-17
Running loss of epoch-5 batch-17 = 0.015148047357797623

Training epoch-5 batch-18
Running loss of epoch-5 batch-18 = 0.014730092138051987

Training epoch-5 batch-19
Running loss of epoch-5 batch-19 = 0.01619456708431244

Training epoch-5 batch-20
Running loss of epoch-5 batch-20 = 0.013615457341074944

Training epoch-5 batch-21
Running loss of epoch-5 batch-21 = 0.014313704334199429

Training epoch-5 batch-22
Running loss of epoch-5 batch-22 = 0.014882124029099941

Training epoch-5 batch-23
Running loss of epoch-5 batch-23 = 0.013604234904050827

Training epoch-5 batch-24
Running loss of epoch-5 batch-24 = 0.017381872981786728

Training epoch-5 batch-25
Running loss of epoch-5 batch-25 = 0.013101567514240742

Training epoch-5 batch-26
Running loss of epoch-5 batch-26 = 0.014568069949746132

Training epoch-5 batch-27
Running loss of epoch-5 batch-27 = 0.015299225226044655

Training epoch-5 batch-28
Running loss of epoch-5 batch-28 = 0.014870662242174149

Training epoch-5 batch-29
Running loss of epoch-5 batch-29 = 0.01208481378853321

Training epoch-5 batch-30
Running loss of epoch-5 batch-30 = 0.019100094214081764

Training epoch-5 batch-31
Running loss of epoch-5 batch-31 = 0.015656815841794014

Training epoch-5 batch-32
Running loss of epoch-5 batch-32 = 0.017819881439208984

Training epoch-5 batch-33
Running loss of epoch-5 batch-33 = 0.01757294125854969

Training epoch-5 batch-34
Running loss of epoch-5 batch-34 = 0.017252255231142044

Training epoch-5 batch-35
Running loss of epoch-5 batch-35 = 0.015122013166546822

Training epoch-5 batch-36
Running loss of epoch-5 batch-36 = 0.01636895351111889

Training epoch-5 batch-37
Running loss of epoch-5 batch-37 = 0.015287012793123722

Training epoch-5 batch-38
Running loss of epoch-5 batch-38 = 0.01868852972984314

Training epoch-5 batch-39
Running loss of epoch-5 batch-39 = 0.016612734645605087

Training epoch-5 batch-40
Running loss of epoch-5 batch-40 = 0.0145127447322011

Training epoch-5 batch-41
Running loss of epoch-5 batch-41 = 0.015032577328383923

Training epoch-5 batch-42
Running loss of epoch-5 batch-42 = 0.01659359410405159

Training epoch-5 batch-43
Running loss of epoch-5 batch-43 = 0.011030923575162888

Training epoch-5 batch-44
Running loss of epoch-5 batch-44 = 0.018139628693461418

Training epoch-5 batch-45
Running loss of epoch-5 batch-45 = 0.0175272598862648

Training epoch-5 batch-46
Running loss of epoch-5 batch-46 = 0.01438709907233715

Training epoch-5 batch-47
Running loss of epoch-5 batch-47 = 0.01636691205203533

Training epoch-5 batch-48
Running loss of epoch-5 batch-48 = 0.01745324209332466

Training epoch-5 batch-49
Running loss of epoch-5 batch-49 = 0.017071517184376717

Training epoch-5 batch-50
Running loss of epoch-5 batch-50 = 0.016751134768128395

Training epoch-5 batch-51
Running loss of epoch-5 batch-51 = 0.01811770163476467

Training epoch-5 batch-52
Running loss of epoch-5 batch-52 = 0.016955995932221413

Training epoch-5 batch-53
Running loss of epoch-5 batch-53 = 0.01760581135749817

Training epoch-5 batch-54
Running loss of epoch-5 batch-54 = 0.019395625218749046

Training epoch-5 batch-55
Running loss of epoch-5 batch-55 = 0.013366642408072948

Training epoch-5 batch-56
Running loss of epoch-5 batch-56 = 0.016180552542209625

Training epoch-5 batch-57
Running loss of epoch-5 batch-57 = 0.019117839634418488

Training epoch-5 batch-58
Running loss of epoch-5 batch-58 = 0.0174129456281662

Training epoch-5 batch-59
Running loss of epoch-5 batch-59 = 0.016096753999590874

Training epoch-5 batch-60
Running loss of epoch-5 batch-60 = 0.017256565392017365

Training epoch-5 batch-61
Running loss of epoch-5 batch-61 = 0.01839612051844597

Training epoch-5 batch-62
Running loss of epoch-5 batch-62 = 0.01784215308725834

Training epoch-5 batch-63
Running loss of epoch-5 batch-63 = 0.01681867428123951

Training epoch-5 batch-64
Running loss of epoch-5 batch-64 = 0.01766182668507099

Training epoch-5 batch-65
Running loss of epoch-5 batch-65 = 0.01523940172046423

Training epoch-5 batch-66
Running loss of epoch-5 batch-66 = 0.014728065580129623

Training epoch-5 batch-67
Running loss of epoch-5 batch-67 = 0.018347572535276413

Training epoch-5 batch-68
Running loss of epoch-5 batch-68 = 0.017311660572886467

Training epoch-5 batch-69
Running loss of epoch-5 batch-69 = 0.020889516919851303

Training epoch-5 batch-70
Running loss of epoch-5 batch-70 = 0.01786060258746147

Training epoch-5 batch-71
Running loss of epoch-5 batch-71 = 0.01660100184381008

Training epoch-5 batch-72
Running loss of epoch-5 batch-72 = 0.01734090782701969

Training epoch-5 batch-73
Running loss of epoch-5 batch-73 = 0.019890381023287773

Training epoch-5 batch-74
Running loss of epoch-5 batch-74 = 0.02005317434668541

Training epoch-5 batch-75
Running loss of epoch-5 batch-75 = 0.019969694316387177

Training epoch-5 batch-76
Running loss of epoch-5 batch-76 = 0.01987152174115181

Training epoch-5 batch-77
Running loss of epoch-5 batch-77 = 0.017148442566394806

Training epoch-5 batch-78
Running loss of epoch-5 batch-78 = 0.018540145829319954

Training epoch-5 batch-79
Running loss of epoch-5 batch-79 = 0.014665928669273853

Training epoch-5 batch-80
Running loss of epoch-5 batch-80 = 0.01613636128604412

Training epoch-5 batch-81
Running loss of epoch-5 batch-81 = 0.017447510734200478

Training epoch-5 batch-82
Running loss of epoch-5 batch-82 = 0.0181907806545496

Training epoch-5 batch-83
Running loss of epoch-5 batch-83 = 0.017576219514012337

Training epoch-5 batch-84
Running loss of epoch-5 batch-84 = 0.015344704501330853

Training epoch-5 batch-85
Running loss of epoch-5 batch-85 = 0.017044171690940857

Training epoch-5 batch-86
Running loss of epoch-5 batch-86 = 0.016893746331334114

Training epoch-5 batch-87
Running loss of epoch-5 batch-87 = 0.016687601804733276

Training epoch-5 batch-88
Running loss of epoch-5 batch-88 = 0.015243152156472206

Training epoch-5 batch-89
Running loss of epoch-5 batch-89 = 0.014148336835205555

Training epoch-5 batch-90
Running loss of epoch-5 batch-90 = 0.013188845477998257

Training epoch-5 batch-91
Running loss of epoch-5 batch-91 = 0.015684112906455994

Training epoch-5 batch-92
Running loss of epoch-5 batch-92 = 0.019801681861281395

Training epoch-5 batch-93
Running loss of epoch-5 batch-93 = 0.01627267338335514

Training epoch-5 batch-94
Running loss of epoch-5 batch-94 = 0.015999186784029007

Training epoch-5 batch-95
Running loss of epoch-5 batch-95 = 0.015674687922000885

Training epoch-5 batch-96
Running loss of epoch-5 batch-96 = 0.017245236784219742

Training epoch-5 batch-97
Running loss of epoch-5 batch-97 = 0.01515500433743

Training epoch-5 batch-98
Running loss of epoch-5 batch-98 = 0.016609016805887222

Training epoch-5 batch-99
Running loss of epoch-5 batch-99 = 0.016957655549049377

Training epoch-5 batch-100
Running loss of epoch-5 batch-100 = 0.014813522808253765

Training epoch-5 batch-101
Running loss of epoch-5 batch-101 = 0.022488223388791084

Training epoch-5 batch-102
Running loss of epoch-5 batch-102 = 0.015996627509593964

Training epoch-5 batch-103
Running loss of epoch-5 batch-103 = 0.014357611536979675

Training epoch-5 batch-104
Running loss of epoch-5 batch-104 = 0.013083351776003838

Training epoch-5 batch-105
Running loss of epoch-5 batch-105 = 0.012738188728690147

Training epoch-5 batch-106
Running loss of epoch-5 batch-106 = 0.020475206896662712

Training epoch-5 batch-107
Running loss of epoch-5 batch-107 = 0.017348740249872208

Training epoch-5 batch-108
Running loss of epoch-5 batch-108 = 0.016215912997722626

Training epoch-5 batch-109
Running loss of epoch-5 batch-109 = 0.015436524525284767

Training epoch-5 batch-110
Running loss of epoch-5 batch-110 = 0.015516407787799835

Training epoch-5 batch-111
Running loss of epoch-5 batch-111 = 0.01650131493806839

Training epoch-5 batch-112
Running loss of epoch-5 batch-112 = 0.01497226394712925

Training epoch-5 batch-113
Running loss of epoch-5 batch-113 = 0.01783675327897072

Training epoch-5 batch-114
Running loss of epoch-5 batch-114 = 0.017361395061016083

Training epoch-5 batch-115
Running loss of epoch-5 batch-115 = 0.016367388889193535

Training epoch-5 batch-116
Running loss of epoch-5 batch-116 = 0.016549089923501015

Training epoch-5 batch-117
Running loss of epoch-5 batch-117 = 0.020960891619324684

Training epoch-5 batch-118
Running loss of epoch-5 batch-118 = 0.016439830884337425

Training epoch-5 batch-119
Running loss of epoch-5 batch-119 = 0.016667427495121956

Training epoch-5 batch-120
Running loss of epoch-5 batch-120 = 0.015434792265295982

Training epoch-5 batch-121
Running loss of epoch-5 batch-121 = 0.019094528630375862

Training epoch-5 batch-122
Running loss of epoch-5 batch-122 = 0.013810083270072937

Training epoch-5 batch-123
Running loss of epoch-5 batch-123 = 0.014288023114204407

Training epoch-5 batch-124
Running loss of epoch-5 batch-124 = 0.01861964724957943

Training epoch-5 batch-125
Running loss of epoch-5 batch-125 = 0.017385315150022507

Training epoch-5 batch-126
Running loss of epoch-5 batch-126 = 0.015552377328276634

Training epoch-5 batch-127
Running loss of epoch-5 batch-127 = 0.017742827534675598

Training epoch-5 batch-128
Running loss of epoch-5 batch-128 = 0.014643850736320019

Training epoch-5 batch-129
Running loss of epoch-5 batch-129 = 0.015298939310014248

Training epoch-5 batch-130
Running loss of epoch-5 batch-130 = 0.018429139629006386

Training epoch-5 batch-131
Running loss of epoch-5 batch-131 = 0.017429543659090996

Training epoch-5 batch-132
Running loss of epoch-5 batch-132 = 0.021721910685300827

Training epoch-5 batch-133
Running loss of epoch-5 batch-133 = 0.014599285088479519

Training epoch-5 batch-134
Running loss of epoch-5 batch-134 = 0.017679162323474884

Training epoch-5 batch-135
Running loss of epoch-5 batch-135 = 0.01477744895964861

Training epoch-5 batch-136
Running loss of epoch-5 batch-136 = 0.015525829046964645

Training epoch-5 batch-137
Running loss of epoch-5 batch-137 = 0.015465307980775833

Training epoch-5 batch-138
Running loss of epoch-5 batch-138 = 0.018245412036776543

Training epoch-5 batch-139
Running loss of epoch-5 batch-139 = 0.016632869839668274

Training epoch-5 batch-140
Running loss of epoch-5 batch-140 = 0.019006717950105667

Training epoch-5 batch-141
Running loss of epoch-5 batch-141 = 0.013946312479674816

Training epoch-5 batch-142
Running loss of epoch-5 batch-142 = 0.019546689465641975

Training epoch-5 batch-143
Running loss of epoch-5 batch-143 = 0.018064210191369057

Training epoch-5 batch-144
Running loss of epoch-5 batch-144 = 0.016751345247030258

Training epoch-5 batch-145
Running loss of epoch-5 batch-145 = 0.021770795807242393

Training epoch-5 batch-146
Running loss of epoch-5 batch-146 = 0.01938806101679802

Training epoch-5 batch-147
Running loss of epoch-5 batch-147 = 0.013240274973213673

Training epoch-5 batch-148
Running loss of epoch-5 batch-148 = 0.01775989681482315

Training epoch-5 batch-149
Running loss of epoch-5 batch-149 = 0.015506641939282417

Training epoch-5 batch-150
Running loss of epoch-5 batch-150 = 0.0167982280254364

Training epoch-5 batch-151
Running loss of epoch-5 batch-151 = 0.01601707562804222

Training epoch-5 batch-152
Running loss of epoch-5 batch-152 = 0.014290791004896164

Training epoch-5 batch-153
Running loss of epoch-5 batch-153 = 0.021025488153100014

Training epoch-5 batch-154
Running loss of epoch-5 batch-154 = 0.016433408483862877

Training epoch-5 batch-155
Running loss of epoch-5 batch-155 = 0.016425935551524162

Training epoch-5 batch-156
Running loss of epoch-5 batch-156 = 0.01661168597638607

Training epoch-5 batch-157
Running loss of epoch-5 batch-157 = 0.07278516888618469

Finished training epoch-5.



Average train loss at epoch-5 = 0.016765133464336394

Started Evaluation

Average val loss at epoch-5 = 1.5791391902848293

Accuracy for classes:
Accuracy for class equals is: 76.57 %
Accuracy for class main is: 54.43 %
Accuracy for class setUp is: 60.66 %
Accuracy for class onCreate is: 41.04 %
Accuracy for class toString is: 37.88 %
Accuracy for class run is: 31.96 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 11.21 %
Accuracy for class execute is: 33.33 %
Accuracy for class get is: 15.64 %

Overall Accuracy = 45.64 %

Finished Evaluation



Started training epoch-6


Training epoch-6 batch-1
Running loss of epoch-6 batch-1 = 0.012908062897622585

Training epoch-6 batch-2
Running loss of epoch-6 batch-2 = 0.01121071632951498

Training epoch-6 batch-3
Running loss of epoch-6 batch-3 = 0.01041689794510603

Training epoch-6 batch-4
Running loss of epoch-6 batch-4 = 0.0139224324375391

Training epoch-6 batch-5
Running loss of epoch-6 batch-5 = 0.014080381952226162

Training epoch-6 batch-6
Running loss of epoch-6 batch-6 = 0.015513870865106583

Training epoch-6 batch-7
Running loss of epoch-6 batch-7 = 0.010851418599486351

Training epoch-6 batch-8
Running loss of epoch-6 batch-8 = 0.008426269516348839

Training epoch-6 batch-9
Running loss of epoch-6 batch-9 = 0.013384340330958366

Training epoch-6 batch-10
Running loss of epoch-6 batch-10 = 0.011726679280400276

Training epoch-6 batch-11
Running loss of epoch-6 batch-11 = 0.011915242299437523

Training epoch-6 batch-12
Running loss of epoch-6 batch-12 = 0.013355482369661331

Training epoch-6 batch-13
Running loss of epoch-6 batch-13 = 0.010224577970802784

Training epoch-6 batch-14
Running loss of epoch-6 batch-14 = 0.010163708589971066

Training epoch-6 batch-15
Running loss of epoch-6 batch-15 = 0.00939182098954916

Training epoch-6 batch-16
Running loss of epoch-6 batch-16 = 0.009009653702378273

Training epoch-6 batch-17
Running loss of epoch-6 batch-17 = 0.008689630776643753

Training epoch-6 batch-18
Running loss of epoch-6 batch-18 = 0.012445119209587574

Training epoch-6 batch-19
Running loss of epoch-6 batch-19 = 0.009037982672452927

Training epoch-6 batch-20
Running loss of epoch-6 batch-20 = 0.010712618939578533

Training epoch-6 batch-21
Running loss of epoch-6 batch-21 = 0.010176427662372589

Training epoch-6 batch-22
Running loss of epoch-6 batch-22 = 0.010892475955188274

Training epoch-6 batch-23
Running loss of epoch-6 batch-23 = 0.012044204398989677

Training epoch-6 batch-24
Running loss of epoch-6 batch-24 = 0.010676460340619087

Training epoch-6 batch-25
Running loss of epoch-6 batch-25 = 0.01020831149071455

Training epoch-6 batch-26
Running loss of epoch-6 batch-26 = 0.007850230671465397

Training epoch-6 batch-27
Running loss of epoch-6 batch-27 = 0.008263571187853813

Training epoch-6 batch-28
Running loss of epoch-6 batch-28 = 0.010313443839550018

Training epoch-6 batch-29
Running loss of epoch-6 batch-29 = 0.010469968430697918

Training epoch-6 batch-30
Running loss of epoch-6 batch-30 = 0.013099422678351402

Training epoch-6 batch-31
Running loss of epoch-6 batch-31 = 0.011830195784568787

Training epoch-6 batch-32
Running loss of epoch-6 batch-32 = 0.00853575300425291

Training epoch-6 batch-33
Running loss of epoch-6 batch-33 = 0.012769531458616257

Training epoch-6 batch-34
Running loss of epoch-6 batch-34 = 0.009746185503900051

Training epoch-6 batch-35
Running loss of epoch-6 batch-35 = 0.00887911207973957

Training epoch-6 batch-36
Running loss of epoch-6 batch-36 = 0.010663267225027084

Training epoch-6 batch-37
Running loss of epoch-6 batch-37 = 0.009781169705092907

Training epoch-6 batch-38
Running loss of epoch-6 batch-38 = 0.012081180699169636

Training epoch-6 batch-39
Running loss of epoch-6 batch-39 = 0.009264427237212658

Training epoch-6 batch-40
Running loss of epoch-6 batch-40 = 0.010505159385502338

Training epoch-6 batch-41
Running loss of epoch-6 batch-41 = 0.008335405960679054

Training epoch-6 batch-42
Running loss of epoch-6 batch-42 = 0.008005545474588871

Training epoch-6 batch-43
Running loss of epoch-6 batch-43 = 0.010025707073509693

Training epoch-6 batch-44
Running loss of epoch-6 batch-44 = 0.009690018370747566

Training epoch-6 batch-45
Running loss of epoch-6 batch-45 = 0.011752061545848846

Training epoch-6 batch-46
Running loss of epoch-6 batch-46 = 0.009483321569859982

Training epoch-6 batch-47
Running loss of epoch-6 batch-47 = 0.008142000995576382

Training epoch-6 batch-48
Running loss of epoch-6 batch-48 = 0.009129327721893787

Training epoch-6 batch-49
Running loss of epoch-6 batch-49 = 0.009152972139418125

Training epoch-6 batch-50
Running loss of epoch-6 batch-50 = 0.008947085589170456

Training epoch-6 batch-51
Running loss of epoch-6 batch-51 = 0.012248322367668152

Training epoch-6 batch-52
Running loss of epoch-6 batch-52 = 0.008074258454144001

Training epoch-6 batch-53
Running loss of epoch-6 batch-53 = 0.008970710448920727

Training epoch-6 batch-54
Running loss of epoch-6 batch-54 = 0.009157941676676273

Training epoch-6 batch-55
Running loss of epoch-6 batch-55 = 0.008031418547034264

Training epoch-6 batch-56
Running loss of epoch-6 batch-56 = 0.01238966453820467

Training epoch-6 batch-57
Running loss of epoch-6 batch-57 = 0.009583416394889355

Training epoch-6 batch-58
Running loss of epoch-6 batch-58 = 0.010235056281089783

Training epoch-6 batch-59
Running loss of epoch-6 batch-59 = 0.0076372926123440266

Training epoch-6 batch-60
Running loss of epoch-6 batch-60 = 0.010944658890366554

Training epoch-6 batch-61
Running loss of epoch-6 batch-61 = 0.008014878258109093

Training epoch-6 batch-62
Running loss of epoch-6 batch-62 = 0.011113323271274567

Training epoch-6 batch-63
Running loss of epoch-6 batch-63 = 0.008754749782383442

Training epoch-6 batch-64
Running loss of epoch-6 batch-64 = 0.0071200039237737656

Training epoch-6 batch-65
Running loss of epoch-6 batch-65 = 0.01283711101859808

Training epoch-6 batch-66
Running loss of epoch-6 batch-66 = 0.007550450507551432

Training epoch-6 batch-67
Running loss of epoch-6 batch-67 = 0.007895187474787235

Training epoch-6 batch-68
Running loss of epoch-6 batch-68 = 0.01398494653403759

Training epoch-6 batch-69
Running loss of epoch-6 batch-69 = 0.012461657635867596

Training epoch-6 batch-70
Running loss of epoch-6 batch-70 = 0.006907140836119652

Training epoch-6 batch-71
Running loss of epoch-6 batch-71 = 0.009491982869803905

Training epoch-6 batch-72
Running loss of epoch-6 batch-72 = 0.011181308887898922

Training epoch-6 batch-73
Running loss of epoch-6 batch-73 = 0.011604906059801579

Training epoch-6 batch-74
Running loss of epoch-6 batch-74 = 0.01154785230755806

Training epoch-6 batch-75
Running loss of epoch-6 batch-75 = 0.00951062235981226

Training epoch-6 batch-76
Running loss of epoch-6 batch-76 = 0.010382834821939468

Training epoch-6 batch-77
Running loss of epoch-6 batch-77 = 0.009462689980864525

Training epoch-6 batch-78
Running loss of epoch-6 batch-78 = 0.012098568491637707

Training epoch-6 batch-79
Running loss of epoch-6 batch-79 = 0.012570125050842762

Training epoch-6 batch-80
Running loss of epoch-6 batch-80 = 0.01071772538125515

Training epoch-6 batch-81
Running loss of epoch-6 batch-81 = 0.008574836887419224

Training epoch-6 batch-82
Running loss of epoch-6 batch-82 = 0.009385650977492332

Training epoch-6 batch-83
Running loss of epoch-6 batch-83 = 0.013367717154324055

Training epoch-6 batch-84
Running loss of epoch-6 batch-84 = 0.011958020739257336

Training epoch-6 batch-85
Running loss of epoch-6 batch-85 = 0.008599855937063694

Training epoch-6 batch-86
Running loss of epoch-6 batch-86 = 0.012394554913043976

Training epoch-6 batch-87
Running loss of epoch-6 batch-87 = 0.01288486085832119

Training epoch-6 batch-88
Running loss of epoch-6 batch-88 = 0.011204085312783718

Training epoch-6 batch-89
Running loss of epoch-6 batch-89 = 0.007454688660800457

Training epoch-6 batch-90
Running loss of epoch-6 batch-90 = 0.009043356403708458

Training epoch-6 batch-91
Running loss of epoch-6 batch-91 = 0.009660026989877224

Training epoch-6 batch-92
Running loss of epoch-6 batch-92 = 0.011494986712932587

Training epoch-6 batch-93
Running loss of epoch-6 batch-93 = 0.008373675867915154

Training epoch-6 batch-94
Running loss of epoch-6 batch-94 = 0.014074807055294514

Training epoch-6 batch-95
Running loss of epoch-6 batch-95 = 0.012351208366453648

Training epoch-6 batch-96
Running loss of epoch-6 batch-96 = 0.008921841159462929

Training epoch-6 batch-97
Running loss of epoch-6 batch-97 = 0.011502977460622787

Training epoch-6 batch-98
Running loss of epoch-6 batch-98 = 0.012401580810546875

Training epoch-6 batch-99
Running loss of epoch-6 batch-99 = 0.010299758054316044

Training epoch-6 batch-100
Running loss of epoch-6 batch-100 = 0.011427800171077251

Training epoch-6 batch-101
Running loss of epoch-6 batch-101 = 0.010247780941426754

Training epoch-6 batch-102
Running loss of epoch-6 batch-102 = 0.008137986995279789

Training epoch-6 batch-103
Running loss of epoch-6 batch-103 = 0.008686848916113377

Training epoch-6 batch-104
Running loss of epoch-6 batch-104 = 0.010795009322464466

Training epoch-6 batch-105
Running loss of epoch-6 batch-105 = 0.009335949085652828

Training epoch-6 batch-106
Running loss of epoch-6 batch-106 = 0.010988610796630383

Training epoch-6 batch-107
Running loss of epoch-6 batch-107 = 0.01004444994032383

Training epoch-6 batch-108
Running loss of epoch-6 batch-108 = 0.012822297401726246

Training epoch-6 batch-109
Running loss of epoch-6 batch-109 = 0.013333985581994057

Training epoch-6 batch-110
Running loss of epoch-6 batch-110 = 0.007208104245364666

Training epoch-6 batch-111
Running loss of epoch-6 batch-111 = 0.013090134598314762

Training epoch-6 batch-112
Running loss of epoch-6 batch-112 = 0.012405901215970516

Training epoch-6 batch-113
Running loss of epoch-6 batch-113 = 0.006279084365814924

Training epoch-6 batch-114
Running loss of epoch-6 batch-114 = 0.0077538928017020226

Training epoch-6 batch-115
Running loss of epoch-6 batch-115 = 0.011261537671089172

Training epoch-6 batch-116
Running loss of epoch-6 batch-116 = 0.01182528492063284

Training epoch-6 batch-117
Running loss of epoch-6 batch-117 = 0.011843014508485794

Training epoch-6 batch-118
Running loss of epoch-6 batch-118 = 0.010381124913692474

Training epoch-6 batch-119
Running loss of epoch-6 batch-119 = 0.0096389539539814

Training epoch-6 batch-120
Running loss of epoch-6 batch-120 = 0.00985273253172636

Training epoch-6 batch-121
Running loss of epoch-6 batch-121 = 0.009165463969111443

Training epoch-6 batch-122
Running loss of epoch-6 batch-122 = 0.012942394241690636

Training epoch-6 batch-123
Running loss of epoch-6 batch-123 = 0.010387524962425232

Training epoch-6 batch-124
Running loss of epoch-6 batch-124 = 0.010158218443393707

Training epoch-6 batch-125
Running loss of epoch-6 batch-125 = 0.009288594126701355

Training epoch-6 batch-126
Running loss of epoch-6 batch-126 = 0.008391357958316803

Training epoch-6 batch-127
Running loss of epoch-6 batch-127 = 0.009243767708539963

Training epoch-6 batch-128
Running loss of epoch-6 batch-128 = 0.010929849930107594

Training epoch-6 batch-129
Running loss of epoch-6 batch-129 = 0.01073621865361929

Training epoch-6 batch-130
Running loss of epoch-6 batch-130 = 0.010849254205822945

Training epoch-6 batch-131
Running loss of epoch-6 batch-131 = 0.010839328169822693

Training epoch-6 batch-132
Running loss of epoch-6 batch-132 = 0.010562701150774956

Training epoch-6 batch-133
Running loss of epoch-6 batch-133 = 0.008218795992434025

Training epoch-6 batch-134
Running loss of epoch-6 batch-134 = 0.008085066452622414

Training epoch-6 batch-135
Running loss of epoch-6 batch-135 = 0.008206460624933243

Training epoch-6 batch-136
Running loss of epoch-6 batch-136 = 0.01201117504388094

Training epoch-6 batch-137
Running loss of epoch-6 batch-137 = 0.01240761298686266

Training epoch-6 batch-138
Running loss of epoch-6 batch-138 = 0.010233254171907902

Training epoch-6 batch-139
Running loss of epoch-6 batch-139 = 0.009631448425352573

Training epoch-6 batch-140
Running loss of epoch-6 batch-140 = 0.007599677890539169

Training epoch-6 batch-141
Running loss of epoch-6 batch-141 = 0.009382929652929306

Training epoch-6 batch-142
Running loss of epoch-6 batch-142 = 0.012050143443048

Training epoch-6 batch-143
Running loss of epoch-6 batch-143 = 0.00892051961272955

Training epoch-6 batch-144
Running loss of epoch-6 batch-144 = 0.008308026939630508

Training epoch-6 batch-145
Running loss of epoch-6 batch-145 = 0.006636326666921377

Training epoch-6 batch-146
Running loss of epoch-6 batch-146 = 0.009180968627333641

Training epoch-6 batch-147
Running loss of epoch-6 batch-147 = 0.0090944217517972

Training epoch-6 batch-148
Running loss of epoch-6 batch-148 = 0.01240520179271698

Training epoch-6 batch-149
Running loss of epoch-6 batch-149 = 0.008370470255613327

Training epoch-6 batch-150
Running loss of epoch-6 batch-150 = 0.01166322361677885

Training epoch-6 batch-151
Running loss of epoch-6 batch-151 = 0.009621177799999714

Training epoch-6 batch-152
Running loss of epoch-6 batch-152 = 0.008658606559038162

Training epoch-6 batch-153
Running loss of epoch-6 batch-153 = 0.010466872714459896

Training epoch-6 batch-154
Running loss of epoch-6 batch-154 = 0.00786187220364809

Training epoch-6 batch-155
Running loss of epoch-6 batch-155 = 0.011589154601097107

Training epoch-6 batch-156
Running loss of epoch-6 batch-156 = 0.008893893100321293

Training epoch-6 batch-157
Running loss of epoch-6 batch-157 = 0.03072485513985157

Finished training epoch-6.



Average train loss at epoch-6 = 0.010320408460497857

Started Evaluation

Average val loss at epoch-6 = 1.9178048152672618

Accuracy for classes:
Accuracy for class equals is: 68.98 %
Accuracy for class main is: 40.33 %
Accuracy for class setUp is: 56.39 %
Accuracy for class onCreate is: 38.59 %
Accuracy for class toString is: 31.06 %
Accuracy for class run is: 23.06 %
Accuracy for class hashCode is: 77.53 %
Accuracy for class init is: 21.75 %
Accuracy for class execute is: 47.39 %
Accuracy for class get is: 17.95 %

Overall Accuracy = 42.38 %

Finished Evaluation



Started training epoch-7


Training epoch-7 batch-1
Running loss of epoch-7 batch-1 = 0.00453540962189436

Training epoch-7 batch-2
Running loss of epoch-7 batch-2 = 0.004834520164877176

Training epoch-7 batch-3
Running loss of epoch-7 batch-3 = 0.005906838458031416

Training epoch-7 batch-4
Running loss of epoch-7 batch-4 = 0.00886774342507124

Training epoch-7 batch-5
Running loss of epoch-7 batch-5 = 0.009233259595930576

Training epoch-7 batch-6
Running loss of epoch-7 batch-6 = 0.005937426351010799

Training epoch-7 batch-7
Running loss of epoch-7 batch-7 = 0.005210164934396744

Training epoch-7 batch-8
Running loss of epoch-7 batch-8 = 0.006026356481015682

Training epoch-7 batch-9
Running loss of epoch-7 batch-9 = 0.005530003923922777

Training epoch-7 batch-10
Running loss of epoch-7 batch-10 = 0.006971088238060474

Training epoch-7 batch-11
Running loss of epoch-7 batch-11 = 0.009739000350236893

Training epoch-7 batch-12
Running loss of epoch-7 batch-12 = 0.006941350642591715

Training epoch-7 batch-13
Running loss of epoch-7 batch-13 = 0.007127594202756882

Training epoch-7 batch-14
Running loss of epoch-7 batch-14 = 0.0069569493643939495

Training epoch-7 batch-15
Running loss of epoch-7 batch-15 = 0.005405785515904427

Training epoch-7 batch-16
Running loss of epoch-7 batch-16 = 0.006766574457287788

Training epoch-7 batch-17
Running loss of epoch-7 batch-17 = 0.006128332111984491

Training epoch-7 batch-18
Running loss of epoch-7 batch-18 = 0.0056294891983270645

Training epoch-7 batch-19
Running loss of epoch-7 batch-19 = 0.005604668520390987

Training epoch-7 batch-20
Running loss of epoch-7 batch-20 = 0.00593252619728446

Training epoch-7 batch-21
Running loss of epoch-7 batch-21 = 0.005870531778782606

Training epoch-7 batch-22
Running loss of epoch-7 batch-22 = 0.005078732036054134

Training epoch-7 batch-23
Running loss of epoch-7 batch-23 = 0.006891192402690649

Training epoch-7 batch-24
Running loss of epoch-7 batch-24 = 0.005653043743222952

Training epoch-7 batch-25
Running loss of epoch-7 batch-25 = 0.006188182160258293

Training epoch-7 batch-26
Running loss of epoch-7 batch-26 = 0.004822853021323681

Training epoch-7 batch-27
Running loss of epoch-7 batch-27 = 0.004889593459665775

Training epoch-7 batch-28
Running loss of epoch-7 batch-28 = 0.007904333993792534

Training epoch-7 batch-29
Running loss of epoch-7 batch-29 = 0.005931805819272995

Training epoch-7 batch-30
Running loss of epoch-7 batch-30 = 0.003750160802155733

Training epoch-7 batch-31
Running loss of epoch-7 batch-31 = 0.005770944990217686

Training epoch-7 batch-32
Running loss of epoch-7 batch-32 = 0.004551769234240055

Training epoch-7 batch-33
Running loss of epoch-7 batch-33 = 0.004089300520718098

Training epoch-7 batch-34
Running loss of epoch-7 batch-34 = 0.00367692019790411

Training epoch-7 batch-35
Running loss of epoch-7 batch-35 = 0.004152180161327124

Training epoch-7 batch-36
Running loss of epoch-7 batch-36 = 0.005639507435262203

Training epoch-7 batch-37
Running loss of epoch-7 batch-37 = 0.006556498818099499

Training epoch-7 batch-38
Running loss of epoch-7 batch-38 = 0.004754609428346157

Training epoch-7 batch-39
Running loss of epoch-7 batch-39 = 0.006062340922653675

Training epoch-7 batch-40
Running loss of epoch-7 batch-40 = 0.0076568881049752235

Training epoch-7 batch-41
Running loss of epoch-7 batch-41 = 0.004618297331035137

Training epoch-7 batch-42
Running loss of epoch-7 batch-42 = 0.005214046221226454

Training epoch-7 batch-43
Running loss of epoch-7 batch-43 = 0.005642583128064871

Training epoch-7 batch-44
Running loss of epoch-7 batch-44 = 0.0043755630031228065

Training epoch-7 batch-45
Running loss of epoch-7 batch-45 = 0.006490302737802267

Training epoch-7 batch-46
Running loss of epoch-7 batch-46 = 0.0036831917241215706

Training epoch-7 batch-47
Running loss of epoch-7 batch-47 = 0.0032701718155294657

Training epoch-7 batch-48
Running loss of epoch-7 batch-48 = 0.003545536659657955

Training epoch-7 batch-49
Running loss of epoch-7 batch-49 = 0.004819248337298632

Training epoch-7 batch-50
Running loss of epoch-7 batch-50 = 0.0045751407742500305

Training epoch-7 batch-51
Running loss of epoch-7 batch-51 = 0.003906921483576298

Training epoch-7 batch-52
Running loss of epoch-7 batch-52 = 0.005515729542821646

Training epoch-7 batch-53
Running loss of epoch-7 batch-53 = 0.004984922241419554

Training epoch-7 batch-54
Running loss of epoch-7 batch-54 = 0.004838360007852316

Training epoch-7 batch-55
Running loss of epoch-7 batch-55 = 0.006260591093450785

Training epoch-7 batch-56
Running loss of epoch-7 batch-56 = 0.004875434562563896

Training epoch-7 batch-57
Running loss of epoch-7 batch-57 = 0.0074448129162192345

Training epoch-7 batch-58
Running loss of epoch-7 batch-58 = 0.003088809084147215

Training epoch-7 batch-59
Running loss of epoch-7 batch-59 = 0.007296799682080746

Training epoch-7 batch-60
Running loss of epoch-7 batch-60 = 0.0067508406937122345

Training epoch-7 batch-61
Running loss of epoch-7 batch-61 = 0.0055927010253071785

Training epoch-7 batch-62
Running loss of epoch-7 batch-62 = 0.005399211775511503

Training epoch-7 batch-63
Running loss of epoch-7 batch-63 = 0.003855696879327297

Training epoch-7 batch-64
Running loss of epoch-7 batch-64 = 0.0037681362591683865

Training epoch-7 batch-65
Running loss of epoch-7 batch-65 = 0.005372398998588324

Training epoch-7 batch-66
Running loss of epoch-7 batch-66 = 0.006250113248825073

Training epoch-7 batch-67
Running loss of epoch-7 batch-67 = 0.005450672470033169

Training epoch-7 batch-68
Running loss of epoch-7 batch-68 = 0.004071423783898354

Training epoch-7 batch-69
Running loss of epoch-7 batch-69 = 0.0041580828838050365

Training epoch-7 batch-70
Running loss of epoch-7 batch-70 = 0.005746901500970125

Training epoch-7 batch-71
Running loss of epoch-7 batch-71 = 0.004329320043325424

Training epoch-7 batch-72
Running loss of epoch-7 batch-72 = 0.0027282130904495716

Training epoch-7 batch-73
Running loss of epoch-7 batch-73 = 0.0030158746521919966

Training epoch-7 batch-74
Running loss of epoch-7 batch-74 = 0.003968094941228628

Training epoch-7 batch-75
Running loss of epoch-7 batch-75 = 0.004147310275584459

Training epoch-7 batch-76
Running loss of epoch-7 batch-76 = 0.005497439298778772

Training epoch-7 batch-77
Running loss of epoch-7 batch-77 = 0.004974728915840387

Training epoch-7 batch-78
Running loss of epoch-7 batch-78 = 0.004819479770958424

Training epoch-7 batch-79
Running loss of epoch-7 batch-79 = 0.005499937571585178

Training epoch-7 batch-80
Running loss of epoch-7 batch-80 = 0.006007520016282797

Training epoch-7 batch-81
Running loss of epoch-7 batch-81 = 0.0068689375184476376

Training epoch-7 batch-82
Running loss of epoch-7 batch-82 = 0.0036165169440209866

Training epoch-7 batch-83
Running loss of epoch-7 batch-83 = 0.003178075421601534

Training epoch-7 batch-84
Running loss of epoch-7 batch-84 = 0.004728600382804871

Training epoch-7 batch-85
Running loss of epoch-7 batch-85 = 0.005463064182549715

Training epoch-7 batch-86
Running loss of epoch-7 batch-86 = 0.0036346386186778545

Training epoch-7 batch-87
Running loss of epoch-7 batch-87 = 0.004149948246777058

Training epoch-7 batch-88
Running loss of epoch-7 batch-88 = 0.00479548517614603

Training epoch-7 batch-89
Running loss of epoch-7 batch-89 = 0.006787474732846022

Training epoch-7 batch-90
Running loss of epoch-7 batch-90 = 0.005487480200827122

Training epoch-7 batch-91
Running loss of epoch-7 batch-91 = 0.006145704537630081

Training epoch-7 batch-92
Running loss of epoch-7 batch-92 = 0.005569427739828825

Training epoch-7 batch-93
Running loss of epoch-7 batch-93 = 0.004755190573632717

Training epoch-7 batch-94
Running loss of epoch-7 batch-94 = 0.0055983662605285645

Training epoch-7 batch-95
Running loss of epoch-7 batch-95 = 0.005013090558350086

Training epoch-7 batch-96
Running loss of epoch-7 batch-96 = 0.005386012606322765

Training epoch-7 batch-97
Running loss of epoch-7 batch-97 = 0.005804453976452351

Training epoch-7 batch-98
Running loss of epoch-7 batch-98 = 0.0055737546645104885

Training epoch-7 batch-99
Running loss of epoch-7 batch-99 = 0.006113369949162006

Training epoch-7 batch-100
Running loss of epoch-7 batch-100 = 0.006887259427458048

Training epoch-7 batch-101
Running loss of epoch-7 batch-101 = 0.004704562947154045

Training epoch-7 batch-102
Running loss of epoch-7 batch-102 = 0.007328449748456478

Training epoch-7 batch-103
Running loss of epoch-7 batch-103 = 0.00367285986430943

Training epoch-7 batch-104
Running loss of epoch-7 batch-104 = 0.007460406981408596

Training epoch-7 batch-105
Running loss of epoch-7 batch-105 = 0.005693872459232807

Training epoch-7 batch-106
Running loss of epoch-7 batch-106 = 0.00503291143104434

Training epoch-7 batch-107
Running loss of epoch-7 batch-107 = 0.004028271418064833

Training epoch-7 batch-108
Running loss of epoch-7 batch-108 = 0.003920324146747589

Training epoch-7 batch-109
Running loss of epoch-7 batch-109 = 0.006261017639189959

Training epoch-7 batch-110
Running loss of epoch-7 batch-110 = 0.005628349259495735

Training epoch-7 batch-111
Running loss of epoch-7 batch-111 = 0.005727335810661316

Training epoch-7 batch-112
Running loss of epoch-7 batch-112 = 0.0077847326174378395

Training epoch-7 batch-113
Running loss of epoch-7 batch-113 = 0.004851454868912697

Training epoch-7 batch-114
Running loss of epoch-7 batch-114 = 0.004547604359686375

Training epoch-7 batch-115
Running loss of epoch-7 batch-115 = 0.00566814886406064

Training epoch-7 batch-116
Running loss of epoch-7 batch-116 = 0.005533697083592415

Training epoch-7 batch-117
Running loss of epoch-7 batch-117 = 0.005413161125034094

Training epoch-7 batch-118
Running loss of epoch-7 batch-118 = 0.005997725296765566

Training epoch-7 batch-119
Running loss of epoch-7 batch-119 = 0.004242141731083393

Training epoch-7 batch-120
Running loss of epoch-7 batch-120 = 0.004250200930982828

Training epoch-7 batch-121
Running loss of epoch-7 batch-121 = 0.0062766121700406075

Training epoch-7 batch-122
Running loss of epoch-7 batch-122 = 0.003921648021787405

Training epoch-7 batch-123
Running loss of epoch-7 batch-123 = 0.005289282649755478

Training epoch-7 batch-124
Running loss of epoch-7 batch-124 = 0.00611039437353611

Training epoch-7 batch-125
Running loss of epoch-7 batch-125 = 0.0037936342414468527

Training epoch-7 batch-126
Running loss of epoch-7 batch-126 = 0.00825983751565218

Training epoch-7 batch-127
Running loss of epoch-7 batch-127 = 0.00453718937933445

Training epoch-7 batch-128
Running loss of epoch-7 batch-128 = 0.007409874349832535

Training epoch-7 batch-129
Running loss of epoch-7 batch-129 = 0.003639781381934881

Training epoch-7 batch-130
Running loss of epoch-7 batch-130 = 0.005788033828139305

Training epoch-7 batch-131
Running loss of epoch-7 batch-131 = 0.005881719756871462

Training epoch-7 batch-132
Running loss of epoch-7 batch-132 = 0.0037143416702747345

Training epoch-7 batch-133
Running loss of epoch-7 batch-133 = 0.006865539588034153

Training epoch-7 batch-134
Running loss of epoch-7 batch-134 = 0.006148198619484901

Training epoch-7 batch-135
Running loss of epoch-7 batch-135 = 0.00826963596045971

Training epoch-7 batch-136
Running loss of epoch-7 batch-136 = 0.005278973840177059

Training epoch-7 batch-137
Running loss of epoch-7 batch-137 = 0.0026372759602963924

Training epoch-7 batch-138
Running loss of epoch-7 batch-138 = 0.003048605751246214

Training epoch-7 batch-139
Running loss of epoch-7 batch-139 = 0.003599113319069147

Training epoch-7 batch-140
Running loss of epoch-7 batch-140 = 0.0044758617877960205

Training epoch-7 batch-141
Running loss of epoch-7 batch-141 = 0.003011181717738509

Training epoch-7 batch-142
Running loss of epoch-7 batch-142 = 0.006104175001382828

Training epoch-7 batch-143
Running loss of epoch-7 batch-143 = 0.005313767120242119

Training epoch-7 batch-144
Running loss of epoch-7 batch-144 = 0.0068169767037034035

Training epoch-7 batch-145
Running loss of epoch-7 batch-145 = 0.0027741913218051195

Training epoch-7 batch-146
Running loss of epoch-7 batch-146 = 0.006661283317953348

Training epoch-7 batch-147
Running loss of epoch-7 batch-147 = 0.006407519802451134

Training epoch-7 batch-148
Running loss of epoch-7 batch-148 = 0.006122416816651821

Training epoch-7 batch-149
Running loss of epoch-7 batch-149 = 0.007037335075438023

Training epoch-7 batch-150
Running loss of epoch-7 batch-150 = 0.006622047163546085

Training epoch-7 batch-151
Running loss of epoch-7 batch-151 = 0.005410476587712765

Training epoch-7 batch-152
Running loss of epoch-7 batch-152 = 0.004617955069988966

Training epoch-7 batch-153
Running loss of epoch-7 batch-153 = 0.006339202169328928

Training epoch-7 batch-154
Running loss of epoch-7 batch-154 = 0.007705529220402241

Training epoch-7 batch-155
Running loss of epoch-7 batch-155 = 0.004515472333878279

Training epoch-7 batch-156
Running loss of epoch-7 batch-156 = 0.005039540119469166

Training epoch-7 batch-157
Running loss of epoch-7 batch-157 = 0.026049885898828506

Finished training epoch-7.



Average train loss at epoch-7 = 0.005441844522953034

Started Evaluation

Average val loss at epoch-7 = 2.201752608543948

Accuracy for classes:
Accuracy for class equals is: 66.83 %
Accuracy for class main is: 60.33 %
Accuracy for class setUp is: 53.77 %
Accuracy for class onCreate is: 37.95 %
Accuracy for class toString is: 43.00 %
Accuracy for class run is: 21.23 %
Accuracy for class hashCode is: 77.53 %
Accuracy for class init is: 33.63 %
Accuracy for class execute is: 21.29 %
Accuracy for class get is: 25.38 %

Overall Accuracy = 45.08 %

Finished Evaluation



Started training epoch-8


Training epoch-8 batch-1
Running loss of epoch-8 batch-1 = 0.004120744299143553

Training epoch-8 batch-2
Running loss of epoch-8 batch-2 = 0.0032992991618812084

Training epoch-8 batch-3
Running loss of epoch-8 batch-3 = 0.0022659581154584885

Training epoch-8 batch-4
Running loss of epoch-8 batch-4 = 0.0034433489199727774

Training epoch-8 batch-5
Running loss of epoch-8 batch-5 = 0.0022102149669080973

Training epoch-8 batch-6
Running loss of epoch-8 batch-6 = 0.004247299861162901

Training epoch-8 batch-7
Running loss of epoch-8 batch-7 = 0.0035874745808541775

Training epoch-8 batch-8
Running loss of epoch-8 batch-8 = 0.003685890231281519

Training epoch-8 batch-9
Running loss of epoch-8 batch-9 = 0.002797714900225401

Training epoch-8 batch-10
Running loss of epoch-8 batch-10 = 0.004587749484926462

Training epoch-8 batch-11
Running loss of epoch-8 batch-11 = 0.002383551327511668

Training epoch-8 batch-12
Running loss of epoch-8 batch-12 = 0.0016958477208390832

Training epoch-8 batch-13
Running loss of epoch-8 batch-13 = 0.002462906762957573

Training epoch-8 batch-14
Running loss of epoch-8 batch-14 = 0.0026533675845712423

Training epoch-8 batch-15
Running loss of epoch-8 batch-15 = 0.002573541598394513

Training epoch-8 batch-16
Running loss of epoch-8 batch-16 = 0.0019275919767096639

Training epoch-8 batch-17
Running loss of epoch-8 batch-17 = 0.002525479532778263

Training epoch-8 batch-18
Running loss of epoch-8 batch-18 = 0.004487066529691219

Training epoch-8 batch-19
Running loss of epoch-8 batch-19 = 0.003209949005395174

Training epoch-8 batch-20
Running loss of epoch-8 batch-20 = 0.003111116588115692

Training epoch-8 batch-21
Running loss of epoch-8 batch-21 = 0.0034663560800254345

Training epoch-8 batch-22
Running loss of epoch-8 batch-22 = 0.002927177120000124

Training epoch-8 batch-23
Running loss of epoch-8 batch-23 = 0.0036567803472280502

Training epoch-8 batch-24
Running loss of epoch-8 batch-24 = 0.0020527346059679985

Training epoch-8 batch-25
Running loss of epoch-8 batch-25 = 0.0016310225473716855

Training epoch-8 batch-26
Running loss of epoch-8 batch-26 = 0.004654484335333109

Training epoch-8 batch-27
Running loss of epoch-8 batch-27 = 0.0034590440336614847

Training epoch-8 batch-28
Running loss of epoch-8 batch-28 = 0.002175884321331978

Training epoch-8 batch-29
Running loss of epoch-8 batch-29 = 0.0030319730285555124

Training epoch-8 batch-30
Running loss of epoch-8 batch-30 = 0.0018738541984930634

Training epoch-8 batch-31
Running loss of epoch-8 batch-31 = 0.002317993436008692

Training epoch-8 batch-32
Running loss of epoch-8 batch-32 = 0.0032819488551467657

Training epoch-8 batch-33
Running loss of epoch-8 batch-33 = 0.003429391421377659

Training epoch-8 batch-34
Running loss of epoch-8 batch-34 = 0.0023551960475742817

Training epoch-8 batch-35
Running loss of epoch-8 batch-35 = 0.004199343733489513

Training epoch-8 batch-36
Running loss of epoch-8 batch-36 = 0.001838210504502058

Training epoch-8 batch-37
Running loss of epoch-8 batch-37 = 0.004139859229326248

Training epoch-8 batch-38
Running loss of epoch-8 batch-38 = 0.002893867203965783

Training epoch-8 batch-39
Running loss of epoch-8 batch-39 = 0.0015593449352309108

Training epoch-8 batch-40
Running loss of epoch-8 batch-40 = 0.0026142317801713943

Training epoch-8 batch-41
Running loss of epoch-8 batch-41 = 0.002248411765322089

Training epoch-8 batch-42
Running loss of epoch-8 batch-42 = 0.0032688130158931017

Training epoch-8 batch-43
Running loss of epoch-8 batch-43 = 0.0031789301428943872

Training epoch-8 batch-44
Running loss of epoch-8 batch-44 = 0.00243285926990211

Training epoch-8 batch-45
Running loss of epoch-8 batch-45 = 0.0014388748677447438

Training epoch-8 batch-46
Running loss of epoch-8 batch-46 = 0.002397063421085477

Training epoch-8 batch-47
Running loss of epoch-8 batch-47 = 0.0020984166767448187

Training epoch-8 batch-48
Running loss of epoch-8 batch-48 = 0.0021756410133093596

Training epoch-8 batch-49
Running loss of epoch-8 batch-49 = 0.0038921243976801634

Training epoch-8 batch-50
Running loss of epoch-8 batch-50 = 0.0031162970699369907

Training epoch-8 batch-51
Running loss of epoch-8 batch-51 = 0.0031981600914150476

Training epoch-8 batch-52
Running loss of epoch-8 batch-52 = 0.0031928764656186104

Training epoch-8 batch-53
Running loss of epoch-8 batch-53 = 0.004196335095912218

Training epoch-8 batch-54
Running loss of epoch-8 batch-54 = 0.003726216033101082

Training epoch-8 batch-55
Running loss of epoch-8 batch-55 = 0.0037282530684024096

Training epoch-8 batch-56
Running loss of epoch-8 batch-56 = 0.0024832533672451973

Training epoch-8 batch-57
Running loss of epoch-8 batch-57 = 0.0018407906172797084

Training epoch-8 batch-58
Running loss of epoch-8 batch-58 = 0.00485186418518424

Training epoch-8 batch-59
Running loss of epoch-8 batch-59 = 0.0019722357392311096

Training epoch-8 batch-60
Running loss of epoch-8 batch-60 = 0.002280562650412321

Training epoch-8 batch-61
Running loss of epoch-8 batch-61 = 0.0023616503458470106

Training epoch-8 batch-62
Running loss of epoch-8 batch-62 = 0.0018746601417660713

Training epoch-8 batch-63
Running loss of epoch-8 batch-63 = 0.002240277361124754

Training epoch-8 batch-64
Running loss of epoch-8 batch-64 = 0.0018475840333849192

Training epoch-8 batch-65
Running loss of epoch-8 batch-65 = 0.000929622387047857

Training epoch-8 batch-66
Running loss of epoch-8 batch-66 = 0.004054476507008076

Training epoch-8 batch-67
Running loss of epoch-8 batch-67 = 0.0034897648729383945

Training epoch-8 batch-68
Running loss of epoch-8 batch-68 = 0.0022048749960958958

Training epoch-8 batch-69
Running loss of epoch-8 batch-69 = 0.0010640417458489537

Training epoch-8 batch-70
Running loss of epoch-8 batch-70 = 0.003558317432180047

Training epoch-8 batch-71
Running loss of epoch-8 batch-71 = 0.0033172443509101868

Training epoch-8 batch-72
Running loss of epoch-8 batch-72 = 0.002253562444821

Training epoch-8 batch-73
Running loss of epoch-8 batch-73 = 0.0020337109453976154

Training epoch-8 batch-74
Running loss of epoch-8 batch-74 = 0.00314554525539279

Training epoch-8 batch-75
Running loss of epoch-8 batch-75 = 0.0036344476975500584

Training epoch-8 batch-76
Running loss of epoch-8 batch-76 = 0.0026569487527012825

Training epoch-8 batch-77
Running loss of epoch-8 batch-77 = 0.001775768818333745

Training epoch-8 batch-78
Running loss of epoch-8 batch-78 = 0.0033744522370398045

Training epoch-8 batch-79
Running loss of epoch-8 batch-79 = 0.0022376123815774918

Training epoch-8 batch-80
Running loss of epoch-8 batch-80 = 0.005702221766114235

Training epoch-8 batch-81
Running loss of epoch-8 batch-81 = 0.0025987187400460243

Training epoch-8 batch-82
Running loss of epoch-8 batch-82 = 0.0035017100162804127

Training epoch-8 batch-83
Running loss of epoch-8 batch-83 = 0.0028735927771776915

Training epoch-8 batch-84
Running loss of epoch-8 batch-84 = 0.003168756142258644

Training epoch-8 batch-85
Running loss of epoch-8 batch-85 = 0.001517946831882

Training epoch-8 batch-86
Running loss of epoch-8 batch-86 = 0.0018984563648700714

Training epoch-8 batch-87
Running loss of epoch-8 batch-87 = 0.003709190059453249

Training epoch-8 batch-88
Running loss of epoch-8 batch-88 = 0.004212562460452318

Training epoch-8 batch-89
Running loss of epoch-8 batch-89 = 0.0014043510891497135

Training epoch-8 batch-90
Running loss of epoch-8 batch-90 = 0.0010752268135547638

Training epoch-8 batch-91
Running loss of epoch-8 batch-91 = 0.0023714799899607897

Training epoch-8 batch-92
Running loss of epoch-8 batch-92 = 0.0019723810255527496

Training epoch-8 batch-93
Running loss of epoch-8 batch-93 = 0.0024548983201384544

Training epoch-8 batch-94
Running loss of epoch-8 batch-94 = 0.0025718731340020895

Training epoch-8 batch-95
Running loss of epoch-8 batch-95 = 0.00225067138671875

Training epoch-8 batch-96
Running loss of epoch-8 batch-96 = 0.001478347578085959

Training epoch-8 batch-97
Running loss of epoch-8 batch-97 = 0.002518163761124015

Training epoch-8 batch-98
Running loss of epoch-8 batch-98 = 0.003529741195961833

Training epoch-8 batch-99
Running loss of epoch-8 batch-99 = 0.0022943085059523582

Training epoch-8 batch-100
Running loss of epoch-8 batch-100 = 0.002883210778236389

Training epoch-8 batch-101
Running loss of epoch-8 batch-101 = 0.0034711069893091917

Training epoch-8 batch-102
Running loss of epoch-8 batch-102 = 0.003067370969802141

Training epoch-8 batch-103
Running loss of epoch-8 batch-103 = 0.0019890160765498877

Training epoch-8 batch-104
Running loss of epoch-8 batch-104 = 0.0020029605366289616

Training epoch-8 batch-105
Running loss of epoch-8 batch-105 = 0.002402014099061489

Training epoch-8 batch-106
Running loss of epoch-8 batch-106 = 0.001654268242418766

Training epoch-8 batch-107
Running loss of epoch-8 batch-107 = 0.004372801166027784

Training epoch-8 batch-108
Running loss of epoch-8 batch-108 = 0.004568030126392841

Training epoch-8 batch-109
Running loss of epoch-8 batch-109 = 0.002447667298838496

Training epoch-8 batch-110
Running loss of epoch-8 batch-110 = 0.003528786124661565

Training epoch-8 batch-111
Running loss of epoch-8 batch-111 = 0.004099414683878422

Training epoch-8 batch-112
Running loss of epoch-8 batch-112 = 0.004765130579471588

Training epoch-8 batch-113
Running loss of epoch-8 batch-113 = 0.003056667745113373

Training epoch-8 batch-114
Running loss of epoch-8 batch-114 = 0.003277186769992113

Training epoch-8 batch-115
Running loss of epoch-8 batch-115 = 0.0026525002904236317

Training epoch-8 batch-116
Running loss of epoch-8 batch-116 = 0.002577736508101225

Training epoch-8 batch-117
Running loss of epoch-8 batch-117 = 0.0037043397314846516

Training epoch-8 batch-118
Running loss of epoch-8 batch-118 = 0.006135481409728527

Training epoch-8 batch-119
Running loss of epoch-8 batch-119 = 0.005026211030781269

Training epoch-8 batch-120
Running loss of epoch-8 batch-120 = 0.004182410892099142

Training epoch-8 batch-121
Running loss of epoch-8 batch-121 = 0.0028707911260426044

Training epoch-8 batch-122
Running loss of epoch-8 batch-122 = 0.0035080972593277693

Training epoch-8 batch-123
Running loss of epoch-8 batch-123 = 0.0034287997987121344

Training epoch-8 batch-124
Running loss of epoch-8 batch-124 = 0.0030796295031905174

Training epoch-8 batch-125
Running loss of epoch-8 batch-125 = 0.0033687050454318523

Training epoch-8 batch-126
Running loss of epoch-8 batch-126 = 0.0027371712494641542

Training epoch-8 batch-127
Running loss of epoch-8 batch-127 = 0.0018327044090256095

Training epoch-8 batch-128
Running loss of epoch-8 batch-128 = 0.00678745424374938

Training epoch-8 batch-129
Running loss of epoch-8 batch-129 = 0.0026928698644042015

Training epoch-8 batch-130
Running loss of epoch-8 batch-130 = 0.002496446017175913

Training epoch-8 batch-131
Running loss of epoch-8 batch-131 = 0.002913926262408495

Training epoch-8 batch-132
Running loss of epoch-8 batch-132 = 0.0027452369686216116

Training epoch-8 batch-133
Running loss of epoch-8 batch-133 = 0.0029294760897755623

Training epoch-8 batch-134
Running loss of epoch-8 batch-134 = 0.004428984597325325

Training epoch-8 batch-135
Running loss of epoch-8 batch-135 = 0.0028985566459596157

Training epoch-8 batch-136
Running loss of epoch-8 batch-136 = 0.004225742071866989

Training epoch-8 batch-137
Running loss of epoch-8 batch-137 = 0.0016283274162560701

Training epoch-8 batch-138
Running loss of epoch-8 batch-138 = 0.002856690436601639

Training epoch-8 batch-139
Running loss of epoch-8 batch-139 = 0.002631007693707943

Training epoch-8 batch-140
Running loss of epoch-8 batch-140 = 0.0036269202828407288

Training epoch-8 batch-141
Running loss of epoch-8 batch-141 = 0.0033741749357432127

Training epoch-8 batch-142
Running loss of epoch-8 batch-142 = 0.002123497426509857

Training epoch-8 batch-143
Running loss of epoch-8 batch-143 = 0.0016579118091613054

Training epoch-8 batch-144
Running loss of epoch-8 batch-144 = 0.00229134620167315

Training epoch-8 batch-145
Running loss of epoch-8 batch-145 = 0.0027593618724495173

Training epoch-8 batch-146
Running loss of epoch-8 batch-146 = 0.0032859789207577705

Training epoch-8 batch-147
Running loss of epoch-8 batch-147 = 0.002645929343998432

Training epoch-8 batch-148
Running loss of epoch-8 batch-148 = 0.0015376816736534238

Training epoch-8 batch-149
Running loss of epoch-8 batch-149 = 0.002609719755128026

Training epoch-8 batch-150
Running loss of epoch-8 batch-150 = 0.002998874057084322

Training epoch-8 batch-151
Running loss of epoch-8 batch-151 = 0.00418925890699029

Training epoch-8 batch-152
Running loss of epoch-8 batch-152 = 0.004911915399134159

Training epoch-8 batch-153
Running loss of epoch-8 batch-153 = 0.004456376191228628

Training epoch-8 batch-154
Running loss of epoch-8 batch-154 = 0.0042378101497888565

Training epoch-8 batch-155
Running loss of epoch-8 batch-155 = 0.0025719590485095978

Training epoch-8 batch-156
Running loss of epoch-8 batch-156 = 0.001833637710660696

Training epoch-8 batch-157
Running loss of epoch-8 batch-157 = 0.017335079610347748

Finished training epoch-8.



Average train loss at epoch-8 = 0.0029765312183648346

Started Evaluation

Average val loss at epoch-8 = 2.543874806479404

Accuracy for classes:
Accuracy for class equals is: 71.12 %
Accuracy for class main is: 50.49 %
Accuracy for class setUp is: 57.05 %
Accuracy for class onCreate is: 41.04 %
Accuracy for class toString is: 36.86 %
Accuracy for class run is: 35.84 %
Accuracy for class hashCode is: 73.78 %
Accuracy for class init is: 11.88 %
Accuracy for class execute is: 20.48 %
Accuracy for class get is: 42.56 %

Overall Accuracy = 45.47 %

Finished Evaluation



Started training epoch-9


Training epoch-9 batch-1
Running loss of epoch-9 batch-1 = 0.0009681156952865422

Training epoch-9 batch-2
Running loss of epoch-9 batch-2 = 0.001246555126272142

Training epoch-9 batch-3
Running loss of epoch-9 batch-3 = 0.0025051869451999664

Training epoch-9 batch-4
Running loss of epoch-9 batch-4 = 0.0018840350676327944

Training epoch-9 batch-5
Running loss of epoch-9 batch-5 = 0.005107601173222065

Training epoch-9 batch-6
Running loss of epoch-9 batch-6 = 0.0024598061572760344

Training epoch-9 batch-7
Running loss of epoch-9 batch-7 = 0.0010110365692526102

Training epoch-9 batch-8
Running loss of epoch-9 batch-8 = 0.0015194392763078213

Training epoch-9 batch-9
Running loss of epoch-9 batch-9 = 0.0012071691453456879

Training epoch-9 batch-10
Running loss of epoch-9 batch-10 = 0.0023495620116591454

Training epoch-9 batch-11
Running loss of epoch-9 batch-11 = 0.0014106809394434094

Training epoch-9 batch-12
Running loss of epoch-9 batch-12 = 0.001127536641433835

Training epoch-9 batch-13
Running loss of epoch-9 batch-13 = 0.004456593655049801

Training epoch-9 batch-14
Running loss of epoch-9 batch-14 = 0.0029975920915603638

Training epoch-9 batch-15
Running loss of epoch-9 batch-15 = 0.001528390683233738

Training epoch-9 batch-16
Running loss of epoch-9 batch-16 = 0.0019897762686014175

Training epoch-9 batch-17
Running loss of epoch-9 batch-17 = 0.00221300614066422

Training epoch-9 batch-18
Running loss of epoch-9 batch-18 = 0.002227955963462591

Training epoch-9 batch-19
Running loss of epoch-9 batch-19 = 0.00236988952383399

Training epoch-9 batch-20
Running loss of epoch-9 batch-20 = 0.002163443248718977

Training epoch-9 batch-21
Running loss of epoch-9 batch-21 = 0.0019670857582241297

Training epoch-9 batch-22
Running loss of epoch-9 batch-22 = 0.002873108722269535

Training epoch-9 batch-23
Running loss of epoch-9 batch-23 = 0.0017162372823804617

Training epoch-9 batch-24
Running loss of epoch-9 batch-24 = 0.0027991740498691797

Training epoch-9 batch-25
Running loss of epoch-9 batch-25 = 0.0022545114625245333

Training epoch-9 batch-26
Running loss of epoch-9 batch-26 = 0.0025159078650176525

Training epoch-9 batch-27
Running loss of epoch-9 batch-27 = 0.0014251762768253684

Training epoch-9 batch-28
Running loss of epoch-9 batch-28 = 0.001771407900378108

Training epoch-9 batch-29
Running loss of epoch-9 batch-29 = 0.002238697139546275

Training epoch-9 batch-30
Running loss of epoch-9 batch-30 = 0.0017787066753953695

Training epoch-9 batch-31
Running loss of epoch-9 batch-31 = 0.0005564556340686977

Training epoch-9 batch-32
Running loss of epoch-9 batch-32 = 0.004282427951693535

Training epoch-9 batch-33
Running loss of epoch-9 batch-33 = 0.001997313229367137

Training epoch-9 batch-34
Running loss of epoch-9 batch-34 = 0.0021246601827442646

Training epoch-9 batch-35
Running loss of epoch-9 batch-35 = 0.00193479482550174

Training epoch-9 batch-36
Running loss of epoch-9 batch-36 = 0.0024915679823607206

Training epoch-9 batch-37
Running loss of epoch-9 batch-37 = 0.0021797080989927053

Training epoch-9 batch-38
Running loss of epoch-9 batch-38 = 0.0017911805771291256

Training epoch-9 batch-39
Running loss of epoch-9 batch-39 = 0.0013242309214547276

Training epoch-9 batch-40
Running loss of epoch-9 batch-40 = 0.001690804259851575

Training epoch-9 batch-41
Running loss of epoch-9 batch-41 = 0.001789136091247201

Training epoch-9 batch-42
Running loss of epoch-9 batch-42 = 0.0021077462006360292

Training epoch-9 batch-43
Running loss of epoch-9 batch-43 = 0.002226781565696001

Training epoch-9 batch-44
Running loss of epoch-9 batch-44 = 0.0017714289715513587

Training epoch-9 batch-45
Running loss of epoch-9 batch-45 = 0.0012500586453825235

Training epoch-9 batch-46
Running loss of epoch-9 batch-46 = 0.0004909834824502468

Training epoch-9 batch-47
Running loss of epoch-9 batch-47 = 0.0021001568529754877

Training epoch-9 batch-48
Running loss of epoch-9 batch-48 = 0.0012746365973725915

Training epoch-9 batch-49
Running loss of epoch-9 batch-49 = 0.002550883451476693

Training epoch-9 batch-50
Running loss of epoch-9 batch-50 = 0.0008754300652071834

Training epoch-9 batch-51
Running loss of epoch-9 batch-51 = 0.0031782498117536306

Training epoch-9 batch-52
Running loss of epoch-9 batch-52 = 0.0028545057866722345

Training epoch-9 batch-53
Running loss of epoch-9 batch-53 = 0.0010114742908626795

Training epoch-9 batch-54
Running loss of epoch-9 batch-54 = 0.0020894792396575212

Training epoch-9 batch-55
Running loss of epoch-9 batch-55 = 0.0023141386918723583

Training epoch-9 batch-56
Running loss of epoch-9 batch-56 = 0.001520326710306108

Training epoch-9 batch-57
Running loss of epoch-9 batch-57 = 0.00065755401737988

Training epoch-9 batch-58
Running loss of epoch-9 batch-58 = 0.0016495450399816036

Training epoch-9 batch-59
Running loss of epoch-9 batch-59 = 0.001098322100006044

Training epoch-9 batch-60
Running loss of epoch-9 batch-60 = 0.0015158833703026175

Training epoch-9 batch-61
Running loss of epoch-9 batch-61 = 0.0014077412197366357

Training epoch-9 batch-62
Running loss of epoch-9 batch-62 = 0.0023304892238229513

Training epoch-9 batch-63
Running loss of epoch-9 batch-63 = 0.0007521257502958179

Training epoch-9 batch-64
Running loss of epoch-9 batch-64 = 0.002526676980778575

Training epoch-9 batch-65
Running loss of epoch-9 batch-65 = 0.0019664857536554337

Training epoch-9 batch-66
Running loss of epoch-9 batch-66 = 0.002536409068852663

Training epoch-9 batch-67
Running loss of epoch-9 batch-67 = 0.001987222582101822

Training epoch-9 batch-68
Running loss of epoch-9 batch-68 = 0.0008118468103930354

Training epoch-9 batch-69
Running loss of epoch-9 batch-69 = 0.000911701237782836

Training epoch-9 batch-70
Running loss of epoch-9 batch-70 = 0.002969802590087056

Training epoch-9 batch-71
Running loss of epoch-9 batch-71 = 0.0017921800026670098

Training epoch-9 batch-72
Running loss of epoch-9 batch-72 = 0.000964665727224201

Training epoch-9 batch-73
Running loss of epoch-9 batch-73 = 0.0013265598099678755

Training epoch-9 batch-74
Running loss of epoch-9 batch-74 = 0.0009047019411809742

Training epoch-9 batch-75
Running loss of epoch-9 batch-75 = 0.001535984454676509

Training epoch-9 batch-76
Running loss of epoch-9 batch-76 = 0.0009256476769223809

Training epoch-9 batch-77
Running loss of epoch-9 batch-77 = 0.0018024957971647382

Training epoch-9 batch-78
Running loss of epoch-9 batch-78 = 0.0013450159458443522

Training epoch-9 batch-79
Running loss of epoch-9 batch-79 = 0.001517902361229062

Training epoch-9 batch-80
Running loss of epoch-9 batch-80 = 0.0011028907028958201

Training epoch-9 batch-81
Running loss of epoch-9 batch-81 = 0.0015893528470769525

Training epoch-9 batch-82
Running loss of epoch-9 batch-82 = 0.0019248391035944223

Training epoch-9 batch-83
Running loss of epoch-9 batch-83 = 0.0009798837127164006

Training epoch-9 batch-84
Running loss of epoch-9 batch-84 = 0.0022121495567262173

Training epoch-9 batch-85
Running loss of epoch-9 batch-85 = 0.0012235932517796755

Training epoch-9 batch-86
Running loss of epoch-9 batch-86 = 0.0014801127836108208

Training epoch-9 batch-87
Running loss of epoch-9 batch-87 = 0.0008511836058460176

Training epoch-9 batch-88
Running loss of epoch-9 batch-88 = 0.0013125489931553602

Training epoch-9 batch-89
Running loss of epoch-9 batch-89 = 0.0021238625049591064

Training epoch-9 batch-90
Running loss of epoch-9 batch-90 = 0.002008608775213361

Training epoch-9 batch-91
Running loss of epoch-9 batch-91 = 0.0005412577884271741

Training epoch-9 batch-92
Running loss of epoch-9 batch-92 = 0.0013720981078222394

Training epoch-9 batch-93
Running loss of epoch-9 batch-93 = 0.0023465948179364204

Training epoch-9 batch-94
Running loss of epoch-9 batch-94 = 0.00471605034545064

Training epoch-9 batch-95
Running loss of epoch-9 batch-95 = 0.0015577117446810007

Training epoch-9 batch-96
Running loss of epoch-9 batch-96 = 0.0027932552620768547

Training epoch-9 batch-97
Running loss of epoch-9 batch-97 = 0.001818274613469839

Training epoch-9 batch-98
Running loss of epoch-9 batch-98 = 0.0016881523188203573

Training epoch-9 batch-99
Running loss of epoch-9 batch-99 = 0.0011906884610652924

Training epoch-9 batch-100
Running loss of epoch-9 batch-100 = 0.0007712384685873985

Training epoch-9 batch-101
Running loss of epoch-9 batch-101 = 0.0013726692413911223

Training epoch-9 batch-102
Running loss of epoch-9 batch-102 = 0.0021519367583096027

Training epoch-9 batch-103
Running loss of epoch-9 batch-103 = 0.002887214533984661

Training epoch-9 batch-104
Running loss of epoch-9 batch-104 = 0.0006973238196223974

Training epoch-9 batch-105
Running loss of epoch-9 batch-105 = 0.0024670695420354605

Training epoch-9 batch-106
Running loss of epoch-9 batch-106 = 0.002032837364822626

Training epoch-9 batch-107
Running loss of epoch-9 batch-107 = 0.0015223684022203088

Training epoch-9 batch-108
Running loss of epoch-9 batch-108 = 0.0033910232596099377

Training epoch-9 batch-109
Running loss of epoch-9 batch-109 = 0.0022299906704574823

Training epoch-9 batch-110
Running loss of epoch-9 batch-110 = 0.0027824114076793194

Training epoch-9 batch-111
Running loss of epoch-9 batch-111 = 0.0014923496637493372

Training epoch-9 batch-112
Running loss of epoch-9 batch-112 = 0.0015495253028348088

Training epoch-9 batch-113
Running loss of epoch-9 batch-113 = 0.003594550769776106

Training epoch-9 batch-114
Running loss of epoch-9 batch-114 = 0.001201647100970149

Training epoch-9 batch-115
Running loss of epoch-9 batch-115 = 0.0021550790406763554

Training epoch-9 batch-116
Running loss of epoch-9 batch-116 = 0.0014357971958816051

Training epoch-9 batch-117
Running loss of epoch-9 batch-117 = 0.002509388839825988

Training epoch-9 batch-118
Running loss of epoch-9 batch-118 = 0.0021098193246871233

Training epoch-9 batch-119
Running loss of epoch-9 batch-119 = 0.0016366601921617985

Training epoch-9 batch-120
Running loss of epoch-9 batch-120 = 0.002956801326945424

Training epoch-9 batch-121
Running loss of epoch-9 batch-121 = 0.0037408028729259968

Training epoch-9 batch-122
Running loss of epoch-9 batch-122 = 0.0020666855853050947

Training epoch-9 batch-123
Running loss of epoch-9 batch-123 = 0.004225199576467276

Training epoch-9 batch-124
Running loss of epoch-9 batch-124 = 0.0030042666476219893

Training epoch-9 batch-125
Running loss of epoch-9 batch-125 = 0.001561701763421297

Training epoch-9 batch-126
Running loss of epoch-9 batch-126 = 0.004682763479650021

Training epoch-9 batch-127
Running loss of epoch-9 batch-127 = 0.0011913473717868328

Training epoch-9 batch-128
Running loss of epoch-9 batch-128 = 0.0017583227017894387

Training epoch-9 batch-129
Running loss of epoch-9 batch-129 = 0.00214535859413445

Training epoch-9 batch-130
Running loss of epoch-9 batch-130 = 0.0017092262860387564

Training epoch-9 batch-131
Running loss of epoch-9 batch-131 = 0.0013126765843480825

Training epoch-9 batch-132
Running loss of epoch-9 batch-132 = 0.0022420762106776237

Training epoch-9 batch-133
Running loss of epoch-9 batch-133 = 0.0028498778119683266

Training epoch-9 batch-134
Running loss of epoch-9 batch-134 = 0.0009736096835695207

Training epoch-9 batch-135
Running loss of epoch-9 batch-135 = 0.0016423898050561547

Training epoch-9 batch-136
Running loss of epoch-9 batch-136 = 0.004837981425225735

Training epoch-9 batch-137
Running loss of epoch-9 batch-137 = 0.0022675117943435907

Training epoch-9 batch-138
Running loss of epoch-9 batch-138 = 0.004344258923083544

Training epoch-9 batch-139
Running loss of epoch-9 batch-139 = 0.003297911025583744

Training epoch-9 batch-140
Running loss of epoch-9 batch-140 = 0.0027564852498471737

Training epoch-9 batch-141
Running loss of epoch-9 batch-141 = 0.0049329097382724285

Training epoch-9 batch-142
Running loss of epoch-9 batch-142 = 0.003843441605567932

Training epoch-9 batch-143
Running loss of epoch-9 batch-143 = 0.001995125086978078

Training epoch-9 batch-144
Running loss of epoch-9 batch-144 = 0.0032563807908445597

Training epoch-9 batch-145
Running loss of epoch-9 batch-145 = 0.0032010262366384268

Training epoch-9 batch-146
Running loss of epoch-9 batch-146 = 0.0009835665114223957

Training epoch-9 batch-147
Running loss of epoch-9 batch-147 = 0.003270885907113552

Training epoch-9 batch-148
Running loss of epoch-9 batch-148 = 0.0043508210219442844

Training epoch-9 batch-149
Running loss of epoch-9 batch-149 = 0.003736775368452072

Training epoch-9 batch-150
Running loss of epoch-9 batch-150 = 0.0034548707772046328

Training epoch-9 batch-151
Running loss of epoch-9 batch-151 = 0.0035709149669855833

Training epoch-9 batch-152
Running loss of epoch-9 batch-152 = 0.001982142450287938

Training epoch-9 batch-153
Running loss of epoch-9 batch-153 = 0.0030884987208992243

Training epoch-9 batch-154
Running loss of epoch-9 batch-154 = 0.004676940385252237

Training epoch-9 batch-155
Running loss of epoch-9 batch-155 = 0.0023418054915964603

Training epoch-9 batch-156
Running loss of epoch-9 batch-156 = 0.003706985618919134

Training epoch-9 batch-157
Running loss of epoch-9 batch-157 = 0.0034118276089429855

Finished training epoch-9.



Average train loss at epoch-9 = 0.002129511792212725

Started Evaluation

Average val loss at epoch-9 = 3.1011871601405896

Accuracy for classes:
Accuracy for class equals is: 58.75 %
Accuracy for class main is: 39.67 %
Accuracy for class setUp is: 59.84 %
Accuracy for class onCreate is: 32.41 %
Accuracy for class toString is: 21.84 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 42.15 %
Accuracy for class execute is: 21.29 %
Accuracy for class get is: 25.38 %

Overall Accuracy = 42.38 %

Finished Evaluation



Started training epoch-10


Training epoch-10 batch-1
Running loss of epoch-10 batch-1 = 0.0023356687743216753

Training epoch-10 batch-2
Running loss of epoch-10 batch-2 = 0.0018151249969378114

Training epoch-10 batch-3
Running loss of epoch-10 batch-3 = 0.0010356903076171875

Training epoch-10 batch-4
Running loss of epoch-10 batch-4 = 0.003715270198881626

Training epoch-10 batch-5
Running loss of epoch-10 batch-5 = 0.0036186575889587402

Training epoch-10 batch-6
Running loss of epoch-10 batch-6 = 0.0036867805756628513

Training epoch-10 batch-7
Running loss of epoch-10 batch-7 = 0.001613432657904923

Training epoch-10 batch-8
Running loss of epoch-10 batch-8 = 0.0018993375124409795

Training epoch-10 batch-9
Running loss of epoch-10 batch-9 = 0.0012319260276854038

Training epoch-10 batch-10
Running loss of epoch-10 batch-10 = 0.0012943013571202755

Training epoch-10 batch-11
Running loss of epoch-10 batch-11 = 0.0012353002093732357

Training epoch-10 batch-12
Running loss of epoch-10 batch-12 = 0.0027789908926934004

Training epoch-10 batch-13
Running loss of epoch-10 batch-13 = 0.0004453804576769471

Training epoch-10 batch-14
Running loss of epoch-10 batch-14 = 0.0016382384346798062

Training epoch-10 batch-15
Running loss of epoch-10 batch-15 = 0.0019223898416385055

Training epoch-10 batch-16
Running loss of epoch-10 batch-16 = 0.0019807873759418726

Training epoch-10 batch-17
Running loss of epoch-10 batch-17 = 0.0017695482820272446

Training epoch-10 batch-18
Running loss of epoch-10 batch-18 = 0.0008216916467063129

Training epoch-10 batch-19
Running loss of epoch-10 batch-19 = 0.0027435575611889362

Training epoch-10 batch-20
Running loss of epoch-10 batch-20 = 0.002286601113155484

Training epoch-10 batch-21
Running loss of epoch-10 batch-21 = 0.0021644190419465303

Training epoch-10 batch-22
Running loss of epoch-10 batch-22 = 0.001903772004880011

Training epoch-10 batch-23
Running loss of epoch-10 batch-23 = 0.0009612604626454413

Training epoch-10 batch-24
Running loss of epoch-10 batch-24 = 0.0016798008000478148

Training epoch-10 batch-25
Running loss of epoch-10 batch-25 = 0.0016122246161103249

Training epoch-10 batch-26
Running loss of epoch-10 batch-26 = 0.0014471834292635322

Training epoch-10 batch-27
Running loss of epoch-10 batch-27 = 0.00373454624786973

Training epoch-10 batch-28
Running loss of epoch-10 batch-28 = 0.00224989652633667

Training epoch-10 batch-29
Running loss of epoch-10 batch-29 = 0.0019312977092340589

Training epoch-10 batch-30
Running loss of epoch-10 batch-30 = 0.0017971305642277002

Training epoch-10 batch-31
Running loss of epoch-10 batch-31 = 0.0019556914921849966

Training epoch-10 batch-32
Running loss of epoch-10 batch-32 = 0.002804805990308523

Training epoch-10 batch-33
Running loss of epoch-10 batch-33 = 0.0026505873538553715

Training epoch-10 batch-34
Running loss of epoch-10 batch-34 = 0.00367674115113914

Training epoch-10 batch-35
Running loss of epoch-10 batch-35 = 0.0009249678114429116

Training epoch-10 batch-36
Running loss of epoch-10 batch-36 = 0.0016862403135746717

Training epoch-10 batch-37
Running loss of epoch-10 batch-37 = 0.0013942663790658116

Training epoch-10 batch-38
Running loss of epoch-10 batch-38 = 0.0023367905523627996

Training epoch-10 batch-39
Running loss of epoch-10 batch-39 = 0.0033684615045785904

Training epoch-10 batch-40
Running loss of epoch-10 batch-40 = 0.0010737444972619414

Training epoch-10 batch-41
Running loss of epoch-10 batch-41 = 0.0013808890944346786

Training epoch-10 batch-42
Running loss of epoch-10 batch-42 = 0.0015225824899971485

Training epoch-10 batch-43
Running loss of epoch-10 batch-43 = 0.0006805224693380296

Training epoch-10 batch-44
Running loss of epoch-10 batch-44 = 0.002220087917521596

Training epoch-10 batch-45
Running loss of epoch-10 batch-45 = 0.0004378806333988905

Training epoch-10 batch-46
Running loss of epoch-10 batch-46 = 0.0020405857358127832

Training epoch-10 batch-47
Running loss of epoch-10 batch-47 = 0.0016885395161807537

Training epoch-10 batch-48
Running loss of epoch-10 batch-48 = 0.002631161594763398

Training epoch-10 batch-49
Running loss of epoch-10 batch-49 = 0.0009048338979482651

Training epoch-10 batch-50
Running loss of epoch-10 batch-50 = 0.0009866879554465413

Training epoch-10 batch-51
Running loss of epoch-10 batch-51 = 0.002085633110255003

Training epoch-10 batch-52
Running loss of epoch-10 batch-52 = 0.004229779355227947

Training epoch-10 batch-53
Running loss of epoch-10 batch-53 = 0.002271711127832532

Training epoch-10 batch-54
Running loss of epoch-10 batch-54 = 0.002256471896544099

Training epoch-10 batch-55
Running loss of epoch-10 batch-55 = 0.0008765856036916375

Training epoch-10 batch-56
Running loss of epoch-10 batch-56 = 0.0014091732446104288

Training epoch-10 batch-57
Running loss of epoch-10 batch-57 = 0.002384945284575224

Training epoch-10 batch-58
Running loss of epoch-10 batch-58 = 0.0025731835048645735

Training epoch-10 batch-59
Running loss of epoch-10 batch-59 = 0.0016926475800573826

Training epoch-10 batch-60
Running loss of epoch-10 batch-60 = 0.001828293432481587

Training epoch-10 batch-61
Running loss of epoch-10 batch-61 = 0.0017890279414132237

Training epoch-10 batch-62
Running loss of epoch-10 batch-62 = 0.001942208269611001

Training epoch-10 batch-63
Running loss of epoch-10 batch-63 = 0.0013823474291712046

Training epoch-10 batch-64
Running loss of epoch-10 batch-64 = 0.0025064684450626373

Training epoch-10 batch-65
Running loss of epoch-10 batch-65 = 0.0014940358232706785

Training epoch-10 batch-66
Running loss of epoch-10 batch-66 = 0.0023067807778716087

Training epoch-10 batch-67
Running loss of epoch-10 batch-67 = 0.0015037646517157555

Training epoch-10 batch-68
Running loss of epoch-10 batch-68 = 0.0027211494743824005

Training epoch-10 batch-69
Running loss of epoch-10 batch-69 = 0.0032654462847858667

Training epoch-10 batch-70
Running loss of epoch-10 batch-70 = 0.0019536421168595552

Training epoch-10 batch-71
Running loss of epoch-10 batch-71 = 0.0036153343971818686

Training epoch-10 batch-72
Running loss of epoch-10 batch-72 = 0.0017946686130017042

Training epoch-10 batch-73
Running loss of epoch-10 batch-73 = 0.0016887786332517862

Training epoch-10 batch-74
Running loss of epoch-10 batch-74 = 0.00266311620362103

Training epoch-10 batch-75
Running loss of epoch-10 batch-75 = 0.0016869065584614873

Training epoch-10 batch-76
Running loss of epoch-10 batch-76 = 0.0042099375277757645

Training epoch-10 batch-77
Running loss of epoch-10 batch-77 = 0.0016857404261827469

Training epoch-10 batch-78
Running loss of epoch-10 batch-78 = 0.0036302818916738033

Training epoch-10 batch-79
Running loss of epoch-10 batch-79 = 0.0018556984141469002

Training epoch-10 batch-80
Running loss of epoch-10 batch-80 = 0.0015554800629615784

Training epoch-10 batch-81
Running loss of epoch-10 batch-81 = 0.003333174856379628

Training epoch-10 batch-82
Running loss of epoch-10 batch-82 = 0.0036625154316425323

Training epoch-10 batch-83
Running loss of epoch-10 batch-83 = 0.0007221705163829029

Training epoch-10 batch-84
Running loss of epoch-10 batch-84 = 0.0016072732396423817

Training epoch-10 batch-85
Running loss of epoch-10 batch-85 = 0.0015930311055853963

Training epoch-10 batch-86
Running loss of epoch-10 batch-86 = 0.004084412008523941

Training epoch-10 batch-87
Running loss of epoch-10 batch-87 = 0.003138866974040866

Training epoch-10 batch-88
Running loss of epoch-10 batch-88 = 0.002066901186481118

Training epoch-10 batch-89
Running loss of epoch-10 batch-89 = 0.0019156812923029065

Training epoch-10 batch-90
Running loss of epoch-10 batch-90 = 0.002639550482854247

Training epoch-10 batch-91
Running loss of epoch-10 batch-91 = 0.002614409662783146

Training epoch-10 batch-92
Running loss of epoch-10 batch-92 = 0.0013439822942018509

Training epoch-10 batch-93
Running loss of epoch-10 batch-93 = 0.0032793409191071987

Training epoch-10 batch-94
Running loss of epoch-10 batch-94 = 0.0014206512132659554

Training epoch-10 batch-95
Running loss of epoch-10 batch-95 = 0.003265478415414691

Training epoch-10 batch-96
Running loss of epoch-10 batch-96 = 0.0009455351391807199

Training epoch-10 batch-97
Running loss of epoch-10 batch-97 = 0.0012815739028155804

Training epoch-10 batch-98
Running loss of epoch-10 batch-98 = 0.003326565958559513

Training epoch-10 batch-99
Running loss of epoch-10 batch-99 = 0.0020506763830780983

Training epoch-10 batch-100
Running loss of epoch-10 batch-100 = 0.0014011868042871356

Training epoch-10 batch-101
Running loss of epoch-10 batch-101 = 0.0005508909234777093

Training epoch-10 batch-102
Running loss of epoch-10 batch-102 = 0.0003015833208337426

Training epoch-10 batch-103
Running loss of epoch-10 batch-103 = 0.002526272088289261

Training epoch-10 batch-104
Running loss of epoch-10 batch-104 = 0.002400551224127412

Training epoch-10 batch-105
Running loss of epoch-10 batch-105 = 0.002112128073349595

Training epoch-10 batch-106
Running loss of epoch-10 batch-106 = 0.004435552284121513

Training epoch-10 batch-107
Running loss of epoch-10 batch-107 = 0.000940618512686342

Training epoch-10 batch-108
Running loss of epoch-10 batch-108 = 0.0013077581534162164

Training epoch-10 batch-109
Running loss of epoch-10 batch-109 = 0.001090683159418404

Training epoch-10 batch-110
Running loss of epoch-10 batch-110 = 0.0023885478731244802

Training epoch-10 batch-111
Running loss of epoch-10 batch-111 = 0.0027070585638284683

Training epoch-10 batch-112
Running loss of epoch-10 batch-112 = 0.0018378696404397488

Training epoch-10 batch-113
Running loss of epoch-10 batch-113 = 0.0012652650475502014

Training epoch-10 batch-114
Running loss of epoch-10 batch-114 = 0.002294874517247081

Training epoch-10 batch-115
Running loss of epoch-10 batch-115 = 0.0012904545292258263

Training epoch-10 batch-116
Running loss of epoch-10 batch-116 = 0.0009232224547304213

Training epoch-10 batch-117
Running loss of epoch-10 batch-117 = 0.0004942091181874275

Training epoch-10 batch-118
Running loss of epoch-10 batch-118 = 0.002940318314358592

Training epoch-10 batch-119
Running loss of epoch-10 batch-119 = 0.0016419095918536186

Training epoch-10 batch-120
Running loss of epoch-10 batch-120 = 0.0011297265300527215

Training epoch-10 batch-121
Running loss of epoch-10 batch-121 = 0.005221519619226456

Training epoch-10 batch-122
Running loss of epoch-10 batch-122 = 0.001902496675029397

Training epoch-10 batch-123
Running loss of epoch-10 batch-123 = 0.0021611920092254877

Training epoch-10 batch-124
Running loss of epoch-10 batch-124 = 0.0021048537455499172

Training epoch-10 batch-125
Running loss of epoch-10 batch-125 = 0.004396301228553057

Training epoch-10 batch-126
Running loss of epoch-10 batch-126 = 0.0036417092196643353

Training epoch-10 batch-127
Running loss of epoch-10 batch-127 = 0.003118763444945216

Training epoch-10 batch-128
Running loss of epoch-10 batch-128 = 0.0018052557716146111

Training epoch-10 batch-129
Running loss of epoch-10 batch-129 = 0.0036698910407721996

Training epoch-10 batch-130
Running loss of epoch-10 batch-130 = 0.0008937675738707185

Training epoch-10 batch-131
Running loss of epoch-10 batch-131 = 0.00278707523830235

Training epoch-10 batch-132
Running loss of epoch-10 batch-132 = 0.0028552054427564144

Training epoch-10 batch-133
Running loss of epoch-10 batch-133 = 0.002084766048938036

Training epoch-10 batch-134
Running loss of epoch-10 batch-134 = 0.0027434329967945814

Training epoch-10 batch-135
Running loss of epoch-10 batch-135 = 0.0020753080025315285

Training epoch-10 batch-136
Running loss of epoch-10 batch-136 = 0.004475949797779322

Training epoch-10 batch-137
Running loss of epoch-10 batch-137 = 0.00214881869032979

Training epoch-10 batch-138
Running loss of epoch-10 batch-138 = 0.0026719055604189634

Training epoch-10 batch-139
Running loss of epoch-10 batch-139 = 0.002629933413118124

Training epoch-10 batch-140
Running loss of epoch-10 batch-140 = 0.003933110274374485

Training epoch-10 batch-141
Running loss of epoch-10 batch-141 = 0.0026046165730804205

Training epoch-10 batch-142
Running loss of epoch-10 batch-142 = 0.001592207234352827

Training epoch-10 batch-143
Running loss of epoch-10 batch-143 = 0.003199812490493059

Training epoch-10 batch-144
Running loss of epoch-10 batch-144 = 0.0028047917876392603

Training epoch-10 batch-145
Running loss of epoch-10 batch-145 = 0.0034036007709801197

Training epoch-10 batch-146
Running loss of epoch-10 batch-146 = 0.0011784465750679374

Training epoch-10 batch-147
Running loss of epoch-10 batch-147 = 0.0014406442642211914

Training epoch-10 batch-148
Running loss of epoch-10 batch-148 = 0.0050194417126476765

Training epoch-10 batch-149
Running loss of epoch-10 batch-149 = 0.0033075527753680944

Training epoch-10 batch-150
Running loss of epoch-10 batch-150 = 0.002872604178264737

Training epoch-10 batch-151
Running loss of epoch-10 batch-151 = 0.0030518705025315285

Training epoch-10 batch-152
Running loss of epoch-10 batch-152 = 0.002204800257459283

Training epoch-10 batch-153
Running loss of epoch-10 batch-153 = 0.0008522458374500275

Training epoch-10 batch-154
Running loss of epoch-10 batch-154 = 0.0039066653698682785

Training epoch-10 batch-155
Running loss of epoch-10 batch-155 = 0.002270692028105259

Training epoch-10 batch-156
Running loss of epoch-10 batch-156 = 0.0028333584778010845

Training epoch-10 batch-157
Running loss of epoch-10 batch-157 = 0.0159287229180336

Finished training epoch-10.



Average train loss at epoch-10 = 0.002218846545368433

Started Evaluation

Average val loss at epoch-10 = 3.068978511973431

Accuracy for classes:
Accuracy for class equals is: 68.65 %
Accuracy for class main is: 48.85 %
Accuracy for class setUp is: 42.95 %
Accuracy for class onCreate is: 38.49 %
Accuracy for class toString is: 39.93 %
Accuracy for class run is: 51.37 %
Accuracy for class hashCode is: 76.40 %
Accuracy for class init is: 28.48 %
Accuracy for class execute is: 10.84 %
Accuracy for class get is: 27.95 %

Overall Accuracy = 44.27 %

Finished Evaluation



Started training epoch-11


Training epoch-11 batch-1
Running loss of epoch-11 batch-1 = 0.0014696872094646096

Training epoch-11 batch-2
Running loss of epoch-11 batch-2 = 0.003939984831959009

Training epoch-11 batch-3
Running loss of epoch-11 batch-3 = 0.005638207774609327

Training epoch-11 batch-4
Running loss of epoch-11 batch-4 = 0.0030221263878047466

Training epoch-11 batch-5
Running loss of epoch-11 batch-5 = 0.003742695553228259

Training epoch-11 batch-6
Running loss of epoch-11 batch-6 = 0.002827094169333577

Training epoch-11 batch-7
Running loss of epoch-11 batch-7 = 0.0024556699208915234

Training epoch-11 batch-8
Running loss of epoch-11 batch-8 = 0.005940133240073919

Training epoch-11 batch-9
Running loss of epoch-11 batch-9 = 0.002885092282667756

Training epoch-11 batch-10
Running loss of epoch-11 batch-10 = 0.0027503895107656717

Training epoch-11 batch-11
Running loss of epoch-11 batch-11 = 0.0015616889577358961

Training epoch-11 batch-12
Running loss of epoch-11 batch-12 = 0.0016649512108415365

Training epoch-11 batch-13
Running loss of epoch-11 batch-13 = 0.00228601461276412

Training epoch-11 batch-14
Running loss of epoch-11 batch-14 = 0.0016503743827342987

Training epoch-11 batch-15
Running loss of epoch-11 batch-15 = 0.0037548427935689688

Training epoch-11 batch-16
Running loss of epoch-11 batch-16 = 0.001960538327693939

Training epoch-11 batch-17
Running loss of epoch-11 batch-17 = 0.0020362825598567724

Training epoch-11 batch-18
Running loss of epoch-11 batch-18 = 0.0035007400438189507

Training epoch-11 batch-19
Running loss of epoch-11 batch-19 = 0.003299874486401677

Training epoch-11 batch-20
Running loss of epoch-11 batch-20 = 0.0010842373594641685

Training epoch-11 batch-21
Running loss of epoch-11 batch-21 = 0.00418490543961525

Training epoch-11 batch-22
Running loss of epoch-11 batch-22 = 0.0042822035029530525

Training epoch-11 batch-23
Running loss of epoch-11 batch-23 = 0.0031247008591890335

Training epoch-11 batch-24
Running loss of epoch-11 batch-24 = 0.005358738359063864

Training epoch-11 batch-25
Running loss of epoch-11 batch-25 = 0.0035244845785200596

Training epoch-11 batch-26
Running loss of epoch-11 batch-26 = 0.002022629603743553

Training epoch-11 batch-27
Running loss of epoch-11 batch-27 = 0.004232955165207386

Training epoch-11 batch-28
Running loss of epoch-11 batch-28 = 0.004360863473266363

Training epoch-11 batch-29
Running loss of epoch-11 batch-29 = 0.0033981651067733765

Training epoch-11 batch-30
Running loss of epoch-11 batch-30 = 0.00205489294603467

Training epoch-11 batch-31
Running loss of epoch-11 batch-31 = 0.0017995490925386548

Training epoch-11 batch-32
Running loss of epoch-11 batch-32 = 0.004508084151893854

Training epoch-11 batch-33
Running loss of epoch-11 batch-33 = 0.003194517455995083

Training epoch-11 batch-34
Running loss of epoch-11 batch-34 = 0.002133358037099242

Training epoch-11 batch-35
Running loss of epoch-11 batch-35 = 0.0014221759047359228

Training epoch-11 batch-36
Running loss of epoch-11 batch-36 = 0.0021567947696894407

Training epoch-11 batch-37
Running loss of epoch-11 batch-37 = 0.002182762371376157

Training epoch-11 batch-38
Running loss of epoch-11 batch-38 = 0.002765053417533636

Training epoch-11 batch-39
Running loss of epoch-11 batch-39 = 0.0018357887165620923

Training epoch-11 batch-40
Running loss of epoch-11 batch-40 = 0.001976416213437915

Training epoch-11 batch-41
Running loss of epoch-11 batch-41 = 0.0029240066651254892

Training epoch-11 batch-42
Running loss of epoch-11 batch-42 = 0.0013952727895230055

Training epoch-11 batch-43
Running loss of epoch-11 batch-43 = 0.0034343330189585686

Training epoch-11 batch-44
Running loss of epoch-11 batch-44 = 0.0006793204229325056

Training epoch-11 batch-45
Running loss of epoch-11 batch-45 = 0.0013651347253471613

Training epoch-11 batch-46
Running loss of epoch-11 batch-46 = 0.002856483682990074

Training epoch-11 batch-47
Running loss of epoch-11 batch-47 = 0.0025372267700731754

Training epoch-11 batch-48
Running loss of epoch-11 batch-48 = 0.003258722834289074

Training epoch-11 batch-49
Running loss of epoch-11 batch-49 = 0.0023393386509269476

Training epoch-11 batch-50
Running loss of epoch-11 batch-50 = 0.0013720307033509016

Training epoch-11 batch-51
Running loss of epoch-11 batch-51 = 0.0019025009823963046

Training epoch-11 batch-52
Running loss of epoch-11 batch-52 = 0.004470087122172117

Training epoch-11 batch-53
Running loss of epoch-11 batch-53 = 0.004054986871778965

Training epoch-11 batch-54
Running loss of epoch-11 batch-54 = 0.0009205478709191084

Training epoch-11 batch-55
Running loss of epoch-11 batch-55 = 0.0017121962737292051

Training epoch-11 batch-56
Running loss of epoch-11 batch-56 = 0.0014256301801651716

Training epoch-11 batch-57
Running loss of epoch-11 batch-57 = 0.0047916327603161335

Training epoch-11 batch-58
Running loss of epoch-11 batch-58 = 0.003968550357967615

Training epoch-11 batch-59
Running loss of epoch-11 batch-59 = 0.004230146296322346

Training epoch-11 batch-60
Running loss of epoch-11 batch-60 = 0.0010091036092489958

Training epoch-11 batch-61
Running loss of epoch-11 batch-61 = 0.0007996098720468581

Training epoch-11 batch-62
Running loss of epoch-11 batch-62 = 0.00402485579252243

Training epoch-11 batch-63
Running loss of epoch-11 batch-63 = 0.002134455367922783

Training epoch-11 batch-64
Running loss of epoch-11 batch-64 = 0.0018444987945258617

Training epoch-11 batch-65
Running loss of epoch-11 batch-65 = 0.0012920073932036757

Training epoch-11 batch-66
Running loss of epoch-11 batch-66 = 0.0014782370999455452

Training epoch-11 batch-67
Running loss of epoch-11 batch-67 = 0.0025841197930276394

Training epoch-11 batch-68
Running loss of epoch-11 batch-68 = 0.0017250380478799343

Training epoch-11 batch-69
Running loss of epoch-11 batch-69 = 0.0022931494750082493

Training epoch-11 batch-70
Running loss of epoch-11 batch-70 = 0.0013982087839394808

Training epoch-11 batch-71
Running loss of epoch-11 batch-71 = 0.0019194487249478698

Training epoch-11 batch-72
Running loss of epoch-11 batch-72 = 0.0036508396733552217

Training epoch-11 batch-73
Running loss of epoch-11 batch-73 = 0.0022205552086234093

Training epoch-11 batch-74
Running loss of epoch-11 batch-74 = 0.0030717598274350166

Training epoch-11 batch-75
Running loss of epoch-11 batch-75 = 0.0020187844056636095

Training epoch-11 batch-76
Running loss of epoch-11 batch-76 = 0.003569924971088767

Training epoch-11 batch-77
Running loss of epoch-11 batch-77 = 0.0021993997506797314

Training epoch-11 batch-78
Running loss of epoch-11 batch-78 = 0.0029925114940851927

Training epoch-11 batch-79
Running loss of epoch-11 batch-79 = 0.003486469155177474

Training epoch-11 batch-80
Running loss of epoch-11 batch-80 = 0.0015257505001500249

Training epoch-11 batch-81
Running loss of epoch-11 batch-81 = 0.002060197526589036

Training epoch-11 batch-82
Running loss of epoch-11 batch-82 = 0.003223240375518799

Training epoch-11 batch-83
Running loss of epoch-11 batch-83 = 0.002444521291181445

Training epoch-11 batch-84
Running loss of epoch-11 batch-84 = 0.0010647587478160858

Training epoch-11 batch-85
Running loss of epoch-11 batch-85 = 0.002471717307344079

Training epoch-11 batch-86
Running loss of epoch-11 batch-86 = 0.004644634202122688

Training epoch-11 batch-87
Running loss of epoch-11 batch-87 = 0.007086095400154591

Training epoch-11 batch-88
Running loss of epoch-11 batch-88 = 0.002674872288480401

Training epoch-11 batch-89
Running loss of epoch-11 batch-89 = 0.00193175976164639

Training epoch-11 batch-90
Running loss of epoch-11 batch-90 = 0.002544066868722439

Training epoch-11 batch-91
Running loss of epoch-11 batch-91 = 0.002047108020633459

Training epoch-11 batch-92
Running loss of epoch-11 batch-92 = 0.0020926224533468485

Training epoch-11 batch-93
Running loss of epoch-11 batch-93 = 0.0021798075176775455

Training epoch-11 batch-94
Running loss of epoch-11 batch-94 = 0.0016920171910896897

Training epoch-11 batch-95
Running loss of epoch-11 batch-95 = 0.002222023904323578

Training epoch-11 batch-96
Running loss of epoch-11 batch-96 = 0.004205459263175726

Training epoch-11 batch-97
Running loss of epoch-11 batch-97 = 0.0027113896794617176

Training epoch-11 batch-98
Running loss of epoch-11 batch-98 = 0.003502977080643177

Training epoch-11 batch-99
Running loss of epoch-11 batch-99 = 0.002988966181874275

Training epoch-11 batch-100
Running loss of epoch-11 batch-100 = 0.004105810075998306

Training epoch-11 batch-101
Running loss of epoch-11 batch-101 = 0.002720294753089547

Training epoch-11 batch-102
Running loss of epoch-11 batch-102 = 0.004977853503078222

Training epoch-11 batch-103
Running loss of epoch-11 batch-103 = 0.001336004352197051

Training epoch-11 batch-104
Running loss of epoch-11 batch-104 = 0.0021108584478497505

Training epoch-11 batch-105
Running loss of epoch-11 batch-105 = 0.001032222411595285

Training epoch-11 batch-106
Running loss of epoch-11 batch-106 = 0.006476894952356815

Training epoch-11 batch-107
Running loss of epoch-11 batch-107 = 0.0021631147246807814

Training epoch-11 batch-108
Running loss of epoch-11 batch-108 = 0.0018490968504920602

Training epoch-11 batch-109
Running loss of epoch-11 batch-109 = 0.0014881088864058256

Training epoch-11 batch-110
Running loss of epoch-11 batch-110 = 0.0021687003318220377

Training epoch-11 batch-111
Running loss of epoch-11 batch-111 = 0.0015202382346615195

Training epoch-11 batch-112
Running loss of epoch-11 batch-112 = 0.003300344804301858

Training epoch-11 batch-113
Running loss of epoch-11 batch-113 = 0.0013580992817878723

Training epoch-11 batch-114
Running loss of epoch-11 batch-114 = 0.0023695207200944424

Training epoch-11 batch-115
Running loss of epoch-11 batch-115 = 0.0023569678887724876

Training epoch-11 batch-116
Running loss of epoch-11 batch-116 = 0.001098490203730762

Training epoch-11 batch-117
Running loss of epoch-11 batch-117 = 0.0017462936230003834

Training epoch-11 batch-118
Running loss of epoch-11 batch-118 = 0.001136187231168151

Training epoch-11 batch-119
Running loss of epoch-11 batch-119 = 0.0012690882431343198

Training epoch-11 batch-120
Running loss of epoch-11 batch-120 = 0.0013671355554834008

Training epoch-11 batch-121
Running loss of epoch-11 batch-121 = 0.0007498872582800686

Training epoch-11 batch-122
Running loss of epoch-11 batch-122 = 0.0011254779528826475

Training epoch-11 batch-123
Running loss of epoch-11 batch-123 = 0.001671579317189753

Training epoch-11 batch-124
Running loss of epoch-11 batch-124 = 0.005487203598022461

Training epoch-11 batch-125
Running loss of epoch-11 batch-125 = 0.0017415517941117287

Training epoch-11 batch-126
Running loss of epoch-11 batch-126 = 0.0031577860936522484

Training epoch-11 batch-127
Running loss of epoch-11 batch-127 = 0.0022665048018097878

Training epoch-11 batch-128
Running loss of epoch-11 batch-128 = 0.0014142156578600407

Training epoch-11 batch-129
Running loss of epoch-11 batch-129 = 0.0024222987703979015

Training epoch-11 batch-130
Running loss of epoch-11 batch-130 = 0.003858523443341255

Training epoch-11 batch-131
Running loss of epoch-11 batch-131 = 0.0029303699266165495

Training epoch-11 batch-132
Running loss of epoch-11 batch-132 = 0.0018225332023575902

Training epoch-11 batch-133
Running loss of epoch-11 batch-133 = 0.002557700965553522

Training epoch-11 batch-134
Running loss of epoch-11 batch-134 = 0.0023803699295967817

Training epoch-11 batch-135
Running loss of epoch-11 batch-135 = 0.0023530488833785057

Training epoch-11 batch-136
Running loss of epoch-11 batch-136 = 0.0032475972548127174

Training epoch-11 batch-137
Running loss of epoch-11 batch-137 = 0.0019799512811005116

Training epoch-11 batch-138
Running loss of epoch-11 batch-138 = 0.0020477608777582645

Training epoch-11 batch-139
Running loss of epoch-11 batch-139 = 0.0022320039570331573

Training epoch-11 batch-140
Running loss of epoch-11 batch-140 = 0.0018986997893080115

Training epoch-11 batch-141
Running loss of epoch-11 batch-141 = 0.004103295970708132

Training epoch-11 batch-142
Running loss of epoch-11 batch-142 = 0.0013977768830955029

Training epoch-11 batch-143
Running loss of epoch-11 batch-143 = 0.0018196197925135493

Training epoch-11 batch-144
Running loss of epoch-11 batch-144 = 0.0026710021775215864

Training epoch-11 batch-145
Running loss of epoch-11 batch-145 = 0.0006101938779465854

Training epoch-11 batch-146
Running loss of epoch-11 batch-146 = 0.00194635265506804

Training epoch-11 batch-147
Running loss of epoch-11 batch-147 = 0.002932658651843667

Training epoch-11 batch-148
Running loss of epoch-11 batch-148 = 0.0016859137685969472

Training epoch-11 batch-149
Running loss of epoch-11 batch-149 = 0.002543410286307335

Training epoch-11 batch-150
Running loss of epoch-11 batch-150 = 0.0035078879445791245

Training epoch-11 batch-151
Running loss of epoch-11 batch-151 = 0.003916418179869652

Training epoch-11 batch-152
Running loss of epoch-11 batch-152 = 0.004433494992554188

Training epoch-11 batch-153
Running loss of epoch-11 batch-153 = 0.001679376931861043

Training epoch-11 batch-154
Running loss of epoch-11 batch-154 = 0.0017023015534505248

Training epoch-11 batch-155
Running loss of epoch-11 batch-155 = 0.003245387226343155

Training epoch-11 batch-156
Running loss of epoch-11 batch-156 = 0.0019016638398170471

Training epoch-11 batch-157
Running loss of epoch-11 batch-157 = 0.005826854147017002

Finished training epoch-11.



Average train loss at epoch-11 = 0.00259522966183722

Started Evaluation

Average val loss at epoch-11 = 3.2006179423708665

Accuracy for classes:
Accuracy for class equals is: 51.32 %
Accuracy for class main is: 48.69 %
Accuracy for class setUp is: 61.80 %
Accuracy for class onCreate is: 42.54 %
Accuracy for class toString is: 46.08 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 13.00 %
Accuracy for class execute is: 11.24 %
Accuracy for class get is: 32.31 %

Overall Accuracy = 43.61 %

Finished Evaluation



Started training epoch-12


Training epoch-12 batch-1
Running loss of epoch-12 batch-1 = 0.0015860582934692502

Training epoch-12 batch-2
Running loss of epoch-12 batch-2 = 0.0004832478007301688

Training epoch-12 batch-3
Running loss of epoch-12 batch-3 = 0.0005084797739982605

Training epoch-12 batch-4
Running loss of epoch-12 batch-4 = 0.001469442038796842

Training epoch-12 batch-5
Running loss of epoch-12 batch-5 = 0.0015582434134557843

Training epoch-12 batch-6
Running loss of epoch-12 batch-6 = 0.002003889065235853

Training epoch-12 batch-7
Running loss of epoch-12 batch-7 = 0.001732214936055243

Training epoch-12 batch-8
Running loss of epoch-12 batch-8 = 0.002877079648897052

Training epoch-12 batch-9
Running loss of epoch-12 batch-9 = 0.0005759640480391681

Training epoch-12 batch-10
Running loss of epoch-12 batch-10 = 0.0011480058310553432

Training epoch-12 batch-11
Running loss of epoch-12 batch-11 = 0.0012431842042133212

Training epoch-12 batch-12
Running loss of epoch-12 batch-12 = 0.0018514377297833562

Training epoch-12 batch-13
Running loss of epoch-12 batch-13 = 0.002088783076032996

Training epoch-12 batch-14
Running loss of epoch-12 batch-14 = 0.0029568858444690704

Training epoch-12 batch-15
Running loss of epoch-12 batch-15 = 0.0013202194822952151

Training epoch-12 batch-16
Running loss of epoch-12 batch-16 = 0.0010889590485021472

Training epoch-12 batch-17
Running loss of epoch-12 batch-17 = 0.0017452831380069256

Training epoch-12 batch-18
Running loss of epoch-12 batch-18 = 0.0008335296297445893

Training epoch-12 batch-19
Running loss of epoch-12 batch-19 = 0.002609420567750931

Training epoch-12 batch-20
Running loss of epoch-12 batch-20 = 0.002212159801274538

Training epoch-12 batch-21
Running loss of epoch-12 batch-21 = 0.0019383265171200037

Training epoch-12 batch-22
Running loss of epoch-12 batch-22 = 0.0003456350532360375

Training epoch-12 batch-23
Running loss of epoch-12 batch-23 = 0.0007052578148432076

Training epoch-12 batch-24
Running loss of epoch-12 batch-24 = 0.0015898651909083128

Training epoch-12 batch-25
Running loss of epoch-12 batch-25 = 0.0007761226734146476

Training epoch-12 batch-26
Running loss of epoch-12 batch-26 = 0.0021574117708951235

Training epoch-12 batch-27
Running loss of epoch-12 batch-27 = 0.003292437642812729

Training epoch-12 batch-28
Running loss of epoch-12 batch-28 = 0.0010345516493543983

Training epoch-12 batch-29
Running loss of epoch-12 batch-29 = 0.002093173563480377

Training epoch-12 batch-30
Running loss of epoch-12 batch-30 = 0.000591374933719635

Training epoch-12 batch-31
Running loss of epoch-12 batch-31 = 0.0017838393105193973

Training epoch-12 batch-32
Running loss of epoch-12 batch-32 = 0.0012315921485424042

Training epoch-12 batch-33
Running loss of epoch-12 batch-33 = 0.0014809390995651484

Training epoch-12 batch-34
Running loss of epoch-12 batch-34 = 0.0011150557547807693

Training epoch-12 batch-35
Running loss of epoch-12 batch-35 = 0.0007264244486577809

Training epoch-12 batch-36
Running loss of epoch-12 batch-36 = 0.002677519805729389

Training epoch-12 batch-37
Running loss of epoch-12 batch-37 = 0.0007538192439824343

Training epoch-12 batch-38
Running loss of epoch-12 batch-38 = 0.0005920700496062636

Training epoch-12 batch-39
Running loss of epoch-12 batch-39 = 0.0007483228691853583

Training epoch-12 batch-40
Running loss of epoch-12 batch-40 = 0.0031066392548382282

Training epoch-12 batch-41
Running loss of epoch-12 batch-41 = 0.0006922179600223899

Training epoch-12 batch-42
Running loss of epoch-12 batch-42 = 0.0026551210321485996

Training epoch-12 batch-43
Running loss of epoch-12 batch-43 = 0.002065965672954917

Training epoch-12 batch-44
Running loss of epoch-12 batch-44 = 0.0008394557517021894

Training epoch-12 batch-45
Running loss of epoch-12 batch-45 = 0.001906365854665637

Training epoch-12 batch-46
Running loss of epoch-12 batch-46 = 0.0016763991443440318

Training epoch-12 batch-47
Running loss of epoch-12 batch-47 = 0.001357505563646555

Training epoch-12 batch-48
Running loss of epoch-12 batch-48 = 0.000980287790298462

Training epoch-12 batch-49
Running loss of epoch-12 batch-49 = 0.0019586419221013784

Training epoch-12 batch-50
Running loss of epoch-12 batch-50 = 0.0011279991595074534

Training epoch-12 batch-51
Running loss of epoch-12 batch-51 = 0.001015817979350686

Training epoch-12 batch-52
Running loss of epoch-12 batch-52 = 0.0008670048555359244

Training epoch-12 batch-53
Running loss of epoch-12 batch-53 = 0.0015270239673554897

Training epoch-12 batch-54
Running loss of epoch-12 batch-54 = 0.0017901159590110183

Training epoch-12 batch-55
Running loss of epoch-12 batch-55 = 0.0002857223153114319

Training epoch-12 batch-56
Running loss of epoch-12 batch-56 = 0.0005335723981261253

Training epoch-12 batch-57
Running loss of epoch-12 batch-57 = 0.0014633722603321075

Training epoch-12 batch-58
Running loss of epoch-12 batch-58 = 0.002274363301694393

Training epoch-12 batch-59
Running loss of epoch-12 batch-59 = 0.0030904095619916916

Training epoch-12 batch-60
Running loss of epoch-12 batch-60 = 0.0033791183959692717

Training epoch-12 batch-61
Running loss of epoch-12 batch-61 = 0.0027959668077528477

Training epoch-12 batch-62
Running loss of epoch-12 batch-62 = 0.0018166503868997097

Training epoch-12 batch-63
Running loss of epoch-12 batch-63 = 0.0005707263480871916

Training epoch-12 batch-64
Running loss of epoch-12 batch-64 = 0.0017021921230480075

Training epoch-12 batch-65
Running loss of epoch-12 batch-65 = 0.0007077037589624524

Training epoch-12 batch-66
Running loss of epoch-12 batch-66 = 0.0017878032522276044

Training epoch-12 batch-67
Running loss of epoch-12 batch-67 = 0.003103549825027585

Training epoch-12 batch-68
Running loss of epoch-12 batch-68 = 0.0020688087679445744

Training epoch-12 batch-69
Running loss of epoch-12 batch-69 = 0.001971360296010971

Training epoch-12 batch-70
Running loss of epoch-12 batch-70 = 0.0005160636501386762

Training epoch-12 batch-71
Running loss of epoch-12 batch-71 = 0.0020470984745770693

Training epoch-12 batch-72
Running loss of epoch-12 batch-72 = 0.0016714985249564052

Training epoch-12 batch-73
Running loss of epoch-12 batch-73 = 0.0008271282422356308

Training epoch-12 batch-74
Running loss of epoch-12 batch-74 = 0.0013848863309249282

Training epoch-12 batch-75
Running loss of epoch-12 batch-75 = 0.0004741571028716862

Training epoch-12 batch-76
Running loss of epoch-12 batch-76 = 0.0014928156742826104

Training epoch-12 batch-77
Running loss of epoch-12 batch-77 = 0.0030342014506459236

Training epoch-12 batch-78
Running loss of epoch-12 batch-78 = 0.0008818246424198151

Training epoch-12 batch-79
Running loss of epoch-12 batch-79 = 0.0008707791566848755

Training epoch-12 batch-80
Running loss of epoch-12 batch-80 = 0.0008622362511232495

Training epoch-12 batch-81
Running loss of epoch-12 batch-81 = 0.0014099039835855365

Training epoch-12 batch-82
Running loss of epoch-12 batch-82 = 0.0015207850374281406

Training epoch-12 batch-83
Running loss of epoch-12 batch-83 = 0.0009045202750712633

Training epoch-12 batch-84
Running loss of epoch-12 batch-84 = 0.0005380059010349214

Training epoch-12 batch-85
Running loss of epoch-12 batch-85 = 0.0007736809784546494

Training epoch-12 batch-86
Running loss of epoch-12 batch-86 = 0.0015530881937593222

Training epoch-12 batch-87
Running loss of epoch-12 batch-87 = 0.00048155576223507524

Training epoch-12 batch-88
Running loss of epoch-12 batch-88 = 0.004136296454817057

Training epoch-12 batch-89
Running loss of epoch-12 batch-89 = 0.001447736518457532

Training epoch-12 batch-90
Running loss of epoch-12 batch-90 = 0.0007278126431629062

Training epoch-12 batch-91
Running loss of epoch-12 batch-91 = 0.001191295450553298

Training epoch-12 batch-92
Running loss of epoch-12 batch-92 = 0.001310951542109251

Training epoch-12 batch-93
Running loss of epoch-12 batch-93 = 0.000732600805349648

Training epoch-12 batch-94
Running loss of epoch-12 batch-94 = 0.00016492430586367846

Training epoch-12 batch-95
Running loss of epoch-12 batch-95 = 0.0016051066340878606

Training epoch-12 batch-96
Running loss of epoch-12 batch-96 = 0.0011970896739512682

Training epoch-12 batch-97
Running loss of epoch-12 batch-97 = 0.001051181461662054

Training epoch-12 batch-98
Running loss of epoch-12 batch-98 = 0.0006467870553024113

Training epoch-12 batch-99
Running loss of epoch-12 batch-99 = 0.0017666019266471267

Training epoch-12 batch-100
Running loss of epoch-12 batch-100 = 0.0008360851206816733

Training epoch-12 batch-101
Running loss of epoch-12 batch-101 = 0.00041714124381542206

Training epoch-12 batch-102
Running loss of epoch-12 batch-102 = 0.0004480057395994663

Training epoch-12 batch-103
Running loss of epoch-12 batch-103 = 0.0025472017005085945

Training epoch-12 batch-104
Running loss of epoch-12 batch-104 = 0.0021305454429239035

Training epoch-12 batch-105
Running loss of epoch-12 batch-105 = 0.0011143955634906888

Training epoch-12 batch-106
Running loss of epoch-12 batch-106 = 0.002715784590691328

Training epoch-12 batch-107
Running loss of epoch-12 batch-107 = 0.0002399067161604762

Training epoch-12 batch-108
Running loss of epoch-12 batch-108 = 0.00036433886270970106

Training epoch-12 batch-109
Running loss of epoch-12 batch-109 = 0.0015227828407660127

Training epoch-12 batch-110
Running loss of epoch-12 batch-110 = 0.0008090872433967888

Training epoch-12 batch-111
Running loss of epoch-12 batch-111 = 0.0007378550944849849

Training epoch-12 batch-112
Running loss of epoch-12 batch-112 = 0.0033507237676531076

Training epoch-12 batch-113
Running loss of epoch-12 batch-113 = 0.0012091523967683315

Training epoch-12 batch-114
Running loss of epoch-12 batch-114 = 0.0016715305391699076

Training epoch-12 batch-115
Running loss of epoch-12 batch-115 = 0.0004210848128423095

Training epoch-12 batch-116
Running loss of epoch-12 batch-116 = 0.0012303636176511645

Training epoch-12 batch-117
Running loss of epoch-12 batch-117 = 0.0014274736167863011

Training epoch-12 batch-118
Running loss of epoch-12 batch-118 = 0.00331813539378345

Training epoch-12 batch-119
Running loss of epoch-12 batch-119 = 0.000845394330099225

Training epoch-12 batch-120
Running loss of epoch-12 batch-120 = 0.0014014936750754714

Training epoch-12 batch-121
Running loss of epoch-12 batch-121 = 0.0015894805546849966

Training epoch-12 batch-122
Running loss of epoch-12 batch-122 = 0.001275392365641892

Training epoch-12 batch-123
Running loss of epoch-12 batch-123 = 0.0020370054990053177

Training epoch-12 batch-124
Running loss of epoch-12 batch-124 = 0.0011051385663449764

Training epoch-12 batch-125
Running loss of epoch-12 batch-125 = 0.002173523884266615

Training epoch-12 batch-126
Running loss of epoch-12 batch-126 = 0.0031517690513283014

Training epoch-12 batch-127
Running loss of epoch-12 batch-127 = 0.002032650401815772

Training epoch-12 batch-128
Running loss of epoch-12 batch-128 = 0.0009377579553984106

Training epoch-12 batch-129
Running loss of epoch-12 batch-129 = 0.0010628891177475452

Training epoch-12 batch-130
Running loss of epoch-12 batch-130 = 0.0009365260484628379

Training epoch-12 batch-131
Running loss of epoch-12 batch-131 = 0.0002670250250957906

Training epoch-12 batch-132
Running loss of epoch-12 batch-132 = 0.0020700537133961916

Training epoch-12 batch-133
Running loss of epoch-12 batch-133 = 0.0031153252348303795

Training epoch-12 batch-134
Running loss of epoch-12 batch-134 = 0.0005135731771588326

Training epoch-12 batch-135
Running loss of epoch-12 batch-135 = 0.001242758589796722

Training epoch-12 batch-136
Running loss of epoch-12 batch-136 = 0.0020118658430874348

Training epoch-12 batch-137
Running loss of epoch-12 batch-137 = 0.001000462332740426

Training epoch-12 batch-138
Running loss of epoch-12 batch-138 = 0.0018982278415933251

Training epoch-12 batch-139
Running loss of epoch-12 batch-139 = 0.001678090076893568

Training epoch-12 batch-140
Running loss of epoch-12 batch-140 = 0.0022980384528636932

Training epoch-12 batch-141
Running loss of epoch-12 batch-141 = 0.0013918395852670074

Training epoch-12 batch-142
Running loss of epoch-12 batch-142 = 0.0009445949690416455

Training epoch-12 batch-143
Running loss of epoch-12 batch-143 = 0.0017057224176824093

Training epoch-12 batch-144
Running loss of epoch-12 batch-144 = 0.0033300784416496754

Training epoch-12 batch-145
Running loss of epoch-12 batch-145 = 0.000795247033238411

Training epoch-12 batch-146
Running loss of epoch-12 batch-146 = 0.0010763462632894516

Training epoch-12 batch-147
Running loss of epoch-12 batch-147 = 0.004085288383066654

Training epoch-12 batch-148
Running loss of epoch-12 batch-148 = 0.00185135449282825

Training epoch-12 batch-149
Running loss of epoch-12 batch-149 = 0.0006770885083824396

Training epoch-12 batch-150
Running loss of epoch-12 batch-150 = 0.0027340694796293974

Training epoch-12 batch-151
Running loss of epoch-12 batch-151 = 0.0018565607024356723

Training epoch-12 batch-152
Running loss of epoch-12 batch-152 = 0.0018929977668449283

Training epoch-12 batch-153
Running loss of epoch-12 batch-153 = 0.0022131921723484993

Training epoch-12 batch-154
Running loss of epoch-12 batch-154 = 0.0008866143180057406

Training epoch-12 batch-155
Running loss of epoch-12 batch-155 = 0.002780552487820387

Training epoch-12 batch-156
Running loss of epoch-12 batch-156 = 0.002428426407277584

Training epoch-12 batch-157
Running loss of epoch-12 batch-157 = 0.00036696717143058777

Finished training epoch-12.



Average train loss at epoch-12 = 0.0015216926332563161

Started Evaluation

Average val loss at epoch-12 = 3.8296336257144024

Accuracy for classes:
Accuracy for class equals is: 64.36 %
Accuracy for class main is: 61.31 %
Accuracy for class setUp is: 37.05 %
Accuracy for class onCreate is: 29.53 %
Accuracy for class toString is: 49.49 %
Accuracy for class run is: 30.59 %
Accuracy for class hashCode is: 65.54 %
Accuracy for class init is: 34.98 %
Accuracy for class execute is: 14.46 %
Accuracy for class get is: 23.85 %

Overall Accuracy = 41.39 %

Finished Evaluation



Started training epoch-13


Training epoch-13 batch-1
Running loss of epoch-13 batch-1 = 0.0006023182068020105

Training epoch-13 batch-2
Running loss of epoch-13 batch-2 = 0.0012705243425443769

Training epoch-13 batch-3
Running loss of epoch-13 batch-3 = 0.0017919230740517378

Training epoch-13 batch-4
Running loss of epoch-13 batch-4 = 0.0005702166236005723

Training epoch-13 batch-5
Running loss of epoch-13 batch-5 = 0.0002916540252044797

Training epoch-13 batch-6
Running loss of epoch-13 batch-6 = 0.00041317904833704233

Training epoch-13 batch-7
Running loss of epoch-13 batch-7 = 0.0023135317023843527

Training epoch-13 batch-8
Running loss of epoch-13 batch-8 = 0.001033086096867919

Training epoch-13 batch-9
Running loss of epoch-13 batch-9 = 0.003535242285579443

Training epoch-13 batch-10
Running loss of epoch-13 batch-10 = 0.0006642746739089489

Training epoch-13 batch-11
Running loss of epoch-13 batch-11 = 0.0008507925667800009

Training epoch-13 batch-12
Running loss of epoch-13 batch-12 = 0.0012012225342914462

Training epoch-13 batch-13
Running loss of epoch-13 batch-13 = 0.0025909547694027424

Training epoch-13 batch-14
Running loss of epoch-13 batch-14 = 0.0005281905177980661

Training epoch-13 batch-15
Running loss of epoch-13 batch-15 = 0.0017451854655519128

Training epoch-13 batch-16
Running loss of epoch-13 batch-16 = 0.001749659888446331

Training epoch-13 batch-17
Running loss of epoch-13 batch-17 = 0.0024891162756830454

Training epoch-13 batch-18
Running loss of epoch-13 batch-18 = 0.0007390645332634449

Training epoch-13 batch-19
Running loss of epoch-13 batch-19 = 0.0022510504350066185

Training epoch-13 batch-20
Running loss of epoch-13 batch-20 = 0.0017735296860337257

Training epoch-13 batch-21
Running loss of epoch-13 batch-21 = 0.0008615956758148968

Training epoch-13 batch-22
Running loss of epoch-13 batch-22 = 0.0003256790805608034

Training epoch-13 batch-23
Running loss of epoch-13 batch-23 = 0.000636434881016612

Training epoch-13 batch-24
Running loss of epoch-13 batch-24 = 0.0020311172120273113

Training epoch-13 batch-25
Running loss of epoch-13 batch-25 = 0.003625304438173771

Training epoch-13 batch-26
Running loss of epoch-13 batch-26 = 0.0005021488177590072

Training epoch-13 batch-27
Running loss of epoch-13 batch-27 = 0.0002763532684184611

Training epoch-13 batch-28
Running loss of epoch-13 batch-28 = 0.000976703129708767

Training epoch-13 batch-29
Running loss of epoch-13 batch-29 = 0.0016132923774421215

Training epoch-13 batch-30
Running loss of epoch-13 batch-30 = 0.0005240129539743066

Training epoch-13 batch-31
Running loss of epoch-13 batch-31 = 0.0013469287659972906

Training epoch-13 batch-32
Running loss of epoch-13 batch-32 = 0.0019582631066441536

Training epoch-13 batch-33
Running loss of epoch-13 batch-33 = 0.0017991091590374708

Training epoch-13 batch-34
Running loss of epoch-13 batch-34 = 0.0009591502603143454

Training epoch-13 batch-35
Running loss of epoch-13 batch-35 = 0.001385785872116685

Training epoch-13 batch-36
Running loss of epoch-13 batch-36 = 0.0007684223237447441

Training epoch-13 batch-37
Running loss of epoch-13 batch-37 = 0.002484260592609644

Training epoch-13 batch-38
Running loss of epoch-13 batch-38 = 0.0004008847754448652

Training epoch-13 batch-39
Running loss of epoch-13 batch-39 = 0.000975875707808882

Training epoch-13 batch-40
Running loss of epoch-13 batch-40 = 0.00307696801610291

Training epoch-13 batch-41
Running loss of epoch-13 batch-41 = 0.0005899661919102073

Training epoch-13 batch-42
Running loss of epoch-13 batch-42 = 0.0010975911282002926

Training epoch-13 batch-43
Running loss of epoch-13 batch-43 = 0.0008064108551479876

Training epoch-13 batch-44
Running loss of epoch-13 batch-44 = 0.0017361592035740614

Training epoch-13 batch-45
Running loss of epoch-13 batch-45 = 0.0007315539987757802

Training epoch-13 batch-46
Running loss of epoch-13 batch-46 = 0.0009647214319556952

Training epoch-13 batch-47
Running loss of epoch-13 batch-47 = 0.0010264315642416477

Training epoch-13 batch-48
Running loss of epoch-13 batch-48 = 0.0017023369437083602

Training epoch-13 batch-49
Running loss of epoch-13 batch-49 = 0.0010245071025565267

Training epoch-13 batch-50
Running loss of epoch-13 batch-50 = 0.0014579608105123043

Training epoch-13 batch-51
Running loss of epoch-13 batch-51 = 0.0013769500656053424

Training epoch-13 batch-52
Running loss of epoch-13 batch-52 = 0.0012581588234752417

Training epoch-13 batch-53
Running loss of epoch-13 batch-53 = 0.0013528455747291446

Training epoch-13 batch-54
Running loss of epoch-13 batch-54 = 0.00045389385195448995

Training epoch-13 batch-55
Running loss of epoch-13 batch-55 = 0.0008302166243083775

Training epoch-13 batch-56
Running loss of epoch-13 batch-56 = 0.000301035528536886

Training epoch-13 batch-57
Running loss of epoch-13 batch-57 = 0.00022307259496301413

Training epoch-13 batch-58
Running loss of epoch-13 batch-58 = 0.0009614063310436904

Training epoch-13 batch-59
Running loss of epoch-13 batch-59 = 0.001432740711607039

Training epoch-13 batch-60
Running loss of epoch-13 batch-60 = 0.0005201184540055692

Training epoch-13 batch-61
Running loss of epoch-13 batch-61 = 0.0025477795861661434

Training epoch-13 batch-62
Running loss of epoch-13 batch-62 = 0.0016099237836897373

Training epoch-13 batch-63
Running loss of epoch-13 batch-63 = 0.001984292408451438

Training epoch-13 batch-64
Running loss of epoch-13 batch-64 = 0.0018782246625050902

Training epoch-13 batch-65
Running loss of epoch-13 batch-65 = 0.0016482028877362609

Training epoch-13 batch-66
Running loss of epoch-13 batch-66 = 0.0006478489376604557

Training epoch-13 batch-67
Running loss of epoch-13 batch-67 = 0.0011816775659099221

Training epoch-13 batch-68
Running loss of epoch-13 batch-68 = 0.0017897069919854403

Training epoch-13 batch-69
Running loss of epoch-13 batch-69 = 0.0017011883901432157

Training epoch-13 batch-70
Running loss of epoch-13 batch-70 = 0.0021458189003169537

Training epoch-13 batch-71
Running loss of epoch-13 batch-71 = 0.0003996979212388396

Training epoch-13 batch-72
Running loss of epoch-13 batch-72 = 0.000790029764175415

Training epoch-13 batch-73
Running loss of epoch-13 batch-73 = 0.0015984767815098166

Training epoch-13 batch-74
Running loss of epoch-13 batch-74 = 0.0010415358701720834

Training epoch-13 batch-75
Running loss of epoch-13 batch-75 = 0.0024089470971375704

Training epoch-13 batch-76
Running loss of epoch-13 batch-76 = 0.0002854487393051386

Training epoch-13 batch-77
Running loss of epoch-13 batch-77 = 0.0014864971162751317

Training epoch-13 batch-78
Running loss of epoch-13 batch-78 = 0.0013179900124669075

Training epoch-13 batch-79
Running loss of epoch-13 batch-79 = 0.0016105412505567074

Training epoch-13 batch-80
Running loss of epoch-13 batch-80 = 0.000892955285962671

Training epoch-13 batch-81
Running loss of epoch-13 batch-81 = 0.0022024987265467644

Training epoch-13 batch-82
Running loss of epoch-13 batch-82 = 0.0023213913664221764

Training epoch-13 batch-83
Running loss of epoch-13 batch-83 = 0.0003359616966918111

Training epoch-13 batch-84
Running loss of epoch-13 batch-84 = 0.001760278595611453

Training epoch-13 batch-85
Running loss of epoch-13 batch-85 = 0.000715245958417654

Training epoch-13 batch-86
Running loss of epoch-13 batch-86 = 0.0004088387358933687

Training epoch-13 batch-87
Running loss of epoch-13 batch-87 = 0.0014853202737867832

Training epoch-13 batch-88
Running loss of epoch-13 batch-88 = 0.0006185485399328172

Training epoch-13 batch-89
Running loss of epoch-13 batch-89 = 0.003440264845266938

Training epoch-13 batch-90
Running loss of epoch-13 batch-90 = 0.0026462175883352757

Training epoch-13 batch-91
Running loss of epoch-13 batch-91 = 0.0033834653440862894

Training epoch-13 batch-92
Running loss of epoch-13 batch-92 = 0.0007553574978373945

Training epoch-13 batch-93
Running loss of epoch-13 batch-93 = 0.0008526426972821355

Training epoch-13 batch-94
Running loss of epoch-13 batch-94 = 0.0031668422743678093

Training epoch-13 batch-95
Running loss of epoch-13 batch-95 = 0.0016903623472899199

Training epoch-13 batch-96
Running loss of epoch-13 batch-96 = 0.0006515755085274577

Training epoch-13 batch-97
Running loss of epoch-13 batch-97 = 0.0011290183756500483

Training epoch-13 batch-98
Running loss of epoch-13 batch-98 = 0.0010479529155418277

Training epoch-13 batch-99
Running loss of epoch-13 batch-99 = 0.00092412467347458

Training epoch-13 batch-100
Running loss of epoch-13 batch-100 = 0.0003984897630289197

Training epoch-13 batch-101
Running loss of epoch-13 batch-101 = 0.00027646380476653576

Training epoch-13 batch-102
Running loss of epoch-13 batch-102 = 0.0005767761613242328

Training epoch-13 batch-103
Running loss of epoch-13 batch-103 = 0.001432663295418024

Training epoch-13 batch-104
Running loss of epoch-13 batch-104 = 0.0016114460304379463

Training epoch-13 batch-105
Running loss of epoch-13 batch-105 = 0.002684059552848339

Training epoch-13 batch-106
Running loss of epoch-13 batch-106 = 0.0021125683560967445

Training epoch-13 batch-107
Running loss of epoch-13 batch-107 = 0.0023019537329673767

Training epoch-13 batch-108
Running loss of epoch-13 batch-108 = 0.0005897116498090327

Training epoch-13 batch-109
Running loss of epoch-13 batch-109 = 0.00030081102158874273

Training epoch-13 batch-110
Running loss of epoch-13 batch-110 = 0.0010953140445053577

Training epoch-13 batch-111
Running loss of epoch-13 batch-111 = 0.0021577104926109314

Training epoch-13 batch-112
Running loss of epoch-13 batch-112 = 0.0018165732035413384

Training epoch-13 batch-113
Running loss of epoch-13 batch-113 = 0.0007701839786022902

Training epoch-13 batch-114
Running loss of epoch-13 batch-114 = 0.0008779079653322697

Training epoch-13 batch-115
Running loss of epoch-13 batch-115 = 0.0012715214397758245

Training epoch-13 batch-116
Running loss of epoch-13 batch-116 = 0.001127307885326445

Training epoch-13 batch-117
Running loss of epoch-13 batch-117 = 0.0009538389276713133

Training epoch-13 batch-118
Running loss of epoch-13 batch-118 = 0.0024861013516783714

Training epoch-13 batch-119
Running loss of epoch-13 batch-119 = 0.0011345123639330268

Training epoch-13 batch-120
Running loss of epoch-13 batch-120 = 0.002356393728405237

Training epoch-13 batch-121
Running loss of epoch-13 batch-121 = 0.0014315685257315636

Training epoch-13 batch-122
Running loss of epoch-13 batch-122 = 0.0006901731248944998

Training epoch-13 batch-123
Running loss of epoch-13 batch-123 = 0.0014547097962349653

Training epoch-13 batch-124
Running loss of epoch-13 batch-124 = 0.0026023744139820337

Training epoch-13 batch-125
Running loss of epoch-13 batch-125 = 0.0009803082793951035

Training epoch-13 batch-126
Running loss of epoch-13 batch-126 = 0.0017605529865249991

Training epoch-13 batch-127
Running loss of epoch-13 batch-127 = 0.0004131861496716738

Training epoch-13 batch-128
Running loss of epoch-13 batch-128 = 0.0009850631467998028

Training epoch-13 batch-129
Running loss of epoch-13 batch-129 = 0.001085001276805997

Training epoch-13 batch-130
Running loss of epoch-13 batch-130 = 0.0006397718680091202

Training epoch-13 batch-131
Running loss of epoch-13 batch-131 = 0.0013957538176327944

Training epoch-13 batch-132
Running loss of epoch-13 batch-132 = 0.001230390160344541

Training epoch-13 batch-133
Running loss of epoch-13 batch-133 = 0.0010057705221697688

Training epoch-13 batch-134
Running loss of epoch-13 batch-134 = 0.0012218847405165434

Training epoch-13 batch-135
Running loss of epoch-13 batch-135 = 0.004482915624976158

Training epoch-13 batch-136
Running loss of epoch-13 batch-136 = 0.0012863791780546308

Training epoch-13 batch-137
Running loss of epoch-13 batch-137 = 0.0009799395920708776

Training epoch-13 batch-138
Running loss of epoch-13 batch-138 = 0.00020927132572978735

Training epoch-13 batch-139
Running loss of epoch-13 batch-139 = 0.0018660961650311947

Training epoch-13 batch-140
Running loss of epoch-13 batch-140 = 0.0014174726093187928

Training epoch-13 batch-141
Running loss of epoch-13 batch-141 = 0.0015888626221567392

Training epoch-13 batch-142
Running loss of epoch-13 batch-142 = 0.0010698899859562516

Training epoch-13 batch-143
Running loss of epoch-13 batch-143 = 0.0008065119036473334

Training epoch-13 batch-144
Running loss of epoch-13 batch-144 = 0.0018870574422180653

Training epoch-13 batch-145
Running loss of epoch-13 batch-145 = 0.0016186792636290193

Training epoch-13 batch-146
Running loss of epoch-13 batch-146 = 0.000843697227537632

Training epoch-13 batch-147
Running loss of epoch-13 batch-147 = 0.0014513622736558318

Training epoch-13 batch-148
Running loss of epoch-13 batch-148 = 0.001392563572153449

Training epoch-13 batch-149
Running loss of epoch-13 batch-149 = 0.0004858700558543205

Training epoch-13 batch-150
Running loss of epoch-13 batch-150 = 0.00034273392520844936

Training epoch-13 batch-151
Running loss of epoch-13 batch-151 = 0.0011898766970261931

Training epoch-13 batch-152
Running loss of epoch-13 batch-152 = 0.001293216017074883

Training epoch-13 batch-153
Running loss of epoch-13 batch-153 = 0.0009865829488262534

Training epoch-13 batch-154
Running loss of epoch-13 batch-154 = 0.001854298054240644

Training epoch-13 batch-155
Running loss of epoch-13 batch-155 = 0.0010272808140143752

Training epoch-13 batch-156
Running loss of epoch-13 batch-156 = 0.0016956909094005823

Training epoch-13 batch-157
Running loss of epoch-13 batch-157 = 0.006143404170870781

Finished training epoch-13.



Average train loss at epoch-13 = 0.0013428780395537616

Started Evaluation

Average val loss at epoch-13 = 3.4651540690346767

Accuracy for classes:
Accuracy for class equals is: 64.69 %
Accuracy for class main is: 37.05 %
Accuracy for class setUp is: 40.49 %
Accuracy for class onCreate is: 51.71 %
Accuracy for class toString is: 37.54 %
Accuracy for class run is: 23.06 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 23.09 %
Accuracy for class execute is: 28.11 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 42.83 %

Finished Evaluation



Started training epoch-14


Training epoch-14 batch-1
Running loss of epoch-14 batch-1 = 0.002467808313667774

Training epoch-14 batch-2
Running loss of epoch-14 batch-2 = 0.0009177932515740395

Training epoch-14 batch-3
Running loss of epoch-14 batch-3 = 0.002939485013484955

Training epoch-14 batch-4
Running loss of epoch-14 batch-4 = 0.0004837543820030987

Training epoch-14 batch-5
Running loss of epoch-14 batch-5 = 0.001196906901896

Training epoch-14 batch-6
Running loss of epoch-14 batch-6 = 0.0007420395850203931

Training epoch-14 batch-7
Running loss of epoch-14 batch-7 = 0.0049623651430010796

Training epoch-14 batch-8
Running loss of epoch-14 batch-8 = 0.0008503753924742341

Training epoch-14 batch-9
Running loss of epoch-14 batch-9 = 0.0006537566659972072

Training epoch-14 batch-10
Running loss of epoch-14 batch-10 = 0.0006008206401020288

Training epoch-14 batch-11
Running loss of epoch-14 batch-11 = 0.0016591992462053895

Training epoch-14 batch-12
Running loss of epoch-14 batch-12 = 0.0015811821212992072

Training epoch-14 batch-13
Running loss of epoch-14 batch-13 = 0.00046974699944257736

Training epoch-14 batch-14
Running loss of epoch-14 batch-14 = 0.0033922570291906595

Training epoch-14 batch-15
Running loss of epoch-14 batch-15 = 0.0016699556726962328

Training epoch-14 batch-16
Running loss of epoch-14 batch-16 = 0.000816575950011611

Training epoch-14 batch-17
Running loss of epoch-14 batch-17 = 0.0013478492619469762

Training epoch-14 batch-18
Running loss of epoch-14 batch-18 = 0.0032818152103573084

Training epoch-14 batch-19
Running loss of epoch-14 batch-19 = 0.000166617042850703

Training epoch-14 batch-20
Running loss of epoch-14 batch-20 = 0.002772871870547533

Training epoch-14 batch-21
Running loss of epoch-14 batch-21 = 0.0008590063080191612

Training epoch-14 batch-22
Running loss of epoch-14 batch-22 = 0.0003862529410980642

Training epoch-14 batch-23
Running loss of epoch-14 batch-23 = 0.0019310268107801676

Training epoch-14 batch-24
Running loss of epoch-14 batch-24 = 0.001022167969495058

Training epoch-14 batch-25
Running loss of epoch-14 batch-25 = 0.001306507852859795

Training epoch-14 batch-26
Running loss of epoch-14 batch-26 = 0.0018679103814065456

Training epoch-14 batch-27
Running loss of epoch-14 batch-27 = 0.0011530625633895397

Training epoch-14 batch-28
Running loss of epoch-14 batch-28 = 0.0010799337178468704

Training epoch-14 batch-29
Running loss of epoch-14 batch-29 = 0.0004684212035499513

Training epoch-14 batch-30
Running loss of epoch-14 batch-30 = 0.0012733209878206253

Training epoch-14 batch-31
Running loss of epoch-14 batch-31 = 0.002381468890234828

Training epoch-14 batch-32
Running loss of epoch-14 batch-32 = 0.00037892383988946676

Training epoch-14 batch-33
Running loss of epoch-14 batch-33 = 0.001032872823998332

Training epoch-14 batch-34
Running loss of epoch-14 batch-34 = 0.001494749914854765

Training epoch-14 batch-35
Running loss of epoch-14 batch-35 = 0.00026662577874958515

Training epoch-14 batch-36
Running loss of epoch-14 batch-36 = 0.0022624151315540075

Training epoch-14 batch-37
Running loss of epoch-14 batch-37 = 0.0026103120762854815

Training epoch-14 batch-38
Running loss of epoch-14 batch-38 = 0.0003408794873394072

Training epoch-14 batch-39
Running loss of epoch-14 batch-39 = 0.0006767901359125972

Training epoch-14 batch-40
Running loss of epoch-14 batch-40 = 0.0012316706124693155

Training epoch-14 batch-41
Running loss of epoch-14 batch-41 = 0.003968818578869104

Training epoch-14 batch-42
Running loss of epoch-14 batch-42 = 0.0007866662926971912

Training epoch-14 batch-43
Running loss of epoch-14 batch-43 = 0.0010260471608489752

Training epoch-14 batch-44
Running loss of epoch-14 batch-44 = 0.0005773912416771054

Training epoch-14 batch-45
Running loss of epoch-14 batch-45 = 0.001081688329577446

Training epoch-14 batch-46
Running loss of epoch-14 batch-46 = 0.0004346930072642863

Training epoch-14 batch-47
Running loss of epoch-14 batch-47 = 0.0013569608563557267

Training epoch-14 batch-48
Running loss of epoch-14 batch-48 = 0.00078621442662552

Training epoch-14 batch-49
Running loss of epoch-14 batch-49 = 0.0012303544208407402

Training epoch-14 batch-50
Running loss of epoch-14 batch-50 = 0.0006017159903421998

Training epoch-14 batch-51
Running loss of epoch-14 batch-51 = 0.0011683224001899362

Training epoch-14 batch-52
Running loss of epoch-14 batch-52 = 0.0010801955359056592

Training epoch-14 batch-53
Running loss of epoch-14 batch-53 = 0.002746041864156723

Training epoch-14 batch-54
Running loss of epoch-14 batch-54 = 0.0018241370562463999

Training epoch-14 batch-55
Running loss of epoch-14 batch-55 = 0.0008231897954829037

Training epoch-14 batch-56
Running loss of epoch-14 batch-56 = 0.0014158275444060564

Training epoch-14 batch-57
Running loss of epoch-14 batch-57 = 0.0013072851579636335

Training epoch-14 batch-58
Running loss of epoch-14 batch-58 = 0.0006249236175790429

Training epoch-14 batch-59
Running loss of epoch-14 batch-59 = 0.0010596573119983077

Training epoch-14 batch-60
Running loss of epoch-14 batch-60 = 0.0016373213147744536

Training epoch-14 batch-61
Running loss of epoch-14 batch-61 = 0.000585731235332787

Training epoch-14 batch-62
Running loss of epoch-14 batch-62 = 0.001095616607926786

Training epoch-14 batch-63
Running loss of epoch-14 batch-63 = 0.001994046848267317

Training epoch-14 batch-64
Running loss of epoch-14 batch-64 = 0.0023166704922914505

Training epoch-14 batch-65
Running loss of epoch-14 batch-65 = 0.0014151616487652063

Training epoch-14 batch-66
Running loss of epoch-14 batch-66 = 0.0013083069352433085

Training epoch-14 batch-67
Running loss of epoch-14 batch-67 = 0.000572592718526721

Training epoch-14 batch-68
Running loss of epoch-14 batch-68 = 0.0013569161528721452

Training epoch-14 batch-69
Running loss of epoch-14 batch-69 = 0.00027243210934102535

Training epoch-14 batch-70
Running loss of epoch-14 batch-70 = 0.00883036945015192

Training epoch-14 batch-71
Running loss of epoch-14 batch-71 = 0.001960250549018383

Training epoch-14 batch-72
Running loss of epoch-14 batch-72 = 0.0011785861570388079

Training epoch-14 batch-73
Running loss of epoch-14 batch-73 = 0.0008524000877514482

Training epoch-14 batch-74
Running loss of epoch-14 batch-74 = 0.00061239447677508

Training epoch-14 batch-75
Running loss of epoch-14 batch-75 = 0.0015838206745684147

Training epoch-14 batch-76
Running loss of epoch-14 batch-76 = 0.005888515617698431

Training epoch-14 batch-77
Running loss of epoch-14 batch-77 = 0.0003866615006700158

Training epoch-14 batch-78
Running loss of epoch-14 batch-78 = 0.00045691424747928977

Training epoch-14 batch-79
Running loss of epoch-14 batch-79 = 0.00028197048231959343

Training epoch-14 batch-80
Running loss of epoch-14 batch-80 = 0.00036131194792687893

Training epoch-14 batch-81
Running loss of epoch-14 batch-81 = 0.0008501000702381134

Training epoch-14 batch-82
Running loss of epoch-14 batch-82 = 0.00223430129699409

Training epoch-14 batch-83
Running loss of epoch-14 batch-83 = 0.002606532536447048

Training epoch-14 batch-84
Running loss of epoch-14 batch-84 = 0.0011359662748873234

Training epoch-14 batch-85
Running loss of epoch-14 batch-85 = 0.0016982399392873049

Training epoch-14 batch-86
Running loss of epoch-14 batch-86 = 0.001703902380540967

Training epoch-14 batch-87
Running loss of epoch-14 batch-87 = 0.0014630855293944478

Training epoch-14 batch-88
Running loss of epoch-14 batch-88 = 0.0008476012735627592

Training epoch-14 batch-89
Running loss of epoch-14 batch-89 = 0.0018705782713368535

Training epoch-14 batch-90
Running loss of epoch-14 batch-90 = 0.0010128209833055735

Training epoch-14 batch-91
Running loss of epoch-14 batch-91 = 0.006310359109193087

Training epoch-14 batch-92
Running loss of epoch-14 batch-92 = 0.0024947402998805046

Training epoch-14 batch-93
Running loss of epoch-14 batch-93 = 0.0012394358636811376

Training epoch-14 batch-94
Running loss of epoch-14 batch-94 = 0.003273060079663992

Training epoch-14 batch-95
Running loss of epoch-14 batch-95 = 0.0015251835575327277

Training epoch-14 batch-96
Running loss of epoch-14 batch-96 = 0.0032576911617070436

Training epoch-14 batch-97
Running loss of epoch-14 batch-97 = 0.0015829458134248853

Training epoch-14 batch-98
Running loss of epoch-14 batch-98 = 0.0020595837850123644

Training epoch-14 batch-99
Running loss of epoch-14 batch-99 = 0.0010307906195521355

Training epoch-14 batch-100
Running loss of epoch-14 batch-100 = 0.0008446493302471936

Training epoch-14 batch-101
Running loss of epoch-14 batch-101 = 0.0008834291948005557

Training epoch-14 batch-102
Running loss of epoch-14 batch-102 = 0.0011652718530967832

Training epoch-14 batch-103
Running loss of epoch-14 batch-103 = 0.0025226545985788107

Training epoch-14 batch-104
Running loss of epoch-14 batch-104 = 0.0004040325293317437

Training epoch-14 batch-105
Running loss of epoch-14 batch-105 = 0.0002949534682556987

Training epoch-14 batch-106
Running loss of epoch-14 batch-106 = 0.0001445453381165862

Training epoch-14 batch-107
Running loss of epoch-14 batch-107 = 0.0006558962049894035

Training epoch-14 batch-108
Running loss of epoch-14 batch-108 = 0.0014717887388542295

Training epoch-14 batch-109
Running loss of epoch-14 batch-109 = 0.0012435723328962922

Training epoch-14 batch-110
Running loss of epoch-14 batch-110 = 0.0007548007415607572

Training epoch-14 batch-111
Running loss of epoch-14 batch-111 = 0.0017149978084489703

Training epoch-14 batch-112
Running loss of epoch-14 batch-112 = 0.0024590427055954933

Training epoch-14 batch-113
Running loss of epoch-14 batch-113 = 0.001019962364807725

Training epoch-14 batch-114
Running loss of epoch-14 batch-114 = 0.0003140001790598035

Training epoch-14 batch-115
Running loss of epoch-14 batch-115 = 0.0004644733853638172

Training epoch-14 batch-116
Running loss of epoch-14 batch-116 = 0.0018978321459144354

Training epoch-14 batch-117
Running loss of epoch-14 batch-117 = 0.0004777269787155092

Training epoch-14 batch-118
Running loss of epoch-14 batch-118 = 0.0018060259753838181

Training epoch-14 batch-119
Running loss of epoch-14 batch-119 = 0.0012049366487190127

Training epoch-14 batch-120
Running loss of epoch-14 batch-120 = 0.0028036509174853563

Training epoch-14 batch-121
Running loss of epoch-14 batch-121 = 0.0003385861637070775

Training epoch-14 batch-122
Running loss of epoch-14 batch-122 = 0.0025488692335784435

Training epoch-14 batch-123
Running loss of epoch-14 batch-123 = 0.0001294114626944065

Training epoch-14 batch-124
Running loss of epoch-14 batch-124 = 0.001081266556866467

Training epoch-14 batch-125
Running loss of epoch-14 batch-125 = 0.0019209585152566433

Training epoch-14 batch-126
Running loss of epoch-14 batch-126 = 0.0006952373078092933

Training epoch-14 batch-127
Running loss of epoch-14 batch-127 = 0.00040444324258714914

Training epoch-14 batch-128
Running loss of epoch-14 batch-128 = 0.0005739356856793165

Training epoch-14 batch-129
Running loss of epoch-14 batch-129 = 0.0013756239786744118

Training epoch-14 batch-130
Running loss of epoch-14 batch-130 = 0.0006029951618984342

Training epoch-14 batch-131
Running loss of epoch-14 batch-131 = 0.001477324403822422

Training epoch-14 batch-132
Running loss of epoch-14 batch-132 = 0.00413121422752738

Training epoch-14 batch-133
Running loss of epoch-14 batch-133 = 0.001191145391203463

Training epoch-14 batch-134
Running loss of epoch-14 batch-134 = 0.00032079714583233

Training epoch-14 batch-135
Running loss of epoch-14 batch-135 = 0.0016180635429918766

Training epoch-14 batch-136
Running loss of epoch-14 batch-136 = 0.0008214258123189211

Training epoch-14 batch-137
Running loss of epoch-14 batch-137 = 0.0015193549916148186

Training epoch-14 batch-138
Running loss of epoch-14 batch-138 = 0.0007618982926942408

Training epoch-14 batch-139
Running loss of epoch-14 batch-139 = 0.0014370583230629563

Training epoch-14 batch-140
Running loss of epoch-14 batch-140 = 0.0036107467021793127

Training epoch-14 batch-141
Running loss of epoch-14 batch-141 = 0.0005925652803853154

Training epoch-14 batch-142
Running loss of epoch-14 batch-142 = 0.0015907736960798502

Training epoch-14 batch-143
Running loss of epoch-14 batch-143 = 0.002900633029639721

Training epoch-14 batch-144
Running loss of epoch-14 batch-144 = 0.0011072821216657758

Training epoch-14 batch-145
Running loss of epoch-14 batch-145 = 0.0004494029562920332

Training epoch-14 batch-146
Running loss of epoch-14 batch-146 = 0.0021657829638570547

Training epoch-14 batch-147
Running loss of epoch-14 batch-147 = 0.0008963289437815547

Training epoch-14 batch-148
Running loss of epoch-14 batch-148 = 0.0003108873497694731

Training epoch-14 batch-149
Running loss of epoch-14 batch-149 = 0.0009214933961629868

Training epoch-14 batch-150
Running loss of epoch-14 batch-150 = 0.00025423953775316477

Training epoch-14 batch-151
Running loss of epoch-14 batch-151 = 0.0018530741799622774

Training epoch-14 batch-152
Running loss of epoch-14 batch-152 = 0.0013056352036073804

Training epoch-14 batch-153
Running loss of epoch-14 batch-153 = 0.0017699014861136675

Training epoch-14 batch-154
Running loss of epoch-14 batch-154 = 0.00042150274384766817

Training epoch-14 batch-155
Running loss of epoch-14 batch-155 = 0.0005990671343170106

Training epoch-14 batch-156
Running loss of epoch-14 batch-156 = 0.0012021338334307075

Training epoch-14 batch-157
Running loss of epoch-14 batch-157 = 0.0015431996434926987

Finished training epoch-14.



Average train loss at epoch-14 = 0.0014328168906271458

Started Evaluation

Average val loss at epoch-14 = 3.628026966985903

Accuracy for classes:
Accuracy for class equals is: 63.20 %
Accuracy for class main is: 44.26 %
Accuracy for class setUp is: 63.93 %
Accuracy for class onCreate is: 39.45 %
Accuracy for class toString is: 30.72 %
Accuracy for class run is: 42.92 %
Accuracy for class hashCode is: 85.39 %
Accuracy for class init is: 23.54 %
Accuracy for class execute is: 16.87 %
Accuracy for class get is: 22.05 %

Overall Accuracy = 44.40 %

Finished Evaluation



Started training epoch-15


Training epoch-15 batch-1
Running loss of epoch-15 batch-1 = 0.0008308523683808744

Training epoch-15 batch-2
Running loss of epoch-15 batch-2 = 0.0003980255569331348

Training epoch-15 batch-3
Running loss of epoch-15 batch-3 = 0.00038515948108397424

Training epoch-15 batch-4
Running loss of epoch-15 batch-4 = 0.0004757480346597731

Training epoch-15 batch-5
Running loss of epoch-15 batch-5 = 0.00016454432625323534

Training epoch-15 batch-6
Running loss of epoch-15 batch-6 = 0.00014800566714257002

Training epoch-15 batch-7
Running loss of epoch-15 batch-7 = 0.0024524519685655832

Training epoch-15 batch-8
Running loss of epoch-15 batch-8 = 0.0016641000984236598

Training epoch-15 batch-9
Running loss of epoch-15 batch-9 = 0.0005109771736897528

Training epoch-15 batch-10
Running loss of epoch-15 batch-10 = 0.00033052265644073486

Training epoch-15 batch-11
Running loss of epoch-15 batch-11 = 0.0008706488879397511

Training epoch-15 batch-12
Running loss of epoch-15 batch-12 = 0.0011541862040758133

Training epoch-15 batch-13
Running loss of epoch-15 batch-13 = 0.0007047424442134798

Training epoch-15 batch-14
Running loss of epoch-15 batch-14 = 0.00040866254130378366

Training epoch-15 batch-15
Running loss of epoch-15 batch-15 = 0.0004250258207321167

Training epoch-15 batch-16
Running loss of epoch-15 batch-16 = 0.0010271425126120448

Training epoch-15 batch-17
Running loss of epoch-15 batch-17 = 0.0011167394695803523

Training epoch-15 batch-18
Running loss of epoch-15 batch-18 = 0.00020564650185406208

Training epoch-15 batch-19
Running loss of epoch-15 batch-19 = 0.0009015584364533424

Training epoch-15 batch-20
Running loss of epoch-15 batch-20 = 0.00014286069199442863

Training epoch-15 batch-21
Running loss of epoch-15 batch-21 = 0.0004978112410753965

Training epoch-15 batch-22
Running loss of epoch-15 batch-22 = 0.0008582179434597492

Training epoch-15 batch-23
Running loss of epoch-15 batch-23 = 0.0004342920146882534

Training epoch-15 batch-24
Running loss of epoch-15 batch-24 = 0.0005701419431716204

Training epoch-15 batch-25
Running loss of epoch-15 batch-25 = 0.0006263622781261802

Training epoch-15 batch-26
Running loss of epoch-15 batch-26 = 0.0021397052332758904

Training epoch-15 batch-27
Running loss of epoch-15 batch-27 = 0.0006046243943274021

Training epoch-15 batch-28
Running loss of epoch-15 batch-28 = 0.0005683087510988116

Training epoch-15 batch-29
Running loss of epoch-15 batch-29 = 0.0002594654797576368

Training epoch-15 batch-30
Running loss of epoch-15 batch-30 = 0.0004059100174345076

Training epoch-15 batch-31
Running loss of epoch-15 batch-31 = 0.0005673287087120116

Training epoch-15 batch-32
Running loss of epoch-15 batch-32 = 0.00012826977763324976

Training epoch-15 batch-33
Running loss of epoch-15 batch-33 = 0.0007226887973956764

Training epoch-15 batch-34
Running loss of epoch-15 batch-34 = 0.0016033605206757784

Training epoch-15 batch-35
Running loss of epoch-15 batch-35 = 5.6210788898169994e-05

Training epoch-15 batch-36
Running loss of epoch-15 batch-36 = 0.0002843050751835108

Training epoch-15 batch-37
Running loss of epoch-15 batch-37 = 0.0013050877023488283

Training epoch-15 batch-38
Running loss of epoch-15 batch-38 = 0.00010994717013090849

Training epoch-15 batch-39
Running loss of epoch-15 batch-39 = 0.001566738705150783

Training epoch-15 batch-40
Running loss of epoch-15 batch-40 = 0.0004354231059551239

Training epoch-15 batch-41
Running loss of epoch-15 batch-41 = 0.0015341788530349731

Training epoch-15 batch-42
Running loss of epoch-15 batch-42 = 0.0006761616095900536

Training epoch-15 batch-43
Running loss of epoch-15 batch-43 = 0.0007848658715374768

Training epoch-15 batch-44
Running loss of epoch-15 batch-44 = 0.0005509110051207244

Training epoch-15 batch-45
Running loss of epoch-15 batch-45 = 0.0004506499390117824

Training epoch-15 batch-46
Running loss of epoch-15 batch-46 = 8.888624142855406e-05

Training epoch-15 batch-47
Running loss of epoch-15 batch-47 = 0.00157726532779634

Training epoch-15 batch-48
Running loss of epoch-15 batch-48 = 0.0005163221503607929

Training epoch-15 batch-49
Running loss of epoch-15 batch-49 = 0.0009025991894304752

Training epoch-15 batch-50
Running loss of epoch-15 batch-50 = 0.00025874091079458594

Training epoch-15 batch-51
Running loss of epoch-15 batch-51 = 0.00024783157277852297

Training epoch-15 batch-52
Running loss of epoch-15 batch-52 = 0.00014066440053284168

Training epoch-15 batch-53
Running loss of epoch-15 batch-53 = 0.0009520003222860396

Training epoch-15 batch-54
Running loss of epoch-15 batch-54 = 0.0007102309609763324

Training epoch-15 batch-55
Running loss of epoch-15 batch-55 = 0.0010188883170485497

Training epoch-15 batch-56
Running loss of epoch-15 batch-56 = 0.0001521972008049488

Training epoch-15 batch-57
Running loss of epoch-15 batch-57 = 0.0005251693073660135

Training epoch-15 batch-58
Running loss of epoch-15 batch-58 = 0.0002121202414855361

Training epoch-15 batch-59
Running loss of epoch-15 batch-59 = 0.00025028089294210076

Training epoch-15 batch-60
Running loss of epoch-15 batch-60 = 0.0002134281676262617

Training epoch-15 batch-61
Running loss of epoch-15 batch-61 = 0.00021800113609060645

Training epoch-15 batch-62
Running loss of epoch-15 batch-62 = 0.0003820769488811493

Training epoch-15 batch-63
Running loss of epoch-15 batch-63 = 0.0003632741281762719

Training epoch-15 batch-64
Running loss of epoch-15 batch-64 = 0.0006135880830697715

Training epoch-15 batch-65
Running loss of epoch-15 batch-65 = 0.0008062849519774318

Training epoch-15 batch-66
Running loss of epoch-15 batch-66 = 0.0014823488891124725

Training epoch-15 batch-67
Running loss of epoch-15 batch-67 = 0.0003650815924629569

Training epoch-15 batch-68
Running loss of epoch-15 batch-68 = 0.0010579568333923817

Training epoch-15 batch-69
Running loss of epoch-15 batch-69 = 0.00041968346340581775

Training epoch-15 batch-70
Running loss of epoch-15 batch-70 = 0.0017353487201035023

Training epoch-15 batch-71
Running loss of epoch-15 batch-71 = 0.0006302812835201621

Training epoch-15 batch-72
Running loss of epoch-15 batch-72 = 0.0002567649935372174

Training epoch-15 batch-73
Running loss of epoch-15 batch-73 = 0.0020562459249049425

Training epoch-15 batch-74
Running loss of epoch-15 batch-74 = 0.0010163295082747936

Training epoch-15 batch-75
Running loss of epoch-15 batch-75 = 0.0014965315349400043

Training epoch-15 batch-76
Running loss of epoch-15 batch-76 = 0.000337701931130141

Training epoch-15 batch-77
Running loss of epoch-15 batch-77 = 0.0006200196221470833

Training epoch-15 batch-78
Running loss of epoch-15 batch-78 = 0.001188505906611681

Training epoch-15 batch-79
Running loss of epoch-15 batch-79 = 0.0007532950839959085

Training epoch-15 batch-80
Running loss of epoch-15 batch-80 = 0.0009613482397980988

Training epoch-15 batch-81
Running loss of epoch-15 batch-81 = 0.0013013267889618874

Training epoch-15 batch-82
Running loss of epoch-15 batch-82 = 0.0005397744243964553

Training epoch-15 batch-83
Running loss of epoch-15 batch-83 = 0.0001078025670722127

Training epoch-15 batch-84
Running loss of epoch-15 batch-84 = 0.0005448485026136041

Training epoch-15 batch-85
Running loss of epoch-15 batch-85 = 0.0011692771222442389

Training epoch-15 batch-86
Running loss of epoch-15 batch-86 = 0.00047691742656752467

Training epoch-15 batch-87
Running loss of epoch-15 batch-87 = 0.0012960243038833141

Training epoch-15 batch-88
Running loss of epoch-15 batch-88 = 0.000892045209184289

Training epoch-15 batch-89
Running loss of epoch-15 batch-89 = 0.0009277053759433329

Training epoch-15 batch-90
Running loss of epoch-15 batch-90 = 0.00044625834561884403

Training epoch-15 batch-91
Running loss of epoch-15 batch-91 = 0.0009208234259858727

Training epoch-15 batch-92
Running loss of epoch-15 batch-92 = 0.0014115091180428863

Training epoch-15 batch-93
Running loss of epoch-15 batch-93 = 0.00033061904832720757

Training epoch-15 batch-94
Running loss of epoch-15 batch-94 = 0.0001562422839924693

Training epoch-15 batch-95
Running loss of epoch-15 batch-95 = 0.00032163539435714483

Training epoch-15 batch-96
Running loss of epoch-15 batch-96 = 0.0006601468776352704

Training epoch-15 batch-97
Running loss of epoch-15 batch-97 = 0.00020349037367850542

Training epoch-15 batch-98
Running loss of epoch-15 batch-98 = 0.00041201728163287044

Training epoch-15 batch-99
Running loss of epoch-15 batch-99 = 0.0002154314424842596

Training epoch-15 batch-100
Running loss of epoch-15 batch-100 = 6.530981045216322e-05

Training epoch-15 batch-101
Running loss of epoch-15 batch-101 = 0.0005529656773433089

Training epoch-15 batch-102
Running loss of epoch-15 batch-102 = 0.0012126998044550419

Training epoch-15 batch-103
Running loss of epoch-15 batch-103 = 0.0007153860642574728

Training epoch-15 batch-104
Running loss of epoch-15 batch-104 = 0.0006975048454478383

Training epoch-15 batch-105
Running loss of epoch-15 batch-105 = 0.0004989025183022022

Training epoch-15 batch-106
Running loss of epoch-15 batch-106 = 0.00036386799183674157

Training epoch-15 batch-107
Running loss of epoch-15 batch-107 = 0.0002859479282051325

Training epoch-15 batch-108
Running loss of epoch-15 batch-108 = 0.0002825831179507077

Training epoch-15 batch-109
Running loss of epoch-15 batch-109 = 0.0014786958927288651

Training epoch-15 batch-110
Running loss of epoch-15 batch-110 = 0.00039222114719450474

Training epoch-15 batch-111
Running loss of epoch-15 batch-111 = 0.0002246466465294361

Training epoch-15 batch-112
Running loss of epoch-15 batch-112 = 0.0008108653128147125

Training epoch-15 batch-113
Running loss of epoch-15 batch-113 = 8.147733751684427e-05

Training epoch-15 batch-114
Running loss of epoch-15 batch-114 = 0.000486809469293803

Training epoch-15 batch-115
Running loss of epoch-15 batch-115 = 0.0012084288755431771

Training epoch-15 batch-116
Running loss of epoch-15 batch-116 = 0.0009141262853518128

Training epoch-15 batch-117
Running loss of epoch-15 batch-117 = 0.0004787167417816818

Training epoch-15 batch-118
Running loss of epoch-15 batch-118 = 0.00024354783818125725

Training epoch-15 batch-119
Running loss of epoch-15 batch-119 = 0.0006890420336276293

Training epoch-15 batch-120
Running loss of epoch-15 batch-120 = 0.002107585547491908

Training epoch-15 batch-121
Running loss of epoch-15 batch-121 = 0.00012209452688694

Training epoch-15 batch-122
Running loss of epoch-15 batch-122 = 0.000855091551784426

Training epoch-15 batch-123
Running loss of epoch-15 batch-123 = 0.00032851676223799586

Training epoch-15 batch-124
Running loss of epoch-15 batch-124 = 0.002046228852123022

Training epoch-15 batch-125
Running loss of epoch-15 batch-125 = 0.0004973124014213681

Training epoch-15 batch-126
Running loss of epoch-15 batch-126 = 0.0005214961711317301

Training epoch-15 batch-127
Running loss of epoch-15 batch-127 = 0.00012288510333746672

Training epoch-15 batch-128
Running loss of epoch-15 batch-128 = 0.000941513862926513

Training epoch-15 batch-129
Running loss of epoch-15 batch-129 = 0.00032263644970953465

Training epoch-15 batch-130
Running loss of epoch-15 batch-130 = 0.00013573269825428724

Training epoch-15 batch-131
Running loss of epoch-15 batch-131 = 0.00014385103713721037

Training epoch-15 batch-132
Running loss of epoch-15 batch-132 = 0.0022644507698714733

Training epoch-15 batch-133
Running loss of epoch-15 batch-133 = 0.0002829458680935204

Training epoch-15 batch-134
Running loss of epoch-15 batch-134 = 0.0007567894645035267

Training epoch-15 batch-135
Running loss of epoch-15 batch-135 = 0.0003020205767825246

Training epoch-15 batch-136
Running loss of epoch-15 batch-136 = 0.0006627300172112882

Training epoch-15 batch-137
Running loss of epoch-15 batch-137 = 0.0006141177145764232

Training epoch-15 batch-138
Running loss of epoch-15 batch-138 = 0.00035085203126072884

Training epoch-15 batch-139
Running loss of epoch-15 batch-139 = 0.0004233297659084201

Training epoch-15 batch-140
Running loss of epoch-15 batch-140 = 0.0021709229331463575

Training epoch-15 batch-141
Running loss of epoch-15 batch-141 = 0.00040233146864920855

Training epoch-15 batch-142
Running loss of epoch-15 batch-142 = 0.00030403153505176306

Training epoch-15 batch-143
Running loss of epoch-15 batch-143 = 0.000867130933329463

Training epoch-15 batch-144
Running loss of epoch-15 batch-144 = 0.00019164173863828182

Training epoch-15 batch-145
Running loss of epoch-15 batch-145 = 7.020297925919294e-05

Training epoch-15 batch-146
Running loss of epoch-15 batch-146 = 0.0013319805730134249

Training epoch-15 batch-147
Running loss of epoch-15 batch-147 = 0.0017330212285742164

Training epoch-15 batch-148
Running loss of epoch-15 batch-148 = 0.0007418605382554233

Training epoch-15 batch-149
Running loss of epoch-15 batch-149 = 0.0007950814324431121

Training epoch-15 batch-150
Running loss of epoch-15 batch-150 = 0.0011978161055594683

Training epoch-15 batch-151
Running loss of epoch-15 batch-151 = 0.0024021617136895657

Training epoch-15 batch-152
Running loss of epoch-15 batch-152 = 0.0012918886495754123

Training epoch-15 batch-153
Running loss of epoch-15 batch-153 = 0.0011236360296607018

Training epoch-15 batch-154
Running loss of epoch-15 batch-154 = 0.0008376111509278417

Training epoch-15 batch-155
Running loss of epoch-15 batch-155 = 0.000961913843639195

Training epoch-15 batch-156
Running loss of epoch-15 batch-156 = 0.0011153094237670302

Training epoch-15 batch-157
Running loss of epoch-15 batch-157 = 0.00012911297380924225

Finished training epoch-15.



Average train loss at epoch-15 = 0.0007166496261954308

Started Evaluation

Average val loss at epoch-15 = 3.7555914834926

Accuracy for classes:
Accuracy for class equals is: 70.30 %
Accuracy for class main is: 56.39 %
Accuracy for class setUp is: 52.30 %
Accuracy for class onCreate is: 38.17 %
Accuracy for class toString is: 47.78 %
Accuracy for class run is: 31.05 %
Accuracy for class hashCode is: 76.40 %
Accuracy for class init is: 12.56 %
Accuracy for class execute is: 16.87 %
Accuracy for class get is: 39.49 %

Overall Accuracy = 44.96 %

Finished Evaluation



Started training epoch-16


Training epoch-16 batch-1
Running loss of epoch-16 batch-1 = 0.0002989860367961228

Training epoch-16 batch-2
Running loss of epoch-16 batch-2 = 0.00034518318716436625

Training epoch-16 batch-3
Running loss of epoch-16 batch-3 = 0.00033446395536884665

Training epoch-16 batch-4
Running loss of epoch-16 batch-4 = 0.0002205371274612844

Training epoch-16 batch-5
Running loss of epoch-16 batch-5 = 0.0008150663925334811

Training epoch-16 batch-6
Running loss of epoch-16 batch-6 = 0.0006777454982511699

Training epoch-16 batch-7
Running loss of epoch-16 batch-7 = 0.0015688969288021326

Training epoch-16 batch-8
Running loss of epoch-16 batch-8 = 0.00039720095810480416

Training epoch-16 batch-9
Running loss of epoch-16 batch-9 = 0.0006708573782816529

Training epoch-16 batch-10
Running loss of epoch-16 batch-10 = 0.00014666002243757248

Training epoch-16 batch-11
Running loss of epoch-16 batch-11 = 0.00013280438724905252

Training epoch-16 batch-12
Running loss of epoch-16 batch-12 = 0.00020653242245316505

Training epoch-16 batch-13
Running loss of epoch-16 batch-13 = 0.0007006103405728936

Training epoch-16 batch-14
Running loss of epoch-16 batch-14 = 0.0004408600216265768

Training epoch-16 batch-15
Running loss of epoch-16 batch-15 = 0.00040114414878189564

Training epoch-16 batch-16
Running loss of epoch-16 batch-16 = 0.00018990045646205544

Training epoch-16 batch-17
Running loss of epoch-16 batch-17 = 0.0011360641801729798

Training epoch-16 batch-18
Running loss of epoch-16 batch-18 = 0.0005355728790163994

Training epoch-16 batch-19
Running loss of epoch-16 batch-19 = 0.00038142193807289004

Training epoch-16 batch-20
Running loss of epoch-16 batch-20 = 0.0001508326968178153

Training epoch-16 batch-21
Running loss of epoch-16 batch-21 = 0.00017245614435523748

Training epoch-16 batch-22
Running loss of epoch-16 batch-22 = 0.00024515273980796337

Training epoch-16 batch-23
Running loss of epoch-16 batch-23 = 0.0010460895719006658

Training epoch-16 batch-24
Running loss of epoch-16 batch-24 = 0.0016384499613195658

Training epoch-16 batch-25
Running loss of epoch-16 batch-25 = 0.00010558555368334055

Training epoch-16 batch-26
Running loss of epoch-16 batch-26 = 9.2262402176857e-05

Training epoch-16 batch-27
Running loss of epoch-16 batch-27 = 0.0006709313602186739

Training epoch-16 batch-28
Running loss of epoch-16 batch-28 = 0.00010030309204012156

Training epoch-16 batch-29
Running loss of epoch-16 batch-29 = 0.0009855730459094048

Training epoch-16 batch-30
Running loss of epoch-16 batch-30 = 0.00046864873729646206

Training epoch-16 batch-31
Running loss of epoch-16 batch-31 = 0.0007024621008895338

Training epoch-16 batch-32
Running loss of epoch-16 batch-32 = 2.6014866307377815e-05

Training epoch-16 batch-33
Running loss of epoch-16 batch-33 = 0.00020863069221377373

Training epoch-16 batch-34
Running loss of epoch-16 batch-34 = 0.001050302991643548

Training epoch-16 batch-35
Running loss of epoch-16 batch-35 = 0.0002800170914269984

Training epoch-16 batch-36
Running loss of epoch-16 batch-36 = 0.0003983237547799945

Training epoch-16 batch-37
Running loss of epoch-16 batch-37 = 0.0006198990158736706

Training epoch-16 batch-38
Running loss of epoch-16 batch-38 = 0.0006609734264202416

Training epoch-16 batch-39
Running loss of epoch-16 batch-39 = 0.000644047511741519

Training epoch-16 batch-40
Running loss of epoch-16 batch-40 = 3.742985427379608e-05

Training epoch-16 batch-41
Running loss of epoch-16 batch-41 = 0.0011626853374764323

Training epoch-16 batch-42
Running loss of epoch-16 batch-42 = 0.00016090681310743093

Training epoch-16 batch-43
Running loss of epoch-16 batch-43 = 0.00019745202735066414

Training epoch-16 batch-44
Running loss of epoch-16 batch-44 = 0.0003099810564890504

Training epoch-16 batch-45
Running loss of epoch-16 batch-45 = 0.00042657385347411036

Training epoch-16 batch-46
Running loss of epoch-16 batch-46 = 0.001343937823548913

Training epoch-16 batch-47
Running loss of epoch-16 batch-47 = 0.0011740043992176652

Training epoch-16 batch-48
Running loss of epoch-16 batch-48 = 0.0012432262301445007

Training epoch-16 batch-49
Running loss of epoch-16 batch-49 = 0.00020315265282988548

Training epoch-16 batch-50
Running loss of epoch-16 batch-50 = 0.0007621580734848976

Training epoch-16 batch-51
Running loss of epoch-16 batch-51 = 5.0851842388510704e-05

Training epoch-16 batch-52
Running loss of epoch-16 batch-52 = 0.00011669029481709003

Training epoch-16 batch-53
Running loss of epoch-16 batch-53 = 0.00033910226193256676

Training epoch-16 batch-54
Running loss of epoch-16 batch-54 = 0.0015917220152914524

Training epoch-16 batch-55
Running loss of epoch-16 batch-55 = 0.0006630250136367977

Training epoch-16 batch-56
Running loss of epoch-16 batch-56 = 0.00033174530835822225

Training epoch-16 batch-57
Running loss of epoch-16 batch-57 = 0.0012802036944776773

Training epoch-16 batch-58
Running loss of epoch-16 batch-58 = 0.0003618904447648674

Training epoch-16 batch-59
Running loss of epoch-16 batch-59 = 0.0006445294129662216

Training epoch-16 batch-60
Running loss of epoch-16 batch-60 = 0.0017251198878511786

Training epoch-16 batch-61
Running loss of epoch-16 batch-61 = 0.0011433534091338515

Training epoch-16 batch-62
Running loss of epoch-16 batch-62 = 0.0008209596271626651

Training epoch-16 batch-63
Running loss of epoch-16 batch-63 = 0.00020383531227707863

Training epoch-16 batch-64
Running loss of epoch-16 batch-64 = 0.0011274074204266071

Training epoch-16 batch-65
Running loss of epoch-16 batch-65 = 0.0031078383326530457

Training epoch-16 batch-66
Running loss of epoch-16 batch-66 = 0.0012816054513677955

Training epoch-16 batch-67
Running loss of epoch-16 batch-67 = 0.00041485344991087914

Training epoch-16 batch-68
Running loss of epoch-16 batch-68 = 0.00013379944721236825

Training epoch-16 batch-69
Running loss of epoch-16 batch-69 = 0.0010605561546981335

Training epoch-16 batch-70
Running loss of epoch-16 batch-70 = 0.00034811091609299183

Training epoch-16 batch-71
Running loss of epoch-16 batch-71 = 0.00022875709692016244

Training epoch-16 batch-72
Running loss of epoch-16 batch-72 = 0.0035789720714092255

Training epoch-16 batch-73
Running loss of epoch-16 batch-73 = 0.0017401467775925994

Training epoch-16 batch-74
Running loss of epoch-16 batch-74 = 0.0008800610667094588

Training epoch-16 batch-75
Running loss of epoch-16 batch-75 = 0.0010136397322639823

Training epoch-16 batch-76
Running loss of epoch-16 batch-76 = 0.0006968636298552155

Training epoch-16 batch-77
Running loss of epoch-16 batch-77 = 0.003521933685988188

Training epoch-16 batch-78
Running loss of epoch-16 batch-78 = 0.0013781387824565172

Training epoch-16 batch-79
Running loss of epoch-16 batch-79 = 0.0007710597710683942

Training epoch-16 batch-80
Running loss of epoch-16 batch-80 = 0.0005809329450130463

Training epoch-16 batch-81
Running loss of epoch-16 batch-81 = 0.0018137816805392504

Training epoch-16 batch-82
Running loss of epoch-16 batch-82 = 0.0002877437509596348

Training epoch-16 batch-83
Running loss of epoch-16 batch-83 = 7.851759437471628e-05

Training epoch-16 batch-84
Running loss of epoch-16 batch-84 = 0.0008059305837377906

Training epoch-16 batch-85
Running loss of epoch-16 batch-85 = 0.0005239590536803007

Training epoch-16 batch-86
Running loss of epoch-16 batch-86 = 0.0019746990874409676

Training epoch-16 batch-87
Running loss of epoch-16 batch-87 = 0.0007555619813501835

Training epoch-16 batch-88
Running loss of epoch-16 batch-88 = 0.00020363187650218606

Training epoch-16 batch-89
Running loss of epoch-16 batch-89 = 0.0018760382663458586

Training epoch-16 batch-90
Running loss of epoch-16 batch-90 = 0.00032721093157306314

Training epoch-16 batch-91
Running loss of epoch-16 batch-91 = 0.0007778194849379361

Training epoch-16 batch-92
Running loss of epoch-16 batch-92 = 0.004664645530283451

Training epoch-16 batch-93
Running loss of epoch-16 batch-93 = 0.003008144674822688

Training epoch-16 batch-94
Running loss of epoch-16 batch-94 = 0.0006833700463175774

Training epoch-16 batch-95
Running loss of epoch-16 batch-95 = 0.00027935014804825187

Training epoch-16 batch-96
Running loss of epoch-16 batch-96 = 0.0005482183187268674

Training epoch-16 batch-97
Running loss of epoch-16 batch-97 = 0.0027734877075999975

Training epoch-16 batch-98
Running loss of epoch-16 batch-98 = 0.00011741707567125559

Training epoch-16 batch-99
Running loss of epoch-16 batch-99 = 0.0003130411496385932

Training epoch-16 batch-100
Running loss of epoch-16 batch-100 = 0.003608040977269411

Training epoch-16 batch-101
Running loss of epoch-16 batch-101 = 0.0004169615567661822

Training epoch-16 batch-102
Running loss of epoch-16 batch-102 = 0.0024203804787248373

Training epoch-16 batch-103
Running loss of epoch-16 batch-103 = 0.00233791908249259

Training epoch-16 batch-104
Running loss of epoch-16 batch-104 = 0.001718766987323761

Training epoch-16 batch-105
Running loss of epoch-16 batch-105 = 0.0011152414372190833

Training epoch-16 batch-106
Running loss of epoch-16 batch-106 = 0.001909273094497621

Training epoch-16 batch-107
Running loss of epoch-16 batch-107 = 0.006170233711600304

Training epoch-16 batch-108
Running loss of epoch-16 batch-108 = 0.0015166806988418102

Training epoch-16 batch-109
Running loss of epoch-16 batch-109 = 0.0021748626604676247

Training epoch-16 batch-110
Running loss of epoch-16 batch-110 = 0.0005043370183557272

Training epoch-16 batch-111
Running loss of epoch-16 batch-111 = 0.002114598173648119

Training epoch-16 batch-112
Running loss of epoch-16 batch-112 = 0.0027882929425686598

Training epoch-16 batch-113
Running loss of epoch-16 batch-113 = 0.0005496174562722445

Training epoch-16 batch-114
Running loss of epoch-16 batch-114 = 0.00037385907489806414

Training epoch-16 batch-115
Running loss of epoch-16 batch-115 = 0.0004815253196284175

Training epoch-16 batch-116
Running loss of epoch-16 batch-116 = 0.0037916889414191246

Training epoch-16 batch-117
Running loss of epoch-16 batch-117 = 0.0018108466174453497

Training epoch-16 batch-118
Running loss of epoch-16 batch-118 = 0.0018393853679299355

Training epoch-16 batch-119
Running loss of epoch-16 batch-119 = 0.0008564525633119047

Training epoch-16 batch-120
Running loss of epoch-16 batch-120 = 0.0010846308432519436

Training epoch-16 batch-121
Running loss of epoch-16 batch-121 = 0.00585802411660552

Training epoch-16 batch-122
Running loss of epoch-16 batch-122 = 0.0007142077665776014

Training epoch-16 batch-123
Running loss of epoch-16 batch-123 = 0.002461908385157585

Training epoch-16 batch-124
Running loss of epoch-16 batch-124 = 0.0010940228821709752

Training epoch-16 batch-125
Running loss of epoch-16 batch-125 = 0.003285792889073491

Training epoch-16 batch-126
Running loss of epoch-16 batch-126 = 0.0017922879196703434

Training epoch-16 batch-127
Running loss of epoch-16 batch-127 = 0.003049964550882578

Training epoch-16 batch-128
Running loss of epoch-16 batch-128 = 0.004208844155073166

Training epoch-16 batch-129
Running loss of epoch-16 batch-129 = 0.0003435102989897132

Training epoch-16 batch-130
Running loss of epoch-16 batch-130 = 0.0019705018494278193

Training epoch-16 batch-131
Running loss of epoch-16 batch-131 = 0.0006927357753738761

Training epoch-16 batch-132
Running loss of epoch-16 batch-132 = 0.0004859195905737579

Training epoch-16 batch-133
Running loss of epoch-16 batch-133 = 0.0005870441091246903

Training epoch-16 batch-134
Running loss of epoch-16 batch-134 = 0.0013450831174850464

Training epoch-16 batch-135
Running loss of epoch-16 batch-135 = 0.003979022614657879

Training epoch-16 batch-136
Running loss of epoch-16 batch-136 = 0.0023044568952172995

Training epoch-16 batch-137
Running loss of epoch-16 batch-137 = 0.0019572116434574127

Training epoch-16 batch-138
Running loss of epoch-16 batch-138 = 0.000787232827860862

Training epoch-16 batch-139
Running loss of epoch-16 batch-139 = 0.0005450611934065819

Training epoch-16 batch-140
Running loss of epoch-16 batch-140 = 0.0005130944191478193

Training epoch-16 batch-141
Running loss of epoch-16 batch-141 = 0.0011448332807049155

Training epoch-16 batch-142
Running loss of epoch-16 batch-142 = 0.001979666529223323

Training epoch-16 batch-143
Running loss of epoch-16 batch-143 = 0.004610641859471798

Training epoch-16 batch-144
Running loss of epoch-16 batch-144 = 0.0013568371068686247

Training epoch-16 batch-145
Running loss of epoch-16 batch-145 = 0.0008498820243403316

Training epoch-16 batch-146
Running loss of epoch-16 batch-146 = 0.0008058915263973176

Training epoch-16 batch-147
Running loss of epoch-16 batch-147 = 0.002993108704686165

Training epoch-16 batch-148
Running loss of epoch-16 batch-148 = 0.0002520527923479676

Training epoch-16 batch-149
Running loss of epoch-16 batch-149 = 0.0009674825123511255

Training epoch-16 batch-150
Running loss of epoch-16 batch-150 = 0.0018449040362611413

Training epoch-16 batch-151
Running loss of epoch-16 batch-151 = 0.0010582610266283154

Training epoch-16 batch-152
Running loss of epoch-16 batch-152 = 0.0009962734766304493

Training epoch-16 batch-153
Running loss of epoch-16 batch-153 = 0.0005371542647480965

Training epoch-16 batch-154
Running loss of epoch-16 batch-154 = 0.001962476409971714

Training epoch-16 batch-155
Running loss of epoch-16 batch-155 = 0.00038967421278357506

Training epoch-16 batch-156
Running loss of epoch-16 batch-156 = 0.002947657136246562

Training epoch-16 batch-157
Running loss of epoch-16 batch-157 = 0.003069566562771797

Finished training epoch-16.



Average train loss at epoch-16 = 0.0011703229304403067

Started Evaluation

Average val loss at epoch-16 = 3.9866030922061517

Accuracy for classes:
Accuracy for class equals is: 59.41 %
Accuracy for class main is: 36.89 %
Accuracy for class setUp is: 43.93 %
Accuracy for class onCreate is: 41.15 %
Accuracy for class toString is: 30.38 %
Accuracy for class run is: 38.81 %
Accuracy for class hashCode is: 82.02 %
Accuracy for class init is: 21.08 %
Accuracy for class execute is: 32.53 %
Accuracy for class get is: 35.64 %

Overall Accuracy = 41.90 %

Finished Evaluation



Started training epoch-17


Training epoch-17 batch-1
Running loss of epoch-17 batch-1 = 0.000839593296404928

Training epoch-17 batch-2
Running loss of epoch-17 batch-2 = 0.0005127015756443143

Training epoch-17 batch-3
Running loss of epoch-17 batch-3 = 0.0028288266621530056

Training epoch-17 batch-4
Running loss of epoch-17 batch-4 = 0.00035238335840404034

Training epoch-17 batch-5
Running loss of epoch-17 batch-5 = 0.0024349880404770374

Training epoch-17 batch-6
Running loss of epoch-17 batch-6 = 0.00016722886357456446

Training epoch-17 batch-7
Running loss of epoch-17 batch-7 = 0.0003324568970128894

Training epoch-17 batch-8
Running loss of epoch-17 batch-8 = 0.0004449042316991836

Training epoch-17 batch-9
Running loss of epoch-17 batch-9 = 0.0010575063060969114

Training epoch-17 batch-10
Running loss of epoch-17 batch-10 = 0.0007076009060256183

Training epoch-17 batch-11
Running loss of epoch-17 batch-11 = 0.0013803319307044148

Training epoch-17 batch-12
Running loss of epoch-17 batch-12 = 0.002573576522991061

Training epoch-17 batch-13
Running loss of epoch-17 batch-13 = 0.0003396092797629535

Training epoch-17 batch-14
Running loss of epoch-17 batch-14 = 0.0031009833328425884

Training epoch-17 batch-15
Running loss of epoch-17 batch-15 = 0.0010517522459849715

Training epoch-17 batch-16
Running loss of epoch-17 batch-16 = 0.0007827738299965858

Training epoch-17 batch-17
Running loss of epoch-17 batch-17 = 0.0001496177865192294

Training epoch-17 batch-18
Running loss of epoch-17 batch-18 = 0.0006375433877110481

Training epoch-17 batch-19
Running loss of epoch-17 batch-19 = 0.0011338576441630721

Training epoch-17 batch-20
Running loss of epoch-17 batch-20 = 0.0014045225689187646

Training epoch-17 batch-21
Running loss of epoch-17 batch-21 = 0.0005011316388845444

Training epoch-17 batch-22
Running loss of epoch-17 batch-22 = 0.0004224239964969456

Training epoch-17 batch-23
Running loss of epoch-17 batch-23 = 0.0007060778443701565

Training epoch-17 batch-24
Running loss of epoch-17 batch-24 = 0.0010616520885378122

Training epoch-17 batch-25
Running loss of epoch-17 batch-25 = 0.0007805244531482458

Training epoch-17 batch-26
Running loss of epoch-17 batch-26 = 0.001376174739561975

Training epoch-17 batch-27
Running loss of epoch-17 batch-27 = 0.001230378751643002

Training epoch-17 batch-28
Running loss of epoch-17 batch-28 = 0.0009288656292483211

Training epoch-17 batch-29
Running loss of epoch-17 batch-29 = 0.0017212695674970746

Training epoch-17 batch-30
Running loss of epoch-17 batch-30 = 0.0008034605416469276

Training epoch-17 batch-31
Running loss of epoch-17 batch-31 = 0.0016954041784629226

Training epoch-17 batch-32
Running loss of epoch-17 batch-32 = 0.0001628922764211893

Training epoch-17 batch-33
Running loss of epoch-17 batch-33 = 0.0005938300746493042

Training epoch-17 batch-34
Running loss of epoch-17 batch-34 = 0.00046052259858697653

Training epoch-17 batch-35
Running loss of epoch-17 batch-35 = 0.00030747830169275403

Training epoch-17 batch-36
Running loss of epoch-17 batch-36 = 0.0015281843952834606

Training epoch-17 batch-37
Running loss of epoch-17 batch-37 = 0.0014535371446982026

Training epoch-17 batch-38
Running loss of epoch-17 batch-38 = 0.0009551241528242826

Training epoch-17 batch-39
Running loss of epoch-17 batch-39 = 0.0006660252111032605

Training epoch-17 batch-40
Running loss of epoch-17 batch-40 = 0.00048134999815374613

Training epoch-17 batch-41
Running loss of epoch-17 batch-41 = 0.0003210793947800994

Training epoch-17 batch-42
Running loss of epoch-17 batch-42 = 0.00029849912971258163

Training epoch-17 batch-43
Running loss of epoch-17 batch-43 = 0.0004409008543007076

Training epoch-17 batch-44
Running loss of epoch-17 batch-44 = 0.0016582147218286991

Training epoch-17 batch-45
Running loss of epoch-17 batch-45 = 0.00038322864565998316

Training epoch-17 batch-46
Running loss of epoch-17 batch-46 = 0.00026657868875190616

Training epoch-17 batch-47
Running loss of epoch-17 batch-47 = 0.0005428393487818539

Training epoch-17 batch-48
Running loss of epoch-17 batch-48 = 0.000527614145539701

Training epoch-17 batch-49
Running loss of epoch-17 batch-49 = 0.0001671918435022235

Training epoch-17 batch-50
Running loss of epoch-17 batch-50 = 0.000698251009453088

Training epoch-17 batch-51
Running loss of epoch-17 batch-51 = 0.0005187616334296763

Training epoch-17 batch-52
Running loss of epoch-17 batch-52 = 0.00034518877509981394

Training epoch-17 batch-53
Running loss of epoch-17 batch-53 = 0.000966724765021354

Training epoch-17 batch-54
Running loss of epoch-17 batch-54 = 0.0001539740478619933

Training epoch-17 batch-55
Running loss of epoch-17 batch-55 = 0.0008532857173122466

Training epoch-17 batch-56
Running loss of epoch-17 batch-56 = 0.002074741292744875

Training epoch-17 batch-57
Running loss of epoch-17 batch-57 = 0.0003641053626779467

Training epoch-17 batch-58
Running loss of epoch-17 batch-58 = 0.0007389777456410229

Training epoch-17 batch-59
Running loss of epoch-17 batch-59 = 5.6765973567962646e-05

Training epoch-17 batch-60
Running loss of epoch-17 batch-60 = 0.0007233594078570604

Training epoch-17 batch-61
Running loss of epoch-17 batch-61 = 0.00048296910244971514

Training epoch-17 batch-62
Running loss of epoch-17 batch-62 = 0.000454247317975387

Training epoch-17 batch-63
Running loss of epoch-17 batch-63 = 0.0012076066341251135

Training epoch-17 batch-64
Running loss of epoch-17 batch-64 = 0.0007970584556460381

Training epoch-17 batch-65
Running loss of epoch-17 batch-65 = 0.0005566643085330725

Training epoch-17 batch-66
Running loss of epoch-17 batch-66 = 0.001025739824399352

Training epoch-17 batch-67
Running loss of epoch-17 batch-67 = 0.0009085208876058459

Training epoch-17 batch-68
Running loss of epoch-17 batch-68 = 0.00034564005909487605

Training epoch-17 batch-69
Running loss of epoch-17 batch-69 = 0.00030339922523126006

Training epoch-17 batch-70
Running loss of epoch-17 batch-70 = 0.00031883083283901215

Training epoch-17 batch-71
Running loss of epoch-17 batch-71 = 0.0005465131835080683

Training epoch-17 batch-72
Running loss of epoch-17 batch-72 = 0.0006248722202144563

Training epoch-17 batch-73
Running loss of epoch-17 batch-73 = 0.00011604186147451401

Training epoch-17 batch-74
Running loss of epoch-17 batch-74 = 0.0008029029704630375

Training epoch-17 batch-75
Running loss of epoch-17 batch-75 = 0.00017381156794726849

Training epoch-17 batch-76
Running loss of epoch-17 batch-76 = 0.002105581108480692

Training epoch-17 batch-77
Running loss of epoch-17 batch-77 = 0.0002392622991465032

Training epoch-17 batch-78
Running loss of epoch-17 batch-78 = 0.00017015362391248345

Training epoch-17 batch-79
Running loss of epoch-17 batch-79 = 0.0004276314575690776

Training epoch-17 batch-80
Running loss of epoch-17 batch-80 = 0.0008472382323816419

Training epoch-17 batch-81
Running loss of epoch-17 batch-81 = 0.00020683679031208158

Training epoch-17 batch-82
Running loss of epoch-17 batch-82 = 0.0002474220236763358

Training epoch-17 batch-83
Running loss of epoch-17 batch-83 = 0.0005327239632606506

Training epoch-17 batch-84
Running loss of epoch-17 batch-84 = 0.0022530462592840195

Training epoch-17 batch-85
Running loss of epoch-17 batch-85 = 0.00017750030383467674

Training epoch-17 batch-86
Running loss of epoch-17 batch-86 = 0.0002145233447663486

Training epoch-17 batch-87
Running loss of epoch-17 batch-87 = 0.0006343034328892827

Training epoch-17 batch-88
Running loss of epoch-17 batch-88 = 0.0010952939046546817

Training epoch-17 batch-89
Running loss of epoch-17 batch-89 = 0.0012645432725548744

Training epoch-17 batch-90
Running loss of epoch-17 batch-90 = 0.00012399547267705202

Training epoch-17 batch-91
Running loss of epoch-17 batch-91 = 0.0008049880852922797

Training epoch-17 batch-92
Running loss of epoch-17 batch-92 = 0.0005537921679206192

Training epoch-17 batch-93
Running loss of epoch-17 batch-93 = 0.0005899121169932187

Training epoch-17 batch-94
Running loss of epoch-17 batch-94 = 0.000611075374763459

Training epoch-17 batch-95
Running loss of epoch-17 batch-95 = 0.0003152845019940287

Training epoch-17 batch-96
Running loss of epoch-17 batch-96 = 0.000607628608122468

Training epoch-17 batch-97
Running loss of epoch-17 batch-97 = 0.0026051304303109646

Training epoch-17 batch-98
Running loss of epoch-17 batch-98 = 0.00017083482816815376

Training epoch-17 batch-99
Running loss of epoch-17 batch-99 = 0.0001844164216890931

Training epoch-17 batch-100
Running loss of epoch-17 batch-100 = 0.00030795246129855514

Training epoch-17 batch-101
Running loss of epoch-17 batch-101 = 0.0006072198739275336

Training epoch-17 batch-102
Running loss of epoch-17 batch-102 = 0.00045338663039729

Training epoch-17 batch-103
Running loss of epoch-17 batch-103 = 0.00046501000178977847

Training epoch-17 batch-104
Running loss of epoch-17 batch-104 = 0.00030331878224387765

Training epoch-17 batch-105
Running loss of epoch-17 batch-105 = 0.0005401141243055463

Training epoch-17 batch-106
Running loss of epoch-17 batch-106 = 0.0006371954805217683

Training epoch-17 batch-107
Running loss of epoch-17 batch-107 = 0.00023798772599548101

Training epoch-17 batch-108
Running loss of epoch-17 batch-108 = 0.000356327451299876

Training epoch-17 batch-109
Running loss of epoch-17 batch-109 = 0.00039474799996241927

Training epoch-17 batch-110
Running loss of epoch-17 batch-110 = 9.80886397883296e-05

Training epoch-17 batch-111
Running loss of epoch-17 batch-111 = 0.0006556726875714958

Training epoch-17 batch-112
Running loss of epoch-17 batch-112 = 0.0002968367771245539

Training epoch-17 batch-113
Running loss of epoch-17 batch-113 = 0.0010979935759678483

Training epoch-17 batch-114
Running loss of epoch-17 batch-114 = 0.00018654519226402044

Training epoch-17 batch-115
Running loss of epoch-17 batch-115 = 0.0004349619266577065

Training epoch-17 batch-116
Running loss of epoch-17 batch-116 = 0.0005487857852131128

Training epoch-17 batch-117
Running loss of epoch-17 batch-117 = 0.0003003955353051424

Training epoch-17 batch-118
Running loss of epoch-17 batch-118 = 0.00033376982901245356

Training epoch-17 batch-119
Running loss of epoch-17 batch-119 = 0.00032467558048665524

Training epoch-17 batch-120
Running loss of epoch-17 batch-120 = 0.00039028091123327613

Training epoch-17 batch-121
Running loss of epoch-17 batch-121 = 0.0009363432181999087

Training epoch-17 batch-122
Running loss of epoch-17 batch-122 = 0.0002906818990595639

Training epoch-17 batch-123
Running loss of epoch-17 batch-123 = 0.00010430882684886456

Training epoch-17 batch-124
Running loss of epoch-17 batch-124 = 0.00011587608605623245

Training epoch-17 batch-125
Running loss of epoch-17 batch-125 = 0.00026373344007879496

Training epoch-17 batch-126
Running loss of epoch-17 batch-126 = 5.8387056924402714e-05

Training epoch-17 batch-127
Running loss of epoch-17 batch-127 = 0.0007160906679928303

Training epoch-17 batch-128
Running loss of epoch-17 batch-128 = 0.0007818588055670261

Training epoch-17 batch-129
Running loss of epoch-17 batch-129 = 0.00016287027392536402

Training epoch-17 batch-130
Running loss of epoch-17 batch-130 = 0.00013536587357521057

Training epoch-17 batch-131
Running loss of epoch-17 batch-131 = 0.0015537242870777845

Training epoch-17 batch-132
Running loss of epoch-17 batch-132 = 0.00012855208478868008

Training epoch-17 batch-133
Running loss of epoch-17 batch-133 = 0.00018638977780938148

Training epoch-17 batch-134
Running loss of epoch-17 batch-134 = 0.00022837799042463303

Training epoch-17 batch-135
Running loss of epoch-17 batch-135 = 0.0001917163608595729

Training epoch-17 batch-136
Running loss of epoch-17 batch-136 = 0.000962788297329098

Training epoch-17 batch-137
Running loss of epoch-17 batch-137 = 0.00010406097862869501

Training epoch-17 batch-138
Running loss of epoch-17 batch-138 = 0.00021910155192017555

Training epoch-17 batch-139
Running loss of epoch-17 batch-139 = 0.0002565545146353543

Training epoch-17 batch-140
Running loss of epoch-17 batch-140 = 0.0007617964292876422

Training epoch-17 batch-141
Running loss of epoch-17 batch-141 = 0.0011889738962054253

Training epoch-17 batch-142
Running loss of epoch-17 batch-142 = 0.00014012912288308144

Training epoch-17 batch-143
Running loss of epoch-17 batch-143 = 0.0003866035840474069

Training epoch-17 batch-144
Running loss of epoch-17 batch-144 = 0.00015684159006923437

Training epoch-17 batch-145
Running loss of epoch-17 batch-145 = 0.0003965107025578618

Training epoch-17 batch-146
Running loss of epoch-17 batch-146 = 0.00019377260468900204

Training epoch-17 batch-147
Running loss of epoch-17 batch-147 = 0.0005436214269138873

Training epoch-17 batch-148
Running loss of epoch-17 batch-148 = 0.00020661286544054747

Training epoch-17 batch-149
Running loss of epoch-17 batch-149 = 0.0003793439536821097

Training epoch-17 batch-150
Running loss of epoch-17 batch-150 = 0.0007523460080847144

Training epoch-17 batch-151
Running loss of epoch-17 batch-151 = 0.000190184626262635

Training epoch-17 batch-152
Running loss of epoch-17 batch-152 = 8.021725807338953e-05

Training epoch-17 batch-153
Running loss of epoch-17 batch-153 = 0.00037688750308007

Training epoch-17 batch-154
Running loss of epoch-17 batch-154 = 0.0004987841239199042

Training epoch-17 batch-155
Running loss of epoch-17 batch-155 = 4.4669839553534985e-05

Training epoch-17 batch-156
Running loss of epoch-17 batch-156 = 0.0003869713400490582

Training epoch-17 batch-157
Running loss of epoch-17 batch-157 = 0.0031400835141539574

Finished training epoch-17.



Average train loss at epoch-17 = 0.0006505895804613829

Started Evaluation

Average val loss at epoch-17 = 3.8583946949557255

Accuracy for classes:
Accuracy for class equals is: 70.63 %
Accuracy for class main is: 55.25 %
Accuracy for class setUp is: 39.67 %
Accuracy for class onCreate is: 39.55 %
Accuracy for class toString is: 39.25 %
Accuracy for class run is: 36.76 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 24.89 %
Accuracy for class execute is: 17.27 %
Accuracy for class get is: 35.90 %

Overall Accuracy = 44.65 %

Finished Evaluation



Started training epoch-18


Training epoch-18 batch-1
Running loss of epoch-18 batch-1 = 0.0003564574581105262

Training epoch-18 batch-2
Running loss of epoch-18 batch-2 = 0.0003273991169407964

Training epoch-18 batch-3
Running loss of epoch-18 batch-3 = 0.0006229792488738894

Training epoch-18 batch-4
Running loss of epoch-18 batch-4 = 0.00019570381846278906

Training epoch-18 batch-5
Running loss of epoch-18 batch-5 = 0.0005505986046046019

Training epoch-18 batch-6
Running loss of epoch-18 batch-6 = 0.0029368943069130182

Training epoch-18 batch-7
Running loss of epoch-18 batch-7 = 2.2019841708242893e-05

Training epoch-18 batch-8
Running loss of epoch-18 batch-8 = 0.0003156909951940179

Training epoch-18 batch-9
Running loss of epoch-18 batch-9 = 0.0006746263825334609

Training epoch-18 batch-10
Running loss of epoch-18 batch-10 = 0.0003439491265453398

Training epoch-18 batch-11
Running loss of epoch-18 batch-11 = 9.472458623349667e-05

Training epoch-18 batch-12
Running loss of epoch-18 batch-12 = 8.170865476131439e-05

Training epoch-18 batch-13
Running loss of epoch-18 batch-13 = 0.00034513341961428523

Training epoch-18 batch-14
Running loss of epoch-18 batch-14 = 0.00025408039800822735

Training epoch-18 batch-15
Running loss of epoch-18 batch-15 = 0.0002381406957283616

Training epoch-18 batch-16
Running loss of epoch-18 batch-16 = 0.0005623196484521031

Training epoch-18 batch-17
Running loss of epoch-18 batch-17 = 0.0006501671159639955

Training epoch-18 batch-18
Running loss of epoch-18 batch-18 = 4.003418143838644e-05

Training epoch-18 batch-19
Running loss of epoch-18 batch-19 = 0.00012960261665284634

Training epoch-18 batch-20
Running loss of epoch-18 batch-20 = 4.492630250751972e-05

Training epoch-18 batch-21
Running loss of epoch-18 batch-21 = 0.00029487963183782995

Training epoch-18 batch-22
Running loss of epoch-18 batch-22 = 0.00043732859194278717

Training epoch-18 batch-23
Running loss of epoch-18 batch-23 = 0.00030624191276729107

Training epoch-18 batch-24
Running loss of epoch-18 batch-24 = 8.117023389786482e-05

Training epoch-18 batch-25
Running loss of epoch-18 batch-25 = 9.216740727424622e-05

Training epoch-18 batch-26
Running loss of epoch-18 batch-26 = 0.00029087025905027986

Training epoch-18 batch-27
Running loss of epoch-18 batch-27 = 0.00026945973513647914

Training epoch-18 batch-28
Running loss of epoch-18 batch-28 = 0.0001602591946721077

Training epoch-18 batch-29
Running loss of epoch-18 batch-29 = 0.00029611558420583606

Training epoch-18 batch-30
Running loss of epoch-18 batch-30 = 0.00015788991004228592

Training epoch-18 batch-31
Running loss of epoch-18 batch-31 = 0.0019352249801158905

Training epoch-18 batch-32
Running loss of epoch-18 batch-32 = 0.0005139041459187865

Training epoch-18 batch-33
Running loss of epoch-18 batch-33 = 0.0004113191389478743

Training epoch-18 batch-34
Running loss of epoch-18 batch-34 = 0.00010418577585369349

Training epoch-18 batch-35
Running loss of epoch-18 batch-35 = 0.00021812744671478868

Training epoch-18 batch-36
Running loss of epoch-18 batch-36 = 0.0008859408553689718

Training epoch-18 batch-37
Running loss of epoch-18 batch-37 = 0.0003663068637251854

Training epoch-18 batch-38
Running loss of epoch-18 batch-38 = 0.0001467560650780797

Training epoch-18 batch-39
Running loss of epoch-18 batch-39 = 0.00017814768943935633

Training epoch-18 batch-40
Running loss of epoch-18 batch-40 = 0.00014067633310332894

Training epoch-18 batch-41
Running loss of epoch-18 batch-41 = 0.000124035170301795

Training epoch-18 batch-42
Running loss of epoch-18 batch-42 = 0.0002503424184396863

Training epoch-18 batch-43
Running loss of epoch-18 batch-43 = 0.0006569816032424569

Training epoch-18 batch-44
Running loss of epoch-18 batch-44 = 0.0007060040952637792

Training epoch-18 batch-45
Running loss of epoch-18 batch-45 = 0.0009891546797007322

Training epoch-18 batch-46
Running loss of epoch-18 batch-46 = 0.00019200355745851994

Training epoch-18 batch-47
Running loss of epoch-18 batch-47 = 0.0007245264714583755

Training epoch-18 batch-48
Running loss of epoch-18 batch-48 = 0.000163666729349643

Training epoch-18 batch-49
Running loss of epoch-18 batch-49 = 0.0007552087190560997

Training epoch-18 batch-50
Running loss of epoch-18 batch-50 = 0.0004871090641245246

Training epoch-18 batch-51
Running loss of epoch-18 batch-51 = 9.516067802906036e-05

Training epoch-18 batch-52
Running loss of epoch-18 batch-52 = 5.93897420912981e-05

Training epoch-18 batch-53
Running loss of epoch-18 batch-53 = 0.00019719416741281748

Training epoch-18 batch-54
Running loss of epoch-18 batch-54 = 0.002327264053747058

Training epoch-18 batch-55
Running loss of epoch-18 batch-55 = 0.00010100851068273187

Training epoch-18 batch-56
Running loss of epoch-18 batch-56 = 7.504294626414776e-05

Training epoch-18 batch-57
Running loss of epoch-18 batch-57 = 8.372718002647161e-05

Training epoch-18 batch-58
Running loss of epoch-18 batch-58 = 0.00013795646373182535

Training epoch-18 batch-59
Running loss of epoch-18 batch-59 = 6.995617877691984e-05

Training epoch-18 batch-60
Running loss of epoch-18 batch-60 = 8.577073458582163e-05

Training epoch-18 batch-61
Running loss of epoch-18 batch-61 = 9.609409607946873e-05

Training epoch-18 batch-62
Running loss of epoch-18 batch-62 = 0.00025863025803118944

Training epoch-18 batch-63
Running loss of epoch-18 batch-63 = 0.0020055219065397978

Training epoch-18 batch-64
Running loss of epoch-18 batch-64 = 0.0010574739426374435

Training epoch-18 batch-65
Running loss of epoch-18 batch-65 = 0.0003764097345992923

Training epoch-18 batch-66
Running loss of epoch-18 batch-66 = 0.0002698778407648206

Training epoch-18 batch-67
Running loss of epoch-18 batch-67 = 4.475587047636509e-05

Training epoch-18 batch-68
Running loss of epoch-18 batch-68 = 0.00016790698282420635

Training epoch-18 batch-69
Running loss of epoch-18 batch-69 = 0.0011369365965947509

Training epoch-18 batch-70
Running loss of epoch-18 batch-70 = 0.0012262887321412563

Training epoch-18 batch-71
Running loss of epoch-18 batch-71 = 0.0006075401324778795

Training epoch-18 batch-72
Running loss of epoch-18 batch-72 = 0.00027633237186819315

Training epoch-18 batch-73
Running loss of epoch-18 batch-73 = 7.39414244890213e-05

Training epoch-18 batch-74
Running loss of epoch-18 batch-74 = 0.0005413898034021258

Training epoch-18 batch-75
Running loss of epoch-18 batch-75 = 0.0006362925050780177

Training epoch-18 batch-76
Running loss of epoch-18 batch-76 = 8.853257168084383e-05

Training epoch-18 batch-77
Running loss of epoch-18 batch-77 = 0.00015861738938838243

Training epoch-18 batch-78
Running loss of epoch-18 batch-78 = 0.0005944222211837769

Training epoch-18 batch-79
Running loss of epoch-18 batch-79 = 0.00012033537495881319

Training epoch-18 batch-80
Running loss of epoch-18 batch-80 = 0.0004183620912954211

Training epoch-18 batch-81
Running loss of epoch-18 batch-81 = 0.0002877672668546438

Training epoch-18 batch-82
Running loss of epoch-18 batch-82 = 0.0007495344616472721

Training epoch-18 batch-83
Running loss of epoch-18 batch-83 = 4.766066558659077e-05

Training epoch-18 batch-84
Running loss of epoch-18 batch-84 = 0.0001985484268516302

Training epoch-18 batch-85
Running loss of epoch-18 batch-85 = 0.0005874510388821363

Training epoch-18 batch-86
Running loss of epoch-18 batch-86 = 0.0002228481462225318

Training epoch-18 batch-87
Running loss of epoch-18 batch-87 = 8.196406997740269e-05

Training epoch-18 batch-88
Running loss of epoch-18 batch-88 = 0.0003233978059142828

Training epoch-18 batch-89
Running loss of epoch-18 batch-89 = 0.001462296349927783

Training epoch-18 batch-90
Running loss of epoch-18 batch-90 = 0.0007312136003747582

Training epoch-18 batch-91
Running loss of epoch-18 batch-91 = 5.283905193209648e-05

Training epoch-18 batch-92
Running loss of epoch-18 batch-92 = 5.6400313042104244e-05

Training epoch-18 batch-93
Running loss of epoch-18 batch-93 = 0.0014155467506498098

Training epoch-18 batch-94
Running loss of epoch-18 batch-94 = 5.371146835386753e-05

Training epoch-18 batch-95
Running loss of epoch-18 batch-95 = 0.000756858556997031

Training epoch-18 batch-96
Running loss of epoch-18 batch-96 = 7.620843825861812e-05

Training epoch-18 batch-97
Running loss of epoch-18 batch-97 = 0.00011483591515570879

Training epoch-18 batch-98
Running loss of epoch-18 batch-98 = 0.0001973247854039073

Training epoch-18 batch-99
Running loss of epoch-18 batch-99 = 0.000157727743498981

Training epoch-18 batch-100
Running loss of epoch-18 batch-100 = 0.00014598120469599962

Training epoch-18 batch-101
Running loss of epoch-18 batch-101 = 0.002222795505076647

Training epoch-18 batch-102
Running loss of epoch-18 batch-102 = 0.0001608595484867692

Training epoch-18 batch-103
Running loss of epoch-18 batch-103 = 0.0006414079107344151

Training epoch-18 batch-104
Running loss of epoch-18 batch-104 = 0.000446765887318179

Training epoch-18 batch-105
Running loss of epoch-18 batch-105 = 0.0005653526750393212

Training epoch-18 batch-106
Running loss of epoch-18 batch-106 = 0.00011207698844373226

Training epoch-18 batch-107
Running loss of epoch-18 batch-107 = 0.00048535759560763836

Training epoch-18 batch-108
Running loss of epoch-18 batch-108 = 0.0004012296558357775

Training epoch-18 batch-109
Running loss of epoch-18 batch-109 = 6.770552136003971e-05

Training epoch-18 batch-110
Running loss of epoch-18 batch-110 = 0.0006389622576534748

Training epoch-18 batch-111
Running loss of epoch-18 batch-111 = 0.0016218196833506227

Training epoch-18 batch-112
Running loss of epoch-18 batch-112 = 0.00021619326435029507

Training epoch-18 batch-113
Running loss of epoch-18 batch-113 = 0.0006967817898839712

Training epoch-18 batch-114
Running loss of epoch-18 batch-114 = 0.0002310965210199356

Training epoch-18 batch-115
Running loss of epoch-18 batch-115 = 0.0006787153542973101

Training epoch-18 batch-116
Running loss of epoch-18 batch-116 = 0.0006463953759521246

Training epoch-18 batch-117
Running loss of epoch-18 batch-117 = 0.0002745150704868138

Training epoch-18 batch-118
Running loss of epoch-18 batch-118 = 0.00047104907571338117

Training epoch-18 batch-119
Running loss of epoch-18 batch-119 = 0.0002515970263630152

Training epoch-18 batch-120
Running loss of epoch-18 batch-120 = 0.00020771456183865666

Training epoch-18 batch-121
Running loss of epoch-18 batch-121 = 0.0007766949711367488

Training epoch-18 batch-122
Running loss of epoch-18 batch-122 = 0.00027943350141867995

Training epoch-18 batch-123
Running loss of epoch-18 batch-123 = 0.001034451648592949

Training epoch-18 batch-124
Running loss of epoch-18 batch-124 = 0.0006525418721139431

Training epoch-18 batch-125
Running loss of epoch-18 batch-125 = 2.58289510384202e-05

Training epoch-18 batch-126
Running loss of epoch-18 batch-126 = 0.00038471980951726437

Training epoch-18 batch-127
Running loss of epoch-18 batch-127 = 6.020138971507549e-05

Training epoch-18 batch-128
Running loss of epoch-18 batch-128 = 5.058071110397577e-05

Training epoch-18 batch-129
Running loss of epoch-18 batch-129 = 0.0028468340169638395

Training epoch-18 batch-130
Running loss of epoch-18 batch-130 = 0.00017237570136785507

Training epoch-18 batch-131
Running loss of epoch-18 batch-131 = 0.00027538504218682647

Training epoch-18 batch-132
Running loss of epoch-18 batch-132 = 0.0004945274558849633

Training epoch-18 batch-133
Running loss of epoch-18 batch-133 = 0.0004521647933870554

Training epoch-18 batch-134
Running loss of epoch-18 batch-134 = 0.000533814134541899

Training epoch-18 batch-135
Running loss of epoch-18 batch-135 = 0.0018050975631922483

Training epoch-18 batch-136
Running loss of epoch-18 batch-136 = 0.00023778906324878335

Training epoch-18 batch-137
Running loss of epoch-18 batch-137 = 0.002094386611133814

Training epoch-18 batch-138
Running loss of epoch-18 batch-138 = 0.00010757579002529383

Training epoch-18 batch-139
Running loss of epoch-18 batch-139 = 0.000145692378282547

Training epoch-18 batch-140
Running loss of epoch-18 batch-140 = 0.00015653338050469756

Training epoch-18 batch-141
Running loss of epoch-18 batch-141 = 0.000705777492839843

Training epoch-18 batch-142
Running loss of epoch-18 batch-142 = 0.000518083106726408

Training epoch-18 batch-143
Running loss of epoch-18 batch-143 = 0.0016895206645131111

Training epoch-18 batch-144
Running loss of epoch-18 batch-144 = 0.00012147985398769379

Training epoch-18 batch-145
Running loss of epoch-18 batch-145 = 6.414088420569897e-05

Training epoch-18 batch-146
Running loss of epoch-18 batch-146 = 6.273621693253517e-05

Training epoch-18 batch-147
Running loss of epoch-18 batch-147 = 0.00014731800183653831

Training epoch-18 batch-148
Running loss of epoch-18 batch-148 = 0.00012972101103514433

Training epoch-18 batch-149
Running loss of epoch-18 batch-149 = 0.0004062217194586992

Training epoch-18 batch-150
Running loss of epoch-18 batch-150 = 0.00016243616119027138

Training epoch-18 batch-151
Running loss of epoch-18 batch-151 = 4.2465515434741974e-05

Training epoch-18 batch-152
Running loss of epoch-18 batch-152 = 1.5143537893891335e-05

Training epoch-18 batch-153
Running loss of epoch-18 batch-153 = 0.00040218239882960916

Training epoch-18 batch-154
Running loss of epoch-18 batch-154 = 0.0002319989143870771

Training epoch-18 batch-155
Running loss of epoch-18 batch-155 = 0.0006006183684803545

Training epoch-18 batch-156
Running loss of epoch-18 batch-156 = 0.00015045213513076305

Training epoch-18 batch-157
Running loss of epoch-18 batch-157 = 0.00011072121560573578

Finished training epoch-18.



Average train loss at epoch-18 = 0.0004565401963889599

Started Evaluation

Average val loss at epoch-18 = 3.84709264573298

Accuracy for classes:
Accuracy for class equals is: 68.15 %
Accuracy for class main is: 43.77 %
Accuracy for class setUp is: 66.39 %
Accuracy for class onCreate is: 42.86 %
Accuracy for class toString is: 38.57 %
Accuracy for class run is: 46.58 %
Accuracy for class hashCode is: 76.03 %
Accuracy for class init is: 15.70 %
Accuracy for class execute is: 20.88 %
Accuracy for class get is: 22.05 %

Overall Accuracy = 45.70 %

Finished Evaluation



Started training epoch-19


Training epoch-19 batch-1
Running loss of epoch-19 batch-1 = 5.345698446035385e-05

Training epoch-19 batch-2
Running loss of epoch-19 batch-2 = 0.00029582501156255603

Training epoch-19 batch-3
Running loss of epoch-19 batch-3 = 0.00043269750312902033

Training epoch-19 batch-4
Running loss of epoch-19 batch-4 = 0.0005831618327647448

Training epoch-19 batch-5
Running loss of epoch-19 batch-5 = 0.0005562463775277138

Training epoch-19 batch-6
Running loss of epoch-19 batch-6 = 2.3074448108673096e-05

Training epoch-19 batch-7
Running loss of epoch-19 batch-7 = 0.0002901054685935378

Training epoch-19 batch-8
Running loss of epoch-19 batch-8 = 0.0003217636258341372

Training epoch-19 batch-9
Running loss of epoch-19 batch-9 = 2.9780552722513676e-05

Training epoch-19 batch-10
Running loss of epoch-19 batch-10 = 0.0010846381774172187

Training epoch-19 batch-11
Running loss of epoch-19 batch-11 = 0.00046099215978756547

Training epoch-19 batch-12
Running loss of epoch-19 batch-12 = 0.0003017127455677837

Training epoch-19 batch-13
Running loss of epoch-19 batch-13 = 1.9769766367971897e-05

Training epoch-19 batch-14
Running loss of epoch-19 batch-14 = 0.0012340930989012122

Training epoch-19 batch-15
Running loss of epoch-19 batch-15 = 9.872741065919399e-05

Training epoch-19 batch-16
Running loss of epoch-19 batch-16 = 5.522998981177807e-05

Training epoch-19 batch-17
Running loss of epoch-19 batch-17 = 0.00014142057625576854

Training epoch-19 batch-18
Running loss of epoch-19 batch-18 = 0.00017354218289256096

Training epoch-19 batch-19
Running loss of epoch-19 batch-19 = 0.000286911497823894

Training epoch-19 batch-20
Running loss of epoch-19 batch-20 = 4.7803507186472416e-05

Training epoch-19 batch-21
Running loss of epoch-19 batch-21 = 0.0001659761182963848

Training epoch-19 batch-22
Running loss of epoch-19 batch-22 = 0.00015630113193765283

Training epoch-19 batch-23
Running loss of epoch-19 batch-23 = 0.00023084011627361178

Training epoch-19 batch-24
Running loss of epoch-19 batch-24 = 0.000441125244833529

Training epoch-19 batch-25
Running loss of epoch-19 batch-25 = 0.0006089925882406533

Training epoch-19 batch-26
Running loss of epoch-19 batch-26 = 0.00046038959408178926

Training epoch-19 batch-27
Running loss of epoch-19 batch-27 = 0.00010863773059099913

Training epoch-19 batch-28
Running loss of epoch-19 batch-28 = 0.00012121652252972126

Training epoch-19 batch-29
Running loss of epoch-19 batch-29 = 0.00037606735713779926

Training epoch-19 batch-30
Running loss of epoch-19 batch-30 = 0.0001573473564349115

Training epoch-19 batch-31
Running loss of epoch-19 batch-31 = 0.0005952569772489369

Training epoch-19 batch-32
Running loss of epoch-19 batch-32 = 0.00027175917057320476

Training epoch-19 batch-33
Running loss of epoch-19 batch-33 = 5.5909156799316406e-05

Training epoch-19 batch-34
Running loss of epoch-19 batch-34 = 0.0003929492086172104

Training epoch-19 batch-35
Running loss of epoch-19 batch-35 = 1.950759906321764e-05

Training epoch-19 batch-36
Running loss of epoch-19 batch-36 = 0.00023578054970130324

Training epoch-19 batch-37
Running loss of epoch-19 batch-37 = 6.319349631667137e-05

Training epoch-19 batch-38
Running loss of epoch-19 batch-38 = 0.0003879830182995647

Training epoch-19 batch-39
Running loss of epoch-19 batch-39 = 8.858181536197662e-05

Training epoch-19 batch-40
Running loss of epoch-19 batch-40 = 0.00013012753333896399

Training epoch-19 batch-41
Running loss of epoch-19 batch-41 = 0.0005904780118726194

Training epoch-19 batch-42
Running loss of epoch-19 batch-42 = 0.0008473802008666098

Training epoch-19 batch-43
Running loss of epoch-19 batch-43 = 0.00011137360706925392

Training epoch-19 batch-44
Running loss of epoch-19 batch-44 = 0.0001226598396897316

Training epoch-19 batch-45
Running loss of epoch-19 batch-45 = 6.40420475974679e-05

Training epoch-19 batch-46
Running loss of epoch-19 batch-46 = 5.3743598982691765e-05

Training epoch-19 batch-47
Running loss of epoch-19 batch-47 = 0.001198042999021709

Training epoch-19 batch-48
Running loss of epoch-19 batch-48 = 0.0006890231743454933

Training epoch-19 batch-49
Running loss of epoch-19 batch-49 = 0.00020109908655285835

Training epoch-19 batch-50
Running loss of epoch-19 batch-50 = 0.00018776534125208855

Training epoch-19 batch-51
Running loss of epoch-19 batch-51 = 4.333234392106533e-05

Training epoch-19 batch-52
Running loss of epoch-19 batch-52 = 0.0002689738175831735

Training epoch-19 batch-53
Running loss of epoch-19 batch-53 = 0.0001060456270352006

Training epoch-19 batch-54
Running loss of epoch-19 batch-54 = 0.0001730236690491438

Training epoch-19 batch-55
Running loss of epoch-19 batch-55 = 9.069906082004309e-05

Training epoch-19 batch-56
Running loss of epoch-19 batch-56 = 0.0004466811951715499

Training epoch-19 batch-57
Running loss of epoch-19 batch-57 = 0.00021898321574553847

Training epoch-19 batch-58
Running loss of epoch-19 batch-58 = 0.0014862902462482452

Training epoch-19 batch-59
Running loss of epoch-19 batch-59 = 0.00018275371985509992

Training epoch-19 batch-60
Running loss of epoch-19 batch-60 = 0.0001348827499896288

Training epoch-19 batch-61
Running loss of epoch-19 batch-61 = 0.00014429748989641666

Training epoch-19 batch-62
Running loss of epoch-19 batch-62 = 4.41443407908082e-05

Training epoch-19 batch-63
Running loss of epoch-19 batch-63 = 0.00011364062083885074

Training epoch-19 batch-64
Running loss of epoch-19 batch-64 = 0.0003555737785063684

Training epoch-19 batch-65
Running loss of epoch-19 batch-65 = 0.00022848974913358688

Training epoch-19 batch-66
Running loss of epoch-19 batch-66 = 9.81832854449749e-05

Training epoch-19 batch-67
Running loss of epoch-19 batch-67 = 0.0006691861199215055

Training epoch-19 batch-68
Running loss of epoch-19 batch-68 = 0.000525431300047785

Training epoch-19 batch-69
Running loss of epoch-19 batch-69 = 5.711824633181095e-05

Training epoch-19 batch-70
Running loss of epoch-19 batch-70 = 0.0003248189459554851

Training epoch-19 batch-71
Running loss of epoch-19 batch-71 = 0.00021293864119797945

Training epoch-19 batch-72
Running loss of epoch-19 batch-72 = 3.7209014408290386e-05

Training epoch-19 batch-73
Running loss of epoch-19 batch-73 = 0.0001774599077180028

Training epoch-19 batch-74
Running loss of epoch-19 batch-74 = 0.00010900798952206969

Training epoch-19 batch-75
Running loss of epoch-19 batch-75 = 0.0001815248397178948

Training epoch-19 batch-76
Running loss of epoch-19 batch-76 = 0.0004184873541817069

Training epoch-19 batch-77
Running loss of epoch-19 batch-77 = 0.0001957606291398406

Training epoch-19 batch-78
Running loss of epoch-19 batch-78 = 0.0005905787693336606

Training epoch-19 batch-79
Running loss of epoch-19 batch-79 = 0.0001923668896779418

Training epoch-19 batch-80
Running loss of epoch-19 batch-80 = 2.8078327886760235e-05

Training epoch-19 batch-81
Running loss of epoch-19 batch-81 = 6.610993295907974e-06

Training epoch-19 batch-82
Running loss of epoch-19 batch-82 = 0.0008257522131316364

Training epoch-19 batch-83
Running loss of epoch-19 batch-83 = 8.304743096232414e-05

Training epoch-19 batch-84
Running loss of epoch-19 batch-84 = 0.0002027361188083887

Training epoch-19 batch-85
Running loss of epoch-19 batch-85 = 2.8564827516674995e-05

Training epoch-19 batch-86
Running loss of epoch-19 batch-86 = 1.407228410243988e-05

Training epoch-19 batch-87
Running loss of epoch-19 batch-87 = 5.151866935193539e-05

Training epoch-19 batch-88
Running loss of epoch-19 batch-88 = 0.0002783678937703371

Training epoch-19 batch-89
Running loss of epoch-19 batch-89 = 0.00014501949772238731

Training epoch-19 batch-90
Running loss of epoch-19 batch-90 = 4.537519998848438e-05

Training epoch-19 batch-91
Running loss of epoch-19 batch-91 = 0.00012419209815561771

Training epoch-19 batch-92
Running loss of epoch-19 batch-92 = 0.0005476539372466505

Training epoch-19 batch-93
Running loss of epoch-19 batch-93 = 2.6635127142071724e-05

Training epoch-19 batch-94
Running loss of epoch-19 batch-94 = 0.0005448159645311534

Training epoch-19 batch-95
Running loss of epoch-19 batch-95 = 5.080143455415964e-05

Training epoch-19 batch-96
Running loss of epoch-19 batch-96 = 0.00020516314543783665

Training epoch-19 batch-97
Running loss of epoch-19 batch-97 = 1.741759479045868e-05

Training epoch-19 batch-98
Running loss of epoch-19 batch-98 = 0.0002554274396970868

Training epoch-19 batch-99
Running loss of epoch-19 batch-99 = 3.697862848639488e-05

Training epoch-19 batch-100
Running loss of epoch-19 batch-100 = 0.00012059073196724057

Training epoch-19 batch-101
Running loss of epoch-19 batch-101 = 0.0004644762957468629

Training epoch-19 batch-102
Running loss of epoch-19 batch-102 = 2.9115471988916397e-05

Training epoch-19 batch-103
Running loss of epoch-19 batch-103 = 4.4431653805077076e-05

Training epoch-19 batch-104
Running loss of epoch-19 batch-104 = 3.910623490810394e-06

Training epoch-19 batch-105
Running loss of epoch-19 batch-105 = 3.921240568161011e-05

Training epoch-19 batch-106
Running loss of epoch-19 batch-106 = 0.0001861922792159021

Training epoch-19 batch-107
Running loss of epoch-19 batch-107 = 0.0003773879143409431

Training epoch-19 batch-108
Running loss of epoch-19 batch-108 = 0.00010448368266224861

Training epoch-19 batch-109
Running loss of epoch-19 batch-109 = 0.00015884259482845664

Training epoch-19 batch-110
Running loss of epoch-19 batch-110 = 0.0002448887098580599

Training epoch-19 batch-111
Running loss of epoch-19 batch-111 = 8.033390622586012e-05

Training epoch-19 batch-112
Running loss of epoch-19 batch-112 = 7.016805466264486e-05

Training epoch-19 batch-113
Running loss of epoch-19 batch-113 = 5.127233453094959e-05

Training epoch-19 batch-114
Running loss of epoch-19 batch-114 = 6.401108112186193e-05

Training epoch-19 batch-115
Running loss of epoch-19 batch-115 = 0.0004982654936611652

Training epoch-19 batch-116
Running loss of epoch-19 batch-116 = 0.0003780582337640226

Training epoch-19 batch-117
Running loss of epoch-19 batch-117 = 6.17011683061719e-05

Training epoch-19 batch-118
Running loss of epoch-19 batch-118 = 6.521143950521946e-05

Training epoch-19 batch-119
Running loss of epoch-19 batch-119 = 0.00014084158465266228

Training epoch-19 batch-120
Running loss of epoch-19 batch-120 = 4.69632213935256e-05

Training epoch-19 batch-121
Running loss of epoch-19 batch-121 = 0.00039513036608695984

Training epoch-19 batch-122
Running loss of epoch-19 batch-122 = 7.512432057410479e-05

Training epoch-19 batch-123
Running loss of epoch-19 batch-123 = 0.0012451724614948034

Training epoch-19 batch-124
Running loss of epoch-19 batch-124 = 5.0858245231211185e-05

Training epoch-19 batch-125
Running loss of epoch-19 batch-125 = 3.2235984690487385e-05

Training epoch-19 batch-126
Running loss of epoch-19 batch-126 = 0.0003432446392253041

Training epoch-19 batch-127
Running loss of epoch-19 batch-127 = 0.00020392262376844883

Training epoch-19 batch-128
Running loss of epoch-19 batch-128 = 0.00035183457657694817

Training epoch-19 batch-129
Running loss of epoch-19 batch-129 = 0.00014752126298844814

Training epoch-19 batch-130
Running loss of epoch-19 batch-130 = 0.00013190635945647955

Training epoch-19 batch-131
Running loss of epoch-19 batch-131 = 0.0001567275612615049

Training epoch-19 batch-132
Running loss of epoch-19 batch-132 = 0.00048587319906800985

Training epoch-19 batch-133
Running loss of epoch-19 batch-133 = 0.00019040587358176708

Training epoch-19 batch-134
Running loss of epoch-19 batch-134 = 0.00010416435543447733

Training epoch-19 batch-135
Running loss of epoch-19 batch-135 = 0.00018340820679441094

Training epoch-19 batch-136
Running loss of epoch-19 batch-136 = 6.815243978053331e-05

Training epoch-19 batch-137
Running loss of epoch-19 batch-137 = 0.00012930494267493486

Training epoch-19 batch-138
Running loss of epoch-19 batch-138 = 0.00014126469613984227

Training epoch-19 batch-139
Running loss of epoch-19 batch-139 = 0.0008476609364151955

Training epoch-19 batch-140
Running loss of epoch-19 batch-140 = 0.00014780310448259115

Training epoch-19 batch-141
Running loss of epoch-19 batch-141 = 0.0004238101828377694

Training epoch-19 batch-142
Running loss of epoch-19 batch-142 = 5.14910789206624e-05

Training epoch-19 batch-143
Running loss of epoch-19 batch-143 = 7.603858830407262e-05

Training epoch-19 batch-144
Running loss of epoch-19 batch-144 = 2.6477035135030746e-05

Training epoch-19 batch-145
Running loss of epoch-19 batch-145 = 0.00027496350230649114

Training epoch-19 batch-146
Running loss of epoch-19 batch-146 = 0.0004426650993991643

Training epoch-19 batch-147
Running loss of epoch-19 batch-147 = 2.019316889345646e-05

Training epoch-19 batch-148
Running loss of epoch-19 batch-148 = 0.00013646134175360203

Training epoch-19 batch-149
Running loss of epoch-19 batch-149 = 8.407142013311386e-05

Training epoch-19 batch-150
Running loss of epoch-19 batch-150 = 0.0003670646692626178

Training epoch-19 batch-151
Running loss of epoch-19 batch-151 = 1.2162956409156322e-05

Training epoch-19 batch-152
Running loss of epoch-19 batch-152 = 0.00010090169962495565

Training epoch-19 batch-153
Running loss of epoch-19 batch-153 = 6.869598291814327e-05

Training epoch-19 batch-154
Running loss of epoch-19 batch-154 = 0.0003348070022184402

Training epoch-19 batch-155
Running loss of epoch-19 batch-155 = 0.00018644845113158226

Training epoch-19 batch-156
Running loss of epoch-19 batch-156 = 0.0005488907918334007

Training epoch-19 batch-157
Running loss of epoch-19 batch-157 = 0.005555769428610802

Finished training epoch-19.



Average train loss at epoch-19 = 0.00025713136550039055

Started Evaluation

Average val loss at epoch-19 = 3.8730236182087348

Accuracy for classes:
Accuracy for class equals is: 68.98 %
Accuracy for class main is: 43.11 %
Accuracy for class setUp is: 59.34 %
Accuracy for class onCreate is: 41.79 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 30.59 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 23.32 %
Accuracy for class execute is: 20.48 %
Accuracy for class get is: 30.51 %

Overall Accuracy = 45.06 %

Finished Evaluation



Started training epoch-20


Training epoch-20 batch-1
Running loss of epoch-20 batch-1 = 0.00027237425092607737

Training epoch-20 batch-2
Running loss of epoch-20 batch-2 = 5.3394120186567307e-05

Training epoch-20 batch-3
Running loss of epoch-20 batch-3 = 2.79243104159832e-05

Training epoch-20 batch-4
Running loss of epoch-20 batch-4 = 0.00042200565803796053

Training epoch-20 batch-5
Running loss of epoch-20 batch-5 = 1.9628205336630344e-05

Training epoch-20 batch-6
Running loss of epoch-20 batch-6 = 0.0004335990233812481

Training epoch-20 batch-7
Running loss of epoch-20 batch-7 = 4.042650107294321e-05

Training epoch-20 batch-8
Running loss of epoch-20 batch-8 = 2.679100725799799e-05

Training epoch-20 batch-9
Running loss of epoch-20 batch-9 = 9.47294756770134e-06

Training epoch-20 batch-10
Running loss of epoch-20 batch-10 = 1.1732568964362144e-05

Training epoch-20 batch-11
Running loss of epoch-20 batch-11 = 0.00011567189358174801

Training epoch-20 batch-12
Running loss of epoch-20 batch-12 = 0.00033174926647916436

Training epoch-20 batch-13
Running loss of epoch-20 batch-13 = 1.4883815310895443e-05

Training epoch-20 batch-14
Running loss of epoch-20 batch-14 = 8.312857244163752e-05

Training epoch-20 batch-15
Running loss of epoch-20 batch-15 = 1.55371380969882e-05

Training epoch-20 batch-16
Running loss of epoch-20 batch-16 = 0.00014826288679614663

Training epoch-20 batch-17
Running loss of epoch-20 batch-17 = 0.000405184953706339

Training epoch-20 batch-18
Running loss of epoch-20 batch-18 = 4.4095213524997234e-05

Training epoch-20 batch-19
Running loss of epoch-20 batch-19 = 0.0001961776870302856

Training epoch-20 batch-20
Running loss of epoch-20 batch-20 = 8.239736780524254e-05

Training epoch-20 batch-21
Running loss of epoch-20 batch-21 = 8.096685633063316e-06

Training epoch-20 batch-22
Running loss of epoch-20 batch-22 = 3.979261964559555e-05

Training epoch-20 batch-23
Running loss of epoch-20 batch-23 = 9.344966383650899e-05

Training epoch-20 batch-24
Running loss of epoch-20 batch-24 = 4.395644646137953e-05

Training epoch-20 batch-25
Running loss of epoch-20 batch-25 = 0.00017948576714843512

Training epoch-20 batch-26
Running loss of epoch-20 batch-26 = 7.986254058778286e-05

Training epoch-20 batch-27
Running loss of epoch-20 batch-27 = 7.565657142549753e-05

Training epoch-20 batch-28
Running loss of epoch-20 batch-28 = 2.7739908546209335e-05

Training epoch-20 batch-29
Running loss of epoch-20 batch-29 = 0.0005225753411650658

Training epoch-20 batch-30
Running loss of epoch-20 batch-30 = 0.00011657335562631488

Training epoch-20 batch-31
Running loss of epoch-20 batch-31 = 4.1256193071603775e-05

Training epoch-20 batch-32
Running loss of epoch-20 batch-32 = 0.00033879204420372844

Training epoch-20 batch-33
Running loss of epoch-20 batch-33 = 3.390491474419832e-05

Training epoch-20 batch-34
Running loss of epoch-20 batch-34 = 0.00016608636360615492

Training epoch-20 batch-35
Running loss of epoch-20 batch-35 = 2.6782043278217316e-05

Training epoch-20 batch-36
Running loss of epoch-20 batch-36 = 0.00020819972269237041

Training epoch-20 batch-37
Running loss of epoch-20 batch-37 = 7.441616617143154e-05

Training epoch-20 batch-38
Running loss of epoch-20 batch-38 = 4.174595233052969e-05

Training epoch-20 batch-39
Running loss of epoch-20 batch-39 = 0.00011431681923568249

Training epoch-20 batch-40
Running loss of epoch-20 batch-40 = 0.00011964316945523024

Training epoch-20 batch-41
Running loss of epoch-20 batch-41 = 1.048622652888298e-05

Training epoch-20 batch-42
Running loss of epoch-20 batch-42 = 0.00017024436965584755

Training epoch-20 batch-43
Running loss of epoch-20 batch-43 = 1.4482531696557999e-05

Training epoch-20 batch-44
Running loss of epoch-20 batch-44 = 0.0001258020638488233

Training epoch-20 batch-45
Running loss of epoch-20 batch-45 = 0.0002760244533419609

Training epoch-20 batch-46
Running loss of epoch-20 batch-46 = 0.00041264842730015516

Training epoch-20 batch-47
Running loss of epoch-20 batch-47 = 0.00012106343638151884

Training epoch-20 batch-48
Running loss of epoch-20 batch-48 = 0.00029869694844819605

Training epoch-20 batch-49
Running loss of epoch-20 batch-49 = 1.612934283912182e-05

Training epoch-20 batch-50
Running loss of epoch-20 batch-50 = 2.4229055270552635e-05

Training epoch-20 batch-51
Running loss of epoch-20 batch-51 = 0.0003173619043081999

Training epoch-20 batch-52
Running loss of epoch-20 batch-52 = 3.946258220821619e-05

Training epoch-20 batch-53
Running loss of epoch-20 batch-53 = 1.7146463505923748e-05

Training epoch-20 batch-54
Running loss of epoch-20 batch-54 = 1.865765079855919e-05

Training epoch-20 batch-55
Running loss of epoch-20 batch-55 = 1.625611912459135e-05

Training epoch-20 batch-56
Running loss of epoch-20 batch-56 = 7.456913590431213e-05

Training epoch-20 batch-57
Running loss of epoch-20 batch-57 = 6.713415496051311e-05

Training epoch-20 batch-58
Running loss of epoch-20 batch-58 = 0.00031863426556810737

Training epoch-20 batch-59
Running loss of epoch-20 batch-59 = 1.127016730606556e-05

Training epoch-20 batch-60
Running loss of epoch-20 batch-60 = 5.367887206375599e-05

Training epoch-20 batch-61
Running loss of epoch-20 batch-61 = 4.920258652418852e-05

Training epoch-20 batch-62
Running loss of epoch-20 batch-62 = 2.4223816581070423e-05

Training epoch-20 batch-63
Running loss of epoch-20 batch-63 = 0.000778783462010324

Training epoch-20 batch-64
Running loss of epoch-20 batch-64 = 0.00043450132943689823

Training epoch-20 batch-65
Running loss of epoch-20 batch-65 = 2.9290211386978626e-05

Training epoch-20 batch-66
Running loss of epoch-20 batch-66 = 5.5044074542820454e-05

Training epoch-20 batch-67
Running loss of epoch-20 batch-67 = 0.00044498767238110304

Training epoch-20 batch-68
Running loss of epoch-20 batch-68 = 7.310300134122372e-06

Training epoch-20 batch-69
Running loss of epoch-20 batch-69 = 3.458792343735695e-05

Training epoch-20 batch-70
Running loss of epoch-20 batch-70 = 0.00020876445341855288

Training epoch-20 batch-71
Running loss of epoch-20 batch-71 = 0.00010150705929845572

Training epoch-20 batch-72
Running loss of epoch-20 batch-72 = 0.00010630080942064524

Training epoch-20 batch-73
Running loss of epoch-20 batch-73 = 0.00018012139480561018

Training epoch-20 batch-74
Running loss of epoch-20 batch-74 = 1.3890443369746208e-05

Training epoch-20 batch-75
Running loss of epoch-20 batch-75 = 1.259299460798502e-05

Training epoch-20 batch-76
Running loss of epoch-20 batch-76 = 6.405636668205261e-06

Training epoch-20 batch-77
Running loss of epoch-20 batch-77 = 0.0006603423389606178

Training epoch-20 batch-78
Running loss of epoch-20 batch-78 = 0.0005574757233262062

Training epoch-20 batch-79
Running loss of epoch-20 batch-79 = 0.00010643794666975737

Training epoch-20 batch-80
Running loss of epoch-20 batch-80 = 6.374937947839499e-05

Training epoch-20 batch-81
Running loss of epoch-20 batch-81 = 0.00045198178850114346

Training epoch-20 batch-82
Running loss of epoch-20 batch-82 = 0.0003755482903216034

Training epoch-20 batch-83
Running loss of epoch-20 batch-83 = 0.00027387021691538393

Training epoch-20 batch-84
Running loss of epoch-20 batch-84 = 4.8727612011134624e-05

Training epoch-20 batch-85
Running loss of epoch-20 batch-85 = 0.00014643691247329116

Training epoch-20 batch-86
Running loss of epoch-20 batch-86 = 0.0002716197050176561

Training epoch-20 batch-87
Running loss of epoch-20 batch-87 = 0.0001831681584008038

Training epoch-20 batch-88
Running loss of epoch-20 batch-88 = 0.00020578986732289195

Training epoch-20 batch-89
Running loss of epoch-20 batch-89 = 0.00010300066787749529

Training epoch-20 batch-90
Running loss of epoch-20 batch-90 = 0.00012578809401020408

Training epoch-20 batch-91
Running loss of epoch-20 batch-91 = 0.00035605012089945376

Training epoch-20 batch-92
Running loss of epoch-20 batch-92 = 6.774847861379385e-05

Training epoch-20 batch-93
Running loss of epoch-20 batch-93 = 1.9168364815413952e-05

Training epoch-20 batch-94
Running loss of epoch-20 batch-94 = 4.584807902574539e-05

Training epoch-20 batch-95
Running loss of epoch-20 batch-95 = 5.2616000175476074e-05

Training epoch-20 batch-96
Running loss of epoch-20 batch-96 = 9.399140253663063e-06

Training epoch-20 batch-97
Running loss of epoch-20 batch-97 = 3.119546454399824e-05

Training epoch-20 batch-98
Running loss of epoch-20 batch-98 = 0.0003219813806936145

Training epoch-20 batch-99
Running loss of epoch-20 batch-99 = 6.704544648528099e-05

Training epoch-20 batch-100
Running loss of epoch-20 batch-100 = 7.24239507690072e-05

Training epoch-20 batch-101
Running loss of epoch-20 batch-101 = 0.00034552154829725623

Training epoch-20 batch-102
Running loss of epoch-20 batch-102 = 0.0008522492134943604

Training epoch-20 batch-103
Running loss of epoch-20 batch-103 = 0.0004882007488049567

Training epoch-20 batch-104
Running loss of epoch-20 batch-104 = 5.148700438439846e-05

Training epoch-20 batch-105
Running loss of epoch-20 batch-105 = 6.039510481059551e-06

Training epoch-20 batch-106
Running loss of epoch-20 batch-106 = 6.369245238602161e-05

Training epoch-20 batch-107
Running loss of epoch-20 batch-107 = 0.00015368318418040872

Training epoch-20 batch-108
Running loss of epoch-20 batch-108 = 0.00024616235168650746

Training epoch-20 batch-109
Running loss of epoch-20 batch-109 = 0.0007909629493951797

Training epoch-20 batch-110
Running loss of epoch-20 batch-110 = 0.00043106358498334885

Training epoch-20 batch-111
Running loss of epoch-20 batch-111 = 1.2120697647333145e-05

Training epoch-20 batch-112
Running loss of epoch-20 batch-112 = 0.00040972104761749506

Training epoch-20 batch-113
Running loss of epoch-20 batch-113 = 2.7279951609671116e-05

Training epoch-20 batch-114
Running loss of epoch-20 batch-114 = 1.3865646906197071e-05

Training epoch-20 batch-115
Running loss of epoch-20 batch-115 = 3.052852116525173e-05

Training epoch-20 batch-116
Running loss of epoch-20 batch-116 = 0.00037638965295627713

Training epoch-20 batch-117
Running loss of epoch-20 batch-117 = 0.0001889636041596532

Training epoch-20 batch-118
Running loss of epoch-20 batch-118 = 8.815689943730831e-05

Training epoch-20 batch-119
Running loss of epoch-20 batch-119 = 2.1221931092441082e-05

Training epoch-20 batch-120
Running loss of epoch-20 batch-120 = 0.00014214590191841125

Training epoch-20 batch-121
Running loss of epoch-20 batch-121 = 0.0003038336872123182

Training epoch-20 batch-122
Running loss of epoch-20 batch-122 = 1.245248131453991e-05

Training epoch-20 batch-123
Running loss of epoch-20 batch-123 = 7.345352787524462e-05

Training epoch-20 batch-124
Running loss of epoch-20 batch-124 = 4.9557886086404324e-05

Training epoch-20 batch-125
Running loss of epoch-20 batch-125 = 0.00015711504966020584

Training epoch-20 batch-126
Running loss of epoch-20 batch-126 = 9.688828140497208e-05

Training epoch-20 batch-127
Running loss of epoch-20 batch-127 = 0.00012253463501110673

Training epoch-20 batch-128
Running loss of epoch-20 batch-128 = 1.802935730665922e-05

Training epoch-20 batch-129
Running loss of epoch-20 batch-129 = 9.271164890378714e-05

Training epoch-20 batch-130
Running loss of epoch-20 batch-130 = 2.755282912403345e-05

Training epoch-20 batch-131
Running loss of epoch-20 batch-131 = 0.0002516817767173052

Training epoch-20 batch-132
Running loss of epoch-20 batch-132 = 0.0001881156931631267

Training epoch-20 batch-133
Running loss of epoch-20 batch-133 = 2.275907900184393e-05

Training epoch-20 batch-134
Running loss of epoch-20 batch-134 = 0.00041981926187872887

Training epoch-20 batch-135
Running loss of epoch-20 batch-135 = 3.21016414090991e-05

Training epoch-20 batch-136
Running loss of epoch-20 batch-136 = 0.0001368243247270584

Training epoch-20 batch-137
Running loss of epoch-20 batch-137 = 0.00045856795622967184

Training epoch-20 batch-138
Running loss of epoch-20 batch-138 = 7.742887828499079e-05

Training epoch-20 batch-139
Running loss of epoch-20 batch-139 = 3.048451617360115e-06

Training epoch-20 batch-140
Running loss of epoch-20 batch-140 = 0.00019547564443200827

Training epoch-20 batch-141
Running loss of epoch-20 batch-141 = 8.57658451423049e-05

Training epoch-20 batch-142
Running loss of epoch-20 batch-142 = 1.4656921848654747e-05

Training epoch-20 batch-143
Running loss of epoch-20 batch-143 = 1.678825356066227e-05

Training epoch-20 batch-144
Running loss of epoch-20 batch-144 = 0.0003675711341202259

Training epoch-20 batch-145
Running loss of epoch-20 batch-145 = 0.00032518006628379226

Training epoch-20 batch-146
Running loss of epoch-20 batch-146 = 0.00043777102837339044

Training epoch-20 batch-147
Running loss of epoch-20 batch-147 = 0.0005399884539656341

Training epoch-20 batch-148
Running loss of epoch-20 batch-148 = 0.0001360195456072688

Training epoch-20 batch-149
Running loss of epoch-20 batch-149 = 3.6856974475085735e-05

Training epoch-20 batch-150
Running loss of epoch-20 batch-150 = 0.00023661984596401453

Training epoch-20 batch-151
Running loss of epoch-20 batch-151 = 0.0001617888337932527

Training epoch-20 batch-152
Running loss of epoch-20 batch-152 = 5.757436156272888e-05

Training epoch-20 batch-153
Running loss of epoch-20 batch-153 = 6.637373007833958e-05

Training epoch-20 batch-154
Running loss of epoch-20 batch-154 = 2.804165706038475e-05

Training epoch-20 batch-155
Running loss of epoch-20 batch-155 = 0.00019372469978407025

Training epoch-20 batch-156
Running loss of epoch-20 batch-156 = 2.8469483368098736e-05

Training epoch-20 batch-157
Running loss of epoch-20 batch-157 = 0.0002225227653980255

Finished training epoch-20.



Average train loss at epoch-20 = 0.00015925699304789306

Started Evaluation

Average val loss at epoch-20 = 3.8725851633046804

Accuracy for classes:
Accuracy for class equals is: 68.15 %
Accuracy for class main is: 55.74 %
Accuracy for class setUp is: 52.30 %
Accuracy for class onCreate is: 46.91 %
Accuracy for class toString is: 44.71 %
Accuracy for class run is: 32.65 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 23.09 %
Accuracy for class execute is: 16.47 %
Accuracy for class get is: 27.95 %

Overall Accuracy = 46.54 %


Best Accuracy = 46.54 % at Epoch-20
Saving model after best epoch-20

Finished Evaluation



Started training epoch-21


Training epoch-21 batch-1
Running loss of epoch-21 batch-1 = 8.52649100124836e-06

Training epoch-21 batch-2
Running loss of epoch-21 batch-2 = 5.1276409067213535e-05

Training epoch-21 batch-3
Running loss of epoch-21 batch-3 = 3.442424349486828e-05

Training epoch-21 batch-4
Running loss of epoch-21 batch-4 = 4.7116889618337154e-05

Training epoch-21 batch-5
Running loss of epoch-21 batch-5 = 0.0001440400956198573

Training epoch-21 batch-6
Running loss of epoch-21 batch-6 = 1.093931496143341e-05

Training epoch-21 batch-7
Running loss of epoch-21 batch-7 = 3.1906296499073505e-05

Training epoch-21 batch-8
Running loss of epoch-21 batch-8 = 7.426633965224028e-05

Training epoch-21 batch-9
Running loss of epoch-21 batch-9 = 0.00019520410569384694

Training epoch-21 batch-10
Running loss of epoch-21 batch-10 = 0.0008858742658048868

Training epoch-21 batch-11
Running loss of epoch-21 batch-11 = 0.00010369508527219296

Training epoch-21 batch-12
Running loss of epoch-21 batch-12 = 3.059918526560068e-05

Training epoch-21 batch-13
Running loss of epoch-21 batch-13 = 6.121746264398098e-05

Training epoch-21 batch-14
Running loss of epoch-21 batch-14 = 2.0153354853391647e-05

Training epoch-21 batch-15
Running loss of epoch-21 batch-15 = 0.000179298163857311

Training epoch-21 batch-16
Running loss of epoch-21 batch-16 = 0.00031172652961686254

Training epoch-21 batch-17
Running loss of epoch-21 batch-17 = 1.8516439013183117e-05

Training epoch-21 batch-18
Running loss of epoch-21 batch-18 = 5.508912727236748e-05

Training epoch-21 batch-19
Running loss of epoch-21 batch-19 = 5.000573582947254e-05

Training epoch-21 batch-20
Running loss of epoch-21 batch-20 = 0.00025862394249998033

Training epoch-21 batch-21
Running loss of epoch-21 batch-21 = 1.0396121069788933e-05

Training epoch-21 batch-22
Running loss of epoch-21 batch-22 = 1.7352518625557423e-05

Training epoch-21 batch-23
Running loss of epoch-21 batch-23 = 0.0002075396478176117

Training epoch-21 batch-24
Running loss of epoch-21 batch-24 = 0.0004065236134920269

Training epoch-21 batch-25
Running loss of epoch-21 batch-25 = 0.00012600678019225597

Training epoch-21 batch-26
Running loss of epoch-21 batch-26 = 0.0004207136225886643

Training epoch-21 batch-27
Running loss of epoch-21 batch-27 = 0.00012840807903558016

Training epoch-21 batch-28
Running loss of epoch-21 batch-28 = 7.020338671281934e-05

Training epoch-21 batch-29
Running loss of epoch-21 batch-29 = 4.3961452320218086e-05

Training epoch-21 batch-30
Running loss of epoch-21 batch-30 = 1.6671721823513508e-05

Training epoch-21 batch-31
Running loss of epoch-21 batch-31 = 0.00027095573022961617

Training epoch-21 batch-32
Running loss of epoch-21 batch-32 = 1.1890311725437641e-05

Training epoch-21 batch-33
Running loss of epoch-21 batch-33 = 0.00040386125328950584

Training epoch-21 batch-34
Running loss of epoch-21 batch-34 = 5.846843123435974e-06

Training epoch-21 batch-35
Running loss of epoch-21 batch-35 = 0.00020795519230887294

Training epoch-21 batch-36
Running loss of epoch-21 batch-36 = 0.00022987998090684414

Training epoch-21 batch-37
Running loss of epoch-21 batch-37 = 7.918814662843943e-05

Training epoch-21 batch-38
Running loss of epoch-21 batch-38 = 2.1114712581038475e-05

Training epoch-21 batch-39
Running loss of epoch-21 batch-39 = 3.377546090632677e-05

Training epoch-21 batch-40
Running loss of epoch-21 batch-40 = 8.235464338213205e-05

Training epoch-21 batch-41
Running loss of epoch-21 batch-41 = 7.547007407993078e-05

Training epoch-21 batch-42
Running loss of epoch-21 batch-42 = 3.0455528758466244e-05

Training epoch-21 batch-43
Running loss of epoch-21 batch-43 = 0.0001060724607668817

Training epoch-21 batch-44
Running loss of epoch-21 batch-44 = 3.797688987106085e-05

Training epoch-21 batch-45
Running loss of epoch-21 batch-45 = 5.115987733006477e-06

Training epoch-21 batch-46
Running loss of epoch-21 batch-46 = 0.0002664616331458092

Training epoch-21 batch-47
Running loss of epoch-21 batch-47 = 2.0629609934985638e-05

Training epoch-21 batch-48
Running loss of epoch-21 batch-48 = 4.793261177837849e-05

Training epoch-21 batch-49
Running loss of epoch-21 batch-49 = 5.707715172320604e-05

Training epoch-21 batch-50
Running loss of epoch-21 batch-50 = 0.00011021591490134597

Training epoch-21 batch-51
Running loss of epoch-21 batch-51 = 0.00024982611648738384

Training epoch-21 batch-52
Running loss of epoch-21 batch-52 = 5.913642235100269e-05

Training epoch-21 batch-53
Running loss of epoch-21 batch-53 = 0.00010654819197952747

Training epoch-21 batch-54
Running loss of epoch-21 batch-54 = 4.6758330427110195e-05

Training epoch-21 batch-55
Running loss of epoch-21 batch-55 = 0.00024288779241032898

Training epoch-21 batch-56
Running loss of epoch-21 batch-56 = 2.5470159016549587e-05

Training epoch-21 batch-57
Running loss of epoch-21 batch-57 = 0.00012482819147408009

Training epoch-21 batch-58
Running loss of epoch-21 batch-58 = 0.0001931526348926127

Training epoch-21 batch-59
Running loss of epoch-21 batch-59 = 0.00011878449004143476

Training epoch-21 batch-60
Running loss of epoch-21 batch-60 = 8.883711416274309e-05

Training epoch-21 batch-61
Running loss of epoch-21 batch-61 = 9.004923049360514e-05

Training epoch-21 batch-62
Running loss of epoch-21 batch-62 = 0.00023676431737840176

Training epoch-21 batch-63
Running loss of epoch-21 batch-63 = 0.00028230727184563875

Training epoch-21 batch-64
Running loss of epoch-21 batch-64 = 2.7581467293202877e-05

Training epoch-21 batch-65
Running loss of epoch-21 batch-65 = 9.276787750422955e-06

Training epoch-21 batch-66
Running loss of epoch-21 batch-66 = 5.453790072351694e-05

Training epoch-21 batch-67
Running loss of epoch-21 batch-67 = 0.0002780861686915159

Training epoch-21 batch-68
Running loss of epoch-21 batch-68 = 4.778767470270395e-05

Training epoch-21 batch-69
Running loss of epoch-21 batch-69 = 0.00035727015347220004

Training epoch-21 batch-70
Running loss of epoch-21 batch-70 = 6.154412403702736e-06

Training epoch-21 batch-71
Running loss of epoch-21 batch-71 = 6.837164983153343e-05

Training epoch-21 batch-72
Running loss of epoch-21 batch-72 = 5.2554765716195107e-05

Training epoch-21 batch-73
Running loss of epoch-21 batch-73 = 0.0003016935079358518

Training epoch-21 batch-74
Running loss of epoch-21 batch-74 = 5.775399040430784e-05

Training epoch-21 batch-75
Running loss of epoch-21 batch-75 = 0.00010716577526181936

Training epoch-21 batch-76
Running loss of epoch-21 batch-76 = 0.000850070733577013

Training epoch-21 batch-77
Running loss of epoch-21 batch-77 = 0.00029659393476322293

Training epoch-21 batch-78
Running loss of epoch-21 batch-78 = 8.675706340000033e-05

Training epoch-21 batch-79
Running loss of epoch-21 batch-79 = 1.6615260392427444e-05

Training epoch-21 batch-80
Running loss of epoch-21 batch-80 = 0.00024564313935115933

Training epoch-21 batch-81
Running loss of epoch-21 batch-81 = 0.00010220689000561833

Training epoch-21 batch-82
Running loss of epoch-21 batch-82 = 0.00011722644558176398

Training epoch-21 batch-83
Running loss of epoch-21 batch-83 = 1.9508646801114082e-05

Training epoch-21 batch-84
Running loss of epoch-21 batch-84 = 9.960995521396399e-05

Training epoch-21 batch-85
Running loss of epoch-21 batch-85 = 1.7257872968912125e-05

Training epoch-21 batch-86
Running loss of epoch-21 batch-86 = 0.000425137608544901

Training epoch-21 batch-87
Running loss of epoch-21 batch-87 = 2.348783891648054e-05

Training epoch-21 batch-88
Running loss of epoch-21 batch-88 = 1.1826399713754654e-05

Training epoch-21 batch-89
Running loss of epoch-21 batch-89 = 5.2033690735697746e-05

Training epoch-21 batch-90
Running loss of epoch-21 batch-90 = 9.08970832824707e-06

Training epoch-21 batch-91
Running loss of epoch-21 batch-91 = 4.335551057010889e-05

Training epoch-21 batch-92
Running loss of epoch-21 batch-92 = 5.4133241064846516e-05

Training epoch-21 batch-93
Running loss of epoch-21 batch-93 = 0.00013212708290666342

Training epoch-21 batch-94
Running loss of epoch-21 batch-94 = 0.00034398259595036507

Training epoch-21 batch-95
Running loss of epoch-21 batch-95 = 0.0004357964498922229

Training epoch-21 batch-96
Running loss of epoch-21 batch-96 = 5.7769357226789e-05

Training epoch-21 batch-97
Running loss of epoch-21 batch-97 = 0.0002982706646434963

Training epoch-21 batch-98
Running loss of epoch-21 batch-98 = 3.4345430321991444e-05

Training epoch-21 batch-99
Running loss of epoch-21 batch-99 = 0.0001054461463354528

Training epoch-21 batch-100
Running loss of epoch-21 batch-100 = 3.498210571706295e-05

Training epoch-21 batch-101
Running loss of epoch-21 batch-101 = 6.732373731210828e-05

Training epoch-21 batch-102
Running loss of epoch-21 batch-102 = 3.030186053365469e-05

Training epoch-21 batch-103
Running loss of epoch-21 batch-103 = 5.2770948968827724e-05

Training epoch-21 batch-104
Running loss of epoch-21 batch-104 = 2.9408372938632965e-05

Training epoch-21 batch-105
Running loss of epoch-21 batch-105 = 2.995238173753023e-05

Training epoch-21 batch-106
Running loss of epoch-21 batch-106 = 2.6176217943429947e-05

Training epoch-21 batch-107
Running loss of epoch-21 batch-107 = 0.0004151877074036747

Training epoch-21 batch-108
Running loss of epoch-21 batch-108 = 7.453851867467165e-05

Training epoch-21 batch-109
Running loss of epoch-21 batch-109 = 7.724110037088394e-05

Training epoch-21 batch-110
Running loss of epoch-21 batch-110 = 3.835046663880348e-05

Training epoch-21 batch-111
Running loss of epoch-21 batch-111 = 9.573000716045499e-05

Training epoch-21 batch-112
Running loss of epoch-21 batch-112 = 8.037051884457469e-05

Training epoch-21 batch-113
Running loss of epoch-21 batch-113 = 0.00017316004959866405

Training epoch-21 batch-114
Running loss of epoch-21 batch-114 = 5.7451718021184206e-05

Training epoch-21 batch-115
Running loss of epoch-21 batch-115 = 6.911286618560553e-05

Training epoch-21 batch-116
Running loss of epoch-21 batch-116 = 3.978924360126257e-05

Training epoch-21 batch-117
Running loss of epoch-21 batch-117 = 3.093620762228966e-05

Training epoch-21 batch-118
Running loss of epoch-21 batch-118 = 0.00012623326620087028

Training epoch-21 batch-119
Running loss of epoch-21 batch-119 = 0.0002719741314649582

Training epoch-21 batch-120
Running loss of epoch-21 batch-120 = 4.242092836648226e-05

Training epoch-21 batch-121
Running loss of epoch-21 batch-121 = 0.0004759212606586516

Training epoch-21 batch-122
Running loss of epoch-21 batch-122 = 3.134203143417835e-05

Training epoch-21 batch-123
Running loss of epoch-21 batch-123 = 0.0004310795629862696

Training epoch-21 batch-124
Running loss of epoch-21 batch-124 = 4.559592343866825e-05

Training epoch-21 batch-125
Running loss of epoch-21 batch-125 = 0.0003225495747756213

Training epoch-21 batch-126
Running loss of epoch-21 batch-126 = 6.378861144185066e-06

Training epoch-21 batch-127
Running loss of epoch-21 batch-127 = 0.0006549210520461202

Training epoch-21 batch-128
Running loss of epoch-21 batch-128 = 0.000174824264831841

Training epoch-21 batch-129
Running loss of epoch-21 batch-129 = 1.3924087397754192e-05

Training epoch-21 batch-130
Running loss of epoch-21 batch-130 = 3.710517194122076e-05

Training epoch-21 batch-131
Running loss of epoch-21 batch-131 = 0.0005417695501819253

Training epoch-21 batch-132
Running loss of epoch-21 batch-132 = 0.000342568033374846

Training epoch-21 batch-133
Running loss of epoch-21 batch-133 = 1.1245021596550941e-05

Training epoch-21 batch-134
Running loss of epoch-21 batch-134 = 1.2854463420808315e-05

Training epoch-21 batch-135
Running loss of epoch-21 batch-135 = 5.749438423663378e-05

Training epoch-21 batch-136
Running loss of epoch-21 batch-136 = 0.0003816826792899519

Training epoch-21 batch-137
Running loss of epoch-21 batch-137 = 0.0003768710303120315

Training epoch-21 batch-138
Running loss of epoch-21 batch-138 = 0.00010619935346767306

Training epoch-21 batch-139
Running loss of epoch-21 batch-139 = 6.945338100194931e-06

Training epoch-21 batch-140
Running loss of epoch-21 batch-140 = 6.157986354082823e-05

Training epoch-21 batch-141
Running loss of epoch-21 batch-141 = 0.00022891885600984097

Training epoch-21 batch-142
Running loss of epoch-21 batch-142 = 7.655343506485224e-05

Training epoch-21 batch-143
Running loss of epoch-21 batch-143 = 0.0001419887412339449

Training epoch-21 batch-144
Running loss of epoch-21 batch-144 = 4.550674930214882e-06

Training epoch-21 batch-145
Running loss of epoch-21 batch-145 = 7.434096187353134e-05

Training epoch-21 batch-146
Running loss of epoch-21 batch-146 = 4.913751035928726e-05

Training epoch-21 batch-147
Running loss of epoch-21 batch-147 = 4.7930749133229256e-05

Training epoch-21 batch-148
Running loss of epoch-21 batch-148 = 0.0008247220539487898

Training epoch-21 batch-149
Running loss of epoch-21 batch-149 = 2.6799156330525875e-05

Training epoch-21 batch-150
Running loss of epoch-21 batch-150 = 2.809939906001091e-05

Training epoch-21 batch-151
Running loss of epoch-21 batch-151 = 1.2092641554772854e-05

Training epoch-21 batch-152
Running loss of epoch-21 batch-152 = 1.3953074812889099e-05

Training epoch-21 batch-153
Running loss of epoch-21 batch-153 = 0.00010718533303588629

Training epoch-21 batch-154
Running loss of epoch-21 batch-154 = 2.849975135177374e-05

Training epoch-21 batch-155
Running loss of epoch-21 batch-155 = 2.410390879958868e-05

Training epoch-21 batch-156
Running loss of epoch-21 batch-156 = 8.647912181913853e-06

Training epoch-21 batch-157
Running loss of epoch-21 batch-157 = 0.0015421612188220024

Finished training epoch-21.



Average train loss at epoch-21 = 0.00013616455085575582

Started Evaluation

Average val loss at epoch-21 = 3.9942237248546197

Accuracy for classes:
Accuracy for class equals is: 69.31 %
Accuracy for class main is: 53.61 %
Accuracy for class setUp is: 56.07 %
Accuracy for class onCreate is: 46.48 %
Accuracy for class toString is: 39.93 %
Accuracy for class run is: 32.42 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 18.39 %
Accuracy for class execute is: 21.69 %
Accuracy for class get is: 32.31 %

Overall Accuracy = 46.69 %


Best Accuracy = 46.69 % at Epoch-21
Saving model after best epoch-21

Finished Evaluation



Started training epoch-22


Training epoch-22 batch-1
Running loss of epoch-22 batch-1 = 3.4262193366885185e-05

Training epoch-22 batch-2
Running loss of epoch-22 batch-2 = 0.0003832877555396408

Training epoch-22 batch-3
Running loss of epoch-22 batch-3 = 0.00040674523916095495

Training epoch-22 batch-4
Running loss of epoch-22 batch-4 = 2.0170933566987514e-05

Training epoch-22 batch-5
Running loss of epoch-22 batch-5 = 1.2432457879185677e-05

Training epoch-22 batch-6
Running loss of epoch-22 batch-6 = 1.2104283086955547e-05

Training epoch-22 batch-7
Running loss of epoch-22 batch-7 = 6.04183878749609e-05

Training epoch-22 batch-8
Running loss of epoch-22 batch-8 = 0.00021598907187581062

Training epoch-22 batch-9
Running loss of epoch-22 batch-9 = 0.00011370162246748805

Training epoch-22 batch-10
Running loss of epoch-22 batch-10 = 6.981287151575089e-05

Training epoch-22 batch-11
Running loss of epoch-22 batch-11 = 1.355423592031002e-05

Training epoch-22 batch-12
Running loss of epoch-22 batch-12 = 1.6613281331956387e-05

Training epoch-22 batch-13
Running loss of epoch-22 batch-13 = 6.350746843963861e-05

Training epoch-22 batch-14
Running loss of epoch-22 batch-14 = 9.483913891017437e-05

Training epoch-22 batch-15
Running loss of epoch-22 batch-15 = 9.021488949656487e-06

Training epoch-22 batch-16
Running loss of epoch-22 batch-16 = 0.00029408797854557633

Training epoch-22 batch-17
Running loss of epoch-22 batch-17 = 1.1062598787248135e-05

Training epoch-22 batch-18
Running loss of epoch-22 batch-18 = 1.4804303646087646e-05

Training epoch-22 batch-19
Running loss of epoch-22 batch-19 = 2.5303103029727936e-05

Training epoch-22 batch-20
Running loss of epoch-22 batch-20 = 6.262946408241987e-05

Training epoch-22 batch-21
Running loss of epoch-22 batch-21 = 9.172945283353329e-06

Training epoch-22 batch-22
Running loss of epoch-22 batch-22 = 4.7959620133042336e-05

Training epoch-22 batch-23
Running loss of epoch-22 batch-23 = 9.036273695528507e-06

Training epoch-22 batch-24
Running loss of epoch-22 batch-24 = 0.00018650200217962265

Training epoch-22 batch-25
Running loss of epoch-22 batch-25 = 3.9864564314484596e-05

Training epoch-22 batch-26
Running loss of epoch-22 batch-26 = 0.0002444246201775968

Training epoch-22 batch-27
Running loss of epoch-22 batch-27 = 0.00035511248279362917

Training epoch-22 batch-28
Running loss of epoch-22 batch-28 = 0.00032374076545238495

Training epoch-22 batch-29
Running loss of epoch-22 batch-29 = 0.0001899784547276795

Training epoch-22 batch-30
Running loss of epoch-22 batch-30 = 4.28762286901474e-05

Training epoch-22 batch-31
Running loss of epoch-22 batch-31 = 0.00010699662379920483

Training epoch-22 batch-32
Running loss of epoch-22 batch-32 = 3.7392135709524155e-05

Training epoch-22 batch-33
Running loss of epoch-22 batch-33 = 5.983945447951555e-05

Training epoch-22 batch-34
Running loss of epoch-22 batch-34 = 5.4968870244920254e-05

Training epoch-22 batch-35
Running loss of epoch-22 batch-35 = 3.0421651899814606e-05

Training epoch-22 batch-36
Running loss of epoch-22 batch-36 = 3.0632480047643185e-05

Training epoch-22 batch-37
Running loss of epoch-22 batch-37 = 2.1447776816785336e-05

Training epoch-22 batch-38
Running loss of epoch-22 batch-38 = 6.050453521311283e-06

Training epoch-22 batch-39
Running loss of epoch-22 batch-39 = 1.64771918207407e-05

Training epoch-22 batch-40
Running loss of epoch-22 batch-40 = 4.1896477341651917e-05

Training epoch-22 batch-41
Running loss of epoch-22 batch-41 = 6.879912689328194e-05

Training epoch-22 batch-42
Running loss of epoch-22 batch-42 = 0.0007185818394646049

Training epoch-22 batch-43
Running loss of epoch-22 batch-43 = 2.3983302526175976e-05

Training epoch-22 batch-44
Running loss of epoch-22 batch-44 = 2.2134976461529732e-05

Training epoch-22 batch-45
Running loss of epoch-22 batch-45 = 0.00013862852938473225

Training epoch-22 batch-46
Running loss of epoch-22 batch-46 = 0.00020482309628278017

Training epoch-22 batch-47
Running loss of epoch-22 batch-47 = 8.630333468317986e-06

Training epoch-22 batch-48
Running loss of epoch-22 batch-48 = 3.122747875750065e-05

Training epoch-22 batch-49
Running loss of epoch-22 batch-49 = 7.865013321861625e-05

Training epoch-22 batch-50
Running loss of epoch-22 batch-50 = 7.825088687241077e-06

Training epoch-22 batch-51
Running loss of epoch-22 batch-51 = 1.1712545529007912e-05

Training epoch-22 batch-52
Running loss of epoch-22 batch-52 = 4.567205905914307e-06

Training epoch-22 batch-53
Running loss of epoch-22 batch-53 = 0.00027310807490721345

Training epoch-22 batch-54
Running loss of epoch-22 batch-54 = 0.00016345211770385504

Training epoch-22 batch-55
Running loss of epoch-22 batch-55 = 0.00016304577002301812

Training epoch-22 batch-56
Running loss of epoch-22 batch-56 = 1.855206210166216e-05

Training epoch-22 batch-57
Running loss of epoch-22 batch-57 = 2.4228240363299847e-05

Training epoch-22 batch-58
Running loss of epoch-22 batch-58 = 4.503922536969185e-05

Training epoch-22 batch-59
Running loss of epoch-22 batch-59 = 0.0004328505019657314

Training epoch-22 batch-60
Running loss of epoch-22 batch-60 = 5.238817539066076e-05

Training epoch-22 batch-61
Running loss of epoch-22 batch-61 = 3.637629561126232e-05

Training epoch-22 batch-62
Running loss of epoch-22 batch-62 = 1.0713469237089157e-05

Training epoch-22 batch-63
Running loss of epoch-22 batch-63 = 2.759310882538557e-05

Training epoch-22 batch-64
Running loss of epoch-22 batch-64 = 3.0222698114812374e-05

Training epoch-22 batch-65
Running loss of epoch-22 batch-65 = 0.00028796319384127855

Training epoch-22 batch-66
Running loss of epoch-22 batch-66 = 0.00013293756637722254

Training epoch-22 batch-67
Running loss of epoch-22 batch-67 = 4.3239910155534744e-05

Training epoch-22 batch-68
Running loss of epoch-22 batch-68 = 1.8302584066987038e-05

Training epoch-22 batch-69
Running loss of epoch-22 batch-69 = 0.00014284724602475762

Training epoch-22 batch-70
Running loss of epoch-22 batch-70 = 7.818453013896942e-06

Training epoch-22 batch-71
Running loss of epoch-22 batch-71 = 4.876032471656799e-05

Training epoch-22 batch-72
Running loss of epoch-22 batch-72 = 6.245786789804697e-05

Training epoch-22 batch-73
Running loss of epoch-22 batch-73 = 3.6821234971284866e-05

Training epoch-22 batch-74
Running loss of epoch-22 batch-74 = 8.674128912389278e-05

Training epoch-22 batch-75
Running loss of epoch-22 batch-75 = 5.767203401774168e-05

Training epoch-22 batch-76
Running loss of epoch-22 batch-76 = 3.1882431358098984e-05

Training epoch-22 batch-77
Running loss of epoch-22 batch-77 = 3.1508971005678177e-06

Training epoch-22 batch-78
Running loss of epoch-22 batch-78 = 0.000131572422105819

Training epoch-22 batch-79
Running loss of epoch-22 batch-79 = 2.0091654732823372e-05

Training epoch-22 batch-80
Running loss of epoch-22 batch-80 = 0.0002644009073264897

Training epoch-22 batch-81
Running loss of epoch-22 batch-81 = 5.1896204240620136e-05

Training epoch-22 batch-82
Running loss of epoch-22 batch-82 = 0.0001587252481840551

Training epoch-22 batch-83
Running loss of epoch-22 batch-83 = 1.8830876797437668e-05

Training epoch-22 batch-84
Running loss of epoch-22 batch-84 = 0.0003061974421143532

Training epoch-22 batch-85
Running loss of epoch-22 batch-85 = 0.0001814651768654585

Training epoch-22 batch-86
Running loss of epoch-22 batch-86 = 0.0002626469067763537

Training epoch-22 batch-87
Running loss of epoch-22 batch-87 = 3.616453614085913e-05

Training epoch-22 batch-88
Running loss of epoch-22 batch-88 = 9.755045175552368e-05

Training epoch-22 batch-89
Running loss of epoch-22 batch-89 = 3.4124008379876614e-05

Training epoch-22 batch-90
Running loss of epoch-22 batch-90 = 0.00011523440480232239

Training epoch-22 batch-91
Running loss of epoch-22 batch-91 = 1.713121309876442e-05

Training epoch-22 batch-92
Running loss of epoch-22 batch-92 = 2.3297849111258984e-05

Training epoch-22 batch-93
Running loss of epoch-22 batch-93 = 2.5211949832737446e-05

Training epoch-22 batch-94
Running loss of epoch-22 batch-94 = 7.002148777246475e-06

Training epoch-22 batch-95
Running loss of epoch-22 batch-95 = 0.0001441744971089065

Training epoch-22 batch-96
Running loss of epoch-22 batch-96 = 0.00020749267423525453

Training epoch-22 batch-97
Running loss of epoch-22 batch-97 = 0.00025591341545805335

Training epoch-22 batch-98
Running loss of epoch-22 batch-98 = 1.063395757228136e-05

Training epoch-22 batch-99
Running loss of epoch-22 batch-99 = 0.00011907168664038181

Training epoch-22 batch-100
Running loss of epoch-22 batch-100 = 3.7869904190301895e-05

Training epoch-22 batch-101
Running loss of epoch-22 batch-101 = 4.909932613372803e-06

Training epoch-22 batch-102
Running loss of epoch-22 batch-102 = 0.0003258440119680017

Training epoch-22 batch-103
Running loss of epoch-22 batch-103 = 9.33394767343998e-06

Training epoch-22 batch-104
Running loss of epoch-22 batch-104 = 2.834887709468603e-05

Training epoch-22 batch-105
Running loss of epoch-22 batch-105 = 2.568052150309086e-05

Training epoch-22 batch-106
Running loss of epoch-22 batch-106 = 0.0002771802246570587

Training epoch-22 batch-107
Running loss of epoch-22 batch-107 = 5.647656507790089e-06

Training epoch-22 batch-108
Running loss of epoch-22 batch-108 = 1.1224881745874882e-05

Training epoch-22 batch-109
Running loss of epoch-22 batch-109 = 9.927432984113693e-06

Training epoch-22 batch-110
Running loss of epoch-22 batch-110 = 6.228825077414513e-05

Training epoch-22 batch-111
Running loss of epoch-22 batch-111 = 0.000352537608705461

Training epoch-22 batch-112
Running loss of epoch-22 batch-112 = 0.00027885293820872903

Training epoch-22 batch-113
Running loss of epoch-22 batch-113 = 9.831041097640991e-06

Training epoch-22 batch-114
Running loss of epoch-22 batch-114 = 0.00014038302469998598

Training epoch-22 batch-115
Running loss of epoch-22 batch-115 = 4.088040441274643e-06

Training epoch-22 batch-116
Running loss of epoch-22 batch-116 = 2.575374674052e-05

Training epoch-22 batch-117
Running loss of epoch-22 batch-117 = 1.2731645256280899e-05

Training epoch-22 batch-118
Running loss of epoch-22 batch-118 = 1.075782347470522e-05

Training epoch-22 batch-119
Running loss of epoch-22 batch-119 = 6.295903585851192e-05

Training epoch-22 batch-120
Running loss of epoch-22 batch-120 = 4.220753908157349e-06

Training epoch-22 batch-121
Running loss of epoch-22 batch-121 = 0.0003689821751322597

Training epoch-22 batch-122
Running loss of epoch-22 batch-122 = 0.0006533124251291156

Training epoch-22 batch-123
Running loss of epoch-22 batch-123 = 4.257308319211006e-05

Training epoch-22 batch-124
Running loss of epoch-22 batch-124 = 0.0011557108955457807

Training epoch-22 batch-125
Running loss of epoch-22 batch-125 = 0.00011865858687087893

Training epoch-22 batch-126
Running loss of epoch-22 batch-126 = 7.006339728832245e-06

Training epoch-22 batch-127
Running loss of epoch-22 batch-127 = 4.6031782403588295e-06

Training epoch-22 batch-128
Running loss of epoch-22 batch-128 = 3.847235348075628e-05

Training epoch-22 batch-129
Running loss of epoch-22 batch-129 = 1.2369127944111824e-05

Training epoch-22 batch-130
Running loss of epoch-22 batch-130 = 5.8199744671583176e-05

Training epoch-22 batch-131
Running loss of epoch-22 batch-131 = 2.7878442779183388e-05

Training epoch-22 batch-132
Running loss of epoch-22 batch-132 = 0.00024598644813522696

Training epoch-22 batch-133
Running loss of epoch-22 batch-133 = 0.0001802079495973885

Training epoch-22 batch-134
Running loss of epoch-22 batch-134 = 1.629674807190895e-05

Training epoch-22 batch-135
Running loss of epoch-22 batch-135 = 3.0004302971065044e-05

Training epoch-22 batch-136
Running loss of epoch-22 batch-136 = 0.00020347791723906994

Training epoch-22 batch-137
Running loss of epoch-22 batch-137 = 0.00016245502047240734

Training epoch-22 batch-138
Running loss of epoch-22 batch-138 = 5.290145054459572e-05

Training epoch-22 batch-139
Running loss of epoch-22 batch-139 = 2.1594343706965446e-05

Training epoch-22 batch-140
Running loss of epoch-22 batch-140 = 0.00017047830624505877

Training epoch-22 batch-141
Running loss of epoch-22 batch-141 = 5.868601147085428e-05

Training epoch-22 batch-142
Running loss of epoch-22 batch-142 = 1.2555159628391266e-05

Training epoch-22 batch-143
Running loss of epoch-22 batch-143 = 1.0061776265501976e-05

Training epoch-22 batch-144
Running loss of epoch-22 batch-144 = 0.0004825850191991776

Training epoch-22 batch-145
Running loss of epoch-22 batch-145 = 0.0004806344513781369

Training epoch-22 batch-146
Running loss of epoch-22 batch-146 = 0.00048735179007053375

Training epoch-22 batch-147
Running loss of epoch-22 batch-147 = 3.523821942508221e-05

Training epoch-22 batch-148
Running loss of epoch-22 batch-148 = 6.952788680791855e-06

Training epoch-22 batch-149
Running loss of epoch-22 batch-149 = 8.1685371696949e-05

Training epoch-22 batch-150
Running loss of epoch-22 batch-150 = 7.523392559960485e-05

Training epoch-22 batch-151
Running loss of epoch-22 batch-151 = 3.2341573387384415e-05

Training epoch-22 batch-152
Running loss of epoch-22 batch-152 = 0.0005349346902221441

Training epoch-22 batch-153
Running loss of epoch-22 batch-153 = 7.638707756996155e-06

Training epoch-22 batch-154
Running loss of epoch-22 batch-154 = 1.0780524462461472e-05

Training epoch-22 batch-155
Running loss of epoch-22 batch-155 = 1.666182652115822e-05

Training epoch-22 batch-156
Running loss of epoch-22 batch-156 = 0.0001731616212055087

Training epoch-22 batch-157
Running loss of epoch-22 batch-157 = 0.0001303013414144516

Finished training epoch-22.



Average train loss at epoch-22 = 0.00011293642204254866

Started Evaluation

Average val loss at epoch-22 = 4.103217626872816

Accuracy for classes:
Accuracy for class equals is: 69.14 %
Accuracy for class main is: 47.21 %
Accuracy for class setUp is: 56.23 %
Accuracy for class onCreate is: 45.20 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 32.65 %
Accuracy for class hashCode is: 81.65 %
Accuracy for class init is: 23.32 %
Accuracy for class execute is: 18.07 %
Accuracy for class get is: 30.00 %

Overall Accuracy = 45.97 %

Finished Evaluation



Started training epoch-23


Training epoch-23 batch-1
Running loss of epoch-23 batch-1 = 5.287176463752985e-05

Training epoch-23 batch-2
Running loss of epoch-23 batch-2 = 2.7752481400966644e-05

Training epoch-23 batch-3
Running loss of epoch-23 batch-3 = 6.83054095134139e-05

Training epoch-23 batch-4
Running loss of epoch-23 batch-4 = 7.276696851477027e-05

Training epoch-23 batch-5
Running loss of epoch-23 batch-5 = 0.00016698392573744059

Training epoch-23 batch-6
Running loss of epoch-23 batch-6 = 6.083399057388306e-06

Training epoch-23 batch-7
Running loss of epoch-23 batch-7 = 2.8341077268123627e-05

Training epoch-23 batch-8
Running loss of epoch-23 batch-8 = 3.1006988137960434e-05

Training epoch-23 batch-9
Running loss of epoch-23 batch-9 = 1.5493715181946754e-05

Training epoch-23 batch-10
Running loss of epoch-23 batch-10 = 5.293404683470726e-06

Training epoch-23 batch-11
Running loss of epoch-23 batch-11 = 8.064799476414919e-05

Training epoch-23 batch-12
Running loss of epoch-23 batch-12 = 0.000155797868501395

Training epoch-23 batch-13
Running loss of epoch-23 batch-13 = 1.4886492863297462e-05

Training epoch-23 batch-14
Running loss of epoch-23 batch-14 = 0.00015316519420593977

Training epoch-23 batch-15
Running loss of epoch-23 batch-15 = 3.0406750738620758e-05

Training epoch-23 batch-16
Running loss of epoch-23 batch-16 = 0.0001692077494226396

Training epoch-23 batch-17
Running loss of epoch-23 batch-17 = 1.0805903002619743e-05

Training epoch-23 batch-18
Running loss of epoch-23 batch-18 = 4.7959620133042336e-05

Training epoch-23 batch-19
Running loss of epoch-23 batch-19 = 5.8354344218969345e-06

Training epoch-23 batch-20
Running loss of epoch-23 batch-20 = 4.708766937255859e-06

Training epoch-23 batch-21
Running loss of epoch-23 batch-21 = 8.48202034831047e-06

Training epoch-23 batch-22
Running loss of epoch-23 batch-22 = 8.891685865819454e-06

Training epoch-23 batch-23
Running loss of epoch-23 batch-23 = 5.629728548228741e-06

Training epoch-23 batch-24
Running loss of epoch-23 batch-24 = 0.00034433219116181135

Training epoch-23 batch-25
Running loss of epoch-23 batch-25 = 0.00011565996101126075

Training epoch-23 batch-26
Running loss of epoch-23 batch-26 = 1.9560218788683414e-05

Training epoch-23 batch-27
Running loss of epoch-23 batch-27 = 0.00023872230667620897

Training epoch-23 batch-28
Running loss of epoch-23 batch-28 = 0.00013204396236687899

Training epoch-23 batch-29
Running loss of epoch-23 batch-29 = 5.138805136084557e-06

Training epoch-23 batch-30
Running loss of epoch-23 batch-30 = 7.980165537446737e-05

Training epoch-23 batch-31
Running loss of epoch-23 batch-31 = 1.6603386029601097e-05

Training epoch-23 batch-32
Running loss of epoch-23 batch-32 = 3.191980067640543e-05

Training epoch-23 batch-33
Running loss of epoch-23 batch-33 = 2.6238267309963703e-05

Training epoch-23 batch-34
Running loss of epoch-23 batch-34 = 0.00025374459801241755

Training epoch-23 batch-35
Running loss of epoch-23 batch-35 = 1.4577992260456085e-05

Training epoch-23 batch-36
Running loss of epoch-23 batch-36 = 8.88574868440628e-06

Training epoch-23 batch-37
Running loss of epoch-23 batch-37 = 0.0006802115822210908

Training epoch-23 batch-38
Running loss of epoch-23 batch-38 = 0.000256334082223475

Training epoch-23 batch-39
Running loss of epoch-23 batch-39 = 1.878943294286728e-06

Training epoch-23 batch-40
Running loss of epoch-23 batch-40 = 0.00015670014545321465

Training epoch-23 batch-41
Running loss of epoch-23 batch-41 = 9.9558150395751e-05

Training epoch-23 batch-42
Running loss of epoch-23 batch-42 = 2.523348666727543e-05

Training epoch-23 batch-43
Running loss of epoch-23 batch-43 = 9.243405656889081e-05

Training epoch-23 batch-44
Running loss of epoch-23 batch-44 = 2.717610914260149e-05

Training epoch-23 batch-45
Running loss of epoch-23 batch-45 = 3.996305167675018e-06

Training epoch-23 batch-46
Running loss of epoch-23 batch-46 = 0.00040068052476271987

Training epoch-23 batch-47
Running loss of epoch-23 batch-47 = 3.103550989180803e-05

Training epoch-23 batch-48
Running loss of epoch-23 batch-48 = 0.0005521138082258403

Training epoch-23 batch-49
Running loss of epoch-23 batch-49 = 0.00024632172426208854

Training epoch-23 batch-50
Running loss of epoch-23 batch-50 = 7.507391273975372e-06

Training epoch-23 batch-51
Running loss of epoch-23 batch-51 = 3.1543197110295296e-05

Training epoch-23 batch-52
Running loss of epoch-23 batch-52 = 0.00013288966147229075

Training epoch-23 batch-53
Running loss of epoch-23 batch-53 = 4.9380469135940075e-05

Training epoch-23 batch-54
Running loss of epoch-23 batch-54 = 2.754153683781624e-06

Training epoch-23 batch-55
Running loss of epoch-23 batch-55 = 3.292062319815159e-05

Training epoch-23 batch-56
Running loss of epoch-23 batch-56 = 6.678950740024447e-05

Training epoch-23 batch-57
Running loss of epoch-23 batch-57 = 3.0417460948228836e-05

Training epoch-23 batch-58
Running loss of epoch-23 batch-58 = 4.681560676544905e-05

Training epoch-23 batch-59
Running loss of epoch-23 batch-59 = 1.8336460925638676e-05

Training epoch-23 batch-60
Running loss of epoch-23 batch-60 = 0.000187086989171803

Training epoch-23 batch-61
Running loss of epoch-23 batch-61 = 0.0002955866511911154

Training epoch-23 batch-62
Running loss of epoch-23 batch-62 = 1.5450059436261654e-05

Training epoch-23 batch-63
Running loss of epoch-23 batch-63 = 7.168296724557877e-05

Training epoch-23 batch-64
Running loss of epoch-23 batch-64 = 4.817510489374399e-05

Training epoch-23 batch-65
Running loss of epoch-23 batch-65 = 0.00019827124197036028

Training epoch-23 batch-66
Running loss of epoch-23 batch-66 = 2.9583927243947983e-05

Training epoch-23 batch-67
Running loss of epoch-23 batch-67 = 5.2912160754203796e-05

Training epoch-23 batch-68
Running loss of epoch-23 batch-68 = 9.92603600025177e-06

Training epoch-23 batch-69
Running loss of epoch-23 batch-69 = 1.5869038179516792e-05

Training epoch-23 batch-70
Running loss of epoch-23 batch-70 = 0.00017681153258308768

Training epoch-23 batch-71
Running loss of epoch-23 batch-71 = 1.9158702343702316e-05

Training epoch-23 batch-72
Running loss of epoch-23 batch-72 = 4.991074092686176e-06

Training epoch-23 batch-73
Running loss of epoch-23 batch-73 = 9.359791874885559e-06

Training epoch-23 batch-74
Running loss of epoch-23 batch-74 = 0.00010745436884462833

Training epoch-23 batch-75
Running loss of epoch-23 batch-75 = 0.0005327296676114202

Training epoch-23 batch-76
Running loss of epoch-23 batch-76 = 0.0002473816857673228

Training epoch-23 batch-77
Running loss of epoch-23 batch-77 = 2.3903092369437218e-05

Training epoch-23 batch-78
Running loss of epoch-23 batch-78 = 1.9073951989412308e-05

Training epoch-23 batch-79
Running loss of epoch-23 batch-79 = 0.0005262858467176557

Training epoch-23 batch-80
Running loss of epoch-23 batch-80 = 9.26211941987276e-06

Training epoch-23 batch-81
Running loss of epoch-23 batch-81 = 8.773722220212221e-05

Training epoch-23 batch-82
Running loss of epoch-23 batch-82 = 2.4551409296691418e-05

Training epoch-23 batch-83
Running loss of epoch-23 batch-83 = 0.00011357455514371395

Training epoch-23 batch-84
Running loss of epoch-23 batch-84 = 0.0004099961370229721

Training epoch-23 batch-85
Running loss of epoch-23 batch-85 = 0.0004210411570966244

Training epoch-23 batch-86
Running loss of epoch-23 batch-86 = 0.00019736430840566754

Training epoch-23 batch-87
Running loss of epoch-23 batch-87 = 3.208883572369814e-05

Training epoch-23 batch-88
Running loss of epoch-23 batch-88 = 1.002172939479351e-05

Training epoch-23 batch-89
Running loss of epoch-23 batch-89 = 1.834775321185589e-05

Training epoch-23 batch-90
Running loss of epoch-23 batch-90 = 1.9027385860681534e-05

Training epoch-23 batch-91
Running loss of epoch-23 batch-91 = 1.9269995391368866e-05

Training epoch-23 batch-92
Running loss of epoch-23 batch-92 = 0.0003101406036876142

Training epoch-23 batch-93
Running loss of epoch-23 batch-93 = 5.24086644873023e-05

Training epoch-23 batch-94
Running loss of epoch-23 batch-94 = 0.000254271668381989

Training epoch-23 batch-95
Running loss of epoch-23 batch-95 = 0.0002117680269293487

Training epoch-23 batch-96
Running loss of epoch-23 batch-96 = 6.035075057297945e-05

Training epoch-23 batch-97
Running loss of epoch-23 batch-97 = 8.526508463546634e-05

Training epoch-23 batch-98
Running loss of epoch-23 batch-98 = 2.680812031030655e-06

Training epoch-23 batch-99
Running loss of epoch-23 batch-99 = 1.0712305083870888e-05

Training epoch-23 batch-100
Running loss of epoch-23 batch-100 = 0.00045764780952595174

Training epoch-23 batch-101
Running loss of epoch-23 batch-101 = 0.0005287844105623662

Training epoch-23 batch-102
Running loss of epoch-23 batch-102 = 3.4223776310682297e-06

Training epoch-23 batch-103
Running loss of epoch-23 batch-103 = 1.0913354344666004e-05

Training epoch-23 batch-104
Running loss of epoch-23 batch-104 = 4.065223038196564e-06

Training epoch-23 batch-105
Running loss of epoch-23 batch-105 = 6.762996781617403e-05

Training epoch-23 batch-106
Running loss of epoch-23 batch-106 = 3.2545067369937897e-06

Training epoch-23 batch-107
Running loss of epoch-23 batch-107 = 0.00019209814490750432

Training epoch-23 batch-108
Running loss of epoch-23 batch-108 = 1.956569030880928e-05

Training epoch-23 batch-109
Running loss of epoch-23 batch-109 = 6.61840895190835e-05

Training epoch-23 batch-110
Running loss of epoch-23 batch-110 = 8.070969488471746e-05

Training epoch-23 batch-111
Running loss of epoch-23 batch-111 = 0.00025389977963641286

Training epoch-23 batch-112
Running loss of epoch-23 batch-112 = 1.071218866854906e-05

Training epoch-23 batch-113
Running loss of epoch-23 batch-113 = 0.0001509584835730493

Training epoch-23 batch-114
Running loss of epoch-23 batch-114 = 6.240443326532841e-06

Training epoch-23 batch-115
Running loss of epoch-23 batch-115 = 1.945137046277523e-05

Training epoch-23 batch-116
Running loss of epoch-23 batch-116 = 0.0001153027405962348

Training epoch-23 batch-117
Running loss of epoch-23 batch-117 = 1.8149148672819138e-05

Training epoch-23 batch-118
Running loss of epoch-23 batch-118 = 7.695861859247088e-05

Training epoch-23 batch-119
Running loss of epoch-23 batch-119 = 1.7345300875604153e-05

Training epoch-23 batch-120
Running loss of epoch-23 batch-120 = 2.955785021185875e-05

Training epoch-23 batch-121
Running loss of epoch-23 batch-121 = 6.210990250110626e-06

Training epoch-23 batch-122
Running loss of epoch-23 batch-122 = 1.6760430298745632e-05

Training epoch-23 batch-123
Running loss of epoch-23 batch-123 = 2.2083637304604053e-05

Training epoch-23 batch-124
Running loss of epoch-23 batch-124 = 2.38658394664526e-05

Training epoch-23 batch-125
Running loss of epoch-23 batch-125 = 1.8857885152101517e-05

Training epoch-23 batch-126
Running loss of epoch-23 batch-126 = 1.1072494089603424e-05

Training epoch-23 batch-127
Running loss of epoch-23 batch-127 = 1.4798133634030819e-05

Training epoch-23 batch-128
Running loss of epoch-23 batch-128 = 5.8873556554317474e-06

Training epoch-23 batch-129
Running loss of epoch-23 batch-129 = 0.00022226705914363265

Training epoch-23 batch-130
Running loss of epoch-23 batch-130 = 3.069487866014242e-05

Training epoch-23 batch-131
Running loss of epoch-23 batch-131 = 3.128137905150652e-05

Training epoch-23 batch-132
Running loss of epoch-23 batch-132 = 2.7150381356477737e-05

Training epoch-23 batch-133
Running loss of epoch-23 batch-133 = 0.00023187685292214155

Training epoch-23 batch-134
Running loss of epoch-23 batch-134 = 2.9798829928040504e-05

Training epoch-23 batch-135
Running loss of epoch-23 batch-135 = 1.1621508747339249e-05

Training epoch-23 batch-136
Running loss of epoch-23 batch-136 = 4.825647920370102e-06

Training epoch-23 batch-137
Running loss of epoch-23 batch-137 = 1.2781820259988308e-05

Training epoch-23 batch-138
Running loss of epoch-23 batch-138 = 4.2822095565497875e-05

Training epoch-23 batch-139
Running loss of epoch-23 batch-139 = 0.00020023802062496543

Training epoch-23 batch-140
Running loss of epoch-23 batch-140 = 0.00018050067592412233

Training epoch-23 batch-141
Running loss of epoch-23 batch-141 = 3.972090780735016e-06

Training epoch-23 batch-142
Running loss of epoch-23 batch-142 = 6.225495599210262e-05

Training epoch-23 batch-143
Running loss of epoch-23 batch-143 = 1.6267411410808563e-05

Training epoch-23 batch-144
Running loss of epoch-23 batch-144 = 4.2713130824267864e-05

Training epoch-23 batch-145
Running loss of epoch-23 batch-145 = 0.00020320608746260405

Training epoch-23 batch-146
Running loss of epoch-23 batch-146 = 0.0003475106495898217

Training epoch-23 batch-147
Running loss of epoch-23 batch-147 = 0.0003188064438290894

Training epoch-23 batch-148
Running loss of epoch-23 batch-148 = 0.00021505996119230986

Training epoch-23 batch-149
Running loss of epoch-23 batch-149 = 1.8405611626803875e-05

Training epoch-23 batch-150
Running loss of epoch-23 batch-150 = 4.52226959168911e-06

Training epoch-23 batch-151
Running loss of epoch-23 batch-151 = 1.1353055015206337e-05

Training epoch-23 batch-152
Running loss of epoch-23 batch-152 = 2.385815605521202e-06

Training epoch-23 batch-153
Running loss of epoch-23 batch-153 = 5.464302375912666e-06

Training epoch-23 batch-154
Running loss of epoch-23 batch-154 = 0.0001651656930334866

Training epoch-23 batch-155
Running loss of epoch-23 batch-155 = 1.0948395356535912e-05

Training epoch-23 batch-156
Running loss of epoch-23 batch-156 = 3.7398538552224636e-05

Training epoch-23 batch-157
Running loss of epoch-23 batch-157 = 3.6191195249557495e-05

Finished training epoch-23.



Average train loss at epoch-23 = 9.69336699694395e-05

Started Evaluation

Average val loss at epoch-23 = 4.2617251637734865

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 49.51 %
Accuracy for class setUp is: 54.43 %
Accuracy for class onCreate is: 42.86 %
Accuracy for class toString is: 43.69 %
Accuracy for class run is: 31.96 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 22.65 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 27.18 %

Overall Accuracy = 45.66 %

Finished Evaluation



Started training epoch-24


Training epoch-24 batch-1
Running loss of epoch-24 batch-1 = 0.0003636262263171375

Training epoch-24 batch-2
Running loss of epoch-24 batch-2 = 9.679037611931562e-05

Training epoch-24 batch-3
Running loss of epoch-24 batch-3 = 1.551362220197916e-05

Training epoch-24 batch-4
Running loss of epoch-24 batch-4 = 0.00012747012078762054

Training epoch-24 batch-5
Running loss of epoch-24 batch-5 = 0.00014164310414344072

Training epoch-24 batch-6
Running loss of epoch-24 batch-6 = 0.0006283728871494532

Training epoch-24 batch-7
Running loss of epoch-24 batch-7 = 0.0002394118346273899

Training epoch-24 batch-8
Running loss of epoch-24 batch-8 = 5.8390083722770214e-05

Training epoch-24 batch-9
Running loss of epoch-24 batch-9 = 0.0002939957194030285

Training epoch-24 batch-10
Running loss of epoch-24 batch-10 = 2.5474349968135357e-05

Training epoch-24 batch-11
Running loss of epoch-24 batch-11 = 4.345609340816736e-05

Training epoch-24 batch-12
Running loss of epoch-24 batch-12 = 6.912974640727043e-06

Training epoch-24 batch-13
Running loss of epoch-24 batch-13 = 7.973751053214073e-06

Training epoch-24 batch-14
Running loss of epoch-24 batch-14 = 1.6880338080227375e-05

Training epoch-24 batch-15
Running loss of epoch-24 batch-15 = 3.676861524581909e-06

Training epoch-24 batch-16
Running loss of epoch-24 batch-16 = 0.0002884963760152459

Training epoch-24 batch-17
Running loss of epoch-24 batch-17 = 1.2608594261109829e-05

Training epoch-24 batch-18
Running loss of epoch-24 batch-18 = 4.678964614868164e-06

Training epoch-24 batch-19
Running loss of epoch-24 batch-19 = 2.6163412258028984e-05

Training epoch-24 batch-20
Running loss of epoch-24 batch-20 = 0.0003517302975524217

Training epoch-24 batch-21
Running loss of epoch-24 batch-21 = 5.825655534863472e-06

Training epoch-24 batch-22
Running loss of epoch-24 batch-22 = 2.1588057279586792e-05

Training epoch-24 batch-23
Running loss of epoch-24 batch-23 = 9.493553079664707e-05

Training epoch-24 batch-24
Running loss of epoch-24 batch-24 = 2.1646381355822086e-05

Training epoch-24 batch-25
Running loss of epoch-24 batch-25 = 0.00014159560669213533

Training epoch-24 batch-26
Running loss of epoch-24 batch-26 = 1.9201193936169147e-05

Training epoch-24 batch-27
Running loss of epoch-24 batch-27 = 0.000284385634586215

Training epoch-24 batch-28
Running loss of epoch-24 batch-28 = 2.971047069877386e-05

Training epoch-24 batch-29
Running loss of epoch-24 batch-29 = 0.00021403032587841153

Training epoch-24 batch-30
Running loss of epoch-24 batch-30 = 3.449548967182636e-05

Training epoch-24 batch-31
Running loss of epoch-24 batch-31 = 1.0639894753694534e-05

Training epoch-24 batch-32
Running loss of epoch-24 batch-32 = 0.0002680384204722941

Training epoch-24 batch-33
Running loss of epoch-24 batch-33 = 9.529461385682225e-05

Training epoch-24 batch-34
Running loss of epoch-24 batch-34 = 1.5447032637894154e-05

Training epoch-24 batch-35
Running loss of epoch-24 batch-35 = 3.6852434277534485e-05

Training epoch-24 batch-36
Running loss of epoch-24 batch-36 = 2.790649887174368e-05

Training epoch-24 batch-37
Running loss of epoch-24 batch-37 = 0.0001788672525435686

Training epoch-24 batch-38
Running loss of epoch-24 batch-38 = 1.552887260913849e-05

Training epoch-24 batch-39
Running loss of epoch-24 batch-39 = 6.262678653001785e-06

Training epoch-24 batch-40
Running loss of epoch-24 batch-40 = 0.0007706376491114497

Training epoch-24 batch-41
Running loss of epoch-24 batch-41 = 0.0001789160305634141

Training epoch-24 batch-42
Running loss of epoch-24 batch-42 = 2.0175357349216938e-05

Training epoch-24 batch-43
Running loss of epoch-24 batch-43 = 1.813494600355625e-05

Training epoch-24 batch-44
Running loss of epoch-24 batch-44 = 9.8842428997159e-06

Training epoch-24 batch-45
Running loss of epoch-24 batch-45 = 7.436005398631096e-05

Training epoch-24 batch-46
Running loss of epoch-24 batch-46 = 3.0916184186935425e-05

Training epoch-24 batch-47
Running loss of epoch-24 batch-47 = 1.351663377135992e-05

Training epoch-24 batch-48
Running loss of epoch-24 batch-48 = 7.291825022548437e-05

Training epoch-24 batch-49
Running loss of epoch-24 batch-49 = 2.272834535688162e-05

Training epoch-24 batch-50
Running loss of epoch-24 batch-50 = 7.814378477633e-06

Training epoch-24 batch-51
Running loss of epoch-24 batch-51 = 4.397355951368809e-06

Training epoch-24 batch-52
Running loss of epoch-24 batch-52 = 6.395624950528145e-06

Training epoch-24 batch-53
Running loss of epoch-24 batch-53 = 2.6134541258215904e-05

Training epoch-24 batch-54
Running loss of epoch-24 batch-54 = 3.6510173231363297e-05

Training epoch-24 batch-55
Running loss of epoch-24 batch-55 = 9.892298839986324e-05

Training epoch-24 batch-56
Running loss of epoch-24 batch-56 = 1.1958065442740917e-05

Training epoch-24 batch-57
Running loss of epoch-24 batch-57 = 1.821969635784626e-05

Training epoch-24 batch-58
Running loss of epoch-24 batch-58 = 0.0003568936954252422

Training epoch-24 batch-59
Running loss of epoch-24 batch-59 = 2.8063077479600906e-06

Training epoch-24 batch-60
Running loss of epoch-24 batch-60 = 1.4610006473958492e-05

Training epoch-24 batch-61
Running loss of epoch-24 batch-61 = 1.9071041606366634e-05

Training epoch-24 batch-62
Running loss of epoch-24 batch-62 = 0.00010225415462628007

Training epoch-24 batch-63
Running loss of epoch-24 batch-63 = 2.6509049348533154e-05

Training epoch-24 batch-64
Running loss of epoch-24 batch-64 = 0.00010766694322228432

Training epoch-24 batch-65
Running loss of epoch-24 batch-65 = 8.60614818520844e-05

Training epoch-24 batch-66
Running loss of epoch-24 batch-66 = 0.00012162351049482822

Training epoch-24 batch-67
Running loss of epoch-24 batch-67 = 8.683971827849746e-05

Training epoch-24 batch-68
Running loss of epoch-24 batch-68 = 0.00010729162022471428

Training epoch-24 batch-69
Running loss of epoch-24 batch-69 = 2.0887702703475952e-05

Training epoch-24 batch-70
Running loss of epoch-24 batch-70 = 2.045067958533764e-05

Training epoch-24 batch-71
Running loss of epoch-24 batch-71 = 1.981679815798998e-05

Training epoch-24 batch-72
Running loss of epoch-24 batch-72 = 0.00022974965395405889

Training epoch-24 batch-73
Running loss of epoch-24 batch-73 = 3.526767250150442e-05

Training epoch-24 batch-74
Running loss of epoch-24 batch-74 = 0.0001252924557775259

Training epoch-24 batch-75
Running loss of epoch-24 batch-75 = 3.0533410608768463e-06

Training epoch-24 batch-76
Running loss of epoch-24 batch-76 = 4.338333383202553e-06

Training epoch-24 batch-77
Running loss of epoch-24 batch-77 = 0.00019454234279692173

Training epoch-24 batch-78
Running loss of epoch-24 batch-78 = 7.973669562488794e-05

Training epoch-24 batch-79
Running loss of epoch-24 batch-79 = 1.7873244360089302e-05

Training epoch-24 batch-80
Running loss of epoch-24 batch-80 = 4.6442728489637375e-05

Training epoch-24 batch-81
Running loss of epoch-24 batch-81 = 5.9243058785796165e-05

Training epoch-24 batch-82
Running loss of epoch-24 batch-82 = 6.464577745646238e-05

Training epoch-24 batch-83
Running loss of epoch-24 batch-83 = 4.3798238039016724e-05

Training epoch-24 batch-84
Running loss of epoch-24 batch-84 = 3.902323078364134e-05

Training epoch-24 batch-85
Running loss of epoch-24 batch-85 = 3.9956881664693356e-05

Training epoch-24 batch-86
Running loss of epoch-24 batch-86 = 6.762531120330095e-05

Training epoch-24 batch-87
Running loss of epoch-24 batch-87 = 1.9420171156525612e-05

Training epoch-24 batch-88
Running loss of epoch-24 batch-88 = 4.370196256786585e-05

Training epoch-24 batch-89
Running loss of epoch-24 batch-89 = 0.00010220950935035944

Training epoch-24 batch-90
Running loss of epoch-24 batch-90 = 1.7791171558201313e-05

Training epoch-24 batch-91
Running loss of epoch-24 batch-91 = 1.3509881682693958e-05

Training epoch-24 batch-92
Running loss of epoch-24 batch-92 = 1.1505326256155968e-05

Training epoch-24 batch-93
Running loss of epoch-24 batch-93 = 7.968815043568611e-05

Training epoch-24 batch-94
Running loss of epoch-24 batch-94 = 8.212635293602943e-05

Training epoch-24 batch-95
Running loss of epoch-24 batch-95 = 0.0004308537463657558

Training epoch-24 batch-96
Running loss of epoch-24 batch-96 = 5.527166649699211e-06

Training epoch-24 batch-97
Running loss of epoch-24 batch-97 = 7.220078259706497e-06

Training epoch-24 batch-98
Running loss of epoch-24 batch-98 = 9.183655492961407e-06

Training epoch-24 batch-99
Running loss of epoch-24 batch-99 = 0.0001543237012811005

Training epoch-24 batch-100
Running loss of epoch-24 batch-100 = 3.994232974946499e-05

Training epoch-24 batch-101
Running loss of epoch-24 batch-101 = 1.5355530194938183e-05

Training epoch-24 batch-102
Running loss of epoch-24 batch-102 = 2.2884574718773365e-05

Training epoch-24 batch-103
Running loss of epoch-24 batch-103 = 1.3619777746498585e-05

Training epoch-24 batch-104
Running loss of epoch-24 batch-104 = 0.0003266120620537549

Training epoch-24 batch-105
Running loss of epoch-24 batch-105 = 6.646395195275545e-05

Training epoch-24 batch-106
Running loss of epoch-24 batch-106 = 2.6945024728775024e-05

Training epoch-24 batch-107
Running loss of epoch-24 batch-107 = 9.314180351793766e-05

Training epoch-24 batch-108
Running loss of epoch-24 batch-108 = 0.00022273830836638808

Training epoch-24 batch-109
Running loss of epoch-24 batch-109 = 3.491225652396679e-05

Training epoch-24 batch-110
Running loss of epoch-24 batch-110 = 1.192605122923851e-05

Training epoch-24 batch-111
Running loss of epoch-24 batch-111 = 1.5267636626958847e-05

Training epoch-24 batch-112
Running loss of epoch-24 batch-112 = 6.16186298429966e-06

Training epoch-24 batch-113
Running loss of epoch-24 batch-113 = 2.451706677675247e-06

Training epoch-24 batch-114
Running loss of epoch-24 batch-114 = 8.74553807079792e-05

Training epoch-24 batch-115
Running loss of epoch-24 batch-115 = 5.109934136271477e-06

Training epoch-24 batch-116
Running loss of epoch-24 batch-116 = 1.703656744211912e-05

Training epoch-24 batch-117
Running loss of epoch-24 batch-117 = 3.960414323955774e-05

Training epoch-24 batch-118
Running loss of epoch-24 batch-118 = 9.904615581035614e-06

Training epoch-24 batch-119
Running loss of epoch-24 batch-119 = 5.8339559473097324e-05

Training epoch-24 batch-120
Running loss of epoch-24 batch-120 = 9.256415069103241e-06

Training epoch-24 batch-121
Running loss of epoch-24 batch-121 = 4.658615216612816e-05

Training epoch-24 batch-122
Running loss of epoch-24 batch-122 = 0.00022328115301206708

Training epoch-24 batch-123
Running loss of epoch-24 batch-123 = 0.0004680375277530402

Training epoch-24 batch-124
Running loss of epoch-24 batch-124 = 8.633709512650967e-06

Training epoch-24 batch-125
Running loss of epoch-24 batch-125 = 3.688328433781862e-05

Training epoch-24 batch-126
Running loss of epoch-24 batch-126 = 4.942994564771652e-06

Training epoch-24 batch-127
Running loss of epoch-24 batch-127 = 7.155933417379856e-06

Training epoch-24 batch-128
Running loss of epoch-24 batch-128 = 0.00010938156628981233

Training epoch-24 batch-129
Running loss of epoch-24 batch-129 = 7.402361370623112e-05

Training epoch-24 batch-130
Running loss of epoch-24 batch-130 = 8.114497177302837e-06

Training epoch-24 batch-131
Running loss of epoch-24 batch-131 = 1.1927681043744087e-05

Training epoch-24 batch-132
Running loss of epoch-24 batch-132 = 9.637093171477318e-06

Training epoch-24 batch-133
Running loss of epoch-24 batch-133 = 0.0007032741559669375

Training epoch-24 batch-134
Running loss of epoch-24 batch-134 = 2.31604790315032e-05

Training epoch-24 batch-135
Running loss of epoch-24 batch-135 = 1.7093028873205185e-05

Training epoch-24 batch-136
Running loss of epoch-24 batch-136 = 1.0840478353202343e-05

Training epoch-24 batch-137
Running loss of epoch-24 batch-137 = 3.012910019606352e-05

Training epoch-24 batch-138
Running loss of epoch-24 batch-138 = 7.569091394543648e-06

Training epoch-24 batch-139
Running loss of epoch-24 batch-139 = 1.7232494428753853e-05

Training epoch-24 batch-140
Running loss of epoch-24 batch-140 = 8.617760613560677e-06

Training epoch-24 batch-141
Running loss of epoch-24 batch-141 = 6.4604682847857475e-06

Training epoch-24 batch-142
Running loss of epoch-24 batch-142 = 3.7227640859782696e-05

Training epoch-24 batch-143
Running loss of epoch-24 batch-143 = 5.8854930102825165e-06

Training epoch-24 batch-144
Running loss of epoch-24 batch-144 = 0.00016446359222754836

Training epoch-24 batch-145
Running loss of epoch-24 batch-145 = 4.030764102935791e-06

Training epoch-24 batch-146
Running loss of epoch-24 batch-146 = 2.5253859348595142e-05

Training epoch-24 batch-147
Running loss of epoch-24 batch-147 = 1.996522769331932e-05

Training epoch-24 batch-148
Running loss of epoch-24 batch-148 = 0.00010086916154250503

Training epoch-24 batch-149
Running loss of epoch-24 batch-149 = 2.616283018141985e-05

Training epoch-24 batch-150
Running loss of epoch-24 batch-150 = 0.00032894063042476773

Training epoch-24 batch-151
Running loss of epoch-24 batch-151 = 0.0004181635449640453

Training epoch-24 batch-152
Running loss of epoch-24 batch-152 = 0.0001813442213460803

Training epoch-24 batch-153
Running loss of epoch-24 batch-153 = 0.0001369453384540975

Training epoch-24 batch-154
Running loss of epoch-24 batch-154 = 1.6215606592595577e-05

Training epoch-24 batch-155
Running loss of epoch-24 batch-155 = 1.3118493370711803e-05

Training epoch-24 batch-156
Running loss of epoch-24 batch-156 = 1.1960277333855629e-05

Training epoch-24 batch-157
Running loss of epoch-24 batch-157 = 1.1555850505828857e-05

Finished training epoch-24.



Average train loss at epoch-24 = 8.492824640125037e-05

Started Evaluation

Average val loss at epoch-24 = 4.280091768816898

Accuracy for classes:
Accuracy for class equals is: 68.98 %
Accuracy for class main is: 46.07 %
Accuracy for class setUp is: 56.07 %
Accuracy for class onCreate is: 43.39 %
Accuracy for class toString is: 41.30 %
Accuracy for class run is: 32.88 %
Accuracy for class hashCode is: 82.02 %
Accuracy for class init is: 27.35 %
Accuracy for class execute is: 15.66 %
Accuracy for class get is: 32.31 %

Overall Accuracy = 45.78 %

Finished Evaluation



Started training epoch-25


Training epoch-25 batch-1
Running loss of epoch-25 batch-1 = 2.845341805368662e-05

Training epoch-25 batch-2
Running loss of epoch-25 batch-2 = 2.3716711439192295e-05

Training epoch-25 batch-3
Running loss of epoch-25 batch-3 = 5.1345559768378735e-05

Training epoch-25 batch-4
Running loss of epoch-25 batch-4 = 6.873690290376544e-05

Training epoch-25 batch-5
Running loss of epoch-25 batch-5 = 9.339535608887672e-05

Training epoch-25 batch-6
Running loss of epoch-25 batch-6 = 4.488660488277674e-05

Training epoch-25 batch-7
Running loss of epoch-25 batch-7 = 3.2184296287596226e-05

Training epoch-25 batch-8
Running loss of epoch-25 batch-8 = 0.00016927719116210938

Training epoch-25 batch-9
Running loss of epoch-25 batch-9 = 3.6095152609050274e-05

Training epoch-25 batch-10
Running loss of epoch-25 batch-10 = 0.00015137484297156334

Training epoch-25 batch-11
Running loss of epoch-25 batch-11 = 6.125308573246002e-06

Training epoch-25 batch-12
Running loss of epoch-25 batch-12 = 3.0759256333112717e-06

Training epoch-25 batch-13
Running loss of epoch-25 batch-13 = 1.8699909560382366e-05

Training epoch-25 batch-14
Running loss of epoch-25 batch-14 = 1.5169498510658741e-05

Training epoch-25 batch-15
Running loss of epoch-25 batch-15 = 9.257980855181813e-05

Training epoch-25 batch-16
Running loss of epoch-25 batch-16 = 5.996786057949066e-06

Training epoch-25 batch-17
Running loss of epoch-25 batch-17 = 8.159060962498188e-05

Training epoch-25 batch-18
Running loss of epoch-25 batch-18 = 3.1165312975645065e-05

Training epoch-25 batch-19
Running loss of epoch-25 batch-19 = 0.00035939953522756696

Training epoch-25 batch-20
Running loss of epoch-25 batch-20 = 0.0002487933961674571

Training epoch-25 batch-21
Running loss of epoch-25 batch-21 = 1.3723387382924557e-05

Training epoch-25 batch-22
Running loss of epoch-25 batch-22 = 3.638199996203184e-05

Training epoch-25 batch-23
Running loss of epoch-25 batch-23 = 0.00019126984989270568

Training epoch-25 batch-24
Running loss of epoch-25 batch-24 = 5.3998082876205444e-06

Training epoch-25 batch-25
Running loss of epoch-25 batch-25 = 2.891791518777609e-05

Training epoch-25 batch-26
Running loss of epoch-25 batch-26 = 2.1132291294634342e-05

Training epoch-25 batch-27
Running loss of epoch-25 batch-27 = 1.2038042768836021e-05

Training epoch-25 batch-28
Running loss of epoch-25 batch-28 = 1.0635238140821457e-05

Training epoch-25 batch-29
Running loss of epoch-25 batch-29 = 2.4870852939784527e-05

Training epoch-25 batch-30
Running loss of epoch-25 batch-30 = 1.959304790943861e-05

Training epoch-25 batch-31
Running loss of epoch-25 batch-31 = 3.7071295082569122e-06

Training epoch-25 batch-32
Running loss of epoch-25 batch-32 = 5.3722294978797436e-05

Training epoch-25 batch-33
Running loss of epoch-25 batch-33 = 1.0258518159389496e-05

Training epoch-25 batch-34
Running loss of epoch-25 batch-34 = 5.055218935012817e-06

Training epoch-25 batch-35
Running loss of epoch-25 batch-35 = 7.887487299740314e-06

Training epoch-25 batch-36
Running loss of epoch-25 batch-36 = 1.1150725185871124e-05

Training epoch-25 batch-37
Running loss of epoch-25 batch-37 = 0.00044617283856496215

Training epoch-25 batch-38
Running loss of epoch-25 batch-38 = 1.1497875675559044e-05

Training epoch-25 batch-39
Running loss of epoch-25 batch-39 = 1.899490598589182e-05

Training epoch-25 batch-40
Running loss of epoch-25 batch-40 = 5.518784746527672e-06

Training epoch-25 batch-41
Running loss of epoch-25 batch-41 = 1.3302662409842014e-05

Training epoch-25 batch-42
Running loss of epoch-25 batch-42 = 1.9115046598017216e-05

Training epoch-25 batch-43
Running loss of epoch-25 batch-43 = 1.6823061741888523e-05

Training epoch-25 batch-44
Running loss of epoch-25 batch-44 = 0.00011417438508942723

Training epoch-25 batch-45
Running loss of epoch-25 batch-45 = 1.1664931662380695e-05

Training epoch-25 batch-46
Running loss of epoch-25 batch-46 = 0.0005841700476594269

Training epoch-25 batch-47
Running loss of epoch-25 batch-47 = 1.3154232874512672e-05

Training epoch-25 batch-48
Running loss of epoch-25 batch-48 = 0.00012581481132656336

Training epoch-25 batch-49
Running loss of epoch-25 batch-49 = 6.532762199640274e-06

Training epoch-25 batch-50
Running loss of epoch-25 batch-50 = 5.3604599088430405e-06

Training epoch-25 batch-51
Running loss of epoch-25 batch-51 = 6.292015314102173e-06

Training epoch-25 batch-52
Running loss of epoch-25 batch-52 = 1.1372845619916916e-05

Training epoch-25 batch-53
Running loss of epoch-25 batch-53 = 9.043706813827157e-05

Training epoch-25 batch-54
Running loss of epoch-25 batch-54 = 1.7121201381087303e-05

Training epoch-25 batch-55
Running loss of epoch-25 batch-55 = 7.525493856519461e-05

Training epoch-25 batch-56
Running loss of epoch-25 batch-56 = 2.6048743166029453e-05

Training epoch-25 batch-57
Running loss of epoch-25 batch-57 = 4.2373896576464176e-05

Training epoch-25 batch-58
Running loss of epoch-25 batch-58 = 0.0001941467053256929

Training epoch-25 batch-59
Running loss of epoch-25 batch-59 = 1.721782609820366e-06

Training epoch-25 batch-60
Running loss of epoch-25 batch-60 = 0.00013303826563060284

Training epoch-25 batch-61
Running loss of epoch-25 batch-61 = 2.3148837499320507e-05

Training epoch-25 batch-62
Running loss of epoch-25 batch-62 = 5.384627729654312e-05

Training epoch-25 batch-63
Running loss of epoch-25 batch-63 = 1.3265525922179222e-05

Training epoch-25 batch-64
Running loss of epoch-25 batch-64 = 0.00019307085312902927

Training epoch-25 batch-65
Running loss of epoch-25 batch-65 = 0.00027541478630155325

Training epoch-25 batch-66
Running loss of epoch-25 batch-66 = 0.0001577099901624024

Training epoch-25 batch-67
Running loss of epoch-25 batch-67 = 4.0424056351184845e-06

Training epoch-25 batch-68
Running loss of epoch-25 batch-68 = 0.0002140470314770937

Training epoch-25 batch-69
Running loss of epoch-25 batch-69 = 8.003250695765018e-05

Training epoch-25 batch-70
Running loss of epoch-25 batch-70 = 0.000185122131370008

Training epoch-25 batch-71
Running loss of epoch-25 batch-71 = 0.00011929497122764587

Training epoch-25 batch-72
Running loss of epoch-25 batch-72 = 7.250113412737846e-06

Training epoch-25 batch-73
Running loss of epoch-25 batch-73 = 0.0001834685099311173

Training epoch-25 batch-74
Running loss of epoch-25 batch-74 = 4.0580169297754765e-05

Training epoch-25 batch-75
Running loss of epoch-25 batch-75 = 6.862916052341461e-06

Training epoch-25 batch-76
Running loss of epoch-25 batch-76 = 0.0002522325376048684

Training epoch-25 batch-77
Running loss of epoch-25 batch-77 = 6.946537178009748e-05

Training epoch-25 batch-78
Running loss of epoch-25 batch-78 = 1.5666126273572445e-05

Training epoch-25 batch-79
Running loss of epoch-25 batch-79 = 0.00019547715783119202

Training epoch-25 batch-80
Running loss of epoch-25 batch-80 = 7.975730113685131e-05

Training epoch-25 batch-81
Running loss of epoch-25 batch-81 = 2.4748733267188072e-05

Training epoch-25 batch-82
Running loss of epoch-25 batch-82 = 6.911228410899639e-06

Training epoch-25 batch-83
Running loss of epoch-25 batch-83 = 1.3463315553963184e-05

Training epoch-25 batch-84
Running loss of epoch-25 batch-84 = 1.6717822290956974e-05

Training epoch-25 batch-85
Running loss of epoch-25 batch-85 = 7.1339309215545654e-06

Training epoch-25 batch-86
Running loss of epoch-25 batch-86 = 4.279729910194874e-05

Training epoch-25 batch-87
Running loss of epoch-25 batch-87 = 6.189686246216297e-06

Training epoch-25 batch-88
Running loss of epoch-25 batch-88 = 1.2964243069291115e-05

Training epoch-25 batch-89
Running loss of epoch-25 batch-89 = 4.32673841714859e-05

Training epoch-25 batch-90
Running loss of epoch-25 batch-90 = 2.4437671527266502e-05

Training epoch-25 batch-91
Running loss of epoch-25 batch-91 = 7.120065856724977e-05

Training epoch-25 batch-92
Running loss of epoch-25 batch-92 = 1.9071856513619423e-05

Training epoch-25 batch-93
Running loss of epoch-25 batch-93 = 0.0006063473410904408

Training epoch-25 batch-94
Running loss of epoch-25 batch-94 = 5.441717803478241e-06

Training epoch-25 batch-95
Running loss of epoch-25 batch-95 = 1.2738746590912342e-05

Training epoch-25 batch-96
Running loss of epoch-25 batch-96 = 0.00013820268213748932

Training epoch-25 batch-97
Running loss of epoch-25 batch-97 = 0.00014440802624449134

Training epoch-25 batch-98
Running loss of epoch-25 batch-98 = 4.136282950639725e-05

Training epoch-25 batch-99
Running loss of epoch-25 batch-99 = 2.1520303562283516e-05

Training epoch-25 batch-100
Running loss of epoch-25 batch-100 = 3.339233808219433e-05

Training epoch-25 batch-101
Running loss of epoch-25 batch-101 = 9.899481665343046e-05

Training epoch-25 batch-102
Running loss of epoch-25 batch-102 = 2.0942301489412785e-05

Training epoch-25 batch-103
Running loss of epoch-25 batch-103 = 4.860630724579096e-05

Training epoch-25 batch-104
Running loss of epoch-25 batch-104 = 6.584497168660164e-05

Training epoch-25 batch-105
Running loss of epoch-25 batch-105 = 1.3124896213412285e-05

Training epoch-25 batch-106
Running loss of epoch-25 batch-106 = 8.580274879932404e-06

Training epoch-25 batch-107
Running loss of epoch-25 batch-107 = 0.00033481596619822085

Training epoch-25 batch-108
Running loss of epoch-25 batch-108 = 7.053487934172153e-05

Training epoch-25 batch-109
Running loss of epoch-25 batch-109 = 8.197693387046456e-05

Training epoch-25 batch-110
Running loss of epoch-25 batch-110 = 5.859090015292168e-05

Training epoch-25 batch-111
Running loss of epoch-25 batch-111 = 4.536006599664688e-06

Training epoch-25 batch-112
Running loss of epoch-25 batch-112 = 1.1590658687055111e-05

Training epoch-25 batch-113
Running loss of epoch-25 batch-113 = 1.467729452997446e-05

Training epoch-25 batch-114
Running loss of epoch-25 batch-114 = 2.6085530407726765e-05

Training epoch-25 batch-115
Running loss of epoch-25 batch-115 = 3.573368303477764e-05

Training epoch-25 batch-116
Running loss of epoch-25 batch-116 = 6.853602826595306e-06

Training epoch-25 batch-117
Running loss of epoch-25 batch-117 = 0.00021080148871988058

Training epoch-25 batch-118
Running loss of epoch-25 batch-118 = 0.00011089298641309142

Training epoch-25 batch-119
Running loss of epoch-25 batch-119 = 5.1609473302960396e-05

Training epoch-25 batch-120
Running loss of epoch-25 batch-120 = 2.904015127569437e-05

Training epoch-25 batch-121
Running loss of epoch-25 batch-121 = 1.9021914340555668e-05

Training epoch-25 batch-122
Running loss of epoch-25 batch-122 = 2.407014835625887e-05

Training epoch-25 batch-123
Running loss of epoch-25 batch-123 = 0.00019454973516985774

Training epoch-25 batch-124
Running loss of epoch-25 batch-124 = 5.705747753381729e-05

Training epoch-25 batch-125
Running loss of epoch-25 batch-125 = 0.0004116418131161481

Training epoch-25 batch-126
Running loss of epoch-25 batch-126 = 0.0003392494691070169

Training epoch-25 batch-127
Running loss of epoch-25 batch-127 = 0.000391934416256845

Training epoch-25 batch-128
Running loss of epoch-25 batch-128 = 2.670439425855875e-05

Training epoch-25 batch-129
Running loss of epoch-25 batch-129 = 2.5550834834575653e-06

Training epoch-25 batch-130
Running loss of epoch-25 batch-130 = 7.133232429623604e-06

Training epoch-25 batch-131
Running loss of epoch-25 batch-131 = 2.014217898249626e-06

Training epoch-25 batch-132
Running loss of epoch-25 batch-132 = 1.0031857527792454e-05

Training epoch-25 batch-133
Running loss of epoch-25 batch-133 = 2.4950364604592323e-05

Training epoch-25 batch-134
Running loss of epoch-25 batch-134 = 0.00020376616157591343

Training epoch-25 batch-135
Running loss of epoch-25 batch-135 = 8.179573342204094e-06

Training epoch-25 batch-136
Running loss of epoch-25 batch-136 = 1.590745523571968e-05

Training epoch-25 batch-137
Running loss of epoch-25 batch-137 = 1.2403121218085289e-05

Training epoch-25 batch-138
Running loss of epoch-25 batch-138 = 3.232620656490326e-06

Training epoch-25 batch-139
Running loss of epoch-25 batch-139 = 0.00020965863950550556

Training epoch-25 batch-140
Running loss of epoch-25 batch-140 = 7.206457667052746e-06

Training epoch-25 batch-141
Running loss of epoch-25 batch-141 = 2.2437190636992455e-05

Training epoch-25 batch-142
Running loss of epoch-25 batch-142 = 1.0220333933830261e-05

Training epoch-25 batch-143
Running loss of epoch-25 batch-143 = 1.2944452464580536e-05

Training epoch-25 batch-144
Running loss of epoch-25 batch-144 = 2.1933927200734615e-05

Training epoch-25 batch-145
Running loss of epoch-25 batch-145 = 1.9562430679798126e-06

Training epoch-25 batch-146
Running loss of epoch-25 batch-146 = 2.125650644302368e-05

Training epoch-25 batch-147
Running loss of epoch-25 batch-147 = 4.431931301951408e-06

Training epoch-25 batch-148
Running loss of epoch-25 batch-148 = 4.6622008085250854e-06

Training epoch-25 batch-149
Running loss of epoch-25 batch-149 = 0.0001308285864070058

Training epoch-25 batch-150
Running loss of epoch-25 batch-150 = 0.00016784516628831625

Training epoch-25 batch-151
Running loss of epoch-25 batch-151 = 0.00017464224947616458

Training epoch-25 batch-152
Running loss of epoch-25 batch-152 = 5.252659320831299e-06

Training epoch-25 batch-153
Running loss of epoch-25 batch-153 = 8.172239176928997e-06

Training epoch-25 batch-154
Running loss of epoch-25 batch-154 = 3.2633543014526367e-06

Training epoch-25 batch-155
Running loss of epoch-25 batch-155 = 8.611282100901008e-05

Training epoch-25 batch-156
Running loss of epoch-25 batch-156 = 8.334754966199398e-06

Training epoch-25 batch-157
Running loss of epoch-25 batch-157 = 1.3574957847595215e-05

Finished training epoch-25.



Average train loss at epoch-25 = 7.316974829882383e-05

Started Evaluation

Average val loss at epoch-25 = 4.334274535116396

Accuracy for classes:
Accuracy for class equals is: 69.14 %
Accuracy for class main is: 47.21 %
Accuracy for class setUp is: 55.90 %
Accuracy for class onCreate is: 45.84 %
Accuracy for class toString is: 45.05 %
Accuracy for class run is: 32.88 %
Accuracy for class hashCode is: 82.02 %
Accuracy for class init is: 22.65 %
Accuracy for class execute is: 19.28 %
Accuracy for class get is: 27.18 %

Overall Accuracy = 45.97 %

Finished Evaluation



Started training epoch-26


Training epoch-26 batch-1
Running loss of epoch-26 batch-1 = 4.948116838932037e-06

Training epoch-26 batch-2
Running loss of epoch-26 batch-2 = 0.00013570720329880714

Training epoch-26 batch-3
Running loss of epoch-26 batch-3 = 6.145332008600235e-06

Training epoch-26 batch-4
Running loss of epoch-26 batch-4 = 3.2302923500537872e-06

Training epoch-26 batch-5
Running loss of epoch-26 batch-5 = 5.342997610569e-06

Training epoch-26 batch-6
Running loss of epoch-26 batch-6 = 7.783295586705208e-06

Training epoch-26 batch-7
Running loss of epoch-26 batch-7 = 7.833587005734444e-06

Training epoch-26 batch-8
Running loss of epoch-26 batch-8 = 9.666895493865013e-06

Training epoch-26 batch-9
Running loss of epoch-26 batch-9 = 6.500398740172386e-06

Training epoch-26 batch-10
Running loss of epoch-26 batch-10 = 2.328353002667427e-05

Training epoch-26 batch-11
Running loss of epoch-26 batch-11 = 5.7012890465557575e-05

Training epoch-26 batch-12
Running loss of epoch-26 batch-12 = 6.5817148424685e-05

Training epoch-26 batch-13
Running loss of epoch-26 batch-13 = 0.00024090404622256756

Training epoch-26 batch-14
Running loss of epoch-26 batch-14 = 9.187881369143724e-05

Training epoch-26 batch-15
Running loss of epoch-26 batch-15 = 1.0423129424452782e-05

Training epoch-26 batch-16
Running loss of epoch-26 batch-16 = 1.3748416677117348e-05

Training epoch-26 batch-17
Running loss of epoch-26 batch-17 = 4.2745377868413925e-06

Training epoch-26 batch-18
Running loss of epoch-26 batch-18 = 0.00017919420497491956

Training epoch-26 batch-19
Running loss of epoch-26 batch-19 = 8.967705070972443e-06

Training epoch-26 batch-20
Running loss of epoch-26 batch-20 = 1.69120030477643e-05

Training epoch-26 batch-21
Running loss of epoch-26 batch-21 = 6.520701572299004e-05

Training epoch-26 batch-22
Running loss of epoch-26 batch-22 = 1.4604069292545319e-05

Training epoch-26 batch-23
Running loss of epoch-26 batch-23 = 0.00011040066601708531

Training epoch-26 batch-24
Running loss of epoch-26 batch-24 = 8.310750126838684e-05

Training epoch-26 batch-25
Running loss of epoch-26 batch-25 = 1.617975067347288e-05

Training epoch-26 batch-26
Running loss of epoch-26 batch-26 = 7.868109969422221e-05

Training epoch-26 batch-27
Running loss of epoch-26 batch-27 = 2.0842067897319794e-05

Training epoch-26 batch-28
Running loss of epoch-26 batch-28 = 3.218557685613632e-05

Training epoch-26 batch-29
Running loss of epoch-26 batch-29 = 0.000293589080683887

Training epoch-26 batch-30
Running loss of epoch-26 batch-30 = 2.805038820952177e-05

Training epoch-26 batch-31
Running loss of epoch-26 batch-31 = 0.00016938743647187948

Training epoch-26 batch-32
Running loss of epoch-26 batch-32 = 7.086584810167551e-05

Training epoch-26 batch-33
Running loss of epoch-26 batch-33 = 2.4461187422275543e-06

Training epoch-26 batch-34
Running loss of epoch-26 batch-34 = 1.310836523771286e-06

Training epoch-26 batch-35
Running loss of epoch-26 batch-35 = 2.1142885088920593e-05

Training epoch-26 batch-36
Running loss of epoch-26 batch-36 = 0.00019532046280801296

Training epoch-26 batch-37
Running loss of epoch-26 batch-37 = 7.244804874062538e-05

Training epoch-26 batch-38
Running loss of epoch-26 batch-38 = 0.00021128513617441058

Training epoch-26 batch-39
Running loss of epoch-26 batch-39 = 1.14756403490901e-05

Training epoch-26 batch-40
Running loss of epoch-26 batch-40 = 4.34190733358264e-05

Training epoch-26 batch-41
Running loss of epoch-26 batch-41 = 9.33127012103796e-06

Training epoch-26 batch-42
Running loss of epoch-26 batch-42 = 4.420173354446888e-05

Training epoch-26 batch-43
Running loss of epoch-26 batch-43 = 0.000108016945887357

Training epoch-26 batch-44
Running loss of epoch-26 batch-44 = 9.117880836129189e-06

Training epoch-26 batch-45
Running loss of epoch-26 batch-45 = 5.9780548326671124e-05

Training epoch-26 batch-46
Running loss of epoch-26 batch-46 = 0.0001261517172679305

Training epoch-26 batch-47
Running loss of epoch-26 batch-47 = 2.8222333639860153e-05

Training epoch-26 batch-48
Running loss of epoch-26 batch-48 = 4.748115316033363e-06

Training epoch-26 batch-49
Running loss of epoch-26 batch-49 = 1.7820042558014393e-05

Training epoch-26 batch-50
Running loss of epoch-26 batch-50 = 0.0005449361633509398

Training epoch-26 batch-51
Running loss of epoch-26 batch-51 = 5.867797881364822e-06

Training epoch-26 batch-52
Running loss of epoch-26 batch-52 = 1.4937017112970352e-05

Training epoch-26 batch-53
Running loss of epoch-26 batch-53 = 5.346722900867462e-06

Training epoch-26 batch-54
Running loss of epoch-26 batch-54 = 5.63750509172678e-05

Training epoch-26 batch-55
Running loss of epoch-26 batch-55 = 0.0004577088402584195

Training epoch-26 batch-56
Running loss of epoch-26 batch-56 = 8.060259278863668e-05

Training epoch-26 batch-57
Running loss of epoch-26 batch-57 = 7.003522478044033e-05

Training epoch-26 batch-58
Running loss of epoch-26 batch-58 = 8.214032277464867e-06

Training epoch-26 batch-59
Running loss of epoch-26 batch-59 = 3.448151983320713e-05

Training epoch-26 batch-60
Running loss of epoch-26 batch-60 = 0.00031138549093157053

Training epoch-26 batch-61
Running loss of epoch-26 batch-61 = 5.656387656927109e-06

Training epoch-26 batch-62
Running loss of epoch-26 batch-62 = 4.884088411927223e-05

Training epoch-26 batch-63
Running loss of epoch-26 batch-63 = 5.279667675495148e-06

Training epoch-26 batch-64
Running loss of epoch-26 batch-64 = 4.870700649917126e-05

Training epoch-26 batch-65
Running loss of epoch-26 batch-65 = 1.065793912857771e-05

Training epoch-26 batch-66
Running loss of epoch-26 batch-66 = 1.4918274246156216e-05

Training epoch-26 batch-67
Running loss of epoch-26 batch-67 = 2.5308108888566494e-05

Training epoch-26 batch-68
Running loss of epoch-26 batch-68 = 7.640686817467213e-06

Training epoch-26 batch-69
Running loss of epoch-26 batch-69 = 9.104609489440918e-06

Training epoch-26 batch-70
Running loss of epoch-26 batch-70 = 8.433160837739706e-05

Training epoch-26 batch-71
Running loss of epoch-26 batch-71 = 0.00028764165472239256

Training epoch-26 batch-72
Running loss of epoch-26 batch-72 = 3.9092847146093845e-05

Training epoch-26 batch-73
Running loss of epoch-26 batch-73 = 8.745200466364622e-05

Training epoch-26 batch-74
Running loss of epoch-26 batch-74 = 2.3222528398036957e-06

Training epoch-26 batch-75
Running loss of epoch-26 batch-75 = 4.752539098262787e-06

Training epoch-26 batch-76
Running loss of epoch-26 batch-76 = 6.838003173470497e-06

Training epoch-26 batch-77
Running loss of epoch-26 batch-77 = 4.896975588053465e-05

Training epoch-26 batch-78
Running loss of epoch-26 batch-78 = 2.2069900296628475e-05

Training epoch-26 batch-79
Running loss of epoch-26 batch-79 = 1.0745599865913391e-05

Training epoch-26 batch-80
Running loss of epoch-26 batch-80 = 0.00038374768337234855

Training epoch-26 batch-81
Running loss of epoch-26 batch-81 = 3.457395359873772e-05

Training epoch-26 batch-82
Running loss of epoch-26 batch-82 = 4.134606570005417e-06

Training epoch-26 batch-83
Running loss of epoch-26 batch-83 = 5.9569720178842545e-06

Training epoch-26 batch-84
Running loss of epoch-26 batch-84 = 3.248453140258789e-05

Training epoch-26 batch-85
Running loss of epoch-26 batch-85 = 1.9048107787966728e-05

Training epoch-26 batch-86
Running loss of epoch-26 batch-86 = 0.00018800015095621347

Training epoch-26 batch-87
Running loss of epoch-26 batch-87 = 5.0637987442314625e-05

Training epoch-26 batch-88
Running loss of epoch-26 batch-88 = 9.10949893295765e-06

Training epoch-26 batch-89
Running loss of epoch-26 batch-89 = 0.000107277010101825

Training epoch-26 batch-90
Running loss of epoch-26 batch-90 = 1.952541060745716e-05

Training epoch-26 batch-91
Running loss of epoch-26 batch-91 = 0.00023831852013245225

Training epoch-26 batch-92
Running loss of epoch-26 batch-92 = 0.00013379723532125354

Training epoch-26 batch-93
Running loss of epoch-26 batch-93 = 1.877720933407545e-05

Training epoch-26 batch-94
Running loss of epoch-26 batch-94 = 1.5041674487292767e-05

Training epoch-26 batch-95
Running loss of epoch-26 batch-95 = 1.4071352779865265e-05

Training epoch-26 batch-96
Running loss of epoch-26 batch-96 = 7.0820096880197525e-06

Training epoch-26 batch-97
Running loss of epoch-26 batch-97 = 6.767222657799721e-06

Training epoch-26 batch-98
Running loss of epoch-26 batch-98 = 9.719165973365307e-06

Training epoch-26 batch-99
Running loss of epoch-26 batch-99 = 4.912959411740303e-06

Training epoch-26 batch-100
Running loss of epoch-26 batch-100 = 6.990507245063782e-06

Training epoch-26 batch-101
Running loss of epoch-26 batch-101 = 4.0585058741271496e-05

Training epoch-26 batch-102
Running loss of epoch-26 batch-102 = 3.0038529075682163e-05

Training epoch-26 batch-103
Running loss of epoch-26 batch-103 = 8.537439862266183e-05

Training epoch-26 batch-104
Running loss of epoch-26 batch-104 = 4.2674364522099495e-06

Training epoch-26 batch-105
Running loss of epoch-26 batch-105 = 3.4877448342740536e-05

Training epoch-26 batch-106
Running loss of epoch-26 batch-106 = 8.678995072841644e-06

Training epoch-26 batch-107
Running loss of epoch-26 batch-107 = 8.533010259270668e-06

Training epoch-26 batch-108
Running loss of epoch-26 batch-108 = 4.9951253458857536e-05

Training epoch-26 batch-109
Running loss of epoch-26 batch-109 = 0.00028760446002706885

Training epoch-26 batch-110
Running loss of epoch-26 batch-110 = 8.931616321206093e-06

Training epoch-26 batch-111
Running loss of epoch-26 batch-111 = 5.649344529956579e-05

Training epoch-26 batch-112
Running loss of epoch-26 batch-112 = 7.771654054522514e-06

Training epoch-26 batch-113
Running loss of epoch-26 batch-113 = 1.4439807273447514e-05

Training epoch-26 batch-114
Running loss of epoch-26 batch-114 = 7.709721103310585e-06

Training epoch-26 batch-115
Running loss of epoch-26 batch-115 = 3.9381557144224644e-05

Training epoch-26 batch-116
Running loss of epoch-26 batch-116 = 8.773524314165115e-06

Training epoch-26 batch-117
Running loss of epoch-26 batch-117 = 7.811933755874634e-06

Training epoch-26 batch-118
Running loss of epoch-26 batch-118 = 0.00017038907390087843

Training epoch-26 batch-119
Running loss of epoch-26 batch-119 = 2.2487365640699863e-05

Training epoch-26 batch-120
Running loss of epoch-26 batch-120 = 4.871143028140068e-05

Training epoch-26 batch-121
Running loss of epoch-26 batch-121 = 2.4003791622817516e-05

Training epoch-26 batch-122
Running loss of epoch-26 batch-122 = 8.93056858330965e-06

Training epoch-26 batch-123
Running loss of epoch-26 batch-123 = 1.693482045084238e-05

Training epoch-26 batch-124
Running loss of epoch-26 batch-124 = 5.80446794629097e-07

Training epoch-26 batch-125
Running loss of epoch-26 batch-125 = 8.124858140945435e-06

Training epoch-26 batch-126
Running loss of epoch-26 batch-126 = 1.2909527868032455e-05

Training epoch-26 batch-127
Running loss of epoch-26 batch-127 = 1.6060657799243927e-05

Training epoch-26 batch-128
Running loss of epoch-26 batch-128 = 3.870413638651371e-05

Training epoch-26 batch-129
Running loss of epoch-26 batch-129 = 1.2512202374637127e-05

Training epoch-26 batch-130
Running loss of epoch-26 batch-130 = 0.00019650737522169948

Training epoch-26 batch-131
Running loss of epoch-26 batch-131 = 4.3038628064095974e-05

Training epoch-26 batch-132
Running loss of epoch-26 batch-132 = 0.00046817780821584165

Training epoch-26 batch-133
Running loss of epoch-26 batch-133 = 2.4533597752451897e-05

Training epoch-26 batch-134
Running loss of epoch-26 batch-134 = 0.00023447367129847407

Training epoch-26 batch-135
Running loss of epoch-26 batch-135 = 8.434872142970562e-06

Training epoch-26 batch-136
Running loss of epoch-26 batch-136 = 0.0004490078426897526

Training epoch-26 batch-137
Running loss of epoch-26 batch-137 = 2.8604757972061634e-05

Training epoch-26 batch-138
Running loss of epoch-26 batch-138 = 2.1278392523527145e-06

Training epoch-26 batch-139
Running loss of epoch-26 batch-139 = 0.00012283987598493695

Training epoch-26 batch-140
Running loss of epoch-26 batch-140 = 5.64113724976778e-06

Training epoch-26 batch-141
Running loss of epoch-26 batch-141 = 3.333762288093567e-05

Training epoch-26 batch-142
Running loss of epoch-26 batch-142 = 7.244059816002846e-06

Training epoch-26 batch-143
Running loss of epoch-26 batch-143 = 2.1845567971467972e-05

Training epoch-26 batch-144
Running loss of epoch-26 batch-144 = 7.760245352983475e-06

Training epoch-26 batch-145
Running loss of epoch-26 batch-145 = 5.134614184498787e-05

Training epoch-26 batch-146
Running loss of epoch-26 batch-146 = 5.631346721202135e-05

Training epoch-26 batch-147
Running loss of epoch-26 batch-147 = 0.00018644740339368582

Training epoch-26 batch-148
Running loss of epoch-26 batch-148 = 1.831690315157175e-05

Training epoch-26 batch-149
Running loss of epoch-26 batch-149 = 9.76793235167861e-05

Training epoch-26 batch-150
Running loss of epoch-26 batch-150 = 2.1967687644064426e-05

Training epoch-26 batch-151
Running loss of epoch-26 batch-151 = 1.2306729331612587e-05

Training epoch-26 batch-152
Running loss of epoch-26 batch-152 = 0.00012871134094893932

Training epoch-26 batch-153
Running loss of epoch-26 batch-153 = 0.00010083464439958334

Training epoch-26 batch-154
Running loss of epoch-26 batch-154 = 0.00019601377425715327

Training epoch-26 batch-155
Running loss of epoch-26 batch-155 = 1.3051088899374008e-05

Training epoch-26 batch-156
Running loss of epoch-26 batch-156 = 2.797145862132311e-05

Training epoch-26 batch-157
Running loss of epoch-26 batch-157 = 1.4785677194595337e-05

Finished training epoch-26.



Average train loss at epoch-26 = 6.508103553205729e-05

Started Evaluation

Average val loss at epoch-26 = 4.425389142412889

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 48.52 %
Accuracy for class setUp is: 55.74 %
Accuracy for class onCreate is: 42.32 %
Accuracy for class toString is: 40.61 %
Accuracy for class run is: 31.51 %
Accuracy for class hashCode is: 82.02 %
Accuracy for class init is: 25.11 %
Accuracy for class execute is: 16.47 %
Accuracy for class get is: 31.79 %

Overall Accuracy = 45.45 %

Finished Evaluation



Started training epoch-27


Training epoch-27 batch-1
Running loss of epoch-27 batch-1 = 8.931383490562439e-06

Training epoch-27 batch-2
Running loss of epoch-27 batch-2 = 9.490293450653553e-06

Training epoch-27 batch-3
Running loss of epoch-27 batch-3 = 0.00013094628229737282

Training epoch-27 batch-4
Running loss of epoch-27 batch-4 = 7.958861533552408e-05

Training epoch-27 batch-5
Running loss of epoch-27 batch-5 = 4.3463194742798805e-05

Training epoch-27 batch-6
Running loss of epoch-27 batch-6 = 3.1864503398537636e-05

Training epoch-27 batch-7
Running loss of epoch-27 batch-7 = 7.620849646627903e-05

Training epoch-27 batch-8
Running loss of epoch-27 batch-8 = 6.458617281168699e-05

Training epoch-27 batch-9
Running loss of epoch-27 batch-9 = 0.00015544146299362183

Training epoch-27 batch-10
Running loss of epoch-27 batch-10 = 4.6567292883992195e-06

Training epoch-27 batch-11
Running loss of epoch-27 batch-11 = 6.452202796936035e-06

Training epoch-27 batch-12
Running loss of epoch-27 batch-12 = 1.3179727829992771e-05

Training epoch-27 batch-13
Running loss of epoch-27 batch-13 = 0.000581264728680253

Training epoch-27 batch-14
Running loss of epoch-27 batch-14 = 1.0068994015455246e-05

Training epoch-27 batch-15
Running loss of epoch-27 batch-15 = 1.3194512575864792e-06

Training epoch-27 batch-16
Running loss of epoch-27 batch-16 = 3.5669654607772827e-06

Training epoch-27 batch-17
Running loss of epoch-27 batch-17 = 5.652324762195349e-05

Training epoch-27 batch-18
Running loss of epoch-27 batch-18 = 5.364837124943733e-05

Training epoch-27 batch-19
Running loss of epoch-27 batch-19 = 0.0004572969628497958

Training epoch-27 batch-20
Running loss of epoch-27 batch-20 = 1.2902775779366493e-05

Training epoch-27 batch-21
Running loss of epoch-27 batch-21 = 4.2798579670488834e-05

Training epoch-27 batch-22
Running loss of epoch-27 batch-22 = 4.999013617634773e-05

Training epoch-27 batch-23
Running loss of epoch-27 batch-23 = 1.6470905393362045e-05

Training epoch-27 batch-24
Running loss of epoch-27 batch-24 = 3.4067779779434204e-06

Training epoch-27 batch-25
Running loss of epoch-27 batch-25 = 3.089127130806446e-05

Training epoch-27 batch-26
Running loss of epoch-27 batch-26 = 0.00011723308125510812

Training epoch-27 batch-27
Running loss of epoch-27 batch-27 = 0.00025977782206609845

Training epoch-27 batch-28
Running loss of epoch-27 batch-28 = 7.371953688561916e-05

Training epoch-27 batch-29
Running loss of epoch-27 batch-29 = 1.84688251465559e-05

Training epoch-27 batch-30
Running loss of epoch-27 batch-30 = 1.9248342141509056e-05

Training epoch-27 batch-31
Running loss of epoch-27 batch-31 = 8.557923138141632e-06

Training epoch-27 batch-32
Running loss of epoch-27 batch-32 = 0.00041404308285564184

Training epoch-27 batch-33
Running loss of epoch-27 batch-33 = 4.623434506356716e-06

Training epoch-27 batch-34
Running loss of epoch-27 batch-34 = 4.563014954328537e-06

Training epoch-27 batch-35
Running loss of epoch-27 batch-35 = 9.475857950747013e-06

Training epoch-27 batch-36
Running loss of epoch-27 batch-36 = 1.3781478628516197e-05

Training epoch-27 batch-37
Running loss of epoch-27 batch-37 = 3.0189286917448044e-05

Training epoch-27 batch-38
Running loss of epoch-27 batch-38 = 2.8188806027173996e-05

Training epoch-27 batch-39
Running loss of epoch-27 batch-39 = 7.679569534957409e-06

Training epoch-27 batch-40
Running loss of epoch-27 batch-40 = 2.5423127226531506e-05

Training epoch-27 batch-41
Running loss of epoch-27 batch-41 = 2.5575514882802963e-05

Training epoch-27 batch-42
Running loss of epoch-27 batch-42 = 1.8503982573747635e-05

Training epoch-27 batch-43
Running loss of epoch-27 batch-43 = 1.537962816655636e-05

Training epoch-27 batch-44
Running loss of epoch-27 batch-44 = 1.8866034224629402e-05

Training epoch-27 batch-45
Running loss of epoch-27 batch-45 = 2.9021757654845715e-05

Training epoch-27 batch-46
Running loss of epoch-27 batch-46 = 0.0001828353269957006

Training epoch-27 batch-47
Running loss of epoch-27 batch-47 = 0.0001255979877896607

Training epoch-27 batch-48
Running loss of epoch-27 batch-48 = 2.8309179469943047e-05

Training epoch-27 batch-49
Running loss of epoch-27 batch-49 = 7.140799425542355e-06

Training epoch-27 batch-50
Running loss of epoch-27 batch-50 = 1.0084710083901882e-05

Training epoch-27 batch-51
Running loss of epoch-27 batch-51 = 4.8889778554439545e-06

Training epoch-27 batch-52
Running loss of epoch-27 batch-52 = 1.6567762941122055e-05

Training epoch-27 batch-53
Running loss of epoch-27 batch-53 = 5.716690793633461e-06

Training epoch-27 batch-54
Running loss of epoch-27 batch-54 = 2.8319191187620163e-06

Training epoch-27 batch-55
Running loss of epoch-27 batch-55 = 0.00015813519712537527

Training epoch-27 batch-56
Running loss of epoch-27 batch-56 = 0.0003303347621113062

Training epoch-27 batch-57
Running loss of epoch-27 batch-57 = 0.0001833634451031685

Training epoch-27 batch-58
Running loss of epoch-27 batch-58 = 1.1297292076051235e-05

Training epoch-27 batch-59
Running loss of epoch-27 batch-59 = 5.088537000119686e-05

Training epoch-27 batch-60
Running loss of epoch-27 batch-60 = 1.949956640601158e-06

Training epoch-27 batch-61
Running loss of epoch-27 batch-61 = 4.943611565977335e-05

Training epoch-27 batch-62
Running loss of epoch-27 batch-62 = 0.00015339755918830633

Training epoch-27 batch-63
Running loss of epoch-27 batch-63 = 5.48921525478363e-06

Training epoch-27 batch-64
Running loss of epoch-27 batch-64 = 1.2018135748803616e-05

Training epoch-27 batch-65
Running loss of epoch-27 batch-65 = 4.101661033928394e-06

Training epoch-27 batch-66
Running loss of epoch-27 batch-66 = 0.00026516185607761145

Training epoch-27 batch-67
Running loss of epoch-27 batch-67 = 5.414115730673075e-05

Training epoch-27 batch-68
Running loss of epoch-27 batch-68 = 1.4043762348592281e-05

Training epoch-27 batch-69
Running loss of epoch-27 batch-69 = 1.772143878042698e-05

Training epoch-27 batch-70
Running loss of epoch-27 batch-70 = 2.6153400540351868e-05

Training epoch-27 batch-71
Running loss of epoch-27 batch-71 = 7.038703188300133e-06

Training epoch-27 batch-72
Running loss of epoch-27 batch-72 = 1.9897823221981525e-05

Training epoch-27 batch-73
Running loss of epoch-27 batch-73 = 3.0695227906107903e-06

Training epoch-27 batch-74
Running loss of epoch-27 batch-74 = 4.013534635305405e-06

Training epoch-27 batch-75
Running loss of epoch-27 batch-75 = 1.3122102245688438e-05

Training epoch-27 batch-76
Running loss of epoch-27 batch-76 = 4.861562047153711e-05

Training epoch-27 batch-77
Running loss of epoch-27 batch-77 = 0.00019161414820700884

Training epoch-27 batch-78
Running loss of epoch-27 batch-78 = 1.2263190001249313e-05

Training epoch-27 batch-79
Running loss of epoch-27 batch-79 = 6.195856258273125e-06

Training epoch-27 batch-80
Running loss of epoch-27 batch-80 = 9.86751401796937e-05

Training epoch-27 batch-81
Running loss of epoch-27 batch-81 = 1.191615592688322e-05

Training epoch-27 batch-82
Running loss of epoch-27 batch-82 = 5.148118361830711e-06

Training epoch-27 batch-83
Running loss of epoch-27 batch-83 = 0.0004017967148683965

Training epoch-27 batch-84
Running loss of epoch-27 batch-84 = 7.148156873881817e-05

Training epoch-27 batch-85
Running loss of epoch-27 batch-85 = 2.9688235372304916e-06

Training epoch-27 batch-86
Running loss of epoch-27 batch-86 = 0.00029270671075209975

Training epoch-27 batch-87
Running loss of epoch-27 batch-87 = 2.4936278350651264e-05

Training epoch-27 batch-88
Running loss of epoch-27 batch-88 = 4.775822162628174e-06

Training epoch-27 batch-89
Running loss of epoch-27 batch-89 = 1.8496066331863403e-05

Training epoch-27 batch-90
Running loss of epoch-27 batch-90 = 5.599111318588257e-06

Training epoch-27 batch-91
Running loss of epoch-27 batch-91 = 0.0001026730751618743

Training epoch-27 batch-92
Running loss of epoch-27 batch-92 = 1.3524550013244152e-05

Training epoch-27 batch-93
Running loss of epoch-27 batch-93 = 1.0782270692288876e-05

Training epoch-27 batch-94
Running loss of epoch-27 batch-94 = 3.7068966776132584e-06

Training epoch-27 batch-95
Running loss of epoch-27 batch-95 = 3.001466393470764e-05

Training epoch-27 batch-96
Running loss of epoch-27 batch-96 = 3.343378193676472e-05

Training epoch-27 batch-97
Running loss of epoch-27 batch-97 = 2.8603593818843365e-05

Training epoch-27 batch-98
Running loss of epoch-27 batch-98 = 6.497488357126713e-06

Training epoch-27 batch-99
Running loss of epoch-27 batch-99 = 0.0001144327106885612

Training epoch-27 batch-100
Running loss of epoch-27 batch-100 = 2.6579946279525757e-06

Training epoch-27 batch-101
Running loss of epoch-27 batch-101 = 5.152076482772827e-06

Training epoch-27 batch-102
Running loss of epoch-27 batch-102 = 0.00012521707685664296

Training epoch-27 batch-103
Running loss of epoch-27 batch-103 = 0.00015127734513953328

Training epoch-27 batch-104
Running loss of epoch-27 batch-104 = 1.7224112525582314e-05

Training epoch-27 batch-105
Running loss of epoch-27 batch-105 = 1.5064375475049019e-05

Training epoch-27 batch-106
Running loss of epoch-27 batch-106 = 3.287568688392639e-06

Training epoch-27 batch-107
Running loss of epoch-27 batch-107 = 2.542976289987564e-06

Training epoch-27 batch-108
Running loss of epoch-27 batch-108 = 1.3616052456200123e-05

Training epoch-27 batch-109
Running loss of epoch-27 batch-109 = 1.1740252375602722e-05

Training epoch-27 batch-110
Running loss of epoch-27 batch-110 = 4.92586987093091e-05

Training epoch-27 batch-111
Running loss of epoch-27 batch-111 = 3.561365883797407e-05

Training epoch-27 batch-112
Running loss of epoch-27 batch-112 = 8.88085924088955e-06

Training epoch-27 batch-113
Running loss of epoch-27 batch-113 = 1.6382895410060883e-05

Training epoch-27 batch-114
Running loss of epoch-27 batch-114 = 2.0556035451591015e-05

Training epoch-27 batch-115
Running loss of epoch-27 batch-115 = 1.0244082659482956e-05

Training epoch-27 batch-116
Running loss of epoch-27 batch-116 = 1.7964746803045273e-05

Training epoch-27 batch-117
Running loss of epoch-27 batch-117 = 2.5445944629609585e-05

Training epoch-27 batch-118
Running loss of epoch-27 batch-118 = 1.4690333046019077e-05

Training epoch-27 batch-119
Running loss of epoch-27 batch-119 = 5.315523594617844e-06

Training epoch-27 batch-120
Running loss of epoch-27 batch-120 = 6.0663733165711164e-05

Training epoch-27 batch-121
Running loss of epoch-27 batch-121 = 2.160563599318266e-05

Training epoch-27 batch-122
Running loss of epoch-27 batch-122 = 1.4551333151757717e-05

Training epoch-27 batch-123
Running loss of epoch-27 batch-123 = 0.00011073262430727482

Training epoch-27 batch-124
Running loss of epoch-27 batch-124 = 6.237172055989504e-05

Training epoch-27 batch-125
Running loss of epoch-27 batch-125 = 3.222085069864988e-05

Training epoch-27 batch-126
Running loss of epoch-27 batch-126 = 7.60320108383894e-06

Training epoch-27 batch-127
Running loss of epoch-27 batch-127 = 1.3666809536516666e-05

Training epoch-27 batch-128
Running loss of epoch-27 batch-128 = 5.926820449531078e-06

Training epoch-27 batch-129
Running loss of epoch-27 batch-129 = 3.5658013075590134e-06

Training epoch-27 batch-130
Running loss of epoch-27 batch-130 = 0.00013905856758356094

Training epoch-27 batch-131
Running loss of epoch-27 batch-131 = 4.736240953207016e-06

Training epoch-27 batch-132
Running loss of epoch-27 batch-132 = 7.082708179950714e-06

Training epoch-27 batch-133
Running loss of epoch-27 batch-133 = 1.1273776181042194e-05

Training epoch-27 batch-134
Running loss of epoch-27 batch-134 = 7.254257798194885e-05

Training epoch-27 batch-135
Running loss of epoch-27 batch-135 = 6.875721737742424e-06

Training epoch-27 batch-136
Running loss of epoch-27 batch-136 = 1.3920944184064865e-05

Training epoch-27 batch-137
Running loss of epoch-27 batch-137 = 0.00015246355906128883

Training epoch-27 batch-138
Running loss of epoch-27 batch-138 = 3.34207434207201e-05

Training epoch-27 batch-139
Running loss of epoch-27 batch-139 = 2.604210749268532e-06

Training epoch-27 batch-140
Running loss of epoch-27 batch-140 = 0.00010691635543480515

Training epoch-27 batch-141
Running loss of epoch-27 batch-141 = 2.64591071754694e-05

Training epoch-27 batch-142
Running loss of epoch-27 batch-142 = 4.759780131280422e-05

Training epoch-27 batch-143
Running loss of epoch-27 batch-143 = 2.9933522455394268e-05

Training epoch-27 batch-144
Running loss of epoch-27 batch-144 = 3.391900099813938e-05

Training epoch-27 batch-145
Running loss of epoch-27 batch-145 = 8.182955207303166e-05

Training epoch-27 batch-146
Running loss of epoch-27 batch-146 = 0.0001800726167857647

Training epoch-27 batch-147
Running loss of epoch-27 batch-147 = 0.00010860833572223783

Training epoch-27 batch-148
Running loss of epoch-27 batch-148 = 0.0003065314667765051

Training epoch-27 batch-149
Running loss of epoch-27 batch-149 = 7.632770575582981e-06

Training epoch-27 batch-150
Running loss of epoch-27 batch-150 = 4.428718239068985e-05

Training epoch-27 batch-151
Running loss of epoch-27 batch-151 = 6.774987559765577e-05

Training epoch-27 batch-152
Running loss of epoch-27 batch-152 = 0.00013825891073793173

Training epoch-27 batch-153
Running loss of epoch-27 batch-153 = 6.048241630196571e-06

Training epoch-27 batch-154
Running loss of epoch-27 batch-154 = 4.013068974018097e-06

Training epoch-27 batch-155
Running loss of epoch-27 batch-155 = 2.4289940483868122e-05

Training epoch-27 batch-156
Running loss of epoch-27 batch-156 = 4.095933400094509e-05

Training epoch-27 batch-157
Running loss of epoch-27 batch-157 = 9.015202522277832e-07

Finished training epoch-27.



Average train loss at epoch-27 = 5.7479902170598504e-05

Started Evaluation

Average val loss at epoch-27 = 4.488616303393715

Accuracy for classes:
Accuracy for class equals is: 69.14 %
Accuracy for class main is: 50.98 %
Accuracy for class setUp is: 55.25 %
Accuracy for class onCreate is: 43.71 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 34.93 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.63 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 27.95 %

Overall Accuracy = 45.88 %

Finished Evaluation



Started training epoch-28


Training epoch-28 batch-1
Running loss of epoch-28 batch-1 = 4.830537363886833e-06

Training epoch-28 batch-2
Running loss of epoch-28 batch-2 = 1.417787279933691e-05

Training epoch-28 batch-3
Running loss of epoch-28 batch-3 = 4.986301064491272e-05

Training epoch-28 batch-4
Running loss of epoch-28 batch-4 = 7.065344834700227e-05

Training epoch-28 batch-5
Running loss of epoch-28 batch-5 = 0.00022796535631641746

Training epoch-28 batch-6
Running loss of epoch-28 batch-6 = 7.655005902051926e-06

Training epoch-28 batch-7
Running loss of epoch-28 batch-7 = 8.166767656803131e-06

Training epoch-28 batch-8
Running loss of epoch-28 batch-8 = 1.3127224519848824e-05

Training epoch-28 batch-9
Running loss of epoch-28 batch-9 = 1.3334909453988075e-05

Training epoch-28 batch-10
Running loss of epoch-28 batch-10 = 4.874914884567261e-05

Training epoch-28 batch-11
Running loss of epoch-28 batch-11 = 9.866547770798206e-06

Training epoch-28 batch-12
Running loss of epoch-28 batch-12 = 3.389699850231409e-05

Training epoch-28 batch-13
Running loss of epoch-28 batch-13 = 2.1466519683599472e-05

Training epoch-28 batch-14
Running loss of epoch-28 batch-14 = 0.00026458996580913663

Training epoch-28 batch-15
Running loss of epoch-28 batch-15 = 1.2016738764941692e-05

Training epoch-28 batch-16
Running loss of epoch-28 batch-16 = 1.9755796529352665e-05

Training epoch-28 batch-17
Running loss of epoch-28 batch-17 = 2.848682925105095e-06

Training epoch-28 batch-18
Running loss of epoch-28 batch-18 = 1.3095210306346416e-05

Training epoch-28 batch-19
Running loss of epoch-28 batch-19 = 9.459909051656723e-07

Training epoch-28 batch-20
Running loss of epoch-28 batch-20 = 4.909001290798187e-06

Training epoch-28 batch-21
Running loss of epoch-28 batch-21 = 2.5668996386229992e-05

Training epoch-28 batch-22
Running loss of epoch-28 batch-22 = 3.461027517914772e-06

Training epoch-28 batch-23
Running loss of epoch-28 batch-23 = 1.0115792974829674e-05

Training epoch-28 batch-24
Running loss of epoch-28 batch-24 = 8.510635234415531e-05

Training epoch-28 batch-25
Running loss of epoch-28 batch-25 = 6.117671728134155e-05

Training epoch-28 batch-26
Running loss of epoch-28 batch-26 = 0.00019115966279059649

Training epoch-28 batch-27
Running loss of epoch-28 batch-27 = 1.2227450497448444e-05

Training epoch-28 batch-28
Running loss of epoch-28 batch-28 = 2.438644878566265e-05

Training epoch-28 batch-29
Running loss of epoch-28 batch-29 = 2.5706365704536438e-05

Training epoch-28 batch-30
Running loss of epoch-28 batch-30 = 0.00021020369604229927

Training epoch-28 batch-31
Running loss of epoch-28 batch-31 = 5.0326576456427574e-05

Training epoch-28 batch-32
Running loss of epoch-28 batch-32 = 3.819819539785385e-06

Training epoch-28 batch-33
Running loss of epoch-28 batch-33 = 0.00014960078988224268

Training epoch-28 batch-34
Running loss of epoch-28 batch-34 = 4.810828249901533e-05

Training epoch-28 batch-35
Running loss of epoch-28 batch-35 = 9.468011558055878e-05

Training epoch-28 batch-36
Running loss of epoch-28 batch-36 = 8.237769361585379e-05

Training epoch-28 batch-37
Running loss of epoch-28 batch-37 = 1.2689270079135895e-05

Training epoch-28 batch-38
Running loss of epoch-28 batch-38 = 1.2336531654000282e-05

Training epoch-28 batch-39
Running loss of epoch-28 batch-39 = 1.0369461961090565e-05

Training epoch-28 batch-40
Running loss of epoch-28 batch-40 = 0.00028618922806344926

Training epoch-28 batch-41
Running loss of epoch-28 batch-41 = 2.1374551579356194e-05

Training epoch-28 batch-42
Running loss of epoch-28 batch-42 = 1.4324439689517021e-05

Training epoch-28 batch-43
Running loss of epoch-28 batch-43 = 0.00023260677698999643

Training epoch-28 batch-44
Running loss of epoch-28 batch-44 = 8.018105290830135e-06

Training epoch-28 batch-45
Running loss of epoch-28 batch-45 = 0.0001241440186277032

Training epoch-28 batch-46
Running loss of epoch-28 batch-46 = 6.734393537044525e-06

Training epoch-28 batch-47
Running loss of epoch-28 batch-47 = 5.2180374041199684e-05

Training epoch-28 batch-48
Running loss of epoch-28 batch-48 = 8.824880933389068e-05

Training epoch-28 batch-49
Running loss of epoch-28 batch-49 = 9.733950719237328e-06

Training epoch-28 batch-50
Running loss of epoch-28 batch-50 = 7.569789886474609e-06

Training epoch-28 batch-51
Running loss of epoch-28 batch-51 = 3.187684342265129e-06

Training epoch-28 batch-52
Running loss of epoch-28 batch-52 = 4.522968083620071e-06

Training epoch-28 batch-53
Running loss of epoch-28 batch-53 = 0.00017545110313221812

Training epoch-28 batch-54
Running loss of epoch-28 batch-54 = 7.989897858351469e-05

Training epoch-28 batch-55
Running loss of epoch-28 batch-55 = 1.6711303032934666e-05

Training epoch-28 batch-56
Running loss of epoch-28 batch-56 = 1.1044437997043133e-05

Training epoch-28 batch-57
Running loss of epoch-28 batch-57 = 1.1887052096426487e-05

Training epoch-28 batch-58
Running loss of epoch-28 batch-58 = 4.499801434576511e-06

Training epoch-28 batch-59
Running loss of epoch-28 batch-59 = 1.6544247046113014e-05

Training epoch-28 batch-60
Running loss of epoch-28 batch-60 = 7.032300345599651e-06

Training epoch-28 batch-61
Running loss of epoch-28 batch-61 = 6.92809117026627e-05

Training epoch-28 batch-62
Running loss of epoch-28 batch-62 = 3.023305907845497e-06

Training epoch-28 batch-63
Running loss of epoch-28 batch-63 = 8.87818168848753e-06

Training epoch-28 batch-64
Running loss of epoch-28 batch-64 = 7.57013913244009e-06

Training epoch-28 batch-65
Running loss of epoch-28 batch-65 = 1.1398456990718842e-05

Training epoch-28 batch-66
Running loss of epoch-28 batch-66 = 1.0055257007479668e-05

Training epoch-28 batch-67
Running loss of epoch-28 batch-67 = 3.214459866285324e-06

Training epoch-28 batch-68
Running loss of epoch-28 batch-68 = 4.7362991608679295e-05

Training epoch-28 batch-69
Running loss of epoch-28 batch-69 = 1.4607328921556473e-05

Training epoch-28 batch-70
Running loss of epoch-28 batch-70 = 5.802838131785393e-06

Training epoch-28 batch-71
Running loss of epoch-28 batch-71 = 3.752298653125763e-06

Training epoch-28 batch-72
Running loss of epoch-28 batch-72 = 1.1797761544585228e-05

Training epoch-28 batch-73
Running loss of epoch-28 batch-73 = 3.269035369157791e-05

Training epoch-28 batch-74
Running loss of epoch-28 batch-74 = 2.8677750378847122e-06

Training epoch-28 batch-75
Running loss of epoch-28 batch-75 = 1.087517011910677e-05

Training epoch-28 batch-76
Running loss of epoch-28 batch-76 = 9.526382200419903e-06

Training epoch-28 batch-77
Running loss of epoch-28 batch-77 = 9.548710659146309e-05

Training epoch-28 batch-78
Running loss of epoch-28 batch-78 = 2.2679916583001614e-05

Training epoch-28 batch-79
Running loss of epoch-28 batch-79 = 0.00019429530948400497

Training epoch-28 batch-80
Running loss of epoch-28 batch-80 = 1.0524643585085869e-05

Training epoch-28 batch-81
Running loss of epoch-28 batch-81 = 1.7701066099107265e-05

Training epoch-28 batch-82
Running loss of epoch-28 batch-82 = 6.080605089664459e-05

Training epoch-28 batch-83
Running loss of epoch-28 batch-83 = 0.00011157541302964091

Training epoch-28 batch-84
Running loss of epoch-28 batch-84 = 7.04858684912324e-05

Training epoch-28 batch-85
Running loss of epoch-28 batch-85 = 4.234630614519119e-05

Training epoch-28 batch-86
Running loss of epoch-28 batch-86 = 8.08248296380043e-06

Training epoch-28 batch-87
Running loss of epoch-28 batch-87 = 2.729077823460102e-05

Training epoch-28 batch-88
Running loss of epoch-28 batch-88 = 8.606817573308945e-06

Training epoch-28 batch-89
Running loss of epoch-28 batch-89 = 8.504628203809261e-05

Training epoch-28 batch-90
Running loss of epoch-28 batch-90 = 3.220397047698498e-05

Training epoch-28 batch-91
Running loss of epoch-28 batch-91 = 2.8734561055898666e-05

Training epoch-28 batch-92
Running loss of epoch-28 batch-92 = 2.4014152586460114e-06

Training epoch-28 batch-93
Running loss of epoch-28 batch-93 = 6.015179678797722e-06

Training epoch-28 batch-94
Running loss of epoch-28 batch-94 = 1.921318471431732e-06

Training epoch-28 batch-95
Running loss of epoch-28 batch-95 = 3.218441270291805e-05

Training epoch-28 batch-96
Running loss of epoch-28 batch-96 = 5.00003807246685e-06

Training epoch-28 batch-97
Running loss of epoch-28 batch-97 = 2.5620334781706333e-05

Training epoch-28 batch-98
Running loss of epoch-28 batch-98 = 2.584420144557953e-06

Training epoch-28 batch-99
Running loss of epoch-28 batch-99 = 2.3952568881213665e-05

Training epoch-28 batch-100
Running loss of epoch-28 batch-100 = 2.396374475210905e-05

Training epoch-28 batch-101
Running loss of epoch-28 batch-101 = 0.00015648850239813328

Training epoch-28 batch-102
Running loss of epoch-28 batch-102 = 4.527857527136803e-06

Training epoch-28 batch-103
Running loss of epoch-28 batch-103 = 2.2721593268215656e-05

Training epoch-28 batch-104
Running loss of epoch-28 batch-104 = 3.7915538996458054e-05

Training epoch-28 batch-105
Running loss of epoch-28 batch-105 = 2.048863098025322e-05

Training epoch-28 batch-106
Running loss of epoch-28 batch-106 = 6.448372732847929e-05

Training epoch-28 batch-107
Running loss of epoch-28 batch-107 = 9.487959323450923e-05

Training epoch-28 batch-108
Running loss of epoch-28 batch-108 = 8.31799115985632e-06

Training epoch-28 batch-109
Running loss of epoch-28 batch-109 = 3.6581652238965034e-05

Training epoch-28 batch-110
Running loss of epoch-28 batch-110 = 1.7403275705873966e-05

Training epoch-28 batch-111
Running loss of epoch-28 batch-111 = 4.5334454625844955e-06

Training epoch-28 batch-112
Running loss of epoch-28 batch-112 = 5.48316165804863e-06

Training epoch-28 batch-113
Running loss of epoch-28 batch-113 = 4.359520971775055e-06

Training epoch-28 batch-114
Running loss of epoch-28 batch-114 = 5.684094503521919e-06

Training epoch-28 batch-115
Running loss of epoch-28 batch-115 = 4.742247983813286e-05

Training epoch-28 batch-116
Running loss of epoch-28 batch-116 = 8.770980639383197e-05

Training epoch-28 batch-117
Running loss of epoch-28 batch-117 = 0.0005316511378623545

Training epoch-28 batch-118
Running loss of epoch-28 batch-118 = 6.959307938814163e-06

Training epoch-28 batch-119
Running loss of epoch-28 batch-119 = 8.81304731592536e-05

Training epoch-28 batch-120
Running loss of epoch-28 batch-120 = 8.771778084337711e-06

Training epoch-28 batch-121
Running loss of epoch-28 batch-121 = 0.0001238134573213756

Training epoch-28 batch-122
Running loss of epoch-28 batch-122 = 1.0465970262885094e-05

Training epoch-28 batch-123
Running loss of epoch-28 batch-123 = 2.7133850380778313e-05

Training epoch-28 batch-124
Running loss of epoch-28 batch-124 = 0.00017763307550922036

Training epoch-28 batch-125
Running loss of epoch-28 batch-125 = 1.393444836139679e-05

Training epoch-28 batch-126
Running loss of epoch-28 batch-126 = 1.3036420568823814e-05

Training epoch-28 batch-127
Running loss of epoch-28 batch-127 = 7.575494237244129e-06

Training epoch-28 batch-128
Running loss of epoch-28 batch-128 = 0.00039950161590240896

Training epoch-28 batch-129
Running loss of epoch-28 batch-129 = 1.0845600627362728e-05

Training epoch-28 batch-130
Running loss of epoch-28 batch-130 = 1.2272503226995468e-06

Training epoch-28 batch-131
Running loss of epoch-28 batch-131 = 1.6684061847627163e-05

Training epoch-28 batch-132
Running loss of epoch-28 batch-132 = 4.355260170996189e-05

Training epoch-28 batch-133
Running loss of epoch-28 batch-133 = 1.041789073497057e-05

Training epoch-28 batch-134
Running loss of epoch-28 batch-134 = 2.544838935136795e-06

Training epoch-28 batch-135
Running loss of epoch-28 batch-135 = 1.439626794308424e-05

Training epoch-28 batch-136
Running loss of epoch-28 batch-136 = 7.211579941213131e-06

Training epoch-28 batch-137
Running loss of epoch-28 batch-137 = 3.6708079278469086e-06

Training epoch-28 batch-138
Running loss of epoch-28 batch-138 = 1.2968666851520538e-05

Training epoch-28 batch-139
Running loss of epoch-28 batch-139 = 9.888538625091314e-05

Training epoch-28 batch-140
Running loss of epoch-28 batch-140 = 1.2864009477198124e-05

Training epoch-28 batch-141
Running loss of epoch-28 batch-141 = 4.573492333292961e-06

Training epoch-28 batch-142
Running loss of epoch-28 batch-142 = 3.880541771650314e-05

Training epoch-28 batch-143
Running loss of epoch-28 batch-143 = 4.175817593932152e-06

Training epoch-28 batch-144
Running loss of epoch-28 batch-144 = 3.858702257275581e-06

Training epoch-28 batch-145
Running loss of epoch-28 batch-145 = 2.3150350898504257e-06

Training epoch-28 batch-146
Running loss of epoch-28 batch-146 = 9.689608123153448e-05

Training epoch-28 batch-147
Running loss of epoch-28 batch-147 = 0.00013757275883108377

Training epoch-28 batch-148
Running loss of epoch-28 batch-148 = 1.6863923519849777e-06

Training epoch-28 batch-149
Running loss of epoch-28 batch-149 = 6.9052912294864655e-06

Training epoch-28 batch-150
Running loss of epoch-28 batch-150 = 0.0004560874658636749

Training epoch-28 batch-151
Running loss of epoch-28 batch-151 = 0.00020872766617685556

Training epoch-28 batch-152
Running loss of epoch-28 batch-152 = 4.536355845630169e-06

Training epoch-28 batch-153
Running loss of epoch-28 batch-153 = 3.49490437656641e-05

Training epoch-28 batch-154
Running loss of epoch-28 batch-154 = 0.00013838999439030886

Training epoch-28 batch-155
Running loss of epoch-28 batch-155 = 7.098453352227807e-05

Training epoch-28 batch-156
Running loss of epoch-28 batch-156 = 2.4345354177057743e-05

Training epoch-28 batch-157
Running loss of epoch-28 batch-157 = 6.296113133430481e-05

Finished training epoch-28.



Average train loss at epoch-28 = 5.019507370889187e-05

Started Evaluation

Average val loss at epoch-28 = 4.532570264841381

Accuracy for classes:
Accuracy for class equals is: 69.47 %
Accuracy for class main is: 50.82 %
Accuracy for class setUp is: 55.25 %
Accuracy for class onCreate is: 45.63 %
Accuracy for class toString is: 44.37 %
Accuracy for class run is: 34.93 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 18.61 %
Accuracy for class execute is: 18.07 %
Accuracy for class get is: 30.77 %

Overall Accuracy = 46.28 %

Finished Evaluation



Started training epoch-29


Training epoch-29 batch-1
Running loss of epoch-29 batch-1 = 0.000118953175842762

Training epoch-29 batch-2
Running loss of epoch-29 batch-2 = 1.1159456335008144e-05

Training epoch-29 batch-3
Running loss of epoch-29 batch-3 = 0.00011752545833587646

Training epoch-29 batch-4
Running loss of epoch-29 batch-4 = 8.986913599073887e-06

Training epoch-29 batch-5
Running loss of epoch-29 batch-5 = 5.417107604444027e-05

Training epoch-29 batch-6
Running loss of epoch-29 batch-6 = 6.822950672358274e-05

Training epoch-29 batch-7
Running loss of epoch-29 batch-7 = 5.531322676688433e-05

Training epoch-29 batch-8
Running loss of epoch-29 batch-8 = 2.477085217833519e-06

Training epoch-29 batch-9
Running loss of epoch-29 batch-9 = 0.00017008086433634162

Training epoch-29 batch-10
Running loss of epoch-29 batch-10 = 6.367219612002373e-05

Training epoch-29 batch-11
Running loss of epoch-29 batch-11 = 5.505746230483055e-06

Training epoch-29 batch-12
Running loss of epoch-29 batch-12 = 4.842923954129219e-05

Training epoch-29 batch-13
Running loss of epoch-29 batch-13 = 2.122845035046339e-05

Training epoch-29 batch-14
Running loss of epoch-29 batch-14 = 0.00011540099512785673

Training epoch-29 batch-15
Running loss of epoch-29 batch-15 = 1.335260458290577e-05

Training epoch-29 batch-16
Running loss of epoch-29 batch-16 = 8.854782208800316e-06

Training epoch-29 batch-17
Running loss of epoch-29 batch-17 = 9.3050766736269e-05

Training epoch-29 batch-18
Running loss of epoch-29 batch-18 = 6.165355443954468e-06

Training epoch-29 batch-19
Running loss of epoch-29 batch-19 = 0.0001238406402990222

Training epoch-29 batch-20
Running loss of epoch-29 batch-20 = 1.3304757885634899e-05

Training epoch-29 batch-21
Running loss of epoch-29 batch-21 = 4.076515324413776e-06

Training epoch-29 batch-22
Running loss of epoch-29 batch-22 = 3.381050191819668e-05

Training epoch-29 batch-23
Running loss of epoch-29 batch-23 = 2.4303211830556393e-05

Training epoch-29 batch-24
Running loss of epoch-29 batch-24 = 8.032075129449368e-06

Training epoch-29 batch-25
Running loss of epoch-29 batch-25 = 9.288429282605648e-06

Training epoch-29 batch-26
Running loss of epoch-29 batch-26 = 6.139575270935893e-05

Training epoch-29 batch-27
Running loss of epoch-29 batch-27 = 8.526258170604706e-06

Training epoch-29 batch-28
Running loss of epoch-29 batch-28 = 9.249313734471798e-06

Training epoch-29 batch-29
Running loss of epoch-29 batch-29 = 9.289104491472244e-05

Training epoch-29 batch-30
Running loss of epoch-29 batch-30 = 3.4356024116277695e-05

Training epoch-29 batch-31
Running loss of epoch-29 batch-31 = 1.646345481276512e-05

Training epoch-29 batch-32
Running loss of epoch-29 batch-32 = 7.075141184031963e-06

Training epoch-29 batch-33
Running loss of epoch-29 batch-33 = 6.202841177582741e-06

Training epoch-29 batch-34
Running loss of epoch-29 batch-34 = 0.00026834593154489994

Training epoch-29 batch-35
Running loss of epoch-29 batch-35 = 4.963716492056847e-06

Training epoch-29 batch-36
Running loss of epoch-29 batch-36 = 2.466188743710518e-05

Training epoch-29 batch-37
Running loss of epoch-29 batch-37 = 9.6058938652277e-06

Training epoch-29 batch-38
Running loss of epoch-29 batch-38 = 2.028013113886118e-05

Training epoch-29 batch-39
Running loss of epoch-29 batch-39 = 0.00013088854029774666

Training epoch-29 batch-40
Running loss of epoch-29 batch-40 = 2.8989743441343307e-06

Training epoch-29 batch-41
Running loss of epoch-29 batch-41 = 3.550283145159483e-05

Training epoch-29 batch-42
Running loss of epoch-29 batch-42 = 4.356959834694862e-06

Training epoch-29 batch-43
Running loss of epoch-29 batch-43 = 0.00020019325893372297

Training epoch-29 batch-44
Running loss of epoch-29 batch-44 = 2.274429425597191e-05

Training epoch-29 batch-45
Running loss of epoch-29 batch-45 = 2.6223831810057163e-05

Training epoch-29 batch-46
Running loss of epoch-29 batch-46 = 4.661036655306816e-06

Training epoch-29 batch-47
Running loss of epoch-29 batch-47 = 0.00024852342903614044

Training epoch-29 batch-48
Running loss of epoch-29 batch-48 = 3.0003138817846775e-05

Training epoch-29 batch-49
Running loss of epoch-29 batch-49 = 0.0001162069384008646

Training epoch-29 batch-50
Running loss of epoch-29 batch-50 = 6.186310201883316e-06

Training epoch-29 batch-51
Running loss of epoch-29 batch-51 = 9.107287041842937e-06

Training epoch-29 batch-52
Running loss of epoch-29 batch-52 = 7.394992280751467e-05

Training epoch-29 batch-53
Running loss of epoch-29 batch-53 = 2.6625581085681915e-05

Training epoch-29 batch-54
Running loss of epoch-29 batch-54 = 2.8784852474927902e-06

Training epoch-29 batch-55
Running loss of epoch-29 batch-55 = 1.8969993107020855e-05

Training epoch-29 batch-56
Running loss of epoch-29 batch-56 = 1.129484735429287e-05

Training epoch-29 batch-57
Running loss of epoch-29 batch-57 = 4.760688170790672e-06

Training epoch-29 batch-58
Running loss of epoch-29 batch-58 = 1.5170080587267876e-05

Training epoch-29 batch-59
Running loss of epoch-29 batch-59 = 1.1913478374481201e-05

Training epoch-29 batch-60
Running loss of epoch-29 batch-60 = 2.289889380335808e-06

Training epoch-29 batch-61
Running loss of epoch-29 batch-61 = 2.2792373783886433e-05

Training epoch-29 batch-62
Running loss of epoch-29 batch-62 = 7.188878953456879e-06

Training epoch-29 batch-63
Running loss of epoch-29 batch-63 = 1.5132827684283257e-05

Training epoch-29 batch-64
Running loss of epoch-29 batch-64 = 5.540205165743828e-06

Training epoch-29 batch-65
Running loss of epoch-29 batch-65 = 1.764006447046995e-05

Training epoch-29 batch-66
Running loss of epoch-29 batch-66 = 7.765134796500206e-06

Training epoch-29 batch-67
Running loss of epoch-29 batch-67 = 2.2671883925795555e-05

Training epoch-29 batch-68
Running loss of epoch-29 batch-68 = 4.409928806126118e-06

Training epoch-29 batch-69
Running loss of epoch-29 batch-69 = 2.5285989977419376e-05

Training epoch-29 batch-70
Running loss of epoch-29 batch-70 = 1.5583354979753494e-06

Training epoch-29 batch-71
Running loss of epoch-29 batch-71 = 8.216360583901405e-06

Training epoch-29 batch-72
Running loss of epoch-29 batch-72 = 5.022576078772545e-05

Training epoch-29 batch-73
Running loss of epoch-29 batch-73 = 0.00011736748274415731

Training epoch-29 batch-74
Running loss of epoch-29 batch-74 = 1.1310679838061333e-05

Training epoch-29 batch-75
Running loss of epoch-29 batch-75 = 1.484784297645092e-05

Training epoch-29 batch-76
Running loss of epoch-29 batch-76 = 2.65706330537796e-06

Training epoch-29 batch-77
Running loss of epoch-29 batch-77 = 1.6243895515799522e-05

Training epoch-29 batch-78
Running loss of epoch-29 batch-78 = 0.0001488722045905888

Training epoch-29 batch-79
Running loss of epoch-29 batch-79 = 3.590504638850689e-05

Training epoch-29 batch-80
Running loss of epoch-29 batch-80 = 7.202726555988193e-05

Training epoch-29 batch-81
Running loss of epoch-29 batch-81 = 1.9506551325321198e-06

Training epoch-29 batch-82
Running loss of epoch-29 batch-82 = 1.6859732568264008e-05

Training epoch-29 batch-83
Running loss of epoch-29 batch-83 = 4.267028998583555e-05

Training epoch-29 batch-84
Running loss of epoch-29 batch-84 = 8.017057552933693e-06

Training epoch-29 batch-85
Running loss of epoch-29 batch-85 = 8.346629329025745e-06

Training epoch-29 batch-86
Running loss of epoch-29 batch-86 = 0.00015508773503825068

Training epoch-29 batch-87
Running loss of epoch-29 batch-87 = 4.175584763288498e-06

Training epoch-29 batch-88
Running loss of epoch-29 batch-88 = 3.318535163998604e-06

Training epoch-29 batch-89
Running loss of epoch-29 batch-89 = 0.0003104887728113681

Training epoch-29 batch-90
Running loss of epoch-29 batch-90 = 3.545079380273819e-06

Training epoch-29 batch-91
Running loss of epoch-29 batch-91 = 6.877491250634193e-05

Training epoch-29 batch-92
Running loss of epoch-29 batch-92 = 5.1741721108555794e-05

Training epoch-29 batch-93
Running loss of epoch-29 batch-93 = 1.3162498362362385e-05

Training epoch-29 batch-94
Running loss of epoch-29 batch-94 = 1.0495539754629135e-05

Training epoch-29 batch-95
Running loss of epoch-29 batch-95 = 7.792492397129536e-06

Training epoch-29 batch-96
Running loss of epoch-29 batch-96 = 2.8440263122320175e-06

Training epoch-29 batch-97
Running loss of epoch-29 batch-97 = 0.00010266667231917381

Training epoch-29 batch-98
Running loss of epoch-29 batch-98 = 4.585832357406616e-06

Training epoch-29 batch-99
Running loss of epoch-29 batch-99 = 0.00012110831448808312

Training epoch-29 batch-100
Running loss of epoch-29 batch-100 = 4.670524504035711e-05

Training epoch-29 batch-101
Running loss of epoch-29 batch-101 = 4.524830728769302e-06

Training epoch-29 batch-102
Running loss of epoch-29 batch-102 = 2.289190888404846e-06

Training epoch-29 batch-103
Running loss of epoch-29 batch-103 = 0.00017905287677422166

Training epoch-29 batch-104
Running loss of epoch-29 batch-104 = 0.00029159398400224745

Training epoch-29 batch-105
Running loss of epoch-29 batch-105 = 1.9720871932804585e-05

Training epoch-29 batch-106
Running loss of epoch-29 batch-106 = 1.0380055755376816e-05

Training epoch-29 batch-107
Running loss of epoch-29 batch-107 = 6.60470686852932e-06

Training epoch-29 batch-108
Running loss of epoch-29 batch-108 = 8.73918179422617e-06

Training epoch-29 batch-109
Running loss of epoch-29 batch-109 = 3.267067950218916e-05

Training epoch-29 batch-110
Running loss of epoch-29 batch-110 = 7.900991477072239e-06

Training epoch-29 batch-111
Running loss of epoch-29 batch-111 = 1.2853415682911873e-05

Training epoch-29 batch-112
Running loss of epoch-29 batch-112 = 5.400390364229679e-05

Training epoch-29 batch-113
Running loss of epoch-29 batch-113 = 0.00010825996287167072

Training epoch-29 batch-114
Running loss of epoch-29 batch-114 = 3.794266376644373e-05

Training epoch-29 batch-115
Running loss of epoch-29 batch-115 = 0.00010019063483923674

Training epoch-29 batch-116
Running loss of epoch-29 batch-116 = 2.0551960915327072e-06

Training epoch-29 batch-117
Running loss of epoch-29 batch-117 = 0.00032755386200733483

Training epoch-29 batch-118
Running loss of epoch-29 batch-118 = 6.421771831810474e-05

Training epoch-29 batch-119
Running loss of epoch-29 batch-119 = 2.0608771592378616e-05

Training epoch-29 batch-120
Running loss of epoch-29 batch-120 = 8.642557077109814e-06

Training epoch-29 batch-121
Running loss of epoch-29 batch-121 = 2.236559521406889e-05

Training epoch-29 batch-122
Running loss of epoch-29 batch-122 = 3.881519660353661e-06

Training epoch-29 batch-123
Running loss of epoch-29 batch-123 = 4.3165404349565506e-05

Training epoch-29 batch-124
Running loss of epoch-29 batch-124 = 3.269501030445099e-05

Training epoch-29 batch-125
Running loss of epoch-29 batch-125 = 5.370238795876503e-06

Training epoch-29 batch-126
Running loss of epoch-29 batch-126 = 0.00010795827256515622

Training epoch-29 batch-127
Running loss of epoch-29 batch-127 = 5.031260661780834e-05

Training epoch-29 batch-128
Running loss of epoch-29 batch-128 = 2.7371570467948914e-06

Training epoch-29 batch-129
Running loss of epoch-29 batch-129 = 1.4608493074774742e-05

Training epoch-29 batch-130
Running loss of epoch-29 batch-130 = 2.7641654014587402e-06

Training epoch-29 batch-131
Running loss of epoch-29 batch-131 = 3.858702257275581e-06

Training epoch-29 batch-132
Running loss of epoch-29 batch-132 = 5.027977749705315e-06

Training epoch-29 batch-133
Running loss of epoch-29 batch-133 = 9.188661351799965e-06

Training epoch-29 batch-134
Running loss of epoch-29 batch-134 = 5.233450792729855e-06

Training epoch-29 batch-135
Running loss of epoch-29 batch-135 = 1.4651333913207054e-05

Training epoch-29 batch-136
Running loss of epoch-29 batch-136 = 4.388450179249048e-05

Training epoch-29 batch-137
Running loss of epoch-29 batch-137 = 7.575133349746466e-05

Training epoch-29 batch-138
Running loss of epoch-29 batch-138 = 8.641742169857025e-06

Training epoch-29 batch-139
Running loss of epoch-29 batch-139 = 0.00010998209472745657

Training epoch-29 batch-140
Running loss of epoch-29 batch-140 = 1.2285308912396431e-05

Training epoch-29 batch-141
Running loss of epoch-29 batch-141 = 5.201087333261967e-05

Training epoch-29 batch-142
Running loss of epoch-29 batch-142 = 1.3067969121038914e-05

Training epoch-29 batch-143
Running loss of epoch-29 batch-143 = 4.275725223124027e-05

Training epoch-29 batch-144
Running loss of epoch-29 batch-144 = 0.00039816959179006517

Training epoch-29 batch-145
Running loss of epoch-29 batch-145 = 4.7979410737752914e-06

Training epoch-29 batch-146
Running loss of epoch-29 batch-146 = 1.952087040990591e-05

Training epoch-29 batch-147
Running loss of epoch-29 batch-147 = 0.000147664628457278

Training epoch-29 batch-148
Running loss of epoch-29 batch-148 = 9.750132448971272e-06

Training epoch-29 batch-149
Running loss of epoch-29 batch-149 = 3.366265445947647e-05

Training epoch-29 batch-150
Running loss of epoch-29 batch-150 = 4.758464638143778e-05

Training epoch-29 batch-151
Running loss of epoch-29 batch-151 = 1.1342228390276432e-05

Training epoch-29 batch-152
Running loss of epoch-29 batch-152 = 0.00014272157568484545

Training epoch-29 batch-153
Running loss of epoch-29 batch-153 = 4.082522355020046e-05

Training epoch-29 batch-154
Running loss of epoch-29 batch-154 = 1.1738622561097145e-05

Training epoch-29 batch-155
Running loss of epoch-29 batch-155 = 2.8388574719429016e-05

Training epoch-29 batch-156
Running loss of epoch-29 batch-156 = 6.606802344322205e-06

Training epoch-29 batch-157
Running loss of epoch-29 batch-157 = 2.4741515517234802e-05

Finished training epoch-29.



Average train loss at epoch-29 = 4.6409864723682405e-05

Started Evaluation

Average val loss at epoch-29 = 4.586493063914149

Accuracy for classes:
Accuracy for class equals is: 69.31 %
Accuracy for class main is: 48.03 %
Accuracy for class setUp is: 55.41 %
Accuracy for class onCreate is: 43.71 %
Accuracy for class toString is: 41.64 %
Accuracy for class run is: 32.19 %
Accuracy for class hashCode is: 81.27 %
Accuracy for class init is: 21.52 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 30.26 %

Overall Accuracy = 45.43 %

Finished Evaluation



Started training epoch-30


Training epoch-30 batch-1
Running loss of epoch-30 batch-1 = 6.155925802886486e-06

Training epoch-30 batch-2
Running loss of epoch-30 batch-2 = 1.6528763808310032e-05

Training epoch-30 batch-3
Running loss of epoch-30 batch-3 = 9.271781891584396e-05

Training epoch-30 batch-4
Running loss of epoch-30 batch-4 = 5.673209670931101e-05

Training epoch-30 batch-5
Running loss of epoch-30 batch-5 = 3.981171175837517e-06

Training epoch-30 batch-6
Running loss of epoch-30 batch-6 = 2.9501854442059994e-05

Training epoch-30 batch-7
Running loss of epoch-30 batch-7 = 6.5105268731713295e-06

Training epoch-30 batch-8
Running loss of epoch-30 batch-8 = 7.777649443596601e-05

Training epoch-30 batch-9
Running loss of epoch-30 batch-9 = 8.103554137051105e-06

Training epoch-30 batch-10
Running loss of epoch-30 batch-10 = 5.435373168438673e-05

Training epoch-30 batch-11
Running loss of epoch-30 batch-11 = 4.771968815475702e-05

Training epoch-30 batch-12
Running loss of epoch-30 batch-12 = 3.709574230015278e-05

Training epoch-30 batch-13
Running loss of epoch-30 batch-13 = 0.00011801155051216483

Training epoch-30 batch-14
Running loss of epoch-30 batch-14 = 2.2271648049354553e-05

Training epoch-30 batch-15
Running loss of epoch-30 batch-15 = 8.705537766218185e-07

Training epoch-30 batch-16
Running loss of epoch-30 batch-16 = 4.9729133024811745e-06

Training epoch-30 batch-17
Running loss of epoch-30 batch-17 = 0.00014297658344730735

Training epoch-30 batch-18
Running loss of epoch-30 batch-18 = 4.688510671257973e-06

Training epoch-30 batch-19
Running loss of epoch-30 batch-19 = 7.660361006855965e-06

Training epoch-30 batch-20
Running loss of epoch-30 batch-20 = 8.953153155744076e-06

Training epoch-30 batch-21
Running loss of epoch-30 batch-21 = 4.628323949873447e-06

Training epoch-30 batch-22
Running loss of epoch-30 batch-22 = 5.618901923298836e-06

Training epoch-30 batch-23
Running loss of epoch-30 batch-23 = 4.1306368075311184e-05

Training epoch-30 batch-24
Running loss of epoch-30 batch-24 = 4.015257582068443e-05

Training epoch-30 batch-25
Running loss of epoch-30 batch-25 = 4.141824319958687e-06

Training epoch-30 batch-26
Running loss of epoch-30 batch-26 = 0.000416991941165179

Training epoch-30 batch-27
Running loss of epoch-30 batch-27 = 5.845678970217705e-06

Training epoch-30 batch-28
Running loss of epoch-30 batch-28 = 7.297028787434101e-06

Training epoch-30 batch-29
Running loss of epoch-30 batch-29 = 2.034008502960205e-06

Training epoch-30 batch-30
Running loss of epoch-30 batch-30 = 3.183877561241388e-05

Training epoch-30 batch-31
Running loss of epoch-30 batch-31 = 2.2849999368190765e-05

Training epoch-30 batch-32
Running loss of epoch-30 batch-32 = 2.612592652440071e-06

Training epoch-30 batch-33
Running loss of epoch-30 batch-33 = 0.00014158134581521153

Training epoch-30 batch-34
Running loss of epoch-30 batch-34 = 4.675239324569702e-06

Training epoch-30 batch-35
Running loss of epoch-30 batch-35 = 8.687761146575212e-05

Training epoch-30 batch-36
Running loss of epoch-30 batch-36 = 1.7860671505331993e-05

Training epoch-30 batch-37
Running loss of epoch-30 batch-37 = 1.5028868801891804e-05

Training epoch-30 batch-38
Running loss of epoch-30 batch-38 = 8.213764522224665e-05

Training epoch-30 batch-39
Running loss of epoch-30 batch-39 = 8.229282684624195e-06

Training epoch-30 batch-40
Running loss of epoch-30 batch-40 = 3.104086499661207e-05

Training epoch-30 batch-41
Running loss of epoch-30 batch-41 = 1.1277035810053349e-05

Training epoch-30 batch-42
Running loss of epoch-30 batch-42 = 2.606713678687811e-05

Training epoch-30 batch-43
Running loss of epoch-30 batch-43 = 2.1055806428194046e-05

Training epoch-30 batch-44
Running loss of epoch-30 batch-44 = 2.238003071397543e-05

Training epoch-30 batch-45
Running loss of epoch-30 batch-45 = 0.00011415808694437146

Training epoch-30 batch-46
Running loss of epoch-30 batch-46 = 2.2404594346880913e-05

Training epoch-30 batch-47
Running loss of epoch-30 batch-47 = 9.123818017542362e-06

Training epoch-30 batch-48
Running loss of epoch-30 batch-48 = 4.042754881083965e-06

Training epoch-30 batch-49
Running loss of epoch-30 batch-49 = 5.253590643405914e-06

Training epoch-30 batch-50
Running loss of epoch-30 batch-50 = 2.6421621441841125e-06

Training epoch-30 batch-51
Running loss of epoch-30 batch-51 = 0.0002779131173156202

Training epoch-30 batch-52
Running loss of epoch-30 batch-52 = 4.408182576298714e-06

Training epoch-30 batch-53
Running loss of epoch-30 batch-53 = 3.4847762435674667e-06

Training epoch-30 batch-54
Running loss of epoch-30 batch-54 = 5.769077688455582e-06

Training epoch-30 batch-55
Running loss of epoch-30 batch-55 = 2.3758038878440857e-06

Training epoch-30 batch-56
Running loss of epoch-30 batch-56 = 1.526565756648779e-05

Training epoch-30 batch-57
Running loss of epoch-30 batch-57 = 1.9723782315850258e-05

Training epoch-30 batch-58
Running loss of epoch-30 batch-58 = 8.508330211043358e-06

Training epoch-30 batch-59
Running loss of epoch-30 batch-59 = 1.602445263415575e-05

Training epoch-30 batch-60
Running loss of epoch-30 batch-60 = 2.316432073712349e-06

Training epoch-30 batch-61
Running loss of epoch-30 batch-61 = 6.376998499035835e-06

Training epoch-30 batch-62
Running loss of epoch-30 batch-62 = 1.2740609236061573e-05

Training epoch-30 batch-63
Running loss of epoch-30 batch-63 = 2.5564804673194885e-06

Training epoch-30 batch-64
Running loss of epoch-30 batch-64 = 7.588090375065804e-05

Training epoch-30 batch-65
Running loss of epoch-30 batch-65 = 1.6744015738368034e-05

Training epoch-30 batch-66
Running loss of epoch-30 batch-66 = 2.7182046324014664e-05

Training epoch-30 batch-67
Running loss of epoch-30 batch-67 = 2.3369211703538895e-06

Training epoch-30 batch-68
Running loss of epoch-30 batch-68 = 7.120752707123756e-05

Training epoch-30 batch-69
Running loss of epoch-30 batch-69 = 0.00019667641026899219

Training epoch-30 batch-70
Running loss of epoch-30 batch-70 = 8.239137241616845e-05

Training epoch-30 batch-71
Running loss of epoch-30 batch-71 = 1.339660957455635e-05

Training epoch-30 batch-72
Running loss of epoch-30 batch-72 = 2.869335003197193e-05

Training epoch-30 batch-73
Running loss of epoch-30 batch-73 = 3.741122782230377e-05

Training epoch-30 batch-74
Running loss of epoch-30 batch-74 = 2.3525557480752468e-05

Training epoch-30 batch-75
Running loss of epoch-30 batch-75 = 6.957707228139043e-05

Training epoch-30 batch-76
Running loss of epoch-30 batch-76 = 1.127051655203104e-05

Training epoch-30 batch-77
Running loss of epoch-30 batch-77 = 2.938089892268181e-05

Training epoch-30 batch-78
Running loss of epoch-30 batch-78 = 2.595584373921156e-05

Training epoch-30 batch-79
Running loss of epoch-30 batch-79 = 2.3071421310305595e-05

Training epoch-30 batch-80
Running loss of epoch-30 batch-80 = 1.7332029528915882e-05

Training epoch-30 batch-81
Running loss of epoch-30 batch-81 = 3.230408765375614e-05

Training epoch-30 batch-82
Running loss of epoch-30 batch-82 = 4.730885848402977e-06

Training epoch-30 batch-83
Running loss of epoch-30 batch-83 = 1.42599456012249e-05

Training epoch-30 batch-84
Running loss of epoch-30 batch-84 = 0.00013078225310891867

Training epoch-30 batch-85
Running loss of epoch-30 batch-85 = 2.0989682525396347e-06

Training epoch-30 batch-86
Running loss of epoch-30 batch-86 = 1.9091647118330002e-05

Training epoch-30 batch-87
Running loss of epoch-30 batch-87 = 4.224013537168503e-06

Training epoch-30 batch-88
Running loss of epoch-30 batch-88 = 1.4049001038074493e-06

Training epoch-30 batch-89
Running loss of epoch-30 batch-89 = 3.814464434981346e-06

Training epoch-30 batch-90
Running loss of epoch-30 batch-90 = 4.801806062459946e-05

Training epoch-30 batch-91
Running loss of epoch-30 batch-91 = 6.243586540222168e-06

Training epoch-30 batch-92
Running loss of epoch-30 batch-92 = 3.0244700610637665e-05

Training epoch-30 batch-93
Running loss of epoch-30 batch-93 = 6.237532943487167e-06

Training epoch-30 batch-94
Running loss of epoch-30 batch-94 = 7.57887028157711e-06

Training epoch-30 batch-95
Running loss of epoch-30 batch-95 = 7.017457392066717e-05

Training epoch-30 batch-96
Running loss of epoch-30 batch-96 = 6.785267032682896e-06

Training epoch-30 batch-97
Running loss of epoch-30 batch-97 = 0.00036506197648122907

Training epoch-30 batch-98
Running loss of epoch-30 batch-98 = 3.098510205745697e-06

Training epoch-30 batch-99
Running loss of epoch-30 batch-99 = 7.996754720807076e-05

Training epoch-30 batch-100
Running loss of epoch-30 batch-100 = 1.1937227100133896e-06

Training epoch-30 batch-101
Running loss of epoch-30 batch-101 = 4.291534423828125e-06

Training epoch-30 batch-102
Running loss of epoch-30 batch-102 = 4.114629700779915e-05

Training epoch-30 batch-103
Running loss of epoch-30 batch-103 = 4.1523948311805725e-05

Training epoch-30 batch-104
Running loss of epoch-30 batch-104 = 0.0004051470896229148

Training epoch-30 batch-105
Running loss of epoch-30 batch-105 = 6.338930688798428e-06

Training epoch-30 batch-106
Running loss of epoch-30 batch-106 = 1.3408716768026352e-06

Training epoch-30 batch-107
Running loss of epoch-30 batch-107 = 4.145083948969841e-06

Training epoch-30 batch-108
Running loss of epoch-30 batch-108 = 8.046557195484638e-05

Training epoch-30 batch-109
Running loss of epoch-30 batch-109 = 2.0912964828312397e-05

Training epoch-30 batch-110
Running loss of epoch-30 batch-110 = 3.825873136520386e-06

Training epoch-30 batch-111
Running loss of epoch-30 batch-111 = 0.00011097895912826061

Training epoch-30 batch-112
Running loss of epoch-30 batch-112 = 2.72504985332489e-06

Training epoch-30 batch-113
Running loss of epoch-30 batch-113 = 0.0001693913945928216

Training epoch-30 batch-114
Running loss of epoch-30 batch-114 = 4.907138645648956e-06

Training epoch-30 batch-115
Running loss of epoch-30 batch-115 = 1.0340474545955658e-05

Training epoch-30 batch-116
Running loss of epoch-30 batch-116 = 6.1801401898264885e-06

Training epoch-30 batch-117
Running loss of epoch-30 batch-117 = 8.516712114214897e-06

Training epoch-30 batch-118
Running loss of epoch-30 batch-118 = 1.330627128481865e-06

Training epoch-30 batch-119
Running loss of epoch-30 batch-119 = 6.563263013958931e-06

Training epoch-30 batch-120
Running loss of epoch-30 batch-120 = 0.00019087281543761492

Training epoch-30 batch-121
Running loss of epoch-30 batch-121 = 5.735526792705059e-05

Training epoch-30 batch-122
Running loss of epoch-30 batch-122 = 6.7623332142829895e-06

Training epoch-30 batch-123
Running loss of epoch-30 batch-123 = 7.570954039692879e-06

Training epoch-30 batch-124
Running loss of epoch-30 batch-124 = 1.5818397514522076e-05

Training epoch-30 batch-125
Running loss of epoch-30 batch-125 = 3.290618769824505e-05

Training epoch-30 batch-126
Running loss of epoch-30 batch-126 = 3.6044511944055557e-06

Training epoch-30 batch-127
Running loss of epoch-30 batch-127 = 4.950212314724922e-06

Training epoch-30 batch-128
Running loss of epoch-30 batch-128 = 1.416075974702835e-05

Training epoch-30 batch-129
Running loss of epoch-30 batch-129 = 4.3602194637060165e-06

Training epoch-30 batch-130
Running loss of epoch-30 batch-130 = 1.151056494563818e-05

Training epoch-30 batch-131
Running loss of epoch-30 batch-131 = 1.0600779205560684e-06

Training epoch-30 batch-132
Running loss of epoch-30 batch-132 = 8.432078175246716e-06

Training epoch-30 batch-133
Running loss of epoch-30 batch-133 = 5.149515345692635e-06

Training epoch-30 batch-134
Running loss of epoch-30 batch-134 = 7.513212040066719e-06

Training epoch-30 batch-135
Running loss of epoch-30 batch-135 = 3.112945705652237e-06

Training epoch-30 batch-136
Running loss of epoch-30 batch-136 = 6.480450974777341e-05

Training epoch-30 batch-137
Running loss of epoch-30 batch-137 = 5.4620904847979546e-06

Training epoch-30 batch-138
Running loss of epoch-30 batch-138 = 0.00015396182425320148

Training epoch-30 batch-139
Running loss of epoch-30 batch-139 = 3.893952816724777e-05

Training epoch-30 batch-140
Running loss of epoch-30 batch-140 = 0.0001812702976167202

Training epoch-30 batch-141
Running loss of epoch-30 batch-141 = 3.6407727748155594e-06

Training epoch-30 batch-142
Running loss of epoch-30 batch-142 = 2.433138433843851e-05

Training epoch-30 batch-143
Running loss of epoch-30 batch-143 = 6.3124462030828e-05

Training epoch-30 batch-144
Running loss of epoch-30 batch-144 = 5.847658030688763e-06

Training epoch-30 batch-145
Running loss of epoch-30 batch-145 = 0.0001798254088498652

Training epoch-30 batch-146
Running loss of epoch-30 batch-146 = 7.803086191415787e-06

Training epoch-30 batch-147
Running loss of epoch-30 batch-147 = 2.4081673473119736e-05

Training epoch-30 batch-148
Running loss of epoch-30 batch-148 = 9.729759767651558e-06

Training epoch-30 batch-149
Running loss of epoch-30 batch-149 = 3.252224996685982e-05

Training epoch-30 batch-150
Running loss of epoch-30 batch-150 = 8.060433901846409e-05

Training epoch-30 batch-151
Running loss of epoch-30 batch-151 = 5.882233381271362e-06

Training epoch-30 batch-152
Running loss of epoch-30 batch-152 = 9.502045577391982e-05

Training epoch-30 batch-153
Running loss of epoch-30 batch-153 = 8.093193173408508e-06

Training epoch-30 batch-154
Running loss of epoch-30 batch-154 = 4.572211764752865e-06

Training epoch-30 batch-155
Running loss of epoch-30 batch-155 = 2.942758146673441e-05

Training epoch-30 batch-156
Running loss of epoch-30 batch-156 = 3.3578835427761078e-06

Training epoch-30 batch-157
Running loss of epoch-30 batch-157 = 9.133666753768921e-05

Finished training epoch-30.



Average train loss at epoch-30 = 4.016575254499912e-05

Started Evaluation

Average val loss at epoch-30 = 4.652059247619228

Accuracy for classes:
Accuracy for class equals is: 68.48 %
Accuracy for class main is: 48.69 %
Accuracy for class setUp is: 54.75 %
Accuracy for class onCreate is: 42.64 %
Accuracy for class toString is: 42.32 %
Accuracy for class run is: 35.62 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 22.42 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 29.23 %

Overall Accuracy = 45.45 %

Finished Evaluation



Started training epoch-31


Training epoch-31 batch-1
Running loss of epoch-31 batch-1 = 1.758965663611889e-05

Training epoch-31 batch-2
Running loss of epoch-31 batch-2 = 9.640934877097607e-06

Training epoch-31 batch-3
Running loss of epoch-31 batch-3 = 2.732954453676939e-05

Training epoch-31 batch-4
Running loss of epoch-31 batch-4 = 9.580980986356735e-07

Training epoch-31 batch-5
Running loss of epoch-31 batch-5 = 1.3168319128453732e-05

Training epoch-31 batch-6
Running loss of epoch-31 batch-6 = 3.5350676625967026e-06

Training epoch-31 batch-7
Running loss of epoch-31 batch-7 = 5.7675642892718315e-06

Training epoch-31 batch-8
Running loss of epoch-31 batch-8 = 0.00013899145415052772

Training epoch-31 batch-9
Running loss of epoch-31 batch-9 = 4.254747182130814e-06

Training epoch-31 batch-10
Running loss of epoch-31 batch-10 = 1.0391464456915855e-05

Training epoch-31 batch-11
Running loss of epoch-31 batch-11 = 2.3262109607458115e-06

Training epoch-31 batch-12
Running loss of epoch-31 batch-12 = 3.0494295060634613e-05

Training epoch-31 batch-13
Running loss of epoch-31 batch-13 = 8.111330680549145e-05

Training epoch-31 batch-14
Running loss of epoch-31 batch-14 = 3.353692591190338e-06

Training epoch-31 batch-15
Running loss of epoch-31 batch-15 = 3.1869858503341675e-06

Training epoch-31 batch-16
Running loss of epoch-31 batch-16 = 2.3709144443273544e-06

Training epoch-31 batch-17
Running loss of epoch-31 batch-17 = 2.3260945454239845e-06

Training epoch-31 batch-18
Running loss of epoch-31 batch-18 = 2.4150474928319454e-05

Training epoch-31 batch-19
Running loss of epoch-31 batch-19 = 4.714354872703552e-05

Training epoch-31 batch-20
Running loss of epoch-31 batch-20 = 1.1583324521780014e-06

Training epoch-31 batch-21
Running loss of epoch-31 batch-21 = 2.7644331566989422e-05

Training epoch-31 batch-22
Running loss of epoch-31 batch-22 = 1.2711039744317532e-05

Training epoch-31 batch-23
Running loss of epoch-31 batch-23 = 4.6855187974870205e-05

Training epoch-31 batch-24
Running loss of epoch-31 batch-24 = 1.1137337423861027e-05

Training epoch-31 batch-25
Running loss of epoch-31 batch-25 = 4.225876182317734e-07

Training epoch-31 batch-26
Running loss of epoch-31 batch-26 = 2.700788900256157e-05

Training epoch-31 batch-27
Running loss of epoch-31 batch-27 = 4.4388347305357456e-05

Training epoch-31 batch-28
Running loss of epoch-31 batch-28 = 3.3979304134845734e-06

Training epoch-31 batch-29
Running loss of epoch-31 batch-29 = 0.00029203854501247406

Training epoch-31 batch-30
Running loss of epoch-31 batch-30 = 5.2884453907608986e-05

Training epoch-31 batch-31
Running loss of epoch-31 batch-31 = 1.77859328687191e-06

Training epoch-31 batch-32
Running loss of epoch-31 batch-32 = 1.2384261935949326e-05

Training epoch-31 batch-33
Running loss of epoch-31 batch-33 = 1.544714905321598e-05

Training epoch-31 batch-34
Running loss of epoch-31 batch-34 = 5.072250496596098e-05

Training epoch-31 batch-35
Running loss of epoch-31 batch-35 = 3.897387068718672e-05

Training epoch-31 batch-36
Running loss of epoch-31 batch-36 = 0.0001401812769472599

Training epoch-31 batch-37
Running loss of epoch-31 batch-37 = 1.2272503226995468e-06

Training epoch-31 batch-38
Running loss of epoch-31 batch-38 = 7.38631933927536e-06

Training epoch-31 batch-39
Running loss of epoch-31 batch-39 = 2.491287887096405e-06

Training epoch-31 batch-40
Running loss of epoch-31 batch-40 = 9.655952453613281e-06

Training epoch-31 batch-41
Running loss of epoch-31 batch-41 = 1.1797179467976093e-05

Training epoch-31 batch-42
Running loss of epoch-31 batch-42 = 1.575134228914976e-05

Training epoch-31 batch-43
Running loss of epoch-31 batch-43 = 1.2975651770830154e-06

Training epoch-31 batch-44
Running loss of epoch-31 batch-44 = 5.1140435971319675e-05

Training epoch-31 batch-45
Running loss of epoch-31 batch-45 = 2.437969669699669e-06

Training epoch-31 batch-46
Running loss of epoch-31 batch-46 = 4.666391760110855e-06

Training epoch-31 batch-47
Running loss of epoch-31 batch-47 = 0.0003488161019049585

Training epoch-31 batch-48
Running loss of epoch-31 batch-48 = 6.2694307416677475e-06

Training epoch-31 batch-49
Running loss of epoch-31 batch-49 = 6.59446232020855e-05

Training epoch-31 batch-50
Running loss of epoch-31 batch-50 = 2.5585060939192772e-05

Training epoch-31 batch-51
Running loss of epoch-31 batch-51 = 4.022533539682627e-05

Training epoch-31 batch-52
Running loss of epoch-31 batch-52 = 3.2365787774324417e-06

Training epoch-31 batch-53
Running loss of epoch-31 batch-53 = 5.407724529504776e-06

Training epoch-31 batch-54
Running loss of epoch-31 batch-54 = 2.596341073513031e-05

Training epoch-31 batch-55
Running loss of epoch-31 batch-55 = 9.494018740952015e-06

Training epoch-31 batch-56
Running loss of epoch-31 batch-56 = 3.059627488255501e-06

Training epoch-31 batch-57
Running loss of epoch-31 batch-57 = 3.492925316095352e-06

Training epoch-31 batch-58
Running loss of epoch-31 batch-58 = 2.6349443942308426e-06

Training epoch-31 batch-59
Running loss of epoch-31 batch-59 = 5.0915172323584557e-05

Training epoch-31 batch-60
Running loss of epoch-31 batch-60 = 0.00019903015345335007

Training epoch-31 batch-61
Running loss of epoch-31 batch-61 = 3.803987056016922e-06

Training epoch-31 batch-62
Running loss of epoch-31 batch-62 = 3.0884984880685806e-06

Training epoch-31 batch-63
Running loss of epoch-31 batch-63 = 1.788849476724863e-05

Training epoch-31 batch-64
Running loss of epoch-31 batch-64 = 8.000642992556095e-06

Training epoch-31 batch-65
Running loss of epoch-31 batch-65 = 3.078952431678772e-06

Training epoch-31 batch-66
Running loss of epoch-31 batch-66 = 1.0026036761701107e-05

Training epoch-31 batch-67
Running loss of epoch-31 batch-67 = 1.5046214684844017e-05

Training epoch-31 batch-68
Running loss of epoch-31 batch-68 = 3.185798414051533e-05

Training epoch-31 batch-69
Running loss of epoch-31 batch-69 = 2.8070411644876003e-05

Training epoch-31 batch-70
Running loss of epoch-31 batch-70 = 8.347444236278534e-06

Training epoch-31 batch-71
Running loss of epoch-31 batch-71 = 1.4719553291797638e-06

Training epoch-31 batch-72
Running loss of epoch-31 batch-72 = 9.99285839498043e-06

Training epoch-31 batch-73
Running loss of epoch-31 batch-73 = 1.4338409528136253e-05

Training epoch-31 batch-74
Running loss of epoch-31 batch-74 = 1.2685311958193779e-05

Training epoch-31 batch-75
Running loss of epoch-31 batch-75 = 0.00013449799735099077

Training epoch-31 batch-76
Running loss of epoch-31 batch-76 = 2.9525253921747208e-06

Training epoch-31 batch-77
Running loss of epoch-31 batch-77 = 5.6706019677221775e-05

Training epoch-31 batch-78
Running loss of epoch-31 batch-78 = 1.715810503810644e-05

Training epoch-31 batch-79
Running loss of epoch-31 batch-79 = 5.0532398745417595e-05

Training epoch-31 batch-80
Running loss of epoch-31 batch-80 = 6.805639714002609e-06

Training epoch-31 batch-81
Running loss of epoch-31 batch-81 = 2.487259916961193e-05

Training epoch-31 batch-82
Running loss of epoch-31 batch-82 = 2.37366184592247e-05

Training epoch-31 batch-83
Running loss of epoch-31 batch-83 = 1.621665433049202e-05

Training epoch-31 batch-84
Running loss of epoch-31 batch-84 = 0.00012215558672323823

Training epoch-31 batch-85
Running loss of epoch-31 batch-85 = 1.3224780559539795e-06

Training epoch-31 batch-86
Running loss of epoch-31 batch-86 = 0.0001433597644791007

Training epoch-31 batch-87
Running loss of epoch-31 batch-87 = 7.140502566471696e-05

Training epoch-31 batch-88
Running loss of epoch-31 batch-88 = 3.8458965718746185e-06

Training epoch-31 batch-89
Running loss of epoch-31 batch-89 = 6.771006155759096e-05

Training epoch-31 batch-90
Running loss of epoch-31 batch-90 = 8.987804176285863e-05

Training epoch-31 batch-91
Running loss of epoch-31 batch-91 = 0.00022987043485045433

Training epoch-31 batch-92
Running loss of epoch-31 batch-92 = 0.00012787367450073361

Training epoch-31 batch-93
Running loss of epoch-31 batch-93 = 5.087349563837051e-06

Training epoch-31 batch-94
Running loss of epoch-31 batch-94 = 4.56138513982296e-06

Training epoch-31 batch-95
Running loss of epoch-31 batch-95 = 1.793564297258854e-05

Training epoch-31 batch-96
Running loss of epoch-31 batch-96 = 4.436587914824486e-06

Training epoch-31 batch-97
Running loss of epoch-31 batch-97 = 9.498093277215958e-06

Training epoch-31 batch-98
Running loss of epoch-31 batch-98 = 1.0858522728085518e-05

Training epoch-31 batch-99
Running loss of epoch-31 batch-99 = 1.7368816770613194e-05

Training epoch-31 batch-100
Running loss of epoch-31 batch-100 = 7.271289359778166e-05

Training epoch-31 batch-101
Running loss of epoch-31 batch-101 = 8.47994233481586e-05

Training epoch-31 batch-102
Running loss of epoch-31 batch-102 = 2.006511203944683e-05

Training epoch-31 batch-103
Running loss of epoch-31 batch-103 = 1.2649688869714737e-05

Training epoch-31 batch-104
Running loss of epoch-31 batch-104 = 4.2942934669554234e-05

Training epoch-31 batch-105
Running loss of epoch-31 batch-105 = 9.111344115808606e-05

Training epoch-31 batch-106
Running loss of epoch-31 batch-106 = 4.278961569070816e-06

Training epoch-31 batch-107
Running loss of epoch-31 batch-107 = 1.876847818493843e-06

Training epoch-31 batch-108
Running loss of epoch-31 batch-108 = 0.00024223257787525654

Training epoch-31 batch-109
Running loss of epoch-31 batch-109 = 2.103857696056366e-06

Training epoch-31 batch-110
Running loss of epoch-31 batch-110 = 3.263656981289387e-05

Training epoch-31 batch-111
Running loss of epoch-31 batch-111 = 0.0001940888469107449

Training epoch-31 batch-112
Running loss of epoch-31 batch-112 = 8.238013833761215e-06

Training epoch-31 batch-113
Running loss of epoch-31 batch-113 = 2.185662742704153e-05

Training epoch-31 batch-114
Running loss of epoch-31 batch-114 = 8.689239621162415e-07

Training epoch-31 batch-115
Running loss of epoch-31 batch-115 = 3.734370693564415e-06

Training epoch-31 batch-116
Running loss of epoch-31 batch-116 = 2.264778595417738e-05

Training epoch-31 batch-117
Running loss of epoch-31 batch-117 = 7.967144483700395e-05

Training epoch-31 batch-118
Running loss of epoch-31 batch-118 = 2.2333115339279175e-06

Training epoch-31 batch-119
Running loss of epoch-31 batch-119 = 2.633070107549429e-05

Training epoch-31 batch-120
Running loss of epoch-31 batch-120 = 6.710959132760763e-05

Training epoch-31 batch-121
Running loss of epoch-31 batch-121 = 4.401186015456915e-05

Training epoch-31 batch-122
Running loss of epoch-31 batch-122 = 3.5443808883428574e-06

Training epoch-31 batch-123
Running loss of epoch-31 batch-123 = 6.445799954235554e-06

Training epoch-31 batch-124
Running loss of epoch-31 batch-124 = 6.454018875956535e-05

Training epoch-31 batch-125
Running loss of epoch-31 batch-125 = 2.880091778934002e-05

Training epoch-31 batch-126
Running loss of epoch-31 batch-126 = 1.0712770745158195e-05

Training epoch-31 batch-127
Running loss of epoch-31 batch-127 = 1.3076583854854107e-05

Training epoch-31 batch-128
Running loss of epoch-31 batch-128 = 2.7362140826880932e-05

Training epoch-31 batch-129
Running loss of epoch-31 batch-129 = 3.520399332046509e-06

Training epoch-31 batch-130
Running loss of epoch-31 batch-130 = 2.930639311671257e-06

Training epoch-31 batch-131
Running loss of epoch-31 batch-131 = 3.547174856066704e-05

Training epoch-31 batch-132
Running loss of epoch-31 batch-132 = 4.3816398829221725e-06

Training epoch-31 batch-133
Running loss of epoch-31 batch-133 = 9.065144695341587e-06

Training epoch-31 batch-134
Running loss of epoch-31 batch-134 = 2.9333634302020073e-05

Training epoch-31 batch-135
Running loss of epoch-31 batch-135 = 7.10261519998312e-06

Training epoch-31 batch-136
Running loss of epoch-31 batch-136 = 6.229383870959282e-06

Training epoch-31 batch-137
Running loss of epoch-31 batch-137 = 4.306319169700146e-06

Training epoch-31 batch-138
Running loss of epoch-31 batch-138 = 1.4159828424453735e-05

Training epoch-31 batch-139
Running loss of epoch-31 batch-139 = 1.8637278117239475e-05

Training epoch-31 batch-140
Running loss of epoch-31 batch-140 = 7.365713827311993e-06

Training epoch-31 batch-141
Running loss of epoch-31 batch-141 = 2.0810402929782867e-06

Training epoch-31 batch-142
Running loss of epoch-31 batch-142 = 7.551861926913261e-06

Training epoch-31 batch-143
Running loss of epoch-31 batch-143 = 5.759415216743946e-06

Training epoch-31 batch-144
Running loss of epoch-31 batch-144 = 8.634052937850356e-05

Training epoch-31 batch-145
Running loss of epoch-31 batch-145 = 4.519475623965263e-06

Training epoch-31 batch-146
Running loss of epoch-31 batch-146 = 6.527337245643139e-05

Training epoch-31 batch-147
Running loss of epoch-31 batch-147 = 2.956017851829529e-06

Training epoch-31 batch-148
Running loss of epoch-31 batch-148 = 1.6387668438255787e-05

Training epoch-31 batch-149
Running loss of epoch-31 batch-149 = 3.760459367185831e-05

Training epoch-31 batch-150
Running loss of epoch-31 batch-150 = 2.341112121939659e-06

Training epoch-31 batch-151
Running loss of epoch-31 batch-151 = 0.00033253166475333273

Training epoch-31 batch-152
Running loss of epoch-31 batch-152 = 8.092145435512066e-06

Training epoch-31 batch-153
Running loss of epoch-31 batch-153 = 1.7683953046798706e-05

Training epoch-31 batch-154
Running loss of epoch-31 batch-154 = 9.398569818586111e-05

Training epoch-31 batch-155
Running loss of epoch-31 batch-155 = 2.7257949113845825e-05

Training epoch-31 batch-156
Running loss of epoch-31 batch-156 = 3.898784052580595e-05

Training epoch-31 batch-157
Running loss of epoch-31 batch-157 = 1.9107013940811157e-05

Finished training epoch-31.



Average train loss at epoch-31 = 3.624203484505415e-05

Started Evaluation

Average val loss at epoch-31 = 4.705069585850365

Accuracy for classes:
Accuracy for class equals is: 68.98 %
Accuracy for class main is: 48.36 %
Accuracy for class setUp is: 54.92 %
Accuracy for class onCreate is: 42.96 %
Accuracy for class toString is: 42.32 %
Accuracy for class run is: 34.47 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.75 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 29.49 %

Overall Accuracy = 45.41 %

Finished Evaluation



Started training epoch-32


Training epoch-32 batch-1
Running loss of epoch-32 batch-1 = 0.00011487078154459596

Training epoch-32 batch-2
Running loss of epoch-32 batch-2 = 4.637520760297775e-06

Training epoch-32 batch-3
Running loss of epoch-32 batch-3 = 3.1045638024806976e-06

Training epoch-32 batch-4
Running loss of epoch-32 batch-4 = 1.0534306056797504e-05

Training epoch-32 batch-5
Running loss of epoch-32 batch-5 = 1.5206634998321533e-05

Training epoch-32 batch-6
Running loss of epoch-32 batch-6 = 1.2174248695373535e-05

Training epoch-32 batch-7
Running loss of epoch-32 batch-7 = 5.6265853345394135e-06

Training epoch-32 batch-8
Running loss of epoch-32 batch-8 = 6.617512553930283e-06

Training epoch-32 batch-9
Running loss of epoch-32 batch-9 = 7.564784027636051e-06

Training epoch-32 batch-10
Running loss of epoch-32 batch-10 = 2.214382402598858e-05

Training epoch-32 batch-11
Running loss of epoch-32 batch-11 = 5.628913640975952e-06

Training epoch-32 batch-12
Running loss of epoch-32 batch-12 = 1.7152633517980576e-06

Training epoch-32 batch-13
Running loss of epoch-32 batch-13 = 2.7397298254072666e-05

Training epoch-32 batch-14
Running loss of epoch-32 batch-14 = 2.1280022338032722e-05

Training epoch-32 batch-15
Running loss of epoch-32 batch-15 = 8.118106052279472e-06

Training epoch-32 batch-16
Running loss of epoch-32 batch-16 = 7.2480179369449615e-06

Training epoch-32 batch-17
Running loss of epoch-32 batch-17 = 1.809629611670971e-05

Training epoch-32 batch-18
Running loss of epoch-32 batch-18 = 0.00011314853327348828

Training epoch-32 batch-19
Running loss of epoch-32 batch-19 = 3.141665365546942e-05

Training epoch-32 batch-20
Running loss of epoch-32 batch-20 = 3.9481441490352154e-05

Training epoch-32 batch-21
Running loss of epoch-32 batch-21 = 0.00016273354412987828

Training epoch-32 batch-22
Running loss of epoch-32 batch-22 = 2.0265928469598293e-05

Training epoch-32 batch-23
Running loss of epoch-32 batch-23 = 6.188871338963509e-06

Training epoch-32 batch-24
Running loss of epoch-32 batch-24 = 1.7704442143440247e-06

Training epoch-32 batch-25
Running loss of epoch-32 batch-25 = 5.825888365507126e-06

Training epoch-32 batch-26
Running loss of epoch-32 batch-26 = 3.551540430635214e-05

Training epoch-32 batch-27
Running loss of epoch-32 batch-27 = 2.780428621917963e-05

Training epoch-32 batch-28
Running loss of epoch-32 batch-28 = 1.6705598682165146e-06

Training epoch-32 batch-29
Running loss of epoch-32 batch-29 = 1.8287100829184055e-05

Training epoch-32 batch-30
Running loss of epoch-32 batch-30 = 1.2596137821674347e-06

Training epoch-32 batch-31
Running loss of epoch-32 batch-31 = 0.00010650762123987079

Training epoch-32 batch-32
Running loss of epoch-32 batch-32 = 3.2836105674505234e-06

Training epoch-32 batch-33
Running loss of epoch-32 batch-33 = 1.5954021364450455e-05

Training epoch-32 batch-34
Running loss of epoch-32 batch-34 = 6.948364898562431e-06

Training epoch-32 batch-35
Running loss of epoch-32 batch-35 = 1.611944753676653e-05

Training epoch-32 batch-36
Running loss of epoch-32 batch-36 = 5.95850870013237e-05

Training epoch-32 batch-37
Running loss of epoch-32 batch-37 = 1.146690919995308e-06

Training epoch-32 batch-38
Running loss of epoch-32 batch-38 = 7.599126547574997e-06

Training epoch-32 batch-39
Running loss of epoch-32 batch-39 = 3.3708056434988976e-05

Training epoch-32 batch-40
Running loss of epoch-32 batch-40 = 5.786074325442314e-06

Training epoch-32 batch-41
Running loss of epoch-32 batch-41 = 6.732647307217121e-06

Training epoch-32 batch-42
Running loss of epoch-32 batch-42 = 1.7874292097985744e-05

Training epoch-32 batch-43
Running loss of epoch-32 batch-43 = 2.9217218980193138e-05

Training epoch-32 batch-44
Running loss of epoch-32 batch-44 = 5.805515684187412e-06

Training epoch-32 batch-45
Running loss of epoch-32 batch-45 = 3.2472889870405197e-06

Training epoch-32 batch-46
Running loss of epoch-32 batch-46 = 0.0001417237799614668

Training epoch-32 batch-47
Running loss of epoch-32 batch-47 = 5.757203325629234e-06

Training epoch-32 batch-48
Running loss of epoch-32 batch-48 = 1.3678218238055706e-05

Training epoch-32 batch-49
Running loss of epoch-32 batch-49 = 8.309842087328434e-06

Training epoch-32 batch-50
Running loss of epoch-32 batch-50 = 0.0003705257549881935

Training epoch-32 batch-51
Running loss of epoch-32 batch-51 = 7.658964022994041e-06

Training epoch-32 batch-52
Running loss of epoch-32 batch-52 = 8.921953849494457e-06

Training epoch-32 batch-53
Running loss of epoch-32 batch-53 = 0.0002209105878137052

Training epoch-32 batch-54
Running loss of epoch-32 batch-54 = 3.5944394767284393e-06

Training epoch-32 batch-55
Running loss of epoch-32 batch-55 = 0.00013060466153547168

Training epoch-32 batch-56
Running loss of epoch-32 batch-56 = 4.550907760858536e-06

Training epoch-32 batch-57
Running loss of epoch-32 batch-57 = 1.0964577086269855e-05

Training epoch-32 batch-58
Running loss of epoch-32 batch-58 = 3.666384145617485e-06

Training epoch-32 batch-59
Running loss of epoch-32 batch-59 = 5.822884850203991e-05

Training epoch-32 batch-60
Running loss of epoch-32 batch-60 = 3.354356158524752e-05

Training epoch-32 batch-61
Running loss of epoch-32 batch-61 = 2.327375113964081e-06

Training epoch-32 batch-62
Running loss of epoch-32 batch-62 = 5.224486812949181e-06

Training epoch-32 batch-63
Running loss of epoch-32 batch-63 = 7.740454748272896e-06

Training epoch-32 batch-64
Running loss of epoch-32 batch-64 = 5.735084414482117e-06

Training epoch-32 batch-65
Running loss of epoch-32 batch-65 = 1.2623029761016369e-05

Training epoch-32 batch-66
Running loss of epoch-32 batch-66 = 2.9392307624220848e-05

Training epoch-32 batch-67
Running loss of epoch-32 batch-67 = 3.2279640436172485e-06

Training epoch-32 batch-68
Running loss of epoch-32 batch-68 = 4.1759107261896133e-05

Training epoch-32 batch-69
Running loss of epoch-32 batch-69 = 1.7737038433551788e-06

Training epoch-32 batch-70
Running loss of epoch-32 batch-70 = 6.345100700855255e-06

Training epoch-32 batch-71
Running loss of epoch-32 batch-71 = 3.1811650842428207e-06

Training epoch-32 batch-72
Running loss of epoch-32 batch-72 = 3.787246532738209e-05

Training epoch-32 batch-73
Running loss of epoch-32 batch-73 = 2.3706583306193352e-05

Training epoch-32 batch-74
Running loss of epoch-32 batch-74 = 3.627035766839981e-06

Training epoch-32 batch-75
Running loss of epoch-32 batch-75 = 4.9192458391189575e-06

Training epoch-32 batch-76
Running loss of epoch-32 batch-76 = 1.0423013009130955e-05

Training epoch-32 batch-77
Running loss of epoch-32 batch-77 = 3.250082954764366e-06

Training epoch-32 batch-78
Running loss of epoch-32 batch-78 = 1.5298021025955677e-05

Training epoch-32 batch-79
Running loss of epoch-32 batch-79 = 1.1230935342609882e-05

Training epoch-32 batch-80
Running loss of epoch-32 batch-80 = 3.961147740483284e-06

Training epoch-32 batch-81
Running loss of epoch-32 batch-81 = 4.083267413079739e-06

Training epoch-32 batch-82
Running loss of epoch-32 batch-82 = 6.105378270149231e-05

Training epoch-32 batch-83
Running loss of epoch-32 batch-83 = 2.045941073447466e-05

Training epoch-32 batch-84
Running loss of epoch-32 batch-84 = 3.20082763209939e-05

Training epoch-32 batch-85
Running loss of epoch-32 batch-85 = 1.428311225026846e-05

Training epoch-32 batch-86
Running loss of epoch-32 batch-86 = 7.57013913244009e-06

Training epoch-32 batch-87
Running loss of epoch-32 batch-87 = 0.00011101365089416504

Training epoch-32 batch-88
Running loss of epoch-32 batch-88 = 1.5085097402334213e-06

Training epoch-32 batch-89
Running loss of epoch-32 batch-89 = 0.0003025099285878241

Training epoch-32 batch-90
Running loss of epoch-32 batch-90 = 6.506801582872868e-06

Training epoch-32 batch-91
Running loss of epoch-32 batch-91 = 1.3695680536329746e-05

Training epoch-32 batch-92
Running loss of epoch-32 batch-92 = 3.44135332852602e-06

Training epoch-32 batch-93
Running loss of epoch-32 batch-93 = 0.00029354728758335114

Training epoch-32 batch-94
Running loss of epoch-32 batch-94 = 1.5792204067111015e-05

Training epoch-32 batch-95
Running loss of epoch-32 batch-95 = 2.55717895925045e-06

Training epoch-32 batch-96
Running loss of epoch-32 batch-96 = 4.226574674248695e-06

Training epoch-32 batch-97
Running loss of epoch-32 batch-97 = 0.00011054490460082889

Training epoch-32 batch-98
Running loss of epoch-32 batch-98 = 5.095032975077629e-06

Training epoch-32 batch-99
Running loss of epoch-32 batch-99 = 8.250819519162178e-06

Training epoch-32 batch-100
Running loss of epoch-32 batch-100 = 8.222705218940973e-05

Training epoch-32 batch-101
Running loss of epoch-32 batch-101 = 6.417511031031609e-06

Training epoch-32 batch-102
Running loss of epoch-32 batch-102 = 1.5390105545520782e-06

Training epoch-32 batch-103
Running loss of epoch-32 batch-103 = 1.2333039194345474e-06

Training epoch-32 batch-104
Running loss of epoch-32 batch-104 = 8.025672286748886e-06

Training epoch-32 batch-105
Running loss of epoch-32 batch-105 = 2.1524843759834766e-05

Training epoch-32 batch-106
Running loss of epoch-32 batch-106 = 1.70412240549922e-05

Training epoch-32 batch-107
Running loss of epoch-32 batch-107 = 8.573988452553749e-06

Training epoch-32 batch-108
Running loss of epoch-32 batch-108 = 1.7245300114154816e-05

Training epoch-32 batch-109
Running loss of epoch-32 batch-109 = 4.590139724314213e-05

Training epoch-32 batch-110
Running loss of epoch-32 batch-110 = 1.1586816981434822e-05

Training epoch-32 batch-111
Running loss of epoch-32 batch-111 = 7.544178515672684e-06

Training epoch-32 batch-112
Running loss of epoch-32 batch-112 = 3.0319206416606903e-06

Training epoch-32 batch-113
Running loss of epoch-32 batch-113 = 6.277812644839287e-05

Training epoch-32 batch-114
Running loss of epoch-32 batch-114 = 7.06652645021677e-06

Training epoch-32 batch-115
Running loss of epoch-32 batch-115 = 5.3031835705041885e-06

Training epoch-32 batch-116
Running loss of epoch-32 batch-116 = 5.42256748303771e-05

Training epoch-32 batch-117
Running loss of epoch-32 batch-117 = 0.0002532529179006815

Training epoch-32 batch-118
Running loss of epoch-32 batch-118 = 3.499910235404968e-05

Training epoch-32 batch-119
Running loss of epoch-32 batch-119 = 3.5280012525618076e-05

Training epoch-32 batch-120
Running loss of epoch-32 batch-120 = 2.3416359908878803e-05

Training epoch-32 batch-121
Running loss of epoch-32 batch-121 = 9.064306505024433e-05

Training epoch-32 batch-122
Running loss of epoch-32 batch-122 = 2.596992999315262e-06

Training epoch-32 batch-123
Running loss of epoch-32 batch-123 = 2.7650967240333557e-06

Training epoch-32 batch-124
Running loss of epoch-32 batch-124 = 7.866881787776947e-06

Training epoch-32 batch-125
Running loss of epoch-32 batch-125 = 1.5896162949502468e-05

Training epoch-32 batch-126
Running loss of epoch-32 batch-126 = 6.471411325037479e-06

Training epoch-32 batch-127
Running loss of epoch-32 batch-127 = 1.5890691429376602e-06

Training epoch-32 batch-128
Running loss of epoch-32 batch-128 = 4.929595161229372e-05

Training epoch-32 batch-129
Running loss of epoch-32 batch-129 = 7.125927368178964e-05

Training epoch-32 batch-130
Running loss of epoch-32 batch-130 = 4.997244104743004e-06

Training epoch-32 batch-131
Running loss of epoch-32 batch-131 = 4.4786371290683746e-05

Training epoch-32 batch-132
Running loss of epoch-32 batch-132 = 1.8721912056207657e-06

Training epoch-32 batch-133
Running loss of epoch-32 batch-133 = 7.239985279738903e-06

Training epoch-32 batch-134
Running loss of epoch-32 batch-134 = 2.1525193005800247e-06

Training epoch-32 batch-135
Running loss of epoch-32 batch-135 = 0.0001568170264363289

Training epoch-32 batch-136
Running loss of epoch-32 batch-136 = 2.443324774503708e-06

Training epoch-32 batch-137
Running loss of epoch-32 batch-137 = 3.297533839941025e-05

Training epoch-32 batch-138
Running loss of epoch-32 batch-138 = 4.3692649342119694e-05

Training epoch-32 batch-139
Running loss of epoch-32 batch-139 = 1.7301645129919052e-06

Training epoch-32 batch-140
Running loss of epoch-32 batch-140 = 3.5187695175409317e-06

Training epoch-32 batch-141
Running loss of epoch-32 batch-141 = 1.1479714885354042e-05

Training epoch-32 batch-142
Running loss of epoch-32 batch-142 = 1.6115722246468067e-05

Training epoch-32 batch-143
Running loss of epoch-32 batch-143 = 2.171844244003296e-06

Training epoch-32 batch-144
Running loss of epoch-32 batch-144 = 8.718110620975494e-06

Training epoch-32 batch-145
Running loss of epoch-32 batch-145 = 2.7478672564029694e-06

Training epoch-32 batch-146
Running loss of epoch-32 batch-146 = 6.22431980445981e-05

Training epoch-32 batch-147
Running loss of epoch-32 batch-147 = 1.3030716218054295e-05

Training epoch-32 batch-148
Running loss of epoch-32 batch-148 = 6.632308941334486e-05

Training epoch-32 batch-149
Running loss of epoch-32 batch-149 = 1.3005919754505157e-06

Training epoch-32 batch-150
Running loss of epoch-32 batch-150 = 1.4248653315007687e-05

Training epoch-32 batch-151
Running loss of epoch-32 batch-151 = 2.1048937924206257e-05

Training epoch-32 batch-152
Running loss of epoch-32 batch-152 = 2.2863969206809998e-06

Training epoch-32 batch-153
Running loss of epoch-32 batch-153 = 0.00019888527458533645

Training epoch-32 batch-154
Running loss of epoch-32 batch-154 = 2.5904737412929535e-06

Training epoch-32 batch-155
Running loss of epoch-32 batch-155 = 2.952397335320711e-05

Training epoch-32 batch-156
Running loss of epoch-32 batch-156 = 5.9657031670212746e-06

Training epoch-32 batch-157
Running loss of epoch-32 batch-157 = 3.639236092567444e-05

Finished training epoch-32.



Average train loss at epoch-32 = 3.249827772378922e-05

Started Evaluation

Average val loss at epoch-32 = 4.713230272656993

Accuracy for classes:
Accuracy for class equals is: 68.98 %
Accuracy for class main is: 50.82 %
Accuracy for class setUp is: 53.28 %
Accuracy for class onCreate is: 45.52 %
Accuracy for class toString is: 42.66 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.85 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 28.21 %

Overall Accuracy = 46.11 %

Finished Evaluation



Started training epoch-33


Training epoch-33 batch-1
Running loss of epoch-33 batch-1 = 1.9866391085088253e-05

Training epoch-33 batch-2
Running loss of epoch-33 batch-2 = 1.792993862181902e-05

Training epoch-33 batch-3
Running loss of epoch-33 batch-3 = 3.902940079569817e-06

Training epoch-33 batch-4
Running loss of epoch-33 batch-4 = 2.5614281184971333e-05

Training epoch-33 batch-5
Running loss of epoch-33 batch-5 = 0.00012322055408731103

Training epoch-33 batch-6
Running loss of epoch-33 batch-6 = 4.760664887726307e-05

Training epoch-33 batch-7
Running loss of epoch-33 batch-7 = 1.0319403372704983e-05

Training epoch-33 batch-8
Running loss of epoch-33 batch-8 = 1.4848308637738228e-05

Training epoch-33 batch-9
Running loss of epoch-33 batch-9 = 8.851633174344897e-05

Training epoch-33 batch-10
Running loss of epoch-33 batch-10 = 1.2758304364979267e-05

Training epoch-33 batch-11
Running loss of epoch-33 batch-11 = 4.112487658858299e-06

Training epoch-33 batch-12
Running loss of epoch-33 batch-12 = 4.494795575737953e-06

Training epoch-33 batch-13
Running loss of epoch-33 batch-13 = 1.292908564209938e-06

Training epoch-33 batch-14
Running loss of epoch-33 batch-14 = 5.0764065235853195e-06

Training epoch-33 batch-15
Running loss of epoch-33 batch-15 = 7.709837518632412e-06

Training epoch-33 batch-16
Running loss of epoch-33 batch-16 = 3.0743307434022427e-05

Training epoch-33 batch-17
Running loss of epoch-33 batch-17 = 1.0132323950529099e-05

Training epoch-33 batch-18
Running loss of epoch-33 batch-18 = 7.81053677201271e-06

Training epoch-33 batch-19
Running loss of epoch-33 batch-19 = 4.6405475586652756e-06

Training epoch-33 batch-20
Running loss of epoch-33 batch-20 = 8.52532684803009e-06

Training epoch-33 batch-21
Running loss of epoch-33 batch-21 = 7.719965651631355e-06

Training epoch-33 batch-22
Running loss of epoch-33 batch-22 = 3.451469819992781e-05

Training epoch-33 batch-23
Running loss of epoch-33 batch-23 = 2.0271982066333294e-05

Training epoch-33 batch-24
Running loss of epoch-33 batch-24 = 3.8046855479478836e-06

Training epoch-33 batch-25
Running loss of epoch-33 batch-25 = 0.00013677054084837437

Training epoch-33 batch-26
Running loss of epoch-33 batch-26 = 3.477209247648716e-06

Training epoch-33 batch-27
Running loss of epoch-33 batch-27 = 0.00014986860333010554

Training epoch-33 batch-28
Running loss of epoch-33 batch-28 = 8.471595356240869e-05

Training epoch-33 batch-29
Running loss of epoch-33 batch-29 = 2.1262094378471375e-06

Training epoch-33 batch-30
Running loss of epoch-33 batch-30 = 2.4586450308561325e-05

Training epoch-33 batch-31
Running loss of epoch-33 batch-31 = 3.31259798258543e-06

Training epoch-33 batch-32
Running loss of epoch-33 batch-32 = 7.293745875358582e-05

Training epoch-33 batch-33
Running loss of epoch-33 batch-33 = 4.085537511855364e-05

Training epoch-33 batch-34
Running loss of epoch-33 batch-34 = 1.0493909940123558e-05

Training epoch-33 batch-35
Running loss of epoch-33 batch-35 = 1.8084305338561535e-05

Training epoch-33 batch-36
Running loss of epoch-33 batch-36 = 7.673504296690226e-05

Training epoch-33 batch-37
Running loss of epoch-33 batch-37 = 4.914239980280399e-06

Training epoch-33 batch-38
Running loss of epoch-33 batch-38 = 0.00029351687408052385

Training epoch-33 batch-39
Running loss of epoch-33 batch-39 = 3.3403048291802406e-05

Training epoch-33 batch-40
Running loss of epoch-33 batch-40 = 1.4295801520347595e-06

Training epoch-33 batch-41
Running loss of epoch-33 batch-41 = 5.50481490790844e-06

Training epoch-33 batch-42
Running loss of epoch-33 batch-42 = 4.866509698331356e-06

Training epoch-33 batch-43
Running loss of epoch-33 batch-43 = 3.126612864434719e-05

Training epoch-33 batch-44
Running loss of epoch-33 batch-44 = 1.1746888048946857e-05

Training epoch-33 batch-45
Running loss of epoch-33 batch-45 = 3.066612407565117e-06

Training epoch-33 batch-46
Running loss of epoch-33 batch-46 = 5.62635250389576e-06

Training epoch-33 batch-47
Running loss of epoch-33 batch-47 = 2.2057676687836647e-05

Training epoch-33 batch-48
Running loss of epoch-33 batch-48 = 5.302950739860535e-06

Training epoch-33 batch-49
Running loss of epoch-33 batch-49 = 1.7576850950717926e-05

Training epoch-33 batch-50
Running loss of epoch-33 batch-50 = 0.00027532828971743584

Training epoch-33 batch-51
Running loss of epoch-33 batch-51 = 6.084446795284748e-06

Training epoch-33 batch-52
Running loss of epoch-33 batch-52 = 1.0306481271982193e-05

Training epoch-33 batch-53
Running loss of epoch-33 batch-53 = 1.10710971057415e-06

Training epoch-33 batch-54
Running loss of epoch-33 batch-54 = 0.00019596045603975654

Training epoch-33 batch-55
Running loss of epoch-33 batch-55 = 3.5613193176686764e-05

Training epoch-33 batch-56
Running loss of epoch-33 batch-56 = 7.867347449064255e-06

Training epoch-33 batch-57
Running loss of epoch-33 batch-57 = 1.5106052160263062e-06

Training epoch-33 batch-58
Running loss of epoch-33 batch-58 = 0.00034910114482045174

Training epoch-33 batch-59
Running loss of epoch-33 batch-59 = 3.834487870335579e-06

Training epoch-33 batch-60
Running loss of epoch-33 batch-60 = 2.4378998205065727e-05

Training epoch-33 batch-61
Running loss of epoch-33 batch-61 = 3.0167866498231888e-05

Training epoch-33 batch-62
Running loss of epoch-33 batch-62 = 3.347429446876049e-05

Training epoch-33 batch-63
Running loss of epoch-33 batch-63 = 4.632049240171909e-06

Training epoch-33 batch-64
Running loss of epoch-33 batch-64 = 2.0161503925919533e-05

Training epoch-33 batch-65
Running loss of epoch-33 batch-65 = 6.679561920464039e-06

Training epoch-33 batch-66
Running loss of epoch-33 batch-66 = 5.30569814145565e-05

Training epoch-33 batch-67
Running loss of epoch-33 batch-67 = 5.617388524115086e-06

Training epoch-33 batch-68
Running loss of epoch-33 batch-68 = 2.122018486261368e-06

Training epoch-33 batch-69
Running loss of epoch-33 batch-69 = 3.0415481887757778e-05

Training epoch-33 batch-70
Running loss of epoch-33 batch-70 = 1.8343445844948292e-05

Training epoch-33 batch-71
Running loss of epoch-33 batch-71 = 8.514383807778358e-06

Training epoch-33 batch-72
Running loss of epoch-33 batch-72 = 2.9900576919317245e-05

Training epoch-33 batch-73
Running loss of epoch-33 batch-73 = 2.7258414775133133e-05

Training epoch-33 batch-74
Running loss of epoch-33 batch-74 = 3.234134055674076e-06

Training epoch-33 batch-75
Running loss of epoch-33 batch-75 = 6.855465471744537e-06

Training epoch-33 batch-76
Running loss of epoch-33 batch-76 = 1.1896365322172642e-05

Training epoch-33 batch-77
Running loss of epoch-33 batch-77 = 1.834845170378685e-05

Training epoch-33 batch-78
Running loss of epoch-33 batch-78 = 3.933010157197714e-05

Training epoch-33 batch-79
Running loss of epoch-33 batch-79 = 3.83483711630106e-06

Training epoch-33 batch-80
Running loss of epoch-33 batch-80 = 2.6961788535118103e-06

Training epoch-33 batch-81
Running loss of epoch-33 batch-81 = 4.482653457671404e-05

Training epoch-33 batch-82
Running loss of epoch-33 batch-82 = 6.747548468410969e-05

Training epoch-33 batch-83
Running loss of epoch-33 batch-83 = 6.12880103290081e-06

Training epoch-33 batch-84
Running loss of epoch-33 batch-84 = 1.0135932825505733e-05

Training epoch-33 batch-85
Running loss of epoch-33 batch-85 = 4.513189196586609e-06

Training epoch-33 batch-86
Running loss of epoch-33 batch-86 = 0.00017089105676859617

Training epoch-33 batch-87
Running loss of epoch-33 batch-87 = 2.0652078092098236e-06

Training epoch-33 batch-88
Running loss of epoch-33 batch-88 = 2.2521009668707848e-05

Training epoch-33 batch-89
Running loss of epoch-33 batch-89 = 3.075145650655031e-05

Training epoch-33 batch-90
Running loss of epoch-33 batch-90 = 3.499723970890045e-05

Training epoch-33 batch-91
Running loss of epoch-33 batch-91 = 5.025067366659641e-06

Training epoch-33 batch-92
Running loss of epoch-33 batch-92 = 1.0011834092438221e-05

Training epoch-33 batch-93
Running loss of epoch-33 batch-93 = 3.6456622183322906e-06

Training epoch-33 batch-94
Running loss of epoch-33 batch-94 = 2.0759296603500843e-05

Training epoch-33 batch-95
Running loss of epoch-33 batch-95 = 4.177214577794075e-06

Training epoch-33 batch-96
Running loss of epoch-33 batch-96 = 5.061132833361626e-05

Training epoch-33 batch-97
Running loss of epoch-33 batch-97 = 0.0001889861887320876

Training epoch-33 batch-98
Running loss of epoch-33 batch-98 = 6.476300768554211e-06

Training epoch-33 batch-99
Running loss of epoch-33 batch-99 = 3.176636528223753e-05

Training epoch-33 batch-100
Running loss of epoch-33 batch-100 = 1.5519093722105026e-05

Training epoch-33 batch-101
Running loss of epoch-33 batch-101 = 5.168491043150425e-06

Training epoch-33 batch-102
Running loss of epoch-33 batch-102 = 1.7584068700671196e-05

Training epoch-33 batch-103
Running loss of epoch-33 batch-103 = 7.429276593029499e-06

Training epoch-33 batch-104
Running loss of epoch-33 batch-104 = 3.177649341523647e-05

Training epoch-33 batch-105
Running loss of epoch-33 batch-105 = 5.9355515986680984e-06

Training epoch-33 batch-106
Running loss of epoch-33 batch-106 = 4.021963104605675e-05

Training epoch-33 batch-107
Running loss of epoch-33 batch-107 = 3.4108408726751804e-05

Training epoch-33 batch-108
Running loss of epoch-33 batch-108 = 1.560663804411888e-05

Training epoch-33 batch-109
Running loss of epoch-33 batch-109 = 0.00014616531552746892

Training epoch-33 batch-110
Running loss of epoch-33 batch-110 = 3.518536686897278e-06

Training epoch-33 batch-111
Running loss of epoch-33 batch-111 = 8.999952115118504e-06

Training epoch-33 batch-112
Running loss of epoch-33 batch-112 = 9.688566206023097e-05

Training epoch-33 batch-113
Running loss of epoch-33 batch-113 = 2.091622445732355e-05

Training epoch-33 batch-114
Running loss of epoch-33 batch-114 = 9.313574992120266e-06

Training epoch-33 batch-115
Running loss of epoch-33 batch-115 = 1.1440599337220192e-05

Training epoch-33 batch-116
Running loss of epoch-33 batch-116 = 2.214300911873579e-05

Training epoch-33 batch-117
Running loss of epoch-33 batch-117 = 2.8409413062036037e-05

Training epoch-33 batch-118
Running loss of epoch-33 batch-118 = 3.302644472569227e-05

Training epoch-33 batch-119
Running loss of epoch-33 batch-119 = 2.3409491404891014e-05

Training epoch-33 batch-120
Running loss of epoch-33 batch-120 = 4.010740667581558e-06

Training epoch-33 batch-121
Running loss of epoch-33 batch-121 = 3.9031729102134705e-06

Training epoch-33 batch-122
Running loss of epoch-33 batch-122 = 4.313187673687935e-06

Training epoch-33 batch-123
Running loss of epoch-33 batch-123 = 5.0598056986927986e-05

Training epoch-33 batch-124
Running loss of epoch-33 batch-124 = 0.00011616689153015614

Training epoch-33 batch-125
Running loss of epoch-33 batch-125 = 1.0458752512931824e-06

Training epoch-33 batch-126
Running loss of epoch-33 batch-126 = 6.890390068292618e-06

Training epoch-33 batch-127
Running loss of epoch-33 batch-127 = 4.19258140027523e-06

Training epoch-33 batch-128
Running loss of epoch-33 batch-128 = 5.579553544521332e-06

Training epoch-33 batch-129
Running loss of epoch-33 batch-129 = 4.4512562453746796e-06

Training epoch-33 batch-130
Running loss of epoch-33 batch-130 = 5.086068995296955e-06

Training epoch-33 batch-131
Running loss of epoch-33 batch-131 = 2.059154212474823e-06

Training epoch-33 batch-132
Running loss of epoch-33 batch-132 = 1.1884490959346294e-05

Training epoch-33 batch-133
Running loss of epoch-33 batch-133 = 9.122304618358612e-07

Training epoch-33 batch-134
Running loss of epoch-33 batch-134 = 5.601602606475353e-05

Training epoch-33 batch-135
Running loss of epoch-33 batch-135 = 1.2494856491684914e-05

Training epoch-33 batch-136
Running loss of epoch-33 batch-136 = 1.2216856703162193e-05

Training epoch-33 batch-137
Running loss of epoch-33 batch-137 = 2.4619512259960175e-06

Training epoch-33 batch-138
Running loss of epoch-33 batch-138 = 2.850079908967018e-06

Training epoch-33 batch-139
Running loss of epoch-33 batch-139 = 5.104613956063986e-05

Training epoch-33 batch-140
Running loss of epoch-33 batch-140 = 2.003973349928856e-06

Training epoch-33 batch-141
Running loss of epoch-33 batch-141 = 2.5706831365823746e-06

Training epoch-33 batch-142
Running loss of epoch-33 batch-142 = 2.9870541766285896e-05

Training epoch-33 batch-143
Running loss of epoch-33 batch-143 = 1.1796015314757824e-05

Training epoch-33 batch-144
Running loss of epoch-33 batch-144 = 1.6859150491654873e-05

Training epoch-33 batch-145
Running loss of epoch-33 batch-145 = 0.00010548083810135722

Training epoch-33 batch-146
Running loss of epoch-33 batch-146 = 6.205867975950241e-06

Training epoch-33 batch-147
Running loss of epoch-33 batch-147 = 2.041459083557129e-06

Training epoch-33 batch-148
Running loss of epoch-33 batch-148 = 1.67149119079113e-06

Training epoch-33 batch-149
Running loss of epoch-33 batch-149 = 5.214475095272064e-06

Training epoch-33 batch-150
Running loss of epoch-33 batch-150 = 9.970273822546005e-06

Training epoch-33 batch-151
Running loss of epoch-33 batch-151 = 1.4917459338903427e-06

Training epoch-33 batch-152
Running loss of epoch-33 batch-152 = 3.698514774441719e-06

Training epoch-33 batch-153
Running loss of epoch-33 batch-153 = 4.97475266456604e-05

Training epoch-33 batch-154
Running loss of epoch-33 batch-154 = 7.424969226121902e-07

Training epoch-33 batch-155
Running loss of epoch-33 batch-155 = 6.550108082592487e-06

Training epoch-33 batch-156
Running loss of epoch-33 batch-156 = 2.214452251791954e-06

Training epoch-33 batch-157
Running loss of epoch-33 batch-157 = 1.8637627363204956e-05

Finished training epoch-33.



Average train loss at epoch-33 = 3.0474347434937953e-05

Started Evaluation

Average val loss at epoch-33 = 4.808752618337932

Accuracy for classes:
Accuracy for class equals is: 68.65 %
Accuracy for class main is: 48.20 %
Accuracy for class setUp is: 55.74 %
Accuracy for class onCreate is: 42.32 %
Accuracy for class toString is: 42.32 %
Accuracy for class run is: 37.90 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.40 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 28.21 %

Overall Accuracy = 45.47 %

Finished Evaluation



Started training epoch-34


Training epoch-34 batch-1
Running loss of epoch-34 batch-1 = 3.455439582467079e-06

Training epoch-34 batch-2
Running loss of epoch-34 batch-2 = 1.8165446817874908e-06

Training epoch-34 batch-3
Running loss of epoch-34 batch-3 = 9.6871517598629e-06

Training epoch-34 batch-4
Running loss of epoch-34 batch-4 = 5.691079422831535e-06

Training epoch-34 batch-5
Running loss of epoch-34 batch-5 = 1.698266714811325e-06

Training epoch-34 batch-6
Running loss of epoch-34 batch-6 = 2.1615298464894295e-05

Training epoch-34 batch-7
Running loss of epoch-34 batch-7 = 6.620422936975956e-06

Training epoch-34 batch-8
Running loss of epoch-34 batch-8 = 1.8176622688770294e-05

Training epoch-34 batch-9
Running loss of epoch-34 batch-9 = 3.1139119528234005e-05

Training epoch-34 batch-10
Running loss of epoch-34 batch-10 = 1.5953555703163147e-06

Training epoch-34 batch-11
Running loss of epoch-34 batch-11 = 9.892391972243786e-06

Training epoch-34 batch-12
Running loss of epoch-34 batch-12 = 8.170027285814285e-07

Training epoch-34 batch-13
Running loss of epoch-34 batch-13 = 9.558862075209618e-06

Training epoch-34 batch-14
Running loss of epoch-34 batch-14 = 9.220873471349478e-05

Training epoch-34 batch-15
Running loss of epoch-34 batch-15 = 1.0025338269770145e-05

Training epoch-34 batch-16
Running loss of epoch-34 batch-16 = 0.00010142032988369465

Training epoch-34 batch-17
Running loss of epoch-34 batch-17 = 2.0344858057796955e-05

Training epoch-34 batch-18
Running loss of epoch-34 batch-18 = 1.3952143490314484e-05

Training epoch-34 batch-19
Running loss of epoch-34 batch-19 = 3.53190116584301e-05

Training epoch-34 batch-20
Running loss of epoch-34 batch-20 = 1.3934914022684097e-06

Training epoch-34 batch-21
Running loss of epoch-34 batch-21 = 1.2633157894015312e-05

Training epoch-34 batch-22
Running loss of epoch-34 batch-22 = 1.8137507140636444e-06

Training epoch-34 batch-23
Running loss of epoch-34 batch-23 = 5.022389814257622e-06

Training epoch-34 batch-24
Running loss of epoch-34 batch-24 = 3.7512858398258686e-05

Training epoch-34 batch-25
Running loss of epoch-34 batch-25 = 9.185750968754292e-06

Training epoch-34 batch-26
Running loss of epoch-34 batch-26 = 2.761371433734894e-06

Training epoch-34 batch-27
Running loss of epoch-34 batch-27 = 2.0938226953148842e-05

Training epoch-34 batch-28
Running loss of epoch-34 batch-28 = 2.2374908439815044e-05

Training epoch-34 batch-29
Running loss of epoch-34 batch-29 = 2.691289409995079e-06

Training epoch-34 batch-30
Running loss of epoch-34 batch-30 = 0.00030593585688620806

Training epoch-34 batch-31
Running loss of epoch-34 batch-31 = 2.500135451555252e-06

Training epoch-34 batch-32
Running loss of epoch-34 batch-32 = 2.8510112315416336e-06

Training epoch-34 batch-33
Running loss of epoch-34 batch-33 = 0.00018030445789918303

Training epoch-34 batch-34
Running loss of epoch-34 batch-34 = 0.00011735328007489443

Training epoch-34 batch-35
Running loss of epoch-34 batch-35 = 5.892815534025431e-05

Training epoch-34 batch-36
Running loss of epoch-34 batch-36 = 5.417969077825546e-06

Training epoch-34 batch-37
Running loss of epoch-34 batch-37 = 5.707843229174614e-06

Training epoch-34 batch-38
Running loss of epoch-34 batch-38 = 1.1708936654031277e-05

Training epoch-34 batch-39
Running loss of epoch-34 batch-39 = 4.526821430772543e-05

Training epoch-34 batch-40
Running loss of epoch-34 batch-40 = 4.202814307063818e-05

Training epoch-34 batch-41
Running loss of epoch-34 batch-41 = 3.5355333238840103e-06

Training epoch-34 batch-42
Running loss of epoch-34 batch-42 = 1.4339922927320004e-05

Training epoch-34 batch-43
Running loss of epoch-34 batch-43 = 2.2214138880372047e-05

Training epoch-34 batch-44
Running loss of epoch-34 batch-44 = 3.8924627006053925e-06

Training epoch-34 batch-45
Running loss of epoch-34 batch-45 = 1.0316027328372002e-05

Training epoch-34 batch-46
Running loss of epoch-34 batch-46 = 4.5461696572601795e-05

Training epoch-34 batch-47
Running loss of epoch-34 batch-47 = 3.7298654206097126e-05

Training epoch-34 batch-48
Running loss of epoch-34 batch-48 = 7.233116775751114e-06

Training epoch-34 batch-49
Running loss of epoch-34 batch-49 = 2.057291567325592e-06

Training epoch-34 batch-50
Running loss of epoch-34 batch-50 = 1.537846401333809e-06

Training epoch-34 batch-51
Running loss of epoch-34 batch-51 = 1.9164057448506355e-05

Training epoch-34 batch-52
Running loss of epoch-34 batch-52 = 2.5345943868160248e-06

Training epoch-34 batch-53
Running loss of epoch-34 batch-53 = 3.2014213502407074e-05

Training epoch-34 batch-54
Running loss of epoch-34 batch-54 = 1.4977995306253433e-06

Training epoch-34 batch-55
Running loss of epoch-34 batch-55 = 3.7457793951034546e-06

Training epoch-34 batch-56
Running loss of epoch-34 batch-56 = 2.7618370950222015e-06

Training epoch-34 batch-57
Running loss of epoch-34 batch-57 = 7.771537639200687e-06

Training epoch-34 batch-58
Running loss of epoch-34 batch-58 = 3.447383642196655e-05

Training epoch-34 batch-59
Running loss of epoch-34 batch-59 = 1.6614794731140137e-05

Training epoch-34 batch-60
Running loss of epoch-34 batch-60 = 7.703137816861272e-05

Training epoch-34 batch-61
Running loss of epoch-34 batch-61 = 2.9106158763170242e-06

Training epoch-34 batch-62
Running loss of epoch-34 batch-62 = 3.494787961244583e-06

Training epoch-34 batch-63
Running loss of epoch-34 batch-63 = 2.920161932706833e-06

Training epoch-34 batch-64
Running loss of epoch-34 batch-64 = 5.9138983488082886e-06

Training epoch-34 batch-65
Running loss of epoch-34 batch-65 = 0.00012285116827115417

Training epoch-34 batch-66
Running loss of epoch-34 batch-66 = 3.5945558920502663e-06

Training epoch-34 batch-67
Running loss of epoch-34 batch-67 = 5.012378096580505e-06

Training epoch-34 batch-68
Running loss of epoch-34 batch-68 = 1.1489377357065678e-05

Training epoch-34 batch-69
Running loss of epoch-34 batch-69 = 2.8691720217466354e-06

Training epoch-34 batch-70
Running loss of epoch-34 batch-70 = 2.463115379214287e-06

Training epoch-34 batch-71
Running loss of epoch-34 batch-71 = 7.643649587407708e-05

Training epoch-34 batch-72
Running loss of epoch-34 batch-72 = 0.00016389950178563595

Training epoch-34 batch-73
Running loss of epoch-34 batch-73 = 2.486736048012972e-05

Training epoch-34 batch-74
Running loss of epoch-34 batch-74 = 1.0443618521094322e-05

Training epoch-34 batch-75
Running loss of epoch-34 batch-75 = 3.973720595240593e-06

Training epoch-34 batch-76
Running loss of epoch-34 batch-76 = 1.4971825294196606e-05

Training epoch-34 batch-77
Running loss of epoch-34 batch-77 = 6.203074008226395e-06

Training epoch-34 batch-78
Running loss of epoch-34 batch-78 = 0.00010907824616879225

Training epoch-34 batch-79
Running loss of epoch-34 batch-79 = 4.30236104875803e-06

Training epoch-34 batch-80
Running loss of epoch-34 batch-80 = 0.00020715699065476656

Training epoch-34 batch-81
Running loss of epoch-34 batch-81 = 7.350777741521597e-05

Training epoch-34 batch-82
Running loss of epoch-34 batch-82 = 4.042149521410465e-05

Training epoch-34 batch-83
Running loss of epoch-34 batch-83 = 5.059875547885895e-06

Training epoch-34 batch-84
Running loss of epoch-34 batch-84 = 1.3353303074836731e-05

Training epoch-34 batch-85
Running loss of epoch-34 batch-85 = 1.2172502465546131e-05

Training epoch-34 batch-86
Running loss of epoch-34 batch-86 = 4.9551017582416534e-06

Training epoch-34 batch-87
Running loss of epoch-34 batch-87 = 8.887844160199165e-06

Training epoch-34 batch-88
Running loss of epoch-34 batch-88 = 1.8409918993711472e-06

Training epoch-34 batch-89
Running loss of epoch-34 batch-89 = 0.00010922132059931755

Training epoch-34 batch-90
Running loss of epoch-34 batch-90 = 5.556677933782339e-05

Training epoch-34 batch-91
Running loss of epoch-34 batch-91 = 5.780952051281929e-06

Training epoch-34 batch-92
Running loss of epoch-34 batch-92 = 1.3294629752635956e-06

Training epoch-34 batch-93
Running loss of epoch-34 batch-93 = 5.219015292823315e-06

Training epoch-34 batch-94
Running loss of epoch-34 batch-94 = 2.6205088943243027e-06

Training epoch-34 batch-95
Running loss of epoch-34 batch-95 = 1.2755277566611767e-05

Training epoch-34 batch-96
Running loss of epoch-34 batch-96 = 5.674653220921755e-05

Training epoch-34 batch-97
Running loss of epoch-34 batch-97 = 1.6701756976544857e-05

Training epoch-34 batch-98
Running loss of epoch-34 batch-98 = 3.578641917556524e-05

Training epoch-34 batch-99
Running loss of epoch-34 batch-99 = 1.0869000107049942e-05

Training epoch-34 batch-100
Running loss of epoch-34 batch-100 = 8.087372407317162e-06

Training epoch-34 batch-101
Running loss of epoch-34 batch-101 = 0.0002373713650740683

Training epoch-34 batch-102
Running loss of epoch-34 batch-102 = 7.86071177572012e-06

Training epoch-34 batch-103
Running loss of epoch-34 batch-103 = 4.555564373731613e-06

Training epoch-34 batch-104
Running loss of epoch-34 batch-104 = 2.545258030295372e-05

Training epoch-34 batch-105
Running loss of epoch-34 batch-105 = 0.00010833301348611712

Training epoch-34 batch-106
Running loss of epoch-34 batch-106 = 4.785601049661636e-06

Training epoch-34 batch-107
Running loss of epoch-34 batch-107 = 8.410518057644367e-05

Training epoch-34 batch-108
Running loss of epoch-34 batch-108 = 2.9343413189053535e-05

Training epoch-34 batch-109
Running loss of epoch-34 batch-109 = 8.299830369651318e-06

Training epoch-34 batch-110
Running loss of epoch-34 batch-110 = 5.4903794080019e-06

Training epoch-34 batch-111
Running loss of epoch-34 batch-111 = 1.524284016340971e-05

Training epoch-34 batch-112
Running loss of epoch-34 batch-112 = 4.7318171709775925e-06

Training epoch-34 batch-113
Running loss of epoch-34 batch-113 = 2.1136831492185593e-05

Training epoch-34 batch-114
Running loss of epoch-34 batch-114 = 4.458602052181959e-05

Training epoch-34 batch-115
Running loss of epoch-34 batch-115 = 2.1295156329870224e-05

Training epoch-34 batch-116
Running loss of epoch-34 batch-116 = 9.519048035144806e-06

Training epoch-34 batch-117
Running loss of epoch-34 batch-117 = 8.520204573869705e-06

Training epoch-34 batch-118
Running loss of epoch-34 batch-118 = 5.236128345131874e-06

Training epoch-34 batch-119
Running loss of epoch-34 batch-119 = 3.023771569132805e-06

Training epoch-34 batch-120
Running loss of epoch-34 batch-120 = 4.742643795907497e-06

Training epoch-34 batch-121
Running loss of epoch-34 batch-121 = 1.400010660290718e-06

Training epoch-34 batch-122
Running loss of epoch-34 batch-122 = 2.3262109607458115e-06

Training epoch-34 batch-123
Running loss of epoch-34 batch-123 = 3.400025889277458e-06

Training epoch-34 batch-124
Running loss of epoch-34 batch-124 = 7.022637873888016e-06

Training epoch-34 batch-125
Running loss of epoch-34 batch-125 = 6.759772077202797e-06

Training epoch-34 batch-126
Running loss of epoch-34 batch-126 = 1.1451775208115578e-05

Training epoch-34 batch-127
Running loss of epoch-34 batch-127 = 6.426224717870355e-05

Training epoch-34 batch-128
Running loss of epoch-34 batch-128 = 2.4694600142538548e-05

Training epoch-34 batch-129
Running loss of epoch-34 batch-129 = 2.963119186460972e-05

Training epoch-34 batch-130
Running loss of epoch-34 batch-130 = 2.348294947296381e-05

Training epoch-34 batch-131
Running loss of epoch-34 batch-131 = 9.280920494347811e-05

Training epoch-34 batch-132
Running loss of epoch-34 batch-132 = 6.424402818083763e-05

Training epoch-34 batch-133
Running loss of epoch-34 batch-133 = 4.458706825971603e-06

Training epoch-34 batch-134
Running loss of epoch-34 batch-134 = 7.817521691322327e-06

Training epoch-34 batch-135
Running loss of epoch-34 batch-135 = 7.413327693939209e-06

Training epoch-34 batch-136
Running loss of epoch-34 batch-136 = 2.2082822397351265e-06

Training epoch-34 batch-137
Running loss of epoch-34 batch-137 = 9.313225746154785e-07

Training epoch-34 batch-138
Running loss of epoch-34 batch-138 = 2.455280628055334e-05

Training epoch-34 batch-139
Running loss of epoch-34 batch-139 = 8.813408203423023e-05

Training epoch-34 batch-140
Running loss of epoch-34 batch-140 = 5.888752639293671e-06

Training epoch-34 batch-141
Running loss of epoch-34 batch-141 = 6.684218533337116e-06

Training epoch-34 batch-142
Running loss of epoch-34 batch-142 = 7.78131652623415e-06

Training epoch-34 batch-143
Running loss of epoch-34 batch-143 = 2.1508894860744476e-06

Training epoch-34 batch-144
Running loss of epoch-34 batch-144 = 1.683877781033516e-05

Training epoch-34 batch-145
Running loss of epoch-34 batch-145 = 9.209034033119678e-06

Training epoch-34 batch-146
Running loss of epoch-34 batch-146 = 8.321949280798435e-06

Training epoch-34 batch-147
Running loss of epoch-34 batch-147 = 6.010755896568298e-06

Training epoch-34 batch-148
Running loss of epoch-34 batch-148 = 1.011404674500227e-05

Training epoch-34 batch-149
Running loss of epoch-34 batch-149 = 2.4416600354015827e-05

Training epoch-34 batch-150
Running loss of epoch-34 batch-150 = 3.309221938252449e-06

Training epoch-34 batch-151
Running loss of epoch-34 batch-151 = 2.1972227841615677e-06

Training epoch-34 batch-152
Running loss of epoch-34 batch-152 = 1.964892726391554e-05

Training epoch-34 batch-153
Running loss of epoch-34 batch-153 = 5.36616425961256e-06

Training epoch-34 batch-154
Running loss of epoch-34 batch-154 = 6.6087814047932625e-06

Training epoch-34 batch-155
Running loss of epoch-34 batch-155 = 1.6652513295412064e-05

Training epoch-34 batch-156
Running loss of epoch-34 batch-156 = 6.773974746465683e-06

Training epoch-34 batch-157
Running loss of epoch-34 batch-157 = 1.4025717973709106e-05

Finished training epoch-34.



Average train loss at epoch-34 = 2.676934264600277e-05

Started Evaluation

Average val loss at epoch-34 = 4.827144431440454

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 49.18 %
Accuracy for class setUp is: 54.26 %
Accuracy for class onCreate is: 42.86 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 36.07 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.75 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 29.23 %

Overall Accuracy = 45.49 %

Finished Evaluation



Started training epoch-35


Training epoch-35 batch-1
Running loss of epoch-35 batch-1 = 3.915047273039818e-06

Training epoch-35 batch-2
Running loss of epoch-35 batch-2 = 2.3602275177836418e-05

Training epoch-35 batch-3
Running loss of epoch-35 batch-3 = 1.4973338693380356e-06

Training epoch-35 batch-4
Running loss of epoch-35 batch-4 = 3.902707248926163e-06

Training epoch-35 batch-5
Running loss of epoch-35 batch-5 = 0.00015511625679209828

Training epoch-35 batch-6
Running loss of epoch-35 batch-6 = 2.784421667456627e-06

Training epoch-35 batch-7
Running loss of epoch-35 batch-7 = 3.4098164178431034e-05

Training epoch-35 batch-8
Running loss of epoch-35 batch-8 = 5.444628186523914e-06

Training epoch-35 batch-9
Running loss of epoch-35 batch-9 = 1.622119452804327e-05

Training epoch-35 batch-10
Running loss of epoch-35 batch-10 = 1.4695455320179462e-05

Training epoch-35 batch-11
Running loss of epoch-35 batch-11 = 7.688067853450775e-07

Training epoch-35 batch-12
Running loss of epoch-35 batch-12 = 1.6997219063341618e-05

Training epoch-35 batch-13
Running loss of epoch-35 batch-13 = 8.12578946352005e-07

Training epoch-35 batch-14
Running loss of epoch-35 batch-14 = 4.519941285252571e-06

Training epoch-35 batch-15
Running loss of epoch-35 batch-15 = 3.337860107421875e-06

Training epoch-35 batch-16
Running loss of epoch-35 batch-16 = 6.229698192328215e-05

Training epoch-35 batch-17
Running loss of epoch-35 batch-17 = 4.227738827466965e-06

Training epoch-35 batch-18
Running loss of epoch-35 batch-18 = 5.867332220077515e-06

Training epoch-35 batch-19
Running loss of epoch-35 batch-19 = 1.2998352758586407e-05

Training epoch-35 batch-20
Running loss of epoch-35 batch-20 = 4.591536708176136e-06

Training epoch-35 batch-21
Running loss of epoch-35 batch-21 = 0.00015911762602627277

Training epoch-35 batch-22
Running loss of epoch-35 batch-22 = 2.2240914404392242e-05

Training epoch-35 batch-23
Running loss of epoch-35 batch-23 = 2.8013018891215324e-06

Training epoch-35 batch-24
Running loss of epoch-35 batch-24 = 4.202243871986866e-05

Training epoch-35 batch-25
Running loss of epoch-35 batch-25 = 3.3122487366199493e-06

Training epoch-35 batch-26
Running loss of epoch-35 batch-26 = 3.0177179723978043e-05

Training epoch-35 batch-27
Running loss of epoch-35 batch-27 = 2.4184933863580227e-05

Training epoch-35 batch-28
Running loss of epoch-35 batch-28 = 5.4424162954092026e-06

Training epoch-35 batch-29
Running loss of epoch-35 batch-29 = 1.0458403266966343e-05

Training epoch-35 batch-30
Running loss of epoch-35 batch-30 = 1.917406916618347e-05

Training epoch-35 batch-31
Running loss of epoch-35 batch-31 = 3.7074205465614796e-05

Training epoch-35 batch-32
Running loss of epoch-35 batch-32 = 2.5131739675998688e-06

Training epoch-35 batch-33
Running loss of epoch-35 batch-33 = 4.3853651732206345e-06

Training epoch-35 batch-34
Running loss of epoch-35 batch-34 = 4.4071581214666367e-05

Training epoch-35 batch-35
Running loss of epoch-35 batch-35 = 8.457805961370468e-06

Training epoch-35 batch-36
Running loss of epoch-35 batch-36 = 1.0631047189235687e-06

Training epoch-35 batch-37
Running loss of epoch-35 batch-37 = 5.487352609634399e-06

Training epoch-35 batch-38
Running loss of epoch-35 batch-38 = 1.3008248060941696e-06

Training epoch-35 batch-39
Running loss of epoch-35 batch-39 = 3.914465196430683e-06

Training epoch-35 batch-40
Running loss of epoch-35 batch-40 = 6.507383659482002e-06

Training epoch-35 batch-41
Running loss of epoch-35 batch-41 = 3.0086375772953033e-06

Training epoch-35 batch-42
Running loss of epoch-35 batch-42 = 3.7280027754604816e-05

Training epoch-35 batch-43
Running loss of epoch-35 batch-43 = 3.206077963113785e-06

Training epoch-35 batch-44
Running loss of epoch-35 batch-44 = 2.810964360833168e-06

Training epoch-35 batch-45
Running loss of epoch-35 batch-45 = 1.0156072676181793e-06

Training epoch-35 batch-46
Running loss of epoch-35 batch-46 = 3.291759639978409e-06

Training epoch-35 batch-47
Running loss of epoch-35 batch-47 = 1.5185214579105377e-06

Training epoch-35 batch-48
Running loss of epoch-35 batch-48 = 5.362322553992271e-06

Training epoch-35 batch-49
Running loss of epoch-35 batch-49 = 7.677706889808178e-06

Training epoch-35 batch-50
Running loss of epoch-35 batch-50 = 8.12539947219193e-05

Training epoch-35 batch-51
Running loss of epoch-35 batch-51 = 1.0351184755563736e-05

Training epoch-35 batch-52
Running loss of epoch-35 batch-52 = 1.811643596738577e-05

Training epoch-35 batch-53
Running loss of epoch-35 batch-53 = 2.871733158826828e-06

Training epoch-35 batch-54
Running loss of epoch-35 batch-54 = 2.2319145500659943e-06

Training epoch-35 batch-55
Running loss of epoch-35 batch-55 = 0.00028574082534760237

Training epoch-35 batch-56
Running loss of epoch-35 batch-56 = 7.75698572397232e-06

Training epoch-35 batch-57
Running loss of epoch-35 batch-57 = 9.778887033462524e-07

Training epoch-35 batch-58
Running loss of epoch-35 batch-58 = 1.5020137652754784e-05

Training epoch-35 batch-59
Running loss of epoch-35 batch-59 = 3.70759516954422e-06

Training epoch-35 batch-60
Running loss of epoch-35 batch-60 = 2.190936356782913e-06

Training epoch-35 batch-61
Running loss of epoch-35 batch-61 = 5.89294359087944e-07

Training epoch-35 batch-62
Running loss of epoch-35 batch-62 = 4.016596358269453e-05

Training epoch-35 batch-63
Running loss of epoch-35 batch-63 = 3.4507829695940018e-06

Training epoch-35 batch-64
Running loss of epoch-35 batch-64 = 0.0002799272770062089

Training epoch-35 batch-65
Running loss of epoch-35 batch-65 = 6.776885129511356e-06

Training epoch-35 batch-66
Running loss of epoch-35 batch-66 = 9.417789988219738e-05

Training epoch-35 batch-67
Running loss of epoch-35 batch-67 = 5.305721424520016e-05

Training epoch-35 batch-68
Running loss of epoch-35 batch-68 = 3.3010728657245636e-06

Training epoch-35 batch-69
Running loss of epoch-35 batch-69 = 1.5522819012403488e-06

Training epoch-35 batch-70
Running loss of epoch-35 batch-70 = 3.3410033211112022e-06

Training epoch-35 batch-71
Running loss of epoch-35 batch-71 = 6.025191396474838e-06

Training epoch-35 batch-72
Running loss of epoch-35 batch-72 = 7.209135219454765e-06

Training epoch-35 batch-73
Running loss of epoch-35 batch-73 = 1.7413869500160217e-05

Training epoch-35 batch-74
Running loss of epoch-35 batch-74 = 7.204478606581688e-06

Training epoch-35 batch-75
Running loss of epoch-35 batch-75 = 2.785096876323223e-05

Training epoch-35 batch-76
Running loss of epoch-35 batch-76 = 6.15555327385664e-05

Training epoch-35 batch-77
Running loss of epoch-35 batch-77 = 3.7324498407542706e-05

Training epoch-35 batch-78
Running loss of epoch-35 batch-78 = 6.980961188673973e-06

Training epoch-35 batch-79
Running loss of epoch-35 batch-79 = 4.816567525267601e-06

Training epoch-35 batch-80
Running loss of epoch-35 batch-80 = 5.292176501825452e-05

Training epoch-35 batch-81
Running loss of epoch-35 batch-81 = 3.1653326004743576e-06

Training epoch-35 batch-82
Running loss of epoch-35 batch-82 = 1.0228948667645454e-05

Training epoch-35 batch-83
Running loss of epoch-35 batch-83 = 8.754391456022859e-05

Training epoch-35 batch-84
Running loss of epoch-35 batch-84 = 2.6052934117615223e-05

Training epoch-35 batch-85
Running loss of epoch-35 batch-85 = 5.647423677146435e-06

Training epoch-35 batch-86
Running loss of epoch-35 batch-86 = 7.39202369004488e-05

Training epoch-35 batch-87
Running loss of epoch-35 batch-87 = 0.00016576197231188416

Training epoch-35 batch-88
Running loss of epoch-35 batch-88 = 3.5248231142759323e-06

Training epoch-35 batch-89
Running loss of epoch-35 batch-89 = 6.6445209085941315e-06

Training epoch-35 batch-90
Running loss of epoch-35 batch-90 = 1.0514631867408752e-06

Training epoch-35 batch-91
Running loss of epoch-35 batch-91 = 5.135894753038883e-06

Training epoch-35 batch-92
Running loss of epoch-35 batch-92 = 8.982024155557156e-06

Training epoch-35 batch-93
Running loss of epoch-35 batch-93 = 3.014272078871727e-05

Training epoch-35 batch-94
Running loss of epoch-35 batch-94 = 1.3135024346411228e-05

Training epoch-35 batch-95
Running loss of epoch-35 batch-95 = 3.937212750315666e-05

Training epoch-35 batch-96
Running loss of epoch-35 batch-96 = 8.08550976216793e-06

Training epoch-35 batch-97
Running loss of epoch-35 batch-97 = 1.0761898010969162e-05

Training epoch-35 batch-98
Running loss of epoch-35 batch-98 = 1.1120806448161602e-05

Training epoch-35 batch-99
Running loss of epoch-35 batch-99 = 8.718465687707067e-05

Training epoch-35 batch-100
Running loss of epoch-35 batch-100 = 2.1373271010816097e-05

Training epoch-35 batch-101
Running loss of epoch-35 batch-101 = 6.485264748334885e-06

Training epoch-35 batch-102
Running loss of epoch-35 batch-102 = 7.990282028913498e-06

Training epoch-35 batch-103
Running loss of epoch-35 batch-103 = 1.651863567531109e-05

Training epoch-35 batch-104
Running loss of epoch-35 batch-104 = 7.010996341705322e-06

Training epoch-35 batch-105
Running loss of epoch-35 batch-105 = 8.588540367782116e-06

Training epoch-35 batch-106
Running loss of epoch-35 batch-106 = 1.2347591109573841e-05

Training epoch-35 batch-107
Running loss of epoch-35 batch-107 = 2.246350049972534e-06

Training epoch-35 batch-108
Running loss of epoch-35 batch-108 = 3.9496342651546e-05

Training epoch-35 batch-109
Running loss of epoch-35 batch-109 = 3.1479867175221443e-06

Training epoch-35 batch-110
Running loss of epoch-35 batch-110 = 1.143570989370346e-05

Training epoch-35 batch-111
Running loss of epoch-35 batch-111 = 5.4121483117341995e-06

Training epoch-35 batch-112
Running loss of epoch-35 batch-112 = 1.9967323169112206e-05

Training epoch-35 batch-113
Running loss of epoch-35 batch-113 = 0.00016624288400635123

Training epoch-35 batch-114
Running loss of epoch-35 batch-114 = 1.38402683660388e-05

Training epoch-35 batch-115
Running loss of epoch-35 batch-115 = 7.355585694313049e-06

Training epoch-35 batch-116
Running loss of epoch-35 batch-116 = 5.559180863201618e-06

Training epoch-35 batch-117
Running loss of epoch-35 batch-117 = 6.553949788212776e-06

Training epoch-35 batch-118
Running loss of epoch-35 batch-118 = 1.3565877452492714e-05

Training epoch-35 batch-119
Running loss of epoch-35 batch-119 = 1.4285556972026825e-05

Training epoch-35 batch-120
Running loss of epoch-35 batch-120 = 9.045004844665527e-06

Training epoch-35 batch-121
Running loss of epoch-35 batch-121 = 7.577007636427879e-06

Training epoch-35 batch-122
Running loss of epoch-35 batch-122 = 3.07777663692832e-05

Training epoch-35 batch-123
Running loss of epoch-35 batch-123 = 1.8542050383985043e-05

Training epoch-35 batch-124
Running loss of epoch-35 batch-124 = 3.84356826543808e-05

Training epoch-35 batch-125
Running loss of epoch-35 batch-125 = 2.175569534301758e-06

Training epoch-35 batch-126
Running loss of epoch-35 batch-126 = 2.472673077136278e-05

Training epoch-35 batch-127
Running loss of epoch-35 batch-127 = 2.3641856387257576e-05

Training epoch-35 batch-128
Running loss of epoch-35 batch-128 = 7.85081647336483e-06

Training epoch-35 batch-129
Running loss of epoch-35 batch-129 = 2.4568289518356323e-06

Training epoch-35 batch-130
Running loss of epoch-35 batch-130 = 7.42392148822546e-06

Training epoch-35 batch-131
Running loss of epoch-35 batch-131 = 2.1541956812143326e-05

Training epoch-35 batch-132
Running loss of epoch-35 batch-132 = 3.273517359048128e-05

Training epoch-35 batch-133
Running loss of epoch-35 batch-133 = 2.1856045350432396e-05

Training epoch-35 batch-134
Running loss of epoch-35 batch-134 = 3.300560638308525e-05

Training epoch-35 batch-135
Running loss of epoch-35 batch-135 = 6.305554416030645e-05

Training epoch-35 batch-136
Running loss of epoch-35 batch-136 = 3.345310688018799e-06

Training epoch-35 batch-137
Running loss of epoch-35 batch-137 = 4.091532900929451e-06

Training epoch-35 batch-138
Running loss of epoch-35 batch-138 = 5.646585486829281e-05

Training epoch-35 batch-139
Running loss of epoch-35 batch-139 = 2.1134852431714535e-05

Training epoch-35 batch-140
Running loss of epoch-35 batch-140 = 3.4351833164691925e-06

Training epoch-35 batch-141
Running loss of epoch-35 batch-141 = 2.0234030671417713e-05

Training epoch-35 batch-142
Running loss of epoch-35 batch-142 = 4.950794391334057e-05

Training epoch-35 batch-143
Running loss of epoch-35 batch-143 = 8.553732186555862e-06

Training epoch-35 batch-144
Running loss of epoch-35 batch-144 = 9.556533768773079e-06

Training epoch-35 batch-145
Running loss of epoch-35 batch-145 = 1.2675300240516663e-06

Training epoch-35 batch-146
Running loss of epoch-35 batch-146 = 2.8207432478666306e-06

Training epoch-35 batch-147
Running loss of epoch-35 batch-147 = 2.9196147806942463e-05

Training epoch-35 batch-148
Running loss of epoch-35 batch-148 = 4.6531204134225845e-06

Training epoch-35 batch-149
Running loss of epoch-35 batch-149 = 3.4207128919661045e-05

Training epoch-35 batch-150
Running loss of epoch-35 batch-150 = 5.564186722040176e-06

Training epoch-35 batch-151
Running loss of epoch-35 batch-151 = 5.684792995452881e-06

Training epoch-35 batch-152
Running loss of epoch-35 batch-152 = 6.764777936041355e-06

Training epoch-35 batch-153
Running loss of epoch-35 batch-153 = 7.983960676938295e-05

Training epoch-35 batch-154
Running loss of epoch-35 batch-154 = 3.931799437850714e-05

Training epoch-35 batch-155
Running loss of epoch-35 batch-155 = 5.970359779894352e-06

Training epoch-35 batch-156
Running loss of epoch-35 batch-156 = 2.8372742235660553e-06

Training epoch-35 batch-157
Running loss of epoch-35 batch-157 = 4.721991717815399e-05

Finished training epoch-35.



Average train loss at epoch-35 = 2.4066711589694022e-05

Started Evaluation

Average val loss at epoch-35 = 4.8119511525881915

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 52.13 %
Accuracy for class setUp is: 54.59 %
Accuracy for class onCreate is: 46.27 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 35.16 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.08 %
Accuracy for class execute is: 16.47 %
Accuracy for class get is: 27.44 %

Overall Accuracy = 46.23 %

Finished Evaluation



Started training epoch-36


Training epoch-36 batch-1
Running loss of epoch-36 batch-1 = 2.6705674827098846e-06

Training epoch-36 batch-2
Running loss of epoch-36 batch-2 = 9.751063771545887e-06

Training epoch-36 batch-3
Running loss of epoch-36 batch-3 = 5.839858204126358e-06

Training epoch-36 batch-4
Running loss of epoch-36 batch-4 = 8.916034130379558e-05

Training epoch-36 batch-5
Running loss of epoch-36 batch-5 = 8.284114301204681e-07

Training epoch-36 batch-6
Running loss of epoch-36 batch-6 = 1.014326699078083e-05

Training epoch-36 batch-7
Running loss of epoch-36 batch-7 = 6.233458407223225e-06

Training epoch-36 batch-8
Running loss of epoch-36 batch-8 = 5.3736031986773014e-05

Training epoch-36 batch-9
Running loss of epoch-36 batch-9 = 1.4770892448723316e-05

Training epoch-36 batch-10
Running loss of epoch-36 batch-10 = 1.2129312381148338e-05

Training epoch-36 batch-11
Running loss of epoch-36 batch-11 = 4.362664185464382e-06

Training epoch-36 batch-12
Running loss of epoch-36 batch-12 = 5.854177288711071e-06

Training epoch-36 batch-13
Running loss of epoch-36 batch-13 = 1.3986136764287949e-06

Training epoch-36 batch-14
Running loss of epoch-36 batch-14 = 3.873463720083237e-05

Training epoch-36 batch-15
Running loss of epoch-36 batch-15 = 2.075859811156988e-05

Training epoch-36 batch-16
Running loss of epoch-36 batch-16 = 3.5404227674007416e-06

Training epoch-36 batch-17
Running loss of epoch-36 batch-17 = 1.4281831681728363e-06

Training epoch-36 batch-18
Running loss of epoch-36 batch-18 = 8.585234172642231e-05

Training epoch-36 batch-19
Running loss of epoch-36 batch-19 = 4.072906449437141e-06

Training epoch-36 batch-20
Running loss of epoch-36 batch-20 = 4.641246050596237e-06

Training epoch-36 batch-21
Running loss of epoch-36 batch-21 = 1.746311318129301e-05

Training epoch-36 batch-22
Running loss of epoch-36 batch-22 = 4.761619493365288e-06

Training epoch-36 batch-23
Running loss of epoch-36 batch-23 = 1.3574492186307907e-05

Training epoch-36 batch-24
Running loss of epoch-36 batch-24 = 5.214707925915718e-06

Training epoch-36 batch-25
Running loss of epoch-36 batch-25 = 2.3801811039447784e-05

Training epoch-36 batch-26
Running loss of epoch-36 batch-26 = 2.6742927730083466e-06

Training epoch-36 batch-27
Running loss of epoch-36 batch-27 = 1.889921259135008e-05

Training epoch-36 batch-28
Running loss of epoch-36 batch-28 = 1.9391998648643494e-05

Training epoch-36 batch-29
Running loss of epoch-36 batch-29 = 8.981791324913502e-06

Training epoch-36 batch-30
Running loss of epoch-36 batch-30 = 1.9026920199394226e-06

Training epoch-36 batch-31
Running loss of epoch-36 batch-31 = 5.648587830364704e-06

Training epoch-36 batch-32
Running loss of epoch-36 batch-32 = 3.441469743847847e-06

Training epoch-36 batch-33
Running loss of epoch-36 batch-33 = 4.189438186585903e-06

Training epoch-36 batch-34
Running loss of epoch-36 batch-34 = 6.420770660042763e-06

Training epoch-36 batch-35
Running loss of epoch-36 batch-35 = 1.8377555534243584e-05

Training epoch-36 batch-36
Running loss of epoch-36 batch-36 = 1.8472783267498016e-06

Training epoch-36 batch-37
Running loss of epoch-36 batch-37 = 2.354150637984276e-06

Training epoch-36 batch-38
Running loss of epoch-36 batch-38 = 1.568265724927187e-05

Training epoch-36 batch-39
Running loss of epoch-36 batch-39 = 1.6385107301175594e-05

Training epoch-36 batch-40
Running loss of epoch-36 batch-40 = 2.3997854441404343e-06

Training epoch-36 batch-41
Running loss of epoch-36 batch-41 = 3.5811681300401688e-06

Training epoch-36 batch-42
Running loss of epoch-36 batch-42 = 8.924398571252823e-06

Training epoch-36 batch-43
Running loss of epoch-36 batch-43 = 1.269858330488205e-06

Training epoch-36 batch-44
Running loss of epoch-36 batch-44 = 6.191025022417307e-05

Training epoch-36 batch-45
Running loss of epoch-36 batch-45 = 0.00024110579397529364

Training epoch-36 batch-46
Running loss of epoch-36 batch-46 = 3.1629111617803574e-05

Training epoch-36 batch-47
Running loss of epoch-36 batch-47 = 1.7317943274974823e-06

Training epoch-36 batch-48
Running loss of epoch-36 batch-48 = 6.880471482872963e-05

Training epoch-36 batch-49
Running loss of epoch-36 batch-49 = 5.5157579481601715e-06

Training epoch-36 batch-50
Running loss of epoch-36 batch-50 = 7.930095307528973e-06

Training epoch-36 batch-51
Running loss of epoch-36 batch-51 = 2.8086360543966293e-06

Training epoch-36 batch-52
Running loss of epoch-36 batch-52 = 2.131576184183359e-05

Training epoch-36 batch-53
Running loss of epoch-36 batch-53 = 0.00011682137846946716

Training epoch-36 batch-54
Running loss of epoch-36 batch-54 = 2.132914960384369e-05

Training epoch-36 batch-55
Running loss of epoch-36 batch-55 = 8.266419172286987e-06

Training epoch-36 batch-56
Running loss of epoch-36 batch-56 = 2.2187945432960987e-05

Training epoch-36 batch-57
Running loss of epoch-36 batch-57 = 5.113240331411362e-05

Training epoch-36 batch-58
Running loss of epoch-36 batch-58 = 1.9490951672196388e-05

Training epoch-36 batch-59
Running loss of epoch-36 batch-59 = 9.740237146615982e-06

Training epoch-36 batch-60
Running loss of epoch-36 batch-60 = 1.7064157873392105e-05

Training epoch-36 batch-61
Running loss of epoch-36 batch-61 = 3.611226566135883e-05

Training epoch-36 batch-62
Running loss of epoch-36 batch-62 = 0.0001909146085381508

Training epoch-36 batch-63
Running loss of epoch-36 batch-63 = 1.9301660358905792e-06

Training epoch-36 batch-64
Running loss of epoch-36 batch-64 = 2.3313332349061966e-06

Training epoch-36 batch-65
Running loss of epoch-36 batch-65 = 2.2748718038201332e-05

Training epoch-36 batch-66
Running loss of epoch-36 batch-66 = 1.4004763215780258e-06

Training epoch-36 batch-67
Running loss of epoch-36 batch-67 = 2.1720770746469498e-06

Training epoch-36 batch-68
Running loss of epoch-36 batch-68 = 1.2650154531002045e-05

Training epoch-36 batch-69
Running loss of epoch-36 batch-69 = 5.908193998038769e-06

Training epoch-36 batch-70
Running loss of epoch-36 batch-70 = 2.376735210418701e-06

Training epoch-36 batch-71
Running loss of epoch-36 batch-71 = 6.945338100194931e-06

Training epoch-36 batch-72
Running loss of epoch-36 batch-72 = 6.271642632782459e-06

Training epoch-36 batch-73
Running loss of epoch-36 batch-73 = 2.3886444978415966e-05

Training epoch-36 batch-74
Running loss of epoch-36 batch-74 = 1.6116537153720856e-06

Training epoch-36 batch-75
Running loss of epoch-36 batch-75 = 3.188115078955889e-05

Training epoch-36 batch-76
Running loss of epoch-36 batch-76 = 4.873261786997318e-06

Training epoch-36 batch-77
Running loss of epoch-36 batch-77 = 4.009110853075981e-06

Training epoch-36 batch-78
Running loss of epoch-36 batch-78 = 3.0245166271924973e-05

Training epoch-36 batch-79
Running loss of epoch-36 batch-79 = 0.0001771785318851471

Training epoch-36 batch-80
Running loss of epoch-36 batch-80 = 0.0001414341968484223

Training epoch-36 batch-81
Running loss of epoch-36 batch-81 = 3.97954136133194e-06

Training epoch-36 batch-82
Running loss of epoch-36 batch-82 = 1.8798746168613434e-06

Training epoch-36 batch-83
Running loss of epoch-36 batch-83 = 7.952330633997917e-06

Training epoch-36 batch-84
Running loss of epoch-36 batch-84 = 7.356400601565838e-06

Training epoch-36 batch-85
Running loss of epoch-36 batch-85 = 2.96090729534626e-06

Training epoch-36 batch-86
Running loss of epoch-36 batch-86 = 3.6098062992095947e-06

Training epoch-36 batch-87
Running loss of epoch-36 batch-87 = 3.434717655181885e-05

Training epoch-36 batch-88
Running loss of epoch-36 batch-88 = 1.644110307097435e-05

Training epoch-36 batch-89
Running loss of epoch-36 batch-89 = 7.169321179389954e-06

Training epoch-36 batch-90
Running loss of epoch-36 batch-90 = 6.35138712823391e-06

Training epoch-36 batch-91
Running loss of epoch-36 batch-91 = 3.244192339479923e-05

Training epoch-36 batch-92
Running loss of epoch-36 batch-92 = 3.1604431569576263e-06

Training epoch-36 batch-93
Running loss of epoch-36 batch-93 = 6.93794572725892e-05

Training epoch-36 batch-94
Running loss of epoch-36 batch-94 = 8.898787200450897e-07

Training epoch-36 batch-95
Running loss of epoch-36 batch-95 = 5.134381353855133e-06

Training epoch-36 batch-96
Running loss of epoch-36 batch-96 = 7.0927198976278305e-06

Training epoch-36 batch-97
Running loss of epoch-36 batch-97 = 4.993926268070936e-05

Training epoch-36 batch-98
Running loss of epoch-36 batch-98 = 3.025284968316555e-05

Training epoch-36 batch-99
Running loss of epoch-36 batch-99 = 8.916598744690418e-06

Training epoch-36 batch-100
Running loss of epoch-36 batch-100 = 1.2391014024615288e-05

Training epoch-36 batch-101
Running loss of epoch-36 batch-101 = 3.1909439712762833e-06

Training epoch-36 batch-102
Running loss of epoch-36 batch-102 = 1.7505954019725323e-05

Training epoch-36 batch-103
Running loss of epoch-36 batch-103 = 2.127664629369974e-05

Training epoch-36 batch-104
Running loss of epoch-36 batch-104 = 6.205867975950241e-06

Training epoch-36 batch-105
Running loss of epoch-36 batch-105 = 2.1168962121009827e-06

Training epoch-36 batch-106
Running loss of epoch-36 batch-106 = 1.1729425750672817e-05

Training epoch-36 batch-107
Running loss of epoch-36 batch-107 = 1.8092221580445766e-05

Training epoch-36 batch-108
Running loss of epoch-36 batch-108 = 5.3026131354272366e-05

Training epoch-36 batch-109
Running loss of epoch-36 batch-109 = 3.5833800211548805e-06

Training epoch-36 batch-110
Running loss of epoch-36 batch-110 = 7.07499566487968e-05

Training epoch-36 batch-111
Running loss of epoch-36 batch-111 = 5.122274160385132e-06

Training epoch-36 batch-112
Running loss of epoch-36 batch-112 = 1.7600483261048794e-05

Training epoch-36 batch-113
Running loss of epoch-36 batch-113 = 1.3242708519101143e-05

Training epoch-36 batch-114
Running loss of epoch-36 batch-114 = 0.00026047980645671487

Training epoch-36 batch-115
Running loss of epoch-36 batch-115 = 5.223904736340046e-06

Training epoch-36 batch-116
Running loss of epoch-36 batch-116 = 0.00013680820120498538

Training epoch-36 batch-117
Running loss of epoch-36 batch-117 = 3.9111822843551636e-05

Training epoch-36 batch-118
Running loss of epoch-36 batch-118 = 2.925517037510872e-06

Training epoch-36 batch-119
Running loss of epoch-36 batch-119 = 3.357650712132454e-06

Training epoch-36 batch-120
Running loss of epoch-36 batch-120 = 1.763482578098774e-05

Training epoch-36 batch-121
Running loss of epoch-36 batch-121 = 1.789536327123642e-06

Training epoch-36 batch-122
Running loss of epoch-36 batch-122 = 6.503891199827194e-06

Training epoch-36 batch-123
Running loss of epoch-36 batch-123 = 4.83065377920866e-06

Training epoch-36 batch-124
Running loss of epoch-36 batch-124 = 2.3077125661075115e-05

Training epoch-36 batch-125
Running loss of epoch-36 batch-125 = 5.013542249798775e-06

Training epoch-36 batch-126
Running loss of epoch-36 batch-126 = 2.4389359168708324e-05

Training epoch-36 batch-127
Running loss of epoch-36 batch-127 = 3.020279109477997e-06

Training epoch-36 batch-128
Running loss of epoch-36 batch-128 = 5.342997610569e-06

Training epoch-36 batch-129
Running loss of epoch-36 batch-129 = 6.570806726813316e-05

Training epoch-36 batch-130
Running loss of epoch-36 batch-130 = 6.059184670448303e-06

Training epoch-36 batch-131
Running loss of epoch-36 batch-131 = 7.511454168707132e-05

Training epoch-36 batch-132
Running loss of epoch-36 batch-132 = 2.5904737412929535e-06

Training epoch-36 batch-133
Running loss of epoch-36 batch-133 = 3.532739356160164e-06

Training epoch-36 batch-134
Running loss of epoch-36 batch-134 = 3.1401868909597397e-06

Training epoch-36 batch-135
Running loss of epoch-36 batch-135 = 2.136023249477148e-05

Training epoch-36 batch-136
Running loss of epoch-36 batch-136 = 3.076379653066397e-05

Training epoch-36 batch-137
Running loss of epoch-36 batch-137 = 7.157912477850914e-06

Training epoch-36 batch-138
Running loss of epoch-36 batch-138 = 1.896638423204422e-06

Training epoch-36 batch-139
Running loss of epoch-36 batch-139 = 3.77558171749115e-06

Training epoch-36 batch-140
Running loss of epoch-36 batch-140 = 3.1152740120887756e-06

Training epoch-36 batch-141
Running loss of epoch-36 batch-141 = 2.1583400666713715e-06

Training epoch-36 batch-142
Running loss of epoch-36 batch-142 = 1.3394397683441639e-05

Training epoch-36 batch-143
Running loss of epoch-36 batch-143 = 4.4030603021383286e-06

Training epoch-36 batch-144
Running loss of epoch-36 batch-144 = 4.605390131473541e-07

Training epoch-36 batch-145
Running loss of epoch-36 batch-145 = 2.345070242881775e-06

Training epoch-36 batch-146
Running loss of epoch-36 batch-146 = 0.00011783174704760313

Training epoch-36 batch-147
Running loss of epoch-36 batch-147 = 1.3271346688270569e-06

Training epoch-36 batch-148
Running loss of epoch-36 batch-148 = 2.6475172489881516e-06

Training epoch-36 batch-149
Running loss of epoch-36 batch-149 = 3.6533456295728683e-06

Training epoch-36 batch-150
Running loss of epoch-36 batch-150 = 4.815054126083851e-06

Training epoch-36 batch-151
Running loss of epoch-36 batch-151 = 1.4422694221138954e-05

Training epoch-36 batch-152
Running loss of epoch-36 batch-152 = 1.0854681022465229e-05

Training epoch-36 batch-153
Running loss of epoch-36 batch-153 = 3.563240170478821e-06

Training epoch-36 batch-154
Running loss of epoch-36 batch-154 = 7.637077942490578e-06

Training epoch-36 batch-155
Running loss of epoch-36 batch-155 = 1.7210841178894043e-06

Training epoch-36 batch-156
Running loss of epoch-36 batch-156 = 2.5284592993557453e-05

Training epoch-36 batch-157
Running loss of epoch-36 batch-157 = 0.0001301933079957962

Finished training epoch-36.



Average train loss at epoch-36 = 2.2826506569981574e-05

Started Evaluation

Average val loss at epoch-36 = 4.904405327219712

Accuracy for classes:
Accuracy for class equals is: 68.98 %
Accuracy for class main is: 49.51 %
Accuracy for class setUp is: 53.93 %
Accuracy for class onCreate is: 42.64 %
Accuracy for class toString is: 43.00 %
Accuracy for class run is: 35.39 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.97 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 27.95 %

Overall Accuracy = 45.45 %

Finished Evaluation



Started training epoch-37


Training epoch-37 batch-1
Running loss of epoch-37 batch-1 = 2.4330802261829376e-06

Training epoch-37 batch-2
Running loss of epoch-37 batch-2 = 8.464674465358257e-06

Training epoch-37 batch-3
Running loss of epoch-37 batch-3 = 0.00016607140423730016

Training epoch-37 batch-4
Running loss of epoch-37 batch-4 = 2.4568289518356323e-06

Training epoch-37 batch-5
Running loss of epoch-37 batch-5 = 2.2293650545179844e-05

Training epoch-37 batch-6
Running loss of epoch-37 batch-6 = 4.6405475586652756e-06

Training epoch-37 batch-7
Running loss of epoch-37 batch-7 = 2.2756634280085564e-05

Training epoch-37 batch-8
Running loss of epoch-37 batch-8 = 2.4174805730581284e-05

Training epoch-37 batch-9
Running loss of epoch-37 batch-9 = 6.682658568024635e-05

Training epoch-37 batch-10
Running loss of epoch-37 batch-10 = 2.7924776077270508e-05

Training epoch-37 batch-11
Running loss of epoch-37 batch-11 = 2.275791484862566e-05

Training epoch-37 batch-12
Running loss of epoch-37 batch-12 = 4.391535185277462e-06

Training epoch-37 batch-13
Running loss of epoch-37 batch-13 = 9.333295747637749e-05

Training epoch-37 batch-14
Running loss of epoch-37 batch-14 = 7.035559974610806e-05

Training epoch-37 batch-15
Running loss of epoch-37 batch-15 = 1.1984957382082939e-05

Training epoch-37 batch-16
Running loss of epoch-37 batch-16 = 6.879447028040886e-06

Training epoch-37 batch-17
Running loss of epoch-37 batch-17 = 7.827766239643097e-07

Training epoch-37 batch-18
Running loss of epoch-37 batch-18 = 4.445016384124756e-05

Training epoch-37 batch-19
Running loss of epoch-37 batch-19 = 1.8773134797811508e-06

Training epoch-37 batch-20
Running loss of epoch-37 batch-20 = 0.00026820716448128223

Training epoch-37 batch-21
Running loss of epoch-37 batch-21 = 4.192115738987923e-06

Training epoch-37 batch-22
Running loss of epoch-37 batch-22 = 1.054664608091116e-05

Training epoch-37 batch-23
Running loss of epoch-37 batch-23 = 9.160605259239674e-06

Training epoch-37 batch-24
Running loss of epoch-37 batch-24 = 2.4360371753573418e-05

Training epoch-37 batch-25
Running loss of epoch-37 batch-25 = 1.3208598829805851e-05

Training epoch-37 batch-26
Running loss of epoch-37 batch-26 = 0.00014331901911646128

Training epoch-37 batch-27
Running loss of epoch-37 batch-27 = 4.3021165765821934e-05

Training epoch-37 batch-28
Running loss of epoch-37 batch-28 = 2.3413565941154957e-05

Training epoch-37 batch-29
Running loss of epoch-37 batch-29 = 2.6887282729148865e-06

Training epoch-37 batch-30
Running loss of epoch-37 batch-30 = 8.079258259385824e-05

Training epoch-37 batch-31
Running loss of epoch-37 batch-31 = 2.817949280142784e-06

Training epoch-37 batch-32
Running loss of epoch-37 batch-32 = 1.5287892892956734e-05

Training epoch-37 batch-33
Running loss of epoch-37 batch-33 = 5.2335672080516815e-06

Training epoch-37 batch-34
Running loss of epoch-37 batch-34 = 3.002118319272995e-06

Training epoch-37 batch-35
Running loss of epoch-37 batch-35 = 4.682806320488453e-06

Training epoch-37 batch-36
Running loss of epoch-37 batch-36 = 6.88270665705204e-06

Training epoch-37 batch-37
Running loss of epoch-37 batch-37 = 5.04701747559011e-05

Training epoch-37 batch-38
Running loss of epoch-37 batch-38 = 2.157408744096756e-06

Training epoch-37 batch-39
Running loss of epoch-37 batch-39 = 1.0610325261950493e-05

Training epoch-37 batch-40
Running loss of epoch-37 batch-40 = 1.4509190805256367e-05

Training epoch-37 batch-41
Running loss of epoch-37 batch-41 = 1.2629316188395023e-05

Training epoch-37 batch-42
Running loss of epoch-37 batch-42 = 8.654198609292507e-06

Training epoch-37 batch-43
Running loss of epoch-37 batch-43 = 3.743753768503666e-05

Training epoch-37 batch-44
Running loss of epoch-37 batch-44 = 1.992960460484028e-05

Training epoch-37 batch-45
Running loss of epoch-37 batch-45 = 4.353094846010208e-05

Training epoch-37 batch-46
Running loss of epoch-37 batch-46 = 1.8640421330928802e-06

Training epoch-37 batch-47
Running loss of epoch-37 batch-47 = 6.0453719925135374e-05

Training epoch-37 batch-48
Running loss of epoch-37 batch-48 = 4.563597030937672e-06

Training epoch-37 batch-49
Running loss of epoch-37 batch-49 = 2.450577449053526e-05

Training epoch-37 batch-50
Running loss of epoch-37 batch-50 = 3.9727892726659775e-06

Training epoch-37 batch-51
Running loss of epoch-37 batch-51 = 5.3171650506556034e-05

Training epoch-37 batch-52
Running loss of epoch-37 batch-52 = 1.3685785233974457e-05

Training epoch-37 batch-53
Running loss of epoch-37 batch-53 = 2.560904249548912e-06

Training epoch-37 batch-54
Running loss of epoch-37 batch-54 = 1.1129304766654968e-06

Training epoch-37 batch-55
Running loss of epoch-37 batch-55 = 1.857476308941841e-05

Training epoch-37 batch-56
Running loss of epoch-37 batch-56 = 4.850677214562893e-06

Training epoch-37 batch-57
Running loss of epoch-37 batch-57 = 0.00021737825591117144

Training epoch-37 batch-58
Running loss of epoch-37 batch-58 = 1.7451588064432144e-05

Training epoch-37 batch-59
Running loss of epoch-37 batch-59 = 1.482584048062563e-05

Training epoch-37 batch-60
Running loss of epoch-37 batch-60 = 5.199923180043697e-06

Training epoch-37 batch-61
Running loss of epoch-37 batch-61 = 2.9548536986112595e-06

Training epoch-37 batch-62
Running loss of epoch-37 batch-62 = 8.309376426041126e-06

Training epoch-37 batch-63
Running loss of epoch-37 batch-63 = 3.3141113817691803e-06

Training epoch-37 batch-64
Running loss of epoch-37 batch-64 = 0.00010578869841992855

Training epoch-37 batch-65
Running loss of epoch-37 batch-65 = 3.237975761294365e-06

Training epoch-37 batch-66
Running loss of epoch-37 batch-66 = 4.431768320500851e-05

Training epoch-37 batch-67
Running loss of epoch-37 batch-67 = 6.467103958129883e-06

Training epoch-37 batch-68
Running loss of epoch-37 batch-68 = 3.0145165510475636e-05

Training epoch-37 batch-69
Running loss of epoch-37 batch-69 = 2.3711472749710083e-06

Training epoch-37 batch-70
Running loss of epoch-37 batch-70 = 1.8482096493244171e-06

Training epoch-37 batch-71
Running loss of epoch-37 batch-71 = 2.88570299744606e-06

Training epoch-37 batch-72
Running loss of epoch-37 batch-72 = 9.286479325965047e-05

Training epoch-37 batch-73
Running loss of epoch-37 batch-73 = 7.838825695216656e-06

Training epoch-37 batch-74
Running loss of epoch-37 batch-74 = 6.744638085365295e-06

Training epoch-37 batch-75
Running loss of epoch-37 batch-75 = 1.957407221198082e-06

Training epoch-37 batch-76
Running loss of epoch-37 batch-76 = 3.503868356347084e-06

Training epoch-37 batch-77
Running loss of epoch-37 batch-77 = 2.2492604330182076e-05

Training epoch-37 batch-78
Running loss of epoch-37 batch-78 = 5.667330697178841e-06

Training epoch-37 batch-79
Running loss of epoch-37 batch-79 = 9.018811397254467e-06

Training epoch-37 batch-80
Running loss of epoch-37 batch-80 = 5.898182280361652e-06

Training epoch-37 batch-81
Running loss of epoch-37 batch-81 = 3.3748801797628403e-06

Training epoch-37 batch-82
Running loss of epoch-37 batch-82 = 2.9766233637928963e-06

Training epoch-37 batch-83
Running loss of epoch-37 batch-83 = 8.67992639541626e-07

Training epoch-37 batch-84
Running loss of epoch-37 batch-84 = 3.3299438655376434e-06

Training epoch-37 batch-85
Running loss of epoch-37 batch-85 = 5.664967466145754e-05

Training epoch-37 batch-86
Running loss of epoch-37 batch-86 = 1.3775192201137543e-05

Training epoch-37 batch-87
Running loss of epoch-37 batch-87 = 1.685810275375843e-05

Training epoch-37 batch-88
Running loss of epoch-37 batch-88 = 8.157105185091496e-06

Training epoch-37 batch-89
Running loss of epoch-37 batch-89 = 2.3264437913894653e-06

Training epoch-37 batch-90
Running loss of epoch-37 batch-90 = 1.7189537174999714e-05

Training epoch-37 batch-91
Running loss of epoch-37 batch-91 = 2.3032771423459053e-06

Training epoch-37 batch-92
Running loss of epoch-37 batch-92 = 1.687521580606699e-05

Training epoch-37 batch-93
Running loss of epoch-37 batch-93 = 9.136972948908806e-06

Training epoch-37 batch-94
Running loss of epoch-37 batch-94 = 3.868713974952698e-06

Training epoch-37 batch-95
Running loss of epoch-37 batch-95 = 1.4116987586021423e-05

Training epoch-37 batch-96
Running loss of epoch-37 batch-96 = 8.533243089914322e-07

Training epoch-37 batch-97
Running loss of epoch-37 batch-97 = 1.0935822501778603e-05

Training epoch-37 batch-98
Running loss of epoch-37 batch-98 = 4.179542884230614e-06

Training epoch-37 batch-99
Running loss of epoch-37 batch-99 = 2.132111694663763e-05

Training epoch-37 batch-100
Running loss of epoch-37 batch-100 = 1.376355066895485e-05

Training epoch-37 batch-101
Running loss of epoch-37 batch-101 = 2.579181455075741e-06

Training epoch-37 batch-102
Running loss of epoch-37 batch-102 = 6.290501914918423e-06

Training epoch-37 batch-103
Running loss of epoch-37 batch-103 = 2.027594018727541e-05

Training epoch-37 batch-104
Running loss of epoch-37 batch-104 = 1.1892523616552353e-05

Training epoch-37 batch-105
Running loss of epoch-37 batch-105 = 7.938127964735031e-06

Training epoch-37 batch-106
Running loss of epoch-37 batch-106 = 1.1913594789803028e-05

Training epoch-37 batch-107
Running loss of epoch-37 batch-107 = 1.3803946785628796e-05

Training epoch-37 batch-108
Running loss of epoch-37 batch-108 = 1.4354824088513851e-05

Training epoch-37 batch-109
Running loss of epoch-37 batch-109 = 3.4603290259838104e-06

Training epoch-37 batch-110
Running loss of epoch-37 batch-110 = 3.30386683344841e-06

Training epoch-37 batch-111
Running loss of epoch-37 batch-111 = 4.038319457322359e-05

Training epoch-37 batch-112
Running loss of epoch-37 batch-112 = 9.311828762292862e-06

Training epoch-37 batch-113
Running loss of epoch-37 batch-113 = 2.6195310056209564e-05

Training epoch-37 batch-114
Running loss of epoch-37 batch-114 = 7.831724360585213e-06

Training epoch-37 batch-115
Running loss of epoch-37 batch-115 = 2.0246952772140503e-06

Training epoch-37 batch-116
Running loss of epoch-37 batch-116 = 1.9459985196590424e-06

Training epoch-37 batch-117
Running loss of epoch-37 batch-117 = 1.4053424820303917e-05

Training epoch-37 batch-118
Running loss of epoch-37 batch-118 = 2.3517641238868237e-05

Training epoch-37 batch-119
Running loss of epoch-37 batch-119 = 2.0042061805725098e-06

Training epoch-37 batch-120
Running loss of epoch-37 batch-120 = 1.9827857613563538e-06

Training epoch-37 batch-121
Running loss of epoch-37 batch-121 = 1.1085066944360733e-06

Training epoch-37 batch-122
Running loss of epoch-37 batch-122 = 8.000642992556095e-06

Training epoch-37 batch-123
Running loss of epoch-37 batch-123 = 2.7667498216032982e-05

Training epoch-37 batch-124
Running loss of epoch-37 batch-124 = 2.0052306354045868e-05

Training epoch-37 batch-125
Running loss of epoch-37 batch-125 = 3.906898200511932e-06

Training epoch-37 batch-126
Running loss of epoch-37 batch-126 = 5.795154720544815e-05

Training epoch-37 batch-127
Running loss of epoch-37 batch-127 = 3.1588133424520493e-06

Training epoch-37 batch-128
Running loss of epoch-37 batch-128 = 5.994690582156181e-06

Training epoch-37 batch-129
Running loss of epoch-37 batch-129 = 1.8409918993711472e-06

Training epoch-37 batch-130
Running loss of epoch-37 batch-130 = 1.0783900506794453e-05

Training epoch-37 batch-131
Running loss of epoch-37 batch-131 = 4.255631938576698e-05

Training epoch-37 batch-132
Running loss of epoch-37 batch-132 = 5.126232281327248e-06

Training epoch-37 batch-133
Running loss of epoch-37 batch-133 = 2.99699604511261e-06

Training epoch-37 batch-134
Running loss of epoch-37 batch-134 = 1.2270058505237103e-05

Training epoch-37 batch-135
Running loss of epoch-37 batch-135 = 3.410619683563709e-05

Training epoch-37 batch-136
Running loss of epoch-37 batch-136 = 1.727137714624405e-06

Training epoch-37 batch-137
Running loss of epoch-37 batch-137 = 8.800998330116272e-07

Training epoch-37 batch-138
Running loss of epoch-37 batch-138 = 8.554314263164997e-06

Training epoch-37 batch-139
Running loss of epoch-37 batch-139 = 2.792919985949993e-06

Training epoch-37 batch-140
Running loss of epoch-37 batch-140 = 2.794666215777397e-06

Training epoch-37 batch-141
Running loss of epoch-37 batch-141 = 8.71031079441309e-06

Training epoch-37 batch-142
Running loss of epoch-37 batch-142 = 6.215646862983704e-06

Training epoch-37 batch-143
Running loss of epoch-37 batch-143 = 1.1918600648641586e-06

Training epoch-37 batch-144
Running loss of epoch-37 batch-144 = 1.1119991540908813e-06

Training epoch-37 batch-145
Running loss of epoch-37 batch-145 = 3.6706216633319855e-05

Training epoch-37 batch-146
Running loss of epoch-37 batch-146 = 1.2868549674749374e-06

Training epoch-37 batch-147
Running loss of epoch-37 batch-147 = 0.00011826690752059221

Training epoch-37 batch-148
Running loss of epoch-37 batch-148 = 1.8677674233913422e-06

Training epoch-37 batch-149
Running loss of epoch-37 batch-149 = 3.270339220762253e-06

Training epoch-37 batch-150
Running loss of epoch-37 batch-150 = 2.0209699869155884e-06

Training epoch-37 batch-151
Running loss of epoch-37 batch-151 = 2.6740250177681446e-05

Training epoch-37 batch-152
Running loss of epoch-37 batch-152 = 1.868000254034996e-06

Training epoch-37 batch-153
Running loss of epoch-37 batch-153 = 1.6801059246063232e-06

Training epoch-37 batch-154
Running loss of epoch-37 batch-154 = 4.9478840082883835e-06

Training epoch-37 batch-155
Running loss of epoch-37 batch-155 = 2.089573536068201e-05

Training epoch-37 batch-156
Running loss of epoch-37 batch-156 = 4.6709319576621056e-06

Training epoch-37 batch-157
Running loss of epoch-37 batch-157 = 2.4028122425079346e-06

Finished training epoch-37.



Average train loss at epoch-37 = 2.0618544518947603e-05

Started Evaluation

Average val loss at epoch-37 = 4.924809397835481

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 48.52 %
Accuracy for class setUp is: 53.61 %
Accuracy for class onCreate is: 42.75 %
Accuracy for class toString is: 40.96 %
Accuracy for class run is: 33.33 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 25.11 %
Accuracy for class execute is: 18.47 %
Accuracy for class get is: 27.69 %

Overall Accuracy = 45.16 %

Finished Evaluation



Started training epoch-38


Training epoch-38 batch-1
Running loss of epoch-38 batch-1 = 3.439723514020443e-06

Training epoch-38 batch-2
Running loss of epoch-38 batch-2 = 1.3697776012122631e-05

Training epoch-38 batch-3
Running loss of epoch-38 batch-3 = 1.3094278983771801e-05

Training epoch-38 batch-4
Running loss of epoch-38 batch-4 = 3.823777660727501e-06

Training epoch-38 batch-5
Running loss of epoch-38 batch-5 = 8.086790330708027e-06

Training epoch-38 batch-6
Running loss of epoch-38 batch-6 = 4.466855898499489e-06

Training epoch-38 batch-7
Running loss of epoch-38 batch-7 = 1.308636274188757e-05

Training epoch-38 batch-8
Running loss of epoch-38 batch-8 = 3.5350676625967026e-06

Training epoch-38 batch-9
Running loss of epoch-38 batch-9 = 6.258254870772362e-06

Training epoch-38 batch-10
Running loss of epoch-38 batch-10 = 2.6135239750146866e-06

Training epoch-38 batch-11
Running loss of epoch-38 batch-11 = 2.342450898140669e-05

Training epoch-38 batch-12
Running loss of epoch-38 batch-12 = 1.2054108083248138e-05

Training epoch-38 batch-13
Running loss of epoch-38 batch-13 = 3.2238662242889404e-05

Training epoch-38 batch-14
Running loss of epoch-38 batch-14 = 2.4673063308000565e-06

Training epoch-38 batch-15
Running loss of epoch-38 batch-15 = 2.7052592486143112e-06

Training epoch-38 batch-16
Running loss of epoch-38 batch-16 = 1.5360419638454914e-05

Training epoch-38 batch-17
Running loss of epoch-38 batch-17 = 2.4186447262763977e-06

Training epoch-38 batch-18
Running loss of epoch-38 batch-18 = 0.0001829442335292697

Training epoch-38 batch-19
Running loss of epoch-38 batch-19 = 6.902962923049927e-06

Training epoch-38 batch-20
Running loss of epoch-38 batch-20 = 1.986988354474306e-05

Training epoch-38 batch-21
Running loss of epoch-38 batch-21 = 1.2030010111629963e-05

Training epoch-38 batch-22
Running loss of epoch-38 batch-22 = 4.387926310300827e-06

Training epoch-38 batch-23
Running loss of epoch-38 batch-23 = 1.2203468941152096e-05

Training epoch-38 batch-24
Running loss of epoch-38 batch-24 = 3.3034011721611023e-06

Training epoch-38 batch-25
Running loss of epoch-38 batch-25 = 1.4319084584712982e-06

Training epoch-38 batch-26
Running loss of epoch-38 batch-26 = 6.811460480093956e-06

Training epoch-38 batch-27
Running loss of epoch-38 batch-27 = 2.03610397875309e-06

Training epoch-38 batch-28
Running loss of epoch-38 batch-28 = 0.00023254280677065253

Training epoch-38 batch-29
Running loss of epoch-38 batch-29 = 5.450914613902569e-06

Training epoch-38 batch-30
Running loss of epoch-38 batch-30 = 5.851848982274532e-06

Training epoch-38 batch-31
Running loss of epoch-38 batch-31 = 4.958827048540115e-06

Training epoch-38 batch-32
Running loss of epoch-38 batch-32 = 1.4828983694314957e-06

Training epoch-38 batch-33
Running loss of epoch-38 batch-33 = 4.251953214406967e-06

Training epoch-38 batch-34
Running loss of epoch-38 batch-34 = 3.475695848464966e-06

Training epoch-38 batch-35
Running loss of epoch-38 batch-35 = 1.314713153988123e-05

Training epoch-38 batch-36
Running loss of epoch-38 batch-36 = 6.444752216339111e-07

Training epoch-38 batch-37
Running loss of epoch-38 batch-37 = 1.0740477591753006e-06

Training epoch-38 batch-38
Running loss of epoch-38 batch-38 = 3.8656871765851974e-06

Training epoch-38 batch-39
Running loss of epoch-38 batch-39 = 6.541609764099121e-06

Training epoch-38 batch-40
Running loss of epoch-38 batch-40 = 4.234025254845619e-06

Training epoch-38 batch-41
Running loss of epoch-38 batch-41 = 2.6496127247810364e-06

Training epoch-38 batch-42
Running loss of epoch-38 batch-42 = 6.691040471196175e-05

Training epoch-38 batch-43
Running loss of epoch-38 batch-43 = 1.6069971024990082e-06

Training epoch-38 batch-44
Running loss of epoch-38 batch-44 = 5.718320608139038e-05

Training epoch-38 batch-45
Running loss of epoch-38 batch-45 = 3.645196557044983e-06

Training epoch-38 batch-46
Running loss of epoch-38 batch-46 = 6.706896238029003e-05

Training epoch-38 batch-47
Running loss of epoch-38 batch-47 = 9.078183211386204e-06

Training epoch-38 batch-48
Running loss of epoch-38 batch-48 = 8.119386620819569e-06

Training epoch-38 batch-49
Running loss of epoch-38 batch-49 = 9.018578566610813e-06

Training epoch-38 batch-50
Running loss of epoch-38 batch-50 = 6.42694067209959e-06

Training epoch-38 batch-51
Running loss of epoch-38 batch-51 = 1.994625199586153e-05

Training epoch-38 batch-52
Running loss of epoch-38 batch-52 = 4.759291186928749e-06

Training epoch-38 batch-53
Running loss of epoch-38 batch-53 = 4.267087206244469e-06

Training epoch-38 batch-54
Running loss of epoch-38 batch-54 = 2.3694592528045177e-05

Training epoch-38 batch-55
Running loss of epoch-38 batch-55 = 2.8817448765039444e-06

Training epoch-38 batch-56
Running loss of epoch-38 batch-56 = 0.00016007112571969628

Training epoch-38 batch-57
Running loss of epoch-38 batch-57 = 1.2007891200482845e-05

Training epoch-38 batch-58
Running loss of epoch-38 batch-58 = 3.183283843100071e-05

Training epoch-38 batch-59
Running loss of epoch-38 batch-59 = 1.3058423064649105e-05

Training epoch-38 batch-60
Running loss of epoch-38 batch-60 = 2.4426262825727463e-06

Training epoch-38 batch-61
Running loss of epoch-38 batch-61 = 2.728775143623352e-07

Training epoch-38 batch-62
Running loss of epoch-38 batch-62 = 2.203742042183876e-06

Training epoch-38 batch-63
Running loss of epoch-38 batch-63 = 0.0001141474349424243

Training epoch-38 batch-64
Running loss of epoch-38 batch-64 = 2.351670991629362e-05

Training epoch-38 batch-65
Running loss of epoch-38 batch-65 = 2.847728319466114e-05

Training epoch-38 batch-66
Running loss of epoch-38 batch-66 = 2.045184373855591e-06

Training epoch-38 batch-67
Running loss of epoch-38 batch-67 = 3.6972342059016228e-06

Training epoch-38 batch-68
Running loss of epoch-38 batch-68 = 5.321810021996498e-06

Training epoch-38 batch-69
Running loss of epoch-38 batch-69 = 6.94308546371758e-05

Training epoch-38 batch-70
Running loss of epoch-38 batch-70 = 2.786691766232252e-05

Training epoch-38 batch-71
Running loss of epoch-38 batch-71 = 4.5049237087368965e-06

Training epoch-38 batch-72
Running loss of epoch-38 batch-72 = 9.097281144931912e-05

Training epoch-38 batch-73
Running loss of epoch-38 batch-73 = 3.7236022762954235e-05

Training epoch-38 batch-74
Running loss of epoch-38 batch-74 = 3.063236363232136e-05

Training epoch-38 batch-75
Running loss of epoch-38 batch-75 = 2.0437873899936676e-05

Training epoch-38 batch-76
Running loss of epoch-38 batch-76 = 1.94901367649436e-05

Training epoch-38 batch-77
Running loss of epoch-38 batch-77 = 7.382500916719437e-05

Training epoch-38 batch-78
Running loss of epoch-38 batch-78 = 3.2373936846852303e-06

Training epoch-38 batch-79
Running loss of epoch-38 batch-79 = 1.1076335795223713e-05

Training epoch-38 batch-80
Running loss of epoch-38 batch-80 = 4.207715392112732e-06

Training epoch-38 batch-81
Running loss of epoch-38 batch-81 = 4.902482032775879e-06

Training epoch-38 batch-82
Running loss of epoch-38 batch-82 = 4.888162948191166e-06

Training epoch-38 batch-83
Running loss of epoch-38 batch-83 = 1.912470906972885e-06

Training epoch-38 batch-84
Running loss of epoch-38 batch-84 = 3.399350680410862e-05

Training epoch-38 batch-85
Running loss of epoch-38 batch-85 = 1.7017009668052197e-05

Training epoch-38 batch-86
Running loss of epoch-38 batch-86 = 1.733819954097271e-05

Training epoch-38 batch-87
Running loss of epoch-38 batch-87 = 8.83906614035368e-06

Training epoch-38 batch-88
Running loss of epoch-38 batch-88 = 6.129615940153599e-06

Training epoch-38 batch-89
Running loss of epoch-38 batch-89 = 1.957640051841736e-06

Training epoch-38 batch-90
Running loss of epoch-38 batch-90 = 1.1855736374855042e-06

Training epoch-38 batch-91
Running loss of epoch-38 batch-91 = 6.06197863817215e-06

Training epoch-38 batch-92
Running loss of epoch-38 batch-92 = 3.1889649108052254e-06

Training epoch-38 batch-93
Running loss of epoch-38 batch-93 = 1.4001037925481796e-05

Training epoch-38 batch-94
Running loss of epoch-38 batch-94 = 1.5227124094963074e-06

Training epoch-38 batch-95
Running loss of epoch-38 batch-95 = 1.444946974515915e-06

Training epoch-38 batch-96
Running loss of epoch-38 batch-96 = 2.7855858206748962e-06

Training epoch-38 batch-97
Running loss of epoch-38 batch-97 = 2.3005297407507896e-05

Training epoch-38 batch-98
Running loss of epoch-38 batch-98 = 2.4728011339902878e-05

Training epoch-38 batch-99
Running loss of epoch-38 batch-99 = 5.507492460310459e-06

Training epoch-38 batch-100
Running loss of epoch-38 batch-100 = 4.63961623609066e-06

Training epoch-38 batch-101
Running loss of epoch-38 batch-101 = 4.666927270591259e-05

Training epoch-38 batch-102
Running loss of epoch-38 batch-102 = 1.9301660358905792e-06

Training epoch-38 batch-103
Running loss of epoch-38 batch-103 = 2.023298293352127e-06

Training epoch-38 batch-104
Running loss of epoch-38 batch-104 = 1.1720694601535797e-06

Training epoch-38 batch-105
Running loss of epoch-38 batch-105 = 4.498986527323723e-06

Training epoch-38 batch-106
Running loss of epoch-38 batch-106 = 1.3129319995641708e-06

Training epoch-38 batch-107
Running loss of epoch-38 batch-107 = 8.942908607423306e-06

Training epoch-38 batch-108
Running loss of epoch-38 batch-108 = 1.4222110621631145e-05

Training epoch-38 batch-109
Running loss of epoch-38 batch-109 = 3.8906000554561615e-07

Training epoch-38 batch-110
Running loss of epoch-38 batch-110 = 5.141133442521095e-06

Training epoch-38 batch-111
Running loss of epoch-38 batch-111 = 2.634129486978054e-05

Training epoch-38 batch-112
Running loss of epoch-38 batch-112 = 1.6478123143315315e-05

Training epoch-38 batch-113
Running loss of epoch-38 batch-113 = 5.227397195994854e-06

Training epoch-38 batch-114
Running loss of epoch-38 batch-114 = 3.2391399145126343e-06

Training epoch-38 batch-115
Running loss of epoch-38 batch-115 = 5.0282105803489685e-06

Training epoch-38 batch-116
Running loss of epoch-38 batch-116 = 4.327390342950821e-06

Training epoch-38 batch-117
Running loss of epoch-38 batch-117 = 9.098596638068557e-05

Training epoch-38 batch-118
Running loss of epoch-38 batch-118 = 3.54227377101779e-05

Training epoch-38 batch-119
Running loss of epoch-38 batch-119 = 2.4330103769898415e-05

Training epoch-38 batch-120
Running loss of epoch-38 batch-120 = 2.408074215054512e-05

Training epoch-38 batch-121
Running loss of epoch-38 batch-121 = 1.1639436706900597e-05

Training epoch-38 batch-122
Running loss of epoch-38 batch-122 = 6.0711754485964775e-06

Training epoch-38 batch-123
Running loss of epoch-38 batch-123 = 3.0635856091976166e-06

Training epoch-38 batch-124
Running loss of epoch-38 batch-124 = 2.1832529455423355e-06

Training epoch-38 batch-125
Running loss of epoch-38 batch-125 = 2.2763852030038834e-06

Training epoch-38 batch-126
Running loss of epoch-38 batch-126 = 7.282942533493042e-07

Training epoch-38 batch-127
Running loss of epoch-38 batch-127 = 6.032525561749935e-06

Training epoch-38 batch-128
Running loss of epoch-38 batch-128 = 4.1716499254107475e-05

Training epoch-38 batch-129
Running loss of epoch-38 batch-129 = 0.0001444817753508687

Training epoch-38 batch-130
Running loss of epoch-38 batch-130 = 5.8675650507211685e-06

Training epoch-38 batch-131
Running loss of epoch-38 batch-131 = 1.8487335182726383e-05

Training epoch-38 batch-132
Running loss of epoch-38 batch-132 = 3.2675452530384064e-06

Training epoch-38 batch-133
Running loss of epoch-38 batch-133 = 2.0547304302453995e-06

Training epoch-38 batch-134
Running loss of epoch-38 batch-134 = 3.560108598321676e-05

Training epoch-38 batch-135
Running loss of epoch-38 batch-135 = 1.7470913007855415e-05

Training epoch-38 batch-136
Running loss of epoch-38 batch-136 = 9.13266558200121e-06

Training epoch-38 batch-137
Running loss of epoch-38 batch-137 = 6.884580943733454e-05

Training epoch-38 batch-138
Running loss of epoch-38 batch-138 = 8.919043466448784e-06

Training epoch-38 batch-139
Running loss of epoch-38 batch-139 = 7.963908137753606e-05

Training epoch-38 batch-140
Running loss of epoch-38 batch-140 = 5.84777444601059e-06

Training epoch-38 batch-141
Running loss of epoch-38 batch-141 = 1.4684395864605904e-05

Training epoch-38 batch-142
Running loss of epoch-38 batch-142 = 3.9497739635407925e-05

Training epoch-38 batch-143
Running loss of epoch-38 batch-143 = 4.295492544770241e-06

Training epoch-38 batch-144
Running loss of epoch-38 batch-144 = 1.1139665730297565e-05

Training epoch-38 batch-145
Running loss of epoch-38 batch-145 = 5.763489753007889e-06

Training epoch-38 batch-146
Running loss of epoch-38 batch-146 = 2.423196565359831e-05

Training epoch-38 batch-147
Running loss of epoch-38 batch-147 = 3.787956666201353e-05

Training epoch-38 batch-148
Running loss of epoch-38 batch-148 = 2.5811605155467987e-06

Training epoch-38 batch-149
Running loss of epoch-38 batch-149 = 9.002513252198696e-06

Training epoch-38 batch-150
Running loss of epoch-38 batch-150 = 5.706213414669037e-06

Training epoch-38 batch-151
Running loss of epoch-38 batch-151 = 6.402959115803242e-06

Training epoch-38 batch-152
Running loss of epoch-38 batch-152 = 5.030306056141853e-06

Training epoch-38 batch-153
Running loss of epoch-38 batch-153 = 2.1769432350993156e-05

Training epoch-38 batch-154
Running loss of epoch-38 batch-154 = 6.954185664653778e-06

Training epoch-38 batch-155
Running loss of epoch-38 batch-155 = 3.339722752571106e-06

Training epoch-38 batch-156
Running loss of epoch-38 batch-156 = 1.1133961379528046e-06

Training epoch-38 batch-157
Running loss of epoch-38 batch-157 = 9.98377799987793e-07

Finished training epoch-38.



Average train loss at epoch-38 = 1.895209774374962e-05

Started Evaluation

Average val loss at epoch-38 = 4.974633825452704

Accuracy for classes:
Accuracy for class equals is: 68.48 %
Accuracy for class main is: 49.18 %
Accuracy for class setUp is: 53.93 %
Accuracy for class onCreate is: 43.07 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 37.44 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.08 %
Accuracy for class execute is: 18.07 %
Accuracy for class get is: 27.95 %

Overall Accuracy = 45.45 %

Finished Evaluation



Started training epoch-39


Training epoch-39 batch-1
Running loss of epoch-39 batch-1 = 3.3634714782238007e-06

Training epoch-39 batch-2
Running loss of epoch-39 batch-2 = 2.7991365641355515e-05

Training epoch-39 batch-3
Running loss of epoch-39 batch-3 = 1.1214637197554111e-05

Training epoch-39 batch-4
Running loss of epoch-39 batch-4 = 1.1628842912614346e-05

Training epoch-39 batch-5
Running loss of epoch-39 batch-5 = 0.00012480741133913398

Training epoch-39 batch-6
Running loss of epoch-39 batch-6 = 9.214738383889198e-06

Training epoch-39 batch-7
Running loss of epoch-39 batch-7 = 4.6794069930911064e-05

Training epoch-39 batch-8
Running loss of epoch-39 batch-8 = 6.338814273476601e-06

Training epoch-39 batch-9
Running loss of epoch-39 batch-9 = 1.1042109690606594e-05

Training epoch-39 batch-10
Running loss of epoch-39 batch-10 = 4.4605694711208344e-05

Training epoch-39 batch-11
Running loss of epoch-39 batch-11 = 4.207249730825424e-06

Training epoch-39 batch-12
Running loss of epoch-39 batch-12 = 4.317611455917358e-06

Training epoch-39 batch-13
Running loss of epoch-39 batch-13 = 3.810855560004711e-06

Training epoch-39 batch-14
Running loss of epoch-39 batch-14 = 3.845198079943657e-05

Training epoch-39 batch-15
Running loss of epoch-39 batch-15 = 1.8415856175124645e-05

Training epoch-39 batch-16
Running loss of epoch-39 batch-16 = 2.0673498511314392e-05

Training epoch-39 batch-17
Running loss of epoch-39 batch-17 = 5.46730007044971e-05

Training epoch-39 batch-18
Running loss of epoch-39 batch-18 = 2.2485037334263325e-05

Training epoch-39 batch-19
Running loss of epoch-39 batch-19 = 1.2426171451807022e-06

Training epoch-39 batch-20
Running loss of epoch-39 batch-20 = 8.170562796294689e-05

Training epoch-39 batch-21
Running loss of epoch-39 batch-21 = 6.852904334664345e-06

Training epoch-39 batch-22
Running loss of epoch-39 batch-22 = 0.0002369732828810811

Training epoch-39 batch-23
Running loss of epoch-39 batch-23 = 2.578494604676962e-05

Training epoch-39 batch-24
Running loss of epoch-39 batch-24 = 3.5402015782892704e-05

Training epoch-39 batch-25
Running loss of epoch-39 batch-25 = 4.851957783102989e-06

Training epoch-39 batch-26
Running loss of epoch-39 batch-26 = 2.7477741241455078e-05

Training epoch-39 batch-27
Running loss of epoch-39 batch-27 = 2.2365129552781582e-05

Training epoch-39 batch-28
Running loss of epoch-39 batch-28 = 2.523197326809168e-05

Training epoch-39 batch-29
Running loss of epoch-39 batch-29 = 4.278961569070816e-06

Training epoch-39 batch-30
Running loss of epoch-39 batch-30 = 4.615401849150658e-06

Training epoch-39 batch-31
Running loss of epoch-39 batch-31 = 1.461012288928032e-06

Training epoch-39 batch-32
Running loss of epoch-39 batch-32 = 7.032998837530613e-06

Training epoch-39 batch-33
Running loss of epoch-39 batch-33 = 1.6994308680295944e-06

Training epoch-39 batch-34
Running loss of epoch-39 batch-34 = 5.2852556109428406e-06

Training epoch-39 batch-35
Running loss of epoch-39 batch-35 = 3.1993258744478226e-06

Training epoch-39 batch-36
Running loss of epoch-39 batch-36 = 8.111819624900818e-07

Training epoch-39 batch-37
Running loss of epoch-39 batch-37 = 9.341281838715076e-06

Training epoch-39 batch-38
Running loss of epoch-39 batch-38 = 1.3010576367378235e-06

Training epoch-39 batch-39
Running loss of epoch-39 batch-39 = 2.0631588995456696e-05

Training epoch-39 batch-40
Running loss of epoch-39 batch-40 = 2.3510539904236794e-05

Training epoch-39 batch-41
Running loss of epoch-39 batch-41 = 1.2035015970468521e-06

Training epoch-39 batch-42
Running loss of epoch-39 batch-42 = 4.231708589941263e-05

Training epoch-39 batch-43
Running loss of epoch-39 batch-43 = 7.201451808214188e-07

Training epoch-39 batch-44
Running loss of epoch-39 batch-44 = 4.836125299334526e-06

Training epoch-39 batch-45
Running loss of epoch-39 batch-45 = 8.149619679898024e-05

Training epoch-39 batch-46
Running loss of epoch-39 batch-46 = 4.5245978981256485e-06

Training epoch-39 batch-47
Running loss of epoch-39 batch-47 = 2.1697254851460457e-05

Training epoch-39 batch-48
Running loss of epoch-39 batch-48 = 5.5857584811747074e-05

Training epoch-39 batch-49
Running loss of epoch-39 batch-49 = 6.006099283695221e-06

Training epoch-39 batch-50
Running loss of epoch-39 batch-50 = 9.078183211386204e-06

Training epoch-39 batch-51
Running loss of epoch-39 batch-51 = 7.378286682069302e-06

Training epoch-39 batch-52
Running loss of epoch-39 batch-52 = 8.896691724658012e-06

Training epoch-39 batch-53
Running loss of epoch-39 batch-53 = 1.836824230849743e-05

Training epoch-39 batch-54
Running loss of epoch-39 batch-54 = 3.00421379506588e-06

Training epoch-39 batch-55
Running loss of epoch-39 batch-55 = 6.0389284044504166e-06

Training epoch-39 batch-56
Running loss of epoch-39 batch-56 = 1.0571093298494816e-05

Training epoch-39 batch-57
Running loss of epoch-39 batch-57 = 8.79575964063406e-06

Training epoch-39 batch-58
Running loss of epoch-39 batch-58 = 2.232380211353302e-06

Training epoch-39 batch-59
Running loss of epoch-39 batch-59 = 6.276881322264671e-06

Training epoch-39 batch-60
Running loss of epoch-39 batch-60 = 1.4605466276407242e-06

Training epoch-39 batch-61
Running loss of epoch-39 batch-61 = 6.164540536701679e-06

Training epoch-39 batch-62
Running loss of epoch-39 batch-62 = 9.440118446946144e-06

Training epoch-39 batch-63
Running loss of epoch-39 batch-63 = 8.903909474611282e-06

Training epoch-39 batch-64
Running loss of epoch-39 batch-64 = 5.742302164435387e-06

Training epoch-39 batch-65
Running loss of epoch-39 batch-65 = 9.604264050722122e-07

Training epoch-39 batch-66
Running loss of epoch-39 batch-66 = 0.00013359298463910818

Training epoch-39 batch-67
Running loss of epoch-39 batch-67 = 1.4852266758680344e-06

Training epoch-39 batch-68
Running loss of epoch-39 batch-68 = 7.3785195127129555e-06

Training epoch-39 batch-69
Running loss of epoch-39 batch-69 = 1.0041054338216782e-05

Training epoch-39 batch-70
Running loss of epoch-39 batch-70 = 1.3033859431743622e-05

Training epoch-39 batch-71
Running loss of epoch-39 batch-71 = 1.0603107511997223e-06

Training epoch-39 batch-72
Running loss of epoch-39 batch-72 = 1.990934833884239e-06

Training epoch-39 batch-73
Running loss of epoch-39 batch-73 = 2.609565854072571e-06

Training epoch-39 batch-74
Running loss of epoch-39 batch-74 = 5.912210326641798e-05

Training epoch-39 batch-75
Running loss of epoch-39 batch-75 = 1.9008293747901917e-06

Training epoch-39 batch-76
Running loss of epoch-39 batch-76 = 4.412606358528137e-06

Training epoch-39 batch-77
Running loss of epoch-39 batch-77 = 1.3877754099667072e-05

Training epoch-39 batch-78
Running loss of epoch-39 batch-78 = 4.9765221774578094e-06

Training epoch-39 batch-79
Running loss of epoch-39 batch-79 = 1.4444463886320591e-05

Training epoch-39 batch-80
Running loss of epoch-39 batch-80 = 3.915047273039818e-06

Training epoch-39 batch-81
Running loss of epoch-39 batch-81 = 1.5007914043962955e-05

Training epoch-39 batch-82
Running loss of epoch-39 batch-82 = 7.177004590630531e-06

Training epoch-39 batch-83
Running loss of epoch-39 batch-83 = 5.269888788461685e-06

Training epoch-39 batch-84
Running loss of epoch-39 batch-84 = 4.201894626021385e-06

Training epoch-39 batch-85
Running loss of epoch-39 batch-85 = 7.146620191633701e-06

Training epoch-39 batch-86
Running loss of epoch-39 batch-86 = 1.4735851436853409e-06

Training epoch-39 batch-87
Running loss of epoch-39 batch-87 = 1.3103708624839783e-06

Training epoch-39 batch-88
Running loss of epoch-39 batch-88 = 9.364448487758636e-07

Training epoch-39 batch-89
Running loss of epoch-39 batch-89 = 4.416797310113907e-06

Training epoch-39 batch-90
Running loss of epoch-39 batch-90 = 1.104909460991621e-05

Training epoch-39 batch-91
Running loss of epoch-39 batch-91 = 2.945307642221451e-07

Training epoch-39 batch-92
Running loss of epoch-39 batch-92 = 2.62516550719738e-06

Training epoch-39 batch-93
Running loss of epoch-39 batch-93 = 7.1766553446650505e-06

Training epoch-39 batch-94
Running loss of epoch-39 batch-94 = 8.23859591037035e-06

Training epoch-39 batch-95
Running loss of epoch-39 batch-95 = 3.6604469642043114e-06

Training epoch-39 batch-96
Running loss of epoch-39 batch-96 = 2.956017851829529e-06

Training epoch-39 batch-97
Running loss of epoch-39 batch-97 = 2.6376917958259583e-05

Training epoch-39 batch-98
Running loss of epoch-39 batch-98 = 5.643931217491627e-06

Training epoch-39 batch-99
Running loss of epoch-39 batch-99 = 2.5134067982435226e-06

Training epoch-39 batch-100
Running loss of epoch-39 batch-100 = 0.00015617557801306248

Training epoch-39 batch-101
Running loss of epoch-39 batch-101 = 6.749643944203854e-06

Training epoch-39 batch-102
Running loss of epoch-39 batch-102 = 1.1139782145619392e-05

Training epoch-39 batch-103
Running loss of epoch-39 batch-103 = 5.461275577545166e-06

Training epoch-39 batch-104
Running loss of epoch-39 batch-104 = 1.0743853636085987e-05

Training epoch-39 batch-105
Running loss of epoch-39 batch-105 = 2.018408849835396e-06

Training epoch-39 batch-106
Running loss of epoch-39 batch-106 = 1.6330741345882416e-06

Training epoch-39 batch-107
Running loss of epoch-39 batch-107 = 5.679519381374121e-05

Training epoch-39 batch-108
Running loss of epoch-39 batch-108 = 7.795169949531555e-07

Training epoch-39 batch-109
Running loss of epoch-39 batch-109 = 3.0051451176404953e-06

Training epoch-39 batch-110
Running loss of epoch-39 batch-110 = 3.261864185333252e-05

Training epoch-39 batch-111
Running loss of epoch-39 batch-111 = 5.941488780081272e-06

Training epoch-39 batch-112
Running loss of epoch-39 batch-112 = 4.113651812076569e-06

Training epoch-39 batch-113
Running loss of epoch-39 batch-113 = 3.256998024880886e-05

Training epoch-39 batch-114
Running loss of epoch-39 batch-114 = 4.047993570566177e-06

Training epoch-39 batch-115
Running loss of epoch-39 batch-115 = 3.64682637155056e-06

Training epoch-39 batch-116
Running loss of epoch-39 batch-116 = 3.5912846215069294e-05

Training epoch-39 batch-117
Running loss of epoch-39 batch-117 = 1.9764993339776993e-06

Training epoch-39 batch-118
Running loss of epoch-39 batch-118 = 1.3955868780612946e-06

Training epoch-39 batch-119
Running loss of epoch-39 batch-119 = 5.592359229922295e-06

Training epoch-39 batch-120
Running loss of epoch-39 batch-120 = 6.309710443019867e-07

Training epoch-39 batch-121
Running loss of epoch-39 batch-121 = 3.534369170665741e-06

Training epoch-39 batch-122
Running loss of epoch-39 batch-122 = 2.825167030096054e-06

Training epoch-39 batch-123
Running loss of epoch-39 batch-123 = 1.1769239790737629e-05

Training epoch-39 batch-124
Running loss of epoch-39 batch-124 = 2.1703424863517284e-05

Training epoch-39 batch-125
Running loss of epoch-39 batch-125 = 8.958857506513596e-06

Training epoch-39 batch-126
Running loss of epoch-39 batch-126 = 4.211557097733021e-06

Training epoch-39 batch-127
Running loss of epoch-39 batch-127 = 2.7475995011627674e-05

Training epoch-39 batch-128
Running loss of epoch-39 batch-128 = 1.1343625374138355e-05

Training epoch-39 batch-129
Running loss of epoch-39 batch-129 = 6.834277883172035e-06

Training epoch-39 batch-130
Running loss of epoch-39 batch-130 = 8.393544703722e-06

Training epoch-39 batch-131
Running loss of epoch-39 batch-131 = 2.8008129447698593e-05

Training epoch-39 batch-132
Running loss of epoch-39 batch-132 = 8.658389560878277e-06

Training epoch-39 batch-133
Running loss of epoch-39 batch-133 = 4.369765520095825e-06

Training epoch-39 batch-134
Running loss of epoch-39 batch-134 = 8.158385753631592e-07

Training epoch-39 batch-135
Running loss of epoch-39 batch-135 = 4.5036431401968e-06

Training epoch-39 batch-136
Running loss of epoch-39 batch-136 = 1.549546141177416e-05

Training epoch-39 batch-137
Running loss of epoch-39 batch-137 = 1.3998826034367085e-05

Training epoch-39 batch-138
Running loss of epoch-39 batch-138 = 1.2274482287466526e-05

Training epoch-39 batch-139
Running loss of epoch-39 batch-139 = 1.7562066204845905e-05

Training epoch-39 batch-140
Running loss of epoch-39 batch-140 = 1.2796022929251194e-05

Training epoch-39 batch-141
Running loss of epoch-39 batch-141 = 4.075118340551853e-06

Training epoch-39 batch-142
Running loss of epoch-39 batch-142 = 2.506561577320099e-05

Training epoch-39 batch-143
Running loss of epoch-39 batch-143 = 1.2537348084151745e-05

Training epoch-39 batch-144
Running loss of epoch-39 batch-144 = 3.6909361369907856e-05

Training epoch-39 batch-145
Running loss of epoch-39 batch-145 = 4.966277629137039e-07

Training epoch-39 batch-146
Running loss of epoch-39 batch-146 = 2.3066997528076172e-05

Training epoch-39 batch-147
Running loss of epoch-39 batch-147 = 1.547369174659252e-05

Training epoch-39 batch-148
Running loss of epoch-39 batch-148 = 4.290486685931683e-06

Training epoch-39 batch-149
Running loss of epoch-39 batch-149 = 5.1836250349879265e-06

Training epoch-39 batch-150
Running loss of epoch-39 batch-150 = 1.0094605386257172e-05

Training epoch-39 batch-151
Running loss of epoch-39 batch-151 = 3.6511337384581566e-06

Training epoch-39 batch-152
Running loss of epoch-39 batch-152 = 3.329350147396326e-05

Training epoch-39 batch-153
Running loss of epoch-39 batch-153 = 3.530876711010933e-06

Training epoch-39 batch-154
Running loss of epoch-39 batch-154 = 1.0317540727555752e-05

Training epoch-39 batch-155
Running loss of epoch-39 batch-155 = 0.0001817577867768705

Training epoch-39 batch-156
Running loss of epoch-39 batch-156 = 4.078377969563007e-06

Training epoch-39 batch-157
Running loss of epoch-39 batch-157 = 1.471489667892456e-06

Finished training epoch-39.



Average train loss at epoch-39 = 1.741473414003849e-05

Started Evaluation

Average val loss at epoch-39 = 4.9825582802295685

Accuracy for classes:
Accuracy for class equals is: 69.47 %
Accuracy for class main is: 49.18 %
Accuracy for class setUp is: 53.77 %
Accuracy for class onCreate is: 44.14 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 34.47 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.30 %
Accuracy for class execute is: 18.47 %
Accuracy for class get is: 28.97 %

Overall Accuracy = 45.62 %

Finished Evaluation



Started training epoch-40


Training epoch-40 batch-1
Running loss of epoch-40 batch-1 = 2.656842116266489e-05

Training epoch-40 batch-2
Running loss of epoch-40 batch-2 = 1.2819655239582062e-06

Training epoch-40 batch-3
Running loss of epoch-40 batch-3 = 2.612592652440071e-06

Training epoch-40 batch-4
Running loss of epoch-40 batch-4 = 1.1264346539974213e-06

Training epoch-40 batch-5
Running loss of epoch-40 batch-5 = 6.662099622189999e-06

Training epoch-40 batch-6
Running loss of epoch-40 batch-6 = 2.788286656141281e-05

Training epoch-40 batch-7
Running loss of epoch-40 batch-7 = 0.00011154939420521259

Training epoch-40 batch-8
Running loss of epoch-40 batch-8 = 1.1700903996825218e-05

Training epoch-40 batch-9
Running loss of epoch-40 batch-9 = 1.108366996049881e-05

Training epoch-40 batch-10
Running loss of epoch-40 batch-10 = 3.3406191505491734e-05

Training epoch-40 batch-11
Running loss of epoch-40 batch-11 = 3.95788811147213e-06

Training epoch-40 batch-12
Running loss of epoch-40 batch-12 = 3.06800939142704e-06

Training epoch-40 batch-13
Running loss of epoch-40 batch-13 = 1.2356322258710861e-06

Training epoch-40 batch-14
Running loss of epoch-40 batch-14 = 7.429113611578941e-05

Training epoch-40 batch-15
Running loss of epoch-40 batch-15 = 1.4237593859434128e-05

Training epoch-40 batch-16
Running loss of epoch-40 batch-16 = 6.560445763170719e-05

Training epoch-40 batch-17
Running loss of epoch-40 batch-17 = 8.227420039474964e-06

Training epoch-40 batch-18
Running loss of epoch-40 batch-18 = 1.5630503185093403e-05

Training epoch-40 batch-19
Running loss of epoch-40 batch-19 = 3.486138302832842e-05

Training epoch-40 batch-20
Running loss of epoch-40 batch-20 = 9.53522976487875e-06

Training epoch-40 batch-21
Running loss of epoch-40 batch-21 = 5.109235644340515e-06

Training epoch-40 batch-22
Running loss of epoch-40 batch-22 = 9.355251677334309e-06

Training epoch-40 batch-23
Running loss of epoch-40 batch-23 = 5.383917596191168e-05

Training epoch-40 batch-24
Running loss of epoch-40 batch-24 = 4.953937605023384e-06

Training epoch-40 batch-25
Running loss of epoch-40 batch-25 = 5.227013025432825e-05

Training epoch-40 batch-26
Running loss of epoch-40 batch-26 = 1.9922154024243355e-05

Training epoch-40 batch-27
Running loss of epoch-40 batch-27 = 2.3262109607458115e-06

Training epoch-40 batch-28
Running loss of epoch-40 batch-28 = 6.519106682389975e-05

Training epoch-40 batch-29
Running loss of epoch-40 batch-29 = 2.998625859618187e-06

Training epoch-40 batch-30
Running loss of epoch-40 batch-30 = 0.00021025235764682293

Training epoch-40 batch-31
Running loss of epoch-40 batch-31 = 4.8345187678933144e-05

Training epoch-40 batch-32
Running loss of epoch-40 batch-32 = 5.461275577545166e-06

Training epoch-40 batch-33
Running loss of epoch-40 batch-33 = 3.2807234674692154e-05

Training epoch-40 batch-34
Running loss of epoch-40 batch-34 = 6.4391642808914185e-06

Training epoch-40 batch-35
Running loss of epoch-40 batch-35 = 9.503215551376343e-06

Training epoch-40 batch-36
Running loss of epoch-40 batch-36 = 6.150687113404274e-06

Training epoch-40 batch-37
Running loss of epoch-40 batch-37 = 4.736939445137978e-06

Training epoch-40 batch-38
Running loss of epoch-40 batch-38 = 1.5597324818372726e-06

Training epoch-40 batch-39
Running loss of epoch-40 batch-39 = 3.548339009284973e-06

Training epoch-40 batch-40
Running loss of epoch-40 batch-40 = 4.026805981993675e-06

Training epoch-40 batch-41
Running loss of epoch-40 batch-41 = 1.6062986105680466e-06

Training epoch-40 batch-42
Running loss of epoch-40 batch-42 = 2.1832529455423355e-06

Training epoch-40 batch-43
Running loss of epoch-40 batch-43 = 1.0831281542778015e-06

Training epoch-40 batch-44
Running loss of epoch-40 batch-44 = 4.310626536607742e-06

Training epoch-40 batch-45
Running loss of epoch-40 batch-45 = 2.643093466758728e-06

Training epoch-40 batch-46
Running loss of epoch-40 batch-46 = 2.311309799551964e-06

Training epoch-40 batch-47
Running loss of epoch-40 batch-47 = 5.159527063369751e-06

Training epoch-40 batch-48
Running loss of epoch-40 batch-48 = 5.618319846689701e-06

Training epoch-40 batch-49
Running loss of epoch-40 batch-49 = 3.3816322684288025e-06

Training epoch-40 batch-50
Running loss of epoch-40 batch-50 = 4.793750122189522e-06

Training epoch-40 batch-51
Running loss of epoch-40 batch-51 = 9.832554496824741e-06

Training epoch-40 batch-52
Running loss of epoch-40 batch-52 = 3.5845907405018806e-05

Training epoch-40 batch-53
Running loss of epoch-40 batch-53 = 5.659181624650955e-06

Training epoch-40 batch-54
Running loss of epoch-40 batch-54 = 3.058928996324539e-06

Training epoch-40 batch-55
Running loss of epoch-40 batch-55 = 9.090406820178032e-06

Training epoch-40 batch-56
Running loss of epoch-40 batch-56 = 2.466142177581787e-06

Training epoch-40 batch-57
Running loss of epoch-40 batch-57 = 3.55718657374382e-06

Training epoch-40 batch-58
Running loss of epoch-40 batch-58 = 1.1601601727306843e-05

Training epoch-40 batch-59
Running loss of epoch-40 batch-59 = 0.00024690700229257345

Training epoch-40 batch-60
Running loss of epoch-40 batch-60 = 1.6489066183567047e-06

Training epoch-40 batch-61
Running loss of epoch-40 batch-61 = 3.903871402144432e-06

Training epoch-40 batch-62
Running loss of epoch-40 batch-62 = 4.2496249079704285e-06

Training epoch-40 batch-63
Running loss of epoch-40 batch-63 = 5.514826625585556e-06

Training epoch-40 batch-64
Running loss of epoch-40 batch-64 = 1.0693329386413097e-05

Training epoch-40 batch-65
Running loss of epoch-40 batch-65 = 4.4300686568021774e-06

Training epoch-40 batch-66
Running loss of epoch-40 batch-66 = 3.0281953513622284e-06

Training epoch-40 batch-67
Running loss of epoch-40 batch-67 = 4.3869949877262115e-06

Training epoch-40 batch-68
Running loss of epoch-40 batch-68 = 3.657420165836811e-06

Training epoch-40 batch-69
Running loss of epoch-40 batch-69 = 7.669441401958466e-07

Training epoch-40 batch-70
Running loss of epoch-40 batch-70 = 2.535758540034294e-06

Training epoch-40 batch-71
Running loss of epoch-40 batch-71 = 5.550333298742771e-06

Training epoch-40 batch-72
Running loss of epoch-40 batch-72 = 5.083158612251282e-06

Training epoch-40 batch-73
Running loss of epoch-40 batch-73 = 3.090989775955677e-05

Training epoch-40 batch-74
Running loss of epoch-40 batch-74 = 3.1333183869719505e-06

Training epoch-40 batch-75
Running loss of epoch-40 batch-75 = 3.996770828962326e-06

Training epoch-40 batch-76
Running loss of epoch-40 batch-76 = 5.419226363301277e-05

Training epoch-40 batch-77
Running loss of epoch-40 batch-77 = 5.109468474984169e-06

Training epoch-40 batch-78
Running loss of epoch-40 batch-78 = 1.5897443518042564e-05

Training epoch-40 batch-79
Running loss of epoch-40 batch-79 = 6.220303475856781e-06

Training epoch-40 batch-80
Running loss of epoch-40 batch-80 = 2.3283064365386963e-06

Training epoch-40 batch-81
Running loss of epoch-40 batch-81 = 1.4924909919500351e-05

Training epoch-40 batch-82
Running loss of epoch-40 batch-82 = 1.265527680516243e-05

Training epoch-40 batch-83
Running loss of epoch-40 batch-83 = 2.4354085326194763e-06

Training epoch-40 batch-84
Running loss of epoch-40 batch-84 = 2.9297079890966415e-06

Training epoch-40 batch-85
Running loss of epoch-40 batch-85 = 1.4470424503087997e-06

Training epoch-40 batch-86
Running loss of epoch-40 batch-86 = 1.2158299796283245e-05

Training epoch-40 batch-87
Running loss of epoch-40 batch-87 = 2.1624844521284103e-05

Training epoch-40 batch-88
Running loss of epoch-40 batch-88 = 3.697117790579796e-06

Training epoch-40 batch-89
Running loss of epoch-40 batch-89 = 1.964624971151352e-06

Training epoch-40 batch-90
Running loss of epoch-40 batch-90 = 1.0886345990002155e-05

Training epoch-40 batch-91
Running loss of epoch-40 batch-91 = 4.673842340707779e-06

Training epoch-40 batch-92
Running loss of epoch-40 batch-92 = 2.1507497876882553e-05

Training epoch-40 batch-93
Running loss of epoch-40 batch-93 = 3.9711594581604e-06

Training epoch-40 batch-94
Running loss of epoch-40 batch-94 = 7.32101034373045e-06

Training epoch-40 batch-95
Running loss of epoch-40 batch-95 = 1.329032238572836e-05

Training epoch-40 batch-96
Running loss of epoch-40 batch-96 = 7.429160177707672e-06

Training epoch-40 batch-97
Running loss of epoch-40 batch-97 = 2.2621825337409973e-06

Training epoch-40 batch-98
Running loss of epoch-40 batch-98 = 1.227203756570816e-05

Training epoch-40 batch-99
Running loss of epoch-40 batch-99 = 1.3168901205062866e-06

Training epoch-40 batch-100
Running loss of epoch-40 batch-100 = 3.404100425541401e-06

Training epoch-40 batch-101
Running loss of epoch-40 batch-101 = 6.695045158267021e-06

Training epoch-40 batch-102
Running loss of epoch-40 batch-102 = 0.00011243484914302826

Training epoch-40 batch-103
Running loss of epoch-40 batch-103 = 2.91608739644289e-06

Training epoch-40 batch-104
Running loss of epoch-40 batch-104 = 2.152286469936371e-06

Training epoch-40 batch-105
Running loss of epoch-40 batch-105 = 3.844965249300003e-06

Training epoch-40 batch-106
Running loss of epoch-40 batch-106 = 2.4097971618175507e-06

Training epoch-40 batch-107
Running loss of epoch-40 batch-107 = 1.9113998860120773e-05

Training epoch-40 batch-108
Running loss of epoch-40 batch-108 = 4.921318031847477e-05

Training epoch-40 batch-109
Running loss of epoch-40 batch-109 = 5.69736585021019e-06

Training epoch-40 batch-110
Running loss of epoch-40 batch-110 = 7.88120087236166e-06

Training epoch-40 batch-111
Running loss of epoch-40 batch-111 = 2.9634800739586353e-05

Training epoch-40 batch-112
Running loss of epoch-40 batch-112 = 1.4504417777061462e-05

Training epoch-40 batch-113
Running loss of epoch-40 batch-113 = 1.168111339211464e-06

Training epoch-40 batch-114
Running loss of epoch-40 batch-114 = 3.1599774956703186e-06

Training epoch-40 batch-115
Running loss of epoch-40 batch-115 = 1.6319099813699722e-06

Training epoch-40 batch-116
Running loss of epoch-40 batch-116 = 3.369757905602455e-06

Training epoch-40 batch-117
Running loss of epoch-40 batch-117 = 2.5045592337846756e-06

Training epoch-40 batch-118
Running loss of epoch-40 batch-118 = 4.4727930799126625e-06

Training epoch-40 batch-119
Running loss of epoch-40 batch-119 = 4.633399657905102e-05

Training epoch-40 batch-120
Running loss of epoch-40 batch-120 = 1.080567017197609e-06

Training epoch-40 batch-121
Running loss of epoch-40 batch-121 = 1.851934939622879e-06

Training epoch-40 batch-122
Running loss of epoch-40 batch-122 = 2.1514715626835823e-05

Training epoch-40 batch-123
Running loss of epoch-40 batch-123 = 1.0350835509598255e-05

Training epoch-40 batch-124
Running loss of epoch-40 batch-124 = 1.0197749361395836e-05

Training epoch-40 batch-125
Running loss of epoch-40 batch-125 = 1.9439728930592537e-05

Training epoch-40 batch-126
Running loss of epoch-40 batch-126 = 3.5022152587771416e-05

Training epoch-40 batch-127
Running loss of epoch-40 batch-127 = 3.8745347410440445e-06

Training epoch-40 batch-128
Running loss of epoch-40 batch-128 = 5.427771247923374e-05

Training epoch-40 batch-129
Running loss of epoch-40 batch-129 = 2.752058207988739e-06

Training epoch-40 batch-130
Running loss of epoch-40 batch-130 = 1.5521305613219738e-05

Training epoch-40 batch-131
Running loss of epoch-40 batch-131 = 2.4638138711452484e-06

Training epoch-40 batch-132
Running loss of epoch-40 batch-132 = 2.203253097832203e-05

Training epoch-40 batch-133
Running loss of epoch-40 batch-133 = 1.7962884157896042e-06

Training epoch-40 batch-134
Running loss of epoch-40 batch-134 = 3.08966264128685e-06

Training epoch-40 batch-135
Running loss of epoch-40 batch-135 = 1.4491379261016846e-06

Training epoch-40 batch-136
Running loss of epoch-40 batch-136 = 3.1705712899565697e-06

Training epoch-40 batch-137
Running loss of epoch-40 batch-137 = 1.3192417100071907e-05

Training epoch-40 batch-138
Running loss of epoch-40 batch-138 = 9.196111932396889e-06

Training epoch-40 batch-139
Running loss of epoch-40 batch-139 = 8.867471478879452e-06

Training epoch-40 batch-140
Running loss of epoch-40 batch-140 = 1.543201506137848e-06

Training epoch-40 batch-141
Running loss of epoch-40 batch-141 = 0.00012652546865865588

Training epoch-40 batch-142
Running loss of epoch-40 batch-142 = 2.0309817045927048e-06

Training epoch-40 batch-143
Running loss of epoch-40 batch-143 = 1.2528616935014725e-06

Training epoch-40 batch-144
Running loss of epoch-40 batch-144 = 8.293194696307182e-06

Training epoch-40 batch-145
Running loss of epoch-40 batch-145 = 4.197936505079269e-06

Training epoch-40 batch-146
Running loss of epoch-40 batch-146 = 3.783963620662689e-06

Training epoch-40 batch-147
Running loss of epoch-40 batch-147 = 3.9280857890844345e-06

Training epoch-40 batch-148
Running loss of epoch-40 batch-148 = 4.4726766645908356e-06

Training epoch-40 batch-149
Running loss of epoch-40 batch-149 = 1.1207652278244495e-05

Training epoch-40 batch-150
Running loss of epoch-40 batch-150 = 1.7287442460656166e-05

Training epoch-40 batch-151
Running loss of epoch-40 batch-151 = 3.3120159059762955e-06

Training epoch-40 batch-152
Running loss of epoch-40 batch-152 = 6.862916052341461e-06

Training epoch-40 batch-153
Running loss of epoch-40 batch-153 = 6.615300662815571e-06

Training epoch-40 batch-154
Running loss of epoch-40 batch-154 = 1.7294310964643955e-05

Training epoch-40 batch-155
Running loss of epoch-40 batch-155 = 2.3057684302330017e-05

Training epoch-40 batch-156
Running loss of epoch-40 batch-156 = 2.157897688448429e-05

Training epoch-40 batch-157
Running loss of epoch-40 batch-157 = 8.892267942428589e-06

Finished training epoch-40.



Average train loss at epoch-40 = 1.6253359988331796e-05

Started Evaluation

Average val loss at epoch-40 = 5.0071665942668915

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 47.05 %
Accuracy for class setUp is: 54.26 %
Accuracy for class onCreate is: 44.46 %
Accuracy for class toString is: 43.00 %
Accuracy for class run is: 34.47 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 23.09 %
Accuracy for class execute is: 19.28 %
Accuracy for class get is: 26.92 %

Overall Accuracy = 45.41 %

Finished Evaluation



Started training epoch-41


Training epoch-41 batch-1
Running loss of epoch-41 batch-1 = 4.403013736009598e-05

Training epoch-41 batch-2
Running loss of epoch-41 batch-2 = 3.964640200138092e-06

Training epoch-41 batch-3
Running loss of epoch-41 batch-3 = 3.07522714138031e-06

Training epoch-41 batch-4
Running loss of epoch-41 batch-4 = 9.650480933487415e-06

Training epoch-41 batch-5
Running loss of epoch-41 batch-5 = 2.3524975404143333e-05

Training epoch-41 batch-6
Running loss of epoch-41 batch-6 = 6.0248421505093575e-06

Training epoch-41 batch-7
Running loss of epoch-41 batch-7 = 2.4936161935329437e-06

Training epoch-41 batch-8
Running loss of epoch-41 batch-8 = 5.9413956478238106e-05

Training epoch-41 batch-9
Running loss of epoch-41 batch-9 = 2.305256202816963e-06

Training epoch-41 batch-10
Running loss of epoch-41 batch-10 = 1.6524572856724262e-05

Training epoch-41 batch-11
Running loss of epoch-41 batch-11 = 2.1238811314105988e-06

Training epoch-41 batch-12
Running loss of epoch-41 batch-12 = 2.798996865749359e-05

Training epoch-41 batch-13
Running loss of epoch-41 batch-13 = 3.847875632345676e-06

Training epoch-41 batch-14
Running loss of epoch-41 batch-14 = 7.239985279738903e-06

Training epoch-41 batch-15
Running loss of epoch-41 batch-15 = 2.552173100411892e-06

Training epoch-41 batch-16
Running loss of epoch-41 batch-16 = 5.343230441212654e-06

Training epoch-41 batch-17
Running loss of epoch-41 batch-17 = 8.757226169109344e-06

Training epoch-41 batch-18
Running loss of epoch-41 batch-18 = 5.364883691072464e-06

Training epoch-41 batch-19
Running loss of epoch-41 batch-19 = 2.3506581783294678e-06

Training epoch-41 batch-20
Running loss of epoch-41 batch-20 = 1.0835705325007439e-05

Training epoch-41 batch-21
Running loss of epoch-41 batch-21 = 3.4086406230926514e-06

Training epoch-41 batch-22
Running loss of epoch-41 batch-22 = 8.982489816844463e-06

Training epoch-41 batch-23
Running loss of epoch-41 batch-23 = 5.7426514104008675e-06

Training epoch-41 batch-24
Running loss of epoch-41 batch-24 = 5.77408354729414e-06

Training epoch-41 batch-25
Running loss of epoch-41 batch-25 = 1.032138243317604e-06

Training epoch-41 batch-26
Running loss of epoch-41 batch-26 = 6.217043846845627e-06

Training epoch-41 batch-27
Running loss of epoch-41 batch-27 = 6.027985364198685e-06

Training epoch-41 batch-28
Running loss of epoch-41 batch-28 = 3.972090780735016e-06

Training epoch-41 batch-29
Running loss of epoch-41 batch-29 = 8.075730875134468e-06

Training epoch-41 batch-30
Running loss of epoch-41 batch-30 = 2.5916611775755882e-05

Training epoch-41 batch-31
Running loss of epoch-41 batch-31 = 2.0589563064277172e-05

Training epoch-41 batch-32
Running loss of epoch-41 batch-32 = 6.349058821797371e-06

Training epoch-41 batch-33
Running loss of epoch-41 batch-33 = 1.6731210052967072e-06

Training epoch-41 batch-34
Running loss of epoch-41 batch-34 = 2.339715138077736e-06

Training epoch-41 batch-35
Running loss of epoch-41 batch-35 = 2.0423904061317444e-06

Training epoch-41 batch-36
Running loss of epoch-41 batch-36 = 4.9710506573319435e-06

Training epoch-41 batch-37
Running loss of epoch-41 batch-37 = 9.266659617424011e-07

Training epoch-41 batch-38
Running loss of epoch-41 batch-38 = 6.381538696587086e-06

Training epoch-41 batch-39
Running loss of epoch-41 batch-39 = 5.830428563058376e-06

Training epoch-41 batch-40
Running loss of epoch-41 batch-40 = 3.068591468036175e-05

Training epoch-41 batch-41
Running loss of epoch-41 batch-41 = 1.4208606444299221e-05

Training epoch-41 batch-42
Running loss of epoch-41 batch-42 = 4.143686965107918e-06

Training epoch-41 batch-43
Running loss of epoch-41 batch-43 = 7.144949631765485e-05

Training epoch-41 batch-44
Running loss of epoch-41 batch-44 = 1.0325806215405464e-05

Training epoch-41 batch-45
Running loss of epoch-41 batch-45 = 5.12425322085619e-06

Training epoch-41 batch-46
Running loss of epoch-41 batch-46 = 2.6274938136339188e-06

Training epoch-41 batch-47
Running loss of epoch-41 batch-47 = 3.300898242741823e-05

Training epoch-41 batch-48
Running loss of epoch-41 batch-48 = 6.388407200574875e-06

Training epoch-41 batch-49
Running loss of epoch-41 batch-49 = 4.0054554119706154e-05

Training epoch-41 batch-50
Running loss of epoch-41 batch-50 = 2.3476313799619675e-06

Training epoch-41 batch-51
Running loss of epoch-41 batch-51 = 1.67149119079113e-06

Training epoch-41 batch-52
Running loss of epoch-41 batch-52 = 8.906936272978783e-06

Training epoch-41 batch-53
Running loss of epoch-41 batch-53 = 1.6218633390963078e-05

Training epoch-41 batch-54
Running loss of epoch-41 batch-54 = 2.0207371562719345e-06

Training epoch-41 batch-55
Running loss of epoch-41 batch-55 = 1.1371448636054993e-06

Training epoch-41 batch-56
Running loss of epoch-41 batch-56 = 5.288049578666687e-06

Training epoch-41 batch-57
Running loss of epoch-41 batch-57 = 5.961768329143524e-05

Training epoch-41 batch-58
Running loss of epoch-41 batch-58 = 2.9528746381402016e-05

Training epoch-41 batch-59
Running loss of epoch-41 batch-59 = 4.2549800127744675e-06

Training epoch-41 batch-60
Running loss of epoch-41 batch-60 = 3.6599813029170036e-06

Training epoch-41 batch-61
Running loss of epoch-41 batch-61 = 1.0707881301641464e-05

Training epoch-41 batch-62
Running loss of epoch-41 batch-62 = 3.048917278647423e-06

Training epoch-41 batch-63
Running loss of epoch-41 batch-63 = 2.834480255842209e-06

Training epoch-41 batch-64
Running loss of epoch-41 batch-64 = 2.7248519472777843e-05

Training epoch-41 batch-65
Running loss of epoch-41 batch-65 = 4.486879333853722e-06

Training epoch-41 batch-66
Running loss of epoch-41 batch-66 = 3.3618765883147717e-05

Training epoch-41 batch-67
Running loss of epoch-41 batch-67 = 2.0314473658800125e-06

Training epoch-41 batch-68
Running loss of epoch-41 batch-68 = 3.0603259801864624e-06

Training epoch-41 batch-69
Running loss of epoch-41 batch-69 = 3.2203970476984978e-06

Training epoch-41 batch-70
Running loss of epoch-41 batch-70 = 3.41096892952919e-06

Training epoch-41 batch-71
Running loss of epoch-41 batch-71 = 1.9919476471841335e-05

Training epoch-41 batch-72
Running loss of epoch-41 batch-72 = 4.9907248467206955e-06

Training epoch-41 batch-73
Running loss of epoch-41 batch-73 = 3.02543630823493e-05

Training epoch-41 batch-74
Running loss of epoch-41 batch-74 = 1.0298099368810654e-06

Training epoch-41 batch-75
Running loss of epoch-41 batch-75 = 5.683279596269131e-06

Training epoch-41 batch-76
Running loss of epoch-41 batch-76 = 3.1867530196905136e-06

Training epoch-41 batch-77
Running loss of epoch-41 batch-77 = 8.400529623031616e-07

Training epoch-41 batch-78
Running loss of epoch-41 batch-78 = 1.8910504877567291e-06

Training epoch-41 batch-79
Running loss of epoch-41 batch-79 = 1.671537756919861e-05

Training epoch-41 batch-80
Running loss of epoch-41 batch-80 = 0.0003254161565564573

Training epoch-41 batch-81
Running loss of epoch-41 batch-81 = 3.6514829844236374e-06

Training epoch-41 batch-82
Running loss of epoch-41 batch-82 = 1.0016257874667645e-05

Training epoch-41 batch-83
Running loss of epoch-41 batch-83 = 8.506118319928646e-06

Training epoch-41 batch-84
Running loss of epoch-41 batch-84 = 1.594854984432459e-05

Training epoch-41 batch-85
Running loss of epoch-41 batch-85 = 5.453126505017281e-06

Training epoch-41 batch-86
Running loss of epoch-41 batch-86 = 1.882167998701334e-05

Training epoch-41 batch-87
Running loss of epoch-41 batch-87 = 2.7032336220145226e-05

Training epoch-41 batch-88
Running loss of epoch-41 batch-88 = 6.061396561563015e-06

Training epoch-41 batch-89
Running loss of epoch-41 batch-89 = 6.761983968317509e-06

Training epoch-41 batch-90
Running loss of epoch-41 batch-90 = 6.527407094836235e-06

Training epoch-41 batch-91
Running loss of epoch-41 batch-91 = 1.138029620051384e-05

Training epoch-41 batch-92
Running loss of epoch-41 batch-92 = 4.038331098854542e-06

Training epoch-41 batch-93
Running loss of epoch-41 batch-93 = 1.8561258912086487e-06

Training epoch-41 batch-94
Running loss of epoch-41 batch-94 = 3.67872416973114e-06

Training epoch-41 batch-95
Running loss of epoch-41 batch-95 = 1.789303496479988e-06

Training epoch-41 batch-96
Running loss of epoch-41 batch-96 = 1.8202699720859528e-06

Training epoch-41 batch-97
Running loss of epoch-41 batch-97 = 2.0314473658800125e-06

Training epoch-41 batch-98
Running loss of epoch-41 batch-98 = 3.917503636330366e-05

Training epoch-41 batch-99
Running loss of epoch-41 batch-99 = 2.044485881924629e-06

Training epoch-41 batch-100
Running loss of epoch-41 batch-100 = 4.0316954255104065e-06

Training epoch-41 batch-101
Running loss of epoch-41 batch-101 = 1.3326527550816536e-05

Training epoch-41 batch-102
Running loss of epoch-41 batch-102 = 1.2966454960405827e-05

Training epoch-41 batch-103
Running loss of epoch-41 batch-103 = 1.0600779205560684e-06

Training epoch-41 batch-104
Running loss of epoch-41 batch-104 = 4.9528200179338455e-05

Training epoch-41 batch-105
Running loss of epoch-41 batch-105 = 4.877941682934761e-05

Training epoch-41 batch-106
Running loss of epoch-41 batch-106 = 0.00022379262372851372

Training epoch-41 batch-107
Running loss of epoch-41 batch-107 = 6.619375199079514e-07

Training epoch-41 batch-108
Running loss of epoch-41 batch-108 = 3.7134159356355667e-06

Training epoch-41 batch-109
Running loss of epoch-41 batch-109 = 1.0523130185902119e-05

Training epoch-41 batch-110
Running loss of epoch-41 batch-110 = 2.587446942925453e-06

Training epoch-41 batch-111
Running loss of epoch-41 batch-111 = 1.2975651770830154e-06

Training epoch-41 batch-112
Running loss of epoch-41 batch-112 = 2.4655833840370178e-05

Training epoch-41 batch-113
Running loss of epoch-41 batch-113 = 8.823582902550697e-06

Training epoch-41 batch-114
Running loss of epoch-41 batch-114 = 4.07086918130517e-05

Training epoch-41 batch-115
Running loss of epoch-41 batch-115 = 2.0756851881742477e-06

Training epoch-41 batch-116
Running loss of epoch-41 batch-116 = 1.4701858162879944e-05

Training epoch-41 batch-117
Running loss of epoch-41 batch-117 = 4.392582923173904e-06

Training epoch-41 batch-118
Running loss of epoch-41 batch-118 = 3.098277375102043e-06

Training epoch-41 batch-119
Running loss of epoch-41 batch-119 = 6.135902367532253e-06

Training epoch-41 batch-120
Running loss of epoch-41 batch-120 = 1.6938778571784496e-05

Training epoch-41 batch-121
Running loss of epoch-41 batch-121 = 5.835667252540588e-06

Training epoch-41 batch-122
Running loss of epoch-41 batch-122 = 6.286194548010826e-06

Training epoch-41 batch-123
Running loss of epoch-41 batch-123 = 6.535905413329601e-06

Training epoch-41 batch-124
Running loss of epoch-41 batch-124 = 2.246582880616188e-06

Training epoch-41 batch-125
Running loss of epoch-41 batch-125 = 1.0104849934577942e-06

Training epoch-41 batch-126
Running loss of epoch-41 batch-126 = 4.240160342305899e-05

Training epoch-41 batch-127
Running loss of epoch-41 batch-127 = 4.285597242414951e-06

Training epoch-41 batch-128
Running loss of epoch-41 batch-128 = 1.6852281987667084e-06

Training epoch-41 batch-129
Running loss of epoch-41 batch-129 = 1.3401731848716736e-06

Training epoch-41 batch-130
Running loss of epoch-41 batch-130 = 4.257308319211006e-06

Training epoch-41 batch-131
Running loss of epoch-41 batch-131 = 7.660244591534138e-06

Training epoch-41 batch-132
Running loss of epoch-41 batch-132 = 3.9068167097866535e-05

Training epoch-41 batch-133
Running loss of epoch-41 batch-133 = 4.596449434757233e-05

Training epoch-41 batch-134
Running loss of epoch-41 batch-134 = 2.1706800907850266e-06

Training epoch-41 batch-135
Running loss of epoch-41 batch-135 = 3.016169648617506e-05

Training epoch-41 batch-136
Running loss of epoch-41 batch-136 = 3.2277312129735947e-06

Training epoch-41 batch-137
Running loss of epoch-41 batch-137 = 5.266629159450531e-07

Training epoch-41 batch-138
Running loss of epoch-41 batch-138 = 1.776963472366333e-06

Training epoch-41 batch-139
Running loss of epoch-41 batch-139 = 2.3758038878440857e-06

Training epoch-41 batch-140
Running loss of epoch-41 batch-140 = 8.926494047045708e-06

Training epoch-41 batch-141
Running loss of epoch-41 batch-141 = 2.1457672119140625e-06

Training epoch-41 batch-142
Running loss of epoch-41 batch-142 = 2.2831372916698456e-06

Training epoch-41 batch-143
Running loss of epoch-41 batch-143 = 2.396421041339636e-05

Training epoch-41 batch-144
Running loss of epoch-41 batch-144 = 4.932167939841747e-06

Training epoch-41 batch-145
Running loss of epoch-41 batch-145 = 2.0526815205812454e-05

Training epoch-41 batch-146
Running loss of epoch-41 batch-146 = 2.3026950657367706e-06

Training epoch-41 batch-147
Running loss of epoch-41 batch-147 = 4.698056727647781e-06

Training epoch-41 batch-148
Running loss of epoch-41 batch-148 = 2.268387470394373e-05

Training epoch-41 batch-149
Running loss of epoch-41 batch-149 = 6.509944796562195e-06

Training epoch-41 batch-150
Running loss of epoch-41 batch-150 = 1.3934215530753136e-05

Training epoch-41 batch-151
Running loss of epoch-41 batch-151 = 5.130539648234844e-06

Training epoch-41 batch-152
Running loss of epoch-41 batch-152 = 0.00010006997035816312

Training epoch-41 batch-153
Running loss of epoch-41 batch-153 = 6.100279279053211e-06

Training epoch-41 batch-154
Running loss of epoch-41 batch-154 = 1.2775417417287827e-06

Training epoch-41 batch-155
Running loss of epoch-41 batch-155 = 2.4720444343984127e-05

Training epoch-41 batch-156
Running loss of epoch-41 batch-156 = 1.8799793906509876e-05

Training epoch-41 batch-157
Running loss of epoch-41 batch-157 = 6.59748911857605e-06

Finished training epoch-41.



Average train loss at epoch-41 = 1.5230705216526985e-05

Started Evaluation

Average val loss at epoch-41 = 5.037180001798429

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 48.36 %
Accuracy for class setUp is: 53.28 %
Accuracy for class onCreate is: 44.24 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 34.93 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.97 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 28.21 %

Overall Accuracy = 45.49 %

Finished Evaluation



Started training epoch-42


Training epoch-42 batch-1
Running loss of epoch-42 batch-1 = 1.685775350779295e-05

Training epoch-42 batch-2
Running loss of epoch-42 batch-2 = 1.491047441959381e-06

Training epoch-42 batch-3
Running loss of epoch-42 batch-3 = 2.086162567138672e-06

Training epoch-42 batch-4
Running loss of epoch-42 batch-4 = 5.662441253662109e-07

Training epoch-42 batch-5
Running loss of epoch-42 batch-5 = 9.090290404856205e-06

Training epoch-42 batch-6
Running loss of epoch-42 batch-6 = 3.937399014830589e-06

Training epoch-42 batch-7
Running loss of epoch-42 batch-7 = 2.991943620145321e-05

Training epoch-42 batch-8
Running loss of epoch-42 batch-8 = 6.719143129885197e-06

Training epoch-42 batch-9
Running loss of epoch-42 batch-9 = 2.2855587303638458e-05

Training epoch-42 batch-10
Running loss of epoch-42 batch-10 = 2.0489096641540527e-06

Training epoch-42 batch-11
Running loss of epoch-42 batch-11 = 1.6076955944299698e-06

Training epoch-42 batch-12
Running loss of epoch-42 batch-12 = 2.6261783204972744e-05

Training epoch-42 batch-13
Running loss of epoch-42 batch-13 = 2.9080547392368317e-06

Training epoch-42 batch-14
Running loss of epoch-42 batch-14 = 1.1313124559819698e-05

Training epoch-42 batch-15
Running loss of epoch-42 batch-15 = 6.831600330770016e-06

Training epoch-42 batch-16
Running loss of epoch-42 batch-16 = 7.029855623841286e-06

Training epoch-42 batch-17
Running loss of epoch-42 batch-17 = 4.016794264316559e-06

Training epoch-42 batch-18
Running loss of epoch-42 batch-18 = 2.8084032237529755e-06

Training epoch-42 batch-19
Running loss of epoch-42 batch-19 = 2.3269210942089558e-05

Training epoch-42 batch-20
Running loss of epoch-42 batch-20 = 1.1845026165246964e-05

Training epoch-42 batch-21
Running loss of epoch-42 batch-21 = 0.0001649975311011076

Training epoch-42 batch-22
Running loss of epoch-42 batch-22 = 2.3906701244413853e-05

Training epoch-42 batch-23
Running loss of epoch-42 batch-23 = 6.402377039194107e-06

Training epoch-42 batch-24
Running loss of epoch-42 batch-24 = 9.072478860616684e-06

Training epoch-42 batch-25
Running loss of epoch-42 batch-25 = 1.6738194972276688e-06

Training epoch-42 batch-26
Running loss of epoch-42 batch-26 = 2.1406449377536774e-06

Training epoch-42 batch-27
Running loss of epoch-42 batch-27 = 2.769753336906433e-06

Training epoch-42 batch-28
Running loss of epoch-42 batch-28 = 2.4442095309495926e-05

Training epoch-42 batch-29
Running loss of epoch-42 batch-29 = 1.7934944480657578e-06

Training epoch-42 batch-30
Running loss of epoch-42 batch-30 = 1.7564743757247925e-06

Training epoch-42 batch-31
Running loss of epoch-42 batch-31 = 8.51508229970932e-06

Training epoch-42 batch-32
Running loss of epoch-42 batch-32 = 9.21427272260189e-06

Training epoch-42 batch-33
Running loss of epoch-42 batch-33 = 9.75094735622406e-07

Training epoch-42 batch-34
Running loss of epoch-42 batch-34 = 2.2316817194223404e-06

Training epoch-42 batch-35
Running loss of epoch-42 batch-35 = 8.083879947662354e-07

Training epoch-42 batch-36
Running loss of epoch-42 batch-36 = 8.13906081020832e-06

Training epoch-42 batch-37
Running loss of epoch-42 batch-37 = 2.546585164964199e-05

Training epoch-42 batch-38
Running loss of epoch-42 batch-38 = 1.352047547698021e-06

Training epoch-42 batch-39
Running loss of epoch-42 batch-39 = 0.00011223781621083617

Training epoch-42 batch-40
Running loss of epoch-42 batch-40 = 1.0291114449501038e-06

Training epoch-42 batch-41
Running loss of epoch-42 batch-41 = 1.1168885976076126e-06

Training epoch-42 batch-42
Running loss of epoch-42 batch-42 = 5.583278834819794e-07

Training epoch-42 batch-43
Running loss of epoch-42 batch-43 = 1.2381933629512787e-06

Training epoch-42 batch-44
Running loss of epoch-42 batch-44 = 1.253222580999136e-05

Training epoch-42 batch-45
Running loss of epoch-42 batch-45 = 7.116427877917886e-05

Training epoch-42 batch-46
Running loss of epoch-42 batch-46 = 6.8837543949484825e-06

Training epoch-42 batch-47
Running loss of epoch-42 batch-47 = 1.0138261131942272e-05

Training epoch-42 batch-48
Running loss of epoch-42 batch-48 = 2.181215677410364e-05

Training epoch-42 batch-49
Running loss of epoch-42 batch-49 = 1.6118865460157394e-06

Training epoch-42 batch-50
Running loss of epoch-42 batch-50 = 1.8520047888159752e-05

Training epoch-42 batch-51
Running loss of epoch-42 batch-51 = 4.885951057076454e-06

Training epoch-42 batch-52
Running loss of epoch-42 batch-52 = 4.3031759560108185e-06

Training epoch-42 batch-53
Running loss of epoch-42 batch-53 = 9.426148608326912e-06

Training epoch-42 batch-54
Running loss of epoch-42 batch-54 = 3.133714199066162e-05

Training epoch-42 batch-55
Running loss of epoch-42 batch-55 = 2.045184373855591e-06

Training epoch-42 batch-56
Running loss of epoch-42 batch-56 = 2.2423919290304184e-06

Training epoch-42 batch-57
Running loss of epoch-42 batch-57 = 2.3497268557548523e-06

Training epoch-42 batch-58
Running loss of epoch-42 batch-58 = 4.751840606331825e-06

Training epoch-42 batch-59
Running loss of epoch-42 batch-59 = 9.456963744014502e-05

Training epoch-42 batch-60
Running loss of epoch-42 batch-60 = 1.9908882677555084e-05

Training epoch-42 batch-61
Running loss of epoch-42 batch-61 = 2.0614825189113617e-05

Training epoch-42 batch-62
Running loss of epoch-42 batch-62 = 3.0063092708587646e-06

Training epoch-42 batch-63
Running loss of epoch-42 batch-63 = 3.7582358345389366e-06

Training epoch-42 batch-64
Running loss of epoch-42 batch-64 = 2.0917505025863647e-06

Training epoch-42 batch-65
Running loss of epoch-42 batch-65 = 5.1551032811403275e-06

Training epoch-42 batch-66
Running loss of epoch-42 batch-66 = 1.1074123904109001e-05

Training epoch-42 batch-67
Running loss of epoch-42 batch-67 = 8.0084428191185e-06

Training epoch-42 batch-68
Running loss of epoch-42 batch-68 = 2.9848888516426086e-06

Training epoch-42 batch-69
Running loss of epoch-42 batch-69 = 5.527865141630173e-06

Training epoch-42 batch-70
Running loss of epoch-42 batch-70 = 3.493740223348141e-05

Training epoch-42 batch-71
Running loss of epoch-42 batch-71 = 3.200024366378784e-06

Training epoch-42 batch-72
Running loss of epoch-42 batch-72 = 1.279544085264206e-05

Training epoch-42 batch-73
Running loss of epoch-42 batch-73 = 5.610054358839989e-06

Training epoch-42 batch-74
Running loss of epoch-42 batch-74 = 4.4834217987954617e-05

Training epoch-42 batch-75
Running loss of epoch-42 batch-75 = 1.5078112483024597e-06

Training epoch-42 batch-76
Running loss of epoch-42 batch-76 = 1.3944227248430252e-06

Training epoch-42 batch-77
Running loss of epoch-42 batch-77 = 1.4959718100726604e-05

Training epoch-42 batch-78
Running loss of epoch-42 batch-78 = 2.0723091438412666e-05

Training epoch-42 batch-79
Running loss of epoch-42 batch-79 = 1.3883691281080246e-05

Training epoch-42 batch-80
Running loss of epoch-42 batch-80 = 1.7996528185904026e-05

Training epoch-42 batch-81
Running loss of epoch-42 batch-81 = 4.3907202780246735e-06

Training epoch-42 batch-82
Running loss of epoch-42 batch-82 = 1.687789335846901e-06

Training epoch-42 batch-83
Running loss of epoch-42 batch-83 = 9.447801858186722e-06

Training epoch-42 batch-84
Running loss of epoch-42 batch-84 = 6.413320079445839e-06

Training epoch-42 batch-85
Running loss of epoch-42 batch-85 = 1.1809170246124268e-06

Training epoch-42 batch-86
Running loss of epoch-42 batch-86 = 3.9959559217095375e-06

Training epoch-42 batch-87
Running loss of epoch-42 batch-87 = 1.1509167961776257e-05

Training epoch-42 batch-88
Running loss of epoch-42 batch-88 = 1.7071142792701721e-06

Training epoch-42 batch-89
Running loss of epoch-42 batch-89 = 1.081626396626234e-05

Training epoch-42 batch-90
Running loss of epoch-42 batch-90 = 1.6995822079479694e-05

Training epoch-42 batch-91
Running loss of epoch-42 batch-91 = 1.6065314412117004e-06

Training epoch-42 batch-92
Running loss of epoch-42 batch-92 = 6.132526323199272e-06

Training epoch-42 batch-93
Running loss of epoch-42 batch-93 = 1.4111865311861038e-06

Training epoch-42 batch-94
Running loss of epoch-42 batch-94 = 1.563248224556446e-05

Training epoch-42 batch-95
Running loss of epoch-42 batch-95 = 7.097609341144562e-06

Training epoch-42 batch-96
Running loss of epoch-42 batch-96 = 8.319038897752762e-07

Training epoch-42 batch-97
Running loss of epoch-42 batch-97 = 8.191954111680388e-05

Training epoch-42 batch-98
Running loss of epoch-42 batch-98 = 8.607166819274426e-06

Training epoch-42 batch-99
Running loss of epoch-42 batch-99 = 1.8222490325570107e-05

Training epoch-42 batch-100
Running loss of epoch-42 batch-100 = 2.5460030883550644e-06

Training epoch-42 batch-101
Running loss of epoch-42 batch-101 = 2.1818559616804123e-06

Training epoch-42 batch-102
Running loss of epoch-42 batch-102 = 3.357185050845146e-06

Training epoch-42 batch-103
Running loss of epoch-42 batch-103 = 3.5061966627836227e-06

Training epoch-42 batch-104
Running loss of epoch-42 batch-104 = 3.965804353356361e-06

Training epoch-42 batch-105
Running loss of epoch-42 batch-105 = 3.933673724532127e-06

Training epoch-42 batch-106
Running loss of epoch-42 batch-106 = 1.0670628398656845e-06

Training epoch-42 batch-107
Running loss of epoch-42 batch-107 = 7.313210517168045e-07

Training epoch-42 batch-108
Running loss of epoch-42 batch-108 = 8.075381629168987e-06

Training epoch-42 batch-109
Running loss of epoch-42 batch-109 = 1.0854564607143402e-06

Training epoch-42 batch-110
Running loss of epoch-42 batch-110 = 0.00012720876839011908

Training epoch-42 batch-111
Running loss of epoch-42 batch-111 = 8.577480912208557e-07

Training epoch-42 batch-112
Running loss of epoch-42 batch-112 = 2.2827298380434513e-05

Training epoch-42 batch-113
Running loss of epoch-42 batch-113 = 2.437969669699669e-06

Training epoch-42 batch-114
Running loss of epoch-42 batch-114 = 4.691537469625473e-07

Training epoch-42 batch-115
Running loss of epoch-42 batch-115 = 1.5415716916322708e-06

Training epoch-42 batch-116
Running loss of epoch-42 batch-116 = 2.260901965200901e-05

Training epoch-42 batch-117
Running loss of epoch-42 batch-117 = 1.3462267816066742e-06

Training epoch-42 batch-118
Running loss of epoch-42 batch-118 = 4.86371573060751e-06

Training epoch-42 batch-119
Running loss of epoch-42 batch-119 = 2.259388566017151e-06

Training epoch-42 batch-120
Running loss of epoch-42 batch-120 = 4.029134288430214e-06

Training epoch-42 batch-121
Running loss of epoch-42 batch-121 = 6.181653589010239e-06

Training epoch-42 batch-122
Running loss of epoch-42 batch-122 = 1.0314746759831905e-05

Training epoch-42 batch-123
Running loss of epoch-42 batch-123 = 4.1699036955833435e-05

Training epoch-42 batch-124
Running loss of epoch-42 batch-124 = 7.579568773508072e-06

Training epoch-42 batch-125
Running loss of epoch-42 batch-125 = 1.0496587492525578e-05

Training epoch-42 batch-126
Running loss of epoch-42 batch-126 = 3.9698672480881214e-05

Training epoch-42 batch-127
Running loss of epoch-42 batch-127 = 7.403199560940266e-06

Training epoch-42 batch-128
Running loss of epoch-42 batch-128 = 1.6042031347751617e-06

Training epoch-42 batch-129
Running loss of epoch-42 batch-129 = 6.216112524271011e-06

Training epoch-42 batch-130
Running loss of epoch-42 batch-130 = 6.957293953746557e-05

Training epoch-42 batch-131
Running loss of epoch-42 batch-131 = 8.477596566081047e-06

Training epoch-42 batch-132
Running loss of epoch-42 batch-132 = 4.359404556453228e-06

Training epoch-42 batch-133
Running loss of epoch-42 batch-133 = 5.192309617996216e-05

Training epoch-42 batch-134
Running loss of epoch-42 batch-134 = 3.311736509203911e-05

Training epoch-42 batch-135
Running loss of epoch-42 batch-135 = 1.3704411685466766e-06

Training epoch-42 batch-136
Running loss of epoch-42 batch-136 = 3.2815150916576385e-06

Training epoch-42 batch-137
Running loss of epoch-42 batch-137 = 2.2226013243198395e-06

Training epoch-42 batch-138
Running loss of epoch-42 batch-138 = 3.1938543543219566e-06

Training epoch-42 batch-139
Running loss of epoch-42 batch-139 = 1.2079370208084583e-05

Training epoch-42 batch-140
Running loss of epoch-42 batch-140 = 9.494833648204803e-06

Training epoch-42 batch-141
Running loss of epoch-42 batch-141 = 2.4973880499601364e-05

Training epoch-42 batch-142
Running loss of epoch-42 batch-142 = 3.372738137841225e-05

Training epoch-42 batch-143
Running loss of epoch-42 batch-143 = 6.524031050503254e-06

Training epoch-42 batch-144
Running loss of epoch-42 batch-144 = 4.8908405005931854e-06

Training epoch-42 batch-145
Running loss of epoch-42 batch-145 = 4.230299964547157e-06

Training epoch-42 batch-146
Running loss of epoch-42 batch-146 = 6.395747186616063e-05

Training epoch-42 batch-147
Running loss of epoch-42 batch-147 = 5.025183781981468e-06

Training epoch-42 batch-148
Running loss of epoch-42 batch-148 = 1.3399403542280197e-06

Training epoch-42 batch-149
Running loss of epoch-42 batch-149 = 1.3718381524085999e-06

Training epoch-42 batch-150
Running loss of epoch-42 batch-150 = 6.313086487352848e-06

Training epoch-42 batch-151
Running loss of epoch-42 batch-151 = 1.81874493137002e-05

Training epoch-42 batch-152
Running loss of epoch-42 batch-152 = 1.7799786292016506e-05

Training epoch-42 batch-153
Running loss of epoch-42 batch-153 = 4.002999048680067e-05

Training epoch-42 batch-154
Running loss of epoch-42 batch-154 = 2.652057446539402e-05

Training epoch-42 batch-155
Running loss of epoch-42 batch-155 = 3.730878233909607e-06

Training epoch-42 batch-156
Running loss of epoch-42 batch-156 = 3.164284862577915e-05

Training epoch-42 batch-157
Running loss of epoch-42 batch-157 = 1.895427703857422e-05

Finished training epoch-42.



Average train loss at epoch-42 = 1.4167324453592301e-05

Started Evaluation

Average val loss at epoch-42 = 5.0774435934267546

Accuracy for classes:
Accuracy for class equals is: 68.48 %
Accuracy for class main is: 47.05 %
Accuracy for class setUp is: 54.26 %
Accuracy for class onCreate is: 43.60 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 36.07 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.30 %
Accuracy for class execute is: 17.67 %
Accuracy for class get is: 28.97 %

Overall Accuracy = 45.29 %

Finished Evaluation



Started training epoch-43


Training epoch-43 batch-1
Running loss of epoch-43 batch-1 = 3.0493829399347305e-06

Training epoch-43 batch-2
Running loss of epoch-43 batch-2 = 1.093139871954918e-06

Training epoch-43 batch-3
Running loss of epoch-43 batch-3 = 2.110842615365982e-06

Training epoch-43 batch-4
Running loss of epoch-43 batch-4 = 2.749962732195854e-06

Training epoch-43 batch-5
Running loss of epoch-43 batch-5 = 3.824359737336636e-06

Training epoch-43 batch-6
Running loss of epoch-43 batch-6 = 4.631117917597294e-06

Training epoch-43 batch-7
Running loss of epoch-43 batch-7 = 6.827176548540592e-06

Training epoch-43 batch-8
Running loss of epoch-43 batch-8 = 1.0825693607330322e-05

Training epoch-43 batch-9
Running loss of epoch-43 batch-9 = 1.2791715562343597e-06

Training epoch-43 batch-10
Running loss of epoch-43 batch-10 = 2.4084001779556274e-06

Training epoch-43 batch-11
Running loss of epoch-43 batch-11 = 4.01446595788002e-06

Training epoch-43 batch-12
Running loss of epoch-43 batch-12 = 2.0310981199145317e-05

Training epoch-43 batch-13
Running loss of epoch-43 batch-13 = 7.738033309578896e-05

Training epoch-43 batch-14
Running loss of epoch-43 batch-14 = 0.00011426926357671618

Training epoch-43 batch-15
Running loss of epoch-43 batch-15 = 8.418806828558445e-06

Training epoch-43 batch-16
Running loss of epoch-43 batch-16 = 1.7601996660232544e-06

Training epoch-43 batch-17
Running loss of epoch-43 batch-17 = 1.264503225684166e-06

Training epoch-43 batch-18
Running loss of epoch-43 batch-18 = 1.3990793377161026e-06

Training epoch-43 batch-19
Running loss of epoch-43 batch-19 = 4.507135599851608e-06

Training epoch-43 batch-20
Running loss of epoch-43 batch-20 = 5.129026249051094e-06

Training epoch-43 batch-21
Running loss of epoch-43 batch-21 = 3.2372772693634033e-06

Training epoch-43 batch-22
Running loss of epoch-43 batch-22 = 1.3170763850212097e-05

Training epoch-43 batch-23
Running loss of epoch-43 batch-23 = 6.846967153251171e-06

Training epoch-43 batch-24
Running loss of epoch-43 batch-24 = 9.362120181322098e-06

Training epoch-43 batch-25
Running loss of epoch-43 batch-25 = 2.9457733035087585e-06

Training epoch-43 batch-26
Running loss of epoch-43 batch-26 = 4.915287718176842e-06

Training epoch-43 batch-27
Running loss of epoch-43 batch-27 = 7.459893822669983e-07

Training epoch-43 batch-28
Running loss of epoch-43 batch-28 = 2.978718839585781e-05

Training epoch-43 batch-29
Running loss of epoch-43 batch-29 = 1.893145963549614e-06

Training epoch-43 batch-30
Running loss of epoch-43 batch-30 = 8.26059840619564e-06

Training epoch-43 batch-31
Running loss of epoch-43 batch-31 = 2.9134098440408707e-06

Training epoch-43 batch-32
Running loss of epoch-43 batch-32 = 2.3024622350931168e-06

Training epoch-43 batch-33
Running loss of epoch-43 batch-33 = 1.6391277313232422e-06

Training epoch-43 batch-34
Running loss of epoch-43 batch-34 = 2.1007144823670387e-05

Training epoch-43 batch-35
Running loss of epoch-43 batch-35 = 1.3765646144747734e-05

Training epoch-43 batch-36
Running loss of epoch-43 batch-36 = 1.5863566659390926e-05

Training epoch-43 batch-37
Running loss of epoch-43 batch-37 = 1.0011019185185432e-05

Training epoch-43 batch-38
Running loss of epoch-43 batch-38 = 3.5403063520789146e-05

Training epoch-43 batch-39
Running loss of epoch-43 batch-39 = 8.991919457912445e-07

Training epoch-43 batch-40
Running loss of epoch-43 batch-40 = 2.0354054868221283e-06

Training epoch-43 batch-41
Running loss of epoch-43 batch-41 = 4.097563214600086e-05

Training epoch-43 batch-42
Running loss of epoch-43 batch-42 = 1.0469462722539902e-05

Training epoch-43 batch-43
Running loss of epoch-43 batch-43 = 5.112960934638977e-07

Training epoch-43 batch-44
Running loss of epoch-43 batch-44 = 1.4354009181261063e-06

Training epoch-43 batch-45
Running loss of epoch-43 batch-45 = 5.8335717767477036e-06

Training epoch-43 batch-46
Running loss of epoch-43 batch-46 = 1.5966128557920456e-05

Training epoch-43 batch-47
Running loss of epoch-43 batch-47 = 4.691886715590954e-06

Training epoch-43 batch-48
Running loss of epoch-43 batch-48 = 2.0584557205438614e-06

Training epoch-43 batch-49
Running loss of epoch-43 batch-49 = 3.5440316423773766e-06

Training epoch-43 batch-50
Running loss of epoch-43 batch-50 = 4.876637831330299e-06

Training epoch-43 batch-51
Running loss of epoch-43 batch-51 = 3.188150003552437e-06

Training epoch-43 batch-52
Running loss of epoch-43 batch-52 = 2.1366868168115616e-06

Training epoch-43 batch-53
Running loss of epoch-43 batch-53 = 1.2296601198613644e-05

Training epoch-43 batch-54
Running loss of epoch-43 batch-54 = 4.820059984922409e-05

Training epoch-43 batch-55
Running loss of epoch-43 batch-55 = 3.2510142773389816e-06

Training epoch-43 batch-56
Running loss of epoch-43 batch-56 = 1.5916302800178528e-06

Training epoch-43 batch-57
Running loss of epoch-43 batch-57 = 6.346730515360832e-06

Training epoch-43 batch-58
Running loss of epoch-43 batch-58 = 7.310736691579223e-05

Training epoch-43 batch-59
Running loss of epoch-43 batch-59 = 2.4670735001564026e-06

Training epoch-43 batch-60
Running loss of epoch-43 batch-60 = 1.2994278222322464e-06

Training epoch-43 batch-61
Running loss of epoch-43 batch-61 = 1.5351572073996067e-05

Training epoch-43 batch-62
Running loss of epoch-43 batch-62 = 9.854906238615513e-06

Training epoch-43 batch-63
Running loss of epoch-43 batch-63 = 7.099704816937447e-06

Training epoch-43 batch-64
Running loss of epoch-43 batch-64 = 4.972517490386963e-05

Training epoch-43 batch-65
Running loss of epoch-43 batch-65 = 9.776558727025986e-07

Training epoch-43 batch-66
Running loss of epoch-43 batch-66 = 3.6295969039201736e-06

Training epoch-43 batch-67
Running loss of epoch-43 batch-67 = 2.571765799075365e-05

Training epoch-43 batch-68
Running loss of epoch-43 batch-68 = 7.71787017583847e-06

Training epoch-43 batch-69
Running loss of epoch-43 batch-69 = 7.795635610818863e-06

Training epoch-43 batch-70
Running loss of epoch-43 batch-70 = 2.869870513677597e-05

Training epoch-43 batch-71
Running loss of epoch-43 batch-71 = 3.945082426071167e-06

Training epoch-43 batch-72
Running loss of epoch-43 batch-72 = 6.065238267183304e-07

Training epoch-43 batch-73
Running loss of epoch-43 batch-73 = 2.818182110786438e-06

Training epoch-43 batch-74
Running loss of epoch-43 batch-74 = 2.1762680262327194e-06

Training epoch-43 batch-75
Running loss of epoch-43 batch-75 = 5.814246833324432e-06

Training epoch-43 batch-76
Running loss of epoch-43 batch-76 = 2.615863922983408e-05

Training epoch-43 batch-77
Running loss of epoch-43 batch-77 = 1.572060864418745e-05

Training epoch-43 batch-78
Running loss of epoch-43 batch-78 = 3.7301797419786453e-06

Training epoch-43 batch-79
Running loss of epoch-43 batch-79 = 2.667820081114769e-05

Training epoch-43 batch-80
Running loss of epoch-43 batch-80 = 1.2196018360555172e-05

Training epoch-43 batch-81
Running loss of epoch-43 batch-81 = 9.748502634465694e-06

Training epoch-43 batch-82
Running loss of epoch-43 batch-82 = 1.013569999486208e-05

Training epoch-43 batch-83
Running loss of epoch-43 batch-83 = 1.8624123185873032e-06

Training epoch-43 batch-84
Running loss of epoch-43 batch-84 = 5.371286533772945e-06

Training epoch-43 batch-85
Running loss of epoch-43 batch-85 = 2.5243498384952545e-06

Training epoch-43 batch-86
Running loss of epoch-43 batch-86 = 3.298278898000717e-06

Training epoch-43 batch-87
Running loss of epoch-43 batch-87 = 1.6977661289274693e-05

Training epoch-43 batch-88
Running loss of epoch-43 batch-88 = 1.7039128579199314e-05

Training epoch-43 batch-89
Running loss of epoch-43 batch-89 = 4.330126103013754e-05

Training epoch-43 batch-90
Running loss of epoch-43 batch-90 = 2.0689330995082855e-06

Training epoch-43 batch-91
Running loss of epoch-43 batch-91 = 8.954433724284172e-06

Training epoch-43 batch-92
Running loss of epoch-43 batch-92 = 4.646682646125555e-05

Training epoch-43 batch-93
Running loss of epoch-43 batch-93 = 1.7166603356599808e-06

Training epoch-43 batch-94
Running loss of epoch-43 batch-94 = 1.9092694856226444e-05

Training epoch-43 batch-95
Running loss of epoch-43 batch-95 = 1.4951569028198719e-05

Training epoch-43 batch-96
Running loss of epoch-43 batch-96 = 1.4678924344480038e-05

Training epoch-43 batch-97
Running loss of epoch-43 batch-97 = 2.1422747522592545e-06

Training epoch-43 batch-98
Running loss of epoch-43 batch-98 = 1.4333752915263176e-05

Training epoch-43 batch-99
Running loss of epoch-43 batch-99 = 1.1585187166929245e-05

Training epoch-43 batch-100
Running loss of epoch-43 batch-100 = 1.6102567315101624e-06

Training epoch-43 batch-101
Running loss of epoch-43 batch-101 = 1.6113161109387875e-05

Training epoch-43 batch-102
Running loss of epoch-43 batch-102 = 9.576469892635942e-05

Training epoch-43 batch-103
Running loss of epoch-43 batch-103 = 3.94647940993309e-06

Training epoch-43 batch-104
Running loss of epoch-43 batch-104 = 2.020038664340973e-06

Training epoch-43 batch-105
Running loss of epoch-43 batch-105 = 1.4435499906539917e-05

Training epoch-43 batch-106
Running loss of epoch-43 batch-106 = 7.702736184000969e-06

Training epoch-43 batch-107
Running loss of epoch-43 batch-107 = 5.055684596300125e-06

Training epoch-43 batch-108
Running loss of epoch-43 batch-108 = 3.02738044410944e-06

Training epoch-43 batch-109
Running loss of epoch-43 batch-109 = 4.5667169615626335e-05

Training epoch-43 batch-110
Running loss of epoch-43 batch-110 = 2.1543819457292557e-06

Training epoch-43 batch-111
Running loss of epoch-43 batch-111 = 9.301584213972092e-06

Training epoch-43 batch-112
Running loss of epoch-43 batch-112 = 2.1183164790272713e-05

Training epoch-43 batch-113
Running loss of epoch-43 batch-113 = 4.799803718924522e-06

Training epoch-43 batch-114
Running loss of epoch-43 batch-114 = 1.0640360414981842e-06

Training epoch-43 batch-115
Running loss of epoch-43 batch-115 = 5.052424967288971e-06

Training epoch-43 batch-116
Running loss of epoch-43 batch-116 = 0.00021235935855656862

Training epoch-43 batch-117
Running loss of epoch-43 batch-117 = 1.5674158930778503e-06

Training epoch-43 batch-118
Running loss of epoch-43 batch-118 = 1.216074451804161e-06

Training epoch-43 batch-119
Running loss of epoch-43 batch-119 = 2.7171336114406586e-06

Training epoch-43 batch-120
Running loss of epoch-43 batch-120 = 4.572165198624134e-05

Training epoch-43 batch-121
Running loss of epoch-43 batch-121 = 2.5918707251548767e-06

Training epoch-43 batch-122
Running loss of epoch-43 batch-122 = 4.186294972896576e-06

Training epoch-43 batch-123
Running loss of epoch-43 batch-123 = 2.1174666471779346e-05

Training epoch-43 batch-124
Running loss of epoch-43 batch-124 = 1.4820951037108898e-05

Training epoch-43 batch-125
Running loss of epoch-43 batch-125 = 3.564869984984398e-06

Training epoch-43 batch-126
Running loss of epoch-43 batch-126 = 1.171603798866272e-05

Training epoch-43 batch-127
Running loss of epoch-43 batch-127 = 3.204215317964554e-06

Training epoch-43 batch-128
Running loss of epoch-43 batch-128 = 4.047993570566177e-06

Training epoch-43 batch-129
Running loss of epoch-43 batch-129 = 1.310382504016161e-05

Training epoch-43 batch-130
Running loss of epoch-43 batch-130 = 2.8517097234725952e-06

Training epoch-43 batch-131
Running loss of epoch-43 batch-131 = 5.746493116021156e-06

Training epoch-43 batch-132
Running loss of epoch-43 batch-132 = 2.5527551770210266e-06

Training epoch-43 batch-133
Running loss of epoch-43 batch-133 = 4.142988473176956e-06

Training epoch-43 batch-134
Running loss of epoch-43 batch-134 = 3.934372216463089e-06

Training epoch-43 batch-135
Running loss of epoch-43 batch-135 = 1.1603347957134247e-05

Training epoch-43 batch-136
Running loss of epoch-43 batch-136 = 1.378590241074562e-06

Training epoch-43 batch-137
Running loss of epoch-43 batch-137 = 1.3425014913082123e-06

Training epoch-43 batch-138
Running loss of epoch-43 batch-138 = 1.9210856407880783e-06

Training epoch-43 batch-139
Running loss of epoch-43 batch-139 = 4.796427674591541e-06

Training epoch-43 batch-140
Running loss of epoch-43 batch-140 = 2.497097011655569e-05

Training epoch-43 batch-141
Running loss of epoch-43 batch-141 = 1.2891832739114761e-06

Training epoch-43 batch-142
Running loss of epoch-43 batch-142 = 2.3654079996049404e-05

Training epoch-43 batch-143
Running loss of epoch-43 batch-143 = 5.149864591658115e-06

Training epoch-43 batch-144
Running loss of epoch-43 batch-144 = 8.472008630633354e-06

Training epoch-43 batch-145
Running loss of epoch-43 batch-145 = 1.1609634384512901e-05

Training epoch-43 batch-146
Running loss of epoch-43 batch-146 = 1.0106945410370827e-05

Training epoch-43 batch-147
Running loss of epoch-43 batch-147 = 2.487562596797943e-06

Training epoch-43 batch-148
Running loss of epoch-43 batch-148 = 9.229406714439392e-07

Training epoch-43 batch-149
Running loss of epoch-43 batch-149 = 8.433475159108639e-06

Training epoch-43 batch-150
Running loss of epoch-43 batch-150 = 5.9991609305143356e-05

Training epoch-43 batch-151
Running loss of epoch-43 batch-151 = 1.22864730656147e-06

Training epoch-43 batch-152
Running loss of epoch-43 batch-152 = 9.209616109728813e-06

Training epoch-43 batch-153
Running loss of epoch-43 batch-153 = 4.062894731760025e-06

Training epoch-43 batch-154
Running loss of epoch-43 batch-154 = 1.050299033522606e-06

Training epoch-43 batch-155
Running loss of epoch-43 batch-155 = 3.397499676793814e-05

Training epoch-43 batch-156
Running loss of epoch-43 batch-156 = 7.506692782044411e-06

Training epoch-43 batch-157
Running loss of epoch-43 batch-157 = 2.3584812879562378e-05

Finished training epoch-43.



Average train loss at epoch-43 = 1.3135020807385444e-05

Started Evaluation

Average val loss at epoch-43 = 5.122247520246003

Accuracy for classes:
Accuracy for class equals is: 68.98 %
Accuracy for class main is: 49.67 %
Accuracy for class setUp is: 54.26 %
Accuracy for class onCreate is: 42.11 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 35.84 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 19.73 %
Accuracy for class execute is: 18.07 %
Accuracy for class get is: 29.49 %

Overall Accuracy = 45.29 %

Finished Evaluation



Started training epoch-44


Training epoch-44 batch-1
Running loss of epoch-44 batch-1 = 1.9206199795007706e-06

Training epoch-44 batch-2
Running loss of epoch-44 batch-2 = 1.7692800611257553e-06

Training epoch-44 batch-3
Running loss of epoch-44 batch-3 = 3.4775584936141968e-06

Training epoch-44 batch-4
Running loss of epoch-44 batch-4 = 0.00012392399366945028

Training epoch-44 batch-5
Running loss of epoch-44 batch-5 = 7.751397788524628e-06

Training epoch-44 batch-6
Running loss of epoch-44 batch-6 = 2.2612744942307472e-05

Training epoch-44 batch-7
Running loss of epoch-44 batch-7 = 9.369105100631714e-07

Training epoch-44 batch-8
Running loss of epoch-44 batch-8 = 4.630815237760544e-05

Training epoch-44 batch-9
Running loss of epoch-44 batch-9 = 7.66245648264885e-07

Training epoch-44 batch-10
Running loss of epoch-44 batch-10 = 2.884538844227791e-06

Training epoch-44 batch-11
Running loss of epoch-44 batch-11 = 6.388872861862183e-07

Training epoch-44 batch-12
Running loss of epoch-44 batch-12 = 2.023065462708473e-06

Training epoch-44 batch-13
Running loss of epoch-44 batch-13 = 4.074769094586372e-06

Training epoch-44 batch-14
Running loss of epoch-44 batch-14 = 1.2757373042404652e-05

Training epoch-44 batch-15
Running loss of epoch-44 batch-15 = 6.869086064398289e-06

Training epoch-44 batch-16
Running loss of epoch-44 batch-16 = 2.889777533710003e-06

Training epoch-44 batch-17
Running loss of epoch-44 batch-17 = 3.2656826078891754e-06

Training epoch-44 batch-18
Running loss of epoch-44 batch-18 = 3.280816599726677e-06

Training epoch-44 batch-19
Running loss of epoch-44 batch-19 = 8.856877684593201e-07

Training epoch-44 batch-20
Running loss of epoch-44 batch-20 = 6.830552592873573e-06

Training epoch-44 batch-21
Running loss of epoch-44 batch-21 = 4.267087206244469e-06

Training epoch-44 batch-22
Running loss of epoch-44 batch-22 = 7.074209861457348e-06

Training epoch-44 batch-23
Running loss of epoch-44 batch-23 = 1.1756434105336666e-05

Training epoch-44 batch-24
Running loss of epoch-44 batch-24 = 5.359295755624771e-06

Training epoch-44 batch-25
Running loss of epoch-44 batch-25 = 8.23743175715208e-06

Training epoch-44 batch-26
Running loss of epoch-44 batch-26 = 2.9215705581009388e-05

Training epoch-44 batch-27
Running loss of epoch-44 batch-27 = 9.948387742042542e-06

Training epoch-44 batch-28
Running loss of epoch-44 batch-28 = 3.0677765607833862e-06

Training epoch-44 batch-29
Running loss of epoch-44 batch-29 = 1.2009404599666595e-06

Training epoch-44 batch-30
Running loss of epoch-44 batch-30 = 3.777211531996727e-06

Training epoch-44 batch-31
Running loss of epoch-44 batch-31 = 1.934124156832695e-06

Training epoch-44 batch-32
Running loss of epoch-44 batch-32 = 2.1748710423707962e-05

Training epoch-44 batch-33
Running loss of epoch-44 batch-33 = 2.5353278033435345e-05

Training epoch-44 batch-34
Running loss of epoch-44 batch-34 = 2.055894583463669e-06

Training epoch-44 batch-35
Running loss of epoch-44 batch-35 = 4.503526724874973e-06

Training epoch-44 batch-36
Running loss of epoch-44 batch-36 = 7.472233846783638e-06

Training epoch-44 batch-37
Running loss of epoch-44 batch-37 = 5.694222636520863e-06

Training epoch-44 batch-38
Running loss of epoch-44 batch-38 = 1.0254443623125553e-05

Training epoch-44 batch-39
Running loss of epoch-44 batch-39 = 3.28640453517437e-06

Training epoch-44 batch-40
Running loss of epoch-44 batch-40 = 8.678180165588856e-06

Training epoch-44 batch-41
Running loss of epoch-44 batch-41 = 9.526032954454422e-06

Training epoch-44 batch-42
Running loss of epoch-44 batch-42 = 1.0316958650946617e-05

Training epoch-44 batch-43
Running loss of epoch-44 batch-43 = 3.549701068550348e-05

Training epoch-44 batch-44
Running loss of epoch-44 batch-44 = 2.1657906472682953e-06

Training epoch-44 batch-45
Running loss of epoch-44 batch-45 = 2.7127214707434177e-05

Training epoch-44 batch-46
Running loss of epoch-44 batch-46 = 4.399335011839867e-06

Training epoch-44 batch-47
Running loss of epoch-44 batch-47 = 5.760928615927696e-06

Training epoch-44 batch-48
Running loss of epoch-44 batch-48 = 6.181653589010239e-07

Training epoch-44 batch-49
Running loss of epoch-44 batch-49 = 3.3280812203884125e-06

Training epoch-44 batch-50
Running loss of epoch-44 batch-50 = 1.031381543725729e-05

Training epoch-44 batch-51
Running loss of epoch-44 batch-51 = 3.0354131013154984e-06

Training epoch-44 batch-52
Running loss of epoch-44 batch-52 = 1.9080471247434616e-06

Training epoch-44 batch-53
Running loss of epoch-44 batch-53 = 4.83635812997818e-06

Training epoch-44 batch-54
Running loss of epoch-44 batch-54 = 1.0095536708831787e-06

Training epoch-44 batch-55
Running loss of epoch-44 batch-55 = 2.0514708012342453e-06

Training epoch-44 batch-56
Running loss of epoch-44 batch-56 = 4.424131475389004e-06

Training epoch-44 batch-57
Running loss of epoch-44 batch-57 = 1.4905817806720734e-06

Training epoch-44 batch-58
Running loss of epoch-44 batch-58 = 1.9770115613937378e-05

Training epoch-44 batch-59
Running loss of epoch-44 batch-59 = 1.607462763786316e-06

Training epoch-44 batch-60
Running loss of epoch-44 batch-60 = 2.930872142314911e-06

Training epoch-44 batch-61
Running loss of epoch-44 batch-61 = 1.687556505203247e-06

Training epoch-44 batch-62
Running loss of epoch-44 batch-62 = 8.649309165775776e-06

Training epoch-44 batch-63
Running loss of epoch-44 batch-63 = 4.619802348315716e-05

Training epoch-44 batch-64
Running loss of epoch-44 batch-64 = 1.927325502038002e-05

Training epoch-44 batch-65
Running loss of epoch-44 batch-65 = 6.174668669700623e-07

Training epoch-44 batch-66
Running loss of epoch-44 batch-66 = 1.0789372026920319e-06

Training epoch-44 batch-67
Running loss of epoch-44 batch-67 = 1.61072239279747e-06

Training epoch-44 batch-68
Running loss of epoch-44 batch-68 = 2.6295892894268036e-06

Training epoch-44 batch-69
Running loss of epoch-44 batch-69 = 6.6802604123950005e-06

Training epoch-44 batch-70
Running loss of epoch-44 batch-70 = 3.5902485251426697e-06

Training epoch-44 batch-71
Running loss of epoch-44 batch-71 = 7.598311640322208e-06

Training epoch-44 batch-72
Running loss of epoch-44 batch-72 = 6.624381057918072e-06

Training epoch-44 batch-73
Running loss of epoch-44 batch-73 = 1.2570526450872421e-06

Training epoch-44 batch-74
Running loss of epoch-44 batch-74 = 1.3541546650230885e-05

Training epoch-44 batch-75
Running loss of epoch-44 batch-75 = 5.110632628202438e-06

Training epoch-44 batch-76
Running loss of epoch-44 batch-76 = 4.534493200480938e-06

Training epoch-44 batch-77
Running loss of epoch-44 batch-77 = 2.1192245185375214e-06

Training epoch-44 batch-78
Running loss of epoch-44 batch-78 = 7.615197682753205e-05

Training epoch-44 batch-79
Running loss of epoch-44 batch-79 = 4.528730642050505e-05

Training epoch-44 batch-80
Running loss of epoch-44 batch-80 = 1.1747470125555992e-05

Training epoch-44 batch-81
Running loss of epoch-44 batch-81 = 2.136477269232273e-05

Training epoch-44 batch-82
Running loss of epoch-44 batch-82 = 0.0001804940984584391

Training epoch-44 batch-83
Running loss of epoch-44 batch-83 = 1.5925616025924683e-06

Training epoch-44 batch-84
Running loss of epoch-44 batch-84 = 2.8866343200206757e-06

Training epoch-44 batch-85
Running loss of epoch-44 batch-85 = 2.1289917640388012e-05

Training epoch-44 batch-86
Running loss of epoch-44 batch-86 = 1.291278749704361e-06

Training epoch-44 batch-87
Running loss of epoch-44 batch-87 = 1.0489020496606827e-06

Training epoch-44 batch-88
Running loss of epoch-44 batch-88 = 2.2863969206809998e-06

Training epoch-44 batch-89
Running loss of epoch-44 batch-89 = 1.939409412443638e-05

Training epoch-44 batch-90
Running loss of epoch-44 batch-90 = 2.3336615413427353e-06

Training epoch-44 batch-91
Running loss of epoch-44 batch-91 = 7.17362854629755e-06

Training epoch-44 batch-92
Running loss of epoch-44 batch-92 = 1.1129304766654968e-06

Training epoch-44 batch-93
Running loss of epoch-44 batch-93 = 3.2871030271053314e-06

Training epoch-44 batch-94
Running loss of epoch-44 batch-94 = 7.010530680418015e-07

Training epoch-44 batch-95
Running loss of epoch-44 batch-95 = 3.287568688392639e-07

Training epoch-44 batch-96
Running loss of epoch-44 batch-96 = 1.6026897355914116e-06

Training epoch-44 batch-97
Running loss of epoch-44 batch-97 = 7.527065463364124e-06

Training epoch-44 batch-98
Running loss of epoch-44 batch-98 = 4.444736987352371e-06

Training epoch-44 batch-99
Running loss of epoch-44 batch-99 = 2.957764081656933e-06

Training epoch-44 batch-100
Running loss of epoch-44 batch-100 = 4.175957292318344e-05

Training epoch-44 batch-101
Running loss of epoch-44 batch-101 = 4.313827957957983e-05

Training epoch-44 batch-102
Running loss of epoch-44 batch-102 = 1.939479261636734e-06

Training epoch-44 batch-103
Running loss of epoch-44 batch-103 = 3.7834979593753815e-07

Training epoch-44 batch-104
Running loss of epoch-44 batch-104 = 2.1471641957759857e-06

Training epoch-44 batch-105
Running loss of epoch-44 batch-105 = 9.538023732602596e-06

Training epoch-44 batch-106
Running loss of epoch-44 batch-106 = 4.856614395976067e-06

Training epoch-44 batch-107
Running loss of epoch-44 batch-107 = 2.148561179637909e-06

Training epoch-44 batch-108
Running loss of epoch-44 batch-108 = 6.3248444348573685e-06

Training epoch-44 batch-109
Running loss of epoch-44 batch-109 = 8.120248094201088e-05

Training epoch-44 batch-110
Running loss of epoch-44 batch-110 = 1.9213533960282803e-05

Training epoch-44 batch-111
Running loss of epoch-44 batch-111 = 7.35139474272728e-06

Training epoch-44 batch-112
Running loss of epoch-44 batch-112 = 6.140093319118023e-06

Training epoch-44 batch-113
Running loss of epoch-44 batch-113 = 5.6192511692643166e-06

Training epoch-44 batch-114
Running loss of epoch-44 batch-114 = 1.2167729437351227e-06

Training epoch-44 batch-115
Running loss of epoch-44 batch-115 = 1.0307412594556808e-05

Training epoch-44 batch-116
Running loss of epoch-44 batch-116 = 2.2370368242263794e-06

Training epoch-44 batch-117
Running loss of epoch-44 batch-117 = 6.259535439312458e-06

Training epoch-44 batch-118
Running loss of epoch-44 batch-118 = 7.639522664248943e-06

Training epoch-44 batch-119
Running loss of epoch-44 batch-119 = 5.07239019498229e-05

Training epoch-44 batch-120
Running loss of epoch-44 batch-120 = 5.014939233660698e-06

Training epoch-44 batch-121
Running loss of epoch-44 batch-121 = 8.831889135763049e-05

Training epoch-44 batch-122
Running loss of epoch-44 batch-122 = 2.5976914912462234e-06

Training epoch-44 batch-123
Running loss of epoch-44 batch-123 = 1.5171244740486145e-06

Training epoch-44 batch-124
Running loss of epoch-44 batch-124 = 5.992129445075989e-06

Training epoch-44 batch-125
Running loss of epoch-44 batch-125 = 5.2086543291807175e-06

Training epoch-44 batch-126
Running loss of epoch-44 batch-126 = 1.7567072063684464e-06

Training epoch-44 batch-127
Running loss of epoch-44 batch-127 = 7.576192729175091e-06

Training epoch-44 batch-128
Running loss of epoch-44 batch-128 = 2.5160610675811768e-05

Training epoch-44 batch-129
Running loss of epoch-44 batch-129 = 1.901877112686634e-05

Training epoch-44 batch-130
Running loss of epoch-44 batch-130 = 3.0475202947854996e-06

Training epoch-44 batch-131
Running loss of epoch-44 batch-131 = 2.6994384825229645e-06

Training epoch-44 batch-132
Running loss of epoch-44 batch-132 = 5.736248567700386e-06

Training epoch-44 batch-133
Running loss of epoch-44 batch-133 = 9.70577821135521e-06

Training epoch-44 batch-134
Running loss of epoch-44 batch-134 = 1.301988959312439e-06

Training epoch-44 batch-135
Running loss of epoch-44 batch-135 = 1.2344680726528168e-06

Training epoch-44 batch-136
Running loss of epoch-44 batch-136 = 3.4116674214601517e-06

Training epoch-44 batch-137
Running loss of epoch-44 batch-137 = 3.434170503169298e-05

Training epoch-44 batch-138
Running loss of epoch-44 batch-138 = 2.2687483578920364e-05

Training epoch-44 batch-139
Running loss of epoch-44 batch-139 = 1.9842758774757385e-05

Training epoch-44 batch-140
Running loss of epoch-44 batch-140 = 1.091044396162033e-06

Training epoch-44 batch-141
Running loss of epoch-44 batch-141 = 2.3979227989912033e-06

Training epoch-44 batch-142
Running loss of epoch-44 batch-142 = 1.5847384929656982e-05

Training epoch-44 batch-143
Running loss of epoch-44 batch-143 = 8.828938007354736e-07

Training epoch-44 batch-144
Running loss of epoch-44 batch-144 = 1.7420155927538872e-05

Training epoch-44 batch-145
Running loss of epoch-44 batch-145 = 2.001808024942875e-05

Training epoch-44 batch-146
Running loss of epoch-44 batch-146 = 6.618676707148552e-06

Training epoch-44 batch-147
Running loss of epoch-44 batch-147 = 2.0531006157398224e-06

Training epoch-44 batch-148
Running loss of epoch-44 batch-148 = 4.034023731946945e-06

Training epoch-44 batch-149
Running loss of epoch-44 batch-149 = 3.853288944810629e-05

Training epoch-44 batch-150
Running loss of epoch-44 batch-150 = 1.796300057321787e-05

Training epoch-44 batch-151
Running loss of epoch-44 batch-151 = 4.742213059216738e-05

Training epoch-44 batch-152
Running loss of epoch-44 batch-152 = 6.166053935885429e-06

Training epoch-44 batch-153
Running loss of epoch-44 batch-153 = 1.3508833944797516e-06

Training epoch-44 batch-154
Running loss of epoch-44 batch-154 = 2.709217369556427e-05

Training epoch-44 batch-155
Running loss of epoch-44 batch-155 = 8.953153155744076e-06

Training epoch-44 batch-156
Running loss of epoch-44 batch-156 = 5.1674433052539825e-06

Training epoch-44 batch-157
Running loss of epoch-44 batch-157 = 1.2006610631942749e-05

Finished training epoch-44.



Average train loss at epoch-44 = 1.2379526719450951e-05

Started Evaluation

Average val loss at epoch-44 = 5.146046644762943

Accuracy for classes:
Accuracy for class equals is: 68.15 %
Accuracy for class main is: 47.38 %
Accuracy for class setUp is: 53.28 %
Accuracy for class onCreate is: 42.11 %
Accuracy for class toString is: 40.96 %
Accuracy for class run is: 35.16 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 22.42 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 29.49 %

Overall Accuracy = 44.91 %

Finished Evaluation



Started training epoch-45


Training epoch-45 batch-1
Running loss of epoch-45 batch-1 = 1.2080883607268333e-05

Training epoch-45 batch-2
Running loss of epoch-45 batch-2 = 2.3854663595557213e-06

Training epoch-45 batch-3
Running loss of epoch-45 batch-3 = 4.014931619167328e-06

Training epoch-45 batch-4
Running loss of epoch-45 batch-4 = 3.081047907471657e-06

Training epoch-45 batch-5
Running loss of epoch-45 batch-5 = 3.2810028642416e-05

Training epoch-45 batch-6
Running loss of epoch-45 batch-6 = 6.501330062747002e-06

Training epoch-45 batch-7
Running loss of epoch-45 batch-7 = 2.6188790798187256e-06

Training epoch-45 batch-8
Running loss of epoch-45 batch-8 = 1.7399434000253677e-06

Training epoch-45 batch-9
Running loss of epoch-45 batch-9 = 1.3669487088918686e-06

Training epoch-45 batch-10
Running loss of epoch-45 batch-10 = 2.8439564630389214e-05

Training epoch-45 batch-11
Running loss of epoch-45 batch-11 = 1.8608872778713703e-05

Training epoch-45 batch-12
Running loss of epoch-45 batch-12 = 1.0204967111349106e-06

Training epoch-45 batch-13
Running loss of epoch-45 batch-13 = 1.7618294805288315e-06

Training epoch-45 batch-14
Running loss of epoch-45 batch-14 = 9.344425052404404e-06

Training epoch-45 batch-15
Running loss of epoch-45 batch-15 = 2.0524137653410435e-05

Training epoch-45 batch-16
Running loss of epoch-45 batch-16 = 2.4398323148489e-06

Training epoch-45 batch-17
Running loss of epoch-45 batch-17 = 1.391395926475525e-06

Training epoch-45 batch-18
Running loss of epoch-45 batch-18 = 7.2338152676820755e-06

Training epoch-45 batch-19
Running loss of epoch-45 batch-19 = 3.338092938065529e-06

Training epoch-45 batch-20
Running loss of epoch-45 batch-20 = 3.9942096918821335e-06

Training epoch-45 batch-21
Running loss of epoch-45 batch-21 = 3.5141129046678543e-06

Training epoch-45 batch-22
Running loss of epoch-45 batch-22 = 7.4427807703614235e-06

Training epoch-45 batch-23
Running loss of epoch-45 batch-23 = 1.3709068298339844e-06

Training epoch-45 batch-24
Running loss of epoch-45 batch-24 = 7.42543488740921e-06

Training epoch-45 batch-25
Running loss of epoch-45 batch-25 = 2.345070242881775e-06

Training epoch-45 batch-26
Running loss of epoch-45 batch-26 = 1.8202699720859528e-06

Training epoch-45 batch-27
Running loss of epoch-45 batch-27 = 6.774673238396645e-06

Training epoch-45 batch-28
Running loss of epoch-45 batch-28 = 1.5277182683348656e-05

Training epoch-45 batch-29
Running loss of epoch-45 batch-29 = 1.193443313241005e-05

Training epoch-45 batch-30
Running loss of epoch-45 batch-30 = 1.3420358300209045e-06

Training epoch-45 batch-31
Running loss of epoch-45 batch-31 = 6.865477189421654e-06

Training epoch-45 batch-32
Running loss of epoch-45 batch-32 = 2.3855827748775482e-06

Training epoch-45 batch-33
Running loss of epoch-45 batch-33 = 4.229019396007061e-06

Training epoch-45 batch-34
Running loss of epoch-45 batch-34 = 2.652406692504883e-06

Training epoch-45 batch-35
Running loss of epoch-45 batch-35 = 6.0771359130740166e-05

Training epoch-45 batch-36
Running loss of epoch-45 batch-36 = 4.462932702153921e-05

Training epoch-45 batch-37
Running loss of epoch-45 batch-37 = 1.189298927783966e-06

Training epoch-45 batch-38
Running loss of epoch-45 batch-38 = 3.92599031329155e-06

Training epoch-45 batch-39
Running loss of epoch-45 batch-39 = 3.1152740120887756e-06

Training epoch-45 batch-40
Running loss of epoch-45 batch-40 = 3.895256668329239e-06

Training epoch-45 batch-41
Running loss of epoch-45 batch-41 = 6.736372597515583e-06

Training epoch-45 batch-42
Running loss of epoch-45 batch-42 = 1.162756234407425e-06

Training epoch-45 batch-43
Running loss of epoch-45 batch-43 = 2.5569461286067963e-06

Training epoch-45 batch-44
Running loss of epoch-45 batch-44 = 2.9816292226314545e-06

Training epoch-45 batch-45
Running loss of epoch-45 batch-45 = 1.7713755369186401e-06

Training epoch-45 batch-46
Running loss of epoch-45 batch-46 = 1.4356803148984909e-05

Training epoch-45 batch-47
Running loss of epoch-45 batch-47 = 4.9080816097557545e-05

Training epoch-45 batch-48
Running loss of epoch-45 batch-48 = 2.827262505888939e-06

Training epoch-45 batch-49
Running loss of epoch-45 batch-49 = 1.0722666047513485e-05

Training epoch-45 batch-50
Running loss of epoch-45 batch-50 = 2.3562461137771606e-06

Training epoch-45 batch-51
Running loss of epoch-45 batch-51 = 4.9576861783862114e-05

Training epoch-45 batch-52
Running loss of epoch-45 batch-52 = 2.3548491299152374e-06

Training epoch-45 batch-53
Running loss of epoch-45 batch-53 = 2.6893918402493e-05

Training epoch-45 batch-54
Running loss of epoch-45 batch-54 = 6.873160600662231e-07

Training epoch-45 batch-55
Running loss of epoch-45 batch-55 = 4.383339546620846e-05

Training epoch-45 batch-56
Running loss of epoch-45 batch-56 = 3.472610842436552e-05

Training epoch-45 batch-57
Running loss of epoch-45 batch-57 = 3.814813680946827e-06

Training epoch-45 batch-58
Running loss of epoch-45 batch-58 = 7.374212145805359e-06

Training epoch-45 batch-59
Running loss of epoch-45 batch-59 = 3.7529971450567245e-06

Training epoch-45 batch-60
Running loss of epoch-45 batch-60 = 1.9101426005363464e-06

Training epoch-45 batch-61
Running loss of epoch-45 batch-61 = 2.6246998459100723e-06

Training epoch-45 batch-62
Running loss of epoch-45 batch-62 = 4.4351909309625626e-06

Training epoch-45 batch-63
Running loss of epoch-45 batch-63 = 2.5047920644283295e-06

Training epoch-45 batch-64
Running loss of epoch-45 batch-64 = 2.2386666387319565e-06

Training epoch-45 batch-65
Running loss of epoch-45 batch-65 = 3.090826794505119e-05

Training epoch-45 batch-66
Running loss of epoch-45 batch-66 = 2.8382055461406708e-06

Training epoch-45 batch-67
Running loss of epoch-45 batch-67 = 2.701766788959503e-06

Training epoch-45 batch-68
Running loss of epoch-45 batch-68 = 5.447654984891415e-06

Training epoch-45 batch-69
Running loss of epoch-45 batch-69 = 2.0917505025863647e-06

Training epoch-45 batch-70
Running loss of epoch-45 batch-70 = 2.6011839509010315e-06

Training epoch-45 batch-71
Running loss of epoch-45 batch-71 = 4.13570087403059e-05

Training epoch-45 batch-72
Running loss of epoch-45 batch-72 = 3.1315721571445465e-07

Training epoch-45 batch-73
Running loss of epoch-45 batch-73 = 4.211673513054848e-06

Training epoch-45 batch-74
Running loss of epoch-45 batch-74 = 0.0001340788439847529

Training epoch-45 batch-75
Running loss of epoch-45 batch-75 = 0.00010229984764009714

Training epoch-45 batch-76
Running loss of epoch-45 batch-76 = 6.557325832545757e-06

Training epoch-45 batch-77
Running loss of epoch-45 batch-77 = 3.774405922740698e-05

Training epoch-45 batch-78
Running loss of epoch-45 batch-78 = 2.23587267100811e-06

Training epoch-45 batch-79
Running loss of epoch-45 batch-79 = 7.393886335194111e-06

Training epoch-45 batch-80
Running loss of epoch-45 batch-80 = 1.1586002074182034e-05

Training epoch-45 batch-81
Running loss of epoch-45 batch-81 = 1.6532139852643013e-05

Training epoch-45 batch-82
Running loss of epoch-45 batch-82 = 9.748619049787521e-07

Training epoch-45 batch-83
Running loss of epoch-45 batch-83 = 2.6533380150794983e-06

Training epoch-45 batch-84
Running loss of epoch-45 batch-84 = 1.0807998478412628e-06

Training epoch-45 batch-85
Running loss of epoch-45 batch-85 = 4.231114871799946e-06

Training epoch-45 batch-86
Running loss of epoch-45 batch-86 = 7.462222129106522e-07

Training epoch-45 batch-87
Running loss of epoch-45 batch-87 = 1.2826523743569851e-05

Training epoch-45 batch-88
Running loss of epoch-45 batch-88 = 3.7942081689834595e-06

Training epoch-45 batch-89
Running loss of epoch-45 batch-89 = 3.5115517675876617e-06

Training epoch-45 batch-90
Running loss of epoch-45 batch-90 = 2.5814049877226353e-05

Training epoch-45 batch-91
Running loss of epoch-45 batch-91 = 6.152433343231678e-06

Training epoch-45 batch-92
Running loss of epoch-45 batch-92 = 2.748332917690277e-06

Training epoch-45 batch-93
Running loss of epoch-45 batch-93 = 3.132270649075508e-06

Training epoch-45 batch-94
Running loss of epoch-45 batch-94 = 4.942994564771652e-06

Training epoch-45 batch-95
Running loss of epoch-45 batch-95 = 7.677590474486351e-06

Training epoch-45 batch-96
Running loss of epoch-45 batch-96 = 8.559669367969036e-06

Training epoch-45 batch-97
Running loss of epoch-45 batch-97 = 9.906245395541191e-06

Training epoch-45 batch-98
Running loss of epoch-45 batch-98 = 5.234044510871172e-05

Training epoch-45 batch-99
Running loss of epoch-45 batch-99 = 8.434872142970562e-06

Training epoch-45 batch-100
Running loss of epoch-45 batch-100 = 6.0603488236665726e-06

Training epoch-45 batch-101
Running loss of epoch-45 batch-101 = 8.245697245001793e-06

Training epoch-45 batch-102
Running loss of epoch-45 batch-102 = 1.8905848264694214e-06

Training epoch-45 batch-103
Running loss of epoch-45 batch-103 = 9.959912858903408e-06

Training epoch-45 batch-104
Running loss of epoch-45 batch-104 = 1.7620623111724854e-06

Training epoch-45 batch-105
Running loss of epoch-45 batch-105 = 8.841161616146564e-06

Training epoch-45 batch-106
Running loss of epoch-45 batch-106 = 1.0093324817717075e-05

Training epoch-45 batch-107
Running loss of epoch-45 batch-107 = 1.2966338545084e-06

Training epoch-45 batch-108
Running loss of epoch-45 batch-108 = 3.649306017905474e-05

Training epoch-45 batch-109
Running loss of epoch-45 batch-109 = 3.532273694872856e-06

Training epoch-45 batch-110
Running loss of epoch-45 batch-110 = 6.920541636645794e-06

Training epoch-45 batch-111
Running loss of epoch-45 batch-111 = 9.054900147020817e-06

Training epoch-45 batch-112
Running loss of epoch-45 batch-112 = 5.227047950029373e-07

Training epoch-45 batch-113
Running loss of epoch-45 batch-113 = 6.89865555614233e-06

Training epoch-45 batch-114
Running loss of epoch-45 batch-114 = 2.2759195417165756e-06

Training epoch-45 batch-115
Running loss of epoch-45 batch-115 = 7.216352969408035e-06

Training epoch-45 batch-116
Running loss of epoch-45 batch-116 = 1.6095582395792007e-06

Training epoch-45 batch-117
Running loss of epoch-45 batch-117 = 3.90014611184597e-06

Training epoch-45 batch-118
Running loss of epoch-45 batch-118 = 1.7939601093530655e-06

Training epoch-45 batch-119
Running loss of epoch-45 batch-119 = 2.1709129214286804e-06

Training epoch-45 batch-120
Running loss of epoch-45 batch-120 = 5.385838449001312e-06

Training epoch-45 batch-121
Running loss of epoch-45 batch-121 = 6.786314770579338e-06

Training epoch-45 batch-122
Running loss of epoch-45 batch-122 = 0.00011403614189475775

Training epoch-45 batch-123
Running loss of epoch-45 batch-123 = 1.1147931218147278e-06

Training epoch-45 batch-124
Running loss of epoch-45 batch-124 = 2.8426293283700943e-06

Training epoch-45 batch-125
Running loss of epoch-45 batch-125 = 5.882116965949535e-06

Training epoch-45 batch-126
Running loss of epoch-45 batch-126 = 2.243323251605034e-06

Training epoch-45 batch-127
Running loss of epoch-45 batch-127 = 1.9925646483898163e-06

Training epoch-45 batch-128
Running loss of epoch-45 batch-128 = 1.762190368026495e-05

Training epoch-45 batch-129
Running loss of epoch-45 batch-129 = 1.1707423254847527e-05

Training epoch-45 batch-130
Running loss of epoch-45 batch-130 = 2.268468961119652e-06

Training epoch-45 batch-131
Running loss of epoch-45 batch-131 = 1.308973878622055e-06

Training epoch-45 batch-132
Running loss of epoch-45 batch-132 = 3.045075573027134e-06

Training epoch-45 batch-133
Running loss of epoch-45 batch-133 = 7.520429790019989e-07

Training epoch-45 batch-134
Running loss of epoch-45 batch-134 = 2.2891908884048462e-05

Training epoch-45 batch-135
Running loss of epoch-45 batch-135 = 3.026693593710661e-05

Training epoch-45 batch-136
Running loss of epoch-45 batch-136 = 6.532762199640274e-06

Training epoch-45 batch-137
Running loss of epoch-45 batch-137 = 1.239357516169548e-06

Training epoch-45 batch-138
Running loss of epoch-45 batch-138 = 3.5099219530820847e-06

Training epoch-45 batch-139
Running loss of epoch-45 batch-139 = 1.5018158592283726e-05

Training epoch-45 batch-140
Running loss of epoch-45 batch-140 = 4.7531211748719215e-06

Training epoch-45 batch-141
Running loss of epoch-45 batch-141 = 3.20264371111989e-05

Training epoch-45 batch-142
Running loss of epoch-45 batch-142 = 1.603737473487854e-06

Training epoch-45 batch-143
Running loss of epoch-45 batch-143 = 1.6701873391866684e-05

Training epoch-45 batch-144
Running loss of epoch-45 batch-144 = 2.987799234688282e-06

Training epoch-45 batch-145
Running loss of epoch-45 batch-145 = 7.383059710264206e-07

Training epoch-45 batch-146
Running loss of epoch-45 batch-146 = 1.748441718518734e-06

Training epoch-45 batch-147
Running loss of epoch-45 batch-147 = 5.2691902965307236e-06

Training epoch-45 batch-148
Running loss of epoch-45 batch-148 = 5.798193160444498e-05

Training epoch-45 batch-149
Running loss of epoch-45 batch-149 = 4.895031452178955e-06

Training epoch-45 batch-150
Running loss of epoch-45 batch-150 = 6.804941222071648e-06

Training epoch-45 batch-151
Running loss of epoch-45 batch-151 = 1.7134007066488266e-06

Training epoch-45 batch-152
Running loss of epoch-45 batch-152 = 2.3047905415296555e-06

Training epoch-45 batch-153
Running loss of epoch-45 batch-153 = 6.5588392317295074e-06

Training epoch-45 batch-154
Running loss of epoch-45 batch-154 = 2.319889608770609e-05

Training epoch-45 batch-155
Running loss of epoch-45 batch-155 = 3.4892000257968903e-06

Training epoch-45 batch-156
Running loss of epoch-45 batch-156 = 1.7459504306316376e-05

Training epoch-45 batch-157
Running loss of epoch-45 batch-157 = 0.0002167392522096634

Finished training epoch-45.



Average train loss at epoch-45 = 1.1780573800206184e-05

Started Evaluation

Average val loss at epoch-45 = 5.148854534876974

Accuracy for classes:
Accuracy for class equals is: 67.99 %
Accuracy for class main is: 46.89 %
Accuracy for class setUp is: 53.11 %
Accuracy for class onCreate is: 43.39 %
Accuracy for class toString is: 42.32 %
Accuracy for class run is: 35.16 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 22.65 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 27.95 %

Overall Accuracy = 45.02 %

Finished Evaluation



Started training epoch-46


Training epoch-46 batch-1
Running loss of epoch-46 batch-1 = 2.8687063604593277e-06

Training epoch-46 batch-2
Running loss of epoch-46 batch-2 = 4.4907908886671066e-05

Training epoch-46 batch-3
Running loss of epoch-46 batch-3 = 2.6512425392866135e-06

Training epoch-46 batch-4
Running loss of epoch-46 batch-4 = 1.8535647541284561e-06

Training epoch-46 batch-5
Running loss of epoch-46 batch-5 = 1.948559656739235e-06

Training epoch-46 batch-6
Running loss of epoch-46 batch-6 = 2.4165143258869648e-05

Training epoch-46 batch-7
Running loss of epoch-46 batch-7 = 3.3890828490257263e-06

Training epoch-46 batch-8
Running loss of epoch-46 batch-8 = 2.747762482613325e-05

Training epoch-46 batch-9
Running loss of epoch-46 batch-9 = 3.4463824704289436e-05

Training epoch-46 batch-10
Running loss of epoch-46 batch-10 = 6.949994713068008e-07

Training epoch-46 batch-11
Running loss of epoch-46 batch-11 = 2.636108547449112e-06

Training epoch-46 batch-12
Running loss of epoch-46 batch-12 = 2.3599714040756226e-06

Training epoch-46 batch-13
Running loss of epoch-46 batch-13 = 3.2938201911747456e-05

Training epoch-46 batch-14
Running loss of epoch-46 batch-14 = 9.348499588668346e-06

Training epoch-46 batch-15
Running loss of epoch-46 batch-15 = 6.166752427816391e-06

Training epoch-46 batch-16
Running loss of epoch-46 batch-16 = 1.800211612135172e-05

Training epoch-46 batch-17
Running loss of epoch-46 batch-17 = 4.007015377283096e-06

Training epoch-46 batch-18
Running loss of epoch-46 batch-18 = 7.139635272324085e-06

Training epoch-46 batch-19
Running loss of epoch-46 batch-19 = 1.375097781419754e-06

Training epoch-46 batch-20
Running loss of epoch-46 batch-20 = 1.956184860318899e-05

Training epoch-46 batch-21
Running loss of epoch-46 batch-21 = 3.5229604691267014e-06

Training epoch-46 batch-22
Running loss of epoch-46 batch-22 = 5.049398168921471e-06

Training epoch-46 batch-23
Running loss of epoch-46 batch-23 = 4.450441338121891e-06

Training epoch-46 batch-24
Running loss of epoch-46 batch-24 = 5.3121475502848625e-06

Training epoch-46 batch-25
Running loss of epoch-46 batch-25 = 1.3846438378095627e-06

Training epoch-46 batch-26
Running loss of epoch-46 batch-26 = 1.2005097232758999e-05

Training epoch-46 batch-27
Running loss of epoch-46 batch-27 = 3.4582335501909256e-06

Training epoch-46 batch-28
Running loss of epoch-46 batch-28 = 1.4511868357658386e-05

Training epoch-46 batch-29
Running loss of epoch-46 batch-29 = 8.314615115523338e-06

Training epoch-46 batch-30
Running loss of epoch-46 batch-30 = 4.879315383732319e-06

Training epoch-46 batch-31
Running loss of epoch-46 batch-31 = 6.533227860927582e-07

Training epoch-46 batch-32
Running loss of epoch-46 batch-32 = 6.421469151973724e-07

Training epoch-46 batch-33
Running loss of epoch-46 batch-33 = 9.66247171163559e-07

Training epoch-46 batch-34
Running loss of epoch-46 batch-34 = 5.188747309148312e-06

Training epoch-46 batch-35
Running loss of epoch-46 batch-35 = 3.5974662750959396e-06

Training epoch-46 batch-36
Running loss of epoch-46 batch-36 = 1.2170057743787766e-06

Training epoch-46 batch-37
Running loss of epoch-46 batch-37 = 4.635658115148544e-06

Training epoch-46 batch-38
Running loss of epoch-46 batch-38 = 1.7827842384576797e-06

Training epoch-46 batch-39
Running loss of epoch-46 batch-39 = 2.3907050490379333e-06

Training epoch-46 batch-40
Running loss of epoch-46 batch-40 = 4.7263456508517265e-06

Training epoch-46 batch-41
Running loss of epoch-46 batch-41 = 2.0742882043123245e-06

Training epoch-46 batch-42
Running loss of epoch-46 batch-42 = 4.0281214751303196e-05

Training epoch-46 batch-43
Running loss of epoch-46 batch-43 = 2.6371795684099197e-05

Training epoch-46 batch-44
Running loss of epoch-46 batch-44 = 1.269625499844551e-06

Training epoch-46 batch-45
Running loss of epoch-46 batch-45 = 2.216314896941185e-06

Training epoch-46 batch-46
Running loss of epoch-46 batch-46 = 1.7597340047359467e-06

Training epoch-46 batch-47
Running loss of epoch-46 batch-47 = 2.3777014575898647e-05

Training epoch-46 batch-48
Running loss of epoch-46 batch-48 = 1.5576370060443878e-06

Training epoch-46 batch-49
Running loss of epoch-46 batch-49 = 2.675340510904789e-06

Training epoch-46 batch-50
Running loss of epoch-46 batch-50 = 1.4968682080507278e-06

Training epoch-46 batch-51
Running loss of epoch-46 batch-51 = 1.8308288417756557e-05

Training epoch-46 batch-52
Running loss of epoch-46 batch-52 = 5.269178654998541e-05

Training epoch-46 batch-53
Running loss of epoch-46 batch-53 = 2.580927684903145e-06

Training epoch-46 batch-54
Running loss of epoch-46 batch-54 = 2.652057446539402e-05

Training epoch-46 batch-55
Running loss of epoch-46 batch-55 = 1.217937096953392e-06

Training epoch-46 batch-56
Running loss of epoch-46 batch-56 = 1.7890706658363342e-05

Training epoch-46 batch-57
Running loss of epoch-46 batch-57 = 2.3958273231983185e-06

Training epoch-46 batch-58
Running loss of epoch-46 batch-58 = 4.203757271170616e-06

Training epoch-46 batch-59
Running loss of epoch-46 batch-59 = 3.859400749206543e-06

Training epoch-46 batch-60
Running loss of epoch-46 batch-60 = 6.591086275875568e-06

Training epoch-46 batch-61
Running loss of epoch-46 batch-61 = 2.9548536986112595e-06

Training epoch-46 batch-62
Running loss of epoch-46 batch-62 = 6.878748536109924e-06

Training epoch-46 batch-63
Running loss of epoch-46 batch-63 = 2.487562596797943e-06

Training epoch-46 batch-64
Running loss of epoch-46 batch-64 = 6.806454621255398e-06

Training epoch-46 batch-65
Running loss of epoch-46 batch-65 = 1.7392449080944061e-06

Training epoch-46 batch-66
Running loss of epoch-46 batch-66 = 4.3156323954463005e-06

Training epoch-46 batch-67
Running loss of epoch-46 batch-67 = 3.5150442272424698e-06

Training epoch-46 batch-68
Running loss of epoch-46 batch-68 = 2.3085158318281174e-06

Training epoch-46 batch-69
Running loss of epoch-46 batch-69 = 7.945927791297436e-06

Training epoch-46 batch-70
Running loss of epoch-46 batch-70 = 9.37609001994133e-07

Training epoch-46 batch-71
Running loss of epoch-46 batch-71 = 1.1897645890712738e-05

Training epoch-46 batch-72
Running loss of epoch-46 batch-72 = 4.589324817061424e-06

Training epoch-46 batch-73
Running loss of epoch-46 batch-73 = 1.4452147297561169e-05

Training epoch-46 batch-74
Running loss of epoch-46 batch-74 = 6.454531103372574e-06

Training epoch-46 batch-75
Running loss of epoch-46 batch-75 = 1.721247099339962e-05

Training epoch-46 batch-76
Running loss of epoch-46 batch-76 = 2.0756851881742477e-06

Training epoch-46 batch-77
Running loss of epoch-46 batch-77 = 1.455994788557291e-05

Training epoch-46 batch-78
Running loss of epoch-46 batch-78 = 1.7140526324510574e-05

Training epoch-46 batch-79
Running loss of epoch-46 batch-79 = 3.6784913390874863e-06

Training epoch-46 batch-80
Running loss of epoch-46 batch-80 = 3.7507852539420128e-06

Training epoch-46 batch-81
Running loss of epoch-46 batch-81 = 1.2794043868780136e-06

Training epoch-46 batch-82
Running loss of epoch-46 batch-82 = 2.059154212474823e-06

Training epoch-46 batch-83
Running loss of epoch-46 batch-83 = 3.569875843822956e-06

Training epoch-46 batch-84
Running loss of epoch-46 batch-84 = 7.318449206650257e-06

Training epoch-46 batch-85
Running loss of epoch-46 batch-85 = 1.221662387251854e-06

Training epoch-46 batch-86
Running loss of epoch-46 batch-86 = 7.314491085708141e-06

Training epoch-46 batch-87
Running loss of epoch-46 batch-87 = 2.0763836801052094e-06

Training epoch-46 batch-88
Running loss of epoch-46 batch-88 = 6.6570937633514404e-06

Training epoch-46 batch-89
Running loss of epoch-46 batch-89 = 1.4526303857564926e-06

Training epoch-46 batch-90
Running loss of epoch-46 batch-90 = 6.542890332639217e-06

Training epoch-46 batch-91
Running loss of epoch-46 batch-91 = 7.997383363544941e-06

Training epoch-46 batch-92
Running loss of epoch-46 batch-92 = 2.0239967852830887e-06

Training epoch-46 batch-93
Running loss of epoch-46 batch-93 = 1.926673576235771e-06

Training epoch-46 batch-94
Running loss of epoch-46 batch-94 = 3.4954864531755447e-06

Training epoch-46 batch-95
Running loss of epoch-46 batch-95 = 1.0845251381397247e-06

Training epoch-46 batch-96
Running loss of epoch-46 batch-96 = 2.2651394829154015e-05

Training epoch-46 batch-97
Running loss of epoch-46 batch-97 = 1.0770163498818874e-05

Training epoch-46 batch-98
Running loss of epoch-46 batch-98 = 3.3692922443151474e-06

Training epoch-46 batch-99
Running loss of epoch-46 batch-99 = 3.725301939994097e-05

Training epoch-46 batch-100
Running loss of epoch-46 batch-100 = 1.2262258678674698e-05

Training epoch-46 batch-101
Running loss of epoch-46 batch-101 = 7.590511813759804e-06

Training epoch-46 batch-102
Running loss of epoch-46 batch-102 = 1.0596122592687607e-06

Training epoch-46 batch-103
Running loss of epoch-46 batch-103 = 8.94232653081417e-06

Training epoch-46 batch-104
Running loss of epoch-46 batch-104 = 1.656683161854744e-05

Training epoch-46 batch-105
Running loss of epoch-46 batch-105 = 2.973712980747223e-06

Training epoch-46 batch-106
Running loss of epoch-46 batch-106 = 4.863250069320202e-06

Training epoch-46 batch-107
Running loss of epoch-46 batch-107 = 1.0593561455607414e-05

Training epoch-46 batch-108
Running loss of epoch-46 batch-108 = 2.051400952041149e-05

Training epoch-46 batch-109
Running loss of epoch-46 batch-109 = 1.1867377907037735e-06

Training epoch-46 batch-110
Running loss of epoch-46 batch-110 = 3.316672518849373e-06

Training epoch-46 batch-111
Running loss of epoch-46 batch-111 = 2.4280045181512833e-05

Training epoch-46 batch-112
Running loss of epoch-46 batch-112 = 2.3813685402274132e-05

Training epoch-46 batch-113
Running loss of epoch-46 batch-113 = 1.0856310836970806e-05

Training epoch-46 batch-114
Running loss of epoch-46 batch-114 = 2.8868671506643295e-06

Training epoch-46 batch-115
Running loss of epoch-46 batch-115 = 1.949956640601158e-06

Training epoch-46 batch-116
Running loss of epoch-46 batch-116 = 0.00013266096357256174

Training epoch-46 batch-117
Running loss of epoch-46 batch-117 = 5.2836257964372635e-06

Training epoch-46 batch-118
Running loss of epoch-46 batch-118 = 3.957655280828476e-06

Training epoch-46 batch-119
Running loss of epoch-46 batch-119 = 1.984415575861931e-06

Training epoch-46 batch-120
Running loss of epoch-46 batch-120 = 4.7957757487893105e-05

Training epoch-46 batch-121
Running loss of epoch-46 batch-121 = 1.3816053979098797e-05

Training epoch-46 batch-122
Running loss of epoch-46 batch-122 = 2.1096784621477127e-06

Training epoch-46 batch-123
Running loss of epoch-46 batch-123 = 1.0257819667458534e-05

Training epoch-46 batch-124
Running loss of epoch-46 batch-124 = 1.0638032108545303e-06

Training epoch-46 batch-125
Running loss of epoch-46 batch-125 = 1.3438984751701355e-06

Training epoch-46 batch-126
Running loss of epoch-46 batch-126 = 3.905268386006355e-06

Training epoch-46 batch-127
Running loss of epoch-46 batch-127 = 1.273350790143013e-06

Training epoch-46 batch-128
Running loss of epoch-46 batch-128 = 1.9944272935390472e-06

Training epoch-46 batch-129
Running loss of epoch-46 batch-129 = 7.977243512868881e-06

Training epoch-46 batch-130
Running loss of epoch-46 batch-130 = 1.6596284694969654e-05

Training epoch-46 batch-131
Running loss of epoch-46 batch-131 = 1.1513475328683853e-06

Training epoch-46 batch-132
Running loss of epoch-46 batch-132 = 4.698289558291435e-06

Training epoch-46 batch-133
Running loss of epoch-46 batch-133 = 7.007620297372341e-06

Training epoch-46 batch-134
Running loss of epoch-46 batch-134 = 1.1385185644030571e-05

Training epoch-46 batch-135
Running loss of epoch-46 batch-135 = 4.1692983359098434e-06

Training epoch-46 batch-136
Running loss of epoch-46 batch-136 = 1.4510005712509155e-06

Training epoch-46 batch-137
Running loss of epoch-46 batch-137 = 9.813887299969792e-05

Training epoch-46 batch-138
Running loss of epoch-46 batch-138 = 8.976319804787636e-06

Training epoch-46 batch-139
Running loss of epoch-46 batch-139 = 3.553694114089012e-06

Training epoch-46 batch-140
Running loss of epoch-46 batch-140 = 0.00012580555630847812

Training epoch-46 batch-141
Running loss of epoch-46 batch-141 = 2.0212028175592422e-06

Training epoch-46 batch-142
Running loss of epoch-46 batch-142 = 1.823296770453453e-06

Training epoch-46 batch-143
Running loss of epoch-46 batch-143 = 9.644526289775968e-05

Training epoch-46 batch-144
Running loss of epoch-46 batch-144 = 2.3802509531378746e-05

Training epoch-46 batch-145
Running loss of epoch-46 batch-145 = 2.046697773039341e-06

Training epoch-46 batch-146
Running loss of epoch-46 batch-146 = 3.6671990528702736e-06

Training epoch-46 batch-147
Running loss of epoch-46 batch-147 = 3.481050953269005e-06

Training epoch-46 batch-148
Running loss of epoch-46 batch-148 = 2.4156179279088974e-06

Training epoch-46 batch-149
Running loss of epoch-46 batch-149 = 1.3953540474176407e-06

Training epoch-46 batch-150
Running loss of epoch-46 batch-150 = 1.200730912387371e-05

Training epoch-46 batch-151
Running loss of epoch-46 batch-151 = 6.257323548197746e-06

Training epoch-46 batch-152
Running loss of epoch-46 batch-152 = 9.016715921461582e-06

Training epoch-46 batch-153
Running loss of epoch-46 batch-153 = 5.480949766933918e-06

Training epoch-46 batch-154
Running loss of epoch-46 batch-154 = 2.054101787507534e-05

Training epoch-46 batch-155
Running loss of epoch-46 batch-155 = 2.230401150882244e-06

Training epoch-46 batch-156
Running loss of epoch-46 batch-156 = 3.292923793196678e-06

Training epoch-46 batch-157
Running loss of epoch-46 batch-157 = 3.44105064868927e-05

Finished training epoch-46.



Average train loss at epoch-46 = 1.1000662669539452e-05

Started Evaluation

Average val loss at epoch-46 = 5.159625489460795

Accuracy for classes:
Accuracy for class equals is: 67.99 %
Accuracy for class main is: 47.54 %
Accuracy for class setUp is: 53.28 %
Accuracy for class onCreate is: 44.88 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 35.62 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.63 %
Accuracy for class execute is: 20.48 %
Accuracy for class get is: 29.74 %

Overall Accuracy = 45.43 %

Finished Evaluation



Started training epoch-47


Training epoch-47 batch-1
Running loss of epoch-47 batch-1 = 9.336508810520172e-06

Training epoch-47 batch-2
Running loss of epoch-47 batch-2 = 5.480600520968437e-06

Training epoch-47 batch-3
Running loss of epoch-47 batch-3 = 8.285394869744778e-06

Training epoch-47 batch-4
Running loss of epoch-47 batch-4 = 5.549401976168156e-06

Training epoch-47 batch-5
Running loss of epoch-47 batch-5 = 6.150919944047928e-06

Training epoch-47 batch-6
Running loss of epoch-47 batch-6 = 2.7707661502063274e-05

Training epoch-47 batch-7
Running loss of epoch-47 batch-7 = 2.175336703658104e-06

Training epoch-47 batch-8
Running loss of epoch-47 batch-8 = 1.2875883840024471e-05

Training epoch-47 batch-9
Running loss of epoch-47 batch-9 = 9.114854037761688e-06

Training epoch-47 batch-10
Running loss of epoch-47 batch-10 = 1.4222459867596626e-05

Training epoch-47 batch-11
Running loss of epoch-47 batch-11 = 4.4442713260650635e-06

Training epoch-47 batch-12
Running loss of epoch-47 batch-12 = 1.1504162102937698e-06

Training epoch-47 batch-13
Running loss of epoch-47 batch-13 = 9.264331310987473e-07

Training epoch-47 batch-14
Running loss of epoch-47 batch-14 = 3.156019374728203e-06

Training epoch-47 batch-15
Running loss of epoch-47 batch-15 = 2.8434558771550655e-05

Training epoch-47 batch-16
Running loss of epoch-47 batch-16 = 3.28203896060586e-05

Training epoch-47 batch-17
Running loss of epoch-47 batch-17 = 4.537636414170265e-06

Training epoch-47 batch-18
Running loss of epoch-47 batch-18 = 5.333218723535538e-06

Training epoch-47 batch-19
Running loss of epoch-47 batch-19 = 9.061535820364952e-06

Training epoch-47 batch-20
Running loss of epoch-47 batch-20 = 2.9088696464896202e-05

Training epoch-47 batch-21
Running loss of epoch-47 batch-21 = 5.370704457163811e-06

Training epoch-47 batch-22
Running loss of epoch-47 batch-22 = 2.5047454982995987e-05

Training epoch-47 batch-23
Running loss of epoch-47 batch-23 = 8.598552085459232e-06

Training epoch-47 batch-24
Running loss of epoch-47 batch-24 = 1.380685716867447e-06

Training epoch-47 batch-25
Running loss of epoch-47 batch-25 = 3.1585805118083954e-06

Training epoch-47 batch-26
Running loss of epoch-47 batch-26 = 2.845190465450287e-06

Training epoch-47 batch-27
Running loss of epoch-47 batch-27 = 3.8230791687965393e-07

Training epoch-47 batch-28
Running loss of epoch-47 batch-28 = 1.7688143998384476e-06

Training epoch-47 batch-29
Running loss of epoch-47 batch-29 = 1.176726073026657e-06

Training epoch-47 batch-30
Running loss of epoch-47 batch-30 = 2.812827005982399e-06

Training epoch-47 batch-31
Running loss of epoch-47 batch-31 = 1.2168544344604015e-05

Training epoch-47 batch-32
Running loss of epoch-47 batch-32 = 2.3117754608392715e-06

Training epoch-47 batch-33
Running loss of epoch-47 batch-33 = 4.4300686568021774e-06

Training epoch-47 batch-34
Running loss of epoch-47 batch-34 = 2.634315751492977e-05

Training epoch-47 batch-35
Running loss of epoch-47 batch-35 = 4.444504156708717e-06

Training epoch-47 batch-36
Running loss of epoch-47 batch-36 = 2.251937985420227e-06

Training epoch-47 batch-37
Running loss of epoch-47 batch-37 = 1.0475050657987595e-06

Training epoch-47 batch-38
Running loss of epoch-47 batch-38 = 3.3641699701547623e-06

Training epoch-47 batch-39
Running loss of epoch-47 batch-39 = 1.2980308383703232e-06

Training epoch-47 batch-40
Running loss of epoch-47 batch-40 = 3.872322849929333e-06

Training epoch-47 batch-41
Running loss of epoch-47 batch-41 = 1.6218982636928558e-06

Training epoch-47 batch-42
Running loss of epoch-47 batch-42 = 1.6083940863609314e-05

Training epoch-47 batch-43
Running loss of epoch-47 batch-43 = 1.0068295523524284e-05

Training epoch-47 batch-44
Running loss of epoch-47 batch-44 = 8.777715265750885e-07

Training epoch-47 batch-45
Running loss of epoch-47 batch-45 = 4.888686817139387e-05

Training epoch-47 batch-46
Running loss of epoch-47 batch-46 = 3.002118319272995e-06

Training epoch-47 batch-47
Running loss of epoch-47 batch-47 = 6.498652510344982e-06

Training epoch-47 batch-48
Running loss of epoch-47 batch-48 = 1.4578108675777912e-05

Training epoch-47 batch-49
Running loss of epoch-47 batch-49 = 2.1546147763729095e-06

Training epoch-47 batch-50
Running loss of epoch-47 batch-50 = 4.903646185994148e-06

Training epoch-47 batch-51
Running loss of epoch-47 batch-51 = 2.7188798412680626e-06

Training epoch-47 batch-52
Running loss of epoch-47 batch-52 = 1.5667174011468887e-06

Training epoch-47 batch-53
Running loss of epoch-47 batch-53 = 2.741115167737007e-06

Training epoch-47 batch-54
Running loss of epoch-47 batch-54 = 1.0880175977945328e-06

Training epoch-47 batch-55
Running loss of epoch-47 batch-55 = 4.650792106986046e-06

Training epoch-47 batch-56
Running loss of epoch-47 batch-56 = 2.5015324354171753e-06

Training epoch-47 batch-57
Running loss of epoch-47 batch-57 = 1.1504162102937698e-06

Training epoch-47 batch-58
Running loss of epoch-47 batch-58 = 4.1191233322024345e-06

Training epoch-47 batch-59
Running loss of epoch-47 batch-59 = 3.0353781767189503e-05

Training epoch-47 batch-60
Running loss of epoch-47 batch-60 = 1.6996636986732483e-06

Training epoch-47 batch-61
Running loss of epoch-47 batch-61 = 2.3257452994585037e-06

Training epoch-47 batch-62
Running loss of epoch-47 batch-62 = 3.0869035981595516e-05

Training epoch-47 batch-63
Running loss of epoch-47 batch-63 = 1.0751420632004738e-05

Training epoch-47 batch-64
Running loss of epoch-47 batch-64 = 7.338821887969971e-06

Training epoch-47 batch-65
Running loss of epoch-47 batch-65 = 8.827773854136467e-06

Training epoch-47 batch-66
Running loss of epoch-47 batch-66 = 3.2772659324109554e-05

Training epoch-47 batch-67
Running loss of epoch-47 batch-67 = 1.8932740204036236e-05

Training epoch-47 batch-68
Running loss of epoch-47 batch-68 = 1.6372650861740112e-06

Training epoch-47 batch-69
Running loss of epoch-47 batch-69 = 4.104338586330414e-06

Training epoch-47 batch-70
Running loss of epoch-47 batch-70 = 1.3138633221387863e-06

Training epoch-47 batch-71
Running loss of epoch-47 batch-71 = 1.0069925338029861e-06

Training epoch-47 batch-72
Running loss of epoch-47 batch-72 = 2.359272912144661e-06

Training epoch-47 batch-73
Running loss of epoch-47 batch-73 = 1.5532132238149643e-06

Training epoch-47 batch-74
Running loss of epoch-47 batch-74 = 1.657067332416773e-05

Training epoch-47 batch-75
Running loss of epoch-47 batch-75 = 3.878260031342506e-06

Training epoch-47 batch-76
Running loss of epoch-47 batch-76 = 4.9405149184167385e-05

Training epoch-47 batch-77
Running loss of epoch-47 batch-77 = 9.236740879714489e-06

Training epoch-47 batch-78
Running loss of epoch-47 batch-78 = 8.503906428813934e-06

Training epoch-47 batch-79
Running loss of epoch-47 batch-79 = 8.520670235157013e-06

Training epoch-47 batch-80
Running loss of epoch-47 batch-80 = 5.06651122123003e-06

Training epoch-47 batch-81
Running loss of epoch-47 batch-81 = 3.4116674214601517e-06

Training epoch-47 batch-82
Running loss of epoch-47 batch-82 = 3.229128196835518e-06

Training epoch-47 batch-83
Running loss of epoch-47 batch-83 = 1.061009243130684e-06

Training epoch-47 batch-84
Running loss of epoch-47 batch-84 = 2.3420434445142746e-06

Training epoch-47 batch-85
Running loss of epoch-47 batch-85 = 3.10409814119339e-06

Training epoch-47 batch-86
Running loss of epoch-47 batch-86 = 1.8195714801549911e-06

Training epoch-47 batch-87
Running loss of epoch-47 batch-87 = 2.866145223379135e-06

Training epoch-47 batch-88
Running loss of epoch-47 batch-88 = 1.1453754268586636e-05

Training epoch-47 batch-89
Running loss of epoch-47 batch-89 = 2.2596213966608047e-06

Training epoch-47 batch-90
Running loss of epoch-47 batch-90 = 1.1160038411617279e-05

Training epoch-47 batch-91
Running loss of epoch-47 batch-91 = 2.3248139768838882e-06

Training epoch-47 batch-92
Running loss of epoch-47 batch-92 = 2.6722322218120098e-05

Training epoch-47 batch-93
Running loss of epoch-47 batch-93 = 1.4276127330958843e-05

Training epoch-47 batch-94
Running loss of epoch-47 batch-94 = 8.261296898126602e-06

Training epoch-47 batch-95
Running loss of epoch-47 batch-95 = 1.0416842997074127e-06

Training epoch-47 batch-96
Running loss of epoch-47 batch-96 = 3.5713892430067062e-06

Training epoch-47 batch-97
Running loss of epoch-47 batch-97 = 1.4028046280145645e-06

Training epoch-47 batch-98
Running loss of epoch-47 batch-98 = 2.057291567325592e-06

Training epoch-47 batch-99
Running loss of epoch-47 batch-99 = 7.517402991652489e-06

Training epoch-47 batch-100
Running loss of epoch-47 batch-100 = 9.541399776935577e-07

Training epoch-47 batch-101
Running loss of epoch-47 batch-101 = 6.6070351749658585e-06

Training epoch-47 batch-102
Running loss of epoch-47 batch-102 = 6.119604222476482e-06

Training epoch-47 batch-103
Running loss of epoch-47 batch-103 = 5.061563570052385e-05

Training epoch-47 batch-104
Running loss of epoch-47 batch-104 = 2.7923379093408585e-06

Training epoch-47 batch-105
Running loss of epoch-47 batch-105 = 6.855418905615807e-05

Training epoch-47 batch-106
Running loss of epoch-47 batch-106 = 2.2186432033777237e-06

Training epoch-47 batch-107
Running loss of epoch-47 batch-107 = 4.8229703679680824e-06

Training epoch-47 batch-108
Running loss of epoch-47 batch-108 = 9.023729944601655e-05

Training epoch-47 batch-109
Running loss of epoch-47 batch-109 = 3.7548597902059555e-06

Training epoch-47 batch-110
Running loss of epoch-47 batch-110 = 1.4596735127270222e-05

Training epoch-47 batch-111
Running loss of epoch-47 batch-111 = 2.5238841772079468e-06

Training epoch-47 batch-112
Running loss of epoch-47 batch-112 = 2.989545464515686e-06

Training epoch-47 batch-113
Running loss of epoch-47 batch-113 = 6.210757419466972e-06

Training epoch-47 batch-114
Running loss of epoch-47 batch-114 = 6.528571248054504e-07

Training epoch-47 batch-115
Running loss of epoch-47 batch-115 = 5.121843423694372e-05

Training epoch-47 batch-116
Running loss of epoch-47 batch-116 = 3.016553819179535e-06

Training epoch-47 batch-117
Running loss of epoch-47 batch-117 = 1.780688762664795e-06

Training epoch-47 batch-118
Running loss of epoch-47 batch-118 = 6.91879540681839e-06

Training epoch-47 batch-119
Running loss of epoch-47 batch-119 = 1.490593422204256e-05

Training epoch-47 batch-120
Running loss of epoch-47 batch-120 = 4.391418769955635e-06

Training epoch-47 batch-121
Running loss of epoch-47 batch-121 = 2.087559551000595e-06

Training epoch-47 batch-122
Running loss of epoch-47 batch-122 = 4.184432327747345e-06

Training epoch-47 batch-123
Running loss of epoch-47 batch-123 = 7.573864422738552e-06

Training epoch-47 batch-124
Running loss of epoch-47 batch-124 = 2.1462328732013702e-06

Training epoch-47 batch-125
Running loss of epoch-47 batch-125 = 4.359986633062363e-06

Training epoch-47 batch-126
Running loss of epoch-47 batch-126 = 1.3557728379964828e-06

Training epoch-47 batch-127
Running loss of epoch-47 batch-127 = 7.602153345942497e-06

Training epoch-47 batch-128
Running loss of epoch-47 batch-128 = 3.996421582996845e-06

Training epoch-47 batch-129
Running loss of epoch-47 batch-129 = 1.0026269592344761e-05

Training epoch-47 batch-130
Running loss of epoch-47 batch-130 = 1.0798685252666473e-06

Training epoch-47 batch-131
Running loss of epoch-47 batch-131 = 3.855442628264427e-06

Training epoch-47 batch-132
Running loss of epoch-47 batch-132 = 7.278285920619965e-07

Training epoch-47 batch-133
Running loss of epoch-47 batch-133 = 5.054287612438202e-06

Training epoch-47 batch-134
Running loss of epoch-47 batch-134 = 8.873175829648972e-07

Training epoch-47 batch-135
Running loss of epoch-47 batch-135 = 4.076631739735603e-06

Training epoch-47 batch-136
Running loss of epoch-47 batch-136 = 3.9003556594252586e-05

Training epoch-47 batch-137
Running loss of epoch-47 batch-137 = 2.927263267338276e-06

Training epoch-47 batch-138
Running loss of epoch-47 batch-138 = 1.9044033251702785e-05

Training epoch-47 batch-139
Running loss of epoch-47 batch-139 = 6.895978003740311e-06

Training epoch-47 batch-140
Running loss of epoch-47 batch-140 = 1.8808059394359589e-06

Training epoch-47 batch-141
Running loss of epoch-47 batch-141 = 3.396428655833006e-05

Training epoch-47 batch-142
Running loss of epoch-47 batch-142 = 4.206644371151924e-05

Training epoch-47 batch-143
Running loss of epoch-47 batch-143 = 4.3067848309874535e-06

Training epoch-47 batch-144
Running loss of epoch-47 batch-144 = 5.448237061500549e-07

Training epoch-47 batch-145
Running loss of epoch-47 batch-145 = 1.247902400791645e-05

Training epoch-47 batch-146
Running loss of epoch-47 batch-146 = 4.463712684810162e-06

Training epoch-47 batch-147
Running loss of epoch-47 batch-147 = 1.917709596455097e-06

Training epoch-47 batch-148
Running loss of epoch-47 batch-148 = 5.452893674373627e-06

Training epoch-47 batch-149
Running loss of epoch-47 batch-149 = 0.00016087136464193463

Training epoch-47 batch-150
Running loss of epoch-47 batch-150 = 3.7811696529388428e-06

Training epoch-47 batch-151
Running loss of epoch-47 batch-151 = 1.1788448318839073e-05

Training epoch-47 batch-152
Running loss of epoch-47 batch-152 = 3.244727849960327e-06

Training epoch-47 batch-153
Running loss of epoch-47 batch-153 = 1.1154916137456894e-06

Training epoch-47 batch-154
Running loss of epoch-47 batch-154 = 4.3050386011600494e-06

Training epoch-47 batch-155
Running loss of epoch-47 batch-155 = 3.303156699985266e-05

Training epoch-47 batch-156
Running loss of epoch-47 batch-156 = 5.4888660088181496e-06

Training epoch-47 batch-157
Running loss of epoch-47 batch-157 = 3.632158041000366e-06

Finished training epoch-47.



Average train loss at epoch-47 = 1.046033725142479e-05

Started Evaluation

Average val loss at epoch-47 = 5.163435487370742

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 50.82 %
Accuracy for class setUp is: 52.79 %
Accuracy for class onCreate is: 44.14 %
Accuracy for class toString is: 43.34 %
Accuracy for class run is: 34.47 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.63 %
Accuracy for class execute is: 17.27 %
Accuracy for class get is: 28.97 %

Overall Accuracy = 45.49 %

Finished Evaluation



Started training epoch-48


Training epoch-48 batch-1
Running loss of epoch-48 batch-1 = 3.0545052140951157e-06

Training epoch-48 batch-2
Running loss of epoch-48 batch-2 = 1.280149444937706e-05

Training epoch-48 batch-3
Running loss of epoch-48 batch-3 = 3.700144588947296e-06

Training epoch-48 batch-4
Running loss of epoch-48 batch-4 = 1.4146789908409119e-06

Training epoch-48 batch-5
Running loss of epoch-48 batch-5 = 1.1918600648641586e-06

Training epoch-48 batch-6
Running loss of epoch-48 batch-6 = 1.0479707270860672e-06

Training epoch-48 batch-7
Running loss of epoch-48 batch-7 = 3.2212119549512863e-06

Training epoch-48 batch-8
Running loss of epoch-48 batch-8 = 1.662224531173706e-05

Training epoch-48 batch-9
Running loss of epoch-48 batch-9 = 2.866610884666443e-06

Training epoch-48 batch-10
Running loss of epoch-48 batch-10 = 5.897483788430691e-06

Training epoch-48 batch-11
Running loss of epoch-48 batch-11 = 8.122413419187069e-06

Training epoch-48 batch-12
Running loss of epoch-48 batch-12 = 6.2462640926241875e-06

Training epoch-48 batch-13
Running loss of epoch-48 batch-13 = 7.702154107391834e-06

Training epoch-48 batch-14
Running loss of epoch-48 batch-14 = 2.1345913410186768e-06

Training epoch-48 batch-15
Running loss of epoch-48 batch-15 = 1.3494864106178284e-06

Training epoch-48 batch-16
Running loss of epoch-48 batch-16 = 9.422656148672104e-07

Training epoch-48 batch-17
Running loss of epoch-48 batch-17 = 2.7068890631198883e-06

Training epoch-48 batch-18
Running loss of epoch-48 batch-18 = 3.784662112593651e-06

Training epoch-48 batch-19
Running loss of epoch-48 batch-19 = 1.6044476069509983e-05

Training epoch-48 batch-20
Running loss of epoch-48 batch-20 = 8.540088310837746e-05

Training epoch-48 batch-21
Running loss of epoch-48 batch-21 = 1.0314397513866425e-06

Training epoch-48 batch-22
Running loss of epoch-48 batch-22 = 3.4461263567209244e-06

Training epoch-48 batch-23
Running loss of epoch-48 batch-23 = 3.050314262509346e-06

Training epoch-48 batch-24
Running loss of epoch-48 batch-24 = 2.523651346564293e-06

Training epoch-48 batch-25
Running loss of epoch-48 batch-25 = 4.64834738522768e-06

Training epoch-48 batch-26
Running loss of epoch-48 batch-26 = 5.301553755998611e-07

Training epoch-48 batch-27
Running loss of epoch-48 batch-27 = 6.726011633872986e-06

Training epoch-48 batch-28
Running loss of epoch-48 batch-28 = 2.819579094648361e-07

Training epoch-48 batch-29
Running loss of epoch-48 batch-29 = 1.4766119420528412e-06

Training epoch-48 batch-30
Running loss of epoch-48 batch-30 = 4.281289875507355e-06

Training epoch-48 batch-31
Running loss of epoch-48 batch-31 = 2.2924505174160004e-06

Training epoch-48 batch-32
Running loss of epoch-48 batch-32 = 2.0093866623938084e-05

Training epoch-48 batch-33
Running loss of epoch-48 batch-33 = 2.403510734438896e-06

Training epoch-48 batch-34
Running loss of epoch-48 batch-34 = 3.603554796427488e-05

Training epoch-48 batch-35
Running loss of epoch-48 batch-35 = 2.5548506528139114e-06

Training epoch-48 batch-36
Running loss of epoch-48 batch-36 = 9.378418326377869e-07

Training epoch-48 batch-37
Running loss of epoch-48 batch-37 = 7.987022399902344e-06

Training epoch-48 batch-38
Running loss of epoch-48 batch-38 = 2.180924639105797e-06

Training epoch-48 batch-39
Running loss of epoch-48 batch-39 = 7.811584509909153e-06

Training epoch-48 batch-40
Running loss of epoch-48 batch-40 = 8.561881259083748e-06

Training epoch-48 batch-41
Running loss of epoch-48 batch-41 = 2.223765477538109e-06

Training epoch-48 batch-42
Running loss of epoch-48 batch-42 = 2.1685846149921417e-06

Training epoch-48 batch-43
Running loss of epoch-48 batch-43 = 1.0362593457102776e-05

Training epoch-48 batch-44
Running loss of epoch-48 batch-44 = 1.2016389518976212e-06

Training epoch-48 batch-45
Running loss of epoch-48 batch-45 = 1.727021299302578e-06

Training epoch-48 batch-46
Running loss of epoch-48 batch-46 = 1.153978519141674e-05

Training epoch-48 batch-47
Running loss of epoch-48 batch-47 = 3.2709212973713875e-06

Training epoch-48 batch-48
Running loss of epoch-48 batch-48 = 3.311317414045334e-06

Training epoch-48 batch-49
Running loss of epoch-48 batch-49 = 1.4652032405138016e-06

Training epoch-48 batch-50
Running loss of epoch-48 batch-50 = 4.0463637560606e-06

Training epoch-48 batch-51
Running loss of epoch-48 batch-51 = 2.536224201321602e-06

Training epoch-48 batch-52
Running loss of epoch-48 batch-52 = 3.393262159079313e-05

Training epoch-48 batch-53
Running loss of epoch-48 batch-53 = 2.5588087737560272e-06

Training epoch-48 batch-54
Running loss of epoch-48 batch-54 = 2.1070591174066067e-05

Training epoch-48 batch-55
Running loss of epoch-48 batch-55 = 1.3436656445264816e-06

Training epoch-48 batch-56
Running loss of epoch-48 batch-56 = 3.5229604691267014e-06

Training epoch-48 batch-57
Running loss of epoch-48 batch-57 = 2.454034984111786e-06

Training epoch-48 batch-58
Running loss of epoch-48 batch-58 = 3.804964944720268e-05

Training epoch-48 batch-59
Running loss of epoch-48 batch-59 = 1.4121527783572674e-05

Training epoch-48 batch-60
Running loss of epoch-48 batch-60 = 8.139642886817455e-06

Training epoch-48 batch-61
Running loss of epoch-48 batch-61 = 1.3623153790831566e-05

Training epoch-48 batch-62
Running loss of epoch-48 batch-62 = 6.353016942739487e-06

Training epoch-48 batch-63
Running loss of epoch-48 batch-63 = 1.9763130694627762e-05

Training epoch-48 batch-64
Running loss of epoch-48 batch-64 = 3.912369720637798e-06

Training epoch-48 batch-65
Running loss of epoch-48 batch-65 = 1.3061799108982086e-06

Training epoch-48 batch-66
Running loss of epoch-48 batch-66 = 1.386797521263361e-05

Training epoch-48 batch-67
Running loss of epoch-48 batch-67 = 3.493437543511391e-05

Training epoch-48 batch-68
Running loss of epoch-48 batch-68 = 8.940696716308594e-07

Training epoch-48 batch-69
Running loss of epoch-48 batch-69 = 1.5818513929843903e-06

Training epoch-48 batch-70
Running loss of epoch-48 batch-70 = 1.5355180948972702e-06

Training epoch-48 batch-71
Running loss of epoch-48 batch-71 = 8.458970114588737e-06

Training epoch-48 batch-72
Running loss of epoch-48 batch-72 = 3.97954136133194e-06

Training epoch-48 batch-73
Running loss of epoch-48 batch-73 = 5.191523814573884e-05

Training epoch-48 batch-74
Running loss of epoch-48 batch-74 = 2.418411895632744e-06

Training epoch-48 batch-75
Running loss of epoch-48 batch-75 = 1.2833625078201294e-06

Training epoch-48 batch-76
Running loss of epoch-48 batch-76 = 1.872633583843708e-05

Training epoch-48 batch-77
Running loss of epoch-48 batch-77 = 9.066425263881683e-07

Training epoch-48 batch-78
Running loss of epoch-48 batch-78 = 3.633275628089905e-05

Training epoch-48 batch-79
Running loss of epoch-48 batch-79 = 4.262698348611593e-05

Training epoch-48 batch-80
Running loss of epoch-48 batch-80 = 1.794658601284027e-06

Training epoch-48 batch-81
Running loss of epoch-48 batch-81 = 1.6898848116397858e-06

Training epoch-48 batch-82
Running loss of epoch-48 batch-82 = 1.0356307029724121e-06

Training epoch-48 batch-83
Running loss of epoch-48 batch-83 = 1.1913944035768509e-06

Training epoch-48 batch-84
Running loss of epoch-48 batch-84 = 2.0810402929782867e-06

Training epoch-48 batch-85
Running loss of epoch-48 batch-85 = 2.5497283786535263e-06

Training epoch-48 batch-86
Running loss of epoch-48 batch-86 = 5.776528269052505e-07

Training epoch-48 batch-87
Running loss of epoch-48 batch-87 = 4.158937372267246e-06

Training epoch-48 batch-88
Running loss of epoch-48 batch-88 = 1.4336081221699715e-05

Training epoch-48 batch-89
Running loss of epoch-48 batch-89 = 7.142079994082451e-06

Training epoch-48 batch-90
Running loss of epoch-48 batch-90 = 9.206123650074005e-07

Training epoch-48 batch-91
Running loss of epoch-48 batch-91 = 1.6936101019382477e-06

Training epoch-48 batch-92
Running loss of epoch-48 batch-92 = 2.325046807527542e-06

Training epoch-48 batch-93
Running loss of epoch-48 batch-93 = 1.3886368833482265e-05

Training epoch-48 batch-94
Running loss of epoch-48 batch-94 = 2.2059772163629532e-05

Training epoch-48 batch-95
Running loss of epoch-48 batch-95 = 6.34416937828064e-06

Training epoch-48 batch-96
Running loss of epoch-48 batch-96 = 2.505374141037464e-06

Training epoch-48 batch-97
Running loss of epoch-48 batch-97 = 4.80900052934885e-06

Training epoch-48 batch-98
Running loss of epoch-48 batch-98 = 4.3477630242705345e-06

Training epoch-48 batch-99
Running loss of epoch-48 batch-99 = 3.1068455427885056e-05

Training epoch-48 batch-100
Running loss of epoch-48 batch-100 = 2.728553954511881e-05

Training epoch-48 batch-101
Running loss of epoch-48 batch-101 = 8.473056368529797e-06

Training epoch-48 batch-102
Running loss of epoch-48 batch-102 = 1.7796293832361698e-05

Training epoch-48 batch-103
Running loss of epoch-48 batch-103 = 1.7513521015644073e-06

Training epoch-48 batch-104
Running loss of epoch-48 batch-104 = 1.7227139323949814e-06

Training epoch-48 batch-105
Running loss of epoch-48 batch-105 = 9.821029379963875e-06

Training epoch-48 batch-106
Running loss of epoch-48 batch-106 = 4.5960769057273865e-07

Training epoch-48 batch-107
Running loss of epoch-48 batch-107 = 2.7548521757125854e-06

Training epoch-48 batch-108
Running loss of epoch-48 batch-108 = 1.8561258912086487e-06

Training epoch-48 batch-109
Running loss of epoch-48 batch-109 = 0.00013698090333491564

Training epoch-48 batch-110
Running loss of epoch-48 batch-110 = 2.860790118575096e-06

Training epoch-48 batch-111
Running loss of epoch-48 batch-111 = 1.6492675058543682e-05

Training epoch-48 batch-112
Running loss of epoch-48 batch-112 = 3.751949407160282e-06

Training epoch-48 batch-113
Running loss of epoch-48 batch-113 = 1.3676472008228302e-06

Training epoch-48 batch-114
Running loss of epoch-48 batch-114 = 5.00422902405262e-06

Training epoch-48 batch-115
Running loss of epoch-48 batch-115 = 2.5941990315914154e-06

Training epoch-48 batch-116
Running loss of epoch-48 batch-116 = 1.2889504432678223e-06

Training epoch-48 batch-117
Running loss of epoch-48 batch-117 = 5.345442332327366e-06

Training epoch-48 batch-118
Running loss of epoch-48 batch-118 = 2.8514768928289413e-06

Training epoch-48 batch-119
Running loss of epoch-48 batch-119 = 1.7790007404983044e-05

Training epoch-48 batch-120
Running loss of epoch-48 batch-120 = 2.436339855194092e-06

Training epoch-48 batch-121
Running loss of epoch-48 batch-121 = 1.8998980522155762e-06

Training epoch-48 batch-122
Running loss of epoch-48 batch-122 = 1.9064173102378845e-06

Training epoch-48 batch-123
Running loss of epoch-48 batch-123 = 3.557885065674782e-06

Training epoch-48 batch-124
Running loss of epoch-48 batch-124 = 1.0132789611816406e-06

Training epoch-48 batch-125
Running loss of epoch-48 batch-125 = 7.251044735312462e-06

Training epoch-48 batch-126
Running loss of epoch-48 batch-126 = 1.5008263289928436e-06

Training epoch-48 batch-127
Running loss of epoch-48 batch-127 = 4.214467480778694e-06

Training epoch-48 batch-128
Running loss of epoch-48 batch-128 = 5.929265171289444e-06

Training epoch-48 batch-129
Running loss of epoch-48 batch-129 = 1.702294684946537e-05

Training epoch-48 batch-130
Running loss of epoch-48 batch-130 = 6.220536306500435e-06

Training epoch-48 batch-131
Running loss of epoch-48 batch-131 = 3.255438059568405e-06

Training epoch-48 batch-132
Running loss of epoch-48 batch-132 = 1.9725412130355835e-06

Training epoch-48 batch-133
Running loss of epoch-48 batch-133 = 5.527981556952e-06

Training epoch-48 batch-134
Running loss of epoch-48 batch-134 = 1.0145013220608234e-05

Training epoch-48 batch-135
Running loss of epoch-48 batch-135 = 2.7122441679239273e-06

Training epoch-48 batch-136
Running loss of epoch-48 batch-136 = 2.710509579628706e-05

Training epoch-48 batch-137
Running loss of epoch-48 batch-137 = 3.020686563104391e-05

Training epoch-48 batch-138
Running loss of epoch-48 batch-138 = 0.0001530083827674389

Training epoch-48 batch-139
Running loss of epoch-48 batch-139 = 2.356991171836853e-05

Training epoch-48 batch-140
Running loss of epoch-48 batch-140 = 5.268258973956108e-06

Training epoch-48 batch-141
Running loss of epoch-48 batch-141 = 1.1918134987354279e-05

Training epoch-48 batch-142
Running loss of epoch-48 batch-142 = 7.42962583899498e-07

Training epoch-48 batch-143
Running loss of epoch-48 batch-143 = 6.888178177177906e-06

Training epoch-48 batch-144
Running loss of epoch-48 batch-144 = 3.7315767258405685e-06

Training epoch-48 batch-145
Running loss of epoch-48 batch-145 = 5.497131496667862e-07

Training epoch-48 batch-146
Running loss of epoch-48 batch-146 = 4.069413989782333e-06

Training epoch-48 batch-147
Running loss of epoch-48 batch-147 = 1.7531332559883595e-05

Training epoch-48 batch-148
Running loss of epoch-48 batch-148 = 2.73948535323143e-06

Training epoch-48 batch-149
Running loss of epoch-48 batch-149 = 1.2348988093435764e-05

Training epoch-48 batch-150
Running loss of epoch-48 batch-150 = 4.71330713480711e-06

Training epoch-48 batch-151
Running loss of epoch-48 batch-151 = 1.3320823200047016e-05

Training epoch-48 batch-152
Running loss of epoch-48 batch-152 = 2.3641623556613922e-06

Training epoch-48 batch-153
Running loss of epoch-48 batch-153 = 1.347903162240982e-05

Training epoch-48 batch-154
Running loss of epoch-48 batch-154 = 6.869318895041943e-06

Training epoch-48 batch-155
Running loss of epoch-48 batch-155 = 6.947549991309643e-06

Training epoch-48 batch-156
Running loss of epoch-48 batch-156 = 7.735798135399818e-06

Training epoch-48 batch-157
Running loss of epoch-48 batch-157 = 4.753470420837402e-06

Finished training epoch-48.



Average train loss at epoch-48 = 1.0023701563477516e-05

Started Evaluation

Average val loss at epoch-48 = 5.208412837041052

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 50.16 %
Accuracy for class setUp is: 53.61 %
Accuracy for class onCreate is: 43.28 %
Accuracy for class toString is: 43.00 %
Accuracy for class run is: 35.39 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.18 %
Accuracy for class execute is: 17.27 %
Accuracy for class get is: 28.97 %

Overall Accuracy = 45.37 %

Finished Evaluation



Started training epoch-49


Training epoch-49 batch-1
Running loss of epoch-49 batch-1 = 3.141118213534355e-05

Training epoch-49 batch-2
Running loss of epoch-49 batch-2 = 2.832384780049324e-06

Training epoch-49 batch-3
Running loss of epoch-49 batch-3 = 8.843373507261276e-06

Training epoch-49 batch-4
Running loss of epoch-49 batch-4 = 1.3654935173690319e-05

Training epoch-49 batch-5
Running loss of epoch-49 batch-5 = 6.177695468068123e-06

Training epoch-49 batch-6
Running loss of epoch-49 batch-6 = 3.476860001683235e-06

Training epoch-49 batch-7
Running loss of epoch-49 batch-7 = 2.7588102966547012e-06

Training epoch-49 batch-8
Running loss of epoch-49 batch-8 = 3.650086000561714e-06

Training epoch-49 batch-9
Running loss of epoch-49 batch-9 = 1.8016435205936432e-06

Training epoch-49 batch-10
Running loss of epoch-49 batch-10 = 4.301546141505241e-06

Training epoch-49 batch-11
Running loss of epoch-49 batch-11 = 1.509056892246008e-05

Training epoch-49 batch-12
Running loss of epoch-49 batch-12 = 1.5125609934329987e-05

Training epoch-49 batch-13
Running loss of epoch-49 batch-13 = 5.8692123275250196e-05

Training epoch-49 batch-14
Running loss of epoch-49 batch-14 = 3.4959521144628525e-06

Training epoch-49 batch-15
Running loss of epoch-49 batch-15 = 8.75513069331646e-06

Training epoch-49 batch-16
Running loss of epoch-49 batch-16 = 8.578004781156778e-05

Training epoch-49 batch-17
Running loss of epoch-49 batch-17 = 1.0756310075521469e-05

Training epoch-49 batch-18
Running loss of epoch-49 batch-18 = 1.0356307029724121e-06

Training epoch-49 batch-19
Running loss of epoch-49 batch-19 = 9.188894182443619e-06

Training epoch-49 batch-20
Running loss of epoch-49 batch-20 = 1.3924785889685154e-05

Training epoch-49 batch-21
Running loss of epoch-49 batch-21 = 1.839594915509224e-06

Training epoch-49 batch-22
Running loss of epoch-49 batch-22 = 9.608222171664238e-06

Training epoch-49 batch-23
Running loss of epoch-49 batch-23 = 5.545676685869694e-06

Training epoch-49 batch-24
Running loss of epoch-49 batch-24 = 3.846944309771061e-06

Training epoch-49 batch-25
Running loss of epoch-49 batch-25 = 6.313668563961983e-06

Training epoch-49 batch-26
Running loss of epoch-49 batch-26 = 1.1448166333138943e-05

Training epoch-49 batch-27
Running loss of epoch-49 batch-27 = 1.896568574011326e-05

Training epoch-49 batch-28
Running loss of epoch-49 batch-28 = 9.547686204314232e-06

Training epoch-49 batch-29
Running loss of epoch-49 batch-29 = 1.1667143553495407e-06

Training epoch-49 batch-30
Running loss of epoch-49 batch-30 = 3.965804353356361e-06

Training epoch-49 batch-31
Running loss of epoch-49 batch-31 = 9.023468010127544e-06

Training epoch-49 batch-32
Running loss of epoch-49 batch-32 = 2.7784844860434532e-06

Training epoch-49 batch-33
Running loss of epoch-49 batch-33 = 5.336361937224865e-06

Training epoch-49 batch-34
Running loss of epoch-49 batch-34 = 3.677327185869217e-06

Training epoch-49 batch-35
Running loss of epoch-49 batch-35 = 1.0215560905635357e-05

Training epoch-49 batch-36
Running loss of epoch-49 batch-36 = 1.4647375792264938e-06

Training epoch-49 batch-37
Running loss of epoch-49 batch-37 = 4.621222615242004e-06

Training epoch-49 batch-38
Running loss of epoch-49 batch-38 = 1.3238750398159027e-06

Training epoch-49 batch-39
Running loss of epoch-49 batch-39 = 1.8820865079760551e-06

Training epoch-49 batch-40
Running loss of epoch-49 batch-40 = 4.512956365942955e-06

Training epoch-49 batch-41
Running loss of epoch-49 batch-41 = 1.976732164621353e-06

Training epoch-49 batch-42
Running loss of epoch-49 batch-42 = 1.2531643733382225e-05

Training epoch-49 batch-43
Running loss of epoch-49 batch-43 = 2.908986061811447e-06

Training epoch-49 batch-44
Running loss of epoch-49 batch-44 = 9.629875421524048e-07

Training epoch-49 batch-45
Running loss of epoch-49 batch-45 = 2.789543941617012e-06

Training epoch-49 batch-46
Running loss of epoch-49 batch-46 = 1.2230593711137772e-06

Training epoch-49 batch-47
Running loss of epoch-49 batch-47 = 5.266396328806877e-06

Training epoch-49 batch-48
Running loss of epoch-49 batch-48 = 2.5122426450252533e-06

Training epoch-49 batch-49
Running loss of epoch-49 batch-49 = 1.434003934264183e-06

Training epoch-49 batch-50
Running loss of epoch-49 batch-50 = 9.589828550815582e-06

Training epoch-49 batch-51
Running loss of epoch-49 batch-51 = 1.402746420353651e-05

Training epoch-49 batch-52
Running loss of epoch-49 batch-52 = 1.5347497537732124e-05

Training epoch-49 batch-53
Running loss of epoch-49 batch-53 = 1.1753290891647339e-06

Training epoch-49 batch-54
Running loss of epoch-49 batch-54 = 2.2382009774446487e-06

Training epoch-49 batch-55
Running loss of epoch-49 batch-55 = 3.0922237783670425e-06

Training epoch-49 batch-56
Running loss of epoch-49 batch-56 = 3.4939730539917946e-06

Training epoch-49 batch-57
Running loss of epoch-49 batch-57 = 5.509704351425171e-06

Training epoch-49 batch-58
Running loss of epoch-49 batch-58 = 2.11973674595356e-05

Training epoch-49 batch-59
Running loss of epoch-49 batch-59 = 9.157578460872173e-06

Training epoch-49 batch-60
Running loss of epoch-49 batch-60 = 1.9494909793138504e-06

Training epoch-49 batch-61
Running loss of epoch-49 batch-61 = 7.366761565208435e-07

Training epoch-49 batch-62
Running loss of epoch-49 batch-62 = 2.473359927535057e-06

Training epoch-49 batch-63
Running loss of epoch-49 batch-63 = 5.7715107686817646e-05

Training epoch-49 batch-64
Running loss of epoch-49 batch-64 = 3.259396180510521e-06

Training epoch-49 batch-65
Running loss of epoch-49 batch-65 = 2.391170710325241e-06

Training epoch-49 batch-66
Running loss of epoch-49 batch-66 = 2.114987000823021e-05

Training epoch-49 batch-67
Running loss of epoch-49 batch-67 = 1.759035512804985e-06

Training epoch-49 batch-68
Running loss of epoch-49 batch-68 = 6.74789771437645e-06

Training epoch-49 batch-69
Running loss of epoch-49 batch-69 = 3.5192351788282394e-06

Training epoch-49 batch-70
Running loss of epoch-49 batch-70 = 1.3241078704595566e-06

Training epoch-49 batch-71
Running loss of epoch-49 batch-71 = 3.71286878362298e-05

Training epoch-49 batch-72
Running loss of epoch-49 batch-72 = 1.8204329535365105e-05

Training epoch-49 batch-73
Running loss of epoch-49 batch-73 = 8.842907845973969e-07

Training epoch-49 batch-74
Running loss of epoch-49 batch-74 = 2.0617968402802944e-05

Training epoch-49 batch-75
Running loss of epoch-49 batch-75 = 4.48769424110651e-06

Training epoch-49 batch-76
Running loss of epoch-49 batch-76 = 1.6533304005861282e-06

Training epoch-49 batch-77
Running loss of epoch-49 batch-77 = 3.750086762011051e-06

Training epoch-49 batch-78
Running loss of epoch-49 batch-78 = 1.6789417713880539e-06

Training epoch-49 batch-79
Running loss of epoch-49 batch-79 = 2.767890691757202e-06

Training epoch-49 batch-80
Running loss of epoch-49 batch-80 = 5.140103166922927e-05

Training epoch-49 batch-81
Running loss of epoch-49 batch-81 = 9.282957762479782e-07

Training epoch-49 batch-82
Running loss of epoch-49 batch-82 = 8.191913366317749e-06

Training epoch-49 batch-83
Running loss of epoch-49 batch-83 = 3.2766256481409073e-06

Training epoch-49 batch-84
Running loss of epoch-49 batch-84 = 1.2284261174499989e-05

Training epoch-49 batch-85
Running loss of epoch-49 batch-85 = 7.26024154573679e-06

Training epoch-49 batch-86
Running loss of epoch-49 batch-86 = 1.73947773873806e-06

Training epoch-49 batch-87
Running loss of epoch-49 batch-87 = 1.010950654745102e-06

Training epoch-49 batch-88
Running loss of epoch-49 batch-88 = 2.375338226556778e-06

Training epoch-49 batch-89
Running loss of epoch-49 batch-89 = 1.022417563945055e-05

Training epoch-49 batch-90
Running loss of epoch-49 batch-90 = 1.4868914149701595e-05

Training epoch-49 batch-91
Running loss of epoch-49 batch-91 = 2.675340510904789e-05

Training epoch-49 batch-92
Running loss of epoch-49 batch-92 = 1.7133308574557304e-05

Training epoch-49 batch-93
Running loss of epoch-49 batch-93 = 8.067581802606583e-07

Training epoch-49 batch-94
Running loss of epoch-49 batch-94 = 1.612817868590355e-06

Training epoch-49 batch-95
Running loss of epoch-49 batch-95 = 9.262003004550934e-07

Training epoch-49 batch-96
Running loss of epoch-49 batch-96 = 6.555579602718353e-06

Training epoch-49 batch-97
Running loss of epoch-49 batch-97 = 1.6328413039445877e-06

Training epoch-49 batch-98
Running loss of epoch-49 batch-98 = 2.021435648202896e-06

Training epoch-49 batch-99
Running loss of epoch-49 batch-99 = 2.3797620087862015e-06

Training epoch-49 batch-100
Running loss of epoch-49 batch-100 = 1.3371463865041733e-06

Training epoch-49 batch-101
Running loss of epoch-49 batch-101 = 3.5124830901622772e-06

Training epoch-49 batch-102
Running loss of epoch-49 batch-102 = 4.056957550346851e-06

Training epoch-49 batch-103
Running loss of epoch-49 batch-103 = 4.514236934483051e-06

Training epoch-49 batch-104
Running loss of epoch-49 batch-104 = 1.6835983842611313e-06

Training epoch-49 batch-105
Running loss of epoch-49 batch-105 = 6.944057531654835e-06

Training epoch-49 batch-106
Running loss of epoch-49 batch-106 = 6.816303357481956e-05

Training epoch-49 batch-107
Running loss of epoch-49 batch-107 = 2.49338336288929e-06

Training epoch-49 batch-108
Running loss of epoch-49 batch-108 = 3.5006552934646606e-05

Training epoch-49 batch-109
Running loss of epoch-49 batch-109 = 3.3276155591011047e-06

Training epoch-49 batch-110
Running loss of epoch-49 batch-110 = 5.222391337156296e-07

Training epoch-49 batch-111
Running loss of epoch-49 batch-111 = 2.575910184532404e-05

Training epoch-49 batch-112
Running loss of epoch-49 batch-112 = 2.0659063011407852e-06

Training epoch-49 batch-113
Running loss of epoch-49 batch-113 = 7.811118848621845e-06

Training epoch-49 batch-114
Running loss of epoch-49 batch-114 = 2.966728061437607e-06

Training epoch-49 batch-115
Running loss of epoch-49 batch-115 = 2.5241170078516006e-06

Training epoch-49 batch-116
Running loss of epoch-49 batch-116 = 8.293427526950836e-07

Training epoch-49 batch-117
Running loss of epoch-49 batch-117 = 0.00013703637523576617

Training epoch-49 batch-118
Running loss of epoch-49 batch-118 = 4.544621333479881e-06

Training epoch-49 batch-119
Running loss of epoch-49 batch-119 = 8.980277925729752e-07

Training epoch-49 batch-120
Running loss of epoch-49 batch-120 = 2.9245857149362564e-06

Training epoch-49 batch-121
Running loss of epoch-49 batch-121 = 3.4320401027798653e-06

Training epoch-49 batch-122
Running loss of epoch-49 batch-122 = 2.820277586579323e-06

Training epoch-49 batch-123
Running loss of epoch-49 batch-123 = 2.6902416720986366e-05

Training epoch-49 batch-124
Running loss of epoch-49 batch-124 = 4.7660432755947113e-07

Training epoch-49 batch-125
Running loss of epoch-49 batch-125 = 1.6025733202695847e-06

Training epoch-49 batch-126
Running loss of epoch-49 batch-126 = 5.1015522330999374e-06

Training epoch-49 batch-127
Running loss of epoch-49 batch-127 = 2.4635810405015945e-06

Training epoch-49 batch-128
Running loss of epoch-49 batch-128 = 4.37319977208972e-05

Training epoch-49 batch-129
Running loss of epoch-49 batch-129 = 1.5273690223693848e-06

Training epoch-49 batch-130
Running loss of epoch-49 batch-130 = 9.213224984705448e-06

Training epoch-49 batch-131
Running loss of epoch-49 batch-131 = 2.1941959857940674e-06

Training epoch-49 batch-132
Running loss of epoch-49 batch-132 = 4.863482899963856e-06

Training epoch-49 batch-133
Running loss of epoch-49 batch-133 = 3.353692591190338e-06

Training epoch-49 batch-134
Running loss of epoch-49 batch-134 = 8.292030543088913e-06

Training epoch-49 batch-135
Running loss of epoch-49 batch-135 = 2.444721758365631e-06

Training epoch-49 batch-136
Running loss of epoch-49 batch-136 = 2.383952960371971e-06

Training epoch-49 batch-137
Running loss of epoch-49 batch-137 = 4.68292273581028e-06

Training epoch-49 batch-138
Running loss of epoch-49 batch-138 = 1.5821540728211403e-05

Training epoch-49 batch-139
Running loss of epoch-49 batch-139 = 2.3816246539354324e-06

Training epoch-49 batch-140
Running loss of epoch-49 batch-140 = 1.1527445167303085e-06

Training epoch-49 batch-141
Running loss of epoch-49 batch-141 = 1.0184478014707565e-05

Training epoch-49 batch-142
Running loss of epoch-49 batch-142 = 1.1294614523649216e-06

Training epoch-49 batch-143
Running loss of epoch-49 batch-143 = 1.0582618415355682e-05

Training epoch-49 batch-144
Running loss of epoch-49 batch-144 = 5.623092874884605e-06

Training epoch-49 batch-145
Running loss of epoch-49 batch-145 = 8.745118975639343e-07

Training epoch-49 batch-146
Running loss of epoch-49 batch-146 = 5.885958671569824e-07

Training epoch-49 batch-147
Running loss of epoch-49 batch-147 = 3.057066351175308e-07

Training epoch-49 batch-148
Running loss of epoch-49 batch-148 = 5.918554961681366e-07

Training epoch-49 batch-149
Running loss of epoch-49 batch-149 = 4.2334431782364845e-06

Training epoch-49 batch-150
Running loss of epoch-49 batch-150 = 2.3592961952090263e-05

Training epoch-49 batch-151
Running loss of epoch-49 batch-151 = 5.023437552154064e-06

Training epoch-49 batch-152
Running loss of epoch-49 batch-152 = 2.1781306713819504e-06

Training epoch-49 batch-153
Running loss of epoch-49 batch-153 = 6.05778768658638e-06

Training epoch-49 batch-154
Running loss of epoch-49 batch-154 = 1.1352822184562683e-06

Training epoch-49 batch-155
Running loss of epoch-49 batch-155 = 1.8087448552250862e-05

Training epoch-49 batch-156
Running loss of epoch-49 batch-156 = 2.3720785975456238e-06

Training epoch-49 batch-157
Running loss of epoch-49 batch-157 = 7.875263690948486e-06

Finished training epoch-49.



Average train loss at epoch-49 = 9.344946965575218e-06

Started Evaluation

Average val loss at epoch-49 = 5.2582960285638505

Accuracy for classes:
Accuracy for class equals is: 68.48 %
Accuracy for class main is: 48.85 %
Accuracy for class setUp is: 53.77 %
Accuracy for class onCreate is: 42.32 %
Accuracy for class toString is: 43.00 %
Accuracy for class run is: 36.30 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.63 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 28.46 %

Overall Accuracy = 45.20 %

Finished Evaluation



Started training epoch-50


Training epoch-50 batch-1
Running loss of epoch-50 batch-1 = 7.416238076984882e-06

Training epoch-50 batch-2
Running loss of epoch-50 batch-2 = 3.984663635492325e-06

Training epoch-50 batch-3
Running loss of epoch-50 batch-3 = 2.198619768023491e-06

Training epoch-50 batch-4
Running loss of epoch-50 batch-4 = 1.4740508049726486e-06

Training epoch-50 batch-5
Running loss of epoch-50 batch-5 = 1.9548460841178894e-06

Training epoch-50 batch-6
Running loss of epoch-50 batch-6 = 2.3674219846725464e-06

Training epoch-50 batch-7
Running loss of epoch-50 batch-7 = 1.5320256352424622e-06

Training epoch-50 batch-8
Running loss of epoch-50 batch-8 = 1.2257834896445274e-05

Training epoch-50 batch-9
Running loss of epoch-50 batch-9 = 1.2353993952274323e-06

Training epoch-50 batch-10
Running loss of epoch-50 batch-10 = 3.9244769141077995e-05

Training epoch-50 batch-11
Running loss of epoch-50 batch-11 = 5.856039933860302e-06

Training epoch-50 batch-12
Running loss of epoch-50 batch-12 = 4.724133759737015e-06

Training epoch-50 batch-13
Running loss of epoch-50 batch-13 = 1.9879196770489216e-05

Training epoch-50 batch-14
Running loss of epoch-50 batch-14 = 2.439366653561592e-06

Training epoch-50 batch-15
Running loss of epoch-50 batch-15 = 7.809139788150787e-07

Training epoch-50 batch-16
Running loss of epoch-50 batch-16 = 8.905772119760513e-07

Training epoch-50 batch-17
Running loss of epoch-50 batch-17 = 5.006324499845505e-06

Training epoch-50 batch-18
Running loss of epoch-50 batch-18 = 5.187583155930042e-06

Training epoch-50 batch-19
Running loss of epoch-50 batch-19 = 1.2968899682164192e-05

Training epoch-50 batch-20
Running loss of epoch-50 batch-20 = 4.500732757151127e-05

Training epoch-50 batch-21
Running loss of epoch-50 batch-21 = 1.2814998626708984e-06

Training epoch-50 batch-22
Running loss of epoch-50 batch-22 = 5.9301964938640594e-06

Training epoch-50 batch-23
Running loss of epoch-50 batch-23 = 4.580826498568058e-06

Training epoch-50 batch-24
Running loss of epoch-50 batch-24 = 5.14741986989975e-06

Training epoch-50 batch-25
Running loss of epoch-50 batch-25 = 2.718414179980755e-05

Training epoch-50 batch-26
Running loss of epoch-50 batch-26 = 5.587935447692871e-07

Training epoch-50 batch-27
Running loss of epoch-50 batch-27 = 1.23586505651474e-06

Training epoch-50 batch-28
Running loss of epoch-50 batch-28 = 4.3639447540044785e-06

Training epoch-50 batch-29
Running loss of epoch-50 batch-29 = 2.788146957755089e-06

Training epoch-50 batch-30
Running loss of epoch-50 batch-30 = 1.7974525690078735e-06

Training epoch-50 batch-31
Running loss of epoch-50 batch-31 = 2.4421606212854385e-06

Training epoch-50 batch-32
Running loss of epoch-50 batch-32 = 2.2352905943989754e-05

Training epoch-50 batch-33
Running loss of epoch-50 batch-33 = 5.248119123280048e-06

Training epoch-50 batch-34
Running loss of epoch-50 batch-34 = 1.161952968686819e-05

Training epoch-50 batch-35
Running loss of epoch-50 batch-35 = 1.1730007827281952e-06

Training epoch-50 batch-36
Running loss of epoch-50 batch-36 = 2.4335458874702454e-06

Training epoch-50 batch-37
Running loss of epoch-50 batch-37 = 3.0067749321460724e-06

Training epoch-50 batch-38
Running loss of epoch-50 batch-38 = 2.554978709667921e-05

Training epoch-50 batch-39
Running loss of epoch-50 batch-39 = 7.584225386381149e-06

Training epoch-50 batch-40
Running loss of epoch-50 batch-40 = 5.454872734844685e-06

Training epoch-50 batch-41
Running loss of epoch-50 batch-41 = 1.2826407328248024e-05

Training epoch-50 batch-42
Running loss of epoch-50 batch-42 = 6.62636011838913e-07

Training epoch-50 batch-43
Running loss of epoch-50 batch-43 = 1.1123716831207275e-05

Training epoch-50 batch-44
Running loss of epoch-50 batch-44 = 9.757583029568195e-06

Training epoch-50 batch-45
Running loss of epoch-50 batch-45 = 4.145433194935322e-06

Training epoch-50 batch-46
Running loss of epoch-50 batch-46 = 1.435866579413414e-06

Training epoch-50 batch-47
Running loss of epoch-50 batch-47 = 2.838205546140671e-07

Training epoch-50 batch-48
Running loss of epoch-50 batch-48 = 6.312038749456406e-07

Training epoch-50 batch-49
Running loss of epoch-50 batch-49 = 2.875342033803463e-06

Training epoch-50 batch-50
Running loss of epoch-50 batch-50 = 6.202026270329952e-06

Training epoch-50 batch-51
Running loss of epoch-50 batch-51 = 9.073008550330997e-05

Training epoch-50 batch-52
Running loss of epoch-50 batch-52 = 5.69014810025692e-06

Training epoch-50 batch-53
Running loss of epoch-50 batch-53 = 6.443355232477188e-06

Training epoch-50 batch-54
Running loss of epoch-50 batch-54 = 1.0726507753133774e-06

Training epoch-50 batch-55
Running loss of epoch-50 batch-55 = 3.082212060689926e-06

Training epoch-50 batch-56
Running loss of epoch-50 batch-56 = 6.267218850553036e-06

Training epoch-50 batch-57
Running loss of epoch-50 batch-57 = 1.444946974515915e-05

Training epoch-50 batch-58
Running loss of epoch-50 batch-58 = 9.671784937381744e-07

Training epoch-50 batch-59
Running loss of epoch-50 batch-59 = 7.048831321299076e-06

Training epoch-50 batch-60
Running loss of epoch-50 batch-60 = 7.671769708395004e-07

Training epoch-50 batch-61
Running loss of epoch-50 batch-61 = 9.684357792139053e-06

Training epoch-50 batch-62
Running loss of epoch-50 batch-62 = 6.1157625168561935e-06

Training epoch-50 batch-63
Running loss of epoch-50 batch-63 = 4.052300937473774e-06

Training epoch-50 batch-64
Running loss of epoch-50 batch-64 = 1.394771970808506e-05

Training epoch-50 batch-65
Running loss of epoch-50 batch-65 = 1.216307282447815e-06

Training epoch-50 batch-66
Running loss of epoch-50 batch-66 = 1.9825529307127e-06

Training epoch-50 batch-67
Running loss of epoch-50 batch-67 = 2.809334546327591e-06

Training epoch-50 batch-68
Running loss of epoch-50 batch-68 = 6.204470992088318e-06

Training epoch-50 batch-69
Running loss of epoch-50 batch-69 = 1.5061814337968826e-06

Training epoch-50 batch-70
Running loss of epoch-50 batch-70 = 2.484302967786789e-06

Training epoch-50 batch-71
Running loss of epoch-50 batch-71 = 1.8777442164719105e-05

Training epoch-50 batch-72
Running loss of epoch-50 batch-72 = 3.2207462936639786e-06

Training epoch-50 batch-73
Running loss of epoch-50 batch-73 = 5.193869583308697e-06

Training epoch-50 batch-74
Running loss of epoch-50 batch-74 = 2.2807391360402107e-05

Training epoch-50 batch-75
Running loss of epoch-50 batch-75 = 3.2529933378100395e-06

Training epoch-50 batch-76
Running loss of epoch-50 batch-76 = 3.4468481317162514e-05

Training epoch-50 batch-77
Running loss of epoch-50 batch-77 = 2.1232874132692814e-05

Training epoch-50 batch-78
Running loss of epoch-50 batch-78 = 1.1301599442958832e-06

Training epoch-50 batch-79
Running loss of epoch-50 batch-79 = 2.31829471886158e-06

Training epoch-50 batch-80
Running loss of epoch-50 batch-80 = 9.436625987291336e-07

Training epoch-50 batch-81
Running loss of epoch-50 batch-81 = 2.066604793071747e-06

Training epoch-50 batch-82
Running loss of epoch-50 batch-82 = 9.017763659358025e-06

Training epoch-50 batch-83
Running loss of epoch-50 batch-83 = 2.4419277906417847e-06

Training epoch-50 batch-84
Running loss of epoch-50 batch-84 = 1.505482941865921e-06

Training epoch-50 batch-85
Running loss of epoch-50 batch-85 = 3.470596857368946e-05

Training epoch-50 batch-86
Running loss of epoch-50 batch-86 = 1.3902201317250729e-05

Training epoch-50 batch-87
Running loss of epoch-50 batch-87 = 4.146480932831764e-06

Training epoch-50 batch-88
Running loss of epoch-50 batch-88 = 9.72894486039877e-06

Training epoch-50 batch-89
Running loss of epoch-50 batch-89 = 1.0952353477478027e-06

Training epoch-50 batch-90
Running loss of epoch-50 batch-90 = 3.071245737373829e-05

Training epoch-50 batch-91
Running loss of epoch-50 batch-91 = 3.2600946724414825e-06

Training epoch-50 batch-92
Running loss of epoch-50 batch-92 = 1.4621764421463013e-05

Training epoch-50 batch-93
Running loss of epoch-50 batch-93 = 5.643698386847973e-06

Training epoch-50 batch-94
Running loss of epoch-50 batch-94 = 4.495144821703434e-06

Training epoch-50 batch-95
Running loss of epoch-50 batch-95 = 4.5425258576869965e-06

Training epoch-50 batch-96
Running loss of epoch-50 batch-96 = 2.8060749173164368e-06

Training epoch-50 batch-97
Running loss of epoch-50 batch-97 = 2.624932676553726e-06

Training epoch-50 batch-98
Running loss of epoch-50 batch-98 = 2.3126602172851562e-05

Training epoch-50 batch-99
Running loss of epoch-50 batch-99 = 2.149958163499832e-06

Training epoch-50 batch-100
Running loss of epoch-50 batch-100 = 0.00011100684059783816

Training epoch-50 batch-101
Running loss of epoch-50 batch-101 = 8.321600034832954e-06

Training epoch-50 batch-102
Running loss of epoch-50 batch-102 = 6.0389284044504166e-06

Training epoch-50 batch-103
Running loss of epoch-50 batch-103 = 4.346482455730438e-06

Training epoch-50 batch-104
Running loss of epoch-50 batch-104 = 1.041789073497057e-05

Training epoch-50 batch-105
Running loss of epoch-50 batch-105 = 3.7194695323705673e-06

Training epoch-50 batch-106
Running loss of epoch-50 batch-106 = 2.958136610686779e-05

Training epoch-50 batch-107
Running loss of epoch-50 batch-107 = 9.583309292793274e-07

Training epoch-50 batch-108
Running loss of epoch-50 batch-108 = 1.059751957654953e-05

Training epoch-50 batch-109
Running loss of epoch-50 batch-109 = 2.040993422269821e-06

Training epoch-50 batch-110
Running loss of epoch-50 batch-110 = 1.6789417713880539e-06

Training epoch-50 batch-111
Running loss of epoch-50 batch-111 = 1.2568198144435883e-06

Training epoch-50 batch-112
Running loss of epoch-50 batch-112 = 1.467578113079071e-05

Training epoch-50 batch-113
Running loss of epoch-50 batch-113 = 1.4030374586582184e-06

Training epoch-50 batch-114
Running loss of epoch-50 batch-114 = 1.4889519661664963e-06

Training epoch-50 batch-115
Running loss of epoch-50 batch-115 = 1.1576339602470398e-06

Training epoch-50 batch-116
Running loss of epoch-50 batch-116 = 4.658708348870277e-06

Training epoch-50 batch-117
Running loss of epoch-50 batch-117 = 3.8133002817630768e-06

Training epoch-50 batch-118
Running loss of epoch-50 batch-118 = 2.3593194782733917e-05

Training epoch-50 batch-119
Running loss of epoch-50 batch-119 = 6.714952178299427e-06

Training epoch-50 batch-120
Running loss of epoch-50 batch-120 = 2.007000148296356e-06

Training epoch-50 batch-121
Running loss of epoch-50 batch-121 = 1.6773119568824768e-06

Training epoch-50 batch-122
Running loss of epoch-50 batch-122 = 1.2971926480531693e-05

Training epoch-50 batch-123
Running loss of epoch-50 batch-123 = 2.1060463041067123e-05

Training epoch-50 batch-124
Running loss of epoch-50 batch-124 = 8.873874321579933e-06

Training epoch-50 batch-125
Running loss of epoch-50 batch-125 = 3.1043309718370438e-06

Training epoch-50 batch-126
Running loss of epoch-50 batch-126 = 2.934597432613373e-06

Training epoch-50 batch-127
Running loss of epoch-50 batch-127 = 2.2388994693756104e-06

Training epoch-50 batch-128
Running loss of epoch-50 batch-128 = 2.646970096975565e-05

Training epoch-50 batch-129
Running loss of epoch-50 batch-129 = 2.5033950805664062e-06

Training epoch-50 batch-130
Running loss of epoch-50 batch-130 = 7.351511158049107e-06

Training epoch-50 batch-131
Running loss of epoch-50 batch-131 = 2.148328348994255e-06

Training epoch-50 batch-132
Running loss of epoch-50 batch-132 = 7.227458991110325e-05

Training epoch-50 batch-133
Running loss of epoch-50 batch-133 = 6.456160917878151e-06

Training epoch-50 batch-134
Running loss of epoch-50 batch-134 = 1.0631047189235687e-06

Training epoch-50 batch-135
Running loss of epoch-50 batch-135 = 9.616021998226643e-06

Training epoch-50 batch-136
Running loss of epoch-50 batch-136 = 1.8882565200328827e-06

Training epoch-50 batch-137
Running loss of epoch-50 batch-137 = 1.0086223483085632e-06

Training epoch-50 batch-138
Running loss of epoch-50 batch-138 = 1.3080425560474396e-06

Training epoch-50 batch-139
Running loss of epoch-50 batch-139 = 9.278301149606705e-07

Training epoch-50 batch-140
Running loss of epoch-50 batch-140 = 1.2924429029226303e-06

Training epoch-50 batch-141
Running loss of epoch-50 batch-141 = 8.14057420939207e-06

Training epoch-50 batch-142
Running loss of epoch-50 batch-142 = 7.3784030973911285e-06

Training epoch-50 batch-143
Running loss of epoch-50 batch-143 = 2.1385494619607925e-06

Training epoch-50 batch-144
Running loss of epoch-50 batch-144 = 4.98676672577858e-06

Training epoch-50 batch-145
Running loss of epoch-50 batch-145 = 3.3886171877384186e-06

Training epoch-50 batch-146
Running loss of epoch-50 batch-146 = 8.261296898126602e-06

Training epoch-50 batch-147
Running loss of epoch-50 batch-147 = 1.0707881301641464e-06

Training epoch-50 batch-148
Running loss of epoch-50 batch-148 = 3.0436785891652107e-06

Training epoch-50 batch-149
Running loss of epoch-50 batch-149 = 4.602887202054262e-05

Training epoch-50 batch-150
Running loss of epoch-50 batch-150 = 7.292255759239197e-07

Training epoch-50 batch-151
Running loss of epoch-50 batch-151 = 1.2461096048355103e-06

Training epoch-50 batch-152
Running loss of epoch-50 batch-152 = 2.2988533601164818e-06

Training epoch-50 batch-153
Running loss of epoch-50 batch-153 = 4.458008334040642e-06

Training epoch-50 batch-154
Running loss of epoch-50 batch-154 = 3.5029370337724686e-06

Training epoch-50 batch-155
Running loss of epoch-50 batch-155 = 2.959277480840683e-06

Training epoch-50 batch-156
Running loss of epoch-50 batch-156 = 1.0624062269926071e-06

Training epoch-50 batch-157
Running loss of epoch-50 batch-157 = 3.725290298461914e-08

Finished training epoch-50.



Average train loss at epoch-50 = 8.818532526493072e-06

Started Evaluation

Average val loss at epoch-50 = 5.257921287887974

Accuracy for classes:
Accuracy for class equals is: 67.99 %
Accuracy for class main is: 47.21 %
Accuracy for class setUp is: 52.95 %
Accuracy for class onCreate is: 43.28 %
Accuracy for class toString is: 41.64 %
Accuracy for class run is: 36.30 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 22.20 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 28.21 %

Overall Accuracy = 45.08 %

Finished Evaluation



Started training epoch-51


Training epoch-51 batch-1
Running loss of epoch-51 batch-1 = 2.5147455744445324e-05

Training epoch-51 batch-2
Running loss of epoch-51 batch-2 = 3.1508971005678177e-06

Training epoch-51 batch-3
Running loss of epoch-51 batch-3 = 1.455657184123993e-06

Training epoch-51 batch-4
Running loss of epoch-51 batch-4 = 1.2347009032964706e-06

Training epoch-51 batch-5
Running loss of epoch-51 batch-5 = 8.288654498755932e-06

Training epoch-51 batch-6
Running loss of epoch-51 batch-6 = 7.525086402893066e-06

Training epoch-51 batch-7
Running loss of epoch-51 batch-7 = 1.0306015610694885e-05

Training epoch-51 batch-8
Running loss of epoch-51 batch-8 = 2.9730144888162613e-06

Training epoch-51 batch-9
Running loss of epoch-51 batch-9 = 9.852810762822628e-06

Training epoch-51 batch-10
Running loss of epoch-51 batch-10 = 1.6749836504459381e-06

Training epoch-51 batch-11
Running loss of epoch-51 batch-11 = 2.673361450433731e-06

Training epoch-51 batch-12
Running loss of epoch-51 batch-12 = 5.010515451431274e-06

Training epoch-51 batch-13
Running loss of epoch-51 batch-13 = 1.8498394638299942e-06

Training epoch-51 batch-14
Running loss of epoch-51 batch-14 = 2.4193432182073593e-06

Training epoch-51 batch-15
Running loss of epoch-51 batch-15 = 5.737761966884136e-06

Training epoch-51 batch-16
Running loss of epoch-51 batch-16 = 9.595532901585102e-06

Training epoch-51 batch-17
Running loss of epoch-51 batch-17 = 1.1599622666835785e-06

Training epoch-51 batch-18
Running loss of epoch-51 batch-18 = 2.5770394131541252e-05

Training epoch-51 batch-19
Running loss of epoch-51 batch-19 = 1.0312069207429886e-06

Training epoch-51 batch-20
Running loss of epoch-51 batch-20 = 8.363043889403343e-06

Training epoch-51 batch-21
Running loss of epoch-51 batch-21 = 5.019525997340679e-05

Training epoch-51 batch-22
Running loss of epoch-51 batch-22 = 1.8598511815071106e-06

Training epoch-51 batch-23
Running loss of epoch-51 batch-23 = 6.204936653375626e-07

Training epoch-51 batch-24
Running loss of epoch-51 batch-24 = 7.685739547014236e-07

Training epoch-51 batch-25
Running loss of epoch-51 batch-25 = 6.623216904699802e-06

Training epoch-51 batch-26
Running loss of epoch-51 batch-26 = 1.4097895473241806e-06

Training epoch-51 batch-27
Running loss of epoch-51 batch-27 = 0.0001318927388638258

Training epoch-51 batch-28
Running loss of epoch-51 batch-28 = 2.8205104172229767e-06

Training epoch-51 batch-29
Running loss of epoch-51 batch-29 = 3.895023837685585e-06

Training epoch-51 batch-30
Running loss of epoch-51 batch-30 = 1.731689553707838e-05

Training epoch-51 batch-31
Running loss of epoch-51 batch-31 = 3.639375790953636e-06

Training epoch-51 batch-32
Running loss of epoch-51 batch-32 = 1.623295247554779e-06

Training epoch-51 batch-33
Running loss of epoch-51 batch-33 = 2.6678084395825863e-05

Training epoch-51 batch-34
Running loss of epoch-51 batch-34 = 5.443464033305645e-06

Training epoch-51 batch-35
Running loss of epoch-51 batch-35 = 2.6432331651449203e-05

Training epoch-51 batch-36
Running loss of epoch-51 batch-36 = 1.1194497346878052e-06

Training epoch-51 batch-37
Running loss of epoch-51 batch-37 = 1.2319069355726242e-06

Training epoch-51 batch-38
Running loss of epoch-51 batch-38 = 2.509518526494503e-05

Training epoch-51 batch-39
Running loss of epoch-51 batch-39 = 1.3492535799741745e-06

Training epoch-51 batch-40
Running loss of epoch-51 batch-40 = 1.9203871488571167e-06

Training epoch-51 batch-41
Running loss of epoch-51 batch-41 = 6.40299404039979e-05

Training epoch-51 batch-42
Running loss of epoch-51 batch-42 = 5.806796252727509e-07

Training epoch-51 batch-43
Running loss of epoch-51 batch-43 = 3.2316893339157104e-06

Training epoch-51 batch-44
Running loss of epoch-51 batch-44 = 8.718227036297321e-06

Training epoch-51 batch-45
Running loss of epoch-51 batch-45 = 1.582317054271698e-06

Training epoch-51 batch-46
Running loss of epoch-51 batch-46 = 2.453802153468132e-06

Training epoch-51 batch-47
Running loss of epoch-51 batch-47 = 1.8477439880371094e-06

Training epoch-51 batch-48
Running loss of epoch-51 batch-48 = 4.690256901085377e-06

Training epoch-51 batch-49
Running loss of epoch-51 batch-49 = 4.289904609322548e-06

Training epoch-51 batch-50
Running loss of epoch-51 batch-50 = 4.492001608014107e-06

Training epoch-51 batch-51
Running loss of epoch-51 batch-51 = 2.6116613298654556e-06

Training epoch-51 batch-52
Running loss of epoch-51 batch-52 = 2.3758038878440857e-06

Training epoch-51 batch-53
Running loss of epoch-51 batch-53 = 6.016693077981472e-06

Training epoch-51 batch-54
Running loss of epoch-51 batch-54 = 1.6662059351801872e-05

Training epoch-51 batch-55
Running loss of epoch-51 batch-55 = 7.93742947280407e-06

Training epoch-51 batch-56
Running loss of epoch-51 batch-56 = 7.430440746247768e-06

Training epoch-51 batch-57
Running loss of epoch-51 batch-57 = 9.595765732228756e-06

Training epoch-51 batch-58
Running loss of epoch-51 batch-58 = 6.34149182587862e-06

Training epoch-51 batch-59
Running loss of epoch-51 batch-59 = 5.555339157581329e-07

Training epoch-51 batch-60
Running loss of epoch-51 batch-60 = 1.507345587015152e-05

Training epoch-51 batch-61
Running loss of epoch-51 batch-61 = 9.017530828714371e-07

Training epoch-51 batch-62
Running loss of epoch-51 batch-62 = 2.7383212000131607e-06

Training epoch-51 batch-63
Running loss of epoch-51 batch-63 = 2.309447154402733e-06

Training epoch-51 batch-64
Running loss of epoch-51 batch-64 = 3.2670795917510986e-06

Training epoch-51 batch-65
Running loss of epoch-51 batch-65 = 6.909715011715889e-06

Training epoch-51 batch-66
Running loss of epoch-51 batch-66 = 1.56032619997859e-05

Training epoch-51 batch-67
Running loss of epoch-51 batch-67 = 1.0075629688799381e-05

Training epoch-51 batch-68
Running loss of epoch-51 batch-68 = 8.062925189733505e-07

Training epoch-51 batch-69
Running loss of epoch-51 batch-69 = 7.255701348185539e-06

Training epoch-51 batch-70
Running loss of epoch-51 batch-70 = 1.2752716429531574e-05

Training epoch-51 batch-71
Running loss of epoch-51 batch-71 = 4.991423338651657e-06

Training epoch-51 batch-72
Running loss of epoch-51 batch-72 = 2.3963977582752705e-05

Training epoch-51 batch-73
Running loss of epoch-51 batch-73 = 1.567881554365158e-06

Training epoch-51 batch-74
Running loss of epoch-51 batch-74 = 3.076856955885887e-06

Training epoch-51 batch-75
Running loss of epoch-51 batch-75 = 1.6617123037576675e-06

Training epoch-51 batch-76
Running loss of epoch-51 batch-76 = 7.086433470249176e-05

Training epoch-51 batch-77
Running loss of epoch-51 batch-77 = 6.582704372704029e-06

Training epoch-51 batch-78
Running loss of epoch-51 batch-78 = 3.257300704717636e-07

Training epoch-51 batch-79
Running loss of epoch-51 batch-79 = 5.98422484472394e-05

Training epoch-51 batch-80
Running loss of epoch-51 batch-80 = 1.685088500380516e-05

Training epoch-51 batch-81
Running loss of epoch-51 batch-81 = 5.563604645431042e-06

Training epoch-51 batch-82
Running loss of epoch-51 batch-82 = 2.289656549692154e-06

Training epoch-51 batch-83
Running loss of epoch-51 batch-83 = 5.803536623716354e-06

Training epoch-51 batch-84
Running loss of epoch-51 batch-84 = 1.4512334018945694e-06

Training epoch-51 batch-85
Running loss of epoch-51 batch-85 = 1.1599739082157612e-05

Training epoch-51 batch-86
Running loss of epoch-51 batch-86 = 9.816139936447144e-07

Training epoch-51 batch-87
Running loss of epoch-51 batch-87 = 6.202026270329952e-06

Training epoch-51 batch-88
Running loss of epoch-51 batch-88 = 4.156376235187054e-06

Training epoch-51 batch-89
Running loss of epoch-51 batch-89 = 1.9116210751235485e-05

Training epoch-51 batch-90
Running loss of epoch-51 batch-90 = 1.3705575838685036e-05

Training epoch-51 batch-91
Running loss of epoch-51 batch-91 = 3.4512486308813095e-06

Training epoch-51 batch-92
Running loss of epoch-51 batch-92 = 2.4895183742046356e-05

Training epoch-51 batch-93
Running loss of epoch-51 batch-93 = 1.3920944184064865e-06

Training epoch-51 batch-94
Running loss of epoch-51 batch-94 = 8.639413863420486e-06

Training epoch-51 batch-95
Running loss of epoch-51 batch-95 = 3.229128196835518e-06

Training epoch-51 batch-96
Running loss of epoch-51 batch-96 = 1.5760306268930435e-06

Training epoch-51 batch-97
Running loss of epoch-51 batch-97 = 1.1552241630852222e-05

Training epoch-51 batch-98
Running loss of epoch-51 batch-98 = 3.7723220884799957e-06

Training epoch-51 batch-99
Running loss of epoch-51 batch-99 = 1.0017654858529568e-05

Training epoch-51 batch-100
Running loss of epoch-51 batch-100 = 4.586332943290472e-05

Training epoch-51 batch-101
Running loss of epoch-51 batch-101 = 1.2025702744722366e-06

Training epoch-51 batch-102
Running loss of epoch-51 batch-102 = 8.038594387471676e-06

Training epoch-51 batch-103
Running loss of epoch-51 batch-103 = 7.601920515298843e-07

Training epoch-51 batch-104
Running loss of epoch-51 batch-104 = 1.1343508958816528e-06

Training epoch-51 batch-105
Running loss of epoch-51 batch-105 = 1.4244578778743744e-06

Training epoch-51 batch-106
Running loss of epoch-51 batch-106 = 2.8584618121385574e-06

Training epoch-51 batch-107
Running loss of epoch-51 batch-107 = 1.5096738934516907e-06

Training epoch-51 batch-108
Running loss of epoch-51 batch-108 = 1.5366822481155396e-06

Training epoch-51 batch-109
Running loss of epoch-51 batch-109 = 1.4691613614559174e-06

Training epoch-51 batch-110
Running loss of epoch-51 batch-110 = 2.112938091158867e-06

Training epoch-51 batch-111
Running loss of epoch-51 batch-111 = 4.883157089352608e-06

Training epoch-51 batch-112
Running loss of epoch-51 batch-112 = 1.8319115042686462e-06

Training epoch-51 batch-113
Running loss of epoch-51 batch-113 = 3.0348310247063637e-06

Training epoch-51 batch-114
Running loss of epoch-51 batch-114 = 1.3087410479784012e-06

Training epoch-51 batch-115
Running loss of epoch-51 batch-115 = 1.300126314163208e-06

Training epoch-51 batch-116
Running loss of epoch-51 batch-116 = 6.568268872797489e-06

Training epoch-51 batch-117
Running loss of epoch-51 batch-117 = 2.78746010735631e-05

Training epoch-51 batch-118
Running loss of epoch-51 batch-118 = 1.766253262758255e-06

Training epoch-51 batch-119
Running loss of epoch-51 batch-119 = 1.4165416359901428e-06

Training epoch-51 batch-120
Running loss of epoch-51 batch-120 = 2.2917636670172215e-05

Training epoch-51 batch-121
Running loss of epoch-51 batch-121 = 1.7713755369186401e-06

Training epoch-51 batch-122
Running loss of epoch-51 batch-122 = 2.6549678295850754e-06

Training epoch-51 batch-123
Running loss of epoch-51 batch-123 = 6.051035597920418e-06

Training epoch-51 batch-124
Running loss of epoch-51 batch-124 = 2.0668376237154007e-06

Training epoch-51 batch-125
Running loss of epoch-51 batch-125 = 1.27498060464859e-06

Training epoch-51 batch-126
Running loss of epoch-51 batch-126 = 1.1771917343139648e-06

Training epoch-51 batch-127
Running loss of epoch-51 batch-127 = 2.14320607483387e-06

Training epoch-51 batch-128
Running loss of epoch-51 batch-128 = 3.484310582280159e-06

Training epoch-51 batch-129
Running loss of epoch-51 batch-129 = 6.399583071470261e-06

Training epoch-51 batch-130
Running loss of epoch-51 batch-130 = 1.294468529522419e-05

Training epoch-51 batch-131
Running loss of epoch-51 batch-131 = 2.0407605916261673e-06

Training epoch-51 batch-132
Running loss of epoch-51 batch-132 = 1.5835976228117943e-06

Training epoch-51 batch-133
Running loss of epoch-51 batch-133 = 1.5958212316036224e-06

Training epoch-51 batch-134
Running loss of epoch-51 batch-134 = 5.6067947298288345e-06

Training epoch-51 batch-135
Running loss of epoch-51 batch-135 = 1.5457626432180405e-06

Training epoch-51 batch-136
Running loss of epoch-51 batch-136 = 1.3004755601286888e-05

Training epoch-51 batch-137
Running loss of epoch-51 batch-137 = 5.501206032931805e-06

Training epoch-51 batch-138
Running loss of epoch-51 batch-138 = 9.038485586643219e-07

Training epoch-51 batch-139
Running loss of epoch-51 batch-139 = 8.158385753631592e-07

Training epoch-51 batch-140
Running loss of epoch-51 batch-140 = 3.3159740269184113e-06

Training epoch-51 batch-141
Running loss of epoch-51 batch-141 = 5.016569048166275e-06

Training epoch-51 batch-142
Running loss of epoch-51 batch-142 = 4.516914486885071e-06

Training epoch-51 batch-143
Running loss of epoch-51 batch-143 = 3.0314549803733826e-06

Training epoch-51 batch-144
Running loss of epoch-51 batch-144 = 1.1296942830085754e-06

Training epoch-51 batch-145
Running loss of epoch-51 batch-145 = 7.799826562404633e-07

Training epoch-51 batch-146
Running loss of epoch-51 batch-146 = 1.4256220310926437e-06

Training epoch-51 batch-147
Running loss of epoch-51 batch-147 = 2.7671921998262405e-06

Training epoch-51 batch-148
Running loss of epoch-51 batch-148 = 1.0883202776312828e-05

Training epoch-51 batch-149
Running loss of epoch-51 batch-149 = 3.1390227377414703e-06

Training epoch-51 batch-150
Running loss of epoch-51 batch-150 = 2.7848873287439346e-06

Training epoch-51 batch-151
Running loss of epoch-51 batch-151 = 1.3243523426353931e-05

Training epoch-51 batch-152
Running loss of epoch-51 batch-152 = 2.9550865292549133e-06

Training epoch-51 batch-153
Running loss of epoch-51 batch-153 = 1.6306759789586067e-05

Training epoch-51 batch-154
Running loss of epoch-51 batch-154 = 6.2087783589959145e-06

Training epoch-51 batch-155
Running loss of epoch-51 batch-155 = 3.1287781894207e-06

Training epoch-51 batch-156
Running loss of epoch-51 batch-156 = 1.0860851034522057e-05

Training epoch-51 batch-157
Running loss of epoch-51 batch-157 = 5.617737770080566e-06

Finished training epoch-51.



Average train loss at epoch-51 = 8.420432358980179e-06

Started Evaluation

Average val loss at epoch-51 = 5.287834766664003

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 47.38 %
Accuracy for class setUp is: 53.77 %
Accuracy for class onCreate is: 42.86 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 34.93 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.52 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 28.72 %

Overall Accuracy = 45.10 %

Finished Evaluation



Started training epoch-52


Training epoch-52 batch-1
Running loss of epoch-52 batch-1 = 1.3608718290925026e-05

Training epoch-52 batch-2
Running loss of epoch-52 batch-2 = 2.8620706871151924e-06

Training epoch-52 batch-3
Running loss of epoch-52 batch-3 = 1.725275069475174e-07

Training epoch-52 batch-4
Running loss of epoch-52 batch-4 = 8.311239071190357e-06

Training epoch-52 batch-5
Running loss of epoch-52 batch-5 = 3.5385601222515106e-06

Training epoch-52 batch-6
Running loss of epoch-52 batch-6 = 4.55568078905344e-06

Training epoch-52 batch-7
Running loss of epoch-52 batch-7 = 1.507345587015152e-06

Training epoch-52 batch-8
Running loss of epoch-52 batch-8 = 1.8365681171417236e-06

Training epoch-52 batch-9
Running loss of epoch-52 batch-9 = 2.2710300981998444e-06

Training epoch-52 batch-10
Running loss of epoch-52 batch-10 = 1.301756128668785e-06

Training epoch-52 batch-11
Running loss of epoch-52 batch-11 = 1.4326069504022598e-06

Training epoch-52 batch-12
Running loss of epoch-52 batch-12 = 6.100977770984173e-06

Training epoch-52 batch-13
Running loss of epoch-52 batch-13 = 3.243330866098404e-06

Training epoch-52 batch-14
Running loss of epoch-52 batch-14 = 2.1739397197961807e-06

Training epoch-52 batch-15
Running loss of epoch-52 batch-15 = 5.722744390368462e-06

Training epoch-52 batch-16
Running loss of epoch-52 batch-16 = 1.1041294783353806e-05

Training epoch-52 batch-17
Running loss of epoch-52 batch-17 = 1.0110670700669289e-05

Training epoch-52 batch-18
Running loss of epoch-52 batch-18 = 1.503853127360344e-06

Training epoch-52 batch-19
Running loss of epoch-52 batch-19 = 1.246575266122818e-06

Training epoch-52 batch-20
Running loss of epoch-52 batch-20 = 1.1131633073091507e-06

Training epoch-52 batch-21
Running loss of epoch-52 batch-21 = 6.221234798431396e-07

Training epoch-52 batch-22
Running loss of epoch-52 batch-22 = 3.1043309718370438e-06

Training epoch-52 batch-23
Running loss of epoch-52 batch-23 = 3.2747630029916763e-06

Training epoch-52 batch-24
Running loss of epoch-52 batch-24 = 1.8256250768899918e-06

Training epoch-52 batch-25
Running loss of epoch-52 batch-25 = 2.994830720126629e-05

Training epoch-52 batch-26
Running loss of epoch-52 batch-26 = 1.4652032405138016e-06

Training epoch-52 batch-27
Running loss of epoch-52 batch-27 = 1.3012904673814774e-06

Training epoch-52 batch-28
Running loss of epoch-52 batch-28 = 3.559165634214878e-06

Training epoch-52 batch-29
Running loss of epoch-52 batch-29 = 8.954666554927826e-07

Training epoch-52 batch-30
Running loss of epoch-52 batch-30 = 5.100853741168976e-06

Training epoch-52 batch-31
Running loss of epoch-52 batch-31 = 9.137904271483421e-06

Training epoch-52 batch-32
Running loss of epoch-52 batch-32 = 2.0512379705905914e-06

Training epoch-52 batch-33
Running loss of epoch-52 batch-33 = 9.60659235715866e-07

Training epoch-52 batch-34
Running loss of epoch-52 batch-34 = 1.2092525139451027e-05

Training epoch-52 batch-35
Running loss of epoch-52 batch-35 = 2.21616355702281e-05

Training epoch-52 batch-36
Running loss of epoch-52 batch-36 = 6.062909960746765e-07

Training epoch-52 batch-37
Running loss of epoch-52 batch-37 = 2.1150335669517517e-06

Training epoch-52 batch-38
Running loss of epoch-52 batch-38 = 3.0319206416606903e-05

Training epoch-52 batch-39
Running loss of epoch-52 batch-39 = 9.834766387939453e-06

Training epoch-52 batch-40
Running loss of epoch-52 batch-40 = 1.78581103682518e-06

Training epoch-52 batch-41
Running loss of epoch-52 batch-41 = 2.739252522587776e-06

Training epoch-52 batch-42
Running loss of epoch-52 batch-42 = 1.196516677737236e-06

Training epoch-52 batch-43
Running loss of epoch-52 batch-43 = 1.4643068425357342e-05

Training epoch-52 batch-44
Running loss of epoch-52 batch-44 = 5.415524356067181e-06

Training epoch-52 batch-45
Running loss of epoch-52 batch-45 = 2.433860208839178e-05

Training epoch-52 batch-46
Running loss of epoch-52 batch-46 = 4.1816383600234985e-06

Training epoch-52 batch-47
Running loss of epoch-52 batch-47 = 3.635883331298828e-06

Training epoch-52 batch-48
Running loss of epoch-52 batch-48 = 1.893145963549614e-06

Training epoch-52 batch-49
Running loss of epoch-52 batch-49 = 2.055894583463669e-06

Training epoch-52 batch-50
Running loss of epoch-52 batch-50 = 5.823094397783279e-07

Training epoch-52 batch-51
Running loss of epoch-52 batch-51 = 7.856753654778004e-06

Training epoch-52 batch-52
Running loss of epoch-52 batch-52 = 9.85199585556984e-06

Training epoch-52 batch-53
Running loss of epoch-52 batch-53 = 5.329377017915249e-06

Training epoch-52 batch-54
Running loss of epoch-52 batch-54 = 1.4400575309991837e-06

Training epoch-52 batch-55
Running loss of epoch-52 batch-55 = 2.2167805582284927e-06

Training epoch-52 batch-56
Running loss of epoch-52 batch-56 = 4.350394010543823e-05

Training epoch-52 batch-57
Running loss of epoch-52 batch-57 = 2.1837186068296432e-06

Training epoch-52 batch-58
Running loss of epoch-52 batch-58 = 2.942979335784912e-06

Training epoch-52 batch-59
Running loss of epoch-52 batch-59 = 2.8215115889906883e-05

Training epoch-52 batch-60
Running loss of epoch-52 batch-60 = 1.2879143469035625e-05

Training epoch-52 batch-61
Running loss of epoch-52 batch-61 = 9.519048035144806e-06

Training epoch-52 batch-62
Running loss of epoch-52 batch-62 = 1.135747879743576e-06

Training epoch-52 batch-63
Running loss of epoch-52 batch-63 = 1.0656309314072132e-05

Training epoch-52 batch-64
Running loss of epoch-52 batch-64 = 2.0083971321582794e-06

Training epoch-52 batch-65
Running loss of epoch-52 batch-65 = 1.2647360563278198e-06

Training epoch-52 batch-66
Running loss of epoch-52 batch-66 = 1.498265191912651e-06

Training epoch-52 batch-67
Running loss of epoch-52 batch-67 = 2.3709377273917198e-05

Training epoch-52 batch-68
Running loss of epoch-52 batch-68 = 1.1743977665901184e-06

Training epoch-52 batch-69
Running loss of epoch-52 batch-69 = 1.631083432585001e-05

Training epoch-52 batch-70
Running loss of epoch-52 batch-70 = 2.8135254979133606e-06

Training epoch-52 batch-71
Running loss of epoch-52 batch-71 = 2.2635795176029205e-06

Training epoch-52 batch-72
Running loss of epoch-52 batch-72 = 6.304704584181309e-06

Training epoch-52 batch-73
Running loss of epoch-52 batch-73 = 4.9782684072852135e-06

Training epoch-52 batch-74
Running loss of epoch-52 batch-74 = 3.2664043828845024e-05

Training epoch-52 batch-75
Running loss of epoch-52 batch-75 = 3.392924554646015e-06

Training epoch-52 batch-76
Running loss of epoch-52 batch-76 = 2.9655639082193375e-06

Training epoch-52 batch-77
Running loss of epoch-52 batch-77 = 2.5301706045866013e-06

Training epoch-52 batch-78
Running loss of epoch-52 batch-78 = 6.673275493085384e-06

Training epoch-52 batch-79
Running loss of epoch-52 batch-79 = 1.2028031051158905e-06

Training epoch-52 batch-80
Running loss of epoch-52 batch-80 = 2.841698005795479e-06

Training epoch-52 batch-81
Running loss of epoch-52 batch-81 = 2.2278400138020515e-05

Training epoch-52 batch-82
Running loss of epoch-52 batch-82 = 6.5583735704422e-06

Training epoch-52 batch-83
Running loss of epoch-52 batch-83 = 2.3439060896635056e-06

Training epoch-52 batch-84
Running loss of epoch-52 batch-84 = 1.8777791410684586e-06

Training epoch-52 batch-85
Running loss of epoch-52 batch-85 = 1.94343738257885e-06

Training epoch-52 batch-86
Running loss of epoch-52 batch-86 = 1.9706785678863525e-06

Training epoch-52 batch-87
Running loss of epoch-52 batch-87 = 6.595626473426819e-06

Training epoch-52 batch-88
Running loss of epoch-52 batch-88 = 4.183733835816383e-06

Training epoch-52 batch-89
Running loss of epoch-52 batch-89 = 8.270144462585449e-07

Training epoch-52 batch-90
Running loss of epoch-52 batch-90 = 1.0665971785783768e-06

Training epoch-52 batch-91
Running loss of epoch-52 batch-91 = 1.8959399312734604e-06

Training epoch-52 batch-92
Running loss of epoch-52 batch-92 = 1.1471332982182503e-05

Training epoch-52 batch-93
Running loss of epoch-52 batch-93 = 4.162546247243881e-06

Training epoch-52 batch-94
Running loss of epoch-52 batch-94 = 7.476308383047581e-06

Training epoch-52 batch-95
Running loss of epoch-52 batch-95 = 1.6640406101942062e-06

Training epoch-52 batch-96
Running loss of epoch-52 batch-96 = 1.6996636986732483e-06

Training epoch-52 batch-97
Running loss of epoch-52 batch-97 = 1.2028845958411694e-05

Training epoch-52 batch-98
Running loss of epoch-52 batch-98 = 1.7564627341926098e-05

Training epoch-52 batch-99
Running loss of epoch-52 batch-99 = 3.4641707316040993e-06

Training epoch-52 batch-100
Running loss of epoch-52 batch-100 = 6.605754606425762e-06

Training epoch-52 batch-101
Running loss of epoch-52 batch-101 = 2.7024652808904648e-06

Training epoch-52 batch-102
Running loss of epoch-52 batch-102 = 4.2836181819438934e-06

Training epoch-52 batch-103
Running loss of epoch-52 batch-103 = 3.1739473342895508e-06

Training epoch-52 batch-104
Running loss of epoch-52 batch-104 = 1.344829797744751e-06

Training epoch-52 batch-105
Running loss of epoch-52 batch-105 = 2.6810448616743088e-06

Training epoch-52 batch-106
Running loss of epoch-52 batch-106 = 2.3336615413427353e-06

Training epoch-52 batch-107
Running loss of epoch-52 batch-107 = 7.439905311912298e-05

Training epoch-52 batch-108
Running loss of epoch-52 batch-108 = 1.8260907381772995e-06

Training epoch-52 batch-109
Running loss of epoch-52 batch-109 = 1.4367979019880295e-06

Training epoch-52 batch-110
Running loss of epoch-52 batch-110 = 1.4833640307188034e-06

Training epoch-52 batch-111
Running loss of epoch-52 batch-111 = 6.467453204095364e-06

Training epoch-52 batch-112
Running loss of epoch-52 batch-112 = 3.947713412344456e-05

Training epoch-52 batch-113
Running loss of epoch-52 batch-113 = 2.339249476790428e-06

Training epoch-52 batch-114
Running loss of epoch-52 batch-114 = 3.0114315450191498e-06

Training epoch-52 batch-115
Running loss of epoch-52 batch-115 = 3.5108532756567e-06

Training epoch-52 batch-116
Running loss of epoch-52 batch-116 = 0.0001166642177850008

Training epoch-52 batch-117
Running loss of epoch-52 batch-117 = 1.796754077076912e-06

Training epoch-52 batch-118
Running loss of epoch-52 batch-118 = 1.1506490409374237e-06

Training epoch-52 batch-119
Running loss of epoch-52 batch-119 = 5.103764124214649e-06

Training epoch-52 batch-120
Running loss of epoch-52 batch-120 = 1.12154521048069e-06

Training epoch-52 batch-121
Running loss of epoch-52 batch-121 = 7.550697773694992e-07

Training epoch-52 batch-122
Running loss of epoch-52 batch-122 = 7.270602509379387e-06

Training epoch-52 batch-123
Running loss of epoch-52 batch-123 = 1.2237578630447388e-06

Training epoch-52 batch-124
Running loss of epoch-52 batch-124 = 1.2993579730391502e-05

Training epoch-52 batch-125
Running loss of epoch-52 batch-125 = 8.314964361488819e-06

Training epoch-52 batch-126
Running loss of epoch-52 batch-126 = 3.866618499159813e-06

Training epoch-52 batch-127
Running loss of epoch-52 batch-127 = 1.655425876379013e-06

Training epoch-52 batch-128
Running loss of epoch-52 batch-128 = 2.5012996047735214e-06

Training epoch-52 batch-129
Running loss of epoch-52 batch-129 = 1.7385464161634445e-06

Training epoch-52 batch-130
Running loss of epoch-52 batch-130 = 4.319706931710243e-06

Training epoch-52 batch-131
Running loss of epoch-52 batch-131 = 6.108195520937443e-06

Training epoch-52 batch-132
Running loss of epoch-52 batch-132 = 4.478381015360355e-06

Training epoch-52 batch-133
Running loss of epoch-52 batch-133 = 3.896420821547508e-06

Training epoch-52 batch-134
Running loss of epoch-52 batch-134 = 5.310983397066593e-06

Training epoch-52 batch-135
Running loss of epoch-52 batch-135 = 4.773028194904327e-07

Training epoch-52 batch-136
Running loss of epoch-52 batch-136 = 1.7550773918628693e-06

Training epoch-52 batch-137
Running loss of epoch-52 batch-137 = 4.234490916132927e-06

Training epoch-52 batch-138
Running loss of epoch-52 batch-138 = 5.658518057316542e-05

Training epoch-52 batch-139
Running loss of epoch-52 batch-139 = 5.152542144060135e-07

Training epoch-52 batch-140
Running loss of epoch-52 batch-140 = 5.1477691158652306e-06

Training epoch-52 batch-141
Running loss of epoch-52 batch-141 = 8.780043572187424e-07

Training epoch-52 batch-142
Running loss of epoch-52 batch-142 = 6.275717169046402e-06

Training epoch-52 batch-143
Running loss of epoch-52 batch-143 = 1.2025004252791405e-05

Training epoch-52 batch-144
Running loss of epoch-52 batch-144 = 3.893859684467316e-06

Training epoch-52 batch-145
Running loss of epoch-52 batch-145 = 4.084780812263489e-06

Training epoch-52 batch-146
Running loss of epoch-52 batch-146 = 1.8023420125246048e-06

Training epoch-52 batch-147
Running loss of epoch-52 batch-147 = 3.3132964745163918e-06

Training epoch-52 batch-148
Running loss of epoch-52 batch-148 = 7.622758857905865e-06

Training epoch-52 batch-149
Running loss of epoch-52 batch-149 = 8.500530384480953e-06

Training epoch-52 batch-150
Running loss of epoch-52 batch-150 = 3.432540688663721e-05

Training epoch-52 batch-151
Running loss of epoch-52 batch-151 = 9.255134500563145e-06

Training epoch-52 batch-152
Running loss of epoch-52 batch-152 = 3.898981958627701e-06

Training epoch-52 batch-153
Running loss of epoch-52 batch-153 = 1.2002186849713326e-05

Training epoch-52 batch-154
Running loss of epoch-52 batch-154 = 8.276896551251411e-06

Training epoch-52 batch-155
Running loss of epoch-52 batch-155 = 6.012921221554279e-05

Training epoch-52 batch-156
Running loss of epoch-52 batch-156 = 7.568858563899994e-06

Training epoch-52 batch-157
Running loss of epoch-52 batch-157 = 3.155320882797241e-06

Finished training epoch-52.



Average train loss at epoch-52 = 8.003861457109451e-06

Started Evaluation

Average val loss at epoch-52 = 5.294991935554304

Accuracy for classes:
Accuracy for class equals is: 68.48 %
Accuracy for class main is: 47.87 %
Accuracy for class setUp is: 53.11 %
Accuracy for class onCreate is: 42.86 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 34.70 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.85 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 28.97 %

Overall Accuracy = 44.96 %

Finished Evaluation



Started training epoch-53


Training epoch-53 batch-1
Running loss of epoch-53 batch-1 = 2.7921167202293873e-05

Training epoch-53 batch-2
Running loss of epoch-53 batch-2 = 7.057096809148788e-07

Training epoch-53 batch-3
Running loss of epoch-53 batch-3 = 2.173474058508873e-06

Training epoch-53 batch-4
Running loss of epoch-53 batch-4 = 2.798624336719513e-07

Training epoch-53 batch-5
Running loss of epoch-53 batch-5 = 2.7641537599265575e-05

Training epoch-53 batch-6
Running loss of epoch-53 batch-6 = 1.7825514078140259e-06

Training epoch-53 batch-7
Running loss of epoch-53 batch-7 = 1.6401754692196846e-06

Training epoch-53 batch-8
Running loss of epoch-53 batch-8 = 1.428648829460144e-06

Training epoch-53 batch-9
Running loss of epoch-53 batch-9 = 2.5276094675064087e-06

Training epoch-53 batch-10
Running loss of epoch-53 batch-10 = 1.6347737982869148e-05

Training epoch-53 batch-11
Running loss of epoch-53 batch-11 = 2.241460606455803e-06

Training epoch-53 batch-12
Running loss of epoch-53 batch-12 = 7.668044418096542e-06

Training epoch-53 batch-13
Running loss of epoch-53 batch-13 = 1.3830140233039856e-07

Training epoch-53 batch-14
Running loss of epoch-53 batch-14 = 5.809590220451355e-06

Training epoch-53 batch-15
Running loss of epoch-53 batch-15 = 9.146751835942268e-06

Training epoch-53 batch-16
Running loss of epoch-53 batch-16 = 3.2561365514993668e-06

Training epoch-53 batch-17
Running loss of epoch-53 batch-17 = 1.7809215933084488e-06

Training epoch-53 batch-18
Running loss of epoch-53 batch-18 = 4.514469765126705e-06

Training epoch-53 batch-19
Running loss of epoch-53 batch-19 = 1.7238780856132507e-06

Training epoch-53 batch-20
Running loss of epoch-53 batch-20 = 1.898501068353653e-06

Training epoch-53 batch-21
Running loss of epoch-53 batch-21 = 4.313653334975243e-06

Training epoch-53 batch-22
Running loss of epoch-53 batch-22 = 7.529743015766144e-07

Training epoch-53 batch-23
Running loss of epoch-53 batch-23 = 2.3543834686279297e-06

Training epoch-53 batch-24
Running loss of epoch-53 batch-24 = 7.706694304943085e-07

Training epoch-53 batch-25
Running loss of epoch-53 batch-25 = 1.3925251550972462e-05

Training epoch-53 batch-26
Running loss of epoch-53 batch-26 = 5.336361937224865e-06

Training epoch-53 batch-27
Running loss of epoch-53 batch-27 = 1.5720492228865623e-05

Training epoch-53 batch-28
Running loss of epoch-53 batch-28 = 1.6404082998633385e-05

Training epoch-53 batch-29
Running loss of epoch-53 batch-29 = 1.0137446224689484e-06

Training epoch-53 batch-30
Running loss of epoch-53 batch-30 = 2.87545844912529e-06

Training epoch-53 batch-31
Running loss of epoch-53 batch-31 = 4.774867556989193e-05

Training epoch-53 batch-32
Running loss of epoch-53 batch-32 = 0.00012084253830835223

Training epoch-53 batch-33
Running loss of epoch-53 batch-33 = 2.2294814698398113e-05

Training epoch-53 batch-34
Running loss of epoch-53 batch-34 = 1.4721881598234177e-06

Training epoch-53 batch-35
Running loss of epoch-53 batch-35 = 3.6266865208745003e-06

Training epoch-53 batch-36
Running loss of epoch-53 batch-36 = 2.237886656075716e-05

Training epoch-53 batch-37
Running loss of epoch-53 batch-37 = 1.2046657502651215e-06

Training epoch-53 batch-38
Running loss of epoch-53 batch-38 = 2.452172338962555e-06

Training epoch-53 batch-39
Running loss of epoch-53 batch-39 = 2.158805727958679e-06

Training epoch-53 batch-40
Running loss of epoch-53 batch-40 = 1.0319403372704983e-05

Training epoch-53 batch-41
Running loss of epoch-53 batch-41 = 1.0494492016732693e-05

Training epoch-53 batch-42
Running loss of epoch-53 batch-42 = 3.192014992237091e-05

Training epoch-53 batch-43
Running loss of epoch-53 batch-43 = 8.742907084524632e-06

Training epoch-53 batch-44
Running loss of epoch-53 batch-44 = 2.6919879019260406e-06

Training epoch-53 batch-45
Running loss of epoch-53 batch-45 = 3.9655715227127075e-06

Training epoch-53 batch-46
Running loss of epoch-53 batch-46 = 4.136585630476475e-06

Training epoch-53 batch-47
Running loss of epoch-53 batch-47 = 1.5050172805786133e-06

Training epoch-53 batch-48
Running loss of epoch-53 batch-48 = 2.1813903003931046e-06

Training epoch-53 batch-49
Running loss of epoch-53 batch-49 = 4.758941940963268e-06

Training epoch-53 batch-50
Running loss of epoch-53 batch-50 = 9.757932275533676e-07

Training epoch-53 batch-51
Running loss of epoch-53 batch-51 = 1.6169622540473938e-05

Training epoch-53 batch-52
Running loss of epoch-53 batch-52 = 2.7843867428600788e-05

Training epoch-53 batch-53
Running loss of epoch-53 batch-53 = 1.5366822481155396e-06

Training epoch-53 batch-54
Running loss of epoch-53 batch-54 = 1.0263058356940746e-05

Training epoch-53 batch-55
Running loss of epoch-53 batch-55 = 5.674082785844803e-07

Training epoch-53 batch-56
Running loss of epoch-53 batch-56 = 1.9532162696123123e-06

Training epoch-53 batch-57
Running loss of epoch-53 batch-57 = 1.2922799214720726e-05

Training epoch-53 batch-58
Running loss of epoch-53 batch-58 = 1.0768184438347816e-05

Training epoch-53 batch-59
Running loss of epoch-53 batch-59 = 1.1308584362268448e-06

Training epoch-53 batch-60
Running loss of epoch-53 batch-60 = 1.8379651010036469e-06

Training epoch-53 batch-61
Running loss of epoch-53 batch-61 = 7.73381907492876e-06

Training epoch-53 batch-62
Running loss of epoch-53 batch-62 = 1.1586002074182034e-05

Training epoch-53 batch-63
Running loss of epoch-53 batch-63 = 4.815403372049332e-06

Training epoch-53 batch-64
Running loss of epoch-53 batch-64 = 6.595754530280828e-05

Training epoch-53 batch-65
Running loss of epoch-53 batch-65 = 7.932540029287338e-07

Training epoch-53 batch-66
Running loss of epoch-53 batch-66 = 7.089343853294849e-06

Training epoch-53 batch-67
Running loss of epoch-53 batch-67 = 1.166015863418579e-06

Training epoch-53 batch-68
Running loss of epoch-53 batch-68 = 4.608882591128349e-06

Training epoch-53 batch-69
Running loss of epoch-53 batch-69 = 2.090493217110634e-05

Training epoch-53 batch-70
Running loss of epoch-53 batch-70 = 1.2159696780145168e-05

Training epoch-53 batch-71
Running loss of epoch-53 batch-71 = 6.998889148235321e-07

Training epoch-53 batch-72
Running loss of epoch-53 batch-72 = 8.437782526016235e-07

Training epoch-53 batch-73
Running loss of epoch-53 batch-73 = 1.7851125448942184e-06

Training epoch-53 batch-74
Running loss of epoch-53 batch-74 = 4.866393283009529e-06

Training epoch-53 batch-75
Running loss of epoch-53 batch-75 = 1.200009137392044e-06

Training epoch-53 batch-76
Running loss of epoch-53 batch-76 = 1.107680145651102e-05

Training epoch-53 batch-77
Running loss of epoch-53 batch-77 = 9.091338142752647e-06

Training epoch-53 batch-78
Running loss of epoch-53 batch-78 = 5.927868187427521e-06

Training epoch-53 batch-79
Running loss of epoch-53 batch-79 = 1.8612481653690338e-06

Training epoch-53 batch-80
Running loss of epoch-53 batch-80 = 1.3532117009162903e-06

Training epoch-53 batch-81
Running loss of epoch-53 batch-81 = 8.301925845444202e-06

Training epoch-53 batch-82
Running loss of epoch-53 batch-82 = 3.0765077099204063e-06

Training epoch-53 batch-83
Running loss of epoch-53 batch-83 = 5.783163942396641e-06

Training epoch-53 batch-84
Running loss of epoch-53 batch-84 = 9.194831363856792e-06

Training epoch-53 batch-85
Running loss of epoch-53 batch-85 = 4.790839739143848e-06

Training epoch-53 batch-86
Running loss of epoch-53 batch-86 = 1.0863877832889557e-06

Training epoch-53 batch-87
Running loss of epoch-53 batch-87 = 1.9674189388751984e-06

Training epoch-53 batch-88
Running loss of epoch-53 batch-88 = 3.3562537282705307e-06

Training epoch-53 batch-89
Running loss of epoch-53 batch-89 = 2.9318034648895264e-06

Training epoch-53 batch-90
Running loss of epoch-53 batch-90 = 1.2379605323076248e-06

Training epoch-53 batch-91
Running loss of epoch-53 batch-91 = 3.8133002817630768e-06

Training epoch-53 batch-92
Running loss of epoch-53 batch-92 = 5.923211574554443e-07

Training epoch-53 batch-93
Running loss of epoch-53 batch-93 = 1.9506551325321198e-06

Training epoch-53 batch-94
Running loss of epoch-53 batch-94 = 5.424022674560547e-06

Training epoch-53 batch-95
Running loss of epoch-53 batch-95 = 1.6873236745595932e-06

Training epoch-53 batch-96
Running loss of epoch-53 batch-96 = 6.659072823822498e-06

Training epoch-53 batch-97
Running loss of epoch-53 batch-97 = 4.648580215871334e-06

Training epoch-53 batch-98
Running loss of epoch-53 batch-98 = 8.211354725062847e-06

Training epoch-53 batch-99
Running loss of epoch-53 batch-99 = 1.1702068150043488e-06

Training epoch-53 batch-100
Running loss of epoch-53 batch-100 = 4.090834408998489e-06

Training epoch-53 batch-101
Running loss of epoch-53 batch-101 = 7.03614205121994e-06

Training epoch-53 batch-102
Running loss of epoch-53 batch-102 = 3.273598849773407e-06

Training epoch-53 batch-103
Running loss of epoch-53 batch-103 = 1.7173588275909424e-06

Training epoch-53 batch-104
Running loss of epoch-53 batch-104 = 3.0868686735630035e-06

Training epoch-53 batch-105
Running loss of epoch-53 batch-105 = 1.2568198144435883e-06

Training epoch-53 batch-106
Running loss of epoch-53 batch-106 = 1.5303376130759716e-05

Training epoch-53 batch-107
Running loss of epoch-53 batch-107 = 3.2263342291116714e-06

Training epoch-53 batch-108
Running loss of epoch-53 batch-108 = 9.72929410636425e-06

Training epoch-53 batch-109
Running loss of epoch-53 batch-109 = 5.10981772094965e-06

Training epoch-53 batch-110
Running loss of epoch-53 batch-110 = 1.8854625523090363e-06

Training epoch-53 batch-111
Running loss of epoch-53 batch-111 = 3.89037886634469e-05

Training epoch-53 batch-112
Running loss of epoch-53 batch-112 = 1.1993222869932652e-05

Training epoch-53 batch-113
Running loss of epoch-53 batch-113 = 3.6761630326509476e-06

Training epoch-53 batch-114
Running loss of epoch-53 batch-114 = 1.2164702638983727e-05

Training epoch-53 batch-115
Running loss of epoch-53 batch-115 = 1.4747492969036102e-06

Training epoch-53 batch-116
Running loss of epoch-53 batch-116 = 1.8479768186807632e-06

Training epoch-53 batch-117
Running loss of epoch-53 batch-117 = 1.667998731136322e-06

Training epoch-53 batch-118
Running loss of epoch-53 batch-118 = 4.1085295379161835e-06

Training epoch-53 batch-119
Running loss of epoch-53 batch-119 = 2.468470484018326e-06

Training epoch-53 batch-120
Running loss of epoch-53 batch-120 = 3.818422555923462e-07

Training epoch-53 batch-121
Running loss of epoch-53 batch-121 = 8.365721441805363e-06

Training epoch-53 batch-122
Running loss of epoch-53 batch-122 = 5.329493433237076e-07

Training epoch-53 batch-123
Running loss of epoch-53 batch-123 = 5.6390417739748955e-06

Training epoch-53 batch-124
Running loss of epoch-53 batch-124 = 9.522656910121441e-06

Training epoch-53 batch-125
Running loss of epoch-53 batch-125 = 1.0943273082375526e-05

Training epoch-53 batch-126
Running loss of epoch-53 batch-126 = 7.729977369308472e-07

Training epoch-53 batch-127
Running loss of epoch-53 batch-127 = 2.0242296159267426e-06

Training epoch-53 batch-128
Running loss of epoch-53 batch-128 = 5.865003913640976e-07

Training epoch-53 batch-129
Running loss of epoch-53 batch-129 = 3.3332034945487976e-06

Training epoch-53 batch-130
Running loss of epoch-53 batch-130 = 2.016546204686165e-06

Training epoch-53 batch-131
Running loss of epoch-53 batch-131 = 5.302950739860535e-06

Training epoch-53 batch-132
Running loss of epoch-53 batch-132 = 1.417030580341816e-05

Training epoch-53 batch-133
Running loss of epoch-53 batch-133 = 1.8423888832330704e-06

Training epoch-53 batch-134
Running loss of epoch-53 batch-134 = 1.126900315284729e-06

Training epoch-53 batch-135
Running loss of epoch-53 batch-135 = 4.964298568665981e-06

Training epoch-53 batch-136
Running loss of epoch-53 batch-136 = 8.556526154279709e-07

Training epoch-53 batch-137
Running loss of epoch-53 batch-137 = 1.887104008346796e-05

Training epoch-53 batch-138
Running loss of epoch-53 batch-138 = 1.8456485122442245e-06

Training epoch-53 batch-139
Running loss of epoch-53 batch-139 = 9.092153050005436e-06

Training epoch-53 batch-140
Running loss of epoch-53 batch-140 = 1.4542602002620697e-06

Training epoch-53 batch-141
Running loss of epoch-53 batch-141 = 1.3047829270362854e-06

Training epoch-53 batch-142
Running loss of epoch-53 batch-142 = 1.1730007827281952e-06

Training epoch-53 batch-143
Running loss of epoch-53 batch-143 = 3.439374268054962e-06

Training epoch-53 batch-144
Running loss of epoch-53 batch-144 = 1.9795261323451996e-06

Training epoch-53 batch-145
Running loss of epoch-53 batch-145 = 2.1995510905981064e-06

Training epoch-53 batch-146
Running loss of epoch-53 batch-146 = 4.580826498568058e-06

Training epoch-53 batch-147
Running loss of epoch-53 batch-147 = 8.441973477602005e-06

Training epoch-53 batch-148
Running loss of epoch-53 batch-148 = 2.0551960915327072e-06

Training epoch-53 batch-149
Running loss of epoch-53 batch-149 = 4.22490993514657e-05

Training epoch-53 batch-150
Running loss of epoch-53 batch-150 = 8.970149792730808e-06

Training epoch-53 batch-151
Running loss of epoch-53 batch-151 = 3.136228770017624e-07

Training epoch-53 batch-152
Running loss of epoch-53 batch-152 = 5.471520125865936e-07

Training epoch-53 batch-153
Running loss of epoch-53 batch-153 = 3.4673139452934265e-06

Training epoch-53 batch-154
Running loss of epoch-53 batch-154 = 1.5744008123874664e-06

Training epoch-53 batch-155
Running loss of epoch-53 batch-155 = 1.3154931366443634e-06

Training epoch-53 batch-156
Running loss of epoch-53 batch-156 = 3.648269921541214e-05

Training epoch-53 batch-157
Running loss of epoch-53 batch-157 = 1.282617449760437e-05

Finished training epoch-53.



Average train loss at epoch-53 = 7.598913088440895e-06

Started Evaluation

Average val loss at epoch-53 = 5.3021836657273145

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 48.69 %
Accuracy for class setUp is: 53.28 %
Accuracy for class onCreate is: 43.39 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 35.39 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 20.85 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 29.23 %

Overall Accuracy = 45.27 %

Finished Evaluation



Started training epoch-54


Training epoch-54 batch-1
Running loss of epoch-54 batch-1 = 1.3301614671945572e-06

Training epoch-54 batch-2
Running loss of epoch-54 batch-2 = 8.207280188798904e-07

Training epoch-54 batch-3
Running loss of epoch-54 batch-3 = 1.7265439964830875e-05

Training epoch-54 batch-4
Running loss of epoch-54 batch-4 = 6.991904228925705e-07

Training epoch-54 batch-5
Running loss of epoch-54 batch-5 = 2.1462328732013702e-06

Training epoch-54 batch-6
Running loss of epoch-54 batch-6 = 8.015544153749943e-06

Training epoch-54 batch-7
Running loss of epoch-54 batch-7 = 5.818670615553856e-06

Training epoch-54 batch-8
Running loss of epoch-54 batch-8 = 2.158805727958679e-06

Training epoch-54 batch-9
Running loss of epoch-54 batch-9 = 5.3853727877140045e-06

Training epoch-54 batch-10
Running loss of epoch-54 batch-10 = 3.1758099794387817e-06

Training epoch-54 batch-11
Running loss of epoch-54 batch-11 = 6.007030606269836e-07

Training epoch-54 batch-12
Running loss of epoch-54 batch-12 = 2.727145329117775e-06

Training epoch-54 batch-13
Running loss of epoch-54 batch-13 = 8.256873115897179e-06

Training epoch-54 batch-14
Running loss of epoch-54 batch-14 = 2.7375994250178337e-05

Training epoch-54 batch-15
Running loss of epoch-54 batch-15 = 6.945431232452393e-05

Training epoch-54 batch-16
Running loss of epoch-54 batch-16 = 3.500725142657757e-06

Training epoch-54 batch-17
Running loss of epoch-54 batch-17 = 1.9951839931309223e-05

Training epoch-54 batch-18
Running loss of epoch-54 batch-18 = 8.865492418408394e-06

Training epoch-54 batch-19
Running loss of epoch-54 batch-19 = 1.895311288535595e-05

Training epoch-54 batch-20
Running loss of epoch-54 batch-20 = 6.721820682287216e-07

Training epoch-54 batch-21
Running loss of epoch-54 batch-21 = 1.512700691819191e-06

Training epoch-54 batch-22
Running loss of epoch-54 batch-22 = 5.062553100287914e-06

Training epoch-54 batch-23
Running loss of epoch-54 batch-23 = 8.779228664934635e-06

Training epoch-54 batch-24
Running loss of epoch-54 batch-24 = 3.3350661396980286e-06

Training epoch-54 batch-25
Running loss of epoch-54 batch-25 = 3.383960574865341e-06

Training epoch-54 batch-26
Running loss of epoch-54 batch-26 = 3.5171397030353546e-06

Training epoch-54 batch-27
Running loss of epoch-54 batch-27 = 8.956994861364365e-07

Training epoch-54 batch-28
Running loss of epoch-54 batch-28 = 1.3173557817935944e-06

Training epoch-54 batch-29
Running loss of epoch-54 batch-29 = 4.938221536576748e-06

Training epoch-54 batch-30
Running loss of epoch-54 batch-30 = 1.3811513781547546e-06

Training epoch-54 batch-31
Running loss of epoch-54 batch-31 = 5.230423994362354e-06

Training epoch-54 batch-32
Running loss of epoch-54 batch-32 = 7.815775461494923e-06

Training epoch-54 batch-33
Running loss of epoch-54 batch-33 = 2.7711503207683563e-06

Training epoch-54 batch-34
Running loss of epoch-54 batch-34 = 1.9811559468507767e-06

Training epoch-54 batch-35
Running loss of epoch-54 batch-35 = 2.1706800907850266e-06

Training epoch-54 batch-36
Running loss of epoch-54 batch-36 = 1.3342010788619518e-05

Training epoch-54 batch-37
Running loss of epoch-54 batch-37 = 5.266978405416012e-06

Training epoch-54 batch-38
Running loss of epoch-54 batch-38 = 5.351536674425006e-05

Training epoch-54 batch-39
Running loss of epoch-54 batch-39 = 9.490177035331726e-07

Training epoch-54 batch-40
Running loss of epoch-54 batch-40 = 2.1336600184440613e-06

Training epoch-54 batch-41
Running loss of epoch-54 batch-41 = 2.4901237338781357e-06

Training epoch-54 batch-42
Running loss of epoch-54 batch-42 = 6.5426575019955635e-06

Training epoch-54 batch-43
Running loss of epoch-54 batch-43 = 5.830079317092896e-07

Training epoch-54 batch-44
Running loss of epoch-54 batch-44 = 2.6044435799121857e-06

Training epoch-54 batch-45
Running loss of epoch-54 batch-45 = 8.169910870492458e-06

Training epoch-54 batch-46
Running loss of epoch-54 batch-46 = 1.0335352271795273e-06

Training epoch-54 batch-47
Running loss of epoch-54 batch-47 = 6.984919309616089e-07

Training epoch-54 batch-48
Running loss of epoch-54 batch-48 = 5.996902473270893e-06

Training epoch-54 batch-49
Running loss of epoch-54 batch-49 = 2.0866282284259796e-06

Training epoch-54 batch-50
Running loss of epoch-54 batch-50 = 1.694774255156517e-06

Training epoch-54 batch-51
Running loss of epoch-54 batch-51 = 6.621703505516052e-07

Training epoch-54 batch-52
Running loss of epoch-54 batch-52 = 2.643093466758728e-06

Training epoch-54 batch-53
Running loss of epoch-54 batch-53 = 4.274304956197739e-06

Training epoch-54 batch-54
Running loss of epoch-54 batch-54 = 1.7157290130853653e-06

Training epoch-54 batch-55
Running loss of epoch-54 batch-55 = 5.012843757867813e-07

Training epoch-54 batch-56
Running loss of epoch-54 batch-56 = 1.2682285159826279e-06

Training epoch-54 batch-57
Running loss of epoch-54 batch-57 = 6.010755896568298e-06

Training epoch-54 batch-58
Running loss of epoch-54 batch-58 = 3.741821274161339e-06

Training epoch-54 batch-59
Running loss of epoch-54 batch-59 = 2.261018380522728e-06

Training epoch-54 batch-60
Running loss of epoch-54 batch-60 = 1.1886004358530045e-06

Training epoch-54 batch-61
Running loss of epoch-54 batch-61 = 2.8321868740022182e-05

Training epoch-54 batch-62
Running loss of epoch-54 batch-62 = 2.055894583463669e-06

Training epoch-54 batch-63
Running loss of epoch-54 batch-63 = 6.083864718675613e-07

Training epoch-54 batch-64
Running loss of epoch-54 batch-64 = 2.1684798412024975e-05

Training epoch-54 batch-65
Running loss of epoch-54 batch-65 = 1.962529495358467e-06

Training epoch-54 batch-66
Running loss of epoch-54 batch-66 = 2.841465175151825e-06

Training epoch-54 batch-67
Running loss of epoch-54 batch-67 = 2.6384368538856506e-06

Training epoch-54 batch-68
Running loss of epoch-54 batch-68 = 6.975024007260799e-06

Training epoch-54 batch-69
Running loss of epoch-54 batch-69 = 3.738212399184704e-06

Training epoch-54 batch-70
Running loss of epoch-54 batch-70 = 1.1275988072156906e-06

Training epoch-54 batch-71
Running loss of epoch-54 batch-71 = 2.8547015972435474e-05

Training epoch-54 batch-72
Running loss of epoch-54 batch-72 = 3.6815181374549866e-06

Training epoch-54 batch-73
Running loss of epoch-54 batch-73 = 1.6978592611849308e-05

Training epoch-54 batch-74
Running loss of epoch-54 batch-74 = 8.82765743881464e-06

Training epoch-54 batch-75
Running loss of epoch-54 batch-75 = 5.235779099166393e-06

Training epoch-54 batch-76
Running loss of epoch-54 batch-76 = 2.0818901248276234e-05

Training epoch-54 batch-77
Running loss of epoch-54 batch-77 = 1.5643774531781673e-05

Training epoch-54 batch-78
Running loss of epoch-54 batch-78 = 7.734633982181549e-07

Training epoch-54 batch-79
Running loss of epoch-54 batch-79 = 1.5185214579105377e-06

Training epoch-54 batch-80
Running loss of epoch-54 batch-80 = 1.2496020644903183e-06

Training epoch-54 batch-81
Running loss of epoch-54 batch-81 = 1.3844110071659088e-06

Training epoch-54 batch-82
Running loss of epoch-54 batch-82 = 5.309353582561016e-06

Training epoch-54 batch-83
Running loss of epoch-54 batch-83 = 1.915963366627693e-06

Training epoch-54 batch-84
Running loss of epoch-54 batch-84 = 5.915528163313866e-06

Training epoch-54 batch-85
Running loss of epoch-54 batch-85 = 4.772329702973366e-06

Training epoch-54 batch-86
Running loss of epoch-54 batch-86 = 3.74671071767807e-06

Training epoch-54 batch-87
Running loss of epoch-54 batch-87 = 1.2386590242385864e-06

Training epoch-54 batch-88
Running loss of epoch-54 batch-88 = 4.265923053026199e-06

Training epoch-54 batch-89
Running loss of epoch-54 batch-89 = 5.810870788991451e-06

Training epoch-54 batch-90
Running loss of epoch-54 batch-90 = 2.216582652181387e-05

Training epoch-54 batch-91
Running loss of epoch-54 batch-91 = 1.635379157960415e-05

Training epoch-54 batch-92
Running loss of epoch-54 batch-92 = 1.0156072676181793e-06

Training epoch-54 batch-93
Running loss of epoch-54 batch-93 = 6.2749022617936134e-06

Training epoch-54 batch-94
Running loss of epoch-54 batch-94 = 0.00010582531103864312

Training epoch-54 batch-95
Running loss of epoch-54 batch-95 = 1.7285346984863281e-06

Training epoch-54 batch-96
Running loss of epoch-54 batch-96 = 7.648486644029617e-07

Training epoch-54 batch-97
Running loss of epoch-54 batch-97 = 4.647299647331238e-07

Training epoch-54 batch-98
Running loss of epoch-54 batch-98 = 2.323649823665619e-06

Training epoch-54 batch-99
Running loss of epoch-54 batch-99 = 6.901100277900696e-07

Training epoch-54 batch-100
Running loss of epoch-54 batch-100 = 2.2675376385450363e-06

Training epoch-54 batch-101
Running loss of epoch-54 batch-101 = 3.0030496418476105e-06

Training epoch-54 batch-102
Running loss of epoch-54 batch-102 = 5.343463271856308e-07

Training epoch-54 batch-103
Running loss of epoch-54 batch-103 = 4.914239980280399e-06

Training epoch-54 batch-104
Running loss of epoch-54 batch-104 = 2.0421575754880905e-06

Training epoch-54 batch-105
Running loss of epoch-54 batch-105 = 1.5175901353359222e-06

Training epoch-54 batch-106
Running loss of epoch-54 batch-106 = 1.4586490578949451e-05

Training epoch-54 batch-107
Running loss of epoch-54 batch-107 = 3.061257302761078e-06

Training epoch-54 batch-108
Running loss of epoch-54 batch-108 = 2.803863026201725e-06

Training epoch-54 batch-109
Running loss of epoch-54 batch-109 = 1.5059835277497768e-05

Training epoch-54 batch-110
Running loss of epoch-54 batch-110 = 2.025393769145012e-06

Training epoch-54 batch-111
Running loss of epoch-54 batch-111 = 1.3292301446199417e-06

Training epoch-54 batch-112
Running loss of epoch-54 batch-112 = 3.9833830669522285e-06

Training epoch-54 batch-113
Running loss of epoch-54 batch-113 = 6.584357470273972e-05

Training epoch-54 batch-114
Running loss of epoch-54 batch-114 = 5.420297384262085e-07

Training epoch-54 batch-115
Running loss of epoch-54 batch-115 = 2.782326191663742e-06

Training epoch-54 batch-116
Running loss of epoch-54 batch-116 = 5.336944013834e-06

Training epoch-54 batch-117
Running loss of epoch-54 batch-117 = 4.485680256038904e-05

Training epoch-54 batch-118
Running loss of epoch-54 batch-118 = 4.338216967880726e-06

Training epoch-54 batch-119
Running loss of epoch-54 batch-119 = 5.867215804755688e-06

Training epoch-54 batch-120
Running loss of epoch-54 batch-120 = 3.5925768315792084e-06

Training epoch-54 batch-121
Running loss of epoch-54 batch-121 = 4.984904080629349e-07

Training epoch-54 batch-122
Running loss of epoch-54 batch-122 = 1.455424353480339e-06

Training epoch-54 batch-123
Running loss of epoch-54 batch-123 = 1.0633375495672226e-06

Training epoch-54 batch-124
Running loss of epoch-54 batch-124 = 1.0500778444111347e-05

Training epoch-54 batch-125
Running loss of epoch-54 batch-125 = 1.6042031347751617e-06

Training epoch-54 batch-126
Running loss of epoch-54 batch-126 = 4.048575647175312e-06

Training epoch-54 batch-127
Running loss of epoch-54 batch-127 = 3.1905947253108025e-06

Training epoch-54 batch-128
Running loss of epoch-54 batch-128 = 1.4469027519226074e-05

Training epoch-54 batch-129
Running loss of epoch-54 batch-129 = 2.5122426450252533e-06

Training epoch-54 batch-130
Running loss of epoch-54 batch-130 = 4.245433956384659e-06

Training epoch-54 batch-131
Running loss of epoch-54 batch-131 = 3.2905954867601395e-06

Training epoch-54 batch-132
Running loss of epoch-54 batch-132 = 8.016359061002731e-07

Training epoch-54 batch-133
Running loss of epoch-54 batch-133 = 5.3497496992349625e-06

Training epoch-54 batch-134
Running loss of epoch-54 batch-134 = 3.5923440009355545e-06

Training epoch-54 batch-135
Running loss of epoch-54 batch-135 = 4.153698682785034e-06

Training epoch-54 batch-136
Running loss of epoch-54 batch-136 = 8.691567927598953e-07

Training epoch-54 batch-137
Running loss of epoch-54 batch-137 = 7.287715561687946e-06

Training epoch-54 batch-138
Running loss of epoch-54 batch-138 = 6.970949470996857e-07

Training epoch-54 batch-139
Running loss of epoch-54 batch-139 = 4.5876833610236645e-05

Training epoch-54 batch-140
Running loss of epoch-54 batch-140 = 7.9302117228508e-07

Training epoch-54 batch-141
Running loss of epoch-54 batch-141 = 1.7348211258649826e-06

Training epoch-54 batch-142
Running loss of epoch-54 batch-142 = 2.4102162569761276e-05

Training epoch-54 batch-143
Running loss of epoch-54 batch-143 = 8.867122232913971e-06

Training epoch-54 batch-144
Running loss of epoch-54 batch-144 = 1.1743977665901184e-06

Training epoch-54 batch-145
Running loss of epoch-54 batch-145 = 9.937211871147156e-06

Training epoch-54 batch-146
Running loss of epoch-54 batch-146 = 8.756411261856556e-06

Training epoch-54 batch-147
Running loss of epoch-54 batch-147 = 3.3974647521972656e-06

Training epoch-54 batch-148
Running loss of epoch-54 batch-148 = 1.4491379261016846e-06

Training epoch-54 batch-149
Running loss of epoch-54 batch-149 = 9.029405191540718e-06

Training epoch-54 batch-150
Running loss of epoch-54 batch-150 = 3.273831680417061e-06

Training epoch-54 batch-151
Running loss of epoch-54 batch-151 = 1.684064045548439e-06

Training epoch-54 batch-152
Running loss of epoch-54 batch-152 = 1.3692770153284073e-06

Training epoch-54 batch-153
Running loss of epoch-54 batch-153 = 1.2959353625774384e-06

Training epoch-54 batch-154
Running loss of epoch-54 batch-154 = 1.998152583837509e-06

Training epoch-54 batch-155
Running loss of epoch-54 batch-155 = 1.4905817806720734e-06

Training epoch-54 batch-156
Running loss of epoch-54 batch-156 = 3.597000613808632e-06

Training epoch-54 batch-157
Running loss of epoch-54 batch-157 = 4.302710294723511e-06

Finished training epoch-54.



Average train loss at epoch-54 = 7.364210486412048e-06

Started Evaluation

Average val loss at epoch-54 = 5.334917730406711

Accuracy for classes:
Accuracy for class equals is: 68.81 %
Accuracy for class main is: 48.03 %
Accuracy for class setUp is: 53.61 %
Accuracy for class onCreate is: 42.54 %
Accuracy for class toString is: 41.64 %
Accuracy for class run is: 34.47 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 21.75 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 30.00 %

Overall Accuracy = 45.10 %

Finished Evaluation



Started training epoch-55


Training epoch-55 batch-1
Running loss of epoch-55 batch-1 = 1.692911610007286e-06

Training epoch-55 batch-2
Running loss of epoch-55 batch-2 = 1.1117663234472275e-06

Training epoch-55 batch-3
Running loss of epoch-55 batch-3 = 1.0759104043245316e-06

Training epoch-55 batch-4
Running loss of epoch-55 batch-4 = 1.7743674106895924e-05

Training epoch-55 batch-5
Running loss of epoch-55 batch-5 = 4.600267857313156e-06

Training epoch-55 batch-6
Running loss of epoch-55 batch-6 = 6.439629942178726e-06

Training epoch-55 batch-7
Running loss of epoch-55 batch-7 = 5.114590749144554e-06

Training epoch-55 batch-8
Running loss of epoch-55 batch-8 = 1.753680408000946e-06

Training epoch-55 batch-9
Running loss of epoch-55 batch-9 = 1.5534460544586182e-06

Training epoch-55 batch-10
Running loss of epoch-55 batch-10 = 3.128545358777046e-06

Training epoch-55 batch-11
Running loss of epoch-55 batch-11 = 4.577916115522385e-06

Training epoch-55 batch-12
Running loss of epoch-55 batch-12 = 2.5024637579917908e-06

Training epoch-55 batch-13
Running loss of epoch-55 batch-13 = 1.5015248209238052e-06

Training epoch-55 batch-14
Running loss of epoch-55 batch-14 = 5.2228569984436035e-06

Training epoch-55 batch-15
Running loss of epoch-55 batch-15 = 1.6370322555303574e-06

Training epoch-55 batch-16
Running loss of epoch-55 batch-16 = 2.5676563382148743e-06

Training epoch-55 batch-17
Running loss of epoch-55 batch-17 = 2.980465069413185e-06

Training epoch-55 batch-18
Running loss of epoch-55 batch-18 = 4.770583473145962e-06

Training epoch-55 batch-19
Running loss of epoch-55 batch-19 = 1.5695113688707352e-06

Training epoch-55 batch-20
Running loss of epoch-55 batch-20 = 1.9667204469442368e-06

Training epoch-55 batch-21
Running loss of epoch-55 batch-21 = 8.663279004395008e-06

Training epoch-55 batch-22
Running loss of epoch-55 batch-22 = 4.841363988816738e-06

Training epoch-55 batch-23
Running loss of epoch-55 batch-23 = 7.629860192537308e-07

Training epoch-55 batch-24
Running loss of epoch-55 batch-24 = 6.377114914357662e-06

Training epoch-55 batch-25
Running loss of epoch-55 batch-25 = 5.489331670105457e-06

Training epoch-55 batch-26
Running loss of epoch-55 batch-26 = 2.0298175513744354e-06

Training epoch-55 batch-27
Running loss of epoch-55 batch-27 = 1.7117708921432495e-06

Training epoch-55 batch-28
Running loss of epoch-55 batch-28 = 3.4787808544933796e-05

Training epoch-55 batch-29
Running loss of epoch-55 batch-29 = 5.1532406359910965e-06

Training epoch-55 batch-30
Running loss of epoch-55 batch-30 = 3.609573468565941e-06

Training epoch-55 batch-31
Running loss of epoch-55 batch-31 = 1.034000888466835e-06

Training epoch-55 batch-32
Running loss of epoch-55 batch-32 = 3.0670780688524246e-06

Training epoch-55 batch-33
Running loss of epoch-55 batch-33 = 1.0081683285534382e-05

Training epoch-55 batch-34
Running loss of epoch-55 batch-34 = 1.5567056834697723e-06

Training epoch-55 batch-35
Running loss of epoch-55 batch-35 = 8.123228326439857e-06

Training epoch-55 batch-36
Running loss of epoch-55 batch-36 = 1.578067895025015e-05

Training epoch-55 batch-37
Running loss of epoch-55 batch-37 = 5.5265845730900764e-06

Training epoch-55 batch-38
Running loss of epoch-55 batch-38 = 2.1443702280521393e-06

Training epoch-55 batch-39
Running loss of epoch-55 batch-39 = 2.4485867470502853e-05

Training epoch-55 batch-40
Running loss of epoch-55 batch-40 = 5.112960934638977e-06

Training epoch-55 batch-41
Running loss of epoch-55 batch-41 = 1.149950549006462e-06

Training epoch-55 batch-42
Running loss of epoch-55 batch-42 = 9.688083082437515e-07

Training epoch-55 batch-43
Running loss of epoch-55 batch-43 = 8.25151801109314e-07

Training epoch-55 batch-44
Running loss of epoch-55 batch-44 = 1.8461141735315323e-06

Training epoch-55 batch-45
Running loss of epoch-55 batch-45 = 1.7869751900434494e-06

Training epoch-55 batch-46
Running loss of epoch-55 batch-46 = 3.965338692069054e-06

Training epoch-55 batch-47
Running loss of epoch-55 batch-47 = 3.1784875318408012e-06

Training epoch-55 batch-48
Running loss of epoch-55 batch-48 = 4.6356581151485443e-07

Training epoch-55 batch-49
Running loss of epoch-55 batch-49 = 7.865019142627716e-07

Training epoch-55 batch-50
Running loss of epoch-55 batch-50 = 7.774913683533669e-06

Training epoch-55 batch-51
Running loss of epoch-55 batch-51 = 3.5588163882493973e-06

Training epoch-55 batch-52
Running loss of epoch-55 batch-52 = 2.54317419603467e-05

Training epoch-55 batch-53
Running loss of epoch-55 batch-53 = 3.3152755349874496e-06

Training epoch-55 batch-54
Running loss of epoch-55 batch-54 = 1.430511474609375e-06

Training epoch-55 batch-55
Running loss of epoch-55 batch-55 = 4.394561983644962e-06

Training epoch-55 batch-56
Running loss of epoch-55 batch-56 = 2.9515940696001053e-06

Training epoch-55 batch-57
Running loss of epoch-55 batch-57 = 1.9795261323451996e-06

Training epoch-55 batch-58
Running loss of epoch-55 batch-58 = 1.054815948009491e-05

Training epoch-55 batch-59
Running loss of epoch-55 batch-59 = 1.9273720681667328e-06

Training epoch-55 batch-60
Running loss of epoch-55 batch-60 = 1.0221265256404877e-06

Training epoch-55 batch-61
Running loss of epoch-55 batch-61 = 2.119690179824829e-06

Training epoch-55 batch-62
Running loss of epoch-55 batch-62 = 1.5210825949907303e-06

Training epoch-55 batch-63
Running loss of epoch-55 batch-63 = 5.569541826844215e-06

Training epoch-55 batch-64
Running loss of epoch-55 batch-64 = 2.760672941803932e-06

Training epoch-55 batch-65
Running loss of epoch-55 batch-65 = 1.4114193618297577e-06

Training epoch-55 batch-66
Running loss of epoch-55 batch-66 = 1.730630174279213e-06

Training epoch-55 batch-67
Running loss of epoch-55 batch-67 = 1.1045485734939575e-05

Training epoch-55 batch-68
Running loss of epoch-55 batch-68 = 8.542323485016823e-06

Training epoch-55 batch-69
Running loss of epoch-55 batch-69 = 8.999835699796677e-06

Training epoch-55 batch-70
Running loss of epoch-55 batch-70 = 1.5897909179329872e-05

Training epoch-55 batch-71
Running loss of epoch-55 batch-71 = 1.178588718175888e-06

Training epoch-55 batch-72
Running loss of epoch-55 batch-72 = 5.7730358093976974e-06

Training epoch-55 batch-73
Running loss of epoch-55 batch-73 = 8.272472769021988e-07

Training epoch-55 batch-74
Running loss of epoch-55 batch-74 = 1.2568198144435883e-06

Training epoch-55 batch-75
Running loss of epoch-55 batch-75 = 8.801231160759926e-06

Training epoch-55 batch-76
Running loss of epoch-55 batch-76 = 1.962995156645775e-06

Training epoch-55 batch-77
Running loss of epoch-55 batch-77 = 1.3656681403517723e-05

Training epoch-55 batch-78
Running loss of epoch-55 batch-78 = 6.472226232290268e-06

Training epoch-55 batch-79
Running loss of epoch-55 batch-79 = 2.24084360525012e-05

Training epoch-55 batch-80
Running loss of epoch-55 batch-80 = 8.682254701852798e-07

Training epoch-55 batch-81
Running loss of epoch-55 batch-81 = 4.057073965668678e-06

Training epoch-55 batch-82
Running loss of epoch-55 batch-82 = 3.821100108325481e-06

Training epoch-55 batch-83
Running loss of epoch-55 batch-83 = 2.1002371795475483e-05

Training epoch-55 batch-84
Running loss of epoch-55 batch-84 = 6.2979524955153465e-06

Training epoch-55 batch-85
Running loss of epoch-55 batch-85 = 2.0500156097114086e-05

Training epoch-55 batch-86
Running loss of epoch-55 batch-86 = 1.4640390872955322e-06

Training epoch-55 batch-87
Running loss of epoch-55 batch-87 = 1.1098221875727177e-05

Training epoch-55 batch-88
Running loss of epoch-55 batch-88 = 3.1063100323081017e-06

Training epoch-55 batch-89
Running loss of epoch-55 batch-89 = 2.5335466489195824e-06

Training epoch-55 batch-90
Running loss of epoch-55 batch-90 = 2.323416993021965e-06

Training epoch-55 batch-91
Running loss of epoch-55 batch-91 = 1.2496020644903183e-06

Training epoch-55 batch-92
Running loss of epoch-55 batch-92 = 2.3418106138706207e-06

Training epoch-55 batch-93
Running loss of epoch-55 batch-93 = 3.316672518849373e-06

Training epoch-55 batch-94
Running loss of epoch-55 batch-94 = 3.1329691410064697e-06

Training epoch-55 batch-95
Running loss of epoch-55 batch-95 = 9.203329682350159e-06

Training epoch-55 batch-96
Running loss of epoch-55 batch-96 = 1.3783574104309082e-06

Training epoch-55 batch-97
Running loss of epoch-55 batch-97 = 7.867347449064255e-07

Training epoch-55 batch-98
Running loss of epoch-55 batch-98 = 7.986091077327728e-07

Training epoch-55 batch-99
Running loss of epoch-55 batch-99 = 1.18983443826437e-05

Training epoch-55 batch-100
Running loss of epoch-55 batch-100 = 3.044959157705307e-06

Training epoch-55 batch-101
Running loss of epoch-55 batch-101 = 5.891581531614065e-05

Training epoch-55 batch-102
Running loss of epoch-55 batch-102 = 1.2624077498912811e-06

Training epoch-55 batch-103
Running loss of epoch-55 batch-103 = 6.0228630900382996e-06

Training epoch-55 batch-104
Running loss of epoch-55 batch-104 = 4.8496294766664505e-06

Training epoch-55 batch-105
Running loss of epoch-55 batch-105 = 5.720183253288269e-06

Training epoch-55 batch-106
Running loss of epoch-55 batch-106 = 1.3869721442461014e-06

Training epoch-55 batch-107
Running loss of epoch-55 batch-107 = 5.735200829803944e-06

Training epoch-55 batch-108
Running loss of epoch-55 batch-108 = 7.743132300674915e-06

Training epoch-55 batch-109
Running loss of epoch-55 batch-109 = 2.393033355474472e-06

Training epoch-55 batch-110
Running loss of epoch-55 batch-110 = 1.048436388373375e-06

Training epoch-55 batch-111
Running loss of epoch-55 batch-111 = 1.4243647456169128e-05

Training epoch-55 batch-112
Running loss of epoch-55 batch-112 = 4.145083948969841e-06

Training epoch-55 batch-113
Running loss of epoch-55 batch-113 = 3.9620790630578995e-06

Training epoch-55 batch-114
Running loss of epoch-55 batch-114 = 6.190792191773653e-05

Training epoch-55 batch-115
Running loss of epoch-55 batch-115 = 4.343455657362938e-06

Training epoch-55 batch-116
Running loss of epoch-55 batch-116 = 1.5203841030597687e-06

Training epoch-55 batch-117
Running loss of epoch-55 batch-117 = 1.8884893506765366e-06

Training epoch-55 batch-118
Running loss of epoch-55 batch-118 = 2.1991319954395294e-05

Training epoch-55 batch-119
Running loss of epoch-55 batch-119 = 2.0785839296877384e-05

Training epoch-55 batch-120
Running loss of epoch-55 batch-120 = 1.3837474398314953e-05

Training epoch-55 batch-121
Running loss of epoch-55 batch-121 = 2.0775478333234787e-06

Training epoch-55 batch-122
Running loss of epoch-55 batch-122 = 1.8684659153223038e-06

Training epoch-55 batch-123
Running loss of epoch-55 batch-123 = 5.4889824241399765e-06

Training epoch-55 batch-124
Running loss of epoch-55 batch-124 = 3.659632056951523e-06

Training epoch-55 batch-125
Running loss of epoch-55 batch-125 = 1.3348646461963654e-05

Training epoch-55 batch-126
Running loss of epoch-55 batch-126 = 3.0511291697621346e-06

Training epoch-55 batch-127
Running loss of epoch-55 batch-127 = 1.0200310498476028e-06

Training epoch-55 batch-128
Running loss of epoch-55 batch-128 = 1.3045617379248142e-05

Training epoch-55 batch-129
Running loss of epoch-55 batch-129 = 7.122289389371872e-07

Training epoch-55 batch-130
Running loss of epoch-55 batch-130 = 1.2223608791828156e-06

Training epoch-55 batch-131
Running loss of epoch-55 batch-131 = 1.1452939361333847e-05

Training epoch-55 batch-132
Running loss of epoch-55 batch-132 = 9.310897439718246e-07

Training epoch-55 batch-133
Running loss of epoch-55 batch-133 = 5.3908443078398705e-06

Training epoch-55 batch-134
Running loss of epoch-55 batch-134 = 1.618964597582817e-05

Training epoch-55 batch-135
Running loss of epoch-55 batch-135 = 5.721347406506538e-06

Training epoch-55 batch-136
Running loss of epoch-55 batch-136 = 1.3175886124372482e-06

Training epoch-55 batch-137
Running loss of epoch-55 batch-137 = 8.880160748958588e-07

Training epoch-55 batch-138
Running loss of epoch-55 batch-138 = 5.811103619635105e-06

Training epoch-55 batch-139
Running loss of epoch-55 batch-139 = 5.75394369661808e-06

Training epoch-55 batch-140
Running loss of epoch-55 batch-140 = 3.8532307371497154e-06

Training epoch-55 batch-141
Running loss of epoch-55 batch-141 = 1.0445713996887207e-05

Training epoch-55 batch-142
Running loss of epoch-55 batch-142 = 5.387701094150543e-07

Training epoch-55 batch-143
Running loss of epoch-55 batch-143 = 5.727633833885193e-07

Training epoch-55 batch-144
Running loss of epoch-55 batch-144 = 3.4389086067676544e-06

Training epoch-55 batch-145
Running loss of epoch-55 batch-145 = 6.32135197520256e-07

Training epoch-55 batch-146
Running loss of epoch-55 batch-146 = 1.307111233472824e-06

Training epoch-55 batch-147
Running loss of epoch-55 batch-147 = 1.0880641639232635e-05

Training epoch-55 batch-148
Running loss of epoch-55 batch-148 = 2.5150366127490997e-06

Training epoch-55 batch-149
Running loss of epoch-55 batch-149 = 1.1008232831954956e-06

Training epoch-55 batch-150
Running loss of epoch-55 batch-150 = 5.4676784202456474e-06

Training epoch-55 batch-151
Running loss of epoch-55 batch-151 = 9.039219003170729e-05

Training epoch-55 batch-152
Running loss of epoch-55 batch-152 = 5.490612238645554e-06

Training epoch-55 batch-153
Running loss of epoch-55 batch-153 = 4.928349517285824e-05

Training epoch-55 batch-154
Running loss of epoch-55 batch-154 = 1.439359039068222e-06

Training epoch-55 batch-155
Running loss of epoch-55 batch-155 = 1.7050188034772873e-06

Training epoch-55 batch-156
Running loss of epoch-55 batch-156 = 3.785942681133747e-06

Training epoch-55 batch-157
Running loss of epoch-55 batch-157 = 1.817941665649414e-06

Finished training epoch-55.



Average train loss at epoch-55 = 6.881754845380783e-06

Started Evaluation

Average val loss at epoch-55 = 5.309579344172227

Accuracy for classes:
Accuracy for class equals is: 68.65 %
Accuracy for class main is: 49.67 %
Accuracy for class setUp is: 53.44 %
Accuracy for class onCreate is: 43.92 %
Accuracy for class toString is: 42.66 %
Accuracy for class run is: 35.39 %
Accuracy for class hashCode is: 80.90 %
Accuracy for class init is: 19.96 %
Accuracy for class execute is: 16.87 %
Accuracy for class get is: 28.46 %

Overall Accuracy = 45.29 %

Finished Evaluation



Started training epoch-56


Training epoch-56 batch-1
Running loss of epoch-56 batch-1 = 3.6512501537799835e-06

Training epoch-56 batch-2
Running loss of epoch-56 batch-2 = 2.568471245467663e-06

Training epoch-56 batch-3
Running loss of epoch-56 batch-3 = 1.2729200534522533e-05

Training epoch-56 batch-4
Running loss of epoch-56 batch-4 = 3.1497329473495483e-06

Training epoch-56 batch-5
Running loss of epoch-56 batch-5 = 2.810731530189514e-06

Training epoch-56 batch-6
Running loss of epoch-56 batch-6 = 1.1335127055644989e-05

Training epoch-56 batch-7
Running loss of epoch-56 batch-7 = 7.865019142627716e-07

Training epoch-56 batch-8
Running loss of epoch-56 batch-8 = 8.81205778568983e-06

Training epoch-56 batch-9
Running loss of epoch-56 batch-9 = 1.1275988072156906e-06

Training epoch-56 batch-10
Running loss of epoch-56 batch-10 = 3.275228664278984e-06

Training epoch-56 batch-11
Running loss of epoch-56 batch-11 = 1.2016389518976212e-06

Training epoch-56 batch-12
Running loss of epoch-56 batch-12 = 9.151292033493519e-06

Training epoch-56 batch-13
Running loss of epoch-56 batch-13 = 1.2421514838933945e-06

Training epoch-56 batch-14
Running loss of epoch-56 batch-14 = 5.136826075613499e-06

Training epoch-56 batch-15
Running loss of epoch-56 batch-15 = 1.2256205081939697e-06

Training epoch-56 batch-16
Running loss of epoch-56 batch-16 = 1.003732904791832e-06

Training epoch-56 batch-17
Running loss of epoch-56 batch-17 = 3.2660318538546562e-06

Training epoch-56 batch-18
Running loss of epoch-56 batch-18 = 2.679182216525078e-06

Training epoch-56 batch-19
Running loss of epoch-56 batch-19 = 1.6188714653253555e-06

Training epoch-56 batch-20
Running loss of epoch-56 batch-20 = 1.5939585864543915e-06

Training epoch-56 batch-21
Running loss of epoch-56 batch-21 = 2.902001142501831e-05

Training epoch-56 batch-22
Running loss of epoch-56 batch-22 = 9.353156201541424e-06

Training epoch-56 batch-23
Running loss of epoch-56 batch-23 = 3.0498486012220383e-06

Training epoch-56 batch-24
Running loss of epoch-56 batch-24 = 3.778841346502304e-06

Training epoch-56 batch-25
Running loss of epoch-56 batch-25 = 2.7265981771051884e-05

Training epoch-56 batch-26
Running loss of epoch-56 batch-26 = 4.100496880710125e-06

Training epoch-56 batch-27
Running loss of epoch-56 batch-27 = 2.226955257356167e-05

Training epoch-56 batch-28
Running loss of epoch-56 batch-28 = 4.411675035953522e-06

Training epoch-56 batch-29
Running loss of epoch-56 batch-29 = 1.728767529129982e-06

Training epoch-56 batch-30
Running loss of epoch-56 batch-30 = 3.019813448190689e-06

Training epoch-56 batch-31
Running loss of epoch-56 batch-31 = 6.234738975763321e-06

Training epoch-56 batch-32
Running loss of epoch-56 batch-32 = 1.8505379557609558e-06

Training epoch-56 batch-33
Running loss of epoch-56 batch-33 = 6.654299795627594e-07

Training epoch-56 batch-34
Running loss of epoch-56 batch-34 = 4.653818905353546e-06

Training epoch-56 batch-35
Running loss of epoch-56 batch-35 = 9.543378837406635e-06

Training epoch-56 batch-36
Running loss of epoch-56 batch-36 = 3.7671998143196106e-07

Training epoch-56 batch-37
Running loss of epoch-56 batch-37 = 3.3444957807660103e-06

Training epoch-56 batch-38
Running loss of epoch-56 batch-38 = 1.0577496141195297e-06

Training epoch-56 batch-39
Running loss of epoch-56 batch-39 = 6.150687113404274e-06

Training epoch-56 batch-40
Running loss of epoch-56 batch-40 = 2.9585789889097214e-06

Training epoch-56 batch-41
Running loss of epoch-56 batch-41 = 4.404457286000252e-06

Training epoch-56 batch-42
Running loss of epoch-56 batch-42 = 6.129732355475426e-06

Training epoch-56 batch-43
Running loss of epoch-56 batch-43 = 8.535571396350861e-07

Training epoch-56 batch-44
Running loss of epoch-56 batch-44 = 1.8663005903363228e-05

Training epoch-56 batch-45
Running loss of epoch-56 batch-45 = 1.5096738934516907e-06

Training epoch-56 batch-46
Running loss of epoch-56 batch-46 = 3.2249372452497482e-06

Training epoch-56 batch-47
Running loss of epoch-56 batch-47 = 1.9508879631757736e-06

Training epoch-56 batch-48
Running loss of epoch-56 batch-48 = 7.697963155806065e-06

Training epoch-56 batch-49
Running loss of epoch-56 batch-49 = 4.664179868996143e-06

Training epoch-56 batch-50
Running loss of epoch-56 batch-50 = 7.380731403827667e-07

Training epoch-56 batch-51
Running loss of epoch-56 batch-51 = 1.3026874512434006e-06

Training epoch-56 batch-52
Running loss of epoch-56 batch-52 = 1.769978553056717e-06

Training epoch-56 batch-53
Running loss of epoch-56 batch-53 = 8.128001354634762e-06

Training epoch-56 batch-54
Running loss of epoch-56 batch-54 = 5.653128027915955e-07

Training epoch-56 batch-55
Running loss of epoch-56 batch-55 = 1.6102567315101624e-06

Training epoch-56 batch-56
Running loss of epoch-56 batch-56 = 3.2670795917510986e-06

Training epoch-56 batch-57
Running loss of epoch-56 batch-57 = 2.0493753254413605e-06

Training epoch-56 batch-58
Running loss of epoch-56 batch-58 = 1.4919787645339966e-06

Training epoch-56 batch-59
Running loss of epoch-56 batch-59 = 1.0121148079633713e-06

Training epoch-56 batch-60
Running loss of epoch-56 batch-60 = 4.681525751948357e-06

Training epoch-56 batch-61
Running loss of epoch-56 batch-61 = 6.776419468224049e-06

Training epoch-56 batch-62
Running loss of epoch-56 batch-62 = 9.791925549507141e-06

Training epoch-56 batch-63
Running loss of epoch-56 batch-63 = 2.596643753349781e-06

Training epoch-56 batch-64
Running loss of epoch-56 batch-64 = 2.4909386411309242e-06

Training epoch-56 batch-65
Running loss of epoch-56 batch-65 = 2.1371524780988693e-06

Training epoch-56 batch-66
Running loss of epoch-56 batch-66 = 1.2729433365166187e-05

Training epoch-56 batch-67
Running loss of epoch-56 batch-67 = 3.230525180697441e-06

Training epoch-56 batch-68
Running loss of epoch-56 batch-68 = 5.163601599633694e-06

Training epoch-56 batch-69
Running loss of epoch-56 batch-69 = 1.2819655239582062e-06

Training epoch-56 batch-70
Running loss of epoch-56 batch-70 = 1.0871444828808308e-05

Training epoch-56 batch-71
Running loss of epoch-56 batch-71 = 3.3159740269184113e-06

Training epoch-56 batch-72
Running loss of epoch-56 batch-72 = 3.693043254315853e-06

Training epoch-56 batch-73
Running loss of epoch-56 batch-73 = 5.997600965201855e-06

Training epoch-56 batch-74
Running loss of epoch-56 batch-74 = 6.741611286997795e-06

Training epoch-56 batch-75
Running loss of epoch-56 batch-75 = 1.673353835940361e-06

Training epoch-56 batch-76
Running loss of epoch-56 batch-76 = 1.3737007975578308e-06

Training epoch-56 batch-77
Running loss of epoch-56 batch-77 = 3.2635871320962906e-06

Training epoch-56 batch-78
Running loss of epoch-56 batch-78 = 2.4491455405950546e-06

Training epoch-56 batch-79
Running loss of epoch-56 batch-79 = 1.1576339602470398e-06

Training epoch-56 batch-80
Running loss of epoch-56 batch-80 = 1.916196197271347e-06

Training epoch-56 batch-81
Running loss of epoch-56 batch-81 = 2.3440225049853325e-06

Training epoch-56 batch-82
Running loss of epoch-56 batch-82 = 1.7918646335601807e-06

Training epoch-56 batch-83
Running loss of epoch-56 batch-83 = 1.1731754057109356e-05

Training epoch-56 batch-84
Running loss of epoch-56 batch-84 = 2.7029309421777725e-06

Training epoch-56 batch-85
Running loss of epoch-56 batch-85 = 3.058928996324539e-06

Training epoch-56 batch-86
Running loss of epoch-56 batch-86 = 1.6307458281517029e-06

Training epoch-56 batch-87
Running loss of epoch-56 batch-87 = 8.682254701852798e-07

Training epoch-56 batch-88
Running loss of epoch-56 batch-88 = 1.1135824024677277e-05

Training epoch-56 batch-89
Running loss of epoch-56 batch-89 = 1.3327226042747498e-06

Training epoch-56 batch-90
Running loss of epoch-56 batch-90 = 4.440313205122948e-06

Training epoch-56 batch-91
Running loss of epoch-56 batch-91 = 1.2309756129980087e-06

Training epoch-56 batch-92
Running loss of epoch-56 batch-92 = 1.7066486179828644e-06

Training epoch-56 batch-93
Running loss of epoch-56 batch-93 = 1.1131633073091507e-06

Training epoch-56 batch-94
Running loss of epoch-56 batch-94 = 7.119262591004372e-06

Training epoch-56 batch-95
Running loss of epoch-56 batch-95 = 6.146728992462158e-07

Training epoch-56 batch-96
Running loss of epoch-56 batch-96 = 4.92902472615242e-07

Training epoch-56 batch-97
Running loss of epoch-56 batch-97 = 1.2847594916820526e-06

Training epoch-56 batch-98
Running loss of epoch-56 batch-98 = 1.0388554073870182e-05

Training epoch-56 batch-99
Running loss of epoch-56 batch-99 = 1.0553747415542603e-05

Training epoch-56 batch-100
Running loss of epoch-56 batch-100 = 4.665344022214413e-06

Training epoch-56 batch-101
Running loss of epoch-56 batch-101 = 2.85380519926548e-06

Training epoch-56 batch-102
Running loss of epoch-56 batch-102 = 3.874767571687698e-06

Training epoch-56 batch-103
Running loss of epoch-56 batch-103 = 2.508983016014099e-06

Training epoch-56 batch-104
Running loss of epoch-56 batch-104 = 6.884336471557617e-06

Training epoch-56 batch-105
Running loss of epoch-56 batch-105 = 7.297971751540899e-05

Training epoch-56 batch-106
Running loss of epoch-56 batch-106 = 6.750342436134815e-06

Training epoch-56 batch-107
Running loss of epoch-56 batch-107 = 4.073604941368103e-06

Training epoch-56 batch-108
Running loss of epoch-56 batch-108 = 1.7691636458039284e-06

Training epoch-56 batch-109
Running loss of epoch-56 batch-109 = 3.2137613743543625e-06

Training epoch-56 batch-110
Running loss of epoch-56 batch-110 = 3.284541890025139e-06

Training epoch-56 batch-111
Running loss of epoch-56 batch-111 = 3.749679308384657e-05

Training epoch-56 batch-112
Running loss of epoch-56 batch-112 = 4.995148628950119e-06

Training epoch-56 batch-113
Running loss of epoch-56 batch-113 = 3.792508505284786e-05

Training epoch-56 batch-114
Running loss of epoch-56 batch-114 = 1.97533518075943e-06

Training epoch-56 batch-115
Running loss of epoch-56 batch-115 = 3.5385601222515106e-06

Training epoch-56 batch-116
Running loss of epoch-56 batch-116 = 1.058459747582674e-05

Training epoch-56 batch-117
Running loss of epoch-56 batch-117 = 3.0852388590574265e-06

Training epoch-56 batch-118
Running loss of epoch-56 batch-118 = 2.552138175815344e-05

Training epoch-56 batch-119
Running loss of epoch-56 batch-119 = 1.2801960110664368e-05

Training epoch-56 batch-120
Running loss of epoch-56 batch-120 = 2.1406449377536774e-06

Training epoch-56 batch-121
Running loss of epoch-56 batch-121 = 1.7802231013774872e-06

Training epoch-56 batch-122
Running loss of epoch-56 batch-122 = 1.8314458429813385e-06

Training epoch-56 batch-123
Running loss of epoch-56 batch-123 = 9.417999535799026e-07

Training epoch-56 batch-124
Running loss of epoch-56 batch-124 = 1.6421545296907425e-06

Training epoch-56 batch-125
Running loss of epoch-56 batch-125 = 5.671754479408264e-07

Training epoch-56 batch-126
Running loss of epoch-56 batch-126 = 1.1027907021343708e-05

Training epoch-56 batch-127
Running loss of epoch-56 batch-127 = 6.070127710700035e-06

Training epoch-56 batch-128
Running loss of epoch-56 batch-128 = 1.8256250768899918e-06

Training epoch-56 batch-129
Running loss of epoch-56 batch-129 = 6.819609552621841e-07

Training epoch-56 batch-130
Running loss of epoch-56 batch-130 = 7.489812560379505e-06

Training epoch-56 batch-131
Running loss of epoch-56 batch-131 = 1.8442748114466667e-05

Training epoch-56 batch-132
Running loss of epoch-56 batch-132 = 8.699949830770493e-06

Training epoch-56 batch-133
Running loss of epoch-56 batch-133 = 1.2951670214533806e-05

Training epoch-56 batch-134
Running loss of epoch-56 batch-134 = 3.824708983302116e-06

Training epoch-56 batch-135
Running loss of epoch-56 batch-135 = 1.0067597031593323e-06

Training epoch-56 batch-136
Running loss of epoch-56 batch-136 = 1.5418045222759247e-06

Training epoch-56 batch-137
Running loss of epoch-56 batch-137 = 1.5415716916322708e-06

Training epoch-56 batch-138
Running loss of epoch-56 batch-138 = 1.5879049897193909e-06

Training epoch-56 batch-139
Running loss of epoch-56 batch-139 = 1.2337695807218552e-06

Training epoch-56 batch-140
Running loss of epoch-56 batch-140 = 2.2333115339279175e-06

Training epoch-56 batch-141
Running loss of epoch-56 batch-141 = 4.027620889246464e-06

Training epoch-56 batch-142
Running loss of epoch-56 batch-142 = 3.645196557044983e-06

Training epoch-56 batch-143
Running loss of epoch-56 batch-143 = 1.66833633556962e-05

Training epoch-56 batch-144
Running loss of epoch-56 batch-144 = 6.282678805291653e-05

Training epoch-56 batch-145
Running loss of epoch-56 batch-145 = 2.5671906769275665e-06

Training epoch-56 batch-146
Running loss of epoch-56 batch-146 = 2.759159542620182e-06

Training epoch-56 batch-147
Running loss of epoch-56 batch-147 = 2.6654452085494995e-06

Training epoch-56 batch-148
Running loss of epoch-56 batch-148 = 1.2825708836317062e-05

Training epoch-56 batch-149
Running loss of epoch-56 batch-149 = 5.107838660478592e-06

Training epoch-56 batch-150
Running loss of epoch-56 batch-150 = 1.953879836946726e-05

Training epoch-56 batch-151
Running loss of epoch-56 batch-151 = 6.146408850327134e-05

Training epoch-56 batch-152
Running loss of epoch-56 batch-152 = 1.0395888239145279e-06

Training epoch-56 batch-153
Running loss of epoch-56 batch-153 = 3.784964792430401e-05

Training epoch-56 batch-154
Running loss of epoch-56 batch-154 = 3.6386772990226746e-06

Training epoch-56 batch-155
Running loss of epoch-56 batch-155 = 1.1383090168237686e-06

Training epoch-56 batch-156
Running loss of epoch-56 batch-156 = 5.520181730389595e-06

Training epoch-56 batch-157
Running loss of epoch-56 batch-157 = 1.0654330253601074e-06

Finished training epoch-56.



Average train loss at epoch-56 = 6.69771321117878e-06

Started Evaluation
