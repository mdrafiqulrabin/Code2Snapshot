
DataLoader:

Started trainloader.
trainset = #10000
Finished trainloader.
Started valloader.
valset = #4847
Finished valloader.

Configuration:

DB_NAME = java-top10, IMG_TYPE = original, TRANS_SIZE = 1024
MAX_EPOCH = 100, BATCH_SIZE = 64

device = cuda:0



Started training epoch-1


Training epoch-1 batch-1
Running loss of epoch-1 batch-1 = 0.03580754995346069

Training epoch-1 batch-2
Running loss of epoch-1 batch-2 = 0.0358937568962574

Training epoch-1 batch-3
Running loss of epoch-1 batch-3 = 0.036074019968509674

Training epoch-1 batch-4
Running loss of epoch-1 batch-4 = 0.03598399832844734

Training epoch-1 batch-5
Running loss of epoch-1 batch-5 = 0.03636929392814636

Training epoch-1 batch-6
Running loss of epoch-1 batch-6 = 0.03688254579901695

Training epoch-1 batch-7
Running loss of epoch-1 batch-7 = 0.03619655966758728

Training epoch-1 batch-8
Running loss of epoch-1 batch-8 = 0.03592005744576454

Training epoch-1 batch-9
Running loss of epoch-1 batch-9 = 0.03596753254532814

Training epoch-1 batch-10
Running loss of epoch-1 batch-10 = 0.03610067814588547

Training epoch-1 batch-11
Running loss of epoch-1 batch-11 = 0.03621135279536247

Training epoch-1 batch-12
Running loss of epoch-1 batch-12 = 0.03603322058916092

Training epoch-1 batch-13
Running loss of epoch-1 batch-13 = 0.036083709448575974

Training epoch-1 batch-14
Running loss of epoch-1 batch-14 = 0.036213792860507965

Training epoch-1 batch-15
Running loss of epoch-1 batch-15 = 0.0360773466527462

Training epoch-1 batch-16
Running loss of epoch-1 batch-16 = 0.03591381013393402

Training epoch-1 batch-17
Running loss of epoch-1 batch-17 = 0.035657189786434174

Training epoch-1 batch-18
Running loss of epoch-1 batch-18 = 0.03623441234230995

Training epoch-1 batch-19
Running loss of epoch-1 batch-19 = 0.035986222326755524

Training epoch-1 batch-20
Running loss of epoch-1 batch-20 = 0.03655562549829483

Training epoch-1 batch-21
Running loss of epoch-1 batch-21 = 0.036162883043289185

Training epoch-1 batch-22
Running loss of epoch-1 batch-22 = 0.036006584763526917

Training epoch-1 batch-23
Running loss of epoch-1 batch-23 = 0.03604065626859665

Training epoch-1 batch-24
Running loss of epoch-1 batch-24 = 0.03605293482542038

Training epoch-1 batch-25
Running loss of epoch-1 batch-25 = 0.03602020815014839

Training epoch-1 batch-26
Running loss of epoch-1 batch-26 = 0.03614036738872528

Training epoch-1 batch-27
Running loss of epoch-1 batch-27 = 0.036030761897563934

Training epoch-1 batch-28
Running loss of epoch-1 batch-28 = 0.03589869663119316

Training epoch-1 batch-29
Running loss of epoch-1 batch-29 = 0.03602117300033569

Training epoch-1 batch-30
Running loss of epoch-1 batch-30 = 0.03597244247794151

Training epoch-1 batch-31
Running loss of epoch-1 batch-31 = 0.035990554839372635

Training epoch-1 batch-32
Running loss of epoch-1 batch-32 = 0.035988714545965195

Training epoch-1 batch-33
Running loss of epoch-1 batch-33 = 0.0358288511633873

Training epoch-1 batch-34
Running loss of epoch-1 batch-34 = 0.03617087006568909

Training epoch-1 batch-35
Running loss of epoch-1 batch-35 = 0.03608791157603264

Training epoch-1 batch-36
Running loss of epoch-1 batch-36 = 0.0360599085688591

Training epoch-1 batch-37
Running loss of epoch-1 batch-37 = 0.03618708997964859

Training epoch-1 batch-38
Running loss of epoch-1 batch-38 = 0.03603513538837433

Training epoch-1 batch-39
Running loss of epoch-1 batch-39 = 0.03618913143873215

Training epoch-1 batch-40
Running loss of epoch-1 batch-40 = 0.03601427376270294

Training epoch-1 batch-41
Running loss of epoch-1 batch-41 = 0.035953227430582047

Training epoch-1 batch-42
Running loss of epoch-1 batch-42 = 0.03627180680632591

Training epoch-1 batch-43
Running loss of epoch-1 batch-43 = 0.036239881068468094

Training epoch-1 batch-44
Running loss of epoch-1 batch-44 = 0.03592608496546745

Training epoch-1 batch-45
Running loss of epoch-1 batch-45 = 0.03593526408076286

Training epoch-1 batch-46
Running loss of epoch-1 batch-46 = 0.03621948882937431

Training epoch-1 batch-47
Running loss of epoch-1 batch-47 = 0.03630859777331352

Training epoch-1 batch-48
Running loss of epoch-1 batch-48 = 0.036040183156728745

Training epoch-1 batch-49
Running loss of epoch-1 batch-49 = 0.036144234240055084

Training epoch-1 batch-50
Running loss of epoch-1 batch-50 = 0.036049406975507736

Training epoch-1 batch-51
Running loss of epoch-1 batch-51 = 0.036465100944042206

Training epoch-1 batch-52
Running loss of epoch-1 batch-52 = 0.03629834204912186

Training epoch-1 batch-53
Running loss of epoch-1 batch-53 = 0.035959914326667786

Training epoch-1 batch-54
Running loss of epoch-1 batch-54 = 0.03573571890592575

Training epoch-1 batch-55
Running loss of epoch-1 batch-55 = 0.03615499660372734

Training epoch-1 batch-56
Running loss of epoch-1 batch-56 = 0.03614922985434532

Training epoch-1 batch-57
Running loss of epoch-1 batch-57 = 0.036045633256435394

Training epoch-1 batch-58
Running loss of epoch-1 batch-58 = 0.03590938076376915

Training epoch-1 batch-59
Running loss of epoch-1 batch-59 = 0.035950127989053726

Training epoch-1 batch-60
Running loss of epoch-1 batch-60 = 0.03597499802708626

Training epoch-1 batch-61
Running loss of epoch-1 batch-61 = 0.036027949303388596

Training epoch-1 batch-62
Running loss of epoch-1 batch-62 = 0.035837892442941666

Training epoch-1 batch-63
Running loss of epoch-1 batch-63 = 0.03605826944112778

Training epoch-1 batch-64
Running loss of epoch-1 batch-64 = 0.03596983850002289

Training epoch-1 batch-65
Running loss of epoch-1 batch-65 = 0.036179523915052414

Training epoch-1 batch-66
Running loss of epoch-1 batch-66 = 0.03595780208706856

Training epoch-1 batch-67
Running loss of epoch-1 batch-67 = 0.03605910390615463

Training epoch-1 batch-68
Running loss of epoch-1 batch-68 = 0.03587586432695389

Training epoch-1 batch-69
Running loss of epoch-1 batch-69 = 0.03596663102507591

Training epoch-1 batch-70
Running loss of epoch-1 batch-70 = 0.03597095236182213

Training epoch-1 batch-71
Running loss of epoch-1 batch-71 = 0.03610752895474434

Training epoch-1 batch-72
Running loss of epoch-1 batch-72 = 0.036152198910713196

Training epoch-1 batch-73
Running loss of epoch-1 batch-73 = 0.03606656938791275

Training epoch-1 batch-74
Running loss of epoch-1 batch-74 = 0.03599034622311592

Training epoch-1 batch-75
Running loss of epoch-1 batch-75 = 0.036005035042762756

Training epoch-1 batch-76
Running loss of epoch-1 batch-76 = 0.03586837649345398

Training epoch-1 batch-77
Running loss of epoch-1 batch-77 = 0.0359886959195137

Training epoch-1 batch-78
Running loss of epoch-1 batch-78 = 0.03590158745646477

Training epoch-1 batch-79
Running loss of epoch-1 batch-79 = 0.035838596522808075

Training epoch-1 batch-80
Running loss of epoch-1 batch-80 = 0.03624042868614197

Training epoch-1 batch-81
Running loss of epoch-1 batch-81 = 0.036148522049188614

Training epoch-1 batch-82
Running loss of epoch-1 batch-82 = 0.03605112433433533

Training epoch-1 batch-83
Running loss of epoch-1 batch-83 = 0.03585246950387955

Training epoch-1 batch-84
Running loss of epoch-1 batch-84 = 0.035773374140262604

Training epoch-1 batch-85
Running loss of epoch-1 batch-85 = 0.03585056588053703

Training epoch-1 batch-86
Running loss of epoch-1 batch-86 = 0.035938967019319534

Training epoch-1 batch-87
Running loss of epoch-1 batch-87 = 0.03621271997690201

Training epoch-1 batch-88
Running loss of epoch-1 batch-88 = 0.03587197884917259

Training epoch-1 batch-89
Running loss of epoch-1 batch-89 = 0.035956140607595444

Training epoch-1 batch-90
Running loss of epoch-1 batch-90 = 0.0360242985188961

Training epoch-1 batch-91
Running loss of epoch-1 batch-91 = 0.035997774451971054

Training epoch-1 batch-92
Running loss of epoch-1 batch-92 = 0.03590404614806175

Training epoch-1 batch-93
Running loss of epoch-1 batch-93 = 0.03611978888511658

Training epoch-1 batch-94
Running loss of epoch-1 batch-94 = 0.03583987429738045

Training epoch-1 batch-95
Running loss of epoch-1 batch-95 = 0.03606448695063591

Training epoch-1 batch-96
Running loss of epoch-1 batch-96 = 0.03586180508136749

Training epoch-1 batch-97
Running loss of epoch-1 batch-97 = 0.03573760762810707

Training epoch-1 batch-98
Running loss of epoch-1 batch-98 = 0.03625793009996414

Training epoch-1 batch-99
Running loss of epoch-1 batch-99 = 0.03600334748625755

Training epoch-1 batch-100
Running loss of epoch-1 batch-100 = 0.03577692061662674

Training epoch-1 batch-101
Running loss of epoch-1 batch-101 = 0.036063581705093384

Training epoch-1 batch-102
Running loss of epoch-1 batch-102 = 0.0359693206846714

Training epoch-1 batch-103
Running loss of epoch-1 batch-103 = 0.036060530692338943

Training epoch-1 batch-104
Running loss of epoch-1 batch-104 = 0.035903558135032654

Training epoch-1 batch-105
Running loss of epoch-1 batch-105 = 0.035953089594841

Training epoch-1 batch-106
Running loss of epoch-1 batch-106 = 0.03598316013813019

Training epoch-1 batch-107
Running loss of epoch-1 batch-107 = 0.036035194993019104

Training epoch-1 batch-108
Running loss of epoch-1 batch-108 = 0.035721149295568466

Training epoch-1 batch-109
Running loss of epoch-1 batch-109 = 0.03575365990400314

Training epoch-1 batch-110
Running loss of epoch-1 batch-110 = 0.03579418361186981

Training epoch-1 batch-111
Running loss of epoch-1 batch-111 = 0.03611445799469948

Training epoch-1 batch-112
Running loss of epoch-1 batch-112 = 0.03607369586825371

Training epoch-1 batch-113
Running loss of epoch-1 batch-113 = 0.03611338511109352

Training epoch-1 batch-114
Running loss of epoch-1 batch-114 = 0.03593815118074417

Training epoch-1 batch-115
Running loss of epoch-1 batch-115 = 0.03569848835468292

Training epoch-1 batch-116
Running loss of epoch-1 batch-116 = 0.035634785890579224

Training epoch-1 batch-117
Running loss of epoch-1 batch-117 = 0.03600957244634628

Training epoch-1 batch-118
Running loss of epoch-1 batch-118 = 0.03617652505636215

Training epoch-1 batch-119
Running loss of epoch-1 batch-119 = 0.035947736352682114

Training epoch-1 batch-120
Running loss of epoch-1 batch-120 = 0.03605785220861435

Training epoch-1 batch-121
Running loss of epoch-1 batch-121 = 0.03608549013733864

Training epoch-1 batch-122
Running loss of epoch-1 batch-122 = 0.035719189792871475

Training epoch-1 batch-123
Running loss of epoch-1 batch-123 = 0.03609250858426094

Training epoch-1 batch-124
Running loss of epoch-1 batch-124 = 0.03600654751062393

Training epoch-1 batch-125
Running loss of epoch-1 batch-125 = 0.03577683866024017

Training epoch-1 batch-126
Running loss of epoch-1 batch-126 = 0.035960759967565536

Training epoch-1 batch-127
Running loss of epoch-1 batch-127 = 0.036081500351428986

Training epoch-1 batch-128
Running loss of epoch-1 batch-128 = 0.03594184294342995

Training epoch-1 batch-129
Running loss of epoch-1 batch-129 = 0.03586142510175705

Training epoch-1 batch-130
Running loss of epoch-1 batch-130 = 0.03599996119737625

Training epoch-1 batch-131
Running loss of epoch-1 batch-131 = 0.036070216447114944

Training epoch-1 batch-132
Running loss of epoch-1 batch-132 = 0.03597978129982948

Training epoch-1 batch-133
Running loss of epoch-1 batch-133 = 0.0359962023794651

Training epoch-1 batch-134
Running loss of epoch-1 batch-134 = 0.03585071861743927

Training epoch-1 batch-135
Running loss of epoch-1 batch-135 = 0.03578554838895798

Training epoch-1 batch-136
Running loss of epoch-1 batch-136 = 0.03587251156568527

Training epoch-1 batch-137
Running loss of epoch-1 batch-137 = 0.035971466451883316

Training epoch-1 batch-138
Running loss of epoch-1 batch-138 = 0.03592458367347717

Training epoch-1 batch-139
Running loss of epoch-1 batch-139 = 0.03600619360804558

Training epoch-1 batch-140
Running loss of epoch-1 batch-140 = 0.03591146320104599

Training epoch-1 batch-141
Running loss of epoch-1 batch-141 = 0.03609003499150276

Training epoch-1 batch-142
Running loss of epoch-1 batch-142 = 0.03597700595855713

Training epoch-1 batch-143
Running loss of epoch-1 batch-143 = 0.03602074831724167

Training epoch-1 batch-144
Running loss of epoch-1 batch-144 = 0.035911377519369125

Training epoch-1 batch-145
Running loss of epoch-1 batch-145 = 0.0360412523150444

Training epoch-1 batch-146
Running loss of epoch-1 batch-146 = 0.03600173816084862

Training epoch-1 batch-147
Running loss of epoch-1 batch-147 = 0.036243144422769547

Training epoch-1 batch-148
Running loss of epoch-1 batch-148 = 0.03596579656004906

Training epoch-1 batch-149
Running loss of epoch-1 batch-149 = 0.036046456545591354

Training epoch-1 batch-150
Running loss of epoch-1 batch-150 = 0.03591300919651985

Training epoch-1 batch-151
Running loss of epoch-1 batch-151 = 0.03601784259080887

Training epoch-1 batch-152
Running loss of epoch-1 batch-152 = 0.035968929529190063

Training epoch-1 batch-153
Running loss of epoch-1 batch-153 = 0.036026809364557266

Training epoch-1 batch-154
Running loss of epoch-1 batch-154 = 0.03586824983358383

Training epoch-1 batch-155
Running loss of epoch-1 batch-155 = 0.03592973202466965

Training epoch-1 batch-156
Running loss of epoch-1 batch-156 = 0.03595982864499092

Training epoch-1 batch-157
Running loss of epoch-1 batch-157 = 0.14421576261520386

Finished training epoch-1.



Average train loss at epoch-1 = 0.036185811853408814

Started Evaluation

Average val loss at epoch-1 = 2.297711607656981

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 0.16 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 8.21 %
Accuracy for class toString is: 0.00 %
Accuracy for class run is: 98.17 %
Accuracy for class hashCode is: 0.00 %
Accuracy for class init is: 6.50 %
Accuracy for class execute is: 0.00 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 11.08 %


Best Accuracy = 11.08 % at Epoch-1
Saving model after best epoch-1

Finished Evaluation



Started training epoch-2


Training epoch-2 batch-1
Running loss of epoch-2 batch-1 = 0.035906944423913956

Training epoch-2 batch-2
Running loss of epoch-2 batch-2 = 0.03584463149309158

Training epoch-2 batch-3
Running loss of epoch-2 batch-3 = 0.035919491201639175

Training epoch-2 batch-4
Running loss of epoch-2 batch-4 = 0.03593477979302406

Training epoch-2 batch-5
Running loss of epoch-2 batch-5 = 0.03589867427945137

Training epoch-2 batch-6
Running loss of epoch-2 batch-6 = 0.03596041724085808

Training epoch-2 batch-7
Running loss of epoch-2 batch-7 = 0.03590982034802437

Training epoch-2 batch-8
Running loss of epoch-2 batch-8 = 0.03593907132744789

Training epoch-2 batch-9
Running loss of epoch-2 batch-9 = 0.035880543291568756

Training epoch-2 batch-10
Running loss of epoch-2 batch-10 = 0.03588202968239784

Training epoch-2 batch-11
Running loss of epoch-2 batch-11 = 0.035844434052705765

Training epoch-2 batch-12
Running loss of epoch-2 batch-12 = 0.036053020507097244

Training epoch-2 batch-13
Running loss of epoch-2 batch-13 = 0.03595024347305298

Training epoch-2 batch-14
Running loss of epoch-2 batch-14 = 0.03595101088285446

Training epoch-2 batch-15
Running loss of epoch-2 batch-15 = 0.03571314364671707

Training epoch-2 batch-16
Running loss of epoch-2 batch-16 = 0.035947609692811966

Training epoch-2 batch-17
Running loss of epoch-2 batch-17 = 0.035854291170835495

Training epoch-2 batch-18
Running loss of epoch-2 batch-18 = 0.035821348428726196

Training epoch-2 batch-19
Running loss of epoch-2 batch-19 = 0.03580862283706665

Training epoch-2 batch-20
Running loss of epoch-2 batch-20 = 0.035798512399196625

Training epoch-2 batch-21
Running loss of epoch-2 batch-21 = 0.035853609442710876

Training epoch-2 batch-22
Running loss of epoch-2 batch-22 = 0.03581751510500908

Training epoch-2 batch-23
Running loss of epoch-2 batch-23 = 0.035787686705589294

Training epoch-2 batch-24
Running loss of epoch-2 batch-24 = 0.0357830636203289

Training epoch-2 batch-25
Running loss of epoch-2 batch-25 = 0.036049749702215195

Training epoch-2 batch-26
Running loss of epoch-2 batch-26 = 0.035785648971796036

Training epoch-2 batch-27
Running loss of epoch-2 batch-27 = 0.035718467086553574

Training epoch-2 batch-28
Running loss of epoch-2 batch-28 = 0.036206357181072235

Training epoch-2 batch-29
Running loss of epoch-2 batch-29 = 0.03591122850775719

Training epoch-2 batch-30
Running loss of epoch-2 batch-30 = 0.035611897706985474

Training epoch-2 batch-31
Running loss of epoch-2 batch-31 = 0.03568486496806145

Training epoch-2 batch-32
Running loss of epoch-2 batch-32 = 0.0358334556221962

Training epoch-2 batch-33
Running loss of epoch-2 batch-33 = 0.036309078335762024

Training epoch-2 batch-34
Running loss of epoch-2 batch-34 = 0.03606783598661423

Training epoch-2 batch-35
Running loss of epoch-2 batch-35 = 0.03577001765370369

Training epoch-2 batch-36
Running loss of epoch-2 batch-36 = 0.03605279326438904

Training epoch-2 batch-37
Running loss of epoch-2 batch-37 = 0.035882581025362015

Training epoch-2 batch-38
Running loss of epoch-2 batch-38 = 0.03600921481847763

Training epoch-2 batch-39
Running loss of epoch-2 batch-39 = 0.035891514271497726

Training epoch-2 batch-40
Running loss of epoch-2 batch-40 = 0.035986535251140594

Training epoch-2 batch-41
Running loss of epoch-2 batch-41 = 0.035781849175691605

Training epoch-2 batch-42
Running loss of epoch-2 batch-42 = 0.035837411880493164

Training epoch-2 batch-43
Running loss of epoch-2 batch-43 = 0.03577903285622597

Training epoch-2 batch-44
Running loss of epoch-2 batch-44 = 0.0360284224152565

Training epoch-2 batch-45
Running loss of epoch-2 batch-45 = 0.03582889586687088

Training epoch-2 batch-46
Running loss of epoch-2 batch-46 = 0.03582955151796341

Training epoch-2 batch-47
Running loss of epoch-2 batch-47 = 0.03574422001838684

Training epoch-2 batch-48
Running loss of epoch-2 batch-48 = 0.03565996140241623

Training epoch-2 batch-49
Running loss of epoch-2 batch-49 = 0.03601052984595299

Training epoch-2 batch-50
Running loss of epoch-2 batch-50 = 0.035718850791454315

Training epoch-2 batch-51
Running loss of epoch-2 batch-51 = 0.03600321710109711

Training epoch-2 batch-52
Running loss of epoch-2 batch-52 = 0.03583623096346855

Training epoch-2 batch-53
Running loss of epoch-2 batch-53 = 0.03598718345165253

Training epoch-2 batch-54
Running loss of epoch-2 batch-54 = 0.03615313023328781

Training epoch-2 batch-55
Running loss of epoch-2 batch-55 = 0.03609391674399376

Training epoch-2 batch-56
Running loss of epoch-2 batch-56 = 0.035996779799461365

Training epoch-2 batch-57
Running loss of epoch-2 batch-57 = 0.03588569909334183

Training epoch-2 batch-58
Running loss of epoch-2 batch-58 = 0.03621784225106239

Training epoch-2 batch-59
Running loss of epoch-2 batch-59 = 0.03602777421474457

Training epoch-2 batch-60
Running loss of epoch-2 batch-60 = 0.03580912575125694

Training epoch-2 batch-61
Running loss of epoch-2 batch-61 = 0.035840488970279694

Training epoch-2 batch-62
Running loss of epoch-2 batch-62 = 0.03574798256158829

Training epoch-2 batch-63
Running loss of epoch-2 batch-63 = 0.03579067438840866

Training epoch-2 batch-64
Running loss of epoch-2 batch-64 = 0.035891588777303696

Training epoch-2 batch-65
Running loss of epoch-2 batch-65 = 0.035715438425540924

Training epoch-2 batch-66
Running loss of epoch-2 batch-66 = 0.03586822748184204

Training epoch-2 batch-67
Running loss of epoch-2 batch-67 = 0.035783667117357254

Training epoch-2 batch-68
Running loss of epoch-2 batch-68 = 0.03592589870095253

Training epoch-2 batch-69
Running loss of epoch-2 batch-69 = 0.03596130758523941

Training epoch-2 batch-70
Running loss of epoch-2 batch-70 = 0.03599657490849495

Training epoch-2 batch-71
Running loss of epoch-2 batch-71 = 0.036051973700523376

Training epoch-2 batch-72
Running loss of epoch-2 batch-72 = 0.035798221826553345

Training epoch-2 batch-73
Running loss of epoch-2 batch-73 = 0.035737086087465286

Training epoch-2 batch-74
Running loss of epoch-2 batch-74 = 0.03568753972649574

Training epoch-2 batch-75
Running loss of epoch-2 batch-75 = 0.03561864048242569

Training epoch-2 batch-76
Running loss of epoch-2 batch-76 = 0.036138441413640976

Training epoch-2 batch-77
Running loss of epoch-2 batch-77 = 0.035756323486566544

Training epoch-2 batch-78
Running loss of epoch-2 batch-78 = 0.03576027601957321

Training epoch-2 batch-79
Running loss of epoch-2 batch-79 = 0.035788193345069885

Training epoch-2 batch-80
Running loss of epoch-2 batch-80 = 0.03581621125340462

Training epoch-2 batch-81
Running loss of epoch-2 batch-81 = 0.03580927848815918

Training epoch-2 batch-82
Running loss of epoch-2 batch-82 = 0.03579047694802284

Training epoch-2 batch-83
Running loss of epoch-2 batch-83 = 0.035688381642103195

Training epoch-2 batch-84
Running loss of epoch-2 batch-84 = 0.035953350365161896

Training epoch-2 batch-85
Running loss of epoch-2 batch-85 = 0.03595846891403198

Training epoch-2 batch-86
Running loss of epoch-2 batch-86 = 0.03607533872127533

Training epoch-2 batch-87
Running loss of epoch-2 batch-87 = 0.03593088313937187

Training epoch-2 batch-88
Running loss of epoch-2 batch-88 = 0.03588772937655449

Training epoch-2 batch-89
Running loss of epoch-2 batch-89 = 0.03597810119390488

Training epoch-2 batch-90
Running loss of epoch-2 batch-90 = 0.03584342077374458

Training epoch-2 batch-91
Running loss of epoch-2 batch-91 = 0.035618219524621964

Training epoch-2 batch-92
Running loss of epoch-2 batch-92 = 0.035857655107975006

Training epoch-2 batch-93
Running loss of epoch-2 batch-93 = 0.035757455974817276

Training epoch-2 batch-94
Running loss of epoch-2 batch-94 = 0.035961784422397614

Training epoch-2 batch-95
Running loss of epoch-2 batch-95 = 0.03581179305911064

Training epoch-2 batch-96
Running loss of epoch-2 batch-96 = 0.035887569189071655

Training epoch-2 batch-97
Running loss of epoch-2 batch-97 = 0.03581174090504646

Training epoch-2 batch-98
Running loss of epoch-2 batch-98 = 0.03574664518237114

Training epoch-2 batch-99
Running loss of epoch-2 batch-99 = 0.035773809999227524

Training epoch-2 batch-100
Running loss of epoch-2 batch-100 = 0.0356888622045517

Training epoch-2 batch-101
Running loss of epoch-2 batch-101 = 0.03574394807219505

Training epoch-2 batch-102
Running loss of epoch-2 batch-102 = 0.035861093550920486

Training epoch-2 batch-103
Running loss of epoch-2 batch-103 = 0.03609969839453697

Training epoch-2 batch-104
Running loss of epoch-2 batch-104 = 0.03579768165946007

Training epoch-2 batch-105
Running loss of epoch-2 batch-105 = 0.03574386611580849

Training epoch-2 batch-106
Running loss of epoch-2 batch-106 = 0.0356961227953434

Training epoch-2 batch-107
Running loss of epoch-2 batch-107 = 0.035734012722969055

Training epoch-2 batch-108
Running loss of epoch-2 batch-108 = 0.03580212965607643

Training epoch-2 batch-109
Running loss of epoch-2 batch-109 = 0.03575654327869415

Training epoch-2 batch-110
Running loss of epoch-2 batch-110 = 0.03586545214056969

Training epoch-2 batch-111
Running loss of epoch-2 batch-111 = 0.03589722514152527

Training epoch-2 batch-112
Running loss of epoch-2 batch-112 = 0.035691265016794205

Training epoch-2 batch-113
Running loss of epoch-2 batch-113 = 0.0358261875808239

Training epoch-2 batch-114
Running loss of epoch-2 batch-114 = 0.03571248799562454

Training epoch-2 batch-115
Running loss of epoch-2 batch-115 = 0.03566538915038109

Training epoch-2 batch-116
Running loss of epoch-2 batch-116 = 0.03559836745262146

Training epoch-2 batch-117
Running loss of epoch-2 batch-117 = 0.03563892841339111

Training epoch-2 batch-118
Running loss of epoch-2 batch-118 = 0.03569076210260391

Training epoch-2 batch-119
Running loss of epoch-2 batch-119 = 0.03556748479604721

Training epoch-2 batch-120
Running loss of epoch-2 batch-120 = 0.03601894527673721

Training epoch-2 batch-121
Running loss of epoch-2 batch-121 = 0.03576868399977684

Training epoch-2 batch-122
Running loss of epoch-2 batch-122 = 0.03580336645245552

Training epoch-2 batch-123
Running loss of epoch-2 batch-123 = 0.0354914516210556

Training epoch-2 batch-124
Running loss of epoch-2 batch-124 = 0.03559692203998566

Training epoch-2 batch-125
Running loss of epoch-2 batch-125 = 0.03579968959093094

Training epoch-2 batch-126
Running loss of epoch-2 batch-126 = 0.036022432148456573

Training epoch-2 batch-127
Running loss of epoch-2 batch-127 = 0.035848554223775864

Training epoch-2 batch-128
Running loss of epoch-2 batch-128 = 0.03572619706392288

Training epoch-2 batch-129
Running loss of epoch-2 batch-129 = 0.03567270562052727

Training epoch-2 batch-130
Running loss of epoch-2 batch-130 = 0.03551289439201355

Training epoch-2 batch-131
Running loss of epoch-2 batch-131 = 0.03563045337796211

Training epoch-2 batch-132
Running loss of epoch-2 batch-132 = 0.03562580794095993

Training epoch-2 batch-133
Running loss of epoch-2 batch-133 = 0.036013927310705185

Training epoch-2 batch-134
Running loss of epoch-2 batch-134 = 0.03595304116606712

Training epoch-2 batch-135
Running loss of epoch-2 batch-135 = 0.035768359899520874

Training epoch-2 batch-136
Running loss of epoch-2 batch-136 = 0.03580430895090103

Training epoch-2 batch-137
Running loss of epoch-2 batch-137 = 0.035501718521118164

Training epoch-2 batch-138
Running loss of epoch-2 batch-138 = 0.03545817732810974

Training epoch-2 batch-139
Running loss of epoch-2 batch-139 = 0.03591897338628769

Training epoch-2 batch-140
Running loss of epoch-2 batch-140 = 0.035697091370821

Training epoch-2 batch-141
Running loss of epoch-2 batch-141 = 0.03575494885444641

Training epoch-2 batch-142
Running loss of epoch-2 batch-142 = 0.03579222038388252

Training epoch-2 batch-143
Running loss of epoch-2 batch-143 = 0.03576144948601723

Training epoch-2 batch-144
Running loss of epoch-2 batch-144 = 0.03586375713348389

Training epoch-2 batch-145
Running loss of epoch-2 batch-145 = 0.03563026338815689

Training epoch-2 batch-146
Running loss of epoch-2 batch-146 = 0.035988032817840576

Training epoch-2 batch-147
Running loss of epoch-2 batch-147 = 0.035660214722156525

Training epoch-2 batch-148
Running loss of epoch-2 batch-148 = 0.03586285561323166

Training epoch-2 batch-149
Running loss of epoch-2 batch-149 = 0.03563854098320007

Training epoch-2 batch-150
Running loss of epoch-2 batch-150 = 0.0356486476957798

Training epoch-2 batch-151
Running loss of epoch-2 batch-151 = 0.035849638283252716

Training epoch-2 batch-152
Running loss of epoch-2 batch-152 = 0.03570615500211716

Training epoch-2 batch-153
Running loss of epoch-2 batch-153 = 0.03579331189393997

Training epoch-2 batch-154
Running loss of epoch-2 batch-154 = 0.0358048640191555

Training epoch-2 batch-155
Running loss of epoch-2 batch-155 = 0.035609401762485504

Training epoch-2 batch-156
Running loss of epoch-2 batch-156 = 0.035501569509506226

Training epoch-2 batch-157
Running loss of epoch-2 batch-157 = 0.14309564232826233

Finished training epoch-2.



Average train loss at epoch-2 = 0.03600146708488464

Started Evaluation

Average val loss at epoch-2 = 2.291791410822617

Accuracy for classes:
Accuracy for class equals is: 15.84 %
Accuracy for class main is: 0.98 %
Accuracy for class setUp is: 2.95 %
Accuracy for class onCreate is: 14.39 %
Accuracy for class toString is: 2.05 %
Accuracy for class run is: 69.86 %
Accuracy for class hashCode is: 0.00 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 4.82 %
Accuracy for class get is: 18.21 %

Overall Accuracy = 13.41 %


Best Accuracy = 13.41 % at Epoch-2
Saving model after best epoch-2

Finished Evaluation



Started training epoch-3


Training epoch-3 batch-1
Running loss of epoch-3 batch-1 = 0.03571578115224838

Training epoch-3 batch-2
Running loss of epoch-3 batch-2 = 0.0357438288629055

Training epoch-3 batch-3
Running loss of epoch-3 batch-3 = 0.035751696676015854

Training epoch-3 batch-4
Running loss of epoch-3 batch-4 = 0.035619888454675674

Training epoch-3 batch-5
Running loss of epoch-3 batch-5 = 0.03550522401928902

Training epoch-3 batch-6
Running loss of epoch-3 batch-6 = 0.035491522401571274

Training epoch-3 batch-7
Running loss of epoch-3 batch-7 = 0.035780128091573715

Training epoch-3 batch-8
Running loss of epoch-3 batch-8 = 0.03563191741704941

Training epoch-3 batch-9
Running loss of epoch-3 batch-9 = 0.03602048009634018

Training epoch-3 batch-10
Running loss of epoch-3 batch-10 = 0.03553460165858269

Training epoch-3 batch-11
Running loss of epoch-3 batch-11 = 0.03542424738407135

Training epoch-3 batch-12
Running loss of epoch-3 batch-12 = 0.035712625831365585

Training epoch-3 batch-13
Running loss of epoch-3 batch-13 = 0.035821426659822464

Training epoch-3 batch-14
Running loss of epoch-3 batch-14 = 0.03599493205547333

Training epoch-3 batch-15
Running loss of epoch-3 batch-15 = 0.03544728457927704

Training epoch-3 batch-16
Running loss of epoch-3 batch-16 = 0.03573209419846535

Training epoch-3 batch-17
Running loss of epoch-3 batch-17 = 0.03552565723657608

Training epoch-3 batch-18
Running loss of epoch-3 batch-18 = 0.035809095948934555

Training epoch-3 batch-19
Running loss of epoch-3 batch-19 = 0.03575366735458374

Training epoch-3 batch-20
Running loss of epoch-3 batch-20 = 0.03547661006450653

Training epoch-3 batch-21
Running loss of epoch-3 batch-21 = 0.035540513694286346

Training epoch-3 batch-22
Running loss of epoch-3 batch-22 = 0.03564488887786865

Training epoch-3 batch-23
Running loss of epoch-3 batch-23 = 0.03556640073657036

Training epoch-3 batch-24
Running loss of epoch-3 batch-24 = 0.03570900857448578

Training epoch-3 batch-25
Running loss of epoch-3 batch-25 = 0.03566912189126015

Training epoch-3 batch-26
Running loss of epoch-3 batch-26 = 0.035568609833717346

Training epoch-3 batch-27
Running loss of epoch-3 batch-27 = 0.03547154366970062

Training epoch-3 batch-28
Running loss of epoch-3 batch-28 = 0.03573006018996239

Training epoch-3 batch-29
Running loss of epoch-3 batch-29 = 0.03529581055045128

Training epoch-3 batch-30
Running loss of epoch-3 batch-30 = 0.03564309701323509

Training epoch-3 batch-31
Running loss of epoch-3 batch-31 = 0.03551961109042168

Training epoch-3 batch-32
Running loss of epoch-3 batch-32 = 0.03557298332452774

Training epoch-3 batch-33
Running loss of epoch-3 batch-33 = 0.035471267998218536

Training epoch-3 batch-34
Running loss of epoch-3 batch-34 = 0.035897206515073776

Training epoch-3 batch-35
Running loss of epoch-3 batch-35 = 0.03574620932340622

Training epoch-3 batch-36
Running loss of epoch-3 batch-36 = 0.03518935292959213

Training epoch-3 batch-37
Running loss of epoch-3 batch-37 = 0.03532746806740761

Training epoch-3 batch-38
Running loss of epoch-3 batch-38 = 0.0355677530169487

Training epoch-3 batch-39
Running loss of epoch-3 batch-39 = 0.03533564880490303

Training epoch-3 batch-40
Running loss of epoch-3 batch-40 = 0.03529949113726616

Training epoch-3 batch-41
Running loss of epoch-3 batch-41 = 0.03548779338598251

Training epoch-3 batch-42
Running loss of epoch-3 batch-42 = 0.03588881343603134

Training epoch-3 batch-43
Running loss of epoch-3 batch-43 = 0.03518019989132881

Training epoch-3 batch-44
Running loss of epoch-3 batch-44 = 0.035719409584999084

Training epoch-3 batch-45
Running loss of epoch-3 batch-45 = 0.035602129995822906

Training epoch-3 batch-46
Running loss of epoch-3 batch-46 = 0.035378195345401764

Training epoch-3 batch-47
Running loss of epoch-3 batch-47 = 0.035197027027606964

Training epoch-3 batch-48
Running loss of epoch-3 batch-48 = 0.035113364458084106

Training epoch-3 batch-49
Running loss of epoch-3 batch-49 = 0.03548693284392357

Training epoch-3 batch-50
Running loss of epoch-3 batch-50 = 0.03530077636241913

Training epoch-3 batch-51
Running loss of epoch-3 batch-51 = 0.03521851450204849

Training epoch-3 batch-52
Running loss of epoch-3 batch-52 = 0.035526372492313385

Training epoch-3 batch-53
Running loss of epoch-3 batch-53 = 0.03521277382969856

Training epoch-3 batch-54
Running loss of epoch-3 batch-54 = 0.03558047115802765

Training epoch-3 batch-55
Running loss of epoch-3 batch-55 = 0.03517841547727585

Training epoch-3 batch-56
Running loss of epoch-3 batch-56 = 0.03555285185575485

Training epoch-3 batch-57
Running loss of epoch-3 batch-57 = 0.03538088873028755

Training epoch-3 batch-58
Running loss of epoch-3 batch-58 = 0.035474587231874466

Training epoch-3 batch-59
Running loss of epoch-3 batch-59 = 0.03552556410431862

Training epoch-3 batch-60
Running loss of epoch-3 batch-60 = 0.03535978123545647

Training epoch-3 batch-61
Running loss of epoch-3 batch-61 = 0.03565285727381706

Training epoch-3 batch-62
Running loss of epoch-3 batch-62 = 0.03567270562052727

Training epoch-3 batch-63
Running loss of epoch-3 batch-63 = 0.03558247536420822

Training epoch-3 batch-64
Running loss of epoch-3 batch-64 = 0.03515876457095146

Training epoch-3 batch-65
Running loss of epoch-3 batch-65 = 0.035496264696121216

Training epoch-3 batch-66
Running loss of epoch-3 batch-66 = 0.03567657247185707

Training epoch-3 batch-67
Running loss of epoch-3 batch-67 = 0.03533667325973511

Training epoch-3 batch-68
Running loss of epoch-3 batch-68 = 0.035418279469013214

Training epoch-3 batch-69
Running loss of epoch-3 batch-69 = 0.03531236574053764

Training epoch-3 batch-70
Running loss of epoch-3 batch-70 = 0.03537387028336525

Training epoch-3 batch-71
Running loss of epoch-3 batch-71 = 0.03579811751842499

Training epoch-3 batch-72
Running loss of epoch-3 batch-72 = 0.03563697636127472

Training epoch-3 batch-73
Running loss of epoch-3 batch-73 = 0.03552678972482681

Training epoch-3 batch-74
Running loss of epoch-3 batch-74 = 0.03526410833001137

Training epoch-3 batch-75
Running loss of epoch-3 batch-75 = 0.035388410091400146

Training epoch-3 batch-76
Running loss of epoch-3 batch-76 = 0.03578729182481766

Training epoch-3 batch-77
Running loss of epoch-3 batch-77 = 0.035319652408361435

Training epoch-3 batch-78
Running loss of epoch-3 batch-78 = 0.03527703508734703

Training epoch-3 batch-79
Running loss of epoch-3 batch-79 = 0.03487231209874153

Training epoch-3 batch-80
Running loss of epoch-3 batch-80 = 0.03534994274377823

Training epoch-3 batch-81
Running loss of epoch-3 batch-81 = 0.03518608212471008

Training epoch-3 batch-82
Running loss of epoch-3 batch-82 = 0.035349197685718536

Training epoch-3 batch-83
Running loss of epoch-3 batch-83 = 0.03543860465288162

Training epoch-3 batch-84
Running loss of epoch-3 batch-84 = 0.03551766276359558

Training epoch-3 batch-85
Running loss of epoch-3 batch-85 = 0.03637334704399109

Training epoch-3 batch-86
Running loss of epoch-3 batch-86 = 0.03503452613949776

Training epoch-3 batch-87
Running loss of epoch-3 batch-87 = 0.034881338477134705

Training epoch-3 batch-88
Running loss of epoch-3 batch-88 = 0.035422183573246

Training epoch-3 batch-89
Running loss of epoch-3 batch-89 = 0.03484470024704933

Training epoch-3 batch-90
Running loss of epoch-3 batch-90 = 0.035053446888923645

Training epoch-3 batch-91
Running loss of epoch-3 batch-91 = 0.03517758473753929

Training epoch-3 batch-92
Running loss of epoch-3 batch-92 = 0.035635434091091156

Training epoch-3 batch-93
Running loss of epoch-3 batch-93 = 0.035404857248067856

Training epoch-3 batch-94
Running loss of epoch-3 batch-94 = 0.035236794501543045

Training epoch-3 batch-95
Running loss of epoch-3 batch-95 = 0.035567499697208405

Training epoch-3 batch-96
Running loss of epoch-3 batch-96 = 0.03527385741472244

Training epoch-3 batch-97
Running loss of epoch-3 batch-97 = 0.03536118566989899

Training epoch-3 batch-98
Running loss of epoch-3 batch-98 = 0.03517007455229759

Training epoch-3 batch-99
Running loss of epoch-3 batch-99 = 0.03551925718784332

Training epoch-3 batch-100
Running loss of epoch-3 batch-100 = 0.035416752099990845

Training epoch-3 batch-101
Running loss of epoch-3 batch-101 = 0.03487318009138107

Training epoch-3 batch-102
Running loss of epoch-3 batch-102 = 0.03505806624889374

Training epoch-3 batch-103
Running loss of epoch-3 batch-103 = 0.03533425182104111

Training epoch-3 batch-104
Running loss of epoch-3 batch-104 = 0.03545812517404556

Training epoch-3 batch-105
Running loss of epoch-3 batch-105 = 0.03539307415485382

Training epoch-3 batch-106
Running loss of epoch-3 batch-106 = 0.035227373242378235

Training epoch-3 batch-107
Running loss of epoch-3 batch-107 = 0.03494510054588318

Training epoch-3 batch-108
Running loss of epoch-3 batch-108 = 0.03497720882296562

Training epoch-3 batch-109
Running loss of epoch-3 batch-109 = 0.03473464772105217

Training epoch-3 batch-110
Running loss of epoch-3 batch-110 = 0.03483596444129944

Training epoch-3 batch-111
Running loss of epoch-3 batch-111 = 0.035789854824543

Training epoch-3 batch-112
Running loss of epoch-3 batch-112 = 0.03494950383901596

Training epoch-3 batch-113
Running loss of epoch-3 batch-113 = 0.034987155348062515

Training epoch-3 batch-114
Running loss of epoch-3 batch-114 = 0.035426121205091476

Training epoch-3 batch-115
Running loss of epoch-3 batch-115 = 0.03541824221611023

Training epoch-3 batch-116
Running loss of epoch-3 batch-116 = 0.03505082428455353

Training epoch-3 batch-117
Running loss of epoch-3 batch-117 = 0.035376083105802536

Training epoch-3 batch-118
Running loss of epoch-3 batch-118 = 0.034681469202041626

Training epoch-3 batch-119
Running loss of epoch-3 batch-119 = 0.03532141447067261

Training epoch-3 batch-120
Running loss of epoch-3 batch-120 = 0.03502271696925163

Training epoch-3 batch-121
Running loss of epoch-3 batch-121 = 0.03457443788647652

Training epoch-3 batch-122
Running loss of epoch-3 batch-122 = 0.03434993699193001

Training epoch-3 batch-123
Running loss of epoch-3 batch-123 = 0.035172391682863235

Training epoch-3 batch-124
Running loss of epoch-3 batch-124 = 0.034634366631507874

Training epoch-3 batch-125
Running loss of epoch-3 batch-125 = 0.03492949530482292

Training epoch-3 batch-126
Running loss of epoch-3 batch-126 = 0.035458143800497055

Training epoch-3 batch-127
Running loss of epoch-3 batch-127 = 0.034855134785175323

Training epoch-3 batch-128
Running loss of epoch-3 batch-128 = 0.03479364886879921

Training epoch-3 batch-129
Running loss of epoch-3 batch-129 = 0.03520491346716881

Training epoch-3 batch-130
Running loss of epoch-3 batch-130 = 0.03450079634785652

Training epoch-3 batch-131
Running loss of epoch-3 batch-131 = 0.034321628510951996

Training epoch-3 batch-132
Running loss of epoch-3 batch-132 = 0.034670718014240265

Training epoch-3 batch-133
Running loss of epoch-3 batch-133 = 0.0343131348490715

Training epoch-3 batch-134
Running loss of epoch-3 batch-134 = 0.03525422886013985

Training epoch-3 batch-135
Running loss of epoch-3 batch-135 = 0.03503580018877983

Training epoch-3 batch-136
Running loss of epoch-3 batch-136 = 0.03405025601387024

Training epoch-3 batch-137
Running loss of epoch-3 batch-137 = 0.03524347394704819

Training epoch-3 batch-138
Running loss of epoch-3 batch-138 = 0.03549486771225929

Training epoch-3 batch-139
Running loss of epoch-3 batch-139 = 0.03530174121260643

Training epoch-3 batch-140
Running loss of epoch-3 batch-140 = 0.03455423191189766

Training epoch-3 batch-141
Running loss of epoch-3 batch-141 = 0.03488963469862938

Training epoch-3 batch-142
Running loss of epoch-3 batch-142 = 0.034693244844675064

Training epoch-3 batch-143
Running loss of epoch-3 batch-143 = 0.034842632710933685

Training epoch-3 batch-144
Running loss of epoch-3 batch-144 = 0.03481663763523102

Training epoch-3 batch-145
Running loss of epoch-3 batch-145 = 0.035231638699769974

Training epoch-3 batch-146
Running loss of epoch-3 batch-146 = 0.034917112439870834

Training epoch-3 batch-147
Running loss of epoch-3 batch-147 = 0.03397827222943306

Training epoch-3 batch-148
Running loss of epoch-3 batch-148 = 0.03547380864620209

Training epoch-3 batch-149
Running loss of epoch-3 batch-149 = 0.03485017269849777

Training epoch-3 batch-150
Running loss of epoch-3 batch-150 = 0.03473459929227829

Training epoch-3 batch-151
Running loss of epoch-3 batch-151 = 0.034752458333969116

Training epoch-3 batch-152
Running loss of epoch-3 batch-152 = 0.034857288002967834

Training epoch-3 batch-153
Running loss of epoch-3 batch-153 = 0.0339362807571888

Training epoch-3 batch-154
Running loss of epoch-3 batch-154 = 0.03494691848754883

Training epoch-3 batch-155
Running loss of epoch-3 batch-155 = 0.0348375104367733

Training epoch-3 batch-156
Running loss of epoch-3 batch-156 = 0.03485376015305519

Training epoch-3 batch-157
Running loss of epoch-3 batch-157 = 0.14047814905643463

Finished training epoch-3.



Average train loss at epoch-3 = 0.0354540461063385

Started Evaluation

Average val loss at epoch-3 = 2.2424892061635067

Accuracy for classes:
Accuracy for class equals is: 0.66 %
Accuracy for class main is: 6.23 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 34.97 %
Accuracy for class toString is: 0.00 %
Accuracy for class run is: 1.60 %
Accuracy for class hashCode is: 64.42 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 80.72 %
Accuracy for class get is: 6.41 %

Overall Accuracy = 15.99 %


Best Accuracy = 15.99 % at Epoch-3
Saving model after best epoch-3

Finished Evaluation



Started training epoch-4


Training epoch-4 batch-1
Running loss of epoch-4 batch-1 = 0.03443390876054764

Training epoch-4 batch-2
Running loss of epoch-4 batch-2 = 0.03452184796333313

Training epoch-4 batch-3
Running loss of epoch-4 batch-3 = 0.034383729100227356

Training epoch-4 batch-4
Running loss of epoch-4 batch-4 = 0.033828526735305786

Training epoch-4 batch-5
Running loss of epoch-4 batch-5 = 0.03545812889933586

Training epoch-4 batch-6
Running loss of epoch-4 batch-6 = 0.03409543260931969

Training epoch-4 batch-7
Running loss of epoch-4 batch-7 = 0.03439090773463249

Training epoch-4 batch-8
Running loss of epoch-4 batch-8 = 0.0344332754611969

Training epoch-4 batch-9
Running loss of epoch-4 batch-9 = 0.03344632685184479

Training epoch-4 batch-10
Running loss of epoch-4 batch-10 = 0.0343366414308548

Training epoch-4 batch-11
Running loss of epoch-4 batch-11 = 0.03440295159816742

Training epoch-4 batch-12
Running loss of epoch-4 batch-12 = 0.0340571403503418

Training epoch-4 batch-13
Running loss of epoch-4 batch-13 = 0.034419771283864975

Training epoch-4 batch-14
Running loss of epoch-4 batch-14 = 0.03478703647851944

Training epoch-4 batch-15
Running loss of epoch-4 batch-15 = 0.033994611352682114

Training epoch-4 batch-16
Running loss of epoch-4 batch-16 = 0.03483865410089493

Training epoch-4 batch-17
Running loss of epoch-4 batch-17 = 0.03457832708954811

Training epoch-4 batch-18
Running loss of epoch-4 batch-18 = 0.034767162054777145

Training epoch-4 batch-19
Running loss of epoch-4 batch-19 = 0.034351885318756104

Training epoch-4 batch-20
Running loss of epoch-4 batch-20 = 0.0352230966091156

Training epoch-4 batch-21
Running loss of epoch-4 batch-21 = 0.03466027230024338

Training epoch-4 batch-22
Running loss of epoch-4 batch-22 = 0.034353289753198624

Training epoch-4 batch-23
Running loss of epoch-4 batch-23 = 0.03491939604282379

Training epoch-4 batch-24
Running loss of epoch-4 batch-24 = 0.03408064320683479

Training epoch-4 batch-25
Running loss of epoch-4 batch-25 = 0.03481123596429825

Training epoch-4 batch-26
Running loss of epoch-4 batch-26 = 0.03513743728399277

Training epoch-4 batch-27
Running loss of epoch-4 batch-27 = 0.03346117213368416

Training epoch-4 batch-28
Running loss of epoch-4 batch-28 = 0.034533824771642685

Training epoch-4 batch-29
Running loss of epoch-4 batch-29 = 0.03535608947277069

Training epoch-4 batch-30
Running loss of epoch-4 batch-30 = 0.03526190295815468

Training epoch-4 batch-31
Running loss of epoch-4 batch-31 = 0.03418377786874771

Training epoch-4 batch-32
Running loss of epoch-4 batch-32 = 0.03357426077127457

Training epoch-4 batch-33
Running loss of epoch-4 batch-33 = 0.03391580283641815

Training epoch-4 batch-34
Running loss of epoch-4 batch-34 = 0.0344303734600544

Training epoch-4 batch-35
Running loss of epoch-4 batch-35 = 0.034173063933849335

Training epoch-4 batch-36
Running loss of epoch-4 batch-36 = 0.033900126814842224

Training epoch-4 batch-37
Running loss of epoch-4 batch-37 = 0.03379802033305168

Training epoch-4 batch-38
Running loss of epoch-4 batch-38 = 0.03344232961535454

Training epoch-4 batch-39
Running loss of epoch-4 batch-39 = 0.03410758450627327

Training epoch-4 batch-40
Running loss of epoch-4 batch-40 = 0.03414381667971611

Training epoch-4 batch-41
Running loss of epoch-4 batch-41 = 0.03350593149662018

Training epoch-4 batch-42
Running loss of epoch-4 batch-42 = 0.033652957528829575

Training epoch-4 batch-43
Running loss of epoch-4 batch-43 = 0.03430786356329918

Training epoch-4 batch-44
Running loss of epoch-4 batch-44 = 0.033823162317276

Training epoch-4 batch-45
Running loss of epoch-4 batch-45 = 0.034720901399850845

Training epoch-4 batch-46
Running loss of epoch-4 batch-46 = 0.03284696489572525

Training epoch-4 batch-47
Running loss of epoch-4 batch-47 = 0.03410902991890907

Training epoch-4 batch-48
Running loss of epoch-4 batch-48 = 0.03447174280881882

Training epoch-4 batch-49
Running loss of epoch-4 batch-49 = 0.03438616544008255

Training epoch-4 batch-50
Running loss of epoch-4 batch-50 = 0.034225139766931534

Training epoch-4 batch-51
Running loss of epoch-4 batch-51 = 0.03440070152282715

Training epoch-4 batch-52
Running loss of epoch-4 batch-52 = 0.033941369503736496

Training epoch-4 batch-53
Running loss of epoch-4 batch-53 = 0.03345532715320587

Training epoch-4 batch-54
Running loss of epoch-4 batch-54 = 0.03418172523379326

Training epoch-4 batch-55
Running loss of epoch-4 batch-55 = 0.0338623933494091

Training epoch-4 batch-56
Running loss of epoch-4 batch-56 = 0.03414794057607651

Training epoch-4 batch-57
Running loss of epoch-4 batch-57 = 0.032363053411245346

Training epoch-4 batch-58
Running loss of epoch-4 batch-58 = 0.03313868120312691

Training epoch-4 batch-59
Running loss of epoch-4 batch-59 = 0.0332798957824707

Training epoch-4 batch-60
Running loss of epoch-4 batch-60 = 0.033774830400943756

Training epoch-4 batch-61
Running loss of epoch-4 batch-61 = 0.03346911072731018

Training epoch-4 batch-62
Running loss of epoch-4 batch-62 = 0.033589139580726624

Training epoch-4 batch-63
Running loss of epoch-4 batch-63 = 0.03388892859220505

Training epoch-4 batch-64
Running loss of epoch-4 batch-64 = 0.03329788148403168

Training epoch-4 batch-65
Running loss of epoch-4 batch-65 = 0.033219002187252045

Training epoch-4 batch-66
Running loss of epoch-4 batch-66 = 0.033795807510614395

Training epoch-4 batch-67
Running loss of epoch-4 batch-67 = 0.03254702687263489

Training epoch-4 batch-68
Running loss of epoch-4 batch-68 = 0.0323667973279953

Training epoch-4 batch-69
Running loss of epoch-4 batch-69 = 0.03358067572116852

Training epoch-4 batch-70
Running loss of epoch-4 batch-70 = 0.03407632187008858

Training epoch-4 batch-71
Running loss of epoch-4 batch-71 = 0.032904259860515594

Training epoch-4 batch-72
Running loss of epoch-4 batch-72 = 0.03282855823636055

Training epoch-4 batch-73
Running loss of epoch-4 batch-73 = 0.032307181507349014

Training epoch-4 batch-74
Running loss of epoch-4 batch-74 = 0.033437732607126236

Training epoch-4 batch-75
Running loss of epoch-4 batch-75 = 0.03430632874369621

Training epoch-4 batch-76
Running loss of epoch-4 batch-76 = 0.032911088317632675

Training epoch-4 batch-77
Running loss of epoch-4 batch-77 = 0.033405087888240814

Training epoch-4 batch-78
Running loss of epoch-4 batch-78 = 0.03338262066245079

Training epoch-4 batch-79
Running loss of epoch-4 batch-79 = 0.033296868205070496

Training epoch-4 batch-80
Running loss of epoch-4 batch-80 = 0.03411222994327545

Training epoch-4 batch-81
Running loss of epoch-4 batch-81 = 0.031602680683135986

Training epoch-4 batch-82
Running loss of epoch-4 batch-82 = 0.03283427655696869

Training epoch-4 batch-83
Running loss of epoch-4 batch-83 = 0.0345497652888298

Training epoch-4 batch-84
Running loss of epoch-4 batch-84 = 0.03254607319831848

Training epoch-4 batch-85
Running loss of epoch-4 batch-85 = 0.03326129540801048

Training epoch-4 batch-86
Running loss of epoch-4 batch-86 = 0.03256511688232422

Training epoch-4 batch-87
Running loss of epoch-4 batch-87 = 0.034427937120199203

Training epoch-4 batch-88
Running loss of epoch-4 batch-88 = 0.03265601024031639

Training epoch-4 batch-89
Running loss of epoch-4 batch-89 = 0.03259517624974251

Training epoch-4 batch-90
Running loss of epoch-4 batch-90 = 0.03342432901263237

Training epoch-4 batch-91
Running loss of epoch-4 batch-91 = 0.03144212067127228

Training epoch-4 batch-92
Running loss of epoch-4 batch-92 = 0.033137883991003036

Training epoch-4 batch-93
Running loss of epoch-4 batch-93 = 0.03189514949917793

Training epoch-4 batch-94
Running loss of epoch-4 batch-94 = 0.032983724027872086

Training epoch-4 batch-95
Running loss of epoch-4 batch-95 = 0.03223664313554764

Training epoch-4 batch-96
Running loss of epoch-4 batch-96 = 0.03253379836678505

Training epoch-4 batch-97
Running loss of epoch-4 batch-97 = 0.03154183551669121

Training epoch-4 batch-98
Running loss of epoch-4 batch-98 = 0.03382743522524834

Training epoch-4 batch-99
Running loss of epoch-4 batch-99 = 0.030633188784122467

Training epoch-4 batch-100
Running loss of epoch-4 batch-100 = 0.03323246166110039

Training epoch-4 batch-101
Running loss of epoch-4 batch-101 = 0.031004123389720917

Training epoch-4 batch-102
Running loss of epoch-4 batch-102 = 0.031019089743494987

Training epoch-4 batch-103
Running loss of epoch-4 batch-103 = 0.032975535839796066

Training epoch-4 batch-104
Running loss of epoch-4 batch-104 = 0.031014394015073776

Training epoch-4 batch-105
Running loss of epoch-4 batch-105 = 0.03256857767701149

Training epoch-4 batch-106
Running loss of epoch-4 batch-106 = 0.034661512821912766

Training epoch-4 batch-107
Running loss of epoch-4 batch-107 = 0.032123591750860214

Training epoch-4 batch-108
Running loss of epoch-4 batch-108 = 0.03346353396773338

Training epoch-4 batch-109
Running loss of epoch-4 batch-109 = 0.03195535019040108

Training epoch-4 batch-110
Running loss of epoch-4 batch-110 = 0.0342462994158268

Training epoch-4 batch-111
Running loss of epoch-4 batch-111 = 0.034381505101919174

Training epoch-4 batch-112
Running loss of epoch-4 batch-112 = 0.031214876100420952

Training epoch-4 batch-113
Running loss of epoch-4 batch-113 = 0.03335665166378021

Training epoch-4 batch-114
Running loss of epoch-4 batch-114 = 0.03352585434913635

Training epoch-4 batch-115
Running loss of epoch-4 batch-115 = 0.0317469947040081

Training epoch-4 batch-116
Running loss of epoch-4 batch-116 = 0.03337882459163666

Training epoch-4 batch-117
Running loss of epoch-4 batch-117 = 0.03342539817094803

Training epoch-4 batch-118
Running loss of epoch-4 batch-118 = 0.03284531459212303

Training epoch-4 batch-119
Running loss of epoch-4 batch-119 = 0.031247343868017197

Training epoch-4 batch-120
Running loss of epoch-4 batch-120 = 0.03312584012746811

Training epoch-4 batch-121
Running loss of epoch-4 batch-121 = 0.03291317820549011

Training epoch-4 batch-122
Running loss of epoch-4 batch-122 = 0.03205082193017006

Training epoch-4 batch-123
Running loss of epoch-4 batch-123 = 0.03382362797856331

Training epoch-4 batch-124
Running loss of epoch-4 batch-124 = 0.03361930325627327

Training epoch-4 batch-125
Running loss of epoch-4 batch-125 = 0.030758490785956383

Training epoch-4 batch-126
Running loss of epoch-4 batch-126 = 0.03174954280257225

Training epoch-4 batch-127
Running loss of epoch-4 batch-127 = 0.033952757716178894

Training epoch-4 batch-128
Running loss of epoch-4 batch-128 = 0.03354582563042641

Training epoch-4 batch-129
Running loss of epoch-4 batch-129 = 0.03220784664154053

Training epoch-4 batch-130
Running loss of epoch-4 batch-130 = 0.030771439895033836

Training epoch-4 batch-131
Running loss of epoch-4 batch-131 = 0.032318659126758575

Training epoch-4 batch-132
Running loss of epoch-4 batch-132 = 0.03286091610789299

Training epoch-4 batch-133
Running loss of epoch-4 batch-133 = 0.03316053748130798

Training epoch-4 batch-134
Running loss of epoch-4 batch-134 = 0.03281763195991516

Training epoch-4 batch-135
Running loss of epoch-4 batch-135 = 0.03321133553981781

Training epoch-4 batch-136
Running loss of epoch-4 batch-136 = 0.03282061591744423

Training epoch-4 batch-137
Running loss of epoch-4 batch-137 = 0.031827595084905624

Training epoch-4 batch-138
Running loss of epoch-4 batch-138 = 0.032446831464767456

Training epoch-4 batch-139
Running loss of epoch-4 batch-139 = 0.03323102742433548

Training epoch-4 batch-140
Running loss of epoch-4 batch-140 = 0.033820584416389465

Training epoch-4 batch-141
Running loss of epoch-4 batch-141 = 0.03297002241015434

Training epoch-4 batch-142
Running loss of epoch-4 batch-142 = 0.030906248837709427

Training epoch-4 batch-143
Running loss of epoch-4 batch-143 = 0.031762219965457916

Training epoch-4 batch-144
Running loss of epoch-4 batch-144 = 0.03149116411805153

Training epoch-4 batch-145
Running loss of epoch-4 batch-145 = 0.031539786607027054

Training epoch-4 batch-146
Running loss of epoch-4 batch-146 = 0.031492721289396286

Training epoch-4 batch-147
Running loss of epoch-4 batch-147 = 0.03136497363448143

Training epoch-4 batch-148
Running loss of epoch-4 batch-148 = 0.03328884765505791

Training epoch-4 batch-149
Running loss of epoch-4 batch-149 = 0.032786983996629715

Training epoch-4 batch-150
Running loss of epoch-4 batch-150 = 0.0347987525165081

Training epoch-4 batch-151
Running loss of epoch-4 batch-151 = 0.03346787393093109

Training epoch-4 batch-152
Running loss of epoch-4 batch-152 = 0.0314103402197361

Training epoch-4 batch-153
Running loss of epoch-4 batch-153 = 0.03140537068247795

Training epoch-4 batch-154
Running loss of epoch-4 batch-154 = 0.03461962193250656

Training epoch-4 batch-155
Running loss of epoch-4 batch-155 = 0.031390052288770676

Training epoch-4 batch-156
Running loss of epoch-4 batch-156 = 0.03172256052494049

Training epoch-4 batch-157
Running loss of epoch-4 batch-157 = 0.1326175183057785

Finished training epoch-4.



Average train loss at epoch-4 = 0.03346560134887695

Started Evaluation

Average val loss at epoch-4 = 2.0518453858400645

Accuracy for classes:
Accuracy for class equals is: 65.68 %
Accuracy for class main is: 37.70 %
Accuracy for class setUp is: 28.36 %
Accuracy for class onCreate is: 22.49 %
Accuracy for class toString is: 6.83 %
Accuracy for class run is: 18.95 %
Accuracy for class hashCode is: 78.65 %
Accuracy for class init is: 13.23 %
Accuracy for class execute is: 0.00 %
Accuracy for class get is: 5.13 %

Overall Accuracy = 28.97 %


Best Accuracy = 28.97 % at Epoch-4
Saving model after best epoch-4

Finished Evaluation



Started training epoch-5


Training epoch-5 batch-1
Running loss of epoch-5 batch-1 = 0.033628497272729874

Training epoch-5 batch-2
Running loss of epoch-5 batch-2 = 0.030575565993785858

Training epoch-5 batch-3
Running loss of epoch-5 batch-3 = 0.03226329758763313

Training epoch-5 batch-4
Running loss of epoch-5 batch-4 = 0.033473365008831024

Training epoch-5 batch-5
Running loss of epoch-5 batch-5 = 0.029770849272608757

Training epoch-5 batch-6
Running loss of epoch-5 batch-6 = 0.03274942934513092

Training epoch-5 batch-7
Running loss of epoch-5 batch-7 = 0.030477264896035194

Training epoch-5 batch-8
Running loss of epoch-5 batch-8 = 0.029780887067317963

Training epoch-5 batch-9
Running loss of epoch-5 batch-9 = 0.03163199871778488

Training epoch-5 batch-10
Running loss of epoch-5 batch-10 = 0.032034434378147125

Training epoch-5 batch-11
Running loss of epoch-5 batch-11 = 0.03219330310821533

Training epoch-5 batch-12
Running loss of epoch-5 batch-12 = 0.030550215393304825

Training epoch-5 batch-13
Running loss of epoch-5 batch-13 = 0.030961185693740845

Training epoch-5 batch-14
Running loss of epoch-5 batch-14 = 0.031843364238739014

Training epoch-5 batch-15
Running loss of epoch-5 batch-15 = 0.02980050817131996

Training epoch-5 batch-16
Running loss of epoch-5 batch-16 = 0.03268604353070259

Training epoch-5 batch-17
Running loss of epoch-5 batch-17 = 0.031150100752711296

Training epoch-5 batch-18
Running loss of epoch-5 batch-18 = 0.031115278601646423

Training epoch-5 batch-19
Running loss of epoch-5 batch-19 = 0.031456634402275085

Training epoch-5 batch-20
Running loss of epoch-5 batch-20 = 0.033029161393642426

Training epoch-5 batch-21
Running loss of epoch-5 batch-21 = 0.03080589696764946

Training epoch-5 batch-22
Running loss of epoch-5 batch-22 = 0.031085368245840073

Training epoch-5 batch-23
Running loss of epoch-5 batch-23 = 0.03201502934098244

Training epoch-5 batch-24
Running loss of epoch-5 batch-24 = 0.03162814676761627

Training epoch-5 batch-25
Running loss of epoch-5 batch-25 = 0.03173771873116493

Training epoch-5 batch-26
Running loss of epoch-5 batch-26 = 0.03099621832370758

Training epoch-5 batch-27
Running loss of epoch-5 batch-27 = 0.02958233281970024

Training epoch-5 batch-28
Running loss of epoch-5 batch-28 = 0.032693613320589066

Training epoch-5 batch-29
Running loss of epoch-5 batch-29 = 0.029507387429475784

Training epoch-5 batch-30
Running loss of epoch-5 batch-30 = 0.032374363392591476

Training epoch-5 batch-31
Running loss of epoch-5 batch-31 = 0.03130452707409859

Training epoch-5 batch-32
Running loss of epoch-5 batch-32 = 0.03178690746426582

Training epoch-5 batch-33
Running loss of epoch-5 batch-33 = 0.03335186466574669

Training epoch-5 batch-34
Running loss of epoch-5 batch-34 = 0.03163541853427887

Training epoch-5 batch-35
Running loss of epoch-5 batch-35 = 0.02998451516032219

Training epoch-5 batch-36
Running loss of epoch-5 batch-36 = 0.0296957865357399

Training epoch-5 batch-37
Running loss of epoch-5 batch-37 = 0.030561409890651703

Training epoch-5 batch-38
Running loss of epoch-5 batch-38 = 0.031093522906303406

Training epoch-5 batch-39
Running loss of epoch-5 batch-39 = 0.03293613716959953

Training epoch-5 batch-40
Running loss of epoch-5 batch-40 = 0.03156992048025131

Training epoch-5 batch-41
Running loss of epoch-5 batch-41 = 0.03406359627842903

Training epoch-5 batch-42
Running loss of epoch-5 batch-42 = 0.030966557562351227

Training epoch-5 batch-43
Running loss of epoch-5 batch-43 = 0.03336997330188751

Training epoch-5 batch-44
Running loss of epoch-5 batch-44 = 0.03055618517100811

Training epoch-5 batch-45
Running loss of epoch-5 batch-45 = 0.032316144555807114

Training epoch-5 batch-46
Running loss of epoch-5 batch-46 = 0.030197415500879288

Training epoch-5 batch-47
Running loss of epoch-5 batch-47 = 0.033507801592350006

Training epoch-5 batch-48
Running loss of epoch-5 batch-48 = 0.03128429129719734

Training epoch-5 batch-49
Running loss of epoch-5 batch-49 = 0.03274359181523323

Training epoch-5 batch-50
Running loss of epoch-5 batch-50 = 0.03154350817203522

Training epoch-5 batch-51
Running loss of epoch-5 batch-51 = 0.03146103024482727

Training epoch-5 batch-52
Running loss of epoch-5 batch-52 = 0.03199147433042526

Training epoch-5 batch-53
Running loss of epoch-5 batch-53 = 0.03087530843913555

Training epoch-5 batch-54
Running loss of epoch-5 batch-54 = 0.03185214102268219

Training epoch-5 batch-55
Running loss of epoch-5 batch-55 = 0.0324428454041481

Training epoch-5 batch-56
Running loss of epoch-5 batch-56 = 0.03013673797249794

Training epoch-5 batch-57
Running loss of epoch-5 batch-57 = 0.02855047397315502

Training epoch-5 batch-58
Running loss of epoch-5 batch-58 = 0.02946649119257927

Training epoch-5 batch-59
Running loss of epoch-5 batch-59 = 0.03012019209563732

Training epoch-5 batch-60
Running loss of epoch-5 batch-60 = 0.03178055211901665

Training epoch-5 batch-61
Running loss of epoch-5 batch-61 = 0.03095690719783306

Training epoch-5 batch-62
Running loss of epoch-5 batch-62 = 0.03299829736351967

Training epoch-5 batch-63
Running loss of epoch-5 batch-63 = 0.029279004782438278

Training epoch-5 batch-64
Running loss of epoch-5 batch-64 = 0.031442075967788696

Training epoch-5 batch-65
Running loss of epoch-5 batch-65 = 0.030509691685438156

Training epoch-5 batch-66
Running loss of epoch-5 batch-66 = 0.030314451083540916

Training epoch-5 batch-67
Running loss of epoch-5 batch-67 = 0.03119758330285549

Training epoch-5 batch-68
Running loss of epoch-5 batch-68 = 0.02987399697303772

Training epoch-5 batch-69
Running loss of epoch-5 batch-69 = 0.029098808765411377

Training epoch-5 batch-70
Running loss of epoch-5 batch-70 = 0.03186647966504097

Training epoch-5 batch-71
Running loss of epoch-5 batch-71 = 0.03318202495574951

Training epoch-5 batch-72
Running loss of epoch-5 batch-72 = 0.030501529574394226

Training epoch-5 batch-73
Running loss of epoch-5 batch-73 = 0.028935857117176056

Training epoch-5 batch-74
Running loss of epoch-5 batch-74 = 0.028691047802567482

Training epoch-5 batch-75
Running loss of epoch-5 batch-75 = 0.031243950128555298

Training epoch-5 batch-76
Running loss of epoch-5 batch-76 = 0.032479386776685715

Training epoch-5 batch-77
Running loss of epoch-5 batch-77 = 0.03129034861922264

Training epoch-5 batch-78
Running loss of epoch-5 batch-78 = 0.030418071895837784

Training epoch-5 batch-79
Running loss of epoch-5 batch-79 = 0.03068680129945278

Training epoch-5 batch-80
Running loss of epoch-5 batch-80 = 0.029218338429927826

Training epoch-5 batch-81
Running loss of epoch-5 batch-81 = 0.03130161762237549

Training epoch-5 batch-82
Running loss of epoch-5 batch-82 = 0.03290201723575592

Training epoch-5 batch-83
Running loss of epoch-5 batch-83 = 0.030620068311691284

Training epoch-5 batch-84
Running loss of epoch-5 batch-84 = 0.030514366924762726

Training epoch-5 batch-85
Running loss of epoch-5 batch-85 = 0.031239360570907593

Training epoch-5 batch-86
Running loss of epoch-5 batch-86 = 0.031084822490811348

Training epoch-5 batch-87
Running loss of epoch-5 batch-87 = 0.0315597727894783

Training epoch-5 batch-88
Running loss of epoch-5 batch-88 = 0.030032260343432426

Training epoch-5 batch-89
Running loss of epoch-5 batch-89 = 0.03189079463481903

Training epoch-5 batch-90
Running loss of epoch-5 batch-90 = 0.03220948204398155

Training epoch-5 batch-91
Running loss of epoch-5 batch-91 = 0.030455078929662704

Training epoch-5 batch-92
Running loss of epoch-5 batch-92 = 0.028338490054011345

Training epoch-5 batch-93
Running loss of epoch-5 batch-93 = 0.03174842521548271

Training epoch-5 batch-94
Running loss of epoch-5 batch-94 = 0.03167755901813507

Training epoch-5 batch-95
Running loss of epoch-5 batch-95 = 0.0299016535282135

Training epoch-5 batch-96
Running loss of epoch-5 batch-96 = 0.03091570921242237

Training epoch-5 batch-97
Running loss of epoch-5 batch-97 = 0.030315600335597992

Training epoch-5 batch-98
Running loss of epoch-5 batch-98 = 0.031090274453163147

Training epoch-5 batch-99
Running loss of epoch-5 batch-99 = 0.02903803251683712

Training epoch-5 batch-100
Running loss of epoch-5 batch-100 = 0.030159994959831238

Training epoch-5 batch-101
Running loss of epoch-5 batch-101 = 0.02991027571260929

Training epoch-5 batch-102
Running loss of epoch-5 batch-102 = 0.03243595361709595

Training epoch-5 batch-103
Running loss of epoch-5 batch-103 = 0.0279682707041502

Training epoch-5 batch-104
Running loss of epoch-5 batch-104 = 0.03077174723148346

Training epoch-5 batch-105
Running loss of epoch-5 batch-105 = 0.027984919026494026

Training epoch-5 batch-106
Running loss of epoch-5 batch-106 = 0.02983212098479271

Training epoch-5 batch-107
Running loss of epoch-5 batch-107 = 0.029158543795347214

Training epoch-5 batch-108
Running loss of epoch-5 batch-108 = 0.031899526715278625

Training epoch-5 batch-109
Running loss of epoch-5 batch-109 = 0.030872752889990807

Training epoch-5 batch-110
Running loss of epoch-5 batch-110 = 0.029098834842443466

Training epoch-5 batch-111
Running loss of epoch-5 batch-111 = 0.02816382795572281

Training epoch-5 batch-112
Running loss of epoch-5 batch-112 = 0.0288691408932209

Training epoch-5 batch-113
Running loss of epoch-5 batch-113 = 0.030101286247372627

Training epoch-5 batch-114
Running loss of epoch-5 batch-114 = 0.03089303895831108

Training epoch-5 batch-115
Running loss of epoch-5 batch-115 = 0.02815251797437668

Training epoch-5 batch-116
Running loss of epoch-5 batch-116 = 0.028928786516189575

Training epoch-5 batch-117
Running loss of epoch-5 batch-117 = 0.031280767172575

Training epoch-5 batch-118
Running loss of epoch-5 batch-118 = 0.030102688819169998

Training epoch-5 batch-119
Running loss of epoch-5 batch-119 = 0.031688280403614044

Training epoch-5 batch-120
Running loss of epoch-5 batch-120 = 0.03339746594429016

Training epoch-5 batch-121
Running loss of epoch-5 batch-121 = 0.030866030603647232

Training epoch-5 batch-122
Running loss of epoch-5 batch-122 = 0.02941960282623768

Training epoch-5 batch-123
Running loss of epoch-5 batch-123 = 0.03069864585995674

Training epoch-5 batch-124
Running loss of epoch-5 batch-124 = 0.02952563762664795

Training epoch-5 batch-125
Running loss of epoch-5 batch-125 = 0.02925722487270832

Training epoch-5 batch-126
Running loss of epoch-5 batch-126 = 0.03241916745901108

Training epoch-5 batch-127
Running loss of epoch-5 batch-127 = 0.031217215582728386

Training epoch-5 batch-128
Running loss of epoch-5 batch-128 = 0.030443884432315826

Training epoch-5 batch-129
Running loss of epoch-5 batch-129 = 0.029252827167510986

Training epoch-5 batch-130
Running loss of epoch-5 batch-130 = 0.028945576399564743

Training epoch-5 batch-131
Running loss of epoch-5 batch-131 = 0.02935788966715336

Training epoch-5 batch-132
Running loss of epoch-5 batch-132 = 0.02916891500353813

Training epoch-5 batch-133
Running loss of epoch-5 batch-133 = 0.03137211874127388

Training epoch-5 batch-134
Running loss of epoch-5 batch-134 = 0.03021657094359398

Training epoch-5 batch-135
Running loss of epoch-5 batch-135 = 0.028221705928444862

Training epoch-5 batch-136
Running loss of epoch-5 batch-136 = 0.031654272228479385

Training epoch-5 batch-137
Running loss of epoch-5 batch-137 = 0.029406480491161346

Training epoch-5 batch-138
Running loss of epoch-5 batch-138 = 0.02958867885172367

Training epoch-5 batch-139
Running loss of epoch-5 batch-139 = 0.03106316551566124

Training epoch-5 batch-140
Running loss of epoch-5 batch-140 = 0.028541680425405502

Training epoch-5 batch-141
Running loss of epoch-5 batch-141 = 0.029980463907122612

Training epoch-5 batch-142
Running loss of epoch-5 batch-142 = 0.028641102835536003

Training epoch-5 batch-143
Running loss of epoch-5 batch-143 = 0.03249334171414375

Training epoch-5 batch-144
Running loss of epoch-5 batch-144 = 0.027261774986982346

Training epoch-5 batch-145
Running loss of epoch-5 batch-145 = 0.0299559086561203

Training epoch-5 batch-146
Running loss of epoch-5 batch-146 = 0.03015727549791336

Training epoch-5 batch-147
Running loss of epoch-5 batch-147 = 0.030849860981106758

Training epoch-5 batch-148
Running loss of epoch-5 batch-148 = 0.027323180809617043

Training epoch-5 batch-149
Running loss of epoch-5 batch-149 = 0.03122486174106598

Training epoch-5 batch-150
Running loss of epoch-5 batch-150 = 0.027323968708515167

Training epoch-5 batch-151
Running loss of epoch-5 batch-151 = 0.028261298313736916

Training epoch-5 batch-152
Running loss of epoch-5 batch-152 = 0.02835140936076641

Training epoch-5 batch-153
Running loss of epoch-5 batch-153 = 0.029409632086753845

Training epoch-5 batch-154
Running loss of epoch-5 batch-154 = 0.029561394825577736

Training epoch-5 batch-155
Running loss of epoch-5 batch-155 = 0.028921518474817276

Training epoch-5 batch-156
Running loss of epoch-5 batch-156 = 0.032783392816782

Training epoch-5 batch-157
Running loss of epoch-5 batch-157 = 0.10737449675798416

Finished training epoch-5.



Average train loss at epoch-5 = 0.030844261527061462

Started Evaluation

Average val loss at epoch-5 = 1.8489313204037516

Accuracy for classes:
Accuracy for class equals is: 56.77 %
Accuracy for class main is: 50.49 %
Accuracy for class setUp is: 42.13 %
Accuracy for class onCreate is: 41.36 %
Accuracy for class toString is: 19.11 %
Accuracy for class run is: 49.54 %
Accuracy for class hashCode is: 70.41 %
Accuracy for class init is: 6.05 %
Accuracy for class execute is: 5.62 %
Accuracy for class get is: 3.85 %

Overall Accuracy = 37.43 %


Best Accuracy = 37.43 % at Epoch-5
Saving model after best epoch-5

Finished Evaluation



Started training epoch-6


Training epoch-6 batch-1
Running loss of epoch-6 batch-1 = 0.02700984850525856

Training epoch-6 batch-2
Running loss of epoch-6 batch-2 = 0.029448963701725006

Training epoch-6 batch-3
Running loss of epoch-6 batch-3 = 0.02761124260723591

Training epoch-6 batch-4
Running loss of epoch-6 batch-4 = 0.02923392876982689

Training epoch-6 batch-5
Running loss of epoch-6 batch-5 = 0.02877931110560894

Training epoch-6 batch-6
Running loss of epoch-6 batch-6 = 0.02807736024260521

Training epoch-6 batch-7
Running loss of epoch-6 batch-7 = 0.02767210640013218

Training epoch-6 batch-8
Running loss of epoch-6 batch-8 = 0.02816476859152317

Training epoch-6 batch-9
Running loss of epoch-6 batch-9 = 0.02818399854004383

Training epoch-6 batch-10
Running loss of epoch-6 batch-10 = 0.0318116694688797

Training epoch-6 batch-11
Running loss of epoch-6 batch-11 = 0.02709590084850788

Training epoch-6 batch-12
Running loss of epoch-6 batch-12 = 0.02611682377755642

Training epoch-6 batch-13
Running loss of epoch-6 batch-13 = 0.030943341553211212

Training epoch-6 batch-14
Running loss of epoch-6 batch-14 = 0.028675444424152374

Training epoch-6 batch-15
Running loss of epoch-6 batch-15 = 0.02940388396382332

Training epoch-6 batch-16
Running loss of epoch-6 batch-16 = 0.029089897871017456

Training epoch-6 batch-17
Running loss of epoch-6 batch-17 = 0.02963261492550373

Training epoch-6 batch-18
Running loss of epoch-6 batch-18 = 0.02823740616440773

Training epoch-6 batch-19
Running loss of epoch-6 batch-19 = 0.028306832537055016

Training epoch-6 batch-20
Running loss of epoch-6 batch-20 = 0.031142093241214752

Training epoch-6 batch-21
Running loss of epoch-6 batch-21 = 0.028224999085068703

Training epoch-6 batch-22
Running loss of epoch-6 batch-22 = 0.028847552835941315

Training epoch-6 batch-23
Running loss of epoch-6 batch-23 = 0.029713299125432968

Training epoch-6 batch-24
Running loss of epoch-6 batch-24 = 0.028639011085033417

Training epoch-6 batch-25
Running loss of epoch-6 batch-25 = 0.027705920860171318

Training epoch-6 batch-26
Running loss of epoch-6 batch-26 = 0.026482535526156425

Training epoch-6 batch-27
Running loss of epoch-6 batch-27 = 0.02624732069671154

Training epoch-6 batch-28
Running loss of epoch-6 batch-28 = 0.028161946684122086

Training epoch-6 batch-29
Running loss of epoch-6 batch-29 = 0.03012065961956978

Training epoch-6 batch-30
Running loss of epoch-6 batch-30 = 0.028711484745144844

Training epoch-6 batch-31
Running loss of epoch-6 batch-31 = 0.026918835937976837

Training epoch-6 batch-32
Running loss of epoch-6 batch-32 = 0.025254597887396812

Training epoch-6 batch-33
Running loss of epoch-6 batch-33 = 0.02563798613846302

Training epoch-6 batch-34
Running loss of epoch-6 batch-34 = 0.029209908097982407

Training epoch-6 batch-35
Running loss of epoch-6 batch-35 = 0.029640935361385345

Training epoch-6 batch-36
Running loss of epoch-6 batch-36 = 0.02774062007665634

Training epoch-6 batch-37
Running loss of epoch-6 batch-37 = 0.02789202705025673

Training epoch-6 batch-38
Running loss of epoch-6 batch-38 = 0.02766018733382225

Training epoch-6 batch-39
Running loss of epoch-6 batch-39 = 0.027765031903982162

Training epoch-6 batch-40
Running loss of epoch-6 batch-40 = 0.02950521558523178

Training epoch-6 batch-41
Running loss of epoch-6 batch-41 = 0.02787226065993309

Training epoch-6 batch-42
Running loss of epoch-6 batch-42 = 0.02786998823285103

Training epoch-6 batch-43
Running loss of epoch-6 batch-43 = 0.026015091687440872

Training epoch-6 batch-44
Running loss of epoch-6 batch-44 = 0.028665361925959587

Training epoch-6 batch-45
Running loss of epoch-6 batch-45 = 0.02716485783457756

Training epoch-6 batch-46
Running loss of epoch-6 batch-46 = 0.023955950513482094

Training epoch-6 batch-47
Running loss of epoch-6 batch-47 = 0.0277389045804739

Training epoch-6 batch-48
Running loss of epoch-6 batch-48 = 0.027383210137486458

Training epoch-6 batch-49
Running loss of epoch-6 batch-49 = 0.028528567403554916

Training epoch-6 batch-50
Running loss of epoch-6 batch-50 = 0.029235336929559708

Training epoch-6 batch-51
Running loss of epoch-6 batch-51 = 0.028049945831298828

Training epoch-6 batch-52
Running loss of epoch-6 batch-52 = 0.03052360564470291

Training epoch-6 batch-53
Running loss of epoch-6 batch-53 = 0.027196405455470085

Training epoch-6 batch-54
Running loss of epoch-6 batch-54 = 0.02763054519891739

Training epoch-6 batch-55
Running loss of epoch-6 batch-55 = 0.03241655230522156

Training epoch-6 batch-56
Running loss of epoch-6 batch-56 = 0.02930278517305851

Training epoch-6 batch-57
Running loss of epoch-6 batch-57 = 0.02882079780101776

Training epoch-6 batch-58
Running loss of epoch-6 batch-58 = 0.02948838658630848

Training epoch-6 batch-59
Running loss of epoch-6 batch-59 = 0.027965646237134933

Training epoch-6 batch-60
Running loss of epoch-6 batch-60 = 0.02833445370197296

Training epoch-6 batch-61
Running loss of epoch-6 batch-61 = 0.0316481739282608

Training epoch-6 batch-62
Running loss of epoch-6 batch-62 = 0.029652858152985573

Training epoch-6 batch-63
Running loss of epoch-6 batch-63 = 0.026926923543214798

Training epoch-6 batch-64
Running loss of epoch-6 batch-64 = 0.02784741297364235

Training epoch-6 batch-65
Running loss of epoch-6 batch-65 = 0.029004085808992386

Training epoch-6 batch-66
Running loss of epoch-6 batch-66 = 0.028162766247987747

Training epoch-6 batch-67
Running loss of epoch-6 batch-67 = 0.027182646095752716

Training epoch-6 batch-68
Running loss of epoch-6 batch-68 = 0.02984803542494774

Training epoch-6 batch-69
Running loss of epoch-6 batch-69 = 0.02547590434551239

Training epoch-6 batch-70
Running loss of epoch-6 batch-70 = 0.02662854827940464

Training epoch-6 batch-71
Running loss of epoch-6 batch-71 = 0.02800898440182209

Training epoch-6 batch-72
Running loss of epoch-6 batch-72 = 0.026805376634001732

Training epoch-6 batch-73
Running loss of epoch-6 batch-73 = 0.026463286951184273

Training epoch-6 batch-74
Running loss of epoch-6 batch-74 = 0.0289089847356081

Training epoch-6 batch-75
Running loss of epoch-6 batch-75 = 0.029655348509550095

Training epoch-6 batch-76
Running loss of epoch-6 batch-76 = 0.027025097981095314

Training epoch-6 batch-77
Running loss of epoch-6 batch-77 = 0.02952403947710991

Training epoch-6 batch-78
Running loss of epoch-6 batch-78 = 0.02872573211789131

Training epoch-6 batch-79
Running loss of epoch-6 batch-79 = 0.02517794445157051

Training epoch-6 batch-80
Running loss of epoch-6 batch-80 = 0.027949901297688484

Training epoch-6 batch-81
Running loss of epoch-6 batch-81 = 0.028815174475312233

Training epoch-6 batch-82
Running loss of epoch-6 batch-82 = 0.0282022412866354

Training epoch-6 batch-83
Running loss of epoch-6 batch-83 = 0.027488332241773605

Training epoch-6 batch-84
Running loss of epoch-6 batch-84 = 0.02823786996304989

Training epoch-6 batch-85
Running loss of epoch-6 batch-85 = 0.026233406737446785

Training epoch-6 batch-86
Running loss of epoch-6 batch-86 = 0.029614467173814774

Training epoch-6 batch-87
Running loss of epoch-6 batch-87 = 0.027558906003832817

Training epoch-6 batch-88
Running loss of epoch-6 batch-88 = 0.028106804937124252

Training epoch-6 batch-89
Running loss of epoch-6 batch-89 = 0.026612624526023865

Training epoch-6 batch-90
Running loss of epoch-6 batch-90 = 0.024442104622721672

Training epoch-6 batch-91
Running loss of epoch-6 batch-91 = 0.023829830810427666

Training epoch-6 batch-92
Running loss of epoch-6 batch-92 = 0.027443857863545418

Training epoch-6 batch-93
Running loss of epoch-6 batch-93 = 0.028419366106390953

Training epoch-6 batch-94
Running loss of epoch-6 batch-94 = 0.026252297684550285

Training epoch-6 batch-95
Running loss of epoch-6 batch-95 = 0.02870434895157814

Training epoch-6 batch-96
Running loss of epoch-6 batch-96 = 0.029927264899015427

Training epoch-6 batch-97
Running loss of epoch-6 batch-97 = 0.02628302201628685

Training epoch-6 batch-98
Running loss of epoch-6 batch-98 = 0.026275813579559326

Training epoch-6 batch-99
Running loss of epoch-6 batch-99 = 0.029141725972294807

Training epoch-6 batch-100
Running loss of epoch-6 batch-100 = 0.0286677535623312

Training epoch-6 batch-101
Running loss of epoch-6 batch-101 = 0.028406325727701187

Training epoch-6 batch-102
Running loss of epoch-6 batch-102 = 0.02809968777000904

Training epoch-6 batch-103
Running loss of epoch-6 batch-103 = 0.02652951143682003

Training epoch-6 batch-104
Running loss of epoch-6 batch-104 = 0.02789887972176075

Training epoch-6 batch-105
Running loss of epoch-6 batch-105 = 0.030718166381120682

Training epoch-6 batch-106
Running loss of epoch-6 batch-106 = 0.028995631262660027

Training epoch-6 batch-107
Running loss of epoch-6 batch-107 = 0.027808180078864098

Training epoch-6 batch-108
Running loss of epoch-6 batch-108 = 0.026256084442138672

Training epoch-6 batch-109
Running loss of epoch-6 batch-109 = 0.02995830774307251

Training epoch-6 batch-110
Running loss of epoch-6 batch-110 = 0.028646139428019524

Training epoch-6 batch-111
Running loss of epoch-6 batch-111 = 0.02971210703253746

Training epoch-6 batch-112
Running loss of epoch-6 batch-112 = 0.030948346480727196

Training epoch-6 batch-113
Running loss of epoch-6 batch-113 = 0.026893235743045807

Training epoch-6 batch-114
Running loss of epoch-6 batch-114 = 0.028577590361237526

Training epoch-6 batch-115
Running loss of epoch-6 batch-115 = 0.026829736307263374

Training epoch-6 batch-116
Running loss of epoch-6 batch-116 = 0.029038352891802788

Training epoch-6 batch-117
Running loss of epoch-6 batch-117 = 0.0278388150036335

Training epoch-6 batch-118
Running loss of epoch-6 batch-118 = 0.02683595009148121

Training epoch-6 batch-119
Running loss of epoch-6 batch-119 = 0.025799114257097244

Training epoch-6 batch-120
Running loss of epoch-6 batch-120 = 0.032539669424295425

Training epoch-6 batch-121
Running loss of epoch-6 batch-121 = 0.027404552325606346

Training epoch-6 batch-122
Running loss of epoch-6 batch-122 = 0.026112312451004982

Training epoch-6 batch-123
Running loss of epoch-6 batch-123 = 0.0288497656583786

Training epoch-6 batch-124
Running loss of epoch-6 batch-124 = 0.027812447398900986

Training epoch-6 batch-125
Running loss of epoch-6 batch-125 = 0.027817189693450928

Training epoch-6 batch-126
Running loss of epoch-6 batch-126 = 0.02538527175784111

Training epoch-6 batch-127
Running loss of epoch-6 batch-127 = 0.025328192859888077

Training epoch-6 batch-128
Running loss of epoch-6 batch-128 = 0.027407744899392128

Training epoch-6 batch-129
Running loss of epoch-6 batch-129 = 0.024099338799715042

Training epoch-6 batch-130
Running loss of epoch-6 batch-130 = 0.027524283155798912

Training epoch-6 batch-131
Running loss of epoch-6 batch-131 = 0.025986533612012863

Training epoch-6 batch-132
Running loss of epoch-6 batch-132 = 0.028525128960609436

Training epoch-6 batch-133
Running loss of epoch-6 batch-133 = 0.025061028078198433

Training epoch-6 batch-134
Running loss of epoch-6 batch-134 = 0.027930887416005135

Training epoch-6 batch-135
Running loss of epoch-6 batch-135 = 0.026465050876140594

Training epoch-6 batch-136
Running loss of epoch-6 batch-136 = 0.02624501660466194

Training epoch-6 batch-137
Running loss of epoch-6 batch-137 = 0.02820890210568905

Training epoch-6 batch-138
Running loss of epoch-6 batch-138 = 0.024809055030345917

Training epoch-6 batch-139
Running loss of epoch-6 batch-139 = 0.026439374312758446

Training epoch-6 batch-140
Running loss of epoch-6 batch-140 = 0.029074741527438164

Training epoch-6 batch-141
Running loss of epoch-6 batch-141 = 0.02808644063770771

Training epoch-6 batch-142
Running loss of epoch-6 batch-142 = 0.027313707396388054

Training epoch-6 batch-143
Running loss of epoch-6 batch-143 = 0.026615889742970467

Training epoch-6 batch-144
Running loss of epoch-6 batch-144 = 0.026915008202195168

Training epoch-6 batch-145
Running loss of epoch-6 batch-145 = 0.027624616399407387

Training epoch-6 batch-146
Running loss of epoch-6 batch-146 = 0.025930535048246384

Training epoch-6 batch-147
Running loss of epoch-6 batch-147 = 0.027846364304423332

Training epoch-6 batch-148
Running loss of epoch-6 batch-148 = 0.027181420475244522

Training epoch-6 batch-149
Running loss of epoch-6 batch-149 = 0.025591112673282623

Training epoch-6 batch-150
Running loss of epoch-6 batch-150 = 0.02755429781973362

Training epoch-6 batch-151
Running loss of epoch-6 batch-151 = 0.02713910862803459

Training epoch-6 batch-152
Running loss of epoch-6 batch-152 = 0.024776851758360863

Training epoch-6 batch-153
Running loss of epoch-6 batch-153 = 0.027867792174220085

Training epoch-6 batch-154
Running loss of epoch-6 batch-154 = 0.027818337082862854

Training epoch-6 batch-155
Running loss of epoch-6 batch-155 = 0.025880148634314537

Training epoch-6 batch-156
Running loss of epoch-6 batch-156 = 0.0271285530179739

Training epoch-6 batch-157
Running loss of epoch-6 batch-157 = 0.10213671624660492

Finished training epoch-6.



Average train loss at epoch-6 = 0.02797680923938751

Started Evaluation

Average val loss at epoch-6 = 1.7811434692458104

Accuracy for classes:
Accuracy for class equals is: 74.92 %
Accuracy for class main is: 50.33 %
Accuracy for class setUp is: 61.80 %
Accuracy for class onCreate is: 17.70 %
Accuracy for class toString is: 19.45 %
Accuracy for class run is: 21.00 %
Accuracy for class hashCode is: 78.28 %
Accuracy for class init is: 3.59 %
Accuracy for class execute is: 5.62 %
Accuracy for class get is: 31.28 %

Overall Accuracy = 37.43 %

Finished Evaluation



Started training epoch-7


Training epoch-7 batch-1
Running loss of epoch-7 batch-1 = 0.026443488895893097

Training epoch-7 batch-2
Running loss of epoch-7 batch-2 = 0.024894723668694496

Training epoch-7 batch-3
Running loss of epoch-7 batch-3 = 0.026411201804876328

Training epoch-7 batch-4
Running loss of epoch-7 batch-4 = 0.02528219297528267

Training epoch-7 batch-5
Running loss of epoch-7 batch-5 = 0.026745636016130447

Training epoch-7 batch-6
Running loss of epoch-7 batch-6 = 0.02559521608054638

Training epoch-7 batch-7
Running loss of epoch-7 batch-7 = 0.02471463568508625

Training epoch-7 batch-8
Running loss of epoch-7 batch-8 = 0.026687463745474815

Training epoch-7 batch-9
Running loss of epoch-7 batch-9 = 0.02556082233786583

Training epoch-7 batch-10
Running loss of epoch-7 batch-10 = 0.024412743747234344

Training epoch-7 batch-11
Running loss of epoch-7 batch-11 = 0.026087813079357147

Training epoch-7 batch-12
Running loss of epoch-7 batch-12 = 0.026517365127801895

Training epoch-7 batch-13
Running loss of epoch-7 batch-13 = 0.026981448754668236

Training epoch-7 batch-14
Running loss of epoch-7 batch-14 = 0.02552742138504982

Training epoch-7 batch-15
Running loss of epoch-7 batch-15 = 0.02570655755698681

Training epoch-7 batch-16
Running loss of epoch-7 batch-16 = 0.02581259049475193

Training epoch-7 batch-17
Running loss of epoch-7 batch-17 = 0.02499779313802719

Training epoch-7 batch-18
Running loss of epoch-7 batch-18 = 0.024774692952632904

Training epoch-7 batch-19
Running loss of epoch-7 batch-19 = 0.025080110877752304

Training epoch-7 batch-20
Running loss of epoch-7 batch-20 = 0.02719045802950859

Training epoch-7 batch-21
Running loss of epoch-7 batch-21 = 0.023790352046489716

Training epoch-7 batch-22
Running loss of epoch-7 batch-22 = 0.027902353554964066

Training epoch-7 batch-23
Running loss of epoch-7 batch-23 = 0.02215944603085518

Training epoch-7 batch-24
Running loss of epoch-7 batch-24 = 0.02611268311738968

Training epoch-7 batch-25
Running loss of epoch-7 batch-25 = 0.02421136200428009

Training epoch-7 batch-26
Running loss of epoch-7 batch-26 = 0.02372157946228981

Training epoch-7 batch-27
Running loss of epoch-7 batch-27 = 0.024277249351143837

Training epoch-7 batch-28
Running loss of epoch-7 batch-28 = 0.025985898450016975

Training epoch-7 batch-29
Running loss of epoch-7 batch-29 = 0.023206530138850212

Training epoch-7 batch-30
Running loss of epoch-7 batch-30 = 0.0226904284209013

Training epoch-7 batch-31
Running loss of epoch-7 batch-31 = 0.02020213007926941

Training epoch-7 batch-32
Running loss of epoch-7 batch-32 = 0.023007387295365334

Training epoch-7 batch-33
Running loss of epoch-7 batch-33 = 0.025876451283693314

Training epoch-7 batch-34
Running loss of epoch-7 batch-34 = 0.023508166894316673

Training epoch-7 batch-35
Running loss of epoch-7 batch-35 = 0.021796302869915962

Training epoch-7 batch-36
Running loss of epoch-7 batch-36 = 0.027335865423083305

Training epoch-7 batch-37
Running loss of epoch-7 batch-37 = 0.025767013430595398

Training epoch-7 batch-38
Running loss of epoch-7 batch-38 = 0.02525249496102333

Training epoch-7 batch-39
Running loss of epoch-7 batch-39 = 0.025689903646707535

Training epoch-7 batch-40
Running loss of epoch-7 batch-40 = 0.0293170977383852

Training epoch-7 batch-41
Running loss of epoch-7 batch-41 = 0.021623577922582626

Training epoch-7 batch-42
Running loss of epoch-7 batch-42 = 0.025963082909584045

Training epoch-7 batch-43
Running loss of epoch-7 batch-43 = 0.025668587535619736

Training epoch-7 batch-44
Running loss of epoch-7 batch-44 = 0.027834897860884666

Training epoch-7 batch-45
Running loss of epoch-7 batch-45 = 0.030308812856674194

Training epoch-7 batch-46
Running loss of epoch-7 batch-46 = 0.024321982637047768

Training epoch-7 batch-47
Running loss of epoch-7 batch-47 = 0.022822799161076546

Training epoch-7 batch-48
Running loss of epoch-7 batch-48 = 0.029664527624845505

Training epoch-7 batch-49
Running loss of epoch-7 batch-49 = 0.024164138361811638

Training epoch-7 batch-50
Running loss of epoch-7 batch-50 = 0.02308041974902153

Training epoch-7 batch-51
Running loss of epoch-7 batch-51 = 0.025007084012031555

Training epoch-7 batch-52
Running loss of epoch-7 batch-52 = 0.02576785907149315

Training epoch-7 batch-53
Running loss of epoch-7 batch-53 = 0.024927379563450813

Training epoch-7 batch-54
Running loss of epoch-7 batch-54 = 0.025104574859142303

Training epoch-7 batch-55
Running loss of epoch-7 batch-55 = 0.024646686390042305

Training epoch-7 batch-56
Running loss of epoch-7 batch-56 = 0.02650349773466587

Training epoch-7 batch-57
Running loss of epoch-7 batch-57 = 0.024576816707849503

Training epoch-7 batch-58
Running loss of epoch-7 batch-58 = 0.02354905940592289

Training epoch-7 batch-59
Running loss of epoch-7 batch-59 = 0.028035419061779976

Training epoch-7 batch-60
Running loss of epoch-7 batch-60 = 0.024771470576524734

Training epoch-7 batch-61
Running loss of epoch-7 batch-61 = 0.022081367671489716

Training epoch-7 batch-62
Running loss of epoch-7 batch-62 = 0.025726035237312317

Training epoch-7 batch-63
Running loss of epoch-7 batch-63 = 0.0219272468239069

Training epoch-7 batch-64
Running loss of epoch-7 batch-64 = 0.024927837774157524

Training epoch-7 batch-65
Running loss of epoch-7 batch-65 = 0.023515084758400917

Training epoch-7 batch-66
Running loss of epoch-7 batch-66 = 0.02746003121137619

Training epoch-7 batch-67
Running loss of epoch-7 batch-67 = 0.023670904338359833

Training epoch-7 batch-68
Running loss of epoch-7 batch-68 = 0.024112850427627563

Training epoch-7 batch-69
Running loss of epoch-7 batch-69 = 0.02503899298608303

Training epoch-7 batch-70
Running loss of epoch-7 batch-70 = 0.026838872581720352

Training epoch-7 batch-71
Running loss of epoch-7 batch-71 = 0.02652134746313095

Training epoch-7 batch-72
Running loss of epoch-7 batch-72 = 0.02524309791624546

Training epoch-7 batch-73
Running loss of epoch-7 batch-73 = 0.02291925810277462

Training epoch-7 batch-74
Running loss of epoch-7 batch-74 = 0.02331664226949215

Training epoch-7 batch-75
Running loss of epoch-7 batch-75 = 0.024340959265828133

Training epoch-7 batch-76
Running loss of epoch-7 batch-76 = 0.020341582596302032

Training epoch-7 batch-77
Running loss of epoch-7 batch-77 = 0.026448184624314308

Training epoch-7 batch-78
Running loss of epoch-7 batch-78 = 0.025911035016179085

Training epoch-7 batch-79
Running loss of epoch-7 batch-79 = 0.023654451593756676

Training epoch-7 batch-80
Running loss of epoch-7 batch-80 = 0.023261122405529022

Training epoch-7 batch-81
Running loss of epoch-7 batch-81 = 0.025724712759256363

Training epoch-7 batch-82
Running loss of epoch-7 batch-82 = 0.025472203269600868

Training epoch-7 batch-83
Running loss of epoch-7 batch-83 = 0.025743048638105392

Training epoch-7 batch-84
Running loss of epoch-7 batch-84 = 0.026369627565145493

Training epoch-7 batch-85
Running loss of epoch-7 batch-85 = 0.023988543078303337

Training epoch-7 batch-86
Running loss of epoch-7 batch-86 = 0.025868957862257957

Training epoch-7 batch-87
Running loss of epoch-7 batch-87 = 0.02423667162656784

Training epoch-7 batch-88
Running loss of epoch-7 batch-88 = 0.02627664804458618

Training epoch-7 batch-89
Running loss of epoch-7 batch-89 = 0.024350589141249657

Training epoch-7 batch-90
Running loss of epoch-7 batch-90 = 0.023988287895917892

Training epoch-7 batch-91
Running loss of epoch-7 batch-91 = 0.024069104343652725

Training epoch-7 batch-92
Running loss of epoch-7 batch-92 = 0.023017579689621925

Training epoch-7 batch-93
Running loss of epoch-7 batch-93 = 0.025396155193448067

Training epoch-7 batch-94
Running loss of epoch-7 batch-94 = 0.022247597575187683

Training epoch-7 batch-95
Running loss of epoch-7 batch-95 = 0.022273778915405273

Training epoch-7 batch-96
Running loss of epoch-7 batch-96 = 0.021736158058047295

Training epoch-7 batch-97
Running loss of epoch-7 batch-97 = 0.022976666688919067

Training epoch-7 batch-98
Running loss of epoch-7 batch-98 = 0.024081256240606308

Training epoch-7 batch-99
Running loss of epoch-7 batch-99 = 0.024400196969509125

Training epoch-7 batch-100
Running loss of epoch-7 batch-100 = 0.026346512138843536

Training epoch-7 batch-101
Running loss of epoch-7 batch-101 = 0.02535882033407688

Training epoch-7 batch-102
Running loss of epoch-7 batch-102 = 0.028597043827176094

Training epoch-7 batch-103
Running loss of epoch-7 batch-103 = 0.019991472363471985

Training epoch-7 batch-104
Running loss of epoch-7 batch-104 = 0.022373536601662636

Training epoch-7 batch-105
Running loss of epoch-7 batch-105 = 0.025465993210673332

Training epoch-7 batch-106
Running loss of epoch-7 batch-106 = 0.026144662871956825

Training epoch-7 batch-107
Running loss of epoch-7 batch-107 = 0.026698768138885498

Training epoch-7 batch-108
Running loss of epoch-7 batch-108 = 0.02330002561211586

Training epoch-7 batch-109
Running loss of epoch-7 batch-109 = 0.027765925973653793

Training epoch-7 batch-110
Running loss of epoch-7 batch-110 = 0.026215896010398865

Training epoch-7 batch-111
Running loss of epoch-7 batch-111 = 0.02357637695968151

Training epoch-7 batch-112
Running loss of epoch-7 batch-112 = 0.024963993579149246

Training epoch-7 batch-113
Running loss of epoch-7 batch-113 = 0.025881880894303322

Training epoch-7 batch-114
Running loss of epoch-7 batch-114 = 0.023802006617188454

Training epoch-7 batch-115
Running loss of epoch-7 batch-115 = 0.023962052538990974

Training epoch-7 batch-116
Running loss of epoch-7 batch-116 = 0.02401171252131462

Training epoch-7 batch-117
Running loss of epoch-7 batch-117 = 0.024496259167790413

Training epoch-7 batch-118
Running loss of epoch-7 batch-118 = 0.02530243992805481

Training epoch-7 batch-119
Running loss of epoch-7 batch-119 = 0.024121586233377457

Training epoch-7 batch-120
Running loss of epoch-7 batch-120 = 0.027113264426589012

Training epoch-7 batch-121
Running loss of epoch-7 batch-121 = 0.025436533614993095

Training epoch-7 batch-122
Running loss of epoch-7 batch-122 = 0.030402418226003647

Training epoch-7 batch-123
Running loss of epoch-7 batch-123 = 0.02091168612241745

Training epoch-7 batch-124
Running loss of epoch-7 batch-124 = 0.022556347772479057

Training epoch-7 batch-125
Running loss of epoch-7 batch-125 = 0.023908298462629318

Training epoch-7 batch-126
Running loss of epoch-7 batch-126 = 0.02536703273653984

Training epoch-7 batch-127
Running loss of epoch-7 batch-127 = 0.0245653223246336

Training epoch-7 batch-128
Running loss of epoch-7 batch-128 = 0.02714385837316513

Training epoch-7 batch-129
Running loss of epoch-7 batch-129 = 0.02124147303402424

Training epoch-7 batch-130
Running loss of epoch-7 batch-130 = 0.02425343357026577

Training epoch-7 batch-131
Running loss of epoch-7 batch-131 = 0.02635849453508854

Training epoch-7 batch-132
Running loss of epoch-7 batch-132 = 0.023801680654287338

Training epoch-7 batch-133
Running loss of epoch-7 batch-133 = 0.024799006059765816

Training epoch-7 batch-134
Running loss of epoch-7 batch-134 = 0.02598002925515175

Training epoch-7 batch-135
Running loss of epoch-7 batch-135 = 0.024675052613019943

Training epoch-7 batch-136
Running loss of epoch-7 batch-136 = 0.0237246323376894

Training epoch-7 batch-137
Running loss of epoch-7 batch-137 = 0.023369306698441505

Training epoch-7 batch-138
Running loss of epoch-7 batch-138 = 0.022376518696546555

Training epoch-7 batch-139
Running loss of epoch-7 batch-139 = 0.023926103487610817

Training epoch-7 batch-140
Running loss of epoch-7 batch-140 = 0.02853328362107277

Training epoch-7 batch-141
Running loss of epoch-7 batch-141 = 0.023080937564373016

Training epoch-7 batch-142
Running loss of epoch-7 batch-142 = 0.025868980213999748

Training epoch-7 batch-143
Running loss of epoch-7 batch-143 = 0.028280453756451607

Training epoch-7 batch-144
Running loss of epoch-7 batch-144 = 0.024278903380036354

Training epoch-7 batch-145
Running loss of epoch-7 batch-145 = 0.024232814088463783

Training epoch-7 batch-146
Running loss of epoch-7 batch-146 = 0.02338777855038643

Training epoch-7 batch-147
Running loss of epoch-7 batch-147 = 0.02157452702522278

Training epoch-7 batch-148
Running loss of epoch-7 batch-148 = 0.02580023929476738

Training epoch-7 batch-149
Running loss of epoch-7 batch-149 = 0.0230544563382864

Training epoch-7 batch-150
Running loss of epoch-7 batch-150 = 0.02386097051203251

Training epoch-7 batch-151
Running loss of epoch-7 batch-151 = 0.023290667682886124

Training epoch-7 batch-152
Running loss of epoch-7 batch-152 = 0.022766483947634697

Training epoch-7 batch-153
Running loss of epoch-7 batch-153 = 0.022674864158034325

Training epoch-7 batch-154
Running loss of epoch-7 batch-154 = 0.024505039677023888

Training epoch-7 batch-155
Running loss of epoch-7 batch-155 = 0.026354815810918808

Training epoch-7 batch-156
Running loss of epoch-7 batch-156 = 0.024757765233516693

Training epoch-7 batch-157
Running loss of epoch-7 batch-157 = 0.0887647420167923

Finished training epoch-7.



Average train loss at epoch-7 = 0.024920537984371185

Started Evaluation

Average val loss at epoch-7 = 1.6679694840782566

Accuracy for classes:
Accuracy for class equals is: 63.20 %
Accuracy for class main is: 36.07 %
Accuracy for class setUp is: 44.75 %
Accuracy for class onCreate is: 51.92 %
Accuracy for class toString is: 40.61 %
Accuracy for class run is: 34.70 %
Accuracy for class hashCode is: 80.15 %
Accuracy for class init is: 6.73 %
Accuracy for class execute is: 38.55 %
Accuracy for class get is: 21.03 %

Overall Accuracy = 42.42 %


Best Accuracy = 42.42 % at Epoch-7
Saving model after best epoch-7

Finished Evaluation



Started training epoch-8


Training epoch-8 batch-1
Running loss of epoch-8 batch-1 = 0.020659523084759712

Training epoch-8 batch-2
Running loss of epoch-8 batch-2 = 0.01980510726571083

Training epoch-8 batch-3
Running loss of epoch-8 batch-3 = 0.02589944563806057

Training epoch-8 batch-4
Running loss of epoch-8 batch-4 = 0.024190178140997887

Training epoch-8 batch-5
Running loss of epoch-8 batch-5 = 0.021894671022892

Training epoch-8 batch-6
Running loss of epoch-8 batch-6 = 0.021195435896515846

Training epoch-8 batch-7
Running loss of epoch-8 batch-7 = 0.01914498396217823

Training epoch-8 batch-8
Running loss of epoch-8 batch-8 = 0.02218622714281082

Training epoch-8 batch-9
Running loss of epoch-8 batch-9 = 0.018627706915140152

Training epoch-8 batch-10
Running loss of epoch-8 batch-10 = 0.024539930745959282

Training epoch-8 batch-11
Running loss of epoch-8 batch-11 = 0.02027328684926033

Training epoch-8 batch-12
Running loss of epoch-8 batch-12 = 0.022091038525104523

Training epoch-8 batch-13
Running loss of epoch-8 batch-13 = 0.021973976865410805

Training epoch-8 batch-14
Running loss of epoch-8 batch-14 = 0.019698522984981537

Training epoch-8 batch-15
Running loss of epoch-8 batch-15 = 0.020693447440862656

Training epoch-8 batch-16
Running loss of epoch-8 batch-16 = 0.023413140326738358

Training epoch-8 batch-17
Running loss of epoch-8 batch-17 = 0.019294677302241325

Training epoch-8 batch-18
Running loss of epoch-8 batch-18 = 0.020314160734415054

Training epoch-8 batch-19
Running loss of epoch-8 batch-19 = 0.02009071409702301

Training epoch-8 batch-20
Running loss of epoch-8 batch-20 = 0.021335437893867493

Training epoch-8 batch-21
Running loss of epoch-8 batch-21 = 0.022942278534173965

Training epoch-8 batch-22
Running loss of epoch-8 batch-22 = 0.019846566021442413

Training epoch-8 batch-23
Running loss of epoch-8 batch-23 = 0.018052181228995323

Training epoch-8 batch-24
Running loss of epoch-8 batch-24 = 0.023081794381141663

Training epoch-8 batch-25
Running loss of epoch-8 batch-25 = 0.022682536393404007

Training epoch-8 batch-26
Running loss of epoch-8 batch-26 = 0.021340858191251755

Training epoch-8 batch-27
Running loss of epoch-8 batch-27 = 0.0239908117800951

Training epoch-8 batch-28
Running loss of epoch-8 batch-28 = 0.021132761612534523

Training epoch-8 batch-29
Running loss of epoch-8 batch-29 = 0.017799291759729385

Training epoch-8 batch-30
Running loss of epoch-8 batch-30 = 0.019838856533169746

Training epoch-8 batch-31
Running loss of epoch-8 batch-31 = 0.02013314701616764

Training epoch-8 batch-32
Running loss of epoch-8 batch-32 = 0.017778486013412476

Training epoch-8 batch-33
Running loss of epoch-8 batch-33 = 0.021925415843725204

Training epoch-8 batch-34
Running loss of epoch-8 batch-34 = 0.021105464547872543

Training epoch-8 batch-35
Running loss of epoch-8 batch-35 = 0.019975662231445312

Training epoch-8 batch-36
Running loss of epoch-8 batch-36 = 0.019466163590550423

Training epoch-8 batch-37
Running loss of epoch-8 batch-37 = 0.021323474124073982

Training epoch-8 batch-38
Running loss of epoch-8 batch-38 = 0.017476841807365417

Training epoch-8 batch-39
Running loss of epoch-8 batch-39 = 0.020394958555698395

Training epoch-8 batch-40
Running loss of epoch-8 batch-40 = 0.025755448266863823

Training epoch-8 batch-41
Running loss of epoch-8 batch-41 = 0.019430704414844513

Training epoch-8 batch-42
Running loss of epoch-8 batch-42 = 0.021480830386281013

Training epoch-8 batch-43
Running loss of epoch-8 batch-43 = 0.016130510717630386

Training epoch-8 batch-44
Running loss of epoch-8 batch-44 = 0.017985278740525246

Training epoch-8 batch-45
Running loss of epoch-8 batch-45 = 0.020952312275767326

Training epoch-8 batch-46
Running loss of epoch-8 batch-46 = 0.021847615018486977

Training epoch-8 batch-47
Running loss of epoch-8 batch-47 = 0.022536983713507652

Training epoch-8 batch-48
Running loss of epoch-8 batch-48 = 0.02297910302877426

Training epoch-8 batch-49
Running loss of epoch-8 batch-49 = 0.020022444427013397

Training epoch-8 batch-50
Running loss of epoch-8 batch-50 = 0.019974229857325554

Training epoch-8 batch-51
Running loss of epoch-8 batch-51 = 0.0246250219643116

Training epoch-8 batch-52
Running loss of epoch-8 batch-52 = 0.024531429633498192

Training epoch-8 batch-53
Running loss of epoch-8 batch-53 = 0.02257794514298439

Training epoch-8 batch-54
Running loss of epoch-8 batch-54 = 0.02192331850528717

Training epoch-8 batch-55
Running loss of epoch-8 batch-55 = 0.023074708878993988

Training epoch-8 batch-56
Running loss of epoch-8 batch-56 = 0.017096778377890587

Training epoch-8 batch-57
Running loss of epoch-8 batch-57 = 0.02005334012210369

Training epoch-8 batch-58
Running loss of epoch-8 batch-58 = 0.019294045865535736

Training epoch-8 batch-59
Running loss of epoch-8 batch-59 = 0.019953442737460136

Training epoch-8 batch-60
Running loss of epoch-8 batch-60 = 0.019991165027022362

Training epoch-8 batch-61
Running loss of epoch-8 batch-61 = 0.02254844643175602

Training epoch-8 batch-62
Running loss of epoch-8 batch-62 = 0.0231039896607399

Training epoch-8 batch-63
Running loss of epoch-8 batch-63 = 0.01917162723839283

Training epoch-8 batch-64
Running loss of epoch-8 batch-64 = 0.02148997038602829

Training epoch-8 batch-65
Running loss of epoch-8 batch-65 = 0.01853143237531185

Training epoch-8 batch-66
Running loss of epoch-8 batch-66 = 0.022682711482048035

Training epoch-8 batch-67
Running loss of epoch-8 batch-67 = 0.02232191525399685

Training epoch-8 batch-68
Running loss of epoch-8 batch-68 = 0.020824464038014412

Training epoch-8 batch-69
Running loss of epoch-8 batch-69 = 0.021368255838751793

Training epoch-8 batch-70
Running loss of epoch-8 batch-70 = 0.022687921300530434

Training epoch-8 batch-71
Running loss of epoch-8 batch-71 = 0.020583374425768852

Training epoch-8 batch-72
Running loss of epoch-8 batch-72 = 0.01813366636633873

Training epoch-8 batch-73
Running loss of epoch-8 batch-73 = 0.020945094525814056

Training epoch-8 batch-74
Running loss of epoch-8 batch-74 = 0.01894211769104004

Training epoch-8 batch-75
Running loss of epoch-8 batch-75 = 0.020992890000343323

Training epoch-8 batch-76
Running loss of epoch-8 batch-76 = 0.022502562031149864

Training epoch-8 batch-77
Running loss of epoch-8 batch-77 = 0.01987786591053009

Training epoch-8 batch-78
Running loss of epoch-8 batch-78 = 0.02184854820370674

Training epoch-8 batch-79
Running loss of epoch-8 batch-79 = 0.01905580423772335

Training epoch-8 batch-80
Running loss of epoch-8 batch-80 = 0.023149969056248665

Training epoch-8 batch-81
Running loss of epoch-8 batch-81 = 0.019970543682575226

Training epoch-8 batch-82
Running loss of epoch-8 batch-82 = 0.02242334559559822

Training epoch-8 batch-83
Running loss of epoch-8 batch-83 = 0.01919201761484146

Training epoch-8 batch-84
Running loss of epoch-8 batch-84 = 0.017613541334867477

Training epoch-8 batch-85
Running loss of epoch-8 batch-85 = 0.025874806568026543

Training epoch-8 batch-86
Running loss of epoch-8 batch-86 = 0.02073761448264122

Training epoch-8 batch-87
Running loss of epoch-8 batch-87 = 0.019669149070978165

Training epoch-8 batch-88
Running loss of epoch-8 batch-88 = 0.02048305608332157

Training epoch-8 batch-89
Running loss of epoch-8 batch-89 = 0.02345610223710537

Training epoch-8 batch-90
Running loss of epoch-8 batch-90 = 0.01871037483215332

Training epoch-8 batch-91
Running loss of epoch-8 batch-91 = 0.02195453643798828

Training epoch-8 batch-92
Running loss of epoch-8 batch-92 = 0.02001650258898735

Training epoch-8 batch-93
Running loss of epoch-8 batch-93 = 0.019495105370879173

Training epoch-8 batch-94
Running loss of epoch-8 batch-94 = 0.020673586055636406

Training epoch-8 batch-95
Running loss of epoch-8 batch-95 = 0.024388594552874565

Training epoch-8 batch-96
Running loss of epoch-8 batch-96 = 0.01757153496146202

Training epoch-8 batch-97
Running loss of epoch-8 batch-97 = 0.018895883113145828

Training epoch-8 batch-98
Running loss of epoch-8 batch-98 = 0.019413761794567108

Training epoch-8 batch-99
Running loss of epoch-8 batch-99 = 0.02433786727488041

Training epoch-8 batch-100
Running loss of epoch-8 batch-100 = 0.02285764180123806

Training epoch-8 batch-101
Running loss of epoch-8 batch-101 = 0.018734300509095192

Training epoch-8 batch-102
Running loss of epoch-8 batch-102 = 0.02102500945329666

Training epoch-8 batch-103
Running loss of epoch-8 batch-103 = 0.02295072190463543

Training epoch-8 batch-104
Running loss of epoch-8 batch-104 = 0.023170290514826775

Training epoch-8 batch-105
Running loss of epoch-8 batch-105 = 0.016069427132606506

Training epoch-8 batch-106
Running loss of epoch-8 batch-106 = 0.01998858153820038

Training epoch-8 batch-107
Running loss of epoch-8 batch-107 = 0.022562142461538315

Training epoch-8 batch-108
Running loss of epoch-8 batch-108 = 0.02075093798339367

Training epoch-8 batch-109
Running loss of epoch-8 batch-109 = 0.020689213648438454

Training epoch-8 batch-110
Running loss of epoch-8 batch-110 = 0.02322525903582573

Training epoch-8 batch-111
Running loss of epoch-8 batch-111 = 0.022194983437657356

Training epoch-8 batch-112
Running loss of epoch-8 batch-112 = 0.021907366812229156

Training epoch-8 batch-113
Running loss of epoch-8 batch-113 = 0.020833313465118408

Training epoch-8 batch-114
Running loss of epoch-8 batch-114 = 0.02017243579030037

Training epoch-8 batch-115
Running loss of epoch-8 batch-115 = 0.021312613040208817

Training epoch-8 batch-116
Running loss of epoch-8 batch-116 = 0.020404163748025894

Training epoch-8 batch-117
Running loss of epoch-8 batch-117 = 0.02073228731751442

Training epoch-8 batch-118
Running loss of epoch-8 batch-118 = 0.02084347791969776

Training epoch-8 batch-119
Running loss of epoch-8 batch-119 = 0.023475579917430878

Training epoch-8 batch-120
Running loss of epoch-8 batch-120 = 0.018955938518047333

Training epoch-8 batch-121
Running loss of epoch-8 batch-121 = 0.019320251420140266

Training epoch-8 batch-122
Running loss of epoch-8 batch-122 = 0.02073669619858265

Training epoch-8 batch-123
Running loss of epoch-8 batch-123 = 0.02074476331472397

Training epoch-8 batch-124
Running loss of epoch-8 batch-124 = 0.017884403467178345

Training epoch-8 batch-125
Running loss of epoch-8 batch-125 = 0.0188023429363966

Training epoch-8 batch-126
Running loss of epoch-8 batch-126 = 0.020365769043564796

Training epoch-8 batch-127
Running loss of epoch-8 batch-127 = 0.019399594515562057

Training epoch-8 batch-128
Running loss of epoch-8 batch-128 = 0.016993563622236252

Training epoch-8 batch-129
Running loss of epoch-8 batch-129 = 0.02011367864906788

Training epoch-8 batch-130
Running loss of epoch-8 batch-130 = 0.017824314534664154

Training epoch-8 batch-131
Running loss of epoch-8 batch-131 = 0.019694721326231956

Training epoch-8 batch-132
Running loss of epoch-8 batch-132 = 0.022240759804844856

Training epoch-8 batch-133
Running loss of epoch-8 batch-133 = 0.019436608999967575

Training epoch-8 batch-134
Running loss of epoch-8 batch-134 = 0.02341369353234768

Training epoch-8 batch-135
Running loss of epoch-8 batch-135 = 0.024665184319019318

Training epoch-8 batch-136
Running loss of epoch-8 batch-136 = 0.0196615569293499

Training epoch-8 batch-137
Running loss of epoch-8 batch-137 = 0.019765544682741165

Training epoch-8 batch-138
Running loss of epoch-8 batch-138 = 0.022421259433031082

Training epoch-8 batch-139
Running loss of epoch-8 batch-139 = 0.019800888374447823

Training epoch-8 batch-140
Running loss of epoch-8 batch-140 = 0.01911460980772972

Training epoch-8 batch-141
Running loss of epoch-8 batch-141 = 0.022661730647087097

Training epoch-8 batch-142
Running loss of epoch-8 batch-142 = 0.020945977419614792

Training epoch-8 batch-143
Running loss of epoch-8 batch-143 = 0.023082979023456573

Training epoch-8 batch-144
Running loss of epoch-8 batch-144 = 0.023156611248850822

Training epoch-8 batch-145
Running loss of epoch-8 batch-145 = 0.016574058681726456

Training epoch-8 batch-146
Running loss of epoch-8 batch-146 = 0.024747129529714584

Training epoch-8 batch-147
Running loss of epoch-8 batch-147 = 0.019325213506817818

Training epoch-8 batch-148
Running loss of epoch-8 batch-148 = 0.01873430423438549

Training epoch-8 batch-149
Running loss of epoch-8 batch-149 = 0.02191501297056675

Training epoch-8 batch-150
Running loss of epoch-8 batch-150 = 0.017191726714372635

Training epoch-8 batch-151
Running loss of epoch-8 batch-151 = 0.018904149532318115

Training epoch-8 batch-152
Running loss of epoch-8 batch-152 = 0.019509758800268173

Training epoch-8 batch-153
Running loss of epoch-8 batch-153 = 0.018025217577815056

Training epoch-8 batch-154
Running loss of epoch-8 batch-154 = 0.024380097165703773

Training epoch-8 batch-155
Running loss of epoch-8 batch-155 = 0.019326912239193916

Training epoch-8 batch-156
Running loss of epoch-8 batch-156 = 0.01950915902853012

Training epoch-8 batch-157
Running loss of epoch-8 batch-157 = 0.07178407162427902

Finished training epoch-8.



Average train loss at epoch-8 = 0.02090661323070526

Started Evaluation

Average val loss at epoch-8 = 1.637758795368044

Accuracy for classes:
Accuracy for class equals is: 60.07 %
Accuracy for class main is: 51.15 %
Accuracy for class setUp is: 62.13 %
Accuracy for class onCreate is: 43.18 %
Accuracy for class toString is: 49.49 %
Accuracy for class run is: 31.28 %
Accuracy for class hashCode is: 76.40 %
Accuracy for class init is: 2.02 %
Accuracy for class execute is: 40.56 %
Accuracy for class get is: 13.85 %

Overall Accuracy = 43.53 %


Best Accuracy = 43.53 % at Epoch-8
Saving model after best epoch-8

Finished Evaluation



Started training epoch-9


Training epoch-9 batch-1
Running loss of epoch-9 batch-1 = 0.015332359820604324

Training epoch-9 batch-2
Running loss of epoch-9 batch-2 = 0.016723066568374634

Training epoch-9 batch-3
Running loss of epoch-9 batch-3 = 0.01693650893867016

Training epoch-9 batch-4
Running loss of epoch-9 batch-4 = 0.016755713149905205

Training epoch-9 batch-5
Running loss of epoch-9 batch-5 = 0.015485519543290138

Training epoch-9 batch-6
Running loss of epoch-9 batch-6 = 0.01681540720164776

Training epoch-9 batch-7
Running loss of epoch-9 batch-7 = 0.016452239826321602

Training epoch-9 batch-8
Running loss of epoch-9 batch-8 = 0.016789846122264862

Training epoch-9 batch-9
Running loss of epoch-9 batch-9 = 0.017234820872545242

Training epoch-9 batch-10
Running loss of epoch-9 batch-10 = 0.017379608005285263

Training epoch-9 batch-11
Running loss of epoch-9 batch-11 = 0.015700628980994225

Training epoch-9 batch-12
Running loss of epoch-9 batch-12 = 0.014953411184251308

Training epoch-9 batch-13
Running loss of epoch-9 batch-13 = 0.012490198947489262

Training epoch-9 batch-14
Running loss of epoch-9 batch-14 = 0.015313471667468548

Training epoch-9 batch-15
Running loss of epoch-9 batch-15 = 0.01670076511800289

Training epoch-9 batch-16
Running loss of epoch-9 batch-16 = 0.015255068428814411

Training epoch-9 batch-17
Running loss of epoch-9 batch-17 = 0.016141004860401154

Training epoch-9 batch-18
Running loss of epoch-9 batch-18 = 0.01675751432776451

Training epoch-9 batch-19
Running loss of epoch-9 batch-19 = 0.012423722073435783

Training epoch-9 batch-20
Running loss of epoch-9 batch-20 = 0.017401741817593575

Training epoch-9 batch-21
Running loss of epoch-9 batch-21 = 0.016275782138109207

Training epoch-9 batch-22
Running loss of epoch-9 batch-22 = 0.016392257064580917

Training epoch-9 batch-23
Running loss of epoch-9 batch-23 = 0.013217931613326073

Training epoch-9 batch-24
Running loss of epoch-9 batch-24 = 0.014708241447806358

Training epoch-9 batch-25
Running loss of epoch-9 batch-25 = 0.015141905285418034

Training epoch-9 batch-26
Running loss of epoch-9 batch-26 = 0.017492147162556648

Training epoch-9 batch-27
Running loss of epoch-9 batch-27 = 0.01496244315057993

Training epoch-9 batch-28
Running loss of epoch-9 batch-28 = 0.01534332800656557

Training epoch-9 batch-29
Running loss of epoch-9 batch-29 = 0.013503124006092548

Training epoch-9 batch-30
Running loss of epoch-9 batch-30 = 0.01654021255671978

Training epoch-9 batch-31
Running loss of epoch-9 batch-31 = 0.013824700377881527

Training epoch-9 batch-32
Running loss of epoch-9 batch-32 = 0.014119156636297703

Training epoch-9 batch-33
Running loss of epoch-9 batch-33 = 0.01583453081548214

Training epoch-9 batch-34
Running loss of epoch-9 batch-34 = 0.018699977546930313

Training epoch-9 batch-35
Running loss of epoch-9 batch-35 = 0.014814051799476147

Training epoch-9 batch-36
Running loss of epoch-9 batch-36 = 0.016451150178909302

Training epoch-9 batch-37
Running loss of epoch-9 batch-37 = 0.018022803589701653

Training epoch-9 batch-38
Running loss of epoch-9 batch-38 = 0.014843390323221684

Training epoch-9 batch-39
Running loss of epoch-9 batch-39 = 0.013479774817824364

Training epoch-9 batch-40
Running loss of epoch-9 batch-40 = 0.015873635187745094

Training epoch-9 batch-41
Running loss of epoch-9 batch-41 = 0.01227389182895422

Training epoch-9 batch-42
Running loss of epoch-9 batch-42 = 0.014586597681045532

Training epoch-9 batch-43
Running loss of epoch-9 batch-43 = 0.01628740131855011

Training epoch-9 batch-44
Running loss of epoch-9 batch-44 = 0.01352772954851389

Training epoch-9 batch-45
Running loss of epoch-9 batch-45 = 0.01613769307732582

Training epoch-9 batch-46
Running loss of epoch-9 batch-46 = 0.01774902641773224

Training epoch-9 batch-47
Running loss of epoch-9 batch-47 = 0.0119185084477067

Training epoch-9 batch-48
Running loss of epoch-9 batch-48 = 0.016979865729808807

Training epoch-9 batch-49
Running loss of epoch-9 batch-49 = 0.011324352584779263

Training epoch-9 batch-50
Running loss of epoch-9 batch-50 = 0.016396768391132355

Training epoch-9 batch-51
Running loss of epoch-9 batch-51 = 0.013904374092817307

Training epoch-9 batch-52
Running loss of epoch-9 batch-52 = 0.013295994140207767

Training epoch-9 batch-53
Running loss of epoch-9 batch-53 = 0.01665576733648777

Training epoch-9 batch-54
Running loss of epoch-9 batch-54 = 0.0166516974568367

Training epoch-9 batch-55
Running loss of epoch-9 batch-55 = 0.014799891039729118

Training epoch-9 batch-56
Running loss of epoch-9 batch-56 = 0.015371065586805344

Training epoch-9 batch-57
Running loss of epoch-9 batch-57 = 0.015013650059700012

Training epoch-9 batch-58
Running loss of epoch-9 batch-58 = 0.01638021692633629

Training epoch-9 batch-59
Running loss of epoch-9 batch-59 = 0.013066999614238739

Training epoch-9 batch-60
Running loss of epoch-9 batch-60 = 0.016252882778644562

Training epoch-9 batch-61
Running loss of epoch-9 batch-61 = 0.013444127514958382

Training epoch-9 batch-62
Running loss of epoch-9 batch-62 = 0.017650188878178596

Training epoch-9 batch-63
Running loss of epoch-9 batch-63 = 0.014753777533769608

Training epoch-9 batch-64
Running loss of epoch-9 batch-64 = 0.012515490874648094

Training epoch-9 batch-65
Running loss of epoch-9 batch-65 = 0.015060162171721458

Training epoch-9 batch-66
Running loss of epoch-9 batch-66 = 0.01565253548324108

Training epoch-9 batch-67
Running loss of epoch-9 batch-67 = 0.01660199463367462

Training epoch-9 batch-68
Running loss of epoch-9 batch-68 = 0.016048813238739967

Training epoch-9 batch-69
Running loss of epoch-9 batch-69 = 0.01632477343082428

Training epoch-9 batch-70
Running loss of epoch-9 batch-70 = 0.015778664499521255

Training epoch-9 batch-71
Running loss of epoch-9 batch-71 = 0.015358511358499527

Training epoch-9 batch-72
Running loss of epoch-9 batch-72 = 0.017831992357969284

Training epoch-9 batch-73
Running loss of epoch-9 batch-73 = 0.019101331010460854

Training epoch-9 batch-74
Running loss of epoch-9 batch-74 = 0.014980573207139969

Training epoch-9 batch-75
Running loss of epoch-9 batch-75 = 0.015248489566147327

Training epoch-9 batch-76
Running loss of epoch-9 batch-76 = 0.013099953532218933

Training epoch-9 batch-77
Running loss of epoch-9 batch-77 = 0.016087178140878677

Training epoch-9 batch-78
Running loss of epoch-9 batch-78 = 0.01992243528366089

Training epoch-9 batch-79
Running loss of epoch-9 batch-79 = 0.013090761378407478

Training epoch-9 batch-80
Running loss of epoch-9 batch-80 = 0.016229940578341484

Training epoch-9 batch-81
Running loss of epoch-9 batch-81 = 0.01615399867296219

Training epoch-9 batch-82
Running loss of epoch-9 batch-82 = 0.013827997259795666

Training epoch-9 batch-83
Running loss of epoch-9 batch-83 = 0.0159927811473608

Training epoch-9 batch-84
Running loss of epoch-9 batch-84 = 0.01581788808107376

Training epoch-9 batch-85
Running loss of epoch-9 batch-85 = 0.012666190043091774

Training epoch-9 batch-86
Running loss of epoch-9 batch-86 = 0.0157766230404377

Training epoch-9 batch-87
Running loss of epoch-9 batch-87 = 0.015084969811141491

Training epoch-9 batch-88
Running loss of epoch-9 batch-88 = 0.016781102865934372

Training epoch-9 batch-89
Running loss of epoch-9 batch-89 = 0.01327715814113617

Training epoch-9 batch-90
Running loss of epoch-9 batch-90 = 0.015621228143572807

Training epoch-9 batch-91
Running loss of epoch-9 batch-91 = 0.013632738962769508

Training epoch-9 batch-92
Running loss of epoch-9 batch-92 = 0.01325503084808588

Training epoch-9 batch-93
Running loss of epoch-9 batch-93 = 0.015241682529449463

Training epoch-9 batch-94
Running loss of epoch-9 batch-94 = 0.01507593784481287

Training epoch-9 batch-95
Running loss of epoch-9 batch-95 = 0.012861859053373337

Training epoch-9 batch-96
Running loss of epoch-9 batch-96 = 0.01384282112121582

Training epoch-9 batch-97
Running loss of epoch-9 batch-97 = 0.017542792484164238

Training epoch-9 batch-98
Running loss of epoch-9 batch-98 = 0.01484772376716137

Training epoch-9 batch-99
Running loss of epoch-9 batch-99 = 0.013578326441347599

Training epoch-9 batch-100
Running loss of epoch-9 batch-100 = 0.01965019479393959

Training epoch-9 batch-101
Running loss of epoch-9 batch-101 = 0.013502012006938457

Training epoch-9 batch-102
Running loss of epoch-9 batch-102 = 0.015007290057837963

Training epoch-9 batch-103
Running loss of epoch-9 batch-103 = 0.015833307057619095

Training epoch-9 batch-104
Running loss of epoch-9 batch-104 = 0.016193244606256485

Training epoch-9 batch-105
Running loss of epoch-9 batch-105 = 0.01610068790614605

Training epoch-9 batch-106
Running loss of epoch-9 batch-106 = 0.015514235943555832

Training epoch-9 batch-107
Running loss of epoch-9 batch-107 = 0.015294557437300682

Training epoch-9 batch-108
Running loss of epoch-9 batch-108 = 0.016757458448410034

Training epoch-9 batch-109
Running loss of epoch-9 batch-109 = 0.016172314062714577

Training epoch-9 batch-110
Running loss of epoch-9 batch-110 = 0.013802845031023026

Training epoch-9 batch-111
Running loss of epoch-9 batch-111 = 0.02073098160326481

Training epoch-9 batch-112
Running loss of epoch-9 batch-112 = 0.01875912956893444

Training epoch-9 batch-113
Running loss of epoch-9 batch-113 = 0.01467089168727398

Training epoch-9 batch-114
Running loss of epoch-9 batch-114 = 0.017006700858473778

Training epoch-9 batch-115
Running loss of epoch-9 batch-115 = 0.01584305800497532

Training epoch-9 batch-116
Running loss of epoch-9 batch-116 = 0.015593595802783966

Training epoch-9 batch-117
Running loss of epoch-9 batch-117 = 0.01643482968211174

Training epoch-9 batch-118
Running loss of epoch-9 batch-118 = 0.01465214416384697

Training epoch-9 batch-119
Running loss of epoch-9 batch-119 = 0.013819556683301926

Training epoch-9 batch-120
Running loss of epoch-9 batch-120 = 0.01428417582064867

Training epoch-9 batch-121
Running loss of epoch-9 batch-121 = 0.013283510692417622

Training epoch-9 batch-122
Running loss of epoch-9 batch-122 = 0.015929508954286575

Training epoch-9 batch-123
Running loss of epoch-9 batch-123 = 0.016509849578142166

Training epoch-9 batch-124
Running loss of epoch-9 batch-124 = 0.016110369935631752

Training epoch-9 batch-125
Running loss of epoch-9 batch-125 = 0.012312790378928185

Training epoch-9 batch-126
Running loss of epoch-9 batch-126 = 0.016410067677497864

Training epoch-9 batch-127
Running loss of epoch-9 batch-127 = 0.016051121056079865

Training epoch-9 batch-128
Running loss of epoch-9 batch-128 = 0.013568337075412273

Training epoch-9 batch-129
Running loss of epoch-9 batch-129 = 0.018468046560883522

Training epoch-9 batch-130
Running loss of epoch-9 batch-130 = 0.01298896037042141

Training epoch-9 batch-131
Running loss of epoch-9 batch-131 = 0.01267455704510212

Training epoch-9 batch-132
Running loss of epoch-9 batch-132 = 0.015434474684298038

Training epoch-9 batch-133
Running loss of epoch-9 batch-133 = 0.01719747483730316

Training epoch-9 batch-134
Running loss of epoch-9 batch-134 = 0.014307031407952309

Training epoch-9 batch-135
Running loss of epoch-9 batch-135 = 0.016950219869613647

Training epoch-9 batch-136
Running loss of epoch-9 batch-136 = 0.014388949610292912

Training epoch-9 batch-137
Running loss of epoch-9 batch-137 = 0.014143368229269981

Training epoch-9 batch-138
Running loss of epoch-9 batch-138 = 0.015303111635148525

Training epoch-9 batch-139
Running loss of epoch-9 batch-139 = 0.015009107068181038

Training epoch-9 batch-140
Running loss of epoch-9 batch-140 = 0.011923741549253464

Training epoch-9 batch-141
Running loss of epoch-9 batch-141 = 0.015836823731660843

Training epoch-9 batch-142
Running loss of epoch-9 batch-142 = 0.013812771067023277

Training epoch-9 batch-143
Running loss of epoch-9 batch-143 = 0.012038362212479115

Training epoch-9 batch-144
Running loss of epoch-9 batch-144 = 0.017571672797203064

Training epoch-9 batch-145
Running loss of epoch-9 batch-145 = 0.014449226669967175

Training epoch-9 batch-146
Running loss of epoch-9 batch-146 = 0.013828106224536896

Training epoch-9 batch-147
Running loss of epoch-9 batch-147 = 0.015253555960953236

Training epoch-9 batch-148
Running loss of epoch-9 batch-148 = 0.014816813170909882

Training epoch-9 batch-149
Running loss of epoch-9 batch-149 = 0.015269408002495766

Training epoch-9 batch-150
Running loss of epoch-9 batch-150 = 0.017047861590981483

Training epoch-9 batch-151
Running loss of epoch-9 batch-151 = 0.014312240295112133

Training epoch-9 batch-152
Running loss of epoch-9 batch-152 = 0.017132911831140518

Training epoch-9 batch-153
Running loss of epoch-9 batch-153 = 0.01614910550415516

Training epoch-9 batch-154
Running loss of epoch-9 batch-154 = 0.017205610871315002

Training epoch-9 batch-155
Running loss of epoch-9 batch-155 = 0.013057438656687737

Training epoch-9 batch-156
Running loss of epoch-9 batch-156 = 0.016437696292996407

Training epoch-9 batch-157
Running loss of epoch-9 batch-157 = 0.0576791949570179

Finished training epoch-9.



Average train loss at epoch-9 = 0.015462689876556397

Started Evaluation

Average val loss at epoch-9 = 1.8817853582532782

Accuracy for classes:
Accuracy for class equals is: 72.44 %
Accuracy for class main is: 67.05 %
Accuracy for class setUp is: 67.87 %
Accuracy for class onCreate is: 20.15 %
Accuracy for class toString is: 25.26 %
Accuracy for class run is: 22.15 %
Accuracy for class hashCode is: 74.91 %
Accuracy for class init is: 17.94 %
Accuracy for class execute is: 30.12 %
Accuracy for class get is: 21.03 %

Overall Accuracy = 42.48 %

Finished Evaluation



Started training epoch-10


Training epoch-10 batch-1
Running loss of epoch-10 batch-1 = 0.008291034959256649

Training epoch-10 batch-2
Running loss of epoch-10 batch-2 = 0.012460630387067795

Training epoch-10 batch-3
Running loss of epoch-10 batch-3 = 0.013291634619235992

Training epoch-10 batch-4
Running loss of epoch-10 batch-4 = 0.007504034787416458

Training epoch-10 batch-5
Running loss of epoch-10 batch-5 = 0.010713397525250912

Training epoch-10 batch-6
Running loss of epoch-10 batch-6 = 0.012049700133502483

Training epoch-10 batch-7
Running loss of epoch-10 batch-7 = 0.012172676622867584

Training epoch-10 batch-8
Running loss of epoch-10 batch-8 = 0.01076461747288704

Training epoch-10 batch-9
Running loss of epoch-10 batch-9 = 0.0146322725340724

Training epoch-10 batch-10
Running loss of epoch-10 batch-10 = 0.007704243063926697

Training epoch-10 batch-11
Running loss of epoch-10 batch-11 = 0.009026560932397842

Training epoch-10 batch-12
Running loss of epoch-10 batch-12 = 0.010561819188296795

Training epoch-10 batch-13
Running loss of epoch-10 batch-13 = 0.008365427143871784

Training epoch-10 batch-14
Running loss of epoch-10 batch-14 = 0.011598033830523491

Training epoch-10 batch-15
Running loss of epoch-10 batch-15 = 0.00955490954220295

Training epoch-10 batch-16
Running loss of epoch-10 batch-16 = 0.009029034525156021

Training epoch-10 batch-17
Running loss of epoch-10 batch-17 = 0.010440651327371597

Training epoch-10 batch-18
Running loss of epoch-10 batch-18 = 0.00945018045604229

Training epoch-10 batch-19
Running loss of epoch-10 batch-19 = 0.009653541259467602

Training epoch-10 batch-20
Running loss of epoch-10 batch-20 = 0.012255671434104443

Training epoch-10 batch-21
Running loss of epoch-10 batch-21 = 0.010468420572578907

Training epoch-10 batch-22
Running loss of epoch-10 batch-22 = 0.010083910077810287

Training epoch-10 batch-23
Running loss of epoch-10 batch-23 = 0.010746212676167488

Training epoch-10 batch-24
Running loss of epoch-10 batch-24 = 0.0074640619568526745

Training epoch-10 batch-25
Running loss of epoch-10 batch-25 = 0.010219518095254898

Training epoch-10 batch-26
Running loss of epoch-10 batch-26 = 0.007403647992759943

Training epoch-10 batch-27
Running loss of epoch-10 batch-27 = 0.008462381549179554

Training epoch-10 batch-28
Running loss of epoch-10 batch-28 = 0.010369818657636642

Training epoch-10 batch-29
Running loss of epoch-10 batch-29 = 0.008682316169142723

Training epoch-10 batch-30
Running loss of epoch-10 batch-30 = 0.011828145012259483

Training epoch-10 batch-31
Running loss of epoch-10 batch-31 = 0.008704442530870438

Training epoch-10 batch-32
Running loss of epoch-10 batch-32 = 0.008091478608548641

Training epoch-10 batch-33
Running loss of epoch-10 batch-33 = 0.010712129063904285

Training epoch-10 batch-34
Running loss of epoch-10 batch-34 = 0.012600365094840527

Training epoch-10 batch-35
Running loss of epoch-10 batch-35 = 0.0093598747625947

Training epoch-10 batch-36
Running loss of epoch-10 batch-36 = 0.009021374396979809

Training epoch-10 batch-37
Running loss of epoch-10 batch-37 = 0.00970415212213993

Training epoch-10 batch-38
Running loss of epoch-10 batch-38 = 0.009889276698231697

Training epoch-10 batch-39
Running loss of epoch-10 batch-39 = 0.010449465364217758

Training epoch-10 batch-40
Running loss of epoch-10 batch-40 = 0.007085511926561594

Training epoch-10 batch-41
Running loss of epoch-10 batch-41 = 0.0090108597651124

Training epoch-10 batch-42
Running loss of epoch-10 batch-42 = 0.009707573801279068

Training epoch-10 batch-43
Running loss of epoch-10 batch-43 = 0.009521887637674809

Training epoch-10 batch-44
Running loss of epoch-10 batch-44 = 0.008568685501813889

Training epoch-10 batch-45
Running loss of epoch-10 batch-45 = 0.01058448851108551

Training epoch-10 batch-46
Running loss of epoch-10 batch-46 = 0.007829934358596802

Training epoch-10 batch-47
Running loss of epoch-10 batch-47 = 0.010165250860154629

Training epoch-10 batch-48
Running loss of epoch-10 batch-48 = 0.010704413056373596

Training epoch-10 batch-49
Running loss of epoch-10 batch-49 = 0.00768240075558424

Training epoch-10 batch-50
Running loss of epoch-10 batch-50 = 0.00912993960082531

Training epoch-10 batch-51
Running loss of epoch-10 batch-51 = 0.009287829510867596

Training epoch-10 batch-52
Running loss of epoch-10 batch-52 = 0.01222373265773058

Training epoch-10 batch-53
Running loss of epoch-10 batch-53 = 0.007478807587176561

Training epoch-10 batch-54
Running loss of epoch-10 batch-54 = 0.010404844768345356

Training epoch-10 batch-55
Running loss of epoch-10 batch-55 = 0.010302766226232052

Training epoch-10 batch-56
Running loss of epoch-10 batch-56 = 0.008407366462051868

Training epoch-10 batch-57
Running loss of epoch-10 batch-57 = 0.0071998704224824905

Training epoch-10 batch-58
Running loss of epoch-10 batch-58 = 0.011028781533241272

Training epoch-10 batch-59
Running loss of epoch-10 batch-59 = 0.012287585064768791

Training epoch-10 batch-60
Running loss of epoch-10 batch-60 = 0.009749221615493298

Training epoch-10 batch-61
Running loss of epoch-10 batch-61 = 0.00977517943829298

Training epoch-10 batch-62
Running loss of epoch-10 batch-62 = 0.009030303917825222

Training epoch-10 batch-63
Running loss of epoch-10 batch-63 = 0.010874825529754162

Training epoch-10 batch-64
Running loss of epoch-10 batch-64 = 0.011941127479076385

Training epoch-10 batch-65
Running loss of epoch-10 batch-65 = 0.010297557339072227

Training epoch-10 batch-66
Running loss of epoch-10 batch-66 = 0.010581097565591335

Training epoch-10 batch-67
Running loss of epoch-10 batch-67 = 0.008730791509151459

Training epoch-10 batch-68
Running loss of epoch-10 batch-68 = 0.009484466165304184

Training epoch-10 batch-69
Running loss of epoch-10 batch-69 = 0.009250249713659286

Training epoch-10 batch-70
Running loss of epoch-10 batch-70 = 0.01019732840359211

Training epoch-10 batch-71
Running loss of epoch-10 batch-71 = 0.008337005041539669

Training epoch-10 batch-72
Running loss of epoch-10 batch-72 = 0.009796908125281334

Training epoch-10 batch-73
Running loss of epoch-10 batch-73 = 0.010492206551134586

Training epoch-10 batch-74
Running loss of epoch-10 batch-74 = 0.011770161800086498

Training epoch-10 batch-75
Running loss of epoch-10 batch-75 = 0.014261769130825996

Training epoch-10 batch-76
Running loss of epoch-10 batch-76 = 0.008884027600288391

Training epoch-10 batch-77
Running loss of epoch-10 batch-77 = 0.007929055951535702

Training epoch-10 batch-78
Running loss of epoch-10 batch-78 = 0.010577932931482792

Training epoch-10 batch-79
Running loss of epoch-10 batch-79 = 0.011331518180668354

Training epoch-10 batch-80
Running loss of epoch-10 batch-80 = 0.009846776723861694

Training epoch-10 batch-81
Running loss of epoch-10 batch-81 = 0.00917017087340355

Training epoch-10 batch-82
Running loss of epoch-10 batch-82 = 0.010459412820637226

Training epoch-10 batch-83
Running loss of epoch-10 batch-83 = 0.0089583033695817

Training epoch-10 batch-84
Running loss of epoch-10 batch-84 = 0.011561810970306396

Training epoch-10 batch-85
Running loss of epoch-10 batch-85 = 0.011223198845982552

Training epoch-10 batch-86
Running loss of epoch-10 batch-86 = 0.008823486045002937

Training epoch-10 batch-87
Running loss of epoch-10 batch-87 = 0.006344793830066919

Training epoch-10 batch-88
Running loss of epoch-10 batch-88 = 0.005488137248903513

Training epoch-10 batch-89
Running loss of epoch-10 batch-89 = 0.01183790247887373

Training epoch-10 batch-90
Running loss of epoch-10 batch-90 = 0.009411938488483429

Training epoch-10 batch-91
Running loss of epoch-10 batch-91 = 0.011394604109227657

Training epoch-10 batch-92
Running loss of epoch-10 batch-92 = 0.008024143986403942

Training epoch-10 batch-93
Running loss of epoch-10 batch-93 = 0.009580843150615692

Training epoch-10 batch-94
Running loss of epoch-10 batch-94 = 0.010680336505174637

Training epoch-10 batch-95
Running loss of epoch-10 batch-95 = 0.00712787639349699

Training epoch-10 batch-96
Running loss of epoch-10 batch-96 = 0.008763563819229603

Training epoch-10 batch-97
Running loss of epoch-10 batch-97 = 0.01189318485558033

Training epoch-10 batch-98
Running loss of epoch-10 batch-98 = 0.009120551869273186

Training epoch-10 batch-99
Running loss of epoch-10 batch-99 = 0.012269926257431507

Training epoch-10 batch-100
Running loss of epoch-10 batch-100 = 0.010545453056693077

Training epoch-10 batch-101
Running loss of epoch-10 batch-101 = 0.009708406403660774

Training epoch-10 batch-102
Running loss of epoch-10 batch-102 = 0.007107059005647898

Training epoch-10 batch-103
Running loss of epoch-10 batch-103 = 0.008866085670888424

Training epoch-10 batch-104
Running loss of epoch-10 batch-104 = 0.011136270128190517

Training epoch-10 batch-105
Running loss of epoch-10 batch-105 = 0.008737522177398205

Training epoch-10 batch-106
Running loss of epoch-10 batch-106 = 0.009892703965306282

Training epoch-10 batch-107
Running loss of epoch-10 batch-107 = 0.011484039016067982

Training epoch-10 batch-108
Running loss of epoch-10 batch-108 = 0.009480401873588562

Training epoch-10 batch-109
Running loss of epoch-10 batch-109 = 0.010499737225472927

Training epoch-10 batch-110
Running loss of epoch-10 batch-110 = 0.008542231284081936

Training epoch-10 batch-111
Running loss of epoch-10 batch-111 = 0.00872412696480751

Training epoch-10 batch-112
Running loss of epoch-10 batch-112 = 0.013155522756278515

Training epoch-10 batch-113
Running loss of epoch-10 batch-113 = 0.010048696771264076

Training epoch-10 batch-114
Running loss of epoch-10 batch-114 = 0.011465633288025856

Training epoch-10 batch-115
Running loss of epoch-10 batch-115 = 0.009976904839277267

Training epoch-10 batch-116
Running loss of epoch-10 batch-116 = 0.007020737510174513

Training epoch-10 batch-117
Running loss of epoch-10 batch-117 = 0.0070508746430277824

Training epoch-10 batch-118
Running loss of epoch-10 batch-118 = 0.005945419892668724

Training epoch-10 batch-119
Running loss of epoch-10 batch-119 = 0.011036554351449013

Training epoch-10 batch-120
Running loss of epoch-10 batch-120 = 0.00928410328924656

Training epoch-10 batch-121
Running loss of epoch-10 batch-121 = 0.008943205699324608

Training epoch-10 batch-122
Running loss of epoch-10 batch-122 = 0.009718872606754303

Training epoch-10 batch-123
Running loss of epoch-10 batch-123 = 0.009511647745966911

Training epoch-10 batch-124
Running loss of epoch-10 batch-124 = 0.008459005504846573

Training epoch-10 batch-125
Running loss of epoch-10 batch-125 = 0.006366892717778683

Training epoch-10 batch-126
Running loss of epoch-10 batch-126 = 0.010422925464808941

Training epoch-10 batch-127
Running loss of epoch-10 batch-127 = 0.009830622002482414

Training epoch-10 batch-128
Running loss of epoch-10 batch-128 = 0.006357706617563963

Training epoch-10 batch-129
Running loss of epoch-10 batch-129 = 0.010567061603069305

Training epoch-10 batch-130
Running loss of epoch-10 batch-130 = 0.008340851403772831

Training epoch-10 batch-131
Running loss of epoch-10 batch-131 = 0.011785869486629963

Training epoch-10 batch-132
Running loss of epoch-10 batch-132 = 0.009881415404379368

Training epoch-10 batch-133
Running loss of epoch-10 batch-133 = 0.008560262620449066

Training epoch-10 batch-134
Running loss of epoch-10 batch-134 = 0.010766852647066116

Training epoch-10 batch-135
Running loss of epoch-10 batch-135 = 0.010258722119033337

Training epoch-10 batch-136
Running loss of epoch-10 batch-136 = 0.00869432371109724

Training epoch-10 batch-137
Running loss of epoch-10 batch-137 = 0.011138362810015678

Training epoch-10 batch-138
Running loss of epoch-10 batch-138 = 0.007773117627948523

Training epoch-10 batch-139
Running loss of epoch-10 batch-139 = 0.011682122014462948

Training epoch-10 batch-140
Running loss of epoch-10 batch-140 = 0.008882720023393631

Training epoch-10 batch-141
Running loss of epoch-10 batch-141 = 0.01038986723870039

Training epoch-10 batch-142
Running loss of epoch-10 batch-142 = 0.008593995124101639

Training epoch-10 batch-143
Running loss of epoch-10 batch-143 = 0.01014956459403038

Training epoch-10 batch-144
Running loss of epoch-10 batch-144 = 0.009745378978550434

Training epoch-10 batch-145
Running loss of epoch-10 batch-145 = 0.012237394228577614

Training epoch-10 batch-146
Running loss of epoch-10 batch-146 = 0.012165198102593422

Training epoch-10 batch-147
Running loss of epoch-10 batch-147 = 0.009422308765351772

Training epoch-10 batch-148
Running loss of epoch-10 batch-148 = 0.009861936792731285

Training epoch-10 batch-149
Running loss of epoch-10 batch-149 = 0.007524499669671059

Training epoch-10 batch-150
Running loss of epoch-10 batch-150 = 0.012148704379796982

Training epoch-10 batch-151
Running loss of epoch-10 batch-151 = 0.012521377764642239

Training epoch-10 batch-152
Running loss of epoch-10 batch-152 = 0.010742023587226868

Training epoch-10 batch-153
Running loss of epoch-10 batch-153 = 0.012190110981464386

Training epoch-10 batch-154
Running loss of epoch-10 batch-154 = 0.006961269769817591

Training epoch-10 batch-155
Running loss of epoch-10 batch-155 = 0.007554857991635799

Training epoch-10 batch-156
Running loss of epoch-10 batch-156 = 0.010087292641401291

Training epoch-10 batch-157
Running loss of epoch-10 batch-157 = 0.04532201588153839

Finished training epoch-10.



Average train loss at epoch-10 = 0.009849245557188987

Started Evaluation

Average val loss at epoch-10 = 2.38970368786862

Accuracy for classes:
Accuracy for class equals is: 50.17 %
Accuracy for class main is: 18.03 %
Accuracy for class setUp is: 46.72 %
Accuracy for class onCreate is: 31.02 %
Accuracy for class toString is: 16.38 %
Accuracy for class run is: 58.45 %
Accuracy for class hashCode is: 74.16 %
Accuracy for class init is: 29.15 %
Accuracy for class execute is: 23.29 %
Accuracy for class get is: 20.26 %

Overall Accuracy = 36.29 %

Finished Evaluation



Started training epoch-11


Training epoch-11 batch-1
Running loss of epoch-11 batch-1 = 0.007126158103346825

Training epoch-11 batch-2
Running loss of epoch-11 batch-2 = 0.006966311018913984

Training epoch-11 batch-3
Running loss of epoch-11 batch-3 = 0.006607711315155029

Training epoch-11 batch-4
Running loss of epoch-11 batch-4 = 0.009183462709188461

Training epoch-11 batch-5
Running loss of epoch-11 batch-5 = 0.007063446566462517

Training epoch-11 batch-6
Running loss of epoch-11 batch-6 = 0.007643241435289383

Training epoch-11 batch-7
Running loss of epoch-11 batch-7 = 0.008562147617340088

Training epoch-11 batch-8
Running loss of epoch-11 batch-8 = 0.0067349825985729694

Training epoch-11 batch-9
Running loss of epoch-11 batch-9 = 0.008286532014608383

Training epoch-11 batch-10
Running loss of epoch-11 batch-10 = 0.006451423279941082

Training epoch-11 batch-11
Running loss of epoch-11 batch-11 = 0.0078096333891153336

Training epoch-11 batch-12
Running loss of epoch-11 batch-12 = 0.007004652637988329

Training epoch-11 batch-13
Running loss of epoch-11 batch-13 = 0.00633899075910449

Training epoch-11 batch-14
Running loss of epoch-11 batch-14 = 0.0058731078170239925

Training epoch-11 batch-15
Running loss of epoch-11 batch-15 = 0.006789985578507185

Training epoch-11 batch-16
Running loss of epoch-11 batch-16 = 0.007113713771104813

Training epoch-11 batch-17
Running loss of epoch-11 batch-17 = 0.007010857108980417

Training epoch-11 batch-18
Running loss of epoch-11 batch-18 = 0.005774238612502813

Training epoch-11 batch-19
Running loss of epoch-11 batch-19 = 0.005200137384235859

Training epoch-11 batch-20
Running loss of epoch-11 batch-20 = 0.007839110679924488

Training epoch-11 batch-21
Running loss of epoch-11 batch-21 = 0.005449219606816769

Training epoch-11 batch-22
Running loss of epoch-11 batch-22 = 0.004782875068485737

Training epoch-11 batch-23
Running loss of epoch-11 batch-23 = 0.007097239606082439

Training epoch-11 batch-24
Running loss of epoch-11 batch-24 = 0.008259192109107971

Training epoch-11 batch-25
Running loss of epoch-11 batch-25 = 0.006169722881168127

Training epoch-11 batch-26
Running loss of epoch-11 batch-26 = 0.007159227505326271

Training epoch-11 batch-27
Running loss of epoch-11 batch-27 = 0.005831091199070215

Training epoch-11 batch-28
Running loss of epoch-11 batch-28 = 0.00787859596312046

Training epoch-11 batch-29
Running loss of epoch-11 batch-29 = 0.004663925617933273

Training epoch-11 batch-30
Running loss of epoch-11 batch-30 = 0.006964957341551781

Training epoch-11 batch-31
Running loss of epoch-11 batch-31 = 0.007639954797923565

Training epoch-11 batch-32
Running loss of epoch-11 batch-32 = 0.004670027643442154

Training epoch-11 batch-33
Running loss of epoch-11 batch-33 = 0.0057775527238845825

Training epoch-11 batch-34
Running loss of epoch-11 batch-34 = 0.004435012582689524

Training epoch-11 batch-35
Running loss of epoch-11 batch-35 = 0.004202305339276791

Training epoch-11 batch-36
Running loss of epoch-11 batch-36 = 0.006075702141970396

Training epoch-11 batch-37
Running loss of epoch-11 batch-37 = 0.006146935746073723

Training epoch-11 batch-38
Running loss of epoch-11 batch-38 = 0.005470597185194492

Training epoch-11 batch-39
Running loss of epoch-11 batch-39 = 0.00574561906978488

Training epoch-11 batch-40
Running loss of epoch-11 batch-40 = 0.007022340781986713

Training epoch-11 batch-41
Running loss of epoch-11 batch-41 = 0.004329882562160492

Training epoch-11 batch-42
Running loss of epoch-11 batch-42 = 0.004216053523123264

Training epoch-11 batch-43
Running loss of epoch-11 batch-43 = 0.004096204414963722

Training epoch-11 batch-44
Running loss of epoch-11 batch-44 = 0.005709583405405283

Training epoch-11 batch-45
Running loss of epoch-11 batch-45 = 0.003855660557746887

Training epoch-11 batch-46
Running loss of epoch-11 batch-46 = 0.005116651766002178

Training epoch-11 batch-47
Running loss of epoch-11 batch-47 = 0.003893125569447875

Training epoch-11 batch-48
Running loss of epoch-11 batch-48 = 0.006539694964885712

Training epoch-11 batch-49
Running loss of epoch-11 batch-49 = 0.004699507728219032

Training epoch-11 batch-50
Running loss of epoch-11 batch-50 = 0.004938013851642609

Training epoch-11 batch-51
Running loss of epoch-11 batch-51 = 0.004711479879915714

Training epoch-11 batch-52
Running loss of epoch-11 batch-52 = 0.005578609649091959

Training epoch-11 batch-53
Running loss of epoch-11 batch-53 = 0.0032202033326029778

Training epoch-11 batch-54
Running loss of epoch-11 batch-54 = 0.004051038529723883

Training epoch-11 batch-55
Running loss of epoch-11 batch-55 = 0.002659846795722842

Training epoch-11 batch-56
Running loss of epoch-11 batch-56 = 0.004891376011073589

Training epoch-11 batch-57
Running loss of epoch-11 batch-57 = 0.004927496425807476

Training epoch-11 batch-58
Running loss of epoch-11 batch-58 = 0.005640360061079264

Training epoch-11 batch-59
Running loss of epoch-11 batch-59 = 0.006076429970562458

Training epoch-11 batch-60
Running loss of epoch-11 batch-60 = 0.004466123878955841

Training epoch-11 batch-61
Running loss of epoch-11 batch-61 = 0.005095010623335838

Training epoch-11 batch-62
Running loss of epoch-11 batch-62 = 0.004205895587801933

Training epoch-11 batch-63
Running loss of epoch-11 batch-63 = 0.004868465941399336

Training epoch-11 batch-64
Running loss of epoch-11 batch-64 = 0.004838964901864529

Training epoch-11 batch-65
Running loss of epoch-11 batch-65 = 0.004241010174155235

Training epoch-11 batch-66
Running loss of epoch-11 batch-66 = 0.006400898098945618

Training epoch-11 batch-67
Running loss of epoch-11 batch-67 = 0.0059645711444318295

Training epoch-11 batch-68
Running loss of epoch-11 batch-68 = 0.005398353561758995

Training epoch-11 batch-69
Running loss of epoch-11 batch-69 = 0.004193627741187811

Training epoch-11 batch-70
Running loss of epoch-11 batch-70 = 0.004649487789720297

Training epoch-11 batch-71
Running loss of epoch-11 batch-71 = 0.004803346935659647

Training epoch-11 batch-72
Running loss of epoch-11 batch-72 = 0.0026793545112013817

Training epoch-11 batch-73
Running loss of epoch-11 batch-73 = 0.0059780036099255085

Training epoch-11 batch-74
Running loss of epoch-11 batch-74 = 0.006182482931762934

Training epoch-11 batch-75
Running loss of epoch-11 batch-75 = 0.0050618089735507965

Training epoch-11 batch-76
Running loss of epoch-11 batch-76 = 0.003899120958521962

Training epoch-11 batch-77
Running loss of epoch-11 batch-77 = 0.008052530698478222

Training epoch-11 batch-78
Running loss of epoch-11 batch-78 = 0.00517708994448185

Training epoch-11 batch-79
Running loss of epoch-11 batch-79 = 0.00393716711550951

Training epoch-11 batch-80
Running loss of epoch-11 batch-80 = 0.004558415152132511

Training epoch-11 batch-81
Running loss of epoch-11 batch-81 = 0.0030409537721425295

Training epoch-11 batch-82
Running loss of epoch-11 batch-82 = 0.003831508569419384

Training epoch-11 batch-83
Running loss of epoch-11 batch-83 = 0.005314590409398079

Training epoch-11 batch-84
Running loss of epoch-11 batch-84 = 0.004080645274370909

Training epoch-11 batch-85
Running loss of epoch-11 batch-85 = 0.004440033808350563

Training epoch-11 batch-86
Running loss of epoch-11 batch-86 = 0.005364533979445696

Training epoch-11 batch-87
Running loss of epoch-11 batch-87 = 0.006003902293741703

Training epoch-11 batch-88
Running loss of epoch-11 batch-88 = 0.004677193239331245

Training epoch-11 batch-89
Running loss of epoch-11 batch-89 = 0.007590602617710829

Training epoch-11 batch-90
Running loss of epoch-11 batch-90 = 0.004834399558603764

Training epoch-11 batch-91
Running loss of epoch-11 batch-91 = 0.0042243776842951775

Training epoch-11 batch-92
Running loss of epoch-11 batch-92 = 0.002834956394508481

Training epoch-11 batch-93
Running loss of epoch-11 batch-93 = 0.004936518147587776

Training epoch-11 batch-94
Running loss of epoch-11 batch-94 = 0.003948765341192484

Training epoch-11 batch-95
Running loss of epoch-11 batch-95 = 0.0035174828954041004

Training epoch-11 batch-96
Running loss of epoch-11 batch-96 = 0.0051681543700397015

Training epoch-11 batch-97
Running loss of epoch-11 batch-97 = 0.0066602155566215515

Training epoch-11 batch-98
Running loss of epoch-11 batch-98 = 0.005215249955654144

Training epoch-11 batch-99
Running loss of epoch-11 batch-99 = 0.0048248376697301865

Training epoch-11 batch-100
Running loss of epoch-11 batch-100 = 0.005647262558341026

Training epoch-11 batch-101
Running loss of epoch-11 batch-101 = 0.004998647142201662

Training epoch-11 batch-102
Running loss of epoch-11 batch-102 = 0.004593315999954939

Training epoch-11 batch-103
Running loss of epoch-11 batch-103 = 0.005742419511079788

Training epoch-11 batch-104
Running loss of epoch-11 batch-104 = 0.004457538947463036

Training epoch-11 batch-105
Running loss of epoch-11 batch-105 = 0.0059851063415408134

Training epoch-11 batch-106
Running loss of epoch-11 batch-106 = 0.006986735388636589

Training epoch-11 batch-107
Running loss of epoch-11 batch-107 = 0.006072844844311476

Training epoch-11 batch-108
Running loss of epoch-11 batch-108 = 0.004013836849480867

Training epoch-11 batch-109
Running loss of epoch-11 batch-109 = 0.006031359080225229

Training epoch-11 batch-110
Running loss of epoch-11 batch-110 = 0.004919895436614752

Training epoch-11 batch-111
Running loss of epoch-11 batch-111 = 0.005699288100004196

Training epoch-11 batch-112
Running loss of epoch-11 batch-112 = 0.004024885594844818

Training epoch-11 batch-113
Running loss of epoch-11 batch-113 = 0.00599072128534317

Training epoch-11 batch-114
Running loss of epoch-11 batch-114 = 0.0061578694730997086

Training epoch-11 batch-115
Running loss of epoch-11 batch-115 = 0.00476305466145277

Training epoch-11 batch-116
Running loss of epoch-11 batch-116 = 0.005324384197592735

Training epoch-11 batch-117
Running loss of epoch-11 batch-117 = 0.005608386360108852

Training epoch-11 batch-118
Running loss of epoch-11 batch-118 = 0.005493702366948128

Training epoch-11 batch-119
Running loss of epoch-11 batch-119 = 0.0051782745867967606

Training epoch-11 batch-120
Running loss of epoch-11 batch-120 = 0.007486063055694103

Training epoch-11 batch-121
Running loss of epoch-11 batch-121 = 0.0077645196579396725

Training epoch-11 batch-122
Running loss of epoch-11 batch-122 = 0.005919337272644043

Training epoch-11 batch-123
Running loss of epoch-11 batch-123 = 0.006287997588515282

Training epoch-11 batch-124
Running loss of epoch-11 batch-124 = 0.00397072546184063

Training epoch-11 batch-125
Running loss of epoch-11 batch-125 = 0.005262048915028572

Training epoch-11 batch-126
Running loss of epoch-11 batch-126 = 0.003673022845759988

Training epoch-11 batch-127
Running loss of epoch-11 batch-127 = 0.0057718465104699135

Training epoch-11 batch-128
Running loss of epoch-11 batch-128 = 0.004944785963743925

Training epoch-11 batch-129
Running loss of epoch-11 batch-129 = 0.0033475321251899004

Training epoch-11 batch-130
Running loss of epoch-11 batch-130 = 0.004810303915292025

Training epoch-11 batch-131
Running loss of epoch-11 batch-131 = 0.005397899076342583

Training epoch-11 batch-132
Running loss of epoch-11 batch-132 = 0.007184592075645924

Training epoch-11 batch-133
Running loss of epoch-11 batch-133 = 0.0031427964568138123

Training epoch-11 batch-134
Running loss of epoch-11 batch-134 = 0.004788629710674286

Training epoch-11 batch-135
Running loss of epoch-11 batch-135 = 0.005387661047279835

Training epoch-11 batch-136
Running loss of epoch-11 batch-136 = 0.005767010618001223

Training epoch-11 batch-137
Running loss of epoch-11 batch-137 = 0.006636797450482845

Training epoch-11 batch-138
Running loss of epoch-11 batch-138 = 0.005663480143994093

Training epoch-11 batch-139
Running loss of epoch-11 batch-139 = 0.00605438556522131

Training epoch-11 batch-140
Running loss of epoch-11 batch-140 = 0.0048279790207743645

Training epoch-11 batch-141
Running loss of epoch-11 batch-141 = 0.005252642557024956

Training epoch-11 batch-142
Running loss of epoch-11 batch-142 = 0.006499067880213261

Training epoch-11 batch-143
Running loss of epoch-11 batch-143 = 0.0051902057603001595

Training epoch-11 batch-144
Running loss of epoch-11 batch-144 = 0.005724051035940647

Training epoch-11 batch-145
Running loss of epoch-11 batch-145 = 0.005324762314558029

Training epoch-11 batch-146
Running loss of epoch-11 batch-146 = 0.007503211498260498

Training epoch-11 batch-147
Running loss of epoch-11 batch-147 = 0.006566795986145735

Training epoch-11 batch-148
Running loss of epoch-11 batch-148 = 0.003637387417256832

Training epoch-11 batch-149
Running loss of epoch-11 batch-149 = 0.0067365774884819984

Training epoch-11 batch-150
Running loss of epoch-11 batch-150 = 0.0058332085609436035

Training epoch-11 batch-151
Running loss of epoch-11 batch-151 = 0.006728161592036486

Training epoch-11 batch-152
Running loss of epoch-11 batch-152 = 0.004854802042245865

Training epoch-11 batch-153
Running loss of epoch-11 batch-153 = 0.0051381527446210384

Training epoch-11 batch-154
Running loss of epoch-11 batch-154 = 0.006060188636183739

Training epoch-11 batch-155
Running loss of epoch-11 batch-155 = 0.004914423916488886

Training epoch-11 batch-156
Running loss of epoch-11 batch-156 = 0.0040557850152254105

Training epoch-11 batch-157
Running loss of epoch-11 batch-157 = 0.011364389210939407

Finished training epoch-11.



Average train loss at epoch-11 = 0.005515348960459233

Started Evaluation

Average val loss at epoch-11 = 2.716947608088192

Accuracy for classes:
Accuracy for class equals is: 58.91 %
Accuracy for class main is: 76.07 %
Accuracy for class setUp is: 55.90 %
Accuracy for class onCreate is: 26.87 %
Accuracy for class toString is: 24.57 %
Accuracy for class run is: 26.94 %
Accuracy for class hashCode is: 68.16 %
Accuracy for class init is: 15.47 %
Accuracy for class execute is: 36.55 %
Accuracy for class get is: 9.23 %

Overall Accuracy = 40.89 %

Finished Evaluation



Started training epoch-12


Training epoch-12 batch-1
Running loss of epoch-12 batch-1 = 0.004868172109127045

Training epoch-12 batch-2
Running loss of epoch-12 batch-2 = 0.00612523453310132

Training epoch-12 batch-3
Running loss of epoch-12 batch-3 = 0.00388717302121222

Training epoch-12 batch-4
Running loss of epoch-12 batch-4 = 0.0031416453421115875

Training epoch-12 batch-5
Running loss of epoch-12 batch-5 = 0.002293902914971113

Training epoch-12 batch-6
Running loss of epoch-12 batch-6 = 0.006359497085213661

Training epoch-12 batch-7
Running loss of epoch-12 batch-7 = 0.003864523023366928

Training epoch-12 batch-8
Running loss of epoch-12 batch-8 = 0.004236043430864811

Training epoch-12 batch-9
Running loss of epoch-12 batch-9 = 0.002804720774292946

Training epoch-12 batch-10
Running loss of epoch-12 batch-10 = 0.0028571714647114277

Training epoch-12 batch-11
Running loss of epoch-12 batch-11 = 0.0034484630450606346

Training epoch-12 batch-12
Running loss of epoch-12 batch-12 = 0.0039893523789942265

Training epoch-12 batch-13
Running loss of epoch-12 batch-13 = 0.0031885826028883457

Training epoch-12 batch-14
Running loss of epoch-12 batch-14 = 0.004431534558534622

Training epoch-12 batch-15
Running loss of epoch-12 batch-15 = 0.0027312415186315775

Training epoch-12 batch-16
Running loss of epoch-12 batch-16 = 0.001738944323733449

Training epoch-12 batch-17
Running loss of epoch-12 batch-17 = 0.00344609166495502

Training epoch-12 batch-18
Running loss of epoch-12 batch-18 = 0.0040117185562849045

Training epoch-12 batch-19
Running loss of epoch-12 batch-19 = 0.0034200374502688646

Training epoch-12 batch-20
Running loss of epoch-12 batch-20 = 0.002789596561342478

Training epoch-12 batch-21
Running loss of epoch-12 batch-21 = 0.0030528618954122066

Training epoch-12 batch-22
Running loss of epoch-12 batch-22 = 0.0027551939710974693

Training epoch-12 batch-23
Running loss of epoch-12 batch-23 = 0.004524934105575085

Training epoch-12 batch-24
Running loss of epoch-12 batch-24 = 0.0024369345046579838

Training epoch-12 batch-25
Running loss of epoch-12 batch-25 = 0.0038115517236292362

Training epoch-12 batch-26
Running loss of epoch-12 batch-26 = 0.0029706149362027645

Training epoch-12 batch-27
Running loss of epoch-12 batch-27 = 0.0014210403896868229

Training epoch-12 batch-28
Running loss of epoch-12 batch-28 = 0.006354887038469315

Training epoch-12 batch-29
Running loss of epoch-12 batch-29 = 0.0033526853658258915

Training epoch-12 batch-30
Running loss of epoch-12 batch-30 = 0.0034275855869054794

Training epoch-12 batch-31
Running loss of epoch-12 batch-31 = 0.003731423756107688

Training epoch-12 batch-32
Running loss of epoch-12 batch-32 = 0.0025708512403070927

Training epoch-12 batch-33
Running loss of epoch-12 batch-33 = 0.0023005851544439793

Training epoch-12 batch-34
Running loss of epoch-12 batch-34 = 0.005962079390883446

Training epoch-12 batch-35
Running loss of epoch-12 batch-35 = 0.0021172822453081608

Training epoch-12 batch-36
Running loss of epoch-12 batch-36 = 0.0022469523828476667

Training epoch-12 batch-37
Running loss of epoch-12 batch-37 = 0.0029417998157441616

Training epoch-12 batch-38
Running loss of epoch-12 batch-38 = 0.0028798989951610565

Training epoch-12 batch-39
Running loss of epoch-12 batch-39 = 0.002957948949187994

Training epoch-12 batch-40
Running loss of epoch-12 batch-40 = 0.003636674489825964

Training epoch-12 batch-41
Running loss of epoch-12 batch-41 = 0.0023084005806595087

Training epoch-12 batch-42
Running loss of epoch-12 batch-42 = 0.00312863290309906

Training epoch-12 batch-43
Running loss of epoch-12 batch-43 = 0.0042824032716453075

Training epoch-12 batch-44
Running loss of epoch-12 batch-44 = 0.0030776329804211855

Training epoch-12 batch-45
Running loss of epoch-12 batch-45 = 0.00366941187530756

Training epoch-12 batch-46
Running loss of epoch-12 batch-46 = 0.004460691474378109

Training epoch-12 batch-47
Running loss of epoch-12 batch-47 = 0.0029817926697432995

Training epoch-12 batch-48
Running loss of epoch-12 batch-48 = 0.0014324486255645752

Training epoch-12 batch-49
Running loss of epoch-12 batch-49 = 0.0030281953513622284

Training epoch-12 batch-50
Running loss of epoch-12 batch-50 = 0.002435957081615925

Training epoch-12 batch-51
Running loss of epoch-12 batch-51 = 0.0027499524876475334

Training epoch-12 batch-52
Running loss of epoch-12 batch-52 = 0.004876863211393356

Training epoch-12 batch-53
Running loss of epoch-12 batch-53 = 0.002253390848636627

Training epoch-12 batch-54
Running loss of epoch-12 batch-54 = 0.0041641369462013245

Training epoch-12 batch-55
Running loss of epoch-12 batch-55 = 0.002280936110764742

Training epoch-12 batch-56
Running loss of epoch-12 batch-56 = 0.002114339731633663

Training epoch-12 batch-57
Running loss of epoch-12 batch-57 = 0.0027141724713146687

Training epoch-12 batch-58
Running loss of epoch-12 batch-58 = 0.004967981018126011

Training epoch-12 batch-59
Running loss of epoch-12 batch-59 = 0.005646783392876387

Training epoch-12 batch-60
Running loss of epoch-12 batch-60 = 0.0042871953919529915

Training epoch-12 batch-61
Running loss of epoch-12 batch-61 = 0.004324834328144789

Training epoch-12 batch-62
Running loss of epoch-12 batch-62 = 0.003472765674814582

Training epoch-12 batch-63
Running loss of epoch-12 batch-63 = 0.003914540633559227

Training epoch-12 batch-64
Running loss of epoch-12 batch-64 = 0.002278002444654703

Training epoch-12 batch-65
Running loss of epoch-12 batch-65 = 0.003766230773180723

Training epoch-12 batch-66
Running loss of epoch-12 batch-66 = 0.0027408411260694265

Training epoch-12 batch-67
Running loss of epoch-12 batch-67 = 0.004061442334204912

Training epoch-12 batch-68
Running loss of epoch-12 batch-68 = 0.0034181582741439342

Training epoch-12 batch-69
Running loss of epoch-12 batch-69 = 0.0021558133885264397

Training epoch-12 batch-70
Running loss of epoch-12 batch-70 = 0.0031933903228491545

Training epoch-12 batch-71
Running loss of epoch-12 batch-71 = 0.0037141242064535618

Training epoch-12 batch-72
Running loss of epoch-12 batch-72 = 0.0038143156562000513

Training epoch-12 batch-73
Running loss of epoch-12 batch-73 = 0.0024533229880034924

Training epoch-12 batch-74
Running loss of epoch-12 batch-74 = 0.0023584552109241486

Training epoch-12 batch-75
Running loss of epoch-12 batch-75 = 0.0029404056258499622

Training epoch-12 batch-76
Running loss of epoch-12 batch-76 = 0.005780953448265791

Training epoch-12 batch-77
Running loss of epoch-12 batch-77 = 0.002645662520080805

Training epoch-12 batch-78
Running loss of epoch-12 batch-78 = 0.003206247463822365

Training epoch-12 batch-79
Running loss of epoch-12 batch-79 = 0.003456859616562724

Training epoch-12 batch-80
Running loss of epoch-12 batch-80 = 0.004317004233598709

Training epoch-12 batch-81
Running loss of epoch-12 batch-81 = 0.0013744147727265954

Training epoch-12 batch-82
Running loss of epoch-12 batch-82 = 0.002203103620558977

Training epoch-12 batch-83
Running loss of epoch-12 batch-83 = 0.002515778411179781

Training epoch-12 batch-84
Running loss of epoch-12 batch-84 = 0.003513770177960396

Training epoch-12 batch-85
Running loss of epoch-12 batch-85 = 0.0030501133296638727

Training epoch-12 batch-86
Running loss of epoch-12 batch-86 = 0.003880713600665331

Training epoch-12 batch-87
Running loss of epoch-12 batch-87 = 0.0016997263301163912

Training epoch-12 batch-88
Running loss of epoch-12 batch-88 = 0.0019937162287533283

Training epoch-12 batch-89
Running loss of epoch-12 batch-89 = 0.001818578690290451

Training epoch-12 batch-90
Running loss of epoch-12 batch-90 = 0.003949018195271492

Training epoch-12 batch-91
Running loss of epoch-12 batch-91 = 0.002284776419401169

Training epoch-12 batch-92
Running loss of epoch-12 batch-92 = 0.002816091990098357

Training epoch-12 batch-93
Running loss of epoch-12 batch-93 = 0.0038332322146743536

Training epoch-12 batch-94
Running loss of epoch-12 batch-94 = 0.002945360029116273

Training epoch-12 batch-95
Running loss of epoch-12 batch-95 = 0.0016445181099697948

Training epoch-12 batch-96
Running loss of epoch-12 batch-96 = 0.004017976578325033

Training epoch-12 batch-97
Running loss of epoch-12 batch-97 = 0.003991742618381977

Training epoch-12 batch-98
Running loss of epoch-12 batch-98 = 0.002377555938437581

Training epoch-12 batch-99
Running loss of epoch-12 batch-99 = 0.00300196441821754

Training epoch-12 batch-100
Running loss of epoch-12 batch-100 = 0.004989488981664181

Training epoch-12 batch-101
Running loss of epoch-12 batch-101 = 0.004364918451756239

Training epoch-12 batch-102
Running loss of epoch-12 batch-102 = 0.002931594382971525

Training epoch-12 batch-103
Running loss of epoch-12 batch-103 = 0.006324916146695614

Training epoch-12 batch-104
Running loss of epoch-12 batch-104 = 0.005586379207670689

Training epoch-12 batch-105
Running loss of epoch-12 batch-105 = 0.0022681760601699352

Training epoch-12 batch-106
Running loss of epoch-12 batch-106 = 0.003170628100633621

Training epoch-12 batch-107
Running loss of epoch-12 batch-107 = 0.005376265849918127

Training epoch-12 batch-108
Running loss of epoch-12 batch-108 = 0.005777065642178059

Training epoch-12 batch-109
Running loss of epoch-12 batch-109 = 0.004509306512773037

Training epoch-12 batch-110
Running loss of epoch-12 batch-110 = 0.0030177119188010693

Training epoch-12 batch-111
Running loss of epoch-12 batch-111 = 0.00372208165936172

Training epoch-12 batch-112
Running loss of epoch-12 batch-112 = 0.00405748700723052

Training epoch-12 batch-113
Running loss of epoch-12 batch-113 = 0.003726521972566843

Training epoch-12 batch-114
Running loss of epoch-12 batch-114 = 0.00312773953191936

Training epoch-12 batch-115
Running loss of epoch-12 batch-115 = 0.0035114665515720844

Training epoch-12 batch-116
Running loss of epoch-12 batch-116 = 0.002260204404592514

Training epoch-12 batch-117
Running loss of epoch-12 batch-117 = 0.003029672894626856

Training epoch-12 batch-118
Running loss of epoch-12 batch-118 = 0.0036825404968112707

Training epoch-12 batch-119
Running loss of epoch-12 batch-119 = 0.0029083197005093098

Training epoch-12 batch-120
Running loss of epoch-12 batch-120 = 0.002713263500481844

Training epoch-12 batch-121
Running loss of epoch-12 batch-121 = 0.0021909642964601517

Training epoch-12 batch-122
Running loss of epoch-12 batch-122 = 0.003009915817528963

Training epoch-12 batch-123
Running loss of epoch-12 batch-123 = 0.003138977335765958

Training epoch-12 batch-124
Running loss of epoch-12 batch-124 = 0.0032971540931612253

Training epoch-12 batch-125
Running loss of epoch-12 batch-125 = 0.002522618044167757

Training epoch-12 batch-126
Running loss of epoch-12 batch-126 = 0.0032939400989562273

Training epoch-12 batch-127
Running loss of epoch-12 batch-127 = 0.004239701200276613

Training epoch-12 batch-128
Running loss of epoch-12 batch-128 = 0.0026202527806162834

Training epoch-12 batch-129
Running loss of epoch-12 batch-129 = 0.0031941512133926153

Training epoch-12 batch-130
Running loss of epoch-12 batch-130 = 0.001956895925104618

Training epoch-12 batch-131
Running loss of epoch-12 batch-131 = 0.0021590760443359613

Training epoch-12 batch-132
Running loss of epoch-12 batch-132 = 0.0022477577440440655

Training epoch-12 batch-133
Running loss of epoch-12 batch-133 = 0.003173721255734563

Training epoch-12 batch-134
Running loss of epoch-12 batch-134 = 0.0019457679009065032

Training epoch-12 batch-135
Running loss of epoch-12 batch-135 = 0.0018520852318033576

Training epoch-12 batch-136
Running loss of epoch-12 batch-136 = 0.002760543255135417

Training epoch-12 batch-137
Running loss of epoch-12 batch-137 = 0.0026596100069582462

Training epoch-12 batch-138
Running loss of epoch-12 batch-138 = 0.006123666185885668

Training epoch-12 batch-139
Running loss of epoch-12 batch-139 = 0.002930079586803913

Training epoch-12 batch-140
Running loss of epoch-12 batch-140 = 0.0033451225608587265

Training epoch-12 batch-141
Running loss of epoch-12 batch-141 = 0.0029300127644091845

Training epoch-12 batch-142
Running loss of epoch-12 batch-142 = 0.002260083332657814

Training epoch-12 batch-143
Running loss of epoch-12 batch-143 = 0.0034593292511999607

Training epoch-12 batch-144
Running loss of epoch-12 batch-144 = 0.004399493802338839

Training epoch-12 batch-145
Running loss of epoch-12 batch-145 = 0.0032139208633452654

Training epoch-12 batch-146
Running loss of epoch-12 batch-146 = 0.0019465999212116003

Training epoch-12 batch-147
Running loss of epoch-12 batch-147 = 0.004503525793552399

Training epoch-12 batch-148
Running loss of epoch-12 batch-148 = 0.0030629481188952923

Training epoch-12 batch-149
Running loss of epoch-12 batch-149 = 0.002288288436830044

Training epoch-12 batch-150
Running loss of epoch-12 batch-150 = 0.003520472440868616

Training epoch-12 batch-151
Running loss of epoch-12 batch-151 = 0.0024423631839454174

Training epoch-12 batch-152
Running loss of epoch-12 batch-152 = 0.0038003488443791866

Training epoch-12 batch-153
Running loss of epoch-12 batch-153 = 0.004619985818862915

Training epoch-12 batch-154
Running loss of epoch-12 batch-154 = 0.004542674869298935

Training epoch-12 batch-155
Running loss of epoch-12 batch-155 = 0.0033556995913386345

Training epoch-12 batch-156
Running loss of epoch-12 batch-156 = 0.004179591313004494

Training epoch-12 batch-157
Running loss of epoch-12 batch-157 = 0.012244196608662605

Finished training epoch-12.



Average train loss at epoch-12 = 0.003354741822183132

Started Evaluation

Average val loss at epoch-12 = 2.8832500059353676

Accuracy for classes:
Accuracy for class equals is: 56.60 %
Accuracy for class main is: 52.30 %
Accuracy for class setUp is: 51.48 %
Accuracy for class onCreate is: 31.98 %
Accuracy for class toString is: 37.54 %
Accuracy for class run is: 23.97 %
Accuracy for class hashCode is: 72.28 %
Accuracy for class init is: 26.23 %
Accuracy for class execute is: 14.46 %
Accuracy for class get is: 32.82 %

Overall Accuracy = 40.54 %

Finished Evaluation



Started training epoch-13


Training epoch-13 batch-1
Running loss of epoch-13 batch-1 = 0.0015181255294010043

Training epoch-13 batch-2
Running loss of epoch-13 batch-2 = 0.0012868910562247038

Training epoch-13 batch-3
Running loss of epoch-13 batch-3 = 0.0027477932162582874

Training epoch-13 batch-4
Running loss of epoch-13 batch-4 = 0.0014614165993407369

Training epoch-13 batch-5
Running loss of epoch-13 batch-5 = 0.0015859141713008285

Training epoch-13 batch-6
Running loss of epoch-13 batch-6 = 0.0027484886813908815

Training epoch-13 batch-7
Running loss of epoch-13 batch-7 = 0.002435721457004547

Training epoch-13 batch-8
Running loss of epoch-13 batch-8 = 0.002163511235266924

Training epoch-13 batch-9
Running loss of epoch-13 batch-9 = 0.0037168192211538553

Training epoch-13 batch-10
Running loss of epoch-13 batch-10 = 0.004111665766686201

Training epoch-13 batch-11
Running loss of epoch-13 batch-11 = 0.0022095362655818462

Training epoch-13 batch-12
Running loss of epoch-13 batch-12 = 0.0024094146210700274

Training epoch-13 batch-13
Running loss of epoch-13 batch-13 = 0.0021085503976792097

Training epoch-13 batch-14
Running loss of epoch-13 batch-14 = 0.0022375243715941906

Training epoch-13 batch-15
Running loss of epoch-13 batch-15 = 0.0020489650778472424

Training epoch-13 batch-16
Running loss of epoch-13 batch-16 = 0.0018045573960989714

Training epoch-13 batch-17
Running loss of epoch-13 batch-17 = 0.002063937485218048

Training epoch-13 batch-18
Running loss of epoch-13 batch-18 = 0.0033535026013851166

Training epoch-13 batch-19
Running loss of epoch-13 batch-19 = 0.0020435776095837355

Training epoch-13 batch-20
Running loss of epoch-13 batch-20 = 0.0026036680210381746

Training epoch-13 batch-21
Running loss of epoch-13 batch-21 = 0.002013844670727849

Training epoch-13 batch-22
Running loss of epoch-13 batch-22 = 0.0036013321951031685

Training epoch-13 batch-23
Running loss of epoch-13 batch-23 = 0.0031843131873756647

Training epoch-13 batch-24
Running loss of epoch-13 batch-24 = 0.001205507549457252

Training epoch-13 batch-25
Running loss of epoch-13 batch-25 = 0.0011435840278863907

Training epoch-13 batch-26
Running loss of epoch-13 batch-26 = 0.0026506618596613407

Training epoch-13 batch-27
Running loss of epoch-13 batch-27 = 0.0016453778371214867

Training epoch-13 batch-28
Running loss of epoch-13 batch-28 = 0.0017461524112150073

Training epoch-13 batch-29
Running loss of epoch-13 batch-29 = 0.001510814530774951

Training epoch-13 batch-30
Running loss of epoch-13 batch-30 = 0.0014861563686281443

Training epoch-13 batch-31
Running loss of epoch-13 batch-31 = 0.0013932876754552126

Training epoch-13 batch-32
Running loss of epoch-13 batch-32 = 0.001975319115445018

Training epoch-13 batch-33
Running loss of epoch-13 batch-33 = 0.0016030060360208154

Training epoch-13 batch-34
Running loss of epoch-13 batch-34 = 0.0015268049901351333

Training epoch-13 batch-35
Running loss of epoch-13 batch-35 = 0.004366337321698666

Training epoch-13 batch-36
Running loss of epoch-13 batch-36 = 0.0019301660358905792

Training epoch-13 batch-37
Running loss of epoch-13 batch-37 = 0.001822560210712254

Training epoch-13 batch-38
Running loss of epoch-13 batch-38 = 0.0014366081450134516

Training epoch-13 batch-39
Running loss of epoch-13 batch-39 = 0.00293722003698349

Training epoch-13 batch-40
Running loss of epoch-13 batch-40 = 0.0013331868685781956

Training epoch-13 batch-41
Running loss of epoch-13 batch-41 = 0.001874579698778689

Training epoch-13 batch-42
Running loss of epoch-13 batch-42 = 0.0013693372020497918

Training epoch-13 batch-43
Running loss of epoch-13 batch-43 = 0.0012849009362980723

Training epoch-13 batch-44
Running loss of epoch-13 batch-44 = 0.0014443097170442343

Training epoch-13 batch-45
Running loss of epoch-13 batch-45 = 0.0020385715179145336

Training epoch-13 batch-46
Running loss of epoch-13 batch-46 = 0.0009757298394106328

Training epoch-13 batch-47
Running loss of epoch-13 batch-47 = 0.0023336498998105526

Training epoch-13 batch-48
Running loss of epoch-13 batch-48 = 0.0014304312644526362

Training epoch-13 batch-49
Running loss of epoch-13 batch-49 = 0.0009927846258506179

Training epoch-13 batch-50
Running loss of epoch-13 batch-50 = 0.0028758735861629248

Training epoch-13 batch-51
Running loss of epoch-13 batch-51 = 0.0019077337346971035

Training epoch-13 batch-52
Running loss of epoch-13 batch-52 = 0.0015587524976581335

Training epoch-13 batch-53
Running loss of epoch-13 batch-53 = 0.0016077982727438211

Training epoch-13 batch-54
Running loss of epoch-13 batch-54 = 0.0010666847229003906

Training epoch-13 batch-55
Running loss of epoch-13 batch-55 = 0.0015590660041198134

Training epoch-13 batch-56
Running loss of epoch-13 batch-56 = 0.0019794334657490253

Training epoch-13 batch-57
Running loss of epoch-13 batch-57 = 0.0016333244275301695

Training epoch-13 batch-58
Running loss of epoch-13 batch-58 = 0.0017089598113670945

Training epoch-13 batch-59
Running loss of epoch-13 batch-59 = 0.001567957573570311

Training epoch-13 batch-60
Running loss of epoch-13 batch-60 = 0.001761529827490449

Training epoch-13 batch-61
Running loss of epoch-13 batch-61 = 0.0008714926079846919

Training epoch-13 batch-62
Running loss of epoch-13 batch-62 = 0.001260129502043128

Training epoch-13 batch-63
Running loss of epoch-13 batch-63 = 0.002648975234478712

Training epoch-13 batch-64
Running loss of epoch-13 batch-64 = 0.0006002942100167274

Training epoch-13 batch-65
Running loss of epoch-13 batch-65 = 0.002074137330055237

Training epoch-13 batch-66
Running loss of epoch-13 batch-66 = 0.0021024742163717747

Training epoch-13 batch-67
Running loss of epoch-13 batch-67 = 0.0018186902161687613

Training epoch-13 batch-68
Running loss of epoch-13 batch-68 = 0.0016235529910773039

Training epoch-13 batch-69
Running loss of epoch-13 batch-69 = 0.000826088769827038

Training epoch-13 batch-70
Running loss of epoch-13 batch-70 = 0.0014890726888552308

Training epoch-13 batch-71
Running loss of epoch-13 batch-71 = 0.00079776142956689

Training epoch-13 batch-72
Running loss of epoch-13 batch-72 = 0.0019730019848793745

Training epoch-13 batch-73
Running loss of epoch-13 batch-73 = 0.0004890121053904295

Training epoch-13 batch-74
Running loss of epoch-13 batch-74 = 0.0017778173787519336

Training epoch-13 batch-75
Running loss of epoch-13 batch-75 = 0.0016175672644749284

Training epoch-13 batch-76
Running loss of epoch-13 batch-76 = 0.0009855050593614578

Training epoch-13 batch-77
Running loss of epoch-13 batch-77 = 0.0022374994587153196

Training epoch-13 batch-78
Running loss of epoch-13 batch-78 = 0.002514764666557312

Training epoch-13 batch-79
Running loss of epoch-13 batch-79 = 0.002262734342366457

Training epoch-13 batch-80
Running loss of epoch-13 batch-80 = 0.0012647269759327173

Training epoch-13 batch-81
Running loss of epoch-13 batch-81 = 0.001606370322406292

Training epoch-13 batch-82
Running loss of epoch-13 batch-82 = 0.00236701313406229

Training epoch-13 batch-83
Running loss of epoch-13 batch-83 = 0.0008957661339081824

Training epoch-13 batch-84
Running loss of epoch-13 batch-84 = 0.002774052321910858

Training epoch-13 batch-85
Running loss of epoch-13 batch-85 = 0.0015751124592497945

Training epoch-13 batch-86
Running loss of epoch-13 batch-86 = 0.0025153651367872953

Training epoch-13 batch-87
Running loss of epoch-13 batch-87 = 0.00174715556204319

Training epoch-13 batch-88
Running loss of epoch-13 batch-88 = 0.0023876731283962727

Training epoch-13 batch-89
Running loss of epoch-13 batch-89 = 0.0011098837712779641

Training epoch-13 batch-90
Running loss of epoch-13 batch-90 = 0.0009092690888792276

Training epoch-13 batch-91
Running loss of epoch-13 batch-91 = 0.002189035527408123

Training epoch-13 batch-92
Running loss of epoch-13 batch-92 = 0.0005315336165949702

Training epoch-13 batch-93
Running loss of epoch-13 batch-93 = 0.0006639861385338008

Training epoch-13 batch-94
Running loss of epoch-13 batch-94 = 0.001677420106716454

Training epoch-13 batch-95
Running loss of epoch-13 batch-95 = 0.0009557914454489946

Training epoch-13 batch-96
Running loss of epoch-13 batch-96 = 0.002594278659671545

Training epoch-13 batch-97
Running loss of epoch-13 batch-97 = 0.0013274112716317177

Training epoch-13 batch-98
Running loss of epoch-13 batch-98 = 0.002163275144994259

Training epoch-13 batch-99
Running loss of epoch-13 batch-99 = 0.00294314743950963

Training epoch-13 batch-100
Running loss of epoch-13 batch-100 = 0.002439766889438033

Training epoch-13 batch-101
Running loss of epoch-13 batch-101 = 0.0008999815909191966

Training epoch-13 batch-102
Running loss of epoch-13 batch-102 = 0.0014901498798280954

Training epoch-13 batch-103
Running loss of epoch-13 batch-103 = 0.0026963939890265465

Training epoch-13 batch-104
Running loss of epoch-13 batch-104 = 0.0010275334352627397

Training epoch-13 batch-105
Running loss of epoch-13 batch-105 = 0.0007272133370861411

Training epoch-13 batch-106
Running loss of epoch-13 batch-106 = 0.0010383636690676212

Training epoch-13 batch-107
Running loss of epoch-13 batch-107 = 0.001464402535930276

Training epoch-13 batch-108
Running loss of epoch-13 batch-108 = 0.0019419958116486669

Training epoch-13 batch-109
Running loss of epoch-13 batch-109 = 0.002124100923538208

Training epoch-13 batch-110
Running loss of epoch-13 batch-110 = 0.002477612579241395

Training epoch-13 batch-111
Running loss of epoch-13 batch-111 = 0.0007963754469528794

Training epoch-13 batch-112
Running loss of epoch-13 batch-112 = 0.0023920172825455666

Training epoch-13 batch-113
Running loss of epoch-13 batch-113 = 0.0016062268987298012

Training epoch-13 batch-114
Running loss of epoch-13 batch-114 = 0.001190893235616386

Training epoch-13 batch-115
Running loss of epoch-13 batch-115 = 0.0020820489153265953

Training epoch-13 batch-116
Running loss of epoch-13 batch-116 = 0.0030173268169164658

Training epoch-13 batch-117
Running loss of epoch-13 batch-117 = 0.0013455114094540477

Training epoch-13 batch-118
Running loss of epoch-13 batch-118 = 0.0015365587314590812

Training epoch-13 batch-119
Running loss of epoch-13 batch-119 = 0.0012269815197214484

Training epoch-13 batch-120
Running loss of epoch-13 batch-120 = 0.0019223352428525686

Training epoch-13 batch-121
Running loss of epoch-13 batch-121 = 0.002936486853286624

Training epoch-13 batch-122
Running loss of epoch-13 batch-122 = 0.002137164818122983

Training epoch-13 batch-123
Running loss of epoch-13 batch-123 = 0.0019746762700378895

Training epoch-13 batch-124
Running loss of epoch-13 batch-124 = 0.0009249126887880266

Training epoch-13 batch-125
Running loss of epoch-13 batch-125 = 0.0014543397119268775

Training epoch-13 batch-126
Running loss of epoch-13 batch-126 = 0.001975251827389002

Training epoch-13 batch-127
Running loss of epoch-13 batch-127 = 0.0023638433776795864

Training epoch-13 batch-128
Running loss of epoch-13 batch-128 = 0.0005264360806904733

Training epoch-13 batch-129
Running loss of epoch-13 batch-129 = 0.0013796364655718207

Training epoch-13 batch-130
Running loss of epoch-13 batch-130 = 0.0006001218571327627

Training epoch-13 batch-131
Running loss of epoch-13 batch-131 = 0.0015298491343855858

Training epoch-13 batch-132
Running loss of epoch-13 batch-132 = 0.0027868321631103754

Training epoch-13 batch-133
Running loss of epoch-13 batch-133 = 0.001369319623336196

Training epoch-13 batch-134
Running loss of epoch-13 batch-134 = 0.0013253764482215047

Training epoch-13 batch-135
Running loss of epoch-13 batch-135 = 0.002755218418315053

Training epoch-13 batch-136
Running loss of epoch-13 batch-136 = 0.002474345499649644

Training epoch-13 batch-137
Running loss of epoch-13 batch-137 = 0.004740449134260416

Training epoch-13 batch-138
Running loss of epoch-13 batch-138 = 0.0019488150719553232

Training epoch-13 batch-139
Running loss of epoch-13 batch-139 = 0.0011225201888009906

Training epoch-13 batch-140
Running loss of epoch-13 batch-140 = 0.0014939632965251803

Training epoch-13 batch-141
Running loss of epoch-13 batch-141 = 0.001610665349289775

Training epoch-13 batch-142
Running loss of epoch-13 batch-142 = 0.0035365051589906216

Training epoch-13 batch-143
Running loss of epoch-13 batch-143 = 0.0016964440001174808

Training epoch-13 batch-144
Running loss of epoch-13 batch-144 = 0.003089978126809001

Training epoch-13 batch-145
Running loss of epoch-13 batch-145 = 0.0036136116832494736

Training epoch-13 batch-146
Running loss of epoch-13 batch-146 = 0.002018456580117345

Training epoch-13 batch-147
Running loss of epoch-13 batch-147 = 0.0036058230325579643

Training epoch-13 batch-148
Running loss of epoch-13 batch-148 = 0.002768332604318857

Training epoch-13 batch-149
Running loss of epoch-13 batch-149 = 0.003239754820242524

Training epoch-13 batch-150
Running loss of epoch-13 batch-150 = 0.0013108996208757162

Training epoch-13 batch-151
Running loss of epoch-13 batch-151 = 0.0017386242980137467

Training epoch-13 batch-152
Running loss of epoch-13 batch-152 = 0.0022524313535541296

Training epoch-13 batch-153
Running loss of epoch-13 batch-153 = 0.002040703548118472

Training epoch-13 batch-154
Running loss of epoch-13 batch-154 = 0.0013995897024869919

Training epoch-13 batch-155
Running loss of epoch-13 batch-155 = 0.0008933302597142756

Training epoch-13 batch-156
Running loss of epoch-13 batch-156 = 0.0011978180846199393

Training epoch-13 batch-157
Running loss of epoch-13 batch-157 = 0.010939860716462135

Finished training epoch-13.



Average train loss at epoch-13 = 0.0018964694514870643

Started Evaluation

Average val loss at epoch-13 = 3.671942132083993

Accuracy for classes:
Accuracy for class equals is: 58.42 %
Accuracy for class main is: 19.51 %
Accuracy for class setUp is: 42.95 %
Accuracy for class onCreate is: 31.34 %
Accuracy for class toString is: 38.57 %
Accuracy for class run is: 51.83 %
Accuracy for class hashCode is: 77.53 %
Accuracy for class init is: 30.27 %
Accuracy for class execute is: 6.43 %
Accuracy for class get is: 22.82 %

Overall Accuracy = 37.47 %

Finished Evaluation



Started training epoch-14


Training epoch-14 batch-1
Running loss of epoch-14 batch-1 = 0.0015708981081843376

Training epoch-14 batch-2
Running loss of epoch-14 batch-2 = 0.0031237276270985603

Training epoch-14 batch-3
Running loss of epoch-14 batch-3 = 0.0020342241041362286

Training epoch-14 batch-4
Running loss of epoch-14 batch-4 = 0.0005326126120053232

Training epoch-14 batch-5
Running loss of epoch-14 batch-5 = 0.002427242463454604

Training epoch-14 batch-6
Running loss of epoch-14 batch-6 = 0.0012256662594154477

Training epoch-14 batch-7
Running loss of epoch-14 batch-7 = 0.001239108038134873

Training epoch-14 batch-8
Running loss of epoch-14 batch-8 = 0.0019558672793209553

Training epoch-14 batch-9
Running loss of epoch-14 batch-9 = 0.0002720580669119954

Training epoch-14 batch-10
Running loss of epoch-14 batch-10 = 0.0019279058324173093

Training epoch-14 batch-11
Running loss of epoch-14 batch-11 = 0.0017224274342879653

Training epoch-14 batch-12
Running loss of epoch-14 batch-12 = 0.0005019050440751016

Training epoch-14 batch-13
Running loss of epoch-14 batch-13 = 0.001728803152218461

Training epoch-14 batch-14
Running loss of epoch-14 batch-14 = 0.0005467670853249729

Training epoch-14 batch-15
Running loss of epoch-14 batch-15 = 0.0007362743490375578

Training epoch-14 batch-16
Running loss of epoch-14 batch-16 = 0.001362923881970346

Training epoch-14 batch-17
Running loss of epoch-14 batch-17 = 0.0009448138880543411

Training epoch-14 batch-18
Running loss of epoch-14 batch-18 = 0.0007098389323800802

Training epoch-14 batch-19
Running loss of epoch-14 batch-19 = 0.0013914738083258271

Training epoch-14 batch-20
Running loss of epoch-14 batch-20 = 0.0006186560494825244

Training epoch-14 batch-21
Running loss of epoch-14 batch-21 = 0.002795371925458312

Training epoch-14 batch-22
Running loss of epoch-14 batch-22 = 0.0012519892770797014

Training epoch-14 batch-23
Running loss of epoch-14 batch-23 = 0.0018903882009908557

Training epoch-14 batch-24
Running loss of epoch-14 batch-24 = 0.0008637821883894503

Training epoch-14 batch-25
Running loss of epoch-14 batch-25 = 0.000967296480666846

Training epoch-14 batch-26
Running loss of epoch-14 batch-26 = 0.0011836802586913109

Training epoch-14 batch-27
Running loss of epoch-14 batch-27 = 0.0024907628539949656

Training epoch-14 batch-28
Running loss of epoch-14 batch-28 = 0.001513100927695632

Training epoch-14 batch-29
Running loss of epoch-14 batch-29 = 0.0019625979475677013

Training epoch-14 batch-30
Running loss of epoch-14 batch-30 = 0.0011211864184588194

Training epoch-14 batch-31
Running loss of epoch-14 batch-31 = 0.0018897553673014045

Training epoch-14 batch-32
Running loss of epoch-14 batch-32 = 0.0009113597334362566

Training epoch-14 batch-33
Running loss of epoch-14 batch-33 = 0.0018297245260328054

Training epoch-14 batch-34
Running loss of epoch-14 batch-34 = 0.0008033348130993545

Training epoch-14 batch-35
Running loss of epoch-14 batch-35 = 0.0008823082898743451

Training epoch-14 batch-36
Running loss of epoch-14 batch-36 = 0.0009155943989753723

Training epoch-14 batch-37
Running loss of epoch-14 batch-37 = 0.0010621717665344477

Training epoch-14 batch-38
Running loss of epoch-14 batch-38 = 0.0017507302109152079

Training epoch-14 batch-39
Running loss of epoch-14 batch-39 = 0.0023061560932546854

Training epoch-14 batch-40
Running loss of epoch-14 batch-40 = 0.00062157231150195

Training epoch-14 batch-41
Running loss of epoch-14 batch-41 = 0.001086568459868431

Training epoch-14 batch-42
Running loss of epoch-14 batch-42 = 0.0014666205970570445

Training epoch-14 batch-43
Running loss of epoch-14 batch-43 = 0.001629422651603818

Training epoch-14 batch-44
Running loss of epoch-14 batch-44 = 0.0008692204137332737

Training epoch-14 batch-45
Running loss of epoch-14 batch-45 = 0.0008898715605027974

Training epoch-14 batch-46
Running loss of epoch-14 batch-46 = 0.0005967816105112433

Training epoch-14 batch-47
Running loss of epoch-14 batch-47 = 0.0009675485780462623

Training epoch-14 batch-48
Running loss of epoch-14 batch-48 = 0.0008832315215840936

Training epoch-14 batch-49
Running loss of epoch-14 batch-49 = 0.0010813599219545722

Training epoch-14 batch-50
Running loss of epoch-14 batch-50 = 0.000703123165294528

Training epoch-14 batch-51
Running loss of epoch-14 batch-51 = 0.0012275937478989363

Training epoch-14 batch-52
Running loss of epoch-14 batch-52 = 0.0013188375160098076

Training epoch-14 batch-53
Running loss of epoch-14 batch-53 = 0.000683849910274148

Training epoch-14 batch-54
Running loss of epoch-14 batch-54 = 0.0015780474059283733

Training epoch-14 batch-55
Running loss of epoch-14 batch-55 = 0.0008058420498855412

Training epoch-14 batch-56
Running loss of epoch-14 batch-56 = 0.0009460553992539644

Training epoch-14 batch-57
Running loss of epoch-14 batch-57 = 0.0010215893853455782

Training epoch-14 batch-58
Running loss of epoch-14 batch-58 = 0.0018948210636153817

Training epoch-14 batch-59
Running loss of epoch-14 batch-59 = 0.0009350338950753212

Training epoch-14 batch-60
Running loss of epoch-14 batch-60 = 0.0027688932605087757

Training epoch-14 batch-61
Running loss of epoch-14 batch-61 = 0.0009278223151341081

Training epoch-14 batch-62
Running loss of epoch-14 batch-62 = 0.0006910801748745143

Training epoch-14 batch-63
Running loss of epoch-14 batch-63 = 0.001600187737494707

Training epoch-14 batch-64
Running loss of epoch-14 batch-64 = 0.0013053900329396129

Training epoch-14 batch-65
Running loss of epoch-14 batch-65 = 0.0016287955222651362

Training epoch-14 batch-66
Running loss of epoch-14 batch-66 = 0.0016657286323606968

Training epoch-14 batch-67
Running loss of epoch-14 batch-67 = 0.000784583215136081

Training epoch-14 batch-68
Running loss of epoch-14 batch-68 = 0.000963429978583008

Training epoch-14 batch-69
Running loss of epoch-14 batch-69 = 0.0018865313613787293

Training epoch-14 batch-70
Running loss of epoch-14 batch-70 = 0.0008517861133441329

Training epoch-14 batch-71
Running loss of epoch-14 batch-71 = 0.0014727544039487839

Training epoch-14 batch-72
Running loss of epoch-14 batch-72 = 0.002013346180319786

Training epoch-14 batch-73
Running loss of epoch-14 batch-73 = 0.002065015025436878

Training epoch-14 batch-74
Running loss of epoch-14 batch-74 = 0.0015437721740454435

Training epoch-14 batch-75
Running loss of epoch-14 batch-75 = 0.0020032774191349745

Training epoch-14 batch-76
Running loss of epoch-14 batch-76 = 0.001384214498102665

Training epoch-14 batch-77
Running loss of epoch-14 batch-77 = 0.001163240522146225

Training epoch-14 batch-78
Running loss of epoch-14 batch-78 = 0.0028449881356209517

Training epoch-14 batch-79
Running loss of epoch-14 batch-79 = 0.0016430218238383532

Training epoch-14 batch-80
Running loss of epoch-14 batch-80 = 0.002486123703420162

Training epoch-14 batch-81
Running loss of epoch-14 batch-81 = 0.0030385805293917656

Training epoch-14 batch-82
Running loss of epoch-14 batch-82 = 0.00269928015768528

Training epoch-14 batch-83
Running loss of epoch-14 batch-83 = 0.00034254114143550396

Training epoch-14 batch-84
Running loss of epoch-14 batch-84 = 0.0009876021649688482

Training epoch-14 batch-85
Running loss of epoch-14 batch-85 = 0.0015691714361310005

Training epoch-14 batch-86
Running loss of epoch-14 batch-86 = 0.0006602954235859215

Training epoch-14 batch-87
Running loss of epoch-14 batch-87 = 0.001552653731778264

Training epoch-14 batch-88
Running loss of epoch-14 batch-88 = 0.0018185722874477506

Training epoch-14 batch-89
Running loss of epoch-14 batch-89 = 0.0009044731850735843

Training epoch-14 batch-90
Running loss of epoch-14 batch-90 = 0.001198219251818955

Training epoch-14 batch-91
Running loss of epoch-14 batch-91 = 0.001271841349080205

Training epoch-14 batch-92
Running loss of epoch-14 batch-92 = 0.0013557587517425418

Training epoch-14 batch-93
Running loss of epoch-14 batch-93 = 0.0008675477001816034

Training epoch-14 batch-94
Running loss of epoch-14 batch-94 = 0.002604779787361622

Training epoch-14 batch-95
Running loss of epoch-14 batch-95 = 0.0017871801974251866

Training epoch-14 batch-96
Running loss of epoch-14 batch-96 = 0.0016729618655517697

Training epoch-14 batch-97
Running loss of epoch-14 batch-97 = 0.002094611758366227

Training epoch-14 batch-98
Running loss of epoch-14 batch-98 = 0.0009859903948381543

Training epoch-14 batch-99
Running loss of epoch-14 batch-99 = 0.0014844664838165045

Training epoch-14 batch-100
Running loss of epoch-14 batch-100 = 0.0013983899261802435

Training epoch-14 batch-101
Running loss of epoch-14 batch-101 = 0.001346689066849649

Training epoch-14 batch-102
Running loss of epoch-14 batch-102 = 0.000581827072892338

Training epoch-14 batch-103
Running loss of epoch-14 batch-103 = 0.0010063565568998456

Training epoch-14 batch-104
Running loss of epoch-14 batch-104 = 0.002797667169943452

Training epoch-14 batch-105
Running loss of epoch-14 batch-105 = 0.0022762957960367203

Training epoch-14 batch-106
Running loss of epoch-14 batch-106 = 0.0017010780284181237

Training epoch-14 batch-107
Running loss of epoch-14 batch-107 = 0.0008634133846499026

Training epoch-14 batch-108
Running loss of epoch-14 batch-108 = 0.0015601783525198698

Training epoch-14 batch-109
Running loss of epoch-14 batch-109 = 0.0012757244985550642

Training epoch-14 batch-110
Running loss of epoch-14 batch-110 = 0.0006910886149853468

Training epoch-14 batch-111
Running loss of epoch-14 batch-111 = 0.0008652967517264187

Training epoch-14 batch-112
Running loss of epoch-14 batch-112 = 0.0027373856864869595

Training epoch-14 batch-113
Running loss of epoch-14 batch-113 = 0.0005146622424945235

Training epoch-14 batch-114
Running loss of epoch-14 batch-114 = 0.0009838419500738382

Training epoch-14 batch-115
Running loss of epoch-14 batch-115 = 0.0018529938533902168

Training epoch-14 batch-116
Running loss of epoch-14 batch-116 = 0.0017706268699839711

Training epoch-14 batch-117
Running loss of epoch-14 batch-117 = 0.001101493602618575

Training epoch-14 batch-118
Running loss of epoch-14 batch-118 = 0.001548462430946529

Training epoch-14 batch-119
Running loss of epoch-14 batch-119 = 0.0008621684974059463

Training epoch-14 batch-120
Running loss of epoch-14 batch-120 = 0.001403267728164792

Training epoch-14 batch-121
Running loss of epoch-14 batch-121 = 0.0015002386644482613

Training epoch-14 batch-122
Running loss of epoch-14 batch-122 = 0.000784292642492801

Training epoch-14 batch-123
Running loss of epoch-14 batch-123 = 0.0008816333138383925

Training epoch-14 batch-124
Running loss of epoch-14 batch-124 = 0.0041895643807947636

Training epoch-14 batch-125
Running loss of epoch-14 batch-125 = 0.0014690712559968233

Training epoch-14 batch-126
Running loss of epoch-14 batch-126 = 0.0011505214497447014

Training epoch-14 batch-127
Running loss of epoch-14 batch-127 = 0.0019407236250117421

Training epoch-14 batch-128
Running loss of epoch-14 batch-128 = 0.0018834801157936454

Training epoch-14 batch-129
Running loss of epoch-14 batch-129 = 0.0008829154539853334

Training epoch-14 batch-130
Running loss of epoch-14 batch-130 = 0.0017546798335388303

Training epoch-14 batch-131
Running loss of epoch-14 batch-131 = 0.0010225849691778421

Training epoch-14 batch-132
Running loss of epoch-14 batch-132 = 0.0021371475886553526

Training epoch-14 batch-133
Running loss of epoch-14 batch-133 = 0.0008918744279071689

Training epoch-14 batch-134
Running loss of epoch-14 batch-134 = 0.0022253645583987236

Training epoch-14 batch-135
Running loss of epoch-14 batch-135 = 0.0012196465395390987

Training epoch-14 batch-136
Running loss of epoch-14 batch-136 = 0.0011141101131215692

Training epoch-14 batch-137
Running loss of epoch-14 batch-137 = 0.0012757485965266824

Training epoch-14 batch-138
Running loss of epoch-14 batch-138 = 0.0013152086175978184

Training epoch-14 batch-139
Running loss of epoch-14 batch-139 = 0.00282850069925189

Training epoch-14 batch-140
Running loss of epoch-14 batch-140 = 0.0035454349126666784

Training epoch-14 batch-141
Running loss of epoch-14 batch-141 = 0.00122649478726089

Training epoch-14 batch-142
Running loss of epoch-14 batch-142 = 0.002564046997576952

Training epoch-14 batch-143
Running loss of epoch-14 batch-143 = 0.0008175995899364352

Training epoch-14 batch-144
Running loss of epoch-14 batch-144 = 0.0017928576562553644

Training epoch-14 batch-145
Running loss of epoch-14 batch-145 = 0.0007202312699519098

Training epoch-14 batch-146
Running loss of epoch-14 batch-146 = 0.0025487272068858147

Training epoch-14 batch-147
Running loss of epoch-14 batch-147 = 0.00574331171810627

Training epoch-14 batch-148
Running loss of epoch-14 batch-148 = 0.0013731635408475995

Training epoch-14 batch-149
Running loss of epoch-14 batch-149 = 0.001827594474889338

Training epoch-14 batch-150
Running loss of epoch-14 batch-150 = 0.004290084820240736

Training epoch-14 batch-151
Running loss of epoch-14 batch-151 = 0.0007225791341625154

Training epoch-14 batch-152
Running loss of epoch-14 batch-152 = 0.002559922868385911

Training epoch-14 batch-153
Running loss of epoch-14 batch-153 = 0.0038560517132282257

Training epoch-14 batch-154
Running loss of epoch-14 batch-154 = 0.0010532222222536802

Training epoch-14 batch-155
Running loss of epoch-14 batch-155 = 0.0027027511969208717

Training epoch-14 batch-156
Running loss of epoch-14 batch-156 = 0.002223961055278778

Training epoch-14 batch-157
Running loss of epoch-14 batch-157 = 0.02593287266790867

Finished training epoch-14.



Average train loss at epoch-14 = 0.001564284511655569

Started Evaluation

Average val loss at epoch-14 = 3.640522516087482

Accuracy for classes:
Accuracy for class equals is: 60.40 %
Accuracy for class main is: 38.03 %
Accuracy for class setUp is: 36.23 %
Accuracy for class onCreate is: 27.93 %
Accuracy for class toString is: 24.23 %
Accuracy for class run is: 27.40 %
Accuracy for class hashCode is: 76.03 %
Accuracy for class init is: 56.50 %
Accuracy for class execute is: 21.69 %
Accuracy for class get is: 13.33 %

Overall Accuracy = 37.82 %

Finished Evaluation



Started training epoch-15


Training epoch-15 batch-1
Running loss of epoch-15 batch-1 = 0.0033006728626787663

Training epoch-15 batch-2
Running loss of epoch-15 batch-2 = 0.002250976162031293

Training epoch-15 batch-3
Running loss of epoch-15 batch-3 = 0.0022135507315397263

Training epoch-15 batch-4
Running loss of epoch-15 batch-4 = 0.0014380165375769138

Training epoch-15 batch-5
Running loss of epoch-15 batch-5 = 0.003173193195834756

Training epoch-15 batch-6
Running loss of epoch-15 batch-6 = 0.0032534142956137657

Training epoch-15 batch-7
Running loss of epoch-15 batch-7 = 0.0030051192734390497

Training epoch-15 batch-8
Running loss of epoch-15 batch-8 = 0.0014679512241855264

Training epoch-15 batch-9
Running loss of epoch-15 batch-9 = 0.003008489729836583

Training epoch-15 batch-10
Running loss of epoch-15 batch-10 = 0.0038674455136060715

Training epoch-15 batch-11
Running loss of epoch-15 batch-11 = 0.0006490033119916916

Training epoch-15 batch-12
Running loss of epoch-15 batch-12 = 0.0012934586266055703

Training epoch-15 batch-13
Running loss of epoch-15 batch-13 = 0.0015543170738965273

Training epoch-15 batch-14
Running loss of epoch-15 batch-14 = 0.002214285312220454

Training epoch-15 batch-15
Running loss of epoch-15 batch-15 = 0.005092880222946405

Training epoch-15 batch-16
Running loss of epoch-15 batch-16 = 0.0010505503742024302

Training epoch-15 batch-17
Running loss of epoch-15 batch-17 = 0.0016256598755717278

Training epoch-15 batch-18
Running loss of epoch-15 batch-18 = 0.0020591833163052797

Training epoch-15 batch-19
Running loss of epoch-15 batch-19 = 0.0007617722731083632

Training epoch-15 batch-20
Running loss of epoch-15 batch-20 = 0.0020292759872972965

Training epoch-15 batch-21
Running loss of epoch-15 batch-21 = 0.0019919387996196747

Training epoch-15 batch-22
Running loss of epoch-15 batch-22 = 0.004635726101696491

Training epoch-15 batch-23
Running loss of epoch-15 batch-23 = 0.0013448289828374982

Training epoch-15 batch-24
Running loss of epoch-15 batch-24 = 0.0018048967467620969

Training epoch-15 batch-25
Running loss of epoch-15 batch-25 = 0.003476266050711274

Training epoch-15 batch-26
Running loss of epoch-15 batch-26 = 0.0028014907147735357

Training epoch-15 batch-27
Running loss of epoch-15 batch-27 = 0.003244255669414997

Training epoch-15 batch-28
Running loss of epoch-15 batch-28 = 0.0020694087725132704

Training epoch-15 batch-29
Running loss of epoch-15 batch-29 = 0.0037828630302101374

Training epoch-15 batch-30
Running loss of epoch-15 batch-30 = 0.0011858048383146524

Training epoch-15 batch-31
Running loss of epoch-15 batch-31 = 0.001761315856128931

Training epoch-15 batch-32
Running loss of epoch-15 batch-32 = 0.0023208954371511936

Training epoch-15 batch-33
Running loss of epoch-15 batch-33 = 0.0018440841231495142

Training epoch-15 batch-34
Running loss of epoch-15 batch-34 = 0.004201523493975401

Training epoch-15 batch-35
Running loss of epoch-15 batch-35 = 0.0007259820704348385

Training epoch-15 batch-36
Running loss of epoch-15 batch-36 = 0.0017871728632599115

Training epoch-15 batch-37
Running loss of epoch-15 batch-37 = 0.0033463004510849714

Training epoch-15 batch-38
Running loss of epoch-15 batch-38 = 0.004210303537547588

Training epoch-15 batch-39
Running loss of epoch-15 batch-39 = 0.0012808077735826373

Training epoch-15 batch-40
Running loss of epoch-15 batch-40 = 0.0022292996291071177

Training epoch-15 batch-41
Running loss of epoch-15 batch-41 = 0.004083002917468548

Training epoch-15 batch-42
Running loss of epoch-15 batch-42 = 0.0014810798456892371

Training epoch-15 batch-43
Running loss of epoch-15 batch-43 = 0.001150011084973812

Training epoch-15 batch-44
Running loss of epoch-15 batch-44 = 0.001268841908313334

Training epoch-15 batch-45
Running loss of epoch-15 batch-45 = 0.0009995324071496725

Training epoch-15 batch-46
Running loss of epoch-15 batch-46 = 0.0008861054666340351

Training epoch-15 batch-47
Running loss of epoch-15 batch-47 = 0.0019870323594659567

Training epoch-15 batch-48
Running loss of epoch-15 batch-48 = 0.0016810867236927152

Training epoch-15 batch-49
Running loss of epoch-15 batch-49 = 0.001615361776202917

Training epoch-15 batch-50
Running loss of epoch-15 batch-50 = 0.001243433216586709

Training epoch-15 batch-51
Running loss of epoch-15 batch-51 = 0.002595661673694849

Training epoch-15 batch-52
Running loss of epoch-15 batch-52 = 0.0010602641850709915

Training epoch-15 batch-53
Running loss of epoch-15 batch-53 = 0.0018091029487550259

Training epoch-15 batch-54
Running loss of epoch-15 batch-54 = 0.0014808994019404054

Training epoch-15 batch-55
Running loss of epoch-15 batch-55 = 0.0012012663064524531

Training epoch-15 batch-56
Running loss of epoch-15 batch-56 = 0.0011152138467878103

Training epoch-15 batch-57
Running loss of epoch-15 batch-57 = 0.0019867438822984695

Training epoch-15 batch-58
Running loss of epoch-15 batch-58 = 0.0007238503312692046

Training epoch-15 batch-59
Running loss of epoch-15 batch-59 = 0.0020499899983406067

Training epoch-15 batch-60
Running loss of epoch-15 batch-60 = 0.0012960548046976328

Training epoch-15 batch-61
Running loss of epoch-15 batch-61 = 0.0011219946900382638

Training epoch-15 batch-62
Running loss of epoch-15 batch-62 = 0.002811325481161475

Training epoch-15 batch-63
Running loss of epoch-15 batch-63 = 0.0013058737386018038

Training epoch-15 batch-64
Running loss of epoch-15 batch-64 = 0.0007952912710607052

Training epoch-15 batch-65
Running loss of epoch-15 batch-65 = 0.0002850571181625128

Training epoch-15 batch-66
Running loss of epoch-15 batch-66 = 0.001382815302349627

Training epoch-15 batch-67
Running loss of epoch-15 batch-67 = 0.00047989876475185156

Training epoch-15 batch-68
Running loss of epoch-15 batch-68 = 0.0003898575669154525

Training epoch-15 batch-69
Running loss of epoch-15 batch-69 = 0.0015750217717140913

Training epoch-15 batch-70
Running loss of epoch-15 batch-70 = 0.0007901540957391262

Training epoch-15 batch-71
Running loss of epoch-15 batch-71 = 0.0005616953130811453

Training epoch-15 batch-72
Running loss of epoch-15 batch-72 = 0.000663891201838851

Training epoch-15 batch-73
Running loss of epoch-15 batch-73 = 0.0006927805952727795

Training epoch-15 batch-74
Running loss of epoch-15 batch-74 = 0.0019100564531981945

Training epoch-15 batch-75
Running loss of epoch-15 batch-75 = 0.001397387939505279

Training epoch-15 batch-76
Running loss of epoch-15 batch-76 = 0.001241929130628705

Training epoch-15 batch-77
Running loss of epoch-15 batch-77 = 0.0007396974833682179

Training epoch-15 batch-78
Running loss of epoch-15 batch-78 = 0.001093040918931365

Training epoch-15 batch-79
Running loss of epoch-15 batch-79 = 0.0003122935304418206

Training epoch-15 batch-80
Running loss of epoch-15 batch-80 = 0.0016368150245398283

Training epoch-15 batch-81
Running loss of epoch-15 batch-81 = 0.002212412655353546

Training epoch-15 batch-82
Running loss of epoch-15 batch-82 = 0.000840826949570328

Training epoch-15 batch-83
Running loss of epoch-15 batch-83 = 0.0005530148046091199

Training epoch-15 batch-84
Running loss of epoch-15 batch-84 = 0.0008321653003804386

Training epoch-15 batch-85
Running loss of epoch-15 batch-85 = 0.0008711489499546587

Training epoch-15 batch-86
Running loss of epoch-15 batch-86 = 0.0009996492881327868

Training epoch-15 batch-87
Running loss of epoch-15 batch-87 = 0.002072259783744812

Training epoch-15 batch-88
Running loss of epoch-15 batch-88 = 0.0009870026260614395

Training epoch-15 batch-89
Running loss of epoch-15 batch-89 = 0.0015454229433089495

Training epoch-15 batch-90
Running loss of epoch-15 batch-90 = 0.0012519999872893095

Training epoch-15 batch-91
Running loss of epoch-15 batch-91 = 0.0012062826426699758

Training epoch-15 batch-92
Running loss of epoch-15 batch-92 = 0.0012013990199193358

Training epoch-15 batch-93
Running loss of epoch-15 batch-93 = 0.0008138202247209847

Training epoch-15 batch-94
Running loss of epoch-15 batch-94 = 0.00076695391908288

Training epoch-15 batch-95
Running loss of epoch-15 batch-95 = 0.000708986830431968

Training epoch-15 batch-96
Running loss of epoch-15 batch-96 = 0.0009401290444657207

Training epoch-15 batch-97
Running loss of epoch-15 batch-97 = 0.0008328541880473495

Training epoch-15 batch-98
Running loss of epoch-15 batch-98 = 0.0017346825916320086

Training epoch-15 batch-99
Running loss of epoch-15 batch-99 = 0.000652781396638602

Training epoch-15 batch-100
Running loss of epoch-15 batch-100 = 0.0015854373341426253

Training epoch-15 batch-101
Running loss of epoch-15 batch-101 = 0.0014084684662520885

Training epoch-15 batch-102
Running loss of epoch-15 batch-102 = 0.0011005192063748837

Training epoch-15 batch-103
Running loss of epoch-15 batch-103 = 0.0005798072670586407

Training epoch-15 batch-104
Running loss of epoch-15 batch-104 = 0.0010288169141858816

Training epoch-15 batch-105
Running loss of epoch-15 batch-105 = 0.0009220641804859042

Training epoch-15 batch-106
Running loss of epoch-15 batch-106 = 0.0026344330981373787

Training epoch-15 batch-107
Running loss of epoch-15 batch-107 = 0.0006446062470786273

Training epoch-15 batch-108
Running loss of epoch-15 batch-108 = 0.00043158739572390914

Training epoch-15 batch-109
Running loss of epoch-15 batch-109 = 0.0018787370063364506

Training epoch-15 batch-110
Running loss of epoch-15 batch-110 = 0.0007264008745551109

Training epoch-15 batch-111
Running loss of epoch-15 batch-111 = 0.0008532402571290731

Training epoch-15 batch-112
Running loss of epoch-15 batch-112 = 0.0019557306077331305

Training epoch-15 batch-113
Running loss of epoch-15 batch-113 = 0.0010613249614834785

Training epoch-15 batch-114
Running loss of epoch-15 batch-114 = 0.0034789908677339554

Training epoch-15 batch-115
Running loss of epoch-15 batch-115 = 0.00036788248689845204

Training epoch-15 batch-116
Running loss of epoch-15 batch-116 = 0.0029861475341022015

Training epoch-15 batch-117
Running loss of epoch-15 batch-117 = 0.0017807458061724901

Training epoch-15 batch-118
Running loss of epoch-15 batch-118 = 0.0023769449908286333

Training epoch-15 batch-119
Running loss of epoch-15 batch-119 = 0.0013978827046230435

Training epoch-15 batch-120
Running loss of epoch-15 batch-120 = 0.0009595801820978522

Training epoch-15 batch-121
Running loss of epoch-15 batch-121 = 0.0010471716523170471

Training epoch-15 batch-122
Running loss of epoch-15 batch-122 = 0.0005835376214236021

Training epoch-15 batch-123
Running loss of epoch-15 batch-123 = 0.0018690606812015176

Training epoch-15 batch-124
Running loss of epoch-15 batch-124 = 0.0008982319268397987

Training epoch-15 batch-125
Running loss of epoch-15 batch-125 = 0.0007531135925091803

Training epoch-15 batch-126
Running loss of epoch-15 batch-126 = 0.0010170876048505306

Training epoch-15 batch-127
Running loss of epoch-15 batch-127 = 0.0007786030764691532

Training epoch-15 batch-128
Running loss of epoch-15 batch-128 = 0.0008786100661382079

Training epoch-15 batch-129
Running loss of epoch-15 batch-129 = 0.0007019824697636068

Training epoch-15 batch-130
Running loss of epoch-15 batch-130 = 0.002093126066029072

Training epoch-15 batch-131
Running loss of epoch-15 batch-131 = 0.001106416922993958

Training epoch-15 batch-132
Running loss of epoch-15 batch-132 = 0.001472054049372673

Training epoch-15 batch-133
Running loss of epoch-15 batch-133 = 0.0004391339607536793

Training epoch-15 batch-134
Running loss of epoch-15 batch-134 = 0.0013745035976171494

Training epoch-15 batch-135
Running loss of epoch-15 batch-135 = 0.0004252739017829299

Training epoch-15 batch-136
Running loss of epoch-15 batch-136 = 0.0006965920911170542

Training epoch-15 batch-137
Running loss of epoch-15 batch-137 = 0.0006137475720606744

Training epoch-15 batch-138
Running loss of epoch-15 batch-138 = 0.0029583682771772146

Training epoch-15 batch-139
Running loss of epoch-15 batch-139 = 0.00019865494687110186

Training epoch-15 batch-140
Running loss of epoch-15 batch-140 = 0.0006541699985973537

Training epoch-15 batch-141
Running loss of epoch-15 batch-141 = 0.0022762848529964685

Training epoch-15 batch-142
Running loss of epoch-15 batch-142 = 0.0012376517988741398

Training epoch-15 batch-143
Running loss of epoch-15 batch-143 = 0.0005472389166243374

Training epoch-15 batch-144
Running loss of epoch-15 batch-144 = 0.00023430411238223314

Training epoch-15 batch-145
Running loss of epoch-15 batch-145 = 0.0018381956033408642

Training epoch-15 batch-146
Running loss of epoch-15 batch-146 = 0.0010853472631424665

Training epoch-15 batch-147
Running loss of epoch-15 batch-147 = 0.0006200360367074609

Training epoch-15 batch-148
Running loss of epoch-15 batch-148 = 0.000808043812867254

Training epoch-15 batch-149
Running loss of epoch-15 batch-149 = 0.0018885820172727108

Training epoch-15 batch-150
Running loss of epoch-15 batch-150 = 0.0007776494021527469

Training epoch-15 batch-151
Running loss of epoch-15 batch-151 = 0.0019007923547178507

Training epoch-15 batch-152
Running loss of epoch-15 batch-152 = 0.0010206276783719659

Training epoch-15 batch-153
Running loss of epoch-15 batch-153 = 0.0012766417348757386

Training epoch-15 batch-154
Running loss of epoch-15 batch-154 = 0.0007925415993668139

Training epoch-15 batch-155
Running loss of epoch-15 batch-155 = 0.00014947436284273863

Training epoch-15 batch-156
Running loss of epoch-15 batch-156 = 0.0005128408083692193

Training epoch-15 batch-157
Running loss of epoch-15 batch-157 = 0.007673696614801884

Finished training epoch-15.



Average train loss at epoch-15 = 0.001528851529210806

Started Evaluation

Average val loss at epoch-15 = 3.8941278739979395

Accuracy for classes:
Accuracy for class equals is: 58.25 %
Accuracy for class main is: 44.92 %
Accuracy for class setUp is: 28.36 %
Accuracy for class onCreate is: 33.48 %
Accuracy for class toString is: 43.69 %
Accuracy for class run is: 35.62 %
Accuracy for class hashCode is: 68.91 %
Accuracy for class init is: 23.54 %
Accuracy for class execute is: 27.31 %
Accuracy for class get is: 27.18 %

Overall Accuracy = 38.39 %

Finished Evaluation



Started training epoch-16


Training epoch-16 batch-1
Running loss of epoch-16 batch-1 = 0.0002983905142173171

Training epoch-16 batch-2
Running loss of epoch-16 batch-2 = 0.0010438206372782588

Training epoch-16 batch-3
Running loss of epoch-16 batch-3 = 0.0013103828532621264

Training epoch-16 batch-4
Running loss of epoch-16 batch-4 = 0.0003280177479609847

Training epoch-16 batch-5
Running loss of epoch-16 batch-5 = 0.0007394267013296485

Training epoch-16 batch-6
Running loss of epoch-16 batch-6 = 0.0007896674796938896

Training epoch-16 batch-7
Running loss of epoch-16 batch-7 = 0.0005160468281246722

Training epoch-16 batch-8
Running loss of epoch-16 batch-8 = 0.00027094478718936443

Training epoch-16 batch-9
Running loss of epoch-16 batch-9 = 0.00030010653426870704

Training epoch-16 batch-10
Running loss of epoch-16 batch-10 = 0.00022393843391910195

Training epoch-16 batch-11
Running loss of epoch-16 batch-11 = 0.00044937641359865665

Training epoch-16 batch-12
Running loss of epoch-16 batch-12 = 0.00044272380182519555

Training epoch-16 batch-13
Running loss of epoch-16 batch-13 = 0.0012842334108427167

Training epoch-16 batch-14
Running loss of epoch-16 batch-14 = 0.0005761185311712325

Training epoch-16 batch-15
Running loss of epoch-16 batch-15 = 0.00024765566922724247

Training epoch-16 batch-16
Running loss of epoch-16 batch-16 = 0.0005504623404704034

Training epoch-16 batch-17
Running loss of epoch-16 batch-17 = 0.0005797147750854492

Training epoch-16 batch-18
Running loss of epoch-16 batch-18 = 0.00032921729143708944

Training epoch-16 batch-19
Running loss of epoch-16 batch-19 = 0.0002575323451310396

Training epoch-16 batch-20
Running loss of epoch-16 batch-20 = 0.0006426367908716202

Training epoch-16 batch-21
Running loss of epoch-16 batch-21 = 0.0002952690701931715

Training epoch-16 batch-22
Running loss of epoch-16 batch-22 = 0.0006303302943706512

Training epoch-16 batch-23
Running loss of epoch-16 batch-23 = 0.0008344304515048862

Training epoch-16 batch-24
Running loss of epoch-16 batch-24 = 0.00043827330227941275

Training epoch-16 batch-25
Running loss of epoch-16 batch-25 = 0.00036962039303034544

Training epoch-16 batch-26
Running loss of epoch-16 batch-26 = 0.0004656386445276439

Training epoch-16 batch-27
Running loss of epoch-16 batch-27 = 0.0007925141253508627

Training epoch-16 batch-28
Running loss of epoch-16 batch-28 = 0.00022036267910152674

Training epoch-16 batch-29
Running loss of epoch-16 batch-29 = 0.0005651087849400938

Training epoch-16 batch-30
Running loss of epoch-16 batch-30 = 0.0007618148229084909

Training epoch-16 batch-31
Running loss of epoch-16 batch-31 = 0.0005927487509325147

Training epoch-16 batch-32
Running loss of epoch-16 batch-32 = 0.00036347343120723963

Training epoch-16 batch-33
Running loss of epoch-16 batch-33 = 0.00020490522729232907

Training epoch-16 batch-34
Running loss of epoch-16 batch-34 = 0.0005809114081785083

Training epoch-16 batch-35
Running loss of epoch-16 batch-35 = 0.00046899711014702916

Training epoch-16 batch-36
Running loss of epoch-16 batch-36 = 0.00040163646917790174

Training epoch-16 batch-37
Running loss of epoch-16 batch-37 = 0.0002199959708377719

Training epoch-16 batch-38
Running loss of epoch-16 batch-38 = 0.0007320357253775001

Training epoch-16 batch-39
Running loss of epoch-16 batch-39 = 0.0011822311207652092

Training epoch-16 batch-40
Running loss of epoch-16 batch-40 = 0.0005575272371061146

Training epoch-16 batch-41
Running loss of epoch-16 batch-41 = 0.0007591261528432369

Training epoch-16 batch-42
Running loss of epoch-16 batch-42 = 0.0004088733112439513

Training epoch-16 batch-43
Running loss of epoch-16 batch-43 = 0.00022561755031347275

Training epoch-16 batch-44
Running loss of epoch-16 batch-44 = 0.0005607368075288832

Training epoch-16 batch-45
Running loss of epoch-16 batch-45 = 0.0011604472529143095

Training epoch-16 batch-46
Running loss of epoch-16 batch-46 = 0.0004291953518986702

Training epoch-16 batch-47
Running loss of epoch-16 batch-47 = 0.0002929788315668702

Training epoch-16 batch-48
Running loss of epoch-16 batch-48 = 0.0020666811615228653

Training epoch-16 batch-49
Running loss of epoch-16 batch-49 = 0.00021866464521735907

Training epoch-16 batch-50
Running loss of epoch-16 batch-50 = 0.0005739022744819522

Training epoch-16 batch-51
Running loss of epoch-16 batch-51 = 0.00021956872660666704

Training epoch-16 batch-52
Running loss of epoch-16 batch-52 = 0.0004954660544171929

Training epoch-16 batch-53
Running loss of epoch-16 batch-53 = 0.001069026067852974

Training epoch-16 batch-54
Running loss of epoch-16 batch-54 = 0.00040025258203968406

Training epoch-16 batch-55
Running loss of epoch-16 batch-55 = 0.000744324061088264

Training epoch-16 batch-56
Running loss of epoch-16 batch-56 = 0.0007650667685084045

Training epoch-16 batch-57
Running loss of epoch-16 batch-57 = 0.00023098947713151574

Training epoch-16 batch-58
Running loss of epoch-16 batch-58 = 0.0005910955369472504

Training epoch-16 batch-59
Running loss of epoch-16 batch-59 = 0.000565263326279819

Training epoch-16 batch-60
Running loss of epoch-16 batch-60 = 0.00020299589959904552

Training epoch-16 batch-61
Running loss of epoch-16 batch-61 = 0.00039149500662460923

Training epoch-16 batch-62
Running loss of epoch-16 batch-62 = 0.0005160053260624409

Training epoch-16 batch-63
Running loss of epoch-16 batch-63 = 0.00022956868633627892

Training epoch-16 batch-64
Running loss of epoch-16 batch-64 = 0.0002372866729274392

Training epoch-16 batch-65
Running loss of epoch-16 batch-65 = 0.000934375450015068

Training epoch-16 batch-66
Running loss of epoch-16 batch-66 = 0.0003833252121694386

Training epoch-16 batch-67
Running loss of epoch-16 batch-67 = 0.0006407786859199405

Training epoch-16 batch-68
Running loss of epoch-16 batch-68 = 0.0016415553400292993

Training epoch-16 batch-69
Running loss of epoch-16 batch-69 = 0.0003022580058313906

Training epoch-16 batch-70
Running loss of epoch-16 batch-70 = 0.00020262901671230793

Training epoch-16 batch-71
Running loss of epoch-16 batch-71 = 0.00042637146543711424

Training epoch-16 batch-72
Running loss of epoch-16 batch-72 = 0.0008220326853916049

Training epoch-16 batch-73
Running loss of epoch-16 batch-73 = 0.0003245435655117035

Training epoch-16 batch-74
Running loss of epoch-16 batch-74 = 0.0006579912733286619

Training epoch-16 batch-75
Running loss of epoch-16 batch-75 = 0.0009635317837819457

Training epoch-16 batch-76
Running loss of epoch-16 batch-76 = 0.0003613941080402583

Training epoch-16 batch-77
Running loss of epoch-16 batch-77 = 0.00019047025125473738

Training epoch-16 batch-78
Running loss of epoch-16 batch-78 = 0.00020974944345653057

Training epoch-16 batch-79
Running loss of epoch-16 batch-79 = 0.000481819617561996

Training epoch-16 batch-80
Running loss of epoch-16 batch-80 = 0.0003134802682325244

Training epoch-16 batch-81
Running loss of epoch-16 batch-81 = 0.0009671759326010942

Training epoch-16 batch-82
Running loss of epoch-16 batch-82 = 0.0002097940305247903

Training epoch-16 batch-83
Running loss of epoch-16 batch-83 = 0.0004227073222864419

Training epoch-16 batch-84
Running loss of epoch-16 batch-84 = 0.00043697503861039877

Training epoch-16 batch-85
Running loss of epoch-16 batch-85 = 0.0001511209411546588

Training epoch-16 batch-86
Running loss of epoch-16 batch-86 = 0.000567272596526891

Training epoch-16 batch-87
Running loss of epoch-16 batch-87 = 0.0005226351204328239

Training epoch-16 batch-88
Running loss of epoch-16 batch-88 = 0.0005050873151049018

Training epoch-16 batch-89
Running loss of epoch-16 batch-89 = 0.0011950836051255465

Training epoch-16 batch-90
Running loss of epoch-16 batch-90 = 0.000984019716270268

Training epoch-16 batch-91
Running loss of epoch-16 batch-91 = 0.00033156853169202805

Training epoch-16 batch-92
Running loss of epoch-16 batch-92 = 0.0002208198420703411

Training epoch-16 batch-93
Running loss of epoch-16 batch-93 = 0.000556032988242805

Training epoch-16 batch-94
Running loss of epoch-16 batch-94 = 0.001293484470807016

Training epoch-16 batch-95
Running loss of epoch-16 batch-95 = 0.0009127978701144457

Training epoch-16 batch-96
Running loss of epoch-16 batch-96 = 0.00030292628798633814

Training epoch-16 batch-97
Running loss of epoch-16 batch-97 = 0.0011987326433882117

Training epoch-16 batch-98
Running loss of epoch-16 batch-98 = 0.0007139833178371191

Training epoch-16 batch-99
Running loss of epoch-16 batch-99 = 0.0003651723964139819

Training epoch-16 batch-100
Running loss of epoch-16 batch-100 = 0.0010502116056159139

Training epoch-16 batch-101
Running loss of epoch-16 batch-101 = 0.0008241578470915556

Training epoch-16 batch-102
Running loss of epoch-16 batch-102 = 0.0003476237179711461

Training epoch-16 batch-103
Running loss of epoch-16 batch-103 = 0.0008076874073594809

Training epoch-16 batch-104
Running loss of epoch-16 batch-104 = 0.000939708377700299

Training epoch-16 batch-105
Running loss of epoch-16 batch-105 = 0.0006580029148608446

Training epoch-16 batch-106
Running loss of epoch-16 batch-106 = 0.00046602124348282814

Training epoch-16 batch-107
Running loss of epoch-16 batch-107 = 0.00207492895424366

Training epoch-16 batch-108
Running loss of epoch-16 batch-108 = 0.0004012502613477409

Training epoch-16 batch-109
Running loss of epoch-16 batch-109 = 0.0006420076824724674

Training epoch-16 batch-110
Running loss of epoch-16 batch-110 = 0.0018612723797559738

Training epoch-16 batch-111
Running loss of epoch-16 batch-111 = 0.00041380932088941336

Training epoch-16 batch-112
Running loss of epoch-16 batch-112 = 0.0005358246271498501

Training epoch-16 batch-113
Running loss of epoch-16 batch-113 = 0.0003039060393348336

Training epoch-16 batch-114
Running loss of epoch-16 batch-114 = 0.0016651770565658808

Training epoch-16 batch-115
Running loss of epoch-16 batch-115 = 0.0007519160863012075

Training epoch-16 batch-116
Running loss of epoch-16 batch-116 = 0.00043441314483061433

Training epoch-16 batch-117
Running loss of epoch-16 batch-117 = 0.0002893969649448991

Training epoch-16 batch-118
Running loss of epoch-16 batch-118 = 0.0009911800734698772

Training epoch-16 batch-119
Running loss of epoch-16 batch-119 = 0.0013686318416148424

Training epoch-16 batch-120
Running loss of epoch-16 batch-120 = 0.000602306448854506

Training epoch-16 batch-121
Running loss of epoch-16 batch-121 = 0.0011973755899816751

Training epoch-16 batch-122
Running loss of epoch-16 batch-122 = 0.0011607857886701822

Training epoch-16 batch-123
Running loss of epoch-16 batch-123 = 0.0006314768688753247

Training epoch-16 batch-124
Running loss of epoch-16 batch-124 = 0.0016766511835157871

Training epoch-16 batch-125
Running loss of epoch-16 batch-125 = 0.002666210988536477

Training epoch-16 batch-126
Running loss of epoch-16 batch-126 = 0.00040007149800658226

Training epoch-16 batch-127
Running loss of epoch-16 batch-127 = 0.0005484265275299549

Training epoch-16 batch-128
Running loss of epoch-16 batch-128 = 0.001356994966045022

Training epoch-16 batch-129
Running loss of epoch-16 batch-129 = 0.0010253782384097576

Training epoch-16 batch-130
Running loss of epoch-16 batch-130 = 0.0012929544318467379

Training epoch-16 batch-131
Running loss of epoch-16 batch-131 = 0.0012645525857806206

Training epoch-16 batch-132
Running loss of epoch-16 batch-132 = 0.0015732896281406283

Training epoch-16 batch-133
Running loss of epoch-16 batch-133 = 0.0012347400188446045

Training epoch-16 batch-134
Running loss of epoch-16 batch-134 = 0.0004907608381472528

Training epoch-16 batch-135
Running loss of epoch-16 batch-135 = 0.0014213271206244826

Training epoch-16 batch-136
Running loss of epoch-16 batch-136 = 0.004719737451523542

Training epoch-16 batch-137
Running loss of epoch-16 batch-137 = 0.0013927652034908533

Training epoch-16 batch-138
Running loss of epoch-16 batch-138 = 0.0008587464690208435

Training epoch-16 batch-139
Running loss of epoch-16 batch-139 = 0.001196090248413384

Training epoch-16 batch-140
Running loss of epoch-16 batch-140 = 0.000929405796341598

Training epoch-16 batch-141
Running loss of epoch-16 batch-141 = 0.004466796759516001

Training epoch-16 batch-142
Running loss of epoch-16 batch-142 = 0.0017888200236484408

Training epoch-16 batch-143
Running loss of epoch-16 batch-143 = 0.0025130794383585453

Training epoch-16 batch-144
Running loss of epoch-16 batch-144 = 0.0017366838874295354

Training epoch-16 batch-145
Running loss of epoch-16 batch-145 = 0.0032584171276539564

Training epoch-16 batch-146
Running loss of epoch-16 batch-146 = 0.0008418623474426568

Training epoch-16 batch-147
Running loss of epoch-16 batch-147 = 0.0032002381049096584

Training epoch-16 batch-148
Running loss of epoch-16 batch-148 = 0.0063031259924173355

Training epoch-16 batch-149
Running loss of epoch-16 batch-149 = 0.001648325240239501

Training epoch-16 batch-150
Running loss of epoch-16 batch-150 = 0.0005817305063828826

Training epoch-16 batch-151
Running loss of epoch-16 batch-151 = 0.0021905479952692986

Training epoch-16 batch-152
Running loss of epoch-16 batch-152 = 0.002336589153856039

Training epoch-16 batch-153
Running loss of epoch-16 batch-153 = 0.0010377098806202412

Training epoch-16 batch-154
Running loss of epoch-16 batch-154 = 0.0030529943760484457

Training epoch-16 batch-155
Running loss of epoch-16 batch-155 = 0.0008331906283274293

Training epoch-16 batch-156
Running loss of epoch-16 batch-156 = 0.002165705431252718

Training epoch-16 batch-157
Running loss of epoch-16 batch-157 = 0.005019039381295443

Finished training epoch-16.



Average train loss at epoch-16 = 0.0008934676144272089

Started Evaluation

Average val loss at epoch-16 = 4.908531217198623

Accuracy for classes:
Accuracy for class equals is: 63.20 %
Accuracy for class main is: 32.30 %
Accuracy for class setUp is: 38.85 %
Accuracy for class onCreate is: 23.56 %
Accuracy for class toString is: 47.78 %
Accuracy for class run is: 15.53 %
Accuracy for class hashCode is: 73.78 %
Accuracy for class init is: 13.90 %
Accuracy for class execute is: 28.51 %
Accuracy for class get is: 40.77 %

Overall Accuracy = 35.80 %

Finished Evaluation



Started training epoch-17


Training epoch-17 batch-1
Running loss of epoch-17 batch-1 = 0.002906875219196081

Training epoch-17 batch-2
Running loss of epoch-17 batch-2 = 0.0028094586450606585

Training epoch-17 batch-3
Running loss of epoch-17 batch-3 = 0.0016472912393510342

Training epoch-17 batch-4
Running loss of epoch-17 batch-4 = 0.002425533253699541

Training epoch-17 batch-5
Running loss of epoch-17 batch-5 = 0.002615170320495963

Training epoch-17 batch-6
Running loss of epoch-17 batch-6 = 0.001369653968140483

Training epoch-17 batch-7
Running loss of epoch-17 batch-7 = 0.0009378056856803596

Training epoch-17 batch-8
Running loss of epoch-17 batch-8 = 0.0005717427120544016

Training epoch-17 batch-9
Running loss of epoch-17 batch-9 = 0.0008477121591567993

Training epoch-17 batch-10
Running loss of epoch-17 batch-10 = 0.0010592901380732656

Training epoch-17 batch-11
Running loss of epoch-17 batch-11 = 0.0005168103380128741

Training epoch-17 batch-12
Running loss of epoch-17 batch-12 = 0.0016333316452801228

Training epoch-17 batch-13
Running loss of epoch-17 batch-13 = 0.0008275836589746177

Training epoch-17 batch-14
Running loss of epoch-17 batch-14 = 0.0014168196357786655

Training epoch-17 batch-15
Running loss of epoch-17 batch-15 = 0.0011405742261558771

Training epoch-17 batch-16
Running loss of epoch-17 batch-16 = 0.0008953780052252114

Training epoch-17 batch-17
Running loss of epoch-17 batch-17 = 0.0012122122570872307

Training epoch-17 batch-18
Running loss of epoch-17 batch-18 = 0.000692558940500021

Training epoch-17 batch-19
Running loss of epoch-17 batch-19 = 0.0014160084538161755

Training epoch-17 batch-20
Running loss of epoch-17 batch-20 = 0.0015978122828528285

Training epoch-17 batch-21
Running loss of epoch-17 batch-21 = 0.0016521494835615158

Training epoch-17 batch-22
Running loss of epoch-17 batch-22 = 0.0009639401687309146

Training epoch-17 batch-23
Running loss of epoch-17 batch-23 = 0.0006148630636744201

Training epoch-17 batch-24
Running loss of epoch-17 batch-24 = 0.0014207030180841684

Training epoch-17 batch-25
Running loss of epoch-17 batch-25 = 0.001239494071342051

Training epoch-17 batch-26
Running loss of epoch-17 batch-26 = 0.0012275429908186197

Training epoch-17 batch-27
Running loss of epoch-17 batch-27 = 0.0010554965119808912

Training epoch-17 batch-28
Running loss of epoch-17 batch-28 = 0.00024050241336226463

Training epoch-17 batch-29
Running loss of epoch-17 batch-29 = 0.0005758958868682384

Training epoch-17 batch-30
Running loss of epoch-17 batch-30 = 0.00013905158266425133

Training epoch-17 batch-31
Running loss of epoch-17 batch-31 = 0.0017556464299559593

Training epoch-17 batch-32
Running loss of epoch-17 batch-32 = 0.00037647876888513565

Training epoch-17 batch-33
Running loss of epoch-17 batch-33 = 0.0014629708603024483

Training epoch-17 batch-34
Running loss of epoch-17 batch-34 = 0.0016151086892932653

Training epoch-17 batch-35
Running loss of epoch-17 batch-35 = 0.0007699016132391989

Training epoch-17 batch-36
Running loss of epoch-17 batch-36 = 0.0007521571242250502

Training epoch-17 batch-37
Running loss of epoch-17 batch-37 = 0.002168772742152214

Training epoch-17 batch-38
Running loss of epoch-17 batch-38 = 0.002348728012293577

Training epoch-17 batch-39
Running loss of epoch-17 batch-39 = 0.0002508679172024131

Training epoch-17 batch-40
Running loss of epoch-17 batch-40 = 0.0010842467891052365

Training epoch-17 batch-41
Running loss of epoch-17 batch-41 = 0.00044471025466918945

Training epoch-17 batch-42
Running loss of epoch-17 batch-42 = 0.0006382467690855265

Training epoch-17 batch-43
Running loss of epoch-17 batch-43 = 0.003676157910376787

Training epoch-17 batch-44
Running loss of epoch-17 batch-44 = 0.002830157056450844

Training epoch-17 batch-45
Running loss of epoch-17 batch-45 = 0.0006027371855452657

Training epoch-17 batch-46
Running loss of epoch-17 batch-46 = 0.0006384920561686158

Training epoch-17 batch-47
Running loss of epoch-17 batch-47 = 0.0010453544091433287

Training epoch-17 batch-48
Running loss of epoch-17 batch-48 = 0.002740072552114725

Training epoch-17 batch-49
Running loss of epoch-17 batch-49 = 0.0017714890418574214

Training epoch-17 batch-50
Running loss of epoch-17 batch-50 = 0.0008704040665179491

Training epoch-17 batch-51
Running loss of epoch-17 batch-51 = 0.0008411958697251976

Training epoch-17 batch-52
Running loss of epoch-17 batch-52 = 0.0011825708206743002

Training epoch-17 batch-53
Running loss of epoch-17 batch-53 = 0.00039407750591635704

Training epoch-17 batch-54
Running loss of epoch-17 batch-54 = 0.0014042691327631474

Training epoch-17 batch-55
Running loss of epoch-17 batch-55 = 0.0008046089205890894

Training epoch-17 batch-56
Running loss of epoch-17 batch-56 = 0.0004868548712693155

Training epoch-17 batch-57
Running loss of epoch-17 batch-57 = 0.0027676422614604235

Training epoch-17 batch-58
Running loss of epoch-17 batch-58 = 0.0010585105046629906

Training epoch-17 batch-59
Running loss of epoch-17 batch-59 = 0.0006321193650364876

Training epoch-17 batch-60
Running loss of epoch-17 batch-60 = 0.0006965833017602563

Training epoch-17 batch-61
Running loss of epoch-17 batch-61 = 0.0021661128848791122

Training epoch-17 batch-62
Running loss of epoch-17 batch-62 = 0.002295087557286024

Training epoch-17 batch-63
Running loss of epoch-17 batch-63 = 0.001344169257208705

Training epoch-17 batch-64
Running loss of epoch-17 batch-64 = 0.0006116873119026423

Training epoch-17 batch-65
Running loss of epoch-17 batch-65 = 0.002332502044737339

Training epoch-17 batch-66
Running loss of epoch-17 batch-66 = 0.0029627098701894283

Training epoch-17 batch-67
Running loss of epoch-17 batch-67 = 0.0037445933558046818

Training epoch-17 batch-68
Running loss of epoch-17 batch-68 = 0.00035278371069580317

Training epoch-17 batch-69
Running loss of epoch-17 batch-69 = 0.002314663026481867

Training epoch-17 batch-70
Running loss of epoch-17 batch-70 = 0.002419431693851948

Training epoch-17 batch-71
Running loss of epoch-17 batch-71 = 0.004711951594799757

Training epoch-17 batch-72
Running loss of epoch-17 batch-72 = 0.0009002478327602148

Training epoch-17 batch-73
Running loss of epoch-17 batch-73 = 0.0010065273381769657

Training epoch-17 batch-74
Running loss of epoch-17 batch-74 = 0.0019472574349492788

Training epoch-17 batch-75
Running loss of epoch-17 batch-75 = 0.0022340044379234314

Training epoch-17 batch-76
Running loss of epoch-17 batch-76 = 0.0007862242637202144

Training epoch-17 batch-77
Running loss of epoch-17 batch-77 = 0.002333511831238866

Training epoch-17 batch-78
Running loss of epoch-17 batch-78 = 0.000900537590496242

Training epoch-17 batch-79
Running loss of epoch-17 batch-79 = 0.0032498857472091913

Training epoch-17 batch-80
Running loss of epoch-17 batch-80 = 0.0013179226079955697

Training epoch-17 batch-81
Running loss of epoch-17 batch-81 = 0.0011085611768066883

Training epoch-17 batch-82
Running loss of epoch-17 batch-82 = 0.002104800892993808

Training epoch-17 batch-83
Running loss of epoch-17 batch-83 = 0.004917053505778313

Training epoch-17 batch-84
Running loss of epoch-17 batch-84 = 0.001518433797173202

Training epoch-17 batch-85
Running loss of epoch-17 batch-85 = 0.0008561891736462712

Training epoch-17 batch-86
Running loss of epoch-17 batch-86 = 0.0012463657185435295

Training epoch-17 batch-87
Running loss of epoch-17 batch-87 = 0.0009182731155306101

Training epoch-17 batch-88
Running loss of epoch-17 batch-88 = 0.000699389900546521

Training epoch-17 batch-89
Running loss of epoch-17 batch-89 = 0.0006000176654197276

Training epoch-17 batch-90
Running loss of epoch-17 batch-90 = 0.0015290207229554653

Training epoch-17 batch-91
Running loss of epoch-17 batch-91 = 0.0016708801267668605

Training epoch-17 batch-92
Running loss of epoch-17 batch-92 = 0.002269374905154109

Training epoch-17 batch-93
Running loss of epoch-17 batch-93 = 0.0006791364285163581

Training epoch-17 batch-94
Running loss of epoch-17 batch-94 = 0.001798466662876308

Training epoch-17 batch-95
Running loss of epoch-17 batch-95 = 0.0005616657435894012

Training epoch-17 batch-96
Running loss of epoch-17 batch-96 = 0.0035078958608210087

Training epoch-17 batch-97
Running loss of epoch-17 batch-97 = 0.0007533825701102614

Training epoch-17 batch-98
Running loss of epoch-17 batch-98 = 0.0007110147853381932

Training epoch-17 batch-99
Running loss of epoch-17 batch-99 = 0.0011948773171752691

Training epoch-17 batch-100
Running loss of epoch-17 batch-100 = 0.0009513322729617357

Training epoch-17 batch-101
Running loss of epoch-17 batch-101 = 0.0008195024565793574

Training epoch-17 batch-102
Running loss of epoch-17 batch-102 = 0.001151503180153668

Training epoch-17 batch-103
Running loss of epoch-17 batch-103 = 0.0006674538017250597

Training epoch-17 batch-104
Running loss of epoch-17 batch-104 = 0.0008428730070590973

Training epoch-17 batch-105
Running loss of epoch-17 batch-105 = 0.0005170917138457298

Training epoch-17 batch-106
Running loss of epoch-17 batch-106 = 0.0008444570703431964

Training epoch-17 batch-107
Running loss of epoch-17 batch-107 = 0.0013575437478721142

Training epoch-17 batch-108
Running loss of epoch-17 batch-108 = 0.00461761886253953

Training epoch-17 batch-109
Running loss of epoch-17 batch-109 = 0.0013469598488882184

Training epoch-17 batch-110
Running loss of epoch-17 batch-110 = 0.0008347220718860626

Training epoch-17 batch-111
Running loss of epoch-17 batch-111 = 0.0016005744691938162

Training epoch-17 batch-112
Running loss of epoch-17 batch-112 = 0.002304420806467533

Training epoch-17 batch-113
Running loss of epoch-17 batch-113 = 0.000573685101699084

Training epoch-17 batch-114
Running loss of epoch-17 batch-114 = 0.0004122086684219539

Training epoch-17 batch-115
Running loss of epoch-17 batch-115 = 0.00022234441712498665

Training epoch-17 batch-116
Running loss of epoch-17 batch-116 = 0.001110058045014739

Training epoch-17 batch-117
Running loss of epoch-17 batch-117 = 0.0010304063325747848

Training epoch-17 batch-118
Running loss of epoch-17 batch-118 = 0.002241589594632387

Training epoch-17 batch-119
Running loss of epoch-17 batch-119 = 0.0003011315129697323

Training epoch-17 batch-120
Running loss of epoch-17 batch-120 = 0.00046617898624390364

Training epoch-17 batch-121
Running loss of epoch-17 batch-121 = 0.0021059021819382906

Training epoch-17 batch-122
Running loss of epoch-17 batch-122 = 0.0006792533677071333

Training epoch-17 batch-123
Running loss of epoch-17 batch-123 = 0.0013374782865867019

Training epoch-17 batch-124
Running loss of epoch-17 batch-124 = 0.00563763827085495

Training epoch-17 batch-125
Running loss of epoch-17 batch-125 = 0.0009449447970837355

Training epoch-17 batch-126
Running loss of epoch-17 batch-126 = 0.0010667919414117932

Training epoch-17 batch-127
Running loss of epoch-17 batch-127 = 0.0013005658984184265

Training epoch-17 batch-128
Running loss of epoch-17 batch-128 = 0.0005434537888504565

Training epoch-17 batch-129
Running loss of epoch-17 batch-129 = 0.0019579813815653324

Training epoch-17 batch-130
Running loss of epoch-17 batch-130 = 0.002001166343688965

Training epoch-17 batch-131
Running loss of epoch-17 batch-131 = 0.002443494973704219

Training epoch-17 batch-132
Running loss of epoch-17 batch-132 = 0.0016053339932113886

Training epoch-17 batch-133
Running loss of epoch-17 batch-133 = 0.0009115947759710252

Training epoch-17 batch-134
Running loss of epoch-17 batch-134 = 0.0012056840350851417

Training epoch-17 batch-135
Running loss of epoch-17 batch-135 = 0.0022200709208846092

Training epoch-17 batch-136
Running loss of epoch-17 batch-136 = 0.0009596882155165076

Training epoch-17 batch-137
Running loss of epoch-17 batch-137 = 0.000504341849591583

Training epoch-17 batch-138
Running loss of epoch-17 batch-138 = 0.0005753337172791362

Training epoch-17 batch-139
Running loss of epoch-17 batch-139 = 0.0021071748342365026

Training epoch-17 batch-140
Running loss of epoch-17 batch-140 = 0.0008187010535039008

Training epoch-17 batch-141
Running loss of epoch-17 batch-141 = 0.0012750766472890973

Training epoch-17 batch-142
Running loss of epoch-17 batch-142 = 0.003268337808549404

Training epoch-17 batch-143
Running loss of epoch-17 batch-143 = 0.0013637160882353783

Training epoch-17 batch-144
Running loss of epoch-17 batch-144 = 0.001755199977196753

Training epoch-17 batch-145
Running loss of epoch-17 batch-145 = 0.000723997363820672

Training epoch-17 batch-146
Running loss of epoch-17 batch-146 = 0.0018242561491206288

Training epoch-17 batch-147
Running loss of epoch-17 batch-147 = 0.0010712390067055821

Training epoch-17 batch-148
Running loss of epoch-17 batch-148 = 0.0007135261548683047

Training epoch-17 batch-149
Running loss of epoch-17 batch-149 = 0.0026364424265921116

Training epoch-17 batch-150
Running loss of epoch-17 batch-150 = 0.0004379251040518284

Training epoch-17 batch-151
Running loss of epoch-17 batch-151 = 0.0004188736202195287

Training epoch-17 batch-152
Running loss of epoch-17 batch-152 = 0.0006291461759246886

Training epoch-17 batch-153
Running loss of epoch-17 batch-153 = 0.0018025576137006283

Training epoch-17 batch-154
Running loss of epoch-17 batch-154 = 0.0009701488306745887

Training epoch-17 batch-155
Running loss of epoch-17 batch-155 = 0.00046788493636995554

Training epoch-17 batch-156
Running loss of epoch-17 batch-156 = 0.0010513634188100696

Training epoch-17 batch-157
Running loss of epoch-17 batch-157 = 0.002626970410346985

Finished training epoch-17.



Average train loss at epoch-17 = 0.0014116076208651066

Started Evaluation

Average val loss at epoch-17 = 4.377515551291014

Accuracy for classes:
Accuracy for class equals is: 62.05 %
Accuracy for class main is: 23.11 %
Accuracy for class setUp is: 50.82 %
Accuracy for class onCreate is: 32.20 %
Accuracy for class toString is: 34.47 %
Accuracy for class run is: 37.67 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 19.06 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 29.23 %

Overall Accuracy = 38.09 %

Finished Evaluation



Started training epoch-18


Training epoch-18 batch-1
Running loss of epoch-18 batch-1 = 0.0013764379546046257

Training epoch-18 batch-2
Running loss of epoch-18 batch-2 = 0.00040427775820717216

Training epoch-18 batch-3
Running loss of epoch-18 batch-3 = 0.0033764273393899202

Training epoch-18 batch-4
Running loss of epoch-18 batch-4 = 0.0004538226639851928

Training epoch-18 batch-5
Running loss of epoch-18 batch-5 = 0.0009384715231135488

Training epoch-18 batch-6
Running loss of epoch-18 batch-6 = 0.0009906416526064277

Training epoch-18 batch-7
Running loss of epoch-18 batch-7 = 0.0007835897267796099

Training epoch-18 batch-8
Running loss of epoch-18 batch-8 = 0.0009569400572218001

Training epoch-18 batch-9
Running loss of epoch-18 batch-9 = 0.0012200617929920554

Training epoch-18 batch-10
Running loss of epoch-18 batch-10 = 0.00025323755107820034

Training epoch-18 batch-11
Running loss of epoch-18 batch-11 = 0.0017634360119700432

Training epoch-18 batch-12
Running loss of epoch-18 batch-12 = 0.000507052056491375

Training epoch-18 batch-13
Running loss of epoch-18 batch-13 = 0.002056780504062772

Training epoch-18 batch-14
Running loss of epoch-18 batch-14 = 0.0012318069348111749

Training epoch-18 batch-15
Running loss of epoch-18 batch-15 = 0.0005417750217020512

Training epoch-18 batch-16
Running loss of epoch-18 batch-16 = 0.0012814289657399058

Training epoch-18 batch-17
Running loss of epoch-18 batch-17 = 0.0014623984461650252

Training epoch-18 batch-18
Running loss of epoch-18 batch-18 = 0.0006409913185052574

Training epoch-18 batch-19
Running loss of epoch-18 batch-19 = 0.0005603824974969029

Training epoch-18 batch-20
Running loss of epoch-18 batch-20 = 0.0009377458482049406

Training epoch-18 batch-21
Running loss of epoch-18 batch-21 = 0.0005909691099077463

Training epoch-18 batch-22
Running loss of epoch-18 batch-22 = 9.081466123461723e-05

Training epoch-18 batch-23
Running loss of epoch-18 batch-23 = 0.0003170085255987942

Training epoch-18 batch-24
Running loss of epoch-18 batch-24 = 0.0005905693396925926

Training epoch-18 batch-25
Running loss of epoch-18 batch-25 = 0.0021935291588306427

Training epoch-18 batch-26
Running loss of epoch-18 batch-26 = 0.0017506290460005403

Training epoch-18 batch-27
Running loss of epoch-18 batch-27 = 0.0003659697249531746

Training epoch-18 batch-28
Running loss of epoch-18 batch-28 = 0.00030013080686330795

Training epoch-18 batch-29
Running loss of epoch-18 batch-29 = 0.0007243721047416329

Training epoch-18 batch-30
Running loss of epoch-18 batch-30 = 0.0007850768161006272

Training epoch-18 batch-31
Running loss of epoch-18 batch-31 = 0.0008358294726349413

Training epoch-18 batch-32
Running loss of epoch-18 batch-32 = 0.0021526184864342213

Training epoch-18 batch-33
Running loss of epoch-18 batch-33 = 0.00023318873718380928

Training epoch-18 batch-34
Running loss of epoch-18 batch-34 = 0.0014900885289534926

Training epoch-18 batch-35
Running loss of epoch-18 batch-35 = 0.0009188437834382057

Training epoch-18 batch-36
Running loss of epoch-18 batch-36 = 0.0004538327921181917

Training epoch-18 batch-37
Running loss of epoch-18 batch-37 = 0.00017617980483919382

Training epoch-18 batch-38
Running loss of epoch-18 batch-38 = 0.0008497412782162428

Training epoch-18 batch-39
Running loss of epoch-18 batch-39 = 0.001283496618270874

Training epoch-18 batch-40
Running loss of epoch-18 batch-40 = 0.0006344764842651784

Training epoch-18 batch-41
Running loss of epoch-18 batch-41 = 0.0020048387814313173

Training epoch-18 batch-42
Running loss of epoch-18 batch-42 = 0.0006253159372135997

Training epoch-18 batch-43
Running loss of epoch-18 batch-43 = 0.0011424105614423752

Training epoch-18 batch-44
Running loss of epoch-18 batch-44 = 0.0006250848527997732

Training epoch-18 batch-45
Running loss of epoch-18 batch-45 = 0.0015915733529254794

Training epoch-18 batch-46
Running loss of epoch-18 batch-46 = 0.00017720181494951248

Training epoch-18 batch-47
Running loss of epoch-18 batch-47 = 0.001710468321107328

Training epoch-18 batch-48
Running loss of epoch-18 batch-48 = 0.00048285757657140493

Training epoch-18 batch-49
Running loss of epoch-18 batch-49 = 0.0021027333568781614

Training epoch-18 batch-50
Running loss of epoch-18 batch-50 = 0.0008867810247465968

Training epoch-18 batch-51
Running loss of epoch-18 batch-51 = 0.003605328733101487

Training epoch-18 batch-52
Running loss of epoch-18 batch-52 = 0.0002746330574154854

Training epoch-18 batch-53
Running loss of epoch-18 batch-53 = 0.0007419473258778453

Training epoch-18 batch-54
Running loss of epoch-18 batch-54 = 0.0017129515763372183

Training epoch-18 batch-55
Running loss of epoch-18 batch-55 = 0.0013114998582750559

Training epoch-18 batch-56
Running loss of epoch-18 batch-56 = 0.003450332209467888

Training epoch-18 batch-57
Running loss of epoch-18 batch-57 = 0.0020425948314368725

Training epoch-18 batch-58
Running loss of epoch-18 batch-58 = 0.0025318122934550047

Training epoch-18 batch-59
Running loss of epoch-18 batch-59 = 0.001999686937779188

Training epoch-18 batch-60
Running loss of epoch-18 batch-60 = 0.0005540178390219808

Training epoch-18 batch-61
Running loss of epoch-18 batch-61 = 0.002615248318761587

Training epoch-18 batch-62
Running loss of epoch-18 batch-62 = 0.00033867149613797665

Training epoch-18 batch-63
Running loss of epoch-18 batch-63 = 0.00044311111560091376

Training epoch-18 batch-64
Running loss of epoch-18 batch-64 = 0.0036623976193368435

Training epoch-18 batch-65
Running loss of epoch-18 batch-65 = 0.002109964843839407

Training epoch-18 batch-66
Running loss of epoch-18 batch-66 = 0.0007081121439114213

Training epoch-18 batch-67
Running loss of epoch-18 batch-67 = 0.00029715034179389477

Training epoch-18 batch-68
Running loss of epoch-18 batch-68 = 0.0010524340905249119

Training epoch-18 batch-69
Running loss of epoch-18 batch-69 = 0.0010448693064972758

Training epoch-18 batch-70
Running loss of epoch-18 batch-70 = 0.0028124283999204636

Training epoch-18 batch-71
Running loss of epoch-18 batch-71 = 0.0014169972855597734

Training epoch-18 batch-72
Running loss of epoch-18 batch-72 = 0.0003517015720717609

Training epoch-18 batch-73
Running loss of epoch-18 batch-73 = 0.000916958786547184

Training epoch-18 batch-74
Running loss of epoch-18 batch-74 = 0.000477789668366313

Training epoch-18 batch-75
Running loss of epoch-18 batch-75 = 0.0019323319429531693

Training epoch-18 batch-76
Running loss of epoch-18 batch-76 = 0.000972350942902267

Training epoch-18 batch-77
Running loss of epoch-18 batch-77 = 0.0013110530562698841

Training epoch-18 batch-78
Running loss of epoch-18 batch-78 = 0.0011771278223022819

Training epoch-18 batch-79
Running loss of epoch-18 batch-79 = 0.0008838713401928544

Training epoch-18 batch-80
Running loss of epoch-18 batch-80 = 0.00037478533340618014

Training epoch-18 batch-81
Running loss of epoch-18 batch-81 = 0.0006213407032191753

Training epoch-18 batch-82
Running loss of epoch-18 batch-82 = 0.0006602916400879622

Training epoch-18 batch-83
Running loss of epoch-18 batch-83 = 0.0006915996200405061

Training epoch-18 batch-84
Running loss of epoch-18 batch-84 = 0.002376010874286294

Training epoch-18 batch-85
Running loss of epoch-18 batch-85 = 0.0006581699708476663

Training epoch-18 batch-86
Running loss of epoch-18 batch-86 = 0.0011532856151461601

Training epoch-18 batch-87
Running loss of epoch-18 batch-87 = 0.0009499916341155767

Training epoch-18 batch-88
Running loss of epoch-18 batch-88 = 0.0009211182477883995

Training epoch-18 batch-89
Running loss of epoch-18 batch-89 = 0.0012483703903853893

Training epoch-18 batch-90
Running loss of epoch-18 batch-90 = 0.0014101524138823152

Training epoch-18 batch-91
Running loss of epoch-18 batch-91 = 0.0006533691193908453

Training epoch-18 batch-92
Running loss of epoch-18 batch-92 = 0.00024132616817951202

Training epoch-18 batch-93
Running loss of epoch-18 batch-93 = 0.0003012357046827674

Training epoch-18 batch-94
Running loss of epoch-18 batch-94 = 0.002901090309023857

Training epoch-18 batch-95
Running loss of epoch-18 batch-95 = 0.0007790813106112182

Training epoch-18 batch-96
Running loss of epoch-18 batch-96 = 0.0016923811053857207

Training epoch-18 batch-97
Running loss of epoch-18 batch-97 = 0.0016087867552414536

Training epoch-18 batch-98
Running loss of epoch-18 batch-98 = 0.0005541790742427111

Training epoch-18 batch-99
Running loss of epoch-18 batch-99 = 0.0011333898873999715

Training epoch-18 batch-100
Running loss of epoch-18 batch-100 = 0.0008625638438388705

Training epoch-18 batch-101
Running loss of epoch-18 batch-101 = 0.0010046917013823986

Training epoch-18 batch-102
Running loss of epoch-18 batch-102 = 0.001778356614522636

Training epoch-18 batch-103
Running loss of epoch-18 batch-103 = 0.0002642543404363096

Training epoch-18 batch-104
Running loss of epoch-18 batch-104 = 0.00197810772806406

Training epoch-18 batch-105
Running loss of epoch-18 batch-105 = 0.00129939045291394

Training epoch-18 batch-106
Running loss of epoch-18 batch-106 = 0.002332117408514023

Training epoch-18 batch-107
Running loss of epoch-18 batch-107 = 0.0008574365638196468

Training epoch-18 batch-108
Running loss of epoch-18 batch-108 = 0.004047582857310772

Training epoch-18 batch-109
Running loss of epoch-18 batch-109 = 0.0014440888771787286

Training epoch-18 batch-110
Running loss of epoch-18 batch-110 = 0.0006282182293944061

Training epoch-18 batch-111
Running loss of epoch-18 batch-111 = 0.0010541895171627402

Training epoch-18 batch-112
Running loss of epoch-18 batch-112 = 0.0014158792328089476

Training epoch-18 batch-113
Running loss of epoch-18 batch-113 = 0.002124147256836295

Training epoch-18 batch-114
Running loss of epoch-18 batch-114 = 0.0008141256985254586

Training epoch-18 batch-115
Running loss of epoch-18 batch-115 = 0.001687382347881794

Training epoch-18 batch-116
Running loss of epoch-18 batch-116 = 0.000608916743658483

Training epoch-18 batch-117
Running loss of epoch-18 batch-117 = 0.0008065655129030347

Training epoch-18 batch-118
Running loss of epoch-18 batch-118 = 0.0009553349809721112

Training epoch-18 batch-119
Running loss of epoch-18 batch-119 = 0.00036320986691862345

Training epoch-18 batch-120
Running loss of epoch-18 batch-120 = 0.0006246032426133752

Training epoch-18 batch-121
Running loss of epoch-18 batch-121 = 0.0013424569042399526

Training epoch-18 batch-122
Running loss of epoch-18 batch-122 = 0.0038092201575636864

Training epoch-18 batch-123
Running loss of epoch-18 batch-123 = 0.00027887988835573196

Training epoch-18 batch-124
Running loss of epoch-18 batch-124 = 0.0015208062250167131

Training epoch-18 batch-125
Running loss of epoch-18 batch-125 = 0.00018000637646764517

Training epoch-18 batch-126
Running loss of epoch-18 batch-126 = 0.0033615967258810997

Training epoch-18 batch-127
Running loss of epoch-18 batch-127 = 0.00045245065120980144

Training epoch-18 batch-128
Running loss of epoch-18 batch-128 = 0.0008722724742256105

Training epoch-18 batch-129
Running loss of epoch-18 batch-129 = 0.0009413780644536018

Training epoch-18 batch-130
Running loss of epoch-18 batch-130 = 0.0011872851755470037

Training epoch-18 batch-131
Running loss of epoch-18 batch-131 = 0.001716095837764442

Training epoch-18 batch-132
Running loss of epoch-18 batch-132 = 0.0024393091443926096

Training epoch-18 batch-133
Running loss of epoch-18 batch-133 = 0.0005018962547183037

Training epoch-18 batch-134
Running loss of epoch-18 batch-134 = 0.0023524737916886806

Training epoch-18 batch-135
Running loss of epoch-18 batch-135 = 0.0013465335359796882

Training epoch-18 batch-136
Running loss of epoch-18 batch-136 = 0.0004115875344723463

Training epoch-18 batch-137
Running loss of epoch-18 batch-137 = 0.0017243552720174193

Training epoch-18 batch-138
Running loss of epoch-18 batch-138 = 0.0010223585413768888

Training epoch-18 batch-139
Running loss of epoch-18 batch-139 = 0.0008762580691836774

Training epoch-18 batch-140
Running loss of epoch-18 batch-140 = 0.0017678950680419803

Training epoch-18 batch-141
Running loss of epoch-18 batch-141 = 0.004484351724386215

Training epoch-18 batch-142
Running loss of epoch-18 batch-142 = 0.0005574979004450142

Training epoch-18 batch-143
Running loss of epoch-18 batch-143 = 0.00044541305396705866

Training epoch-18 batch-144
Running loss of epoch-18 batch-144 = 0.0008632573299109936

Training epoch-18 batch-145
Running loss of epoch-18 batch-145 = 0.001110428711399436

Training epoch-18 batch-146
Running loss of epoch-18 batch-146 = 0.0020372599828988314

Training epoch-18 batch-147
Running loss of epoch-18 batch-147 = 0.0006911208038218319

Training epoch-18 batch-148
Running loss of epoch-18 batch-148 = 0.0007345227058976889

Training epoch-18 batch-149
Running loss of epoch-18 batch-149 = 0.001912673469632864

Training epoch-18 batch-150
Running loss of epoch-18 batch-150 = 0.001088196411728859

Training epoch-18 batch-151
Running loss of epoch-18 batch-151 = 0.0006434574606828392

Training epoch-18 batch-152
Running loss of epoch-18 batch-152 = 0.001956926891580224

Training epoch-18 batch-153
Running loss of epoch-18 batch-153 = 0.0020948494784533978

Training epoch-18 batch-154
Running loss of epoch-18 batch-154 = 0.002611193573102355

Training epoch-18 batch-155
Running loss of epoch-18 batch-155 = 0.001436059596017003

Training epoch-18 batch-156
Running loss of epoch-18 batch-156 = 0.0007775786216370761

Training epoch-18 batch-157
Running loss of epoch-18 batch-157 = 0.0008420329540967941

Finished training epoch-18.



Average train loss at epoch-18 = 0.0012301775474101305

Started Evaluation

Average val loss at epoch-18 = 4.292149502980082

Accuracy for classes:
Accuracy for class equals is: 54.29 %
Accuracy for class main is: 31.80 %
Accuracy for class setUp is: 42.46 %
Accuracy for class onCreate is: 32.41 %
Accuracy for class toString is: 55.63 %
Accuracy for class run is: 35.16 %
Accuracy for class hashCode is: 71.54 %
Accuracy for class init is: 20.40 %
Accuracy for class execute is: 31.33 %
Accuracy for class get is: 13.85 %

Overall Accuracy = 37.49 %

Finished Evaluation



Started training epoch-19


Training epoch-19 batch-1
Running loss of epoch-19 batch-1 = 0.0015101999742910266

Training epoch-19 batch-2
Running loss of epoch-19 batch-2 = 0.0013949372805655003

Training epoch-19 batch-3
Running loss of epoch-19 batch-3 = 0.0006032601231709123

Training epoch-19 batch-4
Running loss of epoch-19 batch-4 = 0.0005818260833621025

Training epoch-19 batch-5
Running loss of epoch-19 batch-5 = 0.0016226771986111999

Training epoch-19 batch-6
Running loss of epoch-19 batch-6 = 0.0012359244283288717

Training epoch-19 batch-7
Running loss of epoch-19 batch-7 = 0.000582715671043843

Training epoch-19 batch-8
Running loss of epoch-19 batch-8 = 0.000624686130322516

Training epoch-19 batch-9
Running loss of epoch-19 batch-9 = 0.0003896665293723345

Training epoch-19 batch-10
Running loss of epoch-19 batch-10 = 0.001483097905293107

Training epoch-19 batch-11
Running loss of epoch-19 batch-11 = 0.0008046227158047259

Training epoch-19 batch-12
Running loss of epoch-19 batch-12 = 0.001975785242393613

Training epoch-19 batch-13
Running loss of epoch-19 batch-13 = 0.0005511301569640636

Training epoch-19 batch-14
Running loss of epoch-19 batch-14 = 0.00020276923896744847

Training epoch-19 batch-15
Running loss of epoch-19 batch-15 = 0.0003808933252003044

Training epoch-19 batch-16
Running loss of epoch-19 batch-16 = 0.00022694736253470182

Training epoch-19 batch-17
Running loss of epoch-19 batch-17 = 0.0006358657265082002

Training epoch-19 batch-18
Running loss of epoch-19 batch-18 = 0.0026775223668664694

Training epoch-19 batch-19
Running loss of epoch-19 batch-19 = 0.000623582280240953

Training epoch-19 batch-20
Running loss of epoch-19 batch-20 = 0.0017676270799711347

Training epoch-19 batch-21
Running loss of epoch-19 batch-21 = 0.00087405473459512

Training epoch-19 batch-22
Running loss of epoch-19 batch-22 = 0.00027169642271474004

Training epoch-19 batch-23
Running loss of epoch-19 batch-23 = 0.00030693976441398263

Training epoch-19 batch-24
Running loss of epoch-19 batch-24 = 0.0003315247013233602

Training epoch-19 batch-25
Running loss of epoch-19 batch-25 = 0.000590464856941253

Training epoch-19 batch-26
Running loss of epoch-19 batch-26 = 0.000724470301065594

Training epoch-19 batch-27
Running loss of epoch-19 batch-27 = 0.0002726367674767971

Training epoch-19 batch-28
Running loss of epoch-19 batch-28 = 0.00021812331397086382

Training epoch-19 batch-29
Running loss of epoch-19 batch-29 = 0.00036273535806685686

Training epoch-19 batch-30
Running loss of epoch-19 batch-30 = 0.0013933115405961871

Training epoch-19 batch-31
Running loss of epoch-19 batch-31 = 0.0007034732843749225

Training epoch-19 batch-32
Running loss of epoch-19 batch-32 = 0.0004688022891059518

Training epoch-19 batch-33
Running loss of epoch-19 batch-33 = 0.00019692949717864394

Training epoch-19 batch-34
Running loss of epoch-19 batch-34 = 0.00018874183297157288

Training epoch-19 batch-35
Running loss of epoch-19 batch-35 = 0.0004057259939145297

Training epoch-19 batch-36
Running loss of epoch-19 batch-36 = 0.0002518356777727604

Training epoch-19 batch-37
Running loss of epoch-19 batch-37 = 0.001047485857270658

Training epoch-19 batch-38
Running loss of epoch-19 batch-38 = 0.000581364962272346

Training epoch-19 batch-39
Running loss of epoch-19 batch-39 = 0.0015222487272694707

Training epoch-19 batch-40
Running loss of epoch-19 batch-40 = 0.00019194354536011815

Training epoch-19 batch-41
Running loss of epoch-19 batch-41 = 0.000387266103643924

Training epoch-19 batch-42
Running loss of epoch-19 batch-42 = 0.0004372194525785744

Training epoch-19 batch-43
Running loss of epoch-19 batch-43 = 0.00023864454124122858

Training epoch-19 batch-44
Running loss of epoch-19 batch-44 = 0.0006595046725124121

Training epoch-19 batch-45
Running loss of epoch-19 batch-45 = 0.00065664725843817

Training epoch-19 batch-46
Running loss of epoch-19 batch-46 = 0.0007746772607788444

Training epoch-19 batch-47
Running loss of epoch-19 batch-47 = 0.0002565532922744751

Training epoch-19 batch-48
Running loss of epoch-19 batch-48 = 0.0002799596404656768

Training epoch-19 batch-49
Running loss of epoch-19 batch-49 = 0.0007463447982445359

Training epoch-19 batch-50
Running loss of epoch-19 batch-50 = 0.00019445177167654037

Training epoch-19 batch-51
Running loss of epoch-19 batch-51 = 0.00022023182827979326

Training epoch-19 batch-52
Running loss of epoch-19 batch-52 = 0.0004738544812425971

Training epoch-19 batch-53
Running loss of epoch-19 batch-53 = 0.00048472126945853233

Training epoch-19 batch-54
Running loss of epoch-19 batch-54 = 0.0003007400082424283

Training epoch-19 batch-55
Running loss of epoch-19 batch-55 = 0.0003678385983221233

Training epoch-19 batch-56
Running loss of epoch-19 batch-56 = 0.0003181819338351488

Training epoch-19 batch-57
Running loss of epoch-19 batch-57 = 0.0011921694967895746

Training epoch-19 batch-58
Running loss of epoch-19 batch-58 = 0.0002659793244674802

Training epoch-19 batch-59
Running loss of epoch-19 batch-59 = 0.00021645851666107774

Training epoch-19 batch-60
Running loss of epoch-19 batch-60 = 0.000172103988006711

Training epoch-19 batch-61
Running loss of epoch-19 batch-61 = 0.0010683712316676974

Training epoch-19 batch-62
Running loss of epoch-19 batch-62 = 0.00020720571046695113

Training epoch-19 batch-63
Running loss of epoch-19 batch-63 = 0.0004988018772564828

Training epoch-19 batch-64
Running loss of epoch-19 batch-64 = 0.0001910093706101179

Training epoch-19 batch-65
Running loss of epoch-19 batch-65 = 0.00020573765505105257

Training epoch-19 batch-66
Running loss of epoch-19 batch-66 = 0.0003129615215584636

Training epoch-19 batch-67
Running loss of epoch-19 batch-67 = 0.0006743103731423616

Training epoch-19 batch-68
Running loss of epoch-19 batch-68 = 0.0004701697616837919

Training epoch-19 batch-69
Running loss of epoch-19 batch-69 = 0.00019148923456668854

Training epoch-19 batch-70
Running loss of epoch-19 batch-70 = 0.0011232357937842607

Training epoch-19 batch-71
Running loss of epoch-19 batch-71 = 0.0005269292159937322

Training epoch-19 batch-72
Running loss of epoch-19 batch-72 = 0.0003032597596757114

Training epoch-19 batch-73
Running loss of epoch-19 batch-73 = 0.0001992982579395175

Training epoch-19 batch-74
Running loss of epoch-19 batch-74 = 0.00028400321025401354

Training epoch-19 batch-75
Running loss of epoch-19 batch-75 = 0.0001334621338173747

Training epoch-19 batch-76
Running loss of epoch-19 batch-76 = 0.0006811170605942607

Training epoch-19 batch-77
Running loss of epoch-19 batch-77 = 0.0006602279026992619

Training epoch-19 batch-78
Running loss of epoch-19 batch-78 = 0.0003239908255636692

Training epoch-19 batch-79
Running loss of epoch-19 batch-79 = 0.0004989859880879521

Training epoch-19 batch-80
Running loss of epoch-19 batch-80 = 0.0005617830320261419

Training epoch-19 batch-81
Running loss of epoch-19 batch-81 = 0.00036745500983670354

Training epoch-19 batch-82
Running loss of epoch-19 batch-82 = 0.0006730150198563933

Training epoch-19 batch-83
Running loss of epoch-19 batch-83 = 0.00017988361651077867

Training epoch-19 batch-84
Running loss of epoch-19 batch-84 = 0.00022903905482962728

Training epoch-19 batch-85
Running loss of epoch-19 batch-85 = 0.000330957118421793

Training epoch-19 batch-86
Running loss of epoch-19 batch-86 = 0.00044644958688877523

Training epoch-19 batch-87
Running loss of epoch-19 batch-87 = 0.0018606308149173856

Training epoch-19 batch-88
Running loss of epoch-19 batch-88 = 0.000445288373157382

Training epoch-19 batch-89
Running loss of epoch-19 batch-89 = 0.0006516043795272708

Training epoch-19 batch-90
Running loss of epoch-19 batch-90 = 0.000845842994749546

Training epoch-19 batch-91
Running loss of epoch-19 batch-91 = 0.0014373420272022486

Training epoch-19 batch-92
Running loss of epoch-19 batch-92 = 0.00028201215900480747

Training epoch-19 batch-93
Running loss of epoch-19 batch-93 = 0.001530345412902534

Training epoch-19 batch-94
Running loss of epoch-19 batch-94 = 0.0002572890371084213

Training epoch-19 batch-95
Running loss of epoch-19 batch-95 = 0.0002604902256280184

Training epoch-19 batch-96
Running loss of epoch-19 batch-96 = 0.0004816026194021106

Training epoch-19 batch-97
Running loss of epoch-19 batch-97 = 0.0006831386126577854

Training epoch-19 batch-98
Running loss of epoch-19 batch-98 = 0.0009574460564181209

Training epoch-19 batch-99
Running loss of epoch-19 batch-99 = 0.00016145058907568455

Training epoch-19 batch-100
Running loss of epoch-19 batch-100 = 0.0021002180874347687

Training epoch-19 batch-101
Running loss of epoch-19 batch-101 = 0.0006841779686510563

Training epoch-19 batch-102
Running loss of epoch-19 batch-102 = 0.00018144515343010426

Training epoch-19 batch-103
Running loss of epoch-19 batch-103 = 0.00021157757146283984

Training epoch-19 batch-104
Running loss of epoch-19 batch-104 = 0.0003921808674931526

Training epoch-19 batch-105
Running loss of epoch-19 batch-105 = 0.00015556230209767818

Training epoch-19 batch-106
Running loss of epoch-19 batch-106 = 0.001671861857175827

Training epoch-19 batch-107
Running loss of epoch-19 batch-107 = 0.0006155446753837168

Training epoch-19 batch-108
Running loss of epoch-19 batch-108 = 0.0025995830073952675

Training epoch-19 batch-109
Running loss of epoch-19 batch-109 = 0.0011561678256839514

Training epoch-19 batch-110
Running loss of epoch-19 batch-110 = 0.00038089422741904855

Training epoch-19 batch-111
Running loss of epoch-19 batch-111 = 0.000429828476626426

Training epoch-19 batch-112
Running loss of epoch-19 batch-112 = 0.0006600084016099572

Training epoch-19 batch-113
Running loss of epoch-19 batch-113 = 0.00044896878534927964

Training epoch-19 batch-114
Running loss of epoch-19 batch-114 = 0.0015137332957237959

Training epoch-19 batch-115
Running loss of epoch-19 batch-115 = 0.0005219564773142338

Training epoch-19 batch-116
Running loss of epoch-19 batch-116 = 0.0018512899987399578

Training epoch-19 batch-117
Running loss of epoch-19 batch-117 = 0.0003496184363029897

Training epoch-19 batch-118
Running loss of epoch-19 batch-118 = 0.001132966484874487

Training epoch-19 batch-119
Running loss of epoch-19 batch-119 = 0.0009935928974300623

Training epoch-19 batch-120
Running loss of epoch-19 batch-120 = 0.0008716232841834426

Training epoch-19 batch-121
Running loss of epoch-19 batch-121 = 0.00023966532899066806

Training epoch-19 batch-122
Running loss of epoch-19 batch-122 = 0.0007218706887215376

Training epoch-19 batch-123
Running loss of epoch-19 batch-123 = 0.0004449948319233954

Training epoch-19 batch-124
Running loss of epoch-19 batch-124 = 0.000555186124984175

Training epoch-19 batch-125
Running loss of epoch-19 batch-125 = 0.0005826150299981236

Training epoch-19 batch-126
Running loss of epoch-19 batch-126 = 0.0005169973592273891

Training epoch-19 batch-127
Running loss of epoch-19 batch-127 = 0.0006993254646658897

Training epoch-19 batch-128
Running loss of epoch-19 batch-128 = 0.00025957083562389016

Training epoch-19 batch-129
Running loss of epoch-19 batch-129 = 0.0004091691807843745

Training epoch-19 batch-130
Running loss of epoch-19 batch-130 = 0.0004867827519774437

Training epoch-19 batch-131
Running loss of epoch-19 batch-131 = 0.0014355479506775737

Training epoch-19 batch-132
Running loss of epoch-19 batch-132 = 0.0011370114516466856

Training epoch-19 batch-133
Running loss of epoch-19 batch-133 = 0.00036713777808472514

Training epoch-19 batch-134
Running loss of epoch-19 batch-134 = 0.00019116036128252745

Training epoch-19 batch-135
Running loss of epoch-19 batch-135 = 0.0010877815075218678

Training epoch-19 batch-136
Running loss of epoch-19 batch-136 = 0.0006670914590358734

Training epoch-19 batch-137
Running loss of epoch-19 batch-137 = 0.0004567856667563319

Training epoch-19 batch-138
Running loss of epoch-19 batch-138 = 0.0002466366859152913

Training epoch-19 batch-139
Running loss of epoch-19 batch-139 = 0.0005962239811196923

Training epoch-19 batch-140
Running loss of epoch-19 batch-140 = 0.0005715851439163089

Training epoch-19 batch-141
Running loss of epoch-19 batch-141 = 0.0006479469011537731

Training epoch-19 batch-142
Running loss of epoch-19 batch-142 = 0.0004264383460395038

Training epoch-19 batch-143
Running loss of epoch-19 batch-143 = 0.0005307361716404557

Training epoch-19 batch-144
Running loss of epoch-19 batch-144 = 0.0019256541272625327

Training epoch-19 batch-145
Running loss of epoch-19 batch-145 = 0.00034768355544656515

Training epoch-19 batch-146
Running loss of epoch-19 batch-146 = 0.0008859833469614387

Training epoch-19 batch-147
Running loss of epoch-19 batch-147 = 0.00039491301868110895

Training epoch-19 batch-148
Running loss of epoch-19 batch-148 = 0.0004894255544058979

Training epoch-19 batch-149
Running loss of epoch-19 batch-149 = 0.0002973346272483468

Training epoch-19 batch-150
Running loss of epoch-19 batch-150 = 0.0004737594863399863

Training epoch-19 batch-151
Running loss of epoch-19 batch-151 = 0.00021833274513483047

Training epoch-19 batch-152
Running loss of epoch-19 batch-152 = 0.0023204456083476543

Training epoch-19 batch-153
Running loss of epoch-19 batch-153 = 0.0004347807844169438

Training epoch-19 batch-154
Running loss of epoch-19 batch-154 = 0.00011604372411966324

Training epoch-19 batch-155
Running loss of epoch-19 batch-155 = 0.00035298807779327035

Training epoch-19 batch-156
Running loss of epoch-19 batch-156 = 0.0012248576385900378

Training epoch-19 batch-157
Running loss of epoch-19 batch-157 = 0.003619479015469551

Finished training epoch-19.



Average train loss at epoch-19 = 0.0006605411855503916

Started Evaluation

Average val loss at epoch-19 = 4.754756234194103

Accuracy for classes:
Accuracy for class equals is: 56.27 %
Accuracy for class main is: 35.57 %
Accuracy for class setUp is: 52.62 %
Accuracy for class onCreate is: 25.16 %
Accuracy for class toString is: 19.11 %
Accuracy for class run is: 31.05 %
Accuracy for class hashCode is: 74.91 %
Accuracy for class init is: 30.94 %
Accuracy for class execute is: 40.16 %
Accuracy for class get is: 18.97 %

Overall Accuracy = 37.53 %

Finished Evaluation



Started training epoch-20


Training epoch-20 batch-1
Running loss of epoch-20 batch-1 = 0.0003279153606854379

Training epoch-20 batch-2
Running loss of epoch-20 batch-2 = 0.0004517217166721821

Training epoch-20 batch-3
Running loss of epoch-20 batch-3 = 0.00019813410472124815

Training epoch-20 batch-4
Running loss of epoch-20 batch-4 = 0.0005431214231066406

Training epoch-20 batch-5
Running loss of epoch-20 batch-5 = 0.000739722978323698

Training epoch-20 batch-6
Running loss of epoch-20 batch-6 = 0.00018061534501612186

Training epoch-20 batch-7
Running loss of epoch-20 batch-7 = 0.0014622032176703215

Training epoch-20 batch-8
Running loss of epoch-20 batch-8 = 0.00033421802800148726

Training epoch-20 batch-9
Running loss of epoch-20 batch-9 = 0.00028776167891919613

Training epoch-20 batch-10
Running loss of epoch-20 batch-10 = 0.0012875995598733425

Training epoch-20 batch-11
Running loss of epoch-20 batch-11 = 0.0005040280520915985

Training epoch-20 batch-12
Running loss of epoch-20 batch-12 = 0.003018040442839265

Training epoch-20 batch-13
Running loss of epoch-20 batch-13 = 0.00013864785432815552

Training epoch-20 batch-14
Running loss of epoch-20 batch-14 = 0.0012297374196350574

Training epoch-20 batch-15
Running loss of epoch-20 batch-15 = 0.0003330814652144909

Training epoch-20 batch-16
Running loss of epoch-20 batch-16 = 0.002267167903482914

Training epoch-20 batch-17
Running loss of epoch-20 batch-17 = 0.00028203550027683377

Training epoch-20 batch-18
Running loss of epoch-20 batch-18 = 0.001638756599277258

Training epoch-20 batch-19
Running loss of epoch-20 batch-19 = 0.0005416050553321838

Training epoch-20 batch-20
Running loss of epoch-20 batch-20 = 0.00036180339520797133

Training epoch-20 batch-21
Running loss of epoch-20 batch-21 = 0.0007963083335198462

Training epoch-20 batch-22
Running loss of epoch-20 batch-22 = 0.0005431125173345208

Training epoch-20 batch-23
Running loss of epoch-20 batch-23 = 0.0002881361870095134

Training epoch-20 batch-24
Running loss of epoch-20 batch-24 = 0.0005745405796915293

Training epoch-20 batch-25
Running loss of epoch-20 batch-25 = 0.00018928898498415947

Training epoch-20 batch-26
Running loss of epoch-20 batch-26 = 0.005755786783993244

Training epoch-20 batch-27
Running loss of epoch-20 batch-27 = 0.0003862854791805148

Training epoch-20 batch-28
Running loss of epoch-20 batch-28 = 0.0002628191141411662

Training epoch-20 batch-29
Running loss of epoch-20 batch-29 = 0.002148360712453723

Training epoch-20 batch-30
Running loss of epoch-20 batch-30 = 0.0007280046120285988

Training epoch-20 batch-31
Running loss of epoch-20 batch-31 = 0.00017919216770678759

Training epoch-20 batch-32
Running loss of epoch-20 batch-32 = 0.00022109848214313388

Training epoch-20 batch-33
Running loss of epoch-20 batch-33 = 0.0001354113919660449

Training epoch-20 batch-34
Running loss of epoch-20 batch-34 = 0.0004682279541157186

Training epoch-20 batch-35
Running loss of epoch-20 batch-35 = 0.0004368454683572054

Training epoch-20 batch-36
Running loss of epoch-20 batch-36 = 0.0004995680646970868

Training epoch-20 batch-37
Running loss of epoch-20 batch-37 = 0.0006806483143009245

Training epoch-20 batch-38
Running loss of epoch-20 batch-38 = 0.00026031548622995615

Training epoch-20 batch-39
Running loss of epoch-20 batch-39 = 0.00029206054750829935

Training epoch-20 batch-40
Running loss of epoch-20 batch-40 = 0.0012991881230846047

Training epoch-20 batch-41
Running loss of epoch-20 batch-41 = 0.00032193592051044106

Training epoch-20 batch-42
Running loss of epoch-20 batch-42 = 0.00035638530971482396

Training epoch-20 batch-43
Running loss of epoch-20 batch-43 = 0.0005794580210931599

Training epoch-20 batch-44
Running loss of epoch-20 batch-44 = 0.00051525654271245

Training epoch-20 batch-45
Running loss of epoch-20 batch-45 = 0.0001933899475261569

Training epoch-20 batch-46
Running loss of epoch-20 batch-46 = 0.00024939014110714197

Training epoch-20 batch-47
Running loss of epoch-20 batch-47 = 0.00024431769270449877

Training epoch-20 batch-48
Running loss of epoch-20 batch-48 = 0.0005297362804412842

Training epoch-20 batch-49
Running loss of epoch-20 batch-49 = 0.00037820066791027784

Training epoch-20 batch-50
Running loss of epoch-20 batch-50 = 0.0012407228350639343

Training epoch-20 batch-51
Running loss of epoch-20 batch-51 = 0.00031504372600466013

Training epoch-20 batch-52
Running loss of epoch-20 batch-52 = 0.00011726515367627144

Training epoch-20 batch-53
Running loss of epoch-20 batch-53 = 0.0002202406176365912

Training epoch-20 batch-54
Running loss of epoch-20 batch-54 = 0.00015070964582264423

Training epoch-20 batch-55
Running loss of epoch-20 batch-55 = 0.00021982693579047918

Training epoch-20 batch-56
Running loss of epoch-20 batch-56 = 0.00046754127833992243

Training epoch-20 batch-57
Running loss of epoch-20 batch-57 = 0.0006571024423465133

Training epoch-20 batch-58
Running loss of epoch-20 batch-58 = 0.0001856698072515428

Training epoch-20 batch-59
Running loss of epoch-20 batch-59 = 0.0006530983373522758

Training epoch-20 batch-60
Running loss of epoch-20 batch-60 = 0.0001656181993894279

Training epoch-20 batch-61
Running loss of epoch-20 batch-61 = 0.0014603518648073077

Training epoch-20 batch-62
Running loss of epoch-20 batch-62 = 0.00019932619761675596

Training epoch-20 batch-63
Running loss of epoch-20 batch-63 = 0.000407184474170208

Training epoch-20 batch-64
Running loss of epoch-20 batch-64 = 0.00038013519952073693

Training epoch-20 batch-65
Running loss of epoch-20 batch-65 = 0.0001557663199491799

Training epoch-20 batch-66
Running loss of epoch-20 batch-66 = 0.0004575687926262617

Training epoch-20 batch-67
Running loss of epoch-20 batch-67 = 0.00031854960252530873

Training epoch-20 batch-68
Running loss of epoch-20 batch-68 = 0.00043009992805309594

Training epoch-20 batch-69
Running loss of epoch-20 batch-69 = 0.00015321921091526747

Training epoch-20 batch-70
Running loss of epoch-20 batch-70 = 8.643307955935597e-05

Training epoch-20 batch-71
Running loss of epoch-20 batch-71 = 0.0003336326335556805

Training epoch-20 batch-72
Running loss of epoch-20 batch-72 = 0.00019717041868716478

Training epoch-20 batch-73
Running loss of epoch-20 batch-73 = 0.00032307050423696637

Training epoch-20 batch-74
Running loss of epoch-20 batch-74 = 0.0007989669684320688

Training epoch-20 batch-75
Running loss of epoch-20 batch-75 = 0.00013208284508436918

Training epoch-20 batch-76
Running loss of epoch-20 batch-76 = 0.00020754258730448782

Training epoch-20 batch-77
Running loss of epoch-20 batch-77 = 0.000559828826226294

Training epoch-20 batch-78
Running loss of epoch-20 batch-78 = 0.00011671357788145542

Training epoch-20 batch-79
Running loss of epoch-20 batch-79 = 6.68712891638279e-05

Training epoch-20 batch-80
Running loss of epoch-20 batch-80 = 0.00027011753991246223

Training epoch-20 batch-81
Running loss of epoch-20 batch-81 = 0.00014234904665499926

Training epoch-20 batch-82
Running loss of epoch-20 batch-82 = 0.00011923536658287048

Training epoch-20 batch-83
Running loss of epoch-20 batch-83 = 0.00022026896476745605

Training epoch-20 batch-84
Running loss of epoch-20 batch-84 = 0.00015078147407621145

Training epoch-20 batch-85
Running loss of epoch-20 batch-85 = 6.016460247337818e-05

Training epoch-20 batch-86
Running loss of epoch-20 batch-86 = 0.00030992855317890644

Training epoch-20 batch-87
Running loss of epoch-20 batch-87 = 0.0003781956620514393

Training epoch-20 batch-88
Running loss of epoch-20 batch-88 = 0.0008782148361206055

Training epoch-20 batch-89
Running loss of epoch-20 batch-89 = 0.00012506748316809535

Training epoch-20 batch-90
Running loss of epoch-20 batch-90 = 9.867979679256678e-05

Training epoch-20 batch-91
Running loss of epoch-20 batch-91 = 0.00013988965656608343

Training epoch-20 batch-92
Running loss of epoch-20 batch-92 = 8.747930405661464e-05

Training epoch-20 batch-93
Running loss of epoch-20 batch-93 = 0.00018443766748532653

Training epoch-20 batch-94
Running loss of epoch-20 batch-94 = 0.00020952074555680156

Training epoch-20 batch-95
Running loss of epoch-20 batch-95 = 0.0003848588967230171

Training epoch-20 batch-96
Running loss of epoch-20 batch-96 = 0.0003481212770566344

Training epoch-20 batch-97
Running loss of epoch-20 batch-97 = 0.00024216360179707408

Training epoch-20 batch-98
Running loss of epoch-20 batch-98 = 0.00013707089237868786

Training epoch-20 batch-99
Running loss of epoch-20 batch-99 = 0.0003714021877385676

Training epoch-20 batch-100
Running loss of epoch-20 batch-100 = 7.090577855706215e-05

Training epoch-20 batch-101
Running loss of epoch-20 batch-101 = 4.787847865372896e-05

Training epoch-20 batch-102
Running loss of epoch-20 batch-102 = 4.84284246340394e-05

Training epoch-20 batch-103
Running loss of epoch-20 batch-103 = 0.00034515029983595014

Training epoch-20 batch-104
Running loss of epoch-20 batch-104 = 6.048684008419514e-05

Training epoch-20 batch-105
Running loss of epoch-20 batch-105 = 0.00022514734882861376

Training epoch-20 batch-106
Running loss of epoch-20 batch-106 = 0.00011221558088436723

Training epoch-20 batch-107
Running loss of epoch-20 batch-107 = 0.00012239813804626465

Training epoch-20 batch-108
Running loss of epoch-20 batch-108 = 7.546739652752876e-05

Training epoch-20 batch-109
Running loss of epoch-20 batch-109 = 0.00011654087575152516

Training epoch-20 batch-110
Running loss of epoch-20 batch-110 = 0.00022984901443123817

Training epoch-20 batch-111
Running loss of epoch-20 batch-111 = 9.21245664358139e-05

Training epoch-20 batch-112
Running loss of epoch-20 batch-112 = 0.00014551635831594467

Training epoch-20 batch-113
Running loss of epoch-20 batch-113 = 0.00010066968388855457

Training epoch-20 batch-114
Running loss of epoch-20 batch-114 = 5.975982639938593e-05

Training epoch-20 batch-115
Running loss of epoch-20 batch-115 = 7.273990195244551e-05

Training epoch-20 batch-116
Running loss of epoch-20 batch-116 = 0.0005355557659640908

Training epoch-20 batch-117
Running loss of epoch-20 batch-117 = 6.521935574710369e-05

Training epoch-20 batch-118
Running loss of epoch-20 batch-118 = 7.990456651896238e-05

Training epoch-20 batch-119
Running loss of epoch-20 batch-119 = 0.00026994076324626803

Training epoch-20 batch-120
Running loss of epoch-20 batch-120 = 0.0008512571221217513

Training epoch-20 batch-121
Running loss of epoch-20 batch-121 = 0.00038266158662736416

Training epoch-20 batch-122
Running loss of epoch-20 batch-122 = 5.1838578656315804e-05

Training epoch-20 batch-123
Running loss of epoch-20 batch-123 = 0.0001678004628047347

Training epoch-20 batch-124
Running loss of epoch-20 batch-124 = 0.00037366943433880806

Training epoch-20 batch-125
Running loss of epoch-20 batch-125 = 0.0003491818206384778

Training epoch-20 batch-126
Running loss of epoch-20 batch-126 = 0.00013831735122948885

Training epoch-20 batch-127
Running loss of epoch-20 batch-127 = 0.0001418370520696044

Training epoch-20 batch-128
Running loss of epoch-20 batch-128 = 0.0001364007475785911

Training epoch-20 batch-129
Running loss of epoch-20 batch-129 = 0.00015680101932957768

Training epoch-20 batch-130
Running loss of epoch-20 batch-130 = 0.0031490174587816

Training epoch-20 batch-131
Running loss of epoch-20 batch-131 = 0.00011498620733618736

Training epoch-20 batch-132
Running loss of epoch-20 batch-132 = 0.00012878247071057558

Training epoch-20 batch-133
Running loss of epoch-20 batch-133 = 8.62970482558012e-05

Training epoch-20 batch-134
Running loss of epoch-20 batch-134 = 8.327211253345013e-05

Training epoch-20 batch-135
Running loss of epoch-20 batch-135 = 0.0002040719846263528

Training epoch-20 batch-136
Running loss of epoch-20 batch-136 = 9.151466656476259e-05

Training epoch-20 batch-137
Running loss of epoch-20 batch-137 = 0.0005722581408917904

Training epoch-20 batch-138
Running loss of epoch-20 batch-138 = 0.00025074236327782273

Training epoch-20 batch-139
Running loss of epoch-20 batch-139 = 0.00016614573542028666

Training epoch-20 batch-140
Running loss of epoch-20 batch-140 = 0.0003351881168782711

Training epoch-20 batch-141
Running loss of epoch-20 batch-141 = 0.0005888448795303702

Training epoch-20 batch-142
Running loss of epoch-20 batch-142 = 0.0001222289865836501

Training epoch-20 batch-143
Running loss of epoch-20 batch-143 = 0.0008187722414731979

Training epoch-20 batch-144
Running loss of epoch-20 batch-144 = 0.0006860070279799402

Training epoch-20 batch-145
Running loss of epoch-20 batch-145 = 0.00014948705211281776

Training epoch-20 batch-146
Running loss of epoch-20 batch-146 = 0.0003249674919061363

Training epoch-20 batch-147
Running loss of epoch-20 batch-147 = 0.00011899170931428671

Training epoch-20 batch-148
Running loss of epoch-20 batch-148 = 0.00011808454291895032

Training epoch-20 batch-149
Running loss of epoch-20 batch-149 = 0.00022701022680848837

Training epoch-20 batch-150
Running loss of epoch-20 batch-150 = 0.00044921640073880553

Training epoch-20 batch-151
Running loss of epoch-20 batch-151 = 0.0012304187985137105

Training epoch-20 batch-152
Running loss of epoch-20 batch-152 = 0.00016360462177544832

Training epoch-20 batch-153
Running loss of epoch-20 batch-153 = 5.1694223657250404e-05

Training epoch-20 batch-154
Running loss of epoch-20 batch-154 = 0.0001407158561050892

Training epoch-20 batch-155
Running loss of epoch-20 batch-155 = 0.0001225133310072124

Training epoch-20 batch-156
Running loss of epoch-20 batch-156 = 0.0007153110345825553

Training epoch-20 batch-157
Running loss of epoch-20 batch-157 = 0.00514540821313858

Finished training epoch-20.



Average train loss at epoch-20 = 0.00044671135172247886

Started Evaluation

Average val loss at epoch-20 = 4.410968427595339

Accuracy for classes:
Accuracy for class equals is: 59.74 %
Accuracy for class main is: 34.92 %
Accuracy for class setUp is: 65.08 %
Accuracy for class onCreate is: 32.09 %
Accuracy for class toString is: 18.43 %
Accuracy for class run is: 26.71 %
Accuracy for class hashCode is: 76.03 %
Accuracy for class init is: 33.41 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 25.13 %

Overall Accuracy = 40.11 %

Finished Evaluation



Started training epoch-21


Training epoch-21 batch-1
Running loss of epoch-21 batch-1 = 0.00014238222502171993

Training epoch-21 batch-2
Running loss of epoch-21 batch-2 = 0.00028829489019699395

Training epoch-21 batch-3
Running loss of epoch-21 batch-3 = 0.00041122070979326963

Training epoch-21 batch-4
Running loss of epoch-21 batch-4 = 0.003256026189774275

Training epoch-21 batch-5
Running loss of epoch-21 batch-5 = 0.0003644120879471302

Training epoch-21 batch-6
Running loss of epoch-21 batch-6 = 0.0008312967838719487

Training epoch-21 batch-7
Running loss of epoch-21 batch-7 = 0.00013605400454252958

Training epoch-21 batch-8
Running loss of epoch-21 batch-8 = 0.00028762815054506063

Training epoch-21 batch-9
Running loss of epoch-21 batch-9 = 0.00015711295418441296

Training epoch-21 batch-10
Running loss of epoch-21 batch-10 = 0.0025995022151619196

Training epoch-21 batch-11
Running loss of epoch-21 batch-11 = 0.0004256896208971739

Training epoch-21 batch-12
Running loss of epoch-21 batch-12 = 0.00026522495318204165

Training epoch-21 batch-13
Running loss of epoch-21 batch-13 = 0.00021900166757404804

Training epoch-21 batch-14
Running loss of epoch-21 batch-14 = 0.0007251178612932563

Training epoch-21 batch-15
Running loss of epoch-21 batch-15 = 0.0007203810382634401

Training epoch-21 batch-16
Running loss of epoch-21 batch-16 = 0.0014250773238018155

Training epoch-21 batch-17
Running loss of epoch-21 batch-17 = 0.0006900313310325146

Training epoch-21 batch-18
Running loss of epoch-21 batch-18 = 0.000227317214012146

Training epoch-21 batch-19
Running loss of epoch-21 batch-19 = 0.0004512403393164277

Training epoch-21 batch-20
Running loss of epoch-21 batch-20 = 0.0009536047582514584

Training epoch-21 batch-21
Running loss of epoch-21 batch-21 = 0.000538353284355253

Training epoch-21 batch-22
Running loss of epoch-21 batch-22 = 0.0004612079355865717

Training epoch-21 batch-23
Running loss of epoch-21 batch-23 = 0.0010434400755912066

Training epoch-21 batch-24
Running loss of epoch-21 batch-24 = 0.0009481227025389671

Training epoch-21 batch-25
Running loss of epoch-21 batch-25 = 9.261153172701597e-05

Training epoch-21 batch-26
Running loss of epoch-21 batch-26 = 0.0007363788317888975

Training epoch-21 batch-27
Running loss of epoch-21 batch-27 = 5.727133248001337e-05

Training epoch-21 batch-28
Running loss of epoch-21 batch-28 = 0.0001869810512289405

Training epoch-21 batch-29
Running loss of epoch-21 batch-29 = 0.00048544869059696794

Training epoch-21 batch-30
Running loss of epoch-21 batch-30 = 6.24428503215313e-05

Training epoch-21 batch-31
Running loss of epoch-21 batch-31 = 0.0018454230157658458

Training epoch-21 batch-32
Running loss of epoch-21 batch-32 = 0.00045792380115017295

Training epoch-21 batch-33
Running loss of epoch-21 batch-33 = 0.001876086462289095

Training epoch-21 batch-34
Running loss of epoch-21 batch-34 = 0.000718691386282444

Training epoch-21 batch-35
Running loss of epoch-21 batch-35 = 0.0022352333180606365

Training epoch-21 batch-36
Running loss of epoch-21 batch-36 = 0.0002812722232192755

Training epoch-21 batch-37
Running loss of epoch-21 batch-37 = 0.00016947078984230757

Training epoch-21 batch-38
Running loss of epoch-21 batch-38 = 0.0012092258548364043

Training epoch-21 batch-39
Running loss of epoch-21 batch-39 = 0.000552729528862983

Training epoch-21 batch-40
Running loss of epoch-21 batch-40 = 0.00044286897173151374

Training epoch-21 batch-41
Running loss of epoch-21 batch-41 = 0.0008715514559298754

Training epoch-21 batch-42
Running loss of epoch-21 batch-42 = 0.0007471690187230706

Training epoch-21 batch-43
Running loss of epoch-21 batch-43 = 0.0016743364976719022

Training epoch-21 batch-44
Running loss of epoch-21 batch-44 = 0.0010829259408637881

Training epoch-21 batch-45
Running loss of epoch-21 batch-45 = 0.0008653496624901891

Training epoch-21 batch-46
Running loss of epoch-21 batch-46 = 0.0014151801588013768

Training epoch-21 batch-47
Running loss of epoch-21 batch-47 = 0.00027373875491321087

Training epoch-21 batch-48
Running loss of epoch-21 batch-48 = 0.0007853275164961815

Training epoch-21 batch-49
Running loss of epoch-21 batch-49 = 0.0008333708974532783

Training epoch-21 batch-50
Running loss of epoch-21 batch-50 = 0.0005682902410626411

Training epoch-21 batch-51
Running loss of epoch-21 batch-51 = 0.0016169872833415866

Training epoch-21 batch-52
Running loss of epoch-21 batch-52 = 0.0002843911061063409

Training epoch-21 batch-53
Running loss of epoch-21 batch-53 = 7.873994763940573e-05

Training epoch-21 batch-54
Running loss of epoch-21 batch-54 = 0.0011352007277309895

Training epoch-21 batch-55
Running loss of epoch-21 batch-55 = 0.0004508146084845066

Training epoch-21 batch-56
Running loss of epoch-21 batch-56 = 0.0006745573482476175

Training epoch-21 batch-57
Running loss of epoch-21 batch-57 = 0.0008608960197307169

Training epoch-21 batch-58
Running loss of epoch-21 batch-58 = 0.0004766261554323137

Training epoch-21 batch-59
Running loss of epoch-21 batch-59 = 0.0012669176794588566

Training epoch-21 batch-60
Running loss of epoch-21 batch-60 = 0.0002901283442042768

Training epoch-21 batch-61
Running loss of epoch-21 batch-61 = 0.000195863947737962

Training epoch-21 batch-62
Running loss of epoch-21 batch-62 = 0.0013212350895628333

Training epoch-21 batch-63
Running loss of epoch-21 batch-63 = 0.00043946292134933174

Training epoch-21 batch-64
Running loss of epoch-21 batch-64 = 0.0015004529850557446

Training epoch-21 batch-65
Running loss of epoch-21 batch-65 = 0.00038689468055963516

Training epoch-21 batch-66
Running loss of epoch-21 batch-66 = 0.0005769230192527175

Training epoch-21 batch-67
Running loss of epoch-21 batch-67 = 0.001820103614591062

Training epoch-21 batch-68
Running loss of epoch-21 batch-68 = 0.0003522137412801385

Training epoch-21 batch-69
Running loss of epoch-21 batch-69 = 0.0002089906483888626

Training epoch-21 batch-70
Running loss of epoch-21 batch-70 = 0.0001917847548611462

Training epoch-21 batch-71
Running loss of epoch-21 batch-71 = 0.00023203599266707897

Training epoch-21 batch-72
Running loss of epoch-21 batch-72 = 0.0005073178326711059

Training epoch-21 batch-73
Running loss of epoch-21 batch-73 = 0.00014046241994947195

Training epoch-21 batch-74
Running loss of epoch-21 batch-74 = 0.00044765486381947994

Training epoch-21 batch-75
Running loss of epoch-21 batch-75 = 0.0003338084788993001

Training epoch-21 batch-76
Running loss of epoch-21 batch-76 = 0.00012629746925085783

Training epoch-21 batch-77
Running loss of epoch-21 batch-77 = 0.001612385967746377

Training epoch-21 batch-78
Running loss of epoch-21 batch-78 = 0.0001478154445067048

Training epoch-21 batch-79
Running loss of epoch-21 batch-79 = 0.0008264241623692214

Training epoch-21 batch-80
Running loss of epoch-21 batch-80 = 0.0006704413099214435

Training epoch-21 batch-81
Running loss of epoch-21 batch-81 = 0.0001385941868647933

Training epoch-21 batch-82
Running loss of epoch-21 batch-82 = 0.00019075011368840933

Training epoch-21 batch-83
Running loss of epoch-21 batch-83 = 0.00010804994963109493

Training epoch-21 batch-84
Running loss of epoch-21 batch-84 = 0.0002350442809984088

Training epoch-21 batch-85
Running loss of epoch-21 batch-85 = 0.00021269673015922308

Training epoch-21 batch-86
Running loss of epoch-21 batch-86 = 0.00037076015723869205

Training epoch-21 batch-87
Running loss of epoch-21 batch-87 = 0.0004527357523329556

Training epoch-21 batch-88
Running loss of epoch-21 batch-88 = 0.00010862469207495451

Training epoch-21 batch-89
Running loss of epoch-21 batch-89 = 0.0008195373229682446

Training epoch-21 batch-90
Running loss of epoch-21 batch-90 = 0.0004172326298430562

Training epoch-21 batch-91
Running loss of epoch-21 batch-91 = 0.00042403890984132886

Training epoch-21 batch-92
Running loss of epoch-21 batch-92 = 0.0006313702906481922

Training epoch-21 batch-93
Running loss of epoch-21 batch-93 = 0.0006708464352414012

Training epoch-21 batch-94
Running loss of epoch-21 batch-94 = 0.00012163468636572361

Training epoch-21 batch-95
Running loss of epoch-21 batch-95 = 9.468966163694859e-05

Training epoch-21 batch-96
Running loss of epoch-21 batch-96 = 0.0006494825938716531

Training epoch-21 batch-97
Running loss of epoch-21 batch-97 = 0.00010735122486948967

Training epoch-21 batch-98
Running loss of epoch-21 batch-98 = 0.00023371860152110457

Training epoch-21 batch-99
Running loss of epoch-21 batch-99 = 7.297703996300697e-05

Training epoch-21 batch-100
Running loss of epoch-21 batch-100 = 0.0001761442981660366

Training epoch-21 batch-101
Running loss of epoch-21 batch-101 = 9.924988262355328e-05

Training epoch-21 batch-102
Running loss of epoch-21 batch-102 = 0.00017220841255038977

Training epoch-21 batch-103
Running loss of epoch-21 batch-103 = 0.00024846382439136505

Training epoch-21 batch-104
Running loss of epoch-21 batch-104 = 0.0006900111911818385

Training epoch-21 batch-105
Running loss of epoch-21 batch-105 = 0.0001752564567141235

Training epoch-21 batch-106
Running loss of epoch-21 batch-106 = 0.00017378083430230618

Training epoch-21 batch-107
Running loss of epoch-21 batch-107 = 0.00023252348182722926

Training epoch-21 batch-108
Running loss of epoch-21 batch-108 = 0.001692002872005105

Training epoch-21 batch-109
Running loss of epoch-21 batch-109 = 0.0002643537591211498

Training epoch-21 batch-110
Running loss of epoch-21 batch-110 = 0.0007426443626172841

Training epoch-21 batch-111
Running loss of epoch-21 batch-111 = 0.0002767182304523885

Training epoch-21 batch-112
Running loss of epoch-21 batch-112 = 0.0003239457728341222

Training epoch-21 batch-113
Running loss of epoch-21 batch-113 = 0.00020556891104206443

Training epoch-21 batch-114
Running loss of epoch-21 batch-114 = 0.0005564198363572359

Training epoch-21 batch-115
Running loss of epoch-21 batch-115 = 0.0008311363053508103

Training epoch-21 batch-116
Running loss of epoch-21 batch-116 = 0.0003787793102674186

Training epoch-21 batch-117
Running loss of epoch-21 batch-117 = 0.0014890712918713689

Training epoch-21 batch-118
Running loss of epoch-21 batch-118 = 0.00010737136472016573

Training epoch-21 batch-119
Running loss of epoch-21 batch-119 = 0.0006418985431082547

Training epoch-21 batch-120
Running loss of epoch-21 batch-120 = 0.0003161309286952019

Training epoch-21 batch-121
Running loss of epoch-21 batch-121 = 0.00010804703924804926

Training epoch-21 batch-122
Running loss of epoch-21 batch-122 = 0.00047722202725708485

Training epoch-21 batch-123
Running loss of epoch-21 batch-123 = 0.00015640922356396914

Training epoch-21 batch-124
Running loss of epoch-21 batch-124 = 0.00017870345618575811

Training epoch-21 batch-125
Running loss of epoch-21 batch-125 = 0.0002159440191462636

Training epoch-21 batch-126
Running loss of epoch-21 batch-126 = 0.00019989616703242064

Training epoch-21 batch-127
Running loss of epoch-21 batch-127 = 0.000148348743095994

Training epoch-21 batch-128
Running loss of epoch-21 batch-128 = 0.00012374005746096373

Training epoch-21 batch-129
Running loss of epoch-21 batch-129 = 0.0003175693564116955

Training epoch-21 batch-130
Running loss of epoch-21 batch-130 = 0.00031785969622433186

Training epoch-21 batch-131
Running loss of epoch-21 batch-131 = 4.948407877236605e-05

Training epoch-21 batch-132
Running loss of epoch-21 batch-132 = 0.00017252285033464432

Training epoch-21 batch-133
Running loss of epoch-21 batch-133 = 0.00048603047616779804

Training epoch-21 batch-134
Running loss of epoch-21 batch-134 = 0.00016244291327893734

Training epoch-21 batch-135
Running loss of epoch-21 batch-135 = 0.00020924361888319254

Training epoch-21 batch-136
Running loss of epoch-21 batch-136 = 0.0022690552286803722

Training epoch-21 batch-137
Running loss of epoch-21 batch-137 = 0.000125269521959126

Training epoch-21 batch-138
Running loss of epoch-21 batch-138 = 0.0005573031958192587

Training epoch-21 batch-139
Running loss of epoch-21 batch-139 = 0.0003974580904468894

Training epoch-21 batch-140
Running loss of epoch-21 batch-140 = 0.0003419945714995265

Training epoch-21 batch-141
Running loss of epoch-21 batch-141 = 0.0002482769777998328

Training epoch-21 batch-142
Running loss of epoch-21 batch-142 = 0.00017707503866404295

Training epoch-21 batch-143
Running loss of epoch-21 batch-143 = 0.000561762135475874

Training epoch-21 batch-144
Running loss of epoch-21 batch-144 = 0.0003359574475325644

Training epoch-21 batch-145
Running loss of epoch-21 batch-145 = 0.0016406793147325516

Training epoch-21 batch-146
Running loss of epoch-21 batch-146 = 0.0003822175203822553

Training epoch-21 batch-147
Running loss of epoch-21 batch-147 = 0.00031250264146365225

Training epoch-21 batch-148
Running loss of epoch-21 batch-148 = 0.00025790627114474773

Training epoch-21 batch-149
Running loss of epoch-21 batch-149 = 7.170543540269136e-05

Training epoch-21 batch-150
Running loss of epoch-21 batch-150 = 0.00024410849437117577

Training epoch-21 batch-151
Running loss of epoch-21 batch-151 = 0.00015554821584373713

Training epoch-21 batch-152
Running loss of epoch-21 batch-152 = 0.0007552528404630721

Training epoch-21 batch-153
Running loss of epoch-21 batch-153 = 0.0001925579272210598

Training epoch-21 batch-154
Running loss of epoch-21 batch-154 = 0.00010992900934070349

Training epoch-21 batch-155
Running loss of epoch-21 batch-155 = 0.0006330643082037568

Training epoch-21 batch-156
Running loss of epoch-21 batch-156 = 0.0003225879045203328

Training epoch-21 batch-157
Running loss of epoch-21 batch-157 = 0.00043660402297973633

Finished training epoch-21.



Average train loss at epoch-21 = 0.000554544067941606

Started Evaluation

Average val loss at epoch-21 = 4.519840980830946

Accuracy for classes:
Accuracy for class equals is: 61.55 %
Accuracy for class main is: 42.62 %
Accuracy for class setUp is: 39.18 %
Accuracy for class onCreate is: 29.85 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 32.19 %
Accuracy for class hashCode is: 72.28 %
Accuracy for class init is: 18.61 %
Accuracy for class execute is: 27.71 %
Accuracy for class get is: 27.95 %

Overall Accuracy = 38.58 %

Finished Evaluation



Started training epoch-22


Training epoch-22 batch-1
Running loss of epoch-22 batch-1 = 0.00020924006821587682

Training epoch-22 batch-2
Running loss of epoch-22 batch-2 = 7.076689507812262e-05

Training epoch-22 batch-3
Running loss of epoch-22 batch-3 = 0.00020657939603552222

Training epoch-22 batch-4
Running loss of epoch-22 batch-4 = 7.11402390152216e-05

Training epoch-22 batch-5
Running loss of epoch-22 batch-5 = 0.0009297872893512249

Training epoch-22 batch-6
Running loss of epoch-22 batch-6 = 0.0003233683528378606

Training epoch-22 batch-7
Running loss of epoch-22 batch-7 = 0.00037933699786663055

Training epoch-22 batch-8
Running loss of epoch-22 batch-8 = 0.00041430321289226413

Training epoch-22 batch-9
Running loss of epoch-22 batch-9 = 0.0002088466426357627

Training epoch-22 batch-10
Running loss of epoch-22 batch-10 = 9.469286305829883e-05

Training epoch-22 batch-11
Running loss of epoch-22 batch-11 = 0.00012301059905439615

Training epoch-22 batch-12
Running loss of epoch-22 batch-12 = 0.00018544273916631937

Training epoch-22 batch-13
Running loss of epoch-22 batch-13 = 0.0002605215413495898

Training epoch-22 batch-14
Running loss of epoch-22 batch-14 = 0.00027277966728433967

Training epoch-22 batch-15
Running loss of epoch-22 batch-15 = 0.00018772570183500648

Training epoch-22 batch-16
Running loss of epoch-22 batch-16 = 0.00019149243598803878

Training epoch-22 batch-17
Running loss of epoch-22 batch-17 = 0.00018590583931654692

Training epoch-22 batch-18
Running loss of epoch-22 batch-18 = 0.0036464333534240723

Training epoch-22 batch-19
Running loss of epoch-22 batch-19 = 5.399668589234352e-05

Training epoch-22 batch-20
Running loss of epoch-22 batch-20 = 0.00013450253754854202

Training epoch-22 batch-21
Running loss of epoch-22 batch-21 = 7.590092718601227e-05

Training epoch-22 batch-22
Running loss of epoch-22 batch-22 = 0.00010251987259835005

Training epoch-22 batch-23
Running loss of epoch-22 batch-23 = 0.00013272592332214117

Training epoch-22 batch-24
Running loss of epoch-22 batch-24 = 0.00011494680074974895

Training epoch-22 batch-25
Running loss of epoch-22 batch-25 = 0.0014494381612166762

Training epoch-22 batch-26
Running loss of epoch-22 batch-26 = 0.0002391870366409421

Training epoch-22 batch-27
Running loss of epoch-22 batch-27 = 0.00038956006756052375

Training epoch-22 batch-28
Running loss of epoch-22 batch-28 = 3.8435100577771664e-05

Training epoch-22 batch-29
Running loss of epoch-22 batch-29 = 0.00010696641402319074

Training epoch-22 batch-30
Running loss of epoch-22 batch-30 = 0.00016682589193806052

Training epoch-22 batch-31
Running loss of epoch-22 batch-31 = 0.00011760118650272489

Training epoch-22 batch-32
Running loss of epoch-22 batch-32 = 0.001175602781586349

Training epoch-22 batch-33
Running loss of epoch-22 batch-33 = 0.00029854237800464034

Training epoch-22 batch-34
Running loss of epoch-22 batch-34 = 0.000420757191022858

Training epoch-22 batch-35
Running loss of epoch-22 batch-35 = 0.00026701902970671654

Training epoch-22 batch-36
Running loss of epoch-22 batch-36 = 0.00010854378342628479

Training epoch-22 batch-37
Running loss of epoch-22 batch-37 = 0.0002125196624547243

Training epoch-22 batch-38
Running loss of epoch-22 batch-38 = 0.00013324001338332891

Training epoch-22 batch-39
Running loss of epoch-22 batch-39 = 0.0002571515506133437

Training epoch-22 batch-40
Running loss of epoch-22 batch-40 = 0.0001249441411346197

Training epoch-22 batch-41
Running loss of epoch-22 batch-41 = 0.00026864412939175963

Training epoch-22 batch-42
Running loss of epoch-22 batch-42 = 0.0001715382095426321

Training epoch-22 batch-43
Running loss of epoch-22 batch-43 = 0.00016332976520061493

Training epoch-22 batch-44
Running loss of epoch-22 batch-44 = 0.000217802997212857

Training epoch-22 batch-45
Running loss of epoch-22 batch-45 = 9.890517685562372e-05

Training epoch-22 batch-46
Running loss of epoch-22 batch-46 = 0.00013265118468552828

Training epoch-22 batch-47
Running loss of epoch-22 batch-47 = 0.000266432820353657

Training epoch-22 batch-48
Running loss of epoch-22 batch-48 = 0.00013827491784468293

Training epoch-22 batch-49
Running loss of epoch-22 batch-49 = 6.57056225463748e-05

Training epoch-22 batch-50
Running loss of epoch-22 batch-50 = 0.00046284403651952744

Training epoch-22 batch-51
Running loss of epoch-22 batch-51 = 9.268138092011213e-05

Training epoch-22 batch-52
Running loss of epoch-22 batch-52 = 0.00043996807653456926

Training epoch-22 batch-53
Running loss of epoch-22 batch-53 = 0.0001606062287464738

Training epoch-22 batch-54
Running loss of epoch-22 batch-54 = 5.6063407100737095e-05

Training epoch-22 batch-55
Running loss of epoch-22 batch-55 = 0.00016158120706677437

Training epoch-22 batch-56
Running loss of epoch-22 batch-56 = 0.0005989482742734253

Training epoch-22 batch-57
Running loss of epoch-22 batch-57 = 3.9987615309655666e-05

Training epoch-22 batch-58
Running loss of epoch-22 batch-58 = 0.00028002209728583694

Training epoch-22 batch-59
Running loss of epoch-22 batch-59 = 5.025602877140045e-05

Training epoch-22 batch-60
Running loss of epoch-22 batch-60 = 9.365490404888988e-05

Training epoch-22 batch-61
Running loss of epoch-22 batch-61 = 4.8043904826045036e-05

Training epoch-22 batch-62
Running loss of epoch-22 batch-62 = 6.521912291646004e-05

Training epoch-22 batch-63
Running loss of epoch-22 batch-63 = 4.2598461732268333e-05

Training epoch-22 batch-64
Running loss of epoch-22 batch-64 = 0.00014676200225949287

Training epoch-22 batch-65
Running loss of epoch-22 batch-65 = 0.00016616401262581348

Training epoch-22 batch-66
Running loss of epoch-22 batch-66 = 0.00020078581292182207

Training epoch-22 batch-67
Running loss of epoch-22 batch-67 = 0.0001290894579142332

Training epoch-22 batch-68
Running loss of epoch-22 batch-68 = 4.028831608593464e-05

Training epoch-22 batch-69
Running loss of epoch-22 batch-69 = 2.246932126581669e-05

Training epoch-22 batch-70
Running loss of epoch-22 batch-70 = 0.00014825823018327355

Training epoch-22 batch-71
Running loss of epoch-22 batch-71 = 5.040387623012066e-05

Training epoch-22 batch-72
Running loss of epoch-22 batch-72 = 0.00017298891907557845

Training epoch-22 batch-73
Running loss of epoch-22 batch-73 = 1.981295645236969e-05

Training epoch-22 batch-74
Running loss of epoch-22 batch-74 = 4.397344309836626e-05

Training epoch-22 batch-75
Running loss of epoch-22 batch-75 = 8.70456569828093e-05

Training epoch-22 batch-76
Running loss of epoch-22 batch-76 = 2.8236419893801212e-05

Training epoch-22 batch-77
Running loss of epoch-22 batch-77 = 0.0005335718160495162

Training epoch-22 batch-78
Running loss of epoch-22 batch-78 = 9.464530739933252e-05

Training epoch-22 batch-79
Running loss of epoch-22 batch-79 = 0.00013029389083385468

Training epoch-22 batch-80
Running loss of epoch-22 batch-80 = 0.0001620156690478325

Training epoch-22 batch-81
Running loss of epoch-22 batch-81 = 6.686907727271318e-05

Training epoch-22 batch-82
Running loss of epoch-22 batch-82 = 4.9138208851218224e-05

Training epoch-22 batch-83
Running loss of epoch-22 batch-83 = 6.88404543325305e-05

Training epoch-22 batch-84
Running loss of epoch-22 batch-84 = 0.000355515192495659

Training epoch-22 batch-85
Running loss of epoch-22 batch-85 = 2.5863759219646454e-05

Training epoch-22 batch-86
Running loss of epoch-22 batch-86 = 6.018707063049078e-05

Training epoch-22 batch-87
Running loss of epoch-22 batch-87 = 0.0003954673884436488

Training epoch-22 batch-88
Running loss of epoch-22 batch-88 = 6.134802242740989e-05

Training epoch-22 batch-89
Running loss of epoch-22 batch-89 = 6.374885560944676e-05

Training epoch-22 batch-90
Running loss of epoch-22 batch-90 = 7.42331612855196e-05

Training epoch-22 batch-91
Running loss of epoch-22 batch-91 = 3.553438000380993e-05

Training epoch-22 batch-92
Running loss of epoch-22 batch-92 = 0.0001466752728447318

Training epoch-22 batch-93
Running loss of epoch-22 batch-93 = 0.00010000320617109537

Training epoch-22 batch-94
Running loss of epoch-22 batch-94 = 0.00012618897017091513

Training epoch-22 batch-95
Running loss of epoch-22 batch-95 = 6.747269071638584e-05

Training epoch-22 batch-96
Running loss of epoch-22 batch-96 = 9.907176718115807e-05

Training epoch-22 batch-97
Running loss of epoch-22 batch-97 = 0.00011224765330553055

Training epoch-22 batch-98
Running loss of epoch-22 batch-98 = 0.00015201204223558307

Training epoch-22 batch-99
Running loss of epoch-22 batch-99 = 9.95490700006485e-05

Training epoch-22 batch-100
Running loss of epoch-22 batch-100 = 6.362900603562593e-05

Training epoch-22 batch-101
Running loss of epoch-22 batch-101 = 0.00016292696818709373

Training epoch-22 batch-102
Running loss of epoch-22 batch-102 = 0.00013393827248364687

Training epoch-22 batch-103
Running loss of epoch-22 batch-103 = 7.713306695222855e-05

Training epoch-22 batch-104
Running loss of epoch-22 batch-104 = 7.06773716956377e-05

Training epoch-22 batch-105
Running loss of epoch-22 batch-105 = 0.0009244302054867148

Training epoch-22 batch-106
Running loss of epoch-22 batch-106 = 0.00015370571054518223

Training epoch-22 batch-107
Running loss of epoch-22 batch-107 = 8.218223229050636e-05

Training epoch-22 batch-108
Running loss of epoch-22 batch-108 = 0.0019231592305004597

Training epoch-22 batch-109
Running loss of epoch-22 batch-109 = 6.0063786804676056e-05

Training epoch-22 batch-110
Running loss of epoch-22 batch-110 = 3.337895032018423e-05

Training epoch-22 batch-111
Running loss of epoch-22 batch-111 = 8.093926589936018e-05

Training epoch-22 batch-112
Running loss of epoch-22 batch-112 = 3.073364496231079e-05

Training epoch-22 batch-113
Running loss of epoch-22 batch-113 = 8.063262794166803e-05

Training epoch-22 batch-114
Running loss of epoch-22 batch-114 = 0.0001772131072357297

Training epoch-22 batch-115
Running loss of epoch-22 batch-115 = 0.00016894156578928232

Training epoch-22 batch-116
Running loss of epoch-22 batch-116 = 4.749069921672344e-05

Training epoch-22 batch-117
Running loss of epoch-22 batch-117 = 0.00029029263532720506

Training epoch-22 batch-118
Running loss of epoch-22 batch-118 = 0.00026738771703094244

Training epoch-22 batch-119
Running loss of epoch-22 batch-119 = 0.0011268726084381342

Training epoch-22 batch-120
Running loss of epoch-22 batch-120 = 8.75275582075119e-05

Training epoch-22 batch-121
Running loss of epoch-22 batch-121 = 7.797835860401392e-05

Training epoch-22 batch-122
Running loss of epoch-22 batch-122 = 6.76894560456276e-05

Training epoch-22 batch-123
Running loss of epoch-22 batch-123 = 0.000603731838054955

Training epoch-22 batch-124
Running loss of epoch-22 batch-124 = 0.000137741444632411

Training epoch-22 batch-125
Running loss of epoch-22 batch-125 = 7.336604176089168e-05

Training epoch-22 batch-126
Running loss of epoch-22 batch-126 = 0.00011974549852311611

Training epoch-22 batch-127
Running loss of epoch-22 batch-127 = 8.455431088805199e-05

Training epoch-22 batch-128
Running loss of epoch-22 batch-128 = 3.846304025501013e-05

Training epoch-22 batch-129
Running loss of epoch-22 batch-129 = 9.07807843759656e-05

Training epoch-22 batch-130
Running loss of epoch-22 batch-130 = 0.00014627189375460148

Training epoch-22 batch-131
Running loss of epoch-22 batch-131 = 0.00014028867008164525

Training epoch-22 batch-132
Running loss of epoch-22 batch-132 = 0.000457396381534636

Training epoch-22 batch-133
Running loss of epoch-22 batch-133 = 0.00014367024414241314

Training epoch-22 batch-134
Running loss of epoch-22 batch-134 = 5.620496813207865e-05

Training epoch-22 batch-135
Running loss of epoch-22 batch-135 = 2.602266613394022e-05

Training epoch-22 batch-136
Running loss of epoch-22 batch-136 = 0.00011679012095555663

Training epoch-22 batch-137
Running loss of epoch-22 batch-137 = 0.00010372104588896036

Training epoch-22 batch-138
Running loss of epoch-22 batch-138 = 0.00011392368469387293

Training epoch-22 batch-139
Running loss of epoch-22 batch-139 = 0.0003616667818278074

Training epoch-22 batch-140
Running loss of epoch-22 batch-140 = 0.00027516589034348726

Training epoch-22 batch-141
Running loss of epoch-22 batch-141 = 0.0001123127294704318

Training epoch-22 batch-142
Running loss of epoch-22 batch-142 = 8.407002314925194e-05

Training epoch-22 batch-143
Running loss of epoch-22 batch-143 = 0.00012640270870178938

Training epoch-22 batch-144
Running loss of epoch-22 batch-144 = 0.00012705172412097454

Training epoch-22 batch-145
Running loss of epoch-22 batch-145 = 4.22257580794394e-05

Training epoch-22 batch-146
Running loss of epoch-22 batch-146 = 4.349881783127785e-05

Training epoch-22 batch-147
Running loss of epoch-22 batch-147 = 6.555300205945969e-05

Training epoch-22 batch-148
Running loss of epoch-22 batch-148 = 0.0002576856641098857

Training epoch-22 batch-149
Running loss of epoch-22 batch-149 = 0.00022123544476926327

Training epoch-22 batch-150
Running loss of epoch-22 batch-150 = 5.974655505269766e-05

Training epoch-22 batch-151
Running loss of epoch-22 batch-151 = 4.748464561998844e-05

Training epoch-22 batch-152
Running loss of epoch-22 batch-152 = 2.891023177653551e-05

Training epoch-22 batch-153
Running loss of epoch-22 batch-153 = 0.00019239552784711123

Training epoch-22 batch-154
Running loss of epoch-22 batch-154 = 0.0006881228182464838

Training epoch-22 batch-155
Running loss of epoch-22 batch-155 = 0.00020407047122716904

Training epoch-22 batch-156
Running loss of epoch-22 batch-156 = 3.6273966543376446e-05

Training epoch-22 batch-157
Running loss of epoch-22 batch-157 = 0.0002047363668680191

Finished training epoch-22.



Average train loss at epoch-22 = 0.00021888950672000647

Started Evaluation

Average val loss at epoch-22 = 4.418322905113823

Accuracy for classes:
Accuracy for class equals is: 63.53 %
Accuracy for class main is: 48.69 %
Accuracy for class setUp is: 48.69 %
Accuracy for class onCreate is: 32.20 %
Accuracy for class toString is: 42.32 %
Accuracy for class run is: 30.14 %
Accuracy for class hashCode is: 71.54 %
Accuracy for class init is: 29.15 %
Accuracy for class execute is: 17.67 %
Accuracy for class get is: 25.13 %

Overall Accuracy = 41.26 %

Finished Evaluation



Started training epoch-23


Training epoch-23 batch-1
Running loss of epoch-23 batch-1 = 4.32207016274333e-05

Training epoch-23 batch-2
Running loss of epoch-23 batch-2 = 3.6941724829375744e-05

Training epoch-23 batch-3
Running loss of epoch-23 batch-3 = 0.00013731548096984625

Training epoch-23 batch-4
Running loss of epoch-23 batch-4 = 5.13795530423522e-05

Training epoch-23 batch-5
Running loss of epoch-23 batch-5 = 3.644824028015137e-05

Training epoch-23 batch-6
Running loss of epoch-23 batch-6 = 3.0322582460939884e-05

Training epoch-23 batch-7
Running loss of epoch-23 batch-7 = 2.279586624354124e-05

Training epoch-23 batch-8
Running loss of epoch-23 batch-8 = 4.6844128519296646e-05

Training epoch-23 batch-9
Running loss of epoch-23 batch-9 = 6.529310485348105e-05

Training epoch-23 batch-10
Running loss of epoch-23 batch-10 = 0.001073462888598442

Training epoch-23 batch-11
Running loss of epoch-23 batch-11 = 0.00048305653035640717

Training epoch-23 batch-12
Running loss of epoch-23 batch-12 = 2.9623741284012794e-05

Training epoch-23 batch-13
Running loss of epoch-23 batch-13 = 7.877533789724112e-05

Training epoch-23 batch-14
Running loss of epoch-23 batch-14 = 0.00011296645971015096

Training epoch-23 batch-15
Running loss of epoch-23 batch-15 = 6.332527846097946e-05

Training epoch-23 batch-16
Running loss of epoch-23 batch-16 = 4.552537575364113e-05

Training epoch-23 batch-17
Running loss of epoch-23 batch-17 = 6.907049100846052e-05

Training epoch-23 batch-18
Running loss of epoch-23 batch-18 = 7.937254849821329e-05

Training epoch-23 batch-19
Running loss of epoch-23 batch-19 = 0.00013739150017499924

Training epoch-23 batch-20
Running loss of epoch-23 batch-20 = 8.729379624128342e-05

Training epoch-23 batch-21
Running loss of epoch-23 batch-21 = 0.000407592102419585

Training epoch-23 batch-22
Running loss of epoch-23 batch-22 = 0.0010011615231633186

Training epoch-23 batch-23
Running loss of epoch-23 batch-23 = 0.00013901927741244435

Training epoch-23 batch-24
Running loss of epoch-23 batch-24 = 0.00014235463459044695

Training epoch-23 batch-25
Running loss of epoch-23 batch-25 = 0.0001148429000750184

Training epoch-23 batch-26
Running loss of epoch-23 batch-26 = 0.0002052229829132557

Training epoch-23 batch-27
Running loss of epoch-23 batch-27 = 9.126123040914536e-05

Training epoch-23 batch-28
Running loss of epoch-23 batch-28 = 3.212760202586651e-05

Training epoch-23 batch-29
Running loss of epoch-23 batch-29 = 5.296710878610611e-05

Training epoch-23 batch-30
Running loss of epoch-23 batch-30 = 0.00019403547048568726

Training epoch-23 batch-31
Running loss of epoch-23 batch-31 = 6.76838681101799e-05

Training epoch-23 batch-32
Running loss of epoch-23 batch-32 = 0.00024267775006592274

Training epoch-23 batch-33
Running loss of epoch-23 batch-33 = 0.0001933977473527193

Training epoch-23 batch-34
Running loss of epoch-23 batch-34 = 0.0004353281110525131

Training epoch-23 batch-35
Running loss of epoch-23 batch-35 = 0.00023163697915151715

Training epoch-23 batch-36
Running loss of epoch-23 batch-36 = 0.00023895525373518467

Training epoch-23 batch-37
Running loss of epoch-23 batch-37 = 8.740538032725453e-05

Training epoch-23 batch-38
Running loss of epoch-23 batch-38 = 0.00018392904894426465

Training epoch-23 batch-39
Running loss of epoch-23 batch-39 = 3.4875934943556786e-05

Training epoch-23 batch-40
Running loss of epoch-23 batch-40 = 4.584505222737789e-05

Training epoch-23 batch-41
Running loss of epoch-23 batch-41 = 0.0003791937488131225

Training epoch-23 batch-42
Running loss of epoch-23 batch-42 = 8.323183283209801e-05

Training epoch-23 batch-43
Running loss of epoch-23 batch-43 = 5.4430216550827026e-05

Training epoch-23 batch-44
Running loss of epoch-23 batch-44 = 0.00028032594127580523

Training epoch-23 batch-45
Running loss of epoch-23 batch-45 = 7.825344800949097e-05

Training epoch-23 batch-46
Running loss of epoch-23 batch-46 = 6.78040087223053e-05

Training epoch-23 batch-47
Running loss of epoch-23 batch-47 = 0.0009512246469967067

Training epoch-23 batch-48
Running loss of epoch-23 batch-48 = 6.951647810637951e-05

Training epoch-23 batch-49
Running loss of epoch-23 batch-49 = 6.952823605388403e-05

Training epoch-23 batch-50
Running loss of epoch-23 batch-50 = 0.00012716109631583095

Training epoch-23 batch-51
Running loss of epoch-23 batch-51 = 0.00014300108887255192

Training epoch-23 batch-52
Running loss of epoch-23 batch-52 = 0.00032104598358273506

Training epoch-23 batch-53
Running loss of epoch-23 batch-53 = 9.498931467533112e-05

Training epoch-23 batch-54
Running loss of epoch-23 batch-54 = 0.00015461421571671963

Training epoch-23 batch-55
Running loss of epoch-23 batch-55 = 7.455458398908377e-05

Training epoch-23 batch-56
Running loss of epoch-23 batch-56 = 6.28236448392272e-05

Training epoch-23 batch-57
Running loss of epoch-23 batch-57 = 0.00024222664069384336

Training epoch-23 batch-58
Running loss of epoch-23 batch-58 = 0.00023444299586117268

Training epoch-23 batch-59
Running loss of epoch-23 batch-59 = 7.742189336568117e-05

Training epoch-23 batch-60
Running loss of epoch-23 batch-60 = 0.00010835309512913227

Training epoch-23 batch-61
Running loss of epoch-23 batch-61 = 6.007344927638769e-05

Training epoch-23 batch-62
Running loss of epoch-23 batch-62 = 0.00010159891098737717

Training epoch-23 batch-63
Running loss of epoch-23 batch-63 = 0.00011300831101834774

Training epoch-23 batch-64
Running loss of epoch-23 batch-64 = 5.606096237897873e-05

Training epoch-23 batch-65
Running loss of epoch-23 batch-65 = 0.00024385959841310978

Training epoch-23 batch-66
Running loss of epoch-23 batch-66 = 0.0007095072069205344

Training epoch-23 batch-67
Running loss of epoch-23 batch-67 = 2.374919131398201e-05

Training epoch-23 batch-68
Running loss of epoch-23 batch-68 = 0.00012073712423443794

Training epoch-23 batch-69
Running loss of epoch-23 batch-69 = 0.0001263811718672514

Training epoch-23 batch-70
Running loss of epoch-23 batch-70 = 5.479808896780014e-05

Training epoch-23 batch-71
Running loss of epoch-23 batch-71 = 7.048493716865778e-05

Training epoch-23 batch-72
Running loss of epoch-23 batch-72 = 0.00013446842785924673

Training epoch-23 batch-73
Running loss of epoch-23 batch-73 = 0.00019011518452316523

Training epoch-23 batch-74
Running loss of epoch-23 batch-74 = 0.000226351257879287

Training epoch-23 batch-75
Running loss of epoch-23 batch-75 = 8.003297261893749e-05

Training epoch-23 batch-76
Running loss of epoch-23 batch-76 = 9.738618973642588e-05

Training epoch-23 batch-77
Running loss of epoch-23 batch-77 = 5.147571209818125e-05

Training epoch-23 batch-78
Running loss of epoch-23 batch-78 = 4.6360655687749386e-05

Training epoch-23 batch-79
Running loss of epoch-23 batch-79 = 8.03273287601769e-05

Training epoch-23 batch-80
Running loss of epoch-23 batch-80 = 5.070958286523819e-05

Training epoch-23 batch-81
Running loss of epoch-23 batch-81 = 0.0001717849518172443

Training epoch-23 batch-82
Running loss of epoch-23 batch-82 = 0.00013809523079544306

Training epoch-23 batch-83
Running loss of epoch-23 batch-83 = 0.00019770779181271791

Training epoch-23 batch-84
Running loss of epoch-23 batch-84 = 0.0003011982189491391

Training epoch-23 batch-85
Running loss of epoch-23 batch-85 = 7.033639121800661e-05

Training epoch-23 batch-86
Running loss of epoch-23 batch-86 = 4.4623506255447865e-05

Training epoch-23 batch-87
Running loss of epoch-23 batch-87 = 6.951333489269018e-05

Training epoch-23 batch-88
Running loss of epoch-23 batch-88 = 0.00012252992019057274

Training epoch-23 batch-89
Running loss of epoch-23 batch-89 = 6.489153020083904e-05

Training epoch-23 batch-90
Running loss of epoch-23 batch-90 = 6.589072290807962e-05

Training epoch-23 batch-91
Running loss of epoch-23 batch-91 = 4.583795089274645e-05

Training epoch-23 batch-92
Running loss of epoch-23 batch-92 = 7.570930756628513e-05

Training epoch-23 batch-93
Running loss of epoch-23 batch-93 = 4.9114576540887356e-05

Training epoch-23 batch-94
Running loss of epoch-23 batch-94 = 1.9255676306784153e-05

Training epoch-23 batch-95
Running loss of epoch-23 batch-95 = 2.93864868581295e-05

Training epoch-23 batch-96
Running loss of epoch-23 batch-96 = 0.00011418689973652363

Training epoch-23 batch-97
Running loss of epoch-23 batch-97 = 8.240691386163235e-05

Training epoch-23 batch-98
Running loss of epoch-23 batch-98 = 8.79800645634532e-05

Training epoch-23 batch-99
Running loss of epoch-23 batch-99 = 0.00017884263070300221

Training epoch-23 batch-100
Running loss of epoch-23 batch-100 = 9.320024400949478e-05

Training epoch-23 batch-101
Running loss of epoch-23 batch-101 = 2.1070591174066067e-05

Training epoch-23 batch-102
Running loss of epoch-23 batch-102 = 0.00021718398784287274

Training epoch-23 batch-103
Running loss of epoch-23 batch-103 = 0.00022225943394005299

Training epoch-23 batch-104
Running loss of epoch-23 batch-104 = 0.00014257995644584298

Training epoch-23 batch-105
Running loss of epoch-23 batch-105 = 1.9715516828000546e-05

Training epoch-23 batch-106
Running loss of epoch-23 batch-106 = 4.230160266160965e-05

Training epoch-23 batch-107
Running loss of epoch-23 batch-107 = 6.199115887284279e-05

Training epoch-23 batch-108
Running loss of epoch-23 batch-108 = 5.9543526731431484e-05

Training epoch-23 batch-109
Running loss of epoch-23 batch-109 = 0.00026726885698735714

Training epoch-23 batch-110
Running loss of epoch-23 batch-110 = 2.6076100766658783e-05

Training epoch-23 batch-111
Running loss of epoch-23 batch-111 = 2.0944862626492977e-05

Training epoch-23 batch-112
Running loss of epoch-23 batch-112 = 0.0008445180719718337

Training epoch-23 batch-113
Running loss of epoch-23 batch-113 = 2.010539174079895e-05

Training epoch-23 batch-114
Running loss of epoch-23 batch-114 = 2.873747143894434e-05

Training epoch-23 batch-115
Running loss of epoch-23 batch-115 = 0.00037839612923562527

Training epoch-23 batch-116
Running loss of epoch-23 batch-116 = 6.102060433477163e-05

Training epoch-23 batch-117
Running loss of epoch-23 batch-117 = 0.00011021958198398352

Training epoch-23 batch-118
Running loss of epoch-23 batch-118 = 3.0098832212388515e-05

Training epoch-23 batch-119
Running loss of epoch-23 batch-119 = 0.00010267109610140324

Training epoch-23 batch-120
Running loss of epoch-23 batch-120 = 0.0006586392992176116

Training epoch-23 batch-121
Running loss of epoch-23 batch-121 = 0.00013513962039723992

Training epoch-23 batch-122
Running loss of epoch-23 batch-122 = 0.000105730548966676

Training epoch-23 batch-123
Running loss of epoch-23 batch-123 = 6.543495692312717e-05

Training epoch-23 batch-124
Running loss of epoch-23 batch-124 = 3.301864489912987e-05

Training epoch-23 batch-125
Running loss of epoch-23 batch-125 = 2.8493348509073257e-05

Training epoch-23 batch-126
Running loss of epoch-23 batch-126 = 8.642533794045448e-05

Training epoch-23 batch-127
Running loss of epoch-23 batch-127 = 5.473743658512831e-05

Training epoch-23 batch-128
Running loss of epoch-23 batch-128 = 2.457143273204565e-05

Training epoch-23 batch-129
Running loss of epoch-23 batch-129 = 9.036081610247493e-05

Training epoch-23 batch-130
Running loss of epoch-23 batch-130 = 0.00023552932543680072

Training epoch-23 batch-131
Running loss of epoch-23 batch-131 = 9.262841194868088e-05

Training epoch-23 batch-132
Running loss of epoch-23 batch-132 = 0.00013160798698663712

Training epoch-23 batch-133
Running loss of epoch-23 batch-133 = 2.785713877528906e-05

Training epoch-23 batch-134
Running loss of epoch-23 batch-134 = 3.7363963201642036e-05

Training epoch-23 batch-135
Running loss of epoch-23 batch-135 = 4.644721047952771e-05

Training epoch-23 batch-136
Running loss of epoch-23 batch-136 = 2.4593202397227287e-05

Training epoch-23 batch-137
Running loss of epoch-23 batch-137 = 3.0968221835792065e-05

Training epoch-23 batch-138
Running loss of epoch-23 batch-138 = 5.481182597577572e-05

Training epoch-23 batch-139
Running loss of epoch-23 batch-139 = 0.0001130215241573751

Training epoch-23 batch-140
Running loss of epoch-23 batch-140 = 0.0005000201053917408

Training epoch-23 batch-141
Running loss of epoch-23 batch-141 = 0.00013511115685105324

Training epoch-23 batch-142
Running loss of epoch-23 batch-142 = 3.798597026616335e-05

Training epoch-23 batch-143
Running loss of epoch-23 batch-143 = 8.030189201235771e-05

Training epoch-23 batch-144
Running loss of epoch-23 batch-144 = 4.2470404878258705e-05

Training epoch-23 batch-145
Running loss of epoch-23 batch-145 = 1.5505822375416756e-05

Training epoch-23 batch-146
Running loss of epoch-23 batch-146 = 0.0001980042434297502

Training epoch-23 batch-147
Running loss of epoch-23 batch-147 = 3.635464236140251e-05

Training epoch-23 batch-148
Running loss of epoch-23 batch-148 = 1.828128006309271e-05

Training epoch-23 batch-149
Running loss of epoch-23 batch-149 = 1.9542290829122066e-05

Training epoch-23 batch-150
Running loss of epoch-23 batch-150 = 0.0001222614082507789

Training epoch-23 batch-151
Running loss of epoch-23 batch-151 = 0.0001840447075664997

Training epoch-23 batch-152
Running loss of epoch-23 batch-152 = 5.978287663310766e-05

Training epoch-23 batch-153
Running loss of epoch-23 batch-153 = 0.0002127940533682704

Training epoch-23 batch-154
Running loss of epoch-23 batch-154 = 2.8311973437666893e-05

Training epoch-23 batch-155
Running loss of epoch-23 batch-155 = 1.180870458483696e-05

Training epoch-23 batch-156
Running loss of epoch-23 batch-156 = 3.720633685588837e-05

Training epoch-23 batch-157
Running loss of epoch-23 batch-157 = 0.00024481862783432007

Finished training epoch-23.



Average train loss at epoch-23 = 0.00013927643243223428

Started Evaluation

Average val loss at epoch-23 = 4.453436459365644

Accuracy for classes:
Accuracy for class equals is: 63.20 %
Accuracy for class main is: 39.18 %
Accuracy for class setUp is: 50.98 %
Accuracy for class onCreate is: 34.86 %
Accuracy for class toString is: 35.84 %
Accuracy for class run is: 30.14 %
Accuracy for class hashCode is: 76.03 %
Accuracy for class init is: 23.77 %
Accuracy for class execute is: 18.47 %
Accuracy for class get is: 35.38 %

Overall Accuracy = 41.06 %

Finished Evaluation



Started training epoch-24


Training epoch-24 batch-1
Running loss of epoch-24 batch-1 = 6.454624235630035e-05

Training epoch-24 batch-2
Running loss of epoch-24 batch-2 = 1.6291276551783085e-05

Training epoch-24 batch-3
Running loss of epoch-24 batch-3 = 3.629853017628193e-05

Training epoch-24 batch-4
Running loss of epoch-24 batch-4 = 1.4242716133594513e-05

Training epoch-24 batch-5
Running loss of epoch-24 batch-5 = 8.754030568525195e-05

Training epoch-24 batch-6
Running loss of epoch-24 batch-6 = 0.00011381536023691297

Training epoch-24 batch-7
Running loss of epoch-24 batch-7 = 3.7975492887198925e-05

Training epoch-24 batch-8
Running loss of epoch-24 batch-8 = 3.428093623369932e-05

Training epoch-24 batch-9
Running loss of epoch-24 batch-9 = 3.45505541190505e-05

Training epoch-24 batch-10
Running loss of epoch-24 batch-10 = 3.9271311834454536e-05

Training epoch-24 batch-11
Running loss of epoch-24 batch-11 = 6.787839811295271e-05

Training epoch-24 batch-12
Running loss of epoch-24 batch-12 = 2.8583803214132786e-05

Training epoch-24 batch-13
Running loss of epoch-24 batch-13 = 4.052440635859966e-05

Training epoch-24 batch-14
Running loss of epoch-24 batch-14 = 1.1228839866816998e-05

Training epoch-24 batch-15
Running loss of epoch-24 batch-15 = 9.965943172574043e-05

Training epoch-24 batch-16
Running loss of epoch-24 batch-16 = 1.5384983271360397e-05

Training epoch-24 batch-17
Running loss of epoch-24 batch-17 = 4.2232335545122623e-05

Training epoch-24 batch-18
Running loss of epoch-24 batch-18 = 2.3869331926107407e-05

Training epoch-24 batch-19
Running loss of epoch-24 batch-19 = 3.748235758394003e-05

Training epoch-24 batch-20
Running loss of epoch-24 batch-20 = 4.6157161705195904e-05

Training epoch-24 batch-21
Running loss of epoch-24 batch-21 = 3.8902624510228634e-05

Training epoch-24 batch-22
Running loss of epoch-24 batch-22 = 7.950759027153254e-05

Training epoch-24 batch-23
Running loss of epoch-24 batch-23 = 2.792815212160349e-05

Training epoch-24 batch-24
Running loss of epoch-24 batch-24 = 5.8988749515265226e-05

Training epoch-24 batch-25
Running loss of epoch-24 batch-25 = 3.9827427826821804e-05

Training epoch-24 batch-26
Running loss of epoch-24 batch-26 = 4.642200656235218e-05

Training epoch-24 batch-27
Running loss of epoch-24 batch-27 = 6.562197813764215e-05

Training epoch-24 batch-28
Running loss of epoch-24 batch-28 = 2.2377236746251583e-05

Training epoch-24 batch-29
Running loss of epoch-24 batch-29 = 5.4493662901222706e-05

Training epoch-24 batch-30
Running loss of epoch-24 batch-30 = 0.0002714098955038935

Training epoch-24 batch-31
Running loss of epoch-24 batch-31 = 1.6978709027171135e-05

Training epoch-24 batch-32
Running loss of epoch-24 batch-32 = 6.00708881393075e-05

Training epoch-24 batch-33
Running loss of epoch-24 batch-33 = 4.6356930397450924e-05

Training epoch-24 batch-34
Running loss of epoch-24 batch-34 = 2.875074278563261e-05

Training epoch-24 batch-35
Running loss of epoch-24 batch-35 = 2.6068533770740032e-05

Training epoch-24 batch-36
Running loss of epoch-24 batch-36 = 2.2845924831926823e-05

Training epoch-24 batch-37
Running loss of epoch-24 batch-37 = 3.6812503822147846e-05

Training epoch-24 batch-38
Running loss of epoch-24 batch-38 = 9.966897778213024e-06

Training epoch-24 batch-39
Running loss of epoch-24 batch-39 = 4.1675870306789875e-05

Training epoch-24 batch-40
Running loss of epoch-24 batch-40 = 1.4815246686339378e-05

Training epoch-24 batch-41
Running loss of epoch-24 batch-41 = 1.021276693791151e-05

Training epoch-24 batch-42
Running loss of epoch-24 batch-42 = 2.7127796784043312e-05

Training epoch-24 batch-43
Running loss of epoch-24 batch-43 = 1.9505037926137447e-05

Training epoch-24 batch-44
Running loss of epoch-24 batch-44 = 4.570581950247288e-05

Training epoch-24 batch-45
Running loss of epoch-24 batch-45 = 4.071719013154507e-05

Training epoch-24 batch-46
Running loss of epoch-24 batch-46 = 2.019840758293867e-05

Training epoch-24 batch-47
Running loss of epoch-24 batch-47 = 2.6217196136713028e-05

Training epoch-24 batch-48
Running loss of epoch-24 batch-48 = 2.541951835155487e-05

Training epoch-24 batch-49
Running loss of epoch-24 batch-49 = 7.651146734133363e-05

Training epoch-24 batch-50
Running loss of epoch-24 batch-50 = 6.254378240555525e-05

Training epoch-24 batch-51
Running loss of epoch-24 batch-51 = 0.00046705760178156197

Training epoch-24 batch-52
Running loss of epoch-24 batch-52 = 3.696326166391373e-05

Training epoch-24 batch-53
Running loss of epoch-24 batch-53 = 8.65123001858592e-05

Training epoch-24 batch-54
Running loss of epoch-24 batch-54 = 4.8025744035840034e-05

Training epoch-24 batch-55
Running loss of epoch-24 batch-55 = 1.3870070688426495e-05

Training epoch-24 batch-56
Running loss of epoch-24 batch-56 = 0.00016327807679772377

Training epoch-24 batch-57
Running loss of epoch-24 batch-57 = 4.046456888318062e-05

Training epoch-24 batch-58
Running loss of epoch-24 batch-58 = 4.0664104744791985e-05

Training epoch-24 batch-59
Running loss of epoch-24 batch-59 = 2.989510539919138e-05

Training epoch-24 batch-60
Running loss of epoch-24 batch-60 = 2.734886948019266e-05

Training epoch-24 batch-61
Running loss of epoch-24 batch-61 = 1.2873904779553413e-05

Training epoch-24 batch-62
Running loss of epoch-24 batch-62 = 2.996041439473629e-05

Training epoch-24 batch-63
Running loss of epoch-24 batch-63 = 0.0002322422224096954

Training epoch-24 batch-64
Running loss of epoch-24 batch-64 = 2.8705107979476452e-05

Training epoch-24 batch-65
Running loss of epoch-24 batch-65 = 1.0228017345070839e-05

Training epoch-24 batch-66
Running loss of epoch-24 batch-66 = 4.9445778131484985e-05

Training epoch-24 batch-67
Running loss of epoch-24 batch-67 = 7.495982572436333e-06

Training epoch-24 batch-68
Running loss of epoch-24 batch-68 = 0.00012485991464927793

Training epoch-24 batch-69
Running loss of epoch-24 batch-69 = 2.5350484065711498e-05

Training epoch-24 batch-70
Running loss of epoch-24 batch-70 = 1.2266216799616814e-05

Training epoch-24 batch-71
Running loss of epoch-24 batch-71 = 2.1088519133627415e-05

Training epoch-24 batch-72
Running loss of epoch-24 batch-72 = 1.6867066733539104e-05

Training epoch-24 batch-73
Running loss of epoch-24 batch-73 = 0.00012575002619996667

Training epoch-24 batch-74
Running loss of epoch-24 batch-74 = 5.5384531151503325e-05

Training epoch-24 batch-75
Running loss of epoch-24 batch-75 = 1.0710908100008965e-05

Training epoch-24 batch-76
Running loss of epoch-24 batch-76 = 2.2411812096834183e-05

Training epoch-24 batch-77
Running loss of epoch-24 batch-77 = 2.2349762730300426e-05

Training epoch-24 batch-78
Running loss of epoch-24 batch-78 = 0.00010346324415877461

Training epoch-24 batch-79
Running loss of epoch-24 batch-79 = 6.707629654556513e-05

Training epoch-24 batch-80
Running loss of epoch-24 batch-80 = 1.621432602405548e-05

Training epoch-24 batch-81
Running loss of epoch-24 batch-81 = 0.00010446814121678472

Training epoch-24 batch-82
Running loss of epoch-24 batch-82 = 4.33711102232337e-05

Training epoch-24 batch-83
Running loss of epoch-24 batch-83 = 8.556258399039507e-05

Training epoch-24 batch-84
Running loss of epoch-24 batch-84 = 4.605844151228666e-05

Training epoch-24 batch-85
Running loss of epoch-24 batch-85 = 5.569233326241374e-05

Training epoch-24 batch-86
Running loss of epoch-24 batch-86 = 1.850258558988571e-05

Training epoch-24 batch-87
Running loss of epoch-24 batch-87 = 3.0931783840060234e-05

Training epoch-24 batch-88
Running loss of epoch-24 batch-88 = 4.54409746453166e-05

Training epoch-24 batch-89
Running loss of epoch-24 batch-89 = 1.9516446627676487e-05

Training epoch-24 batch-90
Running loss of epoch-24 batch-90 = 0.00019716794486157596

Training epoch-24 batch-91
Running loss of epoch-24 batch-91 = 2.5578774511814117e-05

Training epoch-24 batch-92
Running loss of epoch-24 batch-92 = 2.714607398957014e-05

Training epoch-24 batch-93
Running loss of epoch-24 batch-93 = 3.472156822681427e-05

Training epoch-24 batch-94
Running loss of epoch-24 batch-94 = 2.436409704387188e-05

Training epoch-24 batch-95
Running loss of epoch-24 batch-95 = 1.2618489563465118e-05

Training epoch-24 batch-96
Running loss of epoch-24 batch-96 = 2.7319067157804966e-05

Training epoch-24 batch-97
Running loss of epoch-24 batch-97 = 0.00018154841382056475

Training epoch-24 batch-98
Running loss of epoch-24 batch-98 = 9.926967322826385e-06

Training epoch-24 batch-99
Running loss of epoch-24 batch-99 = 4.685809835791588e-05

Training epoch-24 batch-100
Running loss of epoch-24 batch-100 = 5.130062345415354e-05

Training epoch-24 batch-101
Running loss of epoch-24 batch-101 = 0.00011092331260442734

Training epoch-24 batch-102
Running loss of epoch-24 batch-102 = 1.3438519090414047e-05

Training epoch-24 batch-103
Running loss of epoch-24 batch-103 = 4.1973777115345e-05

Training epoch-24 batch-104
Running loss of epoch-24 batch-104 = 3.070395905524492e-05

Training epoch-24 batch-105
Running loss of epoch-24 batch-105 = 4.185654688626528e-05

Training epoch-24 batch-106
Running loss of epoch-24 batch-106 = 2.691859845072031e-05

Training epoch-24 batch-107
Running loss of epoch-24 batch-107 = 1.94028252735734e-05

Training epoch-24 batch-108
Running loss of epoch-24 batch-108 = 9.809969924390316e-06

Training epoch-24 batch-109
Running loss of epoch-24 batch-109 = 9.998626774176955e-05

Training epoch-24 batch-110
Running loss of epoch-24 batch-110 = 6.289489101618528e-05

Training epoch-24 batch-111
Running loss of epoch-24 batch-111 = 1.9063125364482403e-05

Training epoch-24 batch-112
Running loss of epoch-24 batch-112 = 1.0082381777465343e-05

Training epoch-24 batch-113
Running loss of epoch-24 batch-113 = 1.0132323950529099e-05

Training epoch-24 batch-114
Running loss of epoch-24 batch-114 = 2.739217597991228e-05

Training epoch-24 batch-115
Running loss of epoch-24 batch-115 = 1.9200611859560013e-05

Training epoch-24 batch-116
Running loss of epoch-24 batch-116 = 0.00011831120355054736

Training epoch-24 batch-117
Running loss of epoch-24 batch-117 = 1.6191508620977402e-05

Training epoch-24 batch-118
Running loss of epoch-24 batch-118 = 5.293358117341995e-05

Training epoch-24 batch-119
Running loss of epoch-24 batch-119 = 1.4692661352455616e-05

Training epoch-24 batch-120
Running loss of epoch-24 batch-120 = 3.719830419868231e-05

Training epoch-24 batch-121
Running loss of epoch-24 batch-121 = 3.963569179177284e-05

Training epoch-24 batch-122
Running loss of epoch-24 batch-122 = 3.758270759135485e-05

Training epoch-24 batch-123
Running loss of epoch-24 batch-123 = 1.531199086457491e-05

Training epoch-24 batch-124
Running loss of epoch-24 batch-124 = 2.136861439794302e-05

Training epoch-24 batch-125
Running loss of epoch-24 batch-125 = 4.369101952761412e-05

Training epoch-24 batch-126
Running loss of epoch-24 batch-126 = 2.5021028704941273e-05

Training epoch-24 batch-127
Running loss of epoch-24 batch-127 = 7.101916708052158e-06

Training epoch-24 batch-128
Running loss of epoch-24 batch-128 = 4.157237708568573e-05

Training epoch-24 batch-129
Running loss of epoch-24 batch-129 = 2.7521164156496525e-05

Training epoch-24 batch-130
Running loss of epoch-24 batch-130 = 5.313532892614603e-05

Training epoch-24 batch-131
Running loss of epoch-24 batch-131 = 1.2696371413767338e-05

Training epoch-24 batch-132
Running loss of epoch-24 batch-132 = 4.373927367851138e-05

Training epoch-24 batch-133
Running loss of epoch-24 batch-133 = 2.6993686333298683e-05

Training epoch-24 batch-134
Running loss of epoch-24 batch-134 = 1.8527964130043983e-05

Training epoch-24 batch-135
Running loss of epoch-24 batch-135 = 2.0640436559915543e-05

Training epoch-24 batch-136
Running loss of epoch-24 batch-136 = 1.679884735494852e-05

Training epoch-24 batch-137
Running loss of epoch-24 batch-137 = 7.207202725112438e-05

Training epoch-24 batch-138
Running loss of epoch-24 batch-138 = 1.3825483620166779e-05

Training epoch-24 batch-139
Running loss of epoch-24 batch-139 = 2.8845621272921562e-05

Training epoch-24 batch-140
Running loss of epoch-24 batch-140 = 0.0001757919671945274

Training epoch-24 batch-141
Running loss of epoch-24 batch-141 = 0.00024623534409329295

Training epoch-24 batch-142
Running loss of epoch-24 batch-142 = 3.622355870902538e-05

Training epoch-24 batch-143
Running loss of epoch-24 batch-143 = 9.817304089665413e-06

Training epoch-24 batch-144
Running loss of epoch-24 batch-144 = 3.621680662035942e-05

Training epoch-24 batch-145
Running loss of epoch-24 batch-145 = 2.8219539672136307e-05

Training epoch-24 batch-146
Running loss of epoch-24 batch-146 = 0.0002569548669271171

Training epoch-24 batch-147
Running loss of epoch-24 batch-147 = 2.4329405277967453e-05

Training epoch-24 batch-148
Running loss of epoch-24 batch-148 = 5.2818621043115854e-05

Training epoch-24 batch-149
Running loss of epoch-24 batch-149 = 6.025005131959915e-05

Training epoch-24 batch-150
Running loss of epoch-24 batch-150 = 0.00010491785360500216

Training epoch-24 batch-151
Running loss of epoch-24 batch-151 = 3.172876313328743e-05

Training epoch-24 batch-152
Running loss of epoch-24 batch-152 = 4.709197673946619e-05

Training epoch-24 batch-153
Running loss of epoch-24 batch-153 = 1.5039346180856228e-05

Training epoch-24 batch-154
Running loss of epoch-24 batch-154 = 4.961166996508837e-05

Training epoch-24 batch-155
Running loss of epoch-24 batch-155 = 1.738907303661108e-05

Training epoch-24 batch-156
Running loss of epoch-24 batch-156 = 5.3912983275949955e-05

Training epoch-24 batch-157
Running loss of epoch-24 batch-157 = 1.1745840311050415e-05

Finished training epoch-24.



Average train loss at epoch-24 = 5.0504875369369984e-05

Started Evaluation

Average val loss at epoch-24 = 4.389958295382951

Accuracy for classes:
Accuracy for class equals is: 60.23 %
Accuracy for class main is: 55.90 %
Accuracy for class setUp is: 53.61 %
Accuracy for class onCreate is: 37.63 %
Accuracy for class toString is: 40.27 %
Accuracy for class run is: 27.17 %
Accuracy for class hashCode is: 74.53 %
Accuracy for class init is: 29.15 %
Accuracy for class execute is: 20.48 %
Accuracy for class get is: 18.21 %

Overall Accuracy = 42.79 %

Finished Evaluation



Started training epoch-25


Training epoch-25 batch-1
Running loss of epoch-25 batch-1 = 7.440336048603058e-05

Training epoch-25 batch-2
Running loss of epoch-25 batch-2 = 1.9824947230517864e-05

Training epoch-25 batch-3
Running loss of epoch-25 batch-3 = 2.9361224733293056e-05

Training epoch-25 batch-4
Running loss of epoch-25 batch-4 = 2.8155045583844185e-05

Training epoch-25 batch-5
Running loss of epoch-25 batch-5 = 1.6739708371460438e-05

Training epoch-25 batch-6
Running loss of epoch-25 batch-6 = 1.4823977835476398e-05

Training epoch-25 batch-7
Running loss of epoch-25 batch-7 = 1.3018609024584293e-05

Training epoch-25 batch-8
Running loss of epoch-25 batch-8 = 2.809707075357437e-05

Training epoch-25 batch-9
Running loss of epoch-25 batch-9 = 4.342757165431976e-05

Training epoch-25 batch-10
Running loss of epoch-25 batch-10 = 2.1052081137895584e-05

Training epoch-25 batch-11
Running loss of epoch-25 batch-11 = 2.7093919925391674e-05

Training epoch-25 batch-12
Running loss of epoch-25 batch-12 = 7.931591244414449e-05

Training epoch-25 batch-13
Running loss of epoch-25 batch-13 = 1.5148194506764412e-05

Training epoch-25 batch-14
Running loss of epoch-25 batch-14 = 7.295515388250351e-06

Training epoch-25 batch-15
Running loss of epoch-25 batch-15 = 2.472416963428259e-05

Training epoch-25 batch-16
Running loss of epoch-25 batch-16 = 0.00011356920003890991

Training epoch-25 batch-17
Running loss of epoch-25 batch-17 = 2.6698922738432884e-05

Training epoch-25 batch-18
Running loss of epoch-25 batch-18 = 1.2105796486139297e-05

Training epoch-25 batch-19
Running loss of epoch-25 batch-19 = 6.688176654279232e-06

Training epoch-25 batch-20
Running loss of epoch-25 batch-20 = 3.337219823151827e-05

Training epoch-25 batch-21
Running loss of epoch-25 batch-21 = 1.9518542103469372e-05

Training epoch-25 batch-22
Running loss of epoch-25 batch-22 = 5.651323590427637e-05

Training epoch-25 batch-23
Running loss of epoch-25 batch-23 = 1.5894765965640545e-05

Training epoch-25 batch-24
Running loss of epoch-25 batch-24 = 3.921182360500097e-05

Training epoch-25 batch-25
Running loss of epoch-25 batch-25 = 7.164548151195049e-06

Training epoch-25 batch-26
Running loss of epoch-25 batch-26 = 3.6809709854424e-05

Training epoch-25 batch-27
Running loss of epoch-25 batch-27 = 2.2118212655186653e-05

Training epoch-25 batch-28
Running loss of epoch-25 batch-28 = 6.805703742429614e-05

Training epoch-25 batch-29
Running loss of epoch-25 batch-29 = 1.2850039638578892e-05

Training epoch-25 batch-30
Running loss of epoch-25 batch-30 = 1.1804630048573017e-05

Training epoch-25 batch-31
Running loss of epoch-25 batch-31 = 1.6490695998072624e-05

Training epoch-25 batch-32
Running loss of epoch-25 batch-32 = 0.00011647870996966958

Training epoch-25 batch-33
Running loss of epoch-25 batch-33 = 7.2462717071175575e-06

Training epoch-25 batch-34
Running loss of epoch-25 batch-34 = 2.6681809686124325e-05

Training epoch-25 batch-35
Running loss of epoch-25 batch-35 = 3.878946881741285e-05

Training epoch-25 batch-36
Running loss of epoch-25 batch-36 = 4.813866689801216e-05

Training epoch-25 batch-37
Running loss of epoch-25 batch-37 = 5.564151797443628e-05

Training epoch-25 batch-38
Running loss of epoch-25 batch-38 = 1.19735486805439e-05

Training epoch-25 batch-39
Running loss of epoch-25 batch-39 = 1.6634119674563408e-05

Training epoch-25 batch-40
Running loss of epoch-25 batch-40 = 1.1462019756436348e-05

Training epoch-25 batch-41
Running loss of epoch-25 batch-41 = 1.3830256648361683e-05

Training epoch-25 batch-42
Running loss of epoch-25 batch-42 = 4.517089109867811e-05

Training epoch-25 batch-43
Running loss of epoch-25 batch-43 = 1.4061573892831802e-05

Training epoch-25 batch-44
Running loss of epoch-25 batch-44 = 1.8046004697680473e-05

Training epoch-25 batch-45
Running loss of epoch-25 batch-45 = 2.233590930700302e-05

Training epoch-25 batch-46
Running loss of epoch-25 batch-46 = 5.7884957641363144e-05

Training epoch-25 batch-47
Running loss of epoch-25 batch-47 = 2.699834294617176e-05

Training epoch-25 batch-48
Running loss of epoch-25 batch-48 = 0.00033119405270554125

Training epoch-25 batch-49
Running loss of epoch-25 batch-49 = 3.562390338629484e-05

Training epoch-25 batch-50
Running loss of epoch-25 batch-50 = 1.3182754628360271e-05

Training epoch-25 batch-51
Running loss of epoch-25 batch-51 = 1.3075419701635838e-05

Training epoch-25 batch-52
Running loss of epoch-25 batch-52 = 5.9532467275857925e-06

Training epoch-25 batch-53
Running loss of epoch-25 batch-53 = 3.53168579749763e-05

Training epoch-25 batch-54
Running loss of epoch-25 batch-54 = 1.5942612662911415e-05

Training epoch-25 batch-55
Running loss of epoch-25 batch-55 = 2.5038141757249832e-05

Training epoch-25 batch-56
Running loss of epoch-25 batch-56 = 3.780296538025141e-05

Training epoch-25 batch-57
Running loss of epoch-25 batch-57 = 1.1421623639762402e-05

Training epoch-25 batch-58
Running loss of epoch-25 batch-58 = 1.7850659787654877e-05

Training epoch-25 batch-59
Running loss of epoch-25 batch-59 = 1.927092671394348e-05

Training epoch-25 batch-60
Running loss of epoch-25 batch-60 = 3.0935974791646004e-05

Training epoch-25 batch-61
Running loss of epoch-25 batch-61 = 4.987930878996849e-06

Training epoch-25 batch-62
Running loss of epoch-25 batch-62 = 8.388189598917961e-06

Training epoch-25 batch-63
Running loss of epoch-25 batch-63 = 4.368135705590248e-05

Training epoch-25 batch-64
Running loss of epoch-25 batch-64 = 1.6726204194128513e-05

Training epoch-25 batch-65
Running loss of epoch-25 batch-65 = 3.566313534975052e-05

Training epoch-25 batch-66
Running loss of epoch-25 batch-66 = 4.028645344078541e-05

Training epoch-25 batch-67
Running loss of epoch-25 batch-67 = 3.93156660720706e-05

Training epoch-25 batch-68
Running loss of epoch-25 batch-68 = 1.0166550055146217e-05

Training epoch-25 batch-69
Running loss of epoch-25 batch-69 = 2.931058406829834e-05

Training epoch-25 batch-70
Running loss of epoch-25 batch-70 = 2.592278178781271e-05

Training epoch-25 batch-71
Running loss of epoch-25 batch-71 = 4.006491508334875e-05

Training epoch-25 batch-72
Running loss of epoch-25 batch-72 = 1.0698451660573483e-05

Training epoch-25 batch-73
Running loss of epoch-25 batch-73 = 1.611793413758278e-05

Training epoch-25 batch-74
Running loss of epoch-25 batch-74 = 2.0148232579231262e-05

Training epoch-25 batch-75
Running loss of epoch-25 batch-75 = 1.3200799003243446e-05

Training epoch-25 batch-76
Running loss of epoch-25 batch-76 = 2.1855696104466915e-05

Training epoch-25 batch-77
Running loss of epoch-25 batch-77 = 4.8003275878727436e-05

Training epoch-25 batch-78
Running loss of epoch-25 batch-78 = 1.3105804100632668e-05

Training epoch-25 batch-79
Running loss of epoch-25 batch-79 = 1.0659336112439632e-05

Training epoch-25 batch-80
Running loss of epoch-25 batch-80 = 8.858920773491263e-05

Training epoch-25 batch-81
Running loss of epoch-25 batch-81 = 1.265259925276041e-05

Training epoch-25 batch-82
Running loss of epoch-25 batch-82 = 5.9573445469141006e-05

Training epoch-25 batch-83
Running loss of epoch-25 batch-83 = 1.7060781829059124e-05

Training epoch-25 batch-84
Running loss of epoch-25 batch-84 = 2.950045745819807e-05

Training epoch-25 batch-85
Running loss of epoch-25 batch-85 = 2.370437141507864e-05

Training epoch-25 batch-86
Running loss of epoch-25 batch-86 = 8.078786777332425e-05

Training epoch-25 batch-87
Running loss of epoch-25 batch-87 = 2.3031258024275303e-05

Training epoch-25 batch-88
Running loss of epoch-25 batch-88 = 5.267513915896416e-05

Training epoch-25 batch-89
Running loss of epoch-25 batch-89 = 2.4374574422836304e-05

Training epoch-25 batch-90
Running loss of epoch-25 batch-90 = 1.828698441386223e-05

Training epoch-25 batch-91
Running loss of epoch-25 batch-91 = 3.007357008755207e-05

Training epoch-25 batch-92
Running loss of epoch-25 batch-92 = 5.382578819990158e-06

Training epoch-25 batch-93
Running loss of epoch-25 batch-93 = 2.0927051082253456e-05

Training epoch-25 batch-94
Running loss of epoch-25 batch-94 = 7.722293958067894e-06

Training epoch-25 batch-95
Running loss of epoch-25 batch-95 = 8.07599863037467e-05

Training epoch-25 batch-96
Running loss of epoch-25 batch-96 = 2.882594708353281e-05

Training epoch-25 batch-97
Running loss of epoch-25 batch-97 = 2.4028355255723e-05

Training epoch-25 batch-98
Running loss of epoch-25 batch-98 = 1.26685481518507e-05

Training epoch-25 batch-99
Running loss of epoch-25 batch-99 = 4.221161361783743e-05

Training epoch-25 batch-100
Running loss of epoch-25 batch-100 = 1.3712444342672825e-05

Training epoch-25 batch-101
Running loss of epoch-25 batch-101 = 2.7385889552533627e-05

Training epoch-25 batch-102
Running loss of epoch-25 batch-102 = 1.8488266505301e-05

Training epoch-25 batch-103
Running loss of epoch-25 batch-103 = 1.3767974451184273e-05

Training epoch-25 batch-104
Running loss of epoch-25 batch-104 = 1.4848890714347363e-05

Training epoch-25 batch-105
Running loss of epoch-25 batch-105 = 0.00010763481259346008

Training epoch-25 batch-106
Running loss of epoch-25 batch-106 = 5.203596083447337e-05

Training epoch-25 batch-107
Running loss of epoch-25 batch-107 = 1.8357764929533005e-05

Training epoch-25 batch-108
Running loss of epoch-25 batch-108 = 1.834460999816656e-05

Training epoch-25 batch-109
Running loss of epoch-25 batch-109 = 5.7074008509516716e-05

Training epoch-25 batch-110
Running loss of epoch-25 batch-110 = 0.0001297623384743929

Training epoch-25 batch-111
Running loss of epoch-25 batch-111 = 7.586553692817688e-06

Training epoch-25 batch-112
Running loss of epoch-25 batch-112 = 1.4665420167148113e-05

Training epoch-25 batch-113
Running loss of epoch-25 batch-113 = 2.393568865954876e-05

Training epoch-25 batch-114
Running loss of epoch-25 batch-114 = 1.974345650523901e-05

Training epoch-25 batch-115
Running loss of epoch-25 batch-115 = 1.420674379914999e-05

Training epoch-25 batch-116
Running loss of epoch-25 batch-116 = 3.078952431678772e-05

Training epoch-25 batch-117
Running loss of epoch-25 batch-117 = 9.520561434328556e-06

Training epoch-25 batch-118
Running loss of epoch-25 batch-118 = 1.846125815063715e-05

Training epoch-25 batch-119
Running loss of epoch-25 batch-119 = 1.90358841791749e-05

Training epoch-25 batch-120
Running loss of epoch-25 batch-120 = 0.0001341752358712256

Training epoch-25 batch-121
Running loss of epoch-25 batch-121 = 1.8711085431277752e-05

Training epoch-25 batch-122
Running loss of epoch-25 batch-122 = 3.9310427382588387e-05

Training epoch-25 batch-123
Running loss of epoch-25 batch-123 = 1.704005990177393e-05

Training epoch-25 batch-124
Running loss of epoch-25 batch-124 = 2.4238135665655136e-05

Training epoch-25 batch-125
Running loss of epoch-25 batch-125 = 1.9088154658675194e-05

Training epoch-25 batch-126
Running loss of epoch-25 batch-126 = 2.5038723833858967e-05

Training epoch-25 batch-127
Running loss of epoch-25 batch-127 = 3.556231968104839e-05

Training epoch-25 batch-128
Running loss of epoch-25 batch-128 = 1.9192462787032127e-05

Training epoch-25 batch-129
Running loss of epoch-25 batch-129 = 2.313358709216118e-05

Training epoch-25 batch-130
Running loss of epoch-25 batch-130 = 0.00010001560440286994

Training epoch-25 batch-131
Running loss of epoch-25 batch-131 = 2.3913104087114334e-05

Training epoch-25 batch-132
Running loss of epoch-25 batch-132 = 5.930941551923752e-05

Training epoch-25 batch-133
Running loss of epoch-25 batch-133 = 2.4894136004149914e-05

Training epoch-25 batch-134
Running loss of epoch-25 batch-134 = 3.0490919016301632e-05

Training epoch-25 batch-135
Running loss of epoch-25 batch-135 = 6.006821058690548e-05

Training epoch-25 batch-136
Running loss of epoch-25 batch-136 = 3.0329450964927673e-05

Training epoch-25 batch-137
Running loss of epoch-25 batch-137 = 3.48859466612339e-05

Training epoch-25 batch-138
Running loss of epoch-25 batch-138 = 4.683877341449261e-05

Training epoch-25 batch-139
Running loss of epoch-25 batch-139 = 7.203081622719765e-06

Training epoch-25 batch-140
Running loss of epoch-25 batch-140 = 3.924686461687088e-05

Training epoch-25 batch-141
Running loss of epoch-25 batch-141 = 1.7118873074650764e-05

Training epoch-25 batch-142
Running loss of epoch-25 batch-142 = 0.0001788203080650419

Training epoch-25 batch-143
Running loss of epoch-25 batch-143 = 1.527136191725731e-05

Training epoch-25 batch-144
Running loss of epoch-25 batch-144 = 5.341315409168601e-05

Training epoch-25 batch-145
Running loss of epoch-25 batch-145 = 3.0395924113690853e-05

Training epoch-25 batch-146
Running loss of epoch-25 batch-146 = 3.924081102013588e-05

Training epoch-25 batch-147
Running loss of epoch-25 batch-147 = 2.189574297517538e-05

Training epoch-25 batch-148
Running loss of epoch-25 batch-148 = 8.215312846004963e-06

Training epoch-25 batch-149
Running loss of epoch-25 batch-149 = 1.711328513920307e-05

Training epoch-25 batch-150
Running loss of epoch-25 batch-150 = 2.4534761905670166e-05

Training epoch-25 batch-151
Running loss of epoch-25 batch-151 = 2.4299370124936104e-05

Training epoch-25 batch-152
Running loss of epoch-25 batch-152 = 4.3887412175536156e-06

Training epoch-25 batch-153
Running loss of epoch-25 batch-153 = 1.4895456843078136e-05

Training epoch-25 batch-154
Running loss of epoch-25 batch-154 = 0.00013559922808781266

Training epoch-25 batch-155
Running loss of epoch-25 batch-155 = 0.00040165195241570473

Training epoch-25 batch-156
Running loss of epoch-25 batch-156 = 2.5287619791924953e-05

Training epoch-25 batch-157
Running loss of epoch-25 batch-157 = 0.0002697072923183441

Finished training epoch-25.



Average train loss at epoch-25 = 3.659574054181576e-05

Started Evaluation

Average val loss at epoch-25 = 4.487793591461684

Accuracy for classes:
Accuracy for class equals is: 61.88 %
Accuracy for class main is: 48.20 %
Accuracy for class setUp is: 53.28 %
Accuracy for class onCreate is: 36.78 %
Accuracy for class toString is: 39.93 %
Accuracy for class run is: 28.31 %
Accuracy for class hashCode is: 74.53 %
Accuracy for class init is: 26.68 %
Accuracy for class execute is: 20.48 %
Accuracy for class get is: 26.41 %

Overall Accuracy = 42.34 %

Finished Evaluation



Started training epoch-26


Training epoch-26 batch-1
Running loss of epoch-26 batch-1 = 1.997931394726038e-05

Training epoch-26 batch-2
Running loss of epoch-26 batch-2 = 1.4058663509786129e-05

Training epoch-26 batch-3
Running loss of epoch-26 batch-3 = 0.00017756118904799223

Training epoch-26 batch-4
Running loss of epoch-26 batch-4 = 1.093209721148014e-05

Training epoch-26 batch-5
Running loss of epoch-26 batch-5 = 1.9507482647895813e-05

Training epoch-26 batch-6
Running loss of epoch-26 batch-6 = 7.503898814320564e-06

Training epoch-26 batch-7
Running loss of epoch-26 batch-7 = 4.036829341202974e-05

Training epoch-26 batch-8
Running loss of epoch-26 batch-8 = 4.212965723127127e-05

Training epoch-26 batch-9
Running loss of epoch-26 batch-9 = 1.0679475963115692e-05

Training epoch-26 batch-10
Running loss of epoch-26 batch-10 = 1.537124626338482e-05

Training epoch-26 batch-11
Running loss of epoch-26 batch-11 = 1.2668897397816181e-05

Training epoch-26 batch-12
Running loss of epoch-26 batch-12 = 2.241611946374178e-05

Training epoch-26 batch-13
Running loss of epoch-26 batch-13 = 4.479952622205019e-05

Training epoch-26 batch-14
Running loss of epoch-26 batch-14 = 1.7162179574370384e-05

Training epoch-26 batch-15
Running loss of epoch-26 batch-15 = 2.241076435893774e-05

Training epoch-26 batch-16
Running loss of epoch-26 batch-16 = 4.18276758864522e-05

Training epoch-26 batch-17
Running loss of epoch-26 batch-17 = 3.8806116208434105e-05

Training epoch-26 batch-18
Running loss of epoch-26 batch-18 = 1.8123886547982693e-05

Training epoch-26 batch-19
Running loss of epoch-26 batch-19 = 2.2431369870901108e-05

Training epoch-26 batch-20
Running loss of epoch-26 batch-20 = 5.3727475460618734e-05

Training epoch-26 batch-21
Running loss of epoch-26 batch-21 = 1.0198098607361317e-05

Training epoch-26 batch-22
Running loss of epoch-26 batch-22 = 1.5087658539414406e-05

Training epoch-26 batch-23
Running loss of epoch-26 batch-23 = 9.858282282948494e-06

Training epoch-26 batch-24
Running loss of epoch-26 batch-24 = 9.098672308027744e-06

Training epoch-26 batch-25
Running loss of epoch-26 batch-25 = 1.427927054464817e-05

Training epoch-26 batch-26
Running loss of epoch-26 batch-26 = 3.824749728664756e-05

Training epoch-26 batch-27
Running loss of epoch-26 batch-27 = 9.248382411897182e-06

Training epoch-26 batch-28
Running loss of epoch-26 batch-28 = 1.2827920727431774e-05

Training epoch-26 batch-29
Running loss of epoch-26 batch-29 = 3.929692320525646e-05

Training epoch-26 batch-30
Running loss of epoch-26 batch-30 = 5.17776352353394e-05

Training epoch-26 batch-31
Running loss of epoch-26 batch-31 = 1.3402430340647697e-05

Training epoch-26 batch-32
Running loss of epoch-26 batch-32 = 1.30130210891366e-05

Training epoch-26 batch-33
Running loss of epoch-26 batch-33 = 2.2466061636805534e-05

Training epoch-26 batch-34
Running loss of epoch-26 batch-34 = 0.0001389991957694292

Training epoch-26 batch-35
Running loss of epoch-26 batch-35 = 1.5087076462805271e-05

Training epoch-26 batch-36
Running loss of epoch-26 batch-36 = 3.1428528018295765e-05

Training epoch-26 batch-37
Running loss of epoch-26 batch-37 = 6.453949026763439e-06

Training epoch-26 batch-38
Running loss of epoch-26 batch-38 = 5.92739088460803e-05

Training epoch-26 batch-39
Running loss of epoch-26 batch-39 = 5.80446794629097e-05

Training epoch-26 batch-40
Running loss of epoch-26 batch-40 = 2.2963969968259335e-05

Training epoch-26 batch-41
Running loss of epoch-26 batch-41 = 8.614850230515003e-06

Training epoch-26 batch-42
Running loss of epoch-26 batch-42 = 1.627625897526741e-05

Training epoch-26 batch-43
Running loss of epoch-26 batch-43 = 2.4856417439877987e-05

Training epoch-26 batch-44
Running loss of epoch-26 batch-44 = 1.308938954025507e-05

Training epoch-26 batch-45
Running loss of epoch-26 batch-45 = 1.0887975804507732e-05

Training epoch-26 batch-46
Running loss of epoch-26 batch-46 = 2.1993066184222698e-05

Training epoch-26 batch-47
Running loss of epoch-26 batch-47 = 7.953157182782888e-05

Training epoch-26 batch-48
Running loss of epoch-26 batch-48 = 5.651527317240834e-05

Training epoch-26 batch-49
Running loss of epoch-26 batch-49 = 1.5912111848592758e-05

Training epoch-26 batch-50
Running loss of epoch-26 batch-50 = 2.4417415261268616e-05

Training epoch-26 batch-51
Running loss of epoch-26 batch-51 = 0.0001064877724274993

Training epoch-26 batch-52
Running loss of epoch-26 batch-52 = 3.166834358125925e-05

Training epoch-26 batch-53
Running loss of epoch-26 batch-53 = 4.780362360179424e-05

Training epoch-26 batch-54
Running loss of epoch-26 batch-54 = 8.736387826502323e-06

Training epoch-26 batch-55
Running loss of epoch-26 batch-55 = 1.4749006368219852e-05

Training epoch-26 batch-56
Running loss of epoch-26 batch-56 = 4.4253538362681866e-05

Training epoch-26 batch-57
Running loss of epoch-26 batch-57 = 6.388872861862183e-06

Training epoch-26 batch-58
Running loss of epoch-26 batch-58 = 1.472595613449812e-05

Training epoch-26 batch-59
Running loss of epoch-26 batch-59 = 1.837429590523243e-05

Training epoch-26 batch-60
Running loss of epoch-26 batch-60 = 6.857444532215595e-06

Training epoch-26 batch-61
Running loss of epoch-26 batch-61 = 9.9503668025136e-06

Training epoch-26 batch-62
Running loss of epoch-26 batch-62 = 1.1003576219081879e-05

Training epoch-26 batch-63
Running loss of epoch-26 batch-63 = 0.00011044659186154604

Training epoch-26 batch-64
Running loss of epoch-26 batch-64 = 5.040486576035619e-05

Training epoch-26 batch-65
Running loss of epoch-26 batch-65 = 1.0221032425761223e-05

Training epoch-26 batch-66
Running loss of epoch-26 batch-66 = 1.1789496056735516e-05

Training epoch-26 batch-67
Running loss of epoch-26 batch-67 = 6.0893362388014793e-05

Training epoch-26 batch-68
Running loss of epoch-26 batch-68 = 1.5337252989411354e-05

Training epoch-26 batch-69
Running loss of epoch-26 batch-69 = 1.589383464306593e-05

Training epoch-26 batch-70
Running loss of epoch-26 batch-70 = 1.0450952686369419e-05

Training epoch-26 batch-71
Running loss of epoch-26 batch-71 = 6.655073957517743e-05

Training epoch-26 batch-72
Running loss of epoch-26 batch-72 = 8.893548510968685e-06

Training epoch-26 batch-73
Running loss of epoch-26 batch-73 = 3.365939483046532e-05

Training epoch-26 batch-74
Running loss of epoch-26 batch-74 = 6.836722604930401e-06

Training epoch-26 batch-75
Running loss of epoch-26 batch-75 = 5.381600931286812e-05

Training epoch-26 batch-76
Running loss of epoch-26 batch-76 = 2.4845474399626255e-05

Training epoch-26 batch-77
Running loss of epoch-26 batch-77 = 1.2768898159265518e-05

Training epoch-26 batch-78
Running loss of epoch-26 batch-78 = 3.300944808870554e-05

Training epoch-26 batch-79
Running loss of epoch-26 batch-79 = 1.5092897228896618e-05

Training epoch-26 batch-80
Running loss of epoch-26 batch-80 = 4.865601658821106e-05

Training epoch-26 batch-81
Running loss of epoch-26 batch-81 = 1.4276010915637016e-05

Training epoch-26 batch-82
Running loss of epoch-26 batch-82 = 1.3135140761733055e-05

Training epoch-26 batch-83
Running loss of epoch-26 batch-83 = 6.5838685259222984e-06

Training epoch-26 batch-84
Running loss of epoch-26 batch-84 = 7.930898573249578e-05

Training epoch-26 batch-85
Running loss of epoch-26 batch-85 = 7.485854439437389e-06

Training epoch-26 batch-86
Running loss of epoch-26 batch-86 = 5.706562660634518e-06

Training epoch-26 batch-87
Running loss of epoch-26 batch-87 = 6.345938891172409e-05

Training epoch-26 batch-88
Running loss of epoch-26 batch-88 = 1.4864257536828518e-05

Training epoch-26 batch-89
Running loss of epoch-26 batch-89 = 2.0909588783979416e-05

Training epoch-26 batch-90
Running loss of epoch-26 batch-90 = 6.368616595864296e-06

Training epoch-26 batch-91
Running loss of epoch-26 batch-91 = 2.172030508518219e-05

Training epoch-26 batch-92
Running loss of epoch-26 batch-92 = 1.1127674952149391e-05

Training epoch-26 batch-93
Running loss of epoch-26 batch-93 = 2.267281524837017e-05

Training epoch-26 batch-94
Running loss of epoch-26 batch-94 = 1.6328063793480396e-05

Training epoch-26 batch-95
Running loss of epoch-26 batch-95 = 2.9632356017827988e-05

Training epoch-26 batch-96
Running loss of epoch-26 batch-96 = 1.9084778614342213e-05

Training epoch-26 batch-97
Running loss of epoch-26 batch-97 = 4.061870276927948e-05

Training epoch-26 batch-98
Running loss of epoch-26 batch-98 = 1.767429057508707e-05

Training epoch-26 batch-99
Running loss of epoch-26 batch-99 = 1.2173200957477093e-05

Training epoch-26 batch-100
Running loss of epoch-26 batch-100 = 1.0293093509972095e-05

Training epoch-26 batch-101
Running loss of epoch-26 batch-101 = 3.5464297980070114e-05

Training epoch-26 batch-102
Running loss of epoch-26 batch-102 = 9.5051946118474e-06

Training epoch-26 batch-103
Running loss of epoch-26 batch-103 = 1.3846787624061108e-05

Training epoch-26 batch-104
Running loss of epoch-26 batch-104 = 8.600763976573944e-06

Training epoch-26 batch-105
Running loss of epoch-26 batch-105 = 2.968311309814453e-05

Training epoch-26 batch-106
Running loss of epoch-26 batch-106 = 7.040868513286114e-05

Training epoch-26 batch-107
Running loss of epoch-26 batch-107 = 2.3272004909813404e-05

Training epoch-26 batch-108
Running loss of epoch-26 batch-108 = 8.265720680356026e-05

Training epoch-26 batch-109
Running loss of epoch-26 batch-109 = 1.2324890121817589e-05

Training epoch-26 batch-110
Running loss of epoch-26 batch-110 = 2.1063024178147316e-05

Training epoch-26 batch-111
Running loss of epoch-26 batch-111 = 1.7527141608297825e-05

Training epoch-26 batch-112
Running loss of epoch-26 batch-112 = 2.3107859306037426e-05

Training epoch-26 batch-113
Running loss of epoch-26 batch-113 = 2.0023551769554615e-05

Training epoch-26 batch-114
Running loss of epoch-26 batch-114 = 1.3589044101536274e-05

Training epoch-26 batch-115
Running loss of epoch-26 batch-115 = 2.8536655008792877e-05

Training epoch-26 batch-116
Running loss of epoch-26 batch-116 = 2.8758076950907707e-05

Training epoch-26 batch-117
Running loss of epoch-26 batch-117 = 5.234265699982643e-06

Training epoch-26 batch-118
Running loss of epoch-26 batch-118 = 7.031252607703209e-06

Training epoch-26 batch-119
Running loss of epoch-26 batch-119 = 1.6378238797187805e-05

Training epoch-26 batch-120
Running loss of epoch-26 batch-120 = 1.2570060789585114e-05

Training epoch-26 batch-121
Running loss of epoch-26 batch-121 = 0.00018558482406660914

Training epoch-26 batch-122
Running loss of epoch-26 batch-122 = 3.2483949325978756e-05

Training epoch-26 batch-123
Running loss of epoch-26 batch-123 = 1.1635245755314827e-05

Training epoch-26 batch-124
Running loss of epoch-26 batch-124 = 1.3416633009910583e-05

Training epoch-26 batch-125
Running loss of epoch-26 batch-125 = 2.9980787076056004e-05

Training epoch-26 batch-126
Running loss of epoch-26 batch-126 = 3.602227661758661e-05

Training epoch-26 batch-127
Running loss of epoch-26 batch-127 = 1.122232060879469e-05

Training epoch-26 batch-128
Running loss of epoch-26 batch-128 = 6.56861811876297e-06

Training epoch-26 batch-129
Running loss of epoch-26 batch-129 = 1.978222280740738e-05

Training epoch-26 batch-130
Running loss of epoch-26 batch-130 = 1.4077522791922092e-05

Training epoch-26 batch-131
Running loss of epoch-26 batch-131 = 1.082650851458311e-05

Training epoch-26 batch-132
Running loss of epoch-26 batch-132 = 1.3544224202632904e-05

Training epoch-26 batch-133
Running loss of epoch-26 batch-133 = 5.9818674344569445e-05

Training epoch-26 batch-134
Running loss of epoch-26 batch-134 = 1.777801662683487e-05

Training epoch-26 batch-135
Running loss of epoch-26 batch-135 = 2.869381569325924e-05

Training epoch-26 batch-136
Running loss of epoch-26 batch-136 = 1.1234893463551998e-05

Training epoch-26 batch-137
Running loss of epoch-26 batch-137 = 1.1686584912240505e-05

Training epoch-26 batch-138
Running loss of epoch-26 batch-138 = 9.699841029942036e-06

Training epoch-26 batch-139
Running loss of epoch-26 batch-139 = 3.0711409635841846e-05

Training epoch-26 batch-140
Running loss of epoch-26 batch-140 = 4.568323493003845e-05

Training epoch-26 batch-141
Running loss of epoch-26 batch-141 = 1.1511263437569141e-05

Training epoch-26 batch-142
Running loss of epoch-26 batch-142 = 3.328779712319374e-06

Training epoch-26 batch-143
Running loss of epoch-26 batch-143 = 1.0707997716963291e-05

Training epoch-26 batch-144
Running loss of epoch-26 batch-144 = 1.2394157238304615e-05

Training epoch-26 batch-145
Running loss of epoch-26 batch-145 = 2.5154324248433113e-05

Training epoch-26 batch-146
Running loss of epoch-26 batch-146 = 2.108037006109953e-05

Training epoch-26 batch-147
Running loss of epoch-26 batch-147 = 2.7429545298218727e-05

Training epoch-26 batch-148
Running loss of epoch-26 batch-148 = 8.553150109946728e-06

Training epoch-26 batch-149
Running loss of epoch-26 batch-149 = 9.086215868592262e-06

Training epoch-26 batch-150
Running loss of epoch-26 batch-150 = 1.5615602023899555e-05

Training epoch-26 batch-151
Running loss of epoch-26 batch-151 = 1.6028177924454212e-05

Training epoch-26 batch-152
Running loss of epoch-26 batch-152 = 1.2870528735220432e-05

Training epoch-26 batch-153
Running loss of epoch-26 batch-153 = 7.338006980717182e-06

Training epoch-26 batch-154
Running loss of epoch-26 batch-154 = 3.606313839554787e-05

Training epoch-26 batch-155
Running loss of epoch-26 batch-155 = 9.662588126957417e-06

Training epoch-26 batch-156
Running loss of epoch-26 batch-156 = 0.00027680190396495163

Training epoch-26 batch-157
Running loss of epoch-26 batch-157 = 2.7943402528762817e-05

Finished training epoch-26.



Average train loss at epoch-26 = 2.848158348351717e-05

Started Evaluation

Average val loss at epoch-26 = 4.611658187288987

Accuracy for classes:
Accuracy for class equals is: 62.87 %
Accuracy for class main is: 43.44 %
Accuracy for class setUp is: 52.79 %
Accuracy for class onCreate is: 36.35 %
Accuracy for class toString is: 40.61 %
Accuracy for class run is: 29.22 %
Accuracy for class hashCode is: 75.28 %
Accuracy for class init is: 26.68 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 27.69 %

Overall Accuracy = 41.96 %

Finished Evaluation



Started training epoch-27


Training epoch-27 batch-1
Running loss of epoch-27 batch-1 = 8.731265552341938e-06

Training epoch-27 batch-2
Running loss of epoch-27 batch-2 = 2.0777923054993153e-05

Training epoch-27 batch-3
Running loss of epoch-27 batch-3 = 1.2600910849869251e-05

Training epoch-27 batch-4
Running loss of epoch-27 batch-4 = 1.3522105291485786e-05

Training epoch-27 batch-5
Running loss of epoch-27 batch-5 = 6.1105238273739815e-06

Training epoch-27 batch-6
Running loss of epoch-27 batch-6 = 1.0311021469533443e-05

Training epoch-27 batch-7
Running loss of epoch-27 batch-7 = 3.257975913584232e-05

Training epoch-27 batch-8
Running loss of epoch-27 batch-8 = 6.2694307416677475e-06

Training epoch-27 batch-9
Running loss of epoch-27 batch-9 = 3.6950979847460985e-05

Training epoch-27 batch-10
Running loss of epoch-27 batch-10 = 1.7925631254911423e-05

Training epoch-27 batch-11
Running loss of epoch-27 batch-11 = 4.9889786168932915e-06

Training epoch-27 batch-12
Running loss of epoch-27 batch-12 = 2.00195936486125e-05

Training epoch-27 batch-13
Running loss of epoch-27 batch-13 = 4.014180740341544e-05

Training epoch-27 batch-14
Running loss of epoch-27 batch-14 = 2.383394166827202e-05

Training epoch-27 batch-15
Running loss of epoch-27 batch-15 = 1.6998848877847195e-05

Training epoch-27 batch-16
Running loss of epoch-27 batch-16 = 7.91007187217474e-06

Training epoch-27 batch-17
Running loss of epoch-27 batch-17 = 3.92270740121603e-05

Training epoch-27 batch-18
Running loss of epoch-27 batch-18 = 5.778856575489044e-06

Training epoch-27 batch-19
Running loss of epoch-27 batch-19 = 1.7445534467697144e-05

Training epoch-27 batch-20
Running loss of epoch-27 batch-20 = 1.0414165444672108e-05

Training epoch-27 batch-21
Running loss of epoch-27 batch-21 = 4.3844920583069324e-05

Training epoch-27 batch-22
Running loss of epoch-27 batch-22 = 2.1815882064402103e-05

Training epoch-27 batch-23
Running loss of epoch-27 batch-23 = 2.2854306735098362e-05

Training epoch-27 batch-24
Running loss of epoch-27 batch-24 = 4.7443900257349014e-06

Training epoch-27 batch-25
Running loss of epoch-27 batch-25 = 1.5055644325911999e-05

Training epoch-27 batch-26
Running loss of epoch-27 batch-26 = 1.8136692233383656e-05

Training epoch-27 batch-27
Running loss of epoch-27 batch-27 = 3.1513627618551254e-06

Training epoch-27 batch-28
Running loss of epoch-27 batch-28 = 2.231891267001629e-05

Training epoch-27 batch-29
Running loss of epoch-27 batch-29 = 1.72534491866827e-05

Training epoch-27 batch-30
Running loss of epoch-27 batch-30 = 2.4821842089295387e-05

Training epoch-27 batch-31
Running loss of epoch-27 batch-31 = 8.140690624713898e-06

Training epoch-27 batch-32
Running loss of epoch-27 batch-32 = 0.00010018778266385198

Training epoch-27 batch-33
Running loss of epoch-27 batch-33 = 4.9510912504047155e-05

Training epoch-27 batch-34
Running loss of epoch-27 batch-34 = 4.794256528839469e-05

Training epoch-27 batch-35
Running loss of epoch-27 batch-35 = 6.177672185003757e-05

Training epoch-27 batch-36
Running loss of epoch-27 batch-36 = 1.3118493370711803e-05

Training epoch-27 batch-37
Running loss of epoch-27 batch-37 = 2.4247681722044945e-05

Training epoch-27 batch-38
Running loss of epoch-27 batch-38 = 1.1497526429593563e-05

Training epoch-27 batch-39
Running loss of epoch-27 batch-39 = 1.6267760656774044e-05

Training epoch-27 batch-40
Running loss of epoch-27 batch-40 = 3.331934567540884e-05

Training epoch-27 batch-41
Running loss of epoch-27 batch-41 = 4.359986633062363e-06

Training epoch-27 batch-42
Running loss of epoch-27 batch-42 = 4.396308213472366e-06

Training epoch-27 batch-43
Running loss of epoch-27 batch-43 = 3.814802039414644e-05

Training epoch-27 batch-44
Running loss of epoch-27 batch-44 = 9.456765837967396e-06

Training epoch-27 batch-45
Running loss of epoch-27 batch-45 = 1.1461786925792694e-05

Training epoch-27 batch-46
Running loss of epoch-27 batch-46 = 7.87281896919012e-06

Training epoch-27 batch-47
Running loss of epoch-27 batch-47 = 2.0725536160171032e-05

Training epoch-27 batch-48
Running loss of epoch-27 batch-48 = 8.565024472773075e-06

Training epoch-27 batch-49
Running loss of epoch-27 batch-49 = 1.806893851608038e-05

Training epoch-27 batch-50
Running loss of epoch-27 batch-50 = 3.0118157155811787e-05

Training epoch-27 batch-51
Running loss of epoch-27 batch-51 = 6.696092896163464e-06

Training epoch-27 batch-52
Running loss of epoch-27 batch-52 = 1.542398240417242e-05

Training epoch-27 batch-53
Running loss of epoch-27 batch-53 = 1.6815727576613426e-05

Training epoch-27 batch-54
Running loss of epoch-27 batch-54 = 5.348818376660347e-06

Training epoch-27 batch-55
Running loss of epoch-27 batch-55 = 2.7363654226064682e-05

Training epoch-27 batch-56
Running loss of epoch-27 batch-56 = 2.0244624465703964e-05

Training epoch-27 batch-57
Running loss of epoch-27 batch-57 = 4.9365859013050795e-05

Training epoch-27 batch-58
Running loss of epoch-27 batch-58 = 6.764894351363182e-06

Training epoch-27 batch-59
Running loss of epoch-27 batch-59 = 3.0075316317379475e-05

Training epoch-27 batch-60
Running loss of epoch-27 batch-60 = 3.5961857065558434e-06

Training epoch-27 batch-61
Running loss of epoch-27 batch-61 = 1.5758443623781204e-05

Training epoch-27 batch-62
Running loss of epoch-27 batch-62 = 1.596449874341488e-05

Training epoch-27 batch-63
Running loss of epoch-27 batch-63 = 1.0193092748522758e-05

Training epoch-27 batch-64
Running loss of epoch-27 batch-64 = 1.1880532838404179e-05

Training epoch-27 batch-65
Running loss of epoch-27 batch-65 = 1.8974998965859413e-05

Training epoch-27 batch-66
Running loss of epoch-27 batch-66 = 4.7995708882808685e-06

Training epoch-27 batch-67
Running loss of epoch-27 batch-67 = 3.477209247648716e-05

Training epoch-27 batch-68
Running loss of epoch-27 batch-68 = 2.2059539332985878e-05

Training epoch-27 batch-69
Running loss of epoch-27 batch-69 = 1.12846028059721e-05

Training epoch-27 batch-70
Running loss of epoch-27 batch-70 = 4.361034370958805e-06

Training epoch-27 batch-71
Running loss of epoch-27 batch-71 = 1.0784133337438107e-05

Training epoch-27 batch-72
Running loss of epoch-27 batch-72 = 2.215022686868906e-05

Training epoch-27 batch-73
Running loss of epoch-27 batch-73 = 1.8074759282171726e-05

Training epoch-27 batch-74
Running loss of epoch-27 batch-74 = 1.5216297470033169e-05

Training epoch-27 batch-75
Running loss of epoch-27 batch-75 = 4.82914038002491e-06

Training epoch-27 batch-76
Running loss of epoch-27 batch-76 = 4.4780317693948746e-06

Training epoch-27 batch-77
Running loss of epoch-27 batch-77 = 1.2154108844697475e-05

Training epoch-27 batch-78
Running loss of epoch-27 batch-78 = 2.9613380320370197e-05

Training epoch-27 batch-79
Running loss of epoch-27 batch-79 = 2.8211972676217556e-05

Training epoch-27 batch-80
Running loss of epoch-27 batch-80 = 5.30085526406765e-06

Training epoch-27 batch-81
Running loss of epoch-27 batch-81 = 1.0111951269209385e-05

Training epoch-27 batch-82
Running loss of epoch-27 batch-82 = 1.3173907063901424e-05

Training epoch-27 batch-83
Running loss of epoch-27 batch-83 = 1.2509175576269627e-05

Training epoch-27 batch-84
Running loss of epoch-27 batch-84 = 1.2952717952430248e-05

Training epoch-27 batch-85
Running loss of epoch-27 batch-85 = 9.040115401148796e-06

Training epoch-27 batch-86
Running loss of epoch-27 batch-86 = 9.286683052778244e-06

Training epoch-27 batch-87
Running loss of epoch-27 batch-87 = 2.4136505089700222e-05

Training epoch-27 batch-88
Running loss of epoch-27 batch-88 = 1.6003847122192383e-05

Training epoch-27 batch-89
Running loss of epoch-27 batch-89 = 2.8728973120450974e-06

Training epoch-27 batch-90
Running loss of epoch-27 batch-90 = 3.9179809391498566e-05

Training epoch-27 batch-91
Running loss of epoch-27 batch-91 = 2.250575926154852e-05

Training epoch-27 batch-92
Running loss of epoch-27 batch-92 = 8.112681098282337e-05

Training epoch-27 batch-93
Running loss of epoch-27 batch-93 = 1.5423051081597805e-05

Training epoch-27 batch-94
Running loss of epoch-27 batch-94 = 7.829745300114155e-06

Training epoch-27 batch-95
Running loss of epoch-27 batch-95 = 2.2286665625870228e-05

Training epoch-27 batch-96
Running loss of epoch-27 batch-96 = 8.165370672941208e-06

Training epoch-27 batch-97
Running loss of epoch-27 batch-97 = 2.31423182412982e-05

Training epoch-27 batch-98
Running loss of epoch-27 batch-98 = 1.7887214198708534e-05

Training epoch-27 batch-99
Running loss of epoch-27 batch-99 = 2.700171899050474e-05

Training epoch-27 batch-100
Running loss of epoch-27 batch-100 = 1.727743074297905e-05

Training epoch-27 batch-101
Running loss of epoch-27 batch-101 = 6.551248952746391e-05

Training epoch-27 batch-102
Running loss of epoch-27 batch-102 = 1.2453179806470871e-05

Training epoch-27 batch-103
Running loss of epoch-27 batch-103 = 1.3470416888594627e-05

Training epoch-27 batch-104
Running loss of epoch-27 batch-104 = 2.8165406547486782e-05

Training epoch-27 batch-105
Running loss of epoch-27 batch-105 = 7.611117325723171e-06

Training epoch-27 batch-106
Running loss of epoch-27 batch-106 = 1.0553281754255295e-05

Training epoch-27 batch-107
Running loss of epoch-27 batch-107 = 8.111470378935337e-06

Training epoch-27 batch-108
Running loss of epoch-27 batch-108 = 9.701005183160305e-06

Training epoch-27 batch-109
Running loss of epoch-27 batch-109 = 1.1268304660916328e-05

Training epoch-27 batch-110
Running loss of epoch-27 batch-110 = 2.382136881351471e-05

Training epoch-27 batch-111
Running loss of epoch-27 batch-111 = 3.416859544813633e-05

Training epoch-27 batch-112
Running loss of epoch-27 batch-112 = 1.9172788597643375e-05

Training epoch-27 batch-113
Running loss of epoch-27 batch-113 = 1.2762262485921383e-05

Training epoch-27 batch-114
Running loss of epoch-27 batch-114 = 4.167936276644468e-05

Training epoch-27 batch-115
Running loss of epoch-27 batch-115 = 6.9818925112485886e-06

Training epoch-27 batch-116
Running loss of epoch-27 batch-116 = 2.5432556867599487e-05

Training epoch-27 batch-117
Running loss of epoch-27 batch-117 = 1.7201760783791542e-05

Training epoch-27 batch-118
Running loss of epoch-27 batch-118 = 7.652677595615387e-06

Training epoch-27 batch-119
Running loss of epoch-27 batch-119 = 4.876399179920554e-05

Training epoch-27 batch-120
Running loss of epoch-27 batch-120 = 5.296454764902592e-05

Training epoch-27 batch-121
Running loss of epoch-27 batch-121 = 2.147420309484005e-05

Training epoch-27 batch-122
Running loss of epoch-27 batch-122 = 2.5806482881307602e-05

Training epoch-27 batch-123
Running loss of epoch-27 batch-123 = 5.684606730937958e-05

Training epoch-27 batch-124
Running loss of epoch-27 batch-124 = 2.1890737116336823e-06

Training epoch-27 batch-125
Running loss of epoch-27 batch-125 = 2.502754796296358e-05

Training epoch-27 batch-126
Running loss of epoch-27 batch-126 = 1.7379061318933964e-05

Training epoch-27 batch-127
Running loss of epoch-27 batch-127 = 7.997965440154076e-06

Training epoch-27 batch-128
Running loss of epoch-27 batch-128 = 3.1287665478885174e-05

Training epoch-27 batch-129
Running loss of epoch-27 batch-129 = 2.0953011699020863e-05

Training epoch-27 batch-130
Running loss of epoch-27 batch-130 = 4.772935062646866e-05

Training epoch-27 batch-131
Running loss of epoch-27 batch-131 = 0.00013814144767820835

Training epoch-27 batch-132
Running loss of epoch-27 batch-132 = 8.084578439593315e-06

Training epoch-27 batch-133
Running loss of epoch-27 batch-133 = 3.505242057144642e-05

Training epoch-27 batch-134
Running loss of epoch-27 batch-134 = 2.2250344045460224e-05

Training epoch-27 batch-135
Running loss of epoch-27 batch-135 = 6.957113509997725e-05

Training epoch-27 batch-136
Running loss of epoch-27 batch-136 = 8.062936831265688e-05

Training epoch-27 batch-137
Running loss of epoch-27 batch-137 = 0.00017679575830698013

Training epoch-27 batch-138
Running loss of epoch-27 batch-138 = 6.3998159021139145e-06

Training epoch-27 batch-139
Running loss of epoch-27 batch-139 = 1.287669874727726e-05

Training epoch-27 batch-140
Running loss of epoch-27 batch-140 = 3.757001832127571e-05

Training epoch-27 batch-141
Running loss of epoch-27 batch-141 = 3.028358332812786e-05

Training epoch-27 batch-142
Running loss of epoch-27 batch-142 = 3.466266207396984e-06

Training epoch-27 batch-143
Running loss of epoch-27 batch-143 = 8.99401493370533e-06

Training epoch-27 batch-144
Running loss of epoch-27 batch-144 = 5.473382771015167e-06

Training epoch-27 batch-145
Running loss of epoch-27 batch-145 = 1.188693568110466e-05

Training epoch-27 batch-146
Running loss of epoch-27 batch-146 = 7.432029815390706e-05

Training epoch-27 batch-147
Running loss of epoch-27 batch-147 = 1.2373551726341248e-05

Training epoch-27 batch-148
Running loss of epoch-27 batch-148 = 3.4493859857320786e-05

Training epoch-27 batch-149
Running loss of epoch-27 batch-149 = 1.0892748832702637e-05

Training epoch-27 batch-150
Running loss of epoch-27 batch-150 = 1.530663575977087e-05

Training epoch-27 batch-151
Running loss of epoch-27 batch-151 = 1.6403268091380596e-05

Training epoch-27 batch-152
Running loss of epoch-27 batch-152 = 2.304429654031992e-05

Training epoch-27 batch-153
Running loss of epoch-27 batch-153 = 9.843031875789165e-06

Training epoch-27 batch-154
Running loss of epoch-27 batch-154 = 2.215406857430935e-05

Training epoch-27 batch-155
Running loss of epoch-27 batch-155 = 1.4459481462836266e-05

Training epoch-27 batch-156
Running loss of epoch-27 batch-156 = 0.0002308563853148371

Training epoch-27 batch-157
Running loss of epoch-27 batch-157 = 0.00017869658768177032

Finished training epoch-27.



Average train loss at epoch-27 = 2.4388001672923563e-05

Started Evaluation

Average val loss at epoch-27 = 4.672667791968898

Accuracy for classes:
Accuracy for class equals is: 61.72 %
Accuracy for class main is: 43.44 %
Accuracy for class setUp is: 53.44 %
Accuracy for class onCreate is: 36.25 %
Accuracy for class toString is: 39.59 %
Accuracy for class run is: 30.37 %
Accuracy for class hashCode is: 75.28 %
Accuracy for class init is: 30.49 %
Accuracy for class execute is: 18.07 %
Accuracy for class get is: 25.90 %

Overall Accuracy = 42.03 %

Finished Evaluation



Started training epoch-28


Training epoch-28 batch-1
Running loss of epoch-28 batch-1 = 1.6382895410060883e-05

Training epoch-28 batch-2
Running loss of epoch-28 batch-2 = 2.4436158128082752e-05

Training epoch-28 batch-3
Running loss of epoch-28 batch-3 = 1.8763123080134392e-05

Training epoch-28 batch-4
Running loss of epoch-28 batch-4 = 5.718786269426346e-06

Training epoch-28 batch-5
Running loss of epoch-28 batch-5 = 2.4057109840214252e-05

Training epoch-28 batch-6
Running loss of epoch-28 batch-6 = 4.007446113973856e-05

Training epoch-28 batch-7
Running loss of epoch-28 batch-7 = 6.415299139916897e-06

Training epoch-28 batch-8
Running loss of epoch-28 batch-8 = 4.004070069640875e-05

Training epoch-28 batch-9
Running loss of epoch-28 batch-9 = 1.8182676285505295e-05

Training epoch-28 batch-10
Running loss of epoch-28 batch-10 = 2.0576640963554382e-05

Training epoch-28 batch-11
Running loss of epoch-28 batch-11 = 1.1224532499909401e-05

Training epoch-28 batch-12
Running loss of epoch-28 batch-12 = 7.378519512712955e-05

Training epoch-28 batch-13
Running loss of epoch-28 batch-13 = 8.163508027791977e-06

Training epoch-28 batch-14
Running loss of epoch-28 batch-14 = 2.68035801127553e-05

Training epoch-28 batch-15
Running loss of epoch-28 batch-15 = 2.8314185328781605e-05

Training epoch-28 batch-16
Running loss of epoch-28 batch-16 = 0.00015300681116059422

Training epoch-28 batch-17
Running loss of epoch-28 batch-17 = 1.1728261597454548e-05

Training epoch-28 batch-18
Running loss of epoch-28 batch-18 = 4.529842408373952e-05

Training epoch-28 batch-19
Running loss of epoch-28 batch-19 = 2.8561335057020187e-06

Training epoch-28 batch-20
Running loss of epoch-28 batch-20 = 1.2399861589074135e-05

Training epoch-28 batch-21
Running loss of epoch-28 batch-21 = 2.808007411658764e-05

Training epoch-28 batch-22
Running loss of epoch-28 batch-22 = 1.5458674170076847e-05

Training epoch-28 batch-23
Running loss of epoch-28 batch-23 = 1.1389260180294514e-05

Training epoch-28 batch-24
Running loss of epoch-28 batch-24 = 4.574074409902096e-06

Training epoch-28 batch-25
Running loss of epoch-28 batch-25 = 7.017049938440323e-06

Training epoch-28 batch-26
Running loss of epoch-28 batch-26 = 3.308546729385853e-05

Training epoch-28 batch-27
Running loss of epoch-28 batch-27 = 1.4152261428534985e-05

Training epoch-28 batch-28
Running loss of epoch-28 batch-28 = 1.1095311492681503e-05

Training epoch-28 batch-29
Running loss of epoch-28 batch-29 = 1.3130134902894497e-05

Training epoch-28 batch-30
Running loss of epoch-28 batch-30 = 1.2870412319898605e-05

Training epoch-28 batch-31
Running loss of epoch-28 batch-31 = 1.5985453501343727e-05

Training epoch-28 batch-32
Running loss of epoch-28 batch-32 = 0.00014116097008809447

Training epoch-28 batch-33
Running loss of epoch-28 batch-33 = 6.878748536109924e-06

Training epoch-28 batch-34
Running loss of epoch-28 batch-34 = 9.050010703504086e-06

Training epoch-28 batch-35
Running loss of epoch-28 batch-35 = 1.3848883099853992e-05

Training epoch-28 batch-36
Running loss of epoch-28 batch-36 = 1.205748412758112e-05

Training epoch-28 batch-37
Running loss of epoch-28 batch-37 = 4.727626219391823e-06

Training epoch-28 batch-38
Running loss of epoch-28 batch-38 = 1.9849161617457867e-05

Training epoch-28 batch-39
Running loss of epoch-28 batch-39 = 4.731205990538001e-05

Training epoch-28 batch-40
Running loss of epoch-28 batch-40 = 1.660443376749754e-05

Training epoch-28 batch-41
Running loss of epoch-28 batch-41 = 1.0741408914327621e-05

Training epoch-28 batch-42
Running loss of epoch-28 batch-42 = 4.080822691321373e-06

Training epoch-28 batch-43
Running loss of epoch-28 batch-43 = 2.1885382011532784e-05

Training epoch-28 batch-44
Running loss of epoch-28 batch-44 = 1.0507646948099136e-05

Training epoch-28 batch-45
Running loss of epoch-28 batch-45 = 4.209030885249376e-05

Training epoch-28 batch-46
Running loss of epoch-28 batch-46 = 2.747541293501854e-05

Training epoch-28 batch-47
Running loss of epoch-28 batch-47 = 2.2926717065274715e-05

Training epoch-28 batch-48
Running loss of epoch-28 batch-48 = 1.127657014876604e-05

Training epoch-28 batch-49
Running loss of epoch-28 batch-49 = 7.836613804101944e-06

Training epoch-28 batch-50
Running loss of epoch-28 batch-50 = 1.735566183924675e-05

Training epoch-28 batch-51
Running loss of epoch-28 batch-51 = 9.40193422138691e-06

Training epoch-28 batch-52
Running loss of epoch-28 batch-52 = 5.776761099696159e-06

Training epoch-28 batch-53
Running loss of epoch-28 batch-53 = 1.9584782421588898e-05

Training epoch-28 batch-54
Running loss of epoch-28 batch-54 = 1.158425584435463e-05

Training epoch-28 batch-55
Running loss of epoch-28 batch-55 = 6.546732038259506e-06

Training epoch-28 batch-56
Running loss of epoch-28 batch-56 = 5.3426483646035194e-06

Training epoch-28 batch-57
Running loss of epoch-28 batch-57 = 6.305985152721405e-06

Training epoch-28 batch-58
Running loss of epoch-28 batch-58 = 5.727983079850674e-06

Training epoch-28 batch-59
Running loss of epoch-28 batch-59 = 1.4372752048075199e-05

Training epoch-28 batch-60
Running loss of epoch-28 batch-60 = 6.05930108577013e-06

Training epoch-28 batch-61
Running loss of epoch-28 batch-61 = 1.3608136214315891e-05

Training epoch-28 batch-62
Running loss of epoch-28 batch-62 = 1.2456439435482025e-05

Training epoch-28 batch-63
Running loss of epoch-28 batch-63 = 1.0601361282169819e-05

Training epoch-28 batch-64
Running loss of epoch-28 batch-64 = 7.00121745467186e-06

Training epoch-28 batch-65
Running loss of epoch-28 batch-65 = 3.5251257941126823e-05

Training epoch-28 batch-66
Running loss of epoch-28 batch-66 = 8.397269994020462e-06

Training epoch-28 batch-67
Running loss of epoch-28 batch-67 = 3.2971613109111786e-05

Training epoch-28 batch-68
Running loss of epoch-28 batch-68 = 3.3824064303189516e-05

Training epoch-28 batch-69
Running loss of epoch-28 batch-69 = 3.508280497044325e-05

Training epoch-28 batch-70
Running loss of epoch-28 batch-70 = 6.041256710886955e-06

Training epoch-28 batch-71
Running loss of epoch-28 batch-71 = 3.154040314257145e-06

Training epoch-28 batch-72
Running loss of epoch-28 batch-72 = 1.2242235243320465e-05

Training epoch-28 batch-73
Running loss of epoch-28 batch-73 = 9.944313205778599e-06

Training epoch-28 batch-74
Running loss of epoch-28 batch-74 = 8.172472007572651e-06

Training epoch-28 batch-75
Running loss of epoch-28 batch-75 = 9.934185072779655e-06

Training epoch-28 batch-76
Running loss of epoch-28 batch-76 = 1.3605575077235699e-05

Training epoch-28 batch-77
Running loss of epoch-28 batch-77 = 1.5168101526796818e-05

Training epoch-28 batch-78
Running loss of epoch-28 batch-78 = 5.160225555300713e-06

Training epoch-28 batch-79
Running loss of epoch-28 batch-79 = 1.8279417417943478e-05

Training epoch-28 batch-80
Running loss of epoch-28 batch-80 = 6.49609137326479e-06

Training epoch-28 batch-81
Running loss of epoch-28 batch-81 = 2.5717774406075478e-05

Training epoch-28 batch-82
Running loss of epoch-28 batch-82 = 9.154551662504673e-06

Training epoch-28 batch-83
Running loss of epoch-28 batch-83 = 1.145014539361e-05

Training epoch-28 batch-84
Running loss of epoch-28 batch-84 = 1.6948324628174305e-05

Training epoch-28 batch-85
Running loss of epoch-28 batch-85 = 2.4857406970113516e-05

Training epoch-28 batch-86
Running loss of epoch-28 batch-86 = 1.1677620932459831e-05

Training epoch-28 batch-87
Running loss of epoch-28 batch-87 = 4.0402752347290516e-05

Training epoch-28 batch-88
Running loss of epoch-28 batch-88 = 2.6066903956234455e-05

Training epoch-28 batch-89
Running loss of epoch-28 batch-89 = 3.3613527193665504e-05

Training epoch-28 batch-90
Running loss of epoch-28 batch-90 = 2.1041138097643852e-05

Training epoch-28 batch-91
Running loss of epoch-28 batch-91 = 1.164956483989954e-05

Training epoch-28 batch-92
Running loss of epoch-28 batch-92 = 1.4127930626273155e-05

Training epoch-28 batch-93
Running loss of epoch-28 batch-93 = 6.90487795509398e-05

Training epoch-28 batch-94
Running loss of epoch-28 batch-94 = 3.2584648579359055e-05

Training epoch-28 batch-95
Running loss of epoch-28 batch-95 = 7.848255336284637e-06

Training epoch-28 batch-96
Running loss of epoch-28 batch-96 = 8.18353146314621e-06

Training epoch-28 batch-97
Running loss of epoch-28 batch-97 = 1.7946818843483925e-05

Training epoch-28 batch-98
Running loss of epoch-28 batch-98 = 1.707498449832201e-05

Training epoch-28 batch-99
Running loss of epoch-28 batch-99 = 6.288522854447365e-06

Training epoch-28 batch-100
Running loss of epoch-28 batch-100 = 9.420444257557392e-06

Training epoch-28 batch-101
Running loss of epoch-28 batch-101 = 1.819583121687174e-05

Training epoch-28 batch-102
Running loss of epoch-28 batch-102 = 1.720187719911337e-05

Training epoch-28 batch-103
Running loss of epoch-28 batch-103 = 1.1258060112595558e-05

Training epoch-28 batch-104
Running loss of epoch-28 batch-104 = 1.514481846243143e-05

Training epoch-28 batch-105
Running loss of epoch-28 batch-105 = 7.0248497650027275e-06

Training epoch-28 batch-106
Running loss of epoch-28 batch-106 = 5.325971869751811e-05

Training epoch-28 batch-107
Running loss of epoch-28 batch-107 = 6.268598372116685e-05

Training epoch-28 batch-108
Running loss of epoch-28 batch-108 = 2.017221413552761e-05

Training epoch-28 batch-109
Running loss of epoch-28 batch-109 = 1.4624209143221378e-05

Training epoch-28 batch-110
Running loss of epoch-28 batch-110 = 8.999020792543888e-06

Training epoch-28 batch-111
Running loss of epoch-28 batch-111 = 0.0001678581757005304

Training epoch-28 batch-112
Running loss of epoch-28 batch-112 = 6.594113074243069e-06

Training epoch-28 batch-113
Running loss of epoch-28 batch-113 = 1.2995325960218906e-05

Training epoch-28 batch-114
Running loss of epoch-28 batch-114 = 6.0671125538647175e-05

Training epoch-28 batch-115
Running loss of epoch-28 batch-115 = 3.7259887903928757e-06

Training epoch-28 batch-116
Running loss of epoch-28 batch-116 = 5.2068266086280346e-05

Training epoch-28 batch-117
Running loss of epoch-28 batch-117 = 3.5224598832428455e-05

Training epoch-28 batch-118
Running loss of epoch-28 batch-118 = 1.032138243317604e-05

Training epoch-28 batch-119
Running loss of epoch-28 batch-119 = 3.3466960303485394e-05

Training epoch-28 batch-120
Running loss of epoch-28 batch-120 = 1.935265026986599e-05

Training epoch-28 batch-121
Running loss of epoch-28 batch-121 = 1.9350671209394932e-05

Training epoch-28 batch-122
Running loss of epoch-28 batch-122 = 6.817863322794437e-06

Training epoch-28 batch-123
Running loss of epoch-28 batch-123 = 2.2444408386945724e-05

Training epoch-28 batch-124
Running loss of epoch-28 batch-124 = 1.3120938092470169e-05

Training epoch-28 batch-125
Running loss of epoch-28 batch-125 = 1.3573328033089638e-05

Training epoch-28 batch-126
Running loss of epoch-28 batch-126 = 1.3771350495517254e-05

Training epoch-28 batch-127
Running loss of epoch-28 batch-127 = 6.768218008801341e-05

Training epoch-28 batch-128
Running loss of epoch-28 batch-128 = 1.175515353679657e-05

Training epoch-28 batch-129
Running loss of epoch-28 batch-129 = 1.9060447812080383e-05

Training epoch-28 batch-130
Running loss of epoch-28 batch-130 = 1.5693833120167255e-05

Training epoch-28 batch-131
Running loss of epoch-28 batch-131 = 1.7478596419095993e-06

Training epoch-28 batch-132
Running loss of epoch-28 batch-132 = 9.632203727960587e-06

Training epoch-28 batch-133
Running loss of epoch-28 batch-133 = 1.1660624295473099e-05

Training epoch-28 batch-134
Running loss of epoch-28 batch-134 = 8.20134300738573e-06

Training epoch-28 batch-135
Running loss of epoch-28 batch-135 = 1.3886834494769573e-05

Training epoch-28 batch-136
Running loss of epoch-28 batch-136 = 1.8677208572626114e-05

Training epoch-28 batch-137
Running loss of epoch-28 batch-137 = 4.357658326625824e-06

Training epoch-28 batch-138
Running loss of epoch-28 batch-138 = 7.584225386381149e-06

Training epoch-28 batch-139
Running loss of epoch-28 batch-139 = 5.518784746527672e-06

Training epoch-28 batch-140
Running loss of epoch-28 batch-140 = 6.09469716437161e-05

Training epoch-28 batch-141
Running loss of epoch-28 batch-141 = 1.0023592039942741e-05

Training epoch-28 batch-142
Running loss of epoch-28 batch-142 = 1.2016156688332558e-05

Training epoch-28 batch-143
Running loss of epoch-28 batch-143 = 6.3267070800065994e-06

Training epoch-28 batch-144
Running loss of epoch-28 batch-144 = 2.9072631150484085e-05

Training epoch-28 batch-145
Running loss of epoch-28 batch-145 = 6.7533692345023155e-06

Training epoch-28 batch-146
Running loss of epoch-28 batch-146 = 3.38892568834126e-05

Training epoch-28 batch-147
Running loss of epoch-28 batch-147 = 1.1396710760891438e-05

Training epoch-28 batch-148
Running loss of epoch-28 batch-148 = 1.1148396879434586e-05

Training epoch-28 batch-149
Running loss of epoch-28 batch-149 = 1.7410144209861755e-05

Training epoch-28 batch-150
Running loss of epoch-28 batch-150 = 7.105059921741486e-06

Training epoch-28 batch-151
Running loss of epoch-28 batch-151 = 1.0640942491590977e-05

Training epoch-28 batch-152
Running loss of epoch-28 batch-152 = 2.2639171220362186e-05

Training epoch-28 batch-153
Running loss of epoch-28 batch-153 = 1.3912562280893326e-05

Training epoch-28 batch-154
Running loss of epoch-28 batch-154 = 1.3124197721481323e-05

Training epoch-28 batch-155
Running loss of epoch-28 batch-155 = 2.0118546672165394e-05

Training epoch-28 batch-156
Running loss of epoch-28 batch-156 = 1.0905321687459946e-05

Training epoch-28 batch-157
Running loss of epoch-28 batch-157 = 0.0001005902886390686

Finished training epoch-28.



Average train loss at epoch-28 = 2.0760900340974332e-05

Started Evaluation

Average val loss at epoch-28 = 4.724173690143385

Accuracy for classes:
Accuracy for class equals is: 60.89 %
Accuracy for class main is: 45.08 %
Accuracy for class setUp is: 50.49 %
Accuracy for class onCreate is: 36.14 %
Accuracy for class toString is: 41.64 %
Accuracy for class run is: 30.82 %
Accuracy for class hashCode is: 75.28 %
Accuracy for class init is: 27.13 %
Accuracy for class execute is: 20.08 %
Accuracy for class get is: 26.67 %

Overall Accuracy = 41.76 %

Finished Evaluation



Started training epoch-29


Training epoch-29 batch-1
Running loss of epoch-29 batch-1 = 9.148148819804192e-06

Training epoch-29 batch-2
Running loss of epoch-29 batch-2 = 1.5002326108515263e-05

Training epoch-29 batch-3
Running loss of epoch-29 batch-3 = 5.325418896973133e-06

Training epoch-29 batch-4
Running loss of epoch-29 batch-4 = 1.0942225344479084e-05

Training epoch-29 batch-5
Running loss of epoch-29 batch-5 = 4.360848106443882e-05

Training epoch-29 batch-6
Running loss of epoch-29 batch-6 = 6.954767741262913e-06

Training epoch-29 batch-7
Running loss of epoch-29 batch-7 = 8.32101795822382e-06

Training epoch-29 batch-8
Running loss of epoch-29 batch-8 = 3.4766271710395813e-06

Training epoch-29 batch-9
Running loss of epoch-29 batch-9 = 3.3160962630063295e-05

Training epoch-29 batch-10
Running loss of epoch-29 batch-10 = 9.553972631692886e-06

Training epoch-29 batch-11
Running loss of epoch-29 batch-11 = 2.4006003513932228e-05

Training epoch-29 batch-12
Running loss of epoch-29 batch-12 = 9.781448170542717e-06

Training epoch-29 batch-13
Running loss of epoch-29 batch-13 = 5.109701305627823e-06

Training epoch-29 batch-14
Running loss of epoch-29 batch-14 = 4.420103505253792e-05

Training epoch-29 batch-15
Running loss of epoch-29 batch-15 = 1.0793679393827915e-05

Training epoch-29 batch-16
Running loss of epoch-29 batch-16 = 1.4812452718615532e-05

Training epoch-29 batch-17
Running loss of epoch-29 batch-17 = 1.2311269529163837e-05

Training epoch-29 batch-18
Running loss of epoch-29 batch-18 = 2.326001413166523e-05

Training epoch-29 batch-19
Running loss of epoch-29 batch-19 = 1.3286597095429897e-05

Training epoch-29 batch-20
Running loss of epoch-29 batch-20 = 3.476860001683235e-06

Training epoch-29 batch-21
Running loss of epoch-29 batch-21 = 1.2933509424328804e-05

Training epoch-29 batch-22
Running loss of epoch-29 batch-22 = 7.012509740889072e-06

Training epoch-29 batch-23
Running loss of epoch-29 batch-23 = 1.3561570085585117e-05

Training epoch-29 batch-24
Running loss of epoch-29 batch-24 = 2.0995037630200386e-05

Training epoch-29 batch-25
Running loss of epoch-29 batch-25 = 1.828698441386223e-05

Training epoch-29 batch-26
Running loss of epoch-29 batch-26 = 1.4369259588420391e-05

Training epoch-29 batch-27
Running loss of epoch-29 batch-27 = 6.084563210606575e-06

Training epoch-29 batch-28
Running loss of epoch-29 batch-28 = 8.409842848777771e-06

Training epoch-29 batch-29
Running loss of epoch-29 batch-29 = 3.930076491087675e-05

Training epoch-29 batch-30
Running loss of epoch-29 batch-30 = 9.144539944827557e-06

Training epoch-29 batch-31
Running loss of epoch-29 batch-31 = 2.7017667889595032e-05

Training epoch-29 batch-32
Running loss of epoch-29 batch-32 = 5.010748282074928e-06

Training epoch-29 batch-33
Running loss of epoch-29 batch-33 = 1.3807904906570911e-05

Training epoch-29 batch-34
Running loss of epoch-29 batch-34 = 5.972106009721756e-06

Training epoch-29 batch-35
Running loss of epoch-29 batch-35 = 1.1061900295317173e-05

Training epoch-29 batch-36
Running loss of epoch-29 batch-36 = 9.100069291889668e-06

Training epoch-29 batch-37
Running loss of epoch-29 batch-37 = 1.1363066732883453e-05

Training epoch-29 batch-38
Running loss of epoch-29 batch-38 = 7.09539745002985e-06

Training epoch-29 batch-39
Running loss of epoch-29 batch-39 = 6.657961057499051e-05

Training epoch-29 batch-40
Running loss of epoch-29 batch-40 = 4.242756403982639e-06

Training epoch-29 batch-41
Running loss of epoch-29 batch-41 = 2.1559186279773712e-05

Training epoch-29 batch-42
Running loss of epoch-29 batch-42 = 9.230687282979488e-06

Training epoch-29 batch-43
Running loss of epoch-29 batch-43 = 1.5980680473148823e-05

Training epoch-29 batch-44
Running loss of epoch-29 batch-44 = 5.847425200045109e-06

Training epoch-29 batch-45
Running loss of epoch-29 batch-45 = 1.0177143849432468e-05

Training epoch-29 batch-46
Running loss of epoch-29 batch-46 = 1.2761796824634075e-05

Training epoch-29 batch-47
Running loss of epoch-29 batch-47 = 1.0348157957196236e-05

Training epoch-29 batch-48
Running loss of epoch-29 batch-48 = 3.7595455069094896e-05

Training epoch-29 batch-49
Running loss of epoch-29 batch-49 = 2.1308893337845802e-05

Training epoch-29 batch-50
Running loss of epoch-29 batch-50 = 1.691281795501709e-05

Training epoch-29 batch-51
Running loss of epoch-29 batch-51 = 1.8986640498042107e-05

Training epoch-29 batch-52
Running loss of epoch-29 batch-52 = 1.141976099461317e-05

Training epoch-29 batch-53
Running loss of epoch-29 batch-53 = 1.011101994663477e-05

Training epoch-29 batch-54
Running loss of epoch-29 batch-54 = 8.449656888842583e-06

Training epoch-29 batch-55
Running loss of epoch-29 batch-55 = 2.4306238628923893e-05

Training epoch-29 batch-56
Running loss of epoch-29 batch-56 = 1.2457836419343948e-05

Training epoch-29 batch-57
Running loss of epoch-29 batch-57 = 6.591202691197395e-06

Training epoch-29 batch-58
Running loss of epoch-29 batch-58 = 2.1668383851647377e-05

Training epoch-29 batch-59
Running loss of epoch-29 batch-59 = 9.45397187024355e-06

Training epoch-29 batch-60
Running loss of epoch-29 batch-60 = 1.1393916793167591e-05

Training epoch-29 batch-61
Running loss of epoch-29 batch-61 = 2.0157545804977417e-05

Training epoch-29 batch-62
Running loss of epoch-29 batch-62 = 1.1103227734565735e-05

Training epoch-29 batch-63
Running loss of epoch-29 batch-63 = 1.2706615962088108e-05

Training epoch-29 batch-64
Running loss of epoch-29 batch-64 = 1.2011732906103134e-05

Training epoch-29 batch-65
Running loss of epoch-29 batch-65 = 1.6293255612254143e-05

Training epoch-29 batch-66
Running loss of epoch-29 batch-66 = 7.75931403040886e-06

Training epoch-29 batch-67
Running loss of epoch-29 batch-67 = 7.4070412665605545e-06

Training epoch-29 batch-68
Running loss of epoch-29 batch-68 = 1.57160684466362e-05

Training epoch-29 batch-69
Running loss of epoch-29 batch-69 = 9.110546670854092e-06

Training epoch-29 batch-70
Running loss of epoch-29 batch-70 = 3.196648322045803e-06

Training epoch-29 batch-71
Running loss of epoch-29 batch-71 = 0.00011006637942045927

Training epoch-29 batch-72
Running loss of epoch-29 batch-72 = 8.023576810956001e-06

Training epoch-29 batch-73
Running loss of epoch-29 batch-73 = 1.5427940525114536e-05

Training epoch-29 batch-74
Running loss of epoch-29 batch-74 = 1.3713957741856575e-05

Training epoch-29 batch-75
Running loss of epoch-29 batch-75 = 1.7714453861117363e-05

Training epoch-29 batch-76
Running loss of epoch-29 batch-76 = 2.3103668354451656e-05

Training epoch-29 batch-77
Running loss of epoch-29 batch-77 = 6.754533387720585e-06

Training epoch-29 batch-78
Running loss of epoch-29 batch-78 = 7.953145541250706e-06

Training epoch-29 batch-79
Running loss of epoch-29 batch-79 = 6.945570930838585e-06

Training epoch-29 batch-80
Running loss of epoch-29 batch-80 = 4.647998139262199e-06

Training epoch-29 batch-81
Running loss of epoch-29 batch-81 = 1.9802944734692574e-05

Training epoch-29 batch-82
Running loss of epoch-29 batch-82 = 4.480627831071615e-05

Training epoch-29 batch-83
Running loss of epoch-29 batch-83 = 1.413689460605383e-05

Training epoch-29 batch-84
Running loss of epoch-29 batch-84 = 9.334180504083633e-06

Training epoch-29 batch-85
Running loss of epoch-29 batch-85 = 1.675786916166544e-05

Training epoch-29 batch-86
Running loss of epoch-29 batch-86 = 7.3320697993040085e-06

Training epoch-29 batch-87
Running loss of epoch-29 batch-87 = 9.2705013230443e-06

Training epoch-29 batch-88
Running loss of epoch-29 batch-88 = 1.2263422831892967e-05

Training epoch-29 batch-89
Running loss of epoch-29 batch-89 = 5.030975444242358e-05

Training epoch-29 batch-90
Running loss of epoch-29 batch-90 = 2.846017014235258e-05

Training epoch-29 batch-91
Running loss of epoch-29 batch-91 = 9.0734101831913e-06

Training epoch-29 batch-92
Running loss of epoch-29 batch-92 = 3.649911377578974e-05

Training epoch-29 batch-93
Running loss of epoch-29 batch-93 = 4.13660891354084e-05

Training epoch-29 batch-94
Running loss of epoch-29 batch-94 = 7.105409167706966e-06

Training epoch-29 batch-95
Running loss of epoch-29 batch-95 = 5.674548447132111e-06

Training epoch-29 batch-96
Running loss of epoch-29 batch-96 = 9.262003004550934e-06

Training epoch-29 batch-97
Running loss of epoch-29 batch-97 = 4.11132350564003e-06

Training epoch-29 batch-98
Running loss of epoch-29 batch-98 = 1.4980090782046318e-05

Training epoch-29 batch-99
Running loss of epoch-29 batch-99 = 1.4585326425731182e-05

Training epoch-29 batch-100
Running loss of epoch-29 batch-100 = 1.495948527008295e-05

Training epoch-29 batch-101
Running loss of epoch-29 batch-101 = 1.247180625796318e-05

Training epoch-29 batch-102
Running loss of epoch-29 batch-102 = 6.256392225623131e-06

Training epoch-29 batch-103
Running loss of epoch-29 batch-103 = 5.276873707771301e-06

Training epoch-29 batch-104
Running loss of epoch-29 batch-104 = 6.289919838309288e-06

Training epoch-29 batch-105
Running loss of epoch-29 batch-105 = 1.240463461726904e-05

Training epoch-29 batch-106
Running loss of epoch-29 batch-106 = 3.6496901884675026e-05

Training epoch-29 batch-107
Running loss of epoch-29 batch-107 = 5.865260027348995e-05

Training epoch-29 batch-108
Running loss of epoch-29 batch-108 = 1.0269577614963055e-05

Training epoch-29 batch-109
Running loss of epoch-29 batch-109 = 5.757203325629234e-06

Training epoch-29 batch-110
Running loss of epoch-29 batch-110 = 8.015194907784462e-06

Training epoch-29 batch-111
Running loss of epoch-29 batch-111 = 5.230773240327835e-06

Training epoch-29 batch-112
Running loss of epoch-29 batch-112 = 6.665242835879326e-06

Training epoch-29 batch-113
Running loss of epoch-29 batch-113 = 3.560306504368782e-05

Training epoch-29 batch-114
Running loss of epoch-29 batch-114 = 7.854658178985119e-06

Training epoch-29 batch-115
Running loss of epoch-29 batch-115 = 1.1010910384356976e-05

Training epoch-29 batch-116
Running loss of epoch-29 batch-116 = 3.5745324566960335e-06

Training epoch-29 batch-117
Running loss of epoch-29 batch-117 = 1.8450780771672726e-05

Training epoch-29 batch-118
Running loss of epoch-29 batch-118 = 9.402050636708736e-06

Training epoch-29 batch-119
Running loss of epoch-29 batch-119 = 4.820246249437332e-05

Training epoch-29 batch-120
Running loss of epoch-29 batch-120 = 1.0752235539257526e-05

Training epoch-29 batch-121
Running loss of epoch-29 batch-121 = 4.7300709411501884e-06

Training epoch-29 batch-122
Running loss of epoch-29 batch-122 = 1.1717318557202816e-05

Training epoch-29 batch-123
Running loss of epoch-29 batch-123 = 5.6901946663856506e-05

Training epoch-29 batch-124
Running loss of epoch-29 batch-124 = 1.7417711205780506e-05

Training epoch-29 batch-125
Running loss of epoch-29 batch-125 = 7.11437314748764e-06

Training epoch-29 batch-126
Running loss of epoch-29 batch-126 = 6.991554982960224e-06

Training epoch-29 batch-127
Running loss of epoch-29 batch-127 = 2.1722516976296902e-05

Training epoch-29 batch-128
Running loss of epoch-29 batch-128 = 2.0911102183163166e-05

Training epoch-29 batch-129
Running loss of epoch-29 batch-129 = 6.924616172909737e-06

Training epoch-29 batch-130
Running loss of epoch-29 batch-130 = 3.43413557857275e-06

Training epoch-29 batch-131
Running loss of epoch-29 batch-131 = 1.2960867024958134e-05

Training epoch-29 batch-132
Running loss of epoch-29 batch-132 = 3.864290192723274e-06

Training epoch-29 batch-133
Running loss of epoch-29 batch-133 = 1.3464014045894146e-05

Training epoch-29 batch-134
Running loss of epoch-29 batch-134 = 1.1701020412147045e-05

Training epoch-29 batch-135
Running loss of epoch-29 batch-135 = 1.3233395293354988e-05

Training epoch-29 batch-136
Running loss of epoch-29 batch-136 = 1.1075753718614578e-05

Training epoch-29 batch-137
Running loss of epoch-29 batch-137 = 2.519018016755581e-05

Training epoch-29 batch-138
Running loss of epoch-29 batch-138 = 3.359315451234579e-05

Training epoch-29 batch-139
Running loss of epoch-29 batch-139 = 5.045323632657528e-06

Training epoch-29 batch-140
Running loss of epoch-29 batch-140 = 3.753812052309513e-06

Training epoch-29 batch-141
Running loss of epoch-29 batch-141 = 5.9668614994734526e-05

Training epoch-29 batch-142
Running loss of epoch-29 batch-142 = 1.3258308172225952e-05

Training epoch-29 batch-143
Running loss of epoch-29 batch-143 = 4.0613929741084576e-05

Training epoch-29 batch-144
Running loss of epoch-29 batch-144 = 1.1525582522153854e-05

Training epoch-29 batch-145
Running loss of epoch-29 batch-145 = 0.00014103198191151023

Training epoch-29 batch-146
Running loss of epoch-29 batch-146 = 1.219531986862421e-05

Training epoch-29 batch-147
Running loss of epoch-29 batch-147 = 2.20051733776927e-05

Training epoch-29 batch-148
Running loss of epoch-29 batch-148 = 0.00010803277837112546

Training epoch-29 batch-149
Running loss of epoch-29 batch-149 = 1.4660414308309555e-05

Training epoch-29 batch-150
Running loss of epoch-29 batch-150 = 7.981318049132824e-06

Training epoch-29 batch-151
Running loss of epoch-29 batch-151 = 3.1241332180798054e-05

Training epoch-29 batch-152
Running loss of epoch-29 batch-152 = 1.5555182471871376e-05

Training epoch-29 batch-153
Running loss of epoch-29 batch-153 = 1.4502438716590405e-05

Training epoch-29 batch-154
Running loss of epoch-29 batch-154 = 4.7966663260012865e-05

Training epoch-29 batch-155
Running loss of epoch-29 batch-155 = 1.3062963262200356e-05

Training epoch-29 batch-156
Running loss of epoch-29 batch-156 = 2.42976238951087e-05

Training epoch-29 batch-157
Running loss of epoch-29 batch-157 = 6.604567170143127e-05

Finished training epoch-29.



Average train loss at epoch-29 = 1.8040600419044493e-05

Started Evaluation

Average val loss at epoch-29 = 4.8236573652217265

Accuracy for classes:
Accuracy for class equals is: 63.37 %
Accuracy for class main is: 46.56 %
Accuracy for class setUp is: 52.79 %
Accuracy for class onCreate is: 32.84 %
Accuracy for class toString is: 40.27 %
Accuracy for class run is: 30.14 %
Accuracy for class hashCode is: 74.91 %
Accuracy for class init is: 28.48 %
Accuracy for class execute is: 18.88 %
Accuracy for class get is: 26.67 %

Overall Accuracy = 41.80 %

Finished Evaluation



Started training epoch-30


Training epoch-30 batch-1
Running loss of epoch-30 batch-1 = 6.358139216899872e-06

Training epoch-30 batch-2
Running loss of epoch-30 batch-2 = 1.232221256941557e-05

Training epoch-30 batch-3
Running loss of epoch-30 batch-3 = 1.4615943655371666e-05

Training epoch-30 batch-4
Running loss of epoch-30 batch-4 = 3.2240059226751328e-06

Training epoch-30 batch-5
Running loss of epoch-30 batch-5 = 2.0807958208024502e-05

Training epoch-30 batch-6
Running loss of epoch-30 batch-6 = 1.2455042451620102e-05

Training epoch-30 batch-7
Running loss of epoch-30 batch-7 = 6.57583586871624e-06

Training epoch-30 batch-8
Running loss of epoch-30 batch-8 = 7.80331902205944e-06

Training epoch-30 batch-9
Running loss of epoch-30 batch-9 = 1.3088691048324108e-05

Training epoch-30 batch-10
Running loss of epoch-30 batch-10 = 3.413169179111719e-05

Training epoch-30 batch-11
Running loss of epoch-30 batch-11 = 1.7524114809930325e-05

Training epoch-30 batch-12
Running loss of epoch-30 batch-12 = 1.808872912079096e-05

Training epoch-30 batch-13
Running loss of epoch-30 batch-13 = 3.8116704672574997e-06

Training epoch-30 batch-14
Running loss of epoch-30 batch-14 = 1.0270858183503151e-05

Training epoch-30 batch-15
Running loss of epoch-30 batch-15 = 1.3364246115088463e-05

Training epoch-30 batch-16
Running loss of epoch-30 batch-16 = 8.091796189546585e-06

Training epoch-30 batch-17
Running loss of epoch-30 batch-17 = 2.332869917154312e-05

Training epoch-30 batch-18
Running loss of epoch-30 batch-18 = 7.982132956385612e-06

Training epoch-30 batch-19
Running loss of epoch-30 batch-19 = 5.6466611567884684e-05

Training epoch-30 batch-20
Running loss of epoch-30 batch-20 = 1.0143150575459003e-05

Training epoch-30 batch-21
Running loss of epoch-30 batch-21 = 3.0162744224071503e-05

Training epoch-30 batch-22
Running loss of epoch-30 batch-22 = 1.0281568393111229e-05

Training epoch-30 batch-23
Running loss of epoch-30 batch-23 = 8.056173101067543e-06

Training epoch-30 batch-24
Running loss of epoch-30 batch-24 = 3.988289972767234e-05

Training epoch-30 batch-25
Running loss of epoch-30 batch-25 = 1.6386969946324825e-05

Training epoch-30 batch-26
Running loss of epoch-30 batch-26 = 7.502967491745949e-06

Training epoch-30 batch-27
Running loss of epoch-30 batch-27 = 1.585809513926506e-05

Training epoch-30 batch-28
Running loss of epoch-30 batch-28 = 9.981333278119564e-06

Training epoch-30 batch-29
Running loss of epoch-30 batch-29 = 1.3667391613125801e-05

Training epoch-30 batch-30
Running loss of epoch-30 batch-30 = 1.1761556379497051e-05

Training epoch-30 batch-31
Running loss of epoch-30 batch-31 = 7.77258537709713e-06

Training epoch-30 batch-32
Running loss of epoch-30 batch-32 = 1.0260846465826035e-05

Training epoch-30 batch-33
Running loss of epoch-30 batch-33 = 1.2196716852486134e-05

Training epoch-30 batch-34
Running loss of epoch-30 batch-34 = 6.028101779520512e-06

Training epoch-30 batch-35
Running loss of epoch-30 batch-35 = 3.40945553034544e-06

Training epoch-30 batch-36
Running loss of epoch-30 batch-36 = 1.0547926649451256e-05

Training epoch-30 batch-37
Running loss of epoch-30 batch-37 = 6.6213542595505714e-06

Training epoch-30 batch-38
Running loss of epoch-30 batch-38 = 4.227738827466965e-06

Training epoch-30 batch-39
Running loss of epoch-30 batch-39 = 1.5816069208085537e-05

Training epoch-30 batch-40
Running loss of epoch-30 batch-40 = 3.900495357811451e-06

Training epoch-30 batch-41
Running loss of epoch-30 batch-41 = 9.316718205809593e-06

Training epoch-30 batch-42
Running loss of epoch-30 batch-42 = 1.2150616385042667e-05

Training epoch-30 batch-43
Running loss of epoch-30 batch-43 = 8.235685527324677e-06

Training epoch-30 batch-44
Running loss of epoch-30 batch-44 = 2.249924000352621e-05

Training epoch-30 batch-45
Running loss of epoch-30 batch-45 = 2.0902371034026146e-05

Training epoch-30 batch-46
Running loss of epoch-30 batch-46 = 2.1914253011345863e-05

Training epoch-30 batch-47
Running loss of epoch-30 batch-47 = 1.2001139111816883e-05

Training epoch-30 batch-48
Running loss of epoch-30 batch-48 = 1.3683689758181572e-05

Training epoch-30 batch-49
Running loss of epoch-30 batch-49 = 1.1767493560910225e-05

Training epoch-30 batch-50
Running loss of epoch-30 batch-50 = 6.700283847749233e-06

Training epoch-30 batch-51
Running loss of epoch-30 batch-51 = 3.6027398891747e-05

Training epoch-30 batch-52
Running loss of epoch-30 batch-52 = 9.560491889715195e-06

Training epoch-30 batch-53
Running loss of epoch-30 batch-53 = 3.802677383646369e-05

Training epoch-30 batch-54
Running loss of epoch-30 batch-54 = 3.644695971161127e-05

Training epoch-30 batch-55
Running loss of epoch-30 batch-55 = 3.1581148505210876e-06

Training epoch-30 batch-56
Running loss of epoch-30 batch-56 = 1.0128947906196117e-05

Training epoch-30 batch-57
Running loss of epoch-30 batch-57 = 1.140916720032692e-05

Training epoch-30 batch-58
Running loss of epoch-30 batch-58 = 3.869645297527313e-06

Training epoch-30 batch-59
Running loss of epoch-30 batch-59 = 3.24880238622427e-06

Training epoch-30 batch-60
Running loss of epoch-30 batch-60 = 1.808756496757269e-05

Training epoch-30 batch-61
Running loss of epoch-30 batch-61 = 5.7336874306201935e-06

Training epoch-30 batch-62
Running loss of epoch-30 batch-62 = 9.09843947738409e-06

Training epoch-30 batch-63
Running loss of epoch-30 batch-63 = 4.37756534665823e-06

Training epoch-30 batch-64
Running loss of epoch-30 batch-64 = 1.3546785339713097e-05

Training epoch-30 batch-65
Running loss of epoch-30 batch-65 = 4.420243203639984e-05

Training epoch-30 batch-66
Running loss of epoch-30 batch-66 = 7.946992991492152e-05

Training epoch-30 batch-67
Running loss of epoch-30 batch-67 = 7.827184163033962e-06

Training epoch-30 batch-68
Running loss of epoch-30 batch-68 = 1.450337003916502e-05

Training epoch-30 batch-69
Running loss of epoch-30 batch-69 = 3.3578835427761078e-06

Training epoch-30 batch-70
Running loss of epoch-30 batch-70 = 4.0405429899692535e-06

Training epoch-30 batch-71
Running loss of epoch-30 batch-71 = 2.1929503418505192e-05

Training epoch-30 batch-72
Running loss of epoch-30 batch-72 = 3.972090780735016e-06

Training epoch-30 batch-73
Running loss of epoch-30 batch-73 = 2.113962545990944e-05

Training epoch-30 batch-74
Running loss of epoch-30 batch-74 = 2.224626950919628e-05

Training epoch-30 batch-75
Running loss of epoch-30 batch-75 = 2.453883644193411e-05

Training epoch-30 batch-76
Running loss of epoch-30 batch-76 = 1.1154101230204105e-05

Training epoch-30 batch-77
Running loss of epoch-30 batch-77 = 1.10996188595891e-05

Training epoch-30 batch-78
Running loss of epoch-30 batch-78 = 1.6180798411369324e-05

Training epoch-30 batch-79
Running loss of epoch-30 batch-79 = 9.223236702382565e-06

Training epoch-30 batch-80
Running loss of epoch-30 batch-80 = 1.4196150004863739e-05

Training epoch-30 batch-81
Running loss of epoch-30 batch-81 = 1.0111485607922077e-05

Training epoch-30 batch-82
Running loss of epoch-30 batch-82 = 8.139759302139282e-06

Training epoch-30 batch-83
Running loss of epoch-30 batch-83 = 1.0378193110227585e-05

Training epoch-30 batch-84
Running loss of epoch-30 batch-84 = 1.9674422219395638e-05

Training epoch-30 batch-85
Running loss of epoch-30 batch-85 = 6.269081495702267e-06

Training epoch-30 batch-86
Running loss of epoch-30 batch-86 = 9.162467904388905e-06

Training epoch-30 batch-87
Running loss of epoch-30 batch-87 = 6.213551387190819e-06

Training epoch-30 batch-88
Running loss of epoch-30 batch-88 = 4.140764940530062e-05

Training epoch-30 batch-89
Running loss of epoch-30 batch-89 = 5.301903001964092e-06

Training epoch-30 batch-90
Running loss of epoch-30 batch-90 = 1.1519063264131546e-05

Training epoch-30 batch-91
Running loss of epoch-30 batch-91 = 5.366047844290733e-06

Training epoch-30 batch-92
Running loss of epoch-30 batch-92 = 9.27923247218132e-06

Training epoch-30 batch-93
Running loss of epoch-30 batch-93 = 1.2571807019412518e-05

Training epoch-30 batch-94
Running loss of epoch-30 batch-94 = 3.196531906723976e-06

Training epoch-30 batch-95
Running loss of epoch-30 batch-95 = 5.870824679732323e-06

Training epoch-30 batch-96
Running loss of epoch-30 batch-96 = 0.00010722677689045668

Training epoch-30 batch-97
Running loss of epoch-30 batch-97 = 7.830909453332424e-06

Training epoch-30 batch-98
Running loss of epoch-30 batch-98 = 4.8514921218156815e-06

Training epoch-30 batch-99
Running loss of epoch-30 batch-99 = 0.00012573617277666926

Training epoch-30 batch-100
Running loss of epoch-30 batch-100 = 4.075001925230026e-06

Training epoch-30 batch-101
Running loss of epoch-30 batch-101 = 2.0444625988602638e-05

Training epoch-30 batch-102
Running loss of epoch-30 batch-102 = 1.2187520042061806e-05

Training epoch-30 batch-103
Running loss of epoch-30 batch-103 = 1.1777039617300034e-05

Training epoch-30 batch-104
Running loss of epoch-30 batch-104 = 1.8069520592689514e-05

Training epoch-30 batch-105
Running loss of epoch-30 batch-105 = 1.1104857549071312e-05

Training epoch-30 batch-106
Running loss of epoch-30 batch-106 = 1.4266697689890862e-05

Training epoch-30 batch-107
Running loss of epoch-30 batch-107 = 4.701549187302589e-06

Training epoch-30 batch-108
Running loss of epoch-30 batch-108 = 1.1660507880151272e-05

Training epoch-30 batch-109
Running loss of epoch-30 batch-109 = 6.534857675433159e-06

Training epoch-30 batch-110
Running loss of epoch-30 batch-110 = 2.618029247969389e-05

Training epoch-30 batch-111
Running loss of epoch-30 batch-111 = 4.887813702225685e-06

Training epoch-30 batch-112
Running loss of epoch-30 batch-112 = 4.044035449624062e-06

Training epoch-30 batch-113
Running loss of epoch-30 batch-113 = 6.095040589570999e-06

Training epoch-30 batch-114
Running loss of epoch-30 batch-114 = 1.2336880899965763e-05

Training epoch-30 batch-115
Running loss of epoch-30 batch-115 = 8.076196536421776e-06

Training epoch-30 batch-116
Running loss of epoch-30 batch-116 = 6.497139111161232e-06

Training epoch-30 batch-117
Running loss of epoch-30 batch-117 = 2.0617851987481117e-05

Training epoch-30 batch-118
Running loss of epoch-30 batch-118 = 1.995009370148182e-05

Training epoch-30 batch-119
Running loss of epoch-30 batch-119 = 2.7759931981563568e-05

Training epoch-30 batch-120
Running loss of epoch-30 batch-120 = 1.4317804016172886e-05

Training epoch-30 batch-121
Running loss of epoch-30 batch-121 = 8.30052886158228e-06

Training epoch-30 batch-122
Running loss of epoch-30 batch-122 = 7.705879397690296e-06

Training epoch-30 batch-123
Running loss of epoch-30 batch-123 = 2.19360226765275e-05

Training epoch-30 batch-124
Running loss of epoch-30 batch-124 = 8.610659278929234e-06

Training epoch-30 batch-125
Running loss of epoch-30 batch-125 = 9.64535865932703e-06

Training epoch-30 batch-126
Running loss of epoch-30 batch-126 = 7.170950993895531e-06

Training epoch-30 batch-127
Running loss of epoch-30 batch-127 = 5.356170004233718e-05

Training epoch-30 batch-128
Running loss of epoch-30 batch-128 = 7.495749741792679e-06

Training epoch-30 batch-129
Running loss of epoch-30 batch-129 = 2.763152588158846e-05

Training epoch-30 batch-130
Running loss of epoch-30 batch-130 = 1.754285767674446e-05

Training epoch-30 batch-131
Running loss of epoch-30 batch-131 = 3.052310785278678e-05

Training epoch-30 batch-132
Running loss of epoch-30 batch-132 = 2.544739982113242e-05

Training epoch-30 batch-133
Running loss of epoch-30 batch-133 = 2.3581902496516705e-05

Training epoch-30 batch-134
Running loss of epoch-30 batch-134 = 1.7273123376071453e-05

Training epoch-30 batch-135
Running loss of epoch-30 batch-135 = 8.45734030008316e-06

Training epoch-30 batch-136
Running loss of epoch-30 batch-136 = 2.10409052670002e-06

Training epoch-30 batch-137
Running loss of epoch-30 batch-137 = 5.427049472928047e-06

Training epoch-30 batch-138
Running loss of epoch-30 batch-138 = 6.59178476780653e-06

Training epoch-30 batch-139
Running loss of epoch-30 batch-139 = 7.669790647923946e-06

Training epoch-30 batch-140
Running loss of epoch-30 batch-140 = 8.520903065800667e-06

Training epoch-30 batch-141
Running loss of epoch-30 batch-141 = 5.4996926337480545e-06

Training epoch-30 batch-142
Running loss of epoch-30 batch-142 = 4.560570232570171e-06

Training epoch-30 batch-143
Running loss of epoch-30 batch-143 = 1.8031918443739414e-05

Training epoch-30 batch-144
Running loss of epoch-30 batch-144 = 1.075933687388897e-05

Training epoch-30 batch-145
Running loss of epoch-30 batch-145 = 5.2317045629024506e-06

Training epoch-30 batch-146
Running loss of epoch-30 batch-146 = 1.391314435750246e-05

Training epoch-30 batch-147
Running loss of epoch-30 batch-147 = 1.0026036761701107e-05

Training epoch-30 batch-148
Running loss of epoch-30 batch-148 = 1.4183809980750084e-05

Training epoch-30 batch-149
Running loss of epoch-30 batch-149 = 3.234145697206259e-05

Training epoch-30 batch-150
Running loss of epoch-30 batch-150 = 1.0107760317623615e-05

Training epoch-30 batch-151
Running loss of epoch-30 batch-151 = 2.257560845464468e-05

Training epoch-30 batch-152
Running loss of epoch-30 batch-152 = 7.487135007977486e-06

Training epoch-30 batch-153
Running loss of epoch-30 batch-153 = 5.7684199418872595e-05

Training epoch-30 batch-154
Running loss of epoch-30 batch-154 = 2.935994416475296e-05

Training epoch-30 batch-155
Running loss of epoch-30 batch-155 = 3.158336039632559e-05

Training epoch-30 batch-156
Running loss of epoch-30 batch-156 = 2.2512394934892654e-05

Training epoch-30 batch-157
Running loss of epoch-30 batch-157 = 0.0003903098404407501

Finished training epoch-30.



Average train loss at epoch-30 = 1.644849367439747e-05

Started Evaluation

Average val loss at epoch-30 = 4.861244885545028

Accuracy for classes:
Accuracy for class equals is: 62.71 %
Accuracy for class main is: 45.41 %
Accuracy for class setUp is: 51.31 %
Accuracy for class onCreate is: 34.54 %
Accuracy for class toString is: 41.30 %
Accuracy for class run is: 30.82 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 17.67 %
Accuracy for class get is: 26.15 %

Overall Accuracy = 41.88 %

Finished Evaluation



Started training epoch-31


Training epoch-31 batch-1
Running loss of epoch-31 batch-1 = 3.422028385102749e-06

Training epoch-31 batch-2
Running loss of epoch-31 batch-2 = 8.810777217149734e-06

Training epoch-31 batch-3
Running loss of epoch-31 batch-3 = 6.431364454329014e-06

Training epoch-31 batch-4
Running loss of epoch-31 batch-4 = 7.212162017822266e-06

Training epoch-31 batch-5
Running loss of epoch-31 batch-5 = 7.93475192040205e-06

Training epoch-31 batch-6
Running loss of epoch-31 batch-6 = 2.061540726572275e-05

Training epoch-31 batch-7
Running loss of epoch-31 batch-7 = 1.1046882718801498e-05

Training epoch-31 batch-8
Running loss of epoch-31 batch-8 = 4.87244687974453e-06

Training epoch-31 batch-9
Running loss of epoch-31 batch-9 = 1.021998468786478e-05

Training epoch-31 batch-10
Running loss of epoch-31 batch-10 = 6.909715011715889e-06

Training epoch-31 batch-11
Running loss of epoch-31 batch-11 = 3.7282006815075874e-06

Training epoch-31 batch-12
Running loss of epoch-31 batch-12 = 1.0227551683783531e-05

Training epoch-31 batch-13
Running loss of epoch-31 batch-13 = 2.8831418603658676e-06

Training epoch-31 batch-14
Running loss of epoch-31 batch-14 = 2.740544732660055e-05

Training epoch-31 batch-15
Running loss of epoch-31 batch-15 = 2.317421603947878e-05

Training epoch-31 batch-16
Running loss of epoch-31 batch-16 = 2.139818388968706e-05

Training epoch-31 batch-17
Running loss of epoch-31 batch-17 = 7.972936145961285e-06

Training epoch-31 batch-18
Running loss of epoch-31 batch-18 = 7.729663047939539e-05

Training epoch-31 batch-19
Running loss of epoch-31 batch-19 = 1.7749029211699963e-05

Training epoch-31 batch-20
Running loss of epoch-31 batch-20 = 4.184315912425518e-06

Training epoch-31 batch-21
Running loss of epoch-31 batch-21 = 4.834262654185295e-06

Training epoch-31 batch-22
Running loss of epoch-31 batch-22 = 1.4436198398470879e-05

Training epoch-31 batch-23
Running loss of epoch-31 batch-23 = 1.675903331488371e-05

Training epoch-31 batch-24
Running loss of epoch-31 batch-24 = 8.956994861364365e-06

Training epoch-31 batch-25
Running loss of epoch-31 batch-25 = 3.1746458262205124e-06

Training epoch-31 batch-26
Running loss of epoch-31 batch-26 = 1.2282514944672585e-05

Training epoch-31 batch-27
Running loss of epoch-31 batch-27 = 1.225422602146864e-05

Training epoch-31 batch-28
Running loss of epoch-31 batch-28 = 2.5793095119297504e-05

Training epoch-31 batch-29
Running loss of epoch-31 batch-29 = 7.66816083341837e-06

Training epoch-31 batch-30
Running loss of epoch-31 batch-30 = 8.405186235904694e-06

Training epoch-31 batch-31
Running loss of epoch-31 batch-31 = 4.084780812263489e-06

Training epoch-31 batch-32
Running loss of epoch-31 batch-32 = 1.4758436009287834e-05

Training epoch-31 batch-33
Running loss of epoch-31 batch-33 = 7.919850759208202e-06

Training epoch-31 batch-34
Running loss of epoch-31 batch-34 = 8.771935245022178e-05

Training epoch-31 batch-35
Running loss of epoch-31 batch-35 = 9.027193300426006e-06

Training epoch-31 batch-36
Running loss of epoch-31 batch-36 = 2.565572503954172e-05

Training epoch-31 batch-37
Running loss of epoch-31 batch-37 = 7.480615749955177e-06

Training epoch-31 batch-38
Running loss of epoch-31 batch-38 = 8.141156286001205e-06

Training epoch-31 batch-39
Running loss of epoch-31 batch-39 = 8.875387720763683e-06

Training epoch-31 batch-40
Running loss of epoch-31 batch-40 = 4.067202098667622e-06

Training epoch-31 batch-41
Running loss of epoch-31 batch-41 = 5.334150046110153e-06

Training epoch-31 batch-42
Running loss of epoch-31 batch-42 = 3.4638214856386185e-06

Training epoch-31 batch-43
Running loss of epoch-31 batch-43 = 4.076864570379257e-06

Training epoch-31 batch-44
Running loss of epoch-31 batch-44 = 9.938492439687252e-06

Training epoch-31 batch-45
Running loss of epoch-31 batch-45 = 5.248968955129385e-05

Training epoch-31 batch-46
Running loss of epoch-31 batch-46 = 5.702953785657883e-06

Training epoch-31 batch-47
Running loss of epoch-31 batch-47 = 4.100380465388298e-06

Training epoch-31 batch-48
Running loss of epoch-31 batch-48 = 3.814464434981346e-06

Training epoch-31 batch-49
Running loss of epoch-31 batch-49 = 2.3851171135902405e-06

Training epoch-31 batch-50
Running loss of epoch-31 batch-50 = 8.584815077483654e-06

Training epoch-31 batch-51
Running loss of epoch-31 batch-51 = 6.7623332142829895e-06

Training epoch-31 batch-52
Running loss of epoch-31 batch-52 = 1.2526288628578186e-05

Training epoch-31 batch-53
Running loss of epoch-31 batch-53 = 2.1766754798591137e-05

Training epoch-31 batch-54
Running loss of epoch-31 batch-54 = 1.0803109034895897e-05

Training epoch-31 batch-55
Running loss of epoch-31 batch-55 = 7.354537956416607e-06

Training epoch-31 batch-56
Running loss of epoch-31 batch-56 = 3.6940909922122955e-06

Training epoch-31 batch-57
Running loss of epoch-31 batch-57 = 4.2437168303877115e-05

Training epoch-31 batch-58
Running loss of epoch-31 batch-58 = 5.011679604649544e-06

Training epoch-31 batch-59
Running loss of epoch-31 batch-59 = 9.102514013648033e-06

Training epoch-31 batch-60
Running loss of epoch-31 batch-60 = 1.6440637409687042e-05

Training epoch-31 batch-61
Running loss of epoch-31 batch-61 = 7.614842616021633e-06

Training epoch-31 batch-62
Running loss of epoch-31 batch-62 = 4.409812390804291e-06

Training epoch-31 batch-63
Running loss of epoch-31 batch-63 = 1.1456664651632309e-05

Training epoch-31 batch-64
Running loss of epoch-31 batch-64 = 7.056863978505135e-06

Training epoch-31 batch-65
Running loss of epoch-31 batch-65 = 1.4736317098140717e-05

Training epoch-31 batch-66
Running loss of epoch-31 batch-66 = 4.337867721915245e-06

Training epoch-31 batch-67
Running loss of epoch-31 batch-67 = 8.58574640005827e-06

Training epoch-31 batch-68
Running loss of epoch-31 batch-68 = 1.363246701657772e-05

Training epoch-31 batch-69
Running loss of epoch-31 batch-69 = 9.737210348248482e-06

Training epoch-31 batch-70
Running loss of epoch-31 batch-70 = 1.0428251698613167e-05

Training epoch-31 batch-71
Running loss of epoch-31 batch-71 = 5.1036360673606396e-05

Training epoch-31 batch-72
Running loss of epoch-31 batch-72 = 1.4477875083684921e-05

Training epoch-31 batch-73
Running loss of epoch-31 batch-73 = 2.8693117201328278e-05

Training epoch-31 batch-74
Running loss of epoch-31 batch-74 = 9.233132004737854e-06

Training epoch-31 batch-75
Running loss of epoch-31 batch-75 = 1.9352417439222336e-05

Training epoch-31 batch-76
Running loss of epoch-31 batch-76 = 8.824397809803486e-06

Training epoch-31 batch-77
Running loss of epoch-31 batch-77 = 1.1339783668518066e-05

Training epoch-31 batch-78
Running loss of epoch-31 batch-78 = 9.74372960627079e-06

Training epoch-31 batch-79
Running loss of epoch-31 batch-79 = 8.073169738054276e-06

Training epoch-31 batch-80
Running loss of epoch-31 batch-80 = 1.3334094546735287e-05

Training epoch-31 batch-81
Running loss of epoch-31 batch-81 = 2.5832676328718662e-05

Training epoch-31 batch-82
Running loss of epoch-31 batch-82 = 5.708076059818268e-06

Training epoch-31 batch-83
Running loss of epoch-31 batch-83 = 5.10411337018013e-06

Training epoch-31 batch-84
Running loss of epoch-31 batch-84 = 3.898167051374912e-06

Training epoch-31 batch-85
Running loss of epoch-31 batch-85 = 1.1454569175839424e-05

Training epoch-31 batch-86
Running loss of epoch-31 batch-86 = 1.6563222743570805e-05

Training epoch-31 batch-87
Running loss of epoch-31 batch-87 = 3.584427759051323e-06

Training epoch-31 batch-88
Running loss of epoch-31 batch-88 = 3.3288029953837395e-05

Training epoch-31 batch-89
Running loss of epoch-31 batch-89 = 9.746872819960117e-06

Training epoch-31 batch-90
Running loss of epoch-31 batch-90 = 8.317874744534492e-06

Training epoch-31 batch-91
Running loss of epoch-31 batch-91 = 1.0564806871116161e-05

Training epoch-31 batch-92
Running loss of epoch-31 batch-92 = 7.0373062044382095e-06

Training epoch-31 batch-93
Running loss of epoch-31 batch-93 = 8.320319466292858e-06

Training epoch-31 batch-94
Running loss of epoch-31 batch-94 = 6.696674972772598e-06

Training epoch-31 batch-95
Running loss of epoch-31 batch-95 = 6.991904228925705e-06

Training epoch-31 batch-96
Running loss of epoch-31 batch-96 = 1.8447870388627052e-05

Training epoch-31 batch-97
Running loss of epoch-31 batch-97 = 1.505296677350998e-05

Training epoch-31 batch-98
Running loss of epoch-31 batch-98 = 1.1114520020782948e-05

Training epoch-31 batch-99
Running loss of epoch-31 batch-99 = 8.851755410432816e-06

Training epoch-31 batch-100
Running loss of epoch-31 batch-100 = 1.686986070126295e-05

Training epoch-31 batch-101
Running loss of epoch-31 batch-101 = 4.506204277276993e-06

Training epoch-31 batch-102
Running loss of epoch-31 batch-102 = 5.457480438053608e-05

Training epoch-31 batch-103
Running loss of epoch-31 batch-103 = 3.200268838554621e-05

Training epoch-31 batch-104
Running loss of epoch-31 batch-104 = 2.7002766728401184e-05

Training epoch-31 batch-105
Running loss of epoch-31 batch-105 = 2.6189954951405525e-06

Training epoch-31 batch-106
Running loss of epoch-31 batch-106 = 3.9155129343271255e-06

Training epoch-31 batch-107
Running loss of epoch-31 batch-107 = 3.230769652873278e-05

Training epoch-31 batch-108
Running loss of epoch-31 batch-108 = 1.1720345355570316e-05

Training epoch-31 batch-109
Running loss of epoch-31 batch-109 = 2.176209818571806e-05

Training epoch-31 batch-110
Running loss of epoch-31 batch-110 = 8.2431361079216e-06

Training epoch-31 batch-111
Running loss of epoch-31 batch-111 = 1.1173891834914684e-05

Training epoch-31 batch-112
Running loss of epoch-31 batch-112 = 1.3089156709611416e-05

Training epoch-31 batch-113
Running loss of epoch-31 batch-113 = 9.77830495685339e-06

Training epoch-31 batch-114
Running loss of epoch-31 batch-114 = 6.007845513522625e-06

Training epoch-31 batch-115
Running loss of epoch-31 batch-115 = 4.23588789999485e-06

Training epoch-31 batch-116
Running loss of epoch-31 batch-116 = 1.3906159438192844e-05

Training epoch-31 batch-117
Running loss of epoch-31 batch-117 = 4.369765520095825e-06

Training epoch-31 batch-118
Running loss of epoch-31 batch-118 = 3.5511329770088196e-06

Training epoch-31 batch-119
Running loss of epoch-31 batch-119 = 1.545168925076723e-05

Training epoch-31 batch-120
Running loss of epoch-31 batch-120 = 7.428927347064018e-06

Training epoch-31 batch-121
Running loss of epoch-31 batch-121 = 9.11671668291092e-06

Training epoch-31 batch-122
Running loss of epoch-31 batch-122 = 1.0841526091098785e-05

Training epoch-31 batch-123
Running loss of epoch-31 batch-123 = 2.6895373594015837e-05

Training epoch-31 batch-124
Running loss of epoch-31 batch-124 = 1.9608414731919765e-05

Training epoch-31 batch-125
Running loss of epoch-31 batch-125 = 2.578448038548231e-05

Training epoch-31 batch-126
Running loss of epoch-31 batch-126 = 1.0422896593809128e-05

Training epoch-31 batch-127
Running loss of epoch-31 batch-127 = 6.408314220607281e-06

Training epoch-31 batch-128
Running loss of epoch-31 batch-128 = 6.832648068666458e-06

Training epoch-31 batch-129
Running loss of epoch-31 batch-129 = 5.912501364946365e-06

Training epoch-31 batch-130
Running loss of epoch-31 batch-130 = 1.2367148883640766e-05

Training epoch-31 batch-131
Running loss of epoch-31 batch-131 = 2.1812738850712776e-05

Training epoch-31 batch-132
Running loss of epoch-31 batch-132 = 7.790280506014824e-06

Training epoch-31 batch-133
Running loss of epoch-31 batch-133 = 1.2099160812795162e-05

Training epoch-31 batch-134
Running loss of epoch-31 batch-134 = 8.359551429748535e-06

Training epoch-31 batch-135
Running loss of epoch-31 batch-135 = 6.95449416525662e-05

Training epoch-31 batch-136
Running loss of epoch-31 batch-136 = 1.7756130546331406e-05

Training epoch-31 batch-137
Running loss of epoch-31 batch-137 = 1.0130461305379868e-05

Training epoch-31 batch-138
Running loss of epoch-31 batch-138 = 6.7873625084757805e-06

Training epoch-31 batch-139
Running loss of epoch-31 batch-139 = 4.271161742508411e-06

Training epoch-31 batch-140
Running loss of epoch-31 batch-140 = 1.270580105483532e-05

Training epoch-31 batch-141
Running loss of epoch-31 batch-141 = 6.878981366753578e-06

Training epoch-31 batch-142
Running loss of epoch-31 batch-142 = 1.5623518265783787e-05

Training epoch-31 batch-143
Running loss of epoch-31 batch-143 = 3.2747630029916763e-06

Training epoch-31 batch-144
Running loss of epoch-31 batch-144 = 5.115987733006477e-06

Training epoch-31 batch-145
Running loss of epoch-31 batch-145 = 5.527283065021038e-06

Training epoch-31 batch-146
Running loss of epoch-31 batch-146 = 4.014000296592712e-06

Training epoch-31 batch-147
Running loss of epoch-31 batch-147 = 0.00010806781938299537

Training epoch-31 batch-148
Running loss of epoch-31 batch-148 = 2.638273872435093e-05

Training epoch-31 batch-149
Running loss of epoch-31 batch-149 = 5.6138960644602776e-06

Training epoch-31 batch-150
Running loss of epoch-31 batch-150 = 3.488099901005626e-05

Training epoch-31 batch-151
Running loss of epoch-31 batch-151 = 4.564644768834114e-06

Training epoch-31 batch-152
Running loss of epoch-31 batch-152 = 1.122569665312767e-05

Training epoch-31 batch-153
Running loss of epoch-31 batch-153 = 8.991104550659657e-06

Training epoch-31 batch-154
Running loss of epoch-31 batch-154 = 3.8542377296835184e-05

Training epoch-31 batch-155
Running loss of epoch-31 batch-155 = 1.1534662917256355e-05

Training epoch-31 batch-156
Running loss of epoch-31 batch-156 = 3.092363476753235e-05

Training epoch-31 batch-157
Running loss of epoch-31 batch-157 = 7.100030779838562e-05

Finished training epoch-31.



Average train loss at epoch-31 = 1.4356070384383202e-05

Started Evaluation

Average val loss at epoch-31 = 4.831268015660737

Accuracy for classes:
Accuracy for class equals is: 62.05 %
Accuracy for class main is: 44.75 %
Accuracy for class setUp is: 50.16 %
Accuracy for class onCreate is: 37.21 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 29.91 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 26.23 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 26.41 %

Overall Accuracy = 41.86 %

Finished Evaluation



Started training epoch-32


Training epoch-32 batch-1
Running loss of epoch-32 batch-1 = 1.5287892892956734e-05

Training epoch-32 batch-2
Running loss of epoch-32 batch-2 = 1.757137943059206e-05

Training epoch-32 batch-3
Running loss of epoch-32 batch-3 = 6.647082045674324e-06

Training epoch-32 batch-4
Running loss of epoch-32 batch-4 = 5.554058589041233e-06

Training epoch-32 batch-5
Running loss of epoch-32 batch-5 = 1.3682292774319649e-05

Training epoch-32 batch-6
Running loss of epoch-32 batch-6 = 5.994690582156181e-06

Training epoch-32 batch-7
Running loss of epoch-32 batch-7 = 4.509929567575455e-06

Training epoch-32 batch-8
Running loss of epoch-32 batch-8 = 8.617294952273369e-06

Training epoch-32 batch-9
Running loss of epoch-32 batch-9 = 3.859284333884716e-06

Training epoch-32 batch-10
Running loss of epoch-32 batch-10 = 1.0902178473770618e-05

Training epoch-32 batch-11
Running loss of epoch-32 batch-11 = 2.5997404009103775e-05

Training epoch-32 batch-12
Running loss of epoch-32 batch-12 = 7.426482625305653e-06

Training epoch-32 batch-13
Running loss of epoch-32 batch-13 = 4.1405437514185905e-06

Training epoch-32 batch-14
Running loss of epoch-32 batch-14 = 6.675021722912788e-06

Training epoch-32 batch-15
Running loss of epoch-32 batch-15 = 1.9858707673847675e-05

Training epoch-32 batch-16
Running loss of epoch-32 batch-16 = 3.0095106922090054e-05

Training epoch-32 batch-17
Running loss of epoch-32 batch-17 = 6.643007509410381e-06

Training epoch-32 batch-18
Running loss of epoch-32 batch-18 = 1.4191260561347008e-05

Training epoch-32 batch-19
Running loss of epoch-32 batch-19 = 1.28626124933362e-05

Training epoch-32 batch-20
Running loss of epoch-32 batch-20 = 6.605638191103935e-06

Training epoch-32 batch-21
Running loss of epoch-32 batch-21 = 1.1888798326253891e-05

Training epoch-32 batch-22
Running loss of epoch-32 batch-22 = 1.515133772045374e-05

Training epoch-32 batch-23
Running loss of epoch-32 batch-23 = 1.806661020964384e-05

Training epoch-32 batch-24
Running loss of epoch-32 batch-24 = 1.1801370419561863e-05

Training epoch-32 batch-25
Running loss of epoch-32 batch-25 = 9.121838957071304e-06

Training epoch-32 batch-26
Running loss of epoch-32 batch-26 = 6.594229489564896e-06

Training epoch-32 batch-27
Running loss of epoch-32 batch-27 = 2.3547443561255932e-05

Training epoch-32 batch-28
Running loss of epoch-32 batch-28 = 7.5087882578372955e-06

Training epoch-32 batch-29
Running loss of epoch-32 batch-29 = 7.528928108513355e-06

Training epoch-32 batch-30
Running loss of epoch-32 batch-30 = 5.90889248996973e-06

Training epoch-32 batch-31
Running loss of epoch-32 batch-31 = 1.0561896488070488e-05

Training epoch-32 batch-32
Running loss of epoch-32 batch-32 = 1.0043964721262455e-05

Training epoch-32 batch-33
Running loss of epoch-32 batch-33 = 1.1491472832858562e-05

Training epoch-32 batch-34
Running loss of epoch-32 batch-34 = 1.2541771866381168e-05

Training epoch-32 batch-35
Running loss of epoch-32 batch-35 = 1.144141424447298e-05

Training epoch-32 batch-36
Running loss of epoch-32 batch-36 = 5.028792656958103e-06

Training epoch-32 batch-37
Running loss of epoch-32 batch-37 = 3.5688281059265137e-06

Training epoch-32 batch-38
Running loss of epoch-32 batch-38 = 1.0861433111131191e-05

Training epoch-32 batch-39
Running loss of epoch-32 batch-39 = 2.682558260858059e-06

Training epoch-32 batch-40
Running loss of epoch-32 batch-40 = 3.1082890927791595e-06

Training epoch-32 batch-41
Running loss of epoch-32 batch-41 = 4.004104994237423e-06

Training epoch-32 batch-42
Running loss of epoch-32 batch-42 = 1.138297375291586e-05

Training epoch-32 batch-43
Running loss of epoch-32 batch-43 = 6.33997842669487e-06

Training epoch-32 batch-44
Running loss of epoch-32 batch-44 = 9.86829400062561e-06

Training epoch-32 batch-45
Running loss of epoch-32 batch-45 = 7.077649934217334e-05

Training epoch-32 batch-46
Running loss of epoch-32 batch-46 = 3.003748133778572e-06

Training epoch-32 batch-47
Running loss of epoch-32 batch-47 = 2.2104941308498383e-06

Training epoch-32 batch-48
Running loss of epoch-32 batch-48 = 3.5659875720739365e-05

Training epoch-32 batch-49
Running loss of epoch-32 batch-49 = 8.546939352527261e-05

Training epoch-32 batch-50
Running loss of epoch-32 batch-50 = 4.117144271731377e-06

Training epoch-32 batch-51
Running loss of epoch-32 batch-51 = 1.2045842595398426e-05

Training epoch-32 batch-52
Running loss of epoch-32 batch-52 = 4.711677320301533e-06

Training epoch-32 batch-53
Running loss of epoch-32 batch-53 = 2.1314946934580803e-05

Training epoch-32 batch-54
Running loss of epoch-32 batch-54 = 5.707261152565479e-06

Training epoch-32 batch-55
Running loss of epoch-32 batch-55 = 5.9676822274923325e-06

Training epoch-32 batch-56
Running loss of epoch-32 batch-56 = 1.2584496289491653e-06

Training epoch-32 batch-57
Running loss of epoch-32 batch-57 = 4.477100446820259e-06

Training epoch-32 batch-58
Running loss of epoch-32 batch-58 = 2.632150426506996e-06

Training epoch-32 batch-59
Running loss of epoch-32 batch-59 = 8.194358088076115e-06

Training epoch-32 batch-60
Running loss of epoch-32 batch-60 = 3.2386393286287785e-05

Training epoch-32 batch-61
Running loss of epoch-32 batch-61 = 9.858631528913975e-06

Training epoch-32 batch-62
Running loss of epoch-32 batch-62 = 6.50365836918354e-06

Training epoch-32 batch-63
Running loss of epoch-32 batch-63 = 9.709736332297325e-06

Training epoch-32 batch-64
Running loss of epoch-32 batch-64 = 1.046457327902317e-05

Training epoch-32 batch-65
Running loss of epoch-32 batch-65 = 8.395290933549404e-06

Training epoch-32 batch-66
Running loss of epoch-32 batch-66 = 3.4158583730459213e-06

Training epoch-32 batch-67
Running loss of epoch-32 batch-67 = 2.2647669538855553e-05

Training epoch-32 batch-68
Running loss of epoch-32 batch-68 = 1.2551434338092804e-05

Training epoch-32 batch-69
Running loss of epoch-32 batch-69 = 6.550108082592487e-06

Training epoch-32 batch-70
Running loss of epoch-32 batch-70 = 5.660811439156532e-06

Training epoch-32 batch-71
Running loss of epoch-32 batch-71 = 1.2694275937974453e-05

Training epoch-32 batch-72
Running loss of epoch-32 batch-72 = 8.0490717664361e-06

Training epoch-32 batch-73
Running loss of epoch-32 batch-73 = 6.826594471931458e-07

Training epoch-32 batch-74
Running loss of epoch-32 batch-74 = 8.50902870297432e-06

Training epoch-32 batch-75
Running loss of epoch-32 batch-75 = 6.231712177395821e-06

Training epoch-32 batch-76
Running loss of epoch-32 batch-76 = 2.5228597223758698e-05

Training epoch-32 batch-77
Running loss of epoch-32 batch-77 = 1.525611151009798e-05

Training epoch-32 batch-78
Running loss of epoch-32 batch-78 = 1.6857637092471123e-05

Training epoch-32 batch-79
Running loss of epoch-32 batch-79 = 2.6275403797626495e-05

Training epoch-32 batch-80
Running loss of epoch-32 batch-80 = 4.9103982746601105e-06

Training epoch-32 batch-81
Running loss of epoch-32 batch-81 = 1.2155971489846706e-05

Training epoch-32 batch-82
Running loss of epoch-32 batch-82 = 3.7116697058081627e-06

Training epoch-32 batch-83
Running loss of epoch-32 batch-83 = 6.882240995764732e-06

Training epoch-32 batch-84
Running loss of epoch-32 batch-84 = 2.2952095605432987e-05

Training epoch-32 batch-85
Running loss of epoch-32 batch-85 = 4.873261786997318e-06

Training epoch-32 batch-86
Running loss of epoch-32 batch-86 = 2.8289563488215208e-05

Training epoch-32 batch-87
Running loss of epoch-32 batch-87 = 7.549533620476723e-06

Training epoch-32 batch-88
Running loss of epoch-32 batch-88 = 1.0275631211698055e-05

Training epoch-32 batch-89
Running loss of epoch-32 batch-89 = 5.795969627797604e-06

Training epoch-32 batch-90
Running loss of epoch-32 batch-90 = 3.5568373277783394e-06

Training epoch-32 batch-91
Running loss of epoch-32 batch-91 = 5.270121619105339e-06

Training epoch-32 batch-92
Running loss of epoch-32 batch-92 = 5.309819243848324e-06

Training epoch-32 batch-93
Running loss of epoch-32 batch-93 = 8.774572052061558e-06

Training epoch-32 batch-94
Running loss of epoch-32 batch-94 = 6.86675775796175e-06

Training epoch-32 batch-95
Running loss of epoch-32 batch-95 = 3.1557283364236355e-05

Training epoch-32 batch-96
Running loss of epoch-32 batch-96 = 1.14125432446599e-05

Training epoch-32 batch-97
Running loss of epoch-32 batch-97 = 7.514609023928642e-06

Training epoch-32 batch-98
Running loss of epoch-32 batch-98 = 2.2024381905794144e-05

Training epoch-32 batch-99
Running loss of epoch-32 batch-99 = 4.766741767525673e-06

Training epoch-32 batch-100
Running loss of epoch-32 batch-100 = 2.918415702879429e-06

Training epoch-32 batch-101
Running loss of epoch-32 batch-101 = 1.7614802345633507e-05

Training epoch-32 batch-102
Running loss of epoch-32 batch-102 = 1.603702548891306e-05

Training epoch-32 batch-103
Running loss of epoch-32 batch-103 = 1.439021434634924e-05

Training epoch-32 batch-104
Running loss of epoch-32 batch-104 = 1.1288095265626907e-05

Training epoch-32 batch-105
Running loss of epoch-32 batch-105 = 1.648359466344118e-05

Training epoch-32 batch-106
Running loss of epoch-32 batch-106 = 4.253699444234371e-06

Training epoch-32 batch-107
Running loss of epoch-32 batch-107 = 1.4025252312421799e-05

Training epoch-32 batch-108
Running loss of epoch-32 batch-108 = 2.1153362467885017e-05

Training epoch-32 batch-109
Running loss of epoch-32 batch-109 = 1.7056823708117008e-05

Training epoch-32 batch-110
Running loss of epoch-32 batch-110 = 3.7438992876559496e-05

Training epoch-32 batch-111
Running loss of epoch-32 batch-111 = 8.063390851020813e-06

Training epoch-32 batch-112
Running loss of epoch-32 batch-112 = 7.265014573931694e-06

Training epoch-32 batch-113
Running loss of epoch-32 batch-113 = 2.5698565877974033e-05

Training epoch-32 batch-114
Running loss of epoch-32 batch-114 = 1.0796822607517242e-05

Training epoch-32 batch-115
Running loss of epoch-32 batch-115 = 1.1573894880712032e-05

Training epoch-32 batch-116
Running loss of epoch-32 batch-116 = 8.667935617268085e-06

Training epoch-32 batch-117
Running loss of epoch-32 batch-117 = 9.687850251793861e-06

Training epoch-32 batch-118
Running loss of epoch-32 batch-118 = 7.563037797808647e-06

Training epoch-32 batch-119
Running loss of epoch-32 batch-119 = 2.518412657082081e-06

Training epoch-32 batch-120
Running loss of epoch-32 batch-120 = 3.25528671965003e-05

Training epoch-32 batch-121
Running loss of epoch-32 batch-121 = 5.1944167353212833e-05

Training epoch-32 batch-122
Running loss of epoch-32 batch-122 = 1.628277823328972e-05

Training epoch-32 batch-123
Running loss of epoch-32 batch-123 = 1.539813820272684e-05

Training epoch-32 batch-124
Running loss of epoch-32 batch-124 = 2.1286425180733204e-05

Training epoch-32 batch-125
Running loss of epoch-32 batch-125 = 1.0978546924889088e-05

Training epoch-32 batch-126
Running loss of epoch-32 batch-126 = 3.0441442504525185e-06

Training epoch-32 batch-127
Running loss of epoch-32 batch-127 = 9.505311027169228e-06

Training epoch-32 batch-128
Running loss of epoch-32 batch-128 = 1.0676216334104538e-05

Training epoch-32 batch-129
Running loss of epoch-32 batch-129 = 6.32251612842083e-06

Training epoch-32 batch-130
Running loss of epoch-32 batch-130 = 2.9458198696374893e-05

Training epoch-32 batch-131
Running loss of epoch-32 batch-131 = 2.5980290956795216e-05

Training epoch-32 batch-132
Running loss of epoch-32 batch-132 = 4.849396646022797e-06

Training epoch-32 batch-133
Running loss of epoch-32 batch-133 = 2.3262575268745422e-05

Training epoch-32 batch-134
Running loss of epoch-32 batch-134 = 4.5942142605781555e-06

Training epoch-32 batch-135
Running loss of epoch-32 batch-135 = 7.584108971059322e-06

Training epoch-32 batch-136
Running loss of epoch-32 batch-136 = 6.4140185713768e-05

Training epoch-32 batch-137
Running loss of epoch-32 batch-137 = 1.7562531866133213e-05

Training epoch-32 batch-138
Running loss of epoch-32 batch-138 = 6.759073585271835e-06

Training epoch-32 batch-139
Running loss of epoch-32 batch-139 = 3.6298297345638275e-06

Training epoch-32 batch-140
Running loss of epoch-32 batch-140 = 2.8052600100636482e-06

Training epoch-32 batch-141
Running loss of epoch-32 batch-141 = 7.84092117100954e-06

Training epoch-32 batch-142
Running loss of epoch-32 batch-142 = 7.2766561061143875e-06

Training epoch-32 batch-143
Running loss of epoch-32 batch-143 = 4.0458980947732925e-06

Training epoch-32 batch-144
Running loss of epoch-32 batch-144 = 3.8133002817630768e-06

Training epoch-32 batch-145
Running loss of epoch-32 batch-145 = 1.416727900505066e-05

Training epoch-32 batch-146
Running loss of epoch-32 batch-146 = 4.5305583626031876e-05

Training epoch-32 batch-147
Running loss of epoch-32 batch-147 = 7.368740625679493e-06

Training epoch-32 batch-148
Running loss of epoch-32 batch-148 = 6.27199187874794e-06

Training epoch-32 batch-149
Running loss of epoch-32 batch-149 = 5.214591510593891e-06

Training epoch-32 batch-150
Running loss of epoch-32 batch-150 = 7.056747563183308e-06

Training epoch-32 batch-151
Running loss of epoch-32 batch-151 = 3.6674318835139275e-05

Training epoch-32 batch-152
Running loss of epoch-32 batch-152 = 5.286186933517456e-06

Training epoch-32 batch-153
Running loss of epoch-32 batch-153 = 2.073589712381363e-06

Training epoch-32 batch-154
Running loss of epoch-32 batch-154 = 8.760136552155018e-06

Training epoch-32 batch-155
Running loss of epoch-32 batch-155 = 2.8597423806786537e-06

Training epoch-32 batch-156
Running loss of epoch-32 batch-156 = 2.2084685042500496e-05

Training epoch-32 batch-157
Running loss of epoch-32 batch-157 = 2.0675361156463623e-06

Finished training epoch-32.



Average train loss at epoch-32 = 1.2843650579452515e-05

Started Evaluation

Average val loss at epoch-32 = 4.9067775105175215

Accuracy for classes:
Accuracy for class equals is: 62.05 %
Accuracy for class main is: 44.92 %
Accuracy for class setUp is: 50.00 %
Accuracy for class onCreate is: 35.93 %
Accuracy for class toString is: 42.66 %
Accuracy for class run is: 29.91 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 26.01 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 26.67 %

Overall Accuracy = 41.65 %

Finished Evaluation



Started training epoch-33


Training epoch-33 batch-1
Running loss of epoch-33 batch-1 = 9.456067346036434e-06

Training epoch-33 batch-2
Running loss of epoch-33 batch-2 = 7.735216058790684e-06

Training epoch-33 batch-3
Running loss of epoch-33 batch-3 = 1.1324649676680565e-05

Training epoch-33 batch-4
Running loss of epoch-33 batch-4 = 5.1798997446894646e-06

Training epoch-33 batch-5
Running loss of epoch-33 batch-5 = 9.071663953363895e-06

Training epoch-33 batch-6
Running loss of epoch-33 batch-6 = 1.4643301256000996e-05

Training epoch-33 batch-7
Running loss of epoch-33 batch-7 = 2.0533334463834763e-05

Training epoch-33 batch-8
Running loss of epoch-33 batch-8 = 9.782263077795506e-06

Training epoch-33 batch-9
Running loss of epoch-33 batch-9 = 1.9015511497855186e-05

Training epoch-33 batch-10
Running loss of epoch-33 batch-10 = 1.2126401998102665e-05

Training epoch-33 batch-11
Running loss of epoch-33 batch-11 = 6.865098839625716e-05

Training epoch-33 batch-12
Running loss of epoch-33 batch-12 = 1.1179246939718723e-05

Training epoch-33 batch-13
Running loss of epoch-33 batch-13 = 7.618567906320095e-06

Training epoch-33 batch-14
Running loss of epoch-33 batch-14 = 9.388080798089504e-06

Training epoch-33 batch-15
Running loss of epoch-33 batch-15 = 2.0323088392615318e-05

Training epoch-33 batch-16
Running loss of epoch-33 batch-16 = 5.790614522993565e-06

Training epoch-33 batch-17
Running loss of epoch-33 batch-17 = 6.865477189421654e-06

Training epoch-33 batch-18
Running loss of epoch-33 batch-18 = 8.662114851176739e-06

Training epoch-33 batch-19
Running loss of epoch-33 batch-19 = 2.1114246919751167e-06

Training epoch-33 batch-20
Running loss of epoch-33 batch-20 = 3.998400643467903e-06

Training epoch-33 batch-21
Running loss of epoch-33 batch-21 = 2.4518813006579876e-05

Training epoch-33 batch-22
Running loss of epoch-33 batch-22 = 7.0089008659124374e-06

Training epoch-33 batch-23
Running loss of epoch-33 batch-23 = 2.203625626862049e-05

Training epoch-33 batch-24
Running loss of epoch-33 batch-24 = 4.517508205026388e-05

Training epoch-33 batch-25
Running loss of epoch-33 batch-25 = 4.552188329398632e-06

Training epoch-33 batch-26
Running loss of epoch-33 batch-26 = 3.817840479314327e-06

Training epoch-33 batch-27
Running loss of epoch-33 batch-27 = 6.3749030232429504e-06

Training epoch-33 batch-28
Running loss of epoch-33 batch-28 = 3.642146475613117e-05

Training epoch-33 batch-29
Running loss of epoch-33 batch-29 = 1.175818033516407e-05

Training epoch-33 batch-30
Running loss of epoch-33 batch-30 = 1.1396361514925957e-05

Training epoch-33 batch-31
Running loss of epoch-33 batch-31 = 2.7360511012375355e-05

Training epoch-33 batch-32
Running loss of epoch-33 batch-32 = 3.1403033062815666e-06

Training epoch-33 batch-33
Running loss of epoch-33 batch-33 = 8.568400517106056e-06

Training epoch-33 batch-34
Running loss of epoch-33 batch-34 = 8.875387720763683e-06

Training epoch-33 batch-35
Running loss of epoch-33 batch-35 = 9.439303539693356e-06

Training epoch-33 batch-36
Running loss of epoch-33 batch-36 = 3.493274562060833e-06

Training epoch-33 batch-37
Running loss of epoch-33 batch-37 = 7.68445897847414e-06

Training epoch-33 batch-38
Running loss of epoch-33 batch-38 = 6.594927981495857e-06

Training epoch-33 batch-39
Running loss of epoch-33 batch-39 = 2.6545021682977676e-06

Training epoch-33 batch-40
Running loss of epoch-33 batch-40 = 1.6529811546206474e-05

Training epoch-33 batch-41
Running loss of epoch-33 batch-41 = 2.2990512661635876e-05

Training epoch-33 batch-42
Running loss of epoch-33 batch-42 = 5.413778126239777e-06

Training epoch-33 batch-43
Running loss of epoch-33 batch-43 = 6.879214197397232e-06

Training epoch-33 batch-44
Running loss of epoch-33 batch-44 = 1.1367606930434704e-05

Training epoch-33 batch-45
Running loss of epoch-33 batch-45 = 9.196344763040543e-06

Training epoch-33 batch-46
Running loss of epoch-33 batch-46 = 2.364744432270527e-06

Training epoch-33 batch-47
Running loss of epoch-33 batch-47 = 6.242073141038418e-06

Training epoch-33 batch-48
Running loss of epoch-33 batch-48 = 1.4594988897442818e-05

Training epoch-33 batch-49
Running loss of epoch-33 batch-49 = 2.5292392820119858e-06

Training epoch-33 batch-50
Running loss of epoch-33 batch-50 = 1.521548256278038e-06

Training epoch-33 batch-51
Running loss of epoch-33 batch-51 = 6.202375516295433e-06

Training epoch-33 batch-52
Running loss of epoch-33 batch-52 = 3.7694117054343224e-06

Training epoch-33 batch-53
Running loss of epoch-33 batch-53 = 5.362788215279579e-06

Training epoch-33 batch-54
Running loss of epoch-33 batch-54 = 1.1829892173409462e-05

Training epoch-33 batch-55
Running loss of epoch-33 batch-55 = 3.705325070768595e-05

Training epoch-33 batch-56
Running loss of epoch-33 batch-56 = 2.0579318515956402e-05

Training epoch-33 batch-57
Running loss of epoch-33 batch-57 = 8.400878868997097e-06

Training epoch-33 batch-58
Running loss of epoch-33 batch-58 = 4.216097295284271e-06

Training epoch-33 batch-59
Running loss of epoch-33 batch-59 = 1.4819321222603321e-05

Training epoch-33 batch-60
Running loss of epoch-33 batch-60 = 6.190617568790913e-06

Training epoch-33 batch-61
Running loss of epoch-33 batch-61 = 4.143803380429745e-06

Training epoch-33 batch-62
Running loss of epoch-33 batch-62 = 1.5758792869746685e-05

Training epoch-33 batch-63
Running loss of epoch-33 batch-63 = 1.0846531949937344e-05

Training epoch-33 batch-64
Running loss of epoch-33 batch-64 = 5.9197365771979094e-05

Training epoch-33 batch-65
Running loss of epoch-33 batch-65 = 7.201218977570534e-06

Training epoch-33 batch-66
Running loss of epoch-33 batch-66 = 6.5765343606472015e-06

Training epoch-33 batch-67
Running loss of epoch-33 batch-67 = 6.891204975545406e-06

Training epoch-33 batch-68
Running loss of epoch-33 batch-68 = 8.98679718375206e-06

Training epoch-33 batch-69
Running loss of epoch-33 batch-69 = 3.3939722925424576e-06

Training epoch-33 batch-70
Running loss of epoch-33 batch-70 = 8.72055534273386e-06

Training epoch-33 batch-71
Running loss of epoch-33 batch-71 = 3.0284281820058823e-05

Training epoch-33 batch-72
Running loss of epoch-33 batch-72 = 4.16638795286417e-06

Training epoch-33 batch-73
Running loss of epoch-33 batch-73 = 9.596697054803371e-06

Training epoch-33 batch-74
Running loss of epoch-33 batch-74 = 8.069095201790333e-06

Training epoch-33 batch-75
Running loss of epoch-33 batch-75 = 2.0236941054463387e-05

Training epoch-33 batch-76
Running loss of epoch-33 batch-76 = 3.8656871765851974e-06

Training epoch-33 batch-77
Running loss of epoch-33 batch-77 = 2.8853537514805794e-06

Training epoch-33 batch-78
Running loss of epoch-33 batch-78 = 3.6773039028048515e-05

Training epoch-33 batch-79
Running loss of epoch-33 batch-79 = 3.302004188299179e-06

Training epoch-33 batch-80
Running loss of epoch-33 batch-80 = 1.0573538020253181e-05

Training epoch-33 batch-81
Running loss of epoch-33 batch-81 = 8.916365914046764e-06

Training epoch-33 batch-82
Running loss of epoch-33 batch-82 = 5.41971530765295e-06

Training epoch-33 batch-83
Running loss of epoch-33 batch-83 = 1.0826624929904938e-05

Training epoch-33 batch-84
Running loss of epoch-33 batch-84 = 2.2745225578546524e-06

Training epoch-33 batch-85
Running loss of epoch-33 batch-85 = 5.8640725910663605e-06

Training epoch-33 batch-86
Running loss of epoch-33 batch-86 = 7.529743015766144e-06

Training epoch-33 batch-87
Running loss of epoch-33 batch-87 = 2.8774607926607132e-05

Training epoch-33 batch-88
Running loss of epoch-33 batch-88 = 1.561047974973917e-05

Training epoch-33 batch-89
Running loss of epoch-33 batch-89 = 1.0939897038042545e-05

Training epoch-33 batch-90
Running loss of epoch-33 batch-90 = 7.41073745302856e-05

Training epoch-33 batch-91
Running loss of epoch-33 batch-91 = 1.3329670764505863e-05

Training epoch-33 batch-92
Running loss of epoch-33 batch-92 = 6.092246621847153e-06

Training epoch-33 batch-93
Running loss of epoch-33 batch-93 = 1.0036863386631012e-05

Training epoch-33 batch-94
Running loss of epoch-33 batch-94 = 6.87094870954752e-06

Training epoch-33 batch-95
Running loss of epoch-33 batch-95 = 3.2937387004494667e-06

Training epoch-33 batch-96
Running loss of epoch-33 batch-96 = 2.996530383825302e-06

Training epoch-33 batch-97
Running loss of epoch-33 batch-97 = 2.860906533896923e-06

Training epoch-33 batch-98
Running loss of epoch-33 batch-98 = 6.753834895789623e-06

Training epoch-33 batch-99
Running loss of epoch-33 batch-99 = 9.545590728521347e-06

Training epoch-33 batch-100
Running loss of epoch-33 batch-100 = 3.446824848651886e-06

Training epoch-33 batch-101
Running loss of epoch-33 batch-101 = 1.5579978935420513e-05

Training epoch-33 batch-102
Running loss of epoch-33 batch-102 = 3.641704097390175e-06

Training epoch-33 batch-103
Running loss of epoch-33 batch-103 = 4.971516318619251e-06

Training epoch-33 batch-104
Running loss of epoch-33 batch-104 = 2.0605046302080154e-05

Training epoch-33 batch-105
Running loss of epoch-33 batch-105 = 2.6319175958633423e-06

Training epoch-33 batch-106
Running loss of epoch-33 batch-106 = 6.5605854615569115e-06

Training epoch-33 batch-107
Running loss of epoch-33 batch-107 = 3.458373248577118e-05

Training epoch-33 batch-108
Running loss of epoch-33 batch-108 = 1.6649719327688217e-06

Training epoch-33 batch-109
Running loss of epoch-33 batch-109 = 8.850940503180027e-06

Training epoch-33 batch-110
Running loss of epoch-33 batch-110 = 7.076188921928406e-06

Training epoch-33 batch-111
Running loss of epoch-33 batch-111 = 5.912384949624538e-06

Training epoch-33 batch-112
Running loss of epoch-33 batch-112 = 8.66700429469347e-06

Training epoch-33 batch-113
Running loss of epoch-33 batch-113 = 1.152500044554472e-05

Training epoch-33 batch-114
Running loss of epoch-33 batch-114 = 1.2101954780519009e-05

Training epoch-33 batch-115
Running loss of epoch-33 batch-115 = 4.253815859556198e-06

Training epoch-33 batch-116
Running loss of epoch-33 batch-116 = 7.857452146708965e-06

Training epoch-33 batch-117
Running loss of epoch-33 batch-117 = 2.1373736672103405e-05

Training epoch-33 batch-118
Running loss of epoch-33 batch-118 = 1.0665622539818287e-05

Training epoch-33 batch-119
Running loss of epoch-33 batch-119 = 1.965311821550131e-05

Training epoch-33 batch-120
Running loss of epoch-33 batch-120 = 2.107350155711174e-05

Training epoch-33 batch-121
Running loss of epoch-33 batch-121 = 7.733935490250587e-06

Training epoch-33 batch-122
Running loss of epoch-33 batch-122 = 7.312744855880737e-06

Training epoch-33 batch-123
Running loss of epoch-33 batch-123 = 2.8372101951390505e-05

Training epoch-33 batch-124
Running loss of epoch-33 batch-124 = 2.6836059987545013e-06

Training epoch-33 batch-125
Running loss of epoch-33 batch-125 = 1.0579009540379047e-05

Training epoch-33 batch-126
Running loss of epoch-33 batch-126 = 2.5799963623285294e-06

Training epoch-33 batch-127
Running loss of epoch-33 batch-127 = 2.7997884899377823e-05

Training epoch-33 batch-128
Running loss of epoch-33 batch-128 = 5.191424861550331e-06

Training epoch-33 batch-129
Running loss of epoch-33 batch-129 = 3.247801214456558e-05

Training epoch-33 batch-130
Running loss of epoch-33 batch-130 = 3.5462435334920883e-06

Training epoch-33 batch-131
Running loss of epoch-33 batch-131 = 5.3335679695010185e-06

Training epoch-33 batch-132
Running loss of epoch-33 batch-132 = 7.829396054148674e-06

Training epoch-33 batch-133
Running loss of epoch-33 batch-133 = 6.964430212974548e-06

Training epoch-33 batch-134
Running loss of epoch-33 batch-134 = 9.20367892831564e-06

Training epoch-33 batch-135
Running loss of epoch-33 batch-135 = 3.457651473581791e-06

Training epoch-33 batch-136
Running loss of epoch-33 batch-136 = 3.0341034289449453e-05

Training epoch-33 batch-137
Running loss of epoch-33 batch-137 = 7.285154424607754e-06

Training epoch-33 batch-138
Running loss of epoch-33 batch-138 = 3.4318072721362114e-06

Training epoch-33 batch-139
Running loss of epoch-33 batch-139 = 3.485416527837515e-05

Training epoch-33 batch-140
Running loss of epoch-33 batch-140 = 6.5588392317295074e-06

Training epoch-33 batch-141
Running loss of epoch-33 batch-141 = 2.4358276277780533e-05

Training epoch-33 batch-142
Running loss of epoch-33 batch-142 = 3.3420510590076447e-06

Training epoch-33 batch-143
Running loss of epoch-33 batch-143 = 3.929249942302704e-06

Training epoch-33 batch-144
Running loss of epoch-33 batch-144 = 6.134039722383022e-06

Training epoch-33 batch-145
Running loss of epoch-33 batch-145 = 9.404495358467102e-06

Training epoch-33 batch-146
Running loss of epoch-33 batch-146 = 1.0930118151009083e-05

Training epoch-33 batch-147
Running loss of epoch-33 batch-147 = 3.7758145481348038e-06

Training epoch-33 batch-148
Running loss of epoch-33 batch-148 = 6.2568578869104385e-06

Training epoch-33 batch-149
Running loss of epoch-33 batch-149 = 5.094800144433975e-06

Training epoch-33 batch-150
Running loss of epoch-33 batch-150 = 5.947775207459927e-06

Training epoch-33 batch-151
Running loss of epoch-33 batch-151 = 1.3703480362892151e-05

Training epoch-33 batch-152
Running loss of epoch-33 batch-152 = 7.997849024832249e-06

Training epoch-33 batch-153
Running loss of epoch-33 batch-153 = 4.415283910930157e-06

Training epoch-33 batch-154
Running loss of epoch-33 batch-154 = 6.7249638959765434e-06

Training epoch-33 batch-155
Running loss of epoch-33 batch-155 = 6.870832294225693e-06

Training epoch-33 batch-156
Running loss of epoch-33 batch-156 = 2.0829029381275177e-06

Training epoch-33 batch-157
Running loss of epoch-33 batch-157 = 5.2951276302337646e-05

Finished training epoch-33.



Average train loss at epoch-33 = 1.1753811314702035e-05

Started Evaluation

Average val loss at epoch-33 = 4.936393442906831

Accuracy for classes:
Accuracy for class equals is: 62.21 %
Accuracy for class main is: 43.93 %
Accuracy for class setUp is: 50.00 %
Accuracy for class onCreate is: 36.14 %
Accuracy for class toString is: 42.32 %
Accuracy for class run is: 30.14 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 27.58 %
Accuracy for class execute is: 18.47 %
Accuracy for class get is: 26.41 %

Overall Accuracy = 41.65 %

Finished Evaluation



Started training epoch-34


Training epoch-34 batch-1
Running loss of epoch-34 batch-1 = 3.4207478165626526e-06

Training epoch-34 batch-2
Running loss of epoch-34 batch-2 = 5.3176190704107285e-06

Training epoch-34 batch-3
Running loss of epoch-34 batch-3 = 7.639988325536251e-06

Training epoch-34 batch-4
Running loss of epoch-34 batch-4 = 9.350129403173923e-06

Training epoch-34 batch-5
Running loss of epoch-34 batch-5 = 7.446971721947193e-06

Training epoch-34 batch-6
Running loss of epoch-34 batch-6 = 1.3053533621132374e-05

Training epoch-34 batch-7
Running loss of epoch-34 batch-7 = 6.44708052277565e-06

Training epoch-34 batch-8
Running loss of epoch-34 batch-8 = 1.0501244105398655e-05

Training epoch-34 batch-9
Running loss of epoch-34 batch-9 = 5.369773134589195e-06

Training epoch-34 batch-10
Running loss of epoch-34 batch-10 = 8.874456398189068e-06

Training epoch-34 batch-11
Running loss of epoch-34 batch-11 = 2.278492320328951e-05

Training epoch-34 batch-12
Running loss of epoch-34 batch-12 = 7.1838730946183205e-06

Training epoch-34 batch-13
Running loss of epoch-34 batch-13 = 1.0586692951619625e-05

Training epoch-34 batch-14
Running loss of epoch-34 batch-14 = 5.071749910712242e-06

Training epoch-34 batch-15
Running loss of epoch-34 batch-15 = 2.16485932469368e-06

Training epoch-34 batch-16
Running loss of epoch-34 batch-16 = 7.871421985328197e-06

Training epoch-34 batch-17
Running loss of epoch-34 batch-17 = 2.5944318622350693e-06

Training epoch-34 batch-18
Running loss of epoch-34 batch-18 = 4.148692823946476e-06

Training epoch-34 batch-19
Running loss of epoch-34 batch-19 = 3.825989551842213e-06

Training epoch-34 batch-20
Running loss of epoch-34 batch-20 = 2.4558277800679207e-05

Training epoch-34 batch-21
Running loss of epoch-34 batch-21 = 3.428664058446884e-06

Training epoch-34 batch-22
Running loss of epoch-34 batch-22 = 2.9082875698804855e-06

Training epoch-34 batch-23
Running loss of epoch-34 batch-23 = 1.93306477740407e-05

Training epoch-34 batch-24
Running loss of epoch-34 batch-24 = 3.5865232348442078e-06

Training epoch-34 batch-25
Running loss of epoch-34 batch-25 = 2.396001946181059e-05

Training epoch-34 batch-26
Running loss of epoch-34 batch-26 = 1.0400894097983837e-05

Training epoch-34 batch-27
Running loss of epoch-34 batch-27 = 1.9537517800927162e-05

Training epoch-34 batch-28
Running loss of epoch-34 batch-28 = 2.4631619453430176e-05

Training epoch-34 batch-29
Running loss of epoch-34 batch-29 = 1.2805103324353695e-05

Training epoch-34 batch-30
Running loss of epoch-34 batch-30 = 8.811126463115215e-06

Training epoch-34 batch-31
Running loss of epoch-34 batch-31 = 2.516666427254677e-06

Training epoch-34 batch-32
Running loss of epoch-34 batch-32 = 7.481779903173447e-06

Training epoch-34 batch-33
Running loss of epoch-34 batch-33 = 6.076064892113209e-06

Training epoch-34 batch-34
Running loss of epoch-34 batch-34 = 5.000270903110504e-06

Training epoch-34 batch-35
Running loss of epoch-34 batch-35 = 1.093477476388216e-05

Training epoch-34 batch-36
Running loss of epoch-34 batch-36 = 5.335197784006596e-06

Training epoch-34 batch-37
Running loss of epoch-34 batch-37 = 1.4018965885043144e-05

Training epoch-34 batch-38
Running loss of epoch-34 batch-38 = 1.9320053979754448e-05

Training epoch-34 batch-39
Running loss of epoch-34 batch-39 = 4.119472578167915e-06

Training epoch-34 batch-40
Running loss of epoch-34 batch-40 = 2.778833732008934e-06

Training epoch-34 batch-41
Running loss of epoch-34 batch-41 = 6.781076081097126e-06

Training epoch-34 batch-42
Running loss of epoch-34 batch-42 = 4.683388397097588e-06

Training epoch-34 batch-43
Running loss of epoch-34 batch-43 = 7.441965863108635e-06

Training epoch-34 batch-44
Running loss of epoch-34 batch-44 = 7.990514859557152e-06

Training epoch-34 batch-45
Running loss of epoch-34 batch-45 = 6.74522016197443e-06

Training epoch-34 batch-46
Running loss of epoch-34 batch-46 = 3.3285468816757202e-06

Training epoch-34 batch-47
Running loss of epoch-34 batch-47 = 1.2129079550504684e-05

Training epoch-34 batch-48
Running loss of epoch-34 batch-48 = 6.086425855755806e-06

Training epoch-34 batch-49
Running loss of epoch-34 batch-49 = 2.1513900719583035e-05

Training epoch-34 batch-50
Running loss of epoch-34 batch-50 = 1.7847050912678242e-05

Training epoch-34 batch-51
Running loss of epoch-34 batch-51 = 1.1418480426073074e-05

Training epoch-34 batch-52
Running loss of epoch-34 batch-52 = 3.2056355848908424e-05

Training epoch-34 batch-53
Running loss of epoch-34 batch-53 = 1.2276927009224892e-05

Training epoch-34 batch-54
Running loss of epoch-34 batch-54 = 2.5135348550975323e-05

Training epoch-34 batch-55
Running loss of epoch-34 batch-55 = 4.461733624339104e-06

Training epoch-34 batch-56
Running loss of epoch-34 batch-56 = 1.7781276255846024e-05

Training epoch-34 batch-57
Running loss of epoch-34 batch-57 = 1.1273426935076714e-05

Training epoch-34 batch-58
Running loss of epoch-34 batch-58 = 8.61461739987135e-06

Training epoch-34 batch-59
Running loss of epoch-34 batch-59 = 7.867347449064255e-06

Training epoch-34 batch-60
Running loss of epoch-34 batch-60 = 3.7244753912091255e-06

Training epoch-34 batch-61
Running loss of epoch-34 batch-61 = 1.0502873919904232e-05

Training epoch-34 batch-62
Running loss of epoch-34 batch-62 = 7.103779353201389e-06

Training epoch-34 batch-63
Running loss of epoch-34 batch-63 = 8.424161933362484e-06

Training epoch-34 batch-64
Running loss of epoch-34 batch-64 = 1.0881572961807251e-05

Training epoch-34 batch-65
Running loss of epoch-34 batch-65 = 7.387250661849976e-06

Training epoch-34 batch-66
Running loss of epoch-34 batch-66 = 1.5467172488570213e-05

Training epoch-34 batch-67
Running loss of epoch-34 batch-67 = 4.514236934483051e-06

Training epoch-34 batch-68
Running loss of epoch-34 batch-68 = 1.115014310926199e-05

Training epoch-34 batch-69
Running loss of epoch-34 batch-69 = 5.121342837810516e-06

Training epoch-34 batch-70
Running loss of epoch-34 batch-70 = 1.9171275198459625e-06

Training epoch-34 batch-71
Running loss of epoch-34 batch-71 = 5.675246939063072e-06

Training epoch-34 batch-72
Running loss of epoch-34 batch-72 = 1.1143041774630547e-05

Training epoch-34 batch-73
Running loss of epoch-34 batch-73 = 3.1273229978978634e-05

Training epoch-34 batch-74
Running loss of epoch-34 batch-74 = 5.34718856215477e-06

Training epoch-34 batch-75
Running loss of epoch-34 batch-75 = 3.7980498746037483e-06

Training epoch-34 batch-76
Running loss of epoch-34 batch-76 = 2.901931293308735e-05

Training epoch-34 batch-77
Running loss of epoch-34 batch-77 = 7.242080755531788e-06

Training epoch-34 batch-78
Running loss of epoch-34 batch-78 = 3.859866410493851e-06

Training epoch-34 batch-79
Running loss of epoch-34 batch-79 = 8.444534614682198e-06

Training epoch-34 batch-80
Running loss of epoch-34 batch-80 = 7.326947525143623e-06

Training epoch-34 batch-81
Running loss of epoch-34 batch-81 = 2.2532767616212368e-05

Training epoch-34 batch-82
Running loss of epoch-34 batch-82 = 1.0605435818433762e-05

Training epoch-34 batch-83
Running loss of epoch-34 batch-83 = 5.843467079102993e-06

Training epoch-34 batch-84
Running loss of epoch-34 batch-84 = 6.109126843512058e-06

Training epoch-34 batch-85
Running loss of epoch-34 batch-85 = 1.0667252354323864e-05

Training epoch-34 batch-86
Running loss of epoch-34 batch-86 = 1.067272387444973e-05

Training epoch-34 batch-87
Running loss of epoch-34 batch-87 = 4.012836143374443e-06

Training epoch-34 batch-88
Running loss of epoch-34 batch-88 = 8.521019481122494e-06

Training epoch-34 batch-89
Running loss of epoch-34 batch-89 = 8.625560440123081e-06

Training epoch-34 batch-90
Running loss of epoch-34 batch-90 = 4.109344445168972e-06

Training epoch-34 batch-91
Running loss of epoch-34 batch-91 = 8.498318493366241e-06

Training epoch-34 batch-92
Running loss of epoch-34 batch-92 = 1.1253054253757e-05

Training epoch-34 batch-93
Running loss of epoch-34 batch-93 = 8.084694854915142e-06

Training epoch-34 batch-94
Running loss of epoch-34 batch-94 = 8.122297003865242e-06

Training epoch-34 batch-95
Running loss of epoch-34 batch-95 = 2.305721864104271e-05

Training epoch-34 batch-96
Running loss of epoch-34 batch-96 = 8.343602530658245e-06

Training epoch-34 batch-97
Running loss of epoch-34 batch-97 = 1.3306853361427784e-05

Training epoch-34 batch-98
Running loss of epoch-34 batch-98 = 6.028567440807819e-06

Training epoch-34 batch-99
Running loss of epoch-34 batch-99 = 7.607328006997705e-05

Training epoch-34 batch-100
Running loss of epoch-34 batch-100 = 3.0831433832645416e-06

Training epoch-34 batch-101
Running loss of epoch-34 batch-101 = 1.340196467936039e-05

Training epoch-34 batch-102
Running loss of epoch-34 batch-102 = 4.628323949873447e-06

Training epoch-34 batch-103
Running loss of epoch-34 batch-103 = 1.1030700989067554e-05

Training epoch-34 batch-104
Running loss of epoch-34 batch-104 = 5.62786590307951e-06

Training epoch-34 batch-105
Running loss of epoch-34 batch-105 = 6.491201929748058e-06

Training epoch-34 batch-106
Running loss of epoch-34 batch-106 = 3.7492718547582626e-06

Training epoch-34 batch-107
Running loss of epoch-34 batch-107 = 4.727276973426342e-06

Training epoch-34 batch-108
Running loss of epoch-34 batch-108 = 1.6091973520815372e-05

Training epoch-34 batch-109
Running loss of epoch-34 batch-109 = 4.458939656615257e-06

Training epoch-34 batch-110
Running loss of epoch-34 batch-110 = 1.7883721739053726e-06

Training epoch-34 batch-111
Running loss of epoch-34 batch-111 = 2.0561274141073227e-06

Training epoch-34 batch-112
Running loss of epoch-34 batch-112 = 5.953828804194927e-06

Training epoch-34 batch-113
Running loss of epoch-34 batch-113 = 5.946843884885311e-06

Training epoch-34 batch-114
Running loss of epoch-34 batch-114 = 4.243454895913601e-06

Training epoch-34 batch-115
Running loss of epoch-34 batch-115 = 1.9158818759024143e-05

Training epoch-34 batch-116
Running loss of epoch-34 batch-116 = 4.455447196960449e-06

Training epoch-34 batch-117
Running loss of epoch-34 batch-117 = 6.497488357126713e-06

Training epoch-34 batch-118
Running loss of epoch-34 batch-118 = 3.3783726394176483e-06

Training epoch-34 batch-119
Running loss of epoch-34 batch-119 = 1.0445946827530861e-05

Training epoch-34 batch-120
Running loss of epoch-34 batch-120 = 1.037342008203268e-05

Training epoch-34 batch-121
Running loss of epoch-34 batch-121 = 5.4300762712955475e-06

Training epoch-34 batch-122
Running loss of epoch-34 batch-122 = 4.376866854727268e-06

Training epoch-34 batch-123
Running loss of epoch-34 batch-123 = 7.379334419965744e-06

Training epoch-34 batch-124
Running loss of epoch-34 batch-124 = 8.001341484487057e-06

Training epoch-34 batch-125
Running loss of epoch-34 batch-125 = 6.138347089290619e-06

Training epoch-34 batch-126
Running loss of epoch-34 batch-126 = 1.4309189282357693e-05

Training epoch-34 batch-127
Running loss of epoch-34 batch-127 = 6.97083305567503e-06

Training epoch-34 batch-128
Running loss of epoch-34 batch-128 = 4.404224455356598e-06

Training epoch-34 batch-129
Running loss of epoch-34 batch-129 = 1.975568011403084e-06

Training epoch-34 batch-130
Running loss of epoch-34 batch-130 = 1.3618962839245796e-05

Training epoch-34 batch-131
Running loss of epoch-34 batch-131 = 2.4743028916418552e-05

Training epoch-34 batch-132
Running loss of epoch-34 batch-132 = 5.1800161600112915e-06

Training epoch-34 batch-133
Running loss of epoch-34 batch-133 = 5.759065970778465e-06

Training epoch-34 batch-134
Running loss of epoch-34 batch-134 = 1.2400560081005096e-05

Training epoch-34 batch-135
Running loss of epoch-34 batch-135 = 8.048489689826965e-06

Training epoch-34 batch-136
Running loss of epoch-34 batch-136 = 6.0248421505093575e-06

Training epoch-34 batch-137
Running loss of epoch-34 batch-137 = 9.032664820551872e-06

Training epoch-34 batch-138
Running loss of epoch-34 batch-138 = 1.3478798791766167e-05

Training epoch-34 batch-139
Running loss of epoch-34 batch-139 = 3.345869481563568e-05

Training epoch-34 batch-140
Running loss of epoch-34 batch-140 = 1.4875899069011211e-05

Training epoch-34 batch-141
Running loss of epoch-34 batch-141 = 5.721347406506538e-06

Training epoch-34 batch-142
Running loss of epoch-34 batch-142 = 6.966525688767433e-06

Training epoch-34 batch-143
Running loss of epoch-34 batch-143 = 1.1218944564461708e-05

Training epoch-34 batch-144
Running loss of epoch-34 batch-144 = 1.4691497199237347e-05

Training epoch-34 batch-145
Running loss of epoch-34 batch-145 = 8.034403435885906e-06

Training epoch-34 batch-146
Running loss of epoch-34 batch-146 = 1.453980803489685e-05

Training epoch-34 batch-147
Running loss of epoch-34 batch-147 = 1.0194140486419201e-05

Training epoch-34 batch-148
Running loss of epoch-34 batch-148 = 4.554516635835171e-06

Training epoch-34 batch-149
Running loss of epoch-34 batch-149 = 1.0160845704376698e-05

Training epoch-34 batch-150
Running loss of epoch-34 batch-150 = 5.069887265563011e-06

Training epoch-34 batch-151
Running loss of epoch-34 batch-151 = 7.629155879840255e-05

Training epoch-34 batch-152
Running loss of epoch-34 batch-152 = 6.8856170400977135e-06

Training epoch-34 batch-153
Running loss of epoch-34 batch-153 = 1.0870047844946384e-05

Training epoch-34 batch-154
Running loss of epoch-34 batch-154 = 5.330599378794432e-05

Training epoch-34 batch-155
Running loss of epoch-34 batch-155 = 7.71449413150549e-06

Training epoch-34 batch-156
Running loss of epoch-34 batch-156 = 6.944057531654835e-06

Training epoch-34 batch-157
Running loss of epoch-34 batch-157 = 1.0617077350616455e-05

Finished training epoch-34.



Average train loss at epoch-34 = 1.0645966231822968e-05

Started Evaluation

Average val loss at epoch-34 = 4.968222922400424

Accuracy for classes:
Accuracy for class equals is: 63.04 %
Accuracy for class main is: 45.08 %
Accuracy for class setUp is: 51.31 %
Accuracy for class onCreate is: 35.29 %
Accuracy for class toString is: 41.64 %
Accuracy for class run is: 30.14 %
Accuracy for class hashCode is: 76.03 %
Accuracy for class init is: 26.46 %
Accuracy for class execute is: 18.47 %
Accuracy for class get is: 26.67 %

Overall Accuracy = 41.80 %

Finished Evaluation



Started training epoch-35


Training epoch-35 batch-1
Running loss of epoch-35 batch-1 = 1.395191065967083e-05

Training epoch-35 batch-2
Running loss of epoch-35 batch-2 = 4.346948117017746e-06

Training epoch-35 batch-3
Running loss of epoch-35 batch-3 = 3.2508978620171547e-06

Training epoch-35 batch-4
Running loss of epoch-35 batch-4 = 1.107039861381054e-05

Training epoch-35 batch-5
Running loss of epoch-35 batch-5 = 8.39086715131998e-06

Training epoch-35 batch-6
Running loss of epoch-35 batch-6 = 3.5904813557863235e-06

Training epoch-35 batch-7
Running loss of epoch-35 batch-7 = 1.3114768080413342e-05

Training epoch-35 batch-8
Running loss of epoch-35 batch-8 = 3.3529940992593765e-06

Training epoch-35 batch-9
Running loss of epoch-35 batch-9 = 5.559180863201618e-06

Training epoch-35 batch-10
Running loss of epoch-35 batch-10 = 1.808523666113615e-05

Training epoch-35 batch-11
Running loss of epoch-35 batch-11 = 9.730574674904346e-06

Training epoch-35 batch-12
Running loss of epoch-35 batch-12 = 2.474652137607336e-05

Training epoch-35 batch-13
Running loss of epoch-35 batch-13 = 5.289795808494091e-06

Training epoch-35 batch-14
Running loss of epoch-35 batch-14 = 5.217269062995911e-06

Training epoch-35 batch-15
Running loss of epoch-35 batch-15 = 1.80430943146348e-05

Training epoch-35 batch-16
Running loss of epoch-35 batch-16 = 9.750481694936752e-06

Training epoch-35 batch-17
Running loss of epoch-35 batch-17 = 6.236846093088388e-05

Training epoch-35 batch-18
Running loss of epoch-35 batch-18 = 1.2663891538977623e-05

Training epoch-35 batch-19
Running loss of epoch-35 batch-19 = 3.264984115958214e-06

Training epoch-35 batch-20
Running loss of epoch-35 batch-20 = 8.921138942241669e-06

Training epoch-35 batch-21
Running loss of epoch-35 batch-21 = 8.156988769769669e-06

Training epoch-35 batch-22
Running loss of epoch-35 batch-22 = 3.598630428314209e-06

Training epoch-35 batch-23
Running loss of epoch-35 batch-23 = 1.581176184117794e-05

Training epoch-35 batch-24
Running loss of epoch-35 batch-24 = 4.839152097702026e-06

Training epoch-35 batch-25
Running loss of epoch-35 batch-25 = 2.915330696851015e-05

Training epoch-35 batch-26
Running loss of epoch-35 batch-26 = 4.778383299708366e-06

Training epoch-35 batch-27
Running loss of epoch-35 batch-27 = 9.334995411336422e-06

Training epoch-35 batch-28
Running loss of epoch-35 batch-28 = 1.1055613867938519e-05

Training epoch-35 batch-29
Running loss of epoch-35 batch-29 = 6.380956619977951e-06

Training epoch-35 batch-30
Running loss of epoch-35 batch-30 = 6.9158850237727165e-06

Training epoch-35 batch-31
Running loss of epoch-35 batch-31 = 5.504232831299305e-06

Training epoch-35 batch-32
Running loss of epoch-35 batch-32 = 7.91309867054224e-06

Training epoch-35 batch-33
Running loss of epoch-35 batch-33 = 4.789792001247406e-06

Training epoch-35 batch-34
Running loss of epoch-35 batch-34 = 6.093294359743595e-06

Training epoch-35 batch-35
Running loss of epoch-35 batch-35 = 2.6973430067300797e-06

Training epoch-35 batch-36
Running loss of epoch-35 batch-36 = 1.2162025086581707e-05

Training epoch-35 batch-37
Running loss of epoch-35 batch-37 = 5.499226972460747e-06

Training epoch-35 batch-38
Running loss of epoch-35 batch-38 = 6.629270501434803e-06

Training epoch-35 batch-39
Running loss of epoch-35 batch-39 = 7.341266609728336e-06

Training epoch-35 batch-40
Running loss of epoch-35 batch-40 = 2.0059989765286446e-05

Training epoch-35 batch-41
Running loss of epoch-35 batch-41 = 8.284230716526508e-06

Training epoch-35 batch-42
Running loss of epoch-35 batch-42 = 2.505723387002945e-06

Training epoch-35 batch-43
Running loss of epoch-35 batch-43 = 3.745313733816147e-06

Training epoch-35 batch-44
Running loss of epoch-35 batch-44 = 1.4721415936946869e-05

Training epoch-35 batch-45
Running loss of epoch-35 batch-45 = 7.40645918995142e-06

Training epoch-35 batch-46
Running loss of epoch-35 batch-46 = 3.480818122625351e-06

Training epoch-35 batch-47
Running loss of epoch-35 batch-47 = 3.907596692442894e-06

Training epoch-35 batch-48
Running loss of epoch-35 batch-48 = 3.091758117079735e-06

Training epoch-35 batch-49
Running loss of epoch-35 batch-49 = 5.479319952428341e-06

Training epoch-35 batch-50
Running loss of epoch-35 batch-50 = 1.6633421182632446e-06

Training epoch-35 batch-51
Running loss of epoch-35 batch-51 = 4.271743819117546e-06

Training epoch-35 batch-52
Running loss of epoch-35 batch-52 = 4.625529982149601e-06

Training epoch-35 batch-53
Running loss of epoch-35 batch-53 = 3.030989319086075e-06

Training epoch-35 batch-54
Running loss of epoch-35 batch-54 = 2.582673914730549e-06

Training epoch-35 batch-55
Running loss of epoch-35 batch-55 = 7.0712994784116745e-06

Training epoch-35 batch-56
Running loss of epoch-35 batch-56 = 4.843110218644142e-06

Training epoch-35 batch-57
Running loss of epoch-35 batch-57 = 5.2265822887420654e-06

Training epoch-35 batch-58
Running loss of epoch-35 batch-58 = 2.9439106583595276e-06

Training epoch-35 batch-59
Running loss of epoch-35 batch-59 = 2.398155629634857e-06

Training epoch-35 batch-60
Running loss of epoch-35 batch-60 = 1.5258090570569038e-05

Training epoch-35 batch-61
Running loss of epoch-35 batch-61 = 1.5561236068606377e-05

Training epoch-35 batch-62
Running loss of epoch-35 batch-62 = 3.2008392736315727e-06

Training epoch-35 batch-63
Running loss of epoch-35 batch-63 = 5.778507329523563e-06

Training epoch-35 batch-64
Running loss of epoch-35 batch-64 = 4.6782661229372025e-06

Training epoch-35 batch-65
Running loss of epoch-35 batch-65 = 7.581198588013649e-06

Training epoch-35 batch-66
Running loss of epoch-35 batch-66 = 1.2477976270020008e-05

Training epoch-35 batch-67
Running loss of epoch-35 batch-67 = 8.036731742322445e-06

Training epoch-35 batch-68
Running loss of epoch-35 batch-68 = 4.831235855817795e-06

Training epoch-35 batch-69
Running loss of epoch-35 batch-69 = 1.0520918294787407e-05

Training epoch-35 batch-70
Running loss of epoch-35 batch-70 = 4.293397068977356e-06

Training epoch-35 batch-71
Running loss of epoch-35 batch-71 = 4.0495069697499275e-06

Training epoch-35 batch-72
Running loss of epoch-35 batch-72 = 9.614741429686546e-06

Training epoch-35 batch-73
Running loss of epoch-35 batch-73 = 3.7335557863116264e-06

Training epoch-35 batch-74
Running loss of epoch-35 batch-74 = 8.362694643437862e-06

Training epoch-35 batch-75
Running loss of epoch-35 batch-75 = 2.2477470338344574e-06

Training epoch-35 batch-76
Running loss of epoch-35 batch-76 = 4.008994437754154e-05

Training epoch-35 batch-77
Running loss of epoch-35 batch-77 = 7.346505299210548e-06

Training epoch-35 batch-78
Running loss of epoch-35 batch-78 = 1.114560291171074e-06

Training epoch-35 batch-79
Running loss of epoch-35 batch-79 = 4.228251054883003e-05

Training epoch-35 batch-80
Running loss of epoch-35 batch-80 = 2.6349443942308426e-06

Training epoch-35 batch-81
Running loss of epoch-35 batch-81 = 7.095979526638985e-06

Training epoch-35 batch-82
Running loss of epoch-35 batch-82 = 1.7351005226373672e-05

Training epoch-35 batch-83
Running loss of epoch-35 batch-83 = 5.914247594773769e-06

Training epoch-35 batch-84
Running loss of epoch-35 batch-84 = 2.27814307436347e-05

Training epoch-35 batch-85
Running loss of epoch-35 batch-85 = 6.621587090194225e-06

Training epoch-35 batch-86
Running loss of epoch-35 batch-86 = 3.923662006855011e-06

Training epoch-35 batch-87
Running loss of epoch-35 batch-87 = 1.4827004633843899e-05

Training epoch-35 batch-88
Running loss of epoch-35 batch-88 = 8.139642886817455e-06

Training epoch-35 batch-89
Running loss of epoch-35 batch-89 = 5.882466211915016e-06

Training epoch-35 batch-90
Running loss of epoch-35 batch-90 = 3.130990080535412e-06

Training epoch-35 batch-91
Running loss of epoch-35 batch-91 = 7.614609785377979e-06

Training epoch-35 batch-92
Running loss of epoch-35 batch-92 = 5.920650437474251e-06

Training epoch-35 batch-93
Running loss of epoch-35 batch-93 = 4.946254193782806e-06

Training epoch-35 batch-94
Running loss of epoch-35 batch-94 = 9.359675459563732e-06

Training epoch-35 batch-95
Running loss of epoch-35 batch-95 = 8.6913350969553e-06

Training epoch-35 batch-96
Running loss of epoch-35 batch-96 = 2.986169420182705e-06

Training epoch-35 batch-97
Running loss of epoch-35 batch-97 = 2.0696083083748817e-05

Training epoch-35 batch-98
Running loss of epoch-35 batch-98 = 1.486798282712698e-05

Training epoch-35 batch-99
Running loss of epoch-35 batch-99 = 4.710047505795956e-06

Training epoch-35 batch-100
Running loss of epoch-35 batch-100 = 2.8043636120855808e-05

Training epoch-35 batch-101
Running loss of epoch-35 batch-101 = 1.0710791684687138e-05

Training epoch-35 batch-102
Running loss of epoch-35 batch-102 = 8.546747267246246e-06

Training epoch-35 batch-103
Running loss of epoch-35 batch-103 = 3.0997907742857933e-06

Training epoch-35 batch-104
Running loss of epoch-35 batch-104 = 2.7799163945019245e-05

Training epoch-35 batch-105
Running loss of epoch-35 batch-105 = 5.109235644340515e-06

Training epoch-35 batch-106
Running loss of epoch-35 batch-106 = 3.0116294510662556e-05

Training epoch-35 batch-107
Running loss of epoch-35 batch-107 = 1.778744626790285e-05

Training epoch-35 batch-108
Running loss of epoch-35 batch-108 = 9.019975550472736e-06

Training epoch-35 batch-109
Running loss of epoch-35 batch-109 = 4.031462594866753e-06

Training epoch-35 batch-110
Running loss of epoch-35 batch-110 = 4.02319710701704e-06

Training epoch-35 batch-111
Running loss of epoch-35 batch-111 = 3.965920768678188e-06

Training epoch-35 batch-112
Running loss of epoch-35 batch-112 = 1.3153883628547192e-05

Training epoch-35 batch-113
Running loss of epoch-35 batch-113 = 1.0218704119324684e-05

Training epoch-35 batch-114
Running loss of epoch-35 batch-114 = 8.99704173207283e-06

Training epoch-35 batch-115
Running loss of epoch-35 batch-115 = 3.713183104991913e-06

Training epoch-35 batch-116
Running loss of epoch-35 batch-116 = 9.094947017729282e-06

Training epoch-35 batch-117
Running loss of epoch-35 batch-117 = 5.212810356169939e-05

Training epoch-35 batch-118
Running loss of epoch-35 batch-118 = 6.375368684530258e-06

Training epoch-35 batch-119
Running loss of epoch-35 batch-119 = 7.08969309926033e-06

Training epoch-35 batch-120
Running loss of epoch-35 batch-120 = 9.151408448815346e-06

Training epoch-35 batch-121
Running loss of epoch-35 batch-121 = 2.7707312256097794e-05

Training epoch-35 batch-122
Running loss of epoch-35 batch-122 = 1.0676681995391846e-05

Training epoch-35 batch-123
Running loss of epoch-35 batch-123 = 2.720160409808159e-06

Training epoch-35 batch-124
Running loss of epoch-35 batch-124 = 1.226633321493864e-05

Training epoch-35 batch-125
Running loss of epoch-35 batch-125 = 7.999129593372345e-06

Training epoch-35 batch-126
Running loss of epoch-35 batch-126 = 2.9888469725847244e-06

Training epoch-35 batch-127
Running loss of epoch-35 batch-127 = 4.598172381520271e-06

Training epoch-35 batch-128
Running loss of epoch-35 batch-128 = 5.548819899559021e-06

Training epoch-35 batch-129
Running loss of epoch-35 batch-129 = 3.825174644589424e-06

Training epoch-35 batch-130
Running loss of epoch-35 batch-130 = 6.2051694840192795e-06

Training epoch-35 batch-131
Running loss of epoch-35 batch-131 = 8.438597433269024e-06

Training epoch-35 batch-132
Running loss of epoch-35 batch-132 = 1.6262754797935486e-05

Training epoch-35 batch-133
Running loss of epoch-35 batch-133 = 1.749349758028984e-05

Training epoch-35 batch-134
Running loss of epoch-35 batch-134 = 3.838213160634041e-06

Training epoch-35 batch-135
Running loss of epoch-35 batch-135 = 6.571877747774124e-06

Training epoch-35 batch-136
Running loss of epoch-35 batch-136 = 2.436107024550438e-06

Training epoch-35 batch-137
Running loss of epoch-35 batch-137 = 3.4795375540852547e-06

Training epoch-35 batch-138
Running loss of epoch-35 batch-138 = 2.0461506210267544e-05

Training epoch-35 batch-139
Running loss of epoch-35 batch-139 = 5.592161323875189e-05

Training epoch-35 batch-140
Running loss of epoch-35 batch-140 = 9.391107596457005e-06

Training epoch-35 batch-141
Running loss of epoch-35 batch-141 = 1.4618155546486378e-05

Training epoch-35 batch-142
Running loss of epoch-35 batch-142 = 6.877235136926174e-06

Training epoch-35 batch-143
Running loss of epoch-35 batch-143 = 1.2881821021437645e-05

Training epoch-35 batch-144
Running loss of epoch-35 batch-144 = 9.423005394637585e-06

Training epoch-35 batch-145
Running loss of epoch-35 batch-145 = 1.553306356072426e-05

Training epoch-35 batch-146
Running loss of epoch-35 batch-146 = 9.777257218956947e-06

Training epoch-35 batch-147
Running loss of epoch-35 batch-147 = 6.945338100194931e-06

Training epoch-35 batch-148
Running loss of epoch-35 batch-148 = 3.1191157177090645e-06

Training epoch-35 batch-149
Running loss of epoch-35 batch-149 = 9.889830835163593e-06

Training epoch-35 batch-150
Running loss of epoch-35 batch-150 = 5.102483555674553e-06

Training epoch-35 batch-151
Running loss of epoch-35 batch-151 = 8.307513780891895e-06

Training epoch-35 batch-152
Running loss of epoch-35 batch-152 = 7.608672603964806e-06

Training epoch-35 batch-153
Running loss of epoch-35 batch-153 = 5.018198862671852e-06

Training epoch-35 batch-154
Running loss of epoch-35 batch-154 = 9.37306322157383e-06

Training epoch-35 batch-155
Running loss of epoch-35 batch-155 = 4.600151441991329e-06

Training epoch-35 batch-156
Running loss of epoch-35 batch-156 = 9.13301482796669e-06

Training epoch-35 batch-157
Running loss of epoch-35 batch-157 = 4.439428448677063e-05

Finished training epoch-35.



Average train loss at epoch-35 = 9.946714341640472e-06

Started Evaluation

Average val loss at epoch-35 = 5.023117278751574

Accuracy for classes:
Accuracy for class equals is: 62.87 %
Accuracy for class main is: 45.90 %
Accuracy for class setUp is: 50.82 %
Accuracy for class onCreate is: 34.01 %
Accuracy for class toString is: 41.98 %
Accuracy for class run is: 29.68 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 26.68 %
Accuracy for class execute is: 18.07 %
Accuracy for class get is: 26.92 %

Overall Accuracy = 41.55 %

Finished Evaluation



Started training epoch-36


Training epoch-36 batch-1
Running loss of epoch-36 batch-1 = 4.886183887720108e-06

Training epoch-36 batch-2
Running loss of epoch-36 batch-2 = 1.2893578968942165e-05

Training epoch-36 batch-3
Running loss of epoch-36 batch-3 = 5.314475856721401e-06

Training epoch-36 batch-4
Running loss of epoch-36 batch-4 = 6.7695509642362595e-06

Training epoch-36 batch-5
Running loss of epoch-36 batch-5 = 8.036033250391483e-06

Training epoch-36 batch-6
Running loss of epoch-36 batch-6 = 1.5408731997013092e-06

Training epoch-36 batch-7
Running loss of epoch-36 batch-7 = 9.817536920309067e-06

Training epoch-36 batch-8
Running loss of epoch-36 batch-8 = 5.453592166304588e-06

Training epoch-36 batch-9
Running loss of epoch-36 batch-9 = 4.922039806842804e-06

Training epoch-36 batch-10
Running loss of epoch-36 batch-10 = 2.86975409835577e-06

Training epoch-36 batch-11
Running loss of epoch-36 batch-11 = 1.2560631148517132e-05

Training epoch-36 batch-12
Running loss of epoch-36 batch-12 = 3.5747652873396873e-06

Training epoch-36 batch-13
Running loss of epoch-36 batch-13 = 1.2974254786968231e-05

Training epoch-36 batch-14
Running loss of epoch-36 batch-14 = 9.58761665970087e-06

Training epoch-36 batch-15
Running loss of epoch-36 batch-15 = 1.5272176824510098e-05

Training epoch-36 batch-16
Running loss of epoch-36 batch-16 = 4.878966137766838e-06

Training epoch-36 batch-17
Running loss of epoch-36 batch-17 = 4.190253093838692e-06

Training epoch-36 batch-18
Running loss of epoch-36 batch-18 = 3.155786544084549e-06

Training epoch-36 batch-19
Running loss of epoch-36 batch-19 = 4.325062036514282e-06

Training epoch-36 batch-20
Running loss of epoch-36 batch-20 = 1.3946904800832272e-05

Training epoch-36 batch-21
Running loss of epoch-36 batch-21 = 6.081070750951767e-06

Training epoch-36 batch-22
Running loss of epoch-36 batch-22 = 6.72624446451664e-06

Training epoch-36 batch-23
Running loss of epoch-36 batch-23 = 1.2314412742853165e-06

Training epoch-36 batch-24
Running loss of epoch-36 batch-24 = 4.9960799515247345e-06

Training epoch-36 batch-25
Running loss of epoch-36 batch-25 = 6.029265932738781e-06

Training epoch-36 batch-26
Running loss of epoch-36 batch-26 = 1.6922131180763245e-06

Training epoch-36 batch-27
Running loss of epoch-36 batch-27 = 4.180939868092537e-06

Training epoch-36 batch-28
Running loss of epoch-36 batch-28 = 8.33731610327959e-06

Training epoch-36 batch-29
Running loss of epoch-36 batch-29 = 7.579335942864418e-06

Training epoch-36 batch-30
Running loss of epoch-36 batch-30 = 1.0998337529599667e-05

Training epoch-36 batch-31
Running loss of epoch-36 batch-31 = 5.629844963550568e-06

Training epoch-36 batch-32
Running loss of epoch-36 batch-32 = 1.018540933728218e-05

Training epoch-36 batch-33
Running loss of epoch-36 batch-33 = 2.5443383492529392e-05

Training epoch-36 batch-34
Running loss of epoch-36 batch-34 = 7.746974006295204e-06

Training epoch-36 batch-35
Running loss of epoch-36 batch-35 = 8.625560440123081e-06

Training epoch-36 batch-36
Running loss of epoch-36 batch-36 = 4.258239641785622e-06

Training epoch-36 batch-37
Running loss of epoch-36 batch-37 = 2.7187634259462357e-06

Training epoch-36 batch-38
Running loss of epoch-36 batch-38 = 3.7851277738809586e-06

Training epoch-36 batch-39
Running loss of epoch-36 batch-39 = 1.5003024600446224e-05

Training epoch-36 batch-40
Running loss of epoch-36 batch-40 = 6.024027243256569e-06

Training epoch-36 batch-41
Running loss of epoch-36 batch-41 = 7.901457138359547e-06

Training epoch-36 batch-42
Running loss of epoch-36 batch-42 = 7.553026080131531e-06

Training epoch-36 batch-43
Running loss of epoch-36 batch-43 = 6.866059266030788e-06

Training epoch-36 batch-44
Running loss of epoch-36 batch-44 = 1.6691628843545914e-05

Training epoch-36 batch-45
Running loss of epoch-36 batch-45 = 3.352988278493285e-05

Training epoch-36 batch-46
Running loss of epoch-36 batch-46 = 4.430534318089485e-06

Training epoch-36 batch-47
Running loss of epoch-36 batch-47 = 3.5120174288749695e-06

Training epoch-36 batch-48
Running loss of epoch-36 batch-48 = 1.029821578413248e-05

Training epoch-36 batch-49
Running loss of epoch-36 batch-49 = 8.687609806656837e-06

Training epoch-36 batch-50
Running loss of epoch-36 batch-50 = 1.2974604032933712e-05

Training epoch-36 batch-51
Running loss of epoch-36 batch-51 = 6.163259968161583e-06

Training epoch-36 batch-52
Running loss of epoch-36 batch-52 = 3.1675444915890694e-06

Training epoch-36 batch-53
Running loss of epoch-36 batch-53 = 6.641261279582977e-06

Training epoch-36 batch-54
Running loss of epoch-36 batch-54 = 7.63358548283577e-06

Training epoch-36 batch-55
Running loss of epoch-36 batch-55 = 4.561035893857479e-06

Training epoch-36 batch-56
Running loss of epoch-36 batch-56 = 2.3099128156900406e-06

Training epoch-36 batch-57
Running loss of epoch-36 batch-57 = 7.960130460560322e-06

Training epoch-36 batch-58
Running loss of epoch-36 batch-58 = 4.026107490062714e-06

Training epoch-36 batch-59
Running loss of epoch-36 batch-59 = 2.8795329853892326e-06

Training epoch-36 batch-60
Running loss of epoch-36 batch-60 = 3.850553184747696e-06

Training epoch-36 batch-61
Running loss of epoch-36 batch-61 = 1.2910924851894379e-05

Training epoch-36 batch-62
Running loss of epoch-36 batch-62 = 2.4239998310804367e-06

Training epoch-36 batch-63
Running loss of epoch-36 batch-63 = 5.604350008070469e-06

Training epoch-36 batch-64
Running loss of epoch-36 batch-64 = 4.231929779052734e-06

Training epoch-36 batch-65
Running loss of epoch-36 batch-65 = 5.153589881956577e-06

Training epoch-36 batch-66
Running loss of epoch-36 batch-66 = 5.485839210450649e-06

Training epoch-36 batch-67
Running loss of epoch-36 batch-67 = 1.3076351024210453e-05

Training epoch-36 batch-68
Running loss of epoch-36 batch-68 = 1.4556339010596275e-05

Training epoch-36 batch-69
Running loss of epoch-36 batch-69 = 7.579440716654062e-05

Training epoch-36 batch-70
Running loss of epoch-36 batch-70 = 5.464185960590839e-06

Training epoch-36 batch-71
Running loss of epoch-36 batch-71 = 1.014629378914833e-05

Training epoch-36 batch-72
Running loss of epoch-36 batch-72 = 8.501461707055569e-06

Training epoch-36 batch-73
Running loss of epoch-36 batch-73 = 5.476991645991802e-06

Training epoch-36 batch-74
Running loss of epoch-36 batch-74 = 9.182840585708618e-06

Training epoch-36 batch-75
Running loss of epoch-36 batch-75 = 4.755798727273941e-06

Training epoch-36 batch-76
Running loss of epoch-36 batch-76 = 1.1243391782045364e-06

Training epoch-36 batch-77
Running loss of epoch-36 batch-77 = 2.0418083295226097e-06

Training epoch-36 batch-78
Running loss of epoch-36 batch-78 = 9.582261554896832e-06

Training epoch-36 batch-79
Running loss of epoch-36 batch-79 = 1.945422263815999e-05

Training epoch-36 batch-80
Running loss of epoch-36 batch-80 = 7.041613571345806e-06

Training epoch-36 batch-81
Running loss of epoch-36 batch-81 = 7.456284947693348e-06

Training epoch-36 batch-82
Running loss of epoch-36 batch-82 = 1.759117003530264e-05

Training epoch-36 batch-83
Running loss of epoch-36 batch-83 = 9.680632501840591e-06

Training epoch-36 batch-84
Running loss of epoch-36 batch-84 = 8.437666110694408e-06

Training epoch-36 batch-85
Running loss of epoch-36 batch-85 = 3.2176030799746513e-06

Training epoch-36 batch-86
Running loss of epoch-36 batch-86 = 4.870467819273472e-06

Training epoch-36 batch-87
Running loss of epoch-36 batch-87 = 1.0717776603996754e-05

Training epoch-36 batch-88
Running loss of epoch-36 batch-88 = 1.7549493350088596e-05

Training epoch-36 batch-89
Running loss of epoch-36 batch-89 = 5.299923941493034e-06

Training epoch-36 batch-90
Running loss of epoch-36 batch-90 = 8.005183190107346e-06

Training epoch-36 batch-91
Running loss of epoch-36 batch-91 = 3.825989551842213e-06

Training epoch-36 batch-92
Running loss of epoch-36 batch-92 = 2.7498463168740273e-06

Training epoch-36 batch-93
Running loss of epoch-36 batch-93 = 1.423410139977932e-06

Training epoch-36 batch-94
Running loss of epoch-36 batch-94 = 6.193295121192932e-06

Training epoch-36 batch-95
Running loss of epoch-36 batch-95 = 3.414286766201258e-05

Training epoch-36 batch-96
Running loss of epoch-36 batch-96 = 9.092967957258224e-06

Training epoch-36 batch-97
Running loss of epoch-36 batch-97 = 2.4747569113969803e-05

Training epoch-36 batch-98
Running loss of epoch-36 batch-98 = 3.423541784286499e-06

Training epoch-36 batch-99
Running loss of epoch-36 batch-99 = 8.56444239616394e-06

Training epoch-36 batch-100
Running loss of epoch-36 batch-100 = 6.502261385321617e-06

Training epoch-36 batch-101
Running loss of epoch-36 batch-101 = 3.2940879464149475e-06

Training epoch-36 batch-102
Running loss of epoch-36 batch-102 = 7.943948730826378e-06

Training epoch-36 batch-103
Running loss of epoch-36 batch-103 = 5.110166966915131e-06

Training epoch-36 batch-104
Running loss of epoch-36 batch-104 = 6.143469363451004e-06

Training epoch-36 batch-105
Running loss of epoch-36 batch-105 = 3.4446129575371742e-06

Training epoch-36 batch-106
Running loss of epoch-36 batch-106 = 3.709341399371624e-06

Training epoch-36 batch-107
Running loss of epoch-36 batch-107 = 3.151595592498779e-05

Training epoch-36 batch-108
Running loss of epoch-36 batch-108 = 5.65103255212307e-06

Training epoch-36 batch-109
Running loss of epoch-36 batch-109 = 8.769566193223e-06

Training epoch-36 batch-110
Running loss of epoch-36 batch-110 = 5.290494300425053e-06

Training epoch-36 batch-111
Running loss of epoch-36 batch-111 = 2.6801135390996933e-06

Training epoch-36 batch-112
Running loss of epoch-36 batch-112 = 6.822054274380207e-06

Training epoch-36 batch-113
Running loss of epoch-36 batch-113 = 3.493507392704487e-06

Training epoch-36 batch-114
Running loss of epoch-36 batch-114 = 1.7219805158674717e-05

Training epoch-36 batch-115
Running loss of epoch-36 batch-115 = 3.894197288900614e-05

Training epoch-36 batch-116
Running loss of epoch-36 batch-116 = 4.011206328868866e-06

Training epoch-36 batch-117
Running loss of epoch-36 batch-117 = 4.574190825223923e-06

Training epoch-36 batch-118
Running loss of epoch-36 batch-118 = 7.868860848248005e-06

Training epoch-36 batch-119
Running loss of epoch-36 batch-119 = 3.6116689443588257e-06

Training epoch-36 batch-120
Running loss of epoch-36 batch-120 = 1.4288700185716152e-05

Training epoch-36 batch-121
Running loss of epoch-36 batch-121 = 7.113092578947544e-06

Training epoch-36 batch-122
Running loss of epoch-36 batch-122 = 3.3421674743294716e-06

Training epoch-36 batch-123
Running loss of epoch-36 batch-123 = 5.2837422117590904e-06

Training epoch-36 batch-124
Running loss of epoch-36 batch-124 = 3.487803041934967e-06

Training epoch-36 batch-125
Running loss of epoch-36 batch-125 = 2.390914596617222e-05

Training epoch-36 batch-126
Running loss of epoch-36 batch-126 = 6.492133252322674e-06

Training epoch-36 batch-127
Running loss of epoch-36 batch-127 = 1.762830652296543e-05

Training epoch-36 batch-128
Running loss of epoch-36 batch-128 = 8.287257514894009e-06

Training epoch-36 batch-129
Running loss of epoch-36 batch-129 = 1.8205959349870682e-05

Training epoch-36 batch-130
Running loss of epoch-36 batch-130 = 4.754401743412018e-06

Training epoch-36 batch-131
Running loss of epoch-36 batch-131 = 2.4918699637055397e-06

Training epoch-36 batch-132
Running loss of epoch-36 batch-132 = 5.861028330400586e-05

Training epoch-36 batch-133
Running loss of epoch-36 batch-133 = 6.877584382891655e-06

Training epoch-36 batch-134
Running loss of epoch-36 batch-134 = 4.778383299708366e-06

Training epoch-36 batch-135
Running loss of epoch-36 batch-135 = 5.814363248646259e-06

Training epoch-36 batch-136
Running loss of epoch-36 batch-136 = 5.364418029785156e-06

Training epoch-36 batch-137
Running loss of epoch-36 batch-137 = 2.4339184165000916e-05

Training epoch-36 batch-138
Running loss of epoch-36 batch-138 = 6.7302025854587555e-06

Training epoch-36 batch-139
Running loss of epoch-36 batch-139 = 1.6730977222323418e-05

Training epoch-36 batch-140
Running loss of epoch-36 batch-140 = 4.287459887564182e-06

Training epoch-36 batch-141
Running loss of epoch-36 batch-141 = 5.2282121032476425e-06

Training epoch-36 batch-142
Running loss of epoch-36 batch-142 = 2.254487480968237e-05

Training epoch-36 batch-143
Running loss of epoch-36 batch-143 = 4.653353244066238e-06

Training epoch-36 batch-144
Running loss of epoch-36 batch-144 = 4.7674402594566345e-06

Training epoch-36 batch-145
Running loss of epoch-36 batch-145 = 7.817638106644154e-06

Training epoch-36 batch-146
Running loss of epoch-36 batch-146 = 1.0761665180325508e-05

Training epoch-36 batch-147
Running loss of epoch-36 batch-147 = 1.2380769476294518e-05

Training epoch-36 batch-148
Running loss of epoch-36 batch-148 = 4.822388291358948e-06

Training epoch-36 batch-149
Running loss of epoch-36 batch-149 = 1.3137818314135075e-05

Training epoch-36 batch-150
Running loss of epoch-36 batch-150 = 3.455905243754387e-06

Training epoch-36 batch-151
Running loss of epoch-36 batch-151 = 2.8102658689022064e-06

Training epoch-36 batch-152
Running loss of epoch-36 batch-152 = 2.1746964193880558e-05

Training epoch-36 batch-153
Running loss of epoch-36 batch-153 = 8.906936272978783e-06

Training epoch-36 batch-154
Running loss of epoch-36 batch-154 = 5.614710971713066e-06

Training epoch-36 batch-155
Running loss of epoch-36 batch-155 = 2.3439060896635056e-06

Training epoch-36 batch-156
Running loss of epoch-36 batch-156 = 8.480274118483067e-06

Training epoch-36 batch-157
Running loss of epoch-36 batch-157 = 8.624047040939331e-06

Finished training epoch-36.



Average train loss at epoch-36 = 9.164321050047874e-06

Started Evaluation

Average val loss at epoch-36 = 5.037334787218194

Accuracy for classes:
Accuracy for class equals is: 61.88 %
Accuracy for class main is: 43.28 %
Accuracy for class setUp is: 50.66 %
Accuracy for class onCreate is: 35.71 %
Accuracy for class toString is: 41.30 %
Accuracy for class run is: 30.37 %
Accuracy for class hashCode is: 75.66 %
Accuracy for class init is: 29.60 %
Accuracy for class execute is: 18.07 %
Accuracy for class get is: 26.15 %

Overall Accuracy = 41.63 %

Finished Evaluation



Started training epoch-37

