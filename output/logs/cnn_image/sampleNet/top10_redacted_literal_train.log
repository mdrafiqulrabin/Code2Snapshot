
DataLoader:

Started trainloader.
trainset = #10000
Finished trainloader.
Started valloader.
valset = #4847
Finished valloader.

Configuration:

DB_NAME = java-top10, IMG_TYPE = redacted_literal, TRANS_SIZE = 1024
MAX_EPOCH = 100, BATCH_SIZE = 64

device = cuda:0



Started training epoch-1


Training epoch-1 batch-1
Running loss of epoch-1 batch-1 = 0.035944562405347824

Training epoch-1 batch-2
Running loss of epoch-1 batch-2 = 0.03595472499728203

Training epoch-1 batch-3
Running loss of epoch-1 batch-3 = 0.03583276644349098

Training epoch-1 batch-4
Running loss of epoch-1 batch-4 = 0.03599812835454941

Training epoch-1 batch-5
Running loss of epoch-1 batch-5 = 0.03608952835202217

Training epoch-1 batch-6
Running loss of epoch-1 batch-6 = 0.036003388464450836

Training epoch-1 batch-7
Running loss of epoch-1 batch-7 = 0.035787153989076614

Training epoch-1 batch-8
Running loss of epoch-1 batch-8 = 0.03593401238322258

Training epoch-1 batch-9
Running loss of epoch-1 batch-9 = 0.03601936623454094

Training epoch-1 batch-10
Running loss of epoch-1 batch-10 = 0.03613732382655144

Training epoch-1 batch-11
Running loss of epoch-1 batch-11 = 0.03618156909942627

Training epoch-1 batch-12
Running loss of epoch-1 batch-12 = 0.03612501174211502

Training epoch-1 batch-13
Running loss of epoch-1 batch-13 = 0.036102477461099625

Training epoch-1 batch-14
Running loss of epoch-1 batch-14 = 0.036233577877283096

Training epoch-1 batch-15
Running loss of epoch-1 batch-15 = 0.035960808396339417

Training epoch-1 batch-16
Running loss of epoch-1 batch-16 = 0.036231108009815216

Training epoch-1 batch-17
Running loss of epoch-1 batch-17 = 0.0357092060148716

Training epoch-1 batch-18
Running loss of epoch-1 batch-18 = 0.03611842915415764

Training epoch-1 batch-19
Running loss of epoch-1 batch-19 = 0.035861190408468246

Training epoch-1 batch-20
Running loss of epoch-1 batch-20 = 0.03594173863530159

Training epoch-1 batch-21
Running loss of epoch-1 batch-21 = 0.03604605048894882

Training epoch-1 batch-22
Running loss of epoch-1 batch-22 = 0.03581083565950394

Training epoch-1 batch-23
Running loss of epoch-1 batch-23 = 0.03628367558121681

Training epoch-1 batch-24
Running loss of epoch-1 batch-24 = 0.03637375310063362

Training epoch-1 batch-25
Running loss of epoch-1 batch-25 = 0.03586087375879288

Training epoch-1 batch-26
Running loss of epoch-1 batch-26 = 0.03591519966721535

Training epoch-1 batch-27
Running loss of epoch-1 batch-27 = 0.03568670526146889

Training epoch-1 batch-28
Running loss of epoch-1 batch-28 = 0.03615447133779526

Training epoch-1 batch-29
Running loss of epoch-1 batch-29 = 0.035685133188962936

Training epoch-1 batch-30
Running loss of epoch-1 batch-30 = 0.03623182699084282

Training epoch-1 batch-31
Running loss of epoch-1 batch-31 = 0.035960227251052856

Training epoch-1 batch-32
Running loss of epoch-1 batch-32 = 0.03609703481197357

Training epoch-1 batch-33
Running loss of epoch-1 batch-33 = 0.035919394344091415

Training epoch-1 batch-34
Running loss of epoch-1 batch-34 = 0.03617669269442558

Training epoch-1 batch-35
Running loss of epoch-1 batch-35 = 0.03608855977654457

Training epoch-1 batch-36
Running loss of epoch-1 batch-36 = 0.035999007523059845

Training epoch-1 batch-37
Running loss of epoch-1 batch-37 = 0.03620190918445587

Training epoch-1 batch-38
Running loss of epoch-1 batch-38 = 0.03596784546971321

Training epoch-1 batch-39
Running loss of epoch-1 batch-39 = 0.03597379848361015

Training epoch-1 batch-40
Running loss of epoch-1 batch-40 = 0.03584856539964676

Training epoch-1 batch-41
Running loss of epoch-1 batch-41 = 0.03603276610374451

Training epoch-1 batch-42
Running loss of epoch-1 batch-42 = 0.035929229110479355

Training epoch-1 batch-43
Running loss of epoch-1 batch-43 = 0.035905178636312485

Training epoch-1 batch-44
Running loss of epoch-1 batch-44 = 0.035896897315979004

Training epoch-1 batch-45
Running loss of epoch-1 batch-45 = 0.03605261445045471

Training epoch-1 batch-46
Running loss of epoch-1 batch-46 = 0.03584323078393936

Training epoch-1 batch-47
Running loss of epoch-1 batch-47 = 0.03602844476699829

Training epoch-1 batch-48
Running loss of epoch-1 batch-48 = 0.036128733307123184

Training epoch-1 batch-49
Running loss of epoch-1 batch-49 = 0.03625214099884033

Training epoch-1 batch-50
Running loss of epoch-1 batch-50 = 0.03611772134900093

Training epoch-1 batch-51
Running loss of epoch-1 batch-51 = 0.03604168817400932

Training epoch-1 batch-52
Running loss of epoch-1 batch-52 = 0.03595912083983421

Training epoch-1 batch-53
Running loss of epoch-1 batch-53 = 0.03567752242088318

Training epoch-1 batch-54
Running loss of epoch-1 batch-54 = 0.03608323261141777

Training epoch-1 batch-55
Running loss of epoch-1 batch-55 = 0.036077599972486496

Training epoch-1 batch-56
Running loss of epoch-1 batch-56 = 0.03604009002447128

Training epoch-1 batch-57
Running loss of epoch-1 batch-57 = 0.03616992011666298

Training epoch-1 batch-58
Running loss of epoch-1 batch-58 = 0.03593084216117859

Training epoch-1 batch-59
Running loss of epoch-1 batch-59 = 0.03602046146988869

Training epoch-1 batch-60
Running loss of epoch-1 batch-60 = 0.03607841953635216

Training epoch-1 batch-61
Running loss of epoch-1 batch-61 = 0.03591078892350197

Training epoch-1 batch-62
Running loss of epoch-1 batch-62 = 0.03590052202343941

Training epoch-1 batch-63
Running loss of epoch-1 batch-63 = 0.03571418300271034

Training epoch-1 batch-64
Running loss of epoch-1 batch-64 = 0.03602319955825806

Training epoch-1 batch-65
Running loss of epoch-1 batch-65 = 0.036099132150411606

Training epoch-1 batch-66
Running loss of epoch-1 batch-66 = 0.03566247597336769

Training epoch-1 batch-67
Running loss of epoch-1 batch-67 = 0.03587823733687401

Training epoch-1 batch-68
Running loss of epoch-1 batch-68 = 0.03571547940373421

Training epoch-1 batch-69
Running loss of epoch-1 batch-69 = 0.03562528267502785

Training epoch-1 batch-70
Running loss of epoch-1 batch-70 = 0.03594784066081047

Training epoch-1 batch-71
Running loss of epoch-1 batch-71 = 0.03597916290163994

Training epoch-1 batch-72
Running loss of epoch-1 batch-72 = 0.03609013184905052

Training epoch-1 batch-73
Running loss of epoch-1 batch-73 = 0.036125071346759796

Training epoch-1 batch-74
Running loss of epoch-1 batch-74 = 0.03605777397751808

Training epoch-1 batch-75
Running loss of epoch-1 batch-75 = 0.03610556200146675

Training epoch-1 batch-76
Running loss of epoch-1 batch-76 = 0.03599414974451065

Training epoch-1 batch-77
Running loss of epoch-1 batch-77 = 0.036284565925598145

Training epoch-1 batch-78
Running loss of epoch-1 batch-78 = 0.036299511790275574

Training epoch-1 batch-79
Running loss of epoch-1 batch-79 = 0.03635557368397713

Training epoch-1 batch-80
Running loss of epoch-1 batch-80 = 0.035904206335544586

Training epoch-1 batch-81
Running loss of epoch-1 batch-81 = 0.03596371412277222

Training epoch-1 batch-82
Running loss of epoch-1 batch-82 = 0.03581612929701805

Training epoch-1 batch-83
Running loss of epoch-1 batch-83 = 0.03584631532430649

Training epoch-1 batch-84
Running loss of epoch-1 batch-84 = 0.03592720627784729

Training epoch-1 batch-85
Running loss of epoch-1 batch-85 = 0.03594493493437767

Training epoch-1 batch-86
Running loss of epoch-1 batch-86 = 0.03607241064310074

Training epoch-1 batch-87
Running loss of epoch-1 batch-87 = 0.03587782010436058

Training epoch-1 batch-88
Running loss of epoch-1 batch-88 = 0.03591761738061905

Training epoch-1 batch-89
Running loss of epoch-1 batch-89 = 0.03586005046963692

Training epoch-1 batch-90
Running loss of epoch-1 batch-90 = 0.03584996983408928

Training epoch-1 batch-91
Running loss of epoch-1 batch-91 = 0.03581142798066139

Training epoch-1 batch-92
Running loss of epoch-1 batch-92 = 0.03580758348107338

Training epoch-1 batch-93
Running loss of epoch-1 batch-93 = 0.03608695790171623

Training epoch-1 batch-94
Running loss of epoch-1 batch-94 = 0.035925619304180145

Training epoch-1 batch-95
Running loss of epoch-1 batch-95 = 0.03571773320436478

Training epoch-1 batch-96
Running loss of epoch-1 batch-96 = 0.03570445254445076

Training epoch-1 batch-97
Running loss of epoch-1 batch-97 = 0.035822100937366486

Training epoch-1 batch-98
Running loss of epoch-1 batch-98 = 0.03557988256216049

Training epoch-1 batch-99
Running loss of epoch-1 batch-99 = 0.036026403307914734

Training epoch-1 batch-100
Running loss of epoch-1 batch-100 = 0.036064621061086655

Training epoch-1 batch-101
Running loss of epoch-1 batch-101 = 0.035969629883766174

Training epoch-1 batch-102
Running loss of epoch-1 batch-102 = 0.035693664103746414

Training epoch-1 batch-103
Running loss of epoch-1 batch-103 = 0.035854190587997437

Training epoch-1 batch-104
Running loss of epoch-1 batch-104 = 0.035986240953207016

Training epoch-1 batch-105
Running loss of epoch-1 batch-105 = 0.035932306200265884

Training epoch-1 batch-106
Running loss of epoch-1 batch-106 = 0.035840101540088654

Training epoch-1 batch-107
Running loss of epoch-1 batch-107 = 0.03591274470090866

Training epoch-1 batch-108
Running loss of epoch-1 batch-108 = 0.03580408915877342

Training epoch-1 batch-109
Running loss of epoch-1 batch-109 = 0.03589499369263649

Training epoch-1 batch-110
Running loss of epoch-1 batch-110 = 0.03593350946903229

Training epoch-1 batch-111
Running loss of epoch-1 batch-111 = 0.035895757377147675

Training epoch-1 batch-112
Running loss of epoch-1 batch-112 = 0.03610236197710037

Training epoch-1 batch-113
Running loss of epoch-1 batch-113 = 0.03596474230289459

Training epoch-1 batch-114
Running loss of epoch-1 batch-114 = 0.03571324050426483

Training epoch-1 batch-115
Running loss of epoch-1 batch-115 = 0.03584233298897743

Training epoch-1 batch-116
Running loss of epoch-1 batch-116 = 0.035908304154872894

Training epoch-1 batch-117
Running loss of epoch-1 batch-117 = 0.035731036216020584

Training epoch-1 batch-118
Running loss of epoch-1 batch-118 = 0.035710547119379044

Training epoch-1 batch-119
Running loss of epoch-1 batch-119 = 0.03578596189618111

Training epoch-1 batch-120
Running loss of epoch-1 batch-120 = 0.03593667969107628

Training epoch-1 batch-121
Running loss of epoch-1 batch-121 = 0.035914674401283264

Training epoch-1 batch-122
Running loss of epoch-1 batch-122 = 0.0358525887131691

Training epoch-1 batch-123
Running loss of epoch-1 batch-123 = 0.03585910424590111

Training epoch-1 batch-124
Running loss of epoch-1 batch-124 = 0.035877685993909836

Training epoch-1 batch-125
Running loss of epoch-1 batch-125 = 0.035761378705501556

Training epoch-1 batch-126
Running loss of epoch-1 batch-126 = 0.035777200013399124

Training epoch-1 batch-127
Running loss of epoch-1 batch-127 = 0.03578178212046623

Training epoch-1 batch-128
Running loss of epoch-1 batch-128 = 0.03594430163502693

Training epoch-1 batch-129
Running loss of epoch-1 batch-129 = 0.03576749190688133

Training epoch-1 batch-130
Running loss of epoch-1 batch-130 = 0.03604888543486595

Training epoch-1 batch-131
Running loss of epoch-1 batch-131 = 0.03547213599085808

Training epoch-1 batch-132
Running loss of epoch-1 batch-132 = 0.035742443054914474

Training epoch-1 batch-133
Running loss of epoch-1 batch-133 = 0.03582131117582321

Training epoch-1 batch-134
Running loss of epoch-1 batch-134 = 0.03578558191657066

Training epoch-1 batch-135
Running loss of epoch-1 batch-135 = 0.03586026653647423

Training epoch-1 batch-136
Running loss of epoch-1 batch-136 = 0.035549312829971313

Training epoch-1 batch-137
Running loss of epoch-1 batch-137 = 0.03589702397584915

Training epoch-1 batch-138
Running loss of epoch-1 batch-138 = 0.035671863704919815

Training epoch-1 batch-139
Running loss of epoch-1 batch-139 = 0.0357171893119812

Training epoch-1 batch-140
Running loss of epoch-1 batch-140 = 0.035904623568058014

Training epoch-1 batch-141
Running loss of epoch-1 batch-141 = 0.0357750840485096

Training epoch-1 batch-142
Running loss of epoch-1 batch-142 = 0.03566154092550278

Training epoch-1 batch-143
Running loss of epoch-1 batch-143 = 0.035749759525060654

Training epoch-1 batch-144
Running loss of epoch-1 batch-144 = 0.03575274348258972

Training epoch-1 batch-145
Running loss of epoch-1 batch-145 = 0.03578875586390495

Training epoch-1 batch-146
Running loss of epoch-1 batch-146 = 0.035867925733327866

Training epoch-1 batch-147
Running loss of epoch-1 batch-147 = 0.03574250265955925

Training epoch-1 batch-148
Running loss of epoch-1 batch-148 = 0.03576018661260605

Training epoch-1 batch-149
Running loss of epoch-1 batch-149 = 0.03566138073801994

Training epoch-1 batch-150
Running loss of epoch-1 batch-150 = 0.03567938879132271

Training epoch-1 batch-151
Running loss of epoch-1 batch-151 = 0.035681888461112976

Training epoch-1 batch-152
Running loss of epoch-1 batch-152 = 0.035860635340213776

Training epoch-1 batch-153
Running loss of epoch-1 batch-153 = 0.03569331765174866

Training epoch-1 batch-154
Running loss of epoch-1 batch-154 = 0.03558124601840973

Training epoch-1 batch-155
Running loss of epoch-1 batch-155 = 0.035648856312036514

Training epoch-1 batch-156
Running loss of epoch-1 batch-156 = 0.0355633944272995

Training epoch-1 batch-157
Running loss of epoch-1 batch-157 = 0.14130447804927826

Finished training epoch-1.



Average train loss at epoch-1 = 0.03608462908267975

Started Evaluation

Average val loss at epoch-1 = 2.287629889814477

Accuracy for classes:
Accuracy for class equals is: 0.00 %
Accuracy for class main is: 0.00 %
Accuracy for class setUp is: 0.00 %
Accuracy for class onCreate is: 0.00 %
Accuracy for class toString is: 0.00 %
Accuracy for class run is: 0.00 %
Accuracy for class hashCode is: 0.00 %
Accuracy for class init is: 0.00 %
Accuracy for class execute is: 100.00 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 5.14 %


Best Accuracy = 5.14 % at Epoch-1
Saving model after best epoch-1

Finished Evaluation



Started training epoch-2


Training epoch-2 batch-1
Running loss of epoch-2 batch-1 = 0.03547181934118271

Training epoch-2 batch-2
Running loss of epoch-2 batch-2 = 0.03569769486784935

Training epoch-2 batch-3
Running loss of epoch-2 batch-3 = 0.03572620078921318

Training epoch-2 batch-4
Running loss of epoch-2 batch-4 = 0.035867467522621155

Training epoch-2 batch-5
Running loss of epoch-2 batch-5 = 0.03588530421257019

Training epoch-2 batch-6
Running loss of epoch-2 batch-6 = 0.03570054844021797

Training epoch-2 batch-7
Running loss of epoch-2 batch-7 = 0.03567887470126152

Training epoch-2 batch-8
Running loss of epoch-2 batch-8 = 0.03572976961731911

Training epoch-2 batch-9
Running loss of epoch-2 batch-9 = 0.03538164868950844

Training epoch-2 batch-10
Running loss of epoch-2 batch-10 = 0.03552589565515518

Training epoch-2 batch-11
Running loss of epoch-2 batch-11 = 0.035608384758234024

Training epoch-2 batch-12
Running loss of epoch-2 batch-12 = 0.035713836550712585

Training epoch-2 batch-13
Running loss of epoch-2 batch-13 = 0.03554195165634155

Training epoch-2 batch-14
Running loss of epoch-2 batch-14 = 0.03554742410778999

Training epoch-2 batch-15
Running loss of epoch-2 batch-15 = 0.03548828139901161

Training epoch-2 batch-16
Running loss of epoch-2 batch-16 = 0.03545798361301422

Training epoch-2 batch-17
Running loss of epoch-2 batch-17 = 0.03535744175314903

Training epoch-2 batch-18
Running loss of epoch-2 batch-18 = 0.0355757474899292

Training epoch-2 batch-19
Running loss of epoch-2 batch-19 = 0.03562800958752632

Training epoch-2 batch-20
Running loss of epoch-2 batch-20 = 0.03546752408146858

Training epoch-2 batch-21
Running loss of epoch-2 batch-21 = 0.03561133146286011

Training epoch-2 batch-22
Running loss of epoch-2 batch-22 = 0.0354745090007782

Training epoch-2 batch-23
Running loss of epoch-2 batch-23 = 0.035634178668260574

Training epoch-2 batch-24
Running loss of epoch-2 batch-24 = 0.03557151183485985

Training epoch-2 batch-25
Running loss of epoch-2 batch-25 = 0.03573756664991379

Training epoch-2 batch-26
Running loss of epoch-2 batch-26 = 0.035555802285671234

Training epoch-2 batch-27
Running loss of epoch-2 batch-27 = 0.03553221747279167

Training epoch-2 batch-28
Running loss of epoch-2 batch-28 = 0.03550601005554199

Training epoch-2 batch-29
Running loss of epoch-2 batch-29 = 0.03551163151860237

Training epoch-2 batch-30
Running loss of epoch-2 batch-30 = 0.03557666391134262

Training epoch-2 batch-31
Running loss of epoch-2 batch-31 = 0.035572879016399384

Training epoch-2 batch-32
Running loss of epoch-2 batch-32 = 0.035432927310466766

Training epoch-2 batch-33
Running loss of epoch-2 batch-33 = 0.035709645599126816

Training epoch-2 batch-34
Running loss of epoch-2 batch-34 = 0.03542173281311989

Training epoch-2 batch-35
Running loss of epoch-2 batch-35 = 0.03540453314781189

Training epoch-2 batch-36
Running loss of epoch-2 batch-36 = 0.03560749813914299

Training epoch-2 batch-37
Running loss of epoch-2 batch-37 = 0.03563815355300903

Training epoch-2 batch-38
Running loss of epoch-2 batch-38 = 0.035230547189712524

Training epoch-2 batch-39
Running loss of epoch-2 batch-39 = 0.03558061644434929

Training epoch-2 batch-40
Running loss of epoch-2 batch-40 = 0.03538413718342781

Training epoch-2 batch-41
Running loss of epoch-2 batch-41 = 0.03577869012951851

Training epoch-2 batch-42
Running loss of epoch-2 batch-42 = 0.03534819185733795

Training epoch-2 batch-43
Running loss of epoch-2 batch-43 = 0.03547288477420807

Training epoch-2 batch-44
Running loss of epoch-2 batch-44 = 0.03546537458896637

Training epoch-2 batch-45
Running loss of epoch-2 batch-45 = 0.03530474007129669

Training epoch-2 batch-46
Running loss of epoch-2 batch-46 = 0.035570260137319565

Training epoch-2 batch-47
Running loss of epoch-2 batch-47 = 0.03553839027881622

Training epoch-2 batch-48
Running loss of epoch-2 batch-48 = 0.035272106528282166

Training epoch-2 batch-49
Running loss of epoch-2 batch-49 = 0.03539958968758583

Training epoch-2 batch-50
Running loss of epoch-2 batch-50 = 0.0352756567299366

Training epoch-2 batch-51
Running loss of epoch-2 batch-51 = 0.03524060547351837

Training epoch-2 batch-52
Running loss of epoch-2 batch-52 = 0.03532681241631508

Training epoch-2 batch-53
Running loss of epoch-2 batch-53 = 0.035351287573575974

Training epoch-2 batch-54
Running loss of epoch-2 batch-54 = 0.03537582606077194

Training epoch-2 batch-55
Running loss of epoch-2 batch-55 = 0.035241033881902695

Training epoch-2 batch-56
Running loss of epoch-2 batch-56 = 0.0352364219725132

Training epoch-2 batch-57
Running loss of epoch-2 batch-57 = 0.035237256437540054

Training epoch-2 batch-58
Running loss of epoch-2 batch-58 = 0.03545433655381203

Training epoch-2 batch-59
Running loss of epoch-2 batch-59 = 0.035354018211364746

Training epoch-2 batch-60
Running loss of epoch-2 batch-60 = 0.03536376729607582

Training epoch-2 batch-61
Running loss of epoch-2 batch-61 = 0.03498999401926994

Training epoch-2 batch-62
Running loss of epoch-2 batch-62 = 0.03521180897951126

Training epoch-2 batch-63
Running loss of epoch-2 batch-63 = 0.035181812942028046

Training epoch-2 batch-64
Running loss of epoch-2 batch-64 = 0.035088006407022476

Training epoch-2 batch-65
Running loss of epoch-2 batch-65 = 0.03528784215450287

Training epoch-2 batch-66
Running loss of epoch-2 batch-66 = 0.035396330058574677

Training epoch-2 batch-67
Running loss of epoch-2 batch-67 = 0.03530833497643471

Training epoch-2 batch-68
Running loss of epoch-2 batch-68 = 0.03478497266769409

Training epoch-2 batch-69
Running loss of epoch-2 batch-69 = 0.03496647626161575

Training epoch-2 batch-70
Running loss of epoch-2 batch-70 = 0.035016149282455444

Training epoch-2 batch-71
Running loss of epoch-2 batch-71 = 0.03519168123602867

Training epoch-2 batch-72
Running loss of epoch-2 batch-72 = 0.03509586304426193

Training epoch-2 batch-73
Running loss of epoch-2 batch-73 = 0.03501418977975845

Training epoch-2 batch-74
Running loss of epoch-2 batch-74 = 0.035221658647060394

Training epoch-2 batch-75
Running loss of epoch-2 batch-75 = 0.03479745239019394

Training epoch-2 batch-76
Running loss of epoch-2 batch-76 = 0.035121675580739975

Training epoch-2 batch-77
Running loss of epoch-2 batch-77 = 0.03483199328184128

Training epoch-2 batch-78
Running loss of epoch-2 batch-78 = 0.03511909395456314

Training epoch-2 batch-79
Running loss of epoch-2 batch-79 = 0.034959133714437485

Training epoch-2 batch-80
Running loss of epoch-2 batch-80 = 0.03508089482784271

Training epoch-2 batch-81
Running loss of epoch-2 batch-81 = 0.03537434712052345

Training epoch-2 batch-82
Running loss of epoch-2 batch-82 = 0.03473488986492157

Training epoch-2 batch-83
Running loss of epoch-2 batch-83 = 0.03503464162349701

Training epoch-2 batch-84
Running loss of epoch-2 batch-84 = 0.034880951046943665

Training epoch-2 batch-85
Running loss of epoch-2 batch-85 = 0.03502821922302246

Training epoch-2 batch-86
Running loss of epoch-2 batch-86 = 0.034907564520835876

Training epoch-2 batch-87
Running loss of epoch-2 batch-87 = 0.034528814256191254

Training epoch-2 batch-88
Running loss of epoch-2 batch-88 = 0.03491377830505371

Training epoch-2 batch-89
Running loss of epoch-2 batch-89 = 0.03505488112568855

Training epoch-2 batch-90
Running loss of epoch-2 batch-90 = 0.034681640565395355

Training epoch-2 batch-91
Running loss of epoch-2 batch-91 = 0.03533787280321121

Training epoch-2 batch-92
Running loss of epoch-2 batch-92 = 0.034683141857385635

Training epoch-2 batch-93
Running loss of epoch-2 batch-93 = 0.03447972238063812

Training epoch-2 batch-94
Running loss of epoch-2 batch-94 = 0.034861646592617035

Training epoch-2 batch-95
Running loss of epoch-2 batch-95 = 0.034804072231054306

Training epoch-2 batch-96
Running loss of epoch-2 batch-96 = 0.03484313562512398

Training epoch-2 batch-97
Running loss of epoch-2 batch-97 = 0.035094987601041794

Training epoch-2 batch-98
Running loss of epoch-2 batch-98 = 0.03501481935381889

Training epoch-2 batch-99
Running loss of epoch-2 batch-99 = 0.0351574569940567

Training epoch-2 batch-100
Running loss of epoch-2 batch-100 = 0.03449505195021629

Training epoch-2 batch-101
Running loss of epoch-2 batch-101 = 0.03451455011963844

Training epoch-2 batch-102
Running loss of epoch-2 batch-102 = 0.034706272184848785

Training epoch-2 batch-103
Running loss of epoch-2 batch-103 = 0.03479621931910515

Training epoch-2 batch-104
Running loss of epoch-2 batch-104 = 0.034189507365226746

Training epoch-2 batch-105
Running loss of epoch-2 batch-105 = 0.03473175689578056

Training epoch-2 batch-106
Running loss of epoch-2 batch-106 = 0.03486299887299538

Training epoch-2 batch-107
Running loss of epoch-2 batch-107 = 0.03426491096615791

Training epoch-2 batch-108
Running loss of epoch-2 batch-108 = 0.0345027782022953

Training epoch-2 batch-109
Running loss of epoch-2 batch-109 = 0.03455717861652374

Training epoch-2 batch-110
Running loss of epoch-2 batch-110 = 0.034254878759384155

Training epoch-2 batch-111
Running loss of epoch-2 batch-111 = 0.03423411771655083

Training epoch-2 batch-112
Running loss of epoch-2 batch-112 = 0.03432668000459671

Training epoch-2 batch-113
Running loss of epoch-2 batch-113 = 0.034872446209192276

Training epoch-2 batch-114
Running loss of epoch-2 batch-114 = 0.03460169583559036

Training epoch-2 batch-115
Running loss of epoch-2 batch-115 = 0.03428294137120247

Training epoch-2 batch-116
Running loss of epoch-2 batch-116 = 0.03438280150294304

Training epoch-2 batch-117
Running loss of epoch-2 batch-117 = 0.034226350486278534

Training epoch-2 batch-118
Running loss of epoch-2 batch-118 = 0.03417651727795601

Training epoch-2 batch-119
Running loss of epoch-2 batch-119 = 0.034259650856256485

Training epoch-2 batch-120
Running loss of epoch-2 batch-120 = 0.03333284705877304

Training epoch-2 batch-121
Running loss of epoch-2 batch-121 = 0.03440936282277107

Training epoch-2 batch-122
Running loss of epoch-2 batch-122 = 0.034196075052022934

Training epoch-2 batch-123
Running loss of epoch-2 batch-123 = 0.034389957785606384

Training epoch-2 batch-124
Running loss of epoch-2 batch-124 = 0.03389184921979904

Training epoch-2 batch-125
Running loss of epoch-2 batch-125 = 0.033899884670972824

Training epoch-2 batch-126
Running loss of epoch-2 batch-126 = 0.03443516790866852

Training epoch-2 batch-127
Running loss of epoch-2 batch-127 = 0.033515628427267075

Training epoch-2 batch-128
Running loss of epoch-2 batch-128 = 0.03388047218322754

Training epoch-2 batch-129
Running loss of epoch-2 batch-129 = 0.03414878249168396

Training epoch-2 batch-130
Running loss of epoch-2 batch-130 = 0.0343659333884716

Training epoch-2 batch-131
Running loss of epoch-2 batch-131 = 0.034084733575582504

Training epoch-2 batch-132
Running loss of epoch-2 batch-132 = 0.0332304909825325

Training epoch-2 batch-133
Running loss of epoch-2 batch-133 = 0.03324344754219055

Training epoch-2 batch-134
Running loss of epoch-2 batch-134 = 0.034149784594774246

Training epoch-2 batch-135
Running loss of epoch-2 batch-135 = 0.03347310796380043

Training epoch-2 batch-136
Running loss of epoch-2 batch-136 = 0.034126266837120056

Training epoch-2 batch-137
Running loss of epoch-2 batch-137 = 0.0334567129611969

Training epoch-2 batch-138
Running loss of epoch-2 batch-138 = 0.03377971053123474

Training epoch-2 batch-139
Running loss of epoch-2 batch-139 = 0.033169277012348175

Training epoch-2 batch-140
Running loss of epoch-2 batch-140 = 0.03441229090094566

Training epoch-2 batch-141
Running loss of epoch-2 batch-141 = 0.033623553812503815

Training epoch-2 batch-142
Running loss of epoch-2 batch-142 = 0.03413400053977966

Training epoch-2 batch-143
Running loss of epoch-2 batch-143 = 0.033897075802087784

Training epoch-2 batch-144
Running loss of epoch-2 batch-144 = 0.03346031531691551

Training epoch-2 batch-145
Running loss of epoch-2 batch-145 = 0.03295097500085831

Training epoch-2 batch-146
Running loss of epoch-2 batch-146 = 0.03288391977548599

Training epoch-2 batch-147
Running loss of epoch-2 batch-147 = 0.03273467719554901

Training epoch-2 batch-148
Running loss of epoch-2 batch-148 = 0.033845867961645126

Training epoch-2 batch-149
Running loss of epoch-2 batch-149 = 0.033137064427137375

Training epoch-2 batch-150
Running loss of epoch-2 batch-150 = 0.03357313945889473

Training epoch-2 batch-151
Running loss of epoch-2 batch-151 = 0.03278385475277901

Training epoch-2 batch-152
Running loss of epoch-2 batch-152 = 0.03244171291589737

Training epoch-2 batch-153
Running loss of epoch-2 batch-153 = 0.03283844143152237

Training epoch-2 batch-154
Running loss of epoch-2 batch-154 = 0.03302308917045593

Training epoch-2 batch-155
Running loss of epoch-2 batch-155 = 0.03354782238602638

Training epoch-2 batch-156
Running loss of epoch-2 batch-156 = 0.032733019441366196

Training epoch-2 batch-157
Running loss of epoch-2 batch-157 = 0.13163228332996368

Finished training epoch-2.



Average train loss at epoch-2 = 0.03494900984764099

Started Evaluation

Average val loss at epoch-2 = 2.046365786539881

Accuracy for classes:
Accuracy for class equals is: 76.73 %
Accuracy for class main is: 60.49 %
Accuracy for class setUp is: 47.21 %
Accuracy for class onCreate is: 85.82 %
Accuracy for class toString is: 6.48 %
Accuracy for class run is: 5.02 %
Accuracy for class hashCode is: 76.40 %
Accuracy for class init is: 34.53 %
Accuracy for class execute is: 10.44 %
Accuracy for class get is: 0.00 %

Overall Accuracy = 48.52 %


Best Accuracy = 48.52 % at Epoch-2
Saving model after best epoch-2

Finished Evaluation



Started training epoch-3


Training epoch-3 batch-1
Running loss of epoch-3 batch-1 = 0.03229297697544098

Training epoch-3 batch-2
Running loss of epoch-3 batch-2 = 0.03293047472834587

Training epoch-3 batch-3
Running loss of epoch-3 batch-3 = 0.031925927847623825

Training epoch-3 batch-4
Running loss of epoch-3 batch-4 = 0.0324171744287014

Training epoch-3 batch-5
Running loss of epoch-3 batch-5 = 0.03252619877457619

Training epoch-3 batch-6
Running loss of epoch-3 batch-6 = 0.032980386167764664

Training epoch-3 batch-7
Running loss of epoch-3 batch-7 = 0.03182831406593323

Training epoch-3 batch-8
Running loss of epoch-3 batch-8 = 0.03220939263701439

Training epoch-3 batch-9
Running loss of epoch-3 batch-9 = 0.03266102448105812

Training epoch-3 batch-10
Running loss of epoch-3 batch-10 = 0.03250272944569588

Training epoch-3 batch-11
Running loss of epoch-3 batch-11 = 0.03229916840791702

Training epoch-3 batch-12
Running loss of epoch-3 batch-12 = 0.03162142634391785

Training epoch-3 batch-13
Running loss of epoch-3 batch-13 = 0.03291710466146469

Training epoch-3 batch-14
Running loss of epoch-3 batch-14 = 0.031021125614643097

Training epoch-3 batch-15
Running loss of epoch-3 batch-15 = 0.03144562616944313

Training epoch-3 batch-16
Running loss of epoch-3 batch-16 = 0.03208267316222191

Training epoch-3 batch-17
Running loss of epoch-3 batch-17 = 0.03081258200109005

Training epoch-3 batch-18
Running loss of epoch-3 batch-18 = 0.03233194351196289

Training epoch-3 batch-19
Running loss of epoch-3 batch-19 = 0.032024115324020386

Training epoch-3 batch-20
Running loss of epoch-3 batch-20 = 0.03081625886261463

Training epoch-3 batch-21
Running loss of epoch-3 batch-21 = 0.03163662180304527

Training epoch-3 batch-22
Running loss of epoch-3 batch-22 = 0.030747778713703156

Training epoch-3 batch-23
Running loss of epoch-3 batch-23 = 0.03105880133807659

Training epoch-3 batch-24
Running loss of epoch-3 batch-24 = 0.031182216480374336

Training epoch-3 batch-25
Running loss of epoch-3 batch-25 = 0.03100108727812767

Training epoch-3 batch-26
Running loss of epoch-3 batch-26 = 0.03164546936750412

Training epoch-3 batch-27
Running loss of epoch-3 batch-27 = 0.03012935444712639

Training epoch-3 batch-28
Running loss of epoch-3 batch-28 = 0.0302896648645401

Training epoch-3 batch-29
Running loss of epoch-3 batch-29 = 0.031940508633852005

Training epoch-3 batch-30
Running loss of epoch-3 batch-30 = 0.029960986226797104

Training epoch-3 batch-31
Running loss of epoch-3 batch-31 = 0.031406622380018234

Training epoch-3 batch-32
Running loss of epoch-3 batch-32 = 0.030881419777870178

Training epoch-3 batch-33
Running loss of epoch-3 batch-33 = 0.0300533939152956

Training epoch-3 batch-34
Running loss of epoch-3 batch-34 = 0.02999330312013626

Training epoch-3 batch-35
Running loss of epoch-3 batch-35 = 0.03174144774675369

Training epoch-3 batch-36
Running loss of epoch-3 batch-36 = 0.03186556696891785

Training epoch-3 batch-37
Running loss of epoch-3 batch-37 = 0.030293386429548264

Training epoch-3 batch-38
Running loss of epoch-3 batch-38 = 0.030274610966444016

Training epoch-3 batch-39
Running loss of epoch-3 batch-39 = 0.02943379059433937

Training epoch-3 batch-40
Running loss of epoch-3 batch-40 = 0.02952062524855137

Training epoch-3 batch-41
Running loss of epoch-3 batch-41 = 0.02976454235613346

Training epoch-3 batch-42
Running loss of epoch-3 batch-42 = 0.029843514785170555

Training epoch-3 batch-43
Running loss of epoch-3 batch-43 = 0.029066288843750954

Training epoch-3 batch-44
Running loss of epoch-3 batch-44 = 0.02925979346036911

Training epoch-3 batch-45
Running loss of epoch-3 batch-45 = 0.030230365693569183

Training epoch-3 batch-46
Running loss of epoch-3 batch-46 = 0.02952171303331852

Training epoch-3 batch-47
Running loss of epoch-3 batch-47 = 0.027033589780330658

Training epoch-3 batch-48
Running loss of epoch-3 batch-48 = 0.029641006141901016

Training epoch-3 batch-49
Running loss of epoch-3 batch-49 = 0.028905481100082397

Training epoch-3 batch-50
Running loss of epoch-3 batch-50 = 0.029913511127233505

Training epoch-3 batch-51
Running loss of epoch-3 batch-51 = 0.02906118333339691

Training epoch-3 batch-52
Running loss of epoch-3 batch-52 = 0.029967404901981354

Training epoch-3 batch-53
Running loss of epoch-3 batch-53 = 0.028001710772514343

Training epoch-3 batch-54
Running loss of epoch-3 batch-54 = 0.02756916917860508

Training epoch-3 batch-55
Running loss of epoch-3 batch-55 = 0.028384117409586906

Training epoch-3 batch-56
Running loss of epoch-3 batch-56 = 0.030125604942440987

Training epoch-3 batch-57
Running loss of epoch-3 batch-57 = 0.02832210808992386

Training epoch-3 batch-58
Running loss of epoch-3 batch-58 = 0.027267146855592728

Training epoch-3 batch-59
Running loss of epoch-3 batch-59 = 0.029674474149942398

Training epoch-3 batch-60
Running loss of epoch-3 batch-60 = 0.02831418253481388

Training epoch-3 batch-61
Running loss of epoch-3 batch-61 = 0.026469113305211067

Training epoch-3 batch-62
Running loss of epoch-3 batch-62 = 0.028415456414222717

Training epoch-3 batch-63
Running loss of epoch-3 batch-63 = 0.025523807853460312

Training epoch-3 batch-64
Running loss of epoch-3 batch-64 = 0.026272226125001907

Training epoch-3 batch-65
Running loss of epoch-3 batch-65 = 0.02958112396299839

Training epoch-3 batch-66
Running loss of epoch-3 batch-66 = 0.030223364010453224

Training epoch-3 batch-67
Running loss of epoch-3 batch-67 = 0.027320822700858116

Training epoch-3 batch-68
Running loss of epoch-3 batch-68 = 0.02694421261548996

Training epoch-3 batch-69
Running loss of epoch-3 batch-69 = 0.027942506596446037

Training epoch-3 batch-70
Running loss of epoch-3 batch-70 = 0.026163846254348755

Training epoch-3 batch-71
Running loss of epoch-3 batch-71 = 0.025923192501068115

Training epoch-3 batch-72
Running loss of epoch-3 batch-72 = 0.02796727605164051

Training epoch-3 batch-73
Running loss of epoch-3 batch-73 = 0.02898075431585312

Training epoch-3 batch-74
Running loss of epoch-3 batch-74 = 0.028501033782958984

Training epoch-3 batch-75
Running loss of epoch-3 batch-75 = 0.02646281011402607

Training epoch-3 batch-76
Running loss of epoch-3 batch-76 = 0.029099246487021446

Training epoch-3 batch-77
Running loss of epoch-3 batch-77 = 0.026775255799293518

Training epoch-3 batch-78
Running loss of epoch-3 batch-78 = 0.025713540613651276

Training epoch-3 batch-79
Running loss of epoch-3 batch-79 = 0.026226036250591278

Training epoch-3 batch-80
Running loss of epoch-3 batch-80 = 0.027380725368857384

Training epoch-3 batch-81
Running loss of epoch-3 batch-81 = 0.02636141888797283

Training epoch-3 batch-82
Running loss of epoch-3 batch-82 = 0.02538827806711197

Training epoch-3 batch-83
Running loss of epoch-3 batch-83 = 0.02587602660059929

Training epoch-3 batch-84
Running loss of epoch-3 batch-84 = 0.024740025401115417

Training epoch-3 batch-85
Running loss of epoch-3 batch-85 = 0.027391910552978516

Training epoch-3 batch-86
Running loss of epoch-3 batch-86 = 0.028758054599165916

Training epoch-3 batch-87
Running loss of epoch-3 batch-87 = 0.024880534037947655

Training epoch-3 batch-88
Running loss of epoch-3 batch-88 = 0.026841038838028908

Training epoch-3 batch-89
Running loss of epoch-3 batch-89 = 0.025631200522184372

Training epoch-3 batch-90
Running loss of epoch-3 batch-90 = 0.023030664771795273

Training epoch-3 batch-91
Running loss of epoch-3 batch-91 = 0.02340187132358551

Training epoch-3 batch-92
Running loss of epoch-3 batch-92 = 0.02808145061135292

Training epoch-3 batch-93
Running loss of epoch-3 batch-93 = 0.02356094680726528

Training epoch-3 batch-94
Running loss of epoch-3 batch-94 = 0.02433060295879841

Training epoch-3 batch-95
Running loss of epoch-3 batch-95 = 0.026124119758605957

Training epoch-3 batch-96
Running loss of epoch-3 batch-96 = 0.023764286190271378

Training epoch-3 batch-97
Running loss of epoch-3 batch-97 = 0.02601008489727974

Training epoch-3 batch-98
Running loss of epoch-3 batch-98 = 0.025720831006765366

Training epoch-3 batch-99
Running loss of epoch-3 batch-99 = 0.022097483277320862

Training epoch-3 batch-100
Running loss of epoch-3 batch-100 = 0.02633892558515072

Training epoch-3 batch-101
Running loss of epoch-3 batch-101 = 0.02371790260076523

Training epoch-3 batch-102
Running loss of epoch-3 batch-102 = 0.027554325759410858

Training epoch-3 batch-103
Running loss of epoch-3 batch-103 = 0.022022809833288193

Training epoch-3 batch-104
Running loss of epoch-3 batch-104 = 0.02411564067006111

Training epoch-3 batch-105
Running loss of epoch-3 batch-105 = 0.02470768429338932

Training epoch-3 batch-106
Running loss of epoch-3 batch-106 = 0.023003753274679184

Training epoch-3 batch-107
Running loss of epoch-3 batch-107 = 0.025034863501787186

Training epoch-3 batch-108
Running loss of epoch-3 batch-108 = 0.02533627301454544

Training epoch-3 batch-109
Running loss of epoch-3 batch-109 = 0.02406661957502365

Training epoch-3 batch-110
Running loss of epoch-3 batch-110 = 0.02371206134557724

Training epoch-3 batch-111
Running loss of epoch-3 batch-111 = 0.026642290875315666

Training epoch-3 batch-112
Running loss of epoch-3 batch-112 = 0.024571826681494713

Training epoch-3 batch-113
Running loss of epoch-3 batch-113 = 0.025777986273169518

Training epoch-3 batch-114
Running loss of epoch-3 batch-114 = 0.02483963407576084

Training epoch-3 batch-115
Running loss of epoch-3 batch-115 = 0.023820649832487106

Training epoch-3 batch-116
Running loss of epoch-3 batch-116 = 0.0246123094111681

Training epoch-3 batch-117
Running loss of epoch-3 batch-117 = 0.02240489050745964

Training epoch-3 batch-118
Running loss of epoch-3 batch-118 = 0.02118837460875511

Training epoch-3 batch-119
Running loss of epoch-3 batch-119 = 0.023034337908029556

Training epoch-3 batch-120
Running loss of epoch-3 batch-120 = 0.022708913311362267

Training epoch-3 batch-121
Running loss of epoch-3 batch-121 = 0.024608494713902473

Training epoch-3 batch-122
Running loss of epoch-3 batch-122 = 0.022923799231648445

Training epoch-3 batch-123
Running loss of epoch-3 batch-123 = 0.02429964207112789

Training epoch-3 batch-124
Running loss of epoch-3 batch-124 = 0.02175818383693695

Training epoch-3 batch-125
Running loss of epoch-3 batch-125 = 0.025124045088887215

Training epoch-3 batch-126
Running loss of epoch-3 batch-126 = 0.020541541278362274

Training epoch-3 batch-127
Running loss of epoch-3 batch-127 = 0.02126985602080822

Training epoch-3 batch-128
Running loss of epoch-3 batch-128 = 0.023251326754689217

Training epoch-3 batch-129
Running loss of epoch-3 batch-129 = 0.023508045822381973

Training epoch-3 batch-130
Running loss of epoch-3 batch-130 = 0.020280787721276283

Training epoch-3 batch-131
Running loss of epoch-3 batch-131 = 0.022293703630566597

Training epoch-3 batch-132
Running loss of epoch-3 batch-132 = 0.023197084665298462

Training epoch-3 batch-133
Running loss of epoch-3 batch-133 = 0.02375686727464199

Training epoch-3 batch-134
Running loss of epoch-3 batch-134 = 0.023996803909540176

Training epoch-3 batch-135
Running loss of epoch-3 batch-135 = 0.02320769801735878

Training epoch-3 batch-136
Running loss of epoch-3 batch-136 = 0.02530459500849247

Training epoch-3 batch-137
Running loss of epoch-3 batch-137 = 0.0237868782132864

Training epoch-3 batch-138
Running loss of epoch-3 batch-138 = 0.02333012968301773

Training epoch-3 batch-139
Running loss of epoch-3 batch-139 = 0.022952035069465637

Training epoch-3 batch-140
Running loss of epoch-3 batch-140 = 0.02097417786717415

Training epoch-3 batch-141
Running loss of epoch-3 batch-141 = 0.018962150439620018

Training epoch-3 batch-142
Running loss of epoch-3 batch-142 = 0.01936686411499977

Training epoch-3 batch-143
Running loss of epoch-3 batch-143 = 0.022309185937047005

Training epoch-3 batch-144
Running loss of epoch-3 batch-144 = 0.02456360124051571

Training epoch-3 batch-145
Running loss of epoch-3 batch-145 = 0.022819630801677704

Training epoch-3 batch-146
Running loss of epoch-3 batch-146 = 0.023134347051382065

Training epoch-3 batch-147
Running loss of epoch-3 batch-147 = 0.026452479884028435

Training epoch-3 batch-148
Running loss of epoch-3 batch-148 = 0.022016072645783424

Training epoch-3 batch-149
Running loss of epoch-3 batch-149 = 0.022401439025998116

Training epoch-3 batch-150
Running loss of epoch-3 batch-150 = 0.022741587832570076

Training epoch-3 batch-151
Running loss of epoch-3 batch-151 = 0.024481801316142082

Training epoch-3 batch-152
Running loss of epoch-3 batch-152 = 0.021639728918671608

Training epoch-3 batch-153
Running loss of epoch-3 batch-153 = 0.02227606810629368

Training epoch-3 batch-154
Running loss of epoch-3 batch-154 = 0.022600747644901276

Training epoch-3 batch-155
Running loss of epoch-3 batch-155 = 0.022126061841845512

Training epoch-3 batch-156
Running loss of epoch-3 batch-156 = 0.023147445172071457

Training epoch-3 batch-157
Running loss of epoch-3 batch-157 = 0.09669127315282822

Finished training epoch-3.



Average train loss at epoch-3 = 0.027015917873382568

Started Evaluation

Average val loss at epoch-3 = 1.2902418798521946

Accuracy for classes:
Accuracy for class equals is: 79.04 %
Accuracy for class main is: 97.38 %
Accuracy for class setUp is: 63.11 %
Accuracy for class onCreate is: 84.75 %
Accuracy for class toString is: 3.41 %
Accuracy for class run is: 3.88 %
Accuracy for class hashCode is: 96.63 %
Accuracy for class init is: 0.45 %
Accuracy for class execute is: 21.29 %
Accuracy for class get is: 31.54 %

Overall Accuracy = 56.03 %


Best Accuracy = 56.03 % at Epoch-3
Saving model after best epoch-3

Finished Evaluation



Started training epoch-4


Training epoch-4 batch-1
Running loss of epoch-4 batch-1 = 0.02483578771352768

Training epoch-4 batch-2
Running loss of epoch-4 batch-2 = 0.02159048058092594

Training epoch-4 batch-3
Running loss of epoch-4 batch-3 = 0.019071387127041817

Training epoch-4 batch-4
Running loss of epoch-4 batch-4 = 0.019457533955574036

Training epoch-4 batch-5
Running loss of epoch-4 batch-5 = 0.020837154239416122

Training epoch-4 batch-6
Running loss of epoch-4 batch-6 = 0.022049138322472572

Training epoch-4 batch-7
Running loss of epoch-4 batch-7 = 0.0222831591963768

Training epoch-4 batch-8
Running loss of epoch-4 batch-8 = 0.018723666667938232

Training epoch-4 batch-9
Running loss of epoch-4 batch-9 = 0.023181451484560966

Training epoch-4 batch-10
Running loss of epoch-4 batch-10 = 0.02007056586444378

Training epoch-4 batch-11
Running loss of epoch-4 batch-11 = 0.01869538240134716

Training epoch-4 batch-12
Running loss of epoch-4 batch-12 = 0.021102555096149445

Training epoch-4 batch-13
Running loss of epoch-4 batch-13 = 0.02236851304769516

Training epoch-4 batch-14
Running loss of epoch-4 batch-14 = 0.02186393365263939

Training epoch-4 batch-15
Running loss of epoch-4 batch-15 = 0.018915750086307526

Training epoch-4 batch-16
Running loss of epoch-4 batch-16 = 0.018946697935461998

Training epoch-4 batch-17
Running loss of epoch-4 batch-17 = 0.02172078564763069

Training epoch-4 batch-18
Running loss of epoch-4 batch-18 = 0.01922120340168476

Training epoch-4 batch-19
Running loss of epoch-4 batch-19 = 0.019621653482317924

Training epoch-4 batch-20
Running loss of epoch-4 batch-20 = 0.01751605235040188

Training epoch-4 batch-21
Running loss of epoch-4 batch-21 = 0.017242431640625

Training epoch-4 batch-22
Running loss of epoch-4 batch-22 = 0.022408228367567062

Training epoch-4 batch-23
Running loss of epoch-4 batch-23 = 0.01979907788336277

Training epoch-4 batch-24
Running loss of epoch-4 batch-24 = 0.020001115277409554

Training epoch-4 batch-25
Running loss of epoch-4 batch-25 = 0.018769193440675735

Training epoch-4 batch-26
Running loss of epoch-4 batch-26 = 0.021709883585572243

Training epoch-4 batch-27
Running loss of epoch-4 batch-27 = 0.01703636348247528

Training epoch-4 batch-28
Running loss of epoch-4 batch-28 = 0.020374033600091934

Training epoch-4 batch-29
Running loss of epoch-4 batch-29 = 0.017730019986629486

Training epoch-4 batch-30
Running loss of epoch-4 batch-30 = 0.020761340856552124

Training epoch-4 batch-31
Running loss of epoch-4 batch-31 = 0.0211141649633646

Training epoch-4 batch-32
Running loss of epoch-4 batch-32 = 0.022127414122223854

Training epoch-4 batch-33
Running loss of epoch-4 batch-33 = 0.02161129005253315

Training epoch-4 batch-34
Running loss of epoch-4 batch-34 = 0.02215450257062912

Training epoch-4 batch-35
Running loss of epoch-4 batch-35 = 0.018433190882205963

Training epoch-4 batch-36
Running loss of epoch-4 batch-36 = 0.022439653053879738

Training epoch-4 batch-37
Running loss of epoch-4 batch-37 = 0.018643081188201904

Training epoch-4 batch-38
Running loss of epoch-4 batch-38 = 0.018451571464538574

Training epoch-4 batch-39
Running loss of epoch-4 batch-39 = 0.01854640804231167

Training epoch-4 batch-40
Running loss of epoch-4 batch-40 = 0.017163285985589027

Training epoch-4 batch-41
Running loss of epoch-4 batch-41 = 0.01940757781267166

Training epoch-4 batch-42
Running loss of epoch-4 batch-42 = 0.02019342966377735

Training epoch-4 batch-43
Running loss of epoch-4 batch-43 = 0.016372831538319588

Training epoch-4 batch-44
Running loss of epoch-4 batch-44 = 0.017100071534514427

Training epoch-4 batch-45
Running loss of epoch-4 batch-45 = 0.01781720668077469

Training epoch-4 batch-46
Running loss of epoch-4 batch-46 = 0.019947577267885208

Training epoch-4 batch-47
Running loss of epoch-4 batch-47 = 0.01624850369989872

Training epoch-4 batch-48
Running loss of epoch-4 batch-48 = 0.02179216593503952

Training epoch-4 batch-49
Running loss of epoch-4 batch-49 = 0.021837089210748672

Training epoch-4 batch-50
Running loss of epoch-4 batch-50 = 0.02201981283724308

Training epoch-4 batch-51
Running loss of epoch-4 batch-51 = 0.019001971930265427

Training epoch-4 batch-52
Running loss of epoch-4 batch-52 = 0.015844443812966347

Training epoch-4 batch-53
Running loss of epoch-4 batch-53 = 0.016345784068107605

Training epoch-4 batch-54
Running loss of epoch-4 batch-54 = 0.01958814077079296

Training epoch-4 batch-55
Running loss of epoch-4 batch-55 = 0.01585284247994423

Training epoch-4 batch-56
Running loss of epoch-4 batch-56 = 0.016829561442136765

Training epoch-4 batch-57
Running loss of epoch-4 batch-57 = 0.017048252746462822

Training epoch-4 batch-58
Running loss of epoch-4 batch-58 = 0.018369173631072044

Training epoch-4 batch-59
Running loss of epoch-4 batch-59 = 0.017856821417808533

Training epoch-4 batch-60
Running loss of epoch-4 batch-60 = 0.0175041314214468

Training epoch-4 batch-61
Running loss of epoch-4 batch-61 = 0.019848983734846115

Training epoch-4 batch-62
Running loss of epoch-4 batch-62 = 0.016034336760640144

Training epoch-4 batch-63
Running loss of epoch-4 batch-63 = 0.01708187721669674

Training epoch-4 batch-64
Running loss of epoch-4 batch-64 = 0.014577641151845455

Training epoch-4 batch-65
Running loss of epoch-4 batch-65 = 0.020089706405997276

Training epoch-4 batch-66
Running loss of epoch-4 batch-66 = 0.018017126247286797

Training epoch-4 batch-67
Running loss of epoch-4 batch-67 = 0.013765644282102585

Training epoch-4 batch-68
Running loss of epoch-4 batch-68 = 0.016006167978048325

Training epoch-4 batch-69
Running loss of epoch-4 batch-69 = 0.014988940209150314

Training epoch-4 batch-70
Running loss of epoch-4 batch-70 = 0.023175839334726334

Training epoch-4 batch-71
Running loss of epoch-4 batch-71 = 0.01653600111603737

Training epoch-4 batch-72
Running loss of epoch-4 batch-72 = 0.018474290147423744

Training epoch-4 batch-73
Running loss of epoch-4 batch-73 = 0.014002565294504166

Training epoch-4 batch-74
Running loss of epoch-4 batch-74 = 0.017063552513718605

Training epoch-4 batch-75
Running loss of epoch-4 batch-75 = 0.01566806621849537

Training epoch-4 batch-76
Running loss of epoch-4 batch-76 = 0.02231304533779621

Training epoch-4 batch-77
Running loss of epoch-4 batch-77 = 0.016926268115639687

Training epoch-4 batch-78
Running loss of epoch-4 batch-78 = 0.01820835843682289

Training epoch-4 batch-79
Running loss of epoch-4 batch-79 = 0.019660577178001404

Training epoch-4 batch-80
Running loss of epoch-4 batch-80 = 0.020026154816150665

Training epoch-4 batch-81
Running loss of epoch-4 batch-81 = 0.02259751968085766

Training epoch-4 batch-82
Running loss of epoch-4 batch-82 = 0.019894041121006012

Training epoch-4 batch-83
Running loss of epoch-4 batch-83 = 0.015872377902269363

Training epoch-4 batch-84
Running loss of epoch-4 batch-84 = 0.016530906781554222

Training epoch-4 batch-85
Running loss of epoch-4 batch-85 = 0.01496933028101921

Training epoch-4 batch-86
Running loss of epoch-4 batch-86 = 0.01648523099720478

Training epoch-4 batch-87
Running loss of epoch-4 batch-87 = 0.015458197332918644

Training epoch-4 batch-88
Running loss of epoch-4 batch-88 = 0.016452942043542862

Training epoch-4 batch-89
Running loss of epoch-4 batch-89 = 0.018906887620687485

Training epoch-4 batch-90
Running loss of epoch-4 batch-90 = 0.018419042229652405

Training epoch-4 batch-91
Running loss of epoch-4 batch-91 = 0.01491567213088274

Training epoch-4 batch-92
Running loss of epoch-4 batch-92 = 0.017746595665812492

Training epoch-4 batch-93
Running loss of epoch-4 batch-93 = 0.01434951089322567

Training epoch-4 batch-94
Running loss of epoch-4 batch-94 = 0.014673887751996517

Training epoch-4 batch-95
Running loss of epoch-4 batch-95 = 0.015355650335550308

Training epoch-4 batch-96
Running loss of epoch-4 batch-96 = 0.013949282467365265

Training epoch-4 batch-97
Running loss of epoch-4 batch-97 = 0.014336317777633667

Training epoch-4 batch-98
Running loss of epoch-4 batch-98 = 0.01949603110551834

Training epoch-4 batch-99
Running loss of epoch-4 batch-99 = 0.01385424379259348

Training epoch-4 batch-100
Running loss of epoch-4 batch-100 = 0.01592240110039711

Training epoch-4 batch-101
Running loss of epoch-4 batch-101 = 0.019415955990552902

Training epoch-4 batch-102
Running loss of epoch-4 batch-102 = 0.014121410436928272

Training epoch-4 batch-103
Running loss of epoch-4 batch-103 = 0.016544971615076065

Training epoch-4 batch-104
Running loss of epoch-4 batch-104 = 0.01643003150820732

Training epoch-4 batch-105
Running loss of epoch-4 batch-105 = 0.01550342421978712

Training epoch-4 batch-106
Running loss of epoch-4 batch-106 = 0.015423020347952843

Training epoch-4 batch-107
Running loss of epoch-4 batch-107 = 0.02141254022717476

Training epoch-4 batch-108
Running loss of epoch-4 batch-108 = 0.013991223648190498

Training epoch-4 batch-109
Running loss of epoch-4 batch-109 = 0.01469156052917242

Training epoch-4 batch-110
Running loss of epoch-4 batch-110 = 0.018065042793750763

Training epoch-4 batch-111
Running loss of epoch-4 batch-111 = 0.015956567600369453

Training epoch-4 batch-112
Running loss of epoch-4 batch-112 = 0.016110531985759735

Training epoch-4 batch-113
Running loss of epoch-4 batch-113 = 0.013363296166062355

Training epoch-4 batch-114
Running loss of epoch-4 batch-114 = 0.015103817917406559

Training epoch-4 batch-115
Running loss of epoch-4 batch-115 = 0.012733522802591324

Training epoch-4 batch-116
Running loss of epoch-4 batch-116 = 0.01571468450129032

Training epoch-4 batch-117
Running loss of epoch-4 batch-117 = 0.01408474426716566

Training epoch-4 batch-118
Running loss of epoch-4 batch-118 = 0.018664812669157982

Training epoch-4 batch-119
Running loss of epoch-4 batch-119 = 0.013588900677859783

Training epoch-4 batch-120
Running loss of epoch-4 batch-120 = 0.017734196037054062

Training epoch-4 batch-121
Running loss of epoch-4 batch-121 = 0.016806233674287796

Training epoch-4 batch-122
Running loss of epoch-4 batch-122 = 0.014936870895326138

Training epoch-4 batch-123
Running loss of epoch-4 batch-123 = 0.01416755747050047

Training epoch-4 batch-124
Running loss of epoch-4 batch-124 = 0.013857959769666195

Training epoch-4 batch-125
Running loss of epoch-4 batch-125 = 0.01793009601533413

Training epoch-4 batch-126
Running loss of epoch-4 batch-126 = 0.014447925612330437

Training epoch-4 batch-127
Running loss of epoch-4 batch-127 = 0.014574931003153324

Training epoch-4 batch-128
Running loss of epoch-4 batch-128 = 0.012638626620173454

Training epoch-4 batch-129
Running loss of epoch-4 batch-129 = 0.0142650306224823

Training epoch-4 batch-130
Running loss of epoch-4 batch-130 = 0.01571071147918701

Training epoch-4 batch-131
Running loss of epoch-4 batch-131 = 0.01692052185535431

Training epoch-4 batch-132
Running loss of epoch-4 batch-132 = 0.019233185797929764

Training epoch-4 batch-133
Running loss of epoch-4 batch-133 = 0.015914397314190865

Training epoch-4 batch-134
Running loss of epoch-4 batch-134 = 0.016764236614108086

Training epoch-4 batch-135
Running loss of epoch-4 batch-135 = 0.01718279719352722

Training epoch-4 batch-136
Running loss of epoch-4 batch-136 = 0.015508470125496387

Training epoch-4 batch-137
Running loss of epoch-4 batch-137 = 0.014693218283355236

Training epoch-4 batch-138
Running loss of epoch-4 batch-138 = 0.018675392493605614

Training epoch-4 batch-139
Running loss of epoch-4 batch-139 = 0.013807512819766998

Training epoch-4 batch-140
Running loss of epoch-4 batch-140 = 0.015402214601635933

Training epoch-4 batch-141
Running loss of epoch-4 batch-141 = 0.010708911344408989

Training epoch-4 batch-142
Running loss of epoch-4 batch-142 = 0.015220494009554386

Training epoch-4 batch-143
Running loss of epoch-4 batch-143 = 0.011780175380408764

Training epoch-4 batch-144
Running loss of epoch-4 batch-144 = 0.013675821013748646

Training epoch-4 batch-145
Running loss of epoch-4 batch-145 = 0.016608530655503273

Training epoch-4 batch-146
Running loss of epoch-4 batch-146 = 0.012708642520010471

Training epoch-4 batch-147
Running loss of epoch-4 batch-147 = 0.013259053230285645

Training epoch-4 batch-148
Running loss of epoch-4 batch-148 = 0.01276396308094263

Training epoch-4 batch-149
Running loss of epoch-4 batch-149 = 0.01548673864454031

Training epoch-4 batch-150
Running loss of epoch-4 batch-150 = 0.013355418108403683

Training epoch-4 batch-151
Running loss of epoch-4 batch-151 = 0.01226489432156086

Training epoch-4 batch-152
Running loss of epoch-4 batch-152 = 0.012598134577274323

Training epoch-4 batch-153
Running loss of epoch-4 batch-153 = 0.01445809192955494

Training epoch-4 batch-154
Running loss of epoch-4 batch-154 = 0.016687825322151184

Training epoch-4 batch-155
Running loss of epoch-4 batch-155 = 0.014138856902718544

Training epoch-4 batch-156
Running loss of epoch-4 batch-156 = 0.016153402626514435

Training epoch-4 batch-157
Running loss of epoch-4 batch-157 = 0.05740615352988243

Finished training epoch-4.



Average train loss at epoch-4 = 0.017449794191122053

Started Evaluation

Average val loss at epoch-4 = 0.8101294425953376

Accuracy for classes:
Accuracy for class equals is: 93.56 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 86.56 %
Accuracy for class onCreate is: 82.09 %
Accuracy for class toString is: 76.45 %
Accuracy for class run is: 45.43 %
Accuracy for class hashCode is: 97.00 %
Accuracy for class init is: 33.41 %
Accuracy for class execute is: 19.68 %
Accuracy for class get is: 71.03 %

Overall Accuracy = 74.52 %


Best Accuracy = 74.52 % at Epoch-4
Saving model after best epoch-4

Finished Evaluation



Started training epoch-5


Training epoch-5 batch-1
Running loss of epoch-5 batch-1 = 0.01255833264440298

Training epoch-5 batch-2
Running loss of epoch-5 batch-2 = 0.015754517167806625

Training epoch-5 batch-3
Running loss of epoch-5 batch-3 = 0.014059018343687057

Training epoch-5 batch-4
Running loss of epoch-5 batch-4 = 0.012225916609168053

Training epoch-5 batch-5
Running loss of epoch-5 batch-5 = 0.012989181093871593

Training epoch-5 batch-6
Running loss of epoch-5 batch-6 = 0.012770920991897583

Training epoch-5 batch-7
Running loss of epoch-5 batch-7 = 0.013906838372349739

Training epoch-5 batch-8
Running loss of epoch-5 batch-8 = 0.013276432640850544

Training epoch-5 batch-9
Running loss of epoch-5 batch-9 = 0.012973095290362835

Training epoch-5 batch-10
Running loss of epoch-5 batch-10 = 0.017055952921509743

Training epoch-5 batch-11
Running loss of epoch-5 batch-11 = 0.013364830054342747

Training epoch-5 batch-12
Running loss of epoch-5 batch-12 = 0.01294033695012331

Training epoch-5 batch-13
Running loss of epoch-5 batch-13 = 0.015780221670866013

Training epoch-5 batch-14
Running loss of epoch-5 batch-14 = 0.015792014077305794

Training epoch-5 batch-15
Running loss of epoch-5 batch-15 = 0.014617033302783966

Training epoch-5 batch-16
Running loss of epoch-5 batch-16 = 0.01248030923306942

Training epoch-5 batch-17
Running loss of epoch-5 batch-17 = 0.012668697163462639

Training epoch-5 batch-18
Running loss of epoch-5 batch-18 = 0.011622432619333267

Training epoch-5 batch-19
Running loss of epoch-5 batch-19 = 0.013896191492676735

Training epoch-5 batch-20
Running loss of epoch-5 batch-20 = 0.011934546753764153

Training epoch-5 batch-21
Running loss of epoch-5 batch-21 = 0.01412227377295494

Training epoch-5 batch-22
Running loss of epoch-5 batch-22 = 0.012504871934652328

Training epoch-5 batch-23
Running loss of epoch-5 batch-23 = 0.016111349686980247

Training epoch-5 batch-24
Running loss of epoch-5 batch-24 = 0.011244870722293854

Training epoch-5 batch-25
Running loss of epoch-5 batch-25 = 0.013866160996258259

Training epoch-5 batch-26
Running loss of epoch-5 batch-26 = 0.011905296705663204

Training epoch-5 batch-27
Running loss of epoch-5 batch-27 = 0.01666220836341381

Training epoch-5 batch-28
Running loss of epoch-5 batch-28 = 0.013123124837875366

Training epoch-5 batch-29
Running loss of epoch-5 batch-29 = 0.01445810403674841

Training epoch-5 batch-30
Running loss of epoch-5 batch-30 = 0.011865876615047455

Training epoch-5 batch-31
Running loss of epoch-5 batch-31 = 0.012374015524983406

Training epoch-5 batch-32
Running loss of epoch-5 batch-32 = 0.012499911710619926

Training epoch-5 batch-33
Running loss of epoch-5 batch-33 = 0.012389427982270718

Training epoch-5 batch-34
Running loss of epoch-5 batch-34 = 0.01148587092757225

Training epoch-5 batch-35
Running loss of epoch-5 batch-35 = 0.01153371762484312

Training epoch-5 batch-36
Running loss of epoch-5 batch-36 = 0.009008360095322132

Training epoch-5 batch-37
Running loss of epoch-5 batch-37 = 0.01629430428147316

Training epoch-5 batch-38
Running loss of epoch-5 batch-38 = 0.01400469895452261

Training epoch-5 batch-39
Running loss of epoch-5 batch-39 = 0.011096050962805748

Training epoch-5 batch-40
Running loss of epoch-5 batch-40 = 0.013965155929327011

Training epoch-5 batch-41
Running loss of epoch-5 batch-41 = 0.014074989594519138

Training epoch-5 batch-42
Running loss of epoch-5 batch-42 = 0.01490875892341137

Training epoch-5 batch-43
Running loss of epoch-5 batch-43 = 0.016518576070666313

Training epoch-5 batch-44
Running loss of epoch-5 batch-44 = 0.008245263248682022

Training epoch-5 batch-45
Running loss of epoch-5 batch-45 = 0.015998870134353638

Training epoch-5 batch-46
Running loss of epoch-5 batch-46 = 0.011235435493290424

Training epoch-5 batch-47
Running loss of epoch-5 batch-47 = 0.011353413574397564

Training epoch-5 batch-48
Running loss of epoch-5 batch-48 = 0.013725554570555687

Training epoch-5 batch-49
Running loss of epoch-5 batch-49 = 0.01007448323071003

Training epoch-5 batch-50
Running loss of epoch-5 batch-50 = 0.012488463893532753

Training epoch-5 batch-51
Running loss of epoch-5 batch-51 = 0.012859059497714043

Training epoch-5 batch-52
Running loss of epoch-5 batch-52 = 0.010154527612030506

Training epoch-5 batch-53
Running loss of epoch-5 batch-53 = 0.01333631295710802

Training epoch-5 batch-54
Running loss of epoch-5 batch-54 = 0.013772561214864254

Training epoch-5 batch-55
Running loss of epoch-5 batch-55 = 0.017618590965867043

Training epoch-5 batch-56
Running loss of epoch-5 batch-56 = 0.016517892479896545

Training epoch-5 batch-57
Running loss of epoch-5 batch-57 = 0.012037519365549088

Training epoch-5 batch-58
Running loss of epoch-5 batch-58 = 0.015484089963138103

Training epoch-5 batch-59
Running loss of epoch-5 batch-59 = 0.011990330182015896

Training epoch-5 batch-60
Running loss of epoch-5 batch-60 = 0.012344565242528915

Training epoch-5 batch-61
Running loss of epoch-5 batch-61 = 0.014582309871912003

Training epoch-5 batch-62
Running loss of epoch-5 batch-62 = 0.011934473179280758

Training epoch-5 batch-63
Running loss of epoch-5 batch-63 = 0.010723665356636047

Training epoch-5 batch-64
Running loss of epoch-5 batch-64 = 0.013027975335717201

Training epoch-5 batch-65
Running loss of epoch-5 batch-65 = 0.013316327705979347

Training epoch-5 batch-66
Running loss of epoch-5 batch-66 = 0.018693652004003525

Training epoch-5 batch-67
Running loss of epoch-5 batch-67 = 0.011580738238990307

Training epoch-5 batch-68
Running loss of epoch-5 batch-68 = 0.013380948454141617

Training epoch-5 batch-69
Running loss of epoch-5 batch-69 = 0.011388270184397697

Training epoch-5 batch-70
Running loss of epoch-5 batch-70 = 0.013939691707491875

Training epoch-5 batch-71
Running loss of epoch-5 batch-71 = 0.011876030825078487

Training epoch-5 batch-72
Running loss of epoch-5 batch-72 = 0.012243990786373615

Training epoch-5 batch-73
Running loss of epoch-5 batch-73 = 0.01763063669204712

Training epoch-5 batch-74
Running loss of epoch-5 batch-74 = 0.008505254052579403

Training epoch-5 batch-75
Running loss of epoch-5 batch-75 = 0.010763145983219147

Training epoch-5 batch-76
Running loss of epoch-5 batch-76 = 0.012009894475340843

Training epoch-5 batch-77
Running loss of epoch-5 batch-77 = 0.011052709072828293

Training epoch-5 batch-78
Running loss of epoch-5 batch-78 = 0.009861744940280914

Training epoch-5 batch-79
Running loss of epoch-5 batch-79 = 0.01314264815300703

Training epoch-5 batch-80
Running loss of epoch-5 batch-80 = 0.01255503948777914

Training epoch-5 batch-81
Running loss of epoch-5 batch-81 = 0.00984503049403429

Training epoch-5 batch-82
Running loss of epoch-5 batch-82 = 0.010546620935201645

Training epoch-5 batch-83
Running loss of epoch-5 batch-83 = 0.014298615977168083

Training epoch-5 batch-84
Running loss of epoch-5 batch-84 = 0.013678092509508133

Training epoch-5 batch-85
Running loss of epoch-5 batch-85 = 0.014406479895114899

Training epoch-5 batch-86
Running loss of epoch-5 batch-86 = 0.01193646527826786

Training epoch-5 batch-87
Running loss of epoch-5 batch-87 = 0.015975182875990868

Training epoch-5 batch-88
Running loss of epoch-5 batch-88 = 0.01340513490140438

Training epoch-5 batch-89
Running loss of epoch-5 batch-89 = 0.011265797540545464

Training epoch-5 batch-90
Running loss of epoch-5 batch-90 = 0.011204446665942669

Training epoch-5 batch-91
Running loss of epoch-5 batch-91 = 0.012310484424233437

Training epoch-5 batch-92
Running loss of epoch-5 batch-92 = 0.012646378017961979

Training epoch-5 batch-93
Running loss of epoch-5 batch-93 = 0.012751761823892593

Training epoch-5 batch-94
Running loss of epoch-5 batch-94 = 0.011864271946251392

Training epoch-5 batch-95
Running loss of epoch-5 batch-95 = 0.013939330354332924

Training epoch-5 batch-96
Running loss of epoch-5 batch-96 = 0.015338259749114513

Training epoch-5 batch-97
Running loss of epoch-5 batch-97 = 0.010915775783360004

Training epoch-5 batch-98
Running loss of epoch-5 batch-98 = 0.012454173527657986

Training epoch-5 batch-99
Running loss of epoch-5 batch-99 = 0.012129394337534904

Training epoch-5 batch-100
Running loss of epoch-5 batch-100 = 0.010171049274504185

Training epoch-5 batch-101
Running loss of epoch-5 batch-101 = 0.01141436118632555

Training epoch-5 batch-102
Running loss of epoch-5 batch-102 = 0.011127523146569729

Training epoch-5 batch-103
Running loss of epoch-5 batch-103 = 0.011882146820425987

Training epoch-5 batch-104
Running loss of epoch-5 batch-104 = 0.012926015071570873

Training epoch-5 batch-105
Running loss of epoch-5 batch-105 = 0.012570463120937347

Training epoch-5 batch-106
Running loss of epoch-5 batch-106 = 0.009957225061953068

Training epoch-5 batch-107
Running loss of epoch-5 batch-107 = 0.011875426396727562

Training epoch-5 batch-108
Running loss of epoch-5 batch-108 = 0.0133806262165308

Training epoch-5 batch-109
Running loss of epoch-5 batch-109 = 0.012446516193449497

Training epoch-5 batch-110
Running loss of epoch-5 batch-110 = 0.011912209913134575

Training epoch-5 batch-111
Running loss of epoch-5 batch-111 = 0.012654499150812626

Training epoch-5 batch-112
Running loss of epoch-5 batch-112 = 0.008905656635761261

Training epoch-5 batch-113
Running loss of epoch-5 batch-113 = 0.010578449815511703

Training epoch-5 batch-114
Running loss of epoch-5 batch-114 = 0.010983482003211975

Training epoch-5 batch-115
Running loss of epoch-5 batch-115 = 0.011489135213196278

Training epoch-5 batch-116
Running loss of epoch-5 batch-116 = 0.015408904291689396

Training epoch-5 batch-117
Running loss of epoch-5 batch-117 = 0.012334246188402176

Training epoch-5 batch-118
Running loss of epoch-5 batch-118 = 0.008617494255304337

Training epoch-5 batch-119
Running loss of epoch-5 batch-119 = 0.010836750268936157

Training epoch-5 batch-120
Running loss of epoch-5 batch-120 = 0.01076776534318924

Training epoch-5 batch-121
Running loss of epoch-5 batch-121 = 0.013522590510547161

Training epoch-5 batch-122
Running loss of epoch-5 batch-122 = 0.011771478690207005

Training epoch-5 batch-123
Running loss of epoch-5 batch-123 = 0.012163802981376648

Training epoch-5 batch-124
Running loss of epoch-5 batch-124 = 0.012602384202182293

Training epoch-5 batch-125
Running loss of epoch-5 batch-125 = 0.010340158827602863

Training epoch-5 batch-126
Running loss of epoch-5 batch-126 = 0.010298024863004684

Training epoch-5 batch-127
Running loss of epoch-5 batch-127 = 0.010281057097017765

Training epoch-5 batch-128
Running loss of epoch-5 batch-128 = 0.014755358919501305

Training epoch-5 batch-129
Running loss of epoch-5 batch-129 = 0.01217359583824873

Training epoch-5 batch-130
Running loss of epoch-5 batch-130 = 0.012102149426937103

Training epoch-5 batch-131
Running loss of epoch-5 batch-131 = 0.011490320786833763

Training epoch-5 batch-132
Running loss of epoch-5 batch-132 = 0.01254600565880537

Training epoch-5 batch-133
Running loss of epoch-5 batch-133 = 0.012419609352946281

Training epoch-5 batch-134
Running loss of epoch-5 batch-134 = 0.011752549558877945

Training epoch-5 batch-135
Running loss of epoch-5 batch-135 = 0.011535298079252243

Training epoch-5 batch-136
Running loss of epoch-5 batch-136 = 0.01337114442139864

Training epoch-5 batch-137
Running loss of epoch-5 batch-137 = 0.015944115817546844

Training epoch-5 batch-138
Running loss of epoch-5 batch-138 = 0.012035018764436245

Training epoch-5 batch-139
Running loss of epoch-5 batch-139 = 0.013149445876479149

Training epoch-5 batch-140
Running loss of epoch-5 batch-140 = 0.010681156069040298

Training epoch-5 batch-141
Running loss of epoch-5 batch-141 = 0.016140636056661606

Training epoch-5 batch-142
Running loss of epoch-5 batch-142 = 0.012218864634633064

Training epoch-5 batch-143
Running loss of epoch-5 batch-143 = 0.011210203170776367

Training epoch-5 batch-144
Running loss of epoch-5 batch-144 = 0.013252188451588154

Training epoch-5 batch-145
Running loss of epoch-5 batch-145 = 0.008548520505428314

Training epoch-5 batch-146
Running loss of epoch-5 batch-146 = 0.010946281254291534

Training epoch-5 batch-147
Running loss of epoch-5 batch-147 = 0.011516202241182327

Training epoch-5 batch-148
Running loss of epoch-5 batch-148 = 0.011147584766149521

Training epoch-5 batch-149
Running loss of epoch-5 batch-149 = 0.010408188216388226

Training epoch-5 batch-150
Running loss of epoch-5 batch-150 = 0.009308145381510258

Training epoch-5 batch-151
Running loss of epoch-5 batch-151 = 0.011022226884961128

Training epoch-5 batch-152
Running loss of epoch-5 batch-152 = 0.00986649189144373

Training epoch-5 batch-153
Running loss of epoch-5 batch-153 = 0.012258236296474934

Training epoch-5 batch-154
Running loss of epoch-5 batch-154 = 0.013981394469738007

Training epoch-5 batch-155
Running loss of epoch-5 batch-155 = 0.01564883440732956

Training epoch-5 batch-156
Running loss of epoch-5 batch-156 = 0.010898848995566368

Training epoch-5 batch-157
Running loss of epoch-5 batch-157 = 0.047473423182964325

Finished training epoch-5.



Average train loss at epoch-5 = 0.012662137520313263

Started Evaluation

Average val loss at epoch-5 = 0.7147844176935522

Accuracy for classes:
Accuracy for class equals is: 94.39 %
Accuracy for class main is: 98.03 %
Accuracy for class setUp is: 89.34 %
Accuracy for class onCreate is: 81.02 %
Accuracy for class toString is: 88.40 %
Accuracy for class run is: 63.93 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 35.20 %
Accuracy for class execute is: 53.82 %
Accuracy for class get is: 44.10 %

Overall Accuracy = 77.12 %


Best Accuracy = 77.12 % at Epoch-5
Saving model after best epoch-5

Finished Evaluation



Started training epoch-6


Training epoch-6 batch-1
Running loss of epoch-6 batch-1 = 0.009627451188862324

Training epoch-6 batch-2
Running loss of epoch-6 batch-2 = 0.009915867820382118

Training epoch-6 batch-3
Running loss of epoch-6 batch-3 = 0.0089477663859725

Training epoch-6 batch-4
Running loss of epoch-6 batch-4 = 0.009232603013515472

Training epoch-6 batch-5
Running loss of epoch-6 batch-5 = 0.00876331701874733

Training epoch-6 batch-6
Running loss of epoch-6 batch-6 = 0.011203278787434101

Training epoch-6 batch-7
Running loss of epoch-6 batch-7 = 0.00882745161652565

Training epoch-6 batch-8
Running loss of epoch-6 batch-8 = 0.013563420623540878

Training epoch-6 batch-9
Running loss of epoch-6 batch-9 = 0.009084065444767475

Training epoch-6 batch-10
Running loss of epoch-6 batch-10 = 0.008293467573821545

Training epoch-6 batch-11
Running loss of epoch-6 batch-11 = 0.01248304732143879

Training epoch-6 batch-12
Running loss of epoch-6 batch-12 = 0.012856341898441315

Training epoch-6 batch-13
Running loss of epoch-6 batch-13 = 0.008099248632788658

Training epoch-6 batch-14
Running loss of epoch-6 batch-14 = 0.010457808151841164

Training epoch-6 batch-15
Running loss of epoch-6 batch-15 = 0.012222804129123688

Training epoch-6 batch-16
Running loss of epoch-6 batch-16 = 0.013417130336165428

Training epoch-6 batch-17
Running loss of epoch-6 batch-17 = 0.009924582205712795

Training epoch-6 batch-18
Running loss of epoch-6 batch-18 = 0.01062680035829544

Training epoch-6 batch-19
Running loss of epoch-6 batch-19 = 0.009290038608014584

Training epoch-6 batch-20
Running loss of epoch-6 batch-20 = 0.011279165744781494

Training epoch-6 batch-21
Running loss of epoch-6 batch-21 = 0.011451845988631248

Training epoch-6 batch-22
Running loss of epoch-6 batch-22 = 0.011302661150693893

Training epoch-6 batch-23
Running loss of epoch-6 batch-23 = 0.012649085372686386

Training epoch-6 batch-24
Running loss of epoch-6 batch-24 = 0.010705648921430111

Training epoch-6 batch-25
Running loss of epoch-6 batch-25 = 0.01489049382507801

Training epoch-6 batch-26
Running loss of epoch-6 batch-26 = 0.01274133287370205

Training epoch-6 batch-27
Running loss of epoch-6 batch-27 = 0.0075392527505755424

Training epoch-6 batch-28
Running loss of epoch-6 batch-28 = 0.009329047054052353

Training epoch-6 batch-29
Running loss of epoch-6 batch-29 = 0.010194939561188221

Training epoch-6 batch-30
Running loss of epoch-6 batch-30 = 0.008651918731629848

Training epoch-6 batch-31
Running loss of epoch-6 batch-31 = 0.009321155026555061

Training epoch-6 batch-32
Running loss of epoch-6 batch-32 = 0.008949578739702702

Training epoch-6 batch-33
Running loss of epoch-6 batch-33 = 0.012210114859044552

Training epoch-6 batch-34
Running loss of epoch-6 batch-34 = 0.009454258717596531

Training epoch-6 batch-35
Running loss of epoch-6 batch-35 = 0.010946325957775116

Training epoch-6 batch-36
Running loss of epoch-6 batch-36 = 0.009926975704729557

Training epoch-6 batch-37
Running loss of epoch-6 batch-37 = 0.008571606129407883

Training epoch-6 batch-38
Running loss of epoch-6 batch-38 = 0.00950899813324213

Training epoch-6 batch-39
Running loss of epoch-6 batch-39 = 0.00931129977107048

Training epoch-6 batch-40
Running loss of epoch-6 batch-40 = 0.009136646054685116

Training epoch-6 batch-41
Running loss of epoch-6 batch-41 = 0.009181083180010319

Training epoch-6 batch-42
Running loss of epoch-6 batch-42 = 0.007297115866094828

Training epoch-6 batch-43
Running loss of epoch-6 batch-43 = 0.011351536959409714

Training epoch-6 batch-44
Running loss of epoch-6 batch-44 = 0.011859376914799213

Training epoch-6 batch-45
Running loss of epoch-6 batch-45 = 0.009374325163662434

Training epoch-6 batch-46
Running loss of epoch-6 batch-46 = 0.009526416659355164

Training epoch-6 batch-47
Running loss of epoch-6 batch-47 = 0.008374357596039772

Training epoch-6 batch-48
Running loss of epoch-6 batch-48 = 0.010814906097948551

Training epoch-6 batch-49
Running loss of epoch-6 batch-49 = 0.011900849640369415

Training epoch-6 batch-50
Running loss of epoch-6 batch-50 = 0.008594759739935398

Training epoch-6 batch-51
Running loss of epoch-6 batch-51 = 0.007382534444332123

Training epoch-6 batch-52
Running loss of epoch-6 batch-52 = 0.011329375207424164

Training epoch-6 batch-53
Running loss of epoch-6 batch-53 = 0.008927874267101288

Training epoch-6 batch-54
Running loss of epoch-6 batch-54 = 0.008657692931592464

Training epoch-6 batch-55
Running loss of epoch-6 batch-55 = 0.015094042755663395

Training epoch-6 batch-56
Running loss of epoch-6 batch-56 = 0.009507281705737114

Training epoch-6 batch-57
Running loss of epoch-6 batch-57 = 0.007957215420901775

Training epoch-6 batch-58
Running loss of epoch-6 batch-58 = 0.007936644367873669

Training epoch-6 batch-59
Running loss of epoch-6 batch-59 = 0.011718071065843105

Training epoch-6 batch-60
Running loss of epoch-6 batch-60 = 0.011598696000874043

Training epoch-6 batch-61
Running loss of epoch-6 batch-61 = 0.00822717510163784

Training epoch-6 batch-62
Running loss of epoch-6 batch-62 = 0.010680575855076313

Training epoch-6 batch-63
Running loss of epoch-6 batch-63 = 0.010604111477732658

Training epoch-6 batch-64
Running loss of epoch-6 batch-64 = 0.010353831574320793

Training epoch-6 batch-65
Running loss of epoch-6 batch-65 = 0.008142093196511269

Training epoch-6 batch-66
Running loss of epoch-6 batch-66 = 0.006676843855530024

Training epoch-6 batch-67
Running loss of epoch-6 batch-67 = 0.011651973240077496

Training epoch-6 batch-68
Running loss of epoch-6 batch-68 = 0.009314383380115032

Training epoch-6 batch-69
Running loss of epoch-6 batch-69 = 0.011041873134672642

Training epoch-6 batch-70
Running loss of epoch-6 batch-70 = 0.011165935546159744

Training epoch-6 batch-71
Running loss of epoch-6 batch-71 = 0.008603362366557121

Training epoch-6 batch-72
Running loss of epoch-6 batch-72 = 0.008552394807338715

Training epoch-6 batch-73
Running loss of epoch-6 batch-73 = 0.010495636612176895

Training epoch-6 batch-74
Running loss of epoch-6 batch-74 = 0.007426296826452017

Training epoch-6 batch-75
Running loss of epoch-6 batch-75 = 0.008675714954733849

Training epoch-6 batch-76
Running loss of epoch-6 batch-76 = 0.011783837340772152

Training epoch-6 batch-77
Running loss of epoch-6 batch-77 = 0.006412847433239222

Training epoch-6 batch-78
Running loss of epoch-6 batch-78 = 0.011335114948451519

Training epoch-6 batch-79
Running loss of epoch-6 batch-79 = 0.011790825985372066

Training epoch-6 batch-80
Running loss of epoch-6 batch-80 = 0.009171436540782452

Training epoch-6 batch-81
Running loss of epoch-6 batch-81 = 0.010797930881381035

Training epoch-6 batch-82
Running loss of epoch-6 batch-82 = 0.009894788265228271

Training epoch-6 batch-83
Running loss of epoch-6 batch-83 = 0.011350585147738457

Training epoch-6 batch-84
Running loss of epoch-6 batch-84 = 0.012287449091672897

Training epoch-6 batch-85
Running loss of epoch-6 batch-85 = 0.010639318265020847

Training epoch-6 batch-86
Running loss of epoch-6 batch-86 = 0.008009280078113079

Training epoch-6 batch-87
Running loss of epoch-6 batch-87 = 0.010186731815338135

Training epoch-6 batch-88
Running loss of epoch-6 batch-88 = 0.010038964450359344

Training epoch-6 batch-89
Running loss of epoch-6 batch-89 = 0.010367135517299175

Training epoch-6 batch-90
Running loss of epoch-6 batch-90 = 0.009594611823558807

Training epoch-6 batch-91
Running loss of epoch-6 batch-91 = 0.009899819269776344

Training epoch-6 batch-92
Running loss of epoch-6 batch-92 = 0.009885628707706928

Training epoch-6 batch-93
Running loss of epoch-6 batch-93 = 0.008208430372178555

Training epoch-6 batch-94
Running loss of epoch-6 batch-94 = 0.012322746217250824

Training epoch-6 batch-95
Running loss of epoch-6 batch-95 = 0.0111031923443079

Training epoch-6 batch-96
Running loss of epoch-6 batch-96 = 0.012638328596949577

Training epoch-6 batch-97
Running loss of epoch-6 batch-97 = 0.01062410231679678

Training epoch-6 batch-98
Running loss of epoch-6 batch-98 = 0.014133473858237267

Training epoch-6 batch-99
Running loss of epoch-6 batch-99 = 0.009792313911020756

Training epoch-6 batch-100
Running loss of epoch-6 batch-100 = 0.011657016351819038

Training epoch-6 batch-101
Running loss of epoch-6 batch-101 = 0.009981449693441391

Training epoch-6 batch-102
Running loss of epoch-6 batch-102 = 0.008061074651777744

Training epoch-6 batch-103
Running loss of epoch-6 batch-103 = 0.007840734906494617

Training epoch-6 batch-104
Running loss of epoch-6 batch-104 = 0.00991018395870924

Training epoch-6 batch-105
Running loss of epoch-6 batch-105 = 0.013369613327085972

Training epoch-6 batch-106
Running loss of epoch-6 batch-106 = 0.01100461557507515

Training epoch-6 batch-107
Running loss of epoch-6 batch-107 = 0.008503183722496033

Training epoch-6 batch-108
Running loss of epoch-6 batch-108 = 0.00939089898020029

Training epoch-6 batch-109
Running loss of epoch-6 batch-109 = 0.008636955171823502

Training epoch-6 batch-110
Running loss of epoch-6 batch-110 = 0.012566757388412952

Training epoch-6 batch-111
Running loss of epoch-6 batch-111 = 0.011328267864882946

Training epoch-6 batch-112
Running loss of epoch-6 batch-112 = 0.008938932791352272

Training epoch-6 batch-113
Running loss of epoch-6 batch-113 = 0.009890043176710606

Training epoch-6 batch-114
Running loss of epoch-6 batch-114 = 0.008359178900718689

Training epoch-6 batch-115
Running loss of epoch-6 batch-115 = 0.009764134883880615

Training epoch-6 batch-116
Running loss of epoch-6 batch-116 = 0.017611578106880188

Training epoch-6 batch-117
Running loss of epoch-6 batch-117 = 0.011350086890161037

Training epoch-6 batch-118
Running loss of epoch-6 batch-118 = 0.0117916539311409

Training epoch-6 batch-119
Running loss of epoch-6 batch-119 = 0.0072131073102355

Training epoch-6 batch-120
Running loss of epoch-6 batch-120 = 0.011398959904909134

Training epoch-6 batch-121
Running loss of epoch-6 batch-121 = 0.009131875820457935

Training epoch-6 batch-122
Running loss of epoch-6 batch-122 = 0.007884113118052483

Training epoch-6 batch-123
Running loss of epoch-6 batch-123 = 0.009532036259770393

Training epoch-6 batch-124
Running loss of epoch-6 batch-124 = 0.009154478088021278

Training epoch-6 batch-125
Running loss of epoch-6 batch-125 = 0.012842046096920967

Training epoch-6 batch-126
Running loss of epoch-6 batch-126 = 0.009819231927394867

Training epoch-6 batch-127
Running loss of epoch-6 batch-127 = 0.011360537260770798

Training epoch-6 batch-128
Running loss of epoch-6 batch-128 = 0.008638552390038967

Training epoch-6 batch-129
Running loss of epoch-6 batch-129 = 0.010038407519459724

Training epoch-6 batch-130
Running loss of epoch-6 batch-130 = 0.012700265273451805

Training epoch-6 batch-131
Running loss of epoch-6 batch-131 = 0.011879509314894676

Training epoch-6 batch-132
Running loss of epoch-6 batch-132 = 0.009683075360953808

Training epoch-6 batch-133
Running loss of epoch-6 batch-133 = 0.010551455430686474

Training epoch-6 batch-134
Running loss of epoch-6 batch-134 = 0.011589158326387405

Training epoch-6 batch-135
Running loss of epoch-6 batch-135 = 0.01247001625597477

Training epoch-6 batch-136
Running loss of epoch-6 batch-136 = 0.008760172873735428

Training epoch-6 batch-137
Running loss of epoch-6 batch-137 = 0.017227094620466232

Training epoch-6 batch-138
Running loss of epoch-6 batch-138 = 0.008879294618964195

Training epoch-6 batch-139
Running loss of epoch-6 batch-139 = 0.011998657137155533

Training epoch-6 batch-140
Running loss of epoch-6 batch-140 = 0.009058473631739616

Training epoch-6 batch-141
Running loss of epoch-6 batch-141 = 0.011232087388634682

Training epoch-6 batch-142
Running loss of epoch-6 batch-142 = 0.010244411416351795

Training epoch-6 batch-143
Running loss of epoch-6 batch-143 = 0.010889532044529915

Training epoch-6 batch-144
Running loss of epoch-6 batch-144 = 0.00908459722995758

Training epoch-6 batch-145
Running loss of epoch-6 batch-145 = 0.007118081673979759

Training epoch-6 batch-146
Running loss of epoch-6 batch-146 = 0.010428420267999172

Training epoch-6 batch-147
Running loss of epoch-6 batch-147 = 0.009903673082590103

Training epoch-6 batch-148
Running loss of epoch-6 batch-148 = 0.01199622917920351

Training epoch-6 batch-149
Running loss of epoch-6 batch-149 = 0.008262500166893005

Training epoch-6 batch-150
Running loss of epoch-6 batch-150 = 0.007857589982450008

Training epoch-6 batch-151
Running loss of epoch-6 batch-151 = 0.012136680074036121

Training epoch-6 batch-152
Running loss of epoch-6 batch-152 = 0.011279435828328133

Training epoch-6 batch-153
Running loss of epoch-6 batch-153 = 0.010245163924992085

Training epoch-6 batch-154
Running loss of epoch-6 batch-154 = 0.009490608237683773

Training epoch-6 batch-155
Running loss of epoch-6 batch-155 = 0.008815388195216656

Training epoch-6 batch-156
Running loss of epoch-6 batch-156 = 0.011631621979176998

Training epoch-6 batch-157
Running loss of epoch-6 batch-157 = 0.061740290373563766

Finished training epoch-6.



Average train loss at epoch-6 = 0.010335949218273162

Started Evaluation

Average val loss at epoch-6 = 0.609123688166667

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 97.21 %
Accuracy for class setUp is: 88.69 %
Accuracy for class onCreate is: 91.04 %
Accuracy for class toString is: 81.23 %
Accuracy for class run is: 61.87 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 45.52 %
Accuracy for class execute is: 29.72 %
Accuracy for class get is: 74.36 %

Overall Accuracy = 80.59 %


Best Accuracy = 80.59 % at Epoch-6
Saving model after best epoch-6

Finished Evaluation



Started training epoch-7


Training epoch-7 batch-1
Running loss of epoch-7 batch-1 = 0.00832991860806942

Training epoch-7 batch-2
Running loss of epoch-7 batch-2 = 0.006160387769341469

Training epoch-7 batch-3
Running loss of epoch-7 batch-3 = 0.007777833379805088

Training epoch-7 batch-4
Running loss of epoch-7 batch-4 = 0.009352477267384529

Training epoch-7 batch-5
Running loss of epoch-7 batch-5 = 0.014692239463329315

Training epoch-7 batch-6
Running loss of epoch-7 batch-6 = 0.010721748694777489

Training epoch-7 batch-7
Running loss of epoch-7 batch-7 = 0.010891447775065899

Training epoch-7 batch-8
Running loss of epoch-7 batch-8 = 0.008775283582508564

Training epoch-7 batch-9
Running loss of epoch-7 batch-9 = 0.00735230278223753

Training epoch-7 batch-10
Running loss of epoch-7 batch-10 = 0.01021583192050457

Training epoch-7 batch-11
Running loss of epoch-7 batch-11 = 0.009096949361264706

Training epoch-7 batch-12
Running loss of epoch-7 batch-12 = 0.007758202962577343

Training epoch-7 batch-13
Running loss of epoch-7 batch-13 = 0.011102587915956974

Training epoch-7 batch-14
Running loss of epoch-7 batch-14 = 0.006455657538026571

Training epoch-7 batch-15
Running loss of epoch-7 batch-15 = 0.008762810379266739

Training epoch-7 batch-16
Running loss of epoch-7 batch-16 = 0.009369793348014355

Training epoch-7 batch-17
Running loss of epoch-7 batch-17 = 0.012638282030820847

Training epoch-7 batch-18
Running loss of epoch-7 batch-18 = 0.007150644902139902

Training epoch-7 batch-19
Running loss of epoch-7 batch-19 = 0.010224939323961735

Training epoch-7 batch-20
Running loss of epoch-7 batch-20 = 0.009631934575736523

Training epoch-7 batch-21
Running loss of epoch-7 batch-21 = 0.007530903443694115

Training epoch-7 batch-22
Running loss of epoch-7 batch-22 = 0.008425732143223286

Training epoch-7 batch-23
Running loss of epoch-7 batch-23 = 0.007386330980807543

Training epoch-7 batch-24
Running loss of epoch-7 batch-24 = 0.009957357309758663

Training epoch-7 batch-25
Running loss of epoch-7 batch-25 = 0.010123366490006447

Training epoch-7 batch-26
Running loss of epoch-7 batch-26 = 0.010154991410672665

Training epoch-7 batch-27
Running loss of epoch-7 batch-27 = 0.007947455160319805

Training epoch-7 batch-28
Running loss of epoch-7 batch-28 = 0.007455331273376942

Training epoch-7 batch-29
Running loss of epoch-7 batch-29 = 0.01015336625277996

Training epoch-7 batch-30
Running loss of epoch-7 batch-30 = 0.010236275382339954

Training epoch-7 batch-31
Running loss of epoch-7 batch-31 = 0.011454496532678604

Training epoch-7 batch-32
Running loss of epoch-7 batch-32 = 0.008360815234482288

Training epoch-7 batch-33
Running loss of epoch-7 batch-33 = 0.00820911955088377

Training epoch-7 batch-34
Running loss of epoch-7 batch-34 = 0.00737483985722065

Training epoch-7 batch-35
Running loss of epoch-7 batch-35 = 0.008166657760739326

Training epoch-7 batch-36
Running loss of epoch-7 batch-36 = 0.009624450467526913

Training epoch-7 batch-37
Running loss of epoch-7 batch-37 = 0.009543213061988354

Training epoch-7 batch-38
Running loss of epoch-7 batch-38 = 0.006886185146868229

Training epoch-7 batch-39
Running loss of epoch-7 batch-39 = 0.008119134232401848

Training epoch-7 batch-40
Running loss of epoch-7 batch-40 = 0.007680198177695274

Training epoch-7 batch-41
Running loss of epoch-7 batch-41 = 0.00793159008026123

Training epoch-7 batch-42
Running loss of epoch-7 batch-42 = 0.010916457511484623

Training epoch-7 batch-43
Running loss of epoch-7 batch-43 = 0.009124377742409706

Training epoch-7 batch-44
Running loss of epoch-7 batch-44 = 0.007886694744229317

Training epoch-7 batch-45
Running loss of epoch-7 batch-45 = 0.009916475974023342

Training epoch-7 batch-46
Running loss of epoch-7 batch-46 = 0.008833779022097588

Training epoch-7 batch-47
Running loss of epoch-7 batch-47 = 0.008514108136296272

Training epoch-7 batch-48
Running loss of epoch-7 batch-48 = 0.011721654795110226

Training epoch-7 batch-49
Running loss of epoch-7 batch-49 = 0.010507860220968723

Training epoch-7 batch-50
Running loss of epoch-7 batch-50 = 0.008832687512040138

Training epoch-7 batch-51
Running loss of epoch-7 batch-51 = 0.007061022333800793

Training epoch-7 batch-52
Running loss of epoch-7 batch-52 = 0.008195262402296066

Training epoch-7 batch-53
Running loss of epoch-7 batch-53 = 0.0072724344208836555

Training epoch-7 batch-54
Running loss of epoch-7 batch-54 = 0.008207034319639206

Training epoch-7 batch-55
Running loss of epoch-7 batch-55 = 0.006405159831047058

Training epoch-7 batch-56
Running loss of epoch-7 batch-56 = 0.006766974925994873

Training epoch-7 batch-57
Running loss of epoch-7 batch-57 = 0.009806805290281773

Training epoch-7 batch-58
Running loss of epoch-7 batch-58 = 0.007862238213419914

Training epoch-7 batch-59
Running loss of epoch-7 batch-59 = 0.008649754337966442

Training epoch-7 batch-60
Running loss of epoch-7 batch-60 = 0.009816700592637062

Training epoch-7 batch-61
Running loss of epoch-7 batch-61 = 0.0111423684284091

Training epoch-7 batch-62
Running loss of epoch-7 batch-62 = 0.009490763768553734

Training epoch-7 batch-63
Running loss of epoch-7 batch-63 = 0.006810635328292847

Training epoch-7 batch-64
Running loss of epoch-7 batch-64 = 0.012035549618303776

Training epoch-7 batch-65
Running loss of epoch-7 batch-65 = 0.009213423356413841

Training epoch-7 batch-66
Running loss of epoch-7 batch-66 = 0.011230682954192162

Training epoch-7 batch-67
Running loss of epoch-7 batch-67 = 0.008325017988681793

Training epoch-7 batch-68
Running loss of epoch-7 batch-68 = 0.009461541660130024

Training epoch-7 batch-69
Running loss of epoch-7 batch-69 = 0.011530017480254173

Training epoch-7 batch-70
Running loss of epoch-7 batch-70 = 0.0068443831987679005

Training epoch-7 batch-71
Running loss of epoch-7 batch-71 = 0.008178658783435822

Training epoch-7 batch-72
Running loss of epoch-7 batch-72 = 0.00905575044453144

Training epoch-7 batch-73
Running loss of epoch-7 batch-73 = 0.012312149628996849

Training epoch-7 batch-74
Running loss of epoch-7 batch-74 = 0.010173933580517769

Training epoch-7 batch-75
Running loss of epoch-7 batch-75 = 0.007265450432896614

Training epoch-7 batch-76
Running loss of epoch-7 batch-76 = 0.008670821785926819

Training epoch-7 batch-77
Running loss of epoch-7 batch-77 = 0.00606098398566246

Training epoch-7 batch-78
Running loss of epoch-7 batch-78 = 0.008208191953599453

Training epoch-7 batch-79
Running loss of epoch-7 batch-79 = 0.006776697933673859

Training epoch-7 batch-80
Running loss of epoch-7 batch-80 = 0.007243643049150705

Training epoch-7 batch-81
Running loss of epoch-7 batch-81 = 0.008130579255521297

Training epoch-7 batch-82
Running loss of epoch-7 batch-82 = 0.0060945432633161545

Training epoch-7 batch-83
Running loss of epoch-7 batch-83 = 0.01021120510995388

Training epoch-7 batch-84
Running loss of epoch-7 batch-84 = 0.014358090236783028

Training epoch-7 batch-85
Running loss of epoch-7 batch-85 = 0.01381627842783928

Training epoch-7 batch-86
Running loss of epoch-7 batch-86 = 0.005723855458199978

Training epoch-7 batch-87
Running loss of epoch-7 batch-87 = 0.010050615295767784

Training epoch-7 batch-88
Running loss of epoch-7 batch-88 = 0.010153444483876228

Training epoch-7 batch-89
Running loss of epoch-7 batch-89 = 0.013171529397368431

Training epoch-7 batch-90
Running loss of epoch-7 batch-90 = 0.010795290581882

Training epoch-7 batch-91
Running loss of epoch-7 batch-91 = 0.009622452780604362

Training epoch-7 batch-92
Running loss of epoch-7 batch-92 = 0.00926285795867443

Training epoch-7 batch-93
Running loss of epoch-7 batch-93 = 0.01079566590487957

Training epoch-7 batch-94
Running loss of epoch-7 batch-94 = 0.006118626333773136

Training epoch-7 batch-95
Running loss of epoch-7 batch-95 = 0.00889755878597498

Training epoch-7 batch-96
Running loss of epoch-7 batch-96 = 0.00970086082816124

Training epoch-7 batch-97
Running loss of epoch-7 batch-97 = 0.0097119752317667

Training epoch-7 batch-98
Running loss of epoch-7 batch-98 = 0.007827814668416977

Training epoch-7 batch-99
Running loss of epoch-7 batch-99 = 0.011116278357803822

Training epoch-7 batch-100
Running loss of epoch-7 batch-100 = 0.008636360988020897

Training epoch-7 batch-101
Running loss of epoch-7 batch-101 = 0.01040929276496172

Training epoch-7 batch-102
Running loss of epoch-7 batch-102 = 0.009501444175839424

Training epoch-7 batch-103
Running loss of epoch-7 batch-103 = 0.009661908261477947

Training epoch-7 batch-104
Running loss of epoch-7 batch-104 = 0.009941200725734234

Training epoch-7 batch-105
Running loss of epoch-7 batch-105 = 0.008227523416280746

Training epoch-7 batch-106
Running loss of epoch-7 batch-106 = 0.008509877137839794

Training epoch-7 batch-107
Running loss of epoch-7 batch-107 = 0.008961621671915054

Training epoch-7 batch-108
Running loss of epoch-7 batch-108 = 0.011074849404394627

Training epoch-7 batch-109
Running loss of epoch-7 batch-109 = 0.010149278677999973

Training epoch-7 batch-110
Running loss of epoch-7 batch-110 = 0.009265110827982426

Training epoch-7 batch-111
Running loss of epoch-7 batch-111 = 0.006575598381459713

Training epoch-7 batch-112
Running loss of epoch-7 batch-112 = 0.012236851267516613

Training epoch-7 batch-113
Running loss of epoch-7 batch-113 = 0.011832838878035545

Training epoch-7 batch-114
Running loss of epoch-7 batch-114 = 0.0075493017211556435

Training epoch-7 batch-115
Running loss of epoch-7 batch-115 = 0.009517742320895195

Training epoch-7 batch-116
Running loss of epoch-7 batch-116 = 0.007318238262087107

Training epoch-7 batch-117
Running loss of epoch-7 batch-117 = 0.01076793484389782

Training epoch-7 batch-118
Running loss of epoch-7 batch-118 = 0.010154498741030693

Training epoch-7 batch-119
Running loss of epoch-7 batch-119 = 0.0075145140290260315

Training epoch-7 batch-120
Running loss of epoch-7 batch-120 = 0.01041906327009201

Training epoch-7 batch-121
Running loss of epoch-7 batch-121 = 0.006074179895222187

Training epoch-7 batch-122
Running loss of epoch-7 batch-122 = 0.008454637601971626

Training epoch-7 batch-123
Running loss of epoch-7 batch-123 = 0.01075625978410244

Training epoch-7 batch-124
Running loss of epoch-7 batch-124 = 0.006063660141080618

Training epoch-7 batch-125
Running loss of epoch-7 batch-125 = 0.009084823541343212

Training epoch-7 batch-126
Running loss of epoch-7 batch-126 = 0.009294897317886353

Training epoch-7 batch-127
Running loss of epoch-7 batch-127 = 0.0073598837479949

Training epoch-7 batch-128
Running loss of epoch-7 batch-128 = 0.009542722254991531

Training epoch-7 batch-129
Running loss of epoch-7 batch-129 = 0.0073648253455758095

Training epoch-7 batch-130
Running loss of epoch-7 batch-130 = 0.00669274665415287

Training epoch-7 batch-131
Running loss of epoch-7 batch-131 = 0.008553572930395603

Training epoch-7 batch-132
Running loss of epoch-7 batch-132 = 0.00908120721578598

Training epoch-7 batch-133
Running loss of epoch-7 batch-133 = 0.010779859498143196

Training epoch-7 batch-134
Running loss of epoch-7 batch-134 = 0.0072724283672869205

Training epoch-7 batch-135
Running loss of epoch-7 batch-135 = 0.006467327009886503

Training epoch-7 batch-136
Running loss of epoch-7 batch-136 = 0.008432778529822826

Training epoch-7 batch-137
Running loss of epoch-7 batch-137 = 0.007539866492152214

Training epoch-7 batch-138
Running loss of epoch-7 batch-138 = 0.006636661011725664

Training epoch-7 batch-139
Running loss of epoch-7 batch-139 = 0.007678858935832977

Training epoch-7 batch-140
Running loss of epoch-7 batch-140 = 0.006824597250670195

Training epoch-7 batch-141
Running loss of epoch-7 batch-141 = 0.007512949872761965

Training epoch-7 batch-142
Running loss of epoch-7 batch-142 = 0.005681794136762619

Training epoch-7 batch-143
Running loss of epoch-7 batch-143 = 0.008311531506478786

Training epoch-7 batch-144
Running loss of epoch-7 batch-144 = 0.007733786012977362

Training epoch-7 batch-145
Running loss of epoch-7 batch-145 = 0.0076774051412940025

Training epoch-7 batch-146
Running loss of epoch-7 batch-146 = 0.009639165364205837

Training epoch-7 batch-147
Running loss of epoch-7 batch-147 = 0.00895407423377037

Training epoch-7 batch-148
Running loss of epoch-7 batch-148 = 0.010017349384725094

Training epoch-7 batch-149
Running loss of epoch-7 batch-149 = 0.008225109428167343

Training epoch-7 batch-150
Running loss of epoch-7 batch-150 = 0.008817089721560478

Training epoch-7 batch-151
Running loss of epoch-7 batch-151 = 0.009142080321907997

Training epoch-7 batch-152
Running loss of epoch-7 batch-152 = 0.010228531435132027

Training epoch-7 batch-153
Running loss of epoch-7 batch-153 = 0.006675172597169876

Training epoch-7 batch-154
Running loss of epoch-7 batch-154 = 0.008642632514238358

Training epoch-7 batch-155
Running loss of epoch-7 batch-155 = 0.007253284100443125

Training epoch-7 batch-156
Running loss of epoch-7 batch-156 = 0.011389720253646374

Training epoch-7 batch-157
Running loss of epoch-7 batch-157 = 0.03910844027996063

Finished training epoch-7.



Average train loss at epoch-7 = 0.00899155445098877

Started Evaluation

Average val loss at epoch-7 = 0.6731588682404867

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 94.75 %
Accuracy for class setUp is: 87.05 %
Accuracy for class onCreate is: 91.79 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 57.53 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 26.01 %
Accuracy for class execute is: 53.82 %
Accuracy for class get is: 69.23 %

Overall Accuracy = 78.91 %

Finished Evaluation



Started training epoch-8


Training epoch-8 batch-1
Running loss of epoch-8 batch-1 = 0.00820300541818142

Training epoch-8 batch-2
Running loss of epoch-8 batch-2 = 0.0079359645023942

Training epoch-8 batch-3
Running loss of epoch-8 batch-3 = 0.009203802794218063

Training epoch-8 batch-4
Running loss of epoch-8 batch-4 = 0.004758005030453205

Training epoch-8 batch-5
Running loss of epoch-8 batch-5 = 0.010134764946997166

Training epoch-8 batch-6
Running loss of epoch-8 batch-6 = 0.007996827363967896

Training epoch-8 batch-7
Running loss of epoch-8 batch-7 = 0.006660766433924437

Training epoch-8 batch-8
Running loss of epoch-8 batch-8 = 0.009504691697657108

Training epoch-8 batch-9
Running loss of epoch-8 batch-9 = 0.007237698417156935

Training epoch-8 batch-10
Running loss of epoch-8 batch-10 = 0.007891476154327393

Training epoch-8 batch-11
Running loss of epoch-8 batch-11 = 0.0068051316775381565

Training epoch-8 batch-12
Running loss of epoch-8 batch-12 = 0.008765769191086292

Training epoch-8 batch-13
Running loss of epoch-8 batch-13 = 0.010078761726617813

Training epoch-8 batch-14
Running loss of epoch-8 batch-14 = 0.004658225923776627

Training epoch-8 batch-15
Running loss of epoch-8 batch-15 = 0.00649943295866251

Training epoch-8 batch-16
Running loss of epoch-8 batch-16 = 0.006490739062428474

Training epoch-8 batch-17
Running loss of epoch-8 batch-17 = 0.006796608213335276

Training epoch-8 batch-18
Running loss of epoch-8 batch-18 = 0.006070525385439396

Training epoch-8 batch-19
Running loss of epoch-8 batch-19 = 0.008256617933511734

Training epoch-8 batch-20
Running loss of epoch-8 batch-20 = 0.006795461755245924

Training epoch-8 batch-21
Running loss of epoch-8 batch-21 = 0.007258178666234016

Training epoch-8 batch-22
Running loss of epoch-8 batch-22 = 0.006934660486876965

Training epoch-8 batch-23
Running loss of epoch-8 batch-23 = 0.005814661271870136

Training epoch-8 batch-24
Running loss of epoch-8 batch-24 = 0.006917778868228197

Training epoch-8 batch-25
Running loss of epoch-8 batch-25 = 0.009977973066270351

Training epoch-8 batch-26
Running loss of epoch-8 batch-26 = 0.00795183889567852

Training epoch-8 batch-27
Running loss of epoch-8 batch-27 = 0.006947270128875971

Training epoch-8 batch-28
Running loss of epoch-8 batch-28 = 0.008875470608472824

Training epoch-8 batch-29
Running loss of epoch-8 batch-29 = 0.00806486513465643

Training epoch-8 batch-30
Running loss of epoch-8 batch-30 = 0.008054912090301514

Training epoch-8 batch-31
Running loss of epoch-8 batch-31 = 0.006701757200062275

Training epoch-8 batch-32
Running loss of epoch-8 batch-32 = 0.009678134694695473

Training epoch-8 batch-33
Running loss of epoch-8 batch-33 = 0.005173917859792709

Training epoch-8 batch-34
Running loss of epoch-8 batch-34 = 0.00639513460919261

Training epoch-8 batch-35
Running loss of epoch-8 batch-35 = 0.01119769737124443

Training epoch-8 batch-36
Running loss of epoch-8 batch-36 = 0.008951310068368912

Training epoch-8 batch-37
Running loss of epoch-8 batch-37 = 0.010321601293981075

Training epoch-8 batch-38
Running loss of epoch-8 batch-38 = 0.004396223463118076

Training epoch-8 batch-39
Running loss of epoch-8 batch-39 = 0.007214328274130821

Training epoch-8 batch-40
Running loss of epoch-8 batch-40 = 0.010617026127874851

Training epoch-8 batch-41
Running loss of epoch-8 batch-41 = 0.007419214583933353

Training epoch-8 batch-42
Running loss of epoch-8 batch-42 = 0.005237453617155552

Training epoch-8 batch-43
Running loss of epoch-8 batch-43 = 0.005527812987565994

Training epoch-8 batch-44
Running loss of epoch-8 batch-44 = 0.007888241671025753

Training epoch-8 batch-45
Running loss of epoch-8 batch-45 = 0.006029794923961163

Training epoch-8 batch-46
Running loss of epoch-8 batch-46 = 0.006123990751802921

Training epoch-8 batch-47
Running loss of epoch-8 batch-47 = 0.006837744265794754

Training epoch-8 batch-48
Running loss of epoch-8 batch-48 = 0.007499808445572853

Training epoch-8 batch-49
Running loss of epoch-8 batch-49 = 0.007629453204572201

Training epoch-8 batch-50
Running loss of epoch-8 batch-50 = 0.007352549582719803

Training epoch-8 batch-51
Running loss of epoch-8 batch-51 = 0.007496114354580641

Training epoch-8 batch-52
Running loss of epoch-8 batch-52 = 0.005492910277098417

Training epoch-8 batch-53
Running loss of epoch-8 batch-53 = 0.012345649302005768

Training epoch-8 batch-54
Running loss of epoch-8 batch-54 = 0.008646081201732159

Training epoch-8 batch-55
Running loss of epoch-8 batch-55 = 0.008639764040708542

Training epoch-8 batch-56
Running loss of epoch-8 batch-56 = 0.005686743650585413

Training epoch-8 batch-57
Running loss of epoch-8 batch-57 = 0.004628370050340891

Training epoch-8 batch-58
Running loss of epoch-8 batch-58 = 0.009049885906279087

Training epoch-8 batch-59
Running loss of epoch-8 batch-59 = 0.0069490522146224976

Training epoch-8 batch-60
Running loss of epoch-8 batch-60 = 0.005560220219194889

Training epoch-8 batch-61
Running loss of epoch-8 batch-61 = 0.009136755019426346

Training epoch-8 batch-62
Running loss of epoch-8 batch-62 = 0.004585056100040674

Training epoch-8 batch-63
Running loss of epoch-8 batch-63 = 0.008131271228194237

Training epoch-8 batch-64
Running loss of epoch-8 batch-64 = 0.008283689618110657

Training epoch-8 batch-65
Running loss of epoch-8 batch-65 = 0.004490113351494074

Training epoch-8 batch-66
Running loss of epoch-8 batch-66 = 0.0093426164239645

Training epoch-8 batch-67
Running loss of epoch-8 batch-67 = 0.005810881964862347

Training epoch-8 batch-68
Running loss of epoch-8 batch-68 = 0.005879224743694067

Training epoch-8 batch-69
Running loss of epoch-8 batch-69 = 0.005556807853281498

Training epoch-8 batch-70
Running loss of epoch-8 batch-70 = 0.006845041643828154

Training epoch-8 batch-71
Running loss of epoch-8 batch-71 = 0.0061582038179039955

Training epoch-8 batch-72
Running loss of epoch-8 batch-72 = 0.007081438787281513

Training epoch-8 batch-73
Running loss of epoch-8 batch-73 = 0.010417930781841278

Training epoch-8 batch-74
Running loss of epoch-8 batch-74 = 0.008741093799471855

Training epoch-8 batch-75
Running loss of epoch-8 batch-75 = 0.006003105081617832

Training epoch-8 batch-76
Running loss of epoch-8 batch-76 = 0.005437746644020081

Training epoch-8 batch-77
Running loss of epoch-8 batch-77 = 0.006131856702268124

Training epoch-8 batch-78
Running loss of epoch-8 batch-78 = 0.008870525285601616

Training epoch-8 batch-79
Running loss of epoch-8 batch-79 = 0.008034630678594112

Training epoch-8 batch-80
Running loss of epoch-8 batch-80 = 0.007518737576901913

Training epoch-8 batch-81
Running loss of epoch-8 batch-81 = 0.00641958974301815

Training epoch-8 batch-82
Running loss of epoch-8 batch-82 = 0.006917271763086319

Training epoch-8 batch-83
Running loss of epoch-8 batch-83 = 0.008657760918140411

Training epoch-8 batch-84
Running loss of epoch-8 batch-84 = 0.00946715660393238

Training epoch-8 batch-85
Running loss of epoch-8 batch-85 = 0.00791187584400177

Training epoch-8 batch-86
Running loss of epoch-8 batch-86 = 0.007267565932124853

Training epoch-8 batch-87
Running loss of epoch-8 batch-87 = 0.007710928097367287

Training epoch-8 batch-88
Running loss of epoch-8 batch-88 = 0.007714629638940096

Training epoch-8 batch-89
Running loss of epoch-8 batch-89 = 0.008936022408306599

Training epoch-8 batch-90
Running loss of epoch-8 batch-90 = 0.011476509273052216

Training epoch-8 batch-91
Running loss of epoch-8 batch-91 = 0.006808594800531864

Training epoch-8 batch-92
Running loss of epoch-8 batch-92 = 0.007224528584629297

Training epoch-8 batch-93
Running loss of epoch-8 batch-93 = 0.0055387276224792

Training epoch-8 batch-94
Running loss of epoch-8 batch-94 = 0.008297493681311607

Training epoch-8 batch-95
Running loss of epoch-8 batch-95 = 0.008775266818702221

Training epoch-8 batch-96
Running loss of epoch-8 batch-96 = 0.008268273435533047

Training epoch-8 batch-97
Running loss of epoch-8 batch-97 = 0.007182395085692406

Training epoch-8 batch-98
Running loss of epoch-8 batch-98 = 0.008738852106034756

Training epoch-8 batch-99
Running loss of epoch-8 batch-99 = 0.009958595968782902

Training epoch-8 batch-100
Running loss of epoch-8 batch-100 = 0.007318757474422455

Training epoch-8 batch-101
Running loss of epoch-8 batch-101 = 0.009523607790470123

Training epoch-8 batch-102
Running loss of epoch-8 batch-102 = 0.007349236402660608

Training epoch-8 batch-103
Running loss of epoch-8 batch-103 = 0.004452287219464779

Training epoch-8 batch-104
Running loss of epoch-8 batch-104 = 0.005495571997016668

Training epoch-8 batch-105
Running loss of epoch-8 batch-105 = 0.0076976679265499115

Training epoch-8 batch-106
Running loss of epoch-8 batch-106 = 0.011103876866400242

Training epoch-8 batch-107
Running loss of epoch-8 batch-107 = 0.006461167708039284

Training epoch-8 batch-108
Running loss of epoch-8 batch-108 = 0.0074615455232560635

Training epoch-8 batch-109
Running loss of epoch-8 batch-109 = 0.007058747578412294

Training epoch-8 batch-110
Running loss of epoch-8 batch-110 = 0.007396265398710966

Training epoch-8 batch-111
Running loss of epoch-8 batch-111 = 0.006438008975237608

Training epoch-8 batch-112
Running loss of epoch-8 batch-112 = 0.0055879633873701096

Training epoch-8 batch-113
Running loss of epoch-8 batch-113 = 0.005147448740899563

Training epoch-8 batch-114
Running loss of epoch-8 batch-114 = 0.005471224896609783

Training epoch-8 batch-115
Running loss of epoch-8 batch-115 = 0.006000262685120106

Training epoch-8 batch-116
Running loss of epoch-8 batch-116 = 0.008327925577759743

Training epoch-8 batch-117
Running loss of epoch-8 batch-117 = 0.006176643073558807

Training epoch-8 batch-118
Running loss of epoch-8 batch-118 = 0.0061506046913564205

Training epoch-8 batch-119
Running loss of epoch-8 batch-119 = 0.009990433230996132

Training epoch-8 batch-120
Running loss of epoch-8 batch-120 = 0.01134320255368948

Training epoch-8 batch-121
Running loss of epoch-8 batch-121 = 0.008469794876873493

Training epoch-8 batch-122
Running loss of epoch-8 batch-122 = 0.00814122799783945

Training epoch-8 batch-123
Running loss of epoch-8 batch-123 = 0.007760791108012199

Training epoch-8 batch-124
Running loss of epoch-8 batch-124 = 0.006421157158911228

Training epoch-8 batch-125
Running loss of epoch-8 batch-125 = 0.0053441813215613365

Training epoch-8 batch-126
Running loss of epoch-8 batch-126 = 0.008621353656053543

Training epoch-8 batch-127
Running loss of epoch-8 batch-127 = 0.005658661015331745

Training epoch-8 batch-128
Running loss of epoch-8 batch-128 = 0.008645052090287209

Training epoch-8 batch-129
Running loss of epoch-8 batch-129 = 0.008188096806406975

Training epoch-8 batch-130
Running loss of epoch-8 batch-130 = 0.00381915969774127

Training epoch-8 batch-131
Running loss of epoch-8 batch-131 = 0.007802898995578289

Training epoch-8 batch-132
Running loss of epoch-8 batch-132 = 0.005416701082140207

Training epoch-8 batch-133
Running loss of epoch-8 batch-133 = 0.007922749035060406

Training epoch-8 batch-134
Running loss of epoch-8 batch-134 = 0.008630934171378613

Training epoch-8 batch-135
Running loss of epoch-8 batch-135 = 0.006058195605874062

Training epoch-8 batch-136
Running loss of epoch-8 batch-136 = 0.008925463072955608

Training epoch-8 batch-137
Running loss of epoch-8 batch-137 = 0.008135529235005379

Training epoch-8 batch-138
Running loss of epoch-8 batch-138 = 0.00835486501455307

Training epoch-8 batch-139
Running loss of epoch-8 batch-139 = 0.008264019154012203

Training epoch-8 batch-140
Running loss of epoch-8 batch-140 = 0.006631904281675816

Training epoch-8 batch-141
Running loss of epoch-8 batch-141 = 0.007212305907160044

Training epoch-8 batch-142
Running loss of epoch-8 batch-142 = 0.007352998945862055

Training epoch-8 batch-143
Running loss of epoch-8 batch-143 = 0.010802973061800003

Training epoch-8 batch-144
Running loss of epoch-8 batch-144 = 0.00907091610133648

Training epoch-8 batch-145
Running loss of epoch-8 batch-145 = 0.006734547205269337

Training epoch-8 batch-146
Running loss of epoch-8 batch-146 = 0.010140796191990376

Training epoch-8 batch-147
Running loss of epoch-8 batch-147 = 0.008375938981771469

Training epoch-8 batch-148
Running loss of epoch-8 batch-148 = 0.008705124258995056

Training epoch-8 batch-149
Running loss of epoch-8 batch-149 = 0.011565884575247765

Training epoch-8 batch-150
Running loss of epoch-8 batch-150 = 0.010112999007105827

Training epoch-8 batch-151
Running loss of epoch-8 batch-151 = 0.005043438635766506

Training epoch-8 batch-152
Running loss of epoch-8 batch-152 = 0.0063439346849918365

Training epoch-8 batch-153
Running loss of epoch-8 batch-153 = 0.010126419365406036

Training epoch-8 batch-154
Running loss of epoch-8 batch-154 = 0.009471733123064041

Training epoch-8 batch-155
Running loss of epoch-8 batch-155 = 0.007047191262245178

Training epoch-8 batch-156
Running loss of epoch-8 batch-156 = 0.007077299524098635

Training epoch-8 batch-157
Running loss of epoch-8 batch-157 = 0.02629515901207924

Finished training epoch-8.



Average train loss at epoch-8 = 0.007596045523881912

Started Evaluation

Average val loss at epoch-8 = 0.6353305675653055

Accuracy for classes:
Accuracy for class equals is: 94.72 %
Accuracy for class main is: 96.56 %
Accuracy for class setUp is: 94.10 %
Accuracy for class onCreate is: 83.48 %
Accuracy for class toString is: 78.16 %
Accuracy for class run is: 49.32 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 70.85 %
Accuracy for class execute is: 11.65 %
Accuracy for class get is: 82.31 %

Overall Accuracy = 80.34 %

Finished Evaluation



Started training epoch-9


Training epoch-9 batch-1
Running loss of epoch-9 batch-1 = 0.009269529953598976

Training epoch-9 batch-2
Running loss of epoch-9 batch-2 = 0.008098823018372059

Training epoch-9 batch-3
Running loss of epoch-9 batch-3 = 0.007963128387928009

Training epoch-9 batch-4
Running loss of epoch-9 batch-4 = 0.006754678208380938

Training epoch-9 batch-5
Running loss of epoch-9 batch-5 = 0.007331738248467445

Training epoch-9 batch-6
Running loss of epoch-9 batch-6 = 0.007860048674046993

Training epoch-9 batch-7
Running loss of epoch-9 batch-7 = 0.007773061282932758

Training epoch-9 batch-8
Running loss of epoch-9 batch-8 = 0.0061915842816233635

Training epoch-9 batch-9
Running loss of epoch-9 batch-9 = 0.008009620010852814

Training epoch-9 batch-10
Running loss of epoch-9 batch-10 = 0.004674900323152542

Training epoch-9 batch-11
Running loss of epoch-9 batch-11 = 0.007065596058964729

Training epoch-9 batch-12
Running loss of epoch-9 batch-12 = 0.007561765145510435

Training epoch-9 batch-13
Running loss of epoch-9 batch-13 = 0.0050879232585430145

Training epoch-9 batch-14
Running loss of epoch-9 batch-14 = 0.009247653186321259

Training epoch-9 batch-15
Running loss of epoch-9 batch-15 = 0.00697661004960537

Training epoch-9 batch-16
Running loss of epoch-9 batch-16 = 0.009660596027970314

Training epoch-9 batch-17
Running loss of epoch-9 batch-17 = 0.015144486911594868

Training epoch-9 batch-18
Running loss of epoch-9 batch-18 = 0.008700010366737843

Training epoch-9 batch-19
Running loss of epoch-9 batch-19 = 0.005869139917194843

Training epoch-9 batch-20
Running loss of epoch-9 batch-20 = 0.007618208881467581

Training epoch-9 batch-21
Running loss of epoch-9 batch-21 = 0.005196521058678627

Training epoch-9 batch-22
Running loss of epoch-9 batch-22 = 0.0071328370831906796

Training epoch-9 batch-23
Running loss of epoch-9 batch-23 = 0.008231660351157188

Training epoch-9 batch-24
Running loss of epoch-9 batch-24 = 0.007202606648206711

Training epoch-9 batch-25
Running loss of epoch-9 batch-25 = 0.008536464534699917

Training epoch-9 batch-26
Running loss of epoch-9 batch-26 = 0.005492112599313259

Training epoch-9 batch-27
Running loss of epoch-9 batch-27 = 0.0055294218473136425

Training epoch-9 batch-28
Running loss of epoch-9 batch-28 = 0.006059463135898113

Training epoch-9 batch-29
Running loss of epoch-9 batch-29 = 0.006563263479620218

Training epoch-9 batch-30
Running loss of epoch-9 batch-30 = 0.007132617756724358

Training epoch-9 batch-31
Running loss of epoch-9 batch-31 = 0.007663932628929615

Training epoch-9 batch-32
Running loss of epoch-9 batch-32 = 0.00642766896635294

Training epoch-9 batch-33
Running loss of epoch-9 batch-33 = 0.005278837867081165

Training epoch-9 batch-34
Running loss of epoch-9 batch-34 = 0.006468039005994797

Training epoch-9 batch-35
Running loss of epoch-9 batch-35 = 0.0062928213737905025

Training epoch-9 batch-36
Running loss of epoch-9 batch-36 = 0.007833157666027546

Training epoch-9 batch-37
Running loss of epoch-9 batch-37 = 0.0050832610577344894

Training epoch-9 batch-38
Running loss of epoch-9 batch-38 = 0.0060378569178283215

Training epoch-9 batch-39
Running loss of epoch-9 batch-39 = 0.006356795318424702

Training epoch-9 batch-40
Running loss of epoch-9 batch-40 = 0.0051077851094305515

Training epoch-9 batch-41
Running loss of epoch-9 batch-41 = 0.008938498795032501

Training epoch-9 batch-42
Running loss of epoch-9 batch-42 = 0.004744972568005323

Training epoch-9 batch-43
Running loss of epoch-9 batch-43 = 0.007406400516629219

Training epoch-9 batch-44
Running loss of epoch-9 batch-44 = 0.006483275443315506

Training epoch-9 batch-45
Running loss of epoch-9 batch-45 = 0.0060294936411082745

Training epoch-9 batch-46
Running loss of epoch-9 batch-46 = 0.009096953086555004

Training epoch-9 batch-47
Running loss of epoch-9 batch-47 = 0.0061645288951694965

Training epoch-9 batch-48
Running loss of epoch-9 batch-48 = 0.006822295021265745

Training epoch-9 batch-49
Running loss of epoch-9 batch-49 = 0.009882570244371891

Training epoch-9 batch-50
Running loss of epoch-9 batch-50 = 0.007054808549582958

Training epoch-9 batch-51
Running loss of epoch-9 batch-51 = 0.009689349681138992

Training epoch-9 batch-52
Running loss of epoch-9 batch-52 = 0.008140462450683117

Training epoch-9 batch-53
Running loss of epoch-9 batch-53 = 0.007014790549874306

Training epoch-9 batch-54
Running loss of epoch-9 batch-54 = 0.006921231746673584

Training epoch-9 batch-55
Running loss of epoch-9 batch-55 = 0.011335635557770729

Training epoch-9 batch-56
Running loss of epoch-9 batch-56 = 0.006155873648822308

Training epoch-9 batch-57
Running loss of epoch-9 batch-57 = 0.009308289736509323

Training epoch-9 batch-58
Running loss of epoch-9 batch-58 = 0.006089864764362574

Training epoch-9 batch-59
Running loss of epoch-9 batch-59 = 0.012539073824882507

Training epoch-9 batch-60
Running loss of epoch-9 batch-60 = 0.005670161917805672

Training epoch-9 batch-61
Running loss of epoch-9 batch-61 = 0.008349651470780373

Training epoch-9 batch-62
Running loss of epoch-9 batch-62 = 0.004773559980094433

Training epoch-9 batch-63
Running loss of epoch-9 batch-63 = 0.004133074544370174

Training epoch-9 batch-64
Running loss of epoch-9 batch-64 = 0.00813025888055563

Training epoch-9 batch-65
Running loss of epoch-9 batch-65 = 0.006139051169157028

Training epoch-9 batch-66
Running loss of epoch-9 batch-66 = 0.005921454168856144

Training epoch-9 batch-67
Running loss of epoch-9 batch-67 = 0.006902304943650961

Training epoch-9 batch-68
Running loss of epoch-9 batch-68 = 0.005167046561837196

Training epoch-9 batch-69
Running loss of epoch-9 batch-69 = 0.006519159767776728

Training epoch-9 batch-70
Running loss of epoch-9 batch-70 = 0.0029686850029975176

Training epoch-9 batch-71
Running loss of epoch-9 batch-71 = 0.007061926182359457

Training epoch-9 batch-72
Running loss of epoch-9 batch-72 = 0.005907938815653324

Training epoch-9 batch-73
Running loss of epoch-9 batch-73 = 0.006423879414796829

Training epoch-9 batch-74
Running loss of epoch-9 batch-74 = 0.004724147729575634

Training epoch-9 batch-75
Running loss of epoch-9 batch-75 = 0.006247791461646557

Training epoch-9 batch-76
Running loss of epoch-9 batch-76 = 0.00829436257481575

Training epoch-9 batch-77
Running loss of epoch-9 batch-77 = 0.006207739934325218

Training epoch-9 batch-78
Running loss of epoch-9 batch-78 = 0.009377218782901764

Training epoch-9 batch-79
Running loss of epoch-9 batch-79 = 0.005466470029205084

Training epoch-9 batch-80
Running loss of epoch-9 batch-80 = 0.004543503746390343

Training epoch-9 batch-81
Running loss of epoch-9 batch-81 = 0.00740989251062274

Training epoch-9 batch-82
Running loss of epoch-9 batch-82 = 0.006248668767511845

Training epoch-9 batch-83
Running loss of epoch-9 batch-83 = 0.0034601138904690742

Training epoch-9 batch-84
Running loss of epoch-9 batch-84 = 0.007208684459328651

Training epoch-9 batch-85
Running loss of epoch-9 batch-85 = 0.009631532244384289

Training epoch-9 batch-86
Running loss of epoch-9 batch-86 = 0.008514738641679287

Training epoch-9 batch-87
Running loss of epoch-9 batch-87 = 0.003441205248236656

Training epoch-9 batch-88
Running loss of epoch-9 batch-88 = 0.005983286537230015

Training epoch-9 batch-89
Running loss of epoch-9 batch-89 = 0.0049340445548295975

Training epoch-9 batch-90
Running loss of epoch-9 batch-90 = 0.007392326835542917

Training epoch-9 batch-91
Running loss of epoch-9 batch-91 = 0.005726460367441177

Training epoch-9 batch-92
Running loss of epoch-9 batch-92 = 0.00826309248805046

Training epoch-9 batch-93
Running loss of epoch-9 batch-93 = 0.007057689595967531

Training epoch-9 batch-94
Running loss of epoch-9 batch-94 = 0.004360855557024479

Training epoch-9 batch-95
Running loss of epoch-9 batch-95 = 0.00945940613746643

Training epoch-9 batch-96
Running loss of epoch-9 batch-96 = 0.00910201296210289

Training epoch-9 batch-97
Running loss of epoch-9 batch-97 = 0.007562668062746525

Training epoch-9 batch-98
Running loss of epoch-9 batch-98 = 0.008107438683509827

Training epoch-9 batch-99
Running loss of epoch-9 batch-99 = 0.007222811691462994

Training epoch-9 batch-100
Running loss of epoch-9 batch-100 = 0.006084693595767021

Training epoch-9 batch-101
Running loss of epoch-9 batch-101 = 0.007209135685116053

Training epoch-9 batch-102
Running loss of epoch-9 batch-102 = 0.011029021814465523

Training epoch-9 batch-103
Running loss of epoch-9 batch-103 = 0.006085607223212719

Training epoch-9 batch-104
Running loss of epoch-9 batch-104 = 0.007422943599522114

Training epoch-9 batch-105
Running loss of epoch-9 batch-105 = 0.005486760754138231

Training epoch-9 batch-106
Running loss of epoch-9 batch-106 = 0.010911288671195507

Training epoch-9 batch-107
Running loss of epoch-9 batch-107 = 0.007277436088770628

Training epoch-9 batch-108
Running loss of epoch-9 batch-108 = 0.00555384298786521

Training epoch-9 batch-109
Running loss of epoch-9 batch-109 = 0.0057052914053201675

Training epoch-9 batch-110
Running loss of epoch-9 batch-110 = 0.004557941574603319

Training epoch-9 batch-111
Running loss of epoch-9 batch-111 = 0.005500581115484238

Training epoch-9 batch-112
Running loss of epoch-9 batch-112 = 0.007458990439772606

Training epoch-9 batch-113
Running loss of epoch-9 batch-113 = 0.007297133095562458

Training epoch-9 batch-114
Running loss of epoch-9 batch-114 = 0.0092718955129385

Training epoch-9 batch-115
Running loss of epoch-9 batch-115 = 0.008724969811737537

Training epoch-9 batch-116
Running loss of epoch-9 batch-116 = 0.008099670521914959

Training epoch-9 batch-117
Running loss of epoch-9 batch-117 = 0.006344478111714125

Training epoch-9 batch-118
Running loss of epoch-9 batch-118 = 0.005904211662709713

Training epoch-9 batch-119
Running loss of epoch-9 batch-119 = 0.007868480868637562

Training epoch-9 batch-120
Running loss of epoch-9 batch-120 = 0.006605970673263073

Training epoch-9 batch-121
Running loss of epoch-9 batch-121 = 0.005650761537253857

Training epoch-9 batch-122
Running loss of epoch-9 batch-122 = 0.005823736544698477

Training epoch-9 batch-123
Running loss of epoch-9 batch-123 = 0.009242339059710503

Training epoch-9 batch-124
Running loss of epoch-9 batch-124 = 0.006487921811640263

Training epoch-9 batch-125
Running loss of epoch-9 batch-125 = 0.007234899327158928

Training epoch-9 batch-126
Running loss of epoch-9 batch-126 = 0.004970113281160593

Training epoch-9 batch-127
Running loss of epoch-9 batch-127 = 0.007246476598083973

Training epoch-9 batch-128
Running loss of epoch-9 batch-128 = 0.004245697055011988

Training epoch-9 batch-129
Running loss of epoch-9 batch-129 = 0.005883324425667524

Training epoch-9 batch-130
Running loss of epoch-9 batch-130 = 0.00571389589458704

Training epoch-9 batch-131
Running loss of epoch-9 batch-131 = 0.005558310076594353

Training epoch-9 batch-132
Running loss of epoch-9 batch-132 = 0.005982832983136177

Training epoch-9 batch-133
Running loss of epoch-9 batch-133 = 0.01129886694252491

Training epoch-9 batch-134
Running loss of epoch-9 batch-134 = 0.005691646132618189

Training epoch-9 batch-135
Running loss of epoch-9 batch-135 = 0.00658024288713932

Training epoch-9 batch-136
Running loss of epoch-9 batch-136 = 0.007107841782271862

Training epoch-9 batch-137
Running loss of epoch-9 batch-137 = 0.0048561966978013515

Training epoch-9 batch-138
Running loss of epoch-9 batch-138 = 0.007090738974511623

Training epoch-9 batch-139
Running loss of epoch-9 batch-139 = 0.007084512151777744

Training epoch-9 batch-140
Running loss of epoch-9 batch-140 = 0.006745138671249151

Training epoch-9 batch-141
Running loss of epoch-9 batch-141 = 0.0027334189508110285

Training epoch-9 batch-142
Running loss of epoch-9 batch-142 = 0.006199859082698822

Training epoch-9 batch-143
Running loss of epoch-9 batch-143 = 0.006423092447221279

Training epoch-9 batch-144
Running loss of epoch-9 batch-144 = 0.009156636893749237

Training epoch-9 batch-145
Running loss of epoch-9 batch-145 = 0.0058910478837788105

Training epoch-9 batch-146
Running loss of epoch-9 batch-146 = 0.006077690981328487

Training epoch-9 batch-147
Running loss of epoch-9 batch-147 = 0.00780846830457449

Training epoch-9 batch-148
Running loss of epoch-9 batch-148 = 0.0048170872032642365

Training epoch-9 batch-149
Running loss of epoch-9 batch-149 = 0.007425286807119846

Training epoch-9 batch-150
Running loss of epoch-9 batch-150 = 0.006441418081521988

Training epoch-9 batch-151
Running loss of epoch-9 batch-151 = 0.005081706680357456

Training epoch-9 batch-152
Running loss of epoch-9 batch-152 = 0.010129577480256557

Training epoch-9 batch-153
Running loss of epoch-9 batch-153 = 0.006072882562875748

Training epoch-9 batch-154
Running loss of epoch-9 batch-154 = 0.007359611801803112

Training epoch-9 batch-155
Running loss of epoch-9 batch-155 = 0.0038381326012313366

Training epoch-9 batch-156
Running loss of epoch-9 batch-156 = 0.007787596900016069

Training epoch-9 batch-157
Running loss of epoch-9 batch-157 = 0.022979339584708214

Finished training epoch-9.



Average train loss at epoch-9 = 0.006951712635159492

Started Evaluation

Average val loss at epoch-9 = 0.6031674056889882

Accuracy for classes:
Accuracy for class equals is: 95.38 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 92.79 %
Accuracy for class onCreate is: 86.99 %
Accuracy for class toString is: 84.64 %
Accuracy for class run is: 69.63 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 43.50 %
Accuracy for class execute is: 31.73 %
Accuracy for class get is: 73.08 %

Overall Accuracy = 80.92 %


Best Accuracy = 80.92 % at Epoch-9
Saving model after best epoch-9

Finished Evaluation



Started training epoch-10


Training epoch-10 batch-1
Running loss of epoch-10 batch-1 = 0.004924119915813208

Training epoch-10 batch-2
Running loss of epoch-10 batch-2 = 0.007353579625487328

Training epoch-10 batch-3
Running loss of epoch-10 batch-3 = 0.00604200316593051

Training epoch-10 batch-4
Running loss of epoch-10 batch-4 = 0.003903904464095831

Training epoch-10 batch-5
Running loss of epoch-10 batch-5 = 0.010815274901688099

Training epoch-10 batch-6
Running loss of epoch-10 batch-6 = 0.005921750795096159

Training epoch-10 batch-7
Running loss of epoch-10 batch-7 = 0.005836125463247299

Training epoch-10 batch-8
Running loss of epoch-10 batch-8 = 0.008036399260163307

Training epoch-10 batch-9
Running loss of epoch-10 batch-9 = 0.00502394326031208

Training epoch-10 batch-10
Running loss of epoch-10 batch-10 = 0.004038757178932428

Training epoch-10 batch-11
Running loss of epoch-10 batch-11 = 0.0042240554466843605

Training epoch-10 batch-12
Running loss of epoch-10 batch-12 = 0.005521143786609173

Training epoch-10 batch-13
Running loss of epoch-10 batch-13 = 0.004309371579438448

Training epoch-10 batch-14
Running loss of epoch-10 batch-14 = 0.006926029920578003

Training epoch-10 batch-15
Running loss of epoch-10 batch-15 = 0.009096872061491013

Training epoch-10 batch-16
Running loss of epoch-10 batch-16 = 0.005550529342144728

Training epoch-10 batch-17
Running loss of epoch-10 batch-17 = 0.005602374207228422

Training epoch-10 batch-18
Running loss of epoch-10 batch-18 = 0.0051171183586120605

Training epoch-10 batch-19
Running loss of epoch-10 batch-19 = 0.0024210656993091106

Training epoch-10 batch-20
Running loss of epoch-10 batch-20 = 0.008028090000152588

Training epoch-10 batch-21
Running loss of epoch-10 batch-21 = 0.007907501421868801

Training epoch-10 batch-22
Running loss of epoch-10 batch-22 = 0.004448870196938515

Training epoch-10 batch-23
Running loss of epoch-10 batch-23 = 0.0053371861577034

Training epoch-10 batch-24
Running loss of epoch-10 batch-24 = 0.006723099388182163

Training epoch-10 batch-25
Running loss of epoch-10 batch-25 = 0.006428511813282967

Training epoch-10 batch-26
Running loss of epoch-10 batch-26 = 0.006147896405309439

Training epoch-10 batch-27
Running loss of epoch-10 batch-27 = 0.006556909065693617

Training epoch-10 batch-28
Running loss of epoch-10 batch-28 = 0.004427777603268623

Training epoch-10 batch-29
Running loss of epoch-10 batch-29 = 0.004049614537507296

Training epoch-10 batch-30
Running loss of epoch-10 batch-30 = 0.00519048934802413

Training epoch-10 batch-31
Running loss of epoch-10 batch-31 = 0.00596277229487896

Training epoch-10 batch-32
Running loss of epoch-10 batch-32 = 0.004497489891946316

Training epoch-10 batch-33
Running loss of epoch-10 batch-33 = 0.006129234563559294

Training epoch-10 batch-34
Running loss of epoch-10 batch-34 = 0.008612152189016342

Training epoch-10 batch-35
Running loss of epoch-10 batch-35 = 0.006801229901611805

Training epoch-10 batch-36
Running loss of epoch-10 batch-36 = 0.00508907763287425

Training epoch-10 batch-37
Running loss of epoch-10 batch-37 = 0.00800273846834898

Training epoch-10 batch-38
Running loss of epoch-10 batch-38 = 0.005279566161334515

Training epoch-10 batch-39
Running loss of epoch-10 batch-39 = 0.008344998583197594

Training epoch-10 batch-40
Running loss of epoch-10 batch-40 = 0.008617747575044632

Training epoch-10 batch-41
Running loss of epoch-10 batch-41 = 0.00861265417188406

Training epoch-10 batch-42
Running loss of epoch-10 batch-42 = 0.005578445270657539

Training epoch-10 batch-43
Running loss of epoch-10 batch-43 = 0.003648078069090843

Training epoch-10 batch-44
Running loss of epoch-10 batch-44 = 0.0052192434668540955

Training epoch-10 batch-45
Running loss of epoch-10 batch-45 = 0.005083293188363314

Training epoch-10 batch-46
Running loss of epoch-10 batch-46 = 0.005625438876450062

Training epoch-10 batch-47
Running loss of epoch-10 batch-47 = 0.007847269997000694

Training epoch-10 batch-48
Running loss of epoch-10 batch-48 = 0.008226809091866016

Training epoch-10 batch-49
Running loss of epoch-10 batch-49 = 0.0060288975946605206

Training epoch-10 batch-50
Running loss of epoch-10 batch-50 = 0.005072304978966713

Training epoch-10 batch-51
Running loss of epoch-10 batch-51 = 0.005928435828536749

Training epoch-10 batch-52
Running loss of epoch-10 batch-52 = 0.004486768040806055

Training epoch-10 batch-53
Running loss of epoch-10 batch-53 = 0.006947861984372139

Training epoch-10 batch-54
Running loss of epoch-10 batch-54 = 0.005747430957853794

Training epoch-10 batch-55
Running loss of epoch-10 batch-55 = 0.005357023328542709

Training epoch-10 batch-56
Running loss of epoch-10 batch-56 = 0.0038191552739590406

Training epoch-10 batch-57
Running loss of epoch-10 batch-57 = 0.0061295656487345695

Training epoch-10 batch-58
Running loss of epoch-10 batch-58 = 0.007198166102170944

Training epoch-10 batch-59
Running loss of epoch-10 batch-59 = 0.005719225853681564

Training epoch-10 batch-60
Running loss of epoch-10 batch-60 = 0.005121645517647266

Training epoch-10 batch-61
Running loss of epoch-10 batch-61 = 0.008073556236922741

Training epoch-10 batch-62
Running loss of epoch-10 batch-62 = 0.006708583794534206

Training epoch-10 batch-63
Running loss of epoch-10 batch-63 = 0.006783712655305862

Training epoch-10 batch-64
Running loss of epoch-10 batch-64 = 0.008278222754597664

Training epoch-10 batch-65
Running loss of epoch-10 batch-65 = 0.005848410539329052

Training epoch-10 batch-66
Running loss of epoch-10 batch-66 = 0.0035492919851094484

Training epoch-10 batch-67
Running loss of epoch-10 batch-67 = 0.005494943354278803

Training epoch-10 batch-68
Running loss of epoch-10 batch-68 = 0.0064212242141366005

Training epoch-10 batch-69
Running loss of epoch-10 batch-69 = 0.005574547685682774

Training epoch-10 batch-70
Running loss of epoch-10 batch-70 = 0.004457153845578432

Training epoch-10 batch-71
Running loss of epoch-10 batch-71 = 0.006459198892116547

Training epoch-10 batch-72
Running loss of epoch-10 batch-72 = 0.006931142881512642

Training epoch-10 batch-73
Running loss of epoch-10 batch-73 = 0.006827213801443577

Training epoch-10 batch-74
Running loss of epoch-10 batch-74 = 0.00907138455659151

Training epoch-10 batch-75
Running loss of epoch-10 batch-75 = 0.0038399421609938145

Training epoch-10 batch-76
Running loss of epoch-10 batch-76 = 0.007090344093739986

Training epoch-10 batch-77
Running loss of epoch-10 batch-77 = 0.007425130344927311

Training epoch-10 batch-78
Running loss of epoch-10 batch-78 = 0.005448100157082081

Training epoch-10 batch-79
Running loss of epoch-10 batch-79 = 0.006066977046430111

Training epoch-10 batch-80
Running loss of epoch-10 batch-80 = 0.005632984451949596

Training epoch-10 batch-81
Running loss of epoch-10 batch-81 = 0.005707493983209133

Training epoch-10 batch-82
Running loss of epoch-10 batch-82 = 0.008308075368404388

Training epoch-10 batch-83
Running loss of epoch-10 batch-83 = 0.004474921151995659

Training epoch-10 batch-84
Running loss of epoch-10 batch-84 = 0.00496358098462224

Training epoch-10 batch-85
Running loss of epoch-10 batch-85 = 0.004466045182198286

Training epoch-10 batch-86
Running loss of epoch-10 batch-86 = 0.0080605773255229

Training epoch-10 batch-87
Running loss of epoch-10 batch-87 = 0.004306312184780836

Training epoch-10 batch-88
Running loss of epoch-10 batch-88 = 0.004703612998127937

Training epoch-10 batch-89
Running loss of epoch-10 batch-89 = 0.005737402010709047

Training epoch-10 batch-90
Running loss of epoch-10 batch-90 = 0.007361073978245258

Training epoch-10 batch-91
Running loss of epoch-10 batch-91 = 0.008153925649821758

Training epoch-10 batch-92
Running loss of epoch-10 batch-92 = 0.0067078168503940105

Training epoch-10 batch-93
Running loss of epoch-10 batch-93 = 0.0035232449881732464

Training epoch-10 batch-94
Running loss of epoch-10 batch-94 = 0.006878837943077087

Training epoch-10 batch-95
Running loss of epoch-10 batch-95 = 0.004030226264148951

Training epoch-10 batch-96
Running loss of epoch-10 batch-96 = 0.004289614036679268

Training epoch-10 batch-97
Running loss of epoch-10 batch-97 = 0.003603293327614665

Training epoch-10 batch-98
Running loss of epoch-10 batch-98 = 0.007689025718718767

Training epoch-10 batch-99
Running loss of epoch-10 batch-99 = 0.006908872630447149

Training epoch-10 batch-100
Running loss of epoch-10 batch-100 = 0.00566037418320775

Training epoch-10 batch-101
Running loss of epoch-10 batch-101 = 0.0038756246212869883

Training epoch-10 batch-102
Running loss of epoch-10 batch-102 = 0.0045163799077272415

Training epoch-10 batch-103
Running loss of epoch-10 batch-103 = 0.004929746501147747

Training epoch-10 batch-104
Running loss of epoch-10 batch-104 = 0.006993548944592476

Training epoch-10 batch-105
Running loss of epoch-10 batch-105 = 0.005826345644891262

Training epoch-10 batch-106
Running loss of epoch-10 batch-106 = 0.006769901141524315

Training epoch-10 batch-107
Running loss of epoch-10 batch-107 = 0.005888638086616993

Training epoch-10 batch-108
Running loss of epoch-10 batch-108 = 0.0055388957262039185

Training epoch-10 batch-109
Running loss of epoch-10 batch-109 = 0.0063451239839196205

Training epoch-10 batch-110
Running loss of epoch-10 batch-110 = 0.0068599143996834755

Training epoch-10 batch-111
Running loss of epoch-10 batch-111 = 0.00594249228015542

Training epoch-10 batch-112
Running loss of epoch-10 batch-112 = 0.004521378315985203

Training epoch-10 batch-113
Running loss of epoch-10 batch-113 = 0.005127467215061188

Training epoch-10 batch-114
Running loss of epoch-10 batch-114 = 0.00939269084483385

Training epoch-10 batch-115
Running loss of epoch-10 batch-115 = 0.007335254456847906

Training epoch-10 batch-116
Running loss of epoch-10 batch-116 = 0.0033170604147017

Training epoch-10 batch-117
Running loss of epoch-10 batch-117 = 0.005549783818423748

Training epoch-10 batch-118
Running loss of epoch-10 batch-118 = 0.00786215253174305

Training epoch-10 batch-119
Running loss of epoch-10 batch-119 = 0.0070543838664889336

Training epoch-10 batch-120
Running loss of epoch-10 batch-120 = 0.003652765415608883

Training epoch-10 batch-121
Running loss of epoch-10 batch-121 = 0.0057581085711717606

Training epoch-10 batch-122
Running loss of epoch-10 batch-122 = 0.003418315201997757

Training epoch-10 batch-123
Running loss of epoch-10 batch-123 = 0.008436399511992931

Training epoch-10 batch-124
Running loss of epoch-10 batch-124 = 0.006536800879985094

Training epoch-10 batch-125
Running loss of epoch-10 batch-125 = 0.004270527511835098

Training epoch-10 batch-126
Running loss of epoch-10 batch-126 = 0.005833860486745834

Training epoch-10 batch-127
Running loss of epoch-10 batch-127 = 0.005232411436736584

Training epoch-10 batch-128
Running loss of epoch-10 batch-128 = 0.007126488722860813

Training epoch-10 batch-129
Running loss of epoch-10 batch-129 = 0.004427161067724228

Training epoch-10 batch-130
Running loss of epoch-10 batch-130 = 0.005797993391752243

Training epoch-10 batch-131
Running loss of epoch-10 batch-131 = 0.0077340006828308105

Training epoch-10 batch-132
Running loss of epoch-10 batch-132 = 0.005714305676519871

Training epoch-10 batch-133
Running loss of epoch-10 batch-133 = 0.007052587810903788

Training epoch-10 batch-134
Running loss of epoch-10 batch-134 = 0.006704740226268768

Training epoch-10 batch-135
Running loss of epoch-10 batch-135 = 0.004810602869838476

Training epoch-10 batch-136
Running loss of epoch-10 batch-136 = 0.005389790050685406

Training epoch-10 batch-137
Running loss of epoch-10 batch-137 = 0.006029568612575531

Training epoch-10 batch-138
Running loss of epoch-10 batch-138 = 0.0041800434701144695

Training epoch-10 batch-139
Running loss of epoch-10 batch-139 = 0.005026770755648613

Training epoch-10 batch-140
Running loss of epoch-10 batch-140 = 0.006380018312484026

Training epoch-10 batch-141
Running loss of epoch-10 batch-141 = 0.006919698789715767

Training epoch-10 batch-142
Running loss of epoch-10 batch-142 = 0.005588304251432419

Training epoch-10 batch-143
Running loss of epoch-10 batch-143 = 0.007467214018106461

Training epoch-10 batch-144
Running loss of epoch-10 batch-144 = 0.0050761085003614426

Training epoch-10 batch-145
Running loss of epoch-10 batch-145 = 0.00543087488040328

Training epoch-10 batch-146
Running loss of epoch-10 batch-146 = 0.0038879658095538616

Training epoch-10 batch-147
Running loss of epoch-10 batch-147 = 0.0036177830770611763

Training epoch-10 batch-148
Running loss of epoch-10 batch-148 = 0.00660346494987607

Training epoch-10 batch-149
Running loss of epoch-10 batch-149 = 0.0070464108139276505

Training epoch-10 batch-150
Running loss of epoch-10 batch-150 = 0.005238012410700321

Training epoch-10 batch-151
Running loss of epoch-10 batch-151 = 0.008759791031479836

Training epoch-10 batch-152
Running loss of epoch-10 batch-152 = 0.005423446651548147

Training epoch-10 batch-153
Running loss of epoch-10 batch-153 = 0.006423127371817827

Training epoch-10 batch-154
Running loss of epoch-10 batch-154 = 0.006164509803056717

Training epoch-10 batch-155
Running loss of epoch-10 batch-155 = 0.007200600113719702

Training epoch-10 batch-156
Running loss of epoch-10 batch-156 = 0.006578038912266493

Training epoch-10 batch-157
Running loss of epoch-10 batch-157 = 0.013694282621145248

Finished training epoch-10.



Average train loss at epoch-10 = 0.0059914908319711685

Started Evaluation

Average val loss at epoch-10 = 0.6192733322478536

Accuracy for classes:
Accuracy for class equals is: 95.71 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 88.69 %
Accuracy for class onCreate is: 90.41 %
Accuracy for class toString is: 73.72 %
Accuracy for class run is: 51.83 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 60.76 %
Accuracy for class execute is: 50.60 %
Accuracy for class get is: 73.08 %

Overall Accuracy = 81.29 %


Best Accuracy = 81.29 % at Epoch-10
Saving model after best epoch-10

Finished Evaluation



Started training epoch-11


Training epoch-11 batch-1
Running loss of epoch-11 batch-1 = 0.003991289064288139

Training epoch-11 batch-2
Running loss of epoch-11 batch-2 = 0.005608124192804098

Training epoch-11 batch-3
Running loss of epoch-11 batch-3 = 0.005348154343664646

Training epoch-11 batch-4
Running loss of epoch-11 batch-4 = 0.007592653855681419

Training epoch-11 batch-5
Running loss of epoch-11 batch-5 = 0.005766001064330339

Training epoch-11 batch-6
Running loss of epoch-11 batch-6 = 0.004356071352958679

Training epoch-11 batch-7
Running loss of epoch-11 batch-7 = 0.003918295726180077

Training epoch-11 batch-8
Running loss of epoch-11 batch-8 = 0.0046200924552977085

Training epoch-11 batch-9
Running loss of epoch-11 batch-9 = 0.004369986243546009

Training epoch-11 batch-10
Running loss of epoch-11 batch-10 = 0.0035828817635774612

Training epoch-11 batch-11
Running loss of epoch-11 batch-11 = 0.007118832319974899

Training epoch-11 batch-12
Running loss of epoch-11 batch-12 = 0.0075253890827298164

Training epoch-11 batch-13
Running loss of epoch-11 batch-13 = 0.005324601195752621

Training epoch-11 batch-14
Running loss of epoch-11 batch-14 = 0.007470591925084591

Training epoch-11 batch-15
Running loss of epoch-11 batch-15 = 0.004795122891664505

Training epoch-11 batch-16
Running loss of epoch-11 batch-16 = 0.00824543833732605

Training epoch-11 batch-17
Running loss of epoch-11 batch-17 = 0.0037590302526950836

Training epoch-11 batch-18
Running loss of epoch-11 batch-18 = 0.00363136176019907

Training epoch-11 batch-19
Running loss of epoch-11 batch-19 = 0.007229282055050135

Training epoch-11 batch-20
Running loss of epoch-11 batch-20 = 0.005640121176838875

Training epoch-11 batch-21
Running loss of epoch-11 batch-21 = 0.003278080141171813

Training epoch-11 batch-22
Running loss of epoch-11 batch-22 = 0.0037628761492669582

Training epoch-11 batch-23
Running loss of epoch-11 batch-23 = 0.0030573296826332808

Training epoch-11 batch-24
Running loss of epoch-11 batch-24 = 0.006179532501846552

Training epoch-11 batch-25
Running loss of epoch-11 batch-25 = 0.007582058198750019

Training epoch-11 batch-26
Running loss of epoch-11 batch-26 = 0.004119460936635733

Training epoch-11 batch-27
Running loss of epoch-11 batch-27 = 0.007093011401593685

Training epoch-11 batch-28
Running loss of epoch-11 batch-28 = 0.0039008348248898983

Training epoch-11 batch-29
Running loss of epoch-11 batch-29 = 0.002987950574606657

Training epoch-11 batch-30
Running loss of epoch-11 batch-30 = 0.003938562702387571

Training epoch-11 batch-31
Running loss of epoch-11 batch-31 = 0.005911849904805422

Training epoch-11 batch-32
Running loss of epoch-11 batch-32 = 0.005359487608075142

Training epoch-11 batch-33
Running loss of epoch-11 batch-33 = 0.00742829404771328

Training epoch-11 batch-34
Running loss of epoch-11 batch-34 = 0.004672395531088114

Training epoch-11 batch-35
Running loss of epoch-11 batch-35 = 0.00502875167876482

Training epoch-11 batch-36
Running loss of epoch-11 batch-36 = 0.0037042172625660896

Training epoch-11 batch-37
Running loss of epoch-11 batch-37 = 0.0059868465177714825

Training epoch-11 batch-38
Running loss of epoch-11 batch-38 = 0.0069743371568620205

Training epoch-11 batch-39
Running loss of epoch-11 batch-39 = 0.004394796676933765

Training epoch-11 batch-40
Running loss of epoch-11 batch-40 = 0.004860762041062117

Training epoch-11 batch-41
Running loss of epoch-11 batch-41 = 0.0038787589874118567

Training epoch-11 batch-42
Running loss of epoch-11 batch-42 = 0.005947953090071678

Training epoch-11 batch-43
Running loss of epoch-11 batch-43 = 0.004824780393391848

Training epoch-11 batch-44
Running loss of epoch-11 batch-44 = 0.004663874860852957

Training epoch-11 batch-45
Running loss of epoch-11 batch-45 = 0.00474946666508913

Training epoch-11 batch-46
Running loss of epoch-11 batch-46 = 0.0045021772384643555

Training epoch-11 batch-47
Running loss of epoch-11 batch-47 = 0.005075890105217695

Training epoch-11 batch-48
Running loss of epoch-11 batch-48 = 0.005763400346040726

Training epoch-11 batch-49
Running loss of epoch-11 batch-49 = 0.005089502781629562

Training epoch-11 batch-50
Running loss of epoch-11 batch-50 = 0.006378166377544403

Training epoch-11 batch-51
Running loss of epoch-11 batch-51 = 0.0035303281620144844

Training epoch-11 batch-52
Running loss of epoch-11 batch-52 = 0.00364173692651093

Training epoch-11 batch-53
Running loss of epoch-11 batch-53 = 0.0034645446576178074

Training epoch-11 batch-54
Running loss of epoch-11 batch-54 = 0.005351005122065544

Training epoch-11 batch-55
Running loss of epoch-11 batch-55 = 0.00620170496404171

Training epoch-11 batch-56
Running loss of epoch-11 batch-56 = 0.00628505926579237

Training epoch-11 batch-57
Running loss of epoch-11 batch-57 = 0.007257574703544378

Training epoch-11 batch-58
Running loss of epoch-11 batch-58 = 0.005991795565932989

Training epoch-11 batch-59
Running loss of epoch-11 batch-59 = 0.007325983140617609

Training epoch-11 batch-60
Running loss of epoch-11 batch-60 = 0.0062331571243703365

Training epoch-11 batch-61
Running loss of epoch-11 batch-61 = 0.003985028713941574

Training epoch-11 batch-62
Running loss of epoch-11 batch-62 = 0.006162729114294052

Training epoch-11 batch-63
Running loss of epoch-11 batch-63 = 0.005903167650103569

Training epoch-11 batch-64
Running loss of epoch-11 batch-64 = 0.004897385835647583

Training epoch-11 batch-65
Running loss of epoch-11 batch-65 = 0.006232883781194687

Training epoch-11 batch-66
Running loss of epoch-11 batch-66 = 0.005790713708847761

Training epoch-11 batch-67
Running loss of epoch-11 batch-67 = 0.00567561574280262

Training epoch-11 batch-68
Running loss of epoch-11 batch-68 = 0.006848030723631382

Training epoch-11 batch-69
Running loss of epoch-11 batch-69 = 0.005149794742465019

Training epoch-11 batch-70
Running loss of epoch-11 batch-70 = 0.007229887880384922

Training epoch-11 batch-71
Running loss of epoch-11 batch-71 = 0.003934633918106556

Training epoch-11 batch-72
Running loss of epoch-11 batch-72 = 0.005075299181044102

Training epoch-11 batch-73
Running loss of epoch-11 batch-73 = 0.004938199650496244

Training epoch-11 batch-74
Running loss of epoch-11 batch-74 = 0.004294032230973244

Training epoch-11 batch-75
Running loss of epoch-11 batch-75 = 0.003910801373422146

Training epoch-11 batch-76
Running loss of epoch-11 batch-76 = 0.003519189078360796

Training epoch-11 batch-77
Running loss of epoch-11 batch-77 = 0.005356158129870892

Training epoch-11 batch-78
Running loss of epoch-11 batch-78 = 0.006574136205017567

Training epoch-11 batch-79
Running loss of epoch-11 batch-79 = 0.001955068204551935

Training epoch-11 batch-80
Running loss of epoch-11 batch-80 = 0.004009774886071682

Training epoch-11 batch-81
Running loss of epoch-11 batch-81 = 0.0051138135604560375

Training epoch-11 batch-82
Running loss of epoch-11 batch-82 = 0.0035126162692904472

Training epoch-11 batch-83
Running loss of epoch-11 batch-83 = 0.0029681005980819464

Training epoch-11 batch-84
Running loss of epoch-11 batch-84 = 0.0069867949932813644

Training epoch-11 batch-85
Running loss of epoch-11 batch-85 = 0.004965938627719879

Training epoch-11 batch-86
Running loss of epoch-11 batch-86 = 0.005576055496931076

Training epoch-11 batch-87
Running loss of epoch-11 batch-87 = 0.0029099525418132544

Training epoch-11 batch-88
Running loss of epoch-11 batch-88 = 0.0036780706141144037

Training epoch-11 batch-89
Running loss of epoch-11 batch-89 = 0.006795301102101803

Training epoch-11 batch-90
Running loss of epoch-11 batch-90 = 0.007125173695385456

Training epoch-11 batch-91
Running loss of epoch-11 batch-91 = 0.004963764920830727

Training epoch-11 batch-92
Running loss of epoch-11 batch-92 = 0.00442425487563014

Training epoch-11 batch-93
Running loss of epoch-11 batch-93 = 0.0050094714388251305

Training epoch-11 batch-94
Running loss of epoch-11 batch-94 = 0.002817694330587983

Training epoch-11 batch-95
Running loss of epoch-11 batch-95 = 0.004218278452754021

Training epoch-11 batch-96
Running loss of epoch-11 batch-96 = 0.007050653919577599

Training epoch-11 batch-97
Running loss of epoch-11 batch-97 = 0.006462684366852045

Training epoch-11 batch-98
Running loss of epoch-11 batch-98 = 0.005390304606407881

Training epoch-11 batch-99
Running loss of epoch-11 batch-99 = 0.006538549438118935

Training epoch-11 batch-100
Running loss of epoch-11 batch-100 = 0.0030669199768453836

Training epoch-11 batch-101
Running loss of epoch-11 batch-101 = 0.0040822685696184635

Training epoch-11 batch-102
Running loss of epoch-11 batch-102 = 0.005081108771264553

Training epoch-11 batch-103
Running loss of epoch-11 batch-103 = 0.005484508816152811

Training epoch-11 batch-104
Running loss of epoch-11 batch-104 = 0.006050862837582827

Training epoch-11 batch-105
Running loss of epoch-11 batch-105 = 0.006156750489026308

Training epoch-11 batch-106
Running loss of epoch-11 batch-106 = 0.003363693365827203

Training epoch-11 batch-107
Running loss of epoch-11 batch-107 = 0.005752250552177429

Training epoch-11 batch-108
Running loss of epoch-11 batch-108 = 0.005546550266444683

Training epoch-11 batch-109
Running loss of epoch-11 batch-109 = 0.005729177966713905

Training epoch-11 batch-110
Running loss of epoch-11 batch-110 = 0.003487112931907177

Training epoch-11 batch-111
Running loss of epoch-11 batch-111 = 0.005106312222778797

Training epoch-11 batch-112
Running loss of epoch-11 batch-112 = 0.004241614136844873

Training epoch-11 batch-113
Running loss of epoch-11 batch-113 = 0.004320468287914991

Training epoch-11 batch-114
Running loss of epoch-11 batch-114 = 0.003979908302426338

Training epoch-11 batch-115
Running loss of epoch-11 batch-115 = 0.0046226815320551395

Training epoch-11 batch-116
Running loss of epoch-11 batch-116 = 0.0063543436117470264

Training epoch-11 batch-117
Running loss of epoch-11 batch-117 = 0.004871209152042866

Training epoch-11 batch-118
Running loss of epoch-11 batch-118 = 0.003936070017516613

Training epoch-11 batch-119
Running loss of epoch-11 batch-119 = 0.005316633731126785

Training epoch-11 batch-120
Running loss of epoch-11 batch-120 = 0.006510657258331776

Training epoch-11 batch-121
Running loss of epoch-11 batch-121 = 0.0052649639546871185

Training epoch-11 batch-122
Running loss of epoch-11 batch-122 = 0.00443732924759388

Training epoch-11 batch-123
Running loss of epoch-11 batch-123 = 0.006146117113530636

Training epoch-11 batch-124
Running loss of epoch-11 batch-124 = 0.004348931834101677

Training epoch-11 batch-125
Running loss of epoch-11 batch-125 = 0.004498151130974293

Training epoch-11 batch-126
Running loss of epoch-11 batch-126 = 0.007402533665299416

Training epoch-11 batch-127
Running loss of epoch-11 batch-127 = 0.0031002184841781855

Training epoch-11 batch-128
Running loss of epoch-11 batch-128 = 0.0053134821355342865

Training epoch-11 batch-129
Running loss of epoch-11 batch-129 = 0.004245295654982328

Training epoch-11 batch-130
Running loss of epoch-11 batch-130 = 0.004527641460299492

Training epoch-11 batch-131
Running loss of epoch-11 batch-131 = 0.004317842423915863

Training epoch-11 batch-132
Running loss of epoch-11 batch-132 = 0.00800643302500248

Training epoch-11 batch-133
Running loss of epoch-11 batch-133 = 0.007589352782815695

Training epoch-11 batch-134
Running loss of epoch-11 batch-134 = 0.004665948450565338

Training epoch-11 batch-135
Running loss of epoch-11 batch-135 = 0.004544530063867569

Training epoch-11 batch-136
Running loss of epoch-11 batch-136 = 0.0055222888477146626

Training epoch-11 batch-137
Running loss of epoch-11 batch-137 = 0.005448228679597378

Training epoch-11 batch-138
Running loss of epoch-11 batch-138 = 0.00385246891528368

Training epoch-11 batch-139
Running loss of epoch-11 batch-139 = 0.00461406446993351

Training epoch-11 batch-140
Running loss of epoch-11 batch-140 = 0.006718241609632969

Training epoch-11 batch-141
Running loss of epoch-11 batch-141 = 0.00773630291223526

Training epoch-11 batch-142
Running loss of epoch-11 batch-142 = 0.005874298047274351

Training epoch-11 batch-143
Running loss of epoch-11 batch-143 = 0.006359640974551439

Training epoch-11 batch-144
Running loss of epoch-11 batch-144 = 0.0065024495124816895

Training epoch-11 batch-145
Running loss of epoch-11 batch-145 = 0.009186310693621635

Training epoch-11 batch-146
Running loss of epoch-11 batch-146 = 0.003628651611506939

Training epoch-11 batch-147
Running loss of epoch-11 batch-147 = 0.0056708320043981075

Training epoch-11 batch-148
Running loss of epoch-11 batch-148 = 0.00697163213044405

Training epoch-11 batch-149
Running loss of epoch-11 batch-149 = 0.00809619203209877

Training epoch-11 batch-150
Running loss of epoch-11 batch-150 = 0.006103971041738987

Training epoch-11 batch-151
Running loss of epoch-11 batch-151 = 0.0031724306754767895

Training epoch-11 batch-152
Running loss of epoch-11 batch-152 = 0.005525693297386169

Training epoch-11 batch-153
Running loss of epoch-11 batch-153 = 0.004587099887430668

Training epoch-11 batch-154
Running loss of epoch-11 batch-154 = 0.004680062644183636

Training epoch-11 batch-155
Running loss of epoch-11 batch-155 = 0.0042111193761229515

Training epoch-11 batch-156
Running loss of epoch-11 batch-156 = 0.00517377071082592

Training epoch-11 batch-157
Running loss of epoch-11 batch-157 = 0.0065132565796375275

Finished training epoch-11.



Average train loss at epoch-11 = 0.005228515584766865

Started Evaluation

Average val loss at epoch-11 = 0.6574985549944502

Accuracy for classes:
Accuracy for class equals is: 94.06 %
Accuracy for class main is: 97.70 %
Accuracy for class setUp is: 93.28 %
Accuracy for class onCreate is: 90.62 %
Accuracy for class toString is: 80.55 %
Accuracy for class run is: 52.05 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 42.83 %
Accuracy for class execute is: 40.16 %
Accuracy for class get is: 84.10 %

Overall Accuracy = 81.04 %

Finished Evaluation



Started training epoch-12


Training epoch-12 batch-1
Running loss of epoch-12 batch-1 = 0.006397974211722612

Training epoch-12 batch-2
Running loss of epoch-12 batch-2 = 0.0036426768638193607

Training epoch-12 batch-3
Running loss of epoch-12 batch-3 = 0.0023790872655808926

Training epoch-12 batch-4
Running loss of epoch-12 batch-4 = 0.0031451445538550615

Training epoch-12 batch-5
Running loss of epoch-12 batch-5 = 0.004150166176259518

Training epoch-12 batch-6
Running loss of epoch-12 batch-6 = 0.004071990959346294

Training epoch-12 batch-7
Running loss of epoch-12 batch-7 = 0.003686486976221204

Training epoch-12 batch-8
Running loss of epoch-12 batch-8 = 0.002828782657161355

Training epoch-12 batch-9
Running loss of epoch-12 batch-9 = 0.00422784686088562

Training epoch-12 batch-10
Running loss of epoch-12 batch-10 = 0.0027580438181757927

Training epoch-12 batch-11
Running loss of epoch-12 batch-11 = 0.0037992841098457575

Training epoch-12 batch-12
Running loss of epoch-12 batch-12 = 0.0028549209237098694

Training epoch-12 batch-13
Running loss of epoch-12 batch-13 = 0.0035689666401594877

Training epoch-12 batch-14
Running loss of epoch-12 batch-14 = 0.0053664096631109715

Training epoch-12 batch-15
Running loss of epoch-12 batch-15 = 0.0033109812065958977

Training epoch-12 batch-16
Running loss of epoch-12 batch-16 = 0.005268562119454145

Training epoch-12 batch-17
Running loss of epoch-12 batch-17 = 0.0033567743375897408

Training epoch-12 batch-18
Running loss of epoch-12 batch-18 = 0.006478497758507729

Training epoch-12 batch-19
Running loss of epoch-12 batch-19 = 0.003755914280191064

Training epoch-12 batch-20
Running loss of epoch-12 batch-20 = 0.0024908450432121754

Training epoch-12 batch-21
Running loss of epoch-12 batch-21 = 0.002821254078298807

Training epoch-12 batch-22
Running loss of epoch-12 batch-22 = 0.003956801723688841

Training epoch-12 batch-23
Running loss of epoch-12 batch-23 = 0.0030755740590393543

Training epoch-12 batch-24
Running loss of epoch-12 batch-24 = 0.005725318565964699

Training epoch-12 batch-25
Running loss of epoch-12 batch-25 = 0.0035179737024009228

Training epoch-12 batch-26
Running loss of epoch-12 batch-26 = 0.004263661801815033

Training epoch-12 batch-27
Running loss of epoch-12 batch-27 = 0.0040000686421990395

Training epoch-12 batch-28
Running loss of epoch-12 batch-28 = 0.004555757623165846

Training epoch-12 batch-29
Running loss of epoch-12 batch-29 = 0.003931463696062565

Training epoch-12 batch-30
Running loss of epoch-12 batch-30 = 0.0021325736306607723

Training epoch-12 batch-31
Running loss of epoch-12 batch-31 = 0.006435580551624298

Training epoch-12 batch-32
Running loss of epoch-12 batch-32 = 0.0060051982291042805

Training epoch-12 batch-33
Running loss of epoch-12 batch-33 = 0.004066338296979666

Training epoch-12 batch-34
Running loss of epoch-12 batch-34 = 0.00367033202201128

Training epoch-12 batch-35
Running loss of epoch-12 batch-35 = 0.005514037795364857

Training epoch-12 batch-36
Running loss of epoch-12 batch-36 = 0.0016225309809669852

Training epoch-12 batch-37
Running loss of epoch-12 batch-37 = 0.0031069102697074413

Training epoch-12 batch-38
Running loss of epoch-12 batch-38 = 0.006585611030459404

Training epoch-12 batch-39
Running loss of epoch-12 batch-39 = 0.005454298574477434

Training epoch-12 batch-40
Running loss of epoch-12 batch-40 = 0.005487735383212566

Training epoch-12 batch-41
Running loss of epoch-12 batch-41 = 0.00450851209461689

Training epoch-12 batch-42
Running loss of epoch-12 batch-42 = 0.003299986943602562

Training epoch-12 batch-43
Running loss of epoch-12 batch-43 = 0.004689968656748533

Training epoch-12 batch-44
Running loss of epoch-12 batch-44 = 0.005878478288650513

Training epoch-12 batch-45
Running loss of epoch-12 batch-45 = 0.003978085238486528

Training epoch-12 batch-46
Running loss of epoch-12 batch-46 = 0.004682977683842182

Training epoch-12 batch-47
Running loss of epoch-12 batch-47 = 0.0067597320303320885

Training epoch-12 batch-48
Running loss of epoch-12 batch-48 = 0.003338008187711239

Training epoch-12 batch-49
Running loss of epoch-12 batch-49 = 0.006937346886843443

Training epoch-12 batch-50
Running loss of epoch-12 batch-50 = 0.0038566659204661846

Training epoch-12 batch-51
Running loss of epoch-12 batch-51 = 0.003294728696346283

Training epoch-12 batch-52
Running loss of epoch-12 batch-52 = 0.004646388813853264

Training epoch-12 batch-53
Running loss of epoch-12 batch-53 = 0.004516486544162035

Training epoch-12 batch-54
Running loss of epoch-12 batch-54 = 0.0051760487258434296

Training epoch-12 batch-55
Running loss of epoch-12 batch-55 = 0.0034652184695005417

Training epoch-12 batch-56
Running loss of epoch-12 batch-56 = 0.004188882187008858

Training epoch-12 batch-57
Running loss of epoch-12 batch-57 = 0.005256103351712227

Training epoch-12 batch-58
Running loss of epoch-12 batch-58 = 0.0035791154950857162

Training epoch-12 batch-59
Running loss of epoch-12 batch-59 = 0.007128998637199402

Training epoch-12 batch-60
Running loss of epoch-12 batch-60 = 0.0025573542807251215

Training epoch-12 batch-61
Running loss of epoch-12 batch-61 = 0.009399929083883762

Training epoch-12 batch-62
Running loss of epoch-12 batch-62 = 0.0031148234847933054

Training epoch-12 batch-63
Running loss of epoch-12 batch-63 = 0.004180540330708027

Training epoch-12 batch-64
Running loss of epoch-12 batch-64 = 0.005215477664023638

Training epoch-12 batch-65
Running loss of epoch-12 batch-65 = 0.0045897443778812885

Training epoch-12 batch-66
Running loss of epoch-12 batch-66 = 0.004541799891740084

Training epoch-12 batch-67
Running loss of epoch-12 batch-67 = 0.00401055533438921

Training epoch-12 batch-68
Running loss of epoch-12 batch-68 = 0.0056479633785784245

Training epoch-12 batch-69
Running loss of epoch-12 batch-69 = 0.005429545417428017

Training epoch-12 batch-70
Running loss of epoch-12 batch-70 = 0.0052239904180169106

Training epoch-12 batch-71
Running loss of epoch-12 batch-71 = 0.0035971966572105885

Training epoch-12 batch-72
Running loss of epoch-12 batch-72 = 0.0037221184466034174

Training epoch-12 batch-73
Running loss of epoch-12 batch-73 = 0.0049857026897370815

Training epoch-12 batch-74
Running loss of epoch-12 batch-74 = 0.004275163635611534

Training epoch-12 batch-75
Running loss of epoch-12 batch-75 = 0.004915910307317972

Training epoch-12 batch-76
Running loss of epoch-12 batch-76 = 0.004778455942869186

Training epoch-12 batch-77
Running loss of epoch-12 batch-77 = 0.004642040003091097

Training epoch-12 batch-78
Running loss of epoch-12 batch-78 = 0.005016131326556206

Training epoch-12 batch-79
Running loss of epoch-12 batch-79 = 0.002546936273574829

Training epoch-12 batch-80
Running loss of epoch-12 batch-80 = 0.003392630722373724

Training epoch-12 batch-81
Running loss of epoch-12 batch-81 = 0.002596866339445114

Training epoch-12 batch-82
Running loss of epoch-12 batch-82 = 0.006026280578225851

Training epoch-12 batch-83
Running loss of epoch-12 batch-83 = 0.004004341550171375

Training epoch-12 batch-84
Running loss of epoch-12 batch-84 = 0.0035348429810255766

Training epoch-12 batch-85
Running loss of epoch-12 batch-85 = 0.0024070062208920717

Training epoch-12 batch-86
Running loss of epoch-12 batch-86 = 0.004277522675693035

Training epoch-12 batch-87
Running loss of epoch-12 batch-87 = 0.003060833550989628

Training epoch-12 batch-88
Running loss of epoch-12 batch-88 = 0.00255940412171185

Training epoch-12 batch-89
Running loss of epoch-12 batch-89 = 0.002932049799710512

Training epoch-12 batch-90
Running loss of epoch-12 batch-90 = 0.003925385884940624

Training epoch-12 batch-91
Running loss of epoch-12 batch-91 = 0.0027678520418703556

Training epoch-12 batch-92
Running loss of epoch-12 batch-92 = 0.0037956354208290577

Training epoch-12 batch-93
Running loss of epoch-12 batch-93 = 0.0036317231133580208

Training epoch-12 batch-94
Running loss of epoch-12 batch-94 = 0.005373549647629261

Training epoch-12 batch-95
Running loss of epoch-12 batch-95 = 0.0036074193194508553

Training epoch-12 batch-96
Running loss of epoch-12 batch-96 = 0.004762046970427036

Training epoch-12 batch-97
Running loss of epoch-12 batch-97 = 0.0023000261280685663

Training epoch-12 batch-98
Running loss of epoch-12 batch-98 = 0.0039730495773255825

Training epoch-12 batch-99
Running loss of epoch-12 batch-99 = 0.0038798125460743904

Training epoch-12 batch-100
Running loss of epoch-12 batch-100 = 0.004310659598559141

Training epoch-12 batch-101
Running loss of epoch-12 batch-101 = 0.0038058082573115826

Training epoch-12 batch-102
Running loss of epoch-12 batch-102 = 0.0031739010009914637

Training epoch-12 batch-103
Running loss of epoch-12 batch-103 = 0.009991544298827648

Training epoch-12 batch-104
Running loss of epoch-12 batch-104 = 0.0040589370764791965

Training epoch-12 batch-105
Running loss of epoch-12 batch-105 = 0.004830352030694485

Training epoch-12 batch-106
Running loss of epoch-12 batch-106 = 0.006950365845113993

Training epoch-12 batch-107
Running loss of epoch-12 batch-107 = 0.004021068569272757

Training epoch-12 batch-108
Running loss of epoch-12 batch-108 = 0.0037522469647228718

Training epoch-12 batch-109
Running loss of epoch-12 batch-109 = 0.006920133251696825

Training epoch-12 batch-110
Running loss of epoch-12 batch-110 = 0.003610529936850071

Training epoch-12 batch-111
Running loss of epoch-12 batch-111 = 0.008464496582746506

Training epoch-12 batch-112
Running loss of epoch-12 batch-112 = 0.004418055061250925

Training epoch-12 batch-113
Running loss of epoch-12 batch-113 = 0.0033970517106354237

Training epoch-12 batch-114
Running loss of epoch-12 batch-114 = 0.004141543991863728

Training epoch-12 batch-115
Running loss of epoch-12 batch-115 = 0.006826414726674557

Training epoch-12 batch-116
Running loss of epoch-12 batch-116 = 0.0036257870960980654

Training epoch-12 batch-117
Running loss of epoch-12 batch-117 = 0.0033936677500605583

Training epoch-12 batch-118
Running loss of epoch-12 batch-118 = 0.00814831629395485

Training epoch-12 batch-119
Running loss of epoch-12 batch-119 = 0.007730472832918167

Training epoch-12 batch-120
Running loss of epoch-12 batch-120 = 0.003846527310088277

Training epoch-12 batch-121
Running loss of epoch-12 batch-121 = 0.004304060712456703

Training epoch-12 batch-122
Running loss of epoch-12 batch-122 = 0.003885336220264435

Training epoch-12 batch-123
Running loss of epoch-12 batch-123 = 0.00479805376380682

Training epoch-12 batch-124
Running loss of epoch-12 batch-124 = 0.0035261923912912607

Training epoch-12 batch-125
Running loss of epoch-12 batch-125 = 0.004918783437460661

Training epoch-12 batch-126
Running loss of epoch-12 batch-126 = 0.005351500120013952

Training epoch-12 batch-127
Running loss of epoch-12 batch-127 = 0.006064091809093952

Training epoch-12 batch-128
Running loss of epoch-12 batch-128 = 0.004751109052449465

Training epoch-12 batch-129
Running loss of epoch-12 batch-129 = 0.007071960251778364

Training epoch-12 batch-130
Running loss of epoch-12 batch-130 = 0.002485828474164009

Training epoch-12 batch-131
Running loss of epoch-12 batch-131 = 0.004376775119453669

Training epoch-12 batch-132
Running loss of epoch-12 batch-132 = 0.005317610688507557

Training epoch-12 batch-133
Running loss of epoch-12 batch-133 = 0.005700128618627787

Training epoch-12 batch-134
Running loss of epoch-12 batch-134 = 0.003723412286490202

Training epoch-12 batch-135
Running loss of epoch-12 batch-135 = 0.004464512690901756

Training epoch-12 batch-136
Running loss of epoch-12 batch-136 = 0.006822698749601841

Training epoch-12 batch-137
Running loss of epoch-12 batch-137 = 0.005340300500392914

Training epoch-12 batch-138
Running loss of epoch-12 batch-138 = 0.002574475249275565

Training epoch-12 batch-139
Running loss of epoch-12 batch-139 = 0.003749864874407649

Training epoch-12 batch-140
Running loss of epoch-12 batch-140 = 0.0044947233982384205

Training epoch-12 batch-141
Running loss of epoch-12 batch-141 = 0.004029156174510717

Training epoch-12 batch-142
Running loss of epoch-12 batch-142 = 0.004141598474234343

Training epoch-12 batch-143
Running loss of epoch-12 batch-143 = 0.008078139275312424

Training epoch-12 batch-144
Running loss of epoch-12 batch-144 = 0.007962469011545181

Training epoch-12 batch-145
Running loss of epoch-12 batch-145 = 0.006690479815006256

Training epoch-12 batch-146
Running loss of epoch-12 batch-146 = 0.003654006402939558

Training epoch-12 batch-147
Running loss of epoch-12 batch-147 = 0.002993591595441103

Training epoch-12 batch-148
Running loss of epoch-12 batch-148 = 0.0021108509972691536

Training epoch-12 batch-149
Running loss of epoch-12 batch-149 = 0.005252719856798649

Training epoch-12 batch-150
Running loss of epoch-12 batch-150 = 0.006312303710728884

Training epoch-12 batch-151
Running loss of epoch-12 batch-151 = 0.007298558484762907

Training epoch-12 batch-152
Running loss of epoch-12 batch-152 = 0.00666793854907155

Training epoch-12 batch-153
Running loss of epoch-12 batch-153 = 0.0029048745054751635

Training epoch-12 batch-154
Running loss of epoch-12 batch-154 = 0.006865185685455799

Training epoch-12 batch-155
Running loss of epoch-12 batch-155 = 0.006835371721535921

Training epoch-12 batch-156
Running loss of epoch-12 batch-156 = 0.004916446283459663

Training epoch-12 batch-157
Running loss of epoch-12 batch-157 = 0.01538783498108387

Finished training epoch-12.



Average train loss at epoch-12 = 0.004553918182104826

Started Evaluation

Average val loss at epoch-12 = 0.6314048938189731

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 95.08 %
Accuracy for class setUp is: 86.07 %
Accuracy for class onCreate is: 91.26 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 59.36 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 60.54 %
Accuracy for class execute is: 62.25 %
Accuracy for class get is: 56.15 %

Overall Accuracy = 81.45 %


Best Accuracy = 81.45 % at Epoch-12
Saving model after best epoch-12

Finished Evaluation



Started training epoch-13


Training epoch-13 batch-1
Running loss of epoch-13 batch-1 = 0.003351096995174885

Training epoch-13 batch-2
Running loss of epoch-13 batch-2 = 0.00271051237359643

Training epoch-13 batch-3
Running loss of epoch-13 batch-3 = 0.004206742160022259

Training epoch-13 batch-4
Running loss of epoch-13 batch-4 = 0.004200201481580734

Training epoch-13 batch-5
Running loss of epoch-13 batch-5 = 0.003068830817937851

Training epoch-13 batch-6
Running loss of epoch-13 batch-6 = 0.002473114989697933

Training epoch-13 batch-7
Running loss of epoch-13 batch-7 = 0.00387014914304018

Training epoch-13 batch-8
Running loss of epoch-13 batch-8 = 0.005419175140559673

Training epoch-13 batch-9
Running loss of epoch-13 batch-9 = 0.004361077677458525

Training epoch-13 batch-10
Running loss of epoch-13 batch-10 = 0.002758965827524662

Training epoch-13 batch-11
Running loss of epoch-13 batch-11 = 0.0056257168762385845

Training epoch-13 batch-12
Running loss of epoch-13 batch-12 = 0.004529997706413269

Training epoch-13 batch-13
Running loss of epoch-13 batch-13 = 0.003164059715345502

Training epoch-13 batch-14
Running loss of epoch-13 batch-14 = 0.0032314204145222902

Training epoch-13 batch-15
Running loss of epoch-13 batch-15 = 0.004070482216775417

Training epoch-13 batch-16
Running loss of epoch-13 batch-16 = 0.0038736541755497456

Training epoch-13 batch-17
Running loss of epoch-13 batch-17 = 0.0038181617856025696

Training epoch-13 batch-18
Running loss of epoch-13 batch-18 = 0.00538589945062995

Training epoch-13 batch-19
Running loss of epoch-13 batch-19 = 0.005426591262221336

Training epoch-13 batch-20
Running loss of epoch-13 batch-20 = 0.003871643915772438

Training epoch-13 batch-21
Running loss of epoch-13 batch-21 = 0.003975775558501482

Training epoch-13 batch-22
Running loss of epoch-13 batch-22 = 0.0036741248331964016

Training epoch-13 batch-23
Running loss of epoch-13 batch-23 = 0.002850285731256008

Training epoch-13 batch-24
Running loss of epoch-13 batch-24 = 0.0032503041438758373

Training epoch-13 batch-25
Running loss of epoch-13 batch-25 = 0.004186744801700115

Training epoch-13 batch-26
Running loss of epoch-13 batch-26 = 0.0039298925548791885

Training epoch-13 batch-27
Running loss of epoch-13 batch-27 = 0.004626473877578974

Training epoch-13 batch-28
Running loss of epoch-13 batch-28 = 0.003679901361465454

Training epoch-13 batch-29
Running loss of epoch-13 batch-29 = 0.0033912397921085358

Training epoch-13 batch-30
Running loss of epoch-13 batch-30 = 0.0030522590968757868

Training epoch-13 batch-31
Running loss of epoch-13 batch-31 = 0.004416578449308872

Training epoch-13 batch-32
Running loss of epoch-13 batch-32 = 0.003998695407062769

Training epoch-13 batch-33
Running loss of epoch-13 batch-33 = 0.003505936823785305

Training epoch-13 batch-34
Running loss of epoch-13 batch-34 = 0.005467910319566727

Training epoch-13 batch-35
Running loss of epoch-13 batch-35 = 0.0050202179700136185

Training epoch-13 batch-36
Running loss of epoch-13 batch-36 = 0.004564310424029827

Training epoch-13 batch-37
Running loss of epoch-13 batch-37 = 0.003061654046177864

Training epoch-13 batch-38
Running loss of epoch-13 batch-38 = 0.0036759143695235252

Training epoch-13 batch-39
Running loss of epoch-13 batch-39 = 0.003682500682771206

Training epoch-13 batch-40
Running loss of epoch-13 batch-40 = 0.00381613802164793

Training epoch-13 batch-41
Running loss of epoch-13 batch-41 = 0.003733087331056595

Training epoch-13 batch-42
Running loss of epoch-13 batch-42 = 0.004277495667338371

Training epoch-13 batch-43
Running loss of epoch-13 batch-43 = 0.0023431768640875816

Training epoch-13 batch-44
Running loss of epoch-13 batch-44 = 0.003916081972420216

Training epoch-13 batch-45
Running loss of epoch-13 batch-45 = 0.003792499890550971

Training epoch-13 batch-46
Running loss of epoch-13 batch-46 = 0.003933390602469444

Training epoch-13 batch-47
Running loss of epoch-13 batch-47 = 0.005048044957220554

Training epoch-13 batch-48
Running loss of epoch-13 batch-48 = 0.0035317332949489355

Training epoch-13 batch-49
Running loss of epoch-13 batch-49 = 0.005128046497702599

Training epoch-13 batch-50
Running loss of epoch-13 batch-50 = 0.004082704894244671

Training epoch-13 batch-51
Running loss of epoch-13 batch-51 = 0.0031631109304726124

Training epoch-13 batch-52
Running loss of epoch-13 batch-52 = 0.002029022667557001

Training epoch-13 batch-53
Running loss of epoch-13 batch-53 = 0.005199741572141647

Training epoch-13 batch-54
Running loss of epoch-13 batch-54 = 0.00562354177236557

Training epoch-13 batch-55
Running loss of epoch-13 batch-55 = 0.003793554613366723

Training epoch-13 batch-56
Running loss of epoch-13 batch-56 = 0.0042847804725170135

Training epoch-13 batch-57
Running loss of epoch-13 batch-57 = 0.003979644738137722

Training epoch-13 batch-58
Running loss of epoch-13 batch-58 = 0.004547400865703821

Training epoch-13 batch-59
Running loss of epoch-13 batch-59 = 0.004783354699611664

Training epoch-13 batch-60
Running loss of epoch-13 batch-60 = 0.0032317626755684614

Training epoch-13 batch-61
Running loss of epoch-13 batch-61 = 0.002407272346317768

Training epoch-13 batch-62
Running loss of epoch-13 batch-62 = 0.0034074350260198116

Training epoch-13 batch-63
Running loss of epoch-13 batch-63 = 0.004934409167617559

Training epoch-13 batch-64
Running loss of epoch-13 batch-64 = 0.0033096601255238056

Training epoch-13 batch-65
Running loss of epoch-13 batch-65 = 0.003972579259425402

Training epoch-13 batch-66
Running loss of epoch-13 batch-66 = 0.003495401469990611

Training epoch-13 batch-67
Running loss of epoch-13 batch-67 = 0.004179857671260834

Training epoch-13 batch-68
Running loss of epoch-13 batch-68 = 0.0026671255473047495

Training epoch-13 batch-69
Running loss of epoch-13 batch-69 = 0.005111869424581528

Training epoch-13 batch-70
Running loss of epoch-13 batch-70 = 0.002630152739584446

Training epoch-13 batch-71
Running loss of epoch-13 batch-71 = 0.002126625506207347

Training epoch-13 batch-72
Running loss of epoch-13 batch-72 = 0.004868587478995323

Training epoch-13 batch-73
Running loss of epoch-13 batch-73 = 0.003308852668851614

Training epoch-13 batch-74
Running loss of epoch-13 batch-74 = 0.0038531559985131025

Training epoch-13 batch-75
Running loss of epoch-13 batch-75 = 0.0055974330753088

Training epoch-13 batch-76
Running loss of epoch-13 batch-76 = 0.0016688386676833034

Training epoch-13 batch-77
Running loss of epoch-13 batch-77 = 0.0048951623030006886

Training epoch-13 batch-78
Running loss of epoch-13 batch-78 = 0.0028798182029277086

Training epoch-13 batch-79
Running loss of epoch-13 batch-79 = 0.0017368508270010352

Training epoch-13 batch-80
Running loss of epoch-13 batch-80 = 0.0031221911776810884

Training epoch-13 batch-81
Running loss of epoch-13 batch-81 = 0.004408432170748711

Training epoch-13 batch-82
Running loss of epoch-13 batch-82 = 0.003212364623323083

Training epoch-13 batch-83
Running loss of epoch-13 batch-83 = 0.003515522927045822

Training epoch-13 batch-84
Running loss of epoch-13 batch-84 = 0.0028528098482638597

Training epoch-13 batch-85
Running loss of epoch-13 batch-85 = 0.0035351335536688566

Training epoch-13 batch-86
Running loss of epoch-13 batch-86 = 0.004686497151851654

Training epoch-13 batch-87
Running loss of epoch-13 batch-87 = 0.0031824191100895405

Training epoch-13 batch-88
Running loss of epoch-13 batch-88 = 0.003830536501482129

Training epoch-13 batch-89
Running loss of epoch-13 batch-89 = 0.003965562209486961

Training epoch-13 batch-90
Running loss of epoch-13 batch-90 = 0.004457615315914154

Training epoch-13 batch-91
Running loss of epoch-13 batch-91 = 0.004646116867661476

Training epoch-13 batch-92
Running loss of epoch-13 batch-92 = 0.005956161767244339

Training epoch-13 batch-93
Running loss of epoch-13 batch-93 = 0.0041748229414224625

Training epoch-13 batch-94
Running loss of epoch-13 batch-94 = 0.004109903238713741

Training epoch-13 batch-95
Running loss of epoch-13 batch-95 = 0.0032978348899632692

Training epoch-13 batch-96
Running loss of epoch-13 batch-96 = 0.003566762199625373

Training epoch-13 batch-97
Running loss of epoch-13 batch-97 = 0.0018955402774736285

Training epoch-13 batch-98
Running loss of epoch-13 batch-98 = 0.0053618671372532845

Training epoch-13 batch-99
Running loss of epoch-13 batch-99 = 0.0024526717606931925

Training epoch-13 batch-100
Running loss of epoch-13 batch-100 = 0.0034045660868287086

Training epoch-13 batch-101
Running loss of epoch-13 batch-101 = 0.004605686292052269

Training epoch-13 batch-102
Running loss of epoch-13 batch-102 = 0.005903326440602541

Training epoch-13 batch-103
Running loss of epoch-13 batch-103 = 0.0051513295620679855

Training epoch-13 batch-104
Running loss of epoch-13 batch-104 = 0.003038569586351514

Training epoch-13 batch-105
Running loss of epoch-13 batch-105 = 0.003045432036742568

Training epoch-13 batch-106
Running loss of epoch-13 batch-106 = 0.0056699104607105255

Training epoch-13 batch-107
Running loss of epoch-13 batch-107 = 0.0018463472370058298

Training epoch-13 batch-108
Running loss of epoch-13 batch-108 = 0.004075410310178995

Training epoch-13 batch-109
Running loss of epoch-13 batch-109 = 0.004966755397617817

Training epoch-13 batch-110
Running loss of epoch-13 batch-110 = 0.005168309435248375

Training epoch-13 batch-111
Running loss of epoch-13 batch-111 = 0.004325352143496275

Training epoch-13 batch-112
Running loss of epoch-13 batch-112 = 0.00232487334869802

Training epoch-13 batch-113
Running loss of epoch-13 batch-113 = 0.004098852165043354

Training epoch-13 batch-114
Running loss of epoch-13 batch-114 = 0.003162235952913761

Training epoch-13 batch-115
Running loss of epoch-13 batch-115 = 0.004678850993514061

Training epoch-13 batch-116
Running loss of epoch-13 batch-116 = 0.003675150917842984

Training epoch-13 batch-117
Running loss of epoch-13 batch-117 = 0.005309312138706446

Training epoch-13 batch-118
Running loss of epoch-13 batch-118 = 0.005048071499913931

Training epoch-13 batch-119
Running loss of epoch-13 batch-119 = 0.005365059245377779

Training epoch-13 batch-120
Running loss of epoch-13 batch-120 = 0.003119011875241995

Training epoch-13 batch-121
Running loss of epoch-13 batch-121 = 0.0035020564682781696

Training epoch-13 batch-122
Running loss of epoch-13 batch-122 = 0.002414512913674116

Training epoch-13 batch-123
Running loss of epoch-13 batch-123 = 0.004669691435992718

Training epoch-13 batch-124
Running loss of epoch-13 batch-124 = 0.0037318593822419643

Training epoch-13 batch-125
Running loss of epoch-13 batch-125 = 0.00449436716735363

Training epoch-13 batch-126
Running loss of epoch-13 batch-126 = 0.004433344583958387

Training epoch-13 batch-127
Running loss of epoch-13 batch-127 = 0.005371712148189545

Training epoch-13 batch-128
Running loss of epoch-13 batch-128 = 0.0035705366171896458

Training epoch-13 batch-129
Running loss of epoch-13 batch-129 = 0.0061874305829405785

Training epoch-13 batch-130
Running loss of epoch-13 batch-130 = 0.00266555929556489

Training epoch-13 batch-131
Running loss of epoch-13 batch-131 = 0.004548166878521442

Training epoch-13 batch-132
Running loss of epoch-13 batch-132 = 0.004388666246086359

Training epoch-13 batch-133
Running loss of epoch-13 batch-133 = 0.004058594815433025

Training epoch-13 batch-134
Running loss of epoch-13 batch-134 = 0.0040718866512179375

Training epoch-13 batch-135
Running loss of epoch-13 batch-135 = 0.0023894186597317457

Training epoch-13 batch-136
Running loss of epoch-13 batch-136 = 0.004310213960707188

Training epoch-13 batch-137
Running loss of epoch-13 batch-137 = 0.005276958458125591

Training epoch-13 batch-138
Running loss of epoch-13 batch-138 = 0.0028190966695547104

Training epoch-13 batch-139
Running loss of epoch-13 batch-139 = 0.002201144117861986

Training epoch-13 batch-140
Running loss of epoch-13 batch-140 = 0.007202367298305035

Training epoch-13 batch-141
Running loss of epoch-13 batch-141 = 0.003052834887057543

Training epoch-13 batch-142
Running loss of epoch-13 batch-142 = 0.004096793010830879

Training epoch-13 batch-143
Running loss of epoch-13 batch-143 = 0.003869203384965658

Training epoch-13 batch-144
Running loss of epoch-13 batch-144 = 0.005293737631291151

Training epoch-13 batch-145
Running loss of epoch-13 batch-145 = 0.0026639627758413553

Training epoch-13 batch-146
Running loss of epoch-13 batch-146 = 0.0032733099069446325

Training epoch-13 batch-147
Running loss of epoch-13 batch-147 = 0.004158413037657738

Training epoch-13 batch-148
Running loss of epoch-13 batch-148 = 0.0023494865745306015

Training epoch-13 batch-149
Running loss of epoch-13 batch-149 = 0.004504518583416939

Training epoch-13 batch-150
Running loss of epoch-13 batch-150 = 0.002759296214208007

Training epoch-13 batch-151
Running loss of epoch-13 batch-151 = 0.002166139427572489

Training epoch-13 batch-152
Running loss of epoch-13 batch-152 = 0.0023524053394794464

Training epoch-13 batch-153
Running loss of epoch-13 batch-153 = 0.002179305534809828

Training epoch-13 batch-154
Running loss of epoch-13 batch-154 = 0.0011649644002318382

Training epoch-13 batch-155
Running loss of epoch-13 batch-155 = 0.004880854859948158

Training epoch-13 batch-156
Running loss of epoch-13 batch-156 = 0.007405073381960392

Training epoch-13 batch-157
Running loss of epoch-13 batch-157 = 0.03823591768741608

Finished training epoch-13.



Average train loss at epoch-13 = 0.003948593013733626

Started Evaluation

Average val loss at epoch-13 = 0.7115490306536422

Accuracy for classes:
Accuracy for class equals is: 94.22 %
Accuracy for class main is: 97.21 %
Accuracy for class setUp is: 84.26 %
Accuracy for class onCreate is: 86.99 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 65.30 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 43.95 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 75.13 %

Overall Accuracy = 80.36 %

Finished Evaluation



Started training epoch-14


Training epoch-14 batch-1
Running loss of epoch-14 batch-1 = 0.0026181074790656567

Training epoch-14 batch-2
Running loss of epoch-14 batch-2 = 0.0017107485327869654

Training epoch-14 batch-3
Running loss of epoch-14 batch-3 = 0.004863630048930645

Training epoch-14 batch-4
Running loss of epoch-14 batch-4 = 0.003077883506193757

Training epoch-14 batch-5
Running loss of epoch-14 batch-5 = 0.006868935190141201

Training epoch-14 batch-6
Running loss of epoch-14 batch-6 = 0.00390705605968833

Training epoch-14 batch-7
Running loss of epoch-14 batch-7 = 0.0033881752751767635

Training epoch-14 batch-8
Running loss of epoch-14 batch-8 = 0.001925731892697513

Training epoch-14 batch-9
Running loss of epoch-14 batch-9 = 0.004209439270198345

Training epoch-14 batch-10
Running loss of epoch-14 batch-10 = 0.004924795124679804

Training epoch-14 batch-11
Running loss of epoch-14 batch-11 = 0.0038015907630324364

Training epoch-14 batch-12
Running loss of epoch-14 batch-12 = 0.003774752374738455

Training epoch-14 batch-13
Running loss of epoch-14 batch-13 = 0.003712547942996025

Training epoch-14 batch-14
Running loss of epoch-14 batch-14 = 0.0053824554197490215

Training epoch-14 batch-15
Running loss of epoch-14 batch-15 = 0.00481690838932991

Training epoch-14 batch-16
Running loss of epoch-14 batch-16 = 0.0010723101440817118

Training epoch-14 batch-17
Running loss of epoch-14 batch-17 = 0.002701225457713008

Training epoch-14 batch-18
Running loss of epoch-14 batch-18 = 0.004489800427109003

Training epoch-14 batch-19
Running loss of epoch-14 batch-19 = 0.003530174260959029

Training epoch-14 batch-20
Running loss of epoch-14 batch-20 = 0.005820749793201685

Training epoch-14 batch-21
Running loss of epoch-14 batch-21 = 0.002977962139993906

Training epoch-14 batch-22
Running loss of epoch-14 batch-22 = 0.002465104218572378

Training epoch-14 batch-23
Running loss of epoch-14 batch-23 = 0.0018326762365177274

Training epoch-14 batch-24
Running loss of epoch-14 batch-24 = 0.0036494273226708174

Training epoch-14 batch-25
Running loss of epoch-14 batch-25 = 0.005138779059052467

Training epoch-14 batch-26
Running loss of epoch-14 batch-26 = 0.005258549936115742

Training epoch-14 batch-27
Running loss of epoch-14 batch-27 = 0.002908478956669569

Training epoch-14 batch-28
Running loss of epoch-14 batch-28 = 0.003242264036089182

Training epoch-14 batch-29
Running loss of epoch-14 batch-29 = 0.007328738458454609

Training epoch-14 batch-30
Running loss of epoch-14 batch-30 = 0.00488405441865325

Training epoch-14 batch-31
Running loss of epoch-14 batch-31 = 0.0015663125086575747

Training epoch-14 batch-32
Running loss of epoch-14 batch-32 = 0.004544696770608425

Training epoch-14 batch-33
Running loss of epoch-14 batch-33 = 0.0022751116193830967

Training epoch-14 batch-34
Running loss of epoch-14 batch-34 = 0.006359729915857315

Training epoch-14 batch-35
Running loss of epoch-14 batch-35 = 0.006228000856935978

Training epoch-14 batch-36
Running loss of epoch-14 batch-36 = 0.004321390762925148

Training epoch-14 batch-37
Running loss of epoch-14 batch-37 = 0.001988612813875079

Training epoch-14 batch-38
Running loss of epoch-14 batch-38 = 0.0033699257764965296

Training epoch-14 batch-39
Running loss of epoch-14 batch-39 = 0.00586090050637722

Training epoch-14 batch-40
Running loss of epoch-14 batch-40 = 0.0024451185017824173

Training epoch-14 batch-41
Running loss of epoch-14 batch-41 = 0.002200684044510126

Training epoch-14 batch-42
Running loss of epoch-14 batch-42 = 0.0023079754319041967

Training epoch-14 batch-43
Running loss of epoch-14 batch-43 = 0.003814822994172573

Training epoch-14 batch-44
Running loss of epoch-14 batch-44 = 0.0049316976219415665

Training epoch-14 batch-45
Running loss of epoch-14 batch-45 = 0.004322763066738844

Training epoch-14 batch-46
Running loss of epoch-14 batch-46 = 0.004203506745398045

Training epoch-14 batch-47
Running loss of epoch-14 batch-47 = 0.005511013325303793

Training epoch-14 batch-48
Running loss of epoch-14 batch-48 = 0.002508867299184203

Training epoch-14 batch-49
Running loss of epoch-14 batch-49 = 0.004055707715451717

Training epoch-14 batch-50
Running loss of epoch-14 batch-50 = 0.006593257188796997

Training epoch-14 batch-51
Running loss of epoch-14 batch-51 = 0.0030738553032279015

Training epoch-14 batch-52
Running loss of epoch-14 batch-52 = 0.005563339218497276

Training epoch-14 batch-53
Running loss of epoch-14 batch-53 = 0.00476465979591012

Training epoch-14 batch-54
Running loss of epoch-14 batch-54 = 0.002765508834272623

Training epoch-14 batch-55
Running loss of epoch-14 batch-55 = 0.0028983219526708126

Training epoch-14 batch-56
Running loss of epoch-14 batch-56 = 0.003473639255389571

Training epoch-14 batch-57
Running loss of epoch-14 batch-57 = 0.0027986234053969383

Training epoch-14 batch-58
Running loss of epoch-14 batch-58 = 0.005852370522916317

Training epoch-14 batch-59
Running loss of epoch-14 batch-59 = 0.010269169695675373

Training epoch-14 batch-60
Running loss of epoch-14 batch-60 = 0.004576554521918297

Training epoch-14 batch-61
Running loss of epoch-14 batch-61 = 0.004599299747496843

Training epoch-14 batch-62
Running loss of epoch-14 batch-62 = 0.0036396635696291924

Training epoch-14 batch-63
Running loss of epoch-14 batch-63 = 0.0034709996543824673

Training epoch-14 batch-64
Running loss of epoch-14 batch-64 = 0.0019240882247686386

Training epoch-14 batch-65
Running loss of epoch-14 batch-65 = 0.0033909499179571867

Training epoch-14 batch-66
Running loss of epoch-14 batch-66 = 0.0035708975046873093

Training epoch-14 batch-67
Running loss of epoch-14 batch-67 = 0.006183241028338671

Training epoch-14 batch-68
Running loss of epoch-14 batch-68 = 0.0039018408861011267

Training epoch-14 batch-69
Running loss of epoch-14 batch-69 = 0.002713619265705347

Training epoch-14 batch-70
Running loss of epoch-14 batch-70 = 0.003754648147150874

Training epoch-14 batch-71
Running loss of epoch-14 batch-71 = 0.002793399151414633

Training epoch-14 batch-72
Running loss of epoch-14 batch-72 = 0.005563648417592049

Training epoch-14 batch-73
Running loss of epoch-14 batch-73 = 0.003965844865888357

Training epoch-14 batch-74
Running loss of epoch-14 batch-74 = 0.0022720294073224068

Training epoch-14 batch-75
Running loss of epoch-14 batch-75 = 0.0034781063441187143

Training epoch-14 batch-76
Running loss of epoch-14 batch-76 = 0.004479080438613892

Training epoch-14 batch-77
Running loss of epoch-14 batch-77 = 0.0043440042063593864

Training epoch-14 batch-78
Running loss of epoch-14 batch-78 = 0.0029773095156997442

Training epoch-14 batch-79
Running loss of epoch-14 batch-79 = 0.003728242125362158

Training epoch-14 batch-80
Running loss of epoch-14 batch-80 = 0.004483267664909363

Training epoch-14 batch-81
Running loss of epoch-14 batch-81 = 0.002062770538032055

Training epoch-14 batch-82
Running loss of epoch-14 batch-82 = 0.0022133176680654287

Training epoch-14 batch-83
Running loss of epoch-14 batch-83 = 0.005231389310210943

Training epoch-14 batch-84
Running loss of epoch-14 batch-84 = 0.004031076095998287

Training epoch-14 batch-85
Running loss of epoch-14 batch-85 = 0.004017413128167391

Training epoch-14 batch-86
Running loss of epoch-14 batch-86 = 0.003996105864644051

Training epoch-14 batch-87
Running loss of epoch-14 batch-87 = 0.0028902378398925066

Training epoch-14 batch-88
Running loss of epoch-14 batch-88 = 0.004282290115952492

Training epoch-14 batch-89
Running loss of epoch-14 batch-89 = 0.004023168236017227

Training epoch-14 batch-90
Running loss of epoch-14 batch-90 = 0.0021827416494488716

Training epoch-14 batch-91
Running loss of epoch-14 batch-91 = 0.0026288945227861404

Training epoch-14 batch-92
Running loss of epoch-14 batch-92 = 0.002163610653951764

Training epoch-14 batch-93
Running loss of epoch-14 batch-93 = 0.0027624336071312428

Training epoch-14 batch-94
Running loss of epoch-14 batch-94 = 0.003280657809227705

Training epoch-14 batch-95
Running loss of epoch-14 batch-95 = 0.0037066913209855556

Training epoch-14 batch-96
Running loss of epoch-14 batch-96 = 0.00527591910213232

Training epoch-14 batch-97
Running loss of epoch-14 batch-97 = 0.0037936223670840263

Training epoch-14 batch-98
Running loss of epoch-14 batch-98 = 0.004841528832912445

Training epoch-14 batch-99
Running loss of epoch-14 batch-99 = 0.005098987836390734

Training epoch-14 batch-100
Running loss of epoch-14 batch-100 = 0.0030337772332131863

Training epoch-14 batch-101
Running loss of epoch-14 batch-101 = 0.0032082817051559687

Training epoch-14 batch-102
Running loss of epoch-14 batch-102 = 0.0035465972032397985

Training epoch-14 batch-103
Running loss of epoch-14 batch-103 = 0.00427122600376606

Training epoch-14 batch-104
Running loss of epoch-14 batch-104 = 0.0026141128037124872

Training epoch-14 batch-105
Running loss of epoch-14 batch-105 = 0.006261060945689678

Training epoch-14 batch-106
Running loss of epoch-14 batch-106 = 0.003619578666985035

Training epoch-14 batch-107
Running loss of epoch-14 batch-107 = 0.004943494684994221

Training epoch-14 batch-108
Running loss of epoch-14 batch-108 = 0.004817843437194824

Training epoch-14 batch-109
Running loss of epoch-14 batch-109 = 0.004703030921518803

Training epoch-14 batch-110
Running loss of epoch-14 batch-110 = 0.004200815688818693

Training epoch-14 batch-111
Running loss of epoch-14 batch-111 = 0.0065961661748588085

Training epoch-14 batch-112
Running loss of epoch-14 batch-112 = 0.002750149928033352

Training epoch-14 batch-113
Running loss of epoch-14 batch-113 = 0.0012846875470131636

Training epoch-14 batch-114
Running loss of epoch-14 batch-114 = 0.004147651605308056

Training epoch-14 batch-115
Running loss of epoch-14 batch-115 = 0.004752807319164276

Training epoch-14 batch-116
Running loss of epoch-14 batch-116 = 0.004346523433923721

Training epoch-14 batch-117
Running loss of epoch-14 batch-117 = 0.0015485802432522178

Training epoch-14 batch-118
Running loss of epoch-14 batch-118 = 0.00444154953584075

Training epoch-14 batch-119
Running loss of epoch-14 batch-119 = 0.00273198913782835

Training epoch-14 batch-120
Running loss of epoch-14 batch-120 = 0.003923702985048294

Training epoch-14 batch-121
Running loss of epoch-14 batch-121 = 0.0017716914881020784

Training epoch-14 batch-122
Running loss of epoch-14 batch-122 = 0.002231622114777565

Training epoch-14 batch-123
Running loss of epoch-14 batch-123 = 0.0027558112051337957

Training epoch-14 batch-124
Running loss of epoch-14 batch-124 = 0.0034318745601922274

Training epoch-14 batch-125
Running loss of epoch-14 batch-125 = 0.002369966823607683

Training epoch-14 batch-126
Running loss of epoch-14 batch-126 = 0.004743097350001335

Training epoch-14 batch-127
Running loss of epoch-14 batch-127 = 0.0019648787565529346

Training epoch-14 batch-128
Running loss of epoch-14 batch-128 = 0.0019987737759947777

Training epoch-14 batch-129
Running loss of epoch-14 batch-129 = 0.004155202768743038

Training epoch-14 batch-130
Running loss of epoch-14 batch-130 = 0.0026789242401719093

Training epoch-14 batch-131
Running loss of epoch-14 batch-131 = 0.003972398582845926

Training epoch-14 batch-132
Running loss of epoch-14 batch-132 = 0.0021132235415279865

Training epoch-14 batch-133
Running loss of epoch-14 batch-133 = 0.005100194364786148

Training epoch-14 batch-134
Running loss of epoch-14 batch-134 = 0.002189149148762226

Training epoch-14 batch-135
Running loss of epoch-14 batch-135 = 0.005390310660004616

Training epoch-14 batch-136
Running loss of epoch-14 batch-136 = 0.005284715443849564

Training epoch-14 batch-137
Running loss of epoch-14 batch-137 = 0.0020223846659064293

Training epoch-14 batch-138
Running loss of epoch-14 batch-138 = 0.002245980314910412

Training epoch-14 batch-139
Running loss of epoch-14 batch-139 = 0.0024511220399290323

Training epoch-14 batch-140
Running loss of epoch-14 batch-140 = 0.0042102462612092495

Training epoch-14 batch-141
Running loss of epoch-14 batch-141 = 0.0027373991906642914

Training epoch-14 batch-142
Running loss of epoch-14 batch-142 = 0.0017358612967655063

Training epoch-14 batch-143
Running loss of epoch-14 batch-143 = 0.002988832537084818

Training epoch-14 batch-144
Running loss of epoch-14 batch-144 = 0.0032550403848290443

Training epoch-14 batch-145
Running loss of epoch-14 batch-145 = 0.004667970817536116

Training epoch-14 batch-146
Running loss of epoch-14 batch-146 = 0.0034944124054163694

Training epoch-14 batch-147
Running loss of epoch-14 batch-147 = 0.003985874354839325

Training epoch-14 batch-148
Running loss of epoch-14 batch-148 = 0.0019723952282220125

Training epoch-14 batch-149
Running loss of epoch-14 batch-149 = 0.003786819288507104

Training epoch-14 batch-150
Running loss of epoch-14 batch-150 = 0.005557963624596596

Training epoch-14 batch-151
Running loss of epoch-14 batch-151 = 0.004444350488483906

Training epoch-14 batch-152
Running loss of epoch-14 batch-152 = 0.0050146556459367275

Training epoch-14 batch-153
Running loss of epoch-14 batch-153 = 0.004747458267956972

Training epoch-14 batch-154
Running loss of epoch-14 batch-154 = 0.004744107834994793

Training epoch-14 batch-155
Running loss of epoch-14 batch-155 = 0.0020094711799174547

Training epoch-14 batch-156
Running loss of epoch-14 batch-156 = 0.002187090227380395

Training epoch-14 batch-157
Running loss of epoch-14 batch-157 = 0.014924591407179832

Finished training epoch-14.



Average train loss at epoch-14 = 0.0037845265448093416

Started Evaluation

Average val loss at epoch-14 = 0.6727725122151529

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 91.48 %
Accuracy for class onCreate is: 89.98 %
Accuracy for class toString is: 80.20 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 46.19 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 77.95 %

Overall Accuracy = 81.99 %


Best Accuracy = 81.99 % at Epoch-14
Saving model after best epoch-14

Finished Evaluation



Started training epoch-15


Training epoch-15 batch-1
Running loss of epoch-15 batch-1 = 0.0029369546100497246

Training epoch-15 batch-2
Running loss of epoch-15 batch-2 = 0.0018005282618105412

Training epoch-15 batch-3
Running loss of epoch-15 batch-3 = 0.001346711185760796

Training epoch-15 batch-4
Running loss of epoch-15 batch-4 = 0.0015814604703336954

Training epoch-15 batch-5
Running loss of epoch-15 batch-5 = 0.0016928164986893535

Training epoch-15 batch-6
Running loss of epoch-15 batch-6 = 0.0017840316286310554

Training epoch-15 batch-7
Running loss of epoch-15 batch-7 = 0.00410229479894042

Training epoch-15 batch-8
Running loss of epoch-15 batch-8 = 0.003524187719449401

Training epoch-15 batch-9
Running loss of epoch-15 batch-9 = 0.0026945469435304403

Training epoch-15 batch-10
Running loss of epoch-15 batch-10 = 0.003989492543041706

Training epoch-15 batch-11
Running loss of epoch-15 batch-11 = 0.0025439432356506586

Training epoch-15 batch-12
Running loss of epoch-15 batch-12 = 0.002130132867023349

Training epoch-15 batch-13
Running loss of epoch-15 batch-13 = 0.0032792638521641493

Training epoch-15 batch-14
Running loss of epoch-15 batch-14 = 0.0033485135063529015

Training epoch-15 batch-15
Running loss of epoch-15 batch-15 = 0.002622860949486494

Training epoch-15 batch-16
Running loss of epoch-15 batch-16 = 0.002646826673299074

Training epoch-15 batch-17
Running loss of epoch-15 batch-17 = 0.002780091715976596

Training epoch-15 batch-18
Running loss of epoch-15 batch-18 = 0.0025362600572407246

Training epoch-15 batch-19
Running loss of epoch-15 batch-19 = 0.004527934826910496

Training epoch-15 batch-20
Running loss of epoch-15 batch-20 = 0.003902678843587637

Training epoch-15 batch-21
Running loss of epoch-15 batch-21 = 0.0036786606069654226

Training epoch-15 batch-22
Running loss of epoch-15 batch-22 = 0.0036039487458765507

Training epoch-15 batch-23
Running loss of epoch-15 batch-23 = 0.003515696618705988

Training epoch-15 batch-24
Running loss of epoch-15 batch-24 = 0.002022273140028119

Training epoch-15 batch-25
Running loss of epoch-15 batch-25 = 0.0033692028373479843

Training epoch-15 batch-26
Running loss of epoch-15 batch-26 = 0.002649678848683834

Training epoch-15 batch-27
Running loss of epoch-15 batch-27 = 0.002347527304664254

Training epoch-15 batch-28
Running loss of epoch-15 batch-28 = 0.00290015060454607

Training epoch-15 batch-29
Running loss of epoch-15 batch-29 = 0.002494718646630645

Training epoch-15 batch-30
Running loss of epoch-15 batch-30 = 0.003531907917931676

Training epoch-15 batch-31
Running loss of epoch-15 batch-31 = 0.0032773809507489204

Training epoch-15 batch-32
Running loss of epoch-15 batch-32 = 0.003897416638210416

Training epoch-15 batch-33
Running loss of epoch-15 batch-33 = 0.002752584870904684

Training epoch-15 batch-34
Running loss of epoch-15 batch-34 = 0.0013509858399629593

Training epoch-15 batch-35
Running loss of epoch-15 batch-35 = 0.002880526939406991

Training epoch-15 batch-36
Running loss of epoch-15 batch-36 = 0.0012217892799526453

Training epoch-15 batch-37
Running loss of epoch-15 batch-37 = 0.003149944357573986

Training epoch-15 batch-38
Running loss of epoch-15 batch-38 = 0.002439799252897501

Training epoch-15 batch-39
Running loss of epoch-15 batch-39 = 0.0014983976725488901

Training epoch-15 batch-40
Running loss of epoch-15 batch-40 = 0.0026249259244650602

Training epoch-15 batch-41
Running loss of epoch-15 batch-41 = 0.0030852968338876963

Training epoch-15 batch-42
Running loss of epoch-15 batch-42 = 0.003422623500227928

Training epoch-15 batch-43
Running loss of epoch-15 batch-43 = 0.005046197213232517

Training epoch-15 batch-44
Running loss of epoch-15 batch-44 = 0.0028452789410948753

Training epoch-15 batch-45
Running loss of epoch-15 batch-45 = 0.00405895384028554

Training epoch-15 batch-46
Running loss of epoch-15 batch-46 = 0.0025511051062494516

Training epoch-15 batch-47
Running loss of epoch-15 batch-47 = 0.0023965099826455116

Training epoch-15 batch-48
Running loss of epoch-15 batch-48 = 0.0017775790765881538

Training epoch-15 batch-49
Running loss of epoch-15 batch-49 = 0.0024591439869254827

Training epoch-15 batch-50
Running loss of epoch-15 batch-50 = 0.0025066593661904335

Training epoch-15 batch-51
Running loss of epoch-15 batch-51 = 0.004009757656604052

Training epoch-15 batch-52
Running loss of epoch-15 batch-52 = 0.0029103548731654882

Training epoch-15 batch-53
Running loss of epoch-15 batch-53 = 0.0018432525685057044

Training epoch-15 batch-54
Running loss of epoch-15 batch-54 = 0.0028683533892035484

Training epoch-15 batch-55
Running loss of epoch-15 batch-55 = 0.003668040968477726

Training epoch-15 batch-56
Running loss of epoch-15 batch-56 = 0.0020959118846803904

Training epoch-15 batch-57
Running loss of epoch-15 batch-57 = 0.0037997644394636154

Training epoch-15 batch-58
Running loss of epoch-15 batch-58 = 0.0014052761252969503

Training epoch-15 batch-59
Running loss of epoch-15 batch-59 = 0.002704148180782795

Training epoch-15 batch-60
Running loss of epoch-15 batch-60 = 0.001887145801447332

Training epoch-15 batch-61
Running loss of epoch-15 batch-61 = 0.0032898103818297386

Training epoch-15 batch-62
Running loss of epoch-15 batch-62 = 0.0031335188541561365

Training epoch-15 batch-63
Running loss of epoch-15 batch-63 = 0.0016871669795364141

Training epoch-15 batch-64
Running loss of epoch-15 batch-64 = 0.0037400703877210617

Training epoch-15 batch-65
Running loss of epoch-15 batch-65 = 0.0028514822479337454

Training epoch-15 batch-66
Running loss of epoch-15 batch-66 = 0.004011303186416626

Training epoch-15 batch-67
Running loss of epoch-15 batch-67 = 0.0017946154111996293

Training epoch-15 batch-68
Running loss of epoch-15 batch-68 = 0.0018766159191727638

Training epoch-15 batch-69
Running loss of epoch-15 batch-69 = 0.0024168924428522587

Training epoch-15 batch-70
Running loss of epoch-15 batch-70 = 0.001953796250745654

Training epoch-15 batch-71
Running loss of epoch-15 batch-71 = 0.0025637380313128233

Training epoch-15 batch-72
Running loss of epoch-15 batch-72 = 0.003937676548957825

Training epoch-15 batch-73
Running loss of epoch-15 batch-73 = 0.0030949702486395836

Training epoch-15 batch-74
Running loss of epoch-15 batch-74 = 0.00408738199621439

Training epoch-15 batch-75
Running loss of epoch-15 batch-75 = 0.00178932910785079

Training epoch-15 batch-76
Running loss of epoch-15 batch-76 = 0.0028748989570885897

Training epoch-15 batch-77
Running loss of epoch-15 batch-77 = 0.004205339588224888

Training epoch-15 batch-78
Running loss of epoch-15 batch-78 = 0.0044388603419065475

Training epoch-15 batch-79
Running loss of epoch-15 batch-79 = 0.0027330201119184494

Training epoch-15 batch-80
Running loss of epoch-15 batch-80 = 0.004626105539500713

Training epoch-15 batch-81
Running loss of epoch-15 batch-81 = 0.00449368916451931

Training epoch-15 batch-82
Running loss of epoch-15 batch-82 = 0.001915722619742155

Training epoch-15 batch-83
Running loss of epoch-15 batch-83 = 0.004582006484270096

Training epoch-15 batch-84
Running loss of epoch-15 batch-84 = 0.002324094297364354

Training epoch-15 batch-85
Running loss of epoch-15 batch-85 = 0.0022132359445095062

Training epoch-15 batch-86
Running loss of epoch-15 batch-86 = 0.0032918264623731375

Training epoch-15 batch-87
Running loss of epoch-15 batch-87 = 0.002513627987354994

Training epoch-15 batch-88
Running loss of epoch-15 batch-88 = 0.0016627258155494928

Training epoch-15 batch-89
Running loss of epoch-15 batch-89 = 0.0024171953555196524

Training epoch-15 batch-90
Running loss of epoch-15 batch-90 = 0.0024241895880550146

Training epoch-15 batch-91
Running loss of epoch-15 batch-91 = 0.0024886264000087976

Training epoch-15 batch-92
Running loss of epoch-15 batch-92 = 0.0030083777382969856

Training epoch-15 batch-93
Running loss of epoch-15 batch-93 = 0.0022284213919192553

Training epoch-15 batch-94
Running loss of epoch-15 batch-94 = 0.00183500733692199

Training epoch-15 batch-95
Running loss of epoch-15 batch-95 = 0.0033411472104489803

Training epoch-15 batch-96
Running loss of epoch-15 batch-96 = 0.005249339155852795

Training epoch-15 batch-97
Running loss of epoch-15 batch-97 = 0.0029494601767510176

Training epoch-15 batch-98
Running loss of epoch-15 batch-98 = 0.0024859318509697914

Training epoch-15 batch-99
Running loss of epoch-15 batch-99 = 0.0014396448386833072

Training epoch-15 batch-100
Running loss of epoch-15 batch-100 = 0.003465913236141205

Training epoch-15 batch-101
Running loss of epoch-15 batch-101 = 0.003304010722786188

Training epoch-15 batch-102
Running loss of epoch-15 batch-102 = 0.003489526454359293

Training epoch-15 batch-103
Running loss of epoch-15 batch-103 = 0.003465926041826606

Training epoch-15 batch-104
Running loss of epoch-15 batch-104 = 0.0015712017193436623

Training epoch-15 batch-105
Running loss of epoch-15 batch-105 = 0.0015341017860919237

Training epoch-15 batch-106
Running loss of epoch-15 batch-106 = 0.0030581834726035595

Training epoch-15 batch-107
Running loss of epoch-15 batch-107 = 0.002220431575551629

Training epoch-15 batch-108
Running loss of epoch-15 batch-108 = 0.0046597132459282875

Training epoch-15 batch-109
Running loss of epoch-15 batch-109 = 0.0035774041898548603

Training epoch-15 batch-110
Running loss of epoch-15 batch-110 = 0.0015513078542426229

Training epoch-15 batch-111
Running loss of epoch-15 batch-111 = 0.002729183528572321

Training epoch-15 batch-112
Running loss of epoch-15 batch-112 = 0.0032379436306655407

Training epoch-15 batch-113
Running loss of epoch-15 batch-113 = 0.003541829762980342

Training epoch-15 batch-114
Running loss of epoch-15 batch-114 = 0.0023791096173226833

Training epoch-15 batch-115
Running loss of epoch-15 batch-115 = 0.0022159500513225794

Training epoch-15 batch-116
Running loss of epoch-15 batch-116 = 0.003003154881298542

Training epoch-15 batch-117
Running loss of epoch-15 batch-117 = 0.0034263329580426216

Training epoch-15 batch-118
Running loss of epoch-15 batch-118 = 0.002290483331307769

Training epoch-15 batch-119
Running loss of epoch-15 batch-119 = 0.002410168992355466

Training epoch-15 batch-120
Running loss of epoch-15 batch-120 = 0.005611028987914324

Training epoch-15 batch-121
Running loss of epoch-15 batch-121 = 0.0044365837238729

Training epoch-15 batch-122
Running loss of epoch-15 batch-122 = 0.003582637058570981

Training epoch-15 batch-123
Running loss of epoch-15 batch-123 = 0.002154431538656354

Training epoch-15 batch-124
Running loss of epoch-15 batch-124 = 0.006196906790137291

Training epoch-15 batch-125
Running loss of epoch-15 batch-125 = 0.004694432020187378

Training epoch-15 batch-126
Running loss of epoch-15 batch-126 = 0.004665870685130358

Training epoch-15 batch-127
Running loss of epoch-15 batch-127 = 0.003443089546635747

Training epoch-15 batch-128
Running loss of epoch-15 batch-128 = 0.00478382408618927

Training epoch-15 batch-129
Running loss of epoch-15 batch-129 = 0.0035767657682299614

Training epoch-15 batch-130
Running loss of epoch-15 batch-130 = 0.003195499535650015

Training epoch-15 batch-131
Running loss of epoch-15 batch-131 = 0.0026173896621912718

Training epoch-15 batch-132
Running loss of epoch-15 batch-132 = 0.00335146626457572

Training epoch-15 batch-133
Running loss of epoch-15 batch-133 = 0.0026533908676356077

Training epoch-15 batch-134
Running loss of epoch-15 batch-134 = 0.004284205846488476

Training epoch-15 batch-135
Running loss of epoch-15 batch-135 = 0.004551868420094252

Training epoch-15 batch-136
Running loss of epoch-15 batch-136 = 0.002591837663203478

Training epoch-15 batch-137
Running loss of epoch-15 batch-137 = 0.0029703094623982906

Training epoch-15 batch-138
Running loss of epoch-15 batch-138 = 0.0033761863596737385

Training epoch-15 batch-139
Running loss of epoch-15 batch-139 = 0.001888046506792307

Training epoch-15 batch-140
Running loss of epoch-15 batch-140 = 0.00246069161221385

Training epoch-15 batch-141
Running loss of epoch-15 batch-141 = 0.0027118672151118517

Training epoch-15 batch-142
Running loss of epoch-15 batch-142 = 0.0026637341361492872

Training epoch-15 batch-143
Running loss of epoch-15 batch-143 = 0.00416740495711565

Training epoch-15 batch-144
Running loss of epoch-15 batch-144 = 0.004112379625439644

Training epoch-15 batch-145
Running loss of epoch-15 batch-145 = 0.002974947914481163

Training epoch-15 batch-146
Running loss of epoch-15 batch-146 = 0.0024470265489071608

Training epoch-15 batch-147
Running loss of epoch-15 batch-147 = 0.0019756320398300886

Training epoch-15 batch-148
Running loss of epoch-15 batch-148 = 0.004464698955416679

Training epoch-15 batch-149
Running loss of epoch-15 batch-149 = 0.003922321368008852

Training epoch-15 batch-150
Running loss of epoch-15 batch-150 = 0.0028326157480478287

Training epoch-15 batch-151
Running loss of epoch-15 batch-151 = 0.0015800490509718657

Training epoch-15 batch-152
Running loss of epoch-15 batch-152 = 0.004642701707780361

Training epoch-15 batch-153
Running loss of epoch-15 batch-153 = 0.001670368481427431

Training epoch-15 batch-154
Running loss of epoch-15 batch-154 = 0.005167530849575996

Training epoch-15 batch-155
Running loss of epoch-15 batch-155 = 0.002495940774679184

Training epoch-15 batch-156
Running loss of epoch-15 batch-156 = 0.0038551820907741785

Training epoch-15 batch-157
Running loss of epoch-15 batch-157 = 0.008068088442087173

Finished training epoch-15.



Average train loss at epoch-15 = 0.003009350574761629

Started Evaluation

Average val loss at epoch-15 = 0.7129182944451091

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 86.39 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 81.57 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 62.33 %
Accuracy for class execute is: 57.43 %
Accuracy for class get is: 53.59 %

Overall Accuracy = 81.43 %

Finished Evaluation



Started training epoch-16


Training epoch-16 batch-1
Running loss of epoch-16 batch-1 = 0.004440663382411003

Training epoch-16 batch-2
Running loss of epoch-16 batch-2 = 0.0017308887327089906

Training epoch-16 batch-3
Running loss of epoch-16 batch-3 = 0.003450425574555993

Training epoch-16 batch-4
Running loss of epoch-16 batch-4 = 0.0016363498289138079

Training epoch-16 batch-5
Running loss of epoch-16 batch-5 = 0.001575907808728516

Training epoch-16 batch-6
Running loss of epoch-16 batch-6 = 0.0019200661918148398

Training epoch-16 batch-7
Running loss of epoch-16 batch-7 = 0.0038776276633143425

Training epoch-16 batch-8
Running loss of epoch-16 batch-8 = 0.0032352027483284473

Training epoch-16 batch-9
Running loss of epoch-16 batch-9 = 0.0025090035051107407

Training epoch-16 batch-10
Running loss of epoch-16 batch-10 = 0.0024397470988333225

Training epoch-16 batch-11
Running loss of epoch-16 batch-11 = 0.00309273274615407

Training epoch-16 batch-12
Running loss of epoch-16 batch-12 = 0.0023695395793765783

Training epoch-16 batch-13
Running loss of epoch-16 batch-13 = 0.004315830767154694

Training epoch-16 batch-14
Running loss of epoch-16 batch-14 = 0.0024030706845223904

Training epoch-16 batch-15
Running loss of epoch-16 batch-15 = 0.0022477656602859497

Training epoch-16 batch-16
Running loss of epoch-16 batch-16 = 0.0018414651276543736

Training epoch-16 batch-17
Running loss of epoch-16 batch-17 = 0.0019380580633878708

Training epoch-16 batch-18
Running loss of epoch-16 batch-18 = 0.0018195346929132938

Training epoch-16 batch-19
Running loss of epoch-16 batch-19 = 0.0029049133881926537

Training epoch-16 batch-20
Running loss of epoch-16 batch-20 = 0.0016475412994623184

Training epoch-16 batch-21
Running loss of epoch-16 batch-21 = 0.0012411222560331225

Training epoch-16 batch-22
Running loss of epoch-16 batch-22 = 0.00238084071315825

Training epoch-16 batch-23
Running loss of epoch-16 batch-23 = 0.0012687843991443515

Training epoch-16 batch-24
Running loss of epoch-16 batch-24 = 0.0013598483055830002

Training epoch-16 batch-25
Running loss of epoch-16 batch-25 = 0.002661785576492548

Training epoch-16 batch-26
Running loss of epoch-16 batch-26 = 0.001549713546410203

Training epoch-16 batch-27
Running loss of epoch-16 batch-27 = 0.002404205035418272

Training epoch-16 batch-28
Running loss of epoch-16 batch-28 = 0.0031521066557615995

Training epoch-16 batch-29
Running loss of epoch-16 batch-29 = 0.0015540204476565123

Training epoch-16 batch-30
Running loss of epoch-16 batch-30 = 0.002401463221758604

Training epoch-16 batch-31
Running loss of epoch-16 batch-31 = 0.0021374570205807686

Training epoch-16 batch-32
Running loss of epoch-16 batch-32 = 0.0028117643669247627

Training epoch-16 batch-33
Running loss of epoch-16 batch-33 = 0.003293984569609165

Training epoch-16 batch-34
Running loss of epoch-16 batch-34 = 0.002884030807763338

Training epoch-16 batch-35
Running loss of epoch-16 batch-35 = 0.002834934741258621

Training epoch-16 batch-36
Running loss of epoch-16 batch-36 = 0.0029213884845376015

Training epoch-16 batch-37
Running loss of epoch-16 batch-37 = 0.00379873625934124

Training epoch-16 batch-38
Running loss of epoch-16 batch-38 = 0.002819319488480687

Training epoch-16 batch-39
Running loss of epoch-16 batch-39 = 0.0022429616656154394

Training epoch-16 batch-40
Running loss of epoch-16 batch-40 = 0.003533952636644244

Training epoch-16 batch-41
Running loss of epoch-16 batch-41 = 0.0015538446605205536

Training epoch-16 batch-42
Running loss of epoch-16 batch-42 = 0.001477847690694034

Training epoch-16 batch-43
Running loss of epoch-16 batch-43 = 0.0032926693093031645

Training epoch-16 batch-44
Running loss of epoch-16 batch-44 = 0.003303726902231574

Training epoch-16 batch-45
Running loss of epoch-16 batch-45 = 0.0027889313641935587

Training epoch-16 batch-46
Running loss of epoch-16 batch-46 = 0.0015157335437834263

Training epoch-16 batch-47
Running loss of epoch-16 batch-47 = 0.0036872150376439095

Training epoch-16 batch-48
Running loss of epoch-16 batch-48 = 0.002459537936374545

Training epoch-16 batch-49
Running loss of epoch-16 batch-49 = 0.0010671598138287663

Training epoch-16 batch-50
Running loss of epoch-16 batch-50 = 0.002035713056102395

Training epoch-16 batch-51
Running loss of epoch-16 batch-51 = 0.0012744843261316419

Training epoch-16 batch-52
Running loss of epoch-16 batch-52 = 0.0037612260784953833

Training epoch-16 batch-53
Running loss of epoch-16 batch-53 = 0.002525245537981391

Training epoch-16 batch-54
Running loss of epoch-16 batch-54 = 0.0040012807585299015

Training epoch-16 batch-55
Running loss of epoch-16 batch-55 = 0.002745341742411256

Training epoch-16 batch-56
Running loss of epoch-16 batch-56 = 0.002492014318704605

Training epoch-16 batch-57
Running loss of epoch-16 batch-57 = 0.00228307256475091

Training epoch-16 batch-58
Running loss of epoch-16 batch-58 = 0.0038779107853770256

Training epoch-16 batch-59
Running loss of epoch-16 batch-59 = 0.002347094938158989

Training epoch-16 batch-60
Running loss of epoch-16 batch-60 = 0.002561925444751978

Training epoch-16 batch-61
Running loss of epoch-16 batch-61 = 0.006526230834424496

Training epoch-16 batch-62
Running loss of epoch-16 batch-62 = 0.002213807310909033

Training epoch-16 batch-63
Running loss of epoch-16 batch-63 = 0.0024440810084342957

Training epoch-16 batch-64
Running loss of epoch-16 batch-64 = 0.005601122044026852

Training epoch-16 batch-65
Running loss of epoch-16 batch-65 = 0.00102040427736938

Training epoch-16 batch-66
Running loss of epoch-16 batch-66 = 0.003085693344473839

Training epoch-16 batch-67
Running loss of epoch-16 batch-67 = 0.0015944878105074167

Training epoch-16 batch-68
Running loss of epoch-16 batch-68 = 0.0017154195811599493

Training epoch-16 batch-69
Running loss of epoch-16 batch-69 = 0.0016692886129021645

Training epoch-16 batch-70
Running loss of epoch-16 batch-70 = 0.0028304089792072773

Training epoch-16 batch-71
Running loss of epoch-16 batch-71 = 0.0019282716093584895

Training epoch-16 batch-72
Running loss of epoch-16 batch-72 = 0.001694594626314938

Training epoch-16 batch-73
Running loss of epoch-16 batch-73 = 0.0016285915626212955

Training epoch-16 batch-74
Running loss of epoch-16 batch-74 = 0.001289948238991201

Training epoch-16 batch-75
Running loss of epoch-16 batch-75 = 0.0015097642317414284

Training epoch-16 batch-76
Running loss of epoch-16 batch-76 = 0.0021968812216073275

Training epoch-16 batch-77
Running loss of epoch-16 batch-77 = 0.002279564505442977

Training epoch-16 batch-78
Running loss of epoch-16 batch-78 = 0.0030218700412660837

Training epoch-16 batch-79
Running loss of epoch-16 batch-79 = 0.0033982684835791588

Training epoch-16 batch-80
Running loss of epoch-16 batch-80 = 0.0019961553625762463

Training epoch-16 batch-81
Running loss of epoch-16 batch-81 = 0.0019214256899431348

Training epoch-16 batch-82
Running loss of epoch-16 batch-82 = 0.0008971648057922721

Training epoch-16 batch-83
Running loss of epoch-16 batch-83 = 0.0019670650362968445

Training epoch-16 batch-84
Running loss of epoch-16 batch-84 = 0.0029053613543510437

Training epoch-16 batch-85
Running loss of epoch-16 batch-85 = 0.004300864413380623

Training epoch-16 batch-86
Running loss of epoch-16 batch-86 = 0.0022598649375140667

Training epoch-16 batch-87
Running loss of epoch-16 batch-87 = 0.0016038073226809502

Training epoch-16 batch-88
Running loss of epoch-16 batch-88 = 0.003023521276190877

Training epoch-16 batch-89
Running loss of epoch-16 batch-89 = 0.002155585680156946

Training epoch-16 batch-90
Running loss of epoch-16 batch-90 = 0.004103721119463444

Training epoch-16 batch-91
Running loss of epoch-16 batch-91 = 0.0027173124253749847

Training epoch-16 batch-92
Running loss of epoch-16 batch-92 = 0.0018736511701717973

Training epoch-16 batch-93
Running loss of epoch-16 batch-93 = 0.0011270844843238592

Training epoch-16 batch-94
Running loss of epoch-16 batch-94 = 0.002837130567058921

Training epoch-16 batch-95
Running loss of epoch-16 batch-95 = 0.0013183227274566889

Training epoch-16 batch-96
Running loss of epoch-16 batch-96 = 0.0028694705106317997

Training epoch-16 batch-97
Running loss of epoch-16 batch-97 = 0.0031388183124363422

Training epoch-16 batch-98
Running loss of epoch-16 batch-98 = 0.0009462112211622298

Training epoch-16 batch-99
Running loss of epoch-16 batch-99 = 0.0013166801072657108

Training epoch-16 batch-100
Running loss of epoch-16 batch-100 = 0.0023861355148255825

Training epoch-16 batch-101
Running loss of epoch-16 batch-101 = 0.0036765385884791613

Training epoch-16 batch-102
Running loss of epoch-16 batch-102 = 0.001878186478279531

Training epoch-16 batch-103
Running loss of epoch-16 batch-103 = 0.0013922383077442646

Training epoch-16 batch-104
Running loss of epoch-16 batch-104 = 0.005052096210420132

Training epoch-16 batch-105
Running loss of epoch-16 batch-105 = 0.0023800269700586796

Training epoch-16 batch-106
Running loss of epoch-16 batch-106 = 0.0028358036652207375

Training epoch-16 batch-107
Running loss of epoch-16 batch-107 = 0.001679590786807239

Training epoch-16 batch-108
Running loss of epoch-16 batch-108 = 0.0023047092836350203

Training epoch-16 batch-109
Running loss of epoch-16 batch-109 = 0.001437115715816617

Training epoch-16 batch-110
Running loss of epoch-16 batch-110 = 0.0010011702543124557

Training epoch-16 batch-111
Running loss of epoch-16 batch-111 = 0.001448475057259202

Training epoch-16 batch-112
Running loss of epoch-16 batch-112 = 0.0017009737202897668

Training epoch-16 batch-113
Running loss of epoch-16 batch-113 = 0.003198024583980441

Training epoch-16 batch-114
Running loss of epoch-16 batch-114 = 0.0033417074009776115

Training epoch-16 batch-115
Running loss of epoch-16 batch-115 = 0.0024565893691033125

Training epoch-16 batch-116
Running loss of epoch-16 batch-116 = 0.0034097526222467422

Training epoch-16 batch-117
Running loss of epoch-16 batch-117 = 0.0016132461605593562

Training epoch-16 batch-118
Running loss of epoch-16 batch-118 = 0.0028063857462257147

Training epoch-16 batch-119
Running loss of epoch-16 batch-119 = 0.003412663470953703

Training epoch-16 batch-120
Running loss of epoch-16 batch-120 = 0.004829457961022854

Training epoch-16 batch-121
Running loss of epoch-16 batch-121 = 0.0031881099566817284

Training epoch-16 batch-122
Running loss of epoch-16 batch-122 = 0.002338005695492029

Training epoch-16 batch-123
Running loss of epoch-16 batch-123 = 0.002939328085631132

Training epoch-16 batch-124
Running loss of epoch-16 batch-124 = 0.00222947308793664

Training epoch-16 batch-125
Running loss of epoch-16 batch-125 = 0.0024072350934147835

Training epoch-16 batch-126
Running loss of epoch-16 batch-126 = 0.001177095458842814

Training epoch-16 batch-127
Running loss of epoch-16 batch-127 = 0.002236967207863927

Training epoch-16 batch-128
Running loss of epoch-16 batch-128 = 0.004203607328236103

Training epoch-16 batch-129
Running loss of epoch-16 batch-129 = 0.002359138336032629

Training epoch-16 batch-130
Running loss of epoch-16 batch-130 = 0.002771924249827862

Training epoch-16 batch-131
Running loss of epoch-16 batch-131 = 0.0019455694127827883

Training epoch-16 batch-132
Running loss of epoch-16 batch-132 = 0.0055963024497032166

Training epoch-16 batch-133
Running loss of epoch-16 batch-133 = 0.0021298681385815144

Training epoch-16 batch-134
Running loss of epoch-16 batch-134 = 0.0024381924886256456

Training epoch-16 batch-135
Running loss of epoch-16 batch-135 = 0.0021054279059171677

Training epoch-16 batch-136
Running loss of epoch-16 batch-136 = 0.0026446531992405653

Training epoch-16 batch-137
Running loss of epoch-16 batch-137 = 0.0036497595719993114

Training epoch-16 batch-138
Running loss of epoch-16 batch-138 = 0.0021239265333861113

Training epoch-16 batch-139
Running loss of epoch-16 batch-139 = 0.00174555869307369

Training epoch-16 batch-140
Running loss of epoch-16 batch-140 = 0.004200023598968983

Training epoch-16 batch-141
Running loss of epoch-16 batch-141 = 0.001739581348374486

Training epoch-16 batch-142
Running loss of epoch-16 batch-142 = 0.0023989917244762182

Training epoch-16 batch-143
Running loss of epoch-16 batch-143 = 0.0035350394900888205

Training epoch-16 batch-144
Running loss of epoch-16 batch-144 = 0.001872674678452313

Training epoch-16 batch-145
Running loss of epoch-16 batch-145 = 0.0026201317086815834

Training epoch-16 batch-146
Running loss of epoch-16 batch-146 = 0.00361555814743042

Training epoch-16 batch-147
Running loss of epoch-16 batch-147 = 0.0018036090768873692

Training epoch-16 batch-148
Running loss of epoch-16 batch-148 = 0.00347848329693079

Training epoch-16 batch-149
Running loss of epoch-16 batch-149 = 0.0016335189575329423

Training epoch-16 batch-150
Running loss of epoch-16 batch-150 = 0.0021790291648358107

Training epoch-16 batch-151
Running loss of epoch-16 batch-151 = 0.001710088225081563

Training epoch-16 batch-152
Running loss of epoch-16 batch-152 = 0.0018640551716089249

Training epoch-16 batch-153
Running loss of epoch-16 batch-153 = 0.0015618775505572557

Training epoch-16 batch-154
Running loss of epoch-16 batch-154 = 0.0021058882120996714

Training epoch-16 batch-155
Running loss of epoch-16 batch-155 = 0.004334777127951384

Training epoch-16 batch-156
Running loss of epoch-16 batch-156 = 0.002034482778981328

Training epoch-16 batch-157
Running loss of epoch-16 batch-157 = 0.006258206441998482

Finished training epoch-16.



Average train loss at epoch-16 = 0.002517453056201339

Started Evaluation

Average val loss at epoch-16 = 0.7506060578219292

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 97.70 %
Accuracy for class setUp is: 93.61 %
Accuracy for class onCreate is: 91.04 %
Accuracy for class toString is: 80.55 %
Accuracy for class run is: 54.11 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 41.26 %
Accuracy for class execute is: 63.05 %
Accuracy for class get is: 66.15 %

Overall Accuracy = 81.33 %

Finished Evaluation



Started training epoch-17


Training epoch-17 batch-1
Running loss of epoch-17 batch-1 = 0.00139869365375489

Training epoch-17 batch-2
Running loss of epoch-17 batch-2 = 0.001297567505389452

Training epoch-17 batch-3
Running loss of epoch-17 batch-3 = 0.001472025760449469

Training epoch-17 batch-4
Running loss of epoch-17 batch-4 = 0.0011881496757268906

Training epoch-17 batch-5
Running loss of epoch-17 batch-5 = 0.0008968069450929761

Training epoch-17 batch-6
Running loss of epoch-17 batch-6 = 0.0027624734211713076

Training epoch-17 batch-7
Running loss of epoch-17 batch-7 = 0.0017705955542623997

Training epoch-17 batch-8
Running loss of epoch-17 batch-8 = 0.0024490950163453817

Training epoch-17 batch-9
Running loss of epoch-17 batch-9 = 0.001526926178485155

Training epoch-17 batch-10
Running loss of epoch-17 batch-10 = 0.0028025321662425995

Training epoch-17 batch-11
Running loss of epoch-17 batch-11 = 0.0017457028152421117

Training epoch-17 batch-12
Running loss of epoch-17 batch-12 = 0.0018773279152810574

Training epoch-17 batch-13
Running loss of epoch-17 batch-13 = 0.0016565837431699038

Training epoch-17 batch-14
Running loss of epoch-17 batch-14 = 0.0014610455837100744

Training epoch-17 batch-15
Running loss of epoch-17 batch-15 = 0.0013837822480127215

Training epoch-17 batch-16
Running loss of epoch-17 batch-16 = 0.0016288574552163482

Training epoch-17 batch-17
Running loss of epoch-17 batch-17 = 0.0020744451321661472

Training epoch-17 batch-18
Running loss of epoch-17 batch-18 = 0.0037991583812981844

Training epoch-17 batch-19
Running loss of epoch-17 batch-19 = 0.0017439753282815218

Training epoch-17 batch-20
Running loss of epoch-17 batch-20 = 0.0021443539299070835

Training epoch-17 batch-21
Running loss of epoch-17 batch-21 = 0.0016792156966403127

Training epoch-17 batch-22
Running loss of epoch-17 batch-22 = 0.002392478985711932

Training epoch-17 batch-23
Running loss of epoch-17 batch-23 = 0.00193393521476537

Training epoch-17 batch-24
Running loss of epoch-17 batch-24 = 0.002717604860663414

Training epoch-17 batch-25
Running loss of epoch-17 batch-25 = 0.0020128425676375628

Training epoch-17 batch-26
Running loss of epoch-17 batch-26 = 0.0013051339192315936

Training epoch-17 batch-27
Running loss of epoch-17 batch-27 = 0.0012393099023029208

Training epoch-17 batch-28
Running loss of epoch-17 batch-28 = 0.00242178444750607

Training epoch-17 batch-29
Running loss of epoch-17 batch-29 = 0.0013547444250434637

Training epoch-17 batch-30
Running loss of epoch-17 batch-30 = 0.0017930333269760013

Training epoch-17 batch-31
Running loss of epoch-17 batch-31 = 0.0024897530674934387

Training epoch-17 batch-32
Running loss of epoch-17 batch-32 = 0.0019741137512028217

Training epoch-17 batch-33
Running loss of epoch-17 batch-33 = 0.001731890020892024

Training epoch-17 batch-34
Running loss of epoch-17 batch-34 = 0.002846304327249527

Training epoch-17 batch-35
Running loss of epoch-17 batch-35 = 0.0014296890003606677

Training epoch-17 batch-36
Running loss of epoch-17 batch-36 = 0.0007173196645453572

Training epoch-17 batch-37
Running loss of epoch-17 batch-37 = 0.0013680581469088793

Training epoch-17 batch-38
Running loss of epoch-17 batch-38 = 0.00159455556422472

Training epoch-17 batch-39
Running loss of epoch-17 batch-39 = 0.0016353223472833633

Training epoch-17 batch-40
Running loss of epoch-17 batch-40 = 0.0023321830667555332

Training epoch-17 batch-41
Running loss of epoch-17 batch-41 = 0.002118525095283985

Training epoch-17 batch-42
Running loss of epoch-17 batch-42 = 0.002025298774242401

Training epoch-17 batch-43
Running loss of epoch-17 batch-43 = 0.001750992494635284

Training epoch-17 batch-44
Running loss of epoch-17 batch-44 = 0.003396474290639162

Training epoch-17 batch-45
Running loss of epoch-17 batch-45 = 0.0017753603169694543

Training epoch-17 batch-46
Running loss of epoch-17 batch-46 = 0.005275549367070198

Training epoch-17 batch-47
Running loss of epoch-17 batch-47 = 0.002420071978121996

Training epoch-17 batch-48
Running loss of epoch-17 batch-48 = 0.0018917967099696398

Training epoch-17 batch-49
Running loss of epoch-17 batch-49 = 0.001753329997882247

Training epoch-17 batch-50
Running loss of epoch-17 batch-50 = 0.0011094954097643495

Training epoch-17 batch-51
Running loss of epoch-17 batch-51 = 0.0017381849465891719

Training epoch-17 batch-52
Running loss of epoch-17 batch-52 = 0.003189425216987729

Training epoch-17 batch-53
Running loss of epoch-17 batch-53 = 0.002512820530682802

Training epoch-17 batch-54
Running loss of epoch-17 batch-54 = 0.0018126213690266013

Training epoch-17 batch-55
Running loss of epoch-17 batch-55 = 0.00243019824847579

Training epoch-17 batch-56
Running loss of epoch-17 batch-56 = 0.0019213658524677157

Training epoch-17 batch-57
Running loss of epoch-17 batch-57 = 0.0035556433722376823

Training epoch-17 batch-58
Running loss of epoch-17 batch-58 = 0.002715652110055089

Training epoch-17 batch-59
Running loss of epoch-17 batch-59 = 0.0010549947619438171

Training epoch-17 batch-60
Running loss of epoch-17 batch-60 = 0.0028887735679745674

Training epoch-17 batch-61
Running loss of epoch-17 batch-61 = 0.002273038262501359

Training epoch-17 batch-62
Running loss of epoch-17 batch-62 = 0.0029961273539811373

Training epoch-17 batch-63
Running loss of epoch-17 batch-63 = 0.0023616028483957052

Training epoch-17 batch-64
Running loss of epoch-17 batch-64 = 0.0015705283731222153

Training epoch-17 batch-65
Running loss of epoch-17 batch-65 = 0.002131582237780094

Training epoch-17 batch-66
Running loss of epoch-17 batch-66 = 0.0031439559534192085

Training epoch-17 batch-67
Running loss of epoch-17 batch-67 = 0.0016241970006376505

Training epoch-17 batch-68
Running loss of epoch-17 batch-68 = 0.0024183986242860556

Training epoch-17 batch-69
Running loss of epoch-17 batch-69 = 0.0007484594825655222

Training epoch-17 batch-70
Running loss of epoch-17 batch-70 = 0.001489969901740551

Training epoch-17 batch-71
Running loss of epoch-17 batch-71 = 0.0020178984850645065

Training epoch-17 batch-72
Running loss of epoch-17 batch-72 = 0.0020453983452171087

Training epoch-17 batch-73
Running loss of epoch-17 batch-73 = 0.0022452911362051964

Training epoch-17 batch-74
Running loss of epoch-17 batch-74 = 0.0017962371930480003

Training epoch-17 batch-75
Running loss of epoch-17 batch-75 = 0.0034946033265441656

Training epoch-17 batch-76
Running loss of epoch-17 batch-76 = 0.0016500888159498572

Training epoch-17 batch-77
Running loss of epoch-17 batch-77 = 0.004378241486847401

Training epoch-17 batch-78
Running loss of epoch-17 batch-78 = 0.0017640898004174232

Training epoch-17 batch-79
Running loss of epoch-17 batch-79 = 0.0006996077718213201

Training epoch-17 batch-80
Running loss of epoch-17 batch-80 = 0.002157354960218072

Training epoch-17 batch-81
Running loss of epoch-17 batch-81 = 0.0014052798505872488

Training epoch-17 batch-82
Running loss of epoch-17 batch-82 = 0.0029037895146757364

Training epoch-17 batch-83
Running loss of epoch-17 batch-83 = 0.0025932786520570517

Training epoch-17 batch-84
Running loss of epoch-17 batch-84 = 0.003167911199852824

Training epoch-17 batch-85
Running loss of epoch-17 batch-85 = 0.001570616033859551

Training epoch-17 batch-86
Running loss of epoch-17 batch-86 = 0.004273482598364353

Training epoch-17 batch-87
Running loss of epoch-17 batch-87 = 0.0027395570650696754

Training epoch-17 batch-88
Running loss of epoch-17 batch-88 = 0.0012802612036466599

Training epoch-17 batch-89
Running loss of epoch-17 batch-89 = 0.0036162626929581165

Training epoch-17 batch-90
Running loss of epoch-17 batch-90 = 0.0016037699533626437

Training epoch-17 batch-91
Running loss of epoch-17 batch-91 = 0.004990081302821636

Training epoch-17 batch-92
Running loss of epoch-17 batch-92 = 0.0014917352236807346

Training epoch-17 batch-93
Running loss of epoch-17 batch-93 = 0.0024752989411354065

Training epoch-17 batch-94
Running loss of epoch-17 batch-94 = 0.00263779703527689

Training epoch-17 batch-95
Running loss of epoch-17 batch-95 = 0.003118915017694235

Training epoch-17 batch-96
Running loss of epoch-17 batch-96 = 0.0013649767497554421

Training epoch-17 batch-97
Running loss of epoch-17 batch-97 = 0.0029126270674169064

Training epoch-17 batch-98
Running loss of epoch-17 batch-98 = 0.006300842389464378

Training epoch-17 batch-99
Running loss of epoch-17 batch-99 = 0.0026858518831431866

Training epoch-17 batch-100
Running loss of epoch-17 batch-100 = 0.0017008266877382994

Training epoch-17 batch-101
Running loss of epoch-17 batch-101 = 0.0013163990806788206

Training epoch-17 batch-102
Running loss of epoch-17 batch-102 = 0.003974206279963255

Training epoch-17 batch-103
Running loss of epoch-17 batch-103 = 0.0012316125212237239

Training epoch-17 batch-104
Running loss of epoch-17 batch-104 = 0.0016218430828303099

Training epoch-17 batch-105
Running loss of epoch-17 batch-105 = 0.0015608646208420396

Training epoch-17 batch-106
Running loss of epoch-17 batch-106 = 0.002289597410708666

Training epoch-17 batch-107
Running loss of epoch-17 batch-107 = 0.0038066506385803223

Training epoch-17 batch-108
Running loss of epoch-17 batch-108 = 0.0020447508431971073

Training epoch-17 batch-109
Running loss of epoch-17 batch-109 = 0.002969942055642605

Training epoch-17 batch-110
Running loss of epoch-17 batch-110 = 0.00154055783059448

Training epoch-17 batch-111
Running loss of epoch-17 batch-111 = 0.0019533198792487383

Training epoch-17 batch-112
Running loss of epoch-17 batch-112 = 0.0012267649872228503

Training epoch-17 batch-113
Running loss of epoch-17 batch-113 = 0.0034156597685068846

Training epoch-17 batch-114
Running loss of epoch-17 batch-114 = 0.0045672510750591755

Training epoch-17 batch-115
Running loss of epoch-17 batch-115 = 0.00413647573441267

Training epoch-17 batch-116
Running loss of epoch-17 batch-116 = 0.004220248199999332

Training epoch-17 batch-117
Running loss of epoch-17 batch-117 = 0.0030370373278856277

Training epoch-17 batch-118
Running loss of epoch-17 batch-118 = 0.0016194770578294992

Training epoch-17 batch-119
Running loss of epoch-17 batch-119 = 0.002410047221928835

Training epoch-17 batch-120
Running loss of epoch-17 batch-120 = 0.002822962123900652

Training epoch-17 batch-121
Running loss of epoch-17 batch-121 = 0.0022860874887555838

Training epoch-17 batch-122
Running loss of epoch-17 batch-122 = 0.0025657624937593937

Training epoch-17 batch-123
Running loss of epoch-17 batch-123 = 0.0030419444665312767

Training epoch-17 batch-124
Running loss of epoch-17 batch-124 = 0.004003805108368397

Training epoch-17 batch-125
Running loss of epoch-17 batch-125 = 0.00318400701507926

Training epoch-17 batch-126
Running loss of epoch-17 batch-126 = 0.0008791201980784535

Training epoch-17 batch-127
Running loss of epoch-17 batch-127 = 0.00427434965968132

Training epoch-17 batch-128
Running loss of epoch-17 batch-128 = 0.0018801211845129728

Training epoch-17 batch-129
Running loss of epoch-17 batch-129 = 0.001738500315696001

Training epoch-17 batch-130
Running loss of epoch-17 batch-130 = 0.0023535580839961767

Training epoch-17 batch-131
Running loss of epoch-17 batch-131 = 0.0009238636121153831

Training epoch-17 batch-132
Running loss of epoch-17 batch-132 = 0.0019439574098214507

Training epoch-17 batch-133
Running loss of epoch-17 batch-133 = 0.0028235227800905704

Training epoch-17 batch-134
Running loss of epoch-17 batch-134 = 0.003792759496718645

Training epoch-17 batch-135
Running loss of epoch-17 batch-135 = 0.006000370718538761

Training epoch-17 batch-136
Running loss of epoch-17 batch-136 = 0.0029945094138383865

Training epoch-17 batch-137
Running loss of epoch-17 batch-137 = 0.0009216624312102795

Training epoch-17 batch-138
Running loss of epoch-17 batch-138 = 0.004637294448912144

Training epoch-17 batch-139
Running loss of epoch-17 batch-139 = 0.0032791318371891975

Training epoch-17 batch-140
Running loss of epoch-17 batch-140 = 0.0019683593418449163

Training epoch-17 batch-141
Running loss of epoch-17 batch-141 = 0.0023134860675781965

Training epoch-17 batch-142
Running loss of epoch-17 batch-142 = 0.003446426009759307

Training epoch-17 batch-143
Running loss of epoch-17 batch-143 = 0.0012962956679984927

Training epoch-17 batch-144
Running loss of epoch-17 batch-144 = 0.0019372198730707169

Training epoch-17 batch-145
Running loss of epoch-17 batch-145 = 0.0028675489593297243

Training epoch-17 batch-146
Running loss of epoch-17 batch-146 = 0.002727140672504902

Training epoch-17 batch-147
Running loss of epoch-17 batch-147 = 0.0011479698587208986

Training epoch-17 batch-148
Running loss of epoch-17 batch-148 = 0.0026109751779586077

Training epoch-17 batch-149
Running loss of epoch-17 batch-149 = 0.0013751080259680748

Training epoch-17 batch-150
Running loss of epoch-17 batch-150 = 0.0024340245872735977

Training epoch-17 batch-151
Running loss of epoch-17 batch-151 = 0.0014098974643275142

Training epoch-17 batch-152
Running loss of epoch-17 batch-152 = 0.005228755064308643

Training epoch-17 batch-153
Running loss of epoch-17 batch-153 = 0.0038108136504888535

Training epoch-17 batch-154
Running loss of epoch-17 batch-154 = 0.003568061161786318

Training epoch-17 batch-155
Running loss of epoch-17 batch-155 = 0.00312789692543447

Training epoch-17 batch-156
Running loss of epoch-17 batch-156 = 0.0035697342827916145

Training epoch-17 batch-157
Running loss of epoch-17 batch-157 = 0.02144373580813408

Finished training epoch-17.



Average train loss at epoch-17 = 0.0023981850892305376

Started Evaluation

Average val loss at epoch-17 = 0.9378220315740787

Accuracy for classes:
Accuracy for class equals is: 94.06 %
Accuracy for class main is: 98.03 %
Accuracy for class setUp is: 67.05 %
Accuracy for class onCreate is: 85.18 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 55.25 %
Accuracy for class hashCode is: 96.63 %
Accuracy for class init is: 51.12 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 73.59 %

Overall Accuracy = 77.70 %

Finished Evaluation



Started training epoch-18


Training epoch-18 batch-1
Running loss of epoch-18 batch-1 = 0.0012823451543226838

Training epoch-18 batch-2
Running loss of epoch-18 batch-2 = 0.003653143998235464

Training epoch-18 batch-3
Running loss of epoch-18 batch-3 = 0.0014350538840517402

Training epoch-18 batch-4
Running loss of epoch-18 batch-4 = 0.002569830045104027

Training epoch-18 batch-5
Running loss of epoch-18 batch-5 = 0.0023948531597852707

Training epoch-18 batch-6
Running loss of epoch-18 batch-6 = 0.0005907596787437797

Training epoch-18 batch-7
Running loss of epoch-18 batch-7 = 0.0032745006028562784

Training epoch-18 batch-8
Running loss of epoch-18 batch-8 = 0.0024081734009087086

Training epoch-18 batch-9
Running loss of epoch-18 batch-9 = 0.0019351043738424778

Training epoch-18 batch-10
Running loss of epoch-18 batch-10 = 0.0021117159631103277

Training epoch-18 batch-11
Running loss of epoch-18 batch-11 = 0.0019900451879948378

Training epoch-18 batch-12
Running loss of epoch-18 batch-12 = 0.0026997204404324293

Training epoch-18 batch-13
Running loss of epoch-18 batch-13 = 0.0032432381995022297

Training epoch-18 batch-14
Running loss of epoch-18 batch-14 = 0.0026953842025250196

Training epoch-18 batch-15
Running loss of epoch-18 batch-15 = 0.0033646062947809696

Training epoch-18 batch-16
Running loss of epoch-18 batch-16 = 0.0020700767636299133

Training epoch-18 batch-17
Running loss of epoch-18 batch-17 = 0.002809388330206275

Training epoch-18 batch-18
Running loss of epoch-18 batch-18 = 0.0021035741083323956

Training epoch-18 batch-19
Running loss of epoch-18 batch-19 = 0.002640026854351163

Training epoch-18 batch-20
Running loss of epoch-18 batch-20 = 0.0030276125762611628

Training epoch-18 batch-21
Running loss of epoch-18 batch-21 = 0.0007666099118068814

Training epoch-18 batch-22
Running loss of epoch-18 batch-22 = 0.0020823334343731403

Training epoch-18 batch-23
Running loss of epoch-18 batch-23 = 0.001243982114829123

Training epoch-18 batch-24
Running loss of epoch-18 batch-24 = 0.0018119525630027056

Training epoch-18 batch-25
Running loss of epoch-18 batch-25 = 0.00349914887920022

Training epoch-18 batch-26
Running loss of epoch-18 batch-26 = 0.001357021275907755

Training epoch-18 batch-27
Running loss of epoch-18 batch-27 = 0.0011383170494809747

Training epoch-18 batch-28
Running loss of epoch-18 batch-28 = 0.00202674837782979

Training epoch-18 batch-29
Running loss of epoch-18 batch-29 = 0.001446153037250042

Training epoch-18 batch-30
Running loss of epoch-18 batch-30 = 0.0019570845179259777

Training epoch-18 batch-31
Running loss of epoch-18 batch-31 = 0.0011876217322424054

Training epoch-18 batch-32
Running loss of epoch-18 batch-32 = 0.001551333349198103

Training epoch-18 batch-33
Running loss of epoch-18 batch-33 = 0.0012777092633768916

Training epoch-18 batch-34
Running loss of epoch-18 batch-34 = 0.0014648584183305502

Training epoch-18 batch-35
Running loss of epoch-18 batch-35 = 0.001993920188397169

Training epoch-18 batch-36
Running loss of epoch-18 batch-36 = 0.0011126159224659204

Training epoch-18 batch-37
Running loss of epoch-18 batch-37 = 0.0007679194677621126

Training epoch-18 batch-38
Running loss of epoch-18 batch-38 = 0.002270071068778634

Training epoch-18 batch-39
Running loss of epoch-18 batch-39 = 0.0006222103256732225

Training epoch-18 batch-40
Running loss of epoch-18 batch-40 = 0.0009893497917801142

Training epoch-18 batch-41
Running loss of epoch-18 batch-41 = 0.0034382943995296955

Training epoch-18 batch-42
Running loss of epoch-18 batch-42 = 0.002753144595772028

Training epoch-18 batch-43
Running loss of epoch-18 batch-43 = 0.0019168900325894356

Training epoch-18 batch-44
Running loss of epoch-18 batch-44 = 0.0018315439810976386

Training epoch-18 batch-45
Running loss of epoch-18 batch-45 = 0.0008985175518319011

Training epoch-18 batch-46
Running loss of epoch-18 batch-46 = 0.0008355273166671395

Training epoch-18 batch-47
Running loss of epoch-18 batch-47 = 0.0013311115326359868

Training epoch-18 batch-48
Running loss of epoch-18 batch-48 = 0.00313782412558794

Training epoch-18 batch-49
Running loss of epoch-18 batch-49 = 0.001449075061827898

Training epoch-18 batch-50
Running loss of epoch-18 batch-50 = 0.0019104359671473503

Training epoch-18 batch-51
Running loss of epoch-18 batch-51 = 0.0009089141385629773

Training epoch-18 batch-52
Running loss of epoch-18 batch-52 = 0.0018931673839688301

Training epoch-18 batch-53
Running loss of epoch-18 batch-53 = 0.002145106438547373

Training epoch-18 batch-54
Running loss of epoch-18 batch-54 = 0.0011052230838686228

Training epoch-18 batch-55
Running loss of epoch-18 batch-55 = 0.002396224532276392

Training epoch-18 batch-56
Running loss of epoch-18 batch-56 = 0.0002265068469569087

Training epoch-18 batch-57
Running loss of epoch-18 batch-57 = 0.0017270402750000358

Training epoch-18 batch-58
Running loss of epoch-18 batch-58 = 0.002338716760277748

Training epoch-18 batch-59
Running loss of epoch-18 batch-59 = 0.003253513714298606

Training epoch-18 batch-60
Running loss of epoch-18 batch-60 = 0.00125994929112494

Training epoch-18 batch-61
Running loss of epoch-18 batch-61 = 0.0006768802413716912

Training epoch-18 batch-62
Running loss of epoch-18 batch-62 = 0.0009446315234526992

Training epoch-18 batch-63
Running loss of epoch-18 batch-63 = 0.0011820078361779451

Training epoch-18 batch-64
Running loss of epoch-18 batch-64 = 0.0018321566749364138

Training epoch-18 batch-65
Running loss of epoch-18 batch-65 = 0.001713618403300643

Training epoch-18 batch-66
Running loss of epoch-18 batch-66 = 0.0022926153615117073

Training epoch-18 batch-67
Running loss of epoch-18 batch-67 = 0.0012045311741530895

Training epoch-18 batch-68
Running loss of epoch-18 batch-68 = 0.0024567311629652977

Training epoch-18 batch-69
Running loss of epoch-18 batch-69 = 0.0038585015572607517

Training epoch-18 batch-70
Running loss of epoch-18 batch-70 = 0.003110675374045968

Training epoch-18 batch-71
Running loss of epoch-18 batch-71 = 0.002941707381978631

Training epoch-18 batch-72
Running loss of epoch-18 batch-72 = 0.0010555641492828727

Training epoch-18 batch-73
Running loss of epoch-18 batch-73 = 0.0014264945639297366

Training epoch-18 batch-74
Running loss of epoch-18 batch-74 = 0.0010622385889291763

Training epoch-18 batch-75
Running loss of epoch-18 batch-75 = 0.0020696641877293587

Training epoch-18 batch-76
Running loss of epoch-18 batch-76 = 0.0016714875819161534

Training epoch-18 batch-77
Running loss of epoch-18 batch-77 = 0.00044706789776682854

Training epoch-18 batch-78
Running loss of epoch-18 batch-78 = 0.002862759865820408

Training epoch-18 batch-79
Running loss of epoch-18 batch-79 = 0.0025396852288395166

Training epoch-18 batch-80
Running loss of epoch-18 batch-80 = 0.0006899092113599181

Training epoch-18 batch-81
Running loss of epoch-18 batch-81 = 0.0008387486450374126

Training epoch-18 batch-82
Running loss of epoch-18 batch-82 = 0.0018353128107264638

Training epoch-18 batch-83
Running loss of epoch-18 batch-83 = 0.0014922537375241518

Training epoch-18 batch-84
Running loss of epoch-18 batch-84 = 0.004648427478969097

Training epoch-18 batch-85
Running loss of epoch-18 batch-85 = 0.0011478428496047854

Training epoch-18 batch-86
Running loss of epoch-18 batch-86 = 0.0017579903360456228

Training epoch-18 batch-87
Running loss of epoch-18 batch-87 = 0.001649748533964157

Training epoch-18 batch-88
Running loss of epoch-18 batch-88 = 0.004609165247529745

Training epoch-18 batch-89
Running loss of epoch-18 batch-89 = 0.0012046630727127194

Training epoch-18 batch-90
Running loss of epoch-18 batch-90 = 0.0013396100839599967

Training epoch-18 batch-91
Running loss of epoch-18 batch-91 = 0.0010079448111355305

Training epoch-18 batch-92
Running loss of epoch-18 batch-92 = 0.002353303600102663

Training epoch-18 batch-93
Running loss of epoch-18 batch-93 = 0.0013293958036229014

Training epoch-18 batch-94
Running loss of epoch-18 batch-94 = 0.0014299227623268962

Training epoch-18 batch-95
Running loss of epoch-18 batch-95 = 0.0023447296116501093

Training epoch-18 batch-96
Running loss of epoch-18 batch-96 = 0.002857385901734233

Training epoch-18 batch-97
Running loss of epoch-18 batch-97 = 0.0014695326099172235

Training epoch-18 batch-98
Running loss of epoch-18 batch-98 = 0.0016200203681364655

Training epoch-18 batch-99
Running loss of epoch-18 batch-99 = 0.002028221497312188

Training epoch-18 batch-100
Running loss of epoch-18 batch-100 = 0.0028755185194313526

Training epoch-18 batch-101
Running loss of epoch-18 batch-101 = 0.0022151705343276262

Training epoch-18 batch-102
Running loss of epoch-18 batch-102 = 0.001598146976903081

Training epoch-18 batch-103
Running loss of epoch-18 batch-103 = 0.0021150652319192886

Training epoch-18 batch-104
Running loss of epoch-18 batch-104 = 0.0014128228649497032

Training epoch-18 batch-105
Running loss of epoch-18 batch-105 = 0.001606195466592908

Training epoch-18 batch-106
Running loss of epoch-18 batch-106 = 0.0014279154129326344

Training epoch-18 batch-107
Running loss of epoch-18 batch-107 = 0.0019627511501312256

Training epoch-18 batch-108
Running loss of epoch-18 batch-108 = 0.0038970105815678835

Training epoch-18 batch-109
Running loss of epoch-18 batch-109 = 0.0008029061136767268

Training epoch-18 batch-110
Running loss of epoch-18 batch-110 = 0.0018221040954813361

Training epoch-18 batch-111
Running loss of epoch-18 batch-111 = 0.0011415057815611362

Training epoch-18 batch-112
Running loss of epoch-18 batch-112 = 0.0014198215212672949

Training epoch-18 batch-113
Running loss of epoch-18 batch-113 = 0.002157756593078375

Training epoch-18 batch-114
Running loss of epoch-18 batch-114 = 0.0010422354098409414

Training epoch-18 batch-115
Running loss of epoch-18 batch-115 = 0.0008193243993446231

Training epoch-18 batch-116
Running loss of epoch-18 batch-116 = 0.0010012851562350988

Training epoch-18 batch-117
Running loss of epoch-18 batch-117 = 0.002266722498461604

Training epoch-18 batch-118
Running loss of epoch-18 batch-118 = 0.003859930671751499

Training epoch-18 batch-119
Running loss of epoch-18 batch-119 = 0.0011297209421172738

Training epoch-18 batch-120
Running loss of epoch-18 batch-120 = 0.0020029006991535425

Training epoch-18 batch-121
Running loss of epoch-18 batch-121 = 0.0020616212859749794

Training epoch-18 batch-122
Running loss of epoch-18 batch-122 = 0.002450977684929967

Training epoch-18 batch-123
Running loss of epoch-18 batch-123 = 0.0012672282755374908

Training epoch-18 batch-124
Running loss of epoch-18 batch-124 = 0.00213268562220037

Training epoch-18 batch-125
Running loss of epoch-18 batch-125 = 0.0011573893716558814

Training epoch-18 batch-126
Running loss of epoch-18 batch-126 = 0.0026084352284669876

Training epoch-18 batch-127
Running loss of epoch-18 batch-127 = 0.003260531462728977

Training epoch-18 batch-128
Running loss of epoch-18 batch-128 = 0.00130928261205554

Training epoch-18 batch-129
Running loss of epoch-18 batch-129 = 0.0014568011974915862

Training epoch-18 batch-130
Running loss of epoch-18 batch-130 = 0.004478245507925749

Training epoch-18 batch-131
Running loss of epoch-18 batch-131 = 0.0028439497109502554

Training epoch-18 batch-132
Running loss of epoch-18 batch-132 = 0.002396354917436838

Training epoch-18 batch-133
Running loss of epoch-18 batch-133 = 0.0011026852298527956

Training epoch-18 batch-134
Running loss of epoch-18 batch-134 = 0.0015162212075665593

Training epoch-18 batch-135
Running loss of epoch-18 batch-135 = 0.0023771398700773716

Training epoch-18 batch-136
Running loss of epoch-18 batch-136 = 0.0033794718328863382

Training epoch-18 batch-137
Running loss of epoch-18 batch-137 = 0.001823466969653964

Training epoch-18 batch-138
Running loss of epoch-18 batch-138 = 0.001957446802407503

Training epoch-18 batch-139
Running loss of epoch-18 batch-139 = 0.0016752061201259494

Training epoch-18 batch-140
Running loss of epoch-18 batch-140 = 0.002457326278090477

Training epoch-18 batch-141
Running loss of epoch-18 batch-141 = 0.0010989510919898748

Training epoch-18 batch-142
Running loss of epoch-18 batch-142 = 0.0014059937093406916

Training epoch-18 batch-143
Running loss of epoch-18 batch-143 = 0.0034404504112899303

Training epoch-18 batch-144
Running loss of epoch-18 batch-144 = 0.0021579565946012735

Training epoch-18 batch-145
Running loss of epoch-18 batch-145 = 0.0016128598945215344

Training epoch-18 batch-146
Running loss of epoch-18 batch-146 = 0.0013070370769128203

Training epoch-18 batch-147
Running loss of epoch-18 batch-147 = 0.0010209960164502263

Training epoch-18 batch-148
Running loss of epoch-18 batch-148 = 0.0013044888619333506

Training epoch-18 batch-149
Running loss of epoch-18 batch-149 = 0.001025320729240775

Training epoch-18 batch-150
Running loss of epoch-18 batch-150 = 0.0012407208560034633

Training epoch-18 batch-151
Running loss of epoch-18 batch-151 = 0.0012959606247022748

Training epoch-18 batch-152
Running loss of epoch-18 batch-152 = 0.0019010950345546007

Training epoch-18 batch-153
Running loss of epoch-18 batch-153 = 0.0015224528033286333

Training epoch-18 batch-154
Running loss of epoch-18 batch-154 = 0.0013388439547270536

Training epoch-18 batch-155
Running loss of epoch-18 batch-155 = 0.0021899882704019547

Training epoch-18 batch-156
Running loss of epoch-18 batch-156 = 0.002990260487422347

Training epoch-18 batch-157
Running loss of epoch-18 batch-157 = 0.006450014188885689

Finished training epoch-18.



Average train loss at epoch-18 = 0.0019160393372178077

Started Evaluation

Average val loss at epoch-18 = 0.883481420705625

Accuracy for classes:
Accuracy for class equals is: 94.55 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 92.79 %
Accuracy for class onCreate is: 89.77 %
Accuracy for class toString is: 82.25 %
Accuracy for class run is: 55.25 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 42.15 %
Accuracy for class execute is: 49.80 %
Accuracy for class get is: 74.10 %

Overall Accuracy = 80.81 %

Finished Evaluation



Started training epoch-19


Training epoch-19 batch-1
Running loss of epoch-19 batch-1 = 0.0012392443604767323

Training epoch-19 batch-2
Running loss of epoch-19 batch-2 = 0.0025498676113784313

Training epoch-19 batch-3
Running loss of epoch-19 batch-3 = 0.0006417271215468645

Training epoch-19 batch-4
Running loss of epoch-19 batch-4 = 0.0020845376420766115

Training epoch-19 batch-5
Running loss of epoch-19 batch-5 = 0.0028352774679660797

Training epoch-19 batch-6
Running loss of epoch-19 batch-6 = 0.0007546655833721161

Training epoch-19 batch-7
Running loss of epoch-19 batch-7 = 0.001696577062830329

Training epoch-19 batch-8
Running loss of epoch-19 batch-8 = 0.003535304917022586

Training epoch-19 batch-9
Running loss of epoch-19 batch-9 = 0.0036286627873778343

Training epoch-19 batch-10
Running loss of epoch-19 batch-10 = 0.0009898673743009567

Training epoch-19 batch-11
Running loss of epoch-19 batch-11 = 0.0012779947137460113

Training epoch-19 batch-12
Running loss of epoch-19 batch-12 = 0.0012065739138051867

Training epoch-19 batch-13
Running loss of epoch-19 batch-13 = 0.0017334105214104056

Training epoch-19 batch-14
Running loss of epoch-19 batch-14 = 0.001989188836887479

Training epoch-19 batch-15
Running loss of epoch-19 batch-15 = 0.002805196214467287

Training epoch-19 batch-16
Running loss of epoch-19 batch-16 = 0.0008272737031802535

Training epoch-19 batch-17
Running loss of epoch-19 batch-17 = 0.0011786801042035222

Training epoch-19 batch-18
Running loss of epoch-19 batch-18 = 0.0021129189990460873

Training epoch-19 batch-19
Running loss of epoch-19 batch-19 = 0.0008116322569549084

Training epoch-19 batch-20
Running loss of epoch-19 batch-20 = 0.0018609878607094288

Training epoch-19 batch-21
Running loss of epoch-19 batch-21 = 0.0018611920531839132

Training epoch-19 batch-22
Running loss of epoch-19 batch-22 = 0.0016021538758650422

Training epoch-19 batch-23
Running loss of epoch-19 batch-23 = 0.0012218824122101068

Training epoch-19 batch-24
Running loss of epoch-19 batch-24 = 0.0007565738633275032

Training epoch-19 batch-25
Running loss of epoch-19 batch-25 = 0.0024451622739434242

Training epoch-19 batch-26
Running loss of epoch-19 batch-26 = 0.000726517871953547

Training epoch-19 batch-27
Running loss of epoch-19 batch-27 = 0.0011558170663192868

Training epoch-19 batch-28
Running loss of epoch-19 batch-28 = 0.0029798741452395916

Training epoch-19 batch-29
Running loss of epoch-19 batch-29 = 0.002661128994077444

Training epoch-19 batch-30
Running loss of epoch-19 batch-30 = 0.0005479953251779079

Training epoch-19 batch-31
Running loss of epoch-19 batch-31 = 0.0013936416944488883

Training epoch-19 batch-32
Running loss of epoch-19 batch-32 = 0.0005891717737540603

Training epoch-19 batch-33
Running loss of epoch-19 batch-33 = 0.0010507142869755626

Training epoch-19 batch-34
Running loss of epoch-19 batch-34 = 0.0017980068223550916

Training epoch-19 batch-35
Running loss of epoch-19 batch-35 = 0.0008469449821859598

Training epoch-19 batch-36
Running loss of epoch-19 batch-36 = 0.0031081889756023884

Training epoch-19 batch-37
Running loss of epoch-19 batch-37 = 0.002310649724677205

Training epoch-19 batch-38
Running loss of epoch-19 batch-38 = 0.0025551009457558393

Training epoch-19 batch-39
Running loss of epoch-19 batch-39 = 0.0016026367666199803

Training epoch-19 batch-40
Running loss of epoch-19 batch-40 = 0.0017957359086722136

Training epoch-19 batch-41
Running loss of epoch-19 batch-41 = 0.004595374688506126

Training epoch-19 batch-42
Running loss of epoch-19 batch-42 = 0.0006302269175648689

Training epoch-19 batch-43
Running loss of epoch-19 batch-43 = 0.000947082880884409

Training epoch-19 batch-44
Running loss of epoch-19 batch-44 = 0.0015744547126814723

Training epoch-19 batch-45
Running loss of epoch-19 batch-45 = 0.0010929624550044537

Training epoch-19 batch-46
Running loss of epoch-19 batch-46 = 0.0020826568361371756

Training epoch-19 batch-47
Running loss of epoch-19 batch-47 = 0.0009298764634877443

Training epoch-19 batch-48
Running loss of epoch-19 batch-48 = 0.0018227430991828442

Training epoch-19 batch-49
Running loss of epoch-19 batch-49 = 0.0014813177986070514

Training epoch-19 batch-50
Running loss of epoch-19 batch-50 = 0.002978159114718437

Training epoch-19 batch-51
Running loss of epoch-19 batch-51 = 0.0005933964857831597

Training epoch-19 batch-52
Running loss of epoch-19 batch-52 = 0.001294974354095757

Training epoch-19 batch-53
Running loss of epoch-19 batch-53 = 0.001239870791323483

Training epoch-19 batch-54
Running loss of epoch-19 batch-54 = 0.001135924132540822

Training epoch-19 batch-55
Running loss of epoch-19 batch-55 = 0.0008651701500639319

Training epoch-19 batch-56
Running loss of epoch-19 batch-56 = 0.0008853360777720809

Training epoch-19 batch-57
Running loss of epoch-19 batch-57 = 0.002469603205099702

Training epoch-19 batch-58
Running loss of epoch-19 batch-58 = 0.001307277474552393

Training epoch-19 batch-59
Running loss of epoch-19 batch-59 = 0.0006444395985454321

Training epoch-19 batch-60
Running loss of epoch-19 batch-60 = 0.001270701759494841

Training epoch-19 batch-61
Running loss of epoch-19 batch-61 = 0.001502647646702826

Training epoch-19 batch-62
Running loss of epoch-19 batch-62 = 0.0024379161186516285

Training epoch-19 batch-63
Running loss of epoch-19 batch-63 = 0.001019687857478857

Training epoch-19 batch-64
Running loss of epoch-19 batch-64 = 0.0011947789462283254

Training epoch-19 batch-65
Running loss of epoch-19 batch-65 = 0.002136484021320939

Training epoch-19 batch-66
Running loss of epoch-19 batch-66 = 0.0013408000813797116

Training epoch-19 batch-67
Running loss of epoch-19 batch-67 = 0.0001703076995909214

Training epoch-19 batch-68
Running loss of epoch-19 batch-68 = 0.0015694817993789911

Training epoch-19 batch-69
Running loss of epoch-19 batch-69 = 0.0020784204825758934

Training epoch-19 batch-70
Running loss of epoch-19 batch-70 = 0.0017371600260958076

Training epoch-19 batch-71
Running loss of epoch-19 batch-71 = 0.0007954410975798965

Training epoch-19 batch-72
Running loss of epoch-19 batch-72 = 0.002303431509062648

Training epoch-19 batch-73
Running loss of epoch-19 batch-73 = 0.0012250837171450257

Training epoch-19 batch-74
Running loss of epoch-19 batch-74 = 0.0025639631785452366

Training epoch-19 batch-75
Running loss of epoch-19 batch-75 = 0.001505927531979978

Training epoch-19 batch-76
Running loss of epoch-19 batch-76 = 0.0005206787027418613

Training epoch-19 batch-77
Running loss of epoch-19 batch-77 = 0.0008211946114897728

Training epoch-19 batch-78
Running loss of epoch-19 batch-78 = 0.0015201043570414186

Training epoch-19 batch-79
Running loss of epoch-19 batch-79 = 0.0005815732292830944

Training epoch-19 batch-80
Running loss of epoch-19 batch-80 = 0.0009393314830958843

Training epoch-19 batch-81
Running loss of epoch-19 batch-81 = 0.001967279938980937

Training epoch-19 batch-82
Running loss of epoch-19 batch-82 = 0.0032089927699416876

Training epoch-19 batch-83
Running loss of epoch-19 batch-83 = 0.0018460644641891122

Training epoch-19 batch-84
Running loss of epoch-19 batch-84 = 0.003010063199326396

Training epoch-19 batch-85
Running loss of epoch-19 batch-85 = 0.0013959826901555061

Training epoch-19 batch-86
Running loss of epoch-19 batch-86 = 0.00088082579895854

Training epoch-19 batch-87
Running loss of epoch-19 batch-87 = 0.0010203680722042918

Training epoch-19 batch-88
Running loss of epoch-19 batch-88 = 0.0008297829190269113

Training epoch-19 batch-89
Running loss of epoch-19 batch-89 = 0.0019281863933429122

Training epoch-19 batch-90
Running loss of epoch-19 batch-90 = 0.0018997431034222245

Training epoch-19 batch-91
Running loss of epoch-19 batch-91 = 0.0015060880687087774

Training epoch-19 batch-92
Running loss of epoch-19 batch-92 = 0.0017378724878653884

Training epoch-19 batch-93
Running loss of epoch-19 batch-93 = 0.0024040760472416878

Training epoch-19 batch-94
Running loss of epoch-19 batch-94 = 0.0004226446617394686

Training epoch-19 batch-95
Running loss of epoch-19 batch-95 = 0.0017371036810800433

Training epoch-19 batch-96
Running loss of epoch-19 batch-96 = 0.0012402130523696542

Training epoch-19 batch-97
Running loss of epoch-19 batch-97 = 0.0006769833853468299

Training epoch-19 batch-98
Running loss of epoch-19 batch-98 = 0.002653644187375903

Training epoch-19 batch-99
Running loss of epoch-19 batch-99 = 0.0027298135682940483

Training epoch-19 batch-100
Running loss of epoch-19 batch-100 = 0.001505427761003375

Training epoch-19 batch-101
Running loss of epoch-19 batch-101 = 0.0008427557186223567

Training epoch-19 batch-102
Running loss of epoch-19 batch-102 = 0.0020688562653958797

Training epoch-19 batch-103
Running loss of epoch-19 batch-103 = 0.0023239897564053535

Training epoch-19 batch-104
Running loss of epoch-19 batch-104 = 0.0012249544961377978

Training epoch-19 batch-105
Running loss of epoch-19 batch-105 = 0.0026419037021696568

Training epoch-19 batch-106
Running loss of epoch-19 batch-106 = 0.00214237067848444

Training epoch-19 batch-107
Running loss of epoch-19 batch-107 = 0.0009919804288074374

Training epoch-19 batch-108
Running loss of epoch-19 batch-108 = 0.0009531606920063496

Training epoch-19 batch-109
Running loss of epoch-19 batch-109 = 0.0006255784537643194

Training epoch-19 batch-110
Running loss of epoch-19 batch-110 = 0.0012859127018600702

Training epoch-19 batch-111
Running loss of epoch-19 batch-111 = 0.0026176448445767164

Training epoch-19 batch-112
Running loss of epoch-19 batch-112 = 0.0023618778213858604

Training epoch-19 batch-113
Running loss of epoch-19 batch-113 = 0.0022407148499041796

Training epoch-19 batch-114
Running loss of epoch-19 batch-114 = 0.0019367109052836895

Training epoch-19 batch-115
Running loss of epoch-19 batch-115 = 0.000687560997903347

Training epoch-19 batch-116
Running loss of epoch-19 batch-116 = 0.001019987859763205

Training epoch-19 batch-117
Running loss of epoch-19 batch-117 = 0.0015173092251643538

Training epoch-19 batch-118
Running loss of epoch-19 batch-118 = 0.0008509912295266986

Training epoch-19 batch-119
Running loss of epoch-19 batch-119 = 0.0020064115524291992

Training epoch-19 batch-120
Running loss of epoch-19 batch-120 = 0.0024905595928430557

Training epoch-19 batch-121
Running loss of epoch-19 batch-121 = 0.0013756370171904564

Training epoch-19 batch-122
Running loss of epoch-19 batch-122 = 0.0007264292798936367

Training epoch-19 batch-123
Running loss of epoch-19 batch-123 = 0.0037646109703928232

Training epoch-19 batch-124
Running loss of epoch-19 batch-124 = 0.0011554800439625978

Training epoch-19 batch-125
Running loss of epoch-19 batch-125 = 0.0019665230065584183

Training epoch-19 batch-126
Running loss of epoch-19 batch-126 = 0.001574606285430491

Training epoch-19 batch-127
Running loss of epoch-19 batch-127 = 0.0037099404726177454

Training epoch-19 batch-128
Running loss of epoch-19 batch-128 = 0.000991868320852518

Training epoch-19 batch-129
Running loss of epoch-19 batch-129 = 0.003651145612820983

Training epoch-19 batch-130
Running loss of epoch-19 batch-130 = 0.0015137407463043928

Training epoch-19 batch-131
Running loss of epoch-19 batch-131 = 0.003242337377741933

Training epoch-19 batch-132
Running loss of epoch-19 batch-132 = 0.0026852635201066732

Training epoch-19 batch-133
Running loss of epoch-19 batch-133 = 0.0037740529514849186

Training epoch-19 batch-134
Running loss of epoch-19 batch-134 = 0.002575609367340803

Training epoch-19 batch-135
Running loss of epoch-19 batch-135 = 0.0019752250518649817

Training epoch-19 batch-136
Running loss of epoch-19 batch-136 = 0.001977939158678055

Training epoch-19 batch-137
Running loss of epoch-19 batch-137 = 0.0006106413202360272

Training epoch-19 batch-138
Running loss of epoch-19 batch-138 = 0.0013718861155211926

Training epoch-19 batch-139
Running loss of epoch-19 batch-139 = 0.0017699503805488348

Training epoch-19 batch-140
Running loss of epoch-19 batch-140 = 0.002440409502014518

Training epoch-19 batch-141
Running loss of epoch-19 batch-141 = 0.002837017411366105

Training epoch-19 batch-142
Running loss of epoch-19 batch-142 = 0.0016637390945106745

Training epoch-19 batch-143
Running loss of epoch-19 batch-143 = 0.002313162200152874

Training epoch-19 batch-144
Running loss of epoch-19 batch-144 = 0.0020130635239183903

Training epoch-19 batch-145
Running loss of epoch-19 batch-145 = 0.0016232504276558757

Training epoch-19 batch-146
Running loss of epoch-19 batch-146 = 0.0015232510631904006

Training epoch-19 batch-147
Running loss of epoch-19 batch-147 = 0.000908954069018364

Training epoch-19 batch-148
Running loss of epoch-19 batch-148 = 0.0024940739385783672

Training epoch-19 batch-149
Running loss of epoch-19 batch-149 = 0.0016700769774615765

Training epoch-19 batch-150
Running loss of epoch-19 batch-150 = 0.0010347336065024137

Training epoch-19 batch-151
Running loss of epoch-19 batch-151 = 0.0018871108768507838

Training epoch-19 batch-152
Running loss of epoch-19 batch-152 = 0.0005887089064344764

Training epoch-19 batch-153
Running loss of epoch-19 batch-153 = 0.0016964871902018785

Training epoch-19 batch-154
Running loss of epoch-19 batch-154 = 0.002262044232338667

Training epoch-19 batch-155
Running loss of epoch-19 batch-155 = 0.003704889677464962

Training epoch-19 batch-156
Running loss of epoch-19 batch-156 = 0.001918381662108004

Training epoch-19 batch-157
Running loss of epoch-19 batch-157 = 0.0009472854435443878

Finished training epoch-19.



Average train loss at epoch-19 = 0.0017103109929710626

Started Evaluation

Average val loss at epoch-19 = 1.071232294941039

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 90.98 %
Accuracy for class setUp is: 78.52 %
Accuracy for class onCreate is: 91.58 %
Accuracy for class toString is: 80.55 %
Accuracy for class run is: 65.07 %
Accuracy for class hashCode is: 97.38 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 65.46 %
Accuracy for class get is: 38.72 %

Overall Accuracy = 78.67 %

Finished Evaluation



Started training epoch-20


Training epoch-20 batch-1
Running loss of epoch-20 batch-1 = 0.002450954169034958

Training epoch-20 batch-2
Running loss of epoch-20 batch-2 = 0.002258813474327326

Training epoch-20 batch-3
Running loss of epoch-20 batch-3 = 0.0035357605665922165

Training epoch-20 batch-4
Running loss of epoch-20 batch-4 = 0.0016670484328642488

Training epoch-20 batch-5
Running loss of epoch-20 batch-5 = 0.0007467921823263168

Training epoch-20 batch-6
Running loss of epoch-20 batch-6 = 0.0005109759513288736

Training epoch-20 batch-7
Running loss of epoch-20 batch-7 = 0.0025543682277202606

Training epoch-20 batch-8
Running loss of epoch-20 batch-8 = 0.002114086179062724

Training epoch-20 batch-9
Running loss of epoch-20 batch-9 = 0.0028424584306776524

Training epoch-20 batch-10
Running loss of epoch-20 batch-10 = 0.0018792316550388932

Training epoch-20 batch-11
Running loss of epoch-20 batch-11 = 0.002449194435030222

Training epoch-20 batch-12
Running loss of epoch-20 batch-12 = 0.0006186555838212371

Training epoch-20 batch-13
Running loss of epoch-20 batch-13 = 0.0033853896893560886

Training epoch-20 batch-14
Running loss of epoch-20 batch-14 = 0.00462737213820219

Training epoch-20 batch-15
Running loss of epoch-20 batch-15 = 0.007074378430843353

Training epoch-20 batch-16
Running loss of epoch-20 batch-16 = 0.0031756062526255846

Training epoch-20 batch-17
Running loss of epoch-20 batch-17 = 0.0018236414762213826

Training epoch-20 batch-18
Running loss of epoch-20 batch-18 = 0.005933898501098156

Training epoch-20 batch-19
Running loss of epoch-20 batch-19 = 0.0028211832977831364

Training epoch-20 batch-20
Running loss of epoch-20 batch-20 = 0.006352575961500406

Training epoch-20 batch-21
Running loss of epoch-20 batch-21 = 0.003419966669753194

Training epoch-20 batch-22
Running loss of epoch-20 batch-22 = 0.001127519877627492

Training epoch-20 batch-23
Running loss of epoch-20 batch-23 = 0.0004920440260320902

Training epoch-20 batch-24
Running loss of epoch-20 batch-24 = 0.003629945684224367

Training epoch-20 batch-25
Running loss of epoch-20 batch-25 = 0.003750968025997281

Training epoch-20 batch-26
Running loss of epoch-20 batch-26 = 0.004715582821518183

Training epoch-20 batch-27
Running loss of epoch-20 batch-27 = 0.0033866846933960915

Training epoch-20 batch-28
Running loss of epoch-20 batch-28 = 0.0020993868820369244

Training epoch-20 batch-29
Running loss of epoch-20 batch-29 = 0.0009934657718986273

Training epoch-20 batch-30
Running loss of epoch-20 batch-30 = 0.003830361645668745

Training epoch-20 batch-31
Running loss of epoch-20 batch-31 = 0.0025432512629777193

Training epoch-20 batch-32
Running loss of epoch-20 batch-32 = 0.002483728341758251

Training epoch-20 batch-33
Running loss of epoch-20 batch-33 = 0.0017403375823050737

Training epoch-20 batch-34
Running loss of epoch-20 batch-34 = 0.0036976353731006384

Training epoch-20 batch-35
Running loss of epoch-20 batch-35 = 0.002072917064651847

Training epoch-20 batch-36
Running loss of epoch-20 batch-36 = 0.0023978811223059893

Training epoch-20 batch-37
Running loss of epoch-20 batch-37 = 0.0019772793166339397

Training epoch-20 batch-38
Running loss of epoch-20 batch-38 = 0.0018116532592102885

Training epoch-20 batch-39
Running loss of epoch-20 batch-39 = 0.0012137629091739655

Training epoch-20 batch-40
Running loss of epoch-20 batch-40 = 0.003731356468051672

Training epoch-20 batch-41
Running loss of epoch-20 batch-41 = 0.004326787777245045

Training epoch-20 batch-42
Running loss of epoch-20 batch-42 = 0.0040418123826384544

Training epoch-20 batch-43
Running loss of epoch-20 batch-43 = 0.0018361924448981881

Training epoch-20 batch-44
Running loss of epoch-20 batch-44 = 0.0027455666568130255

Training epoch-20 batch-45
Running loss of epoch-20 batch-45 = 0.0005449947202578187

Training epoch-20 batch-46
Running loss of epoch-20 batch-46 = 0.00410697003826499

Training epoch-20 batch-47
Running loss of epoch-20 batch-47 = 0.0017126987222582102

Training epoch-20 batch-48
Running loss of epoch-20 batch-48 = 0.002095041796565056

Training epoch-20 batch-49
Running loss of epoch-20 batch-49 = 0.004004792310297489

Training epoch-20 batch-50
Running loss of epoch-20 batch-50 = 0.00631183385848999

Training epoch-20 batch-51
Running loss of epoch-20 batch-51 = 0.0023421780206263065

Training epoch-20 batch-52
Running loss of epoch-20 batch-52 = 0.0012344996212050319

Training epoch-20 batch-53
Running loss of epoch-20 batch-53 = 0.00214423518627882

Training epoch-20 batch-54
Running loss of epoch-20 batch-54 = 0.0015826082089915872

Training epoch-20 batch-55
Running loss of epoch-20 batch-55 = 0.0028123194351792336

Training epoch-20 batch-56
Running loss of epoch-20 batch-56 = 0.003804342821240425

Training epoch-20 batch-57
Running loss of epoch-20 batch-57 = 0.0005439753876999021

Training epoch-20 batch-58
Running loss of epoch-20 batch-58 = 0.0019687984604388475

Training epoch-20 batch-59
Running loss of epoch-20 batch-59 = 0.0025162810925394297

Training epoch-20 batch-60
Running loss of epoch-20 batch-60 = 0.002419788157567382

Training epoch-20 batch-61
Running loss of epoch-20 batch-61 = 0.003501423867419362

Training epoch-20 batch-62
Running loss of epoch-20 batch-62 = 0.0029948430601507425

Training epoch-20 batch-63
Running loss of epoch-20 batch-63 = 0.0016810231609269977

Training epoch-20 batch-64
Running loss of epoch-20 batch-64 = 0.0015823276480659842

Training epoch-20 batch-65
Running loss of epoch-20 batch-65 = 0.0023671803064644337

Training epoch-20 batch-66
Running loss of epoch-20 batch-66 = 0.0036419278476387262

Training epoch-20 batch-67
Running loss of epoch-20 batch-67 = 0.002888926537707448

Training epoch-20 batch-68
Running loss of epoch-20 batch-68 = 0.0007088017882779241

Training epoch-20 batch-69
Running loss of epoch-20 batch-69 = 0.002365334890782833

Training epoch-20 batch-70
Running loss of epoch-20 batch-70 = 0.003342728829011321

Training epoch-20 batch-71
Running loss of epoch-20 batch-71 = 0.0013911683345213532

Training epoch-20 batch-72
Running loss of epoch-20 batch-72 = 0.0032034360338002443

Training epoch-20 batch-73
Running loss of epoch-20 batch-73 = 0.0015295458724722266

Training epoch-20 batch-74
Running loss of epoch-20 batch-74 = 0.0014106559101492167

Training epoch-20 batch-75
Running loss of epoch-20 batch-75 = 0.00172340776771307

Training epoch-20 batch-76
Running loss of epoch-20 batch-76 = 0.0008174193790182471

Training epoch-20 batch-77
Running loss of epoch-20 batch-77 = 0.0025891524273902178

Training epoch-20 batch-78
Running loss of epoch-20 batch-78 = 0.004358479753136635

Training epoch-20 batch-79
Running loss of epoch-20 batch-79 = 0.003792481031268835

Training epoch-20 batch-80
Running loss of epoch-20 batch-80 = 0.0037245589774101973

Training epoch-20 batch-81
Running loss of epoch-20 batch-81 = 0.003351354505866766

Training epoch-20 batch-82
Running loss of epoch-20 batch-82 = 0.0033451260533183813

Training epoch-20 batch-83
Running loss of epoch-20 batch-83 = 0.003482992062345147

Training epoch-20 batch-84
Running loss of epoch-20 batch-84 = 0.002827050630003214

Training epoch-20 batch-85
Running loss of epoch-20 batch-85 = 0.0020013509783893824

Training epoch-20 batch-86
Running loss of epoch-20 batch-86 = 0.0012605551164597273

Training epoch-20 batch-87
Running loss of epoch-20 batch-87 = 0.003279566764831543

Training epoch-20 batch-88
Running loss of epoch-20 batch-88 = 0.001194958807900548

Training epoch-20 batch-89
Running loss of epoch-20 batch-89 = 0.003267505671828985

Training epoch-20 batch-90
Running loss of epoch-20 batch-90 = 0.0019157584756612778

Training epoch-20 batch-91
Running loss of epoch-20 batch-91 = 0.0019398770527914166

Training epoch-20 batch-92
Running loss of epoch-20 batch-92 = 0.002408635104075074

Training epoch-20 batch-93
Running loss of epoch-20 batch-93 = 0.003774535609409213

Training epoch-20 batch-94
Running loss of epoch-20 batch-94 = 0.0017932228511199355

Training epoch-20 batch-95
Running loss of epoch-20 batch-95 = 0.0009057832648977637

Training epoch-20 batch-96
Running loss of epoch-20 batch-96 = 0.00047643075231462717

Training epoch-20 batch-97
Running loss of epoch-20 batch-97 = 0.001710114418528974

Training epoch-20 batch-98
Running loss of epoch-20 batch-98 = 0.001536236028186977

Training epoch-20 batch-99
Running loss of epoch-20 batch-99 = 0.0020086830481886864

Training epoch-20 batch-100
Running loss of epoch-20 batch-100 = 0.0020558724645525217

Training epoch-20 batch-101
Running loss of epoch-20 batch-101 = 0.00362209789454937

Training epoch-20 batch-102
Running loss of epoch-20 batch-102 = 0.0007935103494673967

Training epoch-20 batch-103
Running loss of epoch-20 batch-103 = 0.0014250584645196795

Training epoch-20 batch-104
Running loss of epoch-20 batch-104 = 0.002123808255419135

Training epoch-20 batch-105
Running loss of epoch-20 batch-105 = 0.001848652958869934

Training epoch-20 batch-106
Running loss of epoch-20 batch-106 = 0.0007266717730090022

Training epoch-20 batch-107
Running loss of epoch-20 batch-107 = 0.003706474555656314

Training epoch-20 batch-108
Running loss of epoch-20 batch-108 = 0.0008866891730576754

Training epoch-20 batch-109
Running loss of epoch-20 batch-109 = 0.001136038568802178

Training epoch-20 batch-110
Running loss of epoch-20 batch-110 = 0.0011138609843328595

Training epoch-20 batch-111
Running loss of epoch-20 batch-111 = 0.004078321158885956

Training epoch-20 batch-112
Running loss of epoch-20 batch-112 = 0.0013776866253465414

Training epoch-20 batch-113
Running loss of epoch-20 batch-113 = 0.0016150707378983498

Training epoch-20 batch-114
Running loss of epoch-20 batch-114 = 0.001362161012366414

Training epoch-20 batch-115
Running loss of epoch-20 batch-115 = 0.002394272480159998

Training epoch-20 batch-116
Running loss of epoch-20 batch-116 = 0.0014118531253188848

Training epoch-20 batch-117
Running loss of epoch-20 batch-117 = 0.0009708265424706042

Training epoch-20 batch-118
Running loss of epoch-20 batch-118 = 0.0012847554171457887

Training epoch-20 batch-119
Running loss of epoch-20 batch-119 = 0.0006048990180715919

Training epoch-20 batch-120
Running loss of epoch-20 batch-120 = 0.002788995625451207

Training epoch-20 batch-121
Running loss of epoch-20 batch-121 = 0.0017273507546633482

Training epoch-20 batch-122
Running loss of epoch-20 batch-122 = 0.0023771836422383785

Training epoch-20 batch-123
Running loss of epoch-20 batch-123 = 0.0019894668366760015

Training epoch-20 batch-124
Running loss of epoch-20 batch-124 = 0.003101668320596218

Training epoch-20 batch-125
Running loss of epoch-20 batch-125 = 0.002167478669434786

Training epoch-20 batch-126
Running loss of epoch-20 batch-126 = 0.0010478683980181813

Training epoch-20 batch-127
Running loss of epoch-20 batch-127 = 0.0008622055174782872

Training epoch-20 batch-128
Running loss of epoch-20 batch-128 = 0.000890089781023562

Training epoch-20 batch-129
Running loss of epoch-20 batch-129 = 0.0014388891868293285

Training epoch-20 batch-130
Running loss of epoch-20 batch-130 = 0.0025788401253521442

Training epoch-20 batch-131
Running loss of epoch-20 batch-131 = 0.0024814375210553408

Training epoch-20 batch-132
Running loss of epoch-20 batch-132 = 0.0016612118342891335

Training epoch-20 batch-133
Running loss of epoch-20 batch-133 = 0.0019415626302361488

Training epoch-20 batch-134
Running loss of epoch-20 batch-134 = 0.002443983918055892

Training epoch-20 batch-135
Running loss of epoch-20 batch-135 = 0.0008699046447873116

Training epoch-20 batch-136
Running loss of epoch-20 batch-136 = 0.0025746566243469715

Training epoch-20 batch-137
Running loss of epoch-20 batch-137 = 0.003670573467388749

Training epoch-20 batch-138
Running loss of epoch-20 batch-138 = 0.002368559595197439

Training epoch-20 batch-139
Running loss of epoch-20 batch-139 = 0.002130369655787945

Training epoch-20 batch-140
Running loss of epoch-20 batch-140 = 0.0013362544123083353

Training epoch-20 batch-141
Running loss of epoch-20 batch-141 = 0.0017086075386032462

Training epoch-20 batch-142
Running loss of epoch-20 batch-142 = 0.003802750026807189

Training epoch-20 batch-143
Running loss of epoch-20 batch-143 = 0.0021767495200037956

Training epoch-20 batch-144
Running loss of epoch-20 batch-144 = 0.001256638439372182

Training epoch-20 batch-145
Running loss of epoch-20 batch-145 = 0.004159686621278524

Training epoch-20 batch-146
Running loss of epoch-20 batch-146 = 0.0027019206900149584

Training epoch-20 batch-147
Running loss of epoch-20 batch-147 = 0.00104416289832443

Training epoch-20 batch-148
Running loss of epoch-20 batch-148 = 0.005081571638584137

Training epoch-20 batch-149
Running loss of epoch-20 batch-149 = 0.003084162948653102

Training epoch-20 batch-150
Running loss of epoch-20 batch-150 = 0.0022753700613975525

Training epoch-20 batch-151
Running loss of epoch-20 batch-151 = 0.002019492443650961

Training epoch-20 batch-152
Running loss of epoch-20 batch-152 = 0.003571332897990942

Training epoch-20 batch-153
Running loss of epoch-20 batch-153 = 0.0037717646919190884

Training epoch-20 batch-154
Running loss of epoch-20 batch-154 = 0.002943004947155714

Training epoch-20 batch-155
Running loss of epoch-20 batch-155 = 0.002134486101567745

Training epoch-20 batch-156
Running loss of epoch-20 batch-156 = 0.003671173471957445

Training epoch-20 batch-157
Running loss of epoch-20 batch-157 = 0.01234842836856842

Finished training epoch-20.



Average train loss at epoch-20 = 0.002435875240340829

Started Evaluation

Average val loss at epoch-20 = 1.0044127330582246

Accuracy for classes:
Accuracy for class equals is: 94.88 %
Accuracy for class main is: 94.43 %
Accuracy for class setUp is: 93.93 %
Accuracy for class onCreate is: 84.22 %
Accuracy for class toString is: 80.20 %
Accuracy for class run is: 47.95 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 60.54 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 66.92 %

Overall Accuracy = 79.91 %

Finished Evaluation



Started training epoch-21


Training epoch-21 batch-1
Running loss of epoch-21 batch-1 = 0.002180554671213031

Training epoch-21 batch-2
Running loss of epoch-21 batch-2 = 0.0012418790720403194

Training epoch-21 batch-3
Running loss of epoch-21 batch-3 = 0.0021691005676984787

Training epoch-21 batch-4
Running loss of epoch-21 batch-4 = 0.0036531169898808002

Training epoch-21 batch-5
Running loss of epoch-21 batch-5 = 0.0020644559990614653

Training epoch-21 batch-6
Running loss of epoch-21 batch-6 = 0.0022615869529545307

Training epoch-21 batch-7
Running loss of epoch-21 batch-7 = 0.0013112658634781837

Training epoch-21 batch-8
Running loss of epoch-21 batch-8 = 0.004642353393137455

Training epoch-21 batch-9
Running loss of epoch-21 batch-9 = 0.0016073284205049276

Training epoch-21 batch-10
Running loss of epoch-21 batch-10 = 0.0015171649865806103

Training epoch-21 batch-11
Running loss of epoch-21 batch-11 = 0.0024260261561721563

Training epoch-21 batch-12
Running loss of epoch-21 batch-12 = 0.0019194857450202107

Training epoch-21 batch-13
Running loss of epoch-21 batch-13 = 0.001947804819792509

Training epoch-21 batch-14
Running loss of epoch-21 batch-14 = 0.0021060514263808727

Training epoch-21 batch-15
Running loss of epoch-21 batch-15 = 0.000733019900508225

Training epoch-21 batch-16
Running loss of epoch-21 batch-16 = 0.0029590141493827105

Training epoch-21 batch-17
Running loss of epoch-21 batch-17 = 0.002158639021217823

Training epoch-21 batch-18
Running loss of epoch-21 batch-18 = 0.0009213757002726197

Training epoch-21 batch-19
Running loss of epoch-21 batch-19 = 0.0009343504789285362

Training epoch-21 batch-20
Running loss of epoch-21 batch-20 = 0.0016938672633841634

Training epoch-21 batch-21
Running loss of epoch-21 batch-21 = 0.0041989488527178764

Training epoch-21 batch-22
Running loss of epoch-21 batch-22 = 0.0025322348810732365

Training epoch-21 batch-23
Running loss of epoch-21 batch-23 = 0.0011200926965102553

Training epoch-21 batch-24
Running loss of epoch-21 batch-24 = 0.0012235322501510382

Training epoch-21 batch-25
Running loss of epoch-21 batch-25 = 0.001178755541332066

Training epoch-21 batch-26
Running loss of epoch-21 batch-26 = 0.0009223289089277387

Training epoch-21 batch-27
Running loss of epoch-21 batch-27 = 0.0015483874594792724

Training epoch-21 batch-28
Running loss of epoch-21 batch-28 = 0.002746100537478924

Training epoch-21 batch-29
Running loss of epoch-21 batch-29 = 0.0015475552063435316

Training epoch-21 batch-30
Running loss of epoch-21 batch-30 = 0.001134169171564281

Training epoch-21 batch-31
Running loss of epoch-21 batch-31 = 0.0018604748183861375

Training epoch-21 batch-32
Running loss of epoch-21 batch-32 = 0.0006476427661255002

Training epoch-21 batch-33
Running loss of epoch-21 batch-33 = 0.0024672462604939938

Training epoch-21 batch-34
Running loss of epoch-21 batch-34 = 0.0023203168530017138

Training epoch-21 batch-35
Running loss of epoch-21 batch-35 = 0.0017133059445768595

Training epoch-21 batch-36
Running loss of epoch-21 batch-36 = 0.0005472147604450583

Training epoch-21 batch-37
Running loss of epoch-21 batch-37 = 0.0005408653523772955

Training epoch-21 batch-38
Running loss of epoch-21 batch-38 = 0.0020624895114451647

Training epoch-21 batch-39
Running loss of epoch-21 batch-39 = 0.002833510749042034

Training epoch-21 batch-40
Running loss of epoch-21 batch-40 = 0.0015951439272612333

Training epoch-21 batch-41
Running loss of epoch-21 batch-41 = 0.0018426376627758145

Training epoch-21 batch-42
Running loss of epoch-21 batch-42 = 0.0041655125096440315

Training epoch-21 batch-43
Running loss of epoch-21 batch-43 = 0.0026463260874152184

Training epoch-21 batch-44
Running loss of epoch-21 batch-44 = 0.0018850768683478236

Training epoch-21 batch-45
Running loss of epoch-21 batch-45 = 0.0024671736173331738

Training epoch-21 batch-46
Running loss of epoch-21 batch-46 = 0.001356446067802608

Training epoch-21 batch-47
Running loss of epoch-21 batch-47 = 0.0026604230515658855

Training epoch-21 batch-48
Running loss of epoch-21 batch-48 = 0.001161014661192894

Training epoch-21 batch-49
Running loss of epoch-21 batch-49 = 0.001322618336416781

Training epoch-21 batch-50
Running loss of epoch-21 batch-50 = 0.0011836286867037416

Training epoch-21 batch-51
Running loss of epoch-21 batch-51 = 0.0009994354331865907

Training epoch-21 batch-52
Running loss of epoch-21 batch-52 = 0.002877915743738413

Training epoch-21 batch-53
Running loss of epoch-21 batch-53 = 0.005488903261721134

Training epoch-21 batch-54
Running loss of epoch-21 batch-54 = 0.000801847199909389

Training epoch-21 batch-55
Running loss of epoch-21 batch-55 = 0.0010548107093200088

Training epoch-21 batch-56
Running loss of epoch-21 batch-56 = 0.0009975971188396215

Training epoch-21 batch-57
Running loss of epoch-21 batch-57 = 0.001209608861245215

Training epoch-21 batch-58
Running loss of epoch-21 batch-58 = 0.0011091086780652404

Training epoch-21 batch-59
Running loss of epoch-21 batch-59 = 0.0008904179558157921

Training epoch-21 batch-60
Running loss of epoch-21 batch-60 = 0.0013849282404407859

Training epoch-21 batch-61
Running loss of epoch-21 batch-61 = 0.0014138580299913883

Training epoch-21 batch-62
Running loss of epoch-21 batch-62 = 0.002419209573417902

Training epoch-21 batch-63
Running loss of epoch-21 batch-63 = 0.000970398192293942

Training epoch-21 batch-64
Running loss of epoch-21 batch-64 = 0.000954945688135922

Training epoch-21 batch-65
Running loss of epoch-21 batch-65 = 0.0013886806555092335

Training epoch-21 batch-66
Running loss of epoch-21 batch-66 = 0.001046536024659872

Training epoch-21 batch-67
Running loss of epoch-21 batch-67 = 0.0014372894074767828

Training epoch-21 batch-68
Running loss of epoch-21 batch-68 = 0.00040046428330242634

Training epoch-21 batch-69
Running loss of epoch-21 batch-69 = 0.0018880835268646479

Training epoch-21 batch-70
Running loss of epoch-21 batch-70 = 0.0011121492134407163

Training epoch-21 batch-71
Running loss of epoch-21 batch-71 = 0.000632769544608891

Training epoch-21 batch-72
Running loss of epoch-21 batch-72 = 0.0017234687693417072

Training epoch-21 batch-73
Running loss of epoch-21 batch-73 = 0.0009114304557442665

Training epoch-21 batch-74
Running loss of epoch-21 batch-74 = 0.0007010295521467924

Training epoch-21 batch-75
Running loss of epoch-21 batch-75 = 0.001202394487336278

Training epoch-21 batch-76
Running loss of epoch-21 batch-76 = 0.0007256371900439262

Training epoch-21 batch-77
Running loss of epoch-21 batch-77 = 0.0007214340148493648

Training epoch-21 batch-78
Running loss of epoch-21 batch-78 = 0.0008289252873510122

Training epoch-21 batch-79
Running loss of epoch-21 batch-79 = 0.0014054653001949191

Training epoch-21 batch-80
Running loss of epoch-21 batch-80 = 0.0013692695647478104

Training epoch-21 batch-81
Running loss of epoch-21 batch-81 = 0.0012008397607132792

Training epoch-21 batch-82
Running loss of epoch-21 batch-82 = 0.0008704193169251084

Training epoch-21 batch-83
Running loss of epoch-21 batch-83 = 0.0017223700415343046

Training epoch-21 batch-84
Running loss of epoch-21 batch-84 = 0.0009424180025234818

Training epoch-21 batch-85
Running loss of epoch-21 batch-85 = 0.0008505776058882475

Training epoch-21 batch-86
Running loss of epoch-21 batch-86 = 0.0010798489674925804

Training epoch-21 batch-87
Running loss of epoch-21 batch-87 = 0.001296321046538651

Training epoch-21 batch-88
Running loss of epoch-21 batch-88 = 0.0009155705338343978

Training epoch-21 batch-89
Running loss of epoch-21 batch-89 = 0.0013340979348868132

Training epoch-21 batch-90
Running loss of epoch-21 batch-90 = 0.0003390779020264745

Training epoch-21 batch-91
Running loss of epoch-21 batch-91 = 0.0005935361841693521

Training epoch-21 batch-92
Running loss of epoch-21 batch-92 = 0.0004509943537414074

Training epoch-21 batch-93
Running loss of epoch-21 batch-93 = 0.0007265561725944281

Training epoch-21 batch-94
Running loss of epoch-21 batch-94 = 0.0009456243133172393

Training epoch-21 batch-95
Running loss of epoch-21 batch-95 = 0.001751607982441783

Training epoch-21 batch-96
Running loss of epoch-21 batch-96 = 0.0016126248519867659

Training epoch-21 batch-97
Running loss of epoch-21 batch-97 = 0.0010091401636600494

Training epoch-21 batch-98
Running loss of epoch-21 batch-98 = 0.001360521069727838

Training epoch-21 batch-99
Running loss of epoch-21 batch-99 = 0.00251352833583951

Training epoch-21 batch-100
Running loss of epoch-21 batch-100 = 0.0006130220135673881

Training epoch-21 batch-101
Running loss of epoch-21 batch-101 = 0.002288255374878645

Training epoch-21 batch-102
Running loss of epoch-21 batch-102 = 0.0010660369880497456

Training epoch-21 batch-103
Running loss of epoch-21 batch-103 = 0.00021080404985696077

Training epoch-21 batch-104
Running loss of epoch-21 batch-104 = 0.002114281989634037

Training epoch-21 batch-105
Running loss of epoch-21 batch-105 = 0.0008543455041944981

Training epoch-21 batch-106
Running loss of epoch-21 batch-106 = 0.0008093466749414802

Training epoch-21 batch-107
Running loss of epoch-21 batch-107 = 0.001385228126309812

Training epoch-21 batch-108
Running loss of epoch-21 batch-108 = 0.0005522299325093627

Training epoch-21 batch-109
Running loss of epoch-21 batch-109 = 0.003514554351568222

Training epoch-21 batch-110
Running loss of epoch-21 batch-110 = 0.0013481767382472754

Training epoch-21 batch-111
Running loss of epoch-21 batch-111 = 0.0017028028378263116

Training epoch-21 batch-112
Running loss of epoch-21 batch-112 = 0.0013493025908246636

Training epoch-21 batch-113
Running loss of epoch-21 batch-113 = 0.001403629663400352

Training epoch-21 batch-114
Running loss of epoch-21 batch-114 = 0.000646494678221643

Training epoch-21 batch-115
Running loss of epoch-21 batch-115 = 0.001216124277561903

Training epoch-21 batch-116
Running loss of epoch-21 batch-116 = 0.0007533328607678413

Training epoch-21 batch-117
Running loss of epoch-21 batch-117 = 0.0018638166366145015

Training epoch-21 batch-118
Running loss of epoch-21 batch-118 = 0.0012182709760963917

Training epoch-21 batch-119
Running loss of epoch-21 batch-119 = 0.0019121217774227262

Training epoch-21 batch-120
Running loss of epoch-21 batch-120 = 0.0010790344094857574

Training epoch-21 batch-121
Running loss of epoch-21 batch-121 = 0.00041828735265880823

Training epoch-21 batch-122
Running loss of epoch-21 batch-122 = 0.0006655544275417924

Training epoch-21 batch-123
Running loss of epoch-21 batch-123 = 0.0008593888487666845

Training epoch-21 batch-124
Running loss of epoch-21 batch-124 = 0.0024846787564456463

Training epoch-21 batch-125
Running loss of epoch-21 batch-125 = 0.001968837808817625

Training epoch-21 batch-126
Running loss of epoch-21 batch-126 = 0.0013177550863474607

Training epoch-21 batch-127
Running loss of epoch-21 batch-127 = 0.001540650730021298

Training epoch-21 batch-128
Running loss of epoch-21 batch-128 = 0.0006441968725994229

Training epoch-21 batch-129
Running loss of epoch-21 batch-129 = 0.001248691463842988

Training epoch-21 batch-130
Running loss of epoch-21 batch-130 = 0.0011459860252216458

Training epoch-21 batch-131
Running loss of epoch-21 batch-131 = 0.00018735707271844149

Training epoch-21 batch-132
Running loss of epoch-21 batch-132 = 0.0014661786844953895

Training epoch-21 batch-133
Running loss of epoch-21 batch-133 = 0.0018586477963253856

Training epoch-21 batch-134
Running loss of epoch-21 batch-134 = 0.0009959504241123796

Training epoch-21 batch-135
Running loss of epoch-21 batch-135 = 0.002344214590266347

Training epoch-21 batch-136
Running loss of epoch-21 batch-136 = 0.001532443449832499

Training epoch-21 batch-137
Running loss of epoch-21 batch-137 = 0.0028813728131353855

Training epoch-21 batch-138
Running loss of epoch-21 batch-138 = 0.0014302681665867567

Training epoch-21 batch-139
Running loss of epoch-21 batch-139 = 0.0024825092405080795

Training epoch-21 batch-140
Running loss of epoch-21 batch-140 = 0.002480266382917762

Training epoch-21 batch-141
Running loss of epoch-21 batch-141 = 0.0014099259860813618

Training epoch-21 batch-142
Running loss of epoch-21 batch-142 = 0.0009768629679456353

Training epoch-21 batch-143
Running loss of epoch-21 batch-143 = 0.001360853435471654

Training epoch-21 batch-144
Running loss of epoch-21 batch-144 = 0.0016719379927963018

Training epoch-21 batch-145
Running loss of epoch-21 batch-145 = 0.00185788469389081

Training epoch-21 batch-146
Running loss of epoch-21 batch-146 = 0.0034060252364724874

Training epoch-21 batch-147
Running loss of epoch-21 batch-147 = 0.0018139986786991358

Training epoch-21 batch-148
Running loss of epoch-21 batch-148 = 0.0029840588103979826

Training epoch-21 batch-149
Running loss of epoch-21 batch-149 = 0.0005260258913040161

Training epoch-21 batch-150
Running loss of epoch-21 batch-150 = 0.002281935652717948

Training epoch-21 batch-151
Running loss of epoch-21 batch-151 = 0.0012001546565443277

Training epoch-21 batch-152
Running loss of epoch-21 batch-152 = 0.0010597975924611092

Training epoch-21 batch-153
Running loss of epoch-21 batch-153 = 0.001157744787633419

Training epoch-21 batch-154
Running loss of epoch-21 batch-154 = 0.0019837520085275173

Training epoch-21 batch-155
Running loss of epoch-21 batch-155 = 0.002207975136116147

Training epoch-21 batch-156
Running loss of epoch-21 batch-156 = 0.0011477334192022681

Training epoch-21 batch-157
Running loss of epoch-21 batch-157 = 0.0007909946143627167

Finished training epoch-21.



Average train loss at epoch-21 = 0.0015523978631943464

Started Evaluation

Average val loss at epoch-21 = 0.927937969545287

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 97.38 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 89.55 %
Accuracy for class toString is: 80.55 %
Accuracy for class run is: 59.82 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 46.19 %
Accuracy for class execute is: 55.02 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 80.83 %

Finished Evaluation



Started training epoch-22


Training epoch-22 batch-1
Running loss of epoch-22 batch-1 = 0.0007682256400585175

Training epoch-22 batch-2
Running loss of epoch-22 batch-2 = 0.0004584752023220062

Training epoch-22 batch-3
Running loss of epoch-22 batch-3 = 0.0004640086553990841

Training epoch-22 batch-4
Running loss of epoch-22 batch-4 = 0.001767401467077434

Training epoch-22 batch-5
Running loss of epoch-22 batch-5 = 0.0010629165917634964

Training epoch-22 batch-6
Running loss of epoch-22 batch-6 = 0.0004066468682140112

Training epoch-22 batch-7
Running loss of epoch-22 batch-7 = 0.002725527621805668

Training epoch-22 batch-8
Running loss of epoch-22 batch-8 = 0.0010772210080176592

Training epoch-22 batch-9
Running loss of epoch-22 batch-9 = 0.0012919165892526507

Training epoch-22 batch-10
Running loss of epoch-22 batch-10 = 0.0020631563384085894

Training epoch-22 batch-11
Running loss of epoch-22 batch-11 = 0.0008807149715721607

Training epoch-22 batch-12
Running loss of epoch-22 batch-12 = 0.0007102236850187182

Training epoch-22 batch-13
Running loss of epoch-22 batch-13 = 0.0016011832049116492

Training epoch-22 batch-14
Running loss of epoch-22 batch-14 = 0.00096976722124964

Training epoch-22 batch-15
Running loss of epoch-22 batch-15 = 0.0005485720466822386

Training epoch-22 batch-16
Running loss of epoch-22 batch-16 = 0.00029964884743094444

Training epoch-22 batch-17
Running loss of epoch-22 batch-17 = 0.002673251088708639

Training epoch-22 batch-18
Running loss of epoch-22 batch-18 = 0.0013116529444232583

Training epoch-22 batch-19
Running loss of epoch-22 batch-19 = 0.0014436895726248622

Training epoch-22 batch-20
Running loss of epoch-22 batch-20 = 0.0013431997504085302

Training epoch-22 batch-21
Running loss of epoch-22 batch-21 = 0.001133327605202794

Training epoch-22 batch-22
Running loss of epoch-22 batch-22 = 0.00036386388819664717

Training epoch-22 batch-23
Running loss of epoch-22 batch-23 = 0.0006244595861062407

Training epoch-22 batch-24
Running loss of epoch-22 batch-24 = 0.0008362720254808664

Training epoch-22 batch-25
Running loss of epoch-22 batch-25 = 0.0006077756406739354

Training epoch-22 batch-26
Running loss of epoch-22 batch-26 = 0.000569518655538559

Training epoch-22 batch-27
Running loss of epoch-22 batch-27 = 0.0005847747670486569

Training epoch-22 batch-28
Running loss of epoch-22 batch-28 = 0.0006903923349454999

Training epoch-22 batch-29
Running loss of epoch-22 batch-29 = 0.001073320396244526

Training epoch-22 batch-30
Running loss of epoch-22 batch-30 = 0.0015989308012649417

Training epoch-22 batch-31
Running loss of epoch-22 batch-31 = 0.0004911476280540228

Training epoch-22 batch-32
Running loss of epoch-22 batch-32 = 0.0003841057186946273

Training epoch-22 batch-33
Running loss of epoch-22 batch-33 = 0.000769516103900969

Training epoch-22 batch-34
Running loss of epoch-22 batch-34 = 0.00047866348177194595

Training epoch-22 batch-35
Running loss of epoch-22 batch-35 = 0.0011119699338451028

Training epoch-22 batch-36
Running loss of epoch-22 batch-36 = 0.002014978090301156

Training epoch-22 batch-37
Running loss of epoch-22 batch-37 = 0.0006869429489597678

Training epoch-22 batch-38
Running loss of epoch-22 batch-38 = 0.000644923304207623

Training epoch-22 batch-39
Running loss of epoch-22 batch-39 = 0.000582172186113894

Training epoch-22 batch-40
Running loss of epoch-22 batch-40 = 0.002740086754783988

Training epoch-22 batch-41
Running loss of epoch-22 batch-41 = 0.0007060200441628695

Training epoch-22 batch-42
Running loss of epoch-22 batch-42 = 0.0010723822051659226

Training epoch-22 batch-43
Running loss of epoch-22 batch-43 = 0.0008164648897945881

Training epoch-22 batch-44
Running loss of epoch-22 batch-44 = 0.00024087680503726006

Training epoch-22 batch-45
Running loss of epoch-22 batch-45 = 0.000889332965016365

Training epoch-22 batch-46
Running loss of epoch-22 batch-46 = 0.0009339713724330068

Training epoch-22 batch-47
Running loss of epoch-22 batch-47 = 0.0013946264516562223

Training epoch-22 batch-48
Running loss of epoch-22 batch-48 = 0.0007591161411255598

Training epoch-22 batch-49
Running loss of epoch-22 batch-49 = 0.0012532140826806426

Training epoch-22 batch-50
Running loss of epoch-22 batch-50 = 0.001178382197394967

Training epoch-22 batch-51
Running loss of epoch-22 batch-51 = 0.0003403176087886095

Training epoch-22 batch-52
Running loss of epoch-22 batch-52 = 0.0011057744268327951

Training epoch-22 batch-53
Running loss of epoch-22 batch-53 = 0.0005330718122422695

Training epoch-22 batch-54
Running loss of epoch-22 batch-54 = 0.0009126928634941578

Training epoch-22 batch-55
Running loss of epoch-22 batch-55 = 0.000343604595400393

Training epoch-22 batch-56
Running loss of epoch-22 batch-56 = 0.0006058835424482822

Training epoch-22 batch-57
Running loss of epoch-22 batch-57 = 0.00025052379351109266

Training epoch-22 batch-58
Running loss of epoch-22 batch-58 = 0.001102672191336751

Training epoch-22 batch-59
Running loss of epoch-22 batch-59 = 0.0011960178380832076

Training epoch-22 batch-60
Running loss of epoch-22 batch-60 = 0.000435375957749784

Training epoch-22 batch-61
Running loss of epoch-22 batch-61 = 0.0024852335918694735

Training epoch-22 batch-62
Running loss of epoch-22 batch-62 = 0.0010896463645622134

Training epoch-22 batch-63
Running loss of epoch-22 batch-63 = 0.001595769077539444

Training epoch-22 batch-64
Running loss of epoch-22 batch-64 = 0.0007248214678838849

Training epoch-22 batch-65
Running loss of epoch-22 batch-65 = 0.0008110296912491322

Training epoch-22 batch-66
Running loss of epoch-22 batch-66 = 0.000718470080755651

Training epoch-22 batch-67
Running loss of epoch-22 batch-67 = 0.0008643808541819453

Training epoch-22 batch-68
Running loss of epoch-22 batch-68 = 0.0009273692267015576

Training epoch-22 batch-69
Running loss of epoch-22 batch-69 = 0.0014577178517356515

Training epoch-22 batch-70
Running loss of epoch-22 batch-70 = 0.001542026293464005

Training epoch-22 batch-71
Running loss of epoch-22 batch-71 = 0.0005659596063196659

Training epoch-22 batch-72
Running loss of epoch-22 batch-72 = 0.000684399507008493

Training epoch-22 batch-73
Running loss of epoch-22 batch-73 = 0.0009860722348093987

Training epoch-22 batch-74
Running loss of epoch-22 batch-74 = 0.0005294998409226537

Training epoch-22 batch-75
Running loss of epoch-22 batch-75 = 0.0007654364453628659

Training epoch-22 batch-76
Running loss of epoch-22 batch-76 = 0.0028524736408144236

Training epoch-22 batch-77
Running loss of epoch-22 batch-77 = 0.0009274039184674621

Training epoch-22 batch-78
Running loss of epoch-22 batch-78 = 0.00142577663064003

Training epoch-22 batch-79
Running loss of epoch-22 batch-79 = 0.001493771793320775

Training epoch-22 batch-80
Running loss of epoch-22 batch-80 = 0.0014235664857551455

Training epoch-22 batch-81
Running loss of epoch-22 batch-81 = 0.0003428563941270113

Training epoch-22 batch-82
Running loss of epoch-22 batch-82 = 0.0009328703163191676

Training epoch-22 batch-83
Running loss of epoch-22 batch-83 = 0.0007353591499850154

Training epoch-22 batch-84
Running loss of epoch-22 batch-84 = 0.0009888618951663375

Training epoch-22 batch-85
Running loss of epoch-22 batch-85 = 0.0010064757661893964

Training epoch-22 batch-86
Running loss of epoch-22 batch-86 = 0.000985112157650292

Training epoch-22 batch-87
Running loss of epoch-22 batch-87 = 0.0007609060266986489

Training epoch-22 batch-88
Running loss of epoch-22 batch-88 = 0.0015188565012067556

Training epoch-22 batch-89
Running loss of epoch-22 batch-89 = 0.000542051624506712

Training epoch-22 batch-90
Running loss of epoch-22 batch-90 = 0.0013841873733326793

Training epoch-22 batch-91
Running loss of epoch-22 batch-91 = 0.0006664278917014599

Training epoch-22 batch-92
Running loss of epoch-22 batch-92 = 0.001738211838528514

Training epoch-22 batch-93
Running loss of epoch-22 batch-93 = 0.000789647689089179

Training epoch-22 batch-94
Running loss of epoch-22 batch-94 = 0.0009490612428635359

Training epoch-22 batch-95
Running loss of epoch-22 batch-95 = 0.0014498026575893164

Training epoch-22 batch-96
Running loss of epoch-22 batch-96 = 0.0010709707858040929

Training epoch-22 batch-97
Running loss of epoch-22 batch-97 = 0.00030204153154045343

Training epoch-22 batch-98
Running loss of epoch-22 batch-98 = 0.0016813433030620217

Training epoch-22 batch-99
Running loss of epoch-22 batch-99 = 0.0008823903044685721

Training epoch-22 batch-100
Running loss of epoch-22 batch-100 = 0.0007391863036900759

Training epoch-22 batch-101
Running loss of epoch-22 batch-101 = 0.0029254560358822346

Training epoch-22 batch-102
Running loss of epoch-22 batch-102 = 0.0008351416327059269

Training epoch-22 batch-103
Running loss of epoch-22 batch-103 = 0.0016164316330105066

Training epoch-22 batch-104
Running loss of epoch-22 batch-104 = 0.0008737012976780534

Training epoch-22 batch-105
Running loss of epoch-22 batch-105 = 0.0013113385066390038

Training epoch-22 batch-106
Running loss of epoch-22 batch-106 = 0.0012874979292973876

Training epoch-22 batch-107
Running loss of epoch-22 batch-107 = 0.0012368317693471909

Training epoch-22 batch-108
Running loss of epoch-22 batch-108 = 0.0008787421975284815

Training epoch-22 batch-109
Running loss of epoch-22 batch-109 = 0.0007147560827434063

Training epoch-22 batch-110
Running loss of epoch-22 batch-110 = 0.00024649640545248985

Training epoch-22 batch-111
Running loss of epoch-22 batch-111 = 0.0012452522059902549

Training epoch-22 batch-112
Running loss of epoch-22 batch-112 = 0.0007437220774590969

Training epoch-22 batch-113
Running loss of epoch-22 batch-113 = 0.0006241557421162724

Training epoch-22 batch-114
Running loss of epoch-22 batch-114 = 0.0017854556208476424

Training epoch-22 batch-115
Running loss of epoch-22 batch-115 = 0.000951936119236052

Training epoch-22 batch-116
Running loss of epoch-22 batch-116 = 0.00119721086230129

Training epoch-22 batch-117
Running loss of epoch-22 batch-117 = 0.0006103172199800611

Training epoch-22 batch-118
Running loss of epoch-22 batch-118 = 0.0022154157049953938

Training epoch-22 batch-119
Running loss of epoch-22 batch-119 = 0.0013348945649340749

Training epoch-22 batch-120
Running loss of epoch-22 batch-120 = 0.00121084856800735

Training epoch-22 batch-121
Running loss of epoch-22 batch-121 = 0.0008113791700452566

Training epoch-22 batch-122
Running loss of epoch-22 batch-122 = 0.0016573233297094703

Training epoch-22 batch-123
Running loss of epoch-22 batch-123 = 0.0014376131584867835

Training epoch-22 batch-124
Running loss of epoch-22 batch-124 = 0.000816891435533762

Training epoch-22 batch-125
Running loss of epoch-22 batch-125 = 0.0012652339646592736

Training epoch-22 batch-126
Running loss of epoch-22 batch-126 = 0.0006617392646148801

Training epoch-22 batch-127
Running loss of epoch-22 batch-127 = 0.0007635029032826424

Training epoch-22 batch-128
Running loss of epoch-22 batch-128 = 0.0005638677394017577

Training epoch-22 batch-129
Running loss of epoch-22 batch-129 = 0.0009364531142637134

Training epoch-22 batch-130
Running loss of epoch-22 batch-130 = 0.0017567155882716179

Training epoch-22 batch-131
Running loss of epoch-22 batch-131 = 0.001418937579728663

Training epoch-22 batch-132
Running loss of epoch-22 batch-132 = 0.0012477434938773513

Training epoch-22 batch-133
Running loss of epoch-22 batch-133 = 0.0006289497250691056

Training epoch-22 batch-134
Running loss of epoch-22 batch-134 = 0.00068520731292665

Training epoch-22 batch-135
Running loss of epoch-22 batch-135 = 0.001157282618805766

Training epoch-22 batch-136
Running loss of epoch-22 batch-136 = 0.002338722813874483

Training epoch-22 batch-137
Running loss of epoch-22 batch-137 = 0.002035372192040086

Training epoch-22 batch-138
Running loss of epoch-22 batch-138 = 0.0007386497454717755

Training epoch-22 batch-139
Running loss of epoch-22 batch-139 = 0.0010419512400403619

Training epoch-22 batch-140
Running loss of epoch-22 batch-140 = 0.0005030252505093813

Training epoch-22 batch-141
Running loss of epoch-22 batch-141 = 0.0015937399584800005

Training epoch-22 batch-142
Running loss of epoch-22 batch-142 = 0.0020169527269899845

Training epoch-22 batch-143
Running loss of epoch-22 batch-143 = 0.0007927487604320049

Training epoch-22 batch-144
Running loss of epoch-22 batch-144 = 0.0004908277187496424

Training epoch-22 batch-145
Running loss of epoch-22 batch-145 = 0.0013551405863836408

Training epoch-22 batch-146
Running loss of epoch-22 batch-146 = 0.0010852301493287086

Training epoch-22 batch-147
Running loss of epoch-22 batch-147 = 0.00043219688814133406

Training epoch-22 batch-148
Running loss of epoch-22 batch-148 = 0.0027100415900349617

Training epoch-22 batch-149
Running loss of epoch-22 batch-149 = 0.003304383484646678

Training epoch-22 batch-150
Running loss of epoch-22 batch-150 = 0.0013591231545433402

Training epoch-22 batch-151
Running loss of epoch-22 batch-151 = 0.0023337379097938538

Training epoch-22 batch-152
Running loss of epoch-22 batch-152 = 0.0030082142911851406

Training epoch-22 batch-153
Running loss of epoch-22 batch-153 = 0.0025039054453372955

Training epoch-22 batch-154
Running loss of epoch-22 batch-154 = 0.002059620339423418

Training epoch-22 batch-155
Running loss of epoch-22 batch-155 = 0.0008556299144402146

Training epoch-22 batch-156
Running loss of epoch-22 batch-156 = 0.0031743100844323635

Training epoch-22 batch-157
Running loss of epoch-22 batch-157 = 0.022617517039179802

Finished training epoch-22.



Average train loss at epoch-22 = 0.0011667497880756856

Started Evaluation

Average val loss at epoch-22 = 1.027168503733839

Accuracy for classes:
Accuracy for class equals is: 96.70 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 90.82 %
Accuracy for class onCreate is: 93.82 %
Accuracy for class toString is: 79.86 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 36.10 %
Accuracy for class execute is: 39.36 %
Accuracy for class get is: 74.62 %

Overall Accuracy = 80.96 %

Finished Evaluation



Started training epoch-23


Training epoch-23 batch-1
Running loss of epoch-23 batch-1 = 0.0023797389585524797

Training epoch-23 batch-2
Running loss of epoch-23 batch-2 = 0.003411819227039814

Training epoch-23 batch-3
Running loss of epoch-23 batch-3 = 0.0011591610964387655

Training epoch-23 batch-4
Running loss of epoch-23 batch-4 = 0.0010447261156514287

Training epoch-23 batch-5
Running loss of epoch-23 batch-5 = 0.002270848723128438

Training epoch-23 batch-6
Running loss of epoch-23 batch-6 = 0.0025404926855117083

Training epoch-23 batch-7
Running loss of epoch-23 batch-7 = 0.001596458489075303

Training epoch-23 batch-8
Running loss of epoch-23 batch-8 = 0.0004054203163832426

Training epoch-23 batch-9
Running loss of epoch-23 batch-9 = 0.001034021144732833

Training epoch-23 batch-10
Running loss of epoch-23 batch-10 = 0.0006301878020167351

Training epoch-23 batch-11
Running loss of epoch-23 batch-11 = 0.0044701281003654

Training epoch-23 batch-12
Running loss of epoch-23 batch-12 = 0.00129234348423779

Training epoch-23 batch-13
Running loss of epoch-23 batch-13 = 0.0003945338539779186

Training epoch-23 batch-14
Running loss of epoch-23 batch-14 = 0.0018511698581278324

Training epoch-23 batch-15
Running loss of epoch-23 batch-15 = 0.0028975694440305233

Training epoch-23 batch-16
Running loss of epoch-23 batch-16 = 0.0029995781369507313

Training epoch-23 batch-17
Running loss of epoch-23 batch-17 = 0.0006860054563730955

Training epoch-23 batch-18
Running loss of epoch-23 batch-18 = 0.0012184502556920052

Training epoch-23 batch-19
Running loss of epoch-23 batch-19 = 0.00043497991282492876

Training epoch-23 batch-20
Running loss of epoch-23 batch-20 = 0.0008392083691433072

Training epoch-23 batch-21
Running loss of epoch-23 batch-21 = 0.0020614489912986755

Training epoch-23 batch-22
Running loss of epoch-23 batch-22 = 0.0025743464939296246

Training epoch-23 batch-23
Running loss of epoch-23 batch-23 = 0.0013721744762733579

Training epoch-23 batch-24
Running loss of epoch-23 batch-24 = 0.001917647896334529

Training epoch-23 batch-25
Running loss of epoch-23 batch-25 = 0.0007972581079229712

Training epoch-23 batch-26
Running loss of epoch-23 batch-26 = 0.002083798870444298

Training epoch-23 batch-27
Running loss of epoch-23 batch-27 = 0.001912771607749164

Training epoch-23 batch-28
Running loss of epoch-23 batch-28 = 0.0005976778920739889

Training epoch-23 batch-29
Running loss of epoch-23 batch-29 = 0.001510725007392466

Training epoch-23 batch-30
Running loss of epoch-23 batch-30 = 0.001960532506927848

Training epoch-23 batch-31
Running loss of epoch-23 batch-31 = 0.0008314340375363827

Training epoch-23 batch-32
Running loss of epoch-23 batch-32 = 0.001985205803066492

Training epoch-23 batch-33
Running loss of epoch-23 batch-33 = 0.0010246909223496914

Training epoch-23 batch-34
Running loss of epoch-23 batch-34 = 0.0009511194657534361

Training epoch-23 batch-35
Running loss of epoch-23 batch-35 = 0.002408168977126479

Training epoch-23 batch-36
Running loss of epoch-23 batch-36 = 0.0023805180098861456

Training epoch-23 batch-37
Running loss of epoch-23 batch-37 = 0.0007599069504067302

Training epoch-23 batch-38
Running loss of epoch-23 batch-38 = 0.001057015615515411

Training epoch-23 batch-39
Running loss of epoch-23 batch-39 = 0.0019514921586960554

Training epoch-23 batch-40
Running loss of epoch-23 batch-40 = 0.0006608356488868594

Training epoch-23 batch-41
Running loss of epoch-23 batch-41 = 0.0060643721371889114

Training epoch-23 batch-42
Running loss of epoch-23 batch-42 = 0.0033278875052928925

Training epoch-23 batch-43
Running loss of epoch-23 batch-43 = 0.0010306242620572448

Training epoch-23 batch-44
Running loss of epoch-23 batch-44 = 0.0010953261516988277

Training epoch-23 batch-45
Running loss of epoch-23 batch-45 = 0.0014174976386129856

Training epoch-23 batch-46
Running loss of epoch-23 batch-46 = 0.0020246412605047226

Training epoch-23 batch-47
Running loss of epoch-23 batch-47 = 0.0017681720200926065

Training epoch-23 batch-48
Running loss of epoch-23 batch-48 = 0.0008009047014638782

Training epoch-23 batch-49
Running loss of epoch-23 batch-49 = 0.001333532971329987

Training epoch-23 batch-50
Running loss of epoch-23 batch-50 = 0.002010814379900694

Training epoch-23 batch-51
Running loss of epoch-23 batch-51 = 0.0012833245564252138

Training epoch-23 batch-52
Running loss of epoch-23 batch-52 = 0.0014757710741832852

Training epoch-23 batch-53
Running loss of epoch-23 batch-53 = 0.0020694402046501637

Training epoch-23 batch-54
Running loss of epoch-23 batch-54 = 0.0012745067942887545

Training epoch-23 batch-55
Running loss of epoch-23 batch-55 = 0.0008536846144124866

Training epoch-23 batch-56
Running loss of epoch-23 batch-56 = 0.0005711984122171998

Training epoch-23 batch-57
Running loss of epoch-23 batch-57 = 0.0006235137116163969

Training epoch-23 batch-58
Running loss of epoch-23 batch-58 = 0.0005807360867038369

Training epoch-23 batch-59
Running loss of epoch-23 batch-59 = 0.0013030193513259292

Training epoch-23 batch-60
Running loss of epoch-23 batch-60 = 0.0005234800046309829

Training epoch-23 batch-61
Running loss of epoch-23 batch-61 = 0.0010311452206224203

Training epoch-23 batch-62
Running loss of epoch-23 batch-62 = 0.0016308859921991825

Training epoch-23 batch-63
Running loss of epoch-23 batch-63 = 0.002045462839305401

Training epoch-23 batch-64
Running loss of epoch-23 batch-64 = 0.0004949569702148438

Training epoch-23 batch-65
Running loss of epoch-23 batch-65 = 0.00029645301401615143

Training epoch-23 batch-66
Running loss of epoch-23 batch-66 = 0.0004439951153472066

Training epoch-23 batch-67
Running loss of epoch-23 batch-67 = 0.000591186573728919

Training epoch-23 batch-68
Running loss of epoch-23 batch-68 = 0.0008481109398417175

Training epoch-23 batch-69
Running loss of epoch-23 batch-69 = 0.0011055192444473505

Training epoch-23 batch-70
Running loss of epoch-23 batch-70 = 0.0014578582486137748

Training epoch-23 batch-71
Running loss of epoch-23 batch-71 = 0.0006731923203915358

Training epoch-23 batch-72
Running loss of epoch-23 batch-72 = 0.0021531875245273113

Training epoch-23 batch-73
Running loss of epoch-23 batch-73 = 0.001348100951872766

Training epoch-23 batch-74
Running loss of epoch-23 batch-74 = 0.0008301484631374478

Training epoch-23 batch-75
Running loss of epoch-23 batch-75 = 0.00032973301131278276

Training epoch-23 batch-76
Running loss of epoch-23 batch-76 = 0.0016971288714557886

Training epoch-23 batch-77
Running loss of epoch-23 batch-77 = 0.002109931316226721

Training epoch-23 batch-78
Running loss of epoch-23 batch-78 = 0.0006225148681551218

Training epoch-23 batch-79
Running loss of epoch-23 batch-79 = 0.0006948432419449091

Training epoch-23 batch-80
Running loss of epoch-23 batch-80 = 0.002003903966397047

Training epoch-23 batch-81
Running loss of epoch-23 batch-81 = 0.0013711827341467142

Training epoch-23 batch-82
Running loss of epoch-23 batch-82 = 0.0027712031733244658

Training epoch-23 batch-83
Running loss of epoch-23 batch-83 = 0.0019284987356513739

Training epoch-23 batch-84
Running loss of epoch-23 batch-84 = 0.0018107106443494558

Training epoch-23 batch-85
Running loss of epoch-23 batch-85 = 0.002005269518122077

Training epoch-23 batch-86
Running loss of epoch-23 batch-86 = 0.0015698206843808293

Training epoch-23 batch-87
Running loss of epoch-23 batch-87 = 0.0012892208760604262

Training epoch-23 batch-88
Running loss of epoch-23 batch-88 = 0.0005627886857837439

Training epoch-23 batch-89
Running loss of epoch-23 batch-89 = 0.0011659122537821531

Training epoch-23 batch-90
Running loss of epoch-23 batch-90 = 0.001226428896188736

Training epoch-23 batch-91
Running loss of epoch-23 batch-91 = 0.0019448591629043221

Training epoch-23 batch-92
Running loss of epoch-23 batch-92 = 0.0011667157523334026

Training epoch-23 batch-93
Running loss of epoch-23 batch-93 = 0.0005576965631917119

Training epoch-23 batch-94
Running loss of epoch-23 batch-94 = 0.0010432383278384805

Training epoch-23 batch-95
Running loss of epoch-23 batch-95 = 0.0025457078590989113

Training epoch-23 batch-96
Running loss of epoch-23 batch-96 = 0.000558274332433939

Training epoch-23 batch-97
Running loss of epoch-23 batch-97 = 0.0008346122922375798

Training epoch-23 batch-98
Running loss of epoch-23 batch-98 = 0.0008120144484564662

Training epoch-23 batch-99
Running loss of epoch-23 batch-99 = 0.0008117094403132796

Training epoch-23 batch-100
Running loss of epoch-23 batch-100 = 0.0008628377690911293

Training epoch-23 batch-101
Running loss of epoch-23 batch-101 = 0.0007446003146469593

Training epoch-23 batch-102
Running loss of epoch-23 batch-102 = 0.0009731804020702839

Training epoch-23 batch-103
Running loss of epoch-23 batch-103 = 0.0006673971656709909

Training epoch-23 batch-104
Running loss of epoch-23 batch-104 = 0.0014983187429606915

Training epoch-23 batch-105
Running loss of epoch-23 batch-105 = 0.0006664849352091551

Training epoch-23 batch-106
Running loss of epoch-23 batch-106 = 0.0004371347604319453

Training epoch-23 batch-107
Running loss of epoch-23 batch-107 = 0.0013666799059137702

Training epoch-23 batch-108
Running loss of epoch-23 batch-108 = 0.0012727597495540977

Training epoch-23 batch-109
Running loss of epoch-23 batch-109 = 0.0008172078523784876

Training epoch-23 batch-110
Running loss of epoch-23 batch-110 = 0.0009150842670351267

Training epoch-23 batch-111
Running loss of epoch-23 batch-111 = 0.0004896038444712758

Training epoch-23 batch-112
Running loss of epoch-23 batch-112 = 0.0007143121911212802

Training epoch-23 batch-113
Running loss of epoch-23 batch-113 = 0.00033312581945210695

Training epoch-23 batch-114
Running loss of epoch-23 batch-114 = 0.00033731013536453247

Training epoch-23 batch-115
Running loss of epoch-23 batch-115 = 0.0016697125975042582

Training epoch-23 batch-116
Running loss of epoch-23 batch-116 = 0.001154631027020514

Training epoch-23 batch-117
Running loss of epoch-23 batch-117 = 0.0016639191890135407

Training epoch-23 batch-118
Running loss of epoch-23 batch-118 = 0.0008328797994181514

Training epoch-23 batch-119
Running loss of epoch-23 batch-119 = 0.0010485772509127855

Training epoch-23 batch-120
Running loss of epoch-23 batch-120 = 0.00045388401485979557

Training epoch-23 batch-121
Running loss of epoch-23 batch-121 = 0.0008963247528299689

Training epoch-23 batch-122
Running loss of epoch-23 batch-122 = 0.001920133363455534

Training epoch-23 batch-123
Running loss of epoch-23 batch-123 = 0.0009477235144004226

Training epoch-23 batch-124
Running loss of epoch-23 batch-124 = 0.0012853086227551103

Training epoch-23 batch-125
Running loss of epoch-23 batch-125 = 0.0009622883517295122

Training epoch-23 batch-126
Running loss of epoch-23 batch-126 = 0.0006529097445309162

Training epoch-23 batch-127
Running loss of epoch-23 batch-127 = 0.0003678735811263323

Training epoch-23 batch-128
Running loss of epoch-23 batch-128 = 0.0017942057456821203

Training epoch-23 batch-129
Running loss of epoch-23 batch-129 = 0.001554681803099811

Training epoch-23 batch-130
Running loss of epoch-23 batch-130 = 0.001274989335797727

Training epoch-23 batch-131
Running loss of epoch-23 batch-131 = 0.0007197854574769735

Training epoch-23 batch-132
Running loss of epoch-23 batch-132 = 0.0006787038873881102

Training epoch-23 batch-133
Running loss of epoch-23 batch-133 = 0.0014250214444473386

Training epoch-23 batch-134
Running loss of epoch-23 batch-134 = 0.0006127958185970783

Training epoch-23 batch-135
Running loss of epoch-23 batch-135 = 0.000692603294737637

Training epoch-23 batch-136
Running loss of epoch-23 batch-136 = 0.0012582768686115742

Training epoch-23 batch-137
Running loss of epoch-23 batch-137 = 0.0005095241358503699

Training epoch-23 batch-138
Running loss of epoch-23 batch-138 = 0.00040445150807499886

Training epoch-23 batch-139
Running loss of epoch-23 batch-139 = 0.0006254479521885514

Training epoch-23 batch-140
Running loss of epoch-23 batch-140 = 0.0013057998148724437

Training epoch-23 batch-141
Running loss of epoch-23 batch-141 = 0.0008477970259264112

Training epoch-23 batch-142
Running loss of epoch-23 batch-142 = 0.0006402231520041823

Training epoch-23 batch-143
Running loss of epoch-23 batch-143 = 0.002182974247261882

Training epoch-23 batch-144
Running loss of epoch-23 batch-144 = 0.001722303219139576

Training epoch-23 batch-145
Running loss of epoch-23 batch-145 = 0.0002803581301122904

Training epoch-23 batch-146
Running loss of epoch-23 batch-146 = 0.0004565600538626313

Training epoch-23 batch-147
Running loss of epoch-23 batch-147 = 0.00032148766331374645

Training epoch-23 batch-148
Running loss of epoch-23 batch-148 = 0.0015490337973460555

Training epoch-23 batch-149
Running loss of epoch-23 batch-149 = 0.0006752052577212453

Training epoch-23 batch-150
Running loss of epoch-23 batch-150 = 0.0013265153393149376

Training epoch-23 batch-151
Running loss of epoch-23 batch-151 = 0.0018948018550872803

Training epoch-23 batch-152
Running loss of epoch-23 batch-152 = 0.0006940501043573022

Training epoch-23 batch-153
Running loss of epoch-23 batch-153 = 0.00042422220576554537

Training epoch-23 batch-154
Running loss of epoch-23 batch-154 = 0.0026000761426985264

Training epoch-23 batch-155
Running loss of epoch-23 batch-155 = 0.0006758683593943715

Training epoch-23 batch-156
Running loss of epoch-23 batch-156 = 0.0012672320008277893

Training epoch-23 batch-157
Running loss of epoch-23 batch-157 = 0.0005654618144035339

Finished training epoch-23.



Average train loss at epoch-23 = 0.0012843567777425051

Started Evaluation

Average val loss at epoch-23 = 1.0000593210591575

Accuracy for classes:
Accuracy for class equals is: 95.54 %
Accuracy for class main is: 96.72 %
Accuracy for class setUp is: 91.97 %
Accuracy for class onCreate is: 90.19 %
Accuracy for class toString is: 79.86 %
Accuracy for class run is: 50.23 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 58.30 %
Accuracy for class execute is: 46.59 %
Accuracy for class get is: 72.05 %

Overall Accuracy = 81.49 %

Finished Evaluation



Started training epoch-24


Training epoch-24 batch-1
Running loss of epoch-24 batch-1 = 0.001212509348988533

Training epoch-24 batch-2
Running loss of epoch-24 batch-2 = 0.0005024011479690671

Training epoch-24 batch-3
Running loss of epoch-24 batch-3 = 0.0006794696673750877

Training epoch-24 batch-4
Running loss of epoch-24 batch-4 = 0.0018358337692916393

Training epoch-24 batch-5
Running loss of epoch-24 batch-5 = 0.00019970873836427927

Training epoch-24 batch-6
Running loss of epoch-24 batch-6 = 0.0009105711942538619

Training epoch-24 batch-7
Running loss of epoch-24 batch-7 = 0.0002573278034105897

Training epoch-24 batch-8
Running loss of epoch-24 batch-8 = 0.000979732722043991

Training epoch-24 batch-9
Running loss of epoch-24 batch-9 = 0.0007253236835822463

Training epoch-24 batch-10
Running loss of epoch-24 batch-10 = 0.001563557772897184

Training epoch-24 batch-11
Running loss of epoch-24 batch-11 = 0.0005459247622638941

Training epoch-24 batch-12
Running loss of epoch-24 batch-12 = 0.0004102510865777731

Training epoch-24 batch-13
Running loss of epoch-24 batch-13 = 0.00047656684182584286

Training epoch-24 batch-14
Running loss of epoch-24 batch-14 = 0.0011763130314648151

Training epoch-24 batch-15
Running loss of epoch-24 batch-15 = 0.000553815858438611

Training epoch-24 batch-16
Running loss of epoch-24 batch-16 = 0.0009651809232309461

Training epoch-24 batch-17
Running loss of epoch-24 batch-17 = 0.0018272685119882226

Training epoch-24 batch-18
Running loss of epoch-24 batch-18 = 0.000810482888482511

Training epoch-24 batch-19
Running loss of epoch-24 batch-19 = 0.0005888942396268249

Training epoch-24 batch-20
Running loss of epoch-24 batch-20 = 0.001597262336872518

Training epoch-24 batch-21
Running loss of epoch-24 batch-21 = 0.0007599712116643786

Training epoch-24 batch-22
Running loss of epoch-24 batch-22 = 0.0011311150155961514

Training epoch-24 batch-23
Running loss of epoch-24 batch-23 = 0.0007552319439128041

Training epoch-24 batch-24
Running loss of epoch-24 batch-24 = 0.00025767553597688675

Training epoch-24 batch-25
Running loss of epoch-24 batch-25 = 0.00086432253010571

Training epoch-24 batch-26
Running loss of epoch-24 batch-26 = 0.0009375960798934102

Training epoch-24 batch-27
Running loss of epoch-24 batch-27 = 0.0011202800087630749

Training epoch-24 batch-28
Running loss of epoch-24 batch-28 = 0.0007558395736850798

Training epoch-24 batch-29
Running loss of epoch-24 batch-29 = 0.00034324510488659143

Training epoch-24 batch-30
Running loss of epoch-24 batch-30 = 0.001790467300452292

Training epoch-24 batch-31
Running loss of epoch-24 batch-31 = 0.0008329239208251238

Training epoch-24 batch-32
Running loss of epoch-24 batch-32 = 0.0014391568256542087

Training epoch-24 batch-33
Running loss of epoch-24 batch-33 = 0.00027684285305440426

Training epoch-24 batch-34
Running loss of epoch-24 batch-34 = 0.0006983295315876603

Training epoch-24 batch-35
Running loss of epoch-24 batch-35 = 0.0010764978360384703

Training epoch-24 batch-36
Running loss of epoch-24 batch-36 = 0.0013401899486780167

Training epoch-24 batch-37
Running loss of epoch-24 batch-37 = 0.00021644786465913057

Training epoch-24 batch-38
Running loss of epoch-24 batch-38 = 0.0009852639632299542

Training epoch-24 batch-39
Running loss of epoch-24 batch-39 = 0.0004323486937209964

Training epoch-24 batch-40
Running loss of epoch-24 batch-40 = 0.000757990637794137

Training epoch-24 batch-41
Running loss of epoch-24 batch-41 = 0.0007483287481591105

Training epoch-24 batch-42
Running loss of epoch-24 batch-42 = 0.0008993510855361819

Training epoch-24 batch-43
Running loss of epoch-24 batch-43 = 0.0022052747663110495

Training epoch-24 batch-44
Running loss of epoch-24 batch-44 = 0.0009739560773596168

Training epoch-24 batch-45
Running loss of epoch-24 batch-45 = 0.00038058124482631683

Training epoch-24 batch-46
Running loss of epoch-24 batch-46 = 0.0006980086909607053

Training epoch-24 batch-47
Running loss of epoch-24 batch-47 = 0.000344963395036757

Training epoch-24 batch-48
Running loss of epoch-24 batch-48 = 0.0016040876507759094

Training epoch-24 batch-49
Running loss of epoch-24 batch-49 = 0.0016527423867955804

Training epoch-24 batch-50
Running loss of epoch-24 batch-50 = 0.0004101063823327422

Training epoch-24 batch-51
Running loss of epoch-24 batch-51 = 0.00019451428670436144

Training epoch-24 batch-52
Running loss of epoch-24 batch-52 = 0.000908118556253612

Training epoch-24 batch-53
Running loss of epoch-24 batch-53 = 0.0002304018707945943

Training epoch-24 batch-54
Running loss of epoch-24 batch-54 = 0.00042255024891346693

Training epoch-24 batch-55
Running loss of epoch-24 batch-55 = 0.0013874192954972386

Training epoch-24 batch-56
Running loss of epoch-24 batch-56 = 0.0019006043439731002

Training epoch-24 batch-57
Running loss of epoch-24 batch-57 = 0.0009175613522529602

Training epoch-24 batch-58
Running loss of epoch-24 batch-58 = 0.001354130101390183

Training epoch-24 batch-59
Running loss of epoch-24 batch-59 = 0.00046315358486026525

Training epoch-24 batch-60
Running loss of epoch-24 batch-60 = 0.0012794092763215303

Training epoch-24 batch-61
Running loss of epoch-24 batch-61 = 0.000656283344142139

Training epoch-24 batch-62
Running loss of epoch-24 batch-62 = 0.0017870928859338164

Training epoch-24 batch-63
Running loss of epoch-24 batch-63 = 0.0011386513942852616

Training epoch-24 batch-64
Running loss of epoch-24 batch-64 = 0.0014519039541482925

Training epoch-24 batch-65
Running loss of epoch-24 batch-65 = 0.001663892180658877

Training epoch-24 batch-66
Running loss of epoch-24 batch-66 = 0.0008010269375517964

Training epoch-24 batch-67
Running loss of epoch-24 batch-67 = 0.00016727868933230639

Training epoch-24 batch-68
Running loss of epoch-24 batch-68 = 0.0006180821219459176

Training epoch-24 batch-69
Running loss of epoch-24 batch-69 = 0.0013819659361615777

Training epoch-24 batch-70
Running loss of epoch-24 batch-70 = 0.0014545678859576583

Training epoch-24 batch-71
Running loss of epoch-24 batch-71 = 0.0016059874324128032

Training epoch-24 batch-72
Running loss of epoch-24 batch-72 = 0.0008857540087774396

Training epoch-24 batch-73
Running loss of epoch-24 batch-73 = 0.0006131310947239399

Training epoch-24 batch-74
Running loss of epoch-24 batch-74 = 0.0010268216719850898

Training epoch-24 batch-75
Running loss of epoch-24 batch-75 = 0.0011015585623681545

Training epoch-24 batch-76
Running loss of epoch-24 batch-76 = 0.0005937600508332253

Training epoch-24 batch-77
Running loss of epoch-24 batch-77 = 0.0007449579425156116

Training epoch-24 batch-78
Running loss of epoch-24 batch-78 = 0.0005409171571955085

Training epoch-24 batch-79
Running loss of epoch-24 batch-79 = 0.0011981651186943054

Training epoch-24 batch-80
Running loss of epoch-24 batch-80 = 0.0008055549114942551

Training epoch-24 batch-81
Running loss of epoch-24 batch-81 = 0.0011673469562083483

Training epoch-24 batch-82
Running loss of epoch-24 batch-82 = 0.0008139413548633456

Training epoch-24 batch-83
Running loss of epoch-24 batch-83 = 0.0005199980223551393

Training epoch-24 batch-84
Running loss of epoch-24 batch-84 = 0.001397940912283957

Training epoch-24 batch-85
Running loss of epoch-24 batch-85 = 0.0030527382623404264

Training epoch-24 batch-86
Running loss of epoch-24 batch-86 = 0.0010248610051348805

Training epoch-24 batch-87
Running loss of epoch-24 batch-87 = 0.00019209249876439571

Training epoch-24 batch-88
Running loss of epoch-24 batch-88 = 0.0017420337535440922

Training epoch-24 batch-89
Running loss of epoch-24 batch-89 = 0.0008968093898147345

Training epoch-24 batch-90
Running loss of epoch-24 batch-90 = 0.0007298521231859922

Training epoch-24 batch-91
Running loss of epoch-24 batch-91 = 0.0012537934817373753

Training epoch-24 batch-92
Running loss of epoch-24 batch-92 = 0.0005123107694089413

Training epoch-24 batch-93
Running loss of epoch-24 batch-93 = 0.0009139127796515822

Training epoch-24 batch-94
Running loss of epoch-24 batch-94 = 0.0009293208131566644

Training epoch-24 batch-95
Running loss of epoch-24 batch-95 = 0.00042654864955693483

Training epoch-24 batch-96
Running loss of epoch-24 batch-96 = 0.00124372320715338

Training epoch-24 batch-97
Running loss of epoch-24 batch-97 = 0.0018009153427556157

Training epoch-24 batch-98
Running loss of epoch-24 batch-98 = 0.001023727934807539

Training epoch-24 batch-99
Running loss of epoch-24 batch-99 = 0.0006808798061683774

Training epoch-24 batch-100
Running loss of epoch-24 batch-100 = 0.0019831592217087746

Training epoch-24 batch-101
Running loss of epoch-24 batch-101 = 0.0027451447676867247

Training epoch-24 batch-102
Running loss of epoch-24 batch-102 = 0.0008543651783838868

Training epoch-24 batch-103
Running loss of epoch-24 batch-103 = 0.0018998951418325305

Training epoch-24 batch-104
Running loss of epoch-24 batch-104 = 0.0009931072127074003

Training epoch-24 batch-105
Running loss of epoch-24 batch-105 = 0.00145542377140373

Training epoch-24 batch-106
Running loss of epoch-24 batch-106 = 0.001563246245495975

Training epoch-24 batch-107
Running loss of epoch-24 batch-107 = 0.0005475598154589534

Training epoch-24 batch-108
Running loss of epoch-24 batch-108 = 0.0005958613473922014

Training epoch-24 batch-109
Running loss of epoch-24 batch-109 = 0.0005851431051269174

Training epoch-24 batch-110
Running loss of epoch-24 batch-110 = 0.0005028315354138613

Training epoch-24 batch-111
Running loss of epoch-24 batch-111 = 0.0006055892445147038

Training epoch-24 batch-112
Running loss of epoch-24 batch-112 = 0.0005077968817204237

Training epoch-24 batch-113
Running loss of epoch-24 batch-113 = 0.0023881045635789633

Training epoch-24 batch-114
Running loss of epoch-24 batch-114 = 0.0010070019634440541

Training epoch-24 batch-115
Running loss of epoch-24 batch-115 = 0.00018012593500316143

Training epoch-24 batch-116
Running loss of epoch-24 batch-116 = 0.0012056016130372882

Training epoch-24 batch-117
Running loss of epoch-24 batch-117 = 0.0011068563908338547

Training epoch-24 batch-118
Running loss of epoch-24 batch-118 = 0.00018383038695901632

Training epoch-24 batch-119
Running loss of epoch-24 batch-119 = 0.0016714236699044704

Training epoch-24 batch-120
Running loss of epoch-24 batch-120 = 0.0018044436583295465

Training epoch-24 batch-121
Running loss of epoch-24 batch-121 = 0.0007190522737801075

Training epoch-24 batch-122
Running loss of epoch-24 batch-122 = 0.000827212817966938

Training epoch-24 batch-123
Running loss of epoch-24 batch-123 = 0.001065076794475317

Training epoch-24 batch-124
Running loss of epoch-24 batch-124 = 0.0003835352836176753

Training epoch-24 batch-125
Running loss of epoch-24 batch-125 = 0.0007144690025597811

Training epoch-24 batch-126
Running loss of epoch-24 batch-126 = 0.0027620894834399223

Training epoch-24 batch-127
Running loss of epoch-24 batch-127 = 0.0004937784979119897

Training epoch-24 batch-128
Running loss of epoch-24 batch-128 = 0.001953465398401022

Training epoch-24 batch-129
Running loss of epoch-24 batch-129 = 0.0005345802055671811

Training epoch-24 batch-130
Running loss of epoch-24 batch-130 = 0.0008380397921428084

Training epoch-24 batch-131
Running loss of epoch-24 batch-131 = 0.0007112228777259588

Training epoch-24 batch-132
Running loss of epoch-24 batch-132 = 0.0014352058060467243

Training epoch-24 batch-133
Running loss of epoch-24 batch-133 = 0.0013853368582203984

Training epoch-24 batch-134
Running loss of epoch-24 batch-134 = 0.0012976177968084812

Training epoch-24 batch-135
Running loss of epoch-24 batch-135 = 0.0015138407470658422

Training epoch-24 batch-136
Running loss of epoch-24 batch-136 = 0.0005887065781280398

Training epoch-24 batch-137
Running loss of epoch-24 batch-137 = 0.0008020985405892134

Training epoch-24 batch-138
Running loss of epoch-24 batch-138 = 0.00023819203488528728

Training epoch-24 batch-139
Running loss of epoch-24 batch-139 = 0.0005376336630433798

Training epoch-24 batch-140
Running loss of epoch-24 batch-140 = 0.002068073721602559

Training epoch-24 batch-141
Running loss of epoch-24 batch-141 = 0.0010147843277081847

Training epoch-24 batch-142
Running loss of epoch-24 batch-142 = 0.0006168507970869541

Training epoch-24 batch-143
Running loss of epoch-24 batch-143 = 0.0005955751985311508

Training epoch-24 batch-144
Running loss of epoch-24 batch-144 = 0.0008472971385344863

Training epoch-24 batch-145
Running loss of epoch-24 batch-145 = 0.00035745278000831604

Training epoch-24 batch-146
Running loss of epoch-24 batch-146 = 0.0008476905059069395

Training epoch-24 batch-147
Running loss of epoch-24 batch-147 = 0.0014325077645480633

Training epoch-24 batch-148
Running loss of epoch-24 batch-148 = 0.0005562923615798354

Training epoch-24 batch-149
Running loss of epoch-24 batch-149 = 0.0005938587710261345

Training epoch-24 batch-150
Running loss of epoch-24 batch-150 = 0.0005653095431625843

Training epoch-24 batch-151
Running loss of epoch-24 batch-151 = 0.0010488859843462706

Training epoch-24 batch-152
Running loss of epoch-24 batch-152 = 0.001598357455804944

Training epoch-24 batch-153
Running loss of epoch-24 batch-153 = 0.0010188488522544503

Training epoch-24 batch-154
Running loss of epoch-24 batch-154 = 0.0011688180966302752

Training epoch-24 batch-155
Running loss of epoch-24 batch-155 = 0.0005736432503908873

Training epoch-24 batch-156
Running loss of epoch-24 batch-156 = 0.0002956831594929099

Training epoch-24 batch-157
Running loss of epoch-24 batch-157 = 0.011892424896359444

Finished training epoch-24.



Average train loss at epoch-24 = 0.000995111669972539

Started Evaluation

Average val loss at epoch-24 = 1.1116552635917047

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 93.93 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 90.51 %
Accuracy for class toString is: 81.91 %
Accuracy for class run is: 50.23 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.04 %
Accuracy for class execute is: 54.22 %
Accuracy for class get is: 70.26 %

Overall Accuracy = 80.77 %

Finished Evaluation



Started training epoch-25


Training epoch-25 batch-1
Running loss of epoch-25 batch-1 = 0.0017226261552423239

Training epoch-25 batch-2
Running loss of epoch-25 batch-2 = 0.0005056582158431411

Training epoch-25 batch-3
Running loss of epoch-25 batch-3 = 0.0003444573376327753

Training epoch-25 batch-4
Running loss of epoch-25 batch-4 = 0.0005290627013891935

Training epoch-25 batch-5
Running loss of epoch-25 batch-5 = 0.0006137668387964368

Training epoch-25 batch-6
Running loss of epoch-25 batch-6 = 0.000521321315318346

Training epoch-25 batch-7
Running loss of epoch-25 batch-7 = 0.000592186814174056

Training epoch-25 batch-8
Running loss of epoch-25 batch-8 = 0.001179264159873128

Training epoch-25 batch-9
Running loss of epoch-25 batch-9 = 0.001274038921110332

Training epoch-25 batch-10
Running loss of epoch-25 batch-10 = 0.0003382685827091336

Training epoch-25 batch-11
Running loss of epoch-25 batch-11 = 0.0006249475991353393

Training epoch-25 batch-12
Running loss of epoch-25 batch-12 = 0.0005092464853078127

Training epoch-25 batch-13
Running loss of epoch-25 batch-13 = 0.002143009565770626

Training epoch-25 batch-14
Running loss of epoch-25 batch-14 = 0.000322651700116694

Training epoch-25 batch-15
Running loss of epoch-25 batch-15 = 0.0010917133186012506

Training epoch-25 batch-16
Running loss of epoch-25 batch-16 = 0.0008439896628260612

Training epoch-25 batch-17
Running loss of epoch-25 batch-17 = 0.0009541461477056146

Training epoch-25 batch-18
Running loss of epoch-25 batch-18 = 0.00045492209028452635

Training epoch-25 batch-19
Running loss of epoch-25 batch-19 = 0.0007002308266237378

Training epoch-25 batch-20
Running loss of epoch-25 batch-20 = 0.0005247342633083463

Training epoch-25 batch-21
Running loss of epoch-25 batch-21 = 0.0006038050632923841

Training epoch-25 batch-22
Running loss of epoch-25 batch-22 = 0.00045490823686122894

Training epoch-25 batch-23
Running loss of epoch-25 batch-23 = 0.0011047832667827606

Training epoch-25 batch-24
Running loss of epoch-25 batch-24 = 0.0014931880868971348

Training epoch-25 batch-25
Running loss of epoch-25 batch-25 = 0.0004873209400102496

Training epoch-25 batch-26
Running loss of epoch-25 batch-26 = 0.0009552771225571632

Training epoch-25 batch-27
Running loss of epoch-25 batch-27 = 0.0007730815559625626

Training epoch-25 batch-28
Running loss of epoch-25 batch-28 = 0.00043454684782773256

Training epoch-25 batch-29
Running loss of epoch-25 batch-29 = 0.0003540364559739828

Training epoch-25 batch-30
Running loss of epoch-25 batch-30 = 0.0003283224068582058

Training epoch-25 batch-31
Running loss of epoch-25 batch-31 = 0.0012316067004576325

Training epoch-25 batch-32
Running loss of epoch-25 batch-32 = 0.001458257669582963

Training epoch-25 batch-33
Running loss of epoch-25 batch-33 = 0.0012182052014395595

Training epoch-25 batch-34
Running loss of epoch-25 batch-34 = 0.000566357746720314

Training epoch-25 batch-35
Running loss of epoch-25 batch-35 = 0.00021887884940952063

Training epoch-25 batch-36
Running loss of epoch-25 batch-36 = 0.000538290711119771

Training epoch-25 batch-37
Running loss of epoch-25 batch-37 = 0.0009649945423007011

Training epoch-25 batch-38
Running loss of epoch-25 batch-38 = 0.0006737386574968696

Training epoch-25 batch-39
Running loss of epoch-25 batch-39 = 0.0008075503865256906

Training epoch-25 batch-40
Running loss of epoch-25 batch-40 = 0.000703891390003264

Training epoch-25 batch-41
Running loss of epoch-25 batch-41 = 0.0007898163748905063

Training epoch-25 batch-42
Running loss of epoch-25 batch-42 = 0.0003650832222774625

Training epoch-25 batch-43
Running loss of epoch-25 batch-43 = 0.0012031806400045753

Training epoch-25 batch-44
Running loss of epoch-25 batch-44 = 0.0004673781804740429

Training epoch-25 batch-45
Running loss of epoch-25 batch-45 = 0.00047152070328593254

Training epoch-25 batch-46
Running loss of epoch-25 batch-46 = 0.0003601893549785018

Training epoch-25 batch-47
Running loss of epoch-25 batch-47 = 0.0005911005428060889

Training epoch-25 batch-48
Running loss of epoch-25 batch-48 = 0.000512181781232357

Training epoch-25 batch-49
Running loss of epoch-25 batch-49 = 0.00039788451977074146

Training epoch-25 batch-50
Running loss of epoch-25 batch-50 = 0.0004504242679104209

Training epoch-25 batch-51
Running loss of epoch-25 batch-51 = 0.00033914530649781227

Training epoch-25 batch-52
Running loss of epoch-25 batch-52 = 0.00026979378890246153

Training epoch-25 batch-53
Running loss of epoch-25 batch-53 = 0.0001955453772097826

Training epoch-25 batch-54
Running loss of epoch-25 batch-54 = 0.0010266992030665278

Training epoch-25 batch-55
Running loss of epoch-25 batch-55 = 0.0011624975595623255

Training epoch-25 batch-56
Running loss of epoch-25 batch-56 = 0.0002580280415713787

Training epoch-25 batch-57
Running loss of epoch-25 batch-57 = 0.000642379280179739

Training epoch-25 batch-58
Running loss of epoch-25 batch-58 = 0.0004781264578923583

Training epoch-25 batch-59
Running loss of epoch-25 batch-59 = 0.0004922030493617058

Training epoch-25 batch-60
Running loss of epoch-25 batch-60 = 0.0004983078688383102

Training epoch-25 batch-61
Running loss of epoch-25 batch-61 = 0.001044004806317389

Training epoch-25 batch-62
Running loss of epoch-25 batch-62 = 0.0003885852638632059

Training epoch-25 batch-63
Running loss of epoch-25 batch-63 = 0.003377363784238696

Training epoch-25 batch-64
Running loss of epoch-25 batch-64 = 8.626282215118408e-05

Training epoch-25 batch-65
Running loss of epoch-25 batch-65 = 0.0011788957053795457

Training epoch-25 batch-66
Running loss of epoch-25 batch-66 = 0.0003242612583562732

Training epoch-25 batch-67
Running loss of epoch-25 batch-67 = 0.00036836881190538406

Training epoch-25 batch-68
Running loss of epoch-25 batch-68 = 0.0004246757598593831

Training epoch-25 batch-69
Running loss of epoch-25 batch-69 = 0.0002555375685915351

Training epoch-25 batch-70
Running loss of epoch-25 batch-70 = 0.001642580726183951

Training epoch-25 batch-71
Running loss of epoch-25 batch-71 = 0.0009489626390859485

Training epoch-25 batch-72
Running loss of epoch-25 batch-72 = 0.0006908958312124014

Training epoch-25 batch-73
Running loss of epoch-25 batch-73 = 0.00041847373358905315

Training epoch-25 batch-74
Running loss of epoch-25 batch-74 = 0.0014392479788511992

Training epoch-25 batch-75
Running loss of epoch-25 batch-75 = 0.00034064147621393204

Training epoch-25 batch-76
Running loss of epoch-25 batch-76 = 0.002133338013663888

Training epoch-25 batch-77
Running loss of epoch-25 batch-77 = 0.0012655194150283933

Training epoch-25 batch-78
Running loss of epoch-25 batch-78 = 0.0009420858696103096

Training epoch-25 batch-79
Running loss of epoch-25 batch-79 = 0.0011254692217335105

Training epoch-25 batch-80
Running loss of epoch-25 batch-80 = 0.0013371818931773305

Training epoch-25 batch-81
Running loss of epoch-25 batch-81 = 0.0012894623214378953

Training epoch-25 batch-82
Running loss of epoch-25 batch-82 = 0.001103767310269177

Training epoch-25 batch-83
Running loss of epoch-25 batch-83 = 0.0019348689820617437

Training epoch-25 batch-84
Running loss of epoch-25 batch-84 = 0.00017425487749278545

Training epoch-25 batch-85
Running loss of epoch-25 batch-85 = 0.0006875507533550262

Training epoch-25 batch-86
Running loss of epoch-25 batch-86 = 0.0009160274639725685

Training epoch-25 batch-87
Running loss of epoch-25 batch-87 = 0.0009869943605735898

Training epoch-25 batch-88
Running loss of epoch-25 batch-88 = 0.0003987010568380356

Training epoch-25 batch-89
Running loss of epoch-25 batch-89 = 0.0004977983189746737

Training epoch-25 batch-90
Running loss of epoch-25 batch-90 = 0.0018638101173564792

Training epoch-25 batch-91
Running loss of epoch-25 batch-91 = 0.0009124013595283031

Training epoch-25 batch-92
Running loss of epoch-25 batch-92 = 0.0004126447020098567

Training epoch-25 batch-93
Running loss of epoch-25 batch-93 = 0.000793672283180058

Training epoch-25 batch-94
Running loss of epoch-25 batch-94 = 0.0008486990118399262

Training epoch-25 batch-95
Running loss of epoch-25 batch-95 = 0.00024537474382668734

Training epoch-25 batch-96
Running loss of epoch-25 batch-96 = 0.0005192351527512074

Training epoch-25 batch-97
Running loss of epoch-25 batch-97 = 0.0009833547519519925

Training epoch-25 batch-98
Running loss of epoch-25 batch-98 = 0.003084154799580574

Training epoch-25 batch-99
Running loss of epoch-25 batch-99 = 0.0008821221999824047

Training epoch-25 batch-100
Running loss of epoch-25 batch-100 = 0.0008940336410887539

Training epoch-25 batch-101
Running loss of epoch-25 batch-101 = 0.00011584511958062649

Training epoch-25 batch-102
Running loss of epoch-25 batch-102 = 0.00042191334068775177

Training epoch-25 batch-103
Running loss of epoch-25 batch-103 = 0.0006126739317551255

Training epoch-25 batch-104
Running loss of epoch-25 batch-104 = 0.0006009538192301989

Training epoch-25 batch-105
Running loss of epoch-25 batch-105 = 0.0009165473747998476

Training epoch-25 batch-106
Running loss of epoch-25 batch-106 = 0.000971064087934792

Training epoch-25 batch-107
Running loss of epoch-25 batch-107 = 0.0002286600647494197

Training epoch-25 batch-108
Running loss of epoch-25 batch-108 = 0.00044447940308600664

Training epoch-25 batch-109
Running loss of epoch-25 batch-109 = 0.0010205055586993694

Training epoch-25 batch-110
Running loss of epoch-25 batch-110 = 0.000508380588144064

Training epoch-25 batch-111
Running loss of epoch-25 batch-111 = 0.0008378257043659687

Training epoch-25 batch-112
Running loss of epoch-25 batch-112 = 0.0009298947406932712

Training epoch-25 batch-113
Running loss of epoch-25 batch-113 = 0.00038046459667384624

Training epoch-25 batch-114
Running loss of epoch-25 batch-114 = 0.0008099506376311183

Training epoch-25 batch-115
Running loss of epoch-25 batch-115 = 0.0006334322970360518

Training epoch-25 batch-116
Running loss of epoch-25 batch-116 = 0.00044720061123371124

Training epoch-25 batch-117
Running loss of epoch-25 batch-117 = 0.00037541845813393593

Training epoch-25 batch-118
Running loss of epoch-25 batch-118 = 0.0009042960591614246

Training epoch-25 batch-119
Running loss of epoch-25 batch-119 = 0.0016672122292220592

Training epoch-25 batch-120
Running loss of epoch-25 batch-120 = 0.0008285229559987783

Training epoch-25 batch-121
Running loss of epoch-25 batch-121 = 0.0007832838455215096

Training epoch-25 batch-122
Running loss of epoch-25 batch-122 = 0.00022263301070779562

Training epoch-25 batch-123
Running loss of epoch-25 batch-123 = 0.00035294098779559135

Training epoch-25 batch-124
Running loss of epoch-25 batch-124 = 0.0002978486008942127

Training epoch-25 batch-125
Running loss of epoch-25 batch-125 = 0.0003421048168092966

Training epoch-25 batch-126
Running loss of epoch-25 batch-126 = 0.0012081286404281855

Training epoch-25 batch-127
Running loss of epoch-25 batch-127 = 0.0001588813029229641

Training epoch-25 batch-128
Running loss of epoch-25 batch-128 = 0.0010658568935468793

Training epoch-25 batch-129
Running loss of epoch-25 batch-129 = 0.001300927484408021

Training epoch-25 batch-130
Running loss of epoch-25 batch-130 = 0.0012867714976891875

Training epoch-25 batch-131
Running loss of epoch-25 batch-131 = 0.00047912157606333494

Training epoch-25 batch-132
Running loss of epoch-25 batch-132 = 0.0006075721466913819

Training epoch-25 batch-133
Running loss of epoch-25 batch-133 = 0.00015688035637140274

Training epoch-25 batch-134
Running loss of epoch-25 batch-134 = 0.0012367451563477516

Training epoch-25 batch-135
Running loss of epoch-25 batch-135 = 0.0023029244039207697

Training epoch-25 batch-136
Running loss of epoch-25 batch-136 = 0.0007666078163310885

Training epoch-25 batch-137
Running loss of epoch-25 batch-137 = 0.0005462882108986378

Training epoch-25 batch-138
Running loss of epoch-25 batch-138 = 0.0008505965815857053

Training epoch-25 batch-139
Running loss of epoch-25 batch-139 = 0.0008048973977565765

Training epoch-25 batch-140
Running loss of epoch-25 batch-140 = 0.0017536304658278823

Training epoch-25 batch-141
Running loss of epoch-25 batch-141 = 0.00027817999944090843

Training epoch-25 batch-142
Running loss of epoch-25 batch-142 = 0.0007694332161918283

Training epoch-25 batch-143
Running loss of epoch-25 batch-143 = 0.0010205028811469674

Training epoch-25 batch-144
Running loss of epoch-25 batch-144 = 0.0003734275233000517

Training epoch-25 batch-145
Running loss of epoch-25 batch-145 = 0.0009651031577959657

Training epoch-25 batch-146
Running loss of epoch-25 batch-146 = 0.0006040565203875303

Training epoch-25 batch-147
Running loss of epoch-25 batch-147 = 0.0009974277345463634

Training epoch-25 batch-148
Running loss of epoch-25 batch-148 = 0.0009206242393702269

Training epoch-25 batch-149
Running loss of epoch-25 batch-149 = 0.0012779840035364032

Training epoch-25 batch-150
Running loss of epoch-25 batch-150 = 0.0006517705041915178

Training epoch-25 batch-151
Running loss of epoch-25 batch-151 = 0.0002025481080636382

Training epoch-25 batch-152
Running loss of epoch-25 batch-152 = 0.0016058305045589805

Training epoch-25 batch-153
Running loss of epoch-25 batch-153 = 0.0017026177374646068

Training epoch-25 batch-154
Running loss of epoch-25 batch-154 = 0.0008264100179076195

Training epoch-25 batch-155
Running loss of epoch-25 batch-155 = 0.00037040049210190773

Training epoch-25 batch-156
Running loss of epoch-25 batch-156 = 0.0015634163282811642

Training epoch-25 batch-157
Running loss of epoch-25 batch-157 = 0.004961762577295303

Finished training epoch-25.



Average train loss at epoch-25 = 0.0008118022862821817

Started Evaluation

Average val loss at epoch-25 = 1.0254741980304396

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 92.30 %
Accuracy for class onCreate is: 89.45 %
Accuracy for class toString is: 82.94 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 59.64 %
Accuracy for class execute is: 32.53 %
Accuracy for class get is: 68.72 %

Overall Accuracy = 82.07 %


Best Accuracy = 82.07 % at Epoch-25
Saving model after best epoch-25

Finished Evaluation



Started training epoch-26


Training epoch-26 batch-1
Running loss of epoch-26 batch-1 = 0.0002023722045123577

Training epoch-26 batch-2
Running loss of epoch-26 batch-2 = 0.00031826936174184084

Training epoch-26 batch-3
Running loss of epoch-26 batch-3 = 0.001249037915840745

Training epoch-26 batch-4
Running loss of epoch-26 batch-4 = 0.0010781659511849284

Training epoch-26 batch-5
Running loss of epoch-26 batch-5 = 0.0005215214332565665

Training epoch-26 batch-6
Running loss of epoch-26 batch-6 = 0.0013485266827046871

Training epoch-26 batch-7
Running loss of epoch-26 batch-7 = 6.219418719410896e-05

Training epoch-26 batch-8
Running loss of epoch-26 batch-8 = 0.000915409647859633

Training epoch-26 batch-9
Running loss of epoch-26 batch-9 = 0.0006150285480543971

Training epoch-26 batch-10
Running loss of epoch-26 batch-10 = 0.00041449046693742275

Training epoch-26 batch-11
Running loss of epoch-26 batch-11 = 0.00037749006878584623

Training epoch-26 batch-12
Running loss of epoch-26 batch-12 = 8.573534432798624e-05

Training epoch-26 batch-13
Running loss of epoch-26 batch-13 = 0.0010431467089802027

Training epoch-26 batch-14
Running loss of epoch-26 batch-14 = 0.00038768420927226543

Training epoch-26 batch-15
Running loss of epoch-26 batch-15 = 0.00012917956337332726

Training epoch-26 batch-16
Running loss of epoch-26 batch-16 = 0.00026476732455193996

Training epoch-26 batch-17
Running loss of epoch-26 batch-17 = 0.002008151961490512

Training epoch-26 batch-18
Running loss of epoch-26 batch-18 = 0.0008743451908230782

Training epoch-26 batch-19
Running loss of epoch-26 batch-19 = 0.000408772611990571

Training epoch-26 batch-20
Running loss of epoch-26 batch-20 = 0.00018779211677610874

Training epoch-26 batch-21
Running loss of epoch-26 batch-21 = 0.0007873764261603355

Training epoch-26 batch-22
Running loss of epoch-26 batch-22 = 0.0003736026119440794

Training epoch-26 batch-23
Running loss of epoch-26 batch-23 = 0.0006450515938922763

Training epoch-26 batch-24
Running loss of epoch-26 batch-24 = 5.29139069840312e-05

Training epoch-26 batch-25
Running loss of epoch-26 batch-25 = 0.00015735707711428404

Training epoch-26 batch-26
Running loss of epoch-26 batch-26 = 0.000315613579005003

Training epoch-26 batch-27
Running loss of epoch-26 batch-27 = 0.00035154062788933516

Training epoch-26 batch-28
Running loss of epoch-26 batch-28 = 0.001057539484463632

Training epoch-26 batch-29
Running loss of epoch-26 batch-29 = 0.0005024904385209084

Training epoch-26 batch-30
Running loss of epoch-26 batch-30 = 0.00025637715589255095

Training epoch-26 batch-31
Running loss of epoch-26 batch-31 = 0.0002397908829152584

Training epoch-26 batch-32
Running loss of epoch-26 batch-32 = 0.00017428561113774776

Training epoch-26 batch-33
Running loss of epoch-26 batch-33 = 0.00048141228035092354

Training epoch-26 batch-34
Running loss of epoch-26 batch-34 = 0.00023917690850794315

Training epoch-26 batch-35
Running loss of epoch-26 batch-35 = 0.0017057951772585511

Training epoch-26 batch-36
Running loss of epoch-26 batch-36 = 0.00029796198941767216

Training epoch-26 batch-37
Running loss of epoch-26 batch-37 = 0.0004239054396748543

Training epoch-26 batch-38
Running loss of epoch-26 batch-38 = 0.0006241932278499007

Training epoch-26 batch-39
Running loss of epoch-26 batch-39 = 0.0005041096592321992

Training epoch-26 batch-40
Running loss of epoch-26 batch-40 = 0.0006946380017325282

Training epoch-26 batch-41
Running loss of epoch-26 batch-41 = 0.0005035215290263295

Training epoch-26 batch-42
Running loss of epoch-26 batch-42 = 0.0005837954813614488

Training epoch-26 batch-43
Running loss of epoch-26 batch-43 = 9.092781692743301e-05

Training epoch-26 batch-44
Running loss of epoch-26 batch-44 = 0.00038455252069979906

Training epoch-26 batch-45
Running loss of epoch-26 batch-45 = 0.0006838208064436913

Training epoch-26 batch-46
Running loss of epoch-26 batch-46 = 0.0003870849031955004

Training epoch-26 batch-47
Running loss of epoch-26 batch-47 = 0.00028769136406481266

Training epoch-26 batch-48
Running loss of epoch-26 batch-48 = 0.0007183399284258485

Training epoch-26 batch-49
Running loss of epoch-26 batch-49 = 0.0018557255389168859

Training epoch-26 batch-50
Running loss of epoch-26 batch-50 = 0.0005184466717764735

Training epoch-26 batch-51
Running loss of epoch-26 batch-51 = 0.00026028428692370653

Training epoch-26 batch-52
Running loss of epoch-26 batch-52 = 0.00019135628826916218

Training epoch-26 batch-53
Running loss of epoch-26 batch-53 = 0.000429872190579772

Training epoch-26 batch-54
Running loss of epoch-26 batch-54 = 0.00028223730623722076

Training epoch-26 batch-55
Running loss of epoch-26 batch-55 = 0.00030089798383414745

Training epoch-26 batch-56
Running loss of epoch-26 batch-56 = 0.0007547324057668447

Training epoch-26 batch-57
Running loss of epoch-26 batch-57 = 0.0015684837708249688

Training epoch-26 batch-58
Running loss of epoch-26 batch-58 = 0.0004167144652456045

Training epoch-26 batch-59
Running loss of epoch-26 batch-59 = 0.000934615614823997

Training epoch-26 batch-60
Running loss of epoch-26 batch-60 = 0.00015674158930778503

Training epoch-26 batch-61
Running loss of epoch-26 batch-61 = 0.0012187520042061806

Training epoch-26 batch-62
Running loss of epoch-26 batch-62 = 0.0006230452563613653

Training epoch-26 batch-63
Running loss of epoch-26 batch-63 = 0.00020715396385639906

Training epoch-26 batch-64
Running loss of epoch-26 batch-64 = 0.0003100874600932002

Training epoch-26 batch-65
Running loss of epoch-26 batch-65 = 0.0004657492972910404

Training epoch-26 batch-66
Running loss of epoch-26 batch-66 = 0.0010458099422976375

Training epoch-26 batch-67
Running loss of epoch-26 batch-67 = 0.00036386761348694563

Training epoch-26 batch-68
Running loss of epoch-26 batch-68 = 0.00063007150311023

Training epoch-26 batch-69
Running loss of epoch-26 batch-69 = 0.0004769789520651102

Training epoch-26 batch-70
Running loss of epoch-26 batch-70 = 0.0001463808584958315

Training epoch-26 batch-71
Running loss of epoch-26 batch-71 = 0.00019903178326785564

Training epoch-26 batch-72
Running loss of epoch-26 batch-72 = 0.0005924704018980265

Training epoch-26 batch-73
Running loss of epoch-26 batch-73 = 0.0008945307927206159

Training epoch-26 batch-74
Running loss of epoch-26 batch-74 = 0.0008063847199082375

Training epoch-26 batch-75
Running loss of epoch-26 batch-75 = 0.00191975268535316

Training epoch-26 batch-76
Running loss of epoch-26 batch-76 = 0.0007224190048873425

Training epoch-26 batch-77
Running loss of epoch-26 batch-77 = 0.0003806644817814231

Training epoch-26 batch-78
Running loss of epoch-26 batch-78 = 0.0005999936256557703

Training epoch-26 batch-79
Running loss of epoch-26 batch-79 = 0.0013152724131941795

Training epoch-26 batch-80
Running loss of epoch-26 batch-80 = 0.0007634147768840194

Training epoch-26 batch-81
Running loss of epoch-26 batch-81 = 0.0002132275840267539

Training epoch-26 batch-82
Running loss of epoch-26 batch-82 = 0.0004234962398186326

Training epoch-26 batch-83
Running loss of epoch-26 batch-83 = 0.0008344163652509451

Training epoch-26 batch-84
Running loss of epoch-26 batch-84 = 0.002737178932875395

Training epoch-26 batch-85
Running loss of epoch-26 batch-85 = 0.0002605405170470476

Training epoch-26 batch-86
Running loss of epoch-26 batch-86 = 0.0005984930321574211

Training epoch-26 batch-87
Running loss of epoch-26 batch-87 = 0.0011041872203350067

Training epoch-26 batch-88
Running loss of epoch-26 batch-88 = 0.0012754640774801373

Training epoch-26 batch-89
Running loss of epoch-26 batch-89 = 0.001280812663026154

Training epoch-26 batch-90
Running loss of epoch-26 batch-90 = 0.0007408899255096912

Training epoch-26 batch-91
Running loss of epoch-26 batch-91 = 0.0009704279946163297

Training epoch-26 batch-92
Running loss of epoch-26 batch-92 = 0.0012348499149084091

Training epoch-26 batch-93
Running loss of epoch-26 batch-93 = 0.0001695198006927967

Training epoch-26 batch-94
Running loss of epoch-26 batch-94 = 0.00020464789122343063

Training epoch-26 batch-95
Running loss of epoch-26 batch-95 = 0.0005792831070721149

Training epoch-26 batch-96
Running loss of epoch-26 batch-96 = 0.001276252674870193

Training epoch-26 batch-97
Running loss of epoch-26 batch-97 = 0.0008773523150011897

Training epoch-26 batch-98
Running loss of epoch-26 batch-98 = 0.0004652426578104496

Training epoch-26 batch-99
Running loss of epoch-26 batch-99 = 0.00029772473499178886

Training epoch-26 batch-100
Running loss of epoch-26 batch-100 = 0.0006978998426347971

Training epoch-26 batch-101
Running loss of epoch-26 batch-101 = 0.0002158869756385684

Training epoch-26 batch-102
Running loss of epoch-26 batch-102 = 0.0003181429347023368

Training epoch-26 batch-103
Running loss of epoch-26 batch-103 = 0.0004447804531082511

Training epoch-26 batch-104
Running loss of epoch-26 batch-104 = 0.0014899239176884294

Training epoch-26 batch-105
Running loss of epoch-26 batch-105 = 0.00018971238750964403

Training epoch-26 batch-106
Running loss of epoch-26 batch-106 = 0.0005503438878804445

Training epoch-26 batch-107
Running loss of epoch-26 batch-107 = 0.0018948313081637025

Training epoch-26 batch-108
Running loss of epoch-26 batch-108 = 0.00035179697442799807

Training epoch-26 batch-109
Running loss of epoch-26 batch-109 = 0.0006260031368583441

Training epoch-26 batch-110
Running loss of epoch-26 batch-110 = 0.00043720018584281206

Training epoch-26 batch-111
Running loss of epoch-26 batch-111 = 0.0007155401399359107

Training epoch-26 batch-112
Running loss of epoch-26 batch-112 = 0.0002711277920752764

Training epoch-26 batch-113
Running loss of epoch-26 batch-113 = 0.0006041197339072824

Training epoch-26 batch-114
Running loss of epoch-26 batch-114 = 0.0003183369990438223

Training epoch-26 batch-115
Running loss of epoch-26 batch-115 = 0.001595415291376412

Training epoch-26 batch-116
Running loss of epoch-26 batch-116 = 0.00031994679011404514

Training epoch-26 batch-117
Running loss of epoch-26 batch-117 = 0.00010965380351990461

Training epoch-26 batch-118
Running loss of epoch-26 batch-118 = 0.00025313301011919975

Training epoch-26 batch-119
Running loss of epoch-26 batch-119 = 0.00036269426345825195

Training epoch-26 batch-120
Running loss of epoch-26 batch-120 = 0.0006051907548680902

Training epoch-26 batch-121
Running loss of epoch-26 batch-121 = 0.0001425946829840541

Training epoch-26 batch-122
Running loss of epoch-26 batch-122 = 0.0007812944240868092

Training epoch-26 batch-123
Running loss of epoch-26 batch-123 = 0.0005701927002519369

Training epoch-26 batch-124
Running loss of epoch-26 batch-124 = 0.00042551744263619184

Training epoch-26 batch-125
Running loss of epoch-26 batch-125 = 0.00012359919492155313

Training epoch-26 batch-126
Running loss of epoch-26 batch-126 = 0.0006293333135545254

Training epoch-26 batch-127
Running loss of epoch-26 batch-127 = 0.0006952855037525296

Training epoch-26 batch-128
Running loss of epoch-26 batch-128 = 0.0010796019341796637

Training epoch-26 batch-129
Running loss of epoch-26 batch-129 = 0.0008654809789732099

Training epoch-26 batch-130
Running loss of epoch-26 batch-130 = 0.00029214052483439445

Training epoch-26 batch-131
Running loss of epoch-26 batch-131 = 0.0003995979204773903

Training epoch-26 batch-132
Running loss of epoch-26 batch-132 = 0.0016588832950219512

Training epoch-26 batch-133
Running loss of epoch-26 batch-133 = 0.0011144960299134254

Training epoch-26 batch-134
Running loss of epoch-26 batch-134 = 0.0004044577945023775

Training epoch-26 batch-135
Running loss of epoch-26 batch-135 = 0.0003396973479539156

Training epoch-26 batch-136
Running loss of epoch-26 batch-136 = 0.0007575945928692818

Training epoch-26 batch-137
Running loss of epoch-26 batch-137 = 0.0008454871131107211

Training epoch-26 batch-138
Running loss of epoch-26 batch-138 = 0.00029845116659998894

Training epoch-26 batch-139
Running loss of epoch-26 batch-139 = 0.0004142601974308491

Training epoch-26 batch-140
Running loss of epoch-26 batch-140 = 0.0015150983817875385

Training epoch-26 batch-141
Running loss of epoch-26 batch-141 = 0.00037822185549885035

Training epoch-26 batch-142
Running loss of epoch-26 batch-142 = 0.0003846465842798352

Training epoch-26 batch-143
Running loss of epoch-26 batch-143 = 0.0002483396092429757

Training epoch-26 batch-144
Running loss of epoch-26 batch-144 = 0.0003411630168557167

Training epoch-26 batch-145
Running loss of epoch-26 batch-145 = 0.0008931439369916916

Training epoch-26 batch-146
Running loss of epoch-26 batch-146 = 0.0008744323858991265

Training epoch-26 batch-147
Running loss of epoch-26 batch-147 = 0.0010497851762920618

Training epoch-26 batch-148
Running loss of epoch-26 batch-148 = 0.00011019525118172169

Training epoch-26 batch-149
Running loss of epoch-26 batch-149 = 9.564170613884926e-05

Training epoch-26 batch-150
Running loss of epoch-26 batch-150 = 0.0006901226006448269

Training epoch-26 batch-151
Running loss of epoch-26 batch-151 = 0.0004721705336123705

Training epoch-26 batch-152
Running loss of epoch-26 batch-152 = 0.00250666169449687

Training epoch-26 batch-153
Running loss of epoch-26 batch-153 = 0.000659256475046277

Training epoch-26 batch-154
Running loss of epoch-26 batch-154 = 0.001187526504509151

Training epoch-26 batch-155
Running loss of epoch-26 batch-155 = 0.00025239819660782814

Training epoch-26 batch-156
Running loss of epoch-26 batch-156 = 0.00034570018760859966

Training epoch-26 batch-157
Running loss of epoch-26 batch-157 = 0.0024597682058811188

Finished training epoch-26.



Average train loss at epoch-26 = 0.0006383487239480019

Started Evaluation

Average val loss at epoch-26 = 1.1555050438420087

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 95.90 %
Accuracy for class setUp is: 90.49 %
Accuracy for class onCreate is: 87.21 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 49.54 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 68.16 %
Accuracy for class execute is: 41.37 %
Accuracy for class get is: 60.77 %

Overall Accuracy = 80.63 %

Finished Evaluation



Started training epoch-27


Training epoch-27 batch-1
Running loss of epoch-27 batch-1 = 0.0007665831362828612

Training epoch-27 batch-2
Running loss of epoch-27 batch-2 = 0.0004830156685784459

Training epoch-27 batch-3
Running loss of epoch-27 batch-3 = 0.0009588837856426835

Training epoch-27 batch-4
Running loss of epoch-27 batch-4 = 0.0004714492242783308

Training epoch-27 batch-5
Running loss of epoch-27 batch-5 = 0.00020556547679007053

Training epoch-27 batch-6
Running loss of epoch-27 batch-6 = 0.0010273504303768277

Training epoch-27 batch-7
Running loss of epoch-27 batch-7 = 0.0003905578050762415

Training epoch-27 batch-8
Running loss of epoch-27 batch-8 = 0.00022280425764620304

Training epoch-27 batch-9
Running loss of epoch-27 batch-9 = 0.0007242789724841714

Training epoch-27 batch-10
Running loss of epoch-27 batch-10 = 0.001818863907828927

Training epoch-27 batch-11
Running loss of epoch-27 batch-11 = 0.0007811365649104118

Training epoch-27 batch-12
Running loss of epoch-27 batch-12 = 0.00029098265804350376

Training epoch-27 batch-13
Running loss of epoch-27 batch-13 = 0.00015756709035485983

Training epoch-27 batch-14
Running loss of epoch-27 batch-14 = 0.00036378507502377033

Training epoch-27 batch-15
Running loss of epoch-27 batch-15 = 0.0006518614245578647

Training epoch-27 batch-16
Running loss of epoch-27 batch-16 = 0.001459939288906753

Training epoch-27 batch-17
Running loss of epoch-27 batch-17 = 0.00046613486483693123

Training epoch-27 batch-18
Running loss of epoch-27 batch-18 = 0.0006553600542247295

Training epoch-27 batch-19
Running loss of epoch-27 batch-19 = 0.00018273235764354467

Training epoch-27 batch-20
Running loss of epoch-27 batch-20 = 0.00013961410149931908

Training epoch-27 batch-21
Running loss of epoch-27 batch-21 = 0.00014552543871104717

Training epoch-27 batch-22
Running loss of epoch-27 batch-22 = 0.0010220662225037813

Training epoch-27 batch-23
Running loss of epoch-27 batch-23 = 0.00036876113153994083

Training epoch-27 batch-24
Running loss of epoch-27 batch-24 = 0.00038287637289613485

Training epoch-27 batch-25
Running loss of epoch-27 batch-25 = 0.00025859649758785963

Training epoch-27 batch-26
Running loss of epoch-27 batch-26 = 0.0005777415353804827

Training epoch-27 batch-27
Running loss of epoch-27 batch-27 = 0.0002799215726554394

Training epoch-27 batch-28
Running loss of epoch-27 batch-28 = 0.00042641686741262674

Training epoch-27 batch-29
Running loss of epoch-27 batch-29 = 0.0006330385804176331

Training epoch-27 batch-30
Running loss of epoch-27 batch-30 = 0.0016572371823713183

Training epoch-27 batch-31
Running loss of epoch-27 batch-31 = 0.0004775791894644499

Training epoch-27 batch-32
Running loss of epoch-27 batch-32 = 0.0003523913910612464

Training epoch-27 batch-33
Running loss of epoch-27 batch-33 = 0.0006875488325022161

Training epoch-27 batch-34
Running loss of epoch-27 batch-34 = 0.00012302573304623365

Training epoch-27 batch-35
Running loss of epoch-27 batch-35 = 0.0008494887733832002

Training epoch-27 batch-36
Running loss of epoch-27 batch-36 = 0.0002026803558692336

Training epoch-27 batch-37
Running loss of epoch-27 batch-37 = 0.00014079362154006958

Training epoch-27 batch-38
Running loss of epoch-27 batch-38 = 0.00017480808310210705

Training epoch-27 batch-39
Running loss of epoch-27 batch-39 = 0.0006346248555928469

Training epoch-27 batch-40
Running loss of epoch-27 batch-40 = 0.0005176938138902187

Training epoch-27 batch-41
Running loss of epoch-27 batch-41 = 0.00019387644715607166

Training epoch-27 batch-42
Running loss of epoch-27 batch-42 = 0.00016675470396876335

Training epoch-27 batch-43
Running loss of epoch-27 batch-43 = 0.0002755140885710716

Training epoch-27 batch-44
Running loss of epoch-27 batch-44 = 0.0004851378034800291

Training epoch-27 batch-45
Running loss of epoch-27 batch-45 = 0.001185699482448399

Training epoch-27 batch-46
Running loss of epoch-27 batch-46 = 0.000670949462801218

Training epoch-27 batch-47
Running loss of epoch-27 batch-47 = 0.00156581518240273

Training epoch-27 batch-48
Running loss of epoch-27 batch-48 = 0.001859077368862927

Training epoch-27 batch-49
Running loss of epoch-27 batch-49 = 0.000429101986810565

Training epoch-27 batch-50
Running loss of epoch-27 batch-50 = 0.0010851994156837463

Training epoch-27 batch-51
Running loss of epoch-27 batch-51 = 0.00017313018906861544

Training epoch-27 batch-52
Running loss of epoch-27 batch-52 = 0.0005734540754929185

Training epoch-27 batch-53
Running loss of epoch-27 batch-53 = 0.0006080713355913758

Training epoch-27 batch-54
Running loss of epoch-27 batch-54 = 0.0007001073099672794

Training epoch-27 batch-55
Running loss of epoch-27 batch-55 = 0.0016509715933352709

Training epoch-27 batch-56
Running loss of epoch-27 batch-56 = 0.0016106574330478907

Training epoch-27 batch-57
Running loss of epoch-27 batch-57 = 0.0004298691637814045

Training epoch-27 batch-58
Running loss of epoch-27 batch-58 = 0.00032495823688805103

Training epoch-27 batch-59
Running loss of epoch-27 batch-59 = 0.00023813755251467228

Training epoch-27 batch-60
Running loss of epoch-27 batch-60 = 0.0009956129360944033

Training epoch-27 batch-61
Running loss of epoch-27 batch-61 = 0.001236325828358531

Training epoch-27 batch-62
Running loss of epoch-27 batch-62 = 0.0020863055251538754

Training epoch-27 batch-63
Running loss of epoch-27 batch-63 = 0.0004137044306844473

Training epoch-27 batch-64
Running loss of epoch-27 batch-64 = 0.0003805236192420125

Training epoch-27 batch-65
Running loss of epoch-27 batch-65 = 0.0015858886763453484

Training epoch-27 batch-66
Running loss of epoch-27 batch-66 = 0.0009417355759069324

Training epoch-27 batch-67
Running loss of epoch-27 batch-67 = 0.00228307256475091

Training epoch-27 batch-68
Running loss of epoch-27 batch-68 = 0.0006803947035223246

Training epoch-27 batch-69
Running loss of epoch-27 batch-69 = 0.0004915688186883926

Training epoch-27 batch-70
Running loss of epoch-27 batch-70 = 0.00038999528624117374

Training epoch-27 batch-71
Running loss of epoch-27 batch-71 = 0.0011180929141119123

Training epoch-27 batch-72
Running loss of epoch-27 batch-72 = 0.0004491569707170129

Training epoch-27 batch-73
Running loss of epoch-27 batch-73 = 0.0006910035153850913

Training epoch-27 batch-74
Running loss of epoch-27 batch-74 = 0.0003974308492615819

Training epoch-27 batch-75
Running loss of epoch-27 batch-75 = 0.0005765964742749929

Training epoch-27 batch-76
Running loss of epoch-27 batch-76 = 0.0008812426822260022

Training epoch-27 batch-77
Running loss of epoch-27 batch-77 = 0.0005849488079547882

Training epoch-27 batch-78
Running loss of epoch-27 batch-78 = 0.0003767304588109255

Training epoch-27 batch-79
Running loss of epoch-27 batch-79 = 0.00014165532775223255

Training epoch-27 batch-80
Running loss of epoch-27 batch-80 = 0.0017535355873405933

Training epoch-27 batch-81
Running loss of epoch-27 batch-81 = 0.001754834083840251

Training epoch-27 batch-82
Running loss of epoch-27 batch-82 = 0.0014856779016554356

Training epoch-27 batch-83
Running loss of epoch-27 batch-83 = 0.00125974602997303

Training epoch-27 batch-84
Running loss of epoch-27 batch-84 = 7.357774302363396e-05

Training epoch-27 batch-85
Running loss of epoch-27 batch-85 = 0.0002588158240541816

Training epoch-27 batch-86
Running loss of epoch-27 batch-86 = 0.000978806521743536

Training epoch-27 batch-87
Running loss of epoch-27 batch-87 = 0.00045893003698438406

Training epoch-27 batch-88
Running loss of epoch-27 batch-88 = 0.00028262799605727196

Training epoch-27 batch-89
Running loss of epoch-27 batch-89 = 0.0023466020356863737

Training epoch-27 batch-90
Running loss of epoch-27 batch-90 = 0.000895346631295979

Training epoch-27 batch-91
Running loss of epoch-27 batch-91 = 0.0008097095414996147

Training epoch-27 batch-92
Running loss of epoch-27 batch-92 = 0.001257393159903586

Training epoch-27 batch-93
Running loss of epoch-27 batch-93 = 0.00037251156754791737

Training epoch-27 batch-94
Running loss of epoch-27 batch-94 = 0.0005438521038740873

Training epoch-27 batch-95
Running loss of epoch-27 batch-95 = 0.0005158239509910345

Training epoch-27 batch-96
Running loss of epoch-27 batch-96 = 0.0004752168897539377

Training epoch-27 batch-97
Running loss of epoch-27 batch-97 = 0.0007112515158951283

Training epoch-27 batch-98
Running loss of epoch-27 batch-98 = 0.0004388297675177455

Training epoch-27 batch-99
Running loss of epoch-27 batch-99 = 0.0006102321203798056

Training epoch-27 batch-100
Running loss of epoch-27 batch-100 = 0.001105173141695559

Training epoch-27 batch-101
Running loss of epoch-27 batch-101 = 0.00032284774351865053

Training epoch-27 batch-102
Running loss of epoch-27 batch-102 = 0.0004346719942986965

Training epoch-27 batch-103
Running loss of epoch-27 batch-103 = 0.00029973406344652176

Training epoch-27 batch-104
Running loss of epoch-27 batch-104 = 0.00022580940276384354

Training epoch-27 batch-105
Running loss of epoch-27 batch-105 = 0.00034668552689254284

Training epoch-27 batch-106
Running loss of epoch-27 batch-106 = 0.001530541107058525

Training epoch-27 batch-107
Running loss of epoch-27 batch-107 = 0.0005638992879539728

Training epoch-27 batch-108
Running loss of epoch-27 batch-108 = 0.0002736541209742427

Training epoch-27 batch-109
Running loss of epoch-27 batch-109 = 0.00010641897097229958

Training epoch-27 batch-110
Running loss of epoch-27 batch-110 = 0.00018247566185891628

Training epoch-27 batch-111
Running loss of epoch-27 batch-111 = 0.000663578393869102

Training epoch-27 batch-112
Running loss of epoch-27 batch-112 = 0.0003002195153385401

Training epoch-27 batch-113
Running loss of epoch-27 batch-113 = 0.00035313202533870935

Training epoch-27 batch-114
Running loss of epoch-27 batch-114 = 0.0007599150994792581

Training epoch-27 batch-115
Running loss of epoch-27 batch-115 = 0.00013458076864480972

Training epoch-27 batch-116
Running loss of epoch-27 batch-116 = 0.0006107424851506948

Training epoch-27 batch-117
Running loss of epoch-27 batch-117 = 0.0008440726669505239

Training epoch-27 batch-118
Running loss of epoch-27 batch-118 = 0.000599695835262537

Training epoch-27 batch-119
Running loss of epoch-27 batch-119 = 0.00027872901409864426

Training epoch-27 batch-120
Running loss of epoch-27 batch-120 = 0.0003257667412981391

Training epoch-27 batch-121
Running loss of epoch-27 batch-121 = 0.00029224297031760216

Training epoch-27 batch-122
Running loss of epoch-27 batch-122 = 0.000222853384912014

Training epoch-27 batch-123
Running loss of epoch-27 batch-123 = 0.0013944108504801989

Training epoch-27 batch-124
Running loss of epoch-27 batch-124 = 0.00011915655340999365

Training epoch-27 batch-125
Running loss of epoch-27 batch-125 = 0.0011195458937436342

Training epoch-27 batch-126
Running loss of epoch-27 batch-126 = 0.0005714514991268516

Training epoch-27 batch-127
Running loss of epoch-27 batch-127 = 0.00033418089151382446

Training epoch-27 batch-128
Running loss of epoch-27 batch-128 = 0.0010815192945301533

Training epoch-27 batch-129
Running loss of epoch-27 batch-129 = 0.00032499164808541536

Training epoch-27 batch-130
Running loss of epoch-27 batch-130 = 0.0005698661552742124

Training epoch-27 batch-131
Running loss of epoch-27 batch-131 = 0.0003361531998962164

Training epoch-27 batch-132
Running loss of epoch-27 batch-132 = 0.0001738639548420906

Training epoch-27 batch-133
Running loss of epoch-27 batch-133 = 0.0007838895544409752

Training epoch-27 batch-134
Running loss of epoch-27 batch-134 = 0.0001314713153988123

Training epoch-27 batch-135
Running loss of epoch-27 batch-135 = 0.0011653363471850753

Training epoch-27 batch-136
Running loss of epoch-27 batch-136 = 0.0005125640891492367

Training epoch-27 batch-137
Running loss of epoch-27 batch-137 = 0.0007009635446593165

Training epoch-27 batch-138
Running loss of epoch-27 batch-138 = 0.0009106856305152178

Training epoch-27 batch-139
Running loss of epoch-27 batch-139 = 0.00019801268354058266

Training epoch-27 batch-140
Running loss of epoch-27 batch-140 = 0.0006872068624943495

Training epoch-27 batch-141
Running loss of epoch-27 batch-141 = 0.0008589279605075717

Training epoch-27 batch-142
Running loss of epoch-27 batch-142 = 0.0012900375295430422

Training epoch-27 batch-143
Running loss of epoch-27 batch-143 = 0.0002013706834986806

Training epoch-27 batch-144
Running loss of epoch-27 batch-144 = 9.465403854846954e-05

Training epoch-27 batch-145
Running loss of epoch-27 batch-145 = 0.0010298349661752582

Training epoch-27 batch-146
Running loss of epoch-27 batch-146 = 8.985446766018867e-05

Training epoch-27 batch-147
Running loss of epoch-27 batch-147 = 0.00046700541861355305

Training epoch-27 batch-148
Running loss of epoch-27 batch-148 = 0.00030676648020744324

Training epoch-27 batch-149
Running loss of epoch-27 batch-149 = 0.001124419504776597

Training epoch-27 batch-150
Running loss of epoch-27 batch-150 = 0.0006503403419628739

Training epoch-27 batch-151
Running loss of epoch-27 batch-151 = 0.00039396993815898895

Training epoch-27 batch-152
Running loss of epoch-27 batch-152 = 0.0008987071923911572

Training epoch-27 batch-153
Running loss of epoch-27 batch-153 = 0.00031657563522458076

Training epoch-27 batch-154
Running loss of epoch-27 batch-154 = 0.0006672940216958523

Training epoch-27 batch-155
Running loss of epoch-27 batch-155 = 0.00015155388973653316

Training epoch-27 batch-156
Running loss of epoch-27 batch-156 = 0.0008206682978197932

Training epoch-27 batch-157
Running loss of epoch-27 batch-157 = 0.0020335763692855835

Finished training epoch-27.



Average train loss at epoch-27 = 0.0006534794006496668

Started Evaluation

Average val loss at epoch-27 = 1.146225512834606

Accuracy for classes:
Accuracy for class equals is: 95.05 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 82.46 %
Accuracy for class onCreate is: 88.59 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 59.82 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 61.21 %
Accuracy for class execute is: 40.96 %
Accuracy for class get is: 71.79 %

Overall Accuracy = 80.83 %

Finished Evaluation



Started training epoch-28


Training epoch-28 batch-1
Running loss of epoch-28 batch-1 = 0.00020161829888820648

Training epoch-28 batch-2
Running loss of epoch-28 batch-2 = 0.00024929072242230177

Training epoch-28 batch-3
Running loss of epoch-28 batch-3 = 0.0006351913325488567

Training epoch-28 batch-4
Running loss of epoch-28 batch-4 = 0.000866838963702321

Training epoch-28 batch-5
Running loss of epoch-28 batch-5 = 0.0004533693427219987

Training epoch-28 batch-6
Running loss of epoch-28 batch-6 = 0.000495689339004457

Training epoch-28 batch-7
Running loss of epoch-28 batch-7 = 0.00023682252503931522

Training epoch-28 batch-8
Running loss of epoch-28 batch-8 = 0.0002192992251366377

Training epoch-28 batch-9
Running loss of epoch-28 batch-9 = 0.0011236992431804538

Training epoch-28 batch-10
Running loss of epoch-28 batch-10 = 0.0011967888567596674

Training epoch-28 batch-11
Running loss of epoch-28 batch-11 = 0.00032694311812520027

Training epoch-28 batch-12
Running loss of epoch-28 batch-12 = 0.00038923020474612713

Training epoch-28 batch-13
Running loss of epoch-28 batch-13 = 0.0003793078940361738

Training epoch-28 batch-14
Running loss of epoch-28 batch-14 = 0.00013369298540055752

Training epoch-28 batch-15
Running loss of epoch-28 batch-15 = 0.0007375486893579364

Training epoch-28 batch-16
Running loss of epoch-28 batch-16 = 0.0006789194885641336

Training epoch-28 batch-17
Running loss of epoch-28 batch-17 = 0.00019772746600210667

Training epoch-28 batch-18
Running loss of epoch-28 batch-18 = 0.00018660561181604862

Training epoch-28 batch-19
Running loss of epoch-28 batch-19 = 0.0005905600264668465

Training epoch-28 batch-20
Running loss of epoch-28 batch-20 = 0.0005971539067104459

Training epoch-28 batch-21
Running loss of epoch-28 batch-21 = 7.48347956687212e-05

Training epoch-28 batch-22
Running loss of epoch-28 batch-22 = 0.0003995620645582676

Training epoch-28 batch-23
Running loss of epoch-28 batch-23 = 0.0016467454843223095

Training epoch-28 batch-24
Running loss of epoch-28 batch-24 = 0.0002841055393218994

Training epoch-28 batch-25
Running loss of epoch-28 batch-25 = 0.0005301145138218999

Training epoch-28 batch-26
Running loss of epoch-28 batch-26 = 0.00027478288393467665

Training epoch-28 batch-27
Running loss of epoch-28 batch-27 = 0.0006373515352606773

Training epoch-28 batch-28
Running loss of epoch-28 batch-28 = 0.0003961882321164012

Training epoch-28 batch-29
Running loss of epoch-28 batch-29 = 0.0006805669981986284

Training epoch-28 batch-30
Running loss of epoch-28 batch-30 = 0.002326121088117361

Training epoch-28 batch-31
Running loss of epoch-28 batch-31 = 0.0001320077572017908

Training epoch-28 batch-32
Running loss of epoch-28 batch-32 = 0.0002996823750436306

Training epoch-28 batch-33
Running loss of epoch-28 batch-33 = 0.00018121232278645039

Training epoch-28 batch-34
Running loss of epoch-28 batch-34 = 0.0005763773806393147

Training epoch-28 batch-35
Running loss of epoch-28 batch-35 = 0.0014659948647022247

Training epoch-28 batch-36
Running loss of epoch-28 batch-36 = 0.00070933997631073

Training epoch-28 batch-37
Running loss of epoch-28 batch-37 = 0.0013461696216836572

Training epoch-28 batch-38
Running loss of epoch-28 batch-38 = 0.00023423577658832073

Training epoch-28 batch-39
Running loss of epoch-28 batch-39 = 0.0004202835261821747

Training epoch-28 batch-40
Running loss of epoch-28 batch-40 = 6.129452958703041e-05

Training epoch-28 batch-41
Running loss of epoch-28 batch-41 = 0.0005930466577410698

Training epoch-28 batch-42
Running loss of epoch-28 batch-42 = 0.00037528888788074255

Training epoch-28 batch-43
Running loss of epoch-28 batch-43 = 0.0027514335233718157

Training epoch-28 batch-44
Running loss of epoch-28 batch-44 = 0.00023537117522209883

Training epoch-28 batch-45
Running loss of epoch-28 batch-45 = 0.0003303063567727804

Training epoch-28 batch-46
Running loss of epoch-28 batch-46 = 0.00038006692193448544

Training epoch-28 batch-47
Running loss of epoch-28 batch-47 = 0.0007286337204277515

Training epoch-28 batch-48
Running loss of epoch-28 batch-48 = 0.00020984478760510683

Training epoch-28 batch-49
Running loss of epoch-28 batch-49 = 0.0026218232233077288

Training epoch-28 batch-50
Running loss of epoch-28 batch-50 = 0.0015541897155344486

Training epoch-28 batch-51
Running loss of epoch-28 batch-51 = 0.00038922857493162155

Training epoch-28 batch-52
Running loss of epoch-28 batch-52 = 0.000485358526930213

Training epoch-28 batch-53
Running loss of epoch-28 batch-53 = 0.0013892200076952577

Training epoch-28 batch-54
Running loss of epoch-28 batch-54 = 0.001058438210748136

Training epoch-28 batch-55
Running loss of epoch-28 batch-55 = 0.0006371531635522842

Training epoch-28 batch-56
Running loss of epoch-28 batch-56 = 0.00038706359919160604

Training epoch-28 batch-57
Running loss of epoch-28 batch-57 = 0.0005298929754644632

Training epoch-28 batch-58
Running loss of epoch-28 batch-58 = 0.0010828576050698757

Training epoch-28 batch-59
Running loss of epoch-28 batch-59 = 0.0014456936623901129

Training epoch-28 batch-60
Running loss of epoch-28 batch-60 = 0.0005166502669453621

Training epoch-28 batch-61
Running loss of epoch-28 batch-61 = 0.0006304309936240315

Training epoch-28 batch-62
Running loss of epoch-28 batch-62 = 0.0007014723960310221

Training epoch-28 batch-63
Running loss of epoch-28 batch-63 = 0.0004716466646641493

Training epoch-28 batch-64
Running loss of epoch-28 batch-64 = 0.0004988336004316807

Training epoch-28 batch-65
Running loss of epoch-28 batch-65 = 0.0007731965743005276

Training epoch-28 batch-66
Running loss of epoch-28 batch-66 = 0.00025485025253146887

Training epoch-28 batch-67
Running loss of epoch-28 batch-67 = 0.0002986516337841749

Training epoch-28 batch-68
Running loss of epoch-28 batch-68 = 0.0002497485838830471

Training epoch-28 batch-69
Running loss of epoch-28 batch-69 = 0.0018989266827702522

Training epoch-28 batch-70
Running loss of epoch-28 batch-70 = 0.001028643804602325

Training epoch-28 batch-71
Running loss of epoch-28 batch-71 = 0.0011380754876881838

Training epoch-28 batch-72
Running loss of epoch-28 batch-72 = 0.0007118192734196782

Training epoch-28 batch-73
Running loss of epoch-28 batch-73 = 0.00039009121246635914

Training epoch-28 batch-74
Running loss of epoch-28 batch-74 = 0.00025690102484077215

Training epoch-28 batch-75
Running loss of epoch-28 batch-75 = 0.0004773536929860711

Training epoch-28 batch-76
Running loss of epoch-28 batch-76 = 0.000323982909321785

Training epoch-28 batch-77
Running loss of epoch-28 batch-77 = 0.0013697835383936763

Training epoch-28 batch-78
Running loss of epoch-28 batch-78 = 0.001152892829850316

Training epoch-28 batch-79
Running loss of epoch-28 batch-79 = 0.0002853204496204853

Training epoch-28 batch-80
Running loss of epoch-28 batch-80 = 0.0006081743631511927

Training epoch-28 batch-81
Running loss of epoch-28 batch-81 = 0.00019919173792004585

Training epoch-28 batch-82
Running loss of epoch-28 batch-82 = 0.00099930539727211

Training epoch-28 batch-83
Running loss of epoch-28 batch-83 = 0.0007932916050776839

Training epoch-28 batch-84
Running loss of epoch-28 batch-84 = 0.0007657625246793032

Training epoch-28 batch-85
Running loss of epoch-28 batch-85 = 0.0005723111098632216

Training epoch-28 batch-86
Running loss of epoch-28 batch-86 = 0.0001940110232681036

Training epoch-28 batch-87
Running loss of epoch-28 batch-87 = 0.0005586659535765648

Training epoch-28 batch-88
Running loss of epoch-28 batch-88 = 0.0007060281932353973

Training epoch-28 batch-89
Running loss of epoch-28 batch-89 = 0.0011054766364395618

Training epoch-28 batch-90
Running loss of epoch-28 batch-90 = 0.0011171222431585193

Training epoch-28 batch-91
Running loss of epoch-28 batch-91 = 0.0011053443886339664

Training epoch-28 batch-92
Running loss of epoch-28 batch-92 = 0.001126425457186997

Training epoch-28 batch-93
Running loss of epoch-28 batch-93 = 0.00067966862116009

Training epoch-28 batch-94
Running loss of epoch-28 batch-94 = 0.0006951979594305158

Training epoch-28 batch-95
Running loss of epoch-28 batch-95 = 0.0010074954479932785

Training epoch-28 batch-96
Running loss of epoch-28 batch-96 = 0.0011560587445273995

Training epoch-28 batch-97
Running loss of epoch-28 batch-97 = 0.000565280788578093

Training epoch-28 batch-98
Running loss of epoch-28 batch-98 = 0.00036522536538541317

Training epoch-28 batch-99
Running loss of epoch-28 batch-99 = 0.0005755053134635091

Training epoch-28 batch-100
Running loss of epoch-28 batch-100 = 0.0006431182846426964

Training epoch-28 batch-101
Running loss of epoch-28 batch-101 = 0.0009765195427462459

Training epoch-28 batch-102
Running loss of epoch-28 batch-102 = 0.0009431388461962342

Training epoch-28 batch-103
Running loss of epoch-28 batch-103 = 0.0012649750569835305

Training epoch-28 batch-104
Running loss of epoch-28 batch-104 = 0.00037996924947947264

Training epoch-28 batch-105
Running loss of epoch-28 batch-105 = 0.0009415493113920093

Training epoch-28 batch-106
Running loss of epoch-28 batch-106 = 0.0006463864119723439

Training epoch-28 batch-107
Running loss of epoch-28 batch-107 = 8.687912486493587e-05

Training epoch-28 batch-108
Running loss of epoch-28 batch-108 = 0.0006606497336179018

Training epoch-28 batch-109
Running loss of epoch-28 batch-109 = 9.924080222845078e-05

Training epoch-28 batch-110
Running loss of epoch-28 batch-110 = 0.0008647608337923884

Training epoch-28 batch-111
Running loss of epoch-28 batch-111 = 0.0005462398985400796

Training epoch-28 batch-112
Running loss of epoch-28 batch-112 = 9.294506162405014e-05

Training epoch-28 batch-113
Running loss of epoch-28 batch-113 = 0.00033530709333717823

Training epoch-28 batch-114
Running loss of epoch-28 batch-114 = 0.0005753510631620884

Training epoch-28 batch-115
Running loss of epoch-28 batch-115 = 0.0011933704372495413

Training epoch-28 batch-116
Running loss of epoch-28 batch-116 = 0.0005709710530936718

Training epoch-28 batch-117
Running loss of epoch-28 batch-117 = 0.0002956964308395982

Training epoch-28 batch-118
Running loss of epoch-28 batch-118 = 0.00032078870572149754

Training epoch-28 batch-119
Running loss of epoch-28 batch-119 = 0.0004288868512958288

Training epoch-28 batch-120
Running loss of epoch-28 batch-120 = 0.0001108876895159483

Training epoch-28 batch-121
Running loss of epoch-28 batch-121 = 0.000235656276345253

Training epoch-28 batch-122
Running loss of epoch-28 batch-122 = 0.00039094150997698307

Training epoch-28 batch-123
Running loss of epoch-28 batch-123 = 0.0001933544408529997

Training epoch-28 batch-124
Running loss of epoch-28 batch-124 = 0.0009068341460078955

Training epoch-28 batch-125
Running loss of epoch-28 batch-125 = 0.00040767365135252476

Training epoch-28 batch-126
Running loss of epoch-28 batch-126 = 0.00030436483211815357

Training epoch-28 batch-127
Running loss of epoch-28 batch-127 = 0.000796872191131115

Training epoch-28 batch-128
Running loss of epoch-28 batch-128 = 0.00023992592468857765

Training epoch-28 batch-129
Running loss of epoch-28 batch-129 = 0.00023003388196229935

Training epoch-28 batch-130
Running loss of epoch-28 batch-130 = 0.00024326087441295385

Training epoch-28 batch-131
Running loss of epoch-28 batch-131 = 0.0001088390126824379

Training epoch-28 batch-132
Running loss of epoch-28 batch-132 = 0.00021781411487609148

Training epoch-28 batch-133
Running loss of epoch-28 batch-133 = 0.0006664253305643797

Training epoch-28 batch-134
Running loss of epoch-28 batch-134 = 0.0004963120445609093

Training epoch-28 batch-135
Running loss of epoch-28 batch-135 = 0.0003586991224437952

Training epoch-28 batch-136
Running loss of epoch-28 batch-136 = 0.0009456510888412595

Training epoch-28 batch-137
Running loss of epoch-28 batch-137 = 0.0004777254071086645

Training epoch-28 batch-138
Running loss of epoch-28 batch-138 = 0.00026778806932270527

Training epoch-28 batch-139
Running loss of epoch-28 batch-139 = 0.00024399440735578537

Training epoch-28 batch-140
Running loss of epoch-28 batch-140 = 0.00012969574891030788

Training epoch-28 batch-141
Running loss of epoch-28 batch-141 = 0.0009403948206454515

Training epoch-28 batch-142
Running loss of epoch-28 batch-142 = 0.0009025076869875193

Training epoch-28 batch-143
Running loss of epoch-28 batch-143 = 0.00017083308193832636

Training epoch-28 batch-144
Running loss of epoch-28 batch-144 = 0.00039113452658057213

Training epoch-28 batch-145
Running loss of epoch-28 batch-145 = 0.0004645731532946229

Training epoch-28 batch-146
Running loss of epoch-28 batch-146 = 0.00015449384227395058

Training epoch-28 batch-147
Running loss of epoch-28 batch-147 = 0.0012460809666663408

Training epoch-28 batch-148
Running loss of epoch-28 batch-148 = 0.0004094252362847328

Training epoch-28 batch-149
Running loss of epoch-28 batch-149 = 0.0004341502208262682

Training epoch-28 batch-150
Running loss of epoch-28 batch-150 = 0.00013181637041270733

Training epoch-28 batch-151
Running loss of epoch-28 batch-151 = 0.00021654681768268347

Training epoch-28 batch-152
Running loss of epoch-28 batch-152 = 0.0005203915061429143

Training epoch-28 batch-153
Running loss of epoch-28 batch-153 = 0.00014617957640439272

Training epoch-28 batch-154
Running loss of epoch-28 batch-154 = 0.0006226199911907315

Training epoch-28 batch-155
Running loss of epoch-28 batch-155 = 0.0002523415023460984

Training epoch-28 batch-156
Running loss of epoch-28 batch-156 = 0.00016657798551023006

Training epoch-28 batch-157
Running loss of epoch-28 batch-157 = 0.0033547785133123398

Finished training epoch-28.



Average train loss at epoch-28 = 0.000613960000127554

Started Evaluation

Average val loss at epoch-28 = 1.2282821109310087

Accuracy for classes:
Accuracy for class equals is: 97.36 %
Accuracy for class main is: 94.26 %
Accuracy for class setUp is: 93.77 %
Accuracy for class onCreate is: 92.64 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 57.99 %
Accuracy for class hashCode is: 98.88 %
Accuracy for class init is: 45.29 %
Accuracy for class execute is: 53.82 %
Accuracy for class get is: 62.05 %

Overall Accuracy = 81.43 %

Finished Evaluation



Started training epoch-29


Training epoch-29 batch-1
Running loss of epoch-29 batch-1 = 0.0004138090880587697

Training epoch-29 batch-2
Running loss of epoch-29 batch-2 = 0.000629234709776938

Training epoch-29 batch-3
Running loss of epoch-29 batch-3 = 0.0008842562092468143

Training epoch-29 batch-4
Running loss of epoch-29 batch-4 = 0.0002545565366744995

Training epoch-29 batch-5
Running loss of epoch-29 batch-5 = 0.0011592693626880646

Training epoch-29 batch-6
Running loss of epoch-29 batch-6 = 0.0010443242499604821

Training epoch-29 batch-7
Running loss of epoch-29 batch-7 = 0.0008644349873065948

Training epoch-29 batch-8
Running loss of epoch-29 batch-8 = 0.0003401231952011585

Training epoch-29 batch-9
Running loss of epoch-29 batch-9 = 0.00039914995431900024

Training epoch-29 batch-10
Running loss of epoch-29 batch-10 = 0.0002856343053281307

Training epoch-29 batch-11
Running loss of epoch-29 batch-11 = 0.00017287698574364185

Training epoch-29 batch-12
Running loss of epoch-29 batch-12 = 0.00034638307988643646

Training epoch-29 batch-13
Running loss of epoch-29 batch-13 = 0.0004116652999073267

Training epoch-29 batch-14
Running loss of epoch-29 batch-14 = 0.00020735803991556168

Training epoch-29 batch-15
Running loss of epoch-29 batch-15 = 0.0010453376453369856

Training epoch-29 batch-16
Running loss of epoch-29 batch-16 = 0.0005250320537015796

Training epoch-29 batch-17
Running loss of epoch-29 batch-17 = 0.00025668658781796694

Training epoch-29 batch-18
Running loss of epoch-29 batch-18 = 0.0001314847031608224

Training epoch-29 batch-19
Running loss of epoch-29 batch-19 = 0.0008223396725952625

Training epoch-29 batch-20
Running loss of epoch-29 batch-20 = 0.00048777542542666197

Training epoch-29 batch-21
Running loss of epoch-29 batch-21 = 0.00033806380815804005

Training epoch-29 batch-22
Running loss of epoch-29 batch-22 = 0.00014694686979055405

Training epoch-29 batch-23
Running loss of epoch-29 batch-23 = 6.550468970090151e-05

Training epoch-29 batch-24
Running loss of epoch-29 batch-24 = 0.0003183409571647644

Training epoch-29 batch-25
Running loss of epoch-29 batch-25 = 7.536495104432106e-05

Training epoch-29 batch-26
Running loss of epoch-29 batch-26 = 0.0002618932630866766

Training epoch-29 batch-27
Running loss of epoch-29 batch-27 = 0.00033374258782714605

Training epoch-29 batch-28
Running loss of epoch-29 batch-28 = 7.37875234335661e-05

Training epoch-29 batch-29
Running loss of epoch-29 batch-29 = 0.00017690728418529034

Training epoch-29 batch-30
Running loss of epoch-29 batch-30 = 0.00032504647970199585

Training epoch-29 batch-31
Running loss of epoch-29 batch-31 = 0.0002532247453927994

Training epoch-29 batch-32
Running loss of epoch-29 batch-32 = 0.00031593418680131435

Training epoch-29 batch-33
Running loss of epoch-29 batch-33 = 0.0003240793012082577

Training epoch-29 batch-34
Running loss of epoch-29 batch-34 = 7.700733840465546e-05

Training epoch-29 batch-35
Running loss of epoch-29 batch-35 = 0.0005924387369304895

Training epoch-29 batch-36
Running loss of epoch-29 batch-36 = 0.00020191294606775045

Training epoch-29 batch-37
Running loss of epoch-29 batch-37 = 0.00037177884951233864

Training epoch-29 batch-38
Running loss of epoch-29 batch-38 = 0.00025049829855561256

Training epoch-29 batch-39
Running loss of epoch-29 batch-39 = 0.00029795069713145494

Training epoch-29 batch-40
Running loss of epoch-29 batch-40 = 0.0005751531571149826

Training epoch-29 batch-41
Running loss of epoch-29 batch-41 = 0.00026353017892688513

Training epoch-29 batch-42
Running loss of epoch-29 batch-42 = 0.00020854664035141468

Training epoch-29 batch-43
Running loss of epoch-29 batch-43 = 0.00020283577032387257

Training epoch-29 batch-44
Running loss of epoch-29 batch-44 = 0.00016189110465347767

Training epoch-29 batch-45
Running loss of epoch-29 batch-45 = 0.0012002297444269061

Training epoch-29 batch-46
Running loss of epoch-29 batch-46 = 0.00033809663727879524

Training epoch-29 batch-47
Running loss of epoch-29 batch-47 = 0.00037672463804483414

Training epoch-29 batch-48
Running loss of epoch-29 batch-48 = 0.0001270161010324955

Training epoch-29 batch-49
Running loss of epoch-29 batch-49 = 0.0001904016826301813

Training epoch-29 batch-50
Running loss of epoch-29 batch-50 = 0.00018359499517828226

Training epoch-29 batch-51
Running loss of epoch-29 batch-51 = 0.00010409322567284107

Training epoch-29 batch-52
Running loss of epoch-29 batch-52 = 0.0002628008369356394

Training epoch-29 batch-53
Running loss of epoch-29 batch-53 = 0.0005586131010204554

Training epoch-29 batch-54
Running loss of epoch-29 batch-54 = 0.0003327218582853675

Training epoch-29 batch-55
Running loss of epoch-29 batch-55 = 0.00022586854174733162

Training epoch-29 batch-56
Running loss of epoch-29 batch-56 = 0.00012844661250710487

Training epoch-29 batch-57
Running loss of epoch-29 batch-57 = 0.0005224683554843068

Training epoch-29 batch-58
Running loss of epoch-29 batch-58 = 0.0002046853769570589

Training epoch-29 batch-59
Running loss of epoch-29 batch-59 = 0.0002367575652897358

Training epoch-29 batch-60
Running loss of epoch-29 batch-60 = 0.00021394481882452965

Training epoch-29 batch-61
Running loss of epoch-29 batch-61 = 0.0003085000207647681

Training epoch-29 batch-62
Running loss of epoch-29 batch-62 = 0.0002806603442877531

Training epoch-29 batch-63
Running loss of epoch-29 batch-63 = 0.0002059396356344223

Training epoch-29 batch-64
Running loss of epoch-29 batch-64 = 6.761006079614162e-05

Training epoch-29 batch-65
Running loss of epoch-29 batch-65 = 0.0006397147662937641

Training epoch-29 batch-66
Running loss of epoch-29 batch-66 = 0.00033946637995541096

Training epoch-29 batch-67
Running loss of epoch-29 batch-67 = 0.00021441583521664143

Training epoch-29 batch-68
Running loss of epoch-29 batch-68 = 0.0002394714392721653

Training epoch-29 batch-69
Running loss of epoch-29 batch-69 = 0.00046162703074514866

Training epoch-29 batch-70
Running loss of epoch-29 batch-70 = 0.00025224429555237293

Training epoch-29 batch-71
Running loss of epoch-29 batch-71 = 0.00027719256468117237

Training epoch-29 batch-72
Running loss of epoch-29 batch-72 = 5.8158766478300095e-05

Training epoch-29 batch-73
Running loss of epoch-29 batch-73 = 8.221832104027271e-05

Training epoch-29 batch-74
Running loss of epoch-29 batch-74 = 4.581012763082981e-05

Training epoch-29 batch-75
Running loss of epoch-29 batch-75 = 6.2687904573977e-05

Training epoch-29 batch-76
Running loss of epoch-29 batch-76 = 0.0003340428229421377

Training epoch-29 batch-77
Running loss of epoch-29 batch-77 = 0.00014151714276522398

Training epoch-29 batch-78
Running loss of epoch-29 batch-78 = 7.496075704693794e-05

Training epoch-29 batch-79
Running loss of epoch-29 batch-79 = 0.0002187641803175211

Training epoch-29 batch-80
Running loss of epoch-29 batch-80 = 0.00026358291506767273

Training epoch-29 batch-81
Running loss of epoch-29 batch-81 = 0.0004279662389308214

Training epoch-29 batch-82
Running loss of epoch-29 batch-82 = 0.0002904883585870266

Training epoch-29 batch-83
Running loss of epoch-29 batch-83 = 5.270249675959349e-05

Training epoch-29 batch-84
Running loss of epoch-29 batch-84 = 0.000500173307955265

Training epoch-29 batch-85
Running loss of epoch-29 batch-85 = 5.641044117510319e-05

Training epoch-29 batch-86
Running loss of epoch-29 batch-86 = 0.0007113700266927481

Training epoch-29 batch-87
Running loss of epoch-29 batch-87 = 0.00028785422910004854

Training epoch-29 batch-88
Running loss of epoch-29 batch-88 = 0.0006183427758514881

Training epoch-29 batch-89
Running loss of epoch-29 batch-89 = 0.00018764170818030834

Training epoch-29 batch-90
Running loss of epoch-29 batch-90 = 0.00030212767887860537

Training epoch-29 batch-91
Running loss of epoch-29 batch-91 = 0.00012252479791641235

Training epoch-29 batch-92
Running loss of epoch-29 batch-92 = 0.0008178027346730232

Training epoch-29 batch-93
Running loss of epoch-29 batch-93 = 0.00020146870519965887

Training epoch-29 batch-94
Running loss of epoch-29 batch-94 = 0.0013929534470662475

Training epoch-29 batch-95
Running loss of epoch-29 batch-95 = 0.00011010956950485706

Training epoch-29 batch-96
Running loss of epoch-29 batch-96 = 0.0005681612528860569

Training epoch-29 batch-97
Running loss of epoch-29 batch-97 = 0.0003679376095533371

Training epoch-29 batch-98
Running loss of epoch-29 batch-98 = 0.0005530151538550854

Training epoch-29 batch-99
Running loss of epoch-29 batch-99 = 0.00017685419879853725

Training epoch-29 batch-100
Running loss of epoch-29 batch-100 = 0.00031166733242571354

Training epoch-29 batch-101
Running loss of epoch-29 batch-101 = 0.00018389010801911354

Training epoch-29 batch-102
Running loss of epoch-29 batch-102 = 0.0002177718561142683

Training epoch-29 batch-103
Running loss of epoch-29 batch-103 = 6.288522854447365e-05

Training epoch-29 batch-104
Running loss of epoch-29 batch-104 = 0.00022233929485082626

Training epoch-29 batch-105
Running loss of epoch-29 batch-105 = 0.00023869331926107407

Training epoch-29 batch-106
Running loss of epoch-29 batch-106 = 0.00022263196296989918

Training epoch-29 batch-107
Running loss of epoch-29 batch-107 = 7.611315231770277e-05

Training epoch-29 batch-108
Running loss of epoch-29 batch-108 = 8.781347423791885e-05

Training epoch-29 batch-109
Running loss of epoch-29 batch-109 = 0.00015091896057128906

Training epoch-29 batch-110
Running loss of epoch-29 batch-110 = 0.0004702795995399356

Training epoch-29 batch-111
Running loss of epoch-29 batch-111 = 0.0002757543697953224

Training epoch-29 batch-112
Running loss of epoch-29 batch-112 = 0.00036033394280821085

Training epoch-29 batch-113
Running loss of epoch-29 batch-113 = 0.0007443535141646862

Training epoch-29 batch-114
Running loss of epoch-29 batch-114 = 0.0004178787348791957

Training epoch-29 batch-115
Running loss of epoch-29 batch-115 = 6.0368096455931664e-05

Training epoch-29 batch-116
Running loss of epoch-29 batch-116 = 5.921628326177597e-05

Training epoch-29 batch-117
Running loss of epoch-29 batch-117 = 0.0002528396435081959

Training epoch-29 batch-118
Running loss of epoch-29 batch-118 = 0.00013336190022528172

Training epoch-29 batch-119
Running loss of epoch-29 batch-119 = 0.001109679345972836

Training epoch-29 batch-120
Running loss of epoch-29 batch-120 = 0.00022124743554741144

Training epoch-29 batch-121
Running loss of epoch-29 batch-121 = 0.0002691249828785658

Training epoch-29 batch-122
Running loss of epoch-29 batch-122 = 0.001275329734198749

Training epoch-29 batch-123
Running loss of epoch-29 batch-123 = 0.0014720242470502853

Training epoch-29 batch-124
Running loss of epoch-29 batch-124 = 0.0006309875752776861

Training epoch-29 batch-125
Running loss of epoch-29 batch-125 = 0.00020899483934044838

Training epoch-29 batch-126
Running loss of epoch-29 batch-126 = 0.00022107921540737152

Training epoch-29 batch-127
Running loss of epoch-29 batch-127 = 0.0001814146526157856

Training epoch-29 batch-128
Running loss of epoch-29 batch-128 = 0.0001232251524925232

Training epoch-29 batch-129
Running loss of epoch-29 batch-129 = 0.00019810069352388382

Training epoch-29 batch-130
Running loss of epoch-29 batch-130 = 0.000598122482188046

Training epoch-29 batch-131
Running loss of epoch-29 batch-131 = 0.0010282879229635

Training epoch-29 batch-132
Running loss of epoch-29 batch-132 = 0.00012086727656424046

Training epoch-29 batch-133
Running loss of epoch-29 batch-133 = 7.951841689646244e-05

Training epoch-29 batch-134
Running loss of epoch-29 batch-134 = 0.00028537085745483637

Training epoch-29 batch-135
Running loss of epoch-29 batch-135 = 0.00016505783423781395

Training epoch-29 batch-136
Running loss of epoch-29 batch-136 = 0.0001482039224356413

Training epoch-29 batch-137
Running loss of epoch-29 batch-137 = 0.0005051703192293644

Training epoch-29 batch-138
Running loss of epoch-29 batch-138 = 0.00041076273191720247

Training epoch-29 batch-139
Running loss of epoch-29 batch-139 = 0.000752086634747684

Training epoch-29 batch-140
Running loss of epoch-29 batch-140 = 0.0001776751596480608

Training epoch-29 batch-141
Running loss of epoch-29 batch-141 = 0.00016739568673074245

Training epoch-29 batch-142
Running loss of epoch-29 batch-142 = 0.00040262402035295963

Training epoch-29 batch-143
Running loss of epoch-29 batch-143 = 0.0006191950524225831

Training epoch-29 batch-144
Running loss of epoch-29 batch-144 = 0.00019881641492247581

Training epoch-29 batch-145
Running loss of epoch-29 batch-145 = 0.00019653304480016232

Training epoch-29 batch-146
Running loss of epoch-29 batch-146 = 0.0002148717176169157

Training epoch-29 batch-147
Running loss of epoch-29 batch-147 = 0.0009921302553266287

Training epoch-29 batch-148
Running loss of epoch-29 batch-148 = 5.716737359762192e-05

Training epoch-29 batch-149
Running loss of epoch-29 batch-149 = 0.00010441732592880726

Training epoch-29 batch-150
Running loss of epoch-29 batch-150 = 0.0002817456843331456

Training epoch-29 batch-151
Running loss of epoch-29 batch-151 = 0.0005251559196040034

Training epoch-29 batch-152
Running loss of epoch-29 batch-152 = 0.0003836022224277258

Training epoch-29 batch-153
Running loss of epoch-29 batch-153 = 0.00032940832898020744

Training epoch-29 batch-154
Running loss of epoch-29 batch-154 = 0.0002633316908031702

Training epoch-29 batch-155
Running loss of epoch-29 batch-155 = 0.0006745078135281801

Training epoch-29 batch-156
Running loss of epoch-29 batch-156 = 0.0002001686953008175

Training epoch-29 batch-157
Running loss of epoch-29 batch-157 = 0.000334911048412323

Finished training epoch-29.



Average train loss at epoch-29 = 0.0003509843699634075

Started Evaluation

Average val loss at epoch-29 = 1.1442423166048876

Accuracy for classes:
Accuracy for class equals is: 95.71 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 88.69 %
Accuracy for class onCreate is: 89.45 %
Accuracy for class toString is: 84.64 %
Accuracy for class run is: 64.61 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 56.50 %
Accuracy for class execute is: 49.00 %
Accuracy for class get is: 64.62 %

Overall Accuracy = 81.84 %

Finished Evaluation



Started training epoch-30


Training epoch-30 batch-1
Running loss of epoch-30 batch-1 = 0.00015497044660151005

Training epoch-30 batch-2
Running loss of epoch-30 batch-2 = 0.00023964536376297474

Training epoch-30 batch-3
Running loss of epoch-30 batch-3 = 0.00010701268911361694

Training epoch-30 batch-4
Running loss of epoch-30 batch-4 = 0.000227632699534297

Training epoch-30 batch-5
Running loss of epoch-30 batch-5 = 0.00021618884056806564

Training epoch-30 batch-6
Running loss of epoch-30 batch-6 = 0.0004058036720380187

Training epoch-30 batch-7
Running loss of epoch-30 batch-7 = 0.00015698722563683987

Training epoch-30 batch-8
Running loss of epoch-30 batch-8 = 0.0004188345046713948

Training epoch-30 batch-9
Running loss of epoch-30 batch-9 = 0.0009318531956523657

Training epoch-30 batch-10
Running loss of epoch-30 batch-10 = 0.00014800229109823704

Training epoch-30 batch-11
Running loss of epoch-30 batch-11 = 6.582168862223625e-05

Training epoch-30 batch-12
Running loss of epoch-30 batch-12 = 0.0005104304291307926

Training epoch-30 batch-13
Running loss of epoch-30 batch-13 = 0.0002531129866838455

Training epoch-30 batch-14
Running loss of epoch-30 batch-14 = 0.00039963098242878914

Training epoch-30 batch-15
Running loss of epoch-30 batch-15 = 0.00015103467740118504

Training epoch-30 batch-16
Running loss of epoch-30 batch-16 = 0.00048458960372954607

Training epoch-30 batch-17
Running loss of epoch-30 batch-17 = 0.00016387971118092537

Training epoch-30 batch-18
Running loss of epoch-30 batch-18 = 0.00012499059084802866

Training epoch-30 batch-19
Running loss of epoch-30 batch-19 = 0.00011103879660367966

Training epoch-30 batch-20
Running loss of epoch-30 batch-20 = 0.00036449311301112175

Training epoch-30 batch-21
Running loss of epoch-30 batch-21 = 0.00023404695093631744

Training epoch-30 batch-22
Running loss of epoch-30 batch-22 = 0.00036672852002084255

Training epoch-30 batch-23
Running loss of epoch-30 batch-23 = 0.0007525226101279259

Training epoch-30 batch-24
Running loss of epoch-30 batch-24 = 0.00020990869961678982

Training epoch-30 batch-25
Running loss of epoch-30 batch-25 = 0.0003351034829393029

Training epoch-30 batch-26
Running loss of epoch-30 batch-26 = 0.00029958796221762896

Training epoch-30 batch-27
Running loss of epoch-30 batch-27 = 5.177222192287445e-05

Training epoch-30 batch-28
Running loss of epoch-30 batch-28 = 0.0004446213133633137

Training epoch-30 batch-29
Running loss of epoch-30 batch-29 = 0.0002850610762834549

Training epoch-30 batch-30
Running loss of epoch-30 batch-30 = 0.0002868417650461197

Training epoch-30 batch-31
Running loss of epoch-30 batch-31 = 0.0005248262314125896

Training epoch-30 batch-32
Running loss of epoch-30 batch-32 = 0.000176955945789814

Training epoch-30 batch-33
Running loss of epoch-30 batch-33 = 0.00020103808492422104

Training epoch-30 batch-34
Running loss of epoch-30 batch-34 = 0.0004890693817287683

Training epoch-30 batch-35
Running loss of epoch-30 batch-35 = 0.0006221832009032369

Training epoch-30 batch-36
Running loss of epoch-30 batch-36 = 0.0001360087189823389

Training epoch-30 batch-37
Running loss of epoch-30 batch-37 = 0.0002713315188884735

Training epoch-30 batch-38
Running loss of epoch-30 batch-38 = 0.0004230657359585166

Training epoch-30 batch-39
Running loss of epoch-30 batch-39 = 8.13888618722558e-05

Training epoch-30 batch-40
Running loss of epoch-30 batch-40 = 0.00045068073086440563

Training epoch-30 batch-41
Running loss of epoch-30 batch-41 = 5.489424802362919e-05

Training epoch-30 batch-42
Running loss of epoch-30 batch-42 = 0.00015406543388962746

Training epoch-30 batch-43
Running loss of epoch-30 batch-43 = 0.0002513702493160963

Training epoch-30 batch-44
Running loss of epoch-30 batch-44 = 0.0001762104220688343

Training epoch-30 batch-45
Running loss of epoch-30 batch-45 = 9.003258310258389e-05

Training epoch-30 batch-46
Running loss of epoch-30 batch-46 = 0.00013315631076693535

Training epoch-30 batch-47
Running loss of epoch-30 batch-47 = 0.00018257438205182552

Training epoch-30 batch-48
Running loss of epoch-30 batch-48 = 0.00033455423545092344

Training epoch-30 batch-49
Running loss of epoch-30 batch-49 = 0.00043920427560806274

Training epoch-30 batch-50
Running loss of epoch-30 batch-50 = 0.00019138050265610218

Training epoch-30 batch-51
Running loss of epoch-30 batch-51 = 9.20359743759036e-05

Training epoch-30 batch-52
Running loss of epoch-30 batch-52 = 8.35084356367588e-05

Training epoch-30 batch-53
Running loss of epoch-30 batch-53 = 0.0012932559475302696

Training epoch-30 batch-54
Running loss of epoch-30 batch-54 = 0.00025059410836547613

Training epoch-30 batch-55
Running loss of epoch-30 batch-55 = 8.472590707242489e-05

Training epoch-30 batch-56
Running loss of epoch-30 batch-56 = 0.0003032187232747674

Training epoch-30 batch-57
Running loss of epoch-30 batch-57 = 7.994892075657845e-05

Training epoch-30 batch-58
Running loss of epoch-30 batch-58 = 0.0003609651466831565

Training epoch-30 batch-59
Running loss of epoch-30 batch-59 = 0.00027740560472011566

Training epoch-30 batch-60
Running loss of epoch-30 batch-60 = 5.704467184841633e-05

Training epoch-30 batch-61
Running loss of epoch-30 batch-61 = 4.6476954594254494e-05

Training epoch-30 batch-62
Running loss of epoch-30 batch-62 = 0.00016109447460621595

Training epoch-30 batch-63
Running loss of epoch-30 batch-63 = 5.576235707849264e-05

Training epoch-30 batch-64
Running loss of epoch-30 batch-64 = 0.0002836028579622507

Training epoch-30 batch-65
Running loss of epoch-30 batch-65 = 7.934996392577887e-05

Training epoch-30 batch-66
Running loss of epoch-30 batch-66 = 3.223191015422344e-05

Training epoch-30 batch-67
Running loss of epoch-30 batch-67 = 9.926431812345982e-05

Training epoch-30 batch-68
Running loss of epoch-30 batch-68 = 0.00011011864989995956

Training epoch-30 batch-69
Running loss of epoch-30 batch-69 = 0.0005041270051151514

Training epoch-30 batch-70
Running loss of epoch-30 batch-70 = 3.321724943816662e-05

Training epoch-30 batch-71
Running loss of epoch-30 batch-71 = 0.00017009745351970196

Training epoch-30 batch-72
Running loss of epoch-30 batch-72 = 0.0003434589598327875

Training epoch-30 batch-73
Running loss of epoch-30 batch-73 = 0.00019182940013706684

Training epoch-30 batch-74
Running loss of epoch-30 batch-74 = 0.0001641486305743456

Training epoch-30 batch-75
Running loss of epoch-30 batch-75 = 0.00021507195197045803

Training epoch-30 batch-76
Running loss of epoch-30 batch-76 = 0.0007608790183439851

Training epoch-30 batch-77
Running loss of epoch-30 batch-77 = 4.845019429922104e-05

Training epoch-30 batch-78
Running loss of epoch-30 batch-78 = 0.0002747531980276108

Training epoch-30 batch-79
Running loss of epoch-30 batch-79 = 0.00015829247422516346

Training epoch-30 batch-80
Running loss of epoch-30 batch-80 = 0.00018874870147556067

Training epoch-30 batch-81
Running loss of epoch-30 batch-81 = 0.00085826450958848

Training epoch-30 batch-82
Running loss of epoch-30 batch-82 = 0.0009153678547590971

Training epoch-30 batch-83
Running loss of epoch-30 batch-83 = 0.0003722838591784239

Training epoch-30 batch-84
Running loss of epoch-30 batch-84 = 6.332062184810638e-05

Training epoch-30 batch-85
Running loss of epoch-30 batch-85 = 5.553197115659714e-05

Training epoch-30 batch-86
Running loss of epoch-30 batch-86 = 0.00030301325023174286

Training epoch-30 batch-87
Running loss of epoch-30 batch-87 = 0.00040525710210204124

Training epoch-30 batch-88
Running loss of epoch-30 batch-88 = 0.00014793546870350838

Training epoch-30 batch-89
Running loss of epoch-30 batch-89 = 0.000238500302657485

Training epoch-30 batch-90
Running loss of epoch-30 batch-90 = 0.00010225409641861916

Training epoch-30 batch-91
Running loss of epoch-30 batch-91 = 9.662029333412647e-05

Training epoch-30 batch-92
Running loss of epoch-30 batch-92 = 0.0007248203037306666

Training epoch-30 batch-93
Running loss of epoch-30 batch-93 = 0.00010085292160511017

Training epoch-30 batch-94
Running loss of epoch-30 batch-94 = 0.00016891886480152607

Training epoch-30 batch-95
Running loss of epoch-30 batch-95 = 0.0009598457254469395

Training epoch-30 batch-96
Running loss of epoch-30 batch-96 = 0.00010652351193130016

Training epoch-30 batch-97
Running loss of epoch-30 batch-97 = 0.000504267169162631

Training epoch-30 batch-98
Running loss of epoch-30 batch-98 = 0.00012007914483547211

Training epoch-30 batch-99
Running loss of epoch-30 batch-99 = 0.0016025016084313393

Training epoch-30 batch-100
Running loss of epoch-30 batch-100 = 0.00015894300304353237

Training epoch-30 batch-101
Running loss of epoch-30 batch-101 = 0.00043925095815211535

Training epoch-30 batch-102
Running loss of epoch-30 batch-102 = 0.000529757235199213

Training epoch-30 batch-103
Running loss of epoch-30 batch-103 = 9.641377255320549e-05

Training epoch-30 batch-104
Running loss of epoch-30 batch-104 = 0.00011453195475041866

Training epoch-30 batch-105
Running loss of epoch-30 batch-105 = 8.812802843749523e-05

Training epoch-30 batch-106
Running loss of epoch-30 batch-106 = 0.0004636391531676054

Training epoch-30 batch-107
Running loss of epoch-30 batch-107 = 0.0004316348349675536

Training epoch-30 batch-108
Running loss of epoch-30 batch-108 = 0.00010124011896550655

Training epoch-30 batch-109
Running loss of epoch-30 batch-109 = 0.0011935544898733497

Training epoch-30 batch-110
Running loss of epoch-30 batch-110 = 0.0003449227660894394

Training epoch-30 batch-111
Running loss of epoch-30 batch-111 = 7.40287359803915e-05

Training epoch-30 batch-112
Running loss of epoch-30 batch-112 = 0.0005082504358142614

Training epoch-30 batch-113
Running loss of epoch-30 batch-113 = 0.00020462926477193832

Training epoch-30 batch-114
Running loss of epoch-30 batch-114 = 0.00038312911055982113

Training epoch-30 batch-115
Running loss of epoch-30 batch-115 = 0.00026345765218138695

Training epoch-30 batch-116
Running loss of epoch-30 batch-116 = 0.00012762052938342094

Training epoch-30 batch-117
Running loss of epoch-30 batch-117 = 0.0003893948160111904

Training epoch-30 batch-118
Running loss of epoch-30 batch-118 = 0.00019042869098484516

Training epoch-30 batch-119
Running loss of epoch-30 batch-119 = 0.00031278934329748154

Training epoch-30 batch-120
Running loss of epoch-30 batch-120 = 0.00019672478083521128

Training epoch-30 batch-121
Running loss of epoch-30 batch-121 = 0.00025482848286628723

Training epoch-30 batch-122
Running loss of epoch-30 batch-122 = 0.00025997322518378496

Training epoch-30 batch-123
Running loss of epoch-30 batch-123 = 0.0003933452535420656

Training epoch-30 batch-124
Running loss of epoch-30 batch-124 = 0.000654035247862339

Training epoch-30 batch-125
Running loss of epoch-30 batch-125 = 0.00026147812604904175

Training epoch-30 batch-126
Running loss of epoch-30 batch-126 = 0.00019160553347319365

Training epoch-30 batch-127
Running loss of epoch-30 batch-127 = 0.00011558970436453819

Training epoch-30 batch-128
Running loss of epoch-30 batch-128 = 0.00018094643019139767

Training epoch-30 batch-129
Running loss of epoch-30 batch-129 = 0.00014477293007075787

Training epoch-30 batch-130
Running loss of epoch-30 batch-130 = 0.00022907741367816925

Training epoch-30 batch-131
Running loss of epoch-30 batch-131 = 0.000605504959821701

Training epoch-30 batch-132
Running loss of epoch-30 batch-132 = 0.00033871689811348915

Training epoch-30 batch-133
Running loss of epoch-30 batch-133 = 0.00018050335347652435

Training epoch-30 batch-134
Running loss of epoch-30 batch-134 = 5.740497726947069e-05

Training epoch-30 batch-135
Running loss of epoch-30 batch-135 = 0.0001804395578801632

Training epoch-30 batch-136
Running loss of epoch-30 batch-136 = 0.00010988395661115646

Training epoch-30 batch-137
Running loss of epoch-30 batch-137 = 0.0003096781438216567

Training epoch-30 batch-138
Running loss of epoch-30 batch-138 = 0.00018089264631271362

Training epoch-30 batch-139
Running loss of epoch-30 batch-139 = 0.00029081839602440596

Training epoch-30 batch-140
Running loss of epoch-30 batch-140 = 4.823110066354275e-05

Training epoch-30 batch-141
Running loss of epoch-30 batch-141 = 3.743334673345089e-05

Training epoch-30 batch-142
Running loss of epoch-30 batch-142 = 7.119658403098583e-05

Training epoch-30 batch-143
Running loss of epoch-30 batch-143 = 0.0001429482363164425

Training epoch-30 batch-144
Running loss of epoch-30 batch-144 = 0.00010704458691179752

Training epoch-30 batch-145
Running loss of epoch-30 batch-145 = 0.00012588384561240673

Training epoch-30 batch-146
Running loss of epoch-30 batch-146 = 0.00017152377404272556

Training epoch-30 batch-147
Running loss of epoch-30 batch-147 = 0.0004739193245768547

Training epoch-30 batch-148
Running loss of epoch-30 batch-148 = 0.0007453282596543431

Training epoch-30 batch-149
Running loss of epoch-30 batch-149 = 0.00038296368438750505

Training epoch-30 batch-150
Running loss of epoch-30 batch-150 = 0.00014941394329071045

Training epoch-30 batch-151
Running loss of epoch-30 batch-151 = 0.00044290232472121716

Training epoch-30 batch-152
Running loss of epoch-30 batch-152 = 0.000283885863609612

Training epoch-30 batch-153
Running loss of epoch-30 batch-153 = 0.00012710923328995705

Training epoch-30 batch-154
Running loss of epoch-30 batch-154 = 0.00012835185043513775

Training epoch-30 batch-155
Running loss of epoch-30 batch-155 = 4.8138201236724854e-05

Training epoch-30 batch-156
Running loss of epoch-30 batch-156 = 9.231641888618469e-05

Training epoch-30 batch-157
Running loss of epoch-30 batch-157 = 0.001724936068058014

Finished training epoch-30.



Average train loss at epoch-30 = 0.0002847857050597668

Started Evaluation

Average val loss at epoch-30 = 1.2244988113506177

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 89.55 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 66.89 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 50.90 %
Accuracy for class execute is: 51.00 %
Accuracy for class get is: 60.26 %

Overall Accuracy = 81.35 %

Finished Evaluation



Started training epoch-31


Training epoch-31 batch-1
Running loss of epoch-31 batch-1 = 0.00014320667833089828

Training epoch-31 batch-2
Running loss of epoch-31 batch-2 = 4.054093733429909e-05

Training epoch-31 batch-3
Running loss of epoch-31 batch-3 = 7.833191193640232e-05

Training epoch-31 batch-4
Running loss of epoch-31 batch-4 = 3.3359741792082787e-05

Training epoch-31 batch-5
Running loss of epoch-31 batch-5 = 0.0006717238575220108

Training epoch-31 batch-6
Running loss of epoch-31 batch-6 = 3.893626853823662e-05

Training epoch-31 batch-7
Running loss of epoch-31 batch-7 = 0.00035522645339369774

Training epoch-31 batch-8
Running loss of epoch-31 batch-8 = 0.00012390664778649807

Training epoch-31 batch-9
Running loss of epoch-31 batch-9 = 0.00017881044186651707

Training epoch-31 batch-10
Running loss of epoch-31 batch-10 = 0.00022523198276758194

Training epoch-31 batch-11
Running loss of epoch-31 batch-11 = 0.00026196963153779507

Training epoch-31 batch-12
Running loss of epoch-31 batch-12 = 0.00016043521463871002

Training epoch-31 batch-13
Running loss of epoch-31 batch-13 = 5.679880268871784e-05

Training epoch-31 batch-14
Running loss of epoch-31 batch-14 = 8.481997065246105e-05

Training epoch-31 batch-15
Running loss of epoch-31 batch-15 = 0.00012288021389394999

Training epoch-31 batch-16
Running loss of epoch-31 batch-16 = 0.00021241605281829834

Training epoch-31 batch-17
Running loss of epoch-31 batch-17 = 0.00011603254824876785

Training epoch-31 batch-18
Running loss of epoch-31 batch-18 = 0.00021940353326499462

Training epoch-31 batch-19
Running loss of epoch-31 batch-19 = 0.000982593628577888

Training epoch-31 batch-20
Running loss of epoch-31 batch-20 = 0.00016364781185984612

Training epoch-31 batch-21
Running loss of epoch-31 batch-21 = 0.00015634275041520596

Training epoch-31 batch-22
Running loss of epoch-31 batch-22 = 8.221389725804329e-05

Training epoch-31 batch-23
Running loss of epoch-31 batch-23 = 0.00031474430579692125

Training epoch-31 batch-24
Running loss of epoch-31 batch-24 = 3.4432392567396164e-05

Training epoch-31 batch-25
Running loss of epoch-31 batch-25 = 0.00020531238988041878

Training epoch-31 batch-26
Running loss of epoch-31 batch-26 = 0.0002121997531503439

Training epoch-31 batch-27
Running loss of epoch-31 batch-27 = 0.0002041610423475504

Training epoch-31 batch-28
Running loss of epoch-31 batch-28 = 0.00038986047729849815

Training epoch-31 batch-29
Running loss of epoch-31 batch-29 = 0.00015760399401187897

Training epoch-31 batch-30
Running loss of epoch-31 batch-30 = 0.00023308279924094677

Training epoch-31 batch-31
Running loss of epoch-31 batch-31 = 5.4377829656004906e-05

Training epoch-31 batch-32
Running loss of epoch-31 batch-32 = 0.00024232815485447645

Training epoch-31 batch-33
Running loss of epoch-31 batch-33 = 0.0002879616804420948

Training epoch-31 batch-34
Running loss of epoch-31 batch-34 = 0.00021614274010062218

Training epoch-31 batch-35
Running loss of epoch-31 batch-35 = 0.0002415659837424755

Training epoch-31 batch-36
Running loss of epoch-31 batch-36 = 9.69800166785717e-05

Training epoch-31 batch-37
Running loss of epoch-31 batch-37 = 7.07434955984354e-05

Training epoch-31 batch-38
Running loss of epoch-31 batch-38 = 0.00020213797688484192

Training epoch-31 batch-39
Running loss of epoch-31 batch-39 = 3.206217661499977e-05

Training epoch-31 batch-40
Running loss of epoch-31 batch-40 = 9.372690692543983e-05

Training epoch-31 batch-41
Running loss of epoch-31 batch-41 = 0.0002583889290690422

Training epoch-31 batch-42
Running loss of epoch-31 batch-42 = 0.00021155551075935364

Training epoch-31 batch-43
Running loss of epoch-31 batch-43 = 0.0005702871130779386

Training epoch-31 batch-44
Running loss of epoch-31 batch-44 = 0.0001091968733817339

Training epoch-31 batch-45
Running loss of epoch-31 batch-45 = 6.676651537418365e-05

Training epoch-31 batch-46
Running loss of epoch-31 batch-46 = 0.00015041022561490536

Training epoch-31 batch-47
Running loss of epoch-31 batch-47 = 0.000321210827678442

Training epoch-31 batch-48
Running loss of epoch-31 batch-48 = 0.0002207714132964611

Training epoch-31 batch-49
Running loss of epoch-31 batch-49 = 0.00029961811378598213

Training epoch-31 batch-50
Running loss of epoch-31 batch-50 = 0.00019591813907027245

Training epoch-31 batch-51
Running loss of epoch-31 batch-51 = 0.0002555176615715027

Training epoch-31 batch-52
Running loss of epoch-31 batch-52 = 0.00020377698820084333

Training epoch-31 batch-53
Running loss of epoch-31 batch-53 = 8.205859921872616e-05

Training epoch-31 batch-54
Running loss of epoch-31 batch-54 = 0.00016640499234199524

Training epoch-31 batch-55
Running loss of epoch-31 batch-55 = 4.6904897317290306e-05

Training epoch-31 batch-56
Running loss of epoch-31 batch-56 = 0.00011078664101660252

Training epoch-31 batch-57
Running loss of epoch-31 batch-57 = 0.00012196321040391922

Training epoch-31 batch-58
Running loss of epoch-31 batch-58 = 3.047659993171692e-05

Training epoch-31 batch-59
Running loss of epoch-31 batch-59 = 0.00018497975543141365

Training epoch-31 batch-60
Running loss of epoch-31 batch-60 = 4.427134990692139e-05

Training epoch-31 batch-61
Running loss of epoch-31 batch-61 = 0.00015594903379678726

Training epoch-31 batch-62
Running loss of epoch-31 batch-62 = 5.323707591742277e-05

Training epoch-31 batch-63
Running loss of epoch-31 batch-63 = 6.017112173140049e-05

Training epoch-31 batch-64
Running loss of epoch-31 batch-64 = 0.00017823930829763412

Training epoch-31 batch-65
Running loss of epoch-31 batch-65 = 0.00047393515706062317

Training epoch-31 batch-66
Running loss of epoch-31 batch-66 = 7.367599755525589e-05

Training epoch-31 batch-67
Running loss of epoch-31 batch-67 = 4.970002919435501e-05

Training epoch-31 batch-68
Running loss of epoch-31 batch-68 = 7.90134072303772e-05

Training epoch-31 batch-69
Running loss of epoch-31 batch-69 = 7.594982162117958e-05

Training epoch-31 batch-70
Running loss of epoch-31 batch-70 = 4.6912115067243576e-05

Training epoch-31 batch-71
Running loss of epoch-31 batch-71 = 0.0006509694503620267

Training epoch-31 batch-72
Running loss of epoch-31 batch-72 = 0.00026230327785015106

Training epoch-31 batch-73
Running loss of epoch-31 batch-73 = 5.5132899433374405e-05

Training epoch-31 batch-74
Running loss of epoch-31 batch-74 = 0.0001344622578471899

Training epoch-31 batch-75
Running loss of epoch-31 batch-75 = 0.00011272495612502098

Training epoch-31 batch-76
Running loss of epoch-31 batch-76 = 0.00016934447921812534

Training epoch-31 batch-77
Running loss of epoch-31 batch-77 = 0.0004862220957875252

Training epoch-31 batch-78
Running loss of epoch-31 batch-78 = 0.000195440836250782

Training epoch-31 batch-79
Running loss of epoch-31 batch-79 = 6.875582039356232e-05

Training epoch-31 batch-80
Running loss of epoch-31 batch-80 = 0.0001442539505660534

Training epoch-31 batch-81
Running loss of epoch-31 batch-81 = 8.931523188948631e-05

Training epoch-31 batch-82
Running loss of epoch-31 batch-82 = 0.00010722409933805466

Training epoch-31 batch-83
Running loss of epoch-31 batch-83 = 0.00040260120294988155

Training epoch-31 batch-84
Running loss of epoch-31 batch-84 = 3.874674439430237e-05

Training epoch-31 batch-85
Running loss of epoch-31 batch-85 = 4.9254391342401505e-05

Training epoch-31 batch-86
Running loss of epoch-31 batch-86 = 6.201164796948433e-05

Training epoch-31 batch-87
Running loss of epoch-31 batch-87 = 0.00024404539726674557

Training epoch-31 batch-88
Running loss of epoch-31 batch-88 = 8.814502507448196e-05

Training epoch-31 batch-89
Running loss of epoch-31 batch-89 = 4.929071292281151e-05

Training epoch-31 batch-90
Running loss of epoch-31 batch-90 = 0.00019300286658108234

Training epoch-31 batch-91
Running loss of epoch-31 batch-91 = 0.00011551310308277607

Training epoch-31 batch-92
Running loss of epoch-31 batch-92 = 0.0005970293423160911

Training epoch-31 batch-93
Running loss of epoch-31 batch-93 = 6.39085192233324e-05

Training epoch-31 batch-94
Running loss of epoch-31 batch-94 = 0.00022382778115570545

Training epoch-31 batch-95
Running loss of epoch-31 batch-95 = 8.195941336452961e-05

Training epoch-31 batch-96
Running loss of epoch-31 batch-96 = 0.00013629859313368797

Training epoch-31 batch-97
Running loss of epoch-31 batch-97 = 0.0001235653180629015

Training epoch-31 batch-98
Running loss of epoch-31 batch-98 = 0.00022661034017801285

Training epoch-31 batch-99
Running loss of epoch-31 batch-99 = 8.852931205183268e-05

Training epoch-31 batch-100
Running loss of epoch-31 batch-100 = 0.00027726427651941776

Training epoch-31 batch-101
Running loss of epoch-31 batch-101 = 0.00010183267295360565

Training epoch-31 batch-102
Running loss of epoch-31 batch-102 = 9.160256013274193e-05

Training epoch-31 batch-103
Running loss of epoch-31 batch-103 = 0.00012724031694233418

Training epoch-31 batch-104
Running loss of epoch-31 batch-104 = 8.414732292294502e-05

Training epoch-31 batch-105
Running loss of epoch-31 batch-105 = 2.493453212082386e-05

Training epoch-31 batch-106
Running loss of epoch-31 batch-106 = 1.3103242963552475e-05

Training epoch-31 batch-107
Running loss of epoch-31 batch-107 = 3.124377690255642e-05

Training epoch-31 batch-108
Running loss of epoch-31 batch-108 = 5.4880743846297264e-05

Training epoch-31 batch-109
Running loss of epoch-31 batch-109 = 8.698482997715473e-05

Training epoch-31 batch-110
Running loss of epoch-31 batch-110 = 7.065082900226116e-05

Training epoch-31 batch-111
Running loss of epoch-31 batch-111 = 0.001483485451899469

Training epoch-31 batch-112
Running loss of epoch-31 batch-112 = 8.576922118663788e-05

Training epoch-31 batch-113
Running loss of epoch-31 batch-113 = 0.00012291851453483105

Training epoch-31 batch-114
Running loss of epoch-31 batch-114 = 6.55431067571044e-05

Training epoch-31 batch-115
Running loss of epoch-31 batch-115 = 7.918989285826683e-05

Training epoch-31 batch-116
Running loss of epoch-31 batch-116 = 0.00013606692664325237

Training epoch-31 batch-117
Running loss of epoch-31 batch-117 = 0.00011444045230746269

Training epoch-31 batch-118
Running loss of epoch-31 batch-118 = 2.6872381567955017e-05

Training epoch-31 batch-119
Running loss of epoch-31 batch-119 = 0.00013457844033837318

Training epoch-31 batch-120
Running loss of epoch-31 batch-120 = 0.0003365107113495469

Training epoch-31 batch-121
Running loss of epoch-31 batch-121 = 0.0001845408696681261

Training epoch-31 batch-122
Running loss of epoch-31 batch-122 = 0.00011357571929693222

Training epoch-31 batch-123
Running loss of epoch-31 batch-123 = 0.00018013548105955124

Training epoch-31 batch-124
Running loss of epoch-31 batch-124 = 0.0004715288523584604

Training epoch-31 batch-125
Running loss of epoch-31 batch-125 = 0.00013997976202517748

Training epoch-31 batch-126
Running loss of epoch-31 batch-126 = 5.926634185016155e-05

Training epoch-31 batch-127
Running loss of epoch-31 batch-127 = 5.880091339349747e-05

Training epoch-31 batch-128
Running loss of epoch-31 batch-128 = 0.00010757520794868469

Training epoch-31 batch-129
Running loss of epoch-31 batch-129 = 0.0003543894272297621

Training epoch-31 batch-130
Running loss of epoch-31 batch-130 = 0.00010422058403491974

Training epoch-31 batch-131
Running loss of epoch-31 batch-131 = 8.065672591328621e-05

Training epoch-31 batch-132
Running loss of epoch-31 batch-132 = 6.326520815491676e-05

Training epoch-31 batch-133
Running loss of epoch-31 batch-133 = 9.183702059090137e-05

Training epoch-31 batch-134
Running loss of epoch-31 batch-134 = 2.0584091544151306e-05

Training epoch-31 batch-135
Running loss of epoch-31 batch-135 = 5.305069498717785e-05

Training epoch-31 batch-136
Running loss of epoch-31 batch-136 = 8.515419904142618e-05

Training epoch-31 batch-137
Running loss of epoch-31 batch-137 = 4.826067015528679e-05

Training epoch-31 batch-138
Running loss of epoch-31 batch-138 = 0.0004974710755050182

Training epoch-31 batch-139
Running loss of epoch-31 batch-139 = 0.0006426051259040833

Training epoch-31 batch-140
Running loss of epoch-31 batch-140 = 0.00014487840235233307

Training epoch-31 batch-141
Running loss of epoch-31 batch-141 = 0.00020696455612778664

Training epoch-31 batch-142
Running loss of epoch-31 batch-142 = 0.00016452930867671967

Training epoch-31 batch-143
Running loss of epoch-31 batch-143 = 0.0001995066413655877

Training epoch-31 batch-144
Running loss of epoch-31 batch-144 = 9.396905079483986e-05

Training epoch-31 batch-145
Running loss of epoch-31 batch-145 = 0.00023859122302383184

Training epoch-31 batch-146
Running loss of epoch-31 batch-146 = 0.00042051984928548336

Training epoch-31 batch-147
Running loss of epoch-31 batch-147 = 0.00012369954492896795

Training epoch-31 batch-148
Running loss of epoch-31 batch-148 = 0.0004123223479837179

Training epoch-31 batch-149
Running loss of epoch-31 batch-149 = 0.000148342689499259

Training epoch-31 batch-150
Running loss of epoch-31 batch-150 = 3.18984966725111e-05

Training epoch-31 batch-151
Running loss of epoch-31 batch-151 = 4.400964826345444e-05

Training epoch-31 batch-152
Running loss of epoch-31 batch-152 = 6.529269739985466e-05

Training epoch-31 batch-153
Running loss of epoch-31 batch-153 = 0.0006334702484309673

Training epoch-31 batch-154
Running loss of epoch-31 batch-154 = 0.0003002322046086192

Training epoch-31 batch-155
Running loss of epoch-31 batch-155 = 3.945734351873398e-05

Training epoch-31 batch-156
Running loss of epoch-31 batch-156 = 0.00020865467377007008

Training epoch-31 batch-157
Running loss of epoch-31 batch-157 = 0.00045428425073623657

Finished training epoch-31.



Average train loss at epoch-31 = 0.00018209828957915306

Started Evaluation

Average val loss at epoch-31 = 1.2254511601924223

Accuracy for classes:
Accuracy for class equals is: 95.21 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 91.31 %
Accuracy for class onCreate is: 88.27 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 63.00 %
Accuracy for class execute is: 43.37 %
Accuracy for class get is: 64.62 %

Overall Accuracy = 81.95 %

Finished Evaluation



Started training epoch-32


Training epoch-32 batch-1
Running loss of epoch-32 batch-1 = 0.0002510007470846176

Training epoch-32 batch-2
Running loss of epoch-32 batch-2 = 7.128180004656315e-05

Training epoch-32 batch-3
Running loss of epoch-32 batch-3 = 9.146146476268768e-05

Training epoch-32 batch-4
Running loss of epoch-32 batch-4 = 0.0003764595603570342

Training epoch-32 batch-5
Running loss of epoch-32 batch-5 = 0.00027950387448072433

Training epoch-32 batch-6
Running loss of epoch-32 batch-6 = 0.00011400948278605938

Training epoch-32 batch-7
Running loss of epoch-32 batch-7 = 0.00022399215959012508

Training epoch-32 batch-8
Running loss of epoch-32 batch-8 = 0.0001935411710292101

Training epoch-32 batch-9
Running loss of epoch-32 batch-9 = 0.00012374413199722767

Training epoch-32 batch-10
Running loss of epoch-32 batch-10 = 0.00015793228521943092

Training epoch-32 batch-11
Running loss of epoch-32 batch-11 = 4.64683398604393e-05

Training epoch-32 batch-12
Running loss of epoch-32 batch-12 = 3.6172568798065186e-05

Training epoch-32 batch-13
Running loss of epoch-32 batch-13 = 0.0006466776831075549

Training epoch-32 batch-14
Running loss of epoch-32 batch-14 = 5.2941497415304184e-05

Training epoch-32 batch-15
Running loss of epoch-32 batch-15 = 8.46801558509469e-05

Training epoch-32 batch-16
Running loss of epoch-32 batch-16 = 3.663310781121254e-05

Training epoch-32 batch-17
Running loss of epoch-32 batch-17 = 4.4208718463778496e-05

Training epoch-32 batch-18
Running loss of epoch-32 batch-18 = 5.472148768603802e-05

Training epoch-32 batch-19
Running loss of epoch-32 batch-19 = 3.8780272006988525e-05

Training epoch-32 batch-20
Running loss of epoch-32 batch-20 = 0.000117813004180789

Training epoch-32 batch-21
Running loss of epoch-32 batch-21 = 4.85018827021122e-05

Training epoch-32 batch-22
Running loss of epoch-32 batch-22 = 7.561093661934137e-05

Training epoch-32 batch-23
Running loss of epoch-32 batch-23 = 0.00010340847074985504

Training epoch-32 batch-24
Running loss of epoch-32 batch-24 = 2.4583423510193825e-05

Training epoch-32 batch-25
Running loss of epoch-32 batch-25 = 0.00011470145545899868

Training epoch-32 batch-26
Running loss of epoch-32 batch-26 = 0.00015234504826366901

Training epoch-32 batch-27
Running loss of epoch-32 batch-27 = 5.161762237548828e-05

Training epoch-32 batch-28
Running loss of epoch-32 batch-28 = 8.427724242210388e-05

Training epoch-32 batch-29
Running loss of epoch-32 batch-29 = 7.320812437683344e-05

Training epoch-32 batch-30
Running loss of epoch-32 batch-30 = 5.5413926020264626e-05

Training epoch-32 batch-31
Running loss of epoch-32 batch-31 = 0.00010828941594809294

Training epoch-32 batch-32
Running loss of epoch-32 batch-32 = 2.921302802860737e-05

Training epoch-32 batch-33
Running loss of epoch-32 batch-33 = 0.00018582562915980816

Training epoch-32 batch-34
Running loss of epoch-32 batch-34 = 0.00022320915013551712

Training epoch-32 batch-35
Running loss of epoch-32 batch-35 = 9.370478801429272e-05

Training epoch-32 batch-36
Running loss of epoch-32 batch-36 = 0.0001325462944805622

Training epoch-32 batch-37
Running loss of epoch-32 batch-37 = 3.148755058646202e-05

Training epoch-32 batch-38
Running loss of epoch-32 batch-38 = 3.906828351318836e-05

Training epoch-32 batch-39
Running loss of epoch-32 batch-39 = 6.632797885686159e-05

Training epoch-32 batch-40
Running loss of epoch-32 batch-40 = 3.5473378375172615e-05

Training epoch-32 batch-41
Running loss of epoch-32 batch-41 = 7.299729622900486e-05

Training epoch-32 batch-42
Running loss of epoch-32 batch-42 = 0.00010641803964972496

Training epoch-32 batch-43
Running loss of epoch-32 batch-43 = 0.00011714035645127296

Training epoch-32 batch-44
Running loss of epoch-32 batch-44 = 0.00021913065575063229

Training epoch-32 batch-45
Running loss of epoch-32 batch-45 = 7.720035500824451e-05

Training epoch-32 batch-46
Running loss of epoch-32 batch-46 = 0.0001762786414474249

Training epoch-32 batch-47
Running loss of epoch-32 batch-47 = 0.0001251662615686655

Training epoch-32 batch-48
Running loss of epoch-32 batch-48 = 0.00013484060764312744

Training epoch-32 batch-49
Running loss of epoch-32 batch-49 = 0.00017886399291455746

Training epoch-32 batch-50
Running loss of epoch-32 batch-50 = 4.789303056895733e-05

Training epoch-32 batch-51
Running loss of epoch-32 batch-51 = 3.41576524078846e-05

Training epoch-32 batch-52
Running loss of epoch-32 batch-52 = 0.00012497790157794952

Training epoch-32 batch-53
Running loss of epoch-32 batch-53 = 0.0001496553886681795

Training epoch-32 batch-54
Running loss of epoch-32 batch-54 = 0.0005049547180533409

Training epoch-32 batch-55
Running loss of epoch-32 batch-55 = 0.00012039835564792156

Training epoch-32 batch-56
Running loss of epoch-32 batch-56 = 5.760928615927696e-05

Training epoch-32 batch-57
Running loss of epoch-32 batch-57 = 9.136879816651344e-05

Training epoch-32 batch-58
Running loss of epoch-32 batch-58 = 8.848123252391815e-05

Training epoch-32 batch-59
Running loss of epoch-32 batch-59 = 6.654229946434498e-05

Training epoch-32 batch-60
Running loss of epoch-32 batch-60 = 6.974884308874607e-05

Training epoch-32 batch-61
Running loss of epoch-32 batch-61 = 0.00011899019591510296

Training epoch-32 batch-62
Running loss of epoch-32 batch-62 = 2.6827910915017128e-05

Training epoch-32 batch-63
Running loss of epoch-32 batch-63 = 0.00022281683050096035

Training epoch-32 batch-64
Running loss of epoch-32 batch-64 = 9.549199603497982e-05

Training epoch-32 batch-65
Running loss of epoch-32 batch-65 = 6.149150431156158e-05

Training epoch-32 batch-66
Running loss of epoch-32 batch-66 = 9.195972234010696e-05

Training epoch-32 batch-67
Running loss of epoch-32 batch-67 = 0.00012831296771764755

Training epoch-32 batch-68
Running loss of epoch-32 batch-68 = 7.004360668361187e-05

Training epoch-32 batch-69
Running loss of epoch-32 batch-69 = 5.5334530770778656e-05

Training epoch-32 batch-70
Running loss of epoch-32 batch-70 = 3.958912566304207e-05

Training epoch-32 batch-71
Running loss of epoch-32 batch-71 = 2.51114834100008e-05

Training epoch-32 batch-72
Running loss of epoch-32 batch-72 = 0.00014041736721992493

Training epoch-32 batch-73
Running loss of epoch-32 batch-73 = 0.0006090010283514857

Training epoch-32 batch-74
Running loss of epoch-32 batch-74 = 4.525994881987572e-05

Training epoch-32 batch-75
Running loss of epoch-32 batch-75 = 0.00010713119991123676

Training epoch-32 batch-76
Running loss of epoch-32 batch-76 = 0.00021649478003382683

Training epoch-32 batch-77
Running loss of epoch-32 batch-77 = 9.798258543014526e-05

Training epoch-32 batch-78
Running loss of epoch-32 batch-78 = 0.00015957001596689224

Training epoch-32 batch-79
Running loss of epoch-32 batch-79 = 0.00019896170124411583

Training epoch-32 batch-80
Running loss of epoch-32 batch-80 = 0.00012646510731428862

Training epoch-32 batch-81
Running loss of epoch-32 batch-81 = 0.0001252677757292986

Training epoch-32 batch-82
Running loss of epoch-32 batch-82 = 5.040201358497143e-05

Training epoch-32 batch-83
Running loss of epoch-32 batch-83 = 3.548804670572281e-05

Training epoch-32 batch-84
Running loss of epoch-32 batch-84 = 6.506498903036118e-05

Training epoch-32 batch-85
Running loss of epoch-32 batch-85 = 4.41037118434906e-05

Training epoch-32 batch-86
Running loss of epoch-32 batch-86 = 0.0001111077144742012

Training epoch-32 batch-87
Running loss of epoch-32 batch-87 = 6.970716640353203e-06

Training epoch-32 batch-88
Running loss of epoch-32 batch-88 = 5.8507779613137245e-05

Training epoch-32 batch-89
Running loss of epoch-32 batch-89 = 0.0001254614908248186

Training epoch-32 batch-90
Running loss of epoch-32 batch-90 = 0.0001678471453487873

Training epoch-32 batch-91
Running loss of epoch-32 batch-91 = 5.3937314078211784e-05

Training epoch-32 batch-92
Running loss of epoch-32 batch-92 = 7.382337935268879e-05

Training epoch-32 batch-93
Running loss of epoch-32 batch-93 = 0.0003476773854345083

Training epoch-32 batch-94
Running loss of epoch-32 batch-94 = 0.0007702242583036423

Training epoch-32 batch-95
Running loss of epoch-32 batch-95 = 0.00010749418288469315

Training epoch-32 batch-96
Running loss of epoch-32 batch-96 = 0.00026042922399938107

Training epoch-32 batch-97
Running loss of epoch-32 batch-97 = 7.37491063773632e-05

Training epoch-32 batch-98
Running loss of epoch-32 batch-98 = 0.00013558496721088886

Training epoch-32 batch-99
Running loss of epoch-32 batch-99 = 4.320102743804455e-05

Training epoch-32 batch-100
Running loss of epoch-32 batch-100 = 0.0002472884953022003

Training epoch-32 batch-101
Running loss of epoch-32 batch-101 = 0.00010474654845893383

Training epoch-32 batch-102
Running loss of epoch-32 batch-102 = 6.241723895072937e-05

Training epoch-32 batch-103
Running loss of epoch-32 batch-103 = 8.035567589104176e-05

Training epoch-32 batch-104
Running loss of epoch-32 batch-104 = 5.3711701184511185e-05

Training epoch-32 batch-105
Running loss of epoch-32 batch-105 = 0.0003080158494412899

Training epoch-32 batch-106
Running loss of epoch-32 batch-106 = 8.213683031499386e-05

Training epoch-32 batch-107
Running loss of epoch-32 batch-107 = 0.00011257524602115154

Training epoch-32 batch-108
Running loss of epoch-32 batch-108 = 8.279760368168354e-05

Training epoch-32 batch-109
Running loss of epoch-32 batch-109 = 0.00015373830683529377

Training epoch-32 batch-110
Running loss of epoch-32 batch-110 = 0.00010643352288752794

Training epoch-32 batch-111
Running loss of epoch-32 batch-111 = 0.0005916217342019081

Training epoch-32 batch-112
Running loss of epoch-32 batch-112 = 7.173651829361916e-05

Training epoch-32 batch-113
Running loss of epoch-32 batch-113 = 0.00019356817938387394

Training epoch-32 batch-114
Running loss of epoch-32 batch-114 = 7.915566675364971e-05

Training epoch-32 batch-115
Running loss of epoch-32 batch-115 = 0.0002479543909430504

Training epoch-32 batch-116
Running loss of epoch-32 batch-116 = 0.00016834610141813755

Training epoch-32 batch-117
Running loss of epoch-32 batch-117 = 0.00012612692080438137

Training epoch-32 batch-118
Running loss of epoch-32 batch-118 = 0.00016624736599624157

Training epoch-32 batch-119
Running loss of epoch-32 batch-119 = 0.0001900666393339634

Training epoch-32 batch-120
Running loss of epoch-32 batch-120 = 7.198634557425976e-05

Training epoch-32 batch-121
Running loss of epoch-32 batch-121 = 5.089025944471359e-05

Training epoch-32 batch-122
Running loss of epoch-32 batch-122 = 4.4314772821962833e-05

Training epoch-32 batch-123
Running loss of epoch-32 batch-123 = 0.00034139968920499086

Training epoch-32 batch-124
Running loss of epoch-32 batch-124 = 7.353094406425953e-05

Training epoch-32 batch-125
Running loss of epoch-32 batch-125 = 8.03822185844183e-05

Training epoch-32 batch-126
Running loss of epoch-32 batch-126 = 0.00019031064584851265

Training epoch-32 batch-127
Running loss of epoch-32 batch-127 = 0.0003371128113940358

Training epoch-32 batch-128
Running loss of epoch-32 batch-128 = 4.1331397369503975e-05

Training epoch-32 batch-129
Running loss of epoch-32 batch-129 = 8.694152347743511e-05

Training epoch-32 batch-130
Running loss of epoch-32 batch-130 = 0.0002069284673780203

Training epoch-32 batch-131
Running loss of epoch-32 batch-131 = 8.51296354085207e-05

Training epoch-32 batch-132
Running loss of epoch-32 batch-132 = 9.932962711900473e-05

Training epoch-32 batch-133
Running loss of epoch-32 batch-133 = 6.509572267532349e-05

Training epoch-32 batch-134
Running loss of epoch-32 batch-134 = 0.0004760081646963954

Training epoch-32 batch-135
Running loss of epoch-32 batch-135 = 4.2260391637682915e-05

Training epoch-32 batch-136
Running loss of epoch-32 batch-136 = 0.00020853057503700256

Training epoch-32 batch-137
Running loss of epoch-32 batch-137 = 0.00043471273966133595

Training epoch-32 batch-138
Running loss of epoch-32 batch-138 = 5.310750566422939e-05

Training epoch-32 batch-139
Running loss of epoch-32 batch-139 = 0.00011754129081964493

Training epoch-32 batch-140
Running loss of epoch-32 batch-140 = 0.0001178104430437088

Training epoch-32 batch-141
Running loss of epoch-32 batch-141 = 0.000330829294398427

Training epoch-32 batch-142
Running loss of epoch-32 batch-142 = 0.0001245106104761362

Training epoch-32 batch-143
Running loss of epoch-32 batch-143 = 8.026440627872944e-05

Training epoch-32 batch-144
Running loss of epoch-32 batch-144 = 0.000134527450427413

Training epoch-32 batch-145
Running loss of epoch-32 batch-145 = 0.00022055534645915031

Training epoch-32 batch-146
Running loss of epoch-32 batch-146 = 0.00022964319214224815

Training epoch-32 batch-147
Running loss of epoch-32 batch-147 = 7.739057764410973e-05

Training epoch-32 batch-148
Running loss of epoch-32 batch-148 = 0.00022990710567682981

Training epoch-32 batch-149
Running loss of epoch-32 batch-149 = 1.3191020116209984e-05

Training epoch-32 batch-150
Running loss of epoch-32 batch-150 = 6.808387115597725e-05

Training epoch-32 batch-151
Running loss of epoch-32 batch-151 = 0.00010742037557065487

Training epoch-32 batch-152
Running loss of epoch-32 batch-152 = 0.00010457076132297516

Training epoch-32 batch-153
Running loss of epoch-32 batch-153 = 7.158913649618626e-05

Training epoch-32 batch-154
Running loss of epoch-32 batch-154 = 9.829620830714703e-05

Training epoch-32 batch-155
Running loss of epoch-32 batch-155 = 0.00018969550728797913

Training epoch-32 batch-156
Running loss of epoch-32 batch-156 = 2.0646024495363235e-05

Training epoch-32 batch-157
Running loss of epoch-32 batch-157 = 0.0001237243413925171

Finished training epoch-32.



Average train loss at epoch-32 = 0.00013576962798833846

Started Evaluation

Average val loss at epoch-32 = 1.3158974884813928

Accuracy for classes:
Accuracy for class equals is: 95.87 %
Accuracy for class main is: 95.90 %
Accuracy for class setUp is: 86.72 %
Accuracy for class onCreate is: 89.02 %
Accuracy for class toString is: 84.64 %
Accuracy for class run is: 68.04 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 49.33 %
Accuracy for class execute is: 48.59 %
Accuracy for class get is: 62.05 %

Overall Accuracy = 80.90 %

Finished Evaluation



Started training epoch-33


Training epoch-33 batch-1
Running loss of epoch-33 batch-1 = 7.884996011853218e-05

Training epoch-33 batch-2
Running loss of epoch-33 batch-2 = 6.121001206338406e-05

Training epoch-33 batch-3
Running loss of epoch-33 batch-3 = 4.824157804250717e-05

Training epoch-33 batch-4
Running loss of epoch-33 batch-4 = 4.668114706873894e-05

Training epoch-33 batch-5
Running loss of epoch-33 batch-5 = 7.439474575221539e-05

Training epoch-33 batch-6
Running loss of epoch-33 batch-6 = 8.983258157968521e-05

Training epoch-33 batch-7
Running loss of epoch-33 batch-7 = 4.6127475798130035e-05

Training epoch-33 batch-8
Running loss of epoch-33 batch-8 = 6.275670602917671e-05

Training epoch-33 batch-9
Running loss of epoch-33 batch-9 = 7.053231820464134e-05

Training epoch-33 batch-10
Running loss of epoch-33 batch-10 = 4.058214835822582e-05

Training epoch-33 batch-11
Running loss of epoch-33 batch-11 = 5.243835039436817e-05

Training epoch-33 batch-12
Running loss of epoch-33 batch-12 = 7.22839031368494e-05

Training epoch-33 batch-13
Running loss of epoch-33 batch-13 = 6.90915621817112e-05

Training epoch-33 batch-14
Running loss of epoch-33 batch-14 = 5.62518835067749e-05

Training epoch-33 batch-15
Running loss of epoch-33 batch-15 = 4.466320388019085e-05

Training epoch-33 batch-16
Running loss of epoch-33 batch-16 = 0.0001059358473867178

Training epoch-33 batch-17
Running loss of epoch-33 batch-17 = 4.3195439502596855e-05

Training epoch-33 batch-18
Running loss of epoch-33 batch-18 = 3.0471011996269226e-05

Training epoch-33 batch-19
Running loss of epoch-33 batch-19 = 6.0971127822995186e-05

Training epoch-33 batch-20
Running loss of epoch-33 batch-20 = 2.3986678570508957e-05

Training epoch-33 batch-21
Running loss of epoch-33 batch-21 = 3.626244142651558e-05

Training epoch-33 batch-22
Running loss of epoch-33 batch-22 = 6.547616794705391e-05

Training epoch-33 batch-23
Running loss of epoch-33 batch-23 = 0.00020435545593500137

Training epoch-33 batch-24
Running loss of epoch-33 batch-24 = 0.0002875459613278508

Training epoch-33 batch-25
Running loss of epoch-33 batch-25 = 4.222150892019272e-05

Training epoch-33 batch-26
Running loss of epoch-33 batch-26 = 0.00027095386758446693

Training epoch-33 batch-27
Running loss of epoch-33 batch-27 = 3.843707963824272e-05

Training epoch-33 batch-28
Running loss of epoch-33 batch-28 = 4.5157503336668015e-05

Training epoch-33 batch-29
Running loss of epoch-33 batch-29 = 3.623799420893192e-05

Training epoch-33 batch-30
Running loss of epoch-33 batch-30 = 0.00015292910393327475

Training epoch-33 batch-31
Running loss of epoch-33 batch-31 = 3.471854142844677e-05

Training epoch-33 batch-32
Running loss of epoch-33 batch-32 = 5.970196798443794e-05

Training epoch-33 batch-33
Running loss of epoch-33 batch-33 = 3.1325966119766235e-05

Training epoch-33 batch-34
Running loss of epoch-33 batch-34 = 3.2725511118769646e-05

Training epoch-33 batch-35
Running loss of epoch-33 batch-35 = 0.0004135307390242815

Training epoch-33 batch-36
Running loss of epoch-33 batch-36 = 0.0007518614875152707

Training epoch-33 batch-37
Running loss of epoch-33 batch-37 = 0.00014821055810898542

Training epoch-33 batch-38
Running loss of epoch-33 batch-38 = 0.00011286395601928234

Training epoch-33 batch-39
Running loss of epoch-33 batch-39 = 6.267311982810497e-05

Training epoch-33 batch-40
Running loss of epoch-33 batch-40 = 7.108552381396294e-05

Training epoch-33 batch-41
Running loss of epoch-33 batch-41 = 0.0003498615697026253

Training epoch-33 batch-42
Running loss of epoch-33 batch-42 = 9.1486144810915e-05

Training epoch-33 batch-43
Running loss of epoch-33 batch-43 = 6.502808537334204e-05

Training epoch-33 batch-44
Running loss of epoch-33 batch-44 = 7.642072159796953e-05

Training epoch-33 batch-45
Running loss of epoch-33 batch-45 = 0.00015290267765522003

Training epoch-33 batch-46
Running loss of epoch-33 batch-46 = 0.00012609432451426983

Training epoch-33 batch-47
Running loss of epoch-33 batch-47 = 0.00020394776947796345

Training epoch-33 batch-48
Running loss of epoch-33 batch-48 = 0.0002901689149439335

Training epoch-33 batch-49
Running loss of epoch-33 batch-49 = 0.00035465124528855085

Training epoch-33 batch-50
Running loss of epoch-33 batch-50 = 4.568742588162422e-05

Training epoch-33 batch-51
Running loss of epoch-33 batch-51 = 6.091431714594364e-05

Training epoch-33 batch-52
Running loss of epoch-33 batch-52 = 0.00028244731947779655

Training epoch-33 batch-53
Running loss of epoch-33 batch-53 = 9.900261647999287e-05

Training epoch-33 batch-54
Running loss of epoch-33 batch-54 = 3.385590389370918e-05

Training epoch-33 batch-55
Running loss of epoch-33 batch-55 = 4.562945105135441e-05

Training epoch-33 batch-56
Running loss of epoch-33 batch-56 = 0.00014630798250436783

Training epoch-33 batch-57
Running loss of epoch-33 batch-57 = 0.00016135512851178646

Training epoch-33 batch-58
Running loss of epoch-33 batch-58 = 5.2770599722862244e-05

Training epoch-33 batch-59
Running loss of epoch-33 batch-59 = 0.00021896115504205227

Training epoch-33 batch-60
Running loss of epoch-33 batch-60 = 9.177066385746002e-05

Training epoch-33 batch-61
Running loss of epoch-33 batch-61 = 0.00015907478518784046

Training epoch-33 batch-62
Running loss of epoch-33 batch-62 = 0.0004646276356652379

Training epoch-33 batch-63
Running loss of epoch-33 batch-63 = 4.122359678149223e-05

Training epoch-33 batch-64
Running loss of epoch-33 batch-64 = 6.458733696490526e-05

Training epoch-33 batch-65
Running loss of epoch-33 batch-65 = 1.4848774299025536e-05

Training epoch-33 batch-66
Running loss of epoch-33 batch-66 = 8.862116374075413e-05

Training epoch-33 batch-67
Running loss of epoch-33 batch-67 = 0.0002958663972094655

Training epoch-33 batch-68
Running loss of epoch-33 batch-68 = 0.00012483005411922932

Training epoch-33 batch-69
Running loss of epoch-33 batch-69 = 0.00033927056938409805

Training epoch-33 batch-70
Running loss of epoch-33 batch-70 = 0.00014782394282519817

Training epoch-33 batch-71
Running loss of epoch-33 batch-71 = 0.00010712118819355965

Training epoch-33 batch-72
Running loss of epoch-33 batch-72 = 5.885818973183632e-05

Training epoch-33 batch-73
Running loss of epoch-33 batch-73 = 7.85833690315485e-05

Training epoch-33 batch-74
Running loss of epoch-33 batch-74 = 7.019843906164169e-05

Training epoch-33 batch-75
Running loss of epoch-33 batch-75 = 0.000336166238412261

Training epoch-33 batch-76
Running loss of epoch-33 batch-76 = 7.932237349450588e-05

Training epoch-33 batch-77
Running loss of epoch-33 batch-77 = 5.626026540994644e-05

Training epoch-33 batch-78
Running loss of epoch-33 batch-78 = 0.0001712243538349867

Training epoch-33 batch-79
Running loss of epoch-33 batch-79 = 0.00010416354052722454

Training epoch-33 batch-80
Running loss of epoch-33 batch-80 = 3.0042603611946106e-05

Training epoch-33 batch-81
Running loss of epoch-33 batch-81 = 6.186123937368393e-05

Training epoch-33 batch-82
Running loss of epoch-33 batch-82 = 3.099720925092697e-05

Training epoch-33 batch-83
Running loss of epoch-33 batch-83 = 6.35290052741766e-05

Training epoch-33 batch-84
Running loss of epoch-33 batch-84 = 5.1426468417048454e-05

Training epoch-33 batch-85
Running loss of epoch-33 batch-85 = 6.293249316513538e-05

Training epoch-33 batch-86
Running loss of epoch-33 batch-86 = 4.9467431381344795e-05

Training epoch-33 batch-87
Running loss of epoch-33 batch-87 = 3.1902920454740524e-05

Training epoch-33 batch-88
Running loss of epoch-33 batch-88 = 0.0011233158875256777

Training epoch-33 batch-89
Running loss of epoch-33 batch-89 = 9.572634007781744e-05

Training epoch-33 batch-90
Running loss of epoch-33 batch-90 = 0.0001789899542927742

Training epoch-33 batch-91
Running loss of epoch-33 batch-91 = 9.604543447494507e-05

Training epoch-33 batch-92
Running loss of epoch-33 batch-92 = 4.5618508011102676e-05

Training epoch-33 batch-93
Running loss of epoch-33 batch-93 = 3.202608786523342e-05

Training epoch-33 batch-94
Running loss of epoch-33 batch-94 = 0.00032787618692964315

Training epoch-33 batch-95
Running loss of epoch-33 batch-95 = 0.00011849659495055676

Training epoch-33 batch-96
Running loss of epoch-33 batch-96 = 0.0004306929185986519

Training epoch-33 batch-97
Running loss of epoch-33 batch-97 = 0.00014360714703798294

Training epoch-33 batch-98
Running loss of epoch-33 batch-98 = 8.497899398207664e-05

Training epoch-33 batch-99
Running loss of epoch-33 batch-99 = 4.289834760129452e-05

Training epoch-33 batch-100
Running loss of epoch-33 batch-100 = 6.560981273651123e-05

Training epoch-33 batch-101
Running loss of epoch-33 batch-101 = 0.00021800235845148563

Training epoch-33 batch-102
Running loss of epoch-33 batch-102 = 5.7106371968984604e-05

Training epoch-33 batch-103
Running loss of epoch-33 batch-103 = 5.770544521510601e-05

Training epoch-33 batch-104
Running loss of epoch-33 batch-104 = 9.247055277228355e-05

Training epoch-33 batch-105
Running loss of epoch-33 batch-105 = 3.091758117079735e-05

Training epoch-33 batch-106
Running loss of epoch-33 batch-106 = 2.2385967895388603e-05

Training epoch-33 batch-107
Running loss of epoch-33 batch-107 = 6.365543231368065e-05

Training epoch-33 batch-108
Running loss of epoch-33 batch-108 = 0.00011552346404641867

Training epoch-33 batch-109
Running loss of epoch-33 batch-109 = 0.00014385092072188854

Training epoch-33 batch-110
Running loss of epoch-33 batch-110 = 5.4167117923498154e-05

Training epoch-33 batch-111
Running loss of epoch-33 batch-111 = 6.728130392730236e-05

Training epoch-33 batch-112
Running loss of epoch-33 batch-112 = 5.491729825735092e-05

Training epoch-33 batch-113
Running loss of epoch-33 batch-113 = 4.152231849730015e-05

Training epoch-33 batch-114
Running loss of epoch-33 batch-114 = 1.623528078198433e-05

Training epoch-33 batch-115
Running loss of epoch-33 batch-115 = 7.204781286418438e-05

Training epoch-33 batch-116
Running loss of epoch-33 batch-116 = 8.916924707591534e-05

Training epoch-33 batch-117
Running loss of epoch-33 batch-117 = 8.179084397852421e-05

Training epoch-33 batch-118
Running loss of epoch-33 batch-118 = 3.736256621778011e-05

Training epoch-33 batch-119
Running loss of epoch-33 batch-119 = 8.98316502571106e-05

Training epoch-33 batch-120
Running loss of epoch-33 batch-120 = 6.669224239885807e-05

Training epoch-33 batch-121
Running loss of epoch-33 batch-121 = 7.53521453589201e-05

Training epoch-33 batch-122
Running loss of epoch-33 batch-122 = 0.00011747423559427261

Training epoch-33 batch-123
Running loss of epoch-33 batch-123 = 0.00016683456487953663

Training epoch-33 batch-124
Running loss of epoch-33 batch-124 = 0.00010293419472873211

Training epoch-33 batch-125
Running loss of epoch-33 batch-125 = 0.0001417337916791439

Training epoch-33 batch-126
Running loss of epoch-33 batch-126 = 3.565545193850994e-05

Training epoch-33 batch-127
Running loss of epoch-33 batch-127 = 2.772524021565914e-05

Training epoch-33 batch-128
Running loss of epoch-33 batch-128 = 4.550395533442497e-05

Training epoch-33 batch-129
Running loss of epoch-33 batch-129 = 9.658723138272762e-05

Training epoch-33 batch-130
Running loss of epoch-33 batch-130 = 0.0001426574308425188

Training epoch-33 batch-131
Running loss of epoch-33 batch-131 = 6.898632273077965e-05

Training epoch-33 batch-132
Running loss of epoch-33 batch-132 = 4.323129542171955e-05

Training epoch-33 batch-133
Running loss of epoch-33 batch-133 = 2.0684441551566124e-05

Training epoch-33 batch-134
Running loss of epoch-33 batch-134 = 7.020379416644573e-05

Training epoch-33 batch-135
Running loss of epoch-33 batch-135 = 0.00010350043885409832

Training epoch-33 batch-136
Running loss of epoch-33 batch-136 = 0.0001149479066953063

Training epoch-33 batch-137
Running loss of epoch-33 batch-137 = 0.00013636471703648567

Training epoch-33 batch-138
Running loss of epoch-33 batch-138 = 0.00010378845036029816

Training epoch-33 batch-139
Running loss of epoch-33 batch-139 = 7.245037704706192e-05

Training epoch-33 batch-140
Running loss of epoch-33 batch-140 = 0.00011435220949351788

Training epoch-33 batch-141
Running loss of epoch-33 batch-141 = 0.00012740329839289188

Training epoch-33 batch-142
Running loss of epoch-33 batch-142 = 0.0003544137580320239

Training epoch-33 batch-143
Running loss of epoch-33 batch-143 = 0.00010026665404438972

Training epoch-33 batch-144
Running loss of epoch-33 batch-144 = 0.00014577736146748066

Training epoch-33 batch-145
Running loss of epoch-33 batch-145 = 3.048148937523365e-05

Training epoch-33 batch-146
Running loss of epoch-33 batch-146 = 4.0646642446517944e-05

Training epoch-33 batch-147
Running loss of epoch-33 batch-147 = 4.3160514906048775e-05

Training epoch-33 batch-148
Running loss of epoch-33 batch-148 = 6.48394925519824e-05

Training epoch-33 batch-149
Running loss of epoch-33 batch-149 = 5.7797180488705635e-05

Training epoch-33 batch-150
Running loss of epoch-33 batch-150 = 0.00011200387962162495

Training epoch-33 batch-151
Running loss of epoch-33 batch-151 = 0.00027923937886953354

Training epoch-33 batch-152
Running loss of epoch-33 batch-152 = 0.00021301675587892532

Training epoch-33 batch-153
Running loss of epoch-33 batch-153 = 7.767393253743649e-05

Training epoch-33 batch-154
Running loss of epoch-33 batch-154 = 0.00010373163968324661

Training epoch-33 batch-155
Running loss of epoch-33 batch-155 = 4.4887419790029526e-05

Training epoch-33 batch-156
Running loss of epoch-33 batch-156 = 7.565878331661224e-05

Training epoch-33 batch-157
Running loss of epoch-33 batch-157 = 6.502494215965271e-05

Finished training epoch-33.



Average train loss at epoch-33 = 0.0001162086933851242

Started Evaluation

Average val loss at epoch-33 = 1.3597879793298107

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 79.84 %
Accuracy for class onCreate is: 90.94 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 60.05 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 55.38 %
Accuracy for class execute is: 53.01 %
Accuracy for class get is: 62.82 %

Overall Accuracy = 80.50 %

Finished Evaluation



Started training epoch-34


Training epoch-34 batch-1
Running loss of epoch-34 batch-1 = 0.00010186177678406239

Training epoch-34 batch-2
Running loss of epoch-34 batch-2 = 0.0001098352950066328

Training epoch-34 batch-3
Running loss of epoch-34 batch-3 = 0.0001632219646126032

Training epoch-34 batch-4
Running loss of epoch-34 batch-4 = 0.00016969162970781326

Training epoch-34 batch-5
Running loss of epoch-34 batch-5 = 7.76476226747036e-05

Training epoch-34 batch-6
Running loss of epoch-34 batch-6 = 0.00011291750706732273

Training epoch-34 batch-7
Running loss of epoch-34 batch-7 = 0.0001254812814295292

Training epoch-34 batch-8
Running loss of epoch-34 batch-8 = 8.645979687571526e-05

Training epoch-34 batch-9
Running loss of epoch-34 batch-9 = 1.7814338207244873e-05

Training epoch-34 batch-10
Running loss of epoch-34 batch-10 = 9.021779987961054e-05

Training epoch-34 batch-11
Running loss of epoch-34 batch-11 = 2.2566644474864006e-05

Training epoch-34 batch-12
Running loss of epoch-34 batch-12 = 5.276571027934551e-05

Training epoch-34 batch-13
Running loss of epoch-34 batch-13 = 0.0001791429240256548

Training epoch-34 batch-14
Running loss of epoch-34 batch-14 = 0.0001627258025109768

Training epoch-34 batch-15
Running loss of epoch-34 batch-15 = 0.0001028105616569519

Training epoch-34 batch-16
Running loss of epoch-34 batch-16 = 3.758072853088379e-05

Training epoch-34 batch-17
Running loss of epoch-34 batch-17 = 0.00012848339974880219

Training epoch-34 batch-18
Running loss of epoch-34 batch-18 = 5.013635382056236e-05

Training epoch-34 batch-19
Running loss of epoch-34 batch-19 = 5.58996107429266e-05

Training epoch-34 batch-20
Running loss of epoch-34 batch-20 = 0.0010012947022914886

Training epoch-34 batch-21
Running loss of epoch-34 batch-21 = 4.4380780309438705e-05

Training epoch-34 batch-22
Running loss of epoch-34 batch-22 = 0.00015136180445551872

Training epoch-34 batch-23
Running loss of epoch-34 batch-23 = 5.729333497583866e-05

Training epoch-34 batch-24
Running loss of epoch-34 batch-24 = 0.00038643181324005127

Training epoch-34 batch-25
Running loss of epoch-34 batch-25 = 0.0001864139921963215

Training epoch-34 batch-26
Running loss of epoch-34 batch-26 = 3.5128090530633926e-05

Training epoch-34 batch-27
Running loss of epoch-34 batch-27 = 0.00010892900172621012

Training epoch-34 batch-28
Running loss of epoch-34 batch-28 = 6.734346970915794e-05

Training epoch-34 batch-29
Running loss of epoch-34 batch-29 = 1.9094441086053848e-05

Training epoch-34 batch-30
Running loss of epoch-34 batch-30 = 3.524823114275932e-05

Training epoch-34 batch-31
Running loss of epoch-34 batch-31 = 6.69828150421381e-05

Training epoch-34 batch-32
Running loss of epoch-34 batch-32 = 3.308989107608795e-05

Training epoch-34 batch-33
Running loss of epoch-34 batch-33 = 2.45820265263319e-05

Training epoch-34 batch-34
Running loss of epoch-34 batch-34 = 2.6796478778123856e-05

Training epoch-34 batch-35
Running loss of epoch-34 batch-35 = 0.00025175209157168865

Training epoch-34 batch-36
Running loss of epoch-34 batch-36 = 2.223113551735878e-05

Training epoch-34 batch-37
Running loss of epoch-34 batch-37 = 0.00010359706357121468

Training epoch-34 batch-38
Running loss of epoch-34 batch-38 = 3.817584365606308e-05

Training epoch-34 batch-39
Running loss of epoch-34 batch-39 = 0.00027149938978254795

Training epoch-34 batch-40
Running loss of epoch-34 batch-40 = 0.0002301088534295559

Training epoch-34 batch-41
Running loss of epoch-34 batch-41 = 9.088520891964436e-05

Training epoch-34 batch-42
Running loss of epoch-34 batch-42 = 4.7715380787849426e-05

Training epoch-34 batch-43
Running loss of epoch-34 batch-43 = 0.00020542973652482033

Training epoch-34 batch-44
Running loss of epoch-34 batch-44 = 6.97143841534853e-05

Training epoch-34 batch-45
Running loss of epoch-34 batch-45 = 0.00023515382781624794

Training epoch-34 batch-46
Running loss of epoch-34 batch-46 = 9.495695121586323e-05

Training epoch-34 batch-47
Running loss of epoch-34 batch-47 = 7.597682997584343e-05

Training epoch-34 batch-48
Running loss of epoch-34 batch-48 = 4.4632237404584885e-05

Training epoch-34 batch-49
Running loss of epoch-34 batch-49 = 3.264774568378925e-05

Training epoch-34 batch-50
Running loss of epoch-34 batch-50 = 2.7722446247935295e-05

Training epoch-34 batch-51
Running loss of epoch-34 batch-51 = 8.564861491322517e-05

Training epoch-34 batch-52
Running loss of epoch-34 batch-52 = 2.478761598467827e-05

Training epoch-34 batch-53
Running loss of epoch-34 batch-53 = 1.3984506949782372e-05

Training epoch-34 batch-54
Running loss of epoch-34 batch-54 = 4.2355386540293694e-05

Training epoch-34 batch-55
Running loss of epoch-34 batch-55 = 4.3496023863554e-05

Training epoch-34 batch-56
Running loss of epoch-34 batch-56 = 6.361911073327065e-05

Training epoch-34 batch-57
Running loss of epoch-34 batch-57 = 1.948513090610504e-05

Training epoch-34 batch-58
Running loss of epoch-34 batch-58 = 8.12725629657507e-05

Training epoch-34 batch-59
Running loss of epoch-34 batch-59 = 7.478753104805946e-05

Training epoch-34 batch-60
Running loss of epoch-34 batch-60 = 8.396105840802193e-05

Training epoch-34 batch-61
Running loss of epoch-34 batch-61 = 2.8145965188741684e-05

Training epoch-34 batch-62
Running loss of epoch-34 batch-62 = 4.5305583626031876e-05

Training epoch-34 batch-63
Running loss of epoch-34 batch-63 = 9.166216477751732e-05

Training epoch-34 batch-64
Running loss of epoch-34 batch-64 = 0.00013949396088719368

Training epoch-34 batch-65
Running loss of epoch-34 batch-65 = 4.5424094423651695e-05

Training epoch-34 batch-66
Running loss of epoch-34 batch-66 = 1.3453653082251549e-05

Training epoch-34 batch-67
Running loss of epoch-34 batch-67 = 8.966540917754173e-05

Training epoch-34 batch-68
Running loss of epoch-34 batch-68 = 0.00011027068831026554

Training epoch-34 batch-69
Running loss of epoch-34 batch-69 = 9.719119407236576e-05

Training epoch-34 batch-70
Running loss of epoch-34 batch-70 = 5.942443385720253e-05

Training epoch-34 batch-71
Running loss of epoch-34 batch-71 = 4.2133964598178864e-05

Training epoch-34 batch-72
Running loss of epoch-34 batch-72 = 8.947588503360748e-05

Training epoch-34 batch-73
Running loss of epoch-34 batch-73 = 8.533732034265995e-05

Training epoch-34 batch-74
Running loss of epoch-34 batch-74 = 6.964430212974548e-05

Training epoch-34 batch-75
Running loss of epoch-34 batch-75 = 9.604578372091055e-05

Training epoch-34 batch-76
Running loss of epoch-34 batch-76 = 5.262834019958973e-05

Training epoch-34 batch-77
Running loss of epoch-34 batch-77 = 1.6588717699050903e-05

Training epoch-34 batch-78
Running loss of epoch-34 batch-78 = 8.010095916688442e-05

Training epoch-34 batch-79
Running loss of epoch-34 batch-79 = 4.514586180448532e-05

Training epoch-34 batch-80
Running loss of epoch-34 batch-80 = 5.9585319831967354e-05

Training epoch-34 batch-81
Running loss of epoch-34 batch-81 = 4.497752524912357e-05

Training epoch-34 batch-82
Running loss of epoch-34 batch-82 = 7.274281233549118e-05

Training epoch-34 batch-83
Running loss of epoch-34 batch-83 = 7.325364276766777e-05

Training epoch-34 batch-84
Running loss of epoch-34 batch-84 = 4.7945184633135796e-05

Training epoch-34 batch-85
Running loss of epoch-34 batch-85 = 3.134086728096008e-05

Training epoch-34 batch-86
Running loss of epoch-34 batch-86 = 0.00031937891617417336

Training epoch-34 batch-87
Running loss of epoch-34 batch-87 = 7.681571878492832e-05

Training epoch-34 batch-88
Running loss of epoch-34 batch-88 = 5.0892122089862823e-05

Training epoch-34 batch-89
Running loss of epoch-34 batch-89 = 0.00013233930803835392

Training epoch-34 batch-90
Running loss of epoch-34 batch-90 = 0.00014218338765203953

Training epoch-34 batch-91
Running loss of epoch-34 batch-91 = 3.453087992966175e-05

Training epoch-34 batch-92
Running loss of epoch-34 batch-92 = 3.74307855963707e-05

Training epoch-34 batch-93
Running loss of epoch-34 batch-93 = 0.00010135234333574772

Training epoch-34 batch-94
Running loss of epoch-34 batch-94 = 4.9194786697626114e-05

Training epoch-34 batch-95
Running loss of epoch-34 batch-95 = 9.099580347537994e-05

Training epoch-34 batch-96
Running loss of epoch-34 batch-96 = 7.168808951973915e-05

Training epoch-34 batch-97
Running loss of epoch-34 batch-97 = 0.00016189063899219036

Training epoch-34 batch-98
Running loss of epoch-34 batch-98 = 6.703287363052368e-05

Training epoch-34 batch-99
Running loss of epoch-34 batch-99 = 1.4336546882987022e-05

Training epoch-34 batch-100
Running loss of epoch-34 batch-100 = 0.00031596666667610407

Training epoch-34 batch-101
Running loss of epoch-34 batch-101 = 4.098447971045971e-05

Training epoch-34 batch-102
Running loss of epoch-34 batch-102 = 5.4251402616500854e-05

Training epoch-34 batch-103
Running loss of epoch-34 batch-103 = 5.838717333972454e-05

Training epoch-34 batch-104
Running loss of epoch-34 batch-104 = 4.416005685925484e-05

Training epoch-34 batch-105
Running loss of epoch-34 batch-105 = 0.000211755046620965

Training epoch-34 batch-106
Running loss of epoch-34 batch-106 = 0.00012431759387254715

Training epoch-34 batch-107
Running loss of epoch-34 batch-107 = 0.00016264920122921467

Training epoch-34 batch-108
Running loss of epoch-34 batch-108 = 1.3819662854075432e-05

Training epoch-34 batch-109
Running loss of epoch-34 batch-109 = 5.032098852097988e-05

Training epoch-34 batch-110
Running loss of epoch-34 batch-110 = 4.719104617834091e-05

Training epoch-34 batch-111
Running loss of epoch-34 batch-111 = 9.454810060560703e-05

Training epoch-34 batch-112
Running loss of epoch-34 batch-112 = 0.00013234931975603104

Training epoch-34 batch-113
Running loss of epoch-34 batch-113 = 2.7851201593875885e-05

Training epoch-34 batch-114
Running loss of epoch-34 batch-114 = 3.416929394006729e-05

Training epoch-34 batch-115
Running loss of epoch-34 batch-115 = 0.00010845903307199478

Training epoch-34 batch-116
Running loss of epoch-34 batch-116 = 5.55266160517931e-05

Training epoch-34 batch-117
Running loss of epoch-34 batch-117 = 5.1560113206505775e-05

Training epoch-34 batch-118
Running loss of epoch-34 batch-118 = 1.5389174222946167e-05

Training epoch-34 batch-119
Running loss of epoch-34 batch-119 = 0.0002570446813479066

Training epoch-34 batch-120
Running loss of epoch-34 batch-120 = 0.00011211365927010775

Training epoch-34 batch-121
Running loss of epoch-34 batch-121 = 5.123531445860863e-05

Training epoch-34 batch-122
Running loss of epoch-34 batch-122 = 0.00010078842751681805

Training epoch-34 batch-123
Running loss of epoch-34 batch-123 = 6.636232137680054e-05

Training epoch-34 batch-124
Running loss of epoch-34 batch-124 = 0.00012655125465244055

Training epoch-34 batch-125
Running loss of epoch-34 batch-125 = 2.231891267001629e-05

Training epoch-34 batch-126
Running loss of epoch-34 batch-126 = 8.109118789434433e-05

Training epoch-34 batch-127
Running loss of epoch-34 batch-127 = 2.3564323782920837e-05

Training epoch-34 batch-128
Running loss of epoch-34 batch-128 = 1.2324657291173935e-05

Training epoch-34 batch-129
Running loss of epoch-34 batch-129 = 2.0665815100073814e-05

Training epoch-34 batch-130
Running loss of epoch-34 batch-130 = 0.00010541081428527832

Training epoch-34 batch-131
Running loss of epoch-34 batch-131 = 3.533810377120972e-05

Training epoch-34 batch-132
Running loss of epoch-34 batch-132 = 2.765841782093048e-05

Training epoch-34 batch-133
Running loss of epoch-34 batch-133 = 4.869210533797741e-05

Training epoch-34 batch-134
Running loss of epoch-34 batch-134 = 0.0001386858057230711

Training epoch-34 batch-135
Running loss of epoch-34 batch-135 = 3.016204573214054e-05

Training epoch-34 batch-136
Running loss of epoch-34 batch-136 = 0.00045159144792705774

Training epoch-34 batch-137
Running loss of epoch-34 batch-137 = 6.39867503196001e-05

Training epoch-34 batch-138
Running loss of epoch-34 batch-138 = 5.191948730498552e-05

Training epoch-34 batch-139
Running loss of epoch-34 batch-139 = 9.343796409666538e-05

Training epoch-34 batch-140
Running loss of epoch-34 batch-140 = 0.0001466950634494424

Training epoch-34 batch-141
Running loss of epoch-34 batch-141 = 9.264401160180569e-05

Training epoch-34 batch-142
Running loss of epoch-34 batch-142 = 2.236827276647091e-05

Training epoch-34 batch-143
Running loss of epoch-34 batch-143 = 0.000122767873108387

Training epoch-34 batch-144
Running loss of epoch-34 batch-144 = 7.650395855307579e-05

Training epoch-34 batch-145
Running loss of epoch-34 batch-145 = 3.880448639392853e-05

Training epoch-34 batch-146
Running loss of epoch-34 batch-146 = 5.656573921442032e-05

Training epoch-34 batch-147
Running loss of epoch-34 batch-147 = 2.0369188860058784e-05

Training epoch-34 batch-148
Running loss of epoch-34 batch-148 = 6.380677223205566e-05

Training epoch-34 batch-149
Running loss of epoch-34 batch-149 = 7.350277155637741e-05

Training epoch-34 batch-150
Running loss of epoch-34 batch-150 = 0.0003714135382324457

Training epoch-34 batch-151
Running loss of epoch-34 batch-151 = 1.1074123904109001e-05

Training epoch-34 batch-152
Running loss of epoch-34 batch-152 = 8.614244870841503e-05

Training epoch-34 batch-153
Running loss of epoch-34 batch-153 = 8.824141696095467e-05

Training epoch-34 batch-154
Running loss of epoch-34 batch-154 = 2.324138768017292e-05

Training epoch-34 batch-155
Running loss of epoch-34 batch-155 = 0.00020119501277804375

Training epoch-34 batch-156
Running loss of epoch-34 batch-156 = 4.622479900717735e-05

Training epoch-34 batch-157
Running loss of epoch-34 batch-157 = 6.051361560821533e-05

Finished training epoch-34.



Average train loss at epoch-34 = 9.321960359811783e-05

Started Evaluation

Average val loss at epoch-34 = 1.3649495467893997

Accuracy for classes:
Accuracy for class equals is: 95.87 %
Accuracy for class main is: 95.90 %
Accuracy for class setUp is: 87.05 %
Accuracy for class onCreate is: 90.19 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 56.85 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.93 %
Accuracy for class execute is: 46.59 %
Accuracy for class get is: 68.46 %

Overall Accuracy = 81.04 %

Finished Evaluation



Started training epoch-35


Training epoch-35 batch-1
Running loss of epoch-35 batch-1 = 2.6849564164876938e-05

Training epoch-35 batch-2
Running loss of epoch-35 batch-2 = 4.7808513045310974e-05

Training epoch-35 batch-3
Running loss of epoch-35 batch-3 = 7.770443335175514e-05

Training epoch-35 batch-4
Running loss of epoch-35 batch-4 = 7.738522253930569e-05

Training epoch-35 batch-5
Running loss of epoch-35 batch-5 = 7.985043339431286e-05

Training epoch-35 batch-6
Running loss of epoch-35 batch-6 = 0.000189576530829072

Training epoch-35 batch-7
Running loss of epoch-35 batch-7 = 4.601827822625637e-05

Training epoch-35 batch-8
Running loss of epoch-35 batch-8 = 0.00022829417139291763

Training epoch-35 batch-9
Running loss of epoch-35 batch-9 = 0.00012802379205822945

Training epoch-35 batch-10
Running loss of epoch-35 batch-10 = 4.8518646508455276e-05

Training epoch-35 batch-11
Running loss of epoch-35 batch-11 = 4.31528314948082e-05

Training epoch-35 batch-12
Running loss of epoch-35 batch-12 = 2.2865599021315575e-05

Training epoch-35 batch-13
Running loss of epoch-35 batch-13 = 2.2082356736063957e-05

Training epoch-35 batch-14
Running loss of epoch-35 batch-14 = 2.856808714568615e-05

Training epoch-35 batch-15
Running loss of epoch-35 batch-15 = 7.866788655519485e-05

Training epoch-35 batch-16
Running loss of epoch-35 batch-16 = 7.83279538154602e-05

Training epoch-35 batch-17
Running loss of epoch-35 batch-17 = 6.0613034293055534e-05

Training epoch-35 batch-18
Running loss of epoch-35 batch-18 = 4.0203332901000977e-05

Training epoch-35 batch-19
Running loss of epoch-35 batch-19 = 4.451465792953968e-05

Training epoch-35 batch-20
Running loss of epoch-35 batch-20 = 9.214738383889198e-06

Training epoch-35 batch-21
Running loss of epoch-35 batch-21 = 2.263602800667286e-05

Training epoch-35 batch-22
Running loss of epoch-35 batch-22 = 7.27442093193531e-05

Training epoch-35 batch-23
Running loss of epoch-35 batch-23 = 0.0001697250409051776

Training epoch-35 batch-24
Running loss of epoch-35 batch-24 = 5.9434911236166954e-05

Training epoch-35 batch-25
Running loss of epoch-35 batch-25 = 5.5141979828476906e-05

Training epoch-35 batch-26
Running loss of epoch-35 batch-26 = 7.940619252622128e-05

Training epoch-35 batch-27
Running loss of epoch-35 batch-27 = 2.4414388462901115e-05

Training epoch-35 batch-28
Running loss of epoch-35 batch-28 = 0.00023907865397632122

Training epoch-35 batch-29
Running loss of epoch-35 batch-29 = 5.188398063182831e-06

Training epoch-35 batch-30
Running loss of epoch-35 batch-30 = 4.5844120904803276e-05

Training epoch-35 batch-31
Running loss of epoch-35 batch-31 = 6.11466821283102e-05

Training epoch-35 batch-32
Running loss of epoch-35 batch-32 = 2.7600210160017014e-05

Training epoch-35 batch-33
Running loss of epoch-35 batch-33 = 0.00012935255654156208

Training epoch-35 batch-34
Running loss of epoch-35 batch-34 = 2.17570923268795e-05

Training epoch-35 batch-35
Running loss of epoch-35 batch-35 = 0.00017960486002266407

Training epoch-35 batch-36
Running loss of epoch-35 batch-36 = 5.1113078370690346e-05

Training epoch-35 batch-37
Running loss of epoch-35 batch-37 = 5.541532300412655e-05

Training epoch-35 batch-38
Running loss of epoch-35 batch-38 = 5.1678624004125595e-05

Training epoch-35 batch-39
Running loss of epoch-35 batch-39 = 8.579948917031288e-05

Training epoch-35 batch-40
Running loss of epoch-35 batch-40 = 1.295376569032669e-05

Training epoch-35 batch-41
Running loss of epoch-35 batch-41 = 9.115086868405342e-06

Training epoch-35 batch-42
Running loss of epoch-35 batch-42 = 2.9464950785040855e-05

Training epoch-35 batch-43
Running loss of epoch-35 batch-43 = 4.327087663114071e-05

Training epoch-35 batch-44
Running loss of epoch-35 batch-44 = 8.4741972386837e-05

Training epoch-35 batch-45
Running loss of epoch-35 batch-45 = 6.323889829218388e-05

Training epoch-35 batch-46
Running loss of epoch-35 batch-46 = 1.6612699255347252e-05

Training epoch-35 batch-47
Running loss of epoch-35 batch-47 = 6.652995944023132e-05

Training epoch-35 batch-48
Running loss of epoch-35 batch-48 = 1.5110708773136139e-05

Training epoch-35 batch-49
Running loss of epoch-35 batch-49 = 1.1446885764598846e-05

Training epoch-35 batch-50
Running loss of epoch-35 batch-50 = 0.0001388776581734419

Training epoch-35 batch-51
Running loss of epoch-35 batch-51 = 2.6017194613814354e-05

Training epoch-35 batch-52
Running loss of epoch-35 batch-52 = 2.5956891477108002e-05

Training epoch-35 batch-53
Running loss of epoch-35 batch-53 = 7.739150896668434e-05

Training epoch-35 batch-54
Running loss of epoch-35 batch-54 = 0.0003191353753209114

Training epoch-35 batch-55
Running loss of epoch-35 batch-55 = 9.537290316075087e-05

Training epoch-35 batch-56
Running loss of epoch-35 batch-56 = 8.024205453693867e-05

Training epoch-35 batch-57
Running loss of epoch-35 batch-57 = 0.0001301111187785864

Training epoch-35 batch-58
Running loss of epoch-35 batch-58 = 6.236345507204533e-05

Training epoch-35 batch-59
Running loss of epoch-35 batch-59 = 2.5211600586771965e-05

Training epoch-35 batch-60
Running loss of epoch-35 batch-60 = 2.4134758859872818e-05

Training epoch-35 batch-61
Running loss of epoch-35 batch-61 = 1.7453450709581375e-05

Training epoch-35 batch-62
Running loss of epoch-35 batch-62 = 4.232488572597504e-05

Training epoch-35 batch-63
Running loss of epoch-35 batch-63 = 7.758103311061859e-05

Training epoch-35 batch-64
Running loss of epoch-35 batch-64 = 2.329982817173004e-05

Training epoch-35 batch-65
Running loss of epoch-35 batch-65 = 9.43569466471672e-05

Training epoch-35 batch-66
Running loss of epoch-35 batch-66 = 0.000271468423306942

Training epoch-35 batch-67
Running loss of epoch-35 batch-67 = 0.0001243592705577612

Training epoch-35 batch-68
Running loss of epoch-35 batch-68 = 7.384922355413437e-05

Training epoch-35 batch-69
Running loss of epoch-35 batch-69 = 8.381064981222153e-05

Training epoch-35 batch-70
Running loss of epoch-35 batch-70 = 0.0006047275383025408

Training epoch-35 batch-71
Running loss of epoch-35 batch-71 = 4.71784733235836e-05

Training epoch-35 batch-72
Running loss of epoch-35 batch-72 = 4.294957034289837e-05

Training epoch-35 batch-73
Running loss of epoch-35 batch-73 = 5.521706771105528e-05

Training epoch-35 batch-74
Running loss of epoch-35 batch-74 = 5.209236405789852e-05

Training epoch-35 batch-75
Running loss of epoch-35 batch-75 = 2.034544013440609e-05

Training epoch-35 batch-76
Running loss of epoch-35 batch-76 = 5.0056842155754566e-05

Training epoch-35 batch-77
Running loss of epoch-35 batch-77 = 4.022871144115925e-05

Training epoch-35 batch-78
Running loss of epoch-35 batch-78 = 9.340723045170307e-05

Training epoch-35 batch-79
Running loss of epoch-35 batch-79 = 0.00013231509365141392

Training epoch-35 batch-80
Running loss of epoch-35 batch-80 = 3.585102967917919e-05

Training epoch-35 batch-81
Running loss of epoch-35 batch-81 = 0.0002722502686083317

Training epoch-35 batch-82
Running loss of epoch-35 batch-82 = 2.8461916372179985e-05

Training epoch-35 batch-83
Running loss of epoch-35 batch-83 = 4.9439724534749985e-05

Training epoch-35 batch-84
Running loss of epoch-35 batch-84 = 2.54723709076643e-05

Training epoch-35 batch-85
Running loss of epoch-35 batch-85 = 7.972074672579765e-05

Training epoch-35 batch-86
Running loss of epoch-35 batch-86 = 4.8915622755885124e-05

Training epoch-35 batch-87
Running loss of epoch-35 batch-87 = 0.00010062637738883495

Training epoch-35 batch-88
Running loss of epoch-35 batch-88 = 1.927558332681656e-05

Training epoch-35 batch-89
Running loss of epoch-35 batch-89 = 2.681068144738674e-05

Training epoch-35 batch-90
Running loss of epoch-35 batch-90 = 2.1411804482340813e-05

Training epoch-35 batch-91
Running loss of epoch-35 batch-91 = 9.533367119729519e-05

Training epoch-35 batch-92
Running loss of epoch-35 batch-92 = 3.14472708851099e-05

Training epoch-35 batch-93
Running loss of epoch-35 batch-93 = 6.0431892052292824e-05

Training epoch-35 batch-94
Running loss of epoch-35 batch-94 = 0.0001407440286129713

Training epoch-35 batch-95
Running loss of epoch-35 batch-95 = 0.00010036444291472435

Training epoch-35 batch-96
Running loss of epoch-35 batch-96 = 3.849179483950138e-05

Training epoch-35 batch-97
Running loss of epoch-35 batch-97 = 4.381290636956692e-05

Training epoch-35 batch-98
Running loss of epoch-35 batch-98 = 7.29602761566639e-05

Training epoch-35 batch-99
Running loss of epoch-35 batch-99 = 0.000105647137388587

Training epoch-35 batch-100
Running loss of epoch-35 batch-100 = 2.2778520360589027e-05

Training epoch-35 batch-101
Running loss of epoch-35 batch-101 = 0.00018276413902640343

Training epoch-35 batch-102
Running loss of epoch-35 batch-102 = 8.969195187091827e-05

Training epoch-35 batch-103
Running loss of epoch-35 batch-103 = 6.58214557915926e-05

Training epoch-35 batch-104
Running loss of epoch-35 batch-104 = 2.6160618290305138e-05

Training epoch-35 batch-105
Running loss of epoch-35 batch-105 = 2.1248124539852142e-05

Training epoch-35 batch-106
Running loss of epoch-35 batch-106 = 7.587345317006111e-05

Training epoch-35 batch-107
Running loss of epoch-35 batch-107 = 1.95247121155262e-05

Training epoch-35 batch-108
Running loss of epoch-35 batch-108 = 9.058951400220394e-05

Training epoch-35 batch-109
Running loss of epoch-35 batch-109 = 5.8463308960199356e-05

Training epoch-35 batch-110
Running loss of epoch-35 batch-110 = 2.3810425773262978e-05

Training epoch-35 batch-111
Running loss of epoch-35 batch-111 = 6.95760827511549e-05

Training epoch-35 batch-112
Running loss of epoch-35 batch-112 = 8.636876009404659e-05

Training epoch-35 batch-113
Running loss of epoch-35 batch-113 = 7.306132465600967e-05

Training epoch-35 batch-114
Running loss of epoch-35 batch-114 = 0.00012170011177659035

Training epoch-35 batch-115
Running loss of epoch-35 batch-115 = 0.00024157355073839426

Training epoch-35 batch-116
Running loss of epoch-35 batch-116 = 5.20248431712389e-05

Training epoch-35 batch-117
Running loss of epoch-35 batch-117 = 2.4830223992466927e-05

Training epoch-35 batch-118
Running loss of epoch-35 batch-118 = 6.183073855936527e-05

Training epoch-35 batch-119
Running loss of epoch-35 batch-119 = 2.913130447268486e-05

Training epoch-35 batch-120
Running loss of epoch-35 batch-120 = 3.775849472731352e-05

Training epoch-35 batch-121
Running loss of epoch-35 batch-121 = 2.78339721262455e-05

Training epoch-35 batch-122
Running loss of epoch-35 batch-122 = 8.377712219953537e-06

Training epoch-35 batch-123
Running loss of epoch-35 batch-123 = 0.0001141098327934742

Training epoch-35 batch-124
Running loss of epoch-35 batch-124 = 2.6592984795570374e-05

Training epoch-35 batch-125
Running loss of epoch-35 batch-125 = 0.00010309484787285328

Training epoch-35 batch-126
Running loss of epoch-35 batch-126 = 1.642131246626377e-05

Training epoch-35 batch-127
Running loss of epoch-35 batch-127 = 5.1655108109116554e-05

Training epoch-35 batch-128
Running loss of epoch-35 batch-128 = 0.0001592810731381178

Training epoch-35 batch-129
Running loss of epoch-35 batch-129 = 1.833215355873108e-05

Training epoch-35 batch-130
Running loss of epoch-35 batch-130 = 2.895924262702465e-05

Training epoch-35 batch-131
Running loss of epoch-35 batch-131 = 8.70071817189455e-05

Training epoch-35 batch-132
Running loss of epoch-35 batch-132 = 5.511799827218056e-05

Training epoch-35 batch-133
Running loss of epoch-35 batch-133 = 7.590767927467823e-05

Training epoch-35 batch-134
Running loss of epoch-35 batch-134 = 0.00029461178928613663

Training epoch-35 batch-135
Running loss of epoch-35 batch-135 = 0.00012978445738554

Training epoch-35 batch-136
Running loss of epoch-35 batch-136 = 9.961100295186043e-05

Training epoch-35 batch-137
Running loss of epoch-35 batch-137 = 3.1615374609827995e-05

Training epoch-35 batch-138
Running loss of epoch-35 batch-138 = 5.009048618376255e-05

Training epoch-35 batch-139
Running loss of epoch-35 batch-139 = 5.064532160758972e-05

Training epoch-35 batch-140
Running loss of epoch-35 batch-140 = 1.9568949937820435e-05

Training epoch-35 batch-141
Running loss of epoch-35 batch-141 = 5.3758732974529266e-05

Training epoch-35 batch-142
Running loss of epoch-35 batch-142 = 8.118059486150742e-05

Training epoch-35 batch-143
Running loss of epoch-35 batch-143 = 0.0001135470811277628

Training epoch-35 batch-144
Running loss of epoch-35 batch-144 = 7.845996879041195e-05

Training epoch-35 batch-145
Running loss of epoch-35 batch-145 = 1.6334932297468185e-05

Training epoch-35 batch-146
Running loss of epoch-35 batch-146 = 2.50211451202631e-05

Training epoch-35 batch-147
Running loss of epoch-35 batch-147 = 0.00013545469846576452

Training epoch-35 batch-148
Running loss of epoch-35 batch-148 = 8.51810909807682e-05

Training epoch-35 batch-149
Running loss of epoch-35 batch-149 = 4.807417280972004e-05

Training epoch-35 batch-150
Running loss of epoch-35 batch-150 = 2.7888454496860504e-05

Training epoch-35 batch-151
Running loss of epoch-35 batch-151 = 7.930700667202473e-05

Training epoch-35 batch-152
Running loss of epoch-35 batch-152 = 6.820284761488438e-05

Training epoch-35 batch-153
Running loss of epoch-35 batch-153 = 5.764700472354889e-05

Training epoch-35 batch-154
Running loss of epoch-35 batch-154 = 6.099441088736057e-05

Training epoch-35 batch-155
Running loss of epoch-35 batch-155 = 3.447243943810463e-05

Training epoch-35 batch-156
Running loss of epoch-35 batch-156 = 0.00012216833420097828

Training epoch-35 batch-157
Running loss of epoch-35 batch-157 = 3.3229589462280273e-05

Finished training epoch-35.



Average train loss at epoch-35 = 7.383241131901741e-05

Started Evaluation

Average val loss at epoch-35 = 1.3566926012544125

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 96.39 %
Accuracy for class setUp is: 86.72 %
Accuracy for class onCreate is: 90.72 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 62.79 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.71 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.39 %

Finished Evaluation



Started training epoch-36


Training epoch-36 batch-1
Running loss of epoch-36 batch-1 = 1.9927741959691048e-05

Training epoch-36 batch-2
Running loss of epoch-36 batch-2 = 9.907293133437634e-05

Training epoch-36 batch-3
Running loss of epoch-36 batch-3 = 1.1137686669826508e-05

Training epoch-36 batch-4
Running loss of epoch-36 batch-4 = 2.2899359464645386e-05

Training epoch-36 batch-5
Running loss of epoch-36 batch-5 = 2.1987129002809525e-05

Training epoch-36 batch-6
Running loss of epoch-36 batch-6 = 8.784700185060501e-06

Training epoch-36 batch-7
Running loss of epoch-36 batch-7 = 9.156111627817154e-05

Training epoch-36 batch-8
Running loss of epoch-36 batch-8 = 1.761317253112793e-05

Training epoch-36 batch-9
Running loss of epoch-36 batch-9 = 7.739104330539703e-05

Training epoch-36 batch-10
Running loss of epoch-36 batch-10 = 8.362275548279285e-05

Training epoch-36 batch-11
Running loss of epoch-36 batch-11 = 2.7995789423584938e-05

Training epoch-36 batch-12
Running loss of epoch-36 batch-12 = 4.840944893658161e-05

Training epoch-36 batch-13
Running loss of epoch-36 batch-13 = 6.0485443100333214e-05

Training epoch-36 batch-14
Running loss of epoch-36 batch-14 = 3.2115262001752853e-05

Training epoch-36 batch-15
Running loss of epoch-36 batch-15 = 1.818896271288395e-05

Training epoch-36 batch-16
Running loss of epoch-36 batch-16 = 1.8734484910964966e-05

Training epoch-36 batch-17
Running loss of epoch-36 batch-17 = 3.510620445013046e-05

Training epoch-36 batch-18
Running loss of epoch-36 batch-18 = 4.363013431429863e-05

Training epoch-36 batch-19
Running loss of epoch-36 batch-19 = 4.2509171180427074e-05

Training epoch-36 batch-20
Running loss of epoch-36 batch-20 = 3.381981514394283e-05

Training epoch-36 batch-21
Running loss of epoch-36 batch-21 = 4.1963765397667885e-05

Training epoch-36 batch-22
Running loss of epoch-36 batch-22 = 1.678219996392727e-05

Training epoch-36 batch-23
Running loss of epoch-36 batch-23 = 1.532072201371193e-05

Training epoch-36 batch-24
Running loss of epoch-36 batch-24 = 5.8696605265140533e-05

Training epoch-36 batch-25
Running loss of epoch-36 batch-25 = 2.841837704181671e-05

Training epoch-36 batch-26
Running loss of epoch-36 batch-26 = 8.03251750767231e-05

Training epoch-36 batch-27
Running loss of epoch-36 batch-27 = 4.52287495136261e-05

Training epoch-36 batch-28
Running loss of epoch-36 batch-28 = 3.627873957157135e-05

Training epoch-36 batch-29
Running loss of epoch-36 batch-29 = 4.711211659014225e-05

Training epoch-36 batch-30
Running loss of epoch-36 batch-30 = 6.649410352110863e-06

Training epoch-36 batch-31
Running loss of epoch-36 batch-31 = 1.819804310798645e-05

Training epoch-36 batch-32
Running loss of epoch-36 batch-32 = 5.852058529853821e-05

Training epoch-36 batch-33
Running loss of epoch-36 batch-33 = 8.239015005528927e-05

Training epoch-36 batch-34
Running loss of epoch-36 batch-34 = 5.770544521510601e-05

Training epoch-36 batch-35
Running loss of epoch-36 batch-35 = 2.5070738047361374e-05

Training epoch-36 batch-36
Running loss of epoch-36 batch-36 = 4.3095787987113e-05

Training epoch-36 batch-37
Running loss of epoch-36 batch-37 = 0.00032326742075383663

Training epoch-36 batch-38
Running loss of epoch-36 batch-38 = 2.5142449885606766e-05

Training epoch-36 batch-39
Running loss of epoch-36 batch-39 = 1.4807796105742455e-05

Training epoch-36 batch-40
Running loss of epoch-36 batch-40 = 7.312535308301449e-05

Training epoch-36 batch-41
Running loss of epoch-36 batch-41 = 4.3492065742611885e-05

Training epoch-36 batch-42
Running loss of epoch-36 batch-42 = 3.3025629818439484e-05

Training epoch-36 batch-43
Running loss of epoch-36 batch-43 = 1.2238044291734695e-05

Training epoch-36 batch-44
Running loss of epoch-36 batch-44 = 4.9658818170428276e-05

Training epoch-36 batch-45
Running loss of epoch-36 batch-45 = 7.421360351145267e-05

Training epoch-36 batch-46
Running loss of epoch-36 batch-46 = 5.5002979934215546e-05

Training epoch-36 batch-47
Running loss of epoch-36 batch-47 = 3.696233034133911e-05

Training epoch-36 batch-48
Running loss of epoch-36 batch-48 = 3.148731775581837e-05

Training epoch-36 batch-49
Running loss of epoch-36 batch-49 = 9.294692426919937e-05

Training epoch-36 batch-50
Running loss of epoch-36 batch-50 = 1.8608756363391876e-05

Training epoch-36 batch-51
Running loss of epoch-36 batch-51 = 0.00010048598051071167

Training epoch-36 batch-52
Running loss of epoch-36 batch-52 = 6.799190305173397e-05

Training epoch-36 batch-53
Running loss of epoch-36 batch-53 = 3.405660390853882e-05

Training epoch-36 batch-54
Running loss of epoch-36 batch-54 = 2.7874018996953964e-05

Training epoch-36 batch-55
Running loss of epoch-36 batch-55 = 4.1016144677996635e-05

Training epoch-36 batch-56
Running loss of epoch-36 batch-56 = 0.0001262458972632885

Training epoch-36 batch-57
Running loss of epoch-36 batch-57 = 1.200730912387371e-05

Training epoch-36 batch-58
Running loss of epoch-36 batch-58 = 2.9765535145998e-05

Training epoch-36 batch-59
Running loss of epoch-36 batch-59 = 6.638257764279842e-05

Training epoch-36 batch-60
Running loss of epoch-36 batch-60 = 5.311751738190651e-05

Training epoch-36 batch-61
Running loss of epoch-36 batch-61 = 0.00010573072358965874

Training epoch-36 batch-62
Running loss of epoch-36 batch-62 = 9.140186011791229e-05

Training epoch-36 batch-63
Running loss of epoch-36 batch-63 = 3.85260209441185e-05

Training epoch-36 batch-64
Running loss of epoch-36 batch-64 = 3.317743539810181e-05

Training epoch-36 batch-65
Running loss of epoch-36 batch-65 = 5.3895870223641396e-05

Training epoch-36 batch-66
Running loss of epoch-36 batch-66 = 0.00010205945000052452

Training epoch-36 batch-67
Running loss of epoch-36 batch-67 = 2.3599714040756226e-05

Training epoch-36 batch-68
Running loss of epoch-36 batch-68 = 8.908542804419994e-05

Training epoch-36 batch-69
Running loss of epoch-36 batch-69 = 8.025253191590309e-05

Training epoch-36 batch-70
Running loss of epoch-36 batch-70 = 5.367700941860676e-05

Training epoch-36 batch-71
Running loss of epoch-36 batch-71 = 3.268406726419926e-05

Training epoch-36 batch-72
Running loss of epoch-36 batch-72 = 6.424658931791782e-05

Training epoch-36 batch-73
Running loss of epoch-36 batch-73 = 0.0002137618139386177

Training epoch-36 batch-74
Running loss of epoch-36 batch-74 = 4.0980055928230286e-05

Training epoch-36 batch-75
Running loss of epoch-36 batch-75 = 3.0064955353736877e-05

Training epoch-36 batch-76
Running loss of epoch-36 batch-76 = 4.4941436499357224e-05

Training epoch-36 batch-77
Running loss of epoch-36 batch-77 = 7.2221620939672e-05

Training epoch-36 batch-78
Running loss of epoch-36 batch-78 = 8.746737148612738e-05

Training epoch-36 batch-79
Running loss of epoch-36 batch-79 = 3.819586709141731e-06

Training epoch-36 batch-80
Running loss of epoch-36 batch-80 = 5.957717075943947e-05

Training epoch-36 batch-81
Running loss of epoch-36 batch-81 = 4.222593270242214e-05

Training epoch-36 batch-82
Running loss of epoch-36 batch-82 = 2.7737580239772797e-05

Training epoch-36 batch-83
Running loss of epoch-36 batch-83 = 0.00039342802483588457

Training epoch-36 batch-84
Running loss of epoch-36 batch-84 = 2.7133850380778313e-05

Training epoch-36 batch-85
Running loss of epoch-36 batch-85 = 1.558498479425907e-05

Training epoch-36 batch-86
Running loss of epoch-36 batch-86 = 0.0001069018617272377

Training epoch-36 batch-87
Running loss of epoch-36 batch-87 = 3.457488492131233e-05

Training epoch-36 batch-88
Running loss of epoch-36 batch-88 = 3.886851482093334e-05

Training epoch-36 batch-89
Running loss of epoch-36 batch-89 = 6.751343607902527e-05

Training epoch-36 batch-90
Running loss of epoch-36 batch-90 = 1.8158461898565292e-05

Training epoch-36 batch-91
Running loss of epoch-36 batch-91 = 3.4942058846354485e-05

Training epoch-36 batch-92
Running loss of epoch-36 batch-92 = 3.926060162484646e-05

Training epoch-36 batch-93
Running loss of epoch-36 batch-93 = 4.6723755076527596e-05

Training epoch-36 batch-94
Running loss of epoch-36 batch-94 = 3.171549178659916e-05

Training epoch-36 batch-95
Running loss of epoch-36 batch-95 = 3.205891698598862e-05

Training epoch-36 batch-96
Running loss of epoch-36 batch-96 = 4.5949360355734825e-05

Training epoch-36 batch-97
Running loss of epoch-36 batch-97 = 6.137695163488388e-05

Training epoch-36 batch-98
Running loss of epoch-36 batch-98 = 4.1113002225756645e-05

Training epoch-36 batch-99
Running loss of epoch-36 batch-99 = 7.624039426445961e-05

Training epoch-36 batch-100
Running loss of epoch-36 batch-100 = 1.857057213783264e-05

Training epoch-36 batch-101
Running loss of epoch-36 batch-101 = 6.23953528702259e-05

Training epoch-36 batch-102
Running loss of epoch-36 batch-102 = 3.091874532401562e-05

Training epoch-36 batch-103
Running loss of epoch-36 batch-103 = 0.0001754160039126873

Training epoch-36 batch-104
Running loss of epoch-36 batch-104 = 2.606934867799282e-05

Training epoch-36 batch-105
Running loss of epoch-36 batch-105 = 4.1321851313114166e-05

Training epoch-36 batch-106
Running loss of epoch-36 batch-106 = 4.183361306786537e-05

Training epoch-36 batch-107
Running loss of epoch-36 batch-107 = 2.6167137548327446e-05

Training epoch-36 batch-108
Running loss of epoch-36 batch-108 = 0.00012803799472749233

Training epoch-36 batch-109
Running loss of epoch-36 batch-109 = 2.2218097001314163e-05

Training epoch-36 batch-110
Running loss of epoch-36 batch-110 = 1.4939345419406891e-05

Training epoch-36 batch-111
Running loss of epoch-36 batch-111 = 2.2506341338157654e-05

Training epoch-36 batch-112
Running loss of epoch-36 batch-112 = 8.87576024979353e-05

Training epoch-36 batch-113
Running loss of epoch-36 batch-113 = 0.0001504406100139022

Training epoch-36 batch-114
Running loss of epoch-36 batch-114 = 4.888209514319897e-05

Training epoch-36 batch-115
Running loss of epoch-36 batch-115 = 3.435625694692135e-05

Training epoch-36 batch-116
Running loss of epoch-36 batch-116 = 2.519926056265831e-05

Training epoch-36 batch-117
Running loss of epoch-36 batch-117 = 4.91519458591938e-05

Training epoch-36 batch-118
Running loss of epoch-36 batch-118 = 2.254336141049862e-05

Training epoch-36 batch-119
Running loss of epoch-36 batch-119 = 8.346349932253361e-05

Training epoch-36 batch-120
Running loss of epoch-36 batch-120 = 2.4775275960564613e-05

Training epoch-36 batch-121
Running loss of epoch-36 batch-121 = 2.64681875705719e-05

Training epoch-36 batch-122
Running loss of epoch-36 batch-122 = 0.00011442299000918865

Training epoch-36 batch-123
Running loss of epoch-36 batch-123 = 8.526258170604706e-06

Training epoch-36 batch-124
Running loss of epoch-36 batch-124 = 2.9660062864422798e-05

Training epoch-36 batch-125
Running loss of epoch-36 batch-125 = 2.1005747839808464e-05

Training epoch-36 batch-126
Running loss of epoch-36 batch-126 = 4.357169382274151e-05

Training epoch-36 batch-127
Running loss of epoch-36 batch-127 = 9.011372458189726e-05

Training epoch-36 batch-128
Running loss of epoch-36 batch-128 = 8.803908713161945e-05

Training epoch-36 batch-129
Running loss of epoch-36 batch-129 = 3.0207214877009392e-05

Training epoch-36 batch-130
Running loss of epoch-36 batch-130 = 3.979075700044632e-05

Training epoch-36 batch-131
Running loss of epoch-36 batch-131 = 6.0236197896301746e-05

Training epoch-36 batch-132
Running loss of epoch-36 batch-132 = 1.877988688647747e-05

Training epoch-36 batch-133
Running loss of epoch-36 batch-133 = 4.722806625068188e-05

Training epoch-36 batch-134
Running loss of epoch-36 batch-134 = 1.349346712231636e-05

Training epoch-36 batch-135
Running loss of epoch-36 batch-135 = 4.2842235416173935e-05

Training epoch-36 batch-136
Running loss of epoch-36 batch-136 = 6.517604924738407e-05

Training epoch-36 batch-137
Running loss of epoch-36 batch-137 = 4.054093733429909e-05

Training epoch-36 batch-138
Running loss of epoch-36 batch-138 = 2.7019064873456955e-05

Training epoch-36 batch-139
Running loss of epoch-36 batch-139 = 5.073414649814367e-05

Training epoch-36 batch-140
Running loss of epoch-36 batch-140 = 1.866067759692669e-05

Training epoch-36 batch-141
Running loss of epoch-36 batch-141 = 9.087263606488705e-05

Training epoch-36 batch-142
Running loss of epoch-36 batch-142 = 2.817763015627861e-05

Training epoch-36 batch-143
Running loss of epoch-36 batch-143 = 3.791879862546921e-05

Training epoch-36 batch-144
Running loss of epoch-36 batch-144 = 4.050671122968197e-05

Training epoch-36 batch-145
Running loss of epoch-36 batch-145 = 3.822077997028828e-05

Training epoch-36 batch-146
Running loss of epoch-36 batch-146 = 0.00018720398657023907

Training epoch-36 batch-147
Running loss of epoch-36 batch-147 = 6.290478631854057e-05

Training epoch-36 batch-148
Running loss of epoch-36 batch-148 = 0.00032379012554883957

Training epoch-36 batch-149
Running loss of epoch-36 batch-149 = 1.4247139915823936e-05

Training epoch-36 batch-150
Running loss of epoch-36 batch-150 = 7.762736640870571e-05

Training epoch-36 batch-151
Running loss of epoch-36 batch-151 = 0.0001431116834282875

Training epoch-36 batch-152
Running loss of epoch-36 batch-152 = 6.437837146222591e-05

Training epoch-36 batch-153
Running loss of epoch-36 batch-153 = 0.00010825227946043015

Training epoch-36 batch-154
Running loss of epoch-36 batch-154 = 4.996894858777523e-05

Training epoch-36 batch-155
Running loss of epoch-36 batch-155 = 9.452109225094318e-05

Training epoch-36 batch-156
Running loss of epoch-36 batch-156 = 6.697745993733406e-05

Training epoch-36 batch-157
Running loss of epoch-36 batch-157 = 0.00017154216766357422

Finished training epoch-36.



Average train loss at epoch-36 = 5.701373964548111e-05

Started Evaluation

Average val loss at epoch-36 = 1.399434930836354

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 95.90 %
Accuracy for class setUp is: 84.75 %
Accuracy for class onCreate is: 91.26 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.14 %
Accuracy for class execute is: 51.81 %
Accuracy for class get is: 65.90 %

Overall Accuracy = 81.37 %

Finished Evaluation



Started training epoch-37


Training epoch-37 batch-1
Running loss of epoch-37 batch-1 = 2.3806001991033554e-05

Training epoch-37 batch-2
Running loss of epoch-37 batch-2 = 2.2511230781674385e-05

Training epoch-37 batch-3
Running loss of epoch-37 batch-3 = 2.4145469069480896e-05

Training epoch-37 batch-4
Running loss of epoch-37 batch-4 = 0.0001489745918661356

Training epoch-37 batch-5
Running loss of epoch-37 batch-5 = 2.2236956283450127e-05

Training epoch-37 batch-6
Running loss of epoch-37 batch-6 = 3.928644582629204e-05

Training epoch-37 batch-7
Running loss of epoch-37 batch-7 = 0.0001320245210081339

Training epoch-37 batch-8
Running loss of epoch-37 batch-8 = 0.00011712685227394104

Training epoch-37 batch-9
Running loss of epoch-37 batch-9 = 3.90706118196249e-05

Training epoch-37 batch-10
Running loss of epoch-37 batch-10 = 0.00011349143460392952

Training epoch-37 batch-11
Running loss of epoch-37 batch-11 = 6.0006510466337204e-05

Training epoch-37 batch-12
Running loss of epoch-37 batch-12 = 4.1040824726223946e-05

Training epoch-37 batch-13
Running loss of epoch-37 batch-13 = 5.074986256659031e-05

Training epoch-37 batch-14
Running loss of epoch-37 batch-14 = 3.119069151580334e-05

Training epoch-37 batch-15
Running loss of epoch-37 batch-15 = 1.4110468327999115e-05

Training epoch-37 batch-16
Running loss of epoch-37 batch-16 = 5.3987372666597366e-05

Training epoch-37 batch-17
Running loss of epoch-37 batch-17 = 8.938240353018045e-05

Training epoch-37 batch-18
Running loss of epoch-37 batch-18 = 2.6736175641417503e-05

Training epoch-37 batch-19
Running loss of epoch-37 batch-19 = 5.075056105852127e-05

Training epoch-37 batch-20
Running loss of epoch-37 batch-20 = 4.588533192873001e-05

Training epoch-37 batch-21
Running loss of epoch-37 batch-21 = 3.460654988884926e-05

Training epoch-37 batch-22
Running loss of epoch-37 batch-22 = 2.3630447685718536e-05

Training epoch-37 batch-23
Running loss of epoch-37 batch-23 = 5.351868458092213e-05

Training epoch-37 batch-24
Running loss of epoch-37 batch-24 = 3.16305086016655e-05

Training epoch-37 batch-25
Running loss of epoch-37 batch-25 = 3.22957057505846e-05

Training epoch-37 batch-26
Running loss of epoch-37 batch-26 = 4.726252518594265e-05

Training epoch-37 batch-27
Running loss of epoch-37 batch-27 = 7.066922262310982e-05

Training epoch-37 batch-28
Running loss of epoch-37 batch-28 = 2.482777927070856e-05

Training epoch-37 batch-29
Running loss of epoch-37 batch-29 = 6.159162148833275e-05

Training epoch-37 batch-30
Running loss of epoch-37 batch-30 = 2.293335273861885e-05

Training epoch-37 batch-31
Running loss of epoch-37 batch-31 = 7.732072845101357e-05

Training epoch-37 batch-32
Running loss of epoch-37 batch-32 = 3.0082417652010918e-05

Training epoch-37 batch-33
Running loss of epoch-37 batch-33 = 3.698491491377354e-05

Training epoch-37 batch-34
Running loss of epoch-37 batch-34 = 7.430044934153557e-05

Training epoch-37 batch-35
Running loss of epoch-37 batch-35 = 2.2719847038388252e-05

Training epoch-37 batch-36
Running loss of epoch-37 batch-36 = 4.950398579239845e-05

Training epoch-37 batch-37
Running loss of epoch-37 batch-37 = 5.8178557083010674e-05

Training epoch-37 batch-38
Running loss of epoch-37 batch-38 = 2.569332718849182e-05

Training epoch-37 batch-39
Running loss of epoch-37 batch-39 = 3.551598638296127e-05

Training epoch-37 batch-40
Running loss of epoch-37 batch-40 = 3.838026896119118e-05

Training epoch-37 batch-41
Running loss of epoch-37 batch-41 = 1.592608168721199e-05

Training epoch-37 batch-42
Running loss of epoch-37 batch-42 = 2.9793940484523773e-05

Training epoch-37 batch-43
Running loss of epoch-37 batch-43 = 2.734665758907795e-05

Training epoch-37 batch-44
Running loss of epoch-37 batch-44 = 3.6952667869627476e-05

Training epoch-37 batch-45
Running loss of epoch-37 batch-45 = 2.3977132514119148e-05

Training epoch-37 batch-46
Running loss of epoch-37 batch-46 = 2.4653272703289986e-05

Training epoch-37 batch-47
Running loss of epoch-37 batch-47 = 9.077508002519608e-05

Training epoch-37 batch-48
Running loss of epoch-37 batch-48 = 3.569317050278187e-05

Training epoch-37 batch-49
Running loss of epoch-37 batch-49 = 2.397899515926838e-05

Training epoch-37 batch-50
Running loss of epoch-37 batch-50 = 0.00011515384539961815

Training epoch-37 batch-51
Running loss of epoch-37 batch-51 = 1.7428072169423103e-05

Training epoch-37 batch-52
Running loss of epoch-37 batch-52 = 3.131246194243431e-05

Training epoch-37 batch-53
Running loss of epoch-37 batch-53 = 2.3722881451249123e-05

Training epoch-37 batch-54
Running loss of epoch-37 batch-54 = 4.7445064410567284e-05

Training epoch-37 batch-55
Running loss of epoch-37 batch-55 = 0.00011814618483185768

Training epoch-37 batch-56
Running loss of epoch-37 batch-56 = 3.193202428519726e-05

Training epoch-37 batch-57
Running loss of epoch-37 batch-57 = 9.255390614271164e-05

Training epoch-37 batch-58
Running loss of epoch-37 batch-58 = 4.868954420089722e-05

Training epoch-37 batch-59
Running loss of epoch-37 batch-59 = 3.2885000109672546e-05

Training epoch-37 batch-60
Running loss of epoch-37 batch-60 = 4.925346001982689e-05

Training epoch-37 batch-61
Running loss of epoch-37 batch-61 = 2.214335836470127e-05

Training epoch-37 batch-62
Running loss of epoch-37 batch-62 = 0.00015531957615166903

Training epoch-37 batch-63
Running loss of epoch-37 batch-63 = 5.328422412276268e-05

Training epoch-37 batch-64
Running loss of epoch-37 batch-64 = 2.8788810595870018e-05

Training epoch-37 batch-65
Running loss of epoch-37 batch-65 = 4.950910806655884e-06

Training epoch-37 batch-66
Running loss of epoch-37 batch-66 = 1.0057352483272552e-05

Training epoch-37 batch-67
Running loss of epoch-37 batch-67 = 2.8172507882118225e-05

Training epoch-37 batch-68
Running loss of epoch-37 batch-68 = 3.885198384523392e-05

Training epoch-37 batch-69
Running loss of epoch-37 batch-69 = 4.796241410076618e-05

Training epoch-37 batch-70
Running loss of epoch-37 batch-70 = 5.171564407646656e-05

Training epoch-37 batch-71
Running loss of epoch-37 batch-71 = 2.3328466340899467e-05

Training epoch-37 batch-72
Running loss of epoch-37 batch-72 = 1.9465340301394463e-05

Training epoch-37 batch-73
Running loss of epoch-37 batch-73 = 3.578094765543938e-05

Training epoch-37 batch-74
Running loss of epoch-37 batch-74 = 3.1009549275040627e-05

Training epoch-37 batch-75
Running loss of epoch-37 batch-75 = 4.261708818376064e-05

Training epoch-37 batch-76
Running loss of epoch-37 batch-76 = 0.0001667562173679471

Training epoch-37 batch-77
Running loss of epoch-37 batch-77 = 2.503138966858387e-05

Training epoch-37 batch-78
Running loss of epoch-37 batch-78 = 3.5653822124004364e-05

Training epoch-37 batch-79
Running loss of epoch-37 batch-79 = 6.331410259008408e-05

Training epoch-37 batch-80
Running loss of epoch-37 batch-80 = 7.931329309940338e-05

Training epoch-37 batch-81
Running loss of epoch-37 batch-81 = 1.3744691386818886e-05

Training epoch-37 batch-82
Running loss of epoch-37 batch-82 = 5.782954394817352e-05

Training epoch-37 batch-83
Running loss of epoch-37 batch-83 = 2.9457267373800278e-05

Training epoch-37 batch-84
Running loss of epoch-37 batch-84 = 2.7931993827223778e-05

Training epoch-37 batch-85
Running loss of epoch-37 batch-85 = 2.6840483769774437e-05

Training epoch-37 batch-86
Running loss of epoch-37 batch-86 = 3.7237536162137985e-05

Training epoch-37 batch-87
Running loss of epoch-37 batch-87 = 4.2283907532691956e-05

Training epoch-37 batch-88
Running loss of epoch-37 batch-88 = 2.201693132519722e-05

Training epoch-37 batch-89
Running loss of epoch-37 batch-89 = 4.028412513434887e-05

Training epoch-37 batch-90
Running loss of epoch-37 batch-90 = 4.610558971762657e-05

Training epoch-37 batch-91
Running loss of epoch-37 batch-91 = 0.00022373581305146217

Training epoch-37 batch-92
Running loss of epoch-37 batch-92 = 4.455354064702988e-05

Training epoch-37 batch-93
Running loss of epoch-37 batch-93 = 3.7658726796507835e-05

Training epoch-37 batch-94
Running loss of epoch-37 batch-94 = 2.5072135031223297e-05

Training epoch-37 batch-95
Running loss of epoch-37 batch-95 = 1.4689052477478981e-05

Training epoch-37 batch-96
Running loss of epoch-37 batch-96 = 5.076848901808262e-05

Training epoch-37 batch-97
Running loss of epoch-37 batch-97 = 0.00011133053340017796

Training epoch-37 batch-98
Running loss of epoch-37 batch-98 = 1.20487529784441e-05

Training epoch-37 batch-99
Running loss of epoch-37 batch-99 = 1.5786150470376015e-05

Training epoch-37 batch-100
Running loss of epoch-37 batch-100 = 3.4181633964180946e-05

Training epoch-37 batch-101
Running loss of epoch-37 batch-101 = 6.193364970386028e-05

Training epoch-37 batch-102
Running loss of epoch-37 batch-102 = 7.849093526601791e-05

Training epoch-37 batch-103
Running loss of epoch-37 batch-103 = 2.772151492536068e-05

Training epoch-37 batch-104
Running loss of epoch-37 batch-104 = 4.1044317185878754e-05

Training epoch-37 batch-105
Running loss of epoch-37 batch-105 = 4.508323036134243e-05

Training epoch-37 batch-106
Running loss of epoch-37 batch-106 = 2.6644906029105186e-05

Training epoch-37 batch-107
Running loss of epoch-37 batch-107 = 3.408454358577728e-05

Training epoch-37 batch-108
Running loss of epoch-37 batch-108 = 0.00031087081879377365

Training epoch-37 batch-109
Running loss of epoch-37 batch-109 = 1.2314645573496819e-05

Training epoch-37 batch-110
Running loss of epoch-37 batch-110 = 4.749721847474575e-05

Training epoch-37 batch-111
Running loss of epoch-37 batch-111 = 4.600314423441887e-05

Training epoch-37 batch-112
Running loss of epoch-37 batch-112 = 1.4943303540349007e-05

Training epoch-37 batch-113
Running loss of epoch-37 batch-113 = 3.253854811191559e-05

Training epoch-37 batch-114
Running loss of epoch-37 batch-114 = 8.619716390967369e-05

Training epoch-37 batch-115
Running loss of epoch-37 batch-115 = 1.9880244508385658e-05

Training epoch-37 batch-116
Running loss of epoch-37 batch-116 = 2.596527338027954e-05

Training epoch-37 batch-117
Running loss of epoch-37 batch-117 = 2.4461885914206505e-05

Training epoch-37 batch-118
Running loss of epoch-37 batch-118 = 1.4565419405698776e-05

Training epoch-37 batch-119
Running loss of epoch-37 batch-119 = 1.5277881175279617e-05

Training epoch-37 batch-120
Running loss of epoch-37 batch-120 = 3.2414449378848076e-05

Training epoch-37 batch-121
Running loss of epoch-37 batch-121 = 4.4474611058831215e-05

Training epoch-37 batch-122
Running loss of epoch-37 batch-122 = 1.7931219190359116e-05

Training epoch-37 batch-123
Running loss of epoch-37 batch-123 = 3.477523569017649e-05

Training epoch-37 batch-124
Running loss of epoch-37 batch-124 = 5.4926611483097076e-05

Training epoch-37 batch-125
Running loss of epoch-37 batch-125 = 2.2282591089606285e-05

Training epoch-37 batch-126
Running loss of epoch-37 batch-126 = 3.576255403459072e-05

Training epoch-37 batch-127
Running loss of epoch-37 batch-127 = 2.1955696865916252e-05

Training epoch-37 batch-128
Running loss of epoch-37 batch-128 = 6.324844434857368e-05

Training epoch-37 batch-129
Running loss of epoch-37 batch-129 = 2.5940826162695885e-05

Training epoch-37 batch-130
Running loss of epoch-37 batch-130 = 2.447119913995266e-05

Training epoch-37 batch-131
Running loss of epoch-37 batch-131 = 3.7958379834890366e-05

Training epoch-37 batch-132
Running loss of epoch-37 batch-132 = 9.620934724807739e-05

Training epoch-37 batch-133
Running loss of epoch-37 batch-133 = 3.329198807477951e-05

Training epoch-37 batch-134
Running loss of epoch-37 batch-134 = 2.64712143689394e-05

Training epoch-37 batch-135
Running loss of epoch-37 batch-135 = 1.3599870726466179e-05

Training epoch-37 batch-136
Running loss of epoch-37 batch-136 = 6.87246210873127e-06

Training epoch-37 batch-137
Running loss of epoch-37 batch-137 = 3.712042234838009e-05

Training epoch-37 batch-138
Running loss of epoch-37 batch-138 = 3.6778394132852554e-05

Training epoch-37 batch-139
Running loss of epoch-37 batch-139 = 5.969032645225525e-05

Training epoch-37 batch-140
Running loss of epoch-37 batch-140 = 5.233660340309143e-05

Training epoch-37 batch-141
Running loss of epoch-37 batch-141 = 0.00010919873602688313

Training epoch-37 batch-142
Running loss of epoch-37 batch-142 = 4.042335785925388e-05

Training epoch-37 batch-143
Running loss of epoch-37 batch-143 = 8.123554289340973e-05

Training epoch-37 batch-144
Running loss of epoch-37 batch-144 = 1.61763746291399e-05

Training epoch-37 batch-145
Running loss of epoch-37 batch-145 = 0.00017032423056662083

Training epoch-37 batch-146
Running loss of epoch-37 batch-146 = 8.19165725260973e-05

Training epoch-37 batch-147
Running loss of epoch-37 batch-147 = 4.5593129470944405e-05

Training epoch-37 batch-148
Running loss of epoch-37 batch-148 = 1.957477070391178e-05

Training epoch-37 batch-149
Running loss of epoch-37 batch-149 = 6.332783959805965e-05

Training epoch-37 batch-150
Running loss of epoch-37 batch-150 = 2.0993873476982117e-05

Training epoch-37 batch-151
Running loss of epoch-37 batch-151 = 6.569200195372105e-05

Training epoch-37 batch-152
Running loss of epoch-37 batch-152 = 4.6724919229745865e-05

Training epoch-37 batch-153
Running loss of epoch-37 batch-153 = 0.0001320124138146639

Training epoch-37 batch-154
Running loss of epoch-37 batch-154 = 4.007783718407154e-05

Training epoch-37 batch-155
Running loss of epoch-37 batch-155 = 1.2836884707212448e-05

Training epoch-37 batch-156
Running loss of epoch-37 batch-156 = 0.00010990328155457973

Training epoch-37 batch-157
Running loss of epoch-37 batch-157 = 0.00010486319661140442

Finished training epoch-37.



Average train loss at epoch-37 = 4.876232445240021e-05

Started Evaluation

Average val loss at epoch-37 = 1.43687905466126

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 95.90 %
Accuracy for class setUp is: 89.67 %
Accuracy for class onCreate is: 90.41 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 64.16 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 49.55 %
Accuracy for class execute is: 50.20 %
Accuracy for class get is: 65.38 %

Overall Accuracy = 81.51 %

Finished Evaluation



Started training epoch-38


Training epoch-38 batch-1
Running loss of epoch-38 batch-1 = 3.95369715988636e-05

Training epoch-38 batch-2
Running loss of epoch-38 batch-2 = 2.5906367227435112e-05

Training epoch-38 batch-3
Running loss of epoch-38 batch-3 = 3.631715662777424e-05

Training epoch-38 batch-4
Running loss of epoch-38 batch-4 = 4.5386143028736115e-05

Training epoch-38 batch-5
Running loss of epoch-38 batch-5 = 9.145354852080345e-06

Training epoch-38 batch-6
Running loss of epoch-38 batch-6 = 3.776606172323227e-05

Training epoch-38 batch-7
Running loss of epoch-38 batch-7 = 6.013084203004837e-05

Training epoch-38 batch-8
Running loss of epoch-38 batch-8 = 5.24371862411499e-05

Training epoch-38 batch-9
Running loss of epoch-38 batch-9 = 3.608153201639652e-05

Training epoch-38 batch-10
Running loss of epoch-38 batch-10 = 0.0003065726486966014

Training epoch-38 batch-11
Running loss of epoch-38 batch-11 = 2.5301706045866013e-05

Training epoch-38 batch-12
Running loss of epoch-38 batch-12 = 1.4373334124684334e-05

Training epoch-38 batch-13
Running loss of epoch-38 batch-13 = 2.2707507014274597e-05

Training epoch-38 batch-14
Running loss of epoch-38 batch-14 = 2.362695522606373e-05

Training epoch-38 batch-15
Running loss of epoch-38 batch-15 = 9.233388118445873e-05

Training epoch-38 batch-16
Running loss of epoch-38 batch-16 = 3.1309667974710464e-05

Training epoch-38 batch-17
Running loss of epoch-38 batch-17 = 2.342858351767063e-05

Training epoch-38 batch-18
Running loss of epoch-38 batch-18 = 1.153675839304924e-05

Training epoch-38 batch-19
Running loss of epoch-38 batch-19 = 3.8088997825980186e-05

Training epoch-38 batch-20
Running loss of epoch-38 batch-20 = 2.53280159085989e-05

Training epoch-38 batch-21
Running loss of epoch-38 batch-21 = 3.2145995646715164e-05

Training epoch-38 batch-22
Running loss of epoch-38 batch-22 = 6.335577927529812e-05

Training epoch-38 batch-23
Running loss of epoch-38 batch-23 = 2.0785722881555557e-05

Training epoch-38 batch-24
Running loss of epoch-38 batch-24 = 9.75453294813633e-05

Training epoch-38 batch-25
Running loss of epoch-38 batch-25 = 2.9302434995770454e-05

Training epoch-38 batch-26
Running loss of epoch-38 batch-26 = 2.6952475309371948e-05

Training epoch-38 batch-27
Running loss of epoch-38 batch-27 = 2.8070062398910522e-05

Training epoch-38 batch-28
Running loss of epoch-38 batch-28 = 3.620539791882038e-05

Training epoch-38 batch-29
Running loss of epoch-38 batch-29 = 4.2557017877697945e-05

Training epoch-38 batch-30
Running loss of epoch-38 batch-30 = 6.178254261612892e-05

Training epoch-38 batch-31
Running loss of epoch-38 batch-31 = 3.263982944190502e-05

Training epoch-38 batch-32
Running loss of epoch-38 batch-32 = 2.6157591491937637e-05

Training epoch-38 batch-33
Running loss of epoch-38 batch-33 = 3.531412221491337e-05

Training epoch-38 batch-34
Running loss of epoch-38 batch-34 = 6.0949474573135376e-05

Training epoch-38 batch-35
Running loss of epoch-38 batch-35 = 5.01326285302639e-05

Training epoch-38 batch-36
Running loss of epoch-38 batch-36 = 3.9327191188931465e-05

Training epoch-38 batch-37
Running loss of epoch-38 batch-37 = 2.9110582545399666e-05

Training epoch-38 batch-38
Running loss of epoch-38 batch-38 = 2.3766420781612396e-05

Training epoch-38 batch-39
Running loss of epoch-38 batch-39 = 1.7400365322828293e-05

Training epoch-38 batch-40
Running loss of epoch-38 batch-40 = 3.8405414670705795e-05

Training epoch-38 batch-41
Running loss of epoch-38 batch-41 = 2.3330794647336006e-05

Training epoch-38 batch-42
Running loss of epoch-38 batch-42 = 3.367871977388859e-05

Training epoch-38 batch-43
Running loss of epoch-38 batch-43 = 4.375120624899864e-05

Training epoch-38 batch-44
Running loss of epoch-38 batch-44 = 3.646686673164368e-05

Training epoch-38 batch-45
Running loss of epoch-38 batch-45 = 4.694168455898762e-05

Training epoch-38 batch-46
Running loss of epoch-38 batch-46 = 4.3411506339907646e-05

Training epoch-38 batch-47
Running loss of epoch-38 batch-47 = 9.67362429946661e-05

Training epoch-38 batch-48
Running loss of epoch-38 batch-48 = 6.170826964080334e-05

Training epoch-38 batch-49
Running loss of epoch-38 batch-49 = 9.028776548802853e-05

Training epoch-38 batch-50
Running loss of epoch-38 batch-50 = 1.2904871255159378e-05

Training epoch-38 batch-51
Running loss of epoch-38 batch-51 = 2.9469840228557587e-05

Training epoch-38 batch-52
Running loss of epoch-38 batch-52 = 3.222259692847729e-05

Training epoch-38 batch-53
Running loss of epoch-38 batch-53 = 1.4178454875946045e-05

Training epoch-38 batch-54
Running loss of epoch-38 batch-54 = 1.8759863451123238e-05

Training epoch-38 batch-55
Running loss of epoch-38 batch-55 = 3.062584437429905e-05

Training epoch-38 batch-56
Running loss of epoch-38 batch-56 = 4.127435386180878e-05

Training epoch-38 batch-57
Running loss of epoch-38 batch-57 = 3.1203730031847954e-05

Training epoch-38 batch-58
Running loss of epoch-38 batch-58 = 1.4931662008166313e-05

Training epoch-38 batch-59
Running loss of epoch-38 batch-59 = 0.00015977281145751476

Training epoch-38 batch-60
Running loss of epoch-38 batch-60 = 9.427848272025585e-05

Training epoch-38 batch-61
Running loss of epoch-38 batch-61 = 3.788340836763382e-05

Training epoch-38 batch-62
Running loss of epoch-38 batch-62 = 2.7667032554745674e-05

Training epoch-38 batch-63
Running loss of epoch-38 batch-63 = 3.4254277125000954e-05

Training epoch-38 batch-64
Running loss of epoch-38 batch-64 = 9.35897696763277e-05

Training epoch-38 batch-65
Running loss of epoch-38 batch-65 = 9.531201794743538e-05

Training epoch-38 batch-66
Running loss of epoch-38 batch-66 = 6.051105447113514e-05

Training epoch-38 batch-67
Running loss of epoch-38 batch-67 = 4.584062844514847e-05

Training epoch-38 batch-68
Running loss of epoch-38 batch-68 = 3.249431028962135e-05

Training epoch-38 batch-69
Running loss of epoch-38 batch-69 = 3.666244447231293e-05

Training epoch-38 batch-70
Running loss of epoch-38 batch-70 = 8.911360055208206e-06

Training epoch-38 batch-71
Running loss of epoch-38 batch-71 = 2.1745683625340462e-05

Training epoch-38 batch-72
Running loss of epoch-38 batch-72 = 2.214079722762108e-05

Training epoch-38 batch-73
Running loss of epoch-38 batch-73 = 8.75513069331646e-06

Training epoch-38 batch-74
Running loss of epoch-38 batch-74 = 3.546965308487415e-05

Training epoch-38 batch-75
Running loss of epoch-38 batch-75 = 1.884554512798786e-05

Training epoch-38 batch-76
Running loss of epoch-38 batch-76 = 5.69645781069994e-05

Training epoch-38 batch-77
Running loss of epoch-38 batch-77 = 1.7618760466575623e-05

Training epoch-38 batch-78
Running loss of epoch-38 batch-78 = 0.0001254039816558361

Training epoch-38 batch-79
Running loss of epoch-38 batch-79 = 7.13719055056572e-06

Training epoch-38 batch-80
Running loss of epoch-38 batch-80 = 2.7988338842988014e-05

Training epoch-38 batch-81
Running loss of epoch-38 batch-81 = 5.5160606279969215e-05

Training epoch-38 batch-82
Running loss of epoch-38 batch-82 = 3.6396319046616554e-05

Training epoch-38 batch-83
Running loss of epoch-38 batch-83 = 1.8702121451497078e-05

Training epoch-38 batch-84
Running loss of epoch-38 batch-84 = 0.00012157834134995937

Training epoch-38 batch-85
Running loss of epoch-38 batch-85 = 5.8882636949419975e-05

Training epoch-38 batch-86
Running loss of epoch-38 batch-86 = 3.4668948501348495e-05

Training epoch-38 batch-87
Running loss of epoch-38 batch-87 = 2.0402483642101288e-05

Training epoch-38 batch-88
Running loss of epoch-38 batch-88 = 4.783668555319309e-05

Training epoch-38 batch-89
Running loss of epoch-38 batch-89 = 0.0001476667821407318

Training epoch-38 batch-90
Running loss of epoch-38 batch-90 = 3.332924097776413e-05

Training epoch-38 batch-91
Running loss of epoch-38 batch-91 = 2.6255613192915916e-05

Training epoch-38 batch-92
Running loss of epoch-38 batch-92 = 4.799594171345234e-05

Training epoch-38 batch-93
Running loss of epoch-38 batch-93 = 3.688805736601353e-05

Training epoch-38 batch-94
Running loss of epoch-38 batch-94 = 1.6323290765285492e-05

Training epoch-38 batch-95
Running loss of epoch-38 batch-95 = 7.993984036147594e-05

Training epoch-38 batch-96
Running loss of epoch-38 batch-96 = 0.0001861988566815853

Training epoch-38 batch-97
Running loss of epoch-38 batch-97 = 3.9510661736130714e-05

Training epoch-38 batch-98
Running loss of epoch-38 batch-98 = 4.134932532906532e-05

Training epoch-38 batch-99
Running loss of epoch-38 batch-99 = 2.1459301933646202e-05

Training epoch-38 batch-100
Running loss of epoch-38 batch-100 = 1.3939104974269867e-05

Training epoch-38 batch-101
Running loss of epoch-38 batch-101 = 2.767634578049183e-05

Training epoch-38 batch-102
Running loss of epoch-38 batch-102 = 6.269523873925209e-05

Training epoch-38 batch-103
Running loss of epoch-38 batch-103 = 6.551435217261314e-05

Training epoch-38 batch-104
Running loss of epoch-38 batch-104 = 2.31890007853508e-05

Training epoch-38 batch-105
Running loss of epoch-38 batch-105 = 3.190035931766033e-05

Training epoch-38 batch-106
Running loss of epoch-38 batch-106 = 4.411477129906416e-05

Training epoch-38 batch-107
Running loss of epoch-38 batch-107 = 3.2556476071476936e-05

Training epoch-38 batch-108
Running loss of epoch-38 batch-108 = 3.924127668142319e-05

Training epoch-38 batch-109
Running loss of epoch-38 batch-109 = 1.9123079255223274e-05

Training epoch-38 batch-110
Running loss of epoch-38 batch-110 = 6.402283906936646e-05

Training epoch-38 batch-111
Running loss of epoch-38 batch-111 = 7.690978236496449e-05

Training epoch-38 batch-112
Running loss of epoch-38 batch-112 = 4.236423410475254e-05

Training epoch-38 batch-113
Running loss of epoch-38 batch-113 = 7.245689630508423e-06

Training epoch-38 batch-114
Running loss of epoch-38 batch-114 = 7.548951543867588e-05

Training epoch-38 batch-115
Running loss of epoch-38 batch-115 = 1.8547987565398216e-05

Training epoch-38 batch-116
Running loss of epoch-38 batch-116 = 4.5747729018330574e-05

Training epoch-38 batch-117
Running loss of epoch-38 batch-117 = 6.062909960746765e-05

Training epoch-38 batch-118
Running loss of epoch-38 batch-118 = 2.4182721972465515e-05

Training epoch-38 batch-119
Running loss of epoch-38 batch-119 = 5.583791062235832e-05

Training epoch-38 batch-120
Running loss of epoch-38 batch-120 = 3.529689274728298e-05

Training epoch-38 batch-121
Running loss of epoch-38 batch-121 = 1.7389655113220215e-05

Training epoch-38 batch-122
Running loss of epoch-38 batch-122 = 4.426692612469196e-05

Training epoch-38 batch-123
Running loss of epoch-38 batch-123 = 2.1525193005800247e-05

Training epoch-38 batch-124
Running loss of epoch-38 batch-124 = 3.954698331654072e-05

Training epoch-38 batch-125
Running loss of epoch-38 batch-125 = 2.288748510181904e-05

Training epoch-38 batch-126
Running loss of epoch-38 batch-126 = 5.474803037941456e-05

Training epoch-38 batch-127
Running loss of epoch-38 batch-127 = 1.4341436326503754e-05

Training epoch-38 batch-128
Running loss of epoch-38 batch-128 = 1.3474375009536743e-05

Training epoch-38 batch-129
Running loss of epoch-38 batch-129 = 1.4761462807655334e-05

Training epoch-38 batch-130
Running loss of epoch-38 batch-130 = 7.329997606575489e-05

Training epoch-38 batch-131
Running loss of epoch-38 batch-131 = 2.0355451852083206e-05

Training epoch-38 batch-132
Running loss of epoch-38 batch-132 = 3.842241130769253e-05

Training epoch-38 batch-133
Running loss of epoch-38 batch-133 = 0.00019554258324205875

Training epoch-38 batch-134
Running loss of epoch-38 batch-134 = 1.4991266652941704e-05

Training epoch-38 batch-135
Running loss of epoch-38 batch-135 = 7.628649473190308e-05

Training epoch-38 batch-136
Running loss of epoch-38 batch-136 = 3.971625119447708e-05

Training epoch-38 batch-137
Running loss of epoch-38 batch-137 = 3.010360524058342e-05

Training epoch-38 batch-138
Running loss of epoch-38 batch-138 = 2.0100269466638565e-05

Training epoch-38 batch-139
Running loss of epoch-38 batch-139 = 3.452179953455925e-05

Training epoch-38 batch-140
Running loss of epoch-38 batch-140 = 2.526422031223774e-05

Training epoch-38 batch-141
Running loss of epoch-38 batch-141 = 2.2498425096273422e-05

Training epoch-38 batch-142
Running loss of epoch-38 batch-142 = 3.429455682635307e-05

Training epoch-38 batch-143
Running loss of epoch-38 batch-143 = 1.1120224371552467e-05

Training epoch-38 batch-144
Running loss of epoch-38 batch-144 = 1.3380544260144234e-05

Training epoch-38 batch-145
Running loss of epoch-38 batch-145 = 0.00036377389915287495

Training epoch-38 batch-146
Running loss of epoch-38 batch-146 = 6.590178236365318e-05

Training epoch-38 batch-147
Running loss of epoch-38 batch-147 = 1.4353776350617409e-05

Training epoch-38 batch-148
Running loss of epoch-38 batch-148 = 2.9482180252671242e-05

Training epoch-38 batch-149
Running loss of epoch-38 batch-149 = 8.266174700111151e-05

Training epoch-38 batch-150
Running loss of epoch-38 batch-150 = 1.6552628949284554e-05

Training epoch-38 batch-151
Running loss of epoch-38 batch-151 = 1.2009404599666595e-06

Training epoch-38 batch-152
Running loss of epoch-38 batch-152 = 2.863304689526558e-05

Training epoch-38 batch-153
Running loss of epoch-38 batch-153 = 9.202538058161736e-05

Training epoch-38 batch-154
Running loss of epoch-38 batch-154 = 2.64591071754694e-05

Training epoch-38 batch-155
Running loss of epoch-38 batch-155 = 0.0002572769299149513

Training epoch-38 batch-156
Running loss of epoch-38 batch-156 = 7.180706597864628e-05

Training epoch-38 batch-157
Running loss of epoch-38 batch-157 = 9.467266499996185e-05

Finished training epoch-38.



Average train loss at epoch-38 = 4.751205965876579e-05

Started Evaluation

Average val loss at epoch-38 = 1.4343519114676095

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 95.57 %
Accuracy for class setUp is: 88.03 %
Accuracy for class onCreate is: 90.51 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 64.84 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 47.79 %
Accuracy for class get is: 64.62 %

Overall Accuracy = 81.45 %

Finished Evaluation



Started training epoch-39


Training epoch-39 batch-1
Running loss of epoch-39 batch-1 = 2.7097994461655617e-05

Training epoch-39 batch-2
Running loss of epoch-39 batch-2 = 1.4907680451869965e-05

Training epoch-39 batch-3
Running loss of epoch-39 batch-3 = 2.3610424250364304e-05

Training epoch-39 batch-4
Running loss of epoch-39 batch-4 = 7.118796929717064e-05

Training epoch-39 batch-5
Running loss of epoch-39 batch-5 = 1.1338619515299797e-05

Training epoch-39 batch-6
Running loss of epoch-39 batch-6 = 3.697327338159084e-05

Training epoch-39 batch-7
Running loss of epoch-39 batch-7 = 9.481958113610744e-05

Training epoch-39 batch-8
Running loss of epoch-39 batch-8 = 2.7159228920936584e-05

Training epoch-39 batch-9
Running loss of epoch-39 batch-9 = 0.00022682640701532364

Training epoch-39 batch-10
Running loss of epoch-39 batch-10 = 3.437907434999943e-05

Training epoch-39 batch-11
Running loss of epoch-39 batch-11 = 2.589426003396511e-05

Training epoch-39 batch-12
Running loss of epoch-39 batch-12 = 3.103516064584255e-05

Training epoch-39 batch-13
Running loss of epoch-39 batch-13 = 5.442928522825241e-05

Training epoch-39 batch-14
Running loss of epoch-39 batch-14 = 0.00011924421414732933

Training epoch-39 batch-15
Running loss of epoch-39 batch-15 = 4.920130595564842e-05

Training epoch-39 batch-16
Running loss of epoch-39 batch-16 = 1.9643688574433327e-05

Training epoch-39 batch-17
Running loss of epoch-39 batch-17 = 8.106790482997894e-05

Training epoch-39 batch-18
Running loss of epoch-39 batch-18 = 1.582736149430275e-05

Training epoch-39 batch-19
Running loss of epoch-39 batch-19 = 4.356773570179939e-05

Training epoch-39 batch-20
Running loss of epoch-39 batch-20 = 1.771841198205948e-05

Training epoch-39 batch-21
Running loss of epoch-39 batch-21 = 2.4080276489257812e-05

Training epoch-39 batch-22
Running loss of epoch-39 batch-22 = 5.354057066142559e-05

Training epoch-39 batch-23
Running loss of epoch-39 batch-23 = 1.1826865375041962e-05

Training epoch-39 batch-24
Running loss of epoch-39 batch-24 = 1.12215057015419e-05

Training epoch-39 batch-25
Running loss of epoch-39 batch-25 = 1.8005957826972008e-05

Training epoch-39 batch-26
Running loss of epoch-39 batch-26 = 0.00013425806537270546

Training epoch-39 batch-27
Running loss of epoch-39 batch-27 = 4.1171908378601074e-05

Training epoch-39 batch-28
Running loss of epoch-39 batch-28 = 2.3334985598921776e-05

Training epoch-39 batch-29
Running loss of epoch-39 batch-29 = 1.2614065781235695e-05

Training epoch-39 batch-30
Running loss of epoch-39 batch-30 = 2.9446091502904892e-05

Training epoch-39 batch-31
Running loss of epoch-39 batch-31 = 0.00014964072033762932

Training epoch-39 batch-32
Running loss of epoch-39 batch-32 = 3.513414412736893e-05

Training epoch-39 batch-33
Running loss of epoch-39 batch-33 = 5.4956646636128426e-05

Training epoch-39 batch-34
Running loss of epoch-39 batch-34 = 1.856079325079918e-05

Training epoch-39 batch-35
Running loss of epoch-39 batch-35 = 1.9232742488384247e-05

Training epoch-39 batch-36
Running loss of epoch-39 batch-36 = 2.8299866244196892e-05

Training epoch-39 batch-37
Running loss of epoch-39 batch-37 = 2.0772218704223633e-05

Training epoch-39 batch-38
Running loss of epoch-39 batch-38 = 3.98857519030571e-05

Training epoch-39 batch-39
Running loss of epoch-39 batch-39 = 2.4087028577923775e-05

Training epoch-39 batch-40
Running loss of epoch-39 batch-40 = 9.001884609460831e-05

Training epoch-39 batch-41
Running loss of epoch-39 batch-41 = 6.94887712597847e-05

Training epoch-39 batch-42
Running loss of epoch-39 batch-42 = 1.839175820350647e-05

Training epoch-39 batch-43
Running loss of epoch-39 batch-43 = 2.2027641534805298e-05

Training epoch-39 batch-44
Running loss of epoch-39 batch-44 = 3.228732384741306e-05

Training epoch-39 batch-45
Running loss of epoch-39 batch-45 = 1.765950582921505e-05

Training epoch-39 batch-46
Running loss of epoch-39 batch-46 = 2.6230933144688606e-05

Training epoch-39 batch-47
Running loss of epoch-39 batch-47 = 6.885966286063194e-06

Training epoch-39 batch-48
Running loss of epoch-39 batch-48 = 1.1818250641226768e-05

Training epoch-39 batch-49
Running loss of epoch-39 batch-49 = 0.0001270519569516182

Training epoch-39 batch-50
Running loss of epoch-39 batch-50 = 0.0002288490068167448

Training epoch-39 batch-51
Running loss of epoch-39 batch-51 = 3.065657801926136e-05

Training epoch-39 batch-52
Running loss of epoch-39 batch-52 = 3.436650149524212e-05

Training epoch-39 batch-53
Running loss of epoch-39 batch-53 = 4.809396341443062e-05

Training epoch-39 batch-54
Running loss of epoch-39 batch-54 = 1.2540025636553764e-05

Training epoch-39 batch-55
Running loss of epoch-39 batch-55 = 6.381049752235413e-05

Training epoch-39 batch-56
Running loss of epoch-39 batch-56 = 6.190245039761066e-05

Training epoch-39 batch-57
Running loss of epoch-39 batch-57 = 7.806462235748768e-05

Training epoch-39 batch-58
Running loss of epoch-39 batch-58 = 3.7826597690582275e-05

Training epoch-39 batch-59
Running loss of epoch-39 batch-59 = 8.69993818923831e-05

Training epoch-39 batch-60
Running loss of epoch-39 batch-60 = 5.6715914979577065e-05

Training epoch-39 batch-61
Running loss of epoch-39 batch-61 = 2.4753157049417496e-05

Training epoch-39 batch-62
Running loss of epoch-39 batch-62 = 4.527624696493149e-06

Training epoch-39 batch-63
Running loss of epoch-39 batch-63 = 2.5881920009851456e-05

Training epoch-39 batch-64
Running loss of epoch-39 batch-64 = 3.4667085856199265e-05

Training epoch-39 batch-65
Running loss of epoch-39 batch-65 = 3.320886753499508e-05

Training epoch-39 batch-66
Running loss of epoch-39 batch-66 = 2.4822307750582695e-05

Training epoch-39 batch-67
Running loss of epoch-39 batch-67 = 3.226730041205883e-05

Training epoch-39 batch-68
Running loss of epoch-39 batch-68 = 3.549107350409031e-05

Training epoch-39 batch-69
Running loss of epoch-39 batch-69 = 2.6497524231672287e-05

Training epoch-39 batch-70
Running loss of epoch-39 batch-70 = 2.1447427570819855e-05

Training epoch-39 batch-71
Running loss of epoch-39 batch-71 = 2.9517803341150284e-05

Training epoch-39 batch-72
Running loss of epoch-39 batch-72 = 0.00012837909162044525

Training epoch-39 batch-73
Running loss of epoch-39 batch-73 = 1.4594756066799164e-05

Training epoch-39 batch-74
Running loss of epoch-39 batch-74 = 5.407235585153103e-05

Training epoch-39 batch-75
Running loss of epoch-39 batch-75 = 2.0675593987107277e-05

Training epoch-39 batch-76
Running loss of epoch-39 batch-76 = 0.00015449104830622673

Training epoch-39 batch-77
Running loss of epoch-39 batch-77 = 2.9469840228557587e-05

Training epoch-39 batch-78
Running loss of epoch-39 batch-78 = 2.13750172406435e-05

Training epoch-39 batch-79
Running loss of epoch-39 batch-79 = 1.2189382687211037e-05

Training epoch-39 batch-80
Running loss of epoch-39 batch-80 = 1.2094620615243912e-05

Training epoch-39 batch-81
Running loss of epoch-39 batch-81 = 3.391597419977188e-05

Training epoch-39 batch-82
Running loss of epoch-39 batch-82 = 6.870296783745289e-05

Training epoch-39 batch-83
Running loss of epoch-39 batch-83 = 1.326599158346653e-05

Training epoch-39 batch-84
Running loss of epoch-39 batch-84 = 1.8125632777810097e-05

Training epoch-39 batch-85
Running loss of epoch-39 batch-85 = 2.3659085854887962e-05

Training epoch-39 batch-86
Running loss of epoch-39 batch-86 = 1.6744248569011688e-05

Training epoch-39 batch-87
Running loss of epoch-39 batch-87 = 4.771072417497635e-05

Training epoch-39 batch-88
Running loss of epoch-39 batch-88 = 4.372745752334595e-05

Training epoch-39 batch-89
Running loss of epoch-39 batch-89 = 6.197881884872913e-05

Training epoch-39 batch-90
Running loss of epoch-39 batch-90 = 3.7757446989417076e-05

Training epoch-39 batch-91
Running loss of epoch-39 batch-91 = 1.0980525985360146e-05

Training epoch-39 batch-92
Running loss of epoch-39 batch-92 = 2.692360430955887e-05

Training epoch-39 batch-93
Running loss of epoch-39 batch-93 = 2.831057645380497e-05

Training epoch-39 batch-94
Running loss of epoch-39 batch-94 = 9.440677240490913e-05

Training epoch-39 batch-95
Running loss of epoch-39 batch-95 = 1.880386844277382e-05

Training epoch-39 batch-96
Running loss of epoch-39 batch-96 = 6.144191138446331e-05

Training epoch-39 batch-97
Running loss of epoch-39 batch-97 = 2.468237653374672e-05

Training epoch-39 batch-98
Running loss of epoch-39 batch-98 = 2.1340325474739075e-05

Training epoch-39 batch-99
Running loss of epoch-39 batch-99 = 4.025967791676521e-05

Training epoch-39 batch-100
Running loss of epoch-39 batch-100 = 1.9783154129981995e-05

Training epoch-39 batch-101
Running loss of epoch-39 batch-101 = 7.959781214594841e-06

Training epoch-39 batch-102
Running loss of epoch-39 batch-102 = 8.302438072860241e-05

Training epoch-39 batch-103
Running loss of epoch-39 batch-103 = 3.420352004468441e-05

Training epoch-39 batch-104
Running loss of epoch-39 batch-104 = 6.21541403234005e-05

Training epoch-39 batch-105
Running loss of epoch-39 batch-105 = 2.4116365239024162e-05

Training epoch-39 batch-106
Running loss of epoch-39 batch-106 = 1.772330142557621e-05

Training epoch-39 batch-107
Running loss of epoch-39 batch-107 = 1.2356555089354515e-05

Training epoch-39 batch-108
Running loss of epoch-39 batch-108 = 2.2751279175281525e-05

Training epoch-39 batch-109
Running loss of epoch-39 batch-109 = 0.00013357773423194885

Training epoch-39 batch-110
Running loss of epoch-39 batch-110 = 7.573748007416725e-05

Training epoch-39 batch-111
Running loss of epoch-39 batch-111 = 2.297293394804001e-05

Training epoch-39 batch-112
Running loss of epoch-39 batch-112 = 5.09356614202261e-05

Training epoch-39 batch-113
Running loss of epoch-39 batch-113 = 3.255321644246578e-05

Training epoch-39 batch-114
Running loss of epoch-39 batch-114 = 3.604963421821594e-05

Training epoch-39 batch-115
Running loss of epoch-39 batch-115 = 1.1207535862922668e-05

Training epoch-39 batch-116
Running loss of epoch-39 batch-116 = 2.2348249331116676e-05

Training epoch-39 batch-117
Running loss of epoch-39 batch-117 = 2.3672357201576233e-05

Training epoch-39 batch-118
Running loss of epoch-39 batch-118 = 1.2818491086363792e-05

Training epoch-39 batch-119
Running loss of epoch-39 batch-119 = 2.8859125450253487e-05

Training epoch-39 batch-120
Running loss of epoch-39 batch-120 = 1.905905082821846e-05

Training epoch-39 batch-121
Running loss of epoch-39 batch-121 = 1.926114782691002e-05

Training epoch-39 batch-122
Running loss of epoch-39 batch-122 = 4.559056833386421e-05

Training epoch-39 batch-123
Running loss of epoch-39 batch-123 = 1.545320264995098e-05

Training epoch-39 batch-124
Running loss of epoch-39 batch-124 = 8.752127178013325e-05

Training epoch-39 batch-125
Running loss of epoch-39 batch-125 = 5.8167846873402596e-05

Training epoch-39 batch-126
Running loss of epoch-39 batch-126 = 4.558824002742767e-05

Training epoch-39 batch-127
Running loss of epoch-39 batch-127 = 4.410557448863983e-05

Training epoch-39 batch-128
Running loss of epoch-39 batch-128 = 7.142545655369759e-05

Training epoch-39 batch-129
Running loss of epoch-39 batch-129 = 1.0088318958878517e-05

Training epoch-39 batch-130
Running loss of epoch-39 batch-130 = 2.8120353817939758e-05

Training epoch-39 batch-131
Running loss of epoch-39 batch-131 = 5.380762740969658e-05

Training epoch-39 batch-132
Running loss of epoch-39 batch-132 = 1.3968907296657562e-05

Training epoch-39 batch-133
Running loss of epoch-39 batch-133 = 6.622890941798687e-05

Training epoch-39 batch-134
Running loss of epoch-39 batch-134 = 6.424402818083763e-05

Training epoch-39 batch-135
Running loss of epoch-39 batch-135 = 0.00012053665705025196

Training epoch-39 batch-136
Running loss of epoch-39 batch-136 = 7.800525054335594e-06

Training epoch-39 batch-137
Running loss of epoch-39 batch-137 = 7.676356472074986e-05

Training epoch-39 batch-138
Running loss of epoch-39 batch-138 = 5.296152085065842e-05

Training epoch-39 batch-139
Running loss of epoch-39 batch-139 = 3.1957169994711876e-05

Training epoch-39 batch-140
Running loss of epoch-39 batch-140 = 3.1272415071725845e-05

Training epoch-39 batch-141
Running loss of epoch-39 batch-141 = 3.290316089987755e-05

Training epoch-39 batch-142
Running loss of epoch-39 batch-142 = 3.028823994100094e-05

Training epoch-39 batch-143
Running loss of epoch-39 batch-143 = 1.813424751162529e-05

Training epoch-39 batch-144
Running loss of epoch-39 batch-144 = 6.272760219871998e-05

Training epoch-39 batch-145
Running loss of epoch-39 batch-145 = 4.9697235226631165e-05

Training epoch-39 batch-146
Running loss of epoch-39 batch-146 = 5.504768341779709e-05

Training epoch-39 batch-147
Running loss of epoch-39 batch-147 = 5.547329783439636e-05

Training epoch-39 batch-148
Running loss of epoch-39 batch-148 = 1.1905794963240623e-05

Training epoch-39 batch-149
Running loss of epoch-39 batch-149 = 1.9404105842113495e-05

Training epoch-39 batch-150
Running loss of epoch-39 batch-150 = 1.705111935734749e-05

Training epoch-39 batch-151
Running loss of epoch-39 batch-151 = 6.635673344135284e-05

Training epoch-39 batch-152
Running loss of epoch-39 batch-152 = 2.3884465917944908e-05

Training epoch-39 batch-153
Running loss of epoch-39 batch-153 = 1.583574339747429e-05

Training epoch-39 batch-154
Running loss of epoch-39 batch-154 = 2.155965194106102e-05

Training epoch-39 batch-155
Running loss of epoch-39 batch-155 = 1.4039222151041031e-05

Training epoch-39 batch-156
Running loss of epoch-39 batch-156 = 0.00012263027019798756

Training epoch-39 batch-157
Running loss of epoch-39 batch-157 = 7.439777255058289e-05

Finished training epoch-39.



Average train loss at epoch-39 = 4.291272535920143e-05

Started Evaluation

Average val loss at epoch-39 = 1.4534531726277284

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 90.82 %
Accuracy for class onCreate is: 90.72 %
Accuracy for class toString is: 84.30 %
Accuracy for class run is: 63.01 %
Accuracy for class hashCode is: 98.50 %
Accuracy for class init is: 50.67 %
Accuracy for class execute is: 48.19 %
Accuracy for class get is: 65.38 %

Overall Accuracy = 81.70 %

Finished Evaluation



Started training epoch-40


Training epoch-40 batch-1
Running loss of epoch-40 batch-1 = 6.763031706213951e-06

Training epoch-40 batch-2
Running loss of epoch-40 batch-2 = 5.988636985421181e-06

Training epoch-40 batch-3
Running loss of epoch-40 batch-3 = 7.26897269487381e-06

Training epoch-40 batch-4
Running loss of epoch-40 batch-4 = 6.656628102064133e-05

Training epoch-40 batch-5
Running loss of epoch-40 batch-5 = 3.313738852739334e-05

Training epoch-40 batch-6
Running loss of epoch-40 batch-6 = 1.8333084881305695e-05

Training epoch-40 batch-7
Running loss of epoch-40 batch-7 = 4.538847133517265e-05

Training epoch-40 batch-8
Running loss of epoch-40 batch-8 = 1.2628501281142235e-05

Training epoch-40 batch-9
Running loss of epoch-40 batch-9 = 1.0990537703037262e-05

Training epoch-40 batch-10
Running loss of epoch-40 batch-10 = 4.3601496145129204e-05

Training epoch-40 batch-11
Running loss of epoch-40 batch-11 = 3.913906402885914e-05

Training epoch-40 batch-12
Running loss of epoch-40 batch-12 = 1.1217547580599785e-05

Training epoch-40 batch-13
Running loss of epoch-40 batch-13 = 3.343215212225914e-05

Training epoch-40 batch-14
Running loss of epoch-40 batch-14 = 6.920425221323967e-06

Training epoch-40 batch-15
Running loss of epoch-40 batch-15 = 3.7551624700427055e-05

Training epoch-40 batch-16
Running loss of epoch-40 batch-16 = 7.233815267682076e-05

Training epoch-40 batch-17
Running loss of epoch-40 batch-17 = 3.3117830753326416e-05

Training epoch-40 batch-18
Running loss of epoch-40 batch-18 = 1.921737566590309e-05

Training epoch-40 batch-19
Running loss of epoch-40 batch-19 = 2.6969239115715027e-05

Training epoch-40 batch-20
Running loss of epoch-40 batch-20 = 8.810381405055523e-05

Training epoch-40 batch-21
Running loss of epoch-40 batch-21 = 1.6300007700920105e-05

Training epoch-40 batch-22
Running loss of epoch-40 batch-22 = 9.373994544148445e-06

Training epoch-40 batch-23
Running loss of epoch-40 batch-23 = 2.9804417863488197e-05

Training epoch-40 batch-24
Running loss of epoch-40 batch-24 = 3.281049430370331e-05

Training epoch-40 batch-25
Running loss of epoch-40 batch-25 = 1.088390126824379e-05

Training epoch-40 batch-26
Running loss of epoch-40 batch-26 = 5.6571559980511665e-05

Training epoch-40 batch-27
Running loss of epoch-40 batch-27 = 4.4022221118211746e-05

Training epoch-40 batch-28
Running loss of epoch-40 batch-28 = 7.487833499908447e-05

Training epoch-40 batch-29
Running loss of epoch-40 batch-29 = 6.805011071264744e-05

Training epoch-40 batch-30
Running loss of epoch-40 batch-30 = 3.509456291794777e-05

Training epoch-40 batch-31
Running loss of epoch-40 batch-31 = 2.043996937572956e-05

Training epoch-40 batch-32
Running loss of epoch-40 batch-32 = 1.1541182175278664e-05

Training epoch-40 batch-33
Running loss of epoch-40 batch-33 = 4.3680425733327866e-05

Training epoch-40 batch-34
Running loss of epoch-40 batch-34 = 4.6980101615190506e-05

Training epoch-40 batch-35
Running loss of epoch-40 batch-35 = 4.749908111989498e-05

Training epoch-40 batch-36
Running loss of epoch-40 batch-36 = 1.7296290025115013e-05

Training epoch-40 batch-37
Running loss of epoch-40 batch-37 = 2.4463282898068428e-05

Training epoch-40 batch-38
Running loss of epoch-40 batch-38 = 3.194971941411495e-05

Training epoch-40 batch-39
Running loss of epoch-40 batch-39 = 1.7522135749459267e-05

Training epoch-40 batch-40
Running loss of epoch-40 batch-40 = 4.7794776037335396e-05

Training epoch-40 batch-41
Running loss of epoch-40 batch-41 = 0.00021333456970751286

Training epoch-40 batch-42
Running loss of epoch-40 batch-42 = 2.2736145183444023e-05

Training epoch-40 batch-43
Running loss of epoch-40 batch-43 = 4.19314019382e-05

Training epoch-40 batch-44
Running loss of epoch-40 batch-44 = 7.172301411628723e-05

Training epoch-40 batch-45
Running loss of epoch-40 batch-45 = 3.747851587831974e-05

Training epoch-40 batch-46
Running loss of epoch-40 batch-46 = 3.381422720849514e-05

Training epoch-40 batch-47
Running loss of epoch-40 batch-47 = 1.2374017387628555e-05

Training epoch-40 batch-48
Running loss of epoch-40 batch-48 = 2.2518448531627655e-05

Training epoch-40 batch-49
Running loss of epoch-40 batch-49 = 3.861403092741966e-05

Training epoch-40 batch-50
Running loss of epoch-40 batch-50 = 1.631048507988453e-05

Training epoch-40 batch-51
Running loss of epoch-40 batch-51 = 5.0455331802368164e-05

Training epoch-40 batch-52
Running loss of epoch-40 batch-52 = 2.6647699996829033e-05

Training epoch-40 batch-53
Running loss of epoch-40 batch-53 = 7.696449756622314e-06

Training epoch-40 batch-54
Running loss of epoch-40 batch-54 = 5.3333351388573647e-05

Training epoch-40 batch-55
Running loss of epoch-40 batch-55 = 1.7528189346194267e-05

Training epoch-40 batch-56
Running loss of epoch-40 batch-56 = 0.00010984833352267742

Training epoch-40 batch-57
Running loss of epoch-40 batch-57 = 2.9420480132102966e-05

Training epoch-40 batch-58
Running loss of epoch-40 batch-58 = 1.9651837646961212e-05

Training epoch-40 batch-59
Running loss of epoch-40 batch-59 = 3.645196557044983e-05

Training epoch-40 batch-60
Running loss of epoch-40 batch-60 = 2.631428651511669e-05

Training epoch-40 batch-61
Running loss of epoch-40 batch-61 = 2.7018366381525993e-05

Training epoch-40 batch-62
Running loss of epoch-40 batch-62 = 1.4065997675061226e-05

Training epoch-40 batch-63
Running loss of epoch-40 batch-63 = 1.1233380064368248e-05

Training epoch-40 batch-64
Running loss of epoch-40 batch-64 = 1.0787509381771088e-05

Training epoch-40 batch-65
Running loss of epoch-40 batch-65 = 3.104354254901409e-05

Training epoch-40 batch-66
Running loss of epoch-40 batch-66 = 1.8663238734006882e-05

Training epoch-40 batch-67
Running loss of epoch-40 batch-67 = 1.8103979527950287e-05

Training epoch-40 batch-68
Running loss of epoch-40 batch-68 = 4.3329549953341484e-05

Training epoch-40 batch-69
Running loss of epoch-40 batch-69 = 5.357665941119194e-05

Training epoch-40 batch-70
Running loss of epoch-40 batch-70 = 1.324433833360672e-05

Training epoch-40 batch-71
Running loss of epoch-40 batch-71 = 4.0748389437794685e-05

Training epoch-40 batch-72
Running loss of epoch-40 batch-72 = 1.4332123100757599e-05

Training epoch-40 batch-73
Running loss of epoch-40 batch-73 = 8.30718781799078e-05

Training epoch-40 batch-74
Running loss of epoch-40 batch-74 = 4.643155261874199e-05

Training epoch-40 batch-75
Running loss of epoch-40 batch-75 = 4.250602796673775e-05

Training epoch-40 batch-76
Running loss of epoch-40 batch-76 = 1.4078337699174881e-05

Training epoch-40 batch-77
Running loss of epoch-40 batch-77 = 2.0096544176340103e-05

Training epoch-40 batch-78
Running loss of epoch-40 batch-78 = 3.5901088267564774e-05

Training epoch-40 batch-79
Running loss of epoch-40 batch-79 = 0.00012697314377874136

Training epoch-40 batch-80
Running loss of epoch-40 batch-80 = 9.580585174262524e-05

Training epoch-40 batch-81
Running loss of epoch-40 batch-81 = 3.328011371195316e-05

Training epoch-40 batch-82
Running loss of epoch-40 batch-82 = 2.472521737217903e-05

Training epoch-40 batch-83
Running loss of epoch-40 batch-83 = 3.1052855774760246e-05

Training epoch-40 batch-84
Running loss of epoch-40 batch-84 = 8.64267349243164e-06

Training epoch-40 batch-85
Running loss of epoch-40 batch-85 = 2.260715700685978e-05

Training epoch-40 batch-86
Running loss of epoch-40 batch-86 = 8.734152652323246e-05

Training epoch-40 batch-87
Running loss of epoch-40 batch-87 = 2.260482870042324e-05

Training epoch-40 batch-88
Running loss of epoch-40 batch-88 = 9.458418935537338e-05

Training epoch-40 batch-89
Running loss of epoch-40 batch-89 = 3.366079181432724e-05

Training epoch-40 batch-90
Running loss of epoch-40 batch-90 = 2.0483974367380142e-05

Training epoch-40 batch-91
Running loss of epoch-40 batch-91 = 8.667865768074989e-05

Training epoch-40 batch-92
Running loss of epoch-40 batch-92 = 3.9483653381466866e-05

Training epoch-40 batch-93
Running loss of epoch-40 batch-93 = 1.8068822100758553e-05

Training epoch-40 batch-94
Running loss of epoch-40 batch-94 = 7.521314546465874e-05

Training epoch-40 batch-95
Running loss of epoch-40 batch-95 = 5.556386895477772e-05

Training epoch-40 batch-96
Running loss of epoch-40 batch-96 = 0.00013869069516658783

Training epoch-40 batch-97
Running loss of epoch-40 batch-97 = 9.500398300588131e-05

Training epoch-40 batch-98
Running loss of epoch-40 batch-98 = 2.3739878088235855e-05

Training epoch-40 batch-99
Running loss of epoch-40 batch-99 = 4.73828986287117e-05

Training epoch-40 batch-100
Running loss of epoch-40 batch-100 = 1.1148396879434586e-05

Training epoch-40 batch-101
Running loss of epoch-40 batch-101 = 5.2549876272678375e-05

Training epoch-40 batch-102
Running loss of epoch-40 batch-102 = 1.3951212167739868e-05

Training epoch-40 batch-103
Running loss of epoch-40 batch-103 = 2.0733801648020744e-05

Training epoch-40 batch-104
Running loss of epoch-40 batch-104 = 2.2733816877007484e-05

Training epoch-40 batch-105
Running loss of epoch-40 batch-105 = 4.410720430314541e-05

Training epoch-40 batch-106
Running loss of epoch-40 batch-106 = 6.641959771513939e-06

Training epoch-40 batch-107
Running loss of epoch-40 batch-107 = 9.546056389808655e-06

Training epoch-40 batch-108
Running loss of epoch-40 batch-108 = 4.581967368721962e-05

Training epoch-40 batch-109
Running loss of epoch-40 batch-109 = 6.000138819217682e-05

Training epoch-40 batch-110
Running loss of epoch-40 batch-110 = 7.23141711205244e-05

Training epoch-40 batch-111
Running loss of epoch-40 batch-111 = 2.4777138605713844e-05

Training epoch-40 batch-112
Running loss of epoch-40 batch-112 = 2.6464229449629784e-05

Training epoch-40 batch-113
Running loss of epoch-40 batch-113 = 4.733121022582054e-05

Training epoch-40 batch-114
Running loss of epoch-40 batch-114 = 7.744180038571358e-06

Training epoch-40 batch-115
Running loss of epoch-40 batch-115 = 3.0013499781489372e-05

Training epoch-40 batch-116
Running loss of epoch-40 batch-116 = 2.4291686713695526e-05

Training epoch-40 batch-117
Running loss of epoch-40 batch-117 = 2.2074906155467033e-05

Training epoch-40 batch-118
Running loss of epoch-40 batch-118 = 5.380064249038696e-05

Training epoch-40 batch-119
Running loss of epoch-40 batch-119 = 3.1687552109360695e-05

Training epoch-40 batch-120
Running loss of epoch-40 batch-120 = 1.8905848264694214e-05

Training epoch-40 batch-121
Running loss of epoch-40 batch-121 = 2.302904613316059e-05

Training epoch-40 batch-122
Running loss of epoch-40 batch-122 = 3.466499038040638e-05

Training epoch-40 batch-123
Running loss of epoch-40 batch-123 = 8.57282429933548e-06

Training epoch-40 batch-124
Running loss of epoch-40 batch-124 = 2.337014302611351e-05

Training epoch-40 batch-125
Running loss of epoch-40 batch-125 = 2.2676773369312286e-05

Training epoch-40 batch-126
Running loss of epoch-40 batch-126 = 1.7960090190172195e-05

Training epoch-40 batch-127
Running loss of epoch-40 batch-127 = 7.690279744565487e-05

Training epoch-40 batch-128
Running loss of epoch-40 batch-128 = 2.557947300374508e-05

Training epoch-40 batch-129
Running loss of epoch-40 batch-129 = 1.887441612780094e-05

Training epoch-40 batch-130
Running loss of epoch-40 batch-130 = 3.647292032837868e-05

Training epoch-40 batch-131
Running loss of epoch-40 batch-131 = 1.13749410957098e-05

Training epoch-40 batch-132
Running loss of epoch-40 batch-132 = 3.002420999109745e-05

Training epoch-40 batch-133
Running loss of epoch-40 batch-133 = 2.5324057787656784e-05

Training epoch-40 batch-134
Running loss of epoch-40 batch-134 = 3.394065424799919e-05

Training epoch-40 batch-135
Running loss of epoch-40 batch-135 = 1.1409632861614227e-05

Training epoch-40 batch-136
Running loss of epoch-40 batch-136 = 4.229974001646042e-05

Training epoch-40 batch-137
Running loss of epoch-40 batch-137 = 4.671746864914894e-05

Training epoch-40 batch-138
Running loss of epoch-40 batch-138 = 8.865026757121086e-06

Training epoch-40 batch-139
Running loss of epoch-40 batch-139 = 2.5430694222450256e-05

Training epoch-40 batch-140
Running loss of epoch-40 batch-140 = 3.438116982579231e-05

Training epoch-40 batch-141
Running loss of epoch-40 batch-141 = 0.00012822984717786312

Training epoch-40 batch-142
Running loss of epoch-40 batch-142 = 2.775643952190876e-05

Training epoch-40 batch-143
Running loss of epoch-40 batch-143 = 1.2462493032217026e-05

Training epoch-40 batch-144
Running loss of epoch-40 batch-144 = 2.7120346203446388e-05

Training epoch-40 batch-145
Running loss of epoch-40 batch-145 = 3.014039248228073e-05

Training epoch-40 batch-146
Running loss of epoch-40 batch-146 = 0.0001726122573018074

Training epoch-40 batch-147
Running loss of epoch-40 batch-147 = 2.3142900317907333e-05

Training epoch-40 batch-148
Running loss of epoch-40 batch-148 = 4.566204734146595e-05

Training epoch-40 batch-149
Running loss of epoch-40 batch-149 = 3.2414915040135384e-05

Training epoch-40 batch-150
Running loss of epoch-40 batch-150 = 2.1152198314666748e-05

Training epoch-40 batch-151
Running loss of epoch-40 batch-151 = 2.586771734058857e-05

Training epoch-40 batch-152
Running loss of epoch-40 batch-152 = 4.41218726336956e-05

Training epoch-40 batch-153
Running loss of epoch-40 batch-153 = 2.3098662495613098e-05

Training epoch-40 batch-154
Running loss of epoch-40 batch-154 = 1.2495089322328568e-05

Training epoch-40 batch-155
Running loss of epoch-40 batch-155 = 2.841581590473652e-05

Training epoch-40 batch-156
Running loss of epoch-40 batch-156 = 1.9874656572937965e-05

Training epoch-40 batch-157
Running loss of epoch-40 batch-157 = 3.312155604362488e-05

Finished training epoch-40.



Average train loss at epoch-40 = 3.712634220719337e-05

Started Evaluation

Average val loss at epoch-40 = 1.4555387026327815

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 96.23 %
Accuracy for class setUp is: 89.84 %
Accuracy for class onCreate is: 90.94 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 64.38 %
Accuracy for class hashCode is: 97.75 %
Accuracy for class init is: 50.90 %
Accuracy for class execute is: 45.78 %
Accuracy for class get is: 63.08 %

Overall Accuracy = 81.45 %

Finished Evaluation



Started training epoch-41


Training epoch-41 batch-1
Running loss of epoch-41 batch-1 = 4.720175638794899e-06

Training epoch-41 batch-2
Running loss of epoch-41 batch-2 = 0.00016741733998060226

Training epoch-41 batch-3
Running loss of epoch-41 batch-3 = 1.5244120731949806e-05

Training epoch-41 batch-4
Running loss of epoch-41 batch-4 = 1.2215692549943924e-05

Training epoch-41 batch-5
Running loss of epoch-41 batch-5 = 3.160769119858742e-05

Training epoch-41 batch-6
Running loss of epoch-41 batch-6 = 1.6904668882489204e-05

Training epoch-41 batch-7
Running loss of epoch-41 batch-7 = 9.845709428191185e-06

Training epoch-41 batch-8
Running loss of epoch-41 batch-8 = 2.5519169867038727e-05

Training epoch-41 batch-9
Running loss of epoch-41 batch-9 = 1.9186874851584435e-05

Training epoch-41 batch-10
Running loss of epoch-41 batch-10 = 3.825174644589424e-05

Training epoch-41 batch-11
Running loss of epoch-41 batch-11 = 1.091696321964264e-05

Training epoch-41 batch-12
Running loss of epoch-41 batch-12 = 5.822209641337395e-05

Training epoch-41 batch-13
Running loss of epoch-41 batch-13 = 7.1784015744924545e-06

Training epoch-41 batch-14
Running loss of epoch-41 batch-14 = 2.3608794435858727e-05

Training epoch-41 batch-15
Running loss of epoch-41 batch-15 = 3.5032397136092186e-05

Training epoch-41 batch-16
Running loss of epoch-41 batch-16 = 4.231021739542484e-05

Training epoch-41 batch-17
Running loss of epoch-41 batch-17 = 3.510015085339546e-05

Training epoch-41 batch-18
Running loss of epoch-41 batch-18 = 1.0739313438534737e-05

Training epoch-41 batch-19
Running loss of epoch-41 batch-19 = 3.54123767465353e-05

Training epoch-41 batch-20
Running loss of epoch-41 batch-20 = 2.2136140614748e-05

Training epoch-41 batch-21
Running loss of epoch-41 batch-21 = 2.7334550395607948e-05

Training epoch-41 batch-22
Running loss of epoch-41 batch-22 = 4.191184416413307e-05

Training epoch-41 batch-23
Running loss of epoch-41 batch-23 = 3.365171141922474e-05

Training epoch-41 batch-24
Running loss of epoch-41 batch-24 = 6.691552698612213e-06

Training epoch-41 batch-25
Running loss of epoch-41 batch-25 = 3.535440191626549e-05

Training epoch-41 batch-26
Running loss of epoch-41 batch-26 = 1.851748675107956e-05

Training epoch-41 batch-27
Running loss of epoch-41 batch-27 = 8.892267942428589e-06

Training epoch-41 batch-28
Running loss of epoch-41 batch-28 = 5.0888629630208015e-05

Training epoch-41 batch-29
Running loss of epoch-41 batch-29 = 2.6818597689270973e-05

Training epoch-41 batch-30
Running loss of epoch-41 batch-30 = 1.2466451153159142e-05

Training epoch-41 batch-31
Running loss of epoch-41 batch-31 = 2.6169465854763985e-05

Training epoch-41 batch-32
Running loss of epoch-41 batch-32 = 2.9272399842739105e-05

Training epoch-41 batch-33
Running loss of epoch-41 batch-33 = 2.1972227841615677e-05

Training epoch-41 batch-34
Running loss of epoch-41 batch-34 = 4.5220134779810905e-05

Training epoch-41 batch-35
Running loss of epoch-41 batch-35 = 4.6528526581823826e-05

Training epoch-41 batch-36
Running loss of epoch-41 batch-36 = 4.004407674074173e-05

Training epoch-41 batch-37
Running loss of epoch-41 batch-37 = 4.612654447555542e-05

Training epoch-41 batch-38
Running loss of epoch-41 batch-38 = 8.555827662348747e-06

Training epoch-41 batch-39
Running loss of epoch-41 batch-39 = 1.7728889361023903e-05

Training epoch-41 batch-40
Running loss of epoch-41 batch-40 = 4.189414903521538e-05

Training epoch-41 batch-41
Running loss of epoch-41 batch-41 = 4.022032953798771e-05

Training epoch-41 batch-42
Running loss of epoch-41 batch-42 = 1.8889782950282097e-05

Training epoch-41 batch-43
Running loss of epoch-41 batch-43 = 4.1157472878694534e-05

Training epoch-41 batch-44
Running loss of epoch-41 batch-44 = 1.3072509318590164e-05

Training epoch-41 batch-45
Running loss of epoch-41 batch-45 = 5.242438055574894e-05

Training epoch-41 batch-46
Running loss of epoch-41 batch-46 = 2.9264716431498528e-05

Training epoch-41 batch-47
Running loss of epoch-41 batch-47 = 3.078952431678772e-06

Training epoch-41 batch-48
Running loss of epoch-41 batch-48 = 1.3577984645962715e-05

Training epoch-41 batch-49
Running loss of epoch-41 batch-49 = 8.935341611504555e-06

Training epoch-41 batch-50
Running loss of epoch-41 batch-50 = 2.1230429410934448e-05

Training epoch-41 batch-51
Running loss of epoch-41 batch-51 = 2.4985987693071365e-05

Training epoch-41 batch-52
Running loss of epoch-41 batch-52 = 7.627299055457115e-06

Training epoch-41 batch-53
Running loss of epoch-41 batch-53 = 2.6324298232793808e-05

Training epoch-41 batch-54
Running loss of epoch-41 batch-54 = 1.1261319741606712e-05

Training epoch-41 batch-55
Running loss of epoch-41 batch-55 = 7.688114419579506e-05

Training epoch-41 batch-56
Running loss of epoch-41 batch-56 = 1.8190359696745872e-05

Training epoch-41 batch-57
Running loss of epoch-41 batch-57 = 2.597365528345108e-05

Training epoch-41 batch-58
Running loss of epoch-41 batch-58 = 1.471303403377533e-05

Training epoch-41 batch-59
Running loss of epoch-41 batch-59 = 1.3996381312608719e-05

Training epoch-41 batch-60
Running loss of epoch-41 batch-60 = 2.053985372185707e-05

Training epoch-41 batch-61
Running loss of epoch-41 batch-61 = 0.00015214458107948303

Training epoch-41 batch-62
Running loss of epoch-41 batch-62 = 7.388414815068245e-06

Training epoch-41 batch-63
Running loss of epoch-41 batch-63 = 1.8154270946979523e-05

Training epoch-41 batch-64
Running loss of epoch-41 batch-64 = 6.001838482916355e-05

Training epoch-41 batch-65
Running loss of epoch-41 batch-65 = 1.9174767658114433e-05

Training epoch-41 batch-66
Running loss of epoch-41 batch-66 = 2.5022076442837715e-05

Training epoch-41 batch-67
Running loss of epoch-41 batch-67 = 6.52349554002285e-05

Training epoch-41 batch-68
Running loss of epoch-41 batch-68 = 1.657474786043167e-05

Training epoch-41 batch-69
Running loss of epoch-41 batch-69 = 2.1587824448943138e-05

Training epoch-41 batch-70
Running loss of epoch-41 batch-70 = 4.639476537704468e-05

Training epoch-41 batch-71
Running loss of epoch-41 batch-71 = 3.1789764761924744e-05

Training epoch-41 batch-72
Running loss of epoch-41 batch-72 = 1.2011267244815826e-05

Training epoch-41 batch-73
Running loss of epoch-41 batch-73 = 5.6935008615255356e-05

Training epoch-41 batch-74
Running loss of epoch-41 batch-74 = 2.8778333216905594e-05

Training epoch-41 batch-75
Running loss of epoch-41 batch-75 = 4.762876778841019e-05

Training epoch-41 batch-76
Running loss of epoch-41 batch-76 = 2.3247208446264267e-05

Training epoch-41 batch-77
Running loss of epoch-41 batch-77 = 1.695915125310421e-05

Training epoch-41 batch-78
Running loss of epoch-41 batch-78 = 4.4968677684664726e-05

Training epoch-41 batch-79
Running loss of epoch-41 batch-79 = 4.256400279700756e-05

Training epoch-41 batch-80
Running loss of epoch-41 batch-80 = 1.59507617354393e-05

Training epoch-41 batch-81
Running loss of epoch-41 batch-81 = 6.202259100973606e-05

Training epoch-41 batch-82
Running loss of epoch-41 batch-82 = 9.73767600953579e-06

Training epoch-41 batch-83
Running loss of epoch-41 batch-83 = 5.601230077445507e-05

Training epoch-41 batch-84
Running loss of epoch-41 batch-84 = 3.810902126133442e-05

Training epoch-41 batch-85
Running loss of epoch-41 batch-85 = 9.679887443780899e-05

Training epoch-41 batch-86
Running loss of epoch-41 batch-86 = 3.3368123695254326e-05

Training epoch-41 batch-87
Running loss of epoch-41 batch-87 = 1.6105594113469124e-05

Training epoch-41 batch-88
Running loss of epoch-41 batch-88 = 2.2005988284945488e-05

Training epoch-41 batch-89
Running loss of epoch-41 batch-89 = 1.9714701920747757e-05

Training epoch-41 batch-90
Running loss of epoch-41 batch-90 = 5.782046355307102e-05

Training epoch-41 batch-91
Running loss of epoch-41 batch-91 = 1.2668315321207047e-05

Training epoch-41 batch-92
Running loss of epoch-41 batch-92 = 1.7731450498104095e-05

Training epoch-41 batch-93
Running loss of epoch-41 batch-93 = 6.9802626967430115e-06

Training epoch-41 batch-94
Running loss of epoch-41 batch-94 = 8.205114863812923e-05

Training epoch-41 batch-95
Running loss of epoch-41 batch-95 = 9.46270301938057e-06

Training epoch-41 batch-96
Running loss of epoch-41 batch-96 = 7.106945849955082e-05

Training epoch-41 batch-97
Running loss of epoch-41 batch-97 = 8.828705176711082e-06

Training epoch-41 batch-98
Running loss of epoch-41 batch-98 = 3.8757920265197754e-05

Training epoch-41 batch-99
Running loss of epoch-41 batch-99 = 2.896832302212715e-05

Training epoch-41 batch-100
Running loss of epoch-41 batch-100 = 2.5644199922680855e-05

Training epoch-41 batch-101
Running loss of epoch-41 batch-101 = 2.4736393243074417e-05

Training epoch-41 batch-102
Running loss of epoch-41 batch-102 = 2.453569322824478e-05

Training epoch-41 batch-103
Running loss of epoch-41 batch-103 = 1.3964250683784485e-05

Training epoch-41 batch-104
Running loss of epoch-41 batch-104 = 2.5961780920624733e-05

Training epoch-41 batch-105
Running loss of epoch-41 batch-105 = 1.866486854851246e-05

Training epoch-41 batch-106
Running loss of epoch-41 batch-106 = 5.2513787522912025e-05

Training epoch-41 batch-107
Running loss of epoch-41 batch-107 = 1.9510509446263313e-05

Training epoch-41 batch-108
Running loss of epoch-41 batch-108 = 2.0607840269804e-05

Training epoch-41 batch-109
Running loss of epoch-41 batch-109 = 1.3587763532996178e-05

Training epoch-41 batch-110
Running loss of epoch-41 batch-110 = 4.8276735469698906e-05

Training epoch-41 batch-111
Running loss of epoch-41 batch-111 = 2.988940104842186e-05

Training epoch-41 batch-112
Running loss of epoch-41 batch-112 = 2.979906275868416e-05

Training epoch-41 batch-113
Running loss of epoch-41 batch-113 = 3.061210736632347e-05

Training epoch-41 batch-114
Running loss of epoch-41 batch-114 = 1.0193558409810066e-05

Training epoch-41 batch-115
Running loss of epoch-41 batch-115 = 2.0233681425452232e-05

Training epoch-41 batch-116
Running loss of epoch-41 batch-116 = 0.0001054881140589714

Training epoch-41 batch-117
Running loss of epoch-41 batch-117 = 4.1679712012410164e-05

Training epoch-41 batch-118
Running loss of epoch-41 batch-118 = 1.1197524145245552e-05

Training epoch-41 batch-119
Running loss of epoch-41 batch-119 = 7.666181772947311e-06

Training epoch-41 batch-120
Running loss of epoch-41 batch-120 = 1.8458114936947823e-05

Training epoch-41 batch-121
Running loss of epoch-41 batch-121 = 8.834293112158775e-06

Training epoch-41 batch-122
Running loss of epoch-41 batch-122 = 2.1121930330991745e-05

Training epoch-41 batch-123
Running loss of epoch-41 batch-123 = 1.720828004181385e-05

Training epoch-41 batch-124
Running loss of epoch-41 batch-124 = 0.00014494778588414192

Training epoch-41 batch-125
Running loss of epoch-41 batch-125 = 1.4940742403268814e-05

Training epoch-41 batch-126
Running loss of epoch-41 batch-126 = 1.5990808606147766e-05

Training epoch-41 batch-127
Running loss of epoch-41 batch-127 = 1.792795956134796e-05

Training epoch-41 batch-128
Running loss of epoch-41 batch-128 = 1.5820609405636787e-05

Training epoch-41 batch-129
Running loss of epoch-41 batch-129 = 3.359094262123108e-05

Training epoch-41 batch-130
Running loss of epoch-41 batch-130 = 6.408733315765858e-05

Training epoch-41 batch-131
Running loss of epoch-41 batch-131 = 2.0659063011407852e-05

Training epoch-41 batch-132
Running loss of epoch-41 batch-132 = 9.638024494051933e-06

Training epoch-41 batch-133
Running loss of epoch-41 batch-133 = 4.7333305701613426e-05

Training epoch-41 batch-134
Running loss of epoch-41 batch-134 = 1.3527227565646172e-05

Training epoch-41 batch-135
Running loss of epoch-41 batch-135 = 4.6418048441410065e-05

Training epoch-41 batch-136
Running loss of epoch-41 batch-136 = 8.784281089901924e-05

Training epoch-41 batch-137
Running loss of epoch-41 batch-137 = 1.0213349014520645e-05

Training epoch-41 batch-138
Running loss of epoch-41 batch-138 = 3.0978117138147354e-05

Training epoch-41 batch-139
Running loss of epoch-41 batch-139 = 1.0802410542964935e-05

Training epoch-41 batch-140
Running loss of epoch-41 batch-140 = 9.924406185746193e-06

Training epoch-41 batch-141
Running loss of epoch-41 batch-141 = 5.239062011241913e-05

Training epoch-41 batch-142
Running loss of epoch-41 batch-142 = 2.007884904742241e-05

Training epoch-41 batch-143
Running loss of epoch-41 batch-143 = 1.8293969333171844e-05

Training epoch-41 batch-144
Running loss of epoch-41 batch-144 = 1.4036893844604492e-05

Training epoch-41 batch-145
Running loss of epoch-41 batch-145 = 3.882520832121372e-05

Training epoch-41 batch-146
Running loss of epoch-41 batch-146 = 9.655486792325974e-06

Training epoch-41 batch-147
Running loss of epoch-41 batch-147 = 2.281786873936653e-05

Training epoch-41 batch-148
Running loss of epoch-41 batch-148 = 5.433429032564163e-05

Training epoch-41 batch-149
Running loss of epoch-41 batch-149 = 2.3723579943180084e-05

Training epoch-41 batch-150
Running loss of epoch-41 batch-150 = 3.845151513814926e-05

Training epoch-41 batch-151
Running loss of epoch-41 batch-151 = 4.0552811697125435e-05

Training epoch-41 batch-152
Running loss of epoch-41 batch-152 = 1.0573770850896835e-05

Training epoch-41 batch-153
Running loss of epoch-41 batch-153 = 1.379009336233139e-05

Training epoch-41 batch-154
Running loss of epoch-41 batch-154 = 3.3361371606588364e-05

Training epoch-41 batch-155
Running loss of epoch-41 batch-155 = 5.626841448247433e-05

Training epoch-41 batch-156
Running loss of epoch-41 batch-156 = 2.0331237465143204e-05

Training epoch-41 batch-157
Running loss of epoch-41 batch-157 = 0.00014888867735862732

Finished training epoch-41.



Average train loss at epoch-41 = 3.117077276110649e-05

Started Evaluation

Average val loss at epoch-41 = 1.4715978610614144

Accuracy for classes:
Accuracy for class equals is: 96.37 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 91.04 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 63.47 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.24 %
Accuracy for class execute is: 44.58 %
Accuracy for class get is: 64.87 %

Overall Accuracy = 81.56 %

Finished Evaluation



Started training epoch-42


Training epoch-42 batch-1
Running loss of epoch-42 batch-1 = 2.1905172616243362e-05

Training epoch-42 batch-2
Running loss of epoch-42 batch-2 = 9.640352800488472e-06

Training epoch-42 batch-3
Running loss of epoch-42 batch-3 = 1.063779927790165e-05

Training epoch-42 batch-4
Running loss of epoch-42 batch-4 = 1.189298927783966e-05

Training epoch-42 batch-5
Running loss of epoch-42 batch-5 = 4.47926577180624e-05

Training epoch-42 batch-6
Running loss of epoch-42 batch-6 = 1.161801628768444e-05

Training epoch-42 batch-7
Running loss of epoch-42 batch-7 = 4.7593144699931145e-05

Training epoch-42 batch-8
Running loss of epoch-42 batch-8 = 2.8094975277781487e-05

Training epoch-42 batch-9
Running loss of epoch-42 batch-9 = 1.917267218232155e-05

Training epoch-42 batch-10
Running loss of epoch-42 batch-10 = 6.126542575657368e-05

Training epoch-42 batch-11
Running loss of epoch-42 batch-11 = 1.067272387444973e-05

Training epoch-42 batch-12
Running loss of epoch-42 batch-12 = 1.4721183106303215e-05

Training epoch-42 batch-13
Running loss of epoch-42 batch-13 = 1.5474623069167137e-05

Training epoch-42 batch-14
Running loss of epoch-42 batch-14 = 1.1492287740111351e-05

Training epoch-42 batch-15
Running loss of epoch-42 batch-15 = 2.1906336769461632e-05

Training epoch-42 batch-16
Running loss of epoch-42 batch-16 = 1.599173992872238e-05

Training epoch-42 batch-17
Running loss of epoch-42 batch-17 = 1.510418951511383e-05

Training epoch-42 batch-18
Running loss of epoch-42 batch-18 = 0.00010204501450061798

Training epoch-42 batch-19
Running loss of epoch-42 batch-19 = 1.3667857274413109e-05

Training epoch-42 batch-20
Running loss of epoch-42 batch-20 = 5.62635250389576e-06

Training epoch-42 batch-21
Running loss of epoch-42 batch-21 = 4.1266437619924545e-05

Training epoch-42 batch-22
Running loss of epoch-42 batch-22 = 2.535153180360794e-05

Training epoch-42 batch-23
Running loss of epoch-42 batch-23 = 2.2594351321458817e-05

Training epoch-42 batch-24
Running loss of epoch-42 batch-24 = 1.604948192834854e-05

Training epoch-42 batch-25
Running loss of epoch-42 batch-25 = 2.2668391466140747e-05

Training epoch-42 batch-26
Running loss of epoch-42 batch-26 = 2.396944910287857e-05

Training epoch-42 batch-27
Running loss of epoch-42 batch-27 = 6.218708585947752e-05

Training epoch-42 batch-28
Running loss of epoch-42 batch-28 = 2.9504066333174706e-05

Training epoch-42 batch-29
Running loss of epoch-42 batch-29 = 1.0998919606208801e-05

Training epoch-42 batch-30
Running loss of epoch-42 batch-30 = 3.2063107937574387e-05

Training epoch-42 batch-31
Running loss of epoch-42 batch-31 = 1.463247463107109e-05

Training epoch-42 batch-32
Running loss of epoch-42 batch-32 = 4.973146133124828e-05

Training epoch-42 batch-33
Running loss of epoch-42 batch-33 = 7.003359496593475e-05

Training epoch-42 batch-34
Running loss of epoch-42 batch-34 = 2.761860378086567e-05

Training epoch-42 batch-35
Running loss of epoch-42 batch-35 = 5.110027268528938e-05

Training epoch-42 batch-36
Running loss of epoch-42 batch-36 = 5.466490983963013e-05

Training epoch-42 batch-37
Running loss of epoch-42 batch-37 = 2.0337523892521858e-05

Training epoch-42 batch-38
Running loss of epoch-42 batch-38 = 1.6758451238274574e-05

Training epoch-42 batch-39
Running loss of epoch-42 batch-39 = 8.129049092531204e-06

Training epoch-42 batch-40
Running loss of epoch-42 batch-40 = 2.5197165086865425e-05

Training epoch-42 batch-41
Running loss of epoch-42 batch-41 = 3.122212365269661e-05

Training epoch-42 batch-42
Running loss of epoch-42 batch-42 = 4.882924258708954e-05

Training epoch-42 batch-43
Running loss of epoch-42 batch-43 = 1.5811994671821594e-05

Training epoch-42 batch-44
Running loss of epoch-42 batch-44 = 1.688278280198574e-05

Training epoch-42 batch-45
Running loss of epoch-42 batch-45 = 4.729581996798515e-05

Training epoch-42 batch-46
Running loss of epoch-42 batch-46 = 1.3295328244566917e-05

Training epoch-42 batch-47
Running loss of epoch-42 batch-47 = 4.191836342215538e-05

Training epoch-42 batch-48
Running loss of epoch-42 batch-48 = 2.1372223272919655e-05

Training epoch-42 batch-49
Running loss of epoch-42 batch-49 = 2.399762161076069e-05

Training epoch-42 batch-50
Running loss of epoch-42 batch-50 = 1.9754283130168915e-05

Training epoch-42 batch-51
Running loss of epoch-42 batch-51 = 1.5014316886663437e-05

Training epoch-42 batch-52
Running loss of epoch-42 batch-52 = 2.1030660718679428e-05

Training epoch-42 batch-53
Running loss of epoch-42 batch-53 = 1.1952128261327744e-05

Training epoch-42 batch-54
Running loss of epoch-42 batch-54 = 8.446397259831429e-06

Training epoch-42 batch-55
Running loss of epoch-42 batch-55 = 0.0001290834043174982

Training epoch-42 batch-56
Running loss of epoch-42 batch-56 = 7.680384442210197e-06

Training epoch-42 batch-57
Running loss of epoch-42 batch-57 = 1.1864583939313889e-05

Training epoch-42 batch-58
Running loss of epoch-42 batch-58 = 2.879556268453598e-05

Training epoch-42 batch-59
Running loss of epoch-42 batch-59 = 1.093931496143341e-05

Training epoch-42 batch-60
Running loss of epoch-42 batch-60 = 3.566546365618706e-05

Training epoch-42 batch-61
Running loss of epoch-42 batch-61 = 1.670699566602707e-05

Training epoch-42 batch-62
Running loss of epoch-42 batch-62 = 4.665134474635124e-05

Training epoch-42 batch-63
Running loss of epoch-42 batch-63 = 3.062933683395386e-05

Training epoch-42 batch-64
Running loss of epoch-42 batch-64 = 2.3825326934456825e-05

Training epoch-42 batch-65
Running loss of epoch-42 batch-65 = 1.4655524864792824e-05

Training epoch-42 batch-66
Running loss of epoch-42 batch-66 = 1.4317454770207405e-05

Training epoch-42 batch-67
Running loss of epoch-42 batch-67 = 4.7662295401096344e-05

Training epoch-42 batch-68
Running loss of epoch-42 batch-68 = 1.884298399090767e-05

Training epoch-42 batch-69
Running loss of epoch-42 batch-69 = 2.230820246040821e-05

Training epoch-42 batch-70
Running loss of epoch-42 batch-70 = 2.5059329345822334e-05

Training epoch-42 batch-71
Running loss of epoch-42 batch-71 = 1.8168240785598755e-05

Training epoch-42 batch-72
Running loss of epoch-42 batch-72 = 3.788108006119728e-05

Training epoch-42 batch-73
Running loss of epoch-42 batch-73 = 3.0579278245568275e-05

Training epoch-42 batch-74
Running loss of epoch-42 batch-74 = 6.095925346016884e-05

Training epoch-42 batch-75
Running loss of epoch-42 batch-75 = 6.6908542066812515e-06

Training epoch-42 batch-76
Running loss of epoch-42 batch-76 = 4.2390311136841774e-05

Training epoch-42 batch-77
Running loss of epoch-42 batch-77 = 1.153978519141674e-05

Training epoch-42 batch-78
Running loss of epoch-42 batch-78 = 1.0768184438347816e-05

Training epoch-42 batch-79
Running loss of epoch-42 batch-79 = 4.423828795552254e-05

Training epoch-42 batch-80
Running loss of epoch-42 batch-80 = 1.0178890079259872e-05

Training epoch-42 batch-81
Running loss of epoch-42 batch-81 = 6.937980651855469e-05

Training epoch-42 batch-82
Running loss of epoch-42 batch-82 = 1.9853468984365463e-05

Training epoch-42 batch-83
Running loss of epoch-42 batch-83 = 2.544955350458622e-05

Training epoch-42 batch-84
Running loss of epoch-42 batch-84 = 2.3449771106243134e-05

Training epoch-42 batch-85
Running loss of epoch-42 batch-85 = 2.0158709958195686e-05

Training epoch-42 batch-86
Running loss of epoch-42 batch-86 = 1.2500910088419914e-05

Training epoch-42 batch-87
Running loss of epoch-42 batch-87 = 7.503130473196507e-05

Training epoch-42 batch-88
Running loss of epoch-42 batch-88 = 6.163842044770718e-05

Training epoch-42 batch-89
Running loss of epoch-42 batch-89 = 3.303424455225468e-05

Training epoch-42 batch-90
Running loss of epoch-42 batch-90 = 3.695092163980007e-05

Training epoch-42 batch-91
Running loss of epoch-42 batch-91 = 1.7947982996702194e-05

Training epoch-42 batch-92
Running loss of epoch-42 batch-92 = 1.8459511920809746e-05

Training epoch-42 batch-93
Running loss of epoch-42 batch-93 = 2.4041393771767616e-05

Training epoch-42 batch-94
Running loss of epoch-42 batch-94 = 7.496913895010948e-06

Training epoch-42 batch-95
Running loss of epoch-42 batch-95 = 1.59537885338068e-05

Training epoch-42 batch-96
Running loss of epoch-42 batch-96 = 7.791910320520401e-06

Training epoch-42 batch-97
Running loss of epoch-42 batch-97 = 1.0563060641288757e-05

Training epoch-42 batch-98
Running loss of epoch-42 batch-98 = 1.9893283024430275e-05

Training epoch-42 batch-99
Running loss of epoch-42 batch-99 = 6.417511031031609e-05

Training epoch-42 batch-100
Running loss of epoch-42 batch-100 = 2.432265318930149e-05

Training epoch-42 batch-101
Running loss of epoch-42 batch-101 = 1.3544689863920212e-05

Training epoch-42 batch-102
Running loss of epoch-42 batch-102 = 2.225162461400032e-05

Training epoch-42 batch-103
Running loss of epoch-42 batch-103 = 3.340491093695164e-05

Training epoch-42 batch-104
Running loss of epoch-42 batch-104 = 3.328779712319374e-05

Training epoch-42 batch-105
Running loss of epoch-42 batch-105 = 1.6752630472183228e-05

Training epoch-42 batch-106
Running loss of epoch-42 batch-106 = 3.0383700504899025e-05

Training epoch-42 batch-107
Running loss of epoch-42 batch-107 = 1.7606886103749275e-05

Training epoch-42 batch-108
Running loss of epoch-42 batch-108 = 3.498140722513199e-05

Training epoch-42 batch-109
Running loss of epoch-42 batch-109 = 6.84661790728569e-06

Training epoch-42 batch-110
Running loss of epoch-42 batch-110 = 1.9701896235346794e-05

Training epoch-42 batch-111
Running loss of epoch-42 batch-111 = 2.1218089386820793e-05

Training epoch-42 batch-112
Running loss of epoch-42 batch-112 = 1.6514211893081665e-05

Training epoch-42 batch-113
Running loss of epoch-42 batch-113 = 3.557396121323109e-05

Training epoch-42 batch-114
Running loss of epoch-42 batch-114 = 9.149545803666115e-06

Training epoch-42 batch-115
Running loss of epoch-42 batch-115 = 9.309500455856323e-06

Training epoch-42 batch-116
Running loss of epoch-42 batch-116 = 4.608905874192715e-05

Training epoch-42 batch-117
Running loss of epoch-42 batch-117 = 3.143143840134144e-05

Training epoch-42 batch-118
Running loss of epoch-42 batch-118 = 4.343525506556034e-05

Training epoch-42 batch-119
Running loss of epoch-42 batch-119 = 2.535991370677948e-05

Training epoch-42 batch-120
Running loss of epoch-42 batch-120 = 5.99944032728672e-05

Training epoch-42 batch-121
Running loss of epoch-42 batch-121 = 5.052145570516586e-05

Training epoch-42 batch-122
Running loss of epoch-42 batch-122 = 8.58998391777277e-05

Training epoch-42 batch-123
Running loss of epoch-42 batch-123 = 2.4499604478478432e-05

Training epoch-42 batch-124
Running loss of epoch-42 batch-124 = 2.3030675947666168e-05

Training epoch-42 batch-125
Running loss of epoch-42 batch-125 = 2.6906374841928482e-05

Training epoch-42 batch-126
Running loss of epoch-42 batch-126 = 1.7894897609949112e-05

Training epoch-42 batch-127
Running loss of epoch-42 batch-127 = 1.7862766981124878e-05

Training epoch-42 batch-128
Running loss of epoch-42 batch-128 = 1.2251315638422966e-05

Training epoch-42 batch-129
Running loss of epoch-42 batch-129 = 4.517659544944763e-05

Training epoch-42 batch-130
Running loss of epoch-42 batch-130 = 5.5863289162516594e-05

Training epoch-42 batch-131
Running loss of epoch-42 batch-131 = 1.5992438420653343e-05

Training epoch-42 batch-132
Running loss of epoch-42 batch-132 = 4.3590087443590164e-05

Training epoch-42 batch-133
Running loss of epoch-42 batch-133 = 1.1325115337967873e-05

Training epoch-42 batch-134
Running loss of epoch-42 batch-134 = 2.652709372341633e-05

Training epoch-42 batch-135
Running loss of epoch-42 batch-135 = 3.9043137803673744e-05

Training epoch-42 batch-136
Running loss of epoch-42 batch-136 = 9.48198139667511e-05

Training epoch-42 batch-137
Running loss of epoch-42 batch-137 = 1.8802937120199203e-05

Training epoch-42 batch-138
Running loss of epoch-42 batch-138 = 2.8003007173538208e-05

Training epoch-42 batch-139
Running loss of epoch-42 batch-139 = 3.8400525227189064e-05

Training epoch-42 batch-140
Running loss of epoch-42 batch-140 = 3.182725049555302e-05

Training epoch-42 batch-141
Running loss of epoch-42 batch-141 = 4.392210394144058e-05

Training epoch-42 batch-142
Running loss of epoch-42 batch-142 = 0.00011129956692457199

Training epoch-42 batch-143
Running loss of epoch-42 batch-143 = 5.4050469771027565e-05

Training epoch-42 batch-144
Running loss of epoch-42 batch-144 = 2.3202970623970032e-05

Training epoch-42 batch-145
Running loss of epoch-42 batch-145 = 4.918687045574188e-05

Training epoch-42 batch-146
Running loss of epoch-42 batch-146 = 2.687610685825348e-05

Training epoch-42 batch-147
Running loss of epoch-42 batch-147 = 1.0211719200015068e-05

Training epoch-42 batch-148
Running loss of epoch-42 batch-148 = 5.002017132937908e-05

Training epoch-42 batch-149
Running loss of epoch-42 batch-149 = 7.481547072529793e-06

Training epoch-42 batch-150
Running loss of epoch-42 batch-150 = 7.111113518476486e-06

Training epoch-42 batch-151
Running loss of epoch-42 batch-151 = 2.0818552002310753e-05

Training epoch-42 batch-152
Running loss of epoch-42 batch-152 = 1.3816170394420624e-06

Training epoch-42 batch-153
Running loss of epoch-42 batch-153 = 1.7087208107113838e-05

Training epoch-42 batch-154
Running loss of epoch-42 batch-154 = 4.5550521463155746e-05

Training epoch-42 batch-155
Running loss of epoch-42 batch-155 = 3.105960786342621e-05

Training epoch-42 batch-156
Running loss of epoch-42 batch-156 = 1.945742405951023e-05

Training epoch-42 batch-157
Running loss of epoch-42 batch-157 = 3.5390257835388184e-06

Finished training epoch-42.



Average train loss at epoch-42 = 2.9157424718141555e-05

Started Evaluation

Average val loss at epoch-42 = 1.5193922831475375

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 95.57 %
Accuracy for class setUp is: 90.00 %
Accuracy for class onCreate is: 91.04 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 60.50 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 52.02 %
Accuracy for class execute is: 52.21 %
Accuracy for class get is: 63.33 %

Overall Accuracy = 81.51 %

Finished Evaluation



Started training epoch-43


Training epoch-43 batch-1
Running loss of epoch-43 batch-1 = 5.165347829461098e-06

Training epoch-43 batch-2
Running loss of epoch-43 batch-2 = 2.3212749511003494e-05

Training epoch-43 batch-3
Running loss of epoch-43 batch-3 = 8.916761726140976e-05

Training epoch-43 batch-4
Running loss of epoch-43 batch-4 = 4.1397521272301674e-05

Training epoch-43 batch-5
Running loss of epoch-43 batch-5 = 1.1081574484705925e-05

Training epoch-43 batch-6
Running loss of epoch-43 batch-6 = 1.4203367754817009e-05

Training epoch-43 batch-7
Running loss of epoch-43 batch-7 = 1.6452977433800697e-05

Training epoch-43 batch-8
Running loss of epoch-43 batch-8 = 2.4248147383332253e-05

Training epoch-43 batch-9
Running loss of epoch-43 batch-9 = 7.884809747338295e-06

Training epoch-43 batch-10
Running loss of epoch-43 batch-10 = 1.597893424332142e-05

Training epoch-43 batch-11
Running loss of epoch-43 batch-11 = 4.298030398786068e-05

Training epoch-43 batch-12
Running loss of epoch-43 batch-12 = 7.13602639734745e-06

Training epoch-43 batch-13
Running loss of epoch-43 batch-13 = 1.9705155864357948e-05

Training epoch-43 batch-14
Running loss of epoch-43 batch-14 = 1.2281816452741623e-05

Training epoch-43 batch-15
Running loss of epoch-43 batch-15 = 1.2130243703722954e-05

Training epoch-43 batch-16
Running loss of epoch-43 batch-16 = 2.4292385205626488e-05

Training epoch-43 batch-17
Running loss of epoch-43 batch-17 = 9.494437836110592e-05

Training epoch-43 batch-18
Running loss of epoch-43 batch-18 = 2.380693331360817e-05

Training epoch-43 batch-19
Running loss of epoch-43 batch-19 = 1.7704907804727554e-05

Training epoch-43 batch-20
Running loss of epoch-43 batch-20 = 2.1851155906915665e-05

Training epoch-43 batch-21
Running loss of epoch-43 batch-21 = 1.92360021173954e-05

Training epoch-43 batch-22
Running loss of epoch-43 batch-22 = 4.992145113646984e-05

Training epoch-43 batch-23
Running loss of epoch-43 batch-23 = 1.9129132851958275e-05

Training epoch-43 batch-24
Running loss of epoch-43 batch-24 = 1.7768237739801407e-05

Training epoch-43 batch-25
Running loss of epoch-43 batch-25 = 2.0222272723913193e-05

Training epoch-43 batch-26
Running loss of epoch-43 batch-26 = 3.245752304792404e-05

Training epoch-43 batch-27
Running loss of epoch-43 batch-27 = 2.0805280655622482e-05

Training epoch-43 batch-28
Running loss of epoch-43 batch-28 = 3.0860770493745804e-05

Training epoch-43 batch-29
Running loss of epoch-43 batch-29 = 2.1330546587705612e-05

Training epoch-43 batch-30
Running loss of epoch-43 batch-30 = 6.44647516310215e-05

Training epoch-43 batch-31
Running loss of epoch-43 batch-31 = 3.4759752452373505e-05

Training epoch-43 batch-32
Running loss of epoch-43 batch-32 = 1.699174754321575e-05

Training epoch-43 batch-33
Running loss of epoch-43 batch-33 = 1.4403136447072029e-05

Training epoch-43 batch-34
Running loss of epoch-43 batch-34 = 2.077152021229267e-05

Training epoch-43 batch-35
Running loss of epoch-43 batch-35 = 2.0978273823857307e-05

Training epoch-43 batch-36
Running loss of epoch-43 batch-36 = 7.348833605647087e-06

Training epoch-43 batch-37
Running loss of epoch-43 batch-37 = 4.8465095460414886e-05

Training epoch-43 batch-38
Running loss of epoch-43 batch-38 = 2.1245330572128296e-05

Training epoch-43 batch-39
Running loss of epoch-43 batch-39 = 1.6875099390745163e-05

Training epoch-43 batch-40
Running loss of epoch-43 batch-40 = 2.3980392143130302e-05

Training epoch-43 batch-41
Running loss of epoch-43 batch-41 = 3.5075703635811806e-05

Training epoch-43 batch-42
Running loss of epoch-43 batch-42 = 1.2181699275970459e-05

Training epoch-43 batch-43
Running loss of epoch-43 batch-43 = 2.2099120542407036e-05

Training epoch-43 batch-44
Running loss of epoch-43 batch-44 = 3.621331416070461e-05

Training epoch-43 batch-45
Running loss of epoch-43 batch-45 = 2.9505696147680283e-05

Training epoch-43 batch-46
Running loss of epoch-43 batch-46 = 2.3331958800554276e-05

Training epoch-43 batch-47
Running loss of epoch-43 batch-47 = 3.087334334850311e-05

Training epoch-43 batch-48
Running loss of epoch-43 batch-48 = 1.384480856359005e-05

Training epoch-43 batch-49
Running loss of epoch-43 batch-49 = 1.8690945580601692e-05

Training epoch-43 batch-50
Running loss of epoch-43 batch-50 = 5.700485780835152e-05

Training epoch-43 batch-51
Running loss of epoch-43 batch-51 = 1.0872725397348404e-05

Training epoch-43 batch-52
Running loss of epoch-43 batch-52 = 2.3272819817066193e-05

Training epoch-43 batch-53
Running loss of epoch-43 batch-53 = 1.827557571232319e-05

Training epoch-43 batch-54
Running loss of epoch-43 batch-54 = 1.1197756975889206e-05

Training epoch-43 batch-55
Running loss of epoch-43 batch-55 = 1.1393101885914803e-05

Training epoch-43 batch-56
Running loss of epoch-43 batch-56 = 2.5261426344513893e-05

Training epoch-43 batch-57
Running loss of epoch-43 batch-57 = 1.1605443432927132e-05

Training epoch-43 batch-58
Running loss of epoch-43 batch-58 = 1.2097647413611412e-05

Training epoch-43 batch-59
Running loss of epoch-43 batch-59 = 8.756760507822037e-06

Training epoch-43 batch-60
Running loss of epoch-43 batch-60 = 1.0348157957196236e-05

Training epoch-43 batch-61
Running loss of epoch-43 batch-61 = 2.6790890842676163e-05

Training epoch-43 batch-62
Running loss of epoch-43 batch-62 = 1.6880221664905548e-05

Training epoch-43 batch-63
Running loss of epoch-43 batch-63 = 1.2996373698115349e-05

Training epoch-43 batch-64
Running loss of epoch-43 batch-64 = 2.1923799067735672e-05

Training epoch-43 batch-65
Running loss of epoch-43 batch-65 = 3.323890268802643e-05

Training epoch-43 batch-66
Running loss of epoch-43 batch-66 = 2.7135014533996582e-05

Training epoch-43 batch-67
Running loss of epoch-43 batch-67 = 8.215894922614098e-06

Training epoch-43 batch-68
Running loss of epoch-43 batch-68 = 8.089002221822739e-06

Training epoch-43 batch-69
Running loss of epoch-43 batch-69 = 1.9508646801114082e-05

Training epoch-43 batch-70
Running loss of epoch-43 batch-70 = 1.9940780475735664e-05

Training epoch-43 batch-71
Running loss of epoch-43 batch-71 = 2.5607645511627197e-05

Training epoch-43 batch-72
Running loss of epoch-43 batch-72 = 1.916801556944847e-05

Training epoch-43 batch-73
Running loss of epoch-43 batch-73 = 2.6099849492311478e-05

Training epoch-43 batch-74
Running loss of epoch-43 batch-74 = 2.3807398974895477e-05

Training epoch-43 batch-75
Running loss of epoch-43 batch-75 = 2.4539418518543243e-05

Training epoch-43 batch-76
Running loss of epoch-43 batch-76 = 2.2721244022250175e-05

Training epoch-43 batch-77
Running loss of epoch-43 batch-77 = 2.986053004860878e-05

Training epoch-43 batch-78
Running loss of epoch-43 batch-78 = 9.52230766415596e-06

Training epoch-43 batch-79
Running loss of epoch-43 batch-79 = 1.271790824830532e-05

Training epoch-43 batch-80
Running loss of epoch-43 batch-80 = 5.990266799926758e-05

Training epoch-43 batch-81
Running loss of epoch-43 batch-81 = 1.1770986020565033e-05

Training epoch-43 batch-82
Running loss of epoch-43 batch-82 = 2.4881213903427124e-05

Training epoch-43 batch-83
Running loss of epoch-43 batch-83 = 2.6358291506767273e-05

Training epoch-43 batch-84
Running loss of epoch-43 batch-84 = 2.116197720170021e-05

Training epoch-43 batch-85
Running loss of epoch-43 batch-85 = 9.261025115847588e-05

Training epoch-43 batch-86
Running loss of epoch-43 batch-86 = 2.896878868341446e-05

Training epoch-43 batch-87
Running loss of epoch-43 batch-87 = 2.1978281438350677e-05

Training epoch-43 batch-88
Running loss of epoch-43 batch-88 = 1.184176653623581e-05

Training epoch-43 batch-89
Running loss of epoch-43 batch-89 = 3.556255251169205e-05

Training epoch-43 batch-90
Running loss of epoch-43 batch-90 = 1.5027588233351707e-05

Training epoch-43 batch-91
Running loss of epoch-43 batch-91 = 1.676240935921669e-05

Training epoch-43 batch-92
Running loss of epoch-43 batch-92 = 8.177943527698517e-06

Training epoch-43 batch-93
Running loss of epoch-43 batch-93 = 2.3448141291737556e-05

Training epoch-43 batch-94
Running loss of epoch-43 batch-94 = 1.4454824849963188e-05

Training epoch-43 batch-95
Running loss of epoch-43 batch-95 = 1.3168901205062866e-05

Training epoch-43 batch-96
Running loss of epoch-43 batch-96 = 2.5581568479537964e-05

Training epoch-43 batch-97
Running loss of epoch-43 batch-97 = 3.356230445206165e-05

Training epoch-43 batch-98
Running loss of epoch-43 batch-98 = 1.3900920748710632e-05

Training epoch-43 batch-99
Running loss of epoch-43 batch-99 = 8.770963177084923e-06

Training epoch-43 batch-100
Running loss of epoch-43 batch-100 = 1.77829060703516e-05

Training epoch-43 batch-101
Running loss of epoch-43 batch-101 = 9.739911183714867e-05

Training epoch-43 batch-102
Running loss of epoch-43 batch-102 = 3.765895962715149e-05

Training epoch-43 batch-103
Running loss of epoch-43 batch-103 = 1.8933555111289024e-05

Training epoch-43 batch-104
Running loss of epoch-43 batch-104 = 1.158122904598713e-05

Training epoch-43 batch-105
Running loss of epoch-43 batch-105 = 0.00015734019689261913

Training epoch-43 batch-106
Running loss of epoch-43 batch-106 = 3.254343755543232e-05

Training epoch-43 batch-107
Running loss of epoch-43 batch-107 = 1.8567778170108795e-05

Training epoch-43 batch-108
Running loss of epoch-43 batch-108 = 2.3819738999009132e-05

Training epoch-43 batch-109
Running loss of epoch-43 batch-109 = 4.8150308430194855e-05

Training epoch-43 batch-110
Running loss of epoch-43 batch-110 = 1.710420474410057e-05

Training epoch-43 batch-111
Running loss of epoch-43 batch-111 = 5.4598087444901466e-05

Training epoch-43 batch-112
Running loss of epoch-43 batch-112 = 1.2958189472556114e-05

Training epoch-43 batch-113
Running loss of epoch-43 batch-113 = 2.751941792666912e-05

Training epoch-43 batch-114
Running loss of epoch-43 batch-114 = 1.7396872863173485e-05

Training epoch-43 batch-115
Running loss of epoch-43 batch-115 = 2.4268869310617447e-05

Training epoch-43 batch-116
Running loss of epoch-43 batch-116 = 1.5042955055832863e-05

Training epoch-43 batch-117
Running loss of epoch-43 batch-117 = 1.9159633666276932e-05

Training epoch-43 batch-118
Running loss of epoch-43 batch-118 = 3.4610042348504066e-05

Training epoch-43 batch-119
Running loss of epoch-43 batch-119 = 9.369570761919022e-06

Training epoch-43 batch-120
Running loss of epoch-43 batch-120 = 2.6322901248931885e-05

Training epoch-43 batch-121
Running loss of epoch-43 batch-121 = 2.6612309738993645e-05

Training epoch-43 batch-122
Running loss of epoch-43 batch-122 = 5.794363096356392e-05

Training epoch-43 batch-123
Running loss of epoch-43 batch-123 = 2.906820736825466e-05

Training epoch-43 batch-124
Running loss of epoch-43 batch-124 = 0.00016861455515027046

Training epoch-43 batch-125
Running loss of epoch-43 batch-125 = 2.8638402000069618e-05

Training epoch-43 batch-126
Running loss of epoch-43 batch-126 = 1.3741431757807732e-05

Training epoch-43 batch-127
Running loss of epoch-43 batch-127 = 4.278728738427162e-06

Training epoch-43 batch-128
Running loss of epoch-43 batch-128 = 1.697544939815998e-05

Training epoch-43 batch-129
Running loss of epoch-43 batch-129 = 2.132914960384369e-05

Training epoch-43 batch-130
Running loss of epoch-43 batch-130 = 7.918104529380798e-06

Training epoch-43 batch-131
Running loss of epoch-43 batch-131 = 1.7230166122317314e-05

Training epoch-43 batch-132
Running loss of epoch-43 batch-132 = 1.1967262253165245e-05

Training epoch-43 batch-133
Running loss of epoch-43 batch-133 = 8.041970431804657e-06

Training epoch-43 batch-134
Running loss of epoch-43 batch-134 = 5.317293107509613e-05

Training epoch-43 batch-135
Running loss of epoch-43 batch-135 = 2.1409010514616966e-05

Training epoch-43 batch-136
Running loss of epoch-43 batch-136 = 2.5410670787096024e-05

Training epoch-43 batch-137
Running loss of epoch-43 batch-137 = 0.00010739453136920929

Training epoch-43 batch-138
Running loss of epoch-43 batch-138 = 1.6873469576239586e-05

Training epoch-43 batch-139
Running loss of epoch-43 batch-139 = 2.0995503291487694e-05

Training epoch-43 batch-140
Running loss of epoch-43 batch-140 = 0.000306661706417799

Training epoch-43 batch-141
Running loss of epoch-43 batch-141 = 0.0001592391636222601

Training epoch-43 batch-142
Running loss of epoch-43 batch-142 = 2.049957402050495e-05

Training epoch-43 batch-143
Running loss of epoch-43 batch-143 = 1.3100448995828629e-05

Training epoch-43 batch-144
Running loss of epoch-43 batch-144 = 7.270253263413906e-05

Training epoch-43 batch-145
Running loss of epoch-43 batch-145 = 1.1339085176587105e-05

Training epoch-43 batch-146
Running loss of epoch-43 batch-146 = 1.6809441149234772e-05

Training epoch-43 batch-147
Running loss of epoch-43 batch-147 = 8.774152956902981e-05

Training epoch-43 batch-148
Running loss of epoch-43 batch-148 = 2.2582709789276123e-05

Training epoch-43 batch-149
Running loss of epoch-43 batch-149 = 9.772134944796562e-06

Training epoch-43 batch-150
Running loss of epoch-43 batch-150 = 4.2933737859129906e-05

Training epoch-43 batch-151
Running loss of epoch-43 batch-151 = 4.536053165793419e-05

Training epoch-43 batch-152
Running loss of epoch-43 batch-152 = 1.6932841390371323e-05

Training epoch-43 batch-153
Running loss of epoch-43 batch-153 = 2.373545430600643e-05

Training epoch-43 batch-154
Running loss of epoch-43 batch-154 = 0.0001683009322732687

Training epoch-43 batch-155
Running loss of epoch-43 batch-155 = 5.067302845418453e-05

Training epoch-43 batch-156
Running loss of epoch-43 batch-156 = 3.4475699067115784e-05

Training epoch-43 batch-157
Running loss of epoch-43 batch-157 = 1.5091150999069214e-05

Finished training epoch-43.



Average train loss at epoch-43 = 3.105649501085281e-05

Started Evaluation

Average val loss at epoch-43 = 1.539166379186841

Accuracy for classes:
Accuracy for class equals is: 95.87 %
Accuracy for class main is: 96.07 %
Accuracy for class setUp is: 89.51 %
Accuracy for class onCreate is: 90.41 %
Accuracy for class toString is: 83.28 %
Accuracy for class run is: 61.42 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 50.22 %
Accuracy for class execute is: 51.00 %
Accuracy for class get is: 65.90 %

Overall Accuracy = 81.37 %

Finished Evaluation



Started training epoch-44


Training epoch-44 batch-1
Running loss of epoch-44 batch-1 = 4.979991354048252e-05

Training epoch-44 batch-2
Running loss of epoch-44 batch-2 = 5.4447678849101067e-05

Training epoch-44 batch-3
Running loss of epoch-44 batch-3 = 1.8825288861989975e-05

Training epoch-44 batch-4
Running loss of epoch-44 batch-4 = 8.767470717430115e-06

Training epoch-44 batch-5
Running loss of epoch-44 batch-5 = 2.2397376596927643e-05

Training epoch-44 batch-6
Running loss of epoch-44 batch-6 = 3.652134910225868e-05

Training epoch-44 batch-7
Running loss of epoch-44 batch-7 = 8.093658834695816e-06

Training epoch-44 batch-8
Running loss of epoch-44 batch-8 = 1.1889729648828506e-05

Training epoch-44 batch-9
Running loss of epoch-44 batch-9 = 1.818477176129818e-05

Training epoch-44 batch-10
Running loss of epoch-44 batch-10 = 2.085138112306595e-05

Training epoch-44 batch-11
Running loss of epoch-44 batch-11 = 1.1000549420714378e-05

Training epoch-44 batch-12
Running loss of epoch-44 batch-12 = 3.168615512549877e-05

Training epoch-44 batch-13
Running loss of epoch-44 batch-13 = 8.081900887191296e-05

Training epoch-44 batch-14
Running loss of epoch-44 batch-14 = 1.1287163943052292e-05

Training epoch-44 batch-15
Running loss of epoch-44 batch-15 = 2.130446955561638e-05

Training epoch-44 batch-16
Running loss of epoch-44 batch-16 = 3.203749656677246e-05

Training epoch-44 batch-17
Running loss of epoch-44 batch-17 = 6.407801993191242e-05

Training epoch-44 batch-18
Running loss of epoch-44 batch-18 = 2.4612294510006905e-05

Training epoch-44 batch-19
Running loss of epoch-44 batch-19 = 8.277851156890392e-05

Training epoch-44 batch-20
Running loss of epoch-44 batch-20 = 1.1920230463147163e-05

Training epoch-44 batch-21
Running loss of epoch-44 batch-21 = 8.077826350927353e-06

Training epoch-44 batch-22
Running loss of epoch-44 batch-22 = 1.3341894373297691e-05

Training epoch-44 batch-23
Running loss of epoch-44 batch-23 = 3.3739954233169556e-05

Training epoch-44 batch-24
Running loss of epoch-44 batch-24 = 4.65314369648695e-05

Training epoch-44 batch-25
Running loss of epoch-44 batch-25 = 2.4792738258838654e-05

Training epoch-44 batch-26
Running loss of epoch-44 batch-26 = 4.385644569993019e-05

Training epoch-44 batch-27
Running loss of epoch-44 batch-27 = 1.0185176506638527e-05

Training epoch-44 batch-28
Running loss of epoch-44 batch-28 = 3.8072699680924416e-05

Training epoch-44 batch-29
Running loss of epoch-44 batch-29 = 1.4033634215593338e-05

Training epoch-44 batch-30
Running loss of epoch-44 batch-30 = 1.3693934306502342e-05

Training epoch-44 batch-31
Running loss of epoch-44 batch-31 = 2.3470958694815636e-05

Training epoch-44 batch-32
Running loss of epoch-44 batch-32 = 3.418140113353729e-05

Training epoch-44 batch-33
Running loss of epoch-44 batch-33 = 1.411139965057373e-05

Training epoch-44 batch-34
Running loss of epoch-44 batch-34 = 1.2741656973958015e-05

Training epoch-44 batch-35
Running loss of epoch-44 batch-35 = 1.3407785445451736e-05

Training epoch-44 batch-36
Running loss of epoch-44 batch-36 = 3.983895294368267e-05

Training epoch-44 batch-37
Running loss of epoch-44 batch-37 = 9.664194658398628e-05

Training epoch-44 batch-38
Running loss of epoch-44 batch-38 = 2.6742462068796158e-05

Training epoch-44 batch-39
Running loss of epoch-44 batch-39 = 3.5558128729462624e-05

Training epoch-44 batch-40
Running loss of epoch-44 batch-40 = 8.682254701852798e-06

Training epoch-44 batch-41
Running loss of epoch-44 batch-41 = 2.3082131519913673e-05

Training epoch-44 batch-42
Running loss of epoch-44 batch-42 = 5.9512676671147346e-05

Training epoch-44 batch-43
Running loss of epoch-44 batch-43 = 1.2143049389123917e-05

Training epoch-44 batch-44
Running loss of epoch-44 batch-44 = 5.0663016736507416e-05

Training epoch-44 batch-45
Running loss of epoch-44 batch-45 = 3.7668272852897644e-05

Training epoch-44 batch-46
Running loss of epoch-44 batch-46 = 2.1601328626275063e-05

Training epoch-44 batch-47
Running loss of epoch-44 batch-47 = 1.1273426935076714e-05

Training epoch-44 batch-48
Running loss of epoch-44 batch-48 = 4.260893911123276e-05

Training epoch-44 batch-49
Running loss of epoch-44 batch-49 = 3.5049160942435265e-05

Training epoch-44 batch-50
Running loss of epoch-44 batch-50 = 4.2954692617058754e-05

Training epoch-44 batch-51
Running loss of epoch-44 batch-51 = 1.4975666999816895e-05

Training epoch-44 batch-52
Running loss of epoch-44 batch-52 = 2.0225765183568e-05

Training epoch-44 batch-53
Running loss of epoch-44 batch-53 = 1.8044840544462204e-05

Training epoch-44 batch-54
Running loss of epoch-44 batch-54 = 1.644901931285858e-05

Training epoch-44 batch-55
Running loss of epoch-44 batch-55 = 1.35740265250206e-05

Training epoch-44 batch-56
Running loss of epoch-44 batch-56 = 2.258620224893093e-05

Training epoch-44 batch-57
Running loss of epoch-44 batch-57 = 2.2141961380839348e-05

Training epoch-44 batch-58
Running loss of epoch-44 batch-58 = 1.1524185538291931e-05

Training epoch-44 batch-59
Running loss of epoch-44 batch-59 = 2.61794775724411e-05

Training epoch-44 batch-60
Running loss of epoch-44 batch-60 = 1.2556090950965881e-05

Training epoch-44 batch-61
Running loss of epoch-44 batch-61 = 1.7933081835508347e-05

Training epoch-44 batch-62
Running loss of epoch-44 batch-62 = 9.595649316906929e-06

Training epoch-44 batch-63
Running loss of epoch-44 batch-63 = 2.5278888642787933e-05

Training epoch-44 batch-64
Running loss of epoch-44 batch-64 = 5.916925147175789e-06

Training epoch-44 batch-65
Running loss of epoch-44 batch-65 = 1.2482516467571259e-05

Training epoch-44 batch-66
Running loss of epoch-44 batch-66 = 1.9385479390621185e-05

Training epoch-44 batch-67
Running loss of epoch-44 batch-67 = 4.166574217379093e-05

Training epoch-44 batch-68
Running loss of epoch-44 batch-68 = 2.8126640245318413e-05

Training epoch-44 batch-69
Running loss of epoch-44 batch-69 = 2.6747118681669235e-05

Training epoch-44 batch-70
Running loss of epoch-44 batch-70 = 3.081001341342926e-05

Training epoch-44 batch-71
Running loss of epoch-44 batch-71 = 2.344651147723198e-05

Training epoch-44 batch-72
Running loss of epoch-44 batch-72 = 7.887370884418488e-06

Training epoch-44 batch-73
Running loss of epoch-44 batch-73 = 1.7791753634810448e-05

Training epoch-44 batch-74
Running loss of epoch-44 batch-74 = 1.8621329218149185e-05

Training epoch-44 batch-75
Running loss of epoch-44 batch-75 = 1.7053913325071335e-05

Training epoch-44 batch-76
Running loss of epoch-44 batch-76 = 1.0990304872393608e-05

Training epoch-44 batch-77
Running loss of epoch-44 batch-77 = 1.5463680028915405e-05

Training epoch-44 batch-78
Running loss of epoch-44 batch-78 = 8.134054951369762e-05

Training epoch-44 batch-79
Running loss of epoch-44 batch-79 = 7.981434464454651e-06

Training epoch-44 batch-80
Running loss of epoch-44 batch-80 = 5.162716843187809e-05

Training epoch-44 batch-81
Running loss of epoch-44 batch-81 = 2.7283094823360443e-06

Training epoch-44 batch-82
Running loss of epoch-44 batch-82 = 1.2425240129232407e-05

Training epoch-44 batch-83
Running loss of epoch-44 batch-83 = 1.8808292225003242e-05

Training epoch-44 batch-84
Running loss of epoch-44 batch-84 = 9.134411811828613e-06

Training epoch-44 batch-85
Running loss of epoch-44 batch-85 = 2.0156148821115494e-05

Training epoch-44 batch-86
Running loss of epoch-44 batch-86 = 4.8005953431129456e-05

Training epoch-44 batch-87
Running loss of epoch-44 batch-87 = 5.730846896767616e-05

Training epoch-44 batch-88
Running loss of epoch-44 batch-88 = 2.466491423547268e-05

Training epoch-44 batch-89
Running loss of epoch-44 batch-89 = 1.8443679437041283e-05

Training epoch-44 batch-90
Running loss of epoch-44 batch-90 = 5.4471660405397415e-05

Training epoch-44 batch-91
Running loss of epoch-44 batch-91 = 3.317021764814854e-05

Training epoch-44 batch-92
Running loss of epoch-44 batch-92 = 2.7742935344576836e-05

Training epoch-44 batch-93
Running loss of epoch-44 batch-93 = 1.4935387298464775e-05

Training epoch-44 batch-94
Running loss of epoch-44 batch-94 = 1.6984762623906136e-05

Training epoch-44 batch-95
Running loss of epoch-44 batch-95 = 1.1088559404015541e-05

Training epoch-44 batch-96
Running loss of epoch-44 batch-96 = 5.3062569350004196e-05

Training epoch-44 batch-97
Running loss of epoch-44 batch-97 = 3.545847721397877e-05

Training epoch-44 batch-98
Running loss of epoch-44 batch-98 = 3.024912439286709e-05

Training epoch-44 batch-99
Running loss of epoch-44 batch-99 = 1.9594095647335052e-05

Training epoch-44 batch-100
Running loss of epoch-44 batch-100 = 1.432560384273529e-05

Training epoch-44 batch-101
Running loss of epoch-44 batch-101 = 4.047597758471966e-05

Training epoch-44 batch-102
Running loss of epoch-44 batch-102 = 1.697055995464325e-05

Training epoch-44 batch-103
Running loss of epoch-44 batch-103 = 1.8387334421277046e-05

Training epoch-44 batch-104
Running loss of epoch-44 batch-104 = 4.027620889246464e-05

Training epoch-44 batch-105
Running loss of epoch-44 batch-105 = 1.3960758224129677e-05

Training epoch-44 batch-106
Running loss of epoch-44 batch-106 = 2.8003938496112823e-05

Training epoch-44 batch-107
Running loss of epoch-44 batch-107 = 3.4815166145563126e-06

Training epoch-44 batch-108
Running loss of epoch-44 batch-108 = 8.000992238521576e-06

Training epoch-44 batch-109
Running loss of epoch-44 batch-109 = 4.315841943025589e-05

Training epoch-44 batch-110
Running loss of epoch-44 batch-110 = 8.790986612439156e-06

Training epoch-44 batch-111
Running loss of epoch-44 batch-111 = 2.8225360438227654e-05

Training epoch-44 batch-112
Running loss of epoch-44 batch-112 = 5.088117904961109e-05

Training epoch-44 batch-113
Running loss of epoch-44 batch-113 = 2.1580373868346214e-05

Training epoch-44 batch-114
Running loss of epoch-44 batch-114 = 1.846812665462494e-05

Training epoch-44 batch-115
Running loss of epoch-44 batch-115 = 8.890405297279358e-06

Training epoch-44 batch-116
Running loss of epoch-44 batch-116 = 7.103895768523216e-06

Training epoch-44 batch-117
Running loss of epoch-44 batch-117 = 6.937654688954353e-06

Training epoch-44 batch-118
Running loss of epoch-44 batch-118 = 3.859656862914562e-05

Training epoch-44 batch-119
Running loss of epoch-44 batch-119 = 1.9785016775131226e-05

Training epoch-44 batch-120
Running loss of epoch-44 batch-120 = 3.5677338019013405e-05

Training epoch-44 batch-121
Running loss of epoch-44 batch-121 = 9.714625775814056e-06

Training epoch-44 batch-122
Running loss of epoch-44 batch-122 = 5.4777367040514946e-05

Training epoch-44 batch-123
Running loss of epoch-44 batch-123 = 3.492715768516064e-05

Training epoch-44 batch-124
Running loss of epoch-44 batch-124 = 1.0963529348373413e-05

Training epoch-44 batch-125
Running loss of epoch-44 batch-125 = 1.6862060874700546e-05

Training epoch-44 batch-126
Running loss of epoch-44 batch-126 = 1.1685071513056755e-05

Training epoch-44 batch-127
Running loss of epoch-44 batch-127 = 1.2768898159265518e-05

Training epoch-44 batch-128
Running loss of epoch-44 batch-128 = 2.0186416804790497e-05

Training epoch-44 batch-129
Running loss of epoch-44 batch-129 = 1.3875076547265053e-05

Training epoch-44 batch-130
Running loss of epoch-44 batch-130 = 1.9141705706715584e-05

Training epoch-44 batch-131
Running loss of epoch-44 batch-131 = 4.789372906088829e-05

Training epoch-44 batch-132
Running loss of epoch-44 batch-132 = 1.6177305951714516e-05

Training epoch-44 batch-133
Running loss of epoch-44 batch-133 = 8.800998330116272e-06

Training epoch-44 batch-134
Running loss of epoch-44 batch-134 = 1.8197111785411835e-05

Training epoch-44 batch-135
Running loss of epoch-44 batch-135 = 1.4425022527575493e-05

Training epoch-44 batch-136
Running loss of epoch-44 batch-136 = 5.003530532121658e-06

Training epoch-44 batch-137
Running loss of epoch-44 batch-137 = 1.3045035302639008e-05

Training epoch-44 batch-138
Running loss of epoch-44 batch-138 = 3.112573176622391e-05

Training epoch-44 batch-139
Running loss of epoch-44 batch-139 = 2.4899141862988472e-05

Training epoch-44 batch-140
Running loss of epoch-44 batch-140 = 1.8512830138206482e-05

Training epoch-44 batch-141
Running loss of epoch-44 batch-141 = 3.072223626077175e-05

Training epoch-44 batch-142
Running loss of epoch-44 batch-142 = 2.8570415452122688e-05

Training epoch-44 batch-143
Running loss of epoch-44 batch-143 = 2.3341737687587738e-05

Training epoch-44 batch-144
Running loss of epoch-44 batch-144 = 3.678095526993275e-05

Training epoch-44 batch-145
Running loss of epoch-44 batch-145 = 2.7209986001253128e-05

Training epoch-44 batch-146
Running loss of epoch-44 batch-146 = 2.4804379791021347e-05

Training epoch-44 batch-147
Running loss of epoch-44 batch-147 = 2.06800177693367e-05

Training epoch-44 batch-148
Running loss of epoch-44 batch-148 = 1.705787144601345e-05

Training epoch-44 batch-149
Running loss of epoch-44 batch-149 = 1.8001534044742584e-05

Training epoch-44 batch-150
Running loss of epoch-44 batch-150 = 8.356524631381035e-06

Training epoch-44 batch-151
Running loss of epoch-44 batch-151 = 1.5897443518042564e-05

Training epoch-44 batch-152
Running loss of epoch-44 batch-152 = 5.69317489862442e-06

Training epoch-44 batch-153
Running loss of epoch-44 batch-153 = 1.0363291949033737e-05

Training epoch-44 batch-154
Running loss of epoch-44 batch-154 = 1.7592916265130043e-05

Training epoch-44 batch-155
Running loss of epoch-44 batch-155 = 1.9572442397475243e-05

Training epoch-44 batch-156
Running loss of epoch-44 batch-156 = 6.34931493550539e-05

Training epoch-44 batch-157
Running loss of epoch-44 batch-157 = 0.00023500993847846985

Finished training epoch-44.



Average train loss at epoch-44 = 2.5396797060966493e-05

Started Evaluation

Average val loss at epoch-44 = 1.5423095528319544

Accuracy for classes:
Accuracy for class equals is: 95.87 %
Accuracy for class main is: 95.90 %
Accuracy for class setUp is: 87.54 %
Accuracy for class onCreate is: 90.30 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 53.36 %
Accuracy for class execute is: 49.80 %
Accuracy for class get is: 63.59 %

Overall Accuracy = 81.25 %

Finished Evaluation



Started training epoch-45


Training epoch-45 batch-1
Running loss of epoch-45 batch-1 = 2.1238811314105988e-05

Training epoch-45 batch-2
Running loss of epoch-45 batch-2 = 1.8132850527763367e-05

Training epoch-45 batch-3
Running loss of epoch-45 batch-3 = 8.178874850273132e-06

Training epoch-45 batch-4
Running loss of epoch-45 batch-4 = 3.361678682267666e-05

Training epoch-45 batch-5
Running loss of epoch-45 batch-5 = 8.693663403391838e-06

Training epoch-45 batch-6
Running loss of epoch-45 batch-6 = 4.054722376167774e-05

Training epoch-45 batch-7
Running loss of epoch-45 batch-7 = 1.950794830918312e-05

Training epoch-45 batch-8
Running loss of epoch-45 batch-8 = 1.0104849934577942e-05

Training epoch-45 batch-9
Running loss of epoch-45 batch-9 = 1.6659963876008987e-05

Training epoch-45 batch-10
Running loss of epoch-45 batch-10 = 1.7035985365509987e-05

Training epoch-45 batch-11
Running loss of epoch-45 batch-11 = 2.4835113435983658e-05

Training epoch-45 batch-12
Running loss of epoch-45 batch-12 = 8.378410711884499e-05

Training epoch-45 batch-13
Running loss of epoch-45 batch-13 = 2.2819964215159416e-05

Training epoch-45 batch-14
Running loss of epoch-45 batch-14 = 1.215049996972084e-05

Training epoch-45 batch-15
Running loss of epoch-45 batch-15 = 2.663559280335903e-05

Training epoch-45 batch-16
Running loss of epoch-45 batch-16 = 1.92930456250906e-05

Training epoch-45 batch-17
Running loss of epoch-45 batch-17 = 4.373420961201191e-05

Training epoch-45 batch-18
Running loss of epoch-45 batch-18 = 2.3398082703351974e-05

Training epoch-45 batch-19
Running loss of epoch-45 batch-19 = 2.925936132669449e-05

Training epoch-45 batch-20
Running loss of epoch-45 batch-20 = 3.1850533559918404e-05

Training epoch-45 batch-21
Running loss of epoch-45 batch-21 = 1.0012881830334663e-05

Training epoch-45 batch-22
Running loss of epoch-45 batch-22 = 8.835922926664352e-06

Training epoch-45 batch-23
Running loss of epoch-45 batch-23 = 3.049825318157673e-05

Training epoch-45 batch-24
Running loss of epoch-45 batch-24 = 1.124851405620575e-05

Training epoch-45 batch-25
Running loss of epoch-45 batch-25 = 1.0863179340958595e-05

Training epoch-45 batch-26
Running loss of epoch-45 batch-26 = 1.1405209079384804e-05

Training epoch-45 batch-27
Running loss of epoch-45 batch-27 = 1.3709068298339844e-05

Training epoch-45 batch-28
Running loss of epoch-45 batch-28 = 5.274219438433647e-05

Training epoch-45 batch-29
Running loss of epoch-45 batch-29 = 1.59386545419693e-05

Training epoch-45 batch-30
Running loss of epoch-45 batch-30 = 1.0858522728085518e-05

Training epoch-45 batch-31
Running loss of epoch-45 batch-31 = 2.9448652639985085e-05

Training epoch-45 batch-32
Running loss of epoch-45 batch-32 = 2.5235582143068314e-05

Training epoch-45 batch-33
Running loss of epoch-45 batch-33 = 6.294110789895058e-06

Training epoch-45 batch-34
Running loss of epoch-45 batch-34 = 2.5008106604218483e-05

Training epoch-45 batch-35
Running loss of epoch-45 batch-35 = 2.1048355847597122e-05

Training epoch-45 batch-36
Running loss of epoch-45 batch-36 = 1.3586599379777908e-05

Training epoch-45 batch-37
Running loss of epoch-45 batch-37 = 2.408679574728012e-05

Training epoch-45 batch-38
Running loss of epoch-45 batch-38 = 7.679173722863197e-05

Training epoch-45 batch-39
Running loss of epoch-45 batch-39 = 6.5730419009923935e-06

Training epoch-45 batch-40
Running loss of epoch-45 batch-40 = 4.669278860092163e-05

Training epoch-45 batch-41
Running loss of epoch-45 batch-41 = 1.1521857231855392e-05

Training epoch-45 batch-42
Running loss of epoch-45 batch-42 = 2.1855579689145088e-05

Training epoch-45 batch-43
Running loss of epoch-45 batch-43 = 1.2895558029413223e-05

Training epoch-45 batch-44
Running loss of epoch-45 batch-44 = 9.924406185746193e-06

Training epoch-45 batch-45
Running loss of epoch-45 batch-45 = 7.4536073952913284e-06

Training epoch-45 batch-46
Running loss of epoch-45 batch-46 = 1.0033603757619858e-05

Training epoch-45 batch-47
Running loss of epoch-45 batch-47 = 5.030538886785507e-06

Training epoch-45 batch-48
Running loss of epoch-45 batch-48 = 3.3860327675938606e-05

Training epoch-45 batch-49
Running loss of epoch-45 batch-49 = 1.2726755812764168e-05

Training epoch-45 batch-50
Running loss of epoch-45 batch-50 = 5.422625690698624e-06

Training epoch-45 batch-51
Running loss of epoch-45 batch-51 = 8.450588211417198e-06

Training epoch-45 batch-52
Running loss of epoch-45 batch-52 = 1.1662254109978676e-05

Training epoch-45 batch-53
Running loss of epoch-45 batch-53 = 1.2069707736372948e-05

Training epoch-45 batch-54
Running loss of epoch-45 batch-54 = 8.91159288585186e-06

Training epoch-45 batch-55
Running loss of epoch-45 batch-55 = 0.00010627042502164841

Training epoch-45 batch-56
Running loss of epoch-45 batch-56 = 4.314235411584377e-05

Training epoch-45 batch-57
Running loss of epoch-45 batch-57 = 1.2753764167428017e-05

Training epoch-45 batch-58
Running loss of epoch-45 batch-58 = 3.577442839741707e-05

Training epoch-45 batch-59
Running loss of epoch-45 batch-59 = 2.4591805413365364e-05

Training epoch-45 batch-60
Running loss of epoch-45 batch-60 = 2.3429980501532555e-05

Training epoch-45 batch-61
Running loss of epoch-45 batch-61 = 1.4545395970344543e-05

Training epoch-45 batch-62
Running loss of epoch-45 batch-62 = 2.162228338420391e-05

Training epoch-45 batch-63
Running loss of epoch-45 batch-63 = 1.6183825209736824e-05

Training epoch-45 batch-64
Running loss of epoch-45 batch-64 = 1.3734912499785423e-05

Training epoch-45 batch-65
Running loss of epoch-45 batch-65 = 3.326288424432278e-05

Training epoch-45 batch-66
Running loss of epoch-45 batch-66 = 1.7397571355104446e-05

Training epoch-45 batch-67
Running loss of epoch-45 batch-67 = 4.165619611740112e-05

Training epoch-45 batch-68
Running loss of epoch-45 batch-68 = 2.160901203751564e-05

Training epoch-45 batch-69
Running loss of epoch-45 batch-69 = 1.6761943697929382e-05

Training epoch-45 batch-70
Running loss of epoch-45 batch-70 = 5.635176785290241e-05

Training epoch-45 batch-71
Running loss of epoch-45 batch-71 = 3.3461255952715874e-05

Training epoch-45 batch-72
Running loss of epoch-45 batch-72 = 6.990740075707436e-06

Training epoch-45 batch-73
Running loss of epoch-45 batch-73 = 2.4178531020879745e-05

Training epoch-45 batch-74
Running loss of epoch-45 batch-74 = 3.805849701166153e-06

Training epoch-45 batch-75
Running loss of epoch-45 batch-75 = 4.4941436499357224e-05

Training epoch-45 batch-76
Running loss of epoch-45 batch-76 = 1.2751203030347824e-05

Training epoch-45 batch-77
Running loss of epoch-45 batch-77 = 1.8605031073093414e-05

Training epoch-45 batch-78
Running loss of epoch-45 batch-78 = 6.444717291742563e-05

Training epoch-45 batch-79
Running loss of epoch-45 batch-79 = 2.8869835659861565e-05

Training epoch-45 batch-80
Running loss of epoch-45 batch-80 = 1.7230166122317314e-05

Training epoch-45 batch-81
Running loss of epoch-45 batch-81 = 1.7317011952400208e-05

Training epoch-45 batch-82
Running loss of epoch-45 batch-82 = 1.471489667892456e-05

Training epoch-45 batch-83
Running loss of epoch-45 batch-83 = 3.260420635342598e-05

Training epoch-45 batch-84
Running loss of epoch-45 batch-84 = 3.1815143302083015e-05

Training epoch-45 batch-85
Running loss of epoch-45 batch-85 = 0.00012966664507985115

Training epoch-45 batch-86
Running loss of epoch-45 batch-86 = 1.529557630419731e-05

Training epoch-45 batch-87
Running loss of epoch-45 batch-87 = 1.688464544713497e-05

Training epoch-45 batch-88
Running loss of epoch-45 batch-88 = 9.607523679733276e-06

Training epoch-45 batch-89
Running loss of epoch-45 batch-89 = 1.2608710676431656e-05

Training epoch-45 batch-90
Running loss of epoch-45 batch-90 = 2.6623252779245377e-05

Training epoch-45 batch-91
Running loss of epoch-45 batch-91 = 1.0053860023617744e-05

Training epoch-45 batch-92
Running loss of epoch-45 batch-92 = 1.7996178939938545e-05

Training epoch-45 batch-93
Running loss of epoch-45 batch-93 = 9.334040805697441e-05

Training epoch-45 batch-94
Running loss of epoch-45 batch-94 = 3.941473551094532e-05

Training epoch-45 batch-95
Running loss of epoch-45 batch-95 = 2.1569663658738136e-05

Training epoch-45 batch-96
Running loss of epoch-45 batch-96 = 1.9374769181013107e-05

Training epoch-45 batch-97
Running loss of epoch-45 batch-97 = 5.522044375538826e-06

Training epoch-45 batch-98
Running loss of epoch-45 batch-98 = 1.8903054296970367e-05

Training epoch-45 batch-99
Running loss of epoch-45 batch-99 = 2.577202394604683e-05

Training epoch-45 batch-100
Running loss of epoch-45 batch-100 = 2.0803650841116905e-05

Training epoch-45 batch-101
Running loss of epoch-45 batch-101 = 1.8493039533495903e-05

Training epoch-45 batch-102
Running loss of epoch-45 batch-102 = 6.13601878285408e-06

Training epoch-45 batch-103
Running loss of epoch-45 batch-103 = 1.3378914445638657e-05

Training epoch-45 batch-104
Running loss of epoch-45 batch-104 = 3.383657895028591e-05

Training epoch-45 batch-105
Running loss of epoch-45 batch-105 = 8.815713226795197e-05

Training epoch-45 batch-106
Running loss of epoch-45 batch-106 = 4.72506508231163e-05

Training epoch-45 batch-107
Running loss of epoch-45 batch-107 = 1.3511627912521362e-05

Training epoch-45 batch-108
Running loss of epoch-45 batch-108 = 1.1196592822670937e-05

Training epoch-45 batch-109
Running loss of epoch-45 batch-109 = 7.0766545832157135e-06

Training epoch-45 batch-110
Running loss of epoch-45 batch-110 = 1.4909077435731888e-05

Training epoch-45 batch-111
Running loss of epoch-45 batch-111 = 7.4964482337236404e-06

Training epoch-45 batch-112
Running loss of epoch-45 batch-112 = 1.5661120414733887e-05

Training epoch-45 batch-113
Running loss of epoch-45 batch-113 = 1.0129064321517944e-05

Training epoch-45 batch-114
Running loss of epoch-45 batch-114 = 1.1279946193099022e-05

Training epoch-45 batch-115
Running loss of epoch-45 batch-115 = 3.2897572964429855e-05

Training epoch-45 batch-116
Running loss of epoch-45 batch-116 = 1.6207806766033173e-05

Training epoch-45 batch-117
Running loss of epoch-45 batch-117 = 1.576961949467659e-05

Training epoch-45 batch-118
Running loss of epoch-45 batch-118 = 6.621470674872398e-06

Training epoch-45 batch-119
Running loss of epoch-45 batch-119 = 5.4667703807353973e-05

Training epoch-45 batch-120
Running loss of epoch-45 batch-120 = 2.581533044576645e-05

Training epoch-45 batch-121
Running loss of epoch-45 batch-121 = 1.4708610251545906e-05

Training epoch-45 batch-122
Running loss of epoch-45 batch-122 = 2.8461450710892677e-05

Training epoch-45 batch-123
Running loss of epoch-45 batch-123 = 8.819857612252235e-06

Training epoch-45 batch-124
Running loss of epoch-45 batch-124 = 1.4864839613437653e-05

Training epoch-45 batch-125
Running loss of epoch-45 batch-125 = 2.2496795281767845e-05

Training epoch-45 batch-126
Running loss of epoch-45 batch-126 = 4.417169839143753e-05

Training epoch-45 batch-127
Running loss of epoch-45 batch-127 = 2.0767096430063248e-05

Training epoch-45 batch-128
Running loss of epoch-45 batch-128 = 1.8368475139141083e-05

Training epoch-45 batch-129
Running loss of epoch-45 batch-129 = 1.07642263174057e-05

Training epoch-45 batch-130
Running loss of epoch-45 batch-130 = 1.808791421353817e-05

Training epoch-45 batch-131
Running loss of epoch-45 batch-131 = 2.896226942539215e-05

Training epoch-45 batch-132
Running loss of epoch-45 batch-132 = 6.246846169233322e-06

Training epoch-45 batch-133
Running loss of epoch-45 batch-133 = 1.8147286027669907e-05

Training epoch-45 batch-134
Running loss of epoch-45 batch-134 = 7.743015885353088e-06

Training epoch-45 batch-135
Running loss of epoch-45 batch-135 = 1.63146760314703e-05

Training epoch-45 batch-136
Running loss of epoch-45 batch-136 = 1.1884607374668121e-05

Training epoch-45 batch-137
Running loss of epoch-45 batch-137 = 2.1305866539478302e-05

Training epoch-45 batch-138
Running loss of epoch-45 batch-138 = 1.7798040062189102e-05

Training epoch-45 batch-139
Running loss of epoch-45 batch-139 = 4.430091939866543e-05

Training epoch-45 batch-140
Running loss of epoch-45 batch-140 = 5.279667675495148e-06

Training epoch-45 batch-141
Running loss of epoch-45 batch-141 = 1.1344440281391144e-05

Training epoch-45 batch-142
Running loss of epoch-45 batch-142 = 3.524427302181721e-05

Training epoch-45 batch-143
Running loss of epoch-45 batch-143 = 1.2660864740610123e-05

Training epoch-45 batch-144
Running loss of epoch-45 batch-144 = 1.995684579014778e-05

Training epoch-45 batch-145
Running loss of epoch-45 batch-145 = 7.199356332421303e-06

Training epoch-45 batch-146
Running loss of epoch-45 batch-146 = 2.5466782972216606e-05

Training epoch-45 batch-147
Running loss of epoch-45 batch-147 = 7.23167322576046e-05

Training epoch-45 batch-148
Running loss of epoch-45 batch-148 = 3.639794886112213e-05

Training epoch-45 batch-149
Running loss of epoch-45 batch-149 = 2.300902269780636e-05

Training epoch-45 batch-150
Running loss of epoch-45 batch-150 = 5.8027682825922966e-05

Training epoch-45 batch-151
Running loss of epoch-45 batch-151 = 5.084089934825897e-06

Training epoch-45 batch-152
Running loss of epoch-45 batch-152 = 2.446933649480343e-05

Training epoch-45 batch-153
Running loss of epoch-45 batch-153 = 3.71329952031374e-05

Training epoch-45 batch-154
Running loss of epoch-45 batch-154 = 1.0650604963302612e-05

Training epoch-45 batch-155
Running loss of epoch-45 batch-155 = 1.550186425447464e-05

Training epoch-45 batch-156
Running loss of epoch-45 batch-156 = 3.898586146533489e-05

Training epoch-45 batch-157
Running loss of epoch-45 batch-157 = 6.23464584350586e-05

Finished training epoch-45.



Average train loss at epoch-45 = 2.3983707278966904e-05

Started Evaluation

Average val loss at epoch-45 = 1.5438614506834998

Accuracy for classes:
Accuracy for class equals is: 96.20 %
Accuracy for class main is: 95.57 %
Accuracy for class setUp is: 87.05 %
Accuracy for class onCreate is: 90.30 %
Accuracy for class toString is: 83.62 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 54.71 %
Accuracy for class execute is: 45.78 %
Accuracy for class get is: 64.10 %

Overall Accuracy = 81.14 %

Finished Evaluation



Started training epoch-46


Training epoch-46 batch-1
Running loss of epoch-46 batch-1 = 1.2976815924048424e-05

Training epoch-46 batch-2
Running loss of epoch-46 batch-2 = 2.3018568754196167e-05

Training epoch-46 batch-3
Running loss of epoch-46 batch-3 = 2.2885389626026154e-05

Training epoch-46 batch-4
Running loss of epoch-46 batch-4 = 1.9091414287686348e-05

Training epoch-46 batch-5
Running loss of epoch-46 batch-5 = 1.1076685041189194e-05

Training epoch-46 batch-6
Running loss of epoch-46 batch-6 = 2.6529887691140175e-05

Training epoch-46 batch-7
Running loss of epoch-46 batch-7 = 2.2603431716561317e-05

Training epoch-46 batch-8
Running loss of epoch-46 batch-8 = 2.1298183128237724e-05

Training epoch-46 batch-9
Running loss of epoch-46 batch-9 = 1.4898832887411118e-05

Training epoch-46 batch-10
Running loss of epoch-46 batch-10 = 3.079441376030445e-05

Training epoch-46 batch-11
Running loss of epoch-46 batch-11 = 1.7847400158643723e-05

Training epoch-46 batch-12
Running loss of epoch-46 batch-12 = 6.295507773756981e-06

Training epoch-46 batch-13
Running loss of epoch-46 batch-13 = 3.11192125082016e-05

Training epoch-46 batch-14
Running loss of epoch-46 batch-14 = 2.3329397663474083e-05

Training epoch-46 batch-15
Running loss of epoch-46 batch-15 = 1.9598985090851784e-05

Training epoch-46 batch-16
Running loss of epoch-46 batch-16 = 2.8565991669893265e-05

Training epoch-46 batch-17
Running loss of epoch-46 batch-17 = 4.7432491555809975e-05

Training epoch-46 batch-18
Running loss of epoch-46 batch-18 = 2.133590169250965e-05

Training epoch-46 batch-19
Running loss of epoch-46 batch-19 = 2.00511422008276e-05

Training epoch-46 batch-20
Running loss of epoch-46 batch-20 = 6.919773295521736e-05

Training epoch-46 batch-21
Running loss of epoch-46 batch-21 = 3.0280323699116707e-05

Training epoch-46 batch-22
Running loss of epoch-46 batch-22 = 1.6965670511126518e-05

Training epoch-46 batch-23
Running loss of epoch-46 batch-23 = 1.6037840396165848e-05

Training epoch-46 batch-24
Running loss of epoch-46 batch-24 = 1.2804055586457253e-05

Training epoch-46 batch-25
Running loss of epoch-46 batch-25 = 2.204766497015953e-05

Training epoch-46 batch-26
Running loss of epoch-46 batch-26 = 1.1839903891086578e-05

Training epoch-46 batch-27
Running loss of epoch-46 batch-27 = 1.6499776393175125e-05

Training epoch-46 batch-28
Running loss of epoch-46 batch-28 = 1.3972166925668716e-05

Training epoch-46 batch-29
Running loss of epoch-46 batch-29 = 1.1041993275284767e-05

Training epoch-46 batch-30
Running loss of epoch-46 batch-30 = 1.3326527550816536e-05

Training epoch-46 batch-31
Running loss of epoch-46 batch-31 = 1.4461344107985497e-05

Training epoch-46 batch-32
Running loss of epoch-46 batch-32 = 9.460141882300377e-06

Training epoch-46 batch-33
Running loss of epoch-46 batch-33 = 1.3823620975017548e-05

Training epoch-46 batch-34
Running loss of epoch-46 batch-34 = 7.809838280081749e-06

Training epoch-46 batch-35
Running loss of epoch-46 batch-35 = 1.2292992323637009e-05

Training epoch-46 batch-36
Running loss of epoch-46 batch-36 = 2.843700349330902e-05

Training epoch-46 batch-37
Running loss of epoch-46 batch-37 = 4.2587751522660255e-05

Training epoch-46 batch-38
Running loss of epoch-46 batch-38 = 1.8562888726592064e-05

Training epoch-46 batch-39
Running loss of epoch-46 batch-39 = 1.640012487769127e-05

Training epoch-46 batch-40
Running loss of epoch-46 batch-40 = 1.6294652596116066e-05

Training epoch-46 batch-41
Running loss of epoch-46 batch-41 = 1.0301591828465462e-05

Training epoch-46 batch-42
Running loss of epoch-46 batch-42 = 1.152954064309597e-05

Training epoch-46 batch-43
Running loss of epoch-46 batch-43 = 4.686112515628338e-05

Training epoch-46 batch-44
Running loss of epoch-46 batch-44 = 1.887488178908825e-05

Training epoch-46 batch-45
Running loss of epoch-46 batch-45 = 1.733703538775444e-05

Training epoch-46 batch-46
Running loss of epoch-46 batch-46 = 2.2311927750706673e-05

Training epoch-46 batch-47
Running loss of epoch-46 batch-47 = 2.2894004359841347e-05

Training epoch-46 batch-48
Running loss of epoch-46 batch-48 = 2.5888904929161072e-05

Training epoch-46 batch-49
Running loss of epoch-46 batch-49 = 1.1845957487821579e-05

Training epoch-46 batch-50
Running loss of epoch-46 batch-50 = 2.2008083760738373e-05

Training epoch-46 batch-51
Running loss of epoch-46 batch-51 = 2.55717895925045e-05

Training epoch-46 batch-52
Running loss of epoch-46 batch-52 = 1.553259789943695e-05

Training epoch-46 batch-53
Running loss of epoch-46 batch-53 = 1.3475073501467705e-05

Training epoch-46 batch-54
Running loss of epoch-46 batch-54 = 1.788395456969738e-05

Training epoch-46 batch-55
Running loss of epoch-46 batch-55 = 8.222879841923714e-06

Training epoch-46 batch-56
Running loss of epoch-46 batch-56 = 2.7196481823921204e-05

Training epoch-46 batch-57
Running loss of epoch-46 batch-57 = 1.1308351531624794e-05

Training epoch-46 batch-58
Running loss of epoch-46 batch-58 = 3.703986294567585e-05

Training epoch-46 batch-59
Running loss of epoch-46 batch-59 = 2.0105857402086258e-05

Training epoch-46 batch-60
Running loss of epoch-46 batch-60 = 2.105417661368847e-05

Training epoch-46 batch-61
Running loss of epoch-46 batch-61 = 5.160924047231674e-06

Training epoch-46 batch-62
Running loss of epoch-46 batch-62 = 8.902279660105705e-06

Training epoch-46 batch-63
Running loss of epoch-46 batch-63 = 1.8031802028417587e-05

Training epoch-46 batch-64
Running loss of epoch-46 batch-64 = 1.6689999029040337e-05

Training epoch-46 batch-65
Running loss of epoch-46 batch-65 = 1.7516082152724266e-05

Training epoch-46 batch-66
Running loss of epoch-46 batch-66 = 2.2690044716000557e-05

Training epoch-46 batch-67
Running loss of epoch-46 batch-67 = 8.882489055395126e-06

Training epoch-46 batch-68
Running loss of epoch-46 batch-68 = 6.734626367688179e-06

Training epoch-46 batch-69
Running loss of epoch-46 batch-69 = 1.3584503903985023e-05

Training epoch-46 batch-70
Running loss of epoch-46 batch-70 = 1.8159626051783562e-05

Training epoch-46 batch-71
Running loss of epoch-46 batch-71 = 5.9856800362467766e-05

Training epoch-46 batch-72
Running loss of epoch-46 batch-72 = 2.6304274797439575e-05

Training epoch-46 batch-73
Running loss of epoch-46 batch-73 = 6.88573345541954e-06

Training epoch-46 batch-74
Running loss of epoch-46 batch-74 = 1.9149156287312508e-05

Training epoch-46 batch-75
Running loss of epoch-46 batch-75 = 1.1379597708582878e-05

Training epoch-46 batch-76
Running loss of epoch-46 batch-76 = 1.4282995834946632e-05

Training epoch-46 batch-77
Running loss of epoch-46 batch-77 = 5.591427907347679e-06

Training epoch-46 batch-78
Running loss of epoch-46 batch-78 = 1.807231456041336e-05

Training epoch-46 batch-79
Running loss of epoch-46 batch-79 = 1.4278339222073555e-05

Training epoch-46 batch-80
Running loss of epoch-46 batch-80 = 4.946952685713768e-06

Training epoch-46 batch-81
Running loss of epoch-46 batch-81 = 3.940309397876263e-05

Training epoch-46 batch-82
Running loss of epoch-46 batch-82 = 7.5320713222026825e-06

Training epoch-46 batch-83
Running loss of epoch-46 batch-83 = 2.2817403078079224e-05

Training epoch-46 batch-84
Running loss of epoch-46 batch-84 = 3.80626879632473e-05

Training epoch-46 batch-85
Running loss of epoch-46 batch-85 = 5.871010944247246e-05

Training epoch-46 batch-86
Running loss of epoch-46 batch-86 = 8.378177881240845e-06

Training epoch-46 batch-87
Running loss of epoch-46 batch-87 = 1.1136522516608238e-05

Training epoch-46 batch-88
Running loss of epoch-46 batch-88 = 3.912579268217087e-05

Training epoch-46 batch-89
Running loss of epoch-46 batch-89 = 4.893657751381397e-05

Training epoch-46 batch-90
Running loss of epoch-46 batch-90 = 1.2503936886787415e-05

Training epoch-46 batch-91
Running loss of epoch-46 batch-91 = 1.4638528227806091e-05

Training epoch-46 batch-92
Running loss of epoch-46 batch-92 = 1.770094968378544e-05

Training epoch-46 batch-93
Running loss of epoch-46 batch-93 = 1.57221220433712e-05

Training epoch-46 batch-94
Running loss of epoch-46 batch-94 = 1.3515353202819824e-05

Training epoch-46 batch-95
Running loss of epoch-46 batch-95 = 1.072743907570839e-05

Training epoch-46 batch-96
Running loss of epoch-46 batch-96 = 2.6258639991283417e-05

Training epoch-46 batch-97
Running loss of epoch-46 batch-97 = 2.6769470423460007e-05

Training epoch-46 batch-98
Running loss of epoch-46 batch-98 = 5.854526534676552e-06

Training epoch-46 batch-99
Running loss of epoch-46 batch-99 = 2.3392029106616974e-05

Training epoch-46 batch-100
Running loss of epoch-46 batch-100 = 1.486344262957573e-05

Training epoch-46 batch-101
Running loss of epoch-46 batch-101 = 1.5618279576301575e-05

Training epoch-46 batch-102
Running loss of epoch-46 batch-102 = 1.2502772733569145e-05

Training epoch-46 batch-103
Running loss of epoch-46 batch-103 = 7.809139788150787e-06

Training epoch-46 batch-104
Running loss of epoch-46 batch-104 = 2.403627149760723e-05

Training epoch-46 batch-105
Running loss of epoch-46 batch-105 = 1.0220101103186607e-05

Training epoch-46 batch-106
Running loss of epoch-46 batch-106 = 1.871306449174881e-05

Training epoch-46 batch-107
Running loss of epoch-46 batch-107 = 9.497161954641342e-06

Training epoch-46 batch-108
Running loss of epoch-46 batch-108 = 8.666911162436008e-05

Training epoch-46 batch-109
Running loss of epoch-46 batch-109 = 7.73649662733078e-06

Training epoch-46 batch-110
Running loss of epoch-46 batch-110 = 1.4603370800614357e-05

Training epoch-46 batch-111
Running loss of epoch-46 batch-111 = 7.634749636054039e-06

Training epoch-46 batch-112
Running loss of epoch-46 batch-112 = 1.4847144484519958e-05

Training epoch-46 batch-113
Running loss of epoch-46 batch-113 = 1.8251826986670494e-05

Training epoch-46 batch-114
Running loss of epoch-46 batch-114 = 2.8902897611260414e-05

Training epoch-46 batch-115
Running loss of epoch-46 batch-115 = 6.519956514239311e-06

Training epoch-46 batch-116
Running loss of epoch-46 batch-116 = 1.5809200704097748e-05

Training epoch-46 batch-117
Running loss of epoch-46 batch-117 = 6.791669875383377e-06

Training epoch-46 batch-118
Running loss of epoch-46 batch-118 = 3.910670056939125e-05

Training epoch-46 batch-119
Running loss of epoch-46 batch-119 = 1.4054123312234879e-05

Training epoch-46 batch-120
Running loss of epoch-46 batch-120 = 1.6016187146306038e-05

Training epoch-46 batch-121
Running loss of epoch-46 batch-121 = 2.573360688984394e-05

Training epoch-46 batch-122
Running loss of epoch-46 batch-122 = 2.2562919184565544e-05

Training epoch-46 batch-123
Running loss of epoch-46 batch-123 = 4.269881173968315e-05

Training epoch-46 batch-124
Running loss of epoch-46 batch-124 = 5.809823051095009e-06

Training epoch-46 batch-125
Running loss of epoch-46 batch-125 = 1.523829996585846e-05

Training epoch-46 batch-126
Running loss of epoch-46 batch-126 = 2.4173874408006668e-05

Training epoch-46 batch-127
Running loss of epoch-46 batch-127 = 2.5740591809153557e-05

Training epoch-46 batch-128
Running loss of epoch-46 batch-128 = 1.1719996109604836e-05

Training epoch-46 batch-129
Running loss of epoch-46 batch-129 = 1.0067597031593323e-05

Training epoch-46 batch-130
Running loss of epoch-46 batch-130 = 1.672212965786457e-05

Training epoch-46 batch-131
Running loss of epoch-46 batch-131 = 0.00011803954839706421

Training epoch-46 batch-132
Running loss of epoch-46 batch-132 = 2.206140197813511e-05

Training epoch-46 batch-133
Running loss of epoch-46 batch-133 = 1.1545838788151741e-05

Training epoch-46 batch-134
Running loss of epoch-46 batch-134 = 3.848341293632984e-05

Training epoch-46 batch-135
Running loss of epoch-46 batch-135 = 5.757901817560196e-06

Training epoch-46 batch-136
Running loss of epoch-46 batch-136 = 1.3976823538541794e-05

Training epoch-46 batch-137
Running loss of epoch-46 batch-137 = 1.4336314052343369e-05

Training epoch-46 batch-138
Running loss of epoch-46 batch-138 = 1.3241544365882874e-05

Training epoch-46 batch-139
Running loss of epoch-46 batch-139 = 2.4287262931466103e-05

Training epoch-46 batch-140
Running loss of epoch-46 batch-140 = 2.8206966817378998e-05

Training epoch-46 batch-141
Running loss of epoch-46 batch-141 = 5.44288195669651e-06

Training epoch-46 batch-142
Running loss of epoch-46 batch-142 = 1.3817567378282547e-05

Training epoch-46 batch-143
Running loss of epoch-46 batch-143 = 5.388772115111351e-05

Training epoch-46 batch-144
Running loss of epoch-46 batch-144 = 7.484573870897293e-06

Training epoch-46 batch-145
Running loss of epoch-46 batch-145 = 1.1427793651819229e-05

Training epoch-46 batch-146
Running loss of epoch-46 batch-146 = 1.2684380635619164e-05

Training epoch-46 batch-147
Running loss of epoch-46 batch-147 = 2.7796020731329918e-05

Training epoch-46 batch-148
Running loss of epoch-46 batch-148 = 4.259101115167141e-05

Training epoch-46 batch-149
Running loss of epoch-46 batch-149 = 3.291061148047447e-06

Training epoch-46 batch-150
Running loss of epoch-46 batch-150 = 1.0856660082936287e-05

Training epoch-46 batch-151
Running loss of epoch-46 batch-151 = 9.499490261077881e-06

Training epoch-46 batch-152
Running loss of epoch-46 batch-152 = 2.208608202636242e-05

Training epoch-46 batch-153
Running loss of epoch-46 batch-153 = 1.3059703633189201e-05

Training epoch-46 batch-154
Running loss of epoch-46 batch-154 = 5.611451342701912e-06

Training epoch-46 batch-155
Running loss of epoch-46 batch-155 = 0.00010902131907641888

Training epoch-46 batch-156
Running loss of epoch-46 batch-156 = 3.5953475162386894e-05

Training epoch-46 batch-157
Running loss of epoch-46 batch-157 = 0.00010363757610321045

Finished training epoch-46.



Average train loss at epoch-46 = 2.1065293252468108e-05

Started Evaluation

Average val loss at epoch-46 = 1.5597526543797358

Accuracy for classes:
Accuracy for class equals is: 96.04 %
Accuracy for class main is: 95.74 %
Accuracy for class setUp is: 86.23 %
Accuracy for class onCreate is: 89.98 %
Accuracy for class toString is: 83.96 %
Accuracy for class run is: 62.56 %
Accuracy for class hashCode is: 98.13 %
Accuracy for class init is: 55.38 %
Accuracy for class execute is: 44.98 %
Accuracy for class get is: 65.64 %

Overall Accuracy = 81.14 %

Finished Evaluation
